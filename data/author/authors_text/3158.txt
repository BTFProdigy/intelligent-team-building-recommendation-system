Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 127?135,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Multilingual Subjectivity Analysis Using Machine Translation
Carmen Banea and Rada Mihalcea
University of North Texas
carmenb@unt.edu, rada@cs.unt.edu
Janyce Wiebe
University of Pittsburgh
wiebe@cs.pitt.edu
Samer Hassan
University of North Texas
samer@unt.edu
Abstract
Although research in other languages is in-
creasing, much of the work in subjectivity
analysis has been applied to English data,
mainly due to the large body of electronic re-
sources and tools that are available for this lan-
guage. In this paper, we propose and evalu-
ate methods that can be employed to transfer a
repository of subjectivity resources across lan-
guages. Specifically, we attempt to leverage
on the resources available for English and, by
employing machine translation, generate re-
sources for subjectivity analysis in other lan-
guages. Through comparative evaluations on
two different languages (Romanian and Span-
ish), we show that automatic translation is a
viable alternative for the construction of re-
sources and tools for subjectivity analysis in
a new target language.
1 Introduction
We have seen a surge in interest towards the ap-
plication of automatic tools and techniques for the
extraction of opinions, emotions, and sentiments in
text (subjectivity). A large number of text process-
ing applications have already employed techniques
for automatic subjectivity analysis, including auto-
matic expressive text-to-speech synthesis (Alm et
al., 2005), text semantic analysis (Wiebe and Mihal-
cea, 2006; Esuli and Sebastiani, 2006), tracking sen-
timent timelines in on-line forums and news (Lloyd
et al, 2005; Balog et al, 2006), mining opinions
from product reviews (Hu and Liu, 2004), and ques-
tion answering (Yu and Hatzivassiloglou, 2003).
A significant fraction of the research work to date
in subjectivity analysis has been applied to English,
which led to several resources and tools available for
this language. In this paper, we explore multiple
paths that employ machine translation while lever-
aging on the resources and tools available for En-
glish, to automatically generate resources for sub-
jectivity analysis for a new target language. Through
experiments carried out with automatic translation
and cross-lingual projections of subjectivity annota-
tions, we try to answer the following questions.
First, assuming an English corpus manually an-
notated for subjectivity, can we use machine trans-
lation to generate a subjectivity-annotated corpus in
the target language? Second, assuming the availabil-
ity of a tool for automatic subjectivity analysis in
English, can we generate a corpus annotated for sub-
jectivity in the target language by using automatic
subjectivity annotations of English text and machine
translation? Finally, third, can these automatically
generated resources be used to effectively train tools
for subjectivity analysis in the target language?
Since our methods are particularly useful for lan-
guages with only a few electronic tools and re-
sources, we chose to conduct our initial experiments
on Romanian, a language with limited text process-
ing resources developed to date. Furthermore, to
validate our results, we carried a second set of ex-
periments on Spanish. Note however that our meth-
ods do not make use of any target language specific
knowledge, and thus they are applicable to any other
language as long as a machine translation engine ex-
ists between the selected language and English.
127
2 Related Work
Research in sentiment and subjectivity analysis has
received increasingly growing interest from the nat-
ural language processing community, particularly
motivated by the widespread need for opinion-based
applications, including product and movie reviews,
entity tracking and analysis, opinion summarization,
and others.
Much of the work in subjectivity analysis has
been applied to English data, though work on other
languages is growing: e.g., Japanese data are used
in (Kobayashi et al, 2004; Suzuki et al, 2006;
Takamura et al, 2006; Kanayama and Nasukawa,
2006), Chinese data are used in (Hu et al, 2005),
and German data are used in (Kim and Hovy, 2006).
In addition, several participants in the Chinese
and Japanese Opinion Extraction tasks of NTCIR-
6 (Kando and Evans, 2007) performed subjectivity
and sentiment analysis in languages other than En-
glish.
In general, efforts on building subjectivity analy-
sis tools for other languages have been hampered by
the high cost involved in creating corpora and lexical
resources for a new language. To address this gap,
we focus on leveraging resources already developed
for one language to derive subjectivity analysis tools
for a new language. This motivates the direction of
our research, in which we use machine translation
coupled with cross-lingual annotation projections to
generate the resources and tools required to perform
subjectivity classification in the target language.
The work closest to ours is the one reported in
(Mihalcea et al, 2007), where a bilingual lexicon
and a manually translated parallel text are used to
generate the resources required to build a subjectiv-
ity classifier in a new language. In that work, we
found that the projection of annotations across par-
allel texts can be successfully used to build a cor-
pus annotated for subjectivity in the target language.
However, parallel texts are not always available for
a given language pair. Therefore, in this paper we
explore a different approach where, instead of rely-
ing on manually translated parallel corpora, we use
machine translation to produce a corpus in the new
language.
3 Machine Translation for Subjectivity
Analysis
We explore the possibility of using machine transla-
tion to generate the resources required to build sub-
jectivity annotation tools in a given target language.
We focus on two main scenarios. First, assuming a
corpus manually annotated for subjectivity exists in
the source language, we can use machine translation
to create a corpus annotated for subjectivity in the
target language. Second, assuming a tool for auto-
matic subjectivity analysis exists in the source lan-
guage, we can use this tool together with machine
translation to create a corpus annotated for subjec-
tivity in the target language.
In order to perform a comprehensive investiga-
tion, we propose three experiments as described be-
low. The first scenario, based on a corpus manu-
ally annotated for subjectivity, is exemplified by the
first experiment. The second scenario, based on a
corpus automatically annotated with a tool for sub-
jectivity analysis, is subsequently divided into two
experiments depending on the direction of the trans-
lation and on the dataset that is translated.
In all three experiments, we use English as a
source language, given that it has both a corpus man-
ually annotated for subjectivity (MPQA (Wiebe et
al., 2005)) and a tool for subjectivity analysis (Opin-
ionFinder (Wiebe and Riloff, 2005)).
3.1 Experiment One: Machine Translation of
Manually Annotated Corpora
In this experiment, we use a corpus in the source
language manually annotated for subjectivity. The
corpus is automatically translated into the target lan-
guage, followed by a projection of the subjectivity
labels from the source to the target language. The
experiment is illustrated in Figure 1.
We use the MPQA corpus (Wiebe et al, 2005),
which is a collection of 535 English-language news
articles from a variety of news sources manually an-
notated for subjectivity. Although the corpus was
originally annotated at clause and phrase level, we
use the sentence-level annotations associated with
the dataset (Wiebe and Riloff, 2005). From the total
of 9,700 sentences in this corpus, 55% of the sen-
tences are labeled as subjective while the rest are
objective. After the automatic translation of the cor-
128
Figure 1: Experiment one: machine translation of man-
ually annotated training data from source language into
target language
pus and the projection of the annotations, we obtain
a large corpus of 9,700 subjectivity-annotated sen-
tences in the target language, which can be used to
train a subjectivity classifier.
3.2 Experiment Two: Machine Translation of
Source Language Training Data
In the second experiment, we assume that the only
resources available are a tool for subjectivity anno-
tation in the source language and a collection of raw
texts, also in the source language. The source lan-
guage text is automatically annotated for subjectiv-
ity and then translated into the target language. In
this way, we produce a subjectivity annotated cor-
pus that we can use to train a subjectivity annotation
tool for the target language. Figure 2 illustrates this
experiment.
In order to generate automatic subjectivity anno-
tations, we use the OpinionFinder tool developed by
(Wiebe and Riloff, 2005). OpinionFinder includes
two classifiers. The first one is a rule-based high-
precision classifier that labels sentences based on the
presence of subjective clues obtained from a large
lexicon. The second one is a high-coverage classi-
fier that starts with an initial corpus annotated us-
ing the high-precision classifier, followed by several
bootstrapping steps that increase the size of the lex-
icon and the coverage of the classifier. For most of
our experiments we use the high-coverage classifier.
Figure 2: Experiment two: machine translation of raw
training data from source language into target language
Table 1 shows the performance of the two Opinion-
Finder classifiers as measured on the MPQA corpus
(Wiebe and Riloff, 2005).
P R F
high-precision 86.7 32.6 47.4
high-coverage 79.4 70.6 74.7
Table 1: Precision (P), Recall (R) and F-measure (F) for
the two OpinionFinder classifiers, as measured on the
MPQA corpus
As a raw corpus, we use a subset of the SemCor
corpus (Miller et al, 1993), consisting of 107 docu-
ments with roughly 11,000 sentences. This is a bal-
anced corpus covering a number of topics in sports,
politics, fashion, education, and others. The reason
for working with this collection is the fact that we
also have a manual translation of the SemCor docu-
ments from English into one of the target languages
used in the experiments (Romanian), which enables
comparative evaluations of different scenarios (see
Section 4).
Note that in this experiment the annotation of sub-
jectivity is carried out on the original source lan-
guage text, and thus expected to be more accurate
than if it were applied on automatically translated
text. However, the training data in the target lan-
guage is produced by automatic translation, and thus
likely to contain errors.
129
3.3 Experiment Three: Machine Translation of
Target Language Training Data
The third experiment is similar to the second one,
except that we reverse the direction of the transla-
tion. We translate raw text that is available in the
target language into the source language, and then
use a subjectivity annotation tool to label the auto-
matically translated source language text. After the
annotation, the labels are projected back into the tar-
get language, and the resulting annotated corpus is
used to train a subjectivity classifier. Figure 3 illus-
trates this experiment.
Figure 3: Experiment three: machine translation of raw
training data from target language into source language
As before, we use the high-coverage classifier
available in OpinionFinder, and the SemCor corpus.
We use a manual translation of this corpus available
in the target language.
In this experiment, the subjectivity annotations
are carried out on automatically generated source
text, and thus expected to be less accurate. How-
ever, since the training data was originally written
in the target language, it is free of translation errors,
and thus training carried out on this data should be
more robust.
3.4 Upper bound: Machine Translation of
Target Language Test Data
For comparison purposes, we also propose an ex-
periment which plays the role of an upper bound on
the methods described so far. This experiment in-
volves the automatic translation of the test data from
the target language into the source language. The
source language text is then annotated for subjectiv-
ity using OpinionFinder, followed by a projection of
the resulting labels back into the target language.
Unlike the previous three experiments, in this
experiment we only generate subjectivity-annotated
resources, and we do not build and evaluate a stan-
dalone subjectivity analysis tool for the target lan-
guage. Further training of a machine learning algo-
rithm, as in experiments two and three, is required in
order to build a subjectivity analysis tool. Thus, this
fourth experiment is an evaluation of the resources
generated in the target language, which represents
an upper bound on the performance of any machine
learning algorithm that would be trained on these re-
sources. Figure 4 illustrates this experiment.
Figure 4: Upper bound: machine translation of test data
from target language into source language
4 Evaluation and Results
Our initial evaluations are carried out on Romanian.
The performance of each of the three methods is
evaluated using a dataset manually annotated for
subjectivity. To evaluate our methods, we generate a
Romanian training corpus annotated for subjectivity
on which we train a subjectivity classifier, which is
then used to label the test data.
We evaluate the results against a gold-standard
corpus consisting of 504 Romanian sentences man-
ually annotated for subjectivity. These sentences
represent the manual translation into Romanian of
a small subset of the SemCor corpus, which was
removed from the training corpora used in experi-
ments two and three. This is the same evaluation
dataset as used in (Mihalcea et al, 2007). Two
Romanian native speakers annotated the sentences
individually, and the differences were adjudicated
130
through discussions. The agreement of the two an-
notators is 0.83% (? = 0.67); when the uncertain an-
notations are removed, the agreement rises to 0.89
(? = 0.77). The two annotators reached consensus
on all sentences for which they disagreed, resulting
in a gold standard dataset with 272 (54%) subjective
sentences and 232 (46%) objective sentences. More
details about this dataset are available in (Mihalcea
et al, 2007).
In order to learn from our annotated data, we ex-
periment with two different classifiers, Na??ve Bayes
and support vector machines (SVM), selected for
their performance and diversity of learning method-
ology. For Na??ve Bayes, we use the multinomial
model (McCallum and Nigam, 1998) with a thresh-
old of 0.3. For SVM (Joachims, 1998), we use the
LibSVM implementation (Fan et al, 2005) with a
linear kernel.
The automatic translation of the MPQA and of
the SemCor corpus was performed using Language
Weaver,1 a commercial statistical machine transla-
tion software. The resulting text was post-processed
by removing diacritics, stopwords and numbers. For
training, we experimented with a series of weight-
ing schemes, yet we only report the results obtained
for binary weighting, as it had the most consistent
behavior.
The results obtained by running the three experi-
ments on Romanian are shown in Table 2. The base-
line on this data set is 54.16%, represented by the
percentage of sentences in the corpus that are sub-
jective, and the upper bound (UB) is 71.83%, which
is the accuracy obtained under the scenario where
the test data is translated into the source language
and then annotated using the high-coverage Opin-
ionFinder tool.
Perhaps not surprisingly, the SVM classifier out-
performs Na??ve Bayes by 2% to 6%, implying that
SVM may be better fitted to lessen the amount of
noise embedded in the dataset and provide more ac-
curate classifications.
The first experiment, involving the automatic
translation of the MPQA corpus enhanced with man-
ual annotations for subjectivity at sentence level,
does not seem to perform well when compared to the
experiments in which automatic subjectivity classi-
1http://www.languageweaver.com/
Romanian
Exp Classifier P R F
E1 Na??ve Bayes 60.91 60.91 60.91
SVM 66.07 66.07 66.07
E2 Na??ve Bayes 63.69 63.69 63.69
SVM 69.44 69.44 69.44
E3 Na??ve Bayes 65.87 65.87 65.87
SVM 67.86 67.86 67.86
UB OpinionFinder 71.83 71.83 71.83
Table 2: Precision (P), Recall (R) and F-measure (F) for
Romanian experiments
fication is used. This could imply that a classifier
cannot be so easily trained on the cues that humans
use to express subjectivity, especially when they are
not overtly expressed in the sentence and thus can
be lost in the translation. Instead, the automatic
annotations produced with a rule-based tool (Opin-
ionFinder), relying on overt mentions of words in
a subjectivity lexicon, seems to be more robust to
translation, further resulting in better classification
results. To exemplify, consider the following sub-
jective sentence from the MPQA corpus, which does
not include overt clues of subjectivity, but was an-
notated as subjective by the human judges because
of the structure of the sentence: It is the Palestini-
ans that are calling for the implementation of the
agreements, understandings, and recommendations
pertaining to the Palestinian-Israeli conflict.
We compare our results with those obtained by
a previously proposed method that was based on
the manual translation of the SemCor subjectivity-
annotated corpus. In (Mihalcea et al, 2007), we
used the manual translation of the SemCor corpus
into Romanian to form an English-Romanian par-
allel data set. The English side was annotated us-
ing the Opinion Finder tool, and the subjectivity la-
bels were projected on the Romanian text. A Na??ve
Bayes classifier was then trained on the subjectivity
annotated Romanian corpus and tested on the same
gold standard as used in our experiments. Table 3
shows the results obtained in those experiments by
using the high-coverage OpinionFinder classifier.
Among our experiments, experiments two and
three are closest to those proposed in (Mihalcea
et al, 2007). By using machine translation, from
131
OpinionFinder classifier P R F
high-coverage 67.85 67.85 67.85
Table 3: Precision (P), Recall (R) and F-measure (F) for
subjectivity analysis in Romanian obtained by using an
English-Romanian parallel corpus
English into Romanian (experiment two) or Roma-
nian into English (experiment three), and annotating
this dataset with the high-coverage OpinionFinder
classifier, we obtain an F-measure of 63.69%, and
65.87% respectively, using Na??ve Bayes (the same
machine learning classifier as used in (Mihalcea et
al., 2007)). This implies that at most 4% in F-
measure can be gained by using a parallel corpus as
compared to an automatically translated corpus, fur-
ther suggesting that machine translation is a viable
alternative to devising subjectivity classification in a
target language leveraged on the tools existent in a
source language.
As English is a language with fewer inflections
when compared to Romanian, which accommodates
for gender and case as a suffix to the base form of a
word, the automatic translation into English is closer
to a human translation (experiment three). Therefore
labeling this data using the OpinionFinder tool and
projecting the labels onto a fully inflected human-
generated Romanian text provides more accurate
classification results, as compared to a setup where
the training is carried out on machine-translated Ro-
manian text (experiment two).
 0.5
 0.55
 0.6
 0.65
 0.7
 0.2  0.4  0.6  0.8  1
F-
m
ea
su
re
Percentage of corpus
NB
SVM
Figure 5: Experiment two: Machine learning F-measure
over an incrementally larger training set
We also wanted to explore the impact that the cor-
 0.5
 0.55
 0.6
 0.65
 0.7
 0.2  0.4  0.6  0.8  1
F-
m
ea
su
re
Percentage of corpus
NB
SVM
Figure 6: Experiment three: Machine learning F-measure
over an incrementally larger training set
pus size may have on the accuracy of the classifiers.
We re-ran experiments two and three with 20% cor-
pus size increments at a time (Figures 5 and 6). It
is interesting to note that a corpus of approximately
6000 sentences is able to achieve a high enough F-
measure (around 66% for both experiments) to be
considered viable for training a subjectivity classi-
fier. Also, at a corpus size over 10,000 sentences, the
Na??ve Bayes classifier performs worse than SVM,
which displays a directly proportional trend between
the number of sentences in the data set and the ob-
served F-measure. This trend could be explained
by the fact that the SVM classifier is more robust
with regard to noisy data, when compared to Na??ve
Bayes.
5 Portability to Other Languages
To test the validity of the results on other languages,
we ran a portability experiment on Spanish.
To build a test dataset, a native speaker of Span-
ish translated the gold standard of 504 sentences into
Spanish. We maintain the same subjectivity anno-
tations as for the Romanian dataset. To create the
training data required by the first two experiments,
we translate both the MPQA corpus and the Sem-
Cor corpus into Spanish using the Google Transla-
tion service,2 a publicly available machine transla-
tion engine also based on statistical machine transla-
tion. We were therefore able to implement all the ex-
periments but the third, which would have required
2http://www.google.com/translate t
132
a manually translated version of the SemCor corpus.
Although we could have used a Spanish text to carry
out a similar experiment, due to the fact that the
dataset would have been different, the results would
not have been directly comparable.
The results of the two experiments exploring the
portability to Spanish are shown in Table 4. Inter-
estingly, all the figures are higher than those ob-
tained for Romanian. We assume this occurs be-
cause Spanish is one of the six official United Na-
tions languages, and the Google translation engine
is using the United Nations parallel corpus to train
their translation engine, therefore implying that a
better quality translation is achieved as compared to
the one available for Romanian. We can therefore
conclude that the more accurate the translation en-
gine, the more accurately the subjective content is
translated, and therefore the better the results. As it
was the case for Romanian, the SVM classifier pro-
duces the best results, with absolute improvements
over the Na??ve Bayes classifier ranging from 0.2%
to 3.5%.
Since the Spanish automatic translation seems to
be closer to a human-quality translation, we are not
surprised that this time the first experiment is able
to generate a more accurate training corpus as com-
pared to the second experiment. The MPQA corpus,
since it is manually annotated and of better quality,
has a higher chance of generating a more reliable
data set in the target language. As in the experiments
on Romanian, when performing automatic transla-
tion of the test data, we obtain the best results with
an F-measure of 73.41%, which is also the upper
bound on our proposed experiments.
Spanish
Exp Classifier P R F
E1 Na??ve Bayes 65.28 65.28 65.28
SVM 68.85 68.85 68.85
E2 Na??ve Bayes 62.50 62.50 62.50
SVM 62.70 62.70 62.70
UB OpinionFinder 73.41 73.41 73.41
Table 4: Precision (P), Recall (R) and F-measure (F) for
Spanish experiments
6 Discussion
Based on our experiments, we can conclude that ma-
chine translation offers a viable approach to gener-
ating resources for subjectivity annotation in a given
target language. The results suggest that either a
manually annotated dataset or an automatically an-
notated one can provide sufficient leverage towards
building a tool for subjectivity analysis.
Since the use of parallel corpora (Mihalcea et al,
2007) requires a large amount of manual labor, one
of the reasons behind our experiments was to asses
the ability of machine translation to transfer subjec-
tive content into a target language with minimal ef-
fort. As demonstrated by our experiments, machine
translation offers a viable alternative in the construc-
tion of resources and tools for subjectivity classifica-
tion in a new target language, with only a small de-
crease in performance as compared to the case when
a parallel corpus is available and used.
To gain further insights, two additional experi-
ments were performed. First, we tried to isolate the
role played by the quality of the subjectivity anno-
tations in the source-language for the cross-lingual
projections of subjectivity. To this end, we used the
high-precision OpinionFinder classifier to annotate
the English datasets. As shown in Table 1, this clas-
sifier has higher precision but lower recall as com-
pared to the high-coverage classifier we used in our
previous experiments. We re-ran the second exper-
iment, this time trained on the 3,700 sentences that
were classified by the OpinionFinder high-precision
classifier as either subjective or objective. For Ro-
manian, we obtained an F-measure of 69.05%, while
for Spanish we obtained an F-measure of 66.47%.
Second, we tried to isolate the role played by
language-specific clues of subjectivity. To this end,
we decided to set up an experiment which, by com-
parison, can suggest the degree to which the lan-
guages are able to accommodate specific markers for
subjectivity. First, we trained an English classifier
using the SemCor training data automatically anno-
tated for subjectivity with the OpinionFinder high-
coverage tool. The classifier was then applied to the
English version of the manually labeled test data set
(the gold standard described in Section 4). Next, we
ran a similar experiment on Romanian, using a clas-
sifier trained on the Romanian version of the same
133
SemCor training data set, annotated with subjectiv-
ity labels projected from English. The classifier was
tested on the same gold standard data set. Thus, the
two classifiers used the same training data, the same
test data, and the same subjectivity annotations, the
only difference being the language used (English or
Romanian).
The results for these experiments are compiled in
Table 5. Interestingly, the experiment conducted on
Romanian shows an improvement of 3.5% to 9.5%
over the results obtained on English, which indi-
cates that subjective content may be easier to learn
in Romanian versus English. The fact that Roma-
nian verbs are inflected for mood (such as indicative,
conditional, subjunctive, presumptive), enables an
automatic classifier to identify additional subjective
markers in text. Some moods such as conditional
and presumptive entail human judgment, and there-
fore allow for clear subjectivity annotation. More-
over, Romanian is a highly inflected language, ac-
commodating for forms of various words based on
number, gender, case, and offering an explicit lex-
icalization of formality and politeness. All these
features may have a cumulative effect in allowing
for better classification. At the same time, English
entails minimal inflection when compared to other
Indo-European languages, as it lacks both gender
and adjective agreement (with very few notable ex-
ceptions such as beautiful girl and handsome boy).
Verb moods are composed with the aid of modals,
while tenses and expressions are built with the aid
of auxiliary verbs. For this reason, a machine learn-
ing algorithm may not be able to identify the same
amount of information on subjective content in an
English versus a Romanian text. It is also interesting
to note that the labeling of the training set was per-
formed using a subjectivity classifier developed for
English, which takes into account a large, human-
annotated, subjectivity lexicon also developed for
English. One would have presumed that any clas-
sifier trained on this annotated text would therefore
provide the best results in English. Yet, as explained
earlier, this was not the case.
7 Conclusion
In this paper, we explored the use of machine trans-
lation for creating resources and tools for subjec-
Exp Classifier P R F
En Na??ve Bayes 60.32 60.32 60.32
SVM 60.32 60.32 60.32
Ro Na??ve Bayes 67.85 67.85 67.85
SVM 69.84 69.84 69.84
Table 5: Precision (P), Recall (R) and F-measure (F) for
identifying language specific information
tivity analysis in other languages, by leveraging on
the resources available in English. We introduced
and evaluated three different approaches to generate
subjectivity annotated corpora in a given target lan-
guage, and exemplified the technique on Romanian
and Spanish.
The experiments show promising results, as they
are comparable to those obtained using manually
translated corpora. While the quality of the trans-
lation is a factor, machine translation offers an effi-
cient and effective alternative in capturing the sub-
jective semantics of a text, coming within 4% F-
measure as compared to the results obtained using
human translated corpora.
In the future, we plan to explore additional
language-specific clues, and integrate them into the
subjectivity classifiers. As shown by some of our
experiments, Romanian seems to entail more subjec-
tivity markers compared to English, and this factor
motivates us to further pursue the use of language-
specific clues of subjectivity.
Our experiments have generated corpora of about
20,000 sentences annotated for subjectivity in Ro-
manian and Spanish, which are available for down-
load at http://lit.csci.unt.edu/index.php/Downloads,
along with the manually annotated data sets.
Acknowledgments
The authors are grateful to Daniel Marcu and Lan-
guageWeaver for kindly providing access to their
Romanian-English and English-Romanian machine
translation engines. This work was partially sup-
ported by a National Science Foundation grant IIS-
#0840608.
134
References
C. Ovesdotter Alm, D. Roth, and R. Sproat. 2005.
Emotions from text: Machine learning for text-based
emotion prediction. In Proceedings of the Hu-
man Language Technologies Conference/Conference
on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP-2005), pages 347?354, Vancouver,
Canada.
K. Balog, G. Mishne, and M. de Rijke. 2006. Why are
they excited? identifying and explaining spikes in blog
mood levels. In Proceedings of the 11th Meeting of
the European Chapter of the Association for Compu-
tational Linguistics (EACL-2006).
A. Esuli and F. Sebastiani. 2006. Determining term sub-
jectivity and term orientation for opinion mining. In
Proceedings the 11th Meeting of the European Chap-
ter of the Association for Computational Linguistics
(EACL-2006), pages 193?200, Trento, IT.
R. Fan, P. Chen, and C. Lin. 2005. Working set selection
using the second order information for training svm.
Journal of Machine Learning Research, 6:1889?1918.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing 2004 (KDD 2004), pages 168?177, Seattle, Wash-
ington.
Y. Hu, J. Duan, X. Chen, B. Pei, and R. Lu. 2005. A new
method for sentiment classification in text retrieval. In
IJCNLP, pages 1?9.
T. Joachims. 1998. Text categorization with Support
Vector Machines: learning with mny relevant features.
In Proceedings of the European Conference on Ma-
chine Learning, pages 137?142.
H. Kanayama and T. Nasukawa. 2006. Fully automatic
lexicon expansion for domain-oriented sentiment anal-
ysis. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2006), pages 355?363, Sydney, Australia.
N. Kando and D. Kirk Evans, editors. 2007. Proceed-
ings of the Sixth NTCIR Workshop Meeting on Evalua-
tion of Information Access Technologies: Information
Retrieval, Question Answering, and Cross-Lingual In-
formation Access, 2-1-2 Hitotsubashi, Chiyoda-ku,
Tokyo 101-8430, Japan, May. National Institute of In-
formatics.
S.-M. Kim and E. Hovy. 2006. Identifying and ana-
lyzing judgment opinions. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
pages 200?207, New York, New York.
N. Kobayashi, K. Inui, Y. Matsumoto, K. Tateishi, and
T. Fukushima. 2004. Collecting evaluative expres-
sions for opinion extraction. In Proceedings of the 1st
International Joint Conference on Natural Language
Processing (IJCNLP-04).
L. Lloyd, D. Kechagias, and S. Skiena. 2005. Lydia: A
system for large-scale news analysis. In String Pro-
cessing and Information Retrieval (SPIRE 2005).
A. McCallum and K. Nigam. 1998. A comparison of
event models for Naive Bayes text classification. In
Proceedings of AAAI-98 Workshop on Learning for
Text Categorization.
R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual pro-
jections. In Proceedings of the Association for Com-
putational Linguistics, Prague, Czech Republic.
G. Miller, C. Leacock, T. Randee, and R. Bunker. 1993.
A semantic concordance. In Proceedings of the 3rd
DARPA Workshop on Human Language Technology,
Plainsboro, New Jersey.
Y. Suzuki, H. Takamura, and M. Okumura. 2006. Ap-
plication of semi-supervised learning to evaluative ex-
pression classification. In Proceedings of the 7th In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics (CICLing-2006),
pages 502?513, Mexico City, Mexico.
H. Takamura, T. Inui, and M. Okumura. 2006. Latent
variable models for semantic orientations of phrases.
In Proceedings of the 11th Meeting of the European
Chapter of the Association for Computational Linguis-
tics (EACL 2006), Trento, Italy.
J. Wiebe and R. Mihalcea. 2006. Word sense and subjec-
tivity. In Proceedings of COLING-ACL 2006.
J. Wiebe and E. Riloff. 2005. Creating subjective and
objective sentence classifiers from unannotated texts.
In Proceedings of the 6th International Conference
on Intelligent Text Processing and Computational Lin-
guistics (CICLing-2005) ( invited paper), Mexico City,
Mexico.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2-3):165?210.
H. Yu and V. Hatzivassiloglou. 2003. Towards answering
opinion questions: Separating facts from opinions and
identifying the polarity of opinion sentences. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP-2003), pages
129?136, Sapporo, Japan.
135
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1192?1201,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Cross-lingual Semantic Relatedness Using Encyclopedic Knowledge
Samer Hassan and Rada Mihalcea
Department of Computer Science
University of North Texas
samer@unt.edu, rada@cs.unt.edu
Abstract
In this paper, we address the task of cross-
lingual semantic relatedness. We intro-
duce a method that relies on the informa-
tion extracted from Wikipedia, by exploit-
ing the interlanguage links available be-
tween Wikipedia versions in multiple lan-
guages. Through experiments performed
on several language pairs, we show that
the method performs well, with a perfor-
mance comparable to monolingual mea-
sures of relatedness.
1 Motivation
Given the accelerated growth of the number of
multilingual documents on the Web and else-
where, the need for effective multilingual and
cross-lingual text processing techniques is becom-
ing increasingly important. In this paper, we
address the task of cross-lingual semantic relat-
edness, and introduce a method that relies on
Wikipedia in order to calculate the relatedness of
words across languages. For instance, given the
word factory in English and the word lavoratore
in Italian (En. worker), the method can measure
the relatedness of these two words despite the fact
that they belong to two different languages.
Measures of cross-language relatedness are use-
ful for a large number of applications, including
cross-language information retrieval (Nie et al,
1999; Monz and Dorr, 2005), cross-language text
classification (Gliozzo and Strapparava, 2006),
lexical choice in machine translation (Och and
Ney, 2000; Bangalore et al, 2007), induction
of translation lexicons (Schafer and Yarowsky,
2002), cross-language annotation and resource
projections to a second language (Riloff et al,
2002; Hwa et al, 2002; Mohammad et al, 2007).
The method we propose is based on a measure
of closeness between concept vectors automati-
cally built from Wikipedia, which are mapped via
the Wikipedia interlanguage links. Unlike previ-
ous methods for cross-language mapping, which
are typically limited by the availability of bilingual
dictionaries or parallel texts, the method proposed
in this paper can be used to measure the related-
ness of word pairs in any of the 250 languages for
which a Wikipedia version exists.
The paper is organized as follows. We first pro-
vide a brief overview of Wikipedia, followed by
a description of the method to build concept vec-
tors based on this encyclopedic resource. We then
show how these concept vectors can be mapped
across languages for a cross-lingual measure of
word relatedness. Through evaluations run on six
language pairs, connecting English, Spanish, Ara-
bic and Romanian, we show that the method is ef-
fective at capturing the cross-lingual relatedness of
words, with results comparable to the monolingual
measures of relatedness.
2 Wikipedia
Wikipedia is a free online encyclopedia, represent-
ing the outcome of a continuous collaborative ef-
fort of a large number of volunteer contributors.
Virtually any Internet user can create or edit a
Wikipedia webpage, and this ?freedom of contri-
bution? has a positive impact on both the quantity
(fast-growing number of articles) and the quality
(potential errors are quickly corrected within the
collaborative environment) of this online resource.
The basic entry in Wikipedia is an article (or
page), which defines and describes an entity or
an event, and consists of a hypertext document
with hyperlinks to other pages within or outside
Wikipedia. The role of the hyperlinks is to guide
the reader to pages that provide additional infor-
mation about the entities or events mentioned in
an article. Articles are organized into categories,
which in turn are organized into hierarchies. For
instance, the article automobile is included in the
category vehicle, which in turn has a parent cate-
1192
Language Articles Users
English 2,221,980 8,944,947
German 864,049 700,980
French 765,350 546,009
Polish 579,170 251,608
Japanese 562,295 284,031
Italian 540,725 354,347
Dutch 519,334 216,938
Portuguese 458,967 503,854
Spanish 444,696 966,134
Russian 359,677 226,602
Table 1: Top ten largest Wikipedias
gory named machine, and so forth.
Each article in Wikipedia is uniquely referenced
by an identifier, consisting of one or more words
separated by spaces or underscores and occasion-
ally a parenthetical explanation. For example, the
article for bar with the meaning of ?counter for
drinks? has the unique identifier bar (counter).
Wikipedia editions are available for more than
250 languages, with a number of entries vary-
ing from a few pages to two millions articles or
more per language. Table 1 shows the ten largest
Wikipedias (as of December 2008), along with
the number of articles and approximate number of
contributors.1
Relevant for the work described in this paper are
the interlanguage links, which explicitly connect
articles in different languages. For instance, the
English article for bar (unit) is connected, among
others, to the Italian article bar (unit?a di misura)
and the Polish article bar (jednostka). On average,
about half of the articles in a Wikipedia version
include interlanguage links to articles in other lan-
guages. The number of interlanguage links per ar-
ticle varies from an average of five in the English
Wikipedia, to ten in the Spanish Wikipedia, and as
many as 23 in the Arabic Wikipedia.
3 Concept Vector Representations using
Explicit Semantic Analysis
To calculate the cross-lingual relatedness of two
words, we measure the closeness of their con-
cept vector representations, which are built from
Wikipedia using explicit semantic analysis (ESA).
Encyclopedic knowledge is typically organized
into concepts (or topics), each concept being
further described using definitions, examples,
1http://meta.wikimedia.org/wiki/List of Wikipedias
#Grand Total
and possibly links to other concepts. ESA
(Gabrilovich and Markovitch, 2007) relies on the
distribution of words inside the encyclopedic de-
scriptions, and builds semantic representations for
a given word in the form of a vector of the encyclo-
pedic concepts in which the word appears. In this
vector representation, each encyclopedic concept
is assigned with a weight, calculated as the term
frequency of the given word inside the concept?s
article.
Formally, let C be the set of all the Wikipedia
concepts, and let a be any content word. We define
~a as the ESA concept vector of term a:
~a = {w
c
1
, w
c
2
...w
c
n
} , (1)
where w
c
i
is the weight of the concept c
i
with re-
spect to a. ESA assumes the weight w
c
i
to be the
term frequency tf
i
of the word a in the article cor-
responding to concept c
i
.
We use a revised version of the ESA algorithm.
The original ESA semantic relatedness between
the words in a given word pair a ? b is defined as
the cosine similarity between their corresponding
vectors:
Relatedness(a, b) =
~a ?
~
b
?~a?
?
?
?
~
b
?
?
?
. (2)
To illustrate, consider for example the construc-
tion of the ESA concept vector for the word bird.
The top ten concepts containing this word, along
with the associated weight (calculated using equa-
tion 7), are listed in table 2. Note that the the ESA
vector considers all the possible senses of bird, in-
cluding Bird as a surname as in e.g., ?Larry Bird.?
Weight Wikipedia concept
51.4 Lists Of Birds By Region
44.8 Bird
40.3 British Birds Rarities Committee
32.8 Origin Of Birds
31.5 Ornithology
30.1 List Of Years In Birding And Ornithology
29.8 Bird Vocalization
27.4 Global Spread Of H5n1 In 2006
26.5 Larry Bird
22.3 Birdwatching
Table 2: Top ten Wikipedia concepts for the word
?bird?
In our ESA implementation, we make three
changes with respect to the original ESA algo-
rithm. First, we replace the cosine similarity with
1193
a Lesk-like metric (Lesk, 1986), which places less
emphasis on the distributional differences between
the vector weights and more emphasis on the over-
lap (mutual coverage) between the vector features,
and thus it is likely to be more appropriate for the
sparse ESA vectors, and for the possible asymme-
try between languages. Let a and b be two terms
with the corresponding ESA concept vectors ~A
and ~B respectively. Let A and B represent the sets
of concepts with a non-zero weight encountered in
~
A and ~B respectively. The coverage of ~A by ~B is
defined as:
G(
~
B|
~
A) =
?
i?B
w
a
i
(3)
and similarly, the coverage of ~B by ~A is:
G(
~
A|
~
B) =
?
i?A
w
b
i
(4)
where wa
i
and wb
i
represent the weight associ-
ated with concept c
i
in vectors ~A and ~B respec-
tively. By averaging these two asymmetric scores,
we redefine the relatedness as:
Relatedness(a, b) =
G(
~
B|
~
A) + G(
~
A|
~
B)
2
(5)
Second, we refine the ESA weighting schema
to account for the length of the articles describing
the concept. Since some concepts have lengthy
descriptions, they may be favored due to their high
term frequencies when compared to more compact
descriptions. To eliminate this bias, we calculate
the weight associated with a concept c
i
as follows:
w
c
i
= tf
i
? log(M/ |c
i
|), (6)
where tf
i
represents the term frequency of the
word a in concept c
i
, M is a constant representing
the maximum vocabulary size of Wikipedia con-
cepts, and |c
i
| is the size of the vocabulary used in
the description of concept c
i
.
Finally, we use the Wikipedia category graph
to promote category-type concepts in our feature
vectors. This is done by scaling the concept?s
weight by the inverse of the distance d
i
to the
root category. The concepts that are not categories
are treated as leaves, and therefore their weight is
scaled down by the inverse of the maximum depth
in the category graph. The resulting weighting
scheme is:
w
c
i
= tf
i
? log(M/ |c
i
|)/d
i
(7)
4 Cross-lingual Relatedness
We measure the relatedness of concepts in differ-
ent languages by using their ESA concept vector
representations in their own languages, along with
the Wikipedia interlanguage links that connect ar-
ticles written in a given language to their corre-
sponding Wikipedia articles in other languages.
For example, the English Wikipedia article moon
contains interlanguage links to Q ?

? in the Ara-
bic Wikipedia, luna in the Spanish Wikipedia, and
luna? in the Romanian Wikipedia. The interlan-
guage links can map concepts across languages,
and correspondingly map concept vector represen-
tations in different languages.
Formally, let C
x
and C
y
be the sets of all
Wikipedia concepts in languages x and y, with
corresponding translations in the y and x lan-
guages, respectively. If tr
xy
() is a translation
function that maps a concept c
i
? C
x
into the con-
cept c?
i
? C
y
via the interlanguage links, we can
write:
tr
xy
(c
i
) = c
?
i
, (8)
The projection of the ESA vector ~t from lan-
guage x onto y can be written as:
tr
xy
(
~
t) =
{
w
tr
xy
(c
1
)
...w
tr
xy
(c
n
)
}
. (9)
Using equations 5, 7, and 9, we can calculate the
cross-lingual semantic relatedness between any
two content terms a
x
and b
y
in given languages
x and y as:
sim(a
x
, b
y
) =
G(tr
yx
(
~
B)|
~
A) + G(
~
A|tr
yx
(
~
B))
2
.
(10)
Note that the weights assigned to Wikipedia
concepts inside the concept vectors are language
specific. That is, two Wikipedia concepts from
different languages, mapped via an interlanguage
link, can, and often do have different weights.
Intuitively, the relation described by the inter-
language links should be reflective and transi-
tive. However, due to Wikipedia?s editorial pol-
icy, which accredits users with the responsibility
1194
of maintaining the articles, these properties are not
always met. Table 3 shows real cases where the
transitive and the reflective properties fail due to
missing interlanguage links.
Relation Exists
Reflectivity
Kafr-El-Dawwar Battle(en) 7? P@ ?Y? @ Q ?? ??Q??(ar) Yes
P@

?Y? @ Q

??

??Q??(ar) 7? Kafr-El-Dawwar Battle(en) No
Transitivity
Intifada(en) 7? Intifada(es) Yes
Intifada(es) 7? ? ?A ?J K @(ar) Yes
Intifada(en) 7? ? ?A ?J K @(ar) No
Table 3: Reflectivity and transitivity in Wikipedia
We solve this problem by iterating over the
translation tables and extracting all the missing
links by enforcing the reflectivity and the transi-
tivity properties. Table 4 shows the initial number
of interlanguage links and the discovered links for
the four languages used in our experiments. The
table also shows the coverage of the interlanguage
links, measured as the ratio between the total num-
ber of interlanguage links (initial plus discovered)
originating in the source language towards the tar-
get language, divided by the total number of arti-
cles in the source language.
Interlanguage links
Language pair Initial Discov. Cover.
English ? Spanish 293,957 12,659 0.14
English ? Romanian 86,719 4,641 0.04
English ? Arabic 56,233 3,916 0.03
Spanish ? English 294,266 7,328 0.58
Spanish ? Romanian 39,830 3,281 0.08
Spanish ? Arabic 33,889 3,319 0.07
Romanian ? English 75,685 6,783 0.46
Romanian ? Spanish 36,002 3,546 0.22
Romanian ? Arabic 15,777 1,698 0.10
Arabic ? English 46,072 3,170 0.33
Arabic ? Spanish 28,142 3,109 0.21
Arabic ? Romanian 15,965 1,970 0.12
Table 4: Interlanguage links (initial and discov-
ered) and their coverage in Wikipedia versions in
four languages.
5 Experiments and Evaluations
We run our experiments on four languages: En-
glish, Spanish, Romanian and Arabic. For each
of these languages, we use a Wikipedia down-
load from October 2008. The articles were pre-
processed using Wikipedia Miner (Milne, 2007)
to extract structural information such as general-
ity, and interlanguage links. Furthermore, arti-
cles were also processed to remove numerical con-
tent, as well as any characters not included in the
given language?s alphabet. The content words are
stemmed, and words shorter than three characters
are removed (a heuristic which we use as an ap-
proximation for stopword removal). Table 5 shows
the number of articles in each Wikipedia version
and the size of their vocabularies, as obtained af-
ter the pre-processing step.
Articles Vocabulary
English 2, 221, 980 1, 231, 609
Spanish 520, 154 406, 134
Arabic 149, 340 216, 317
Romanian 179, 440 623, 358
Table 5: Number of articles and size of vocabulary
for the four Wikipedia versions
After pre-processing, the articles are indexed
to generate the ESA concept vectors. From each
Wikipedia version, we also extract other features
including article titles, interlanguage links, and
Wikipedia category graphs. The interlanguage
links are further processed to recover any missing
links, as described in the previous section.
5.1 Data
For the evaluation, we build several cross-lingual
datasets based on the standard Miller-Charles
(Miller and Charles, 1998) and WordSimilarity-
353 (Finkelstein et al, 2001) English word relat-
edness datasets.
The Miller-Charles dataset (Miller and Charles,
1998) consists of 30-word pairs ranging from syn-
onymy pairs (e.g., car - automobile) to completely
unrelated terms (e.g., noon - string). The relat-
edness of each word pair was rated by 38 hu-
man subjects, using a scale from 0 (not-related)
to 4 (perfect synonymy). The dataset is avail-
able only in English and has been widely used
in previous semantic relatedness evaluations (e.g.,
(Resnik, 1995; Hughes and Ramage, 2007; Zesch
et al, 2008)).
The WordSimilarity-353 dataset (also known as
Finkelstein-353) (Finkelstein et al, 2001) consists
of 353 word pairs annotated by 13 human experts,
on a scale from 0 (unrelated) to 10 (very closely
related or identical). The Miller-Charles set is a
subset in the WordSimilarity-353 data set. Unlike
the Miller-Charles data set, which consists only of
1195
Word pair
English coast - shore car - automobile brother - monk
Spanish costa - orilla coche - automovil hermano - monje
Arabic ?gA ? - Z?

?A


?

?PA

J

? - ?K
.
Q?

?J


?

? - I
.
?@

P
Romanian t?a?rm - mal mas?fina? - automobil frate - ca?luga?r
Table 6: Word pair translation examples
single words, the WordSimilarity-353 set alo fea-
tures phrases (e.g., ?Wednesday news?), therefore
posing an additional degree of difficulty for a re-
latedness metric applied on this data.
Native speakers of Spanish, Romanian and Ara-
bic, who were also highly proficient in English,
were asked to translate the words in the two data
sets. The annotators were provided one word pair
at a time, and asked to provide the appropriate
translation for each word while taking into account
their relatedness within the word pair. The relat-
edness was meant as a hint to disambiguate the
words, when multiple translations were possible.
The annotators were also instructed not to use
multi-word expressions in their translations. They
were also allowed to use replacement words to
overcome slang or culturally-biased terms. For ex-
ample, in the case of the word pair dollar-buck,
annotators were allowed to use PA


JK

X
2 as a transla-
tion for buck.
To test the ability of the bilingual judges to pro-
vide correct translations by using this annotation
setting, we carried out the following experiment.
We collected Spanish translations from five differ-
ent human judges, which were then merged into
a single selection based on the annotators? trans-
lation agreement; the merge was done by a sixth
human judge, who also played the role of adjudi-
cator when no agreement was reached between the
initial annotators.
Subsequently, five additional human experts re-
scored the word-pair Spanish translations by using
the same scale that was used in the construction of
the English data set. The correlation between the
2Arabic for dinars ? the commonly used currency in the
Middle East.
relatedness scores assigned during this experiment
and the scores assigned in the original English ex-
periment was 0.86, indicating that the translations
provided by the bilingual judges were correct and
preserved the word relatedness.
For the translations provided by the five human
judges, in more than 74% of the cases at least three
human judges agreed on the same translation for a
word pair. When the judges did not provide iden-
tical translations, they typically used a close syn-
onym. The high agreement between their trans-
lations indicates that the annotation setting was
effective in pinpointing the correct translation for
each word, even in the case of ambiguous words.
Motivated by the validation of the annotation
setting obtained for Spanish, we used only one hu-
man annotator to collect the translations for Arabic
and Romanian. Table 6 shows examples of trans-
lations in the three languages for three word pairs
from our data sets.
Using these translations, we create six cross-
lingual data sets, one for each possible language
pair (English-Spanish, English-Arabic, English-
Romanian, Spanish-Arabic, Spanish-Romanian,
Arabic-Romanian). Given a source-target lan-
guage pair, a data set is created by first using the
source language for the first word and the target
language for the second word, and then reversing
the order, i.e., using the source language for the
second word and the target language for the first
word. The size of the data sets is thus doubled
in this way (e.g., the 30 word pairs in the English
Miller-Charles set are transformed into 60 word
pairs in the English-Spanish Miller-Charles set).
5.2 Results
We evaluate the cross-lingual measure of related-
ness on each of the six language pairs. For com-
parison purposes, we also evaluate the monolin-
gual relatedness on the four languages.
For the evaluation, we use the Pearson (r)
and Spearman (?) correlation coefficients, which
are the standard metrics used in the past for the
evaluation of semantic relatedness (Finkelstein et
1196
al., 2001; Zesch et al, 2008; Gabrilovich and
Markovitch, 2007). While the Pearson correla-
tion is highly dependent on the linear relationship
between the distributions in question, Spearman
mainly emphasizes the ability of the distributions
to maintain their relative ranking.
Tables 7 and 8 show the results of the evalua-
tions of the cross-lingual relatedness, when using
an ESA concept vector with a size of maximum
10,000 concepts.3
English Spanish Arabic Romanian
Miller-Charles
English 0.58 0.43 0.32 0.50
Spanish 0.44 0.20 0.38
Arabic 0.36 0.32
Romanian 0.58
WordSimilarity-353
English 0.55 0.32 0.31 0.29
Spanish 0.45 0.32 0.28
Arabic 0.28 0.25
Romanian 0.30
Table 7: Pearson correlation for cross-
lingual relatedness on the Miller-Charles and
WordSimilarity-353 data sets
English Spanish Arabic Romanian
Miller-Charles
English 0.75 0.56 0.27 0.55
Spanish 0.64 0.17 0.32
Arabic 0.33 0.21
Romanian 0.61
WordSimilarity-353
English 0.71 0.55 0.35 0.38
Spanish 0.50 0.29 0.30
Arabic 0.26 0.20
Romanian 0.28
Table 8: Spearman correlation for cross-
lingual relatedness on the Miller-Charles and
WordSimilarity-353 data sets
As a validation of our ESA implementation, we
compared the results obtained for the monolingual
English relatedness with other results reported in
the past for the same data sets. Gabrilovich and
Markovitch (2007) reported a Spearman correla-
tion of 0.72 for the Miller-Charles data set and
0.75 for the WordSimilarity-353 data set, respec-
3The concepts are selected in reversed order of their
weight inside the vector in the respective language. Note that
the cross-lingual mapping between the concepts in the ESA
vectors is done after the selection of the top 10,000 concepts
in each language.
tively. Zesch et al (2008) reported a Spear-
man correlation of 0.67 for the Miller-Charles set.
These values are comparable to the Spearman cor-
relation scores obtained in our experiments for the
English data sets (see Table 8), with a fairly large
improvement obtained on the Miller-Charles data
set when using our implementation.
6 Discussion
Overall, our method succeeds in capturing the
cross-lingual semantic relatedness between words.
As a point of comparison, one can use the mono-
lingual measures of relatedness as reflected by the
diagonals in Tables 7 and 8.
Looking at the monolingual evaluations, the re-
sults seem to be correlated with the Wikipedia size
for the corresponding language, with the English
measure scoring the highest. These results are not
surprising, given the direct relation between the
Wikipedia size and the sparseness of the ESA con-
cept vectors. A similar trend is observed for the
cross-lingual relatedness, with higher results ob-
tained for the languages with large Wikipedia ver-
sions (e.g., English-Spanish), and lower results for
the languages with a smaller size Wikipedia (e.g.,
Arabic-Spanish).
For comparison, we ran two additional experi-
ments. In the first experiment, we compared the
coverage of our cross-lingual relatedness method
to a direct use of the translation links available in
Wikipedia. The cross-lingual relatedness is turned
into a monolingual relatedness by using the in-
terlanguage Wikipedia links to translate the first
of the two words in a cross-lingual pair into the
language of the second word in the pair.4 From
the total of 433 word pairs available in the two
data sets, this method can produce translations
for an average of 103 word pairs per language
pair. This means that the direct Wikipedia inter-
language links allow the cross-lingual relatedness
measure to be transformed into a monolingual re-
latedness in about 24% of the cases, which is a
low coverage compared to the full coverage that
can be obtained with our cross-lingual method of
relatedness.
In an attempt to raise the coverage of the trans-
lation, we ran a second experiment where we used
a state-of-the-art translation engine to translate the
first word in a pair into the language of the sec-
4We use all the interlanguage links obtained by combining
the initial and the discovered links, as described in Section 4.
1197
ond word in the pair. We use Google Translate,
which is a statistical machine translation engine
that relies on large parallel corpora, to find the
most likely translation for a given word. Unlike
the previous experiment, this time we can achieve
full translation coverage, and thus we are able to
produce data sets of equal size that can be used
for a comparison between relatedness measures.
Specifically, using the translation produced by the
machine translation engine for the first word in a
pair, we calculate the relatedness within the space
of the language of the second word using a mono-
lingual ESA also based on Wikipedia. The results
obtained with this method are compared against
the results obtained with our cross-lingual ESA re-
latedness.
Using a Pearson correlation, our cross-lingual
relatedness method achieves an average score
across all six language pairs of 0.36 for the Miller-
Charles data set and 0.30 for the WordSimilarity-
353 data set,5 which is higher than the 0.33 and
0.28 scores achieved for the same data sets when
using a translation obtained with Google Trans-
late followed by a monolingual measure of re-
latedness. These results are encouraging, also
given that the translation-based method is limited
to those language pairs for which a translation en-
gine exists (e.g., Google Translate covers 40 lan-
guages), whereas our method can be applied to any
language pair from the set of 250 languages for
which a Wikipedia version exists.
To gain further insights, we also determined the
impact of the vector length in the ESA concept
vector representation, by calculating the Pearson
correlation for vectors of different lengths. Fig-
ures 1 and 2 show the Pearson score as a func-
tion of the vector length for the Miller-Charles
and WordSimilarity-353 data sets. The plots show
that the cross-lingual measure of relatedness is not
significantly affected by the reduction or increase
of the vector length. Thus, the use of vectors of
length 10,000 (as used in most of our experiments)
appears as a reasonable tradeoff between accuracy
and performance.
Furthermore, by comparing the performance of
the proposed Lesk-like model to the traditional
cosine-similarity (Figures 3 and 4), we note that
the Lesk-like model outperforms the cosine model
on most language pairs. We believe that this is
5This average considers all the cross-lingual relatedness
scores listed in Table 7; it does not include the monolingual
scores listed on the table diagonal.
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 5000  10000  15000  20000  25000  30000
Pe
ar
so
n 
co
rre
la
tio
n
Vector length
ar?ar
ar?en
ar?es
ar?ro
en?en
en?es
es?es
es?ro
en?ro
ro?ro
Figure 1: Pearson correlation vs. ESA vector
length on the Miller-Charles data set
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 5000  10000  15000  20000  25000  30000
Av
er
ag
e 
Pe
ar
so
n
Vector Size
ar?ar
ar?en
ar?es
ar?ro
en?en
en?es
es?es
es?ro
en?ro
ro?ro
Figure 2: Pearson correlation vs. ESA vector
length on the WordSimilarity-353 data set
due to the stricter correlation conditions imposed
by the cosine-metric in such sparse vector-based
representations, as compared to the more relaxed
hypothesis used by the Lesk model.
Finally, we also looked at the relation between
the number of interlanguage links found for the
concepts in a vector and the length of the vector.
Figures 5 and 6 display the average number of in-
terlanguage links as a function of the concept vec-
tor length.
By analyzing the effect of the average number
of interlanguage links found per word in the given
datasets (Figures 5 and 6), we notice that these
links increase proportionally with the vector size,
as expected. However, this increase does not lead
to any significant improvements in accuracy (Fig-
ures 1 and 2). This implies that while the presence
of interlanguage links is a prerequisite for the mea-
1198
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 5000  10000  15000  20000  25000  30000
Pe
ar
so
n
Vector Size
lsk(ar?ar)
lsk(en?en)
lsk(es?es)
lsk(ro?ro)
cos(ar?ar)
cos(en?en)
cos(es?es)
cos(ro?ro)
Figure 3: Lesk vs. cosine similarity for the Miller-
Charles data set
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 5000  10000  15000  20000  25000  30000
Pe
ar
so
n
Vector Size
lsk(ar?ar)
lsk(en?en)
lsk(es?es)
lsk(ro?ro)
cos(ar?ar)
cos(en?en)
cos(es?es)
cos(ro?ro)
Figure 4: Lesk vs. cosine similarity for the
WordSimilarity-353 data set
sure of relatedness,6 their effect is only significant
for the top ranked concepts in a vector. Therefore,
increasing the vectors size to maximize the match-
ing of the projected dimensions does not necessar-
ily lead to accuracy improvements.
7 Related Work
Measures of word relatedness were found useful in
a large number of natural language processing ap-
plications, including word sense disambiguation
(Patwardhan et al, 2003), synonym identification
(Turney, 2001), automated essay scoring (Foltz et
al., 1999), malapropism detection (Budanitsky and
Hirst, 2001), coreference resolution (Strube and
Ponzetto, 2006), and others. Most of the work to
date has focused on measures of word relatedness
for English, by using methods applied on knowl-
6Two languages with no interlanguage links between
them will lead to a relatedness score of zero for any word
pair across these languages, no matter how strongly related
the words are.
 0
 500
 1000
 1500
 2000
 5000  10000  15000  20000  25000  30000
N
um
be
r o
f i
nt
er
la
ng
ua
ge
 lin
ks
Vector length
ar?en
ar?es
ar?ro
en?es
en?ro
es?ro
Figure 5: Number of interlanguage links vs. vec-
tor length for the Miller-Charles data set
 0
 500
 1000
 1500
 2000
 2500
 3000
 3500
 4000
 5000  10000  15000  20000  25000  30000
N
um
be
r o
f i
nt
er
la
ng
ua
ge
 lin
ks
Vector length
ar?en
ar?es
ar?ro
en?es
en?ro
es?ro
Figure 6: Number of interlanguage links vs. vec-
tor length for the WordSimilarity-353 data set
edge bases (Lesk, 1986; Wu and Palmer, 1994;
Resnik, 1995; Jiang and Conrath, 1997; Hughes
and Ramage, 2007) or on large corpora (Salton
et al, 1997; Landauer et al, 1998; Turney, 2001;
Gabrilovich and Markovitch, 2007).
Although to a lesser extent, measures of word
relatedness have also been applied on other lan-
guages, including German (Zesch et al, 2007;
Zesch et al, 2008; Mohammad et al, 2007), Chi-
nese (Wang et al, 2008), Dutch (Heylen et al,
2008) and others. Moreover, assuming resources
similar to those available for English, e.g., Word-
Net structures or large corpora, the measures of
relatedness developed for English can be in prin-
ciple applied to other languages as well.
All these methods proposed in the past have
been concerned with monolingual word related-
ness calculated within the boundaries of one lan-
guage, as opposed to cross-lingual relatedness,
which is the focus of our work.
The research area closest to the task of cross-
1199
lingual relatedness is perhaps cross-language in-
formation retrieval, which is concerned with
matching queries posed in one language to docu-
ment collections in a second language. Note how-
ever that most of the approaches to date for cross-
language information retrieval have been based on
direct translations obtained for words in the query
or in the documents, by using bilingual dictionar-
ies (Monz and Dorr, 2005) or parallel corpora (Nie
et al, 1999). Such explicit translations can iden-
tify a direct correspondence between words in two
languages (e.g., they will find that fabbrica (It.)
and factory (En.) are translations of each other),
but will not capture similarities of a different de-
gree (e.g., they will not find that lavoratore (It.;
worker in En.) is similar to factory (En.).
Also related are the areas of word alignment
for machine translation (Och and Ney, 2000),
induction of translation lexicons (Schafer and
Yarowsky, 2002), and cross-language annotation
projections to a second language (Riloff et al,
2002; Hwa et al, 2002; Mohammad et al,
2007). As with cross-language information re-
trieval, these areas have primarily considered di-
rect translations between words, rather than an en-
tire spectrum of relatedness, as we do in our work.
8 Conclusions
In this paper, we addressed the problem of
cross-lingual semantic relatedness, which is a
core task for a number of applications, includ-
ing cross-language information retrieval, cross-
language text classification, lexical choice for ma-
chine translation, cross-language projections of re-
sources and annotations, and others.
We introduced a method based on concept vec-
tors built from Wikipedia, which are mapped
across the interlanguage links available between
Wikipedia versions in multiple languages. Ex-
periments performed on six language pairs, con-
necting English, Spanish, Arabic and Romanian,
showed that the method is effective at captur-
ing the cross-lingual relatedness of words. The
method was shown to be competitive when com-
pared to methods based on a translation using the
direct Wikipedia links or using a statistical trans-
lation engine. Moreover, our method has wide ap-
plicability across languages, as it can be used for
any language pair from the set of 250 languages
for which a Wikipedia version exists.
The cross-lingual data sets introduced
in this paper can be downloaded from
http://lit.csci.unt.edu/index.php/Downloads.
Acknowledgments
The authors are grateful to Carmen Banea for her
help with the construction of the data sets. This
material is based in part upon work supported by
the National Science Foundation CAREER award
#0747340. Any opinions, findings, and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of the National Science Founda-
tion.
References
S. Bangalore, P. Haffner, and S. Kanthak. 2007. Statis-
tical machine translation through global lexical se-
lection and sentence reconstruction. In Proceedings
of the Annual Meeting of the Association of Compu-
tational Linguistics, Prague, Czech Republic.
A. Budanitsky and G. Hirst. 2001. Semantic distance
in WordNet: An experimental, application-oriented
evaluation of five measures. In Proceedings of the
NAACL Workshop on WordNet and Other Lexical
Resources, Pittsburgh.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2001. Plac-
ing search in context: the concept revisited. In
WWW, pages 406?414.
P. Foltz, D. Laham, and T. Landauer. 1999. Automated
essay scoring: Applications to educational technol-
ogy. In Proceedings of World Conference on Edu-
cational Multimedia, Hypermedia and Telecommu-
nications, Chesapeake, Virginia.
E. Gabrilovich and S. Markovitch. 2007. Comput-
ing semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In Proceedings of the Inter-
national Joint Conference on Artificial Intelligence,
pages 1606?1611.
A. Gliozzo and C. Strapparava. 2006. Exploiting com-
parable corpora and bilingual dictionaries for cross-
language text categorization. In Proceedings of the
Conference of the Association for Computational
Linguistics, Sydney, Australia.
K. Heylen, Y. Peirsman, D. Geeraerts, and D. Speel-
man. 2008. Modelling word similarity: an evalu-
ation of automatic synonymy extraction algorithms.
In Proceedings of the Sixth International Language
Resources and Evaluation, Marrakech, Morocco.
T. Hughes and D. Ramage. 2007. Lexical semantic
knowledge with random graph walks. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, Prague, Czech Republic.
R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002.
Evaluating translational correspondence using anno-
tation projection. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2002), Philadelphia, July.
1200
J. Jiang and D. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of the International Conference on Re-
search in Computational Linguistics, Taiwan.
T. K. Landauer, P. Foltz, and D. Laham. 1998. Intro-
duction to latent semantic analysis. Discourse Pro-
cesses, 25.
M.E. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
SIGDOC Conference 1986, Toronto, June.
G. Miller and W. Charles. 1998. Contextual corre-
lates of semantic similarity. Language and Cogni-
tive Processes, 6(1).
D. Milne. 2007. Computing semantic relatedness us-
ing wikipedia link structure. In European Language
Resources Association (ELRA), editor, In Proceed-
ings of the New Zealand Computer Science Re-
search Student Conference (NZCSRSC 2007), New
Zealand.
S. Mohammad, I. Gurevych, G. Hirst, and T. Zesch.
2007. Cross-lingual distributional profiles of
concepts for measuring semantic distance. In
Proceedings of the Joint Conference on Empir-
ical Methods in Natural Language Processing
and Computational Natural Language Learning
(EMNLP/CoNLL-2007), Prague, Czech Republic.
C. Monz and B.J. Dorr. 2005. Iterative translation
disambiguation for cross-language information re-
trieval. In Proceedings of the 28th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, Salvador,
Brazil.
J.-Y. Nie, M. Simard, P. Isabelle, and R. Durand. 1999.
Cross-language information retrieval based on paral-
lel texts and automatic mining of parallel texts from
the Web. In Proceedings of the 22nd annual inter-
national ACM SIGIR conference on Research and
development in information retrieval.
F. Och and H. Ney. 2000. A comparison of align-
ment models for statistical machine translation. In
Proceedings of the 18th International Conference on
Computational Linguistics (COLING 2000), Saar-
brucken, Germany, August.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003.
Using measures of semantic relatedness for word
sense disambiguation. In Proceedings of the Fourth
International Conference on Intelligent Text Pro-
cessing and Computational Linguistics, Mexico
City, February.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity. In Proceedings of the 14th
International Joint Conference on Artificial Intelli-
gence, Montreal, Canada.
E. Riloff, C. Schafer, and D. Yarowsky. 2002. In-
ducing information extraction systems for new lan-
guages via cross-language projection. In Proceed-
ings of the 19th International Conference on Com-
putational Linguistics, Taipei, Taiwan, August.
G. Salton, A. Wong, and C.S. Yang. 1997. A vec-
tor space model for automatic indexing. In Read-
ings in Information Retrieval, pages 273?280. Mor-
gan Kaufmann Publishers, San Francisco, CA.
C. Schafer and D. Yarowsky. 2002. Inducing trans-
lation lexicons via diverse similarity measures and
bridge languages. In Proceedings of the 6th Confer-
ence on Natural Language Learning (CoNLL 2003),
Taipei, Taiwan, August.
M. Strube and S. P. Ponzetto. 2006. Wikirelate! com-
puting semantic relatedeness using Wikipedia. In
Proceedings of the American Association for Artifi-
cial Intelligence, Boston, MA.
P. Turney. 2001. Mining the web for synonyms: PMI-
IR versus LSA on TOEFL. In Proceedings of the
Twelfth European Conference on Machine Learning
(ECML-2001), Freiburg, Germany.
X. Wang, S. Ju, and S. Wu. 2008. A survey of chi-
nese text similarity computation. In Proceedings of
the Asia Information Retrieval Symposium, Harbin,
China.
Z. Wu and M. Palmer. 1994. Verb semantics and lex-
ical selection. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Lin-
guistics, Las Cruces, New Mexico.
T. Zesch, I. Gurevych, and M. Mu?hlha?user. 2007.
Comparing Wikipedia and German Wordnet by
Evaluating Semantic Relatedness on Multiple
Datasets. In Proceedings of Human Language Tech-
nologies: The Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
T. Zesch, C. Mu?ller, and I. Gurevych. 2008. Using
Wiktionary for Computing Semantic Relatedness.
In Proceedings of the American Association for Ar-
tificial Intelligence, Chicago.
1201
Workshop on TextGraphs, at HLT-NAACL 2006, pages 53?60,
New York City, June 2006. c?2006 Association for Computational Linguistics
Random-Walk Term Weighting
for Improved Text Classification
Samer Hassan and Carmen Banea
Department of Computer Science
University of North Texas
Denton, TX 76203
samer@unt.edu, carmen@unt.edu
Abstract
This paper describes a new approach for
estimating term weights in a text classifi-
cation task. The approach uses term co-
occurrence as a measure of dependency
between word features. A random walk
model is applied on a graph encoding
words and co-occurrence dependencies,
resulting in scores that represent a quan-
tification of how a particular word feature
contributes to a given context. We argue
that by modeling feature weights using
these scores, as opposed to the traditional
frequency-based scores, we can achieve
better results in a text classification task.
Experiments performed on four standard
classification datasets show that the new
random-walk based approach outperforms
the traditional term frequency approach to
feature weighting.
1 Introduction
Term frequency has long been adapted as a measure
of term significance in a specific context (Robert-
son and Jones, 1997). The logic behind it is that the
more a certain term is encountered in a certain con-
text, the more it carries or contributes to the mean-
ing of the context. Due to this belief, term frequency
has been a major factor in estimating the probabilis-
tic distribution of features using maximum likeli-
hood estimates and hence has been incorporated in a
broad spectrum of tasks ranging from feature selec-
tion techniques (Yang and Pedersen, 1997; Schutze
et al, 1995) to language models (Bahl et al, 1983).
In this paper we introduce a new measure of term
weighting, which integrates the locality of a term
and its relation to the surrounding context. We
model this local contribution using a co-occurrence
relation in which terms that co-occur in a certain
context are likely to share between them some of
their importance (or significance). Note that in this
model the relation between a given term and its con-
text is not linear, since the context itself consists of
a collection of other terms, which in turn have a
dependency relation with their own context, which
might include the original given term. In order to
model this recursive relation we use a graph-based
ranking algorithm, namely the PageRank random-
walk algorithms (Brin and Page, 1998), and its Text-
Rank adaption to text processing applications (Mi-
halcea and Tarau, 2004). TextRank takes as in-
put a set of textual entities and relations between
them, and uses a graph-based ranking algorithm
(also known as random walk algorithm) to produce
a set of scores that represent the accumulated weight
or rank for each textual entity in their context. The
TextRank model was so far evaluated on three nat-
ural language processing tasks: document summa-
rization, word sense disambiguation, and keyword
extraction, and despite being fully unsupervised, it
has been shown to be competitive with other some-
time supervised state-of-the-art algorithms.
In this paper, we show how TextRank can be
used to model the probabilistic distribution of word
features in a document, by making further use of
the scores produced by the random-walk model.
53
Through experiments performed on a text classifi-
cation task, we show that these random walk scores
outperform the traditional term frequencies typically
used to model the feature weights for this task.
2 Graph-based Ranking Algorithms
The basic idea implemented by an iterative graph-
based ranking algorithm is that of ?voting? or ?rec-
ommendation?. When one vertex links to another
one, it is basically casting a vote for that other ver-
tex. The higher the number of votes that are cast
for a vertex, the higher the importance of the ver-
tex. Moreover, the importance of the vertex casting
a vote determines how important the vote itself is,
and this information is also taken into account by
the ranking algorithm. Hence, the score associated
with a vertex is determined based on the votes that
are cast for it, and the scores of the vertices casting
these votes.
While there are several graph-based ranking algo-
rithms previously proposed in the literature (Herings
et al, 2001), we focus on only one such algorithm,
namely PageRank (Brin and Page, 1998), as it was
previously found successful in a number of applica-
tions, including Web link analysis (Brin and Page,
1998), social networks (Dom et al, 2003), citation
analysis, and more recently in several text process-
ing applications (Mihalcea and Tarau, 2004), (Erkan
and Radev, 2004).
Given a graph G = (V,E), let In(Va) be the
set of vertices that point to vertex Va (predecessors),
and let Out(Va) be the set of vertices that vertex Va
points to (successors). The PageRank score associ-
ated with the vertex Va is then defined using a recur-
sive function that integrates the scores of its prede-
cessors:
S(Va) = (1 ? d) + d ?
?
Vb?In(Va)
S(Vb)
|Out(Vb)|
(1)
where d is a parameter that is set between 0 and 11.
The score of each vertex is recalculated upon each
iteration based on the new weights that the neighbor-
ing vertices have accumulated. The algorithm termi-
nates when the convergence point is reached for all
the vertices, meaning that the error rate for each ver-
tex falls below a pre-defined threshold. Formally,
1The typical value for d is 0.85 (Brin and Page, 1998), and
this is the value we are also using in our implementation.
for a vertex Vi let Sk(Vi) be the rank or the score
at iteration k and Sk+1(Vi) be the score at iteration
k + 1. The error rate ER is defined as:
ER = Sk+1(Vi) ? Sk(Vi) (2)
This vertex scoring scheme is based on a ran-
dom walk model, where a walker takes random steps
on the graph G, with the walk being modeled as
a Markov process ? that is, the decision on what
edge to follow is solely based on the vertex where
the walker is currently located. Under certain con-
ditions, this model converges to a stationary dis-
tribution of probabilities, associated with vertices
in the graph. Based on the Ergodic theorem for
Markov chains (Grimmett and Stirzaker, 1989), the
algorithm is guaranteed to converge if the graph is
both aperiodic and irreducible. The first condition is
achieved for any graph that is a non-bipartite graph,
while the second condition holds for any strongly
connected graph ? property achieved by PageRank
through the random jumps introduced by the (1?d)
factor. In matrix notation, the PageRank vector of
stationary probabilities is the principal eigenvector
for the matrix Arow, which is obtained from the ad-
jacency matrix A representing the graph, with all
rows normalized to sum to 1: (P = ATrowP ).
Intuitively, the stationary probability associated
with a vertex in the graph represents the probability
of finding the walker at that vertex during the ran-
dom walk, and thus it represents the importance of
the vertex within the graph. In the context of se-
quence data labeling, the random walk is performed
on the label graph associated with a sequence of
words, and thus the resulting stationary distribution
of probabilities can be used to decide on the most
probable set of labels for the given sequence.
2.1 TextRank
Given a natural language processing task, the Text-
Rank model includes four general steps for the
application of a graph-based ranking algorithm to
graph structures derived from natural language texts:
1. Identify text units that best define the proposed
task and add them as vertices in the graph.
2. Identify relations that connect such test units,
and use these relations to draw edges between
54
vertices in the graph. Edges can be directed or
undirected, weighted or un-weighted.
3. Iterate the graph ranking algorithm to conver-
gence.
4. Sort vertices based on their final score. Use the
values attached to each vertex for ranking.
The strength of this model lies in the global repre-
sentation of the context and its ability to model how
the co-occurrence between features might propagate
across the context and affect other distant features.
While TextRank has already been applied to sev-
eral language processing tasks, we focus here on the
keyword extraction task, since it best relates to our
approach. The goal of a keyword extraction tool is
to find a set of words or phrases that best describe a
given document. The co-occurrence relation within
a specific window is used to portray the correlation
between words, which are represented as vertices in
the graph. Two vertices are connected if their cor-
responding lexical units co-occur within a window
of at most N words, where N can be set to any
value greater than two. The TextRank application
to keyword extraction has also used different syn-
tactic filters for vertex selection, including all open
class words, nouns and verbs, nouns and adjectives,
and others. The algorithm was found to provide the
best results using nouns and adjectives with a win-
dow size of two.
Our approach follows the same main steps as used
in the TextRank keyword extraction application. We
are however incorporating a larger number of lexical
units, and we use different window sizes, as we will
show in the following section.
3 TextRank for Term Weighting
The goal of the work reported in this paper is to
study the ranking scores obtained using TextRank,
and evaluate their potential usefulness as a new mea-
sure of term weighting.
To understand how the random-walk weights
(rw) might be a good replacement for the traditional
term frequency weights (tf ), consider the example
in Figure 1. The example represents a sample doc-
ument from the Reuters collection. A graph is con-
structed as follows. If a term has not been previously
seen, then a node is added to the graph to represent
this term. A term can only be represented by one
node in the graph. An undirected edge is drawn be-
tween two nodes if they co-occur within a certain
window size. This example assumes a window size
of two, corresponding to two consecutive terms in
the text (e.g. London is linked to based).
London-based sugar operator Kaines Ltd con-
firmed it sold two cargoes of white sugar to India
out of an estimated overall sales total of four or five
cargoes in which other brokers participated. The
sugar, for April/May and April/June shipment, was
sold at between 214 and 218 dlrs a tonne cif, it said.
Figure 1: Sample Reuters document
London
based
sugar
operator
Kaines
confirmed
sold
cargoes
white
Indiaestimatedsales
total
brokers
participated
April
MayJune
shipment
dlrs
tonne
cif
Figure 2: Sample graph
Table 1 shows the tf and rw weights, also plotted
in Figure 3. By analyzing the rw weights, we can
observe a non-linear correlation with the tf weights,
with an emphasis given to terms surrounding impor-
tant key term like e.g. ?sugar? or ?cargoes.? This
spatial locality has resulted in higher ranks for terms
like ?operator? compared to other terms like ?lon-
don?2.
2All the missing words (e.g. ?Ltd,? ?it?) that are not shown
in the graph are common-words that were eliminated in the pre-
processing phase.
55
Term rw tf
sugar 2.248 3
sold 1.594 2
april 1.407 2
cargoes 1.542 2
cif 0.600 1
sales 0.891 1
london 0.546 1
tonne 1.059 1
shipment 0.829 1
based 0.933 1
estimated 0.888 1
dlrs 0.938 1
kaines 0.871 1
confirmed 0.859 1
total 0.856 1
white 0.796 1
india 0.846 1
operator 0.839 1
brokers 0.826 1
june 0.801 1
participated 0.819 1
Table 1: tf & rw scores
0
0.5
1
1.5
2
2.5
3
3.5
su
ga
r
so
ld
ca
rg
oe
s
ap
ril
to
nn
e
dl
rs
ba
se
d
sa
le
s
es
tim
at
ed
ka
in
es
co
nf
irm
ed
to
ta
l
in
di
a
op
er
at
or
sh
ip
m
en
t
br
ok
er
s
pa
rti
ci
pa
te
d
ju
ne
wh
ite ci
f
lo
nd
on
Fr
eq
ue
nc
y
r.w
t.f
Figure 3: tf & rw plots
4 Experimental Setup
To evaluate our random-walk based approach to fea-
ture weighting, we integrate it in a text classification
algorithm, and evaluate its performance on several
standard text classification data sets.
4.1 Random-Walk Term Weighting
Starting with a given document, we determine a
ranking over the words in the document by using the
approach described in Section 3.
First, we tokenize the document for punctuation,
special symbols, word abbreviations. We also re-
move the common words, using a list of approx-
imately 500 frequently used words as used in the
Smart retrieval system 3.
Next, the resulting text is processed to extract both
tf and rw weights for each term in the document.
Note that we do not apply any syntactic filters, as
it was previously done in applications of TextRank.
Instead, we consider each word as a potential fea-
ture. To determine tf we simply count the frequen-
cies of each word in the document. To determine
rw, all the terms are added as vertices in a graph
representing the document. A co-occurrence scan-
ner is then applied to the text to relate the terms that
co-occur within a given window size . For a given
term, all the terms that fall in the vicinity of this
term are considered dependent terms. This is rep-
resented by a set of edges that connect this term to
all the other terms in the window. Experiments are
performed for window sizes of 2, 4, 6, and 8. Once
the graph is constructed and the edges are in place,
the TextRank algorithm is applied4. The result of the
ranking process is a list of all input terms and their
corresponding rw scores.
We then calculate tf.idf and rw.idf as follows:
tf.idf = tf ? logNDn
where ND represent the total number of documents
in the collection and n is the number of documents
in which the target term appeared at least once.
Similarly,
rw.idf = rw ? logNDn
These term weights (tf.idf or rw.idf ) are then
used to create a feature vector for each document.
The vectors are fed to a traditional text classifica-
tion system, using one of the learning algorithms de-
scribed below. The results obtained using tf.idf will
act as a baseline in our evaluation.
4.2 Text Classification
Text classification is a problem typically formulated
as a machine learning task, where a classifier learns
how to distinguish between categories in a given set
3ftp://ftp.cs.cornell.edu/pub/smart.
4We use an implementation where the maximum number of
iterations is limited to 100, the damping factor is set to 0.85, and
convergence threshold to 0.0001. Each graph node is assigned
with an initial weight of 0.25.
56
using features automatically extracted from a collec-
tion of training documents. There is a large body
of algorithms previously tested on text classification
problems, due also to the fact that this task is one
of the testbeds of choice for machine learning algo-
rithms. In the experiments reported here, we com-
pare results obtained with four frequently used text
classifiers ? Rocchio, Na??ve Bayes, Nearest Neigh-
bor, and Support Vector Machines, selected based on
their diversity of learning methodologies.
Na??ve Bayes. The basic idea in a Na??ve Bayes
text classifier is to estimate the probability of a
category given a document using joint probabili-
ties of words and documents. Na??ve Bayes as-
sumes word independence, which means that the
conditional probability of a word given a category
is assumed to be independent of the conditional
probability of other words given the same category.
Despite this simplification, Na??ve Bayes classifiers
were shown to perform surprisingly well on text
classification (Joachims, 1997), (Schneider, 2004).
While there are several versions of Na??ve Bayes
classifiers (variations of multinomial and multivari-
ate Bernoulli), we use the multinomial model (Mc-
Callum and Nigam, 1998), which was shown to be
more effective.
Rocchio. This is an adaptation of the relevance
feedback method developed in information retrieval
(Rocchio, 1971). It uses standard tf.idf weighted
vectors to represent documents, and builds a pro-
totype vector for each category by summing up the
vectors of the training documents in each category.
Test documents are then assigned to the category
that has the closest prototype vector, based on a
cosine similarity. Text classification experiments
with different versions of the Rocchio algorithm
showed competitive results on standard benchmarks
(Joachims, 1997), (Moschitti, 2003).
KNN. K-Nearest Neighbor is one of the earliest text
categorization approaches (Makoto and Takenobu,
1995; Masand et al, 1992). The algorithm classifies
a test document based on the best class label identi-
fied for the nearest K-neighbors in the training doc-
uments. The best class label is chosen by weighting
the class of each similar training document with its
similarity to the target test document.
SVM. Support Vector Machines (Vapnik, 1995) is
a state-of-the-art machine learning approach based
on decision plans. The algorithm defines the best
hyper-plan which separates set of points associated
with different class labels with a maximum-margin.
The unlabeled examples are then classified by de-
ciding in which side of the hyper-surface they re-
side. The hyper-plan can be a simple linear plan as
first proposed by Vapnik, or a non-linear plan such
as e.g. polynomial, radial, or sigmoid. In our eval-
uation we used the linear kernel since it was proved
to be as powerful as the other kernels when tested on
text classification data sets (Yang and Liu, 1999).
4.3 Data Sets
In our experiments we use Reuters-21578,
WebKB, 20Newsgroups, and LingSpam
datasets. These datasets are commonly used for text
classification evaluations (Joachims, 1996; Craven
et al, 1998; Androutsopoulos et al, 2000; Mihalcea
and Hassan, 2005).
Reuter-21578. This is a publicly available subset of
the Reuters news, containing about 120 categories.
We use the standard ModApte data split (Apte et
al., 1994). The unlabeled documents were discarded
and only the documents with one or more class la-
bels were used in the classification experiments.
WebKB. This is a data set collected from com-
puter science departments of various universities by
the CMU text learning group. The dataset contains
seven class labels which are Project, Student, De-
partment, Faculty, Staff, Course, and Other. The
Other label was removed from the dataset for evalu-
ation purposes. Most of the evaluations in the liter-
ature have been performed on only four of the cate-
gories (Project, Student, Faculty, and Course) since
they represent the largest categories. However, since
we wanted to see how our system behaves when only
a few training examples were available as e.g. in the
Staff and the Department classes, we performed our
evaluations on two versions of WebKB: one with
the four categories version (WebKB4) and one with
the six categories (WebKB6).
20-Newsgroups. This is a collection of 20,000 mes-
sages from 20 different newsgroups, corresponding
to different topics or subjects. Each newsgroup has
about 1000 message split into 400 test and 600 train
documents.
LingSpam. This is a spam corpus, consisting of
email messages organized in 10 collections to al-
57
low for 10-fold cross validation. Each collection has
roughly 300 spam and legitimate messages. There
are four versions of the corpus standing for bare,
stop-word filtered, lemmatized, and stop-word and
lemmatized. We use the bare collection with a stan-
dard 10-fold cross validation.
4.4 Performance Measures
To evaluate the classification system we used the tra-
ditional accuracy measure defined as the number of
correct predictions divided with the number of eval-
uated examples.
We also use the correlation coefficient (?) as
a diversity measure to evaluate the dissimilarity
between the weighting models. Pairwise diver-
sity measures have been traditionally used to mea-
sure the statistical independence among ensemble of
classifiers (Kuncheva and Whitaker, 2003). Here,
we use them to measure the correlation between our
random-walk approach and the traditional term fre-
quency approach. The typical setting in which the
pairwise diversity measures are used is a set of dif-
ferent classifiers which are used to classify the same
set of feature vectors or documents over a given
dataset. In our evaluation we use the same classifier
to evaluate two different sets of feature vectors that
are produced by different weighting features: the rw
random walk weighting, and the tf term frequency
weighting. Since the two feature vector collections
are evaluated by one classifier at a time, the resulted
diversity scores will reflect the diversity of the two
systems.
Let Di and Dj be two feature weighting models
with the following contingency table.
Dj correct=Y Dj correct=N
Di correct=Y a b
Di correct=N c d
Table 2: Di & Dj Contingency table
The correlation coefficient (?) is defined as:
?ij =
ad ? bc
?
(a + b)(c + d)(a + c)(b + d)
5The symbol ?indicates a statistically significant result using
Table 3: Naive Bayes Results5
N.B. tf rw2 rw4 rw6 rw8
WebKB4 81.9 81.9 82.8 82.7 81.2
WebKB6 71.7 73.0 74.2? 74.4? 73.5
Reuter 83.2 82.5 82.9 83.0 82.8
20NG 81.7 82.0 82.3? 82.3? 82.1?
LSpam 99.3 99.4 99.3 99.3 99.3
Table 4: Rocchio Results
ROC tf rw2 rw4 rw6 rw8
WebKB4 71.9 77.5? 78.6? 80.8? 80.9?
WebKB6 58.3 69.6? 72.0? 76.5? 76.2?
Reuter 78.2 80.8? 81.1? 81.0? 81.4?
20NG 76.2 77.3? 77.1? 77.2? 77.4?
LSpam 97.5 97.8 97.8 97.7 97.8
5 Evaluation and Discussion
Tables 3, 4, 5, 6 show the classification results for
WebKB4, WebKB6, LingSpam, Reuter, and
20Newsgroups respectively. The rw2, rw4, rw6,
and rw8 represent the accuracies achieved using
random-walk weighting under window sizes of 2,
4, 6, and 8 respectively. The tf column represents
the results obtained with a term frequency weighting
scheme.
By examining the results we can see that the
rw.idf model outperforms the tf.idf model on all
the classifiers and datasets with only one excep-
tion in the case of a Na??ve Bayes classifier under
Reuter. The error reductions range from 3.5% as in
{20Newsgroups, NaiveBayes, rw4} to 44% as in
the case of {WebKB6, Rocchio, rw6}. The system
gives, in its worst performance, a comparable result
to the tf.idf baseline. The system shows a consis-
tent performance with different window sizes, with
no clear cut window size that would give the best
result. By further analyzing the results using statis-
tical paired t-tests we can see that windows of size
4 and 6 supply the most significant results across all
the classifiers as well as the datasets.
Comparing WebKB4 and WebKB6 fine-grained
results, we found that both systems failed to pre-
dict the class Staff; however the significant improve-
a paired t-test, with p < 0.05. The result is marked by ? when
p < 0.001.
58
Table 5: KNN Results
KNN tf rw2 rw4 rw6 rw8
WebKB4 59.2 68.6? 67.0? 64.6? 66.6?
WebKB6 55.8 63.7? 55.8 59.9? 61.0?
Reuter 73.6 76.9? 78.1? 78.5? 78.5?
20NG 70.3 76.1? 76.5? 77.2? 77.8?
LSpam 97.5 97.8 97.8 98.1? 97.9
Table 6: SVM Results
SVM tf rw2 rw4 rw6 rw8
WebKB4 87.7 87.9 87.9 89? 88.5
WebKB6 82.5 84.5? 85.2? 85.2? 84.6?
Reuter 83.2 84.5? 84.4? 84.6? 84.1?
20NG 95.2 95.5? 95.6? 95.6? 95.4?
LSpam 95.6 96.4? 96.4? 96.2? 96.3?
ment was over the class Department, in which our
rw model scores an accuracy of 47% compared to
4% in using tf.idf . This indicates how successful
rw.idf model is in cases where there are few train-
ing examples. This could be due to the ability of the
model to extract more realistic and smoother distri-
bution of terms as seen in the rw curve plotted in
Figure 3, hence reducing the feature bias imposed
by the limited number of training examples.
Table 7: Naive Bayes Correlation ?
N.B. rw2 rw4 rw6 rw8
WebKB4 0.68 0.70 0.70 0.66
WebKB6 0.71 0.71 0.71 0.65
Reuter 0.86 0.87 0.87 0.85
20NG 0.82 0.84 0.83 0.82
LSpam 0.89 0.89 0.92 0.92
By also examining the diversity of the classifi-
cation systems based on rw and tf weighting, as
shown in Table 7, 8, 9, 10, we can see an inter-
esting property of the system. The two models are
generally more diverse and less correlated when us-
ing windows of size 6 and 8 than using windows of
size 2 and 4. This could be due to the increasing
drift from the feature independence assumption that
is implied by tf.idf . However increasing the depen-
dency is not always desirable as seen in the reported
accuracies. We expect that at a certain window size
the system performance will degrade to tf.idf . This
Table 8: Rocchio Correlation ?
ROC rw2 rw4 rw6 rw8
WebKB4 0.49 0.51 0.53 0.54
WebKB6 0.40 0.40 0.41 0.42
Reuter 0.75 0.77 0.75 0.71
20NG 0.77 0.77 0.77 0.77
LSpam 0.82 0.85 0.81 0.78
Table 9: KNN Correlation ?
KNN rw2 rw4 rw6 rw8
WebKB4 0.35 0.32 0.36 0.37
WebKB6 0.35 0.35 0.37 0.37
Reuter 0.74 0.70 0.68 0.67
20NG 0.62 0.64 0.63 0.59
LSpam 0.66 0.69 0.63 0.57
threshold window size will be equal to the document
size. In such a case each term will depend on all the
remaining terms resulting in an almost completely
connected graph. Consequently, each feature contri-
bution to the surrounding will be equal resulting in
similar rw scores to all the features.
6 Conclusions and Future Work
Based on results obtained in text classification ex-
periments, the TextRank random-walk model to
term weighting was found to achieve error rate re-
ductions of 3.5?44% as compared to the traditional
frequency-based approach. The evaluation results
have shown that the system performance varies de-
pending on window size, dataset, as well as classi-
fier, with the greatest boost in performance recorded
for KNN ,Rocchio, and SVM. We believe that these
results support our claim that random-walk models
can accurately estimate term weights, and can be
used as a technique to model the probabilistic dis-
tribution of features in a document.
The evaluations reported in this paper has shown
that the TextRank model can accurately provide uni-
gram probabilities for a sequence of words. In future
work we will try to extend the TextRank model and
use it to define a formal language model in which
we can estimate the probability of entire sequences
of words (n-grams).
59
Table 10: SVM Correlation ?
SVM rw2 rw4 rw6 rw8
WebKB4 0.73 0.77 0.78 0.82
WebKB6 0.73 0.76 0.78 0.80
Reuter 0.80 0.83 0.82 0.82
20NG 0.80 0.78 0.82 0.83
LSpam 0.86 0.88 0.88 0.89
References
I. Androutsopoulos, J. Koutsias, K. V. Chandrinos,
G. Paliouras, and C. D. Spyropoulos. 2000. An eval-
uation of naive bayesian anti-spam filtering. In Pro-
ceedings of the workshop on Machine Learning in the
New Information Age.
C. Apte, F. Damerau, and S. M. Weiss. 1994. Towards
language independent automated learning of text cat-
egorisation models. In Proceedings of the 17th ACM
SIGIR Conference on Research and Development in
Information Retrieval.
L. Bahl, F. Jelinek, and R. Mercer. 1983. A maximum
likelihood approach to continuous speech recognition.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 5(2).
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual Web search engine. Computer Networks
and ISDN Systems, 30(1?7).
M. Craven, D. DiPasquo, D. Freitag, A. McCallum,
T. Mitchell, K. Nigam, and S. Slattery. 1998. Learn-
ing to extract symbolic knowledge from the World
Wide Web. In Proceedings of the 15th Conference of
the American Association for Artificial Intelligence.
B. Dom, I. Eiron, A. Cozzi, and Y. Shang. 2003. Graph-
based ranking algorithms for e-mail expertise analysis.
In Proceedings of the 8th ACM SIGMOD workshop on
Research issues in data mining and knowledge discov-
ery, San Diego, California.
G. Erkan and D. Radev. 2004. Lexpagerank: Prestige in
multi-document text summarization. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, Barcelona, Spain, July.
G. Grimmett and D. Stirzaker. 1989. Probability and
Random Processes. Oxford University Press.
P.J. Herings, G. van der Laan, and D. Talman. 2001.
Measuring the power of nodes in digraphs. Technical
report, Tinbergen Institute.
T. Joachims. 1996. A probabilistic analysis of the roc-
chio algorithm with tf.idf for text categorization. In
Proceedings of the 14th International Conference on
Machine Learning.
T. Joachims. 1997. A probabilistic analysis of the Roc-
chio algorithm with TFIDF for text categorization. In
Proceedings of ICML-97, 14th International Confer-
ence on Machine Learning, Nashville, US.
L. Kuncheva and C. Whitaker. 2003. Measures of diver-
sity in classifier ensembles and their relationship with
the ensemble accuracy. Machine Learning, 51.
I. Makoto and T. Takenobu. 1995. Cluster-based text cat-
egorization: A comparison of category search starte-
gies. In Proceedings of the 18th ACM International
Conference on Research and Development in Informa-
tion Retrieval.
B. Masand, G. Linoff, and D. Waltz. 1992. Classify-
ing news stories using memory based reasoning. In
Proceedings of the 15th International Conference on
Research and Development in information Retrieval.
A. McCallum and K. Nigam. 1998. A comparison of
event models for Naive Bayes text classification. In
Proceedings of AAAI-98 Workshop on Learning for
Text Categorization.
R. Mihalcea and S. Hassan. 2005. Using the essence of
texts to improve document classification. In Proceed-
ings of the Conference on Recent Advances in Natural
Language Processing (RANLP), Borovetz, Bulgaria.
R. Mihalcea and P. Tarau. 2004. TextRank ? bringing
order into texts. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2004), Barcelona, Spain.
A. Moschitti. 2003. A study on optimal paramter tun-
ing for Rocchio text classifier. In Proceedings of the
European Conference on Information Retrieval, Pisa,
Italy.
R. Robertson and K. Sparck Jones. 1997. Simple, proven
approaches to text retrieval. Technical report.
J. Rocchio, 1971. Relevance feedback in information re-
trieval. Prentice Hall, Ing. Englewood Cliffs, New Jer-
sey.
K. Schneider. 2004. A new feature selection score for
multinomial naive bayes text classification based on
kl-divergence. In The Companion Volume to the Pro-
ceedings of 42st Annual Meeting of the Association for
Computational Linguistics, Barcelona, Spain, July.
H. Schutze, D. A. Hull, and J. O. Pedersen. 1995. A
comparison of classifiers and document representa-
tions for the routing problem. In Proceedings of the
18th annual international ACM SIGIR conference on
Research and development in information retrieval,
Seattle, Washington.
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer, New York.
Y. Yang and X. Liu. 1999. A reexamination of text cate-
gorization methods. In Proceedings of the 22nd ACM
SIGIR Conference on Research and Development in
Information Retrieval.
Y. Yang and J. O. Pedersen. 1997. A comparative study
on feature selection in text categorization. In Proceed-
ings of the 14th International Conference on Machine
Learning, Nashville, US.
60
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 410?413,
Prague, June 2007. c?2007 Association for Computational Linguistics
UNT: SubFinder: Combining Knowledge Sources for
Automatic Lexical Substitution
Samer Hassan, Andras Csomai, Carmen Banea, Ravi Sinha, Rada Mihalcea?
Department of Computer Science and Engineering
University of North Texas
samer@unt.edu, csomaia@unt.edu, carmenb@unt.edu, rss0089@unt.edu, rada@cs.unt.edu
Abstract
This paper describes the University of North
Texas SUBFINDER system. The system is
able to provide the most likely set of sub-
stitutes for a word in a given context, by
combining several techniques and knowl-
edge sources. SUBFINDER has successfully
participated in the best and out of ten (oot)
tracks in the SEMEVAL lexical substitution
task, consistently ranking in the first or sec-
ond place.
1 Introduction
Lexical substitution is defined as the task of identify-
ing the most likely alternatives (substitutes) for a tar-
get word, given its context (McCarthy, 2002). Many
natural language processing applications can bene-
fit from the availability of such alternative words,
including word sense disambiguation, lexical ac-
quisition, machine translation, information retrieval,
question answering, text simplification, and others.
The task is closely related to the problem of word
sense disambiguation, with the substitutes acting as
synonyms for the input word meaning. Unlike word
sense disambiguation however, lexical substitution
is not performed with respect to a given sense inven-
tory, but instead candidate synonyms are generated
?on the fly? for a given word occurrence. Thus, lexi-
cal substitution can be regarded in a way as a hybrid
task that combines word sense disambiguation and
distributional similarity, targeting the identification
of semantically similar words that fit the context.
2 A system for lexical substitution
SUBFINDER is a system able to provide the most
likely set of substitutes for a word in a given context.
?Contact author.
In SUBFINDER, the lexical substitution task is car-
ried out as a sequence of two steps. First, candidates
are extracted from a variety of knowledge sources;
so far, we experimented with WordNet (Fellbaum,
1998), Microsoft Encarta encyclopedia, Roget, as
well as synonym sets generated from bilingual dic-
tionaries, but additional knowledge sources can be
integrated as well. Second, provided a list of candi-
dates, a number of ranking methods are applied in
a weighted combination, resulting in a final list of
lexical substitutes ranked by their semantic fit with
both the input target word and the context.
3 Candidate Extraction
Candidates are extracted using several lexical re-
sources, which are combined into a larger compre-
hensive resource.
WordNet: WordNet is a large lexical database of
English, with words grouped into synonym sets
called synsets. A problem we encountered with this
resource is that often times the only candidate in the
synset is the target word itself. Thus, to enlarge the
set of candidates, we use both the synonyms and the
hypernyms of the target word. We also remove the
target word from the synset, to ensure that only vi-
able candidates are considered.
Microsoft Encarta encyclopedia: The Microsoft
Encarta is an online encyclopedia and thesaurus re-
source, which provides for each word the part of
speech and a list of synonyms. Using the part of
speech as identified in the context, we are able to ex-
tract synsets for the target word. An important fea-
ture in the Encarta Thesaurus is that the first word
in the synset acts as a definition for the synset, and
therefore disambiguates the target word. This defi-
nition is maintained as a separate entry in the com-
410
prehensive resource, and it is also added to its corre-
sponding synset.
Other Lexical Resources: We have also experi-
mented with two other lexical resources, namely the
Roget thesaurus and a thesaurus built using bilingual
dictionaries. In evaluations carried out on the devel-
opment data set, the best results were obtained using
only WordNet and Encarta, and thus these are the
resources used in the final SUBFINDER system.
All these resources entail different forms of synset
clustering. In order to merge them, we use the
largest overlap among them. It is important to note
that the choice of the first resource considered has
a bearing on the way the synsets are clustered. In
experiments ran on the development data set, the
best results were obtained using a lexical resource
constructed starting with the Microsoft Encarta The-
saurus and then mapping the WordNet synsets to it.
4 Candidate Ranking
Several ranking methods are used to score the can-
didate substitutes, as described below.
Lexical Baseline (LB): In this approach we use
the pre-existing lexical resources to provide a rank-
ing over the candidate substitutes. We rank the can-
didates based on their occurrence in the two selected
lexical resources WordNet and Encarta, with those
occurring in both resources being assigned a higher
ranking. This technique emphasizes the resources
annotators? agreement that the candidates belong in-
deed to the same synset.
Machine Translation (MT): We use machine
translation to translate the test sentences back-and-
forth between English and a second language. From
the resulting English translation, we extract the re-
placement that the machine translation engine pro-
vides for the target word. To locate the translated
word we scan the translation for any of the can-
didates (and their inflections) as obtained from the
comprehensive resource, and score the candidate
synset accordingly.
We experimented with a range of languages such
as French, Italian, Spanish, Simplified Chinese, and
German, but the best results obtained on the devel-
opment data were based on the French translations.
This could be explained because French is part of
the Romance languages family and synonyms to En-
glish words often find their roots in Latin. If we
consider again the word bright, it was translated
into French as intelligent and then translated back
into English as intelligent for obvious reasons. In
one instance, intelligent was the best replacement
for bright in the trial data. Despite the fact that we
also used Italian and Spanish (which are both Latin-
based) we can only assume that French worked bet-
ter because translation engines are better trained on
French. From the resulting English translation, we
extract the replacement that the machine translation
engine provides for the target word. To locate the
translated word we scan the translation for any of the
candidates (and their inflections) as obtained from
the comprehensive resource, and score the candidate
synset accordingly. The translation process was car-
ried out using Google and AltaVista translation en-
gines resulting in two systems MTG and MTA re-
spectively. The translation systems feature high pre-
cision when a candidate is found (about 20% of the
time), at the cost of low recall. The lexical baseline
method is therefore used when no candidates are re-
turned by the translation method.
Most Common Sense (MCS): Another method
we use for ranking candidates is to consider the
first word appearing in the first synset returned by
WordNet. When no words other than the target
word are available in this synset, the method recur-
sively searches the next synset available for the tar-
get word. In order to guarantee a sufficient number
of candidates, we use the lexical baseline method as
a baseline.
Language Model (LM): We model the semantic
fit of a candidate substitute within the given context
using a language model, expressed using the condi-
tional probability:
P (c|g) = P (c, g)/P (g) ? Count(c, g) (1)
where c represents a possible candidate and g rep-
resents the context. The probability P (g) of the
context is the same for all the candidates, hence we
can ignore it and estimate P (c|g) as the N-gram fre-
quency of the context where the target word is re-
placed by the proposed candidate. To avoid skewed
counts that can arise from the different morpholog-
ical inflections of the target word or the candidate
and the bias that the context might have toward any
specific inflection, we generalize P (c|g) to take into
account all the inflections of the selected candidate
as shown in equation 2.
Pn(c|g) ?
n
?
i=1
Count(ci, g) (2)
where n is the number of possible inflections for the
candidate c.
We use the Google N-gram dataset to calculate the
term Count(ci g). The Google N-gram corpus is a
411
collection of English N-grams, ranging from one to
five N-grams, and their respective frequency counts
observed on the Web (Brants and Franz, 2006). In
order for the model to give high preference to the
longer N-grams, while maintaining the relative fre-
quencies of the shorter N-grams (typically more fre-
quent), we augment the counts of the higher order
N-grams with the maximum counts of the lower or-
der N-grams, hence guaranteeing that the score as-
signed to an N-gram of order N is higher than the
the score of an N-gram of order N ? 1.
Semantic Relatedness using Latent Semantic
Analysis (LSA): We expect to find a strong se-
mantic relationship between a good candidate and
the target context. A relatively simple and efficient
way to measure such a relatedness is the Latent Se-
mantic Analysis (Landauer et al, 1998). Documents
and terms are mapped into a 300 dimensional latent
semantic space, providing the ability to measure the
semantic relatedness between two words or a word
and a context. We use the InfoMap package from
Stanford University?s Center for the Study of Lan-
guage and Information, trained on a collection of
approximately one million Wikipedia articles. The
rank of a candidate is given by its semantic related-
ness to the entire context sentence.
Information Retrieval (IR): Although the Lan-
guage Model approach is successful in ranking the
candidates, it suffers from the small N-gram size im-
posed by using the Google N-grams corpus. Such
a restriction is obvious in the following 5-gram ex-
ample who was a bright boy in which the context
is not sufficient to disambiguate between happy and
smart as possible candidates. As a result, we adapt
an information retrieval approach which uses all the
content words available in the given context. Similar
to the previous models, the target word in the con-
text is replaced by all the generated inflections of
the selected candidate and then queried using a web
search engine. The resulting rank represents the sum
of the total number of pages in which the candidate
or any of its inflections occur together with the con-
text. This also reflects the semantic relatedness or
the relevance of the candidate to the context.
Word Sense Disambiguation (WSD): Since pre-
vious work indicated the usefulness of word sense
disambiguation systems in lexical substitution (Da-
gan et al, 2006), we use the SenseLearner word
sense disambiguation tool (Mihalcea and Csomai,
2005) to disambiguate the target word and, accord-
ingly, to propose its synonyms as candidates.
Final System: Our candidate ranking methods are
aimed at different aspects of what constitutes a good
candidate. On one hand, we measure the semantic
relatedness of a candidate with the original context
(the LSA and WSD methods fall under this cate-
gory). On the other hand, we also want to ensure
that the candidate fits the context and leads to a well
formed English sentence (e.g., the language model
method). Given that the methods described earlier
aim at orthogonal aspects of the problem, it is ex-
pected that a combination of these will provide a
better overall ranking.
We use a voting mechanism, where we consider
the reciprocal of the rank of each candidates as given
by one of the described methods. The final score of
a candidate is given by the decreasing order of the
weighted sum of the reciprocal ranks:
score (ci) =
?
m?rankings
?m
1
rmci
To determine the weight ? of each individual
ranking we run a genetic algorithm on the develop-
ment data, optimized for the mode precision and re-
call. Separate sets of weights are obtained for the
best and oot tasks. Table 1 shows the weights of
the individual ranking methods. As expected, for
the best task, the language model type of methods
obtain higher weights, whereas for the oot task, the
semantic methods seem to perform better.
5 Results and Discussion
The SUBFINDER system participated in the best and
the oot tracks of the lexical substitution task. The
best track calls for any number of best guesses,
with the most promising one listed first. The credit
for each correct guess is divided by the number of
guesses. The oot track allows systems to make up to
10 guesses, without penalizing, and without being of
any benefit if less than 10 substitutes are provided.
The ordering of guesses in the oot metric is unim-
portant.
For both tracks, the evaluation is carried out using
precision and recall, calculated based on the number
of matching responses between the system and the
human annotators, respectively. A ?mode? evalua-
tion is also conducted, which measures the ability of
the systems to capture the most frequent response
(the ?mode?) from the gold standard annotations.
For details, please refer to the official task descrip-
tion document (McCarthy and Navigli, 2007).
Tables 2 and 3 show the results obtained by SUB-
FINDER in the best and oot tracks respectively. The
tables also show a breakdown of the results based
412
on: only target words that were not identified as
multiwords (NMWT); only substitutes that were not
identified as multiwords (NMWS); only items with
sentences randomly selected from the Internet cor-
pus (RAND); only items with sentences manually se-
lected from the Internet corpus (MAN).
WSD LSA IR LB MCS MTA MTG LM
best 34 2 64 63 56 69 38 97
oot 6 82 7 28 46 14 32 68
Table 1: Weights of the individual ranking methods
P R Mode P Mode R
OVERALL 12.77 12.77 20.73 20.73
Further Analysis
NMWT 13.46 13.46 21.63 21.63
NMWS 13.79 13.79 21.59 21.59
RAND 12.85 12.85 20.18 20.18
MAN 12.69 12.69 21.35 21.35
Baselines
WORDNET 9.95 9.95 15.28 15.28
LIN 8.84 8.53 14.69 14.23
Table 2: BEST results
P R Mode P Mode R
OVERALL 49.19 49.19 66.26 66.26
Further Analysis
NMWT 51.13 51.13 68.03 68.03
NMWS 54.01 54.01 70.15 70.15
RAND 51.71 51.71 68.04 68.04
MAN 46.26 46.26 64.24 64.24
Baselines
WORDNET 29.70 29.35 40.57 40.57
LIN 27.70 26.72 40.47 39.19
Table 3: OOT results
Compared to other systems participating in this
task, our system consistently ranks on the first or
second place. SUBFINDER clearly outperforms all
the other systems for the ?mode? evaluation, show-
ing the ability of the system to find the substitute
most often preferred by the human annotators. In
addition, the system exceeds by a large margin all
the baselines calculated for the task, which select
substitutes based on existing lexical resources (e.g.,
WordNet or Lin distributional similarity).
Separate from the ?official? submission, we ran
a second experiment where we optimized the com-
bination weights targeting high precision and recall
(rather than high mode). An evaluation of the system
using this new set of weights yields a precision and
recall of 13.34 with a mode of 21.71 for the best task,
surpassing the best system according to the anony-
mous results report. For the oot task, the precision
and recall increased to 50.30, still maintaining sec-
ond place.
6 Conclusions
The lexical substitution task goes beyond simple
word sense disambiguation. To approach such a
task, we first need a good comprehensive and precise
lexical resource for candidate extraction. Secondly,
we need to semantically filter the highly diverse and
ambiguous set of candidates, while taking into ac-
count their fitness in the context in order to form
a proper linguistic expression. To accomplish this,
we built a system that incorporates lexical, semantic,
and probabilistic methods to capture both the seman-
tic similarity with the target word and the semantic
fit in the context. Compared to other systems partic-
ipating in this task, our system consistently ranks on
the first or second place. SUBFINDER clearly out-
performs all the other systems for the ?mode? eval-
uation, proving its ability to find the substitute most
often preferred by the human annotators.
Acknowledgments
This work was supported in part by the Texas Ad-
vanced Research Program under Grant #003594.
The authors are grateful to the Language and Infor-
mation Technologies research group at the Univer-
sity of North Texas for many useful discussions and
feedback on this work.
References
T. Brants and A. Franz. 2006. Web 1t 5-gram version 1.
Linguistic Data Consortium.
I. Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein,
and C. Strapparava. 2006. Direct word sense match-
ing for lexical substitution. In Proceedings of the In-
ternational Conference on Computational Linguistics
ACL/COLING 2006.
C. Fellbaum. 1998. WordNet, An Electronic Lexical
Database. The MIT Press.
T. K. Landauer, P. Foltz, and D. Laham. 1998. Introduc-
tion to latent semantic analysis. Discourse Processes,
25.
D. McCarthy and R. Navigli. 2007. The semeval English
lexical substitution task. In Proceedings of the ACL
Semeval workshop.
D. McCarthy. 2002. Lexical substitution as a task for
wsd evaluation. In Proceedings of the ACL Workshop
on Word Sense Disambiguation: Recent Successes and
Future Directions, Philadelphia.
R. Mihalcea and A. Csomai. 2005. Senselearner: Word
sense disambiguation for all words in unrestricted text.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics, Ann Arbor,
MI.
413
Coling 2010: Poster Volume, pages 647?655,
Beijing, August 2010
Text Mining for Automatic Image Tagging
Chee Wee Leong and Rada Mihalcea and Samer Hassan
Department of Computer Science and Engineering
University of North Texas
cheeweeleong@my.unt.edu, rada@cs.unt.edu, samer@unt.edu
Abstract
This paper introduces several extractive
approaches for automatic image tagging,
relying exclusively on information mined
from texts. Through evaluations on two
datasets, we show that our methods ex-
ceed competitive baselines by a large mar-
gin, and compare favorably with the state-
of-the-art that uses both textual and image
features.
1 Introduction
With continuously increasing amounts of images
available on the Web and elsewhere, it is impor-
tant to find methods to annotate and organize im-
age databases in meaningful ways. Tagging im-
ages with words describing their content can con-
tribute to faster and more effective image search
and classification. In fact, a large number of ap-
plications, including the image search feature of
current search engines (e.g., Yahoo!, Google) or
the various sites providing picture storage services
(e.g., Flickr, Picasa) rely exclusively on the tags
associated with an image in order to search for rel-
evant images for a given query.
However, the task of developing accurate and
robust automatic image annotation models entails
daunting challenges. First, the availability of large
and correctly annotated image databases is cru-
cial for the training and testing of new annotation
models. Although a number of image databases
have emerged to serve as evaluation benchmarks
for different applications, including image anno-
tation (Duygulu et al, 2002), content-based im-
age retrieval (Li and Wang, 2008) and cross
language information retrieval (Grubinger et al,
2006), such databases are almost exclusively cre-
ated by manual labeling of keywords, requiring
significant human effort and time. The content of
these image databases is often restricted only to a
few domains, such as medical and natural photo
scenes (Grubinger et al, 2006), and specific ob-
jects like cars, airplanes, or buildings (Fergus et
al., 2003). For obvious practical reasons, it is im-
portant to develop models trained and evaluated
on more realistic and diverse image collections.
The second challenge concerns the extraction
of useful image and text features for the construc-
tion of reliable annotation models. Most tradi-
tional approaches relied on the extraction of image
colors and textures (Li and Wang, 2008), or the
identification of similar image regions clustered as
blobs (Duygulu et al, 2002) to derive correlations
between image features and annotation keywords.
In comparison, there are only a few efforts that
leverage on the multitude of resources available
for natural language processing to derive robust
linguistic-based image annotation models. One
of the earliest efforts involved the use of captions
for face recognition in photographs through the
construction of a specific lexicon that integrates
linguistic and photographic information (Srihari
and Burhans, 1994). More recently, several ap-
proaches have proposed the use of WordNet as
a knowledge-base to improve content-based im-
age annotation models, either by removing noisy
keywords through semantic clustering (Jin et al,
2005) or by inducing a hierarchical classification
of candidate labels (Srikanth et al, 2005).
In this paper, we explore the use of several natu-
ral language resources to construct image annota-
tion models that are capable of automatically tag-
ging images from unrestricted domains with good
accuracy. Unlike traditional image annotation
methodologies that generate tags using image-
based features, we propose to extract them in a
manner analogous to keyword extraction. Given a
target image and its surrounding text, we extract
those words and phrases that are most likely to
represent meaningful tags. More importantly, we
647
are interested to investigate the potential of such
linguistic-based models on image annotation ac-
curacy and reliability. Our work is motivated by
the need for annotation models that can be effi-
ciently applied on a very large scale (e.g. har-
vesting images from the web), which are required
in applications that cannot afford the complexity
and time associated with current image process-
ing techniques.
The paper makes the following contributions.
We first propose a new evaluation framework for
image tagging, which is based on an analogy
drawn between the tasks of image labeling and
lexical substitution. Next, we present three extrac-
tive approaches for the task of image annotation.
The methods proposed are based only on the text
surrounding an image, without the use of image
features. Finally, by combining several orthogo-
nal methods through machine learning, we show
that it is possible to achieve a performance that is
competitive to a state-of-the-art image annotation
system that relies on visual and textual features,
thus demonstrating the effectiveness of text-based
extractive annotation models.
2 Related Work
Several online systems have sprung into exis-
tence to achieve annotation of real world images
through human collaborative efforts (Flickr) and
stimulating competition (von Ahn and Dabbish,
2004). Although a large number of image tags can
be generated in short time, these approaches de-
pend on the availability of human annotators and
are far from being automatic. Similarly, research
in the other direction via text-to-image synthesis
(Li and Fei-Fei, 2008; Collins et al, 2008; Mi-
halcea and Leong, 2009) has also helped to har-
vest images, mostly for concrete words, by refin-
ing image search engines.
Most approaches to automatic image annota-
tion have focused on the generation of image la-
bels using annotation models trained with image
features and human annotated keywords (Barnard
and Forsyth, 2001; Jeon et al, 2003; Makadia et
al., 2008; Wang et al, 2009). Instead of predict-
ing specific words, these methods generally target
the generation of semantic classes (e.g. vegeta-
tion, animal, building, places etc), which they can
achieve with a reasonable amount of success. Re-
cent work has also considered the generation of
labels for real-world images (Li and Wang, 2008;
Feng and Lapata, 2008). To our knowledge, we
are unaware of any other work that performs ex-
tractive annotation for images from unrestricted
domains through the exclusive use of textual fea-
tures.
3 Dataset
As the methods we propose are extractive, stan-
dard image databases with no surrounding text
such as Corel (Duygulu et al, 2002) are not suit-
able, nor are they representative for the challenges
associated with raw data from unrestricted do-
mains. We thus create our own dataset using im-
ages randomly extracted from the Web.
To avoid sparse searches, we use a list of the
most frequent words in the British National Cor-
pus as seed words, and query the web using the
Google Image API. A webpage is randomly se-
lected from the query results if it contains a single
image in the specified size range (width and height
of 275 to 1000 pixels1) and its text contains more
than 10 words. Next, we use a Document Object
Model (DOM) HTML parser2 to extract the con-
tent of the webpage. Note that we do not perform
manual filtering of our images except where they
contain undesirable qualities (e.g. porn, corrupted
or blank images).
In total, we collected 300 image-text pairs from
the web. The average image size is 496 pixels
width and 461 pixels height. The average text
length is 278 tokens and the average document ti-
tle length is 6 tokens. In total, there are 83,522
words and the total vocabulary is 8,409 words.
For each image, we also create a gold stan-
dard of manually assigned tags, by using the la-
bels assigned by five human annotators. The im-
age annotation is conducted via Amazon Mechan-
ical Turk, which was shown in the past to produce
reliable annotations (Snow et al, 2008). For in-
creased annotation reliability, we only accept an-
notators with an approval rating of 98%.
Given an image, an annotator extracts from
the associated text a minimum of five words or
collocations.Annotators can choose words freely
from the text, while collocation candidates are re-
stricted to a fixed set obtained from the n-grams (n
? 7) in the text that also appear as article names or
surface forms in Wikipedia. Moreover, when in-
terpreting the image, the annotators are instructed
to focus on both the denotational and conotational
attributes present in the image3.
1Empirically determined to filter advertisements, banners
and undersized images.
2http://search.cpan.org/dist/HTML-ContentExtractor/
3Annotation instructions, dataset and gold standard can
648
Normal Image Mode Image
Gold standard czech (5), festival (5), oklahoma (4), yukon (4),
october (4), web page (2), the first (2), event (2),
success (1), every (1), year (1)
train (5), station (4), steam (4), trans siberian (4),
steam train (4), travel (3), park (3), siberian (3),
old (3), photo (1), trans (2), yekaterinburg (2),
the web (2), photo host (1)
Table 1: Two sample images. The number besides each label indicates the number of human annotators
agreeing on that label. Note that the mode image has a tag (i.e.?train?) in the gold standard set most
frequently selected by the annotators
4 A New Evaluation Framework : Image
Tagging as Lexical Substitution
While evaluations of previous work in image an-
notation were often based on labels provided with
the images, such as tags or image captions, in our
dataset such annotations are either missing or un-
reliable. We rely instead on human-produced ex-
tractive annotations (as described in the previous
section), and formulate a new evaluation frame-
work based on the intuition that an image can be
substituted with one or more tags that convey the
same meaning as the image itself. Ideally, there is
a single tag that ?best? describes the image over-
all (i.e. the gold standard tag agreed by the major-
ity of human annotators), but there are also mul-
tiple tags that describe the fine-grained concepts
present in the image. Our evaluation framework
is inspired by the lexical substitution task (Mc-
Carthy and Navigli, 2007), where a system at-
tempts to generate a word (or a set of words) to
replace a target word, such that the meaning of
the sentence is preserved.
Given this analogy, the evaluation metrics used
for lexical substitution can be adapted to the eval-
uation of image tagging. Specifically, we measure
the precision and the recall of a tagging method
using four subtasks: best normal: provides preci-
sion and recall for the top-ranked tag returned by a
method; best mode: provides precision and recall
only if the top-ranked tag by a method matches the
tag in the gold standard that was most frequently
selected by the annotators; out of ten (oot) nor-
be downloaded at
http://lit.csci.unt.edu/index.php/Downloads
mal: provides precision and recall for the top ten
tags by the system; and out of ten (oot) mode:
similar to best mode, but it considers the top ten
tags returned by the system instead of one. Table
1 show examples of a normal and a mode image.
Formally, let us assume that H is the set
of annotators, namely {h1, h2, h3, ...}, and I,
{i1, i2, i3, ...} is the set of images for which each
human annotator provide at least five tags. For
each ij, we calculate mj, which is the most fre-
quent tag for that image, if available. We also col-
lect all rkj, which is the set of tags for the image
ij from the annotator hk.
Let the set of those images where there is a tag
agreed upon by the most annotators (i.e. the im-
ages with a mode) be denoted by IM, such that
IM ? I. Also, let A ? I be the set of images for
which the system provides more than one tag. Let
the corresponding set for the images with modes
be denoted by AM, such that AM ? IM. Let aj ? A
be the set of system?s extracted tags for the image
ij.
Thus, for each image ij, we have the set of tags
extracted by the system, and the set of tags from
the human annotators. As the next step, the multi-
set union of the human tags is calculated, and the
frequencies of the unique tags is noted. Therefore,
for image ij, we calculate Rj, which is
?
rkj, and
the individual unique tag in Rj, say res, will have
a frequency associated with it, namely freqres.
Given this setting, the precision (P ) and recall
(R) metrics we use are defined below.
649
Best measures:
P =
?
aj :ij?A
?
res?aj
freqres
|aj |
|Rj |
|A|
R =
?
aj :ij?I
?
res?aj
freqres
|aj |
|Rj |
|I|
modeP =
?
bestguessj?AM (1if best guess = mj)
|AM |
modeR =
?
bestguessj?IM (1if best guess = mj)
|IM |
Out of ten (oot) measures:
P =
?
aj :ij?A
?
res?aj
freqres
|Rj |
|A|
R =
?
aj :ij?I
?
res?aj
freqres
|Rj |
|I|
modeP =
?
aj :ij?AM (1if any guess ? aj = mj)
|AM |
modeR =
?
aj :ij?IM (1if any guess ? aj = mj)
|IM |
As a simplified example (with less tags), con-
sider ij showing a picture of a Chihuahua being
labeled by five annotators with the following tags :
Annotator Tags
1 dog,pet
2 chihuahua
3 animal,dog
4 dog,chihuahua
5 dog
In this case, r1j = {dog,pet}, r2j = {chihuahua},
r3j = {animal,dog} and so on. The tag ?dog? ap-
pears the most frequent among the five annotators,
hence mj = {dog}. Rj={dog, dog, dog, dog, chi-
huahua, chihuahua, animal, pet}. The res with
associated frequencies would be dog 4, chihuahua
2, animal 1, pet 1. If the system?s proposed tag for
ij is {dog, animal}, then the numerator of P and
R for best subtask would be
4+1
2
8 = 0.313. Simi-
larly, the numerator of P and R for oot subtask is
4+1
8 = 0.625.
5 Extractive Image Annotation
The main idea underlying our work is that we can
perform effective image annotation using infor-
mation drawn from the associated text. Follow-
ing (Feng and Lapata, 2008), we propose that an
image can be annotated with keywords capturing
the denotative (entities or objects depicted) and
connotative (semantics or ideologies interpreted)
attributes in the image. For instance, a picture
showing a group of athletes and a ball may also be
tagged with words like ?soccer,? or ?sports activ-
ity.? Specifically, we use a combination of knowl-
edge sources to model the denotative quality of a
word as its picturability, and the connotative at-
tribute as its saliency. The idea of visualness and
salience as textual features for discovering named
entities in an image was first pursued by (De-
schacht and Moens, 2007), using data from the
news domain. In contrast, we are able to per-
form annotation of images from unrestricted do-
mains using content words (nouns, verbs and ad-
jectives). In the following, we first describe three
unsupervised extractive approaches for image an-
notation, followed by a supervised method using a
re-ranking hypothesis that combines all the meth-
ods.
5.1 Flickr Picturability
Featuring a repository of four billion images,
Flickr (http://www.flickr.com) is one of the most
comprehensive image resources on the web. As a
photo management and sharing application, it pro-
vides users with the ability to tag, organize, and
share their photos online. Interestingly, an inspec-
tion of Flickr tags for randomly selected images
reveal that users tend to describe the denotational
attributes of images, using concrete and picturable
words such as cat, bug, car etc. This observation
lends evidence to Flickr?s suitability as a resource
to model the picturability of words.
Given the text (T ) of an image, we can use
the getRelatedTags API to retrieve the most fre-
quent Flickr tags associated with a given word,
and use them as corpus evidence to filter or pro-
mote words in the text. In the filtering phase
we ignore any words that return an empty list of
Flickr?s related tags, based on the assumption that
these words are not used in the Flickr tags repos-
itory. We also discard words with a length that is
less than three characters (?=3). In the promotion
phase, we reward any retrieved tags that appear as
surface forms in the text. This reward is propor-
tional to the term frequency of these tags in the
650
Algorithm 1 Flickr Picturability Algorithm
Start : L[]=? , TF[]=tf of each word in T
for each word in T do
if length(word) ? ? then
RelatedTags=getRelatedTags(word);
if size(RelatedTags) > 0 then
L[word]+=?*TF[word]
for each tag in RelatedTags do
if exists TF [tag] then
L[tag]+=TF[tag]
end if
end for
end if
end if
end for
text. Additionally, we also include in the final la-
bel set any word that returns a non-empty related
tags set with a discounted weight (?=0.5) of its
term frequency, to the end of enriching our labels
set while assuring more credit are given to the pic-
turable words.
To extract multiword labels, we locate all n-
grams formed exclusively from our extracted set
of possible labels. The subsequent score for each
of these n-grams is:
L[wi..wi+k] = (
j=i+k?
j=i
L[wj])/k
By reverse sorting the associative array in L, we
can retrieve the top K words to label the image.
For illustration, let us consider the following text
snippet.
On the Origin of Species, published by
Charles Darwin in 1859, is considered
to be the foundation of evolutionary bi-
ology.
After removing stopwords, we consider the re-
maining words as candidate labels. For each
of these candidates wi (i.e. origin, species,
published, charles, darwin, foundation,
evolutionary, and biology), we query Flickr and
obtain their related tag set Ri. origin, published,
and foundation return an empty set of related
tags and hence are removed from our set of can-
didate labels, leaving species, charles, darwin,
evolutionary, and biology as possible annotation
keywords with the initial score of 0.5. In the pro-
motion phase, we score each wi based on the num-
ber of votes it receives from the remaining wj
(Figure 1). Each vote represents an occurrence
of the candidate tag wi in the related tag set Rj
of the candidate tag wj . For example, darwin
appeared in the Flickr related tags for charles,
evolutionary, and biology, hence it has a weight
of 3.5. The final list of candidate labels are shown
in Table 2.
... Species, published by Charles Darwin ? founda!on of evolu!onary biology
Figure 1: Flickr Picturability Labels
Label S(wi)
darwin 3.5
charles darwin 2.5
charles 1.5
biology 1.5
evolutionary biology 1.0
evolutionary 0.5
species 0.5
Table 2: Candidate labels obtained for a sample
text using the Flickr model
5.2 Wikipedia Salience
We hypothesize that an image often describes the
most important concepts in the associated text.
Thus, the keywords selected from a text could be
used as candidate labels for the image. We use
a graph-based keyword extraction method similar
to (Mihalcea and Tarau, 2004), enhanced with a
semantic similarity measure. Starting with a text,
we extract all the candidate labels and add them as
vertices in the graph. A measure of word similar-
ity is then used to draw weighted edges between
the nodes. Using the PageRank algorithm, the
words are assigned with a score indicating their
salience within the given text.
To determine the similarity between words, we
use a directed measure of similarity. Most word
similarity metrics provide a single-valued score
between a pair of words w1 and w2 to indicate
their semantic similarity. Intuitively, this is not al-
ways the case, as w1 may be represented by con-
cepts that are entirely embedded in other concepts,
represented by w2. In psycholinguistics terms, ut-
tering w1 may bring to mind w2, while the appear-
ance of w2 without any contextual clues may not
associate with w1. For example, Obama brings
to mind the concept of president, but president
651
may trigger other concepts such as Washington,
Lincoln, Ford etc., depending on the existing
contextual clues. Thus, the degree of similarity
of w1 with respect to w2 should be separated from
that of w2 with respect to w1. Specifically, we use
the following measure of similarity, based on the
Explicit Semantic Analysis (ESA) vectors derived
from Wikipedia (Gabrilovich and Markovitch,
2007):
DSim(wi, wj) =
Cij
Ci
? Sim(wi, wj)
where Cij is the count of articles in Wikipedia
containing words wi and wj , Ci is the count of ar-
ticles containing words wi, and Sim(wi, wj) is the
cosine similarity of the ESA vectors representing
the input words.The directional weight (Cij /Ci)
amounts to the degree of association of wi with re-
spect to wj . Using the directional inferential sim-
ilarity scores as directed edges and distinct words
as vertices, we obtain a graph for each text. The
directed edges denotes the idea of ?recommenda-
tion? where we say w1 recommends w2 if and
only if there is a directed edge from w1 to w2, with
the weight of the recommendation being the direc-
tional similarity score. Starting with this graph,
we use the graph iteration algorithm from (Mi-
halcea and Tarau, 2004) to calculate a score for
each vertex in the graph. The output is a sorted
list of words in decreasing order of their ranks,
which are used as candidate labels to annotate the
image. This is achieved by using Cj instead of Ci
for the denominator in the directional weight. As
an example, consider the text snippet :
Microsoft Corporation is a multina-
tional computer technology corporation
that develops, manufactures, licenses,
and supports a wide range of software
products for computing devices
after stopword removal, the list of nouns ex-
tracted is Microsoft, computer, corporation, de-
vices, products, technology, software. Note that
the top-ranked word must infer some or all of the
words in the text. In this case, the word Microsoft
infers the terms computer, technology and soft-
ware.
To calculate the semantic relatedness between
two collocations, we use a simplified version of
the text-to-text relatedness technique proposed by
and (Mihalcea et al, 2006) that incorporate the
directional inferential similarity as an underlying
semantic metric.
5.3 Topical Modeling
Intuitively, every text is written with a topic in
mind, and the associated image serves as an illus-
tration of the text meaning. In this paper, we in-
vestigate the effect of topical modeling on image
annotation accuracy directly. We use the Pachinko
Allocation Model (PAM) (Li and McCallum,
2006) to model the topics in a text, where key-
words forming the dominant topic are assumed as
our set of annotation keywords. Compared with
previous topic modeling approaches, such as La-
tent Dirichlet alocation (LDA) or its improved
variant Correlated Topic Model (CTM) (Blei and
Lafferty, 2007), PAM captures correlations be-
tween all the topic pairs using a directed acyclic
graph (DAG). It also supports finer-grained topic
modeling, and has state-of-the-art performance on
the tasks of document classification and topical
keyword coherence. Given a text, we use the PAM
model to infer a list of super-topics and sub-topics
together with words weighted according to the
likelihood that they belong to each of these topics.
For each text, we retrieve the top words belong-
ing to the dominant super-topic and sub-topic. We
use 50 super-topics and 100 sub-topics as operat-
ing parameters for PAM, since these values were
found to provide good results in previous work on
topic modeling. Default values are used for other
parameters in the model.
5.4 Supervised Learning
The three tagging methods target different aspects
of what constitutes a good label for an image. We
use them as features in a machine learning frame-
work, and introduce a final rank attribute S(tj),
which is a linear combination of the reciprocals of
the rank of each tag as given by each method,
S(tj) =
?
m?methods
?m
1
rmtj
where rmtj is the rank for tag tj given by method
m. The weight of each method ?m is estimated
from the training set using information gain val-
ues. Since our predicted variable (mode precision
or recall) is continuous, we use the Support Vec-
tor Algorithm (nu-SVR) implementation of SVM
(Chang and Lin, 2001) to perform regression anal-
ysis on the weights for each method via a radial
basis function kernel. A ten-fold cross-validation
is applied on the entire dataset of 300 images.
652
Best out-of-ten (oot)
Normal Mode Normal Mode
Models P R P R P R P R
Flickr picturability 6.32 6.32 78.57 78.57 35.61 35.61 92.86 92.86
Wikipedia Salience 6.40 6.40 7.14 7.14 35.19 35.19 92.86 92.86
Topic modeling 5.99 5.99 42.86 42.86 37.13 37.13 85.71 85.71
Combined (SVM) 6.87 6.87 67.49 67.49 37.85 37.85 100.00 100.00
Doc Title 6.40 6.40 75.00 75.00 18.97 18.97 82.14 82.14
tf * idf 5.94 5.94 14.29 14.29 38.40 38.40 78.57 78.57
Random 3.76 3.76 3.57 3.57 30.20 30.20 50.00 50.00
Upper bound (human) 12.23 12.07 81.48 81.48 82.44 81.55 100.00 100.00
Table 3: Results obtained on the Web dataset
6 Experiments and Evaluations
We evaluate the performance of each of the three
tagging methods separately, followed by an eval-
uation of the combined method. Each system pro-
duces a ranked list of K words or collocations
as tags assigned to a given image. A system can
discretionary generate less (but not more) than K
tags, depending on its confidence level.
For comparison, we implement three baselines:
tf*idf, Doc Title and Random. For tf*idf, we use
the British National Corpus to calculate the idf
scores, while the frequency of a term is calcu-
lated from the entire text associated with an im-
age. The Doc Title baseline is similar, except that
the term frequency is calculated based on the title
of the document. The Random baseline randomly
selects words from a co-occurrence window of
size K before and after an image as its annota-
tion. Following other tagging methods, we apply a
pre-processing stage, where we part-of-speech tag
the text (to retain only nouns), followed by stem-
ming. We also determine an upper bound, which
is calculated as follows. For each image, the la-
bels assigned by each of the five annotators are
in turn evaluated against a gold standard consist-
ing of the annotations of the other four annotators.
The best performing annotator is then recorded.
This process is repeated for each of the 300 im-
ages, and the average precision and recall are cal-
culated. This represents an upper bound, as it is
the best performance that a human can achieve on
this dataset. Table 3 shows our experimental re-
sults.
Among the individual methods, the method im-
plementing Flickr picturability has the highest in-
dividual score for best and oot modes, yielding
a precision and recall of 78.57% and 92.86% re-
spectively. The Wikipedia Saliency method also
scores the highest (jointly with Flickr) in the oot
mode, but for the best mode achieves a score only
marginally better than the random baseline. A
plausible explanation is that it tends to favor ?all-
inferring? over-specific labels, while the most fre-
quently selected tags in mode pictures are typi-
cally more ?picturable? than being specific (e.g.
?train? for the mode picture in Table 1). The topic
modeling method has mixed results: its scores
for oot normal and mode are somewhat compet-
itive with tf*idf, but it scores consistently lower
than the DocTitle in the best subtask, possibly
due to the absence of a more sophisticated re-
ranking algorithm tailored for the image annota-
tion task other than the intrinsic ranking mecha-
nism in PAM. It is worth noting that the combined
supervised system provides the overall best results
(6.87%) on the best normal, and achieves a perfect
precision and recall (100%) for oot mode, which
means perfect agreement with the human tagging.
7 Comparison with Related Work
We also compare our work against (Feng and Lap-
ata, 2008) as it allows for a direct comparison with
models using both image and textual features un-
der a standard evaluation framework. We obtained
the BBC dataset used in their experiments, which
consists of 3121 training and 240 testing images.
In this dataset, images are implicitly tagged with
captions by the author of the corresponding BBC
article. The evaluations are run against these cap-
tions.
In their experiments, Feng and Lapata created
four annotation models. The first two (tf*idf and
Document Title) are the same as used in our base-
line experiments. The third model (Lavrenko03)
is an application of the continuous relevance
model in (Jeon et al, 2003), trained with the BBC
image features and captions. Finally, the forth
(ExtModel) is an extension of the relevance model
using additional information in auxiliary texts.
Briefly, the model assumes a multiple Bernoulli
distribution for words in a caption, and generates
tags for a test image using a weighted combina-
tion of the accompanying document, caption and
image features learned during training.
653
Top 10 Top 15 Top 20
Models P R F1 P R F1 P R F1
tf*idf 4.37 7.09 5.41 3.57 8.12 4.86 2.65 8.89 4.00
DocTitle 9.22 7.03 7.20 9.22 7.03 7.20 9.22 7.03 7.20
Lavrenko03 9.05 16.01 11.81 7.73 17.87 10.71 6.55 19.38 9.79
ExtModel 14.72 27.95 19.82 11.62 32.99 17.18 9.72 36.77 15.39
Flickr picturability 12.13 22.82 15.84 9.52 26.82 14.05 8.23 29.80 12.90
Wikipedia Salience 11.63 21.89 15.18 9.28 26.20 13.70 7.81 29.41 12.35
Topic Modeling 11.42 21.49 14.91 9.28 26.20 13.70 7.86 29.57 12.42
Combined (SVM) 13.38 25.17 17.47 11.08 31.29 16.37 9.50 35.76 15.01
Table 4: Results obtained on the BBC dataset used in (Feng and Lapata, 2008)
The experimental setup is similar to the earlier
section, but a few modifications are made for a fair
and direct comparison. First, we extend our mod-
els coverage to include content words (i.e. nouns,
verbs, adjectives) determined using the Tree Tag-
ger (Schmid, 1994). Second, no collocations are
used. Third, we adopt the evaluation framework
used by Feng and Lapata to extract the top 10, 15
and 20 tags. Note that in our methods, the extrac-
tion of tags for a test image is only done on the
document surrounding the image, after excluding
the caption. As the number of negative examples
(words not present in the caption) greatly outnum-
ber the positive instances, we employ an under-
sampling method (Kubat and Matwin, 1997) to
balance the dataset for training.
The results are shown in Table 4. Interest-
ingly, all our unsupervised extraction-based mod-
els perform consistently above the supervised
Lavrenko03 model, indicating that textual fea-
tures are more informative than captions and im-
age features taken together. Comparing with mod-
els using significantly less document informa-
tion (tf*idf and Doc title), our models gain even
greater advantage. Note that the title of any BBC
article does not exceed 10 words, hence compar-
ison is only meaningful given the top 10 tags re-
trieved.
Feng and Lapata used LDA to perform rerank-
ing of final candidates in their ExtModel. How-
ever, when used as a model alone, the PAM topic
model achieved promising scores in all the cate-
gories, performing best for top 10 keywords (F1
of 14.91%). Flickr picturability stands out as
the best performing unsupervised method, scor-
ing the highest precision (12.13%, top 10), recall
(29.80%, top 20) and F1 (15.84%, top 10).
Overall, this comparative evaluation yields
some important insights. First, our combined
model using SVM is statistically better (p<0.1 for
top 10, 15, 20) than the Laverenko03 model, but
not statistically different from the ExtModel. This
demonstrates the effectiveness of textual-based
models over traditional models trained with im-
age features and captions. While it is intuitively
clear that image features help in improving tag-
ging performance, we show that mining only the
text surrounding an image, where it exists, can
yield a performance that is comparable to a state-
of-the-art system that uses both textual and vi-
sual features. Moreover, an increase in complex-
ity of a model by using more features may hinder
its applicability to large datasets, but not neces-
sarily improving annotation performance (Maka-
dia et al, 2008). On this, text-based annotation
models can provide a desirable compromise. For
instance, our unsupervised models implementing
Flickr picturability and Wikipedia Salience are
able to extract annotations from a BBC article (av-
erage 133.85 tokens) in approximately 1 second
and 20 seconds respectively.
8 Conclusions and Future Work
In this paper, we introduced several text-based ex-
tractive approaches for automatic image annota-
tion and showed that they compare favorably with
the state-of-the-art in image annotation using both
text and image features. We believe our work
has practical applications in mining and annotat-
ing images over the Web, where texts are nat-
urally associated with images, and scalability is
important. Our next direction seeks to derive ro-
bust annotation models using additional ontolog-
ical knowledge-bases. We would also like to ad-
vance the the state-of-the-art by augmenting cur-
rent textual models with image features.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340. Any opinions, findings,
and conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views of the National Sci-
ence Foundation.
654
References
Kobus Barnard and David Forsyth. 2001. Learning the
semantics of words and pictures. In Proceedings of
International Conference on Computer Vision.
David Blei and John Lafferty. 2007. A correlated topic
model of science. In Annals of Applied Statistics,
volume 1, pages 17?35.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM:
a library for support vector machines.
Brendan Collins, Jia Deng, Kai Li, and Li Fei-Fei.
2008. Towards scalable dataset construction: An
active learning approach. In Proceedings of Euro-
pean Conference on Computer Vision.
Koen Deschacht and Marie-Francine Moens. 2007.
Text analysis for automatic image annotation. In
Proceedings of the Association for Computational
Linguistics.
Pinar Duygulu, Kobus Barnard, Nando de Freitas, and
David Forsyth. 2002. Object recognition as ma-
chine translation:learning a lexicon for a fixed im-
age vocabulary. In Proceedings of the 7th European
Conference on Computer Vision.
Yansong Feng and Mirella Lapata. 2008. Automatic
image annotation using auxiliary text information.
In Proceedings of the Association for Computa-
tional Linguistics.
Rob Fergus, Pietro Perona, and Andrew Zisserman.
2003. Object class recognition by unsupervised
scale-invariant learning. In Proceedings of the In-
ternational Conference on Computer Vision and
Pattern Recognition.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In International
Joint Conferences on Artificial Intelligence.
Michael Grubinger, Clough Paul, Mller Henning, and
Deselaers Thomas. 2006. The iapr benchmark: A
new evaluation resource for visual information sys-
tems. In International Conference on Language Re-
sources and Evaluation.
Jiwoon Jeon, Victor Lavrenko, and R Manmatha.
2003. Automatic image annotation and retrieval us-
ing cross-media relevance models. In Proceedings
of the ACM SIGIR Conference on Research and De-
velopment in Information Retrieval.
Yohan Jin, Latifur Khan, Lei Wang, and Mamoun
Awad. 2005. Image annotations by combining mul-
tiple evidence & wordnet. In Proceedings of Annual
ACM Multimedia.
Miroslav Kubat and Stan Matwin. 1997. Addressing
the curse of imbalanced training sets: one-sided se-
lection. In Proceedings of International Conference
on Machine Learning.
Li-Jia Li and Li Fei-Fei. 2008. Optimol: au-
tomatic online picture collection via incremental
model learning. In International Journal of Com-
puter Vision.
Wei Li and Andrew McCallum. 2006. Pachinko allo-
cation: Dag-structured mixture models of topic cor-
relations. In Proceedings of the International Con-
ference on Machine learning.
Jia Li and James Wang. 2008. Real-time computer-
ized annotation of pictures. In Proceedings of Inter-
national Conference on Computer Vision.
Ameesh Makadia, Vladimir Pavlovic, and Sanjiv Ku-
mar. 2008. A new baseline for image annotation. In
Proceedings of European Conference on Computer
Vision.
Diana McCarthy and Roberto Navigli. 2007. The se-
meval English lexical substitution task. In Proceed-
ings of the ACL Semeval workshop.
Rada Mihalcea and Chee Wee Leong. 2009. To-
wards communicating simple sentences using pic-
torial representations. In Machine Translation, vol-
ume 22, pages 153?173.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. In Proceedings of Em-
pirical Methods in Natural Language Processing.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceed-
ings of Association for the Advancement of Artificial
Intelligence, pages 775?780.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast - but is it good?
evaluating non-expert annotations for natural lan-
guage tasks. In Proceedings of Empirical Methods
in Natural Language Processing.
Srihari and Burhans. 1994. Visual semantics: Extract-
ing visual information from text accompanying pic-
tures. In Proceedings of the American Association
for Artificial Intelligence.
Munirathnam Srikanth, Joshua Varner, Mitchell Bow-
den, and Dan Moldovan. 2005. Exploiting ontolo-
gies for automatic image annotation. In Proceed-
ings of the ACM Special Interest Group on Research
and Development in Information Retrieval.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of the
ACM Special Interest Group on Computer Human
Interaction.
Chong Wang, David Blei, and Li Fei-Fei. 2009. Si-
multaneous image classification and annotation. In
Proceedings of IEEE Conference on Computer Vi-
sion and Pattern Recognition.
655
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 20?29,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Measuring Semantic Relatedness using Multilingual Representations
Samer Hassan
University of North Texas
Denton, TX
samer@unt.edu
Carmen Banea
University of North Texas
Denton, TX
carmenbanea@my.unt.edu
Rada Mihalcea
University of North Texas
Denton, TX
rada@cs.unt.edu
Abstract
This paper explores the hypothesis that se-
mantic relatedness may be more reliably in-
ferred by using a multilingual space, as com-
pared to the typical monolingual representa-
tion. Through evaluations using several state-
of-the-art semantic relatedness systems, ap-
plied on standard datasets, we show that a
multilingual approach is better suited for this
task, and leads to improvements of up to 47%
with respect to the monolingual baseline.
1 Introduction
Semantic relatedness is the task of quantifying the
strength of the semantic connection between tex-
tual units, be they words, sentences, or documents.
For instance, one may want to determine how se-
mantically related are two words such as car and
automobile, or two pieces of text such as I love an-
imals and I own a pet. It is one of the main tasks
explored in the field of natural language processing,
as it lies at the core of a large number of applica-
tions such as information retrieval (Ponte and Croft,
1998), query reformulation (Metzler et al, 2007;
Yih and Meek, 2007; Sahami and Heilman, 2006;
Broder et al, 2008), image retrieval (Leong and Mi-
halcea, 2009; Goodrum, 2000), plagiarism detection
(Hoad and Zobel, 2003; Shivakumar and Garcia-
Molina, 1995; Broder et al, 1997; Heintze, 1996;
Brin et al, 1995; Manber, 1994), information flow
(Metzler et al, 2005), sponsored search (Broder et
al., 2008), short answer grading (Mohler and Mihal-
cea, 2009a; Pulman and Sukkarieh, 2005; Mitchell
et al, 2002), and textual entailment (Dagan et al,
2005).
The typical approach to semantic relatedness is to
either measure the distance between the constituent
words by using a knowledge base such as Word-
Net or Roget (e.g., (Leacock and Chodorow, 1998;
Lesk, 1986; Jarmasz and Szpakowicz, 2003; Peder-
sen et al, 2004)), or to calculate the similarity be-
tween the word distributions in very large corpora
(e.g., (Landauer et al, 1991; Lin, 1998; Gabrilovich
and Markovitch, 2007)). With almost no exception,
these methods have been applied on one language at
a time ? English, most of the time, although mea-
sures of relatedness have also been explored on lan-
guages such as German (Zesch et al, 2007), Chinese
(Li et al, 2005), Japanese (Kazama et al, 2010), and
others.
In this paper, we take a step further and ex-
plore a joint multilingual semantic relatedness met-
ric, which aggregates semantic relatedness scores
measured on several different languages. Specifi-
cally, in our method, in order to measure the re-
latedness of two textual units, we first determine
their relatedness in multiple languages, and conse-
quently infer a final relatedness score by averaging
the scores calculated in the individual languages.
Our hypothesis is that a multilingual representa-
tion can enrich the relatedness space and address
relevant issues such as polysemy (i.e., find that two
occurrences of the same word in language L1 rep-
resent two different meanings because of different
translations in language L2) and synonymy (i.e., find
that two words in language L1 are related because
they have the same translation in language L2). We
show that by measuring relatedness in a multilingual
space, we are able to improve over a traditional re-
latedness measure that relies exclusively on a mono-
lingual representation.
Through experiments using several state-of-the-
art measures of relatedness, applied on a multilin-
gual space including English, Arabic, Spanish, and
Romanian, we aim to answer the following research
20
questions: (1) Does the task of semantic relatedness
benefit from a multilingual representation, as com-
pared to a monolingual one? (2) Does the translation
quality affect the results? and (3) Do the findings
hold for different relatedness datasets?
The paper is organized as follows. First, we
overview related work on word and text related-
ness, and on multilingual natural language process-
ing. We then briefly describe three corpus-based
measures of relatedness, and present several word
and text datasets that have been used in the past to
evaluate relatedness. We then present evaluations
and experiments addressing each of the three re-
search questions, and discuss our findings.
2 Related Work
Semantic relatedness. The approaches for seman-
tic relatedness that have been considered to date
can be grouped into knowledge-based and corpus-
based. Knowledge-based methods derive a measure
of relatedness by utilizing lexical resources and on-
tologies such as WordNet (Miller, 1995) to mea-
sure definitional overlap (Lesk, 1986), term dis-
tance within a graphical taxonomy (Leacock and
Chodorow, 1998), term depth in the taxonomy as a
measure of specificity (Wu and Palmer, 1994), and
others. The application of such measures to a lan-
guage other than English requires the availability of
the lexical resource in that language; furthermore,
even though taxonomies such as WordNet (Miller,
1995) are available in a number of languages1, their
coverage is still limited, and often times they are not
publicly available. For these reasons, in multilingual
settings, these measures often become untractable.
On the other side, corpus-based measures
such as Latent Semantic Analysis (LSA) (Lan-
dauer et al, 1991), Explicit Semantic Analy-
sis (ESA) (Gabrilovich and Markovitch, 2007),
Salient Semantic Analysis (SSA) (Hassan and Mi-
halcea, 2011), Pointwise Mutual Information (PMI)
(Church and Hanks, 1990), PMI-IR (Turney, 2001),
Second Order PMI (Islam and Inkpen, 2006), Hy-
perspace Analogues to Language (HAL) (Burgess
et al, 1998) and distributional similarity (Lin, 1998)
employ probabilistic approaches to decode the se-
mantics of words. They consist of unsupervised
methods that utilize the contextual information and
patterns observed in raw text to build semantic pro-
files of words, and thus they can be easily transferred
1http://www.illc.uva.nl/EuroWordNet/
to a new language provided that a large corpus in that
language is available.
Multilingual natural language processing. Also
relevant is the work done on multilingual text pro-
cessing, which attempts to improve the performance
of different natural language processing tasks by
integrating information drawn from multiple lan-
guages. For instance, (Cohn and Lapata, 2007) ex-
plore the use of triangulation for machine transla-
tion, where multiple translation models are learned
using multilingual parallel corpora. The model was
found especially beneficial for languages where the
training dataset was small, thus suggesting that this
method may be particularly useful for languages
with scarce resources. (Davidov and Rappoport,
2009) experiment with the use of multiple languages
to enhance an existing lexicon. In their experiments,
using three source languages and 45 intermediate
languages, they find that the multilingual resources
can lead to significant improvements in concept ex-
pansion. (Banea et al, 2010) explore the use of
parallel multilingual corpora to improve subjectivity
classification in a target language, finding that the
use of multilingual representations for subjectivity
analysis improves over the monolingual classifiers.
Similarly, (Banea and Mihalcea, 2011) investigate
the use of multilingual contexts for word sense dis-
ambiguation. By leveraging on the translations of
the annotated contexts in multiple languages, a mul-
tilingual thematic space emerges that better disam-
biguates target words.
Finally, there are two lines of work that explore
semantic distances in a multilingual space. First,
(Besanc?on and Rajman, 2002) examine the notion
that the distances between document vectors within
a language correlate with the distances between their
corresponding vectors in a parallel corpus. These
findings provide clues about the possibility of reli-
able semantic knowledge transfer across language
boundaries. Second, (Hassan and Mihalcea, 2009)
propose a framework to compute semantic relat-
edness between two words in different languages,
by considering Wikipedia articles in multiple lan-
guages. The method differs from the one proposed
here, as we aggregate relatedness over monolingual
spaces rather than measuring cross-lingual related-
ness, and we do not specifically use the inter-wiki
links between Wikipedia pages.
21
3 Measures of Text Relatedness
In this work, we focus on corpus-based metrics
because of their unsupervised nature, their flexi-
bility, scalability, and portability to different lan-
guages. Specifically, we utilize three popular mod-
els, LSA (Landauer et al, 1991), ESA (Gabrilovich
and Markovitch, 2007), and SSA (Hassan and Mi-
halcea, 2011). In these models, the semantic profile
of a word is expressed in terms of the explicit (ESA),
implicit (LSA), or salient (SSA) concepts. All three
models are trained on the Wikipedia 2010 corpora
corresponding to the four languages of interest (En-
glish, Arabic, Spanish, Romanian).
Explicit Semantic Analysis. ESA (Gabrilovich
and Markovitch, 2007) uses encyclopedic knowl-
edge in an information retrieval framework to gen-
erate a semantic interpretation of words. Since en-
cyclopedic knowledge is typically organized into
concepts (or topics), each concept is described us-
ing definitions and examples. ESA relies on the
distribution of words inside the encyclopedic de-
scriptions. It builds semantic representations for
a given word using a word-document association,
where each document represents a Wikipedia article.
In this vector representation, the semantic interpre-
tation of a text can be modeled as an aggregation of
the semantic vectors of its individual words.
Latent Semantic Analysis. In LSA (Landauer et
al., 1991), term-context associations are captured by
means of a dimensionality reduction operated by a
singular value decomposition (SVD) on the term-by-
context matrix T, where the matrix is induced from
a large corpus. This reduction entails the abstraction
of meaning by collapsing similar contexts and dis-
counting noisy and irrelevant ones, hence transform-
ing the real world term-context space into a word-
latent-concept space which achieves a much deeper
and concrete semantic representation of words.
Salient Semantic Analysis. SSA (Hassan and
Mihalcea, 2011) incorporates a similar semantic
abstraction and interpretation of words, by using
salient concepts gathered from encyclopedic knowl-
edge, where a concept is defined as an unambigu-
ous word or phrase with a concrete meaning, which
can afford an encyclopedic definition. The links
available between Wikipedia articles, obtained ei-
ther through manual annotation by the Wikipedia
users or using an automatic annotation process, are
regarded as clues or salient features within the text
that help define and disambiguate its context. This
method seeks to determine the semantic relatedness
of words by measuring the distance between their
concept-based profiles, where a profile consists of
co-occurring salient concepts found within a given
window size in a very large corpus.
4 Datasets
To evaluate the representation strength of a multilin-
gual semantic relatedness model we employ several
standard word-to-word and text-to-text datasets. For
each of these datasets, we make use of their repre-
sentation in the four languages of interest.
4.1 Word Relatedness
We construct our multilingual word-to-word
datasets building upon three word relatedness
datasets that have been widely used in the past.
Rubenstein and Goodenough (Rubenstein and
Goodenough, 1965) (RG65) consists of 65 word
pairs ranging from synonymy pairs (e.g., car -
automobile) to completely unrelated words (e.g.,
noon - string). The participating terms in all the
pairs are non-technical nouns annotated by 51 hu-
man judges on a scale from 0 (unrelated) to 4 (syn-
onyms).
Miller-Charles (Miller and Charles, 1991) (MC30)
is a subset of RG65, consisting of 30 word pairs an-
notated for relatedness by 38 human subjects, using
the same 0 to 4 scale.
WordSimilarity-353 (Finkelstein et al, 2001)
(WS353), also known as Finkelstein-353, consists
of 353 word pairs annotated by 13 human experts,
on a scale from 0 (unrelated) to 10 (synonyms).
While containing the MC30 set, it poses an addi-
tional degree of difficulty by also including phrases
(e.g., ?Wednesday news?), proper names and tech-
nical terms.
To enable a multilingual representation, we use
the multilingual datasets introduced by (Hassan and
Mihalcea, 2009), which are based upon MC30 and
WS353. These multilingual datasets are built us-
ing manual translations, following the same guide-
lines adopted for the generation and the annotation
of their original English counterparts. These manu-
ally translated collections, available in Arabic, Span-
ish, and Romanian, allow us to infer an upper bound
for the multilingual semantic relatedness model.
Moreover, in order to provide a more realistic
scenario, where manual translations are not avail-
able, we also create multilingual datasets by auto-
matically translating the three English datasets into
22
Arabic, Spanish and Romanian.2 Similar to how the
manually translated datasets were created by provid-
ing the bilingual speakers with one word pair at a
time, for the automatic translation each word pair is
processed as a single query to the translation engine.
Thus, the co-occurrence metrics derived from large
corpora are able to play a role in providing a dis-
ambiguated translation instead of defaulting to the
most frequently used sense if the words were to be
processed individually. This allows for the embed-
ded word pair relatedness to be transferred to other
languages as well.
4.2 Text Relatedness
We use three standard text-to-text datasets.
Lee50 (Lee and Welsh, 2005) is a compilation of
50 documents collected from the Australian Broad-
casting Corporation?s news mail service. Each doc-
ument is scored by ten annotators on a scale from 1
(unrelated) to 5 (alike) based on its semantic related-
ness to all the other documents. The users? annota-
tion is then averaged per document pair, resulting in
2,500 document pairs annotated with their similarity
scores. Since it was found that there was no signif-
icant difference between annotations given a differ-
ent order of the documents in a pair (Lee and Welsh,
2005), the evaluations are carried out on only 1225
document pairs after ignoring duplicates.
Li30 (Li et al, 2006) is a sentence pair similar-
ity dataset obtained by replacing each of the RG65
word-pairs with their respective definitions extracted
from the Collins Cobuild dictionary (Sinclair, 2001).
Each sentence pair was scored between 0 (unrelated)
to 4 (alike) by 32 native English speakers, and their
annotations were averaged. Due to the skew in the
scores toward low similarity sentence-pairs, they se-
lected a subset of 30 sentences from the 65 sentence
pairs to maintain an even relatedness distribution.
AG400 (Mohler and Mihalcea, 2009b) is a domain
specific dataset from the field of computer science,
used to evaluate the application of semantic relat-
edness measures to real world applications such as
short answer grading. We employ the version pro-
posed by (Hassan and Mihalcea, 2011) which con-
sists of 400 student answers along with the corre-
sponding questions and correct instructor answers.
Each student answer was graded by two judges on
a scale from 0 (completely wrong) to 5 (perfect an-
swer). The correlation between human judges was
2For all the automatic translations we used the Google
Translate service.
measured at 0.64.
First, we construct a multilingual, manually trans-
lated text-to-text relatedness dataset based on the
standard Li30 corpus.3 Native speakers of Spanish,
Romanian and Arabic, who were also highly profi-
cient in English, were asked to translate the entries
drawn from the English collection. They were pre-
sented with one sentence at a time, and asked to pro-
vide the appropriate translation into their native lan-
guage. Since we had five Spanish, two Arabic, and
two Romanian translators, an arbitrator (native to the
language) was charged with merging the candidate
translations by proposing one sentence per language.
Furthermore, to test the abstraction of semantics
from the choice of underlying language, we asked
three different Spanish human experts to re-score the
Spanish text-pair translations on the same scale used
in the construction of the English collection. The
correlation between the relatedness scores assigned
during this experiment and the scores assigned to the
original English experiment was 0.77 ? 0.86, indi-
cating that the translations provided by the bilingual
judges were correct and preserved the semantics of
the original English text-pairs. As was the case
for the manually constructed word-to-word datasets
previously described, the metrics obtained on the
manually translated Li30 dataset will also act as an
upper bound for the text-to-text evaluations.
Finally, for a more sensible scenario where the
text fragments do not require manual translations
in order to compute their semantic relatedness, we
create a multilingual version of the three English
datasets by employing statistical machine translation
to translate the texts into the other three languages.
Each text pair was processed through two separate
queries to the translation engine, since the two text
fragments contain sufficient information to prompt
an in-context translation on their own.
5 Framework
We generate SSA, LSA and ESA vectorial models
for English, Romanian, Arabic, and Spanish, using
the same Wikipedia 2010 versions for all the sys-
tems (e.g., the SSA, LSA and ESA relatedness
measures for Spanish are all trained on the same
Spanish Wikipedia version).
We construct a multilingual model by considering
a word- or text-pair from a source language along
3Dataset is available for download at lit.csci.unt.
edu/index.php?P=research/downloads
23
with its translations in the other languages. To eval-
uate this multilingual model in a way that reduces
the bias that may arise from choosing one language
over the other, we do the following: we start from a
source language and generate all the possible combi-
nations of this language with the available language
set {ar, en, es, ro}. Within each combination, we
average the monolingual model scores for the lan-
guages in this combination with respect to the target
word- or text-pair into a final relatedness score.
For example, let us consider Spanish as the source
language, then the possible combinations of the lan-
guages that include the source language will be
{{es}, {es, ar}, {es, ro}, {es, en}, {es, ar, en},
{es, ar, ro}, {es, en, ro}, and {es, ar, en, ro}}.
For each possible combination, we aggregate the
scores of the languages in that combination. In this
setting, a combination of size (cardinality) one will
always be the source language and will serve as the
baseline. For every combination (e.g. {es, ar}),
we average the individual monolingual relatedness
scores for a given word- or text-pair in this set.
Finally, to calculate the overall correlation of
these generated multilingual models (one system per
combination size) with the human scores, we av-
erage the correlation scores achieved over all the
datasets in a given combination (e.g., {es, ar}) with
all correlation scores achieved under other combina-
tions of the same size (e.g., {es, ro}, {es, en}). This
in effect allows us to observe the cumulative perfor-
mance irrespective of language choice, as we extend
the multilingual model to include more languages.
Formally, let N be the number of languages, Cn
be the set of all language combinations of size n, and
ci be one of the possible combinations of size n,
Cn = {ci | |ci| = n, 0 < i <
(
N
n
)
} (1)
then the relatedness of a word- or text-pair p from
the dataset P under this combination can be repre-
sented as:
Simci(p) =
1
|ci|
?
l?ci
Siml(p) (2)
where Siml(p) is the relatedness score of the word-
or text-pair p in the monolingual model of language
l. To evaluate the performance of the multilingual
model, let Di be the generated relatedness distribu-
tion for the dataset P using the combination ci:
Di = {?p, Simci(p)? | p ? P}. (3)
Then, the correlation between the gold standard
distribution G and the generated scores can be cal-
culated as follows:
CorrelCn(D,G) =
1
|Cn|
?
ci?Cn
Correlci(Di, G),
(4)
where Correl can stand for Pearson (r), Spearman
(?), or their harmonic mean (?), as also reported in
(Hassan and Mihalcea, 2011).
6 Evaluations
In this section we revisit the questions formulated in
the introduction, and based on different experiment
setups following the framework introduced in Sec-
tion 5, we provide an answer to each one of them.
Does the task of semantic relatedness benefit
from a multilingual representation? We evalu-
ate the three semantic relatedness models, namely
LSA, ESA and SSA on our manually constructed
multilingual word relatedness (MC30, WS353)
and text relatedness datasets (LI30), as described in
Section 4.
Figure 1 plots the correlation scores achieved
across all the languages against the gold stan-
dard and then averaged across all the multilingual
datasets. The figure shows a clear and steady im-
provement (25% - 28% with respect to the mono-
lingual baseline) achieved when more languages are
incorporated into the relatedness model. It is worth
noting that both the Pearson and Spearman correla-
tions exhibit the same improvement pattern, which
confirms our hypothesis that adding more languages
has a positive impact on the relatedness scores. The
fact that this trend is visible across all the systems
supports the idea that a multilingual representation
constitutes a better model for determining semantic
relatedness. Furthermore, we notice that SSA is the
best performing system under these settings, with a
correlation improvement of approximately 15%.
To further analyze the role of the multilingual
model and to explore whether some languages ben-
efit from using this abstraction more than others,
we plot the correlation scores achieved by the indi-
vidual languages averaged over all the systems and
the datasets in Figure 2. We notice a sharp rise in
performance associated with the addition of more
languages to the Arabic (42%) and the Romanian
(47%) models, and a slower rise for Spanish (23%).
The performance of English is also affected, but on
a smaller scale (4%) when compared to the other
24
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1  2  3  4
Number of Languages
?
ESA
LSA
SSA
 0.4
 0.6
 0.8
 1  2  3  4
r
 0.4
 0.6
 0.8
 1  2  3  4
?
Figure 1: Manual translation - average correlation (?,
r, ?) obtained from incorporating scores from models in
other languages
languages. Not surprisingly, this correlates with the
size of each corpus, where Arabic and Romanian are
the smallest, while English is the largest.
The results support the notion that resource poor
languages can benefit from languages with richer
and larger resources, such as English or Spanish.
Furthermore, incorporating additional languages to
English also leads to small improvements, which in-
dicates that the benefit, while disproportionate, is
mutual.
Does the quality of translations affect the results?
As a natural next step, we investigate the role played
by the manual translations in the performance of the
multilingual model. Since the previous evaluations
require the availability of the word- or text-pairs
in multiple languages, we attempt to see if we can
eliminate this restriction by automating the trans-
lation process using statistical machine translation
(MT). Therefore, for a multilingual model employ-
ing automated settings, the manual models proposed
previously constitute an upper bound.
We use the Google MT engine4 to translate our
multilingual datasets into the target languages (en,
es, ar, and ro). We then repeat all the evaluations
using the newly constructed datasets.
Figure 3 shows the correlation scores achieved
across all the languages and averaged across all the
multilingual datasets constructed using automatic
translation. We again see a clear and steady im-
4This API is now offered as a paid service; Microsoft or
Babelfish automatic translation services are publicly available.
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1  2  3  4
Number of Languages
?
ar
en
es
ro
 0.4
 0.6
 0.8
 1  2  3  4
r
 0.4
 0.6
 0.8
 1  2  3  4
?
Figure 2: Manual translation - average correlation (?,
r, ?) obtained by supplementing a source language with
scores from other languages
provement (12% - 35% with respect to the mono-
lingual baseline) similar to the observed pattern in
the corresponding manual evaluations (Figure 1).
While the overall achieved performance for SSA
has dropped (from ? = 0.793 to ? = 0.71) when
compared to the manual settings, we are still able
to improve over the baseline (? = 0.635). LSA
seems to experience the highest relative improve-
ment (35%), which might be due to its ability to
handle noise in these automatic settings. Over-
all Pearson and Spearman correlations exhibit the
same improvement pattern, which supports the no-
tion that even with the possibility of introducing
noise through miss-translations, the models overall
benefit from the additional clues provided by the
multilingual representation.
To explore the effect of automatic translation on
the individual languages, we plot the correlation
scores achieved vis-a`-vis a reference language, and
average over all the systems and the automatically
translated datasets in Figure 4, in a similar fashion
to Figure 2.
We notice the similar rise in performance asso-
ciated with the addition of more languages to the
Arabic (20%) and the Romanian (37%) models, and
a slower rise for Spanish (16%) and English (8%).
The effect of the automatic translation quality is ev-
ident for the Arabic language where the automatic
translation seems to slow down the improvement
when compared to the manual translations (Figure
2). A similar behavior is also observed in Spanish
and Romanian but on a lower scale.
25
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1  2  3  4
Number of Languages
?
ESA
LSA
SSA
 0.4
 0.6
 0.8
 1  2  3  4
r
 0.4
 0.6
 0.8
 1  2  3  4
?
Figure 3: Automatic translation - average correlation (?,
r, ?) obtained from incorporating scores from models in
other languages
A very interesting consideration is that English
experiences a stronger improvement when using au-
tomatic translations (8%) compared to manual trans-
lations (4%). This can be attributed to the trans-
lation engine quality in transferring English text to
other languages and to the fact that the statistical
translation (when accurate) can lead to a transla-
tion that makes use of more frequently used words,
which contribute to more robust relatedness mea-
sures. When presented with a word pair, human
judges may provide a translation influenced by the
form/root of the word in the source language, which
may not be as commonly used as the output of a
MT system. For example, when presented with the
pair ?coast - shore,? a Romanian translator may be
tempted to provide ?coasta?? as a translation candi-
date for the first word in the pair, as it resembles the
English word in form. However, the Romanian word
is highly ambiguous, and in an authoritative Roma-
nian dictionary5 its primary sense is that of rib, fol-
lowed by side, slope, and ultimately coast. Thus, a
MT system using a statistical inference may provide
a stronger translation such as ?t?a?rm? that is far less
ambiguous, and whose primary meaning is the one
intended by the original pair.
Overall, the trend is positive and follows the
pattern previously observed on the manually con-
structed datasets. This suggests that an automatic
translation, even if more noisy, is beneficial and pro-
vides a way to reinforce semantic relatedness in a
5http://dexonline.ro/definitie/coasta
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1  2  3  4
Number of Languages
?
ar
en
es
ro
 0.4
 0.6
 0.8
 1  2  3  4
r
 0.4
 0.6
 0.8
 1  2  3  4
?
Figure 4: Automatic translation - average correlation (?,
r, ?) obtained by supplementing a source language with
scores from other languages
given language with information coming from mul-
tiple languages with no manual effort.
Do our findings hold for different relatedness
datasets? At last, encouraged by the small perfor-
mance difference between the use of manual ver-
sus automatic translations, we seek to explore how
this multilingual model behaves under the different
paradigms dictated by word relatedness versus text
relatedness scenarios. Since our previous experi-
ments were constrained to collections for which we
also had a manual translation, we perform a larger
scale evaluation by including automatically trans-
lated word relatedness (RG65) and text relatedness
(LEE50 and AG400) datasets into all the languages
in our language set, and repeat all the word-to-word
and text-to-text evaluations.
Table 1 shows the correlation scores achieved us-
ing automatic translations on the word relatedness
datasets. Most models on most datasets benefit from
the multilingual representation (as shown by the fig-
ures in bold). Specifically, the SSA model has an
improvement in ? of 26% for WS353 and 15% for
MC30. This improvement is most evident in the
case of the largest dataset WS353, where all the
multilingual models exhibit a consistent and strong
performance.
Table 2 reports the results obtained for the text
relatedness datasets using automatic translation.
While the ESA performance suffers in the multi-
lingual model, it is overshadowed by the improve-
ment experienced by LSA and SSA. The multilin-
26
r ? ?
Models MC30 RG65 WS353 MC30 RG65 WS353 MC30 RG65 WS353
ESAen 0.645 0.644 0.487 0.742 0.768 0.525 0.690 0.701 0.506
ESAml 0.723 0.741 0.515 0.766 0.759 0.519 0.744 0.75 0.517
LSAen 0.509 0.450 0.435 0.525 0.499 0.436 0.517 0.473 0.436
LSAml 0.538 0.566 0.487 0.484 0.569 0.517 0.510 0.567 0.502
SSAen 0.771 0.824 0.543 0.688 0.772 0.553 0.727 0.797 0.548
SSAml 0.873 0.807 0.674 0.803 0.795 0.713 0.836 0.801 0.693
Table 1: Automatic translation - r, ?, ? correlations on the word relatedness datasets using multilingual models.
r ? ?
Models LI30 LEE50 AG400 LI30 LEE50 AG400 LI30 LEE50 AG400
ESAen 0.792 0.756 0.434 0.797 0.48 0.392 0.795 0.587 0.412
ESAml 0.776 0.648 0.382 0.742 0.339 0.358 0.759 0.445 0.369
LSAen 0.829 0.776 0.400 0.824 0.523 0.359 0.826 0.625 0.379
LSAml 0.856 0.765 0.46 0.855 0.502 0.404 0.856 0.606 0.43
SSAen 0.840 0.744 0.520 0.843 0.371 0.501 0.841 0.495 0.510
SSAml 0.829 0.743 0.539 0.87 0.41 0.521 0.849 0.528 0.53
Table 2: Automatic translation - r, ?, ? correlations on the text relatedness datasets using multilingual models.
gual model reports some of the best scores in the
literature, such as a correlations of r = 0.856 and
? = 0.87 for LI30 achieved by LSA and SSA, re-
spectively. Not surprisingly, SSA is still a top con-
tender, achieving the highest scores for AG400 and
LI30. In AG400, SSA reports a ? of 0.53 which
represents a 4% improvement over the English SSA
model (? = 0.51) and a 16% improvement over the
best knowledge-based system J&C (? = 0.457).
It is important to note that the evaluation in Ta-
bles 1 and 2 are restricted to data translated from En-
glish into a target language. English, as a resource-
rich language, has an extensive and robust monolin-
gual model, yet it can still be enhanced with addi-
tional clues originating from other languages. Ac-
cordingly, we only expected small improvements in
these two experiments, unlike the cases where we
start from resource-poor languages such as Roma-
nian or Arabic (see Figures 2 and 4).
7 Conclusion
In this paper, we showed how a semantic relatedness
measure computed in a multilingual space is able
to acquire and leverage additional information from
the multilingual representation, and thus be strength-
ened as more languages are taken into considera-
tion. Our experiments seem to suggest that combi-
nations of multiple languages supply additional in-
formation to derive a semantic relatedness between
texts in an automatic framework. Since establishing
semantic relatedness requires us to employ cogni-
tive processes that are in large part independent of
the language that we speak, it comes at no surprise
that using relatedness clues originating from more
than one language allows for a better identification
of relationships between texts. While efficiency may
be a concern, it is worth noting that the method is
highly parallelizable, as the individual relatedness
measures obtained before the aggregation step can
be calculated in parallel.
Notably, all the relatedness measures that we ex-
perimented with exhibited the same improvement
trend. While this framework allows languages with
scarce electronic resources, such as Romanian and
Arabic, to obtain very large improvements in seman-
tic relatedness as compared to the monolingual mea-
sures, improvements are also noticed for languages
with richer resources such as English.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340 and IIS award #1018613.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the National Science Foundation.
27
References
C. Banea and R. Mihalcea. 2011. Word sense disam-
biguation with multilingual features. In International
Conference on Semantic Computing, Oxford, UK.
C. Banea, R. Mihalcea, and J. Wiebe. 2010. Multilingual
subjectivity: Are more languages better? In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (Coling 2010), pages 28?36, Bei-
jing, China, August.
R. Besanc?on and M. Rajman. 2002. Evaluation of a vec-
tor space similarity measure in a multilingual frame-
work. In Proceedings of the Third International Con-
ference on Language Resource and Evaluation (LREC
2002), Las Palmas, Spain.
S. Brin, J. Davis, and H. Garcia-Molina. 1995. Copy de-
tection mechanisms for digital documents. In ACM In-
ternational Conference on Management of Data (SIG-
MOD 1995).
A. Z. Broder, S. C. Glassman, M. S. Manasse, and
G. Zweig. 1997. Syntactic clustering of the web.
Comput. Netw. ISDN Syst., 29(8-13):1157?1166.
A Z. Broder, P. Ciccolo, M. Fontoura, E. Gabrilovich,
V. Josifovski, and L. Riedel. 2008. Search advertising
using web relevance feedback. In CIKM ?08: Pro-
ceeding of the 17th ACM conference on Information
and knowledge management, pages 1013?1022, New
York, NY, USA. ACM.
C. Burgess, K. Livesay, and K. Lund. 1998. Explorations
in context space: words, sentences, discourse. Dis-
course Processes, 25(2):211?257.
K. Church and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics, 16(1):22?29.
T. Cohn and M. Lapata. 2007. Machine translation by
triangulation: making effective use of multi-parallel
corpora. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, pages
728?735, Prague, Czech Republic.
I. Dagan, O. Glickman, and B. Magnini. 2005. The PAS-
CAL recognising textual entailment challenge. In Pro-
ceedings of the PASCAL Workshop.
D. Davidov and A. Rappoport. 2009. Enhancement
of lexical concepts using cross-lingual web mining.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 852?
861, Singapore.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2001. Plac-
ing search in context: the concept revisited. In ACM
Press, editor, The Tenth International World Wide Web
Conference, pages 406?414, Hong Kong.
E. Gabrilovich and S. Markovitch. 2007. Computing
semantic relatedness using Wikipedia-based explicit
semantic analysis. In Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence,
pages 1606?1611, Hyderabad, India.
A. Goodrum. 2000. Image information retrieval: An
overview of current research. Informing Science,
3(2):63?66.
S. Hassan and R. Mihalcea. 2009. Cross-lingual seman-
tic relatedness using encyclopedic knowledge. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1192?
1201, Singapore. Association for Computational Lin-
guistics.
S. Hassan and R. Mihalcea. 2011. Measuring semantic
relatedness using salient encyclopedic concepts. Arti-
ficial Intelligence, Special Issue, xx(xx).
N. Heintze. 1996. Scalable document fingerprinting. In
In Proc. USENIX Workshop on Electronic Commerce.
T. C. Hoad and J. Zobel. 2003. Methods for identifying
versioned and plagiarized documents. J. Am. Soc. Inf.
Sci. Technol., 54(3):203?215.
A. Islam and D. Inkpen. 2006. Second order co-
occurrence PMI for determining the semantic similar-
ity of words. In Proceedings of the Fifth Conference
on Language Resources and Evaluation, volume 2,
Genoa, Italy, July.
M. Jarmasz and S. Szpakowicz. 2003. Roget?s thesaurus
and semantic similarity. In Proceedings of the confer-
ence on Recent Advances in Natural Language Pro-
cessing RANLP-2003, Borovetz, Bulgaria, September.
J. Kazama, S. De Saeger, K. Kuroda, M. Murata, and
K. Torisawa. 2010. A bayesian method for robust
estimation of distributional similarities. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, Uppsala, Sweden.
T. K. Landauer, D. Laham, B. Rehder, and M. E.
Schreiner. 1991. How well can passage meaning be
derived without using word order? A comparison of
latent semantic analysis and humans. In Proceedings
of the 19th annual meeting of the Cognitive Science
Society, pages 412?417, Mawhwah, N. Erlbaum.
C. Leacock and M. Chodorow, 1998. Combining local
context and WordNet similarity for word sense identi-
fication, pages 305?332.
M. D. Lee and M. Welsh. 2005. An empirical evaluation
of models of text document similarity. In Proceedings
of the 27th annual meeting of the Cognitive Science
Society, pages 1254?1259, Stresa, Italy.
C. W. Leong and R. Mihalcea. 2009. Explorations in
automatic image annotation using textual features. In
Proceedings of the Third Linguistic Annotation Work-
shop, pages 56?59, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
28
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries. In Proceedings of the
5th annual international conference on Systems docu-
mentation - SIGDOC ?86, pages 24?26, Toronto, On-
tario. ACM Press.
W. Li, Q. Lu, and R. Xu. 2005. Similarity based chinese
synonym collocation extraction. International Journal
of Computational Linguistics and Chinese Language
Processing, 10(1).
Y. Li, D. McLean, Z. A. Bandar, J. D. O?Shea, and
K. Crockett. 2006. Sentence similarity based on se-
mantic nets and corpus statistics. IEEE Transactions
on Knowledge and Data Engineering, 18(8):1138?
1150, August.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Interna-
tional Conference on Machine Learning, pages 296?
304, Madison, Wisconsin.
U. Manber. 1994. Finding similar files in a large file sys-
tem. In USENIX WINTER 1994 TECHNICAL CON-
FERENCE, pages 1?10.
D. Metzler, Y. Bernstein, W. Bruce Croft, A. Moffat,
and J. Zobel. 2005. Similarity measures for track-
ing information flow. In CIKM ?05: Proceedings
of the 14th ACM international conference on Infor-
mation and knowledge management, pages 517?524,
New York, NY, USA. ACM.
D. Metzler, S. T. Dumais, and C. Meek. 2007. Similarity
measures for short segments of text. In Giambattista
Amati, Claudio Carpineto, and Giovanni Romano, edi-
tors, ECIR, volume 4425 of Lecture Notes in Computer
Science, pages 16?27. Springer.
G. A. Miller and W. G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
G. A. Miller. 1995. WordNet: a Lexical database for
english. Communications of the Association for Com-
puting Machinery, 38(11):39?41.
T. Mitchell, T. Russell, P. Broomhead, and N. Aldridge.
2002. Towards robust computerised marking of free-
text responses. In roceedings of the 6th Interna-
tional Computer Assisted Assessment (CAA) Confer-
ence, Loughborough, UK. Loughborough University.
M. Mohler and R. Mihalcea. 2009a. Text-to-text seman-
tic similarity for automatic short answer grading. In
EACL, pages 567?575. The Association for Computer
Linguistics.
M. Mohler and R. Mihalcea. 2009b. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 567?575, Stroudsburg, PA, USA.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet::Similarity - measuring the relatedness of
concepts. In Proceedings of the Nineteenth Na-
tional Conference on Artificial Intelligence (AAAI-04),
demonstrations, San Jose, CA.
J. Ponte and W. Croft. 1998. A language modeling ap-
proach to information retrieval. In Proceedings of the
Annual International SIGIR Conference on Research
and Development in Information Retrieval, pages 275?
281, Melbourne, Australia.
S. G. Pulman and J. Z. Sukkarieh. 2005. Automatic
short answer marking. In EdAppsNLP 05: Proceed-
ings of the second workshop on Building Educational
Applications Using NLP, pages 9?16, Morristown, NJ,
USA. Association for Computational Linguistics.
H. Rubenstein and J. B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627?633, October.
M. Sahami and T. D. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. In WWW ?06: Proceedings of the 15th inter-
national conference on World Wide Web, pages 377?
386, New York, NY, USA. ACM.
N. Shivakumar and H. Garcia-Molina. 1995. Scam: A
copy detection mechanism for digital documents. In
2nd International Conference in Theory and Practice
of Digital Libraries (DL 1995).
J. Sinclair. 2001. Collins cobuild English dictionary for
advanced learners. Harper Collins, 3rd edition.
P. D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning,
pages 491?502, Freiburg, Germany.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexical
selection. In Proceedings of the 32nd annual meeting
on Association for Computational Linguistics, pages
133?-138, Las Cruces, New Mexico.
W. T. Yih and C. Meek. 2007. Improving similarity mea-
sures for short segments of text. In AAAI?07: Pro-
ceedings of the 22nd national conference on Artificial
intelligence, pages 1489?1494. AAAI Press.
T. Zesch, I. Gurevych, and M. Mu?hlha?user. 2007. Com-
paring Wikipedia and German Wordnet by Evaluating
Semantic Relatedness on Multiple Datasets. In Pro-
ceedings of Human Language Technologies: The An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics.
29
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 635?642,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UNT: A Supervised Synergistic Approach
to Semantic Text Similarity
Carmen Banea, Samer Hassan, Michael Mohler, Rada Mihalcea
University of North Texas
Denton, TX, USA
{CarmenBanea,SamerHassan,MichaelMohler}@my.unt.edu, rada@cs.unt.edu
Abstract
This paper presents the systems that we par-
ticipated with in the Semantic Text Similar-
ity task at SEMEVAL 2012. Based on prior
research in semantic similarity and related-
ness, we combine various methods in a ma-
chine learning framework. The three varia-
tions submitted during the task evaluation pe-
riod ranked number 5, 9 and 14 among the 89
participating systems. Our evaluations show
that corpus-based methods display a more ro-
bust behavior on the training data, yet com-
bining a variety of methods allows a learning
algorithm to achieve a superior decision than
that achievable by any of the individual parts.
1 Introduction
Measures of text similarity have been used for a
long time in applications in natural language pro-
cessing and related areas. One of the earliest ap-
plications of text similarity is perhaps the vector-
space model used in information retrieval, where the
document most relevant to an input query is deter-
mined by ranking documents in a collection in re-
versed order of their similarity to the given query
(Salton and Lesk, 1971). Text similarity has also
been used for relevance feedback and text classifi-
cation (Rocchio, 1971), word sense disambiguation
(Lesk, 1986; Schutze, 1998), and more recently for
extractive summarization (Salton et al, 1997), and
methods for automatic evaluation of machine trans-
lation (Papineni et al, 2002) or text summarization
(Lin and Hovy, 2003). Measures of text similarity
were also found useful for the evaluation of text co-
herence (Lapata and Barzilay, 2005).
Earlier work on this task has primarily focused on
simple lexical matching methods, which produce a
similarity score based on the number of lexical units
that occur in both input segments. Improvements
to this simple method have considered stemming,
stop-word removal, part-of-speech tagging, longest
subsequence matching, as well as various weight-
ing and normalization factors (Salton and Buckley,
1997). While successful to a certain degree, these
lexical similarity methods cannot always identify the
semantic similarity of texts. For instance, there is
an obvious similarity between the text segments I
own a dog and I have an animal, but most of the
current text similarity metrics will fail in identifying
any kind of connection between these texts.
More recently, researchers have started to con-
sider the possibility of combining the large number
of word-to-word semantic similarity measures (e.g.,
(Jiang and Conrath, 1997; Leacock and Chodorow,
1998; Lin, 1998; Resnik, 1995)) within a semantic
similarity method that works for entire texts. The
methods proposed to date in this direction mainly
consist of either bipartite-graph matching strate-
gies that aggregate word-to-word similarity into a
text similarity score (Mihalcea et al, 2006; Islam
and Inkpen, 2009; Hassan and Mihalcea, 2011;
Mohler et al, 2011), or data-driven methods that
perform component-wise additions of semantic vec-
tor representations as obtained with corpus measures
such as Latent Semantic Analysis (Landauer et al,
1997), Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007), or Salient Semantic Analysis
(Hassan and Mihalcea, 2011).
In this paper, we describe the system with which
635
we participated in the SEMEVAL 2012 task on se-
mantic text similarity (Agirre et al, 2012). The sys-
tem builds upon our earlier work on corpus-based
and knowledge-based methods of text semantic sim-
ilarity (Mihalcea et al, 2006; Hassan and Mihal-
cea, 2011; Mohler et al, 2011), and combines all
these previous methods into a meta-system by us-
ing machine learning. The framework provided by
the task organizers also enabled us to perform an in-
depth analysis of the various components used in our
system, and draw conclusions concerning the role
played by the different resources, features, and al-
gorithms in building a state-of-the-art semantic text
similarity system.
2 Related Work
Over the past years, the research community has
focused on computing semantic relatedness using
methods that are either knowledge-based or corpus-
based. Knowledge-based methods derive a measure
of relatedness by utilizing lexical resources and on-
tologies such as WordNet (Miller, 1995) to measure
definitional overlap, term distance within a graph-
ical taxonomy, or term depth in the taxonomy as
a measure of specificity. We explore several of
these measures in depth in Section 3.3.1. On the
other side, corpus-based measures such as Latent
Semantic Analysis (LSA) (Landauer et al, 1997),
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007), Salient Semantic Analysis
(SSA) (Hassan and Mihalcea, 2011), Pointwise Mu-
tual Information (PMI) (Church and Hanks, 1990),
PMI-IR (Turney, 2001), Second Order PMI (Islam
and Inkpen, 2006), Hyperspace Analogues to Lan-
guage (Burgess et al, 1998) and distributional simi-
larity (Lin, 1998) employ probabilistic approaches
to decode the semantics of words. They consist
of unsupervised methods that utilize the contextual
information and patterns observed in raw text to
build semantic profiles of words. Unlike knowledge-
based methods, which suffer from limited coverage,
corpus-based measures are able to induce a similar-
ity between any given two words, as long as they
appear in the very large corpus used as training.
3 Semantic Textual Similarity System
The system we proposed for the SEMEVAL 2012
Semantic Textual Similarity task builds upon both
knowledge- and corpus-based methods previously
described in (Mihalcea et al, 2006; Hassan and Mi-
halcea, 2011; Mohler et al, 2011). The predictions
of these independent systems, paired with additional
salient features, are leveraged by a meta-system that
employs machine learning. In this section, we will
elaborate further on the resources we use, our fea-
tures, and the components of our machine learning
system. We will start by describing the task setup.
3.1 Task Setup
The training data released by the task organiz-
ers consists of three datasets showcasing two sen-
tences per line and a manually assigned similarity
score ranging from 0 (no relation) to 5 (semanti-
cally equivalent). The datasets1 provided are taken
from the Microsoft Research Paraphrase Corpus
(MSRpar), the Microsoft Research Video Descrip-
tion Corpus (MSRvid), and the WMT2008 devel-
opment dataset (Europarl section)(SMTeuroparl);
they each consist of about 750 sentence pairs with
the class distribution varying with each dataset. The
testing data contains additional sentences from the
same collections as the training data as well as
from two additional unknown sets (OnWN and
SMTnews); they range from 399 to 750 sentence
pairs. The reader may refer to (Agirre et al, 2012)
for additional information regarding this task.
3.2 Resources
Wikipedia2 is a free on-line encyclopedia, represent-
ing the outcome of a continuous collaborative effort
of a large number of volunteer contributors. Virtu-
ally any Internet user can create or edit a Wikipedia
web page, and this ?freedom of contribution? has a
positive impact on both the quantity (fast-growing
number of articles) and the quality (potential mis-
takes are quickly corrected within the collaborative
environment) of this on-line resource. The basic en-
try in Wikipedia is an article which describes an en-
tity or an event, and which, in addition to untagged
1http://www.cs.york.ac.uk/semeval-2012/
task6/data/uploads/datasets/train-readme.
txt
2www.wikipedia.org
636
content, also consists of hyperlinked text to other
pages within or outside of Wikipedia. These hyper-
links are meant to guide the reader to pages that pro-
vide additional information / clarifications, so that
a better understanding of the primary concept can
be achieved. The structure of Wikipedia in terms of
pages and hyperlinks is exploited directly by seman-
tic similarity methods such as ESA (Gabrilovich and
Markovitch, 2007), or SSA (Hassan and Mihalcea,
2011).
WordNet (Miller, 1995) is a manually crafted lex-
ical resource that maintains semantic relationships
between basic units of meaning, or synsets. A synset
groups together senses of different words that share
a very similar meaning, which act in a particu-
lar context as synonyms. Each synset is accompa-
nied by a gloss or definition, and one or two ex-
amples illustrating usage in the given context. Un-
like a traditional thesaurus, the structure of Word-
Net is able to encode additional relationships be-
side synonymy, such as antonymy, hypernymy, hy-
ponymy, meronymy, entailment, etc., which vari-
ous knowledge-based methods use to derive seman-
tic similarity.
3.3 Features
Our meta-system uses several features, which can
be grouped into knowledge-based, corpus-based,
and bipartite graph matching, as described below.
The abbreviations appearing between parentheses
by each method allow for easy cross-referencing
with the evaluations provided in Table 1.
3.3.1 Knowledge-based Semantic Similarity
Features
Following prior work from our group (Mihalcea
et al, 2006; Mohler and Mihalcea, 2009), we em-
ploy several WordNet-based similarity metrics for
the task of sentence-level similarity. Briefly, for
each open-class word in one of the input texts, we
compute the maximum semantic similarity (using
the WordNet::Similarity package (Pedersen et al,
2004)) that can be obtained by pairing it with any
open-class word in the other input text. All the
word-to-word similarity scores obtained in this way
are summed and normalized to the length of the two
input texts. We provide below a short description
for each of the similarity metrics employed by this
system3.
The shortest path (Path) similarity is determined
as:
Simpath =
1
length
(1)
where length is the length of the shortest path be-
tween two concepts using node-counting (including
the end nodes).
The Leacock & Chodorow (Leacock and
Chodorow, 1998) (LCH) similarity is determined
as:
Simlch = ? log
length
2 ?D
(2)
where length is the length of the shortest path be-
tween two concepts using node-counting, and D is
the maximum depth of the taxonomy.
The Lesk (Lesk) similarity of two concepts is de-
fined as a function of the overlap between the cor-
responding definitions, as provided by a dictionary.
It is based on an algorithm proposed by Lesk (1986)
as a solution for word sense disambiguation.
The Wu & Palmer (Wu and Palmer, 1994) (WUP )
similarity metric measures the depth of two given
concepts in the WordNet taxonomy, and the depth
of the least common subsumer (LCS), and combines
these figures into a similarity score:
Simwup =
2 ? depth(LCS)
depth(concept1) + depth(concept2)
(3)
The measure introduced by Resnik (Resnik, 1995)
(RES) returns the information content (IC) of the
LCS of two concepts:
Simres = IC(LCS) (4)
where IC is defined as:
IC(c) = ? logP (c) (5)
and P (c) is the probability of encountering an in-
stance of concept c in a large corpus.
The measure introduced by Lin (Lin, 1998) (Lin)
builds on Resnik?s measure of similarity, and adds
a normalization factor consisting of the information
content of the two input concepts:
Simlin =
2 ? IC(LCS)
IC(concept1) + IC(concept2)
(6)
3We point out that the similarity metric proposed by Hirst &
St. Onge was not considered due to the time constraints associ-
ated with the STS task.
637
We also consider the Jiang & Conrath (Jiang and
Conrath, 1997) (JCN ) measure of similarity:
Simjnc =
1
IC(concept1) + IC(concept2)? 2 ? IC(LCS)
(7)
Each of the measures listed above is used as a fea-
ture by our meta-system.
3.3.2 Corpus-based Semantic Similarity
Features
While most of the corpus-based methods induce
semantic profiles in a word-space, where the seman-
tic profile of a word is expressed in terms of its co-
occurrence with other words, LSA, ESA and SSA
stand out as different, since they rely on a concept-
space representation. In these methods, the semantic
profile of a word is expressed in terms of the im-
plicit (LSA), explicit (ESA), or salient (SSA) con-
cepts. This departure from the sparse word-space to
a denser, richer, and unambiguous concept-space re-
solves one of the fundamental problems in semantic
relatedness, namely the vocabulary mismatch. In the
experiments reported in this paper, all the corpus-
based methods are trained on the English Wikipedia
download from October 2008, with approximately
6 million articles, and more than 9.5 million hyper-
links.
Latent Semantic Analysis (LSA) (Landauer et al,
1997). In LSA, term-context associations are cap-
tured by means of a dimensionality reduction op-
erated by a singular value decomposition (SVD)
on the term-by-context matrix T, where the ma-
trix is induced from a large corpus. This reduc-
tion entails the abstraction of meaning by collaps-
ing similar contexts and discounting noisy and ir-
relevant ones, hence transforming the real world
term-context space into a word-latent-concept space
which achieves a much deeper and concrete seman-
tic representation of words.
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007). ESA uses encyclopedic
knowledge in an information retrieval framework to
generate a semantic interpretation of words. Since
encyclopedic knowledge is typically organized into
concepts (or topics), each concept is further de-
scribed using definitions and examples. ESA relies
on the distribution of words inside the encyclopedic
descriptions. It builds semantic representations for
a given word using a word-document association,
where the document represents a Wikipedia article
(concept). ESA is in effect a Vector Space Model
(VSM) built using Wikipedia corpus, where vectors
represents word-articles association.
Salient Semantic Analysis (SSA) (Hassan and Mi-
halcea, 2011). SSA incorporates a similar seman-
tic abstraction and interpretation of words as ESA,
yet it uses salient concepts gathered from encyclo-
pedic knowledge, where a ?concept? represents an
unambiguous word or phrase with a concrete mean-
ing, and which affords an encyclopedic definition.
Saliency in this case is determined based on the
word being hyperlinked (either trough manual or au-
tomatic annotations) in context, implying that they
are highly relevant to the given text. SSA is an ex-
ample of Generalized Vector Space Model (GVSM),
where vectors represent word-concepts associations.
In order to determine the similarity of two text
fragments , we employ two variations: the typical
cosine similarity (cos) and a best alignment strat-
egy (align), which we explain in more detail below.
Both variations were paired with the LSA, ESA,
and SSA systems resulting in six similarity scores
that were used as features by our meta-system,
namely LSAcos, LSAalign, ESAcos, ESAalign,
SSAcos, and SSAalign.
Best Alignment Strategy (align). Let Ta and Tb be
two text fragments of size a and b respectively. After
removing all stopwords, we first determine the num-
ber of shared terms (?) between Ta and Tb. Second,
we calculate the semantic relatedness of all possible
pairings between non-shared terms in Ta and Tb. We
further filter these possible combinations by creating
a list ? which holds the strongest semantic pairings
between the fragments? terms, such that each term
can only belong to one and only one pair.
Sim(Ta, Tb) =
(? +
?|?|
i=1 ?i)? (2ab)
a + b
(8)
where ? is the number of shared terms between the
text fragments and ?i is the similarity score for the
ith pairing.
3.3.3 Bipartite Graph Matching
In an attempt to move beyond the bag-of-words
paradigm described thus far, we attempt to compute
638
a set of dependency graph alignment scores based
on previous work in automatic short-answer grading
(Mohler et al, 2011). This score, computed in two
stages, is used as a feature by our meta-system.
In the first stage, the system is provided with the
dependency graphs for each pair of sentences4. For
each node in one dependency graph, we compute a
similarity score for each node in the other depen-
dency graph based upon a set of lexical, semantic,
and syntactic features applied to both the pair of
nodes and their corresponding subgraphs (i.e. the set
of nodes reachable from a given node by following
directional governor-to-dependant links). The scor-
ing function is trained on a small set of manually
aligned graphs using the averaged perceptron algo-
rithm.
We define a total of 64 features5 to be used to train
a machine learning system to compute subgraph-
subgraph similarity. Of these, 32 are based upon the
bag-of-words semantic similarity of the subgraphs
using the metrics described in Section 3.3.1 as well
as a Wikipedia-trained LSA model. The remaining
32 features are lexico-syntactic features associated
with the parent nodes of the subgraphs and are de-
scribed in more detail in our earlier paper.
We then calculate weights associated with these
features using an averaged version of the percep-
tron algorithm (Freund and Schapire, 1999; Collins,
2002) trained on a set of 32 manually annotated
instructor/student answer pairs selected from the
short-answer grading corpus (MM2011). These
pairs contain 7303 node pairs (656 matches, 6647
non-matches). Once the weights are calculated, a
similarity score for each pair of nodes can be com-
puted by taking the dot product of the feature vector
with the weights.
In the second stage, the node similarity scores cal-
culated in the previous step are used to find an op-
timal alignment for the pair of dependency graphs.
We begin with a bipartite graph where each node
in one graph is represented by a node on the left
side of the bipartite graph and each node in the other
4We here use the output of the Stanford Dependency Parser
in collapse/propagate mode with some modifications as de-
scribed in our earlier work.
5With the exception of the four features based upon the Hirst
& St.Onge similarity metric, these are equivalent to the features
used in previous work.
graph is represented by a node on the right side. The
weight associated with each edge is the score com-
puted for each node-node pair in the previous stage.
The bipartite graph is then augmented by adding
dummy nodes to both sides which are allowed to
match any node with a score of zero. An optimal
alignment between the two graphs is then computed
efficiently using the Hungarian algorithm. Note that
this results in an optimal matching, not a mapping,
so that an individual node is associated with at most
one node in the other answer. After finding the opti-
mal match, we produce four alignment-based scores
by optionally normalizing by the number of nodes
and/or weighting the node-alignments according to
the idf scores of the words.6 This results in four
alignment scores listed as graphnone, graphnorm,
graphidf , graphidfnorm.
3.3.4 Baselines
As a baseline, we also utilize several lexical bag-
of-words approaches where each sentence is repre-
sented by a vector of tokens and the similarity of the
two sentences can be computed by finding the co-
sine of the angle between their representative vectors
using term frequency (tf ) or term frequency mul-
tiplied by inverse document frequency (tf.idf )6, or
by using simple overlap between the vectors? dimen-
sions (overlap).
3.4 Machine Learning
3.4.1 Algorithms
All the systems described above are used to gen-
erate a score for each training and test sample (see
Section 3.1). These scores are then aggregated per
sample, and used in a supervised learning frame-
work. We decided to use a regression model, instead
of classification, since the requirements for the task
specify that we should provide a score in the range of
0 to 5. We could have used classification paired with
bucketed ranges, yet classification does not take into
consideration the underlying ordinality of the scores
(i.e. a score of 4.5 is closer to either 4 or 5, but
farther away from 0), which is a noticeable handi-
cap in this scenario. We tried both linear and sup-
6The document frequency scores were taken from the British
National Corpus (BNC).
639
port vector regression7 by performing 10 fold cross-
validation on the train data, yet the latter algorithm
consistently performs better, no matter what kernel
was chosen. Thus we decided to use support vec-
tor regression (Smola and Schoelkopf, 1998) with a
Pearson VII function-based kernel.
Due to its different learning methodology, and
since it is suited for predicting continuous classes,
our second system uses the M5P decision tree al-
gorithm (Quinlan, 1992; Wang and Witten, 1997),
which outperforms support vector regression on the
10 fold cross-validation performed on the SMTeu-
roparl train set, while providing competitive results
on the other train sets (within .01 Pearson correla-
tion).
3.4.2 Setup
We submitted three system variations, namely
IndividualRegression, IndividualDecTree,
and CombinedRegression. The first word de-
scribes the training data; for individual, for the
known test sets we trained on the corresponding
train sets, while for the unknown test sets we trained
on all the train sets combined; for combined,
for each test set we trained on all the train sets
combined. The second word refers to the learning
methodology, where Regression stands for support
vector regression, and DecTree stands for M5P
decision tree.
4 Results and Discussion
We include in Table 1 the Pearson correlations ob-
tained by comparing the predictions of each fea-
ture to the gold standard for the three train datasets.
We notice that the corpus based metrics display a
consistent performance across the three train sets,
when compared to the other methods, including
knowledge-based. Furthermore, the best alignment
strategy (align) for corpus based models outper-
forms similarity scores based on traditional cosine
similarity. It is interesting to note that simple base-
lines such as tf , tf.idf and overlap offer signifi-
cant correlations with all the train sets without ac-
cess to additional knowledge inferred by knowledge
or corpus-based methods. In the case of the bipar-
7Implementations provided through the Weka framework
(Hall et al, 2009).
System MSRpar MSRvid SMTeuroparl
Path 0.49 0.62 0.50
LCH 0.48 0.49 0.45
Lesk 0.48 0.59 0.50
WUP 0.46 0.38 0.42
RES 0.47 0.55 0.48
Lin 0.49 0.54 0.48
JCN 0.49 0.63 0.51
LSAalign 0.44 0.57 0.61
LSAcos 0.37 0.74 0.56
ESAalign 0.52 0.70 0.62
ESAcos 0.30 0.71 0.53
SSAalign 0.46 0.61 0.65
SSAcos 0.22 0.63 0.39
graphnone 0.42 0.50 0.21
graphnorm 0.48 0.43 0.59
graphidf 0.16 0.67 0.16
graphidfnorm 0.08 0.60 0.19
tf.idf 0.45 0.63 0.41
tf 0.45 0.69 0.51
overlap 0.44 0.69 0.27
Table 1: Correlation of individual features for the training
sets with the gold standards
tite graph matching, the graphnorm variation pro-
vides the strongest correlation results across all the
datasets.
We include the evaluation results provided by the
task organizers in Table 2. They indicate that our in-
tuition in using a support vector regression strategy
was correct. While the IndividualRegression was
our strongest system on the training data, the same
ranking applies to the test data (including the addi-
tional two surprise datasets) as well, earning it the
fifth place among the 89 participating systems, with
a Pearson correlation of 0.7846.
Regarding the decision tree based learning
(IndividualDecTree), despite its more robust be-
havior on the train sets, it achieved slightly lower
outcome on the test data, at 0.7677 correlation. We
believe this happened because decision trees have a
tendency to overfit training data, as they generate a
rigid structure which is unforgiving to minor devia-
tions in the test data. Nonetheless, this second vari-
ation still ranks in the top 10% of the submitted sys-
tems.
As an alternative approach to handle unknown test
data (e.g. different distributions, genres), we opted
640
Run ALL Rank Mean RankMean MSRpar MSRvid SMTeuroparl OnWN SMTnews
IndividualRegression 0.7846 5 0.6162 13 0.5353 0.8750 0.4203 0.6715 0.4033
IndividualDecTree 0.7677 9 0.5947 25 0.5693 0.8688 0.4203 0.6491 0.2256
CombinedRegression 0.7418 14 0.6159 14 0.5032 0.8695 0.4797 0.6715 0.4033
Table 2: Evaluation results and ranking published by the task organizers
to also include the CombinedRegression strategy
as our third variation. This seems to have been fruit-
ful for MSRvid, SMTeuroparl, and the two sur-
prise datasets (ONWn and SMTnews). In the
case of SMTeuroparl, this expanded training set
achieves a better performance than learning from
the corresponding training set alne, gaining an im-
provement of 0.0776 correlation points. Unfortu-
nately, the variation has some losses, particularly for
the MSRpar dataset (0.0321), yet it is able to con-
sistently model and handle a wider variety of text
types.
5 Conclusion
This paper describes the three system variations our
team participated with in the Semantic Text Similar-
ity task in SEMEVAL 2012. Our focus has been to
produce a synergistic approach, striving to achieve a
superior result than attainable by each system indi-
vidually. We have considered a variety of methods
for inferring semantic similarity, including knowl-
edge and corpus-based methods. These were lever-
aged in a machine-learning framework, where our
preferred learning algorithm is support vector re-
gression, due to its ability to deal with continuous
classes and to dampen the effect of noisy features,
while augmenting more robust ones. While it is al-
ways preferable to use similar test and train sets,
when information regarding the test dataset is un-
available, we show that a robust performance can
be achieved by combining all train data from dif-
ferent sources into a single set and allowing a ma-
chine learner to make predictions. Overall, it was
interesting to note that corpus-based methods main-
tain strong results on all train datasets in compari-
son to knowledge-based methods. Our three systems
ranked number 5, 9 and 14 among the 89 systems
participating in the task.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340 and IIS award #1018613.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the National Science Foundation.
References
E. Agirre, D. Cer, M. Diab, and A. Gonzalez. 2012.
Semeval-2012 task 6: A pilot on semantic textual sim-
ilarity. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), in con-
junction with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
C. Burgess, K. Livesay, and K. Lund. 1998. Explorations
in context space: words, sentences, discourse. Dis-
course Processes, 25(2):211?257.
K. Church and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics, 16(1):22?29.
M. Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and experiments
with perceptron algorithms. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP-02), Philadelphia, PA,
July.
Y. Freund and R. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37:277?296.
E. Gabrilovich and S. Markovitch. 2007. Computing
semantic relatedness using Wikipedia-based explicit
semantic analysis. In Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence,
pages 1606?1611, Hyderabad, India.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The WEKA Data
Mining Software: An Update. SIGKDD Explorations,
11(1).
S. Hassan and R. Mihalcea. 2011. Measuring semantic
relatedness using salient encyclopedic concepts. Arti-
ficial Intelligence, Special Issue, xx(xx).
641
A. Islam and D. Inkpen. 2006. Second order co-
occurrence PMI for determining the semantic similar-
ity of words. In Proceedings of the Fifth Conference
on Language Resources and Evaluation, volume 2,
Genoa, Italy, July.
A. Islam and D. Inkpen. 2009. Semantic Similarity of
Short Texts. In Nicolas Nicolov, Galia Angelova, and
Ruslan Mitkov, editors, Recent Advances in Natural
Language Processing V, volume 309 of Current Issues
in Linguistic Theory, pages 227?236. John Benjamins,
Amsterdam & Philadelphia.
J. J. Jiang and D. W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
International Conference Research on Computational
Linguistics (ROCLING X), pages 9008+, September.
T. K. Landauer, D. Laham, B. Rehder, and M. E.
Schreiner. 1997. How well can passage meaning be
derived without using word order? a comparison of
latent semantic analysis and humans.
M. Lapata and R. Barzilay. 2005. Automatic evaluation
of text coherence: Models and representations. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence, Edinburgh.
C. Leacock and M. Chodorow, 1998. Combining local
context and WordNet similarity for word sense identi-
fication, pages 305?332.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In SIGDOC ?86: Pro-
ceedings of the 5th annual international conference on
Systems documentation, pages 24?26, New York, NY,
USA. ACM.
C. Y. Lin and E. H. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of Human Language Technology Confer-
ence (HLT-NAACL 2003), Edmonton, Canada, May.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Interna-
tional Conference on Machine Learning, pages 296?
304, Madison, Wisconsin.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of text
semantic similarity. In Proceedings of the American
Association for Artificial Intelligence (AAAI 2006),
pages 775?780, Boston, MA, US.
G. A. Miller. 1995. WordNet: a Lexical database for
english. Communications of the Association for Com-
puting Machinery, 38(11):39?41.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the European Association for Compu-
tational Linguistics (EACL 2009), Athens, Greece.
M. Mohler, R. Bunescu, and R. Mihalcea. 2011. Learn-
ing to grade short answer questions using semantic
similarity measures and dependency graph alignments.
In Proceedings of the Association for Computational
Linguistics ? Human Language Technologies (ACL-
HLT 2011), Portland, Oregon, USA.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318, Philadelphia, PA.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet:: Similarity-Measuring the Relatedness of
Concepts. Proceedings of the National Conference on
Artificial Intelligence, pages 1024?1025.
R. J. Quinlan. 1992. Learning with continuous classes.
In 5th Australian Joint Conference on Artificial Intel-
ligence, pages 343?348, Singapore. World Scientific.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence, pages 448?453.
J. Rocchio, 1971. Relevance feedback in information re-
trieval. Prentice Hall, Ing. Englewood Cliffs, New Jer-
sey.
G. Salton and C. Buckley. 1997. Term weighting ap-
proaches in automatic text retrieval. In Readings in
Information Retrieval. Morgan Kaufmann Publishers,
San Francisco, CA.
G. Salton and M.E. Lesk, 1971. The SMART Retrieval
System: Experiments in Automatic Document Process-
ing, chapter Computer evaluation of indexing and text
processing. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997.
Automatic text structuring and summarization. Infor-
mation Processing and Management, 2(32).
H. Schutze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?124.
A. J. Smola and B. Schoelkopf. 1998. A tutorial on sup-
port vector regression. NeuroCOLT2 Technical Re-
port NC2-TR-1998-030.
P. D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning,
pages 491?502, Freiburg, Germany.
Y. Wang and I. H. Witten. 1997. Induction of model trees
for predicting continuous classes. In Poster papers of
the 9th European Conference on Machine Learning.
Springer.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexical
selection. In Proceedings of the 32nd annual meeting
on Association for Computational Linguistics, pages
133?-138, Las Cruces, New Mexico.
642
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 221?228, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
CPN-CORE: A Text Semantic Similarity System Infused
with Opinion Knowledge
Carmen Banea[?, Yoonjung Choi?, Lingjia Deng?, Samer Hassan?, Michael Mohler?
Bishan Yang
?
, Claire Cardie
?
, Rada Mihalcea[?, Janyce Wiebe?
[University of North Texas
Denton, TX
?University of Pittsburgh
Pittsburgh, PA
?Google Inc.
Mountain View, CA
?Language Computer Corp.
Richardson, TX
?
Cornell University
Ithaca, NY
Abstract
This article provides a detailed overview of the
CPN text-to-text similarity system that we par-
ticipated with in the Semantic Textual Similar-
ity task evaluations hosted at *SEM 2013. In
addition to more traditional components, such
as knowledge-based and corpus-based met-
rics leveraged in a machine learning frame-
work, we also use opinion analysis features to
achieve a stronger semantic representation of
textual units. While the evaluation datasets are
not designed to test the similarity of opinions,
as a component of textual similarity, nonethe-
less, our system variations ranked number 38,
39 and 45 among the 88 participating systems.
1 Introduction
Measures of text similarity have been used for a long
time in applications in natural language processing
and related areas. One of the earliest applications
of text similarity is perhaps the vector-space model
used in information retrieval, where the document
most relevant to an input query is determined by
ranking documents in a collection in reversed or-
der of their angular distance with the given query
(Salton and Lesk, 1971). Text similarity has also
been used for relevance feedback and text classifi-
cation (Rocchio, 1971), word sense disambiguation
(Lesk, 1986; Schutze, 1998), and extractive summa-
rization (Salton et al, 1997), in the automatic evalu-
ation of machine translation (Papineni et al, 2002),
?carmen.banea@gmail.com
? rada@cs.unt.edu
text summarization (Lin and Hovy, 2003), text co-
herence (Lapata and Barzilay, 2005) and in plagia-
rism detection (Nawab et al, 2011).
Earlier work on this task has primarily focused on
simple lexical matching methods, which produce a
similarity score based on the number of lexical units
that occur in both input segments. Improvements
to this simple method have considered stemming,
stopword removal, part-of-speech tagging, longest
subsequence matching, as well as various weight-
ing and normalization factors (Salton and Buckley,
1997). While successful to a certain degree, these
lexical similarity methods cannot always identify the
semantic similarity of texts. For instance, there is an
obvious similarity between the text segments ?she
owns a dog? and ?she has an animal,? yet these
methods will mostly fail to identify it.
More recently, researchers have started to con-
sider the possibility of combining the large number
of word-to-word semantic similarity measures (e.g.,
(Jiang and Conrath, 1997; Leacock and Chodorow,
1998; Lin, 1998; Resnik, 1995)) within a semantic
similarity method that works for entire texts. The
methods proposed to date in this direction mainly
consist of either bipartite-graph matching strate-
gies that aggregate word-to-word similarity into a
text similarity score (Mihalcea et al, 2006; Islam
and Inkpen, 2009; Hassan and Mihalcea, 2011;
Mohler et al, 2011), or data-driven methods that
perform component-wise additions of semantic vec-
tor representations as obtained with corpus mea-
sures such as latent semantic analysis (Landauer et
al., 1997), explicit semantic analysis (Gabrilovich
and Markovitch, 2007), or salient semantic analysis
221
(Hassan and Mihalcea, 2011).
In this paper, we describe the system variations
with which we participated in the *SEM 2013 task
on semantic textual similarity (Agirre et al, 2013).
The system builds upon our earlier work on corpus-
based and knowledge-based methods of text seman-
tic similarity (Mihalcea et al, 2006; Hassan and
Mihalcea, 2011; Mohler et al, 2011; Banea et al,
2012), while also incorporating opinion aware fea-
tures. Our observation is that text is not only similar
on a semantic level, but also with respect to opin-
ions. Let us consider the following text segments:
?she owns a dog? and ?I believe she owns a dog.?
The question then becomes how similar these text
fragments truly are. Current systems will consider
the two sentences semantically equivalent, yet to a
human, they are not. A belief is not equivalent to a
fact (and for the case in point, the person may very
well have a cat or some other pet), and this should
consequently lower the relatedness score. For this
reason, we advocate that STS systems should also
consider the opinions expressed and their equiva-
lence. While the *SEM STS task is not formulated
to evaluate this type of similarity, we complement
more traditional corpus and knowledge-based meth-
ods with opinion aware features, and use them in
a meta-learning framework in an arguably first at-
tempt at incorporating this type of information to in-
fer text-to-text similarity.
2 Related Work
Over the past years, the research community has
focused on computing semantic relatedness using
methods that are either knowledge-based or corpus-
based. Knowledge-based methods derive a measure
of relatedness by utilizing lexical resources and on-
tologies such as WordNet (Miller, 1995) to measure
definitional overlap, term distance within a graph-
ical taxonomy, or term depth in the taxonomy as
a measure of specificity. We explore several of
these measures in depth in Section 3.3.1. On the
other side, corpus-based measures such as Latent
Semantic Analysis (LSA) (Landauer et al, 1997),
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007), Salient Semantic Analysis
(SSA) (Hassan and Mihalcea, 2011), Pointwise Mu-
tual Information (PMI) (Church and Hanks, 1990),
PMI-IR (Turney, 2001), Second Order PMI (Islam
and Inkpen, 2006), Hyperspace Analogues to Lan-
guage (Burgess et al, 1998) and distributional simi-
larity (Lin, 1998) employ probabilistic approaches
to decode the semantics of words. They consist
of unsupervised methods that utilize the contextual
information and patterns observed in raw text to
build semantic profiles of words. Unlike knowledge-
based methods, which suffer from limited coverage,
corpus-based measures are able to induce a similar-
ity between any given two words, as long as they
appear in the very large corpus used as training.
3 Semantic Textual Similarity System
3.1 Task Setup
The STS task consists of labeling one sentence pair
at a time, based on the semantic similarity existent
between its two component sentences. Human as-
signed similarity scores range from 0 (no relation)
to 5 (semantivally equivalent). The *SEM 2013 STS
task did not provide additional labeled data to the
training and testing sets released as part of the STS
task hosted at SEMEVAL 2012 (Agirre et al, 2012);
our system variations were trained on SEMEVAL
2012 data.
The test sets (Agirre et al, 2013) consist of
text pairs extracted from headlines (headlines,
750 pairs), sense definitions from WordNet and
OntoNotes (OnWN, 561 pairs), sense definitions
from WordNet and FrameNet (FNWN, 189 pairs),
and data used in the evaluation of machine transla-
tion systems (SMT, 750 pairs).
3.2 Resources
Various subparts of our framework use several re-
sources that are described in more detail below.
Wikipedia1 is the most comprehensive encyclo-
pedia to date, and it is an open collaborative effort
hosted on-line. Its basic entry is an article which in
addition to describing an entity or an event also con-
tains hyperlinks to other pages within or outside of
Wikipedia. This structure (articles and hyperlinks)
is directly exploited by semantic similarity methods
such as ESA (Gabrilovich and Markovitch, 2007),
or SSA (Hassan and Mihalcea, 2011)2.
1www.wikipedia.org
2In the experiments reported in this paper, all the corpus-
based methods are trained on the English Wikipedia download
from October 2008.
222
WordNet (Miller, 1995) is a manually crafted lex-
ical resource that maintains semantic relationships
such as synonymy, antonymy, hypernymy, etc., be-
tween basic units of meaning, or synsets. These rela-
tionships are employed by various knowledge-based
methods to derive semantic similarity.
The MPQA corpus (Wiebe and Riloff, 2005) is
a newswire data set that was manually annotated
at the expression level for opinion-related content.
Some of the features derived by our opinion extrac-
tion models were based on training on this corpus.
3.3 Features
Our system variations derive the similarity score of a
given sentence-pair by integrating information from
knowledge, corpus, and opinion-based sources3.
3.3.1 Knowledge-Based Features
Following prior work from our group (Mihalcea
et al, 2006; Mohler and Mihalcea, 2009), we em-
ploy several WordNet-based similarity metrics for
the task of sentence-level similarity. Briefly, for each
open-class word in one of the input texts, we com-
pute the maximum semantic similarity4 that can be
obtained by pairing it with any open-class word in
the other input text. All the word-to-word similarity
scores obtained in this way are summed and normal-
ized to the length of the two input texts. We provide
below a short description for each of the similarity
metrics employed by this system.
The shortest path (Path) similarity is equal to:
Simpath =
1
length
(1)
where length is the length of the shortest path be-
tween two concepts using node-counting.
The Leacock & Chodorow (Leacock and
Chodorow, 1998) (LCH) metric is equal to:
Simlch = ? log
length
2 ?D
(2)
where length is the length of the shortest path be-
tween two concepts using node-counting, and D is
the maximum depth of the taxonomy.
The Lesk (Lesk) similarity of two concepts is de-
fined as a function of the overlap between the cor-
responding definitions, as provided by a dictionary.
3The abbreviation in italics accompanying each method al-
lows for cross-referencing with the results listed in Table 2.
4We use the WordNet::Similarity package (Pedersen et al,
2004).
It is based on an algorithm proposed by Lesk (1986)
as a solution for word sense disambiguation.
The Wu & Palmer (Wu and Palmer, 1994) (WUP )
similarity metric measures the depth of two given
concepts in the WordNet taxonomy, and the depth
of the least common subsumer (LCS), and combines
these figures into a similarity score:
Simwup =
2 ? depth(LCS)
depth(concept1) + depth(concept2)
(3)
The measure introduced by Resnik (Resnik, 1995)
(RES) returns the information content (IC) of the
LCS of two concepts:
Simres = IC(LCS) (4)
where IC is defined as:
IC(c) = ? logP (c) (5)
and P (c) is the probability of encountering an in-
stance of concept c in a large corpus.
The measure introduced by Lin (Lin, 1998) (Lin)
builds on Resnik?s measure of similarity, and adds
a normalization factor consisting of the information
content of the two input concepts:
Simlin =
2 ? IC(LCS)
IC(concept1) + IC(concept2)
(6)
We also consider the Jiang & Conrath (Jiang and
Conrath, 1997) (JCN ) measure of similarity:
Simjnc =
1
IC(concept1) + IC(concept2)? 2 ? IC(LCS)
(7)
3.3.2 Corpus Based Features
While most of the corpus-based methods induce
semantic profiles in a word-space, where the seman-
tic profile of a word is expressed in terms of its co-
occurrence with other words, LSA, ESA and SSA
rely on a concept-space representation, thus express-
ing a word?s semantic profile in terms of the im-
plicit (LSA), explicit (ESA), or salient (SSA) con-
cepts. This departure from the sparse word-space to
a denser, richer, and unambiguous concept-space re-
solves one of the fundamental problems in semantic
relatedness, namely the vocabulary mismatch.
Latent Semantic Analysis (LSA) (Landauer et al,
1997). In LSA, term-context associations are cap-
tured by means of a dimensionality reduction op-
erated by a singular value decomposition (SVD)
223
on the term-by-context matrix T, where the ma-
trix is induced from a large corpus. This reduc-
tion entails the abstraction of meaning by collaps-
ing similar contexts and discounting noisy and ir-
relevant ones, hence transforming the real world
term-context space into a word-latent-concept space
which achieves a much deeper and concrete seman-
tic representation of words5.
Random Projection (RP ) (Dasgupta, 1999). In RP,
a high dimensional space is projected onto a lower
dimensional one, using a randomly generated ma-
trix. (Bingham and Mannila, 2001) show that unlike
LSA or principal component analysis (PCA), RP
is computationally efficient for large corpora, while
also retaining accurate vector similarity and yielding
comparable results.
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007). ESA uses encyclopedic
knowledge in an information retrieval framework to
generate a semantic interpretation of words. It relies
on the distribution of words inside Wikipedia arti-
cles, thus building a semantic representation for a
given word using a word-document association.
Salient Semantic Analysis (SSA) (Hassan and Mi-
halcea, 2011). SSA incorporates a similar seman-
tic abstraction as ESA, yet it uses salient con-
cepts gathered from encyclopedic knowledge, where
a ?concept? represents an unambiguous expression
which affords an encyclopedic definition. Saliency
in this case is determined based on the word being
hyperlinked in context, implying that it is highly rel-
evant to the given text.
In order to determine the similarity of two text
fragments, we employ two variations: the typical
cosine similarity (cos) and a best alignment strat-
egy (align), which we explain in more detail in
the paragraph below. Both variations were paired
with the ESA, and SSA systems resulting in four
similarity scores that were used as features by our
meta-system, namely ESAcos, ESAalign, SSAcos,
and SSAalign; in addition, we also used BOWcos,
LSAcos, and RPcos.
Best Alignment Strategy (align). Let Ta and Tb be
two text fragments of size a and b respectively. After
removing all stopwords, we first determine the num-
5We use the LSA implementation available at code.
google.com/p/semanticvectors/.
ber of shared terms (?) between Ta and Tb. Second,
we calculate the semantic relatedness of all possible
pairings between non-shared terms in Ta and Tb. We
further filter these possible combinations by creating
a list ? which holds the strongest semantic pairings
between the fragments? terms, such that each term
can only belong to one and only one pair.
Sim(Ta, Tb) =
(? +
?|?|
i=1 ?i)? (2ab)
a+ b
(8)
where ?i is the similarity score for the ith pairing.
3.3.3 Opinion Aware Features
We design opinion-aware features to capture sen-
tence similarity on the subjectivity level based on the
output of three subjectivity analysis systems. Intu-
itively, two sentences are similar in terms of sub-
jectivity if there exists similar opinion expressions
which also share similar opinion holders.
OpinionFinder (Wilson et al, 2005) is a publicly
available opinion extraction model that annotates the
subjectivity of new text based on the presence (or
absence) of words or phrases in a large lexicon. The
system consists of a two step process, by feeding
the sentences identified as subjective or objective
by a rule-based high-precision classifier to a high-
recall classifier that iteratively learns from the re-
maining corpus. For each sentence in a STS pair,
the two classifiers provide two predictions; a subjec-
tivity similarity score (SUBJSL) is computed as fol-
lows. If both sentences are classified as subjective
or objective, the score is 1; if one is subjective and
the other one is objective, the score is -1; otherwise
it is 0. We also make use of the output of the sub-
jective expression identifier in OpinionFinder. We
first record how many expressions the two sentences
have: feature NUMEX1 and NUMEX2. Then we
compare how many tokens these expressions share
and we normalize by the total number of expressions
(feature EXPR).
We compute the difference between the probabil-
ities of the two sentences being subjective (SUBJD-
IFF), by employing a logistic regression classifier
using LIBLINEAR (Fan et al, 2008) trained on the
MPQA corpus. The smaller the difference, the more
similar the sentences are in terms of subjectivity.
We also employ features produced by the opinion-
extraction model of Yang and Cardie (Yang and
Cardie, 2012), which is better suited to process ex-
224
pressions of arbitrary length. Specifically, for each
sentence, we extract subjective expressions and gen-
erate the following features. SUBJCNT is a binary
feature which is equal to 1 if both sentences con-
tain a subjective expression. DSEALGN marks the
number of shared words between subjective expres-
sions in two sentences, while DSESIM represents
their similarity beyond the word level. We repre-
sent the subjective expressions in each sentence as
a feature vector, containing unigrams extracted from
the expressions, their part-of-speech, their WordNet
hypernyms and their subjectivity label6, and com-
pute the cosine similarity between the feature vec-
tors. The holder of the opinion expressions is ex-
tracted with the aid of a dependency parser7. In most
cases, the opinion holder and the opinion expression
are related by the dependency relation subj. This re-
lation is used to expand the verb dependents in the
opinion expression and identify the opinion holder
or AGENT.
3.4 Meta-learning
Each metric described above provides one individ-
ual score for every sentence-pair in both the train-
ing and test set. These scores then serve as in-
put to a meta-learner, which adjusts their impor-
tance, and thus their bearing on the overall similar-
ity score predicted by the system. We experimented
with regression and decision tree based algorithms
by performing 10-fold cross validation on the 2012
training data; these types of learners are particularly
well suited to maintain the ordinality of the seman-
tic similarity scores (i.e. a score of 4.5 is closer
to either 4 or 5, implying that the two sentences
are mostly or fully equivalent, while also being far
further away from 0, implying no semantic relat-
edness between the two sentences). We obtained
consistent results when using support vector regres-
sion with polynomial kernel (Drucker et al, 1997;
Smola and Schoelkopf, 1998) (SV R) and random
subspace meta-classification with tree learners (Ho,
1998) (RandSubspace)8.
We submitted three system variations based
on the training corpus (first word in the sys-
6Label is based on the OpinionFinder subjectivity lexicon
(Wiebe et al, 2005).
7nlp.stanford.edu/software/
8Included with the Weka framework (Hall et al, 2009); we
used the default values for both algorithms.
System FNWN headlines OnWN SMT Mean
comb.RandSubSpace 0.331 0.677 0.514 0.337 0.494
comb.SVR 0.362 0.669 0.510 0.341 0.494
indv.RandSubspace 0.331 0.677 0.548 0.277 0.483
baseline-tokencos 0.215 0.540 0.283 0.286 0.364
Table 1: Evaluation results (Agirre et al, 2013).
tem name) or the learning methodology (second
word) used: comb.RandSubspace, comb.SV R and
indv.RandSubspace. For comb, training was per-
formed on the merged version of the entire 2012 SE-
MEVAL dataset. For indv, predictions for OnWN
and SMT test data were based on training on
matching OnWN and SMT 9 data from 2012, pre-
dictions for the other test sets were computed using
the combined version (comb).
4 Results and Discussion
Table 2 lists the correlations obtained between
the scores assigned by each one of the features
we used and the scores assigned by the human
judges. It is interesting to note that overall, corpus-
based measures are stronger performers compared to
knowledge-based measures. The top contenders in
the former group are ESAalign, SSAalign, LSAcos,
and RPcos, indicating that these methods are able to
leverage a significant amount of semantic informa-
tion from text. While LSAcos achieves high corre-
lations on many of the datasets, replacing the singu-
lar value decomposition operation by random pro-
jection to a lower-dimension space (RP ) achieves
competitive results while also being computation-
ally efficient. This observation is in line with prior
literature (Bingham and Mannila, 2001). Among
the knowledge-based methods, JCN and Path
achieve high performance on more than five of the
datasets. In some cases, particularly on the 2013
test data, the shortest path method (Path) peforms
better or on par with the performance attained by
other knowledge-based measures, despite its com-
putational simplicity. While opinion-based mea-
sures do not exhibit the same high correlation, we
should remember that none of the datasets displays
consistent opinion content, nor were they anno-
tated with this aspect in mind, in order for this in-
formation to be properly leveraged and evaluated.
9The SMT training set is a combination of SMTeuroparl
(in this paper abbreviated as SMTep) and SMTnews data.
225
Train 2012 Test 2012 Test 2013
Feature SMTep MSRpar MSRvid SMTep MSRpar MSRvid OnWN SMTnews FNWN headlines OnWN SMT
Knowledge-based measures
JCN 0.51 0.49 0.63 0.48 0.48 0.64 0.62 0.28 0.38 0.72 0.71 0.34
LCH 0.45 0.48 0.49 0.47 0.49 0.54 0.54 0.3 0.39 0.69 0.69 0.32
Lesk 0.5 0.48 0.59 0.5 0.47 0.63 0.64 0.4 0.4 0.71 0.7 0.33
Lin 0.48 0.49 0.54 0.48 0.48 0.56 0.57 0.27 0.28 0.65 0.66 0.3
Path 0.5 0.49 0.62 0.48 0.49 0.65 0.62 0.35 0.43 0.72 0.73 0.34
RES 0.48 0.47 0.55 0.49 0.47 0.6 0.62 0.33 0.28 0.64 0.7 0.31
WUP 0.42 0.46 0.38 0.44 0.48 0.42 0.48 0.26 0.19 0.55 0.6 0.25
Corpus-based measures
BOW cos 0.51 0.47 0.69 0.32 0.44 0.71 0.66 0.37 0.34 0.68 0.52 0.32
ESA cos 0.53 0.34 0.71 0.44 0.3 0.77 0.63 0.44 0.34 0.55 0.35 0.27
ESA align 0.55 0.56 0.75 0.49 0.52 0.78 0.69 0.38 0.46 0.71 0.47 0.34
SSA cos 0.4 0.34 0.63 0.4 0.22 0.71 0.6 0.42 0.35 0.48 0.47 0.26
SSA align 0.54 0.56 0.74 0.49 0.51 0.77 0.68 0.38 0.44 0.69 0.46 0.34
LSA cos 0.65 0.48 0.76 0.36 0.45 0.79 0.67 0.45 0.25 0.63 0.61 0.32
RP cos 0.6 0.49 0.78 0.46 0.43 0.79 0.7 0.45 0.38 0.68 0.57 0.34
Opinion-aware measures
AGENT 0.16 0.15 0.05 0.11 0.12 0.03 n/a -0.01 n/a 0.08 -0.04 0.11
DSEALGN 0.18 0.2 0.11 0.05 0.11 0.11 0.07 0.06 -0.1 0.08 0.13 0.1
DSESIM 0.12 0.15 0.05 0.1 0.08 0.07 0.04 0.08 0.05 0.08 0.04 0.08
EXPR 0.17 0.19 0.06 0.18 0.18 0.02 0.07 0 0.13 0.08 0.18 0.17
NUMEX1 0.12 0.22 -0.03 0.07 0.16 -0.05 -0.01 -0.01 -0.01 -0.03 0.08 0.1
NUMEX2 -0.25 0.19 0.01 0.06 0.14 -0.03 0.01 0.06 0.09 -0.05 0.03 0.11
SUBJCNT 0.14 0.19 0.01 0.09 0.07 0.03 0.02 0.08 0.05 0.05 0.05 0.09
SUBJDIFF -0.07 -0.07 -0.17 -0.27 -0.13 -0.22 -0.17 -0.12 -0.04 -0.12 -0.2 -0.12
SUBJSL 0.15 -0.11 0.07 0.23 0.01 0.07 0.11 -0.08 0.15 0.07 -0.03 0
Table 2: Correlation of individual features for the training and test sets with the gold standard.
Nonetheless, we notice several promising features,
such as DSEALIGN and EXPR. Lower cor-
relations seem to be associated with shorter spans
of text, since when averaging all opinion-based cor-
relations per dataset, MSRvid (x2), OnWN (x2),
and headlines display the lowest average correla-
tion, ranging from 0 to 0.03. This matches the
expectation that opinionated content can be easier
identified in longer contexts, as additional subjective
elements amount to a stronger prediction. The other
seven datasets consist of longer spans of text; they
display an average opinion-based correlation be-
tween 0.07 and 0.12, with the exception of FNWN
and SMTnews at 0.04 and 0.01, respectively.
Our systems performed well, ranking 38, 39 and
45 among the 88 competing systems in *SEM 2013
(see Table 1), with the best being comb.SVR and
comb.RandSubspace, both with a mean correlation
of 0.494. We noticed from our participation in
SEMEVAL 2012 (Banea et al, 2012), that training
and testing on the same type of data achieves the
best results; this receives further support when con-
sidering the performance of the indv.RandSubspace
variation on the OnWN data10, which exhibits a
10The SMT test data is not part of the same corpus as either
0.034 correlation increase over our next best sys-
tem (comb.RandSubspace). While we do surpass the
bag-of-words cosine baseline (baseline-tokencos)
computed by the task organizers by a 0.13 differ-
ence in correlation, we fall short by 0.124 from the
performance of the best system in the STS task.
5 Conclusions
To participate in the STS *SEM 2013 task, we con-
structed a meta-learner framework that combines
traditional knowledge and corpus-based methods,
while also introducing novel opinion analysis based
metrics. While the *SEM data is not particularly
suited for evaluating the performance of opinion fea-
tures, this is nonetheless a first step toward conduct-
ing text similarity research while also considering
the subjective dimension of text. Our system varia-
tions ranked 38, 39 and 45 among the 88 participat-
ing systems.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340 and IIS awards #1018613,
SMTep or SMTnews.
226
#0208798 and #0916046. This work was sup-
ported in part by DARPA-BAA-12-47 DEFT grant
#12475008. Any opinions, findings, and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily reflect
the views of the National Science Foundation or the
Defense Advanced Research Projects Agency.
References
E. Agirre, D. Cer, M. Diab, and A. Gonzalez. 2012.
Semeval-2012 task 6: A pilot on semantic textual sim-
ilarity. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), in con-
junction with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
E. Agirre, D. Cer, M. Diab, A. Gonzalez-Agirre, and W.
Guo. 2013. *SEM 2013 Shared Task: Semantic Tex-
tual Similarity, including a Pilot on Typed-Similarity.
In Proceedings of the Second Joint Conference on Lex-
ical and Computational Semantics (*SEM 2013), At-
lanta, GA, USA.
C. Banea, S. Hassan, M. Mohler, and R. Mihalcea. 2012.
UNT: A supervised synergistic approach to seman-
tic text similarity. In Proceedings of the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012), pages 635?642, Montreal, Canada.
E. Bingham and H. Mannila. 2001. Random projection
in dimensionality reduction: applications to image and
text data. In Proceedings of the seventh ACM SIGKDD
international conference on Knowledge discovery and
data mining (KDD 2001), pages 245?250, San Fran-
cisco, CA, USA.
C. Burgess, K. Livesay, and K. Lund. 1998. Explorations
in context space: words, sentences, discourse. Dis-
course Processes, 25(2):211?257.
K. Church and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics, 16(1):22?29.
S. Dasgupta. 1999. Learning mixtures of Gaussians. In
40th Annual Symposium on Foundations of Computer
Science (FOCS 1999), pages 634?644, New York, NY,
USA.
H. Drucker, C. J. Burges, L. Kaufman, A. Smola, and
Vladimir Vapnik. 1997. Support vector regression
machines. Advances in Neural Information Process-
ing Systems, 9:155?161.
R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. 2008.
Liblinear: A library for large linear classification. The
Journal of Machine Learning Research, 9:1871?1874.
E. Gabrilovich and S. Markovitch. 2007. Comput-
ing semantic relatedness using Wikipedia-based ex-
plicit semantic analysis. In Proceedings of the 20th
AAAI International Conference on Artificial Intelli-
gence (AAAI?07), pages 1606?1611, Hyderabad, In-
dia.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and Ian H. Witten. 2009. The WEKA data
mining software: An update. SIGKDD Explorations,
11(1).
S. Hassan and R. Mihalcea. 2011. Measuring semantic
relatedness using salient encyclopedic concepts. Arti-
ficial Intelligence, Special Issue.
T. K. Ho. 1998. The Random Subspace Method for
Constructing Decision Forests. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 20(8):832?
844.
A. Islam and D. Inkpen. 2006. Second order co-
occurrence PMI for determining the semantic similar-
ity of words. In Proceedings of the 5th Conference on
Language Resources and Evaluation (LREC 06), vol-
ume 2, pages 1033?1038, Genoa, Italy, July.
A. Islam and D. Inkpen. 2009. Semantic Similarity of
Short Texts. In Nicolas Nicolov, Galia Angelova, and
Ruslan Mitkov, editors, Recent Advances in Natural
Language Processing V, volume 309 of Current Issues
in Linguistic Theory, pages 227?236. John Benjamins,
Amsterdam & Philadelphia.
J. J. Jiang and D. W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
International Conference Research on Computational
Linguistics (ROCLING X), pages 9008+, September.
T. K. Landauer, T. K. L, D. Laham, B. Rehder, and M.
E. Schreiner. 1997. How well can passage meaning
be derived without using word order? a comparison of
latent semantic analysis and humans.
M. Lapata and R. Barzilay. 2005. Automatic evaluation
of text coherence: Models and representations. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence, Edinburgh.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense identi-
fication. In WordNet: An Electronic Lexical Database,
pages 305?332.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In SIGDOC ?86: Pro-
ceedings of the 5th annual international conference on
Systems documentation, pages 24?26, New York, NY,
USA. ACM.
C. Lin and E. Hovy. 2003. Automatic evaluation of sum-
maries using n-gram co-occurrence statistics. In Pro-
ceedings of Human Language Technology Conference
(HLT-NAACL 2003), Edmonton, Canada, May.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Interna-
227
tional Conference on Machine Learning, pages 296?
304, Madison, Wisconsin.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of text
semantic similarity. In Proceedings of the American
Association for Artificial Intelligence (AAAI 2006),
pages 775?780, Boston, MA, US.
G. A. Miller. 1995. WordNet: a Lexical database for
English. Communications of the Association for Com-
puting Machinery, 38(11):39?41.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the European Association for Compu-
tational Linguistics (EACL 2009), Athens, Greece.
M. Mohler, R. Bunescu, and R. Mihalcea. 2011. Learn-
ing to grade short answer questions using semantic
similarity measures and dependency graph alignments.
In Proceedings of the Association for Computational
Linguistics ? Human Language Technologies (ACL-
HLT 2011), Portland, Oregon, USA.
R. M. A. Nawab, M. Stevenson, and P. Clough. 2011.
External plagiarism detection using information re-
trieval and sequence alignment: Notebook for PAN at
CLEF 2011. In Proceedings of the 5th International
Workshop on Uncovering Plagiarism, Authorship, and
Social Software Misuse (PAN 2011).
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318, Philadelphia, PA.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet:: Similarity-Measuring the Relatedness of
Concepts. Proceedings of the National Conference on
Artificial Intelligence, pages 1024?1025.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence, pages 448?453.
J. Rocchio, 1971. Relevance feedback in information re-
trieval. Prentice Hall, Ing. Englewood Cliffs, New Jer-
sey.
G. Salton and C. Buckley. 1997. Term weighting ap-
proaches in automatic text retrieval. In Readings in
Information Retrieval. Morgan Kaufmann Publishers,
San Francisco, CA.
G. Salton and M. Lesk, 1971. The SMART Retrieval Sys-
tem: Experiments in Automatic Document Processing,
chapter Computer evaluation of indexing and text pro-
cessing. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997.
Automatic text structuring and summarization. Infor-
mation Processing and Management, 2(32).
H. Schutze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?124.
A. Smola and B. Schoelkopf. 1998. A tutorial on sup-
port vector regression. NeuroCOLT2 Technical Re-
port NC2-TR-1998-030.
P. D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning
(ECML?01), pages 491?502, Freiburg, Germany.
J. Wiebe and E. Riloff. 2005. Creating subjective and
objective sentence classifiers from unannotated texts.
In Proceedings of the 6th international conference on
Computational Linguistics and Intelligent Text Pro-
cessing (CICLing 2005), pages 486?497, Mexico City,
Mexico.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2-3):165?210.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff,
and Siddharth Patwardhan. 2005. OpinionFinder:
A system for subjectivity analysis. In Proceedings
of HLT/EMNLP on Interactive Demonstrations, pages
34?35, Vancouver, BC, Canada.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexical
selection. In Proceedings of the 32nd annual meeting
on Association for Computational Linguistics, pages
133?-138, Las Cruces, New Mexico.
B. Yang and C. Cardie. 2012. Extracting opinion expres-
sions with semi-markov conditional random fields. In
Proceedings of the conference on Empirical Meth-
ods in Natural Language Processing. Association for
Computational Linguistics.
228
