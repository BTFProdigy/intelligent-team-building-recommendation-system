Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 940?948,
Beijing, August 2010
Fast, Greedy Model Minimization for Unsupervised Tagging
Sujith Ravi and Ashish Vaswani and Kevin Knight and David Chiang
University of Southern California
Information Sciences Institute
{sravi,avaswani,knight,chiang}@isi.edu
Abstract
Model minimization has been shown to
work well for the task of unsupervised
part-of-speech tagging with a dictionary.
In (Ravi and Knight, 2009), the authors in-
voke an integer programming (IP) solver
to do model minimization. However,
solving this problem exactly using an
integer programming formulation is in-
tractable for practical purposes. We pro-
pose a novel two-stage greedy approxima-
tion scheme to replace the IP. Our method
runs fast, while yielding highly accurate
tagging results. We also compare our
method against standard EM training, and
show that we consistently obtain better
tagging accuracies on test data of varying
sizes for English and Italian.
1 Introduction
The task of unsupervised part-of-speech (POS)
tagging with a dictionary as formulated by Meri-
aldo (1994) is: given a raw word sequence and a
dictionary of legal POS tags for each word type,
tag each word token in the text. A common ap-
proach to modeling such sequence labeling prob-
lems is to build a bigram Hidden Markov Model
(HMM) parameterized by tag-bigram transition
probabilities P (ti|ti?1) and word-tag emission
probabilities P (wi|ti). Given a word sequence w
and a tag sequence t, of length N , the joint prob-
ability P (w, t) is given by:
P (w, t) =
N?
i=1
P (wi|ti) ? P (ti|ti?1) (1)
We can train this model using the Expectation
Maximization (EM) algorithm (Dempster and Ru-
bin, 1977) which learns P (wi|ti) and P (ti|ti?1)
that maximize the likelihood of the observed data.
Once the parameters are learnt, we can find the
best tagging using the Viterbi algorithm.
t? = arg max
t
P (w, t) (2)
Ravi and Knight (2009) attack the Merialdo
task in two stages. In the first stage, they search
for a minimized transition model (i.e., the small-
est set of tag bigrams) that can explain the data
using an integer programming (IP) formulation.
In the second stage, they build a smaller HMM
by restricting the transition parameters to only
those tag bigrams selected in the minimization
step. They employ the EM algorithm to train this
model, which prunes away some of the emission
parameters. Next, they use the pruned emission
model along with the original transition model
(which uses the full set of tag bigrams) and re-
train using EM. This alternating EM training pro-
cedure is repeated until the number of tag bigrams
in the Viterbi tagging output does not change be-
tween subsequent iterations. The final Viterbi tag-
ging output from their method achieves state-of-
the-art accuracy for this task. However, their mini-
mization step involves solving an integer program,
which can be very slow, especially when scal-
ing to large-scale data and more complex tagging
problems which use bigger tagsets. In this pa-
per, we present a novel method that optimizes the
same objective function using a fast greedy model
selection strategy. Our contributions are summa-
rized below:
940
? We present an efficient two-phase greedy-
selection method for solving the minimiza-
tion objective from Ravi and Knight (2009),
which runs much faster than their IP.
? Our method easily scales to large data
sizes (and big tagsets), unlike the previ-
ous minimization-based approaches and we
show runtime comparisons for different data
sizes.
? We achieve very high tagging accuracies
comparable to state-of-the-art results for un-
supervised POS tagging for English.
? Unlike previous approaches, we also show
results obtained when testing on the entire
Penn Treebank data (973k word tokens) in
addition to the standard 24k test data used for
this task. We also show the effectiveness of
this approach for Italian POS tagging.
2 Previous work
There has been much work on the unsupervised
part-of-speech tagging problem. Goldwater and
Griffiths (2007) also learn small models employ-
ing a fully Bayesian approach with sparse pri-
ors. They report 86.8% tagging accuracy with
manual hyperparameter selection. Smith and Eis-
ner (2005) design a contrastive estimation tech-
nique which yields a higher accuracy of 88.6%.
Goldberg et al (2008) use linguistic knowledge to
initialize the the parameters of the HMM model
prior to EM training. They achieve 91.4% ac-
curacy. Ravi and Knight (2009) use a Minimum
Description Length (MDL) method and achieve
the best results on this task thus far (91.6% word
token accuracy, 91.8% with random restarts for
EM). Our work follows a similar approach using a
model minimization component and alternate EM
training.
Recently, the integer programming framework
has been widely adopted by researchers to solve
other NLP tasks besides POS tagging such as se-
mantic role labeling (Punyakanok et al, 2004),
sentence compression (Clarke and Lapata, 2008),
decipherment (Ravi and Knight, 2008) and depen-
dency parsing (Martins et al, 2009).
3 Model minimization formulated as a
Path Problem
The complexity of the model minimization step
in (Ravi and Knight, 2009) and its proposed ap-
proximate solution can be best understood if we
formulate it as a path problem in a graph.
Let w = w0, w1, . . . , wN , wN+1 be a word se-
quence where w1, . . . , wN are the input word to-
kens and {w0, wN+1} are the start/end tokens.
Let T = {T1, . . . , TK}?{T0, TK+1} be the fixed
set of all possible tags. T0 and TK+1 are special
tags that we add for convenience. These would be
the start and end tags that one typically adds to
the HMM lattice. The tag dictionary D contains
entries of the form (wi, Tj) for all the possible
tags Tj that word token wi can have. We add en-
tries (w0, T0) and (wK+1, TK+1) to D. Given this
input, we now create a directed graph G(V,E).
Let C0, C1 . . . , CK+1 be columns of nodes in G,
where column Ci corresponds to word token wi.
For all i = 0, . . . , N+1 and j = 0, . . . ,K+1, we
add node Ci,j in column Ci if (wi, Tj) ? D. Now,
?i = 0, . . . , N , we create directed edges from ev-
ery node in Ci to every node in Ci+1. Each of
these edges e = (Ci,j , Ci+1,k) is given the label
(Tj , Tk) which corresponds to a tag bigram. This
creates our directed graph. Let l(e) be the tag bi-
gram label of edges e ? E. For every path P from
C0,0 to CN+1,K+1, we say that P uses an edge la-
bel or tag bigram (Tj , Tk) if there exists an edge
e in P such that l(e) = (Tj , Tk). We can now
formulate the the optimization problem as: Find
the smallest set S of tag bigrams such that there
exists at least one path from C0,0 to CN+1,K+1 us-
ing only the tag bigrams in S. Let us call this the
Minimal Tag Bigram Path (MinTagPath) problem.
Figure 1 shows an example graph where the
input word sequence is w1, . . . , w4 and T =
{T1, . . . , T3} is the input tagset. We add the
start/end word tokens {w0, w5} and correspond-
ing tags {T0, T4}. The edges in the graph are in-
stantiated according to the word/tag dictionary D
provided as input. The node and edge labels are
also illustrated in the graph. Our goal is to find a
path from C0,0 to C5,4 using the smallest set of tag
bigrams.
941
T0
T
1
T
2
T
3
T
4
w
0
w
1
w
2
w
3
w
4
w
5
T
0
,T
1
T
0
,T
3
T
1
,T
2
T
1
,T
2
T
2
,T
1
T
2
,T
2
T
3
,T
2
T
3
,T
4
T
2
,T
4
T
2
,T
3
T
2
,T
2
T
1
,T
3
C
0,0
C
1,1
C
1,3
C
2,2
C
3,1
C
3,2 C
4,2
C
4,3
C
5,4
word sequence: 
POS tags
Initial graph: G (V, E)
Figure 1: Graph instantiation for the MinTagPath problem.
4 Problem complexity
Having defined the problem, we now show that
it can be solved in polynomial time even though
the number of paths from C0,0 to CN+1,K+1 is
exponential in N , the input size. This relies on the
assumption that the tagset T is fixed in advance,
which is the case for most tagging tasks.1 Let B
be the set of all the tag bigram labels in the graph,
B = {l(e), ?e ? E}. Now, the size of B would
be at most K2 + 2K where every word could be
tagged with every possible tag. For m = 1 . . . |B|,
let Bm be the set of subsets of B each of which
have size m. Algorithm 1 optimally solves the
MinTagPath problem.
Algorithm 1 basically enumerates all the possi-
ble subsets of B, from the smallest to the largest,
and checks if there is a path. It exits the first time a
path is found and therefore finds the smallest pos-
sible set si of size m such that a path exists that
uses only the tag bigrams in si. This implies the
correctness of the algorithm. To check for path ex-
istence, we could either throw away all the edges
from E not having a label in si, and then execute
a Breadth-First-Search (BFS) or we could traverse
1If K, the size of the tagset, is a variable as well, then we
suspect the problem is NP-hard.
Algorithm 1 Brute Force solution to MinTagPath
for m = 1 to |B| do
for si ? Bm do
Use Breadth First Search (BFS) to check
if ? path P from C0,0 to CN+1,K+1 using
only the tag bigrams in si.
if P exists then
return si,m
end if
end for
end for
only the edges with labels in si during BFS. The
running time of Algorithm 1 is easy to calculate.
Since, in the worst case we go over all the sub-
sets of size m = 1, . . . , |B| of B, the number of
iterations we can perform is at most 2|B|, the size
of the powerset P of B. In each iteration, we do
a BFS through the lattice, which has O(N) time
complexity2 since the lattice size is linear in N
and BFS is linear in the lattice size. Hence the run-
ning time is? 2|B| ?O(N) = O(N). Even though
this shows that MinTagPath can be solved in poly-
nomial time, the time complexity is prohibitively
large. For the Penn Treebank, K = 45 and the
2Including throwing away edges or not.
942
worst case running time would be ? 1013.55 ? N .
Clearly, for all practical purposes, this approach is
intractable.
5 Greedy Model Minimization
We do not know of an efficient, exact algorithm
to solve the MinTagPath problem. Therefore, we
present a simple and fast two-stage greedy ap-
proximation scheme. Notice that an optimal path
P (or any path) covers all the input words i.e., ev-
ery word token wi has one of its possible taggings
in P . Exploiting this property, in the first phase,
we set our goal to cover all the word tokens using
the least possible number of tag bigrams. This can
be cast as a set cover problem (Garey and John-
son, 1979) and we use the set cover greedy ap-
proximation algorithm in this stage. The output
tag bigrams from this phase might still not allow
any path from C0,0 to CN+1,K+1. So we carry out
a second phase, where we greedily add a few tag
bigrams until a path is created.
5.1 Phase 1: Greedy Set Cover
In this phase, our goal is to cover all the word to-
kens using the least number of tag bigrams. The
covering problem is exactly that of set cover. Let
U = {w0, . . . , wN +1} be the set of elements that
needs to be covered (in this case, the word tokens).
For each tag bigram (Ti, Tj) ? B, we define its
corresponding covering set STi,Tj as follows:
STi,Tj = {wn : ((wn, Ti) ? D
? (Cn,i, Cn+1,j) ? E
? l(Cn,i, Cn+1,j) = (Ti, Tj))?
((wn, Tj) ? D
? (Cn?1,i, Cn,j) ? E
? l(Cn?1,i, Cn,j) = (Ti, Tj))}
Let the set of covering sets be X . We assign
a cost of 1 to each covering set in X . The goal
is to select a set CHOSEN ? X such that?
STi,Tj?CHOSEN
= U , minimizing the total cost
of CHOSEN . This corresponds to covering all
the words with the least possible number of tag
bigrams. We now use the greedy approximation
algorithm for set cover to solve this problem. The
pseudo code is shown in Algorithm 2.
Algorithm 2 Set Cover : Phase 1
Definitions
Define CAND : Set of candidate covering sets
in the current iteration
Define Urem : Number of elements in U re-
maining to be covered
Define ESTi,Tj : Current effective cost of a setDefine Itr : Iteration number
Initializations
LET CAND = X
LET CHOSEN = ?
LET Urem = U
LET Itr = 0
LET ESTi,Tj = 1|STi,Tj | , ? STi,Tj ? CAND
while Urem 6= ? do
Itr ? Itr + 1
Define S?Itr = argmin
STi,Tj?CAND
ESTi,Tj
CHOSEN = CHOSEN
?
S?Itr
Remove S?Itr from CAND
Remove all the current elements in S?Itr from
Urem
Remove all the current elements in S?Itr from
every STi,Tj ? CAND
Update effective costs, ? STi,Tj ? CAND,
ESTi,Tj =
1
|STi,Tj |end while
return CHOSEN
For the graph shown in Figure 1, here are a few
possible covering sets STi,Tj and their initial ef-
fective costs ESTi,Tj .
? ST0,T1 = {w0, w1}, EST0,T1 = 1/2
? ST1,T2 = {w1, w2, w3, w4}, EST1,T2 = 1/4
? ST2,T2 = {w2, w3, w4}, EST2,T2 = 1/3
In every iteration Itr of Algorithm 2, we pick a
set S?Itr that is most cost effective. The elements
that S?Itr covers are then removed from all the re-
maining candidate sets and Urem and the effec-
tiveness of the candidate sets is recalculated for
the next iteration. The algorithm stops when all
elements of U i.e., all the word tokens are cov-
ered. Let, BCHOSEN = {(Ti, Tj) : STi,Tj ?
943
CHOSEN}, be the set of tag bigrams that have
been chosen by set cover. Now, we check, using
BFS, if there exists a path from C0,0 to CN+1,K+1
using only the tag bigrams in BCHOSEN . If not,
then we have to add tag bigrams to BCHOSEN to
enable a path. To accomplish this, we carry out the
second phase of this scheme with another greedy
strategy (described in the next section).
For the example graph in Figure 1,
one possible solution BCHOSEN =
{(T0, T1), (T1, T2), (T2, T4)}.
5.2 Phase 2: Greedy Path Completion
We define a graph GCHOSEN (V ?, E?) ?
G(V,E) that contains the edges e ? E such
l(e) ? BCHOSEN .
Let BCAND = B \ BCHOSEN , be the current
set of candidate tag bigrams that can be added to
the final solution which would create a path. We
would like to know how many holes a particular
tag bigram (Ti, Tj) can fill. We define a hole as an
edge e such that e ? G \ GCHOSEN and there
exists e?, e?? ? GCHOSEN such that tail(e?) =
head(e) ? tail(e) = head(e??).
Figure 2 illustrates the graph GCHOSEN using
tag bigrams from the example solution to Phase 1
(Section 5.1). The dotted edge (C2,2, C3,1) rep-
resents a hole, which has to be filled in the cur-
rent phase in order to complete a path from C0,0
to C5,4.
In Algorithm 3, we define the effectiveness of a
candidate tag bigram H(Ti, Tj) to be the number
of holes it covers. In every iteration, we pick the
most effective tag bigram, fill the holes and recal-
culate the effectiveness of the remaining candidate
tag bigrams.
Algorithm 3 returns BFINAL, the final set of
chosen tag bigrams. It terminates when a path has
been found.
5.3 Fitting the Model
Once the greedy algorithm terminates and returns
a minimized grammar of tag bigrams, we follow
the approach of Ravi and Knight (2009) and fit
the minimized model to the data using the alter-
nating EM strategy. The alternating EM iterations
are terminated when the change in the size of the
observed grammar (i.e., the number of unique tag
Algorithm 3 Greedy Path Complete : Phase 2
Define BFINAL : Final set of tag bigrams se-
lected by the two-phase greedy approach
LET BFINAL = BCHOSEN
LET H(Ti, Tj) = |{e}| such that l(e) =
(Ti, Tj) and e is a hole, ? (Ti, Tj) ? BCAND
while @ path P from C0,0 to CN+1,K+1 using
only (Ti, Tj) ? BCHOSEN do
Define (T?i, T?j) = argmax
(Ti,Tj)?BCAND
H(Ti, Tj)
BFINAL = BFINAL
?
(T?i, T?j)
Remove (T?i, T?j) from BCAND
GCHOSEN = GCHOSEN
?{e} such that
l(e) = (Ti, Tj)
? (Ti, Tj) ? BCAND, Recalculate H(Ti, Tj)
end while
return BFINAL
bigrams in the tagging output) is ? 5%. We refer
to our entire approach using greedy minimization
followed by EM training as MIN-GREEDY.
6 Experiments and Results
6.1 English POS Tagging
Data: We use a standard test set (consisting of
24,115 word tokens from the Penn Treebank) for
the POS tagging task (described in Section 1). The
tagset consists of 45 distinct tag labels and the
dictionary contains 57,388 word/tag pairs derived
from the entire Penn Treebank. Per-token ambi-
guity for the test data is about 1.5 tags/token. In
addition to the standard 24k dataset, we also train
and test on larger data sets of 48k, 96k, 193k, and
the entire Penn Treebank (973k).
Methods: We perform comparative evaluations
for POS tagging using three different methods:
1. EM: Training a bigram HMM model using
EM algorithm.
2. IP: Minimizing grammar size using inte-
ger programming, followed by EM training
(Ravi and Knight, 2009).
3. MIN-GREEDY: Minimizing grammar size
using the Greedy method described in Sec-
944
T0
T
1
T
2
T
3
T
4
w
0
w
1
w
2
w
3
w
4
w
5
T
0
,T
1
T
1
,T
2
T
1
,T
2
T
2
,T
1
T
2
,T
4
C
0,0
C
1,1
C
1,3
C
2,2
C
3,1
C
3,2 C
4,2
C
4,3
C
5,4
word sequence: 
POS tags
 T
0
,T
1
 T
1
,T
2
 T
2
,T
4
Tag bigrams chosen after Phase 1 
(B
CHOSEN
)
Hole in graph: Edge e = (C
2,2
, C
3,1
)
Graph after Phase 1: G
CHOSEN 
(V?, E?)
Figure 2: Graph constructed with tag bigrams chosen in Phase 1 of the MIN-GREEDY method.
tion 5, followed by EM training.
Results: Figure 3 shows the tagging perfor-
mance (word token accuracy %) achieved by the
three methods on the standard test (24k tokens) as
well as Penn Treebank test (PTB = 973k tokens).
On the 24k test data, the MIN-GREEDY method
achieves a high tagging accuracy comparable to
the previous best from the IP method. However,
the IP method does not scale well which makes
it infeasible to run this method in a much larger
data setting (the entire Penn Treebank). MIN-
GREEDY on the other hand, faces no such prob-
lem and in fact it achieves high tagging accuracies
on all four datasets, consistently beating EM by
significant margins. When tagging all the 973k
word tokens in the Penn Treebank data, it pro-
duces an accuracy of 87.1% which is much better
than EM (82.3%) run on the same data.
Ravi and Knight (2009) mention that it is pos-
sible to interrupt the IP solver and obtain a sub-
optimal solution faster. However, the IP solver did
not return any solution when provided the same
amount of time as taken by MIN-GREEDY for
any of the data settings. Also, our algorithms
were implemented in Python while the IP method
employs the best available commercial software
package (CPLEX) for solving integer programs.
Figure 4 compares the running time efficiency
for the IP method versus MIN-GREEDY method
Test set Efficiency
(average running time in secs.)
IP MIN-GREEDY
24k test 93.0 34.3
48k test 111.7 64.3
96k test 397.8 93.3
193k test 2347.0 331.0
PTB (973k) test ? 1485.0
Figure 4: Comparison of MIN-GREEDY versus
MIN-GREEDY approach in terms of efficiency
(average running time in seconds) for different
data sizes. All the experiments were run on a sin-
gle machine with a 64-bit, 2.4 GHz AMD Opteron
850 processor.
as we scale to larger datasets. Since the IP solver
shows variations in running times for different
datasets of the same size, we show the average
running times for both methods (for each row in
the figure, we run a particular method on three
different datasets with similar sizes and average
the running times). The figure shows that the
greedy approach can scale comfortably to large
data sizes, and a complete run on the entire Penn
Treebank data finishes in just 1485 seconds. In
contrast, the IP method does not scale well?on
average, it takes 93 seconds to finish on the 24k
test (versus 34 seconds for MIN-GREEDY) and
on the larger PTB test data, the IP solver runs for
945
Method Tagging accuracy (%)
when training & testing on:
24k 48k 96k 193k PTB (973k)
EM 81.7 81.4 82.8 82.0 82.3
IP 91.6 89.3 89.5 91.6 ?
MIN-GREEDY 91.6 88.9 89.4 89.1 87.1
Figure 3: Comparison of tagging accuracies on test data of varying sizes for the task of unsupervised
English POS tagging with a dictionary using a 45-tagset. (? IP method does not scale to large data).
 400
 600
 800
 1000
 1200
 1400
 1600
Ob
se
rv
ed
 g
ra
mm
ar
 s
iz
e 
(#
 o
f 
ta
g 
bi
gr
am
s)
 
 i
n 
fi
na
l 
ta
gg
in
g 
ou
tp
ut
Size of test data (# of word tokens)
24k 48k 96k 193k PTB (973k)
EM
IP
Greedy
Figure 5: Comparison of observed grammar size
(# of tag bigram types) in the final tagging output
from EM, IP and MIN-GREEDY.
more than 3 hours without returning a solution.
It is interesting to see that for the 24k dataset,
the greedy strategy finds a grammar set (contain-
ing only 478 tag bigrams). We observe that MIN-
GREEDY produces 452 tag bigrams in the first
minimization step (phase 1), and phase 2 adds an-
other 26 entries, yielding a total of 478 tag bi-
grams in the final minimized grammar set. That
is almost as good as the optimal solution (459
tag bigrams from IP) for the same problem. But
MIN-GREEDY clearly has an advantage since it
runs much faster than IP (as shown in Figure 4).
Figure 5 shows a plot with the size of the ob-
served grammar (i.e., number of tag bigram types
in the final tagging output) versus the size of the
test data for EM, IP and MIN-GREEDY methods.
The figure shows that unlike EM, the other two
approaches reduce the grammar size considerably
and we observe the same trend even when scaling
Test set Average Speedup Optimality Ratio
24k test 2.7 0.96
48k test 1.7 0.98
96k test 4.3 0.98
193k test 7.1 0.93
Figure 6: Average speedup versus Optimality ra-
tio computed for the model minimization step
(when using MIN-GREEDY over IP) on different
datasets.
to larger data. Minimizing the grammar size helps
remove many spurious tag combinations from the
grammar set, thereby yielding huge improvements
in tagging accuracy over the EM method (Fig-
ure 3). We observe that for the 193k dataset, the
final observed grammar size is greater for IP than
MIN-GREEDY. This is because the alternating
EM steps following the model minimization step
add more tag bigrams to the grammar.
We compute the optimality ratio of the MIN-
GREEDY approach with respect to the grammar
size as follows:
Optimality ratio = Size of IP grammarSize of MIN-GREEDY grammar
A value of 1 for this ratio implies that the solu-
tion found by MIN-GREEDY algorithm is exact.
Figure 6 compares the optimality ratio versus av-
erage speedup (in terms of running time) achieved
in the minimization step for the two approaches.
The figure illustrates that our solution is nearly op-
timal for all data settings with significant speedup.
The MIN-GREEDY algorithm presented here
can also be applied to scenarios where the dictio-
nary is incomplete (i.e., entries for all word types
are not present in the dictionary) and rare words
946
Method Tagging accuracy (%) Number of unique tag bigrams in final tagging output
EM 83.4 1195
IP 88.0 875
MIN-GREEDY 88.0 880
Figure 7: Results for unsupervised Italian POS tagging with a dictionary using a set of 90 tags.
take on all tag labels. In such cases, we can fol-
low a similar approach as Ravi and Knight (2009)
to assign tag possibilities to every unknown word
using information from the known word/tag pairs
present in the dictionary. Once the completed dic-
tionary is available, we can use the procedure de-
scribed in Section 5 to minimize the size of the
grammar, followed by EM training.
6.2 Italian POS Tagging
We also compare the three approaches for Italian
POS tagging and show results.
Data: We use the Italian CCG-TUT corpus (Bos
et al, 2009), which contains 1837 sentences. It
has three sections: newspaper texts, civil code
texts and European law texts from the JRC-Acquis
Multilingual Parallel Corpus. For our experi-
ments, we use the POS-tagged data from the
CCG-TUT corpus, which uses a set of 90 tags.
We created a tag dictionary consisting of 8,733
word/tag pairs derived from the entire corpus
(42,100 word tokens). We then created a test set
consisting of 926 sentences (21,878 word tokens)
from the original corpus. The per-token ambiguity
for the test data is about 1.6 tags/token.
Results: Figure 7 shows the results on Italian
POS tagging. We observe that MIN-GREEDY
achieves significant improvements in tagging ac-
curacy over the EM method and comparable to IP
method. This also shows that the idea of model
minimization is a general-purpose technique for
such applications and provides good tagging ac-
curacies on other languages as well.
7 Conclusion
We present a fast and efficient two-stage greedy
minimization approach that can replace the inte-
ger programming step in (Ravi and Knight, 2009).
The greedy approach finds close-to-optimal solu-
tions for the minimization problem. Our algo-
rithm runs much faster and achieves accuracies
close to state-of-the-art. We also evaluate our
method on test sets of varying sizes and show that
our approach outperforms standard EM by a sig-
nificant margin. For future work, we would like
to incorporate some linguistic constraints within
the greedy method. For example, we can assign
higher costs to unlikely tag combinations (such as
?SYM SYM?, etc.).
Our greedy method can also be used for solving
other unsupervised tasks where model minimiza-
tion using integer programming has proven suc-
cessful, such as word alignment (Bodrumlu et al,
2009).
Acknowledgments
The authors would like to thank Shang-Hua Teng
and Anup Rao for their helpful comments and
also the anonymous reviewers. This work was
jointly supported by NSF grant IIS-0904684,
DARPA contract HR0011-06-C-0022 under sub-
contract to BBN Technologies and DARPA con-
tract HR0011-09-1-0028.
References
Bodrumlu, T., K. Knight, and S. Ravi. 2009. A new
objective function for word alignment. In Proceed-
ings of the NAACL/HLT Workshop on Integer Pro-
gramming for Natural Language Processing.
Bos, J., C. Bosco, and A. Mazzei. 2009. Converting a
dependency treebank to a categorial grammar tree-
bank for Italian. In Proceedings of the Eighth In-
ternational Workshop on Treebanks and Linguistic
Theories (TLT8).
Clarke, J. and M. Lapata. 2008. Global inference for
sentence compression: An integer linear program-
ming approach. Journal of Artificial Intelligence
Research (JAIR), 31(4):399?429.
Dempster, A.P., N.M. Laird and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via the
947
EM algorithm. Journal of the Royal Statistical So-
ciety, 39(1):1?38.
Garey, M. R. and D. S. Johnson. 1979. Computers
and Intractability: A Guide to the Theory of NP-
Completeness. John Wiley & Sons.
Goldberg, Y., M. Adler, and M. Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proceedings of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
(ACL/HLT).
Goldwater, Sharon and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Martins, A., N. A. Smith, and E. P. Xing. 2009. Con-
cise integer linear programming formulations for
dependency parsing. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the As-
sociation for Computational Linguistics (ACL) and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP.
Merialdo, B. 1994. Tagging English text with a
probabilistic model. Computational Linguistics,
20(2):155?171.
Punyakanok, V., D. Roth, W. Yih, and D. Zimak.
2004. Semantic role labeling via integer linear pro-
gramming inference. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING).
Ravi, S. and K. Knight. 2008. Attacking decipher-
ment problems optimally with low-order n-gram
models. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
Ravi, S. and K. Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Pro-
ceedings of the Joint Conference of the 47th An-
nual Meeting of the Association for Computational
Linguistics (ACL) and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP.
Smith, N. and J. Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL).
948
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1387?1392,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Decoding with Large-Scale Neural Language Models
Improves Translation
Ashish Vaswani
University of Southern California
Department of Computer Science
avaswani@isi.edu
Yinggong Zhao
Nanjing University, State Key Laboratory
for Novel Software Technology
zhaoyg@nlp.nju.edu.cn
Victoria Fossum and David Chiang
University of Southern California
Information Sciences Institute
{vfossum,chiang}@isi.edu
Abstract
We explore the application of neural language
models to machine translation. We develop a
new model that combines the neural proba-
bilistic language model of Bengio et al, rec-
tified linear units, and noise-contrastive esti-
mation, and we incorporate it into a machine
translation system both by reranking k-best
lists and by direct integration into the decoder.
Our large-scale, large-vocabulary experiments
across four language pairs show that our neu-
ral language model improves translation qual-
ity by up to 1.1 Bleu.
1 Introduction
Machine translation (MT) systems rely upon lan-
guage models (LMs) during decoding to ensure flu-
ent output in the target language. Typically, these
LMs are n-gram models over discrete representa-
tions of words. Such models are susceptible to data
sparsity?that is, the probability of an n-gram ob-
served only few times is difficult to estimate reli-
ably, because these models do not use any informa-
tion about similarities between words.
To address this issue, Bengio et al (2003) pro-
pose distributed word representations, in which each
word is represented as a real-valued vector in a
high-dimensional feature space. Bengio et al (2003)
introduce a feed-forward neural probabilistic LM
(NPLM) that operates over these distributed repre-
sentations. During training, the NPLM learns both a
distributed representation for each word in the vo-
cabulary and an n-gram probability distribution over
words in terms of these distributed representations.
Although neural LMs have begun to rival or even
surpass traditional n-gram LMs (Mnih and Hin-
ton, 2009; Mikolov et al, 2011), they have not yet
been widely adopted in large-vocabulary applica-
tions such as MT, because standard maximum like-
lihood estimation (MLE) requires repeated summa-
tions over all words in the vocabulary. A variety of
strategies have been proposed to combat this issue,
many of which require severe restrictions on the size
of the network or the size of the data.
In this work, we extend the NPLM of Bengio et
al. (2003) in two ways. First, we use rectified lin-
ear units (Nair and Hinton, 2010), whose activa-
tions are cheaper to compute than sigmoid or tanh
units. There is also evidence that deep neural net-
works with rectified linear units can be trained suc-
cessfully without pre-training (Zeiler et al, 2013).
Second, we train using noise-contrastive estimation
or NCE (Gutmann and Hyva?rinen, 2010; Mnih and
Teh, 2012), which does not require repeated summa-
tions over the whole vocabulary. This enables us to
efficiently build NPLMs on a larger scale than would
be possible otherwise.
We then apply this LM to MT in two ways. First,
we use it to rerank the k-best output of a hierarchi-
cal phrase-based decoder (Chiang, 2007). Second,
we integrate it directly into the decoder, allowing the
neural LM to more strongly influence the model. We
achieve gains of up to 0.6 Bleu translating French,
German, and Spanish to English, and up to 1.1 Bleu
on Chinese-English translation.
1387
u1 u2
input
words
input
embeddings
hidden
h1
hidden
h2
output
P(w | u)
D?
M
C1 C2
D
Figure 1: Neural probabilistic language model (Bengio et
al., 2003).
2 Neural Language Models
Let V be the vocabulary, and n be the order of
the language model; let u range over contexts, i.e.,
strings of length (n?1), and w range over words. For
simplicity, we assume that the training data is a sin-
gle very long string, w1 ? ? ?wN , where wN is a special
stop symbol, </s>. We write ui for wi?n+1 ? ? ?wi?1,
where, for i ? 0, wi is a special start symbol, <s>.
2.1 Model
We use a feedforward neural network as shown in
Figure 1, following Bengio et al (2003). The input
to the network is a sequence of one-hot represen-
tations of the words in context u, which we write
u j (1 ? j ? n ? 1). The output is the probability
P(w | u) for each word w, which the network com-
putes as follows.
The hidden layers consist of rec-
tified linear units (Nair and Hinton,
2010), which use the activation func-
tion ?(x) = max(0, x) (see graph at
right).
The output of the first hidden layer h1 is
h1 = ?
?
????????
n?1?
j=1
C jDu j
?
????????
(1)
where D is a matrix of input word embeddings
which is shared across all positions, the C j are the
context matrices for each word in u, and ? is applied
elementwise. The output of the second layer h2 is
h2 = ? (Mh1) ,
where M is the matrix of connection weights be-
tween h1 and h2. Finally, the output layer is a soft-
max layer,
P(w | u) ? exp
(
D?h2 + b
)
(2)
where D? is the output word embedding matrix and b
is a vector of biases for every word in the vocabulary.
2.2 Training
The typical way to train neural LMs is to maximize
the likelihood of the training data by gradient ascent.
But the softmax layer requires, at each iteration, a
summation over all the units in the output layer, that
is, all words in the whole vocabulary. If the vocabu-
lary is large, this can be prohibitively expensive.
Noise-contrastive estimation or NCE (Gutmann
and Hyva?rinen, 2010) is an alternative estimation
principle that allows one to avoid these repeated
summations. It has been applied previously to log-
bilinear LMs (Mnih and Teh, 2012), and we apply it
here to the NPLM described above.
We can write the probability of a word w given a
context u under the NPLM as
P(w | u) =
1
Z(u)
p(w | u)
p(w | u) = exp
(
D?h2 + b
)
Z(u) =
?
w?
p(w? | u) (3)
where p(w | u) is the unnormalized output of the unit
corresponding to w, and Z(u) is the normalization
factor. Let ? stand for the parameters of the model.
One possibility would be to treat Z(u), instead of
being defined by (3), as an additional set of model
parameters which are learned along with ?. But it is
easy to see that we can make the likelihood arbitrar-
ily large by making the Z(u) arbitrarily small.
In NCE, we create a noise distribution q(w).
For each example uiwi, we add k noise samples
w?i1, . . . , w?ik into the data, and extend the model to
account for noise samples by introducing a random
1388
variable C which is 1 for training examples and 0 for
noise samples:
P(C = 1,w | u) =
1
1 + k
?
1
Z(u)
p(w | u)
P(C = 0,w | u) =
k
1 + k
? q(w).
We then train the model to classify examples as
training data or noise, that is, to maximize the con-
ditional likelihood,
L =
N?
i=1
(
log P(C = 1 | uiwi) +
k?
j=1
log P(C = 0 | uiw?i j)
)
with respect to both ? and Z(u).
We do this by stochastic gradient ascent. The gra-
dient with respect to ? turns out to be
?L
??
=
N?
i=1
(
P(C = 0 | uiwi)
?
??
log p(wi | ui) ?
k?
j=1
P(C = 1 | uiw?i j)
?
??
log p(w?i j | ui)
)
and similarly for the gradient with respect to Z(u).
These can be computed by backpropagation. Unlike
before, the Z(u) will converge to a value that normal-
izes the model, satisfying (3), and, under appropriate
conditions, the parameters will converge to a value
that maximizes the likelihood of the data.
3 Implementation
Both training and scoring of neural LMs are compu-
tationally expensive at the scale needed for machine
translation. In this section, we describe some of the
techniques used to make them practical for transla-
tion.
3.1 Training
During training, we compute gradients on an en-
tire minibatch at a time, allowing the use of matrix-
matrix multiplications instead of matrix-vector mul-
tiplications (Bengio, 2012). We represent the inputs
as a sparse matrix, allowing the computation of the
input layer (1) to use sparse matrix-matrix multi-
plications. The output activations (2) are computed
only for the word types that occur as the positive ex-
ample or one of the noise samples, yielding a sparse
matrix of outputs. Similarly, during backpropaga-
tion, sparse matrix multiplications are used at both
the output and input layer.
In most of these operations, the examples in a
minibatch can be processed in parallel. However, in
the sparse-dense products used when updating the
parameters D and D?, we found it was best to di-
vide the vocabulary into blocks (16 per thread) and
to process the blocks in parallel.
3.2 Translation
To incorporate this neural LM into a MT system, we
can use the LM to rerank k-best lists, as has been
done in previous work. But since the NPLM scores
n-grams, it can also be integrated into a phrase-based
or hierarchical phrase-based decoder just as a con-
ventional n-gram model can, unlike a RNN.
The most time-consuming step in computing n-
gram probabilities is the computation of the nor-
malization constants Z(u). Following Mnih and Teh
(2012), we set al the normalization constants to one
during training, so that the model learns to produce
approximately normalized probabilities. Then, when
applying the LM, we can simply ignore normaliza-
tion. A similar strategy was taken by Niehues and
Waibel (2012). We find that a single n-gram lookup
takes about 40 ?s.
The technique, described above, of grouping ex-
amples into minibatches works for scoring of k-best
lists, but not while decoding. But caching n-gram
probabilities helps to reduce the cost of the many
lookups required during decoding.
A final issue when decoding with a neural LM
is that, in order to estimate future costs, we need
to be able to estimate probabilities of n?-grams for
n? < n. In conventional LMs, this information is
readily available,1 but not in NPLMs. Therefore, we
defined a special word <null> whose embedding is
the weighted average of the (input) embeddings of
all the other words in the vocabulary. Then, to esti-
mate the probability of an n?-gram u?w, we used the
probability of P(w | <null>n?n
?
u?).
1However, in Kneser-Ney smoothed LMs, this information
is also incorrect (Heafield et al, 2012).
1389
setting dev 2004 2005 2006
baseline 38.2 38.4 37.7 34.3
reranking 38.5 38.6 37.8 34.7
decoding 39.1 39.5 38.8 34.9
Table 1: Results for Chinese-English experiments, with-
out neural LM (baseline) and with neural LM for rerank-
ing and integrated decoding. Reranking with the neural
LM improves translation quality, while integrating it into
the decoder improves even more.
4 Experiments
We ran experiments on four language pairs ? Chi-
nese to English and French, German, and Spanish
to English ? using a hierarchical phrase-based MT
system (Chiang, 2007) and GIZA++ (Och and Ney,
2003) for word alignments.
For all experiments, we used four LMs. The base-
lines used conventional 5-gram LMs, estimated with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998) on the English side of the bitext and the
329M-word Xinhua portion of English Gigaword
(LDC2011T07). Against these baselines, we tested
systems that included the two conventional LMs as
well as two 5-gram NPLMs trained on the same
datasets. The Europarl bitext NPLMs had a vocab-
ulary size of 50k, while the other NPLMs had a vo-
cabulary size of 100k. We used 150 dimensions for
word embeddings, 750 units in hidden layer h1, and
150 units in hidden layer h2. We initialized the net-
work parameters uniformly from (?0.01, 0.01) and
the output biases to ? log |V |, and optimized them by
10 epochs of stochastic gradient ascent, using mini-
batches of size 1000 and a learning rate of 1. We
drew 100 noise samples per training example from
the unigram distribution, using the alias method for
efficiency (Kronmal and Peterson, 1979).
We trained the discriminative models with MERT
(Och, 2003) and the discriminative rerankers on
1000-best lists with MERT. Except where noted, we
ran MERT three times and report the average score.
We evaluated using case-insensitive NIST Bleu.
4.1 NIST Chinese-English
For the Chinese-English task (Table 1), the training
data came from the NIST 2012 constrained track,
excluding sentences longer than 60 words. Rules
Fr-En De-En Es-En
setting dev test dev test dev test
baseline 33.5 25.5 28.8 21.5 33.5 32.0
reranking 33.9 26.0 29.1 21.5 34.1 32.2
decoding 34.12 26.12 29.3 21.9 34.22 32.12
Table 2: Results for Europarl MT experiments, without
neural LM (baseline) and with neural LM for reranking
and integrated decoding. The neural LM gives improve-
ments across three different language pairs. Superscript 2
indicates a score averaged between two runs; all other
scores were averaged over three runs.
without nonterminals were extracted from all train-
ing data, while rules with nonterminals were ex-
tracted from the FBIS corpus (LDC2003E14). We
ran MERT on the development data, which was the
NIST 2003 test data, and tested on the NIST 2004?
2006 test data.
Reranking using the neural LM yielded improve-
ments of 0.2?0.4 Bleu, while integrating the neural
LM yielded larger improvements, between 0.6 and
1.1 Bleu.
4.2 Europarl
For French, German, and Spanish translation, we
used a parallel text of about 50M words from Eu-
roparl v7. Rules without nonterminals were ex-
tracted from all the data, while rules with nonter-
minals were extracted from the first 200k words. We
ran MERT on the development data, which was the
WMT 2005 test data, and tested on the WMT 2006
news commentary test data (nc-test2006).
The improvements, shown in Table 2, were more
modest than on Chinese-English. Reranking with
the neural LM yielded improvements of up to 0.5
Bleu, and integrating the neural LM into the decoder
yielded improvements of up to 0.6 Bleu. In one
case (Spanish-English), integrated decoding scored
higher than reranking on the development data but
lower on the test data ? perhaps due to the differ-
ence in domain between the two. On the other tasks,
integrated decoding outperformed reranking.
4.3 Speed comparison
We measured the speed of training a NPLM by NCE,
compared with MLE as implemented by the CSLM
toolkit (Schwenk, 2013). We used the first 200k
1390
10 20 30 40 50 60 70
0
1,
00
0
2,
00
0
3,
00
0
4,
00
0
Vocabulary size (?1000)
T
ra
in
in
g
tim
e
(s
)
CSLM
NCE k = 1000
NCE k = 100
NCE k = 10
Figure 2: Noise contrastive estimation (NCE) is much
faster, and much less dependent on vocabulary size, than
MLE as implemented by the CSLM toolkit (Schwenk,
2013).
lines (5.2M words) of the Xinhua portion of Giga-
word and timed one epoch of training, for various
values of k and |V |, on a dual hex-core 2.67 GHz
Xeon X5650 machine. For these experiments, we
used minibatches of 128 examples. The timings are
plotted in Figure 2. We see that NCE is considerably
faster than MLE; moreover, as expected, the MLE
training time is roughly linear in |V |, whereas the
NCE training time is basically constant.
5 Related Work
The problem of training with large vocabularies in
NPLMs has received much attention. One strategy
has been to restructure the network to be more hi-
erarchical (Morin and Bengio, 2005; Mnih and Hin-
ton, 2009) or to group words into classes (Le et al,
2011). Other strategies include restricting the vocab-
ulary of the NPLM to a shortlist and reverting to a
traditional n-gram LM for other words (Schwenk,
2004), and limiting the number of training examples
using resampling (Schwenk and Gauvain, 2005) or
selecting a subset of the training data (Schwenk et
al., 2012). Our approach can be efficiently applied
to large-scale tasks without limiting either the model
or the data.
NPLMs have previously been applied to MT, most
notably feed-forward NPLMs (Schwenk, 2007;
Schwenk, 2010) and RNN-LMs (Mikolov, 2012).
However, their use in MT has largely been limited
to reranking k-best lists for MT tasks with restricted
vocabularies. Niehues and Waibel (2012) integrate a
RBM-based language model directly into a decoder,
but they only train the RBM LM on a small amount
of data. To our knowledge, our approach is the first
to integrate a large-vocabulary NPLM directly into a
decoder for a large-scale MT task.
6 Conclusion
We introduced a new variant of NPLMs that com-
bines the network architecture of Bengio et al
(2003), rectified linear units (Nair and Hinton,
2010), and noise-contrastive estimation (Gutmann
and Hyva?rinen, 2010). This model is dramatically
faster to train than previous neural LMs, and can be
trained on a large corpus with a large vocabulary and
directly integrated into the decoder of a MT system.
Our experiments across four language pairs demon-
strated improvements of up to 1.1 Bleu. Code for
training and using our NPLMs is available for down-
load.2
Acknowledgements
We would like to thank the anonymous reviewers for
their very helpful comments. This research was sup-
ported in part by DOI IBC grant D12AP00225. This
work was done while the second author was visit-
ing USC/ISI supported by China Scholarship Coun-
cil. He was also supported by the Research Fund for
the Doctoral Program of Higher Education of China
(No. 20110091110003) and the National Fundamen-
tal Research Program of China (2010CB327903).
References
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research.
Yoshua Bengio. 2012. Practical recommendations for
gradient-based training of deep architectures. CoRR,
abs/1206.5533.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
2http://nlg.isi.edu/software/nplm
1391
eling. Technical Report TR-10-98, Harvard University
Center for Research in Computing Technology.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Gutmann and Aapo Hyva?rinen. 2010. Noise-
contrastive estimation: A new estimation principle for
unnormalized statistical models. In Proceedings of
AISTATS.
Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2012.
Language model rest costs and space-efficient storage.
In Proceedings of EMNLP-CoNLL, pages 1169?1178.
Richard Kronmal and Arthur Peterson. 1979. On the
alias method for generating random variables from
a discrete distribution. The American Statistician,
33(4):214?218.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In Proceedings
of ICASSP, pages 5524?5527.
Toma?s? Mikolov, Anoop Deoras, Stefan Kombrink, Luka?s?
Burget, and Jan ?Honza? C?ernocky?. 2011. Em-
pirical evaluation and combination of advanced lan-
guage modeling techniques. In Proceedings of IN-
TERSPEECH, pages 605?608.
Toma?s? Mikolov. 2012. Statistical Language Models
Based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Andriy Mnih and Geoffrey Hinton. 2009. A scalable
hierarchical distributed language model. In Advances
in Neural Information Processing Systems.
Andriy Mnih and Yee Whye Teh. 2012. A fast and sim-
ple algorithm for training neural probabilistic language
models. In Proceedings of ICML.
Frederic Morin and Yoshua Bengio. 2005. Hierarchical
probabilistic neural network language model. In Pro-
ceedings of AISTATS, pages 246?252.
Vinod Nair and Geoffrey E. Hinton. 2010. Rectified lin-
ear units improve restricted Boltzmann machines. In
Proceedings of ICML, pages 807?814.
Jan Niehues and Alex Waibel. 2012. Continuous
space language models using Restricted Boltzmann
Machines. In Proceedings of IWSLT.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Holger Schwenk and Jean-Luc Gauvain. 2005. Training
neural network language models on very large corpora.
In Proceedings of EMNLP.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space lan-
guage models on a GPU for statistical machine trans-
lation. In Proceedings of the NAACL-HLT 2012 Work-
shop: Will We Ever Really Replace the N-gramModel?
On the Future of Language Modeling for HLT, pages
11?19.
Holger Schwenk. 2004. Efficient training of large neural
networks for language modeling. In Proceedings of
IJCNN, pages 3059?3062.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Holger Schwenk. 2010. Continuous-space language
models for statistical machine translation. Prague Bul-
letin of Mathematical Linguistics, 93:137?146.
Holger Schwenk. 2013. CSLM - a modular open-source
continuous space language modeling toolkit. In Pro-
ceedings of Interspeech.
M.D. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang,
Q.V. Le, P. Nguyen, A. Senior, V. Vanhoucke, J. Dean,
and G.E. Hinton. 2013. On rectified linear units for
speech processing. In Proceedings of ICASSP.
1392
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 233?243,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Aligning context-based statistical models of language
with brain activity during reading
Leila Wehbe
1,2
, Ashish Vaswani
3
, Kevin Knight
3
and Tom Mitchell
1,2
1
Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA
2
Center for the Neural Basis of Computation, Carnegie Mellon University, Pittsburgh, PA
3
Information Sciences Institute, University of Southern California, Los Angeles, CA
lwehbe@cs.cmu.edu, vaswani@usc.edu, knight@isi.edu, tom.mitchell@cs.cmu.edu
Abstract
Many statistical models for natural language pro-
cessing exist, including context-based neural net-
works that (1) model the previously seen context
as a latent feature vector, (2) integrate successive
words into the context using some learned represen-
tation (embedding), and (3) compute output proba-
bilities for incoming words given the context. On
the other hand, brain imaging studies have sug-
gested that during reading, the brain (a) continu-
ously builds a context from the successive words
and every time it encounters a word it (b) fetches its
properties from memory and (c) integrates it with
the previous context with a degree of effort that is
inversely proportional to how probable the word is.
This hints to a parallelism between the neural net-
works and the brain in modeling context (1 and a),
representing the incoming words (2 and b) and in-
tegrating it (3 and c). We explore this parallelism to
better understand the brain processes and the neu-
ral networks representations. We study the align-
ment between the latent vectors used by neural net-
works and brain activity observed via Magnetoen-
cephalography (MEG) when subjects read a story.
For that purpose we apply the neural network to the
same text the subjects are reading, and explore the
ability of these three vector representations to pre-
dict the observed word-by-word brain activity.
Our novel results show that: before a new word i
is read, brain activity is well predicted by the neural
network latent representation of context and the pre-
dictability decreases as the brain integrates the word
and changes its own representation of context. Sec-
ondly, the neural network embedding of word i can
predict the MEG activity when word i is presented
to the subject, revealing that it is correlated with the
brain?s own representation of word i. Moreover, we
obtain that the activity is predicted in different re-
gions of the brain with varying delay. The delay is
consistent with the placement of each region on the
processing pathway that starts in the visual cortex
and moves to higher level regions. Finally, we show
that the output probability computed by the neural
networks agrees with the brain?s own assessment of
the probability of word i, as it can be used to predict
the brain activity after the word i?s properties have
been fetched from memory and the brain is in the
process of integrating it into the context.
1 Introduction
Natural language processing has recently seen a
surge in increasingly complex models that achieve
impressive goals. Models like deep neural net-
works and vector space models have become pop-
ular to solve diverse tasks like sentiment analy-
sis and machine translation. Because of the com-
plexity of these models, it is not always clear how
to assess and compare their performances as they
might be useful for one task and not the other.
It is also not easy to interpret their very high-
dimensional and mostly unsupervised representa-
tions. The brain is another computational system
that processes language. Since we can record brain
activity using neuroimaging, we propose a new di-
rection that promises to improve our understand-
ing of both how the brain is processing language
and of what the neural networks are modeling by
aligning the brain data with the neural networks
representations.
In this paper we study the representations of two
kinds of neural networks that are built to predict
the incoming word: recurrent and finite context
models. The first model is the Recurrent Neural
Network Language Model (Mikolov et al., 2011)
which uses the entire history of words to model
context. The second is the Neural Probabilistic
Language Model (NPLM) which uses limited con-
text constrained to the recent words (3 grams or 5
grams). We trained these models on a large Harry
Potter fan fiction corpus and we then used them to
predict the words of chapter 9 of Harry Potter and
the Sorcerer?s Stone (Rowling, 2012). In paral-
lel, we ran an MEG experiment in which 3 subject
read the words of chapter 9 one by one while their
brain activity was recorded. We then looked for
the alignment between the word-by-word vectors
produced by the neural networks and the word-by-
word neural activity recorded by MEG.
Our neural networks have 3 key constituents:
a hidden layer that summarizes the history of the
previous words ; an embeddings vector that sum-
marizes the (constant) properties of a given word
and finally the output probability of a word given
233
Reading comprehension is reflected in the subsequent acti-
vation of the left superior temporal cortex at 200?600 ms
(Halgren et al., 2002; Helenius et al., 1998; Pylkka?nen
et al., 2002, 2006; Pylkka?nen and Marantz, 2003; Simos
et al., 1997). This sustained activation differentiates
between words and nonwords (Salmelin et al., 1996; Wil-
son et al., 2005; Wydell et al., 2003). Apart from lexical-se-
mantic aspects it also seems to be sensitive to phonological
manipulation (Wydell et al., 2003).
As discussed above, in speech perception activation is
concentrated to a rather small area in the brain and we
have to rely on time information to dissociate between dif-
ferent processes. Here, the different processes are separable
both in timing and location. Because of that, one might
think that it is easier to characterize language-related pro-
cesses in the visual than auditory modality. However, here
the difficulties appear at another level. In reading, activa-
tion is detected bilaterally in the occipital cortex, along
the temporal lobes, in the parietal cortex and, in vocalized
reading, also in the frontal lobes, at various times with
respect to stimulus onset. Interindividual variability further
complicates the picture, resulting in practically excessive
amounts of temporal and spatial information. The areas
and time windows depicted in Fig. 5, with specific roles
in reading, form a limited subset of all active areas
observed during reading. In order to perform proper func-
tional localization one needs to vary the stimuli and tasks
systematically, in a parametric fashion. Let us now consid-
er how one may extract activation reflecting pre-lexical let-
ter-string analysis and lexical-semantic processing.
3.2. Pre-lexical analysis
In order to tease apart early pre-lexical processes in
reading, Tarkiainen and colleagues (Tarkiainen et al.,
1999) used words, syllables, and single letters, imbedded
in a noisy background, at four different noise levels
(Fig. 6). For control, the sequences also contained symbol
strings. One sequence was composed of plain noise stimuli.
The stimuli were thus varied along two major dimensions:
the amount of features to process increased with noise and
with the number of items, letters or symbols. On the other
hand, word-likeness was highest for clearly visible complete
words and lowest for symbols and noise.
At the level of the brain, as illustrated in Fig. 7, the data
showed a clear dissociation between two processes within
the first 200 ms: visual feature analysis occurred at about
100 ms after stimulus presentation, with the active areas
around the occipital midline, along the ventral stream. In
these areas, the signal increased with increasing noise and
with the number of items in the string, similarly for letters
and symbols. Only 50 ms later, at about 150 ms, the left
inferior occipitotemporal cortex showed letter-string spe-
cific activation. This signal increased with the visibility of
the letter strings. It was strongest for words, weaker for syl-
lables, and still weaker for single letters. Crucially, the acti-
vation was significantly stronger for letter than symbol
strings of equal length.
Bilateral occipitotemporal activation at about 200 ms
post-stimulus is consistently reported in MEG studies of
reading (Cornelissen et al., 2003b; Pammer et al., 2004; Sal-
melin et al., 1996, 2000b) but, interestingly, functional
specificity for letter-strings is found most systematically
in the left hemisphere. The MEG data on letter-string spe-
cific activation are in good agreement with intracranial
recordings, both with respect to timing and location and
the pre-lexical nature of the activation (Nobre et al., 1994).
3.3. Lexical-semantic analysis
To identify cortical dynamics of reading comprehension,
Helenius and colleagues (Helenius et al., 1998) employed a
Visual feature
analysis
Non-specific Words =Nonwords Nonwords
Letter-string
analysis
Time (ms)
0 400 800 0 400 800 0 400 800
Lexical-semantic
analysis
Fig. 5. Cortical dynamics of silent reading. Dots represent centres of active cortical patches collected from individual subjects. The curves display the
mean time course of activation in the depicted source areas. Visual feature analysis in the occipital cortex (!100 ms) is stimulus non-specific. The stimulus
content starts to matter by !150 ms when activation reflecting letter-string analysis is observed in the left occipitotemporal cortex. Subsequent activation
of the left superior temporal cortex at !200?600 ms reflects lexical-semantic analysis and, probably, also phonological analysis. Modified from Salmelin
et al. (2000a).
!L
ipp
inc
ott
W
illi
am
s&
W
ilk
ins
20
00
R. Salmelin / Clinical Neurophysiology xxx (2006) xxx?xxx 5
ARTICLE IN PRESS
Please cite this article as: Riitta Salmelin, Clinical neurophysiology of language: The MEG approach, Clinical Neurophysiology
(2006), doi:10.1016/j.clinph.2006.07.316
Figure 1: Cortical dynamics of silent reading. This figure
is adapted from (Salmelin, 2007). Dots represent projected
sources of activity in the visual cortex (left brain sketch) and
the temporal cortex (right brain sketch). The curves display
the m an time course of activation in the depicted ource ar-
eas for different conditions. The initial visual feature anal-
ysis in the visual cortex at ?100 ms is non-specific to lan-
guage. Comparing responses to letter strings and other vi-
sual stimuli rev als that letter string nalysis ccurs around
150 ms. Finally comparing the responses to words and non-
words (made-up words) reveals lexical-se antic analysis in
the temporal cortex at ?200-500ms.
the context. We set out to find the brain analogs
of these model constituents using an MEG decod-
ing task. We compare the different models and
h ir represen ations n term of how well they
can be used to decode the word being read from
MEG data. We obtain correspondences between
the models and the brain data that are consistent
with model of language processing in which
brain activity encodes story context, and where
each new word generates additional brain activity,
flowing generally from visual processing areas to
more high level areas, culminating in an updated
story cont xt, and reflecting a overall m gnitude
of neural effort influenced by the probability of
that new word given the previous context.
1.1 Neural processes involved in reading
Humans read with an average speed of 3 words
per second. Reading requires us to perceive in-
coming words and gradually integrate them into
a representation of the meaning. As words are
read, it takes 100ms for the visual input to reach
the visual cortex. 50ms later, the visual input is
processed as letter strings in a specialized region
of the left visual cortex (Salmelin, 2007). Be-
tween 200-500ms, the word?s semantic properties
are processed (see Fig. 1). Less is understood
about the cortical dynamics of word integration, as
multiple theories exist (Friederici, 2002; Hagoort,
2003).
Magnetoencephalography (MEG) is a brain-
imaging tool that is well suited for studying lan-
guage. MEG records the change in the magnetic
field on the surface of the head that is caused by
a large set of aligned neurons that are changing
their firing patterns in synchrony in response to
a stimulus. Because of the nature of the signal,
MEG recordings are directly related to neural ac-
tivity and have no latency. They are sampled at
a high frequency (typically 1kHz) that is ideal for
tracking the fast dynamics of language processing.
In this work, we are interested in the mecha-
nism of human text understanding as the meaning
of ncoming words is fetched from memory and
integrated with the context. Interestingly, this is
analogous to neural network models of language
that are used to predict the incoming word. The
mental representation of the previous context is
analogous to the latent layer of the neural network
which summarizes the relevant context before see-
ing the word. The representation of the meaning
of a word is analogous to the embedding that the
neural network learns in training and then uses.
Finally, one common hypotheses is that the brain
integrates the word with inversely proportional ef-
fort to how predictable the word is (Frank et al.,
2013). There is a well studied response known as
the N400 that is an increase of the activity in the
mporal cortex that has been recently shown to be
graded by the amount of surprisal of the incoming
word given the context (Frank et al., 2013). This is
analogous to the output probability of the incom-
ing word from the neural network.
Fig. 2 shows a hypothetical activity in an MEG
sensor as a subject reads a story in our experi-
ment, in which words are presented one at a time
for 500ms each. We conjecture that the activity in
time window a, i.e. before word i is understood, is
mostly related to the previous context before see-
ing word i. We also conjecture that the activity in
time window b is related to understanding word i
and integrating it into the context, leading to a new
representation of context in window c.
Using three types of features from neural net-
works (hidden layer context representation, output
probabilities and word embeddings) from three
different models of language (one recurrent model
and two finite context models), we therefore set to
predict the activity in the brain in different time
windows. We want to align the brain data with the
various model constituents to understand where
and when different types of processes are com-
puted in the brain, and simultaneously, we want to
234
Harry'
Harry'
had'
had'
never'
never'
embedding(iC1)' embedding(i)' embedding(i+1)'
context(iC1)'context(iC2)' context(i)'
out.'prob.(iC1)' out.'prob.(i)' out.'prob.(i+1)'
Studying'the'construc<on'of'meaning'
3'
word i+1 word i-1 word i 
0.5's'0.5's' 0.5's'
a b
c
Leila'Wehbe'
'?''Harry'''''''had''''''''never'?''
Figure 2: [Top] Sketch of the updates of a neural network
reading chapter 9 after it has been trained. Every word cor-
responds to a fixed embedding vector (magenta). A context
vector (blue) is computed before the word is seen given the
previous words. Given the context vector, the probability of
every word can be computed (symbolized by the histogram
in green). We only use the output probability of the actual
word (red circle). [Bottom] Hypothetical activity in an MEG
sensor when the subject reads the corresponding words. The
time periods approximated as a, b and c can be tested for in-
formation content relating to: the context of the story before
seeing word i (modeled by the context vector at i), the repre-
sentation of the properties of word i (the embedding of word
i) and the integration of word i into the context (the output
probability of word i). The periods drawn here are only a
conjecture on the timings of such cognitive events.
use the brain data to shed light on what the neural
network vectors are representing.
Related work
Decoding cognitive states from brain data is a
recent field that has been growing in popularity.
Most decoding studies that study language use
functional Magnetic Resonance Imaging (fMRI),
while some studies use MEG. MEG?s high tempo-
ral resolution makes it invaluable for looking at the
dynamics of language understanding. (Sudre et
al., 2012) decode from MEG the word a subject is
reading. The authors estimate from the MEG data
the semantic features of the word and use these as
an intermediate step to decode what the word is.
This is in principle similar to the classification ap-
proach we follow, as we will also use the feature
vectors as an intermediate step for word classifica-
tion. However the experimental paradigm in (Su-
dre et al., 2012) is to present to the subjects sin-
gle isolated words and to find how the brain rep-
resents their semantic features; whereas we have a
much more complex and ?naturalistic? experiment
in which the subjects read a non-artificial passage
of text, and we look at processes that exceed in-
dividual word processing: the construction of the
meanings of the successive words and the predic-
tion/integration of incoming words.
In (Frank et al., 2013), the amount of surprisal
that a word has given its context is used to pre-
dict the intensity of the N400 response described
previously. This is the closest study we could find
to our approach. This study was concerned with
analyzing the brain processes related only to sur-
prisal while we propose a more integral account
of the processes in the brain. The study also didn?t
address the major contribution we propose here,
which is to shed light on the inner constituents of
language models using brain imaging.
1.2 Recurrent and finite context neural
networks
Similar to standard language models, neural lan-
guage models also learn probability distributions
over words given their previous context. However,
unlike standard language models, words are rep-
resented as real-valued vectors in a high dimen-
sional space. These word vectors, referred to as
word embeddings, can be different for input and
output words, and are learned from training data.
Thus, although at training and test time, the in-
put and output to the neural language models are
one-hot representation of words, it is their em-
beddings that are used to compute word proba-
bility distributions. After training the embedding
vectors are fixed and it is these vectors that we
will use later on to predict MEG data. To predict
MEG data, we will also use the latent vector rep-
resentations of context that these neural networks
produce, as well as the probability of the current
word given the context. In this section, we will
describe how recurrent neural network language
models and feedforward neural probabilistic lan-
guage models compute word probabilities. In the
interest of space, we keep this description brief,
and for details, the reader is requested to refer to
the original papers describing these models.
235
w(t) s(t? 1)
y(t) c(t)
hidden
s(t)
output
P (w
t+1
| s(t))
D W
D
?
X
Figure 3: Recurrent neural network language model.
Recurrent Neural Network Language Model
Unlike standard feedforward neural language
models that only look at a fixed number of past
words, recurrent neural network language models
use all the previous history from position 1 to t?1
to predict the next word. This is typically achieved
by feedback connections, where the hidden layer
activations used for predicting the word in posi-
tion t ? 1 are fed back into the network to com-
pute the hidden layer activations for predicting the
next word. The hidden layer thus stores the history
of all previous words. We use the RNNLM archi-
tecture as described in Mikolov (2012), shown in
Figure 3. The input to the RNNLM at position t
are the one-hot representation of the current word,
w(t), and the activations from the hidden layer at
position t ? 1, s(t ? 1). The output of the hidden
layer at position t? 1 is
s(t) = ? (Dw(t) + Ws(t? 1)) ,
where D is the matrix of input word embeddings,
W is a matrix that transforms the activations from
the hidden layer in position t ? 1, and ? is a
sigmoid function, defined as ?(x) =
1
1+exp(?x)
,
that is applied elementwise. We need to compute
the probability of the next word w(t + 1) given
the hidden state s(t). For fast estimation of out-
put word probabilities, Mikolov (2012) divides the
computation into two stages: First, the probability
distribution over word classes is computed, after
which the probability distribution over the subset
of words belonging to the class are computed. The
class probability of a particular class with indexm
at position t is computed as:
P (c
m
(t) | s(t)) =
exp (s(t)Xv
m
)
?
C
c=1
(exp (s(t)Xv
c
))
,
where X is a matrix of class embeddings and v
m
is a one-hot vector representing the class with in-
dex m. The normalization constant is computed
u
1
u
2
input
words
input
embeddings
hidden
h
1
hidden
h
2
output
P (w | u)
D
?
M
C
1
C
2
D
Figure 4: Neural probabilistic language model
over all classes C. Each class specifies a subset
V
?
of words, potentially smaller than the entire vo-
cabulary V . The probability of an output word l at
position t + 1 given that its class is m is defined
as:
P (y
l
(t+ 1) | c
m
(t), s(t)) =
exp (s(t)D
?
v
l
)
?
V
?
k=1
(exp (s(t)D
?
v
k
))
,
where D
?
is a matrix of output word embeddings
and v
l
is a one hot vector representing the word
with index l. The probability of the word w(t+1)
given its class c
i
can now be computed as:
P (w(t+ 1) | s(t)) =P (w(t+ 1) | c
i
, s(t))
P (c
i
| s(t)).
Neural Probabilistic Language Model
We use the feedforward neural probabilistic lan-
guage model architecture of Vaswani et al. (2013),
as shown in Figure 4. Each context u comprises
a sequence of words u
j
(1 ? j ? n ? 1) repre-
sented as one-hot vectors, which are fed as input
to the neural network. At the output layer, the neu-
ral network computes the probability P (w | u) for
each word w, as follows.
The output of the first hidden layer h
1
is
h
1
= ?
?
?
n?1
?
j=1
C
j
Du
j
+ b
1
?
?
,
where D is a matrix of input word embeddings
which is shared across all positions, the C
j
are the
context matrices for each word in u, b
1
is a vec-
tor of biases with the same dimension as h
1
, and ?
is applied elementwise. Vaswani et al. (2013) use
rectified linear units (Nair and Hinton, 2010) for
236
the hidden layers h
1
and h
2
, which use the activa-
tion function ?(x) = max(0, x).
The output of the second layer h
2
is
h
2
= ? (Mh
1
+ b
2
) ,
where M is a weight matrix between h
1
and h
2
and b
2
is a vector of biases for h
2
. The probabil-
ity of the output word is computed at the output
softmax layer as:
P (w | u) =
exp
(
v
w
D
?
h
2
+ b
T
v
w
)
?
V
w
?
=1
exp (v
w
?
D
?
h
2
+ b
T
v
w
?
)
,
where D
?
is the matrix of output word embed-
dings, b is a vector of biases for every output word
and v
w
its the one hot representation of the word
w in the vocabulary.
2 Methods
We describe in this section our approach. In sum-
mary, we trained the neural network models on
a Harry Potter fan fiction database. We then ran
these models on chapter 9 of Harry Potter and the
Sorcerer?s Stone (Rowling, 2012) and computed
the context and embedding vectors and the output
probability for each word. In parallel, 3 subjects
read the same chapter in an MEG scanner. We
build models that predict the MEG data for each
word as a function of the different neural network
constituents. We then test these models with a
classification task that we explain below. We de-
tect correspondences between the neural network
components and the brain processes that under-
lie reading in the following fashion. If using a
neural network vector (e.g. the RNNLM embed-
ding vector) allows us to classify significantly bet-
ter than chance in a given region of the brain at
a given time (e.g. the visual cortex at time 100-
200ms), then we can hypothesize a relationship
between that neural network constituent and the
time/location of the analogous brain process.
2.1 Training the Neural Networks
We used the freely available training tools pro-
vided by Mikolov (2012)
1
and Vaswani et al.
(2013)
2
to train our RNNLM and NPLM models
used in our brain data classification experiments.
Our training data comprised around 67.5 million
1
http://rnnlm.org/
2
http://nlg.isi.edu/software/nplm
words for training and 100 thousand words for val-
idation from the Harry Potter fan fiction database
(http://harrypotterfanfiction.com). We restricted
the vocabulary to the top 100 thousand words
which covered all but 4 words from Chapter 9 of
Harry Potter and the Sorcerer?s Stone.
For the RNNLM, we trained models with differ-
ent hidden layers and learning rates and found the
RNNLM with 250 hidden units to perform best on
the validation set. We extracted our word embed-
dings from the input matrix D (Figure 3). We used
the default settings for all other hyper parameters.
We trained 3-gram and 5-gram NPLMs with
150 dimensional word embeddings and experi-
mented with different number of units for the first
hidden layer (h
1
in Figure 4), and different learn-
ing rates. For both the 3-gram and 5-gram mod-
els, we found 750 hidden units to perform the best
on the validation set and chose those models for
our final experiments. We used the output word
embeddings D
?
in our experiments. We visually
inspected the nearest neighbors in the 150 dimen-
sional word embedding space for some words and
didn?t find the neighbors from D
?
or D to be dis-
tinctly better than each other. We leave the com-
parison of input and output embeddings on brain
activity prediction for future work.
2.2 MEG paradigm
We recorded MEG data for three subjects (2 fe-
males and one male) while they read chapter 9
of Harry Potter and the Sorcerer?s Stone (Rowl-
ing, 2012). The participants were native English
speakers and right handed. They were chosen to
be familiar with the material: we made sure they
had read the Harry Potter books or seen the movies
series and were familiar with the characters and
the story. All the participants signed the consent
form, which was approved by the University of
Pittsburgh Institutional Review Board, and were
compensated for their participation.
The words of the story were presented in rapid
serial visual format (Buchweitz et al., 2009):
words were presented one by one at the center
of the screen for 0.5 seconds each. The text was
shown in 4 experimental blocks of ?11 minutes.
In total, 5176 words were presented. Chapter 9
was presented in its entirety without modifications
and each subject read the chapter only once.
One can think of an MEG machine as a large
helmet, with sensors located on the helmet that
237
record the magnetic activity. Our MEG recordings
were acquired on an Elekta Neuromag device at
the University of Pittsburgh Medical Center Pres-
byterian Hospital. This machine has 306 sensors
distributed into 102 locations on the surface of the
subject?s head. Each location groups 3 sensors or
two types: one magnometer that records the in-
tensity of the magnetic field and two planar gra-
diometers that record the change in the magnetic
field along two orthogonal planes
3
.
Our sampling frequency was 1kHz. For prepro-
cessing, we used Signal Space Separation method
(SSS, (Taulu et al., 2004)), followed by its tempo-
ral extension (tSSS, (Taulu and Simola, 2006)).
For each subject, the experiment data consists
therefore of a 306 dimensional time series of
length?45 minutes. We averaged the signal in ev-
ery sensor into 100ms non-overlapping time bins.
Since words were presented for 500ms each, we
therefore obtain for every word p = 306 ? 5 val-
ues corresponding to 306 vectors of 5 points.
2.3 Decoding experiment
To find which parts of brain activity are related to
the neural network constituents (e.g. the RNNLM
context vector), we run a prediction and classifica-
tion experiment in a 10-fold cross validated fash-
ion. At every fold, we train a linear model to pre-
dict MEG data as a function of one of the feature
sets, using 90% of the data. On the remaining 10%
of the data, we run a classification experiment.
MEG data is very noisy. Therefore, classify-
ing single word waveforms yields a low accuracy,
peaking at 60%, which might lead to false nega-
tives when looking for correspondences between
neural network features and brain data. To reveal
informative features, one can boost signal by ei-
ther having several repetitions of the stimuli in the
experiment and then averaging (Sudre et al., 2012)
or by combining the words into larger chunks (We-
hbe et al., 2014). We chose the latter because the
former sacrifices word and feature diversity.
At testing, we therefore repeat the following
300 times. Two sets of words are chosen ran-
domly from the test fold. To form the first set, 20
words are sampled without replacement from the
test sample (unseen by the classifier). To form the
second set, the k
th
word is chosen randomly from
all words in the test fold having the same length as
3
In this paper, we treat these three different sensors as
three different dimensions without further exploiting their
physical properties.
the k
th
word of the first set. Since every fold of
the data was used 9 times in the training phase and
once in the testing phase, and since we use a high
number of randomized comparisons, this averages
out biases in the accuracy estimation. Classifying
sets of 20 words improves the classification accu-
racy greatly while lowering its variance and makes
it dissociable from chance performance. We com-
pare only between words of equal length, to mini-
mize the effect of the low level visual features on
the classification accuracy.
After averaging out the results of multiple folds,
we end up with average accuracies that reveal how
related one of the models? constituents (e.g. the
RNNLM context vector) is to brain data.
2.3.1 Annotation of the stimulus text
We have 9 sets of annotations for the words of the
experiment. Each set j can be described as a ma-
trix F
j
in which each row i corresponds to the vec-
tor of annotations of word i. Our annotations cor-
respond to the 3 model constituents for each of the
3 models: the hidden layer representation before
word i, the output probability of word i and the
learned embeddings for word i.
2.3.2 Classification
In order to align the brain processes and the differ-
ent constituents of the different models, we use a
classification task. The task is to classify the word
a subject is reading out of two possible choices
from its MEG recording. The classifier uses one
type of feature in an intermediate classification
step. For example, the classifier learns to predict
the MEG activity for any setting of the RNNLM
hidden layer. Given an unseen MEG recording for
an unknown word i and two possible story words
i
?
and i
??
(one of which being the true word i), the
classifier predicts the MEG activity when reading
i
?
and i
??
from their hidden layer vectors. It then
assigns the label i
?
or i
??
to the word recording i
depending on which prediction is the closest to the
recording. The following are the detailed steps of
this complex classification task. However, for the
rest of the paper the most useful point to keep in
mind is that the main purpose of the classification
is to find a correspondence between the brain data
and a given feature set j.
1. Normalize the columns of M (zero mean,
standard deviation = 1). Pick feature set F
j
and normalize its columns to a minimum of 0
and a maximum of 1.
238
2. Divide the data into 10 folds, for each fold b:
(a) Isolate M
b
and F
b
j
as test data. The re-
mainder M
?b
and F
?b
j
will be used for
training
4
.
(b) Subtract the mean of the columns of
M
?b
from M
b
and M
?b
and the mean
of the columns of F
?b
j
from F
b
j
and F
?b
j
(c) Use ridge regression to solve
M
?b
= F
?b
j
? ?
t
j
by tuning the ? parameter to every one
of the p output dimensions indepen-
dently. ? is chosen via generalized cross
validation (Golub et al., 1979).
(d) Perform a binary classification. Sample
from the set of words in b a set c of 20
words. Then sample from b another set
of 20 words such that the k
th
word in c
and d have the same number of letters.
For every sample (c,d):
i. predict the MEG data for c and d as:
P
c
= F
c
j
? ?
b
j
and P
d
= F
d
j
? ?
b
j
ii. assign to M
c
the label c or d depend-
ing on which of P
c
or P
d
is closest
(Euclidean distance).
iii. assign to M
d
the label c or d de-
pending on which of P
c
or P
d
is
closest (Euclidean distance).
3. Compute the average accuracy.
2.3.3 Restricting the analysis spatially: a
searchlight equivalent
We adapt the searchlight method (Kriegeskorte et
al., 2006) to MEG. The searchlight is a discovery
procedure used in fMRI in which a cube is slid
over the brain and an analysis is performed in each
location separately. It allows to find regions in the
brain where a specific phenomenon is occurring.
In the MEG sensor space, for every one of the 102
sensor locations `, we assign a group of sensors g
`
.
For every location `, we identify the locations that
immediately surround it in any direction (Anterior,
Right Anterior, Right etc...) when looking at the
2D flat representation of the location of the sensors
in the MEG helmet (see Fig. 9 for an illustration of
the 2D helmet). g
`
therefore contains the 3 sensors
at location ` and at the neighboring locations. The
maximum number of sensors in a group is 3 ? 9.
4
The rows from M
?b
and F
?b
j
that correspond to the five
words before or after the test set are ignored in order to make
the test set independent.
The locations at the edge of the helmet have fewer
sensors because of the missing neighbor locations.
2.3.4 Restricting the analysis temporally
Instead of using the entire time course of the word,
we can use only one of the corresponding 100ms
time windows. Obtaining a high classification ac-
curacy using one of the time windows and feature
set j means that the analogous type of information
is encoded at that time.
2.3.5 Classification accuracy by time and
region
The above steps compute whole brain accuracy us-
ing all the time series. In order to perform a more
precise spatio-temporal analysis, one can use only
one time windowm and one location ` for the clas-
sification. This can answer the question of when
and where different information is represented by
brain activity. For every location, we will use only
the columns corresponding to the time pointm for
the sensors belonging to the group g
`
. Step (d) of
the classification procedure is changed as such:
(d) Perform a binary classification. Sample from
the set of words in b a set c of 20 words. Then
sample from b another set of 20 words such
that the k
th
word in c and d have the same
number of letters. For every sample (c,d), and
for every setting of {m, `}:
i. predict the MEG data for c and d as:
P
c
{m,`}
= F
c
j
? ?
b
j,{m,`}
and
P
d
{m,`}
= F
d
j
? ?
b
j,{m,`}
ii. assign to M
c
{m,`}
the label c or d depend-
ing on which of P
c
{m,`}
or P
d
{m,`}
is clos-
est (Euclidean distance).
iii. assign to M
d
{m,`}
the label c or d depend-
ing on which of P
c
{m,`}
or P
d
{m,`}
is clos-
est (Euclidean distance).
2.3.6 Statistical significance testing
We determine the distribution for chance perfor-
mance empirically. Because the successive word
samples in our MEG and feature matrices are not
independent and identically distributed, we break
the relationship between the MEG and feature ma-
trices by shifting the feature matrices by large de-
lays (e.g. 2000 to 2500 words) and we repeat
the classification using the delayed matrices. This
simulates chance performance more fairly than a
permutation test because it keeps the time struc-
ture of the matrices. It was used in (Wehbe et al.,
239
2014) and inspired by (Chwialkowski and Gret-
ton, 2014). For every {m, `} setting we can there-
fore compute a standardized z-value by subtract-
ing the mean of the shifted classifications and di-
viding by the standard deviation. We then com-
pute the p-value for the true classification accu-
racy being due to chance. Since the three p-values
for the three subjects for a given {m, `} are inde-
pendent, we combine them using Fisher?s method
for independent test statistics (Fisher, 1925). The
statistics we obtain for every {m, `} are depen-
dent because they comprise nearby time and space
windows. We control the false discovery rate us-
ing (Benjamini and Yekutieli, 2001) to adjust for
the testing at multiple locations and time windows.
This method doesn?t assume any kind of indepen-
dence or positive dependence.
3 Results
We present in Fig. 5 the accuracy using all the time
windows and sensors. In Fig. 6 we present the
classification accuracy when running the classifi-
cation at every time window exclusively. In Fig. 9
we present the accuracy when running the classifi-
cation using different time windows and groups of
sensors centered at every one of the 102 locations.
It is important to lay down some conventions
to understand the complex results in these plots.
To recap, we are trying to find parallels between
model constituents and brain processes. We use:
? a subset of the data (for example the time
window 0-100ms and all the sensors)
? one type of feature (for example the hidden
context layer from the NPLM 3g model)
and we obtain a classification accuracy A. If A
is low, there is probably no relationship between
the feature set and the subset of data. If A is high,
it hints to an association between the subset of data
and the mental process that is analogous to the fea-
ture set. For example, when using all the sensors
and time window 0-100ms, along with the NPLM
3g hidden layer, we obtain an accuracy of 0.70
(higher than chance with p < 10
?14
, see Fig. 6).
Since the NPLM 3g hidden layer summarizes the
context of the story before seeing word i, this sug-
gests that the brain is still processing the context
of the story before word i between 0-100ms.
Fig. 6 shows the accuracy for different types
of features when using all of the time points and
all the sensors to classify a word. We can see
NPLM 3g NPLM 5g RNNLM0.6
0.8
1
class
ificati
on ac
curac
y
 
 hidden layer output probability embeddings
hidden layer output probability embeddings0.6
0.8
1
class
ificati
on ac
curac
y
 
 NPLM 3g NPLM 5g RNNLM
Figure 5: Average accuracy using all time windows and
sensors, grouped by model (top) and type of feature (bot-
tom). All accuracies are significantly higher than chance
(p < 10
?8
).
0 200 4000.5
0.6
0.7
NPLM 3g
0 200 4000.5
0.6
0.7
NPLM 5g
0 200 4000.5
0.6
0.7
RNNLM
 
 hidden layeroutput probabilityembeddings
Figure 6: Average accuracy in different time windows
when using different types of features as input to the clas-
sifier, for different models. Accuracy is plotted in the center
of the respective time window. Points marked with a circle
are significantly higher than chance accuracy for the given
feature set and time window after correction.
similar classification accuracies for the three types
of models, with RNNLM ahead for the hidden
layer and embeddings and behind for the output
probability features. The hidden layer features
are the most powerful for classification. Between
the three types of features, the hidden layer fea-
tures are the best at capturing the information con-
tained in the brain data, suggesting that most of
the brain activity is encoding the previous context.
The embedding features are the second best. Fi-
nally the output probability have the smallest ac-
curacies. This makes sense considering that they
capture much less information than the other two
high dimensional descriptive vectors, as they do
not represent the complex properties of the words,
only a numerical assessment of their likelihood.
Fig. 6 shows the accuracy when using different
windows of time exclusively, for the 100ms time
240
windows starting at 0, 100 . . . 400ms after word
presentation. We can see that using the embed-
ding vector becomes increasingly more useful for
classification until 300-400ms, and then its perfor-
mance starts decreasing. This results aligns with
the following hypothesis: the word is being per-
ceived and understood by the brain gradually after
its presentation, and therefore the brain represen-
tation of the word becomes gradually similar to the
neural network representation of the word (i.e. the
embedding vector). The output probability feature
accuracy peaks at a later time than the embeddings
accuracy. Obtaining a higher than chance accu-
racy at time window m using the output probabil-
ity as input to the classifier suggests strongly that
the brain is integrating the word at time window
m, because it is responding differently for pre-
dictable and unpredictable words
5
. The integra-
tion step happens after the perception step, which
is probably why the output probability curves peak
later than the embeddings curves.
?500 0 500 10000.5
0.6
0.7
0.8
Hidden Layer
 
 NPLM 3GNPLM 5GRNNLM
Figure 7: Average accuracy in time for the different hidden
layers. The analysis is extended to the time windows before
and after the word is presented, the input feature is restricted
to be the hidden layer before the central word is seen. The
first vertical bar indicates the onset of the word, the second
one indicates the end of its presentation.
?1000 0 10000.6
0.8 Subject 1
?1000 0 10000.6
0.8 Subject 2
?1000 0 10000.6
0.8 Subject 3
 
 
hidden layer output probability embeddings
Figure 8: Accuracy in time when using the RNNLM fea-
tures for each of the three subjects.
To understand the time dynamics of the hidden
layer accuracy we need to see a larger time scale
than the word itself. The hidden layer captures the
5
the fact that we can classify accurately during windows
300-400ms indicates that the classifier is taking advantage of
the N400 response discussed in the introduction
context before word i is seen. Therefore it seems
reasonable that the hidden layer is not only related
to the activity when the word is on the screen, but
also related to the activity before the word is pre-
sented, which is the time when the brain is inte-
grating the previous words to build that context.
On the other hand, as the word i and subsequent
words are integrated, the context starts diverging
from the context of word i (computed before see-
ing word i). We therefore ran the same analysis
as before, but this time we also included the time
windows before and after word i in the analysis,
while maintaining the hidden layer vector to be the
context before word i is seen. We see the behav-
ior we predicted in the results: the context before
seeing word i becomes gradually more useful for
classification until word i is seen, and then it grad-
ually decreases until it is no longer useful since
the context has changed. We observe the RNNLM
hidden layer has a higher classification accuracy
than the finite context NPLMs. This might be due
to the fact that the RNNLM has a more complete
representation of context that captures more of the
properties of the previous words.
To show the consistency of the results, we plot
as illustration the three curves we obtain for each
subject for the RNNLM (Fig. 8). The patterns
seem very consistent indicating the phenomena we
described can be detected at the subject level.
We now move on to the spatial decomposition
of the analysis. When the visual input enters the
brain, it first reaches the visual cortex at the back
of the head, and then moves anteriorly towards the
left and right temporal cortices and eventually the
frontal cortex. As it flows through these areas, it
is processed to higher levels of interpretations. In
Fig. 9, we plot the accuracy for different regions
of the brain and different time windows for the
RNNLM features. To make the plots simpler we
multiplied by zero the accuracies which were not
significantly higher than chance. We expand a few
characteristic plots. We see that in the back of the
head the embedding features have an accuracy that
seems to peak very early on. As we move forward
in the brain towards the left and right temporal cor-
tices, we see the embeddings accuracy peaking at
a later time, reflecting the delay it takes for the in-
formation to reach this part of the brain. The out-
put probability start being useful for classification
after the embeddings, and specifically in the left
temporal cortex which is the cite where the N400
241
Back%
 
 
hidden layer
output probability
embeddings 0 5000.50.6
0.7
time (s)ac
cur
acy
Le(% Right%
 
 
hidden layer
output probability
embeddings 0 5000.5
0.60.7
time (s)a
ccu
rac
y
 
 
hidden layer
output probability
embeddings 0 5000.5
0.60.7
time (s)a
ccu
rac
y
 
 
hidden layer
output probability
embeddings
0 500
0.5
0.6
0.7
time (s)
a
c
c
u
ra
c
y
 
 
hidden layer
output probability
embeddings
0 500
0.5
0.6
0.7
time (s)
a
c
c
u
ra
c
y
 
 
hidden layer
output probability
embeddings
0 500
0.5
0.6
0.7
time (s)
a
c
c
u
ra
c
y
Figure 9: Average accuracy in time and space on the MEG helmet when using the RNNLM features. For each of the 102
locations the average accuracy for the group of sensors centered at that location is plotted versus time. The axes are defined
in the rightmost, empty plot. Three plots have been magnified to show the increasing delay in high accuracy when using the
embeddings feature, reflecting the delay in processing the incoming word as information travels through the brain. A sensor
map is provided in the lower right corner: visual cortex = cyan, temporal = red, frontal = dark green.
is reported in the literature. Finally, as we reach
the frontal cortex, we see that the embeddings fea-
tures have an even later accuracy peak.
4 Conclusion and contributions
Novel brain data exploration We present here
a novel and revealing approach to shed light on
the brain processes involved in reading. This is a
departure from the classical approach of control-
ling for a few variables in the text (e.g. showing
a sentence with an expected target word versus an
unexpected one). While we cannot make clear cut
causal claims because we did not control for our
variables, we are able to explore the data much
more and offer a much richer interpretation than
is possible with artificially constrained stimuli.
Comparing two models of language Adding
brain data into the equation allowed us to com-
pare the performance of the models and to identify
a slight advantage for the RNNLM in capturing
the text contents. Numerical comparison is how-
ever a secondary contribution of our approach. We
showed that it might be possible to use brain data
to understand, interpret and illustrate what exactly
is being encoded by the obscure vectors that neural
networks compute, by drawing parallels between
the models constituents and brain processes.
Anecdotally, in the process of running the ex-
periments, we noticed that the accuracy for the
hidden layer of the RNNLM was peaking in the
time window corresponding to word i?2, and that
it was decreasing during word i ? 1. Since this
was against our expectations, we went back and
looked at the code and found that it was indeed
returning a delayed value and corrected the fea-
tures. We therefore used the brain data in order to
correct a mis-specification in our neural network
model. This hints if not proves the potential of our
approach for assessing language models.
Future Work The work described here is our
first attempt along the promising endeavor of
matching complex computational models of lan-
guage with brain processes using brain recordings.
We plan to extend our efforts by (1) collecting data
from more subjects and using various types of text
and (2) make the brain data help us with training
better statistical language models by using it to de-
termine whether the models are expressive enough
or have reached a sufficient degree of convergence.
Acknowledgements
This research was supported in part by NICHD
grant 5R01HD07328-02. We thank Nicole Rafidi
for help with data acquisition.
242
References
Yoav Benjamini and Daniel Yekutieli. 2001. The con-
trol of the false discovery rate in multiple testing un-
der dependency. Annals of statistics, pages 1165?
1188.
Augusto Buchweitz, Robert A Mason, L?eda Tomitch,
and Marcel Adam Just. 2009. Brain activation
for reading and listening comprehension: An fMRI
study of modality effects and individual differences
in language comprehension. Psychology & neuro-
science, 2(2):111?123.
Kacper Chwialkowski and Arthur Gretton. 2014.
A kernel independence test for random processes.
arXiv preprint arXiv:1402.4501.
Ronald Aylmer Fisher. 1925. Statistical methods for
research workers. Genesis Publishing Pvt Ltd.
Stefan L Frank, Leun J Otten, Giulia Galli, and
Gabriella Vigliocco. 2013. Word surprisal predicts
N400 amplitude during reading. In Proceedings of
the 51st annual meeting of the Association for Com-
putational Linguistics, pages 878?883.
Angela D Friederici. 2002. Towards a neural basis
of auditory sentence processing. Trends in cognitive
sciences, 6(2):78?84.
Gene H Golub, Michael Heath, and Grace Wahba.
1979. Generalized cross-validation as a method for
choosing a good ridge parameter. Technometrics,
21(2):215?223.
Peter Hagoort. 2003. How the brain solves the binding
problem for language: a neurocomputational model
of syntactic processing. Neuroimage, 20:S18?S29.
Nikolaus Kriegeskorte, Rainer Goebel, and Peter Ban-
dettini. 2006. Information-based functional brain
mapping. Proceedings of the National Academy
of Sciences of the United States of America,
103(10):3863?3868.
Tomas Mikolov, Stefan Kombrink, Anoop Deoras,
Lukar Burget, and J Cernocky. 2011. RNNLM-
recurrent neural network language modeling toolkit.
In Proc. of the 2011 ASRU Workshop, pages 196?
201.
Tomas Mikolov. 2012. Statistical Language Models
Based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Vinod Nair and Geoffrey E. Hinton. 2010. Rectified
linear units improve restricted Boltzmann machines.
In Proceedings of ICML, pages 807?814.
Joanne K. Rowling. 2012. Harry Potter and the Sor-
cerer?s Stone. Harry Potter US. Pottermore Limited.
Riitta Salmelin. 2007. Clinical neurophysiology of
language: the MEG approach. Clinical Neurophysi-
ology, 118(2):237?254.
Gustavo Sudre, Dean Pomerleau, Mark Palatucci, Leila
Wehbe, Alona Fyshe, Riitta Salmelin, and Tom
Mitchell. 2012. Tracking neural coding of percep-
tual and semantic features of concrete nouns. Neu-
roImage, 62(1):451?463.
Samu Taulu and Juha Simola. 2006. Spatiotem-
poral signal space separation method for rejecting
nearby interference in MEG measurements. Physics
in medicine and biology, 51(7):1759.
Samu Taulu, Matti Kajola, and Juha Simola. 2004.
Suppression of interference and artifacts by the sig-
nal space separation method. Brain topography,
16(4):269?275.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with large-scale
neural language models improves translation.
Leila Wehbe, Brian Murphy, Partha Talukdar, Alona
Fyshe, Aaditya Ramdas, and Tom Mitchell. 2014.
Simultaneously uncovering the patterns of brain re-
gions involved in different story reading subpro-
cesses. in press.
243
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 557?565,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Beyond Parallel Data: Joint Word Alignment and Decipherment
Improves Machine Translation
Qing Dou , Ashish Vaswani, and Kevin Knight
Information Sciences Institute
Department of Computer Science
University of Southern California
{qdou,avaswani,knight}@isi.edu
Abstract
Inspired by previous work, where decipher-
ment is used to improve machine translation,
we propose a new idea to combine word align-
ment and decipherment into a single learning
process. We use EM to estimate the model pa-
rameters, not only to maximize the probabil-
ity of parallel corpus, but also the monolingual
corpus. We apply our approach to improve
Malagasy-English machine translation, where
only a small amount of parallel data is avail-
able. In our experiments, we observe gains of
0.9 to 2.1 Bleu over a strong baseline.
1 Introduction
State-of-the-art machine translation (MT) systems ap-
ply statistical techniques to learn translation rules au-
tomatically from parallel data. However, this reliance
on parallel data seriously limits the scope of MT ap-
plication in the real world, as for many languages and
domains, there is not enough parallel data to train a de-
cent quality MT system.
However, compared with parallel data, there are
much larger amounts of non parallel data. The abil-
ity to learn a translation lexicon or even build a ma-
chine translation system using monolingual data helps
address the problems of insufficient parallel data. Ravi
and Knight (2011) are among the first to learn a full
MT system using only non parallel data through deci-
pherment. However, the performance of such systems
is much lower compared with those trained with par-
allel data. In another work, Klementiev et al. (2012)
show that, given a phrase table, it is possible to esti-
mate parameters for a phrase-based MT system from
non parallel data.
Given that we often have some parallel data, it is
more practical to improve a translation system trained
on parallel data by using additional non parallel data.
Rapp (1995) shows that with a seed lexicon, it is possi-
ble to induce new word level translations from non par-
allel data. Motivated by the idea that a translation lexi-
con induced from non parallel data can be used to trans-
late out of vocabulary words (OOV), a variety of prior
research has tried to build a translation lexicon from
non parallel or comparable data (Fung and Yee, 1998;
Koehn and Knight, 2002; Haghighi et al., 2008; Garera
Figure 1: Combine word alignment and decipherment
into a single learning process.
et al., 2009; Bergsma and Van Durme, 2011; Daum?e
and Jagarlamudi, 2011; Irvine and Callison-Burch,
2013b; Irvine and Callison-Burch, 2013a; Irvine et al.,
2013).
Lately, there has been increasing interest in learn-
ing translation lexicons from non parallel data with de-
cipherment techniques (Ravi and Knight, 2011; Dou
and Knight, 2012; Nuhn et al., 2012; Dou and Knight,
2013). Decipherment views one language as a cipher
for another and learns a translation lexicon that pro-
duces fluent text in the target (plaintext) language. Pre-
vious work has shown that decipherment not only helps
find translations for OOVs (Dou and Knight, 2012), but
also improves translations of observed words (Dou and
Knight, 2013).
We find that previous work using monolingual or
comparable data to improve quality of machine transla-
tion separates two learning tasks: first, translation rules
are learned from parallel data, and then the information
learned from parallel data is used to bootstrap learning
with non parallel data. Inspired by approaches where
joint inference reduces the problems of error propaga-
tion and improves system performance, we combine
the two separate learning processes into a single one,
as shown in Figure 1. The contributions of this work
are:
557
? We propose a new objective function for word
alignment that combines the process of word
alignment and decipherment into a single learning
task.
? In experiments, we find that the joint process out-
performs the previous pipeline approach, and ob-
serve Bleu gains of 0.9 and 2.1 on two different
test sets.
? We release 15.3 million tokens of monolingual
Malagasy data from the web, as well as a small
Malagasy dependency tree bank containing 20k
tokens.
2 Joint Word Alignment and
Decipherment
2.1 A New Objective Function
In previous work that uses monolingual data to im-
prove machine translation, a seed translation lexicon
learned from parallel data is used to find new transla-
tions through either word vector based approaches or
decipherment. In return, selection of a seed lexicon
needs to be careful as using a poor quality seed lexi-
con could hurt the downstream process. Evidence from
a number of previous work shows that a joint inference
process leads to better performance in both tasks (Jiang
et al., 2008; Zhang and Clark, 2008).
In the presence of parallel and monolingual data, we
would like the alignment and decipherment models to
benefit from each other. Since the decipherment and
word alignment models contain word-to-word transla-
tion probabilities t( f | e), having them share these pa-
rameters during learning will allow us to pool infor-
mation from both data types. This leads us to de-
velop a new objective function that takes both learn-
ing processes into account. Given our parallel data,
(E
1
,F
1
), . . . , (E
m
,F
m
), . . . , (E
M
,F
M
), and monolingual
data F
1
mono
, . . . ,F
n
mono
, . . . ,F
N
mono
, we seek to maximize
the likelihood of both. Our new objective function is
defined as:
F
joint
=
M
?
m=1
log P(F
m
| E
m
) + ?
N
?
n=1
log P(F
n
mono
) (1)
The goal of training is to learn the parameters that
maximize this objective, that is
?
?
= arg max
?
F
joint
(2)
In the next two sections, we describe the word align-
ment and decipherment models, and present how they
are combined to perform joint optimization.
2.2 Word Alignment
Given a source sentence F = f
1
, . . . , f
j
, . . . , f
J
and a
target sentence E = e
1
, . . . , e
i
, . . . , e
I
, word alignment
models describe the generative process employed to
produce the French sentence from the English sentence
through alignments a = a
1
, . . . , a
j
, . . . , a
J
.
The IBM models 1-2 (Brown et al., 1993) and the
HMM word alignment model (Vogel et al., 1996) use
two sets of parameters, distortion probabilities and
translation probabilities, to define the joint probabil-
ity of a target sentence and alignment given a source
sentence.
P(F, a | E) =
J
?
j=1
d(a
j
| a
j?1
, j)t( f
j
| e
a
j
). (3)
These alignment models share the same translation
probabilities t( f
j
| e
a
j
), but differ in their treatment of
the distortion probabilities d(a
j
| a
j?1
, j). Brown et
al. (1993) introduce more advanced models for word
alignment, such as Model 3 and Model 4, which use
more parameters to describe the generative process. We
do not go into details of those models here and the
reader is referred to the paper describing them.
Under the Model 1-2 and HMM alignment models,
the probability of target sentence given source sentence
is:
P(F | E) =
?
a
J
?
j=1
d(a
j
| a
j?1
, j)t( f
j
| e
a
j
).
Let ? denote all the parameters of the word align-
ment model. Given a corpus of sentence pairs
(E
1
,F
1
), . . . , (E
m
,F
m
), . . . , (E
M
,F
M
), the standard ap-
proach for training is to learn the maximum likelihood
estimate of the parameters, that is,
?
?
= arg max
?
M
?
m=1
log P(F
m
| E
m
)
= arg max
?
log
?
?
?
?
?
?
?
?
a
P(F
m
, a | E
m
)
?
?
?
?
?
?
?
.
We typically use the EM algorithm (Dempster et al.,
1977), to carry out this optimization.
2.3 Decipherment
Given a corpus of N foreign text sequences (cipher-
text), F
1
mono
, . . . ,F
n
mono
, . . . ,F
N
mono
, decipherment finds
word-to-word translations that best describe the cipher-
text.
Knight et al. (2006) are the first to study several natu-
ral language decipherment problems with unsupervised
learning. Since then, there has been increasing interest
in improving decipherment techniques and its applica-
tion to machine translation (Ravi and Knight, 2011;
558
Dou and Knight, 2012; Nuhn et al., 2012; Dou and
Knight, 2013; Nuhn et al., 2013).
In order to speed up decipherment, Dou and Knight
(2012) suggest that a frequency list of bigrams might
contain enough information for decipherment. Accord-
ing to them, a monolingual ciphertext bigram F
mono
is
generated through the following generative story:
? Generate a sequence of two plaintext tokens e
1
e
2
with probability P(e
1
e
2
) given by a language
model built from large numbers of plaintext bi-
grams.
? Substitute e
1
with f
1
and e
2
with f
2
with probabil-
ity t( f
1
|e
1
) ? t( f
2
|e
2
).
The probability of any cipher bigram F is:
P(F
mono
) =
?
e
1
e
2
P(e
1
e
2
) ? t( f
1
|e
1
) ? t( f
2
|e
2
) (4)
And the probability of the corpus is:
P(corpus) =
N
?
n=1
P(F
n
mono
) (5)
Given a plaintext bigram language model, the goal is
to manipulate t( f |e) to maximize P(corpus). Theoret-
ically, one can directly apply EM to solve the problem
(Knight et al., 2006). However, EM has time complex-
ity O(N ?V
2
e
) and space complexity O(V
f
?V
e
), where V
f
,
V
e
are the sizes of ciphertext and plaintext vocabularies
respectively, and N is the number of cipher bigrams.
There have been previous attempts to make decipher-
ment faster. Ravi and Knight (2011) apply Bayesian
learning to reduce the space complexity. However,
Bayesian decipherment is still very slow with Gibbs
sampling (Geman and Geman, 1987). Dou and Knight
(2012) make sampling faster by introducing slice sam-
pling (Neal, 2000) to Bayesian decipherment. Besides
Bayesian decipherment, Nuhn et al. (2013) show that
beam search can be used to solve a very large 1:1 word
substitution cipher. In subsection 2.4.1, we describe
our approach that uses slice sampling to compute ex-
pected counts for decipherment in the EM algorithm.
2.4 Joint Optimization
We now describe our EM approach to learn the param-
eters that maximize F
joint
(equation 2), where the dis-
tortion probabilities, d(a
j
| a
j?1
, j) in the word align-
ment model are only learned from parallel data, and
the translation probabilities, t( f | e) are learned using
both parallel and non parallel data. The E step and M
step are illustrated in Figure 2.
Our algorithm starts with EM learning only on par-
allel data for a few iterations. When the joint inference
starts, we first compute expected counts from parallel
data and non parallel data using parameter values from
the last M step separately. Then, we add the expected
counts from both parallel data and non parallel data to-
gether with different weights for the two. Finally we
Figure 2: Joint Word Alignment and Decipherment
with EM
renormalize the translation table and distortion table to
update parameters in the new M step.
The E step for parallel part can be computed effi-
ciently using the forward-backward algorithm (Vogel et
al., 1996). However, as we pointed out in Section 2.3,
the E step for the non parallel part has a time com-
plexity of O(V
2
) with the forward-backward algorithm,
where V is the size of English vocabulary, and is usu-
ally very large. Previous work has tried to make de-
cipherment scalable (Ravi and Knight, 2011; Dou and
Knight, 2012; Nuhn et al., 2013; Ravi, 2013). How-
ever, all of them are designed for decipherment with ei-
ther Bayesian inference or beam search. In contrast, we
need an algorithm to make EM decipherment scalable.
To overcome this problem, we modify the slice sam-
pling (Neal, 2000) approach used by Dou and Knight
(2012) to compute expected counts from non parallel
data needed for the EM algorithm.
2.4.1 Draw Samples with Slice Sampling
To start the sampling process, we initialize the first
sample by performing approximate Viterbi decoding
using results from the last EM iteration. For each for-
eign dependency bigram f
1
, f
2
, we find the top 50 can-
didates for f
1
and f
2
ranked by t(e| f ), and find the En-
glish sequence e
1
, e
2
that maximizes t(e
1
| f
1
) ? t(e
2
| f
2
) ?
P(e
1
, e
2
).
Suppose the derivation probability for current sam-
ple e current is P(e current), we use slice sampling to
draw a new sample in two steps:
? Select a threshold T uniformly between 0 and
P(e current).
? Draw a new sample e new uniformly from a pool
559
of candidates: {e new|P(e new) > T }.
The first step is straightforward to implement. How-
ever, it is not trivial to implement the second step. We
adapt the idea from Dou and Knight (2012) for EM
learning.
Suppose our current sample e current contains En-
glish tokens e
i?1
, e
i
, and e
i+1
at position i ? 1, i, and
i+1 respectively, and f
i
be the foreign token at position
i. Using point-wise sampling, we draw a new sample
by changing token e
i
to a new token e
?
. Since the rest
of the sample remains the same, only the probability of
the trigram P(e
i?1
e
?
e
i+1
) (The probability is given by a
bigram language model.), and the channel model prob-
ability t( f
i
|e
?
) change. Therefore, the probability of a
sample is simplified as shown Equation 6.
P(e
i?1
e
?
e
i+1
) ? t( f
i
|e
?
) (6)
Remember that in slice sampling, a new sample is
drawn in two steps. For the first step, we choose a
threshold T uniformly between 0 and P(e
i?1
e
i
e
i+1
) ?
t( f
i
|e
i
). We divide the second step into two cases based
on the observation that two types of samples are more
likely to have a probability higher than T (Dou and
Knight, 2012): (1) those whose trigram probability is
high, and (2) those whose channel model probability is
high. To find candidates that have high trigram proba-
bility, Dou and Knight (2012) build a top k sorted lists
ranked by P(e
i?1
e
?
e
i+1
), which can be pre-computed
off-line. Then, they test if the last item e
k
in the list
satisfies the following inequality:
P(e
i?1
e
k
e
i+1
) ? c < T (7)
where c is a small constant and is set to prior in their
work. In contrast, we choose c empirically as we do
not have a prior in our model. When the inequality in
Equation 7 is satisfied, a sample is drawn in the fol-
lowing way: Let set A = {e
?
|e
i?1
e
?
e
i+1
? c > T } and
set B = {e
?
|t( f
i
|e
?
) > c}. Then we only need to sample
e
?
uniformly from A ? B until P(e
i?1
e
?
e
i+1
) ? t( f
i
|e
?
) is
greater than T . It is easy to prove that all other candi-
dates that are not in the sorted list and with t( f
i
|e
?
) ? c
have a upper bound probability: P(e
i?1
e
k
e
i+1
)?c. There-
fore, they do not need to be considered.
Second, when the last item e
k
in the list does not
meet the condition in Equation 7, we keep drawing
samples e
?
randomly until its probability is greater than
the threshold T .
As we mentioned before, the choice of the small con-
stant c is empirical. A large c reduces the number of
items in set B, but makes the condition P(e
i?1
e
k
e
i+1
) ?
c < T less likely to satisfy, which slows down the sam-
pling. On the contrary, a small c increases the number
of items in set B significantly as EM does not encour-
age a sparse distribution, which also slows down the
sampling. In our experiments, we set c to 0.001 based
on the speed of decipherment. Furthermore, to reduce
the size of set B, we rank all the candidate translations
Spanish English
Parallel 10.3k 9.9k
Non Parallel 80 million 400 million
Table 1: Size of parallel and non parallel data for word
alignment experiments (Measured in number of tokens)
of f
i
by t(e
?
| f
i
), then we add maximum the first 1000
candidates whose t( f
i
|e
?
) >= c into set B. For the rest
of the candidates, we set t( f
i
|e
?
) to a value smaller than
c (0.00001 in experiments).
2.4.2 Compute Expected Counts from Samples
With the ability to draw samples efficiently for deci-
pherment using EM, we now describe how to compute
expected counts from those samples. Let f
1
, f
2
be a
specific ciphertext bigram, N be the number of sam-
ples we want to use to compute expected counts, and
e
1
, e
2
be one of the N samples. The expected counts
for pairs ( f
1
, e
1
) and ( f
2
, e
2
) are computed as:
? ?
count( f
1
, f
2
)
N
where count( f
1
, f
2
) is count of the bigram, and ? is the
weight for non parallel data as shown in Equation 1.
Expected counts collected for f
1
, f
2
are accumulated
from each of its N samples. Finally, we collect ex-
pected counts using the same approach from each for-
eign bigram.
3 Word Alignment Experiments
In this section, we show that joint word alignment and
decipherment improves the quality of word alignment.
We choose to evaluate word alignment performance
for Spanish and English as manual gold alignments
are available. In experiments, our approach improves
alignment F score by as much as 8 points.
3.1 Experiment Setup
As shown in Table 1, we work with a small amount of
parallel, manually aligned Spanish-English data (Lam-
bert et al., 2005), and a much larger amount of mono-
lingual data.
The parallel data is extracted from Europarl, which
consists of articles from European parliament plenary
sessions. The monolingual data comes from English
and Spanish versions of Gigaword corpra containing
news articles from different news agencies.
We view Spanish as a cipher of English, and follow
the approach proposed by Dou and Knight (2013) to
extract dependency bigrams from parsed Spanish and
English monolingual data for decipherment. We only
keep bigrams where both tokens appear in the paral-
lel data. Then, we perform Spanish to English (En-
glish generating Spanish) word alignment and Span-
ish to English decipherment simultaneously with the
method discussed in section 2.
560
3.1.1 Results
We align all 500 sentences in the parallel corpus, and
tune the decipherment weight (?) for Model 1 and
HMM using the last 100 sentences. The best weights
are 0.1 for Model 1, and 0.005 for HMM. We start with
Model 1 with only parallel data for 5 iterations, and
switch to the joint process for another 5 iterations with
Model 1 and 5 more iterations of HMM. In the end, we
use the first 100 sentence pairs of the corpus for evalu-
ation.
Figure 3 compares the learning curve of alignment
F-score between EM without decipherment (baseline)
and our joint word alignment and decipherment. From
the learning curve, we find that at the 6th iteration, 2
iterations after we start the joint process, alignment F-
score is improved from 34 to 43, and this improvement
is held through the rest of the Model 1 iterations. The
alignment model switches to HMM from the 11th iter-
ation, and at the 12th iteration, we see a sudden jump
in F-score for both the baseline and the joint approach.
We see consistent improvement of F-score till the end
of HMM iterations.
4 Improving Low Density Languages
Machine Translation with Joint Word
Alignment and Decipherment
In the previous section, we show that the joint word
alignment and decipherment process improves quality
of word alignment significantly for Spanish and En-
glish. In this section, we test our approach in a more
challenging setting: improving the quality of machine
translation in a real low density language setting.
In this task, our goal is to build a system to trans-
late Malagasy news into English. We have a small
amount of parallel data, and larger amounts of mono-
lingual data collected from online websites. We build a
dependency parser for Malagasy to parse the monolin-
gual data to perform dependency based decipherment
(Dou and Knight, 2013). In the end, we perform joint
word alignment and decipherment, and show that the
joint learning process improves Bleu scores by up to
2.1 points over a phrase-based MT baseline.
4.1 The Malagasy Language
Malagasy is the official language of Madagascar. It has
around 18 million native speakers. Although Mada-
gascar is an African country, Malagasy belongs to the
Malayo-Polynesian branch of the Austronesian lan-
guage family. Malagasy and English have very dif-
ferent word orders. First of all, in contrast to En-
glish, which has a subject-verb-object (SVO) word or-
der, Malagasy has a verb-object-subject (VOS) word
order. Besides that, Malagasy is a typical head ini-
tial language: Determiners precede nouns, while other
modifiers and relative clauses follow nouns (e.g. ny
?the? boky ?book? mena ?red?). The significant dif-
ferences in word order pose great challenges for both
Source Malagasy English
Parallel
Global Voices 2.0 million 1.8 million
Web News 2.2k 2.1k
Non Parallel
Gigaword N/A 2.4 billion
allAfrica N/A 396 million
Local News 15.3 million N/A
Table 2: Size of Malagasy and English data used in our
experiments (Measured in number of tokens)
machine translation and decipherment.
4.2 Data
Table 2 shows the data available to us in our experi-
ments. The majority of parallel text comes from Global
Voices
1
(GV). The website contains international news
translated into different foreign languages. Besides
that, we also have a very small amount of parallel text
containing local web news, with English translations
provided by native speakers at the University of Texas,
Austin. The Malagasy side of this small parallel corpus
also has syntactical annotation, which is used to train a
very basic Malagasy part of speech tagger and depen-
dency parser.
We also have much larger amounts of non paral-
lel data for both languages. For Malagasy, we spent
two months manually collecting 15.3 million tokens of
news text from local news websites in Madagascar.
2
We have released this data for future research use. For
English, we have 2.4 billion tokens from the Gigaword
corpus. Since the Malagasy monolingual data is col-
lected from local websites, it is reasonable to argue that
those data contain significant amount of information re-
lated to Africa. Therefore, we also collect 396 million
tokens of African news in English from allAfrica.com.
4.3 Building A Dependency Parser for Malagasy
Since Malagasy and English have very different word
orders, we decide to apply dependency based decipher-
ment for the two languages as suggested by Dou and
Knight (2013). To extract dependency relations, we
need to parse monolingual data in Malagasy and En-
glish. For English, there are already many good parsers
available. In our experiments, we use Turbo parser
(Martins et al., 2013) trained on the English Penn Tree-
bank (Marcus et al., 1993) to parse all our English
monolingual data. However, there is no existing good
parser for Malagasy.
The quality of a dependency parser depends on the
amount of training data available. State-of-the-art En-
glish parsers are built from Penn Treebank, which con-
tains over 1 million tokens of annotated syntactical
1
globalvoicesonline.org
2
aoraha.com, gazetiko.com, inovaovao.com,
expressmada.com, lakroa.com
561
Figure 3: Learning curve showing our joint word alignment and decipherment approach improves word alignment
quality over the traditional EM without decipherment (Model 1: Iteration 1 to 10, HMM: Iteration 11 to 15)
trees. In contrast, the available data for training a Mala-
gasy parser is rather limited, with only 168 sentences,
and 2.8k tokens, as shown in Table 2. At the very be-
ginning, we use the last 120 sentences as training data
to train a part of speech (POS) tagger using a toolkit
provided by Garrette et al. (2013) and a dependency
parser with the Turbo parser. We test the performance
of the parser on the first 48 sentences and obtain 72.4%
accuracy.
One obvious way to improve tagging and parsing ac-
curacy is to get more annotated data. We find more data
with only part of speech tags containing 465 sentences
and 10k tokens released by (Garrette et al., 2013), and
add this data as extra training data for POS tagger.
Also, we download an online dictionary that contains
POS tags for over 60k Malagasy word types from mala-
gasyword.org. The dictionary is very helpful for tag-
ging words never seen in the training data.
It is natural to think that creation of annotated data
for training a POS tagger and a parser requires large
amounts of efforts from annotators who understand the
language well. However, we find that through the help
of parallel data and dictionaries, we are able to create
more annotated data by ourselves to improve tagging
and parsing accuracy. This idea is inspired by previ-
ous work that tries to learn a semi-supervised parser
by projecting dependency relations from one language
(with good dependency parsers) to another (Yarowsky
and Ngai, 2001; Ganchev et al., 2009). However, we
find those automatic approaches do not work well for
Malagasy.
To further expand our Malagasy training data, we
first use a POS tagger and parser with poor perfor-
mance to parse 788 sentences (20k tokens) on the
Malagasy side of the parallel corpus from Global
Voices. Then, we correct both the dependency links
and POS tags based on information from dictionaries
3
and the English translation of the parsed sentence. We
spent 3 months to manually project English dependen-
cies to Malagasy and eventually improve test set pars-
ing accuracy from 72.4% to 80.0%. We also make this
data available for future research use.
4.4 Machine Translation Experiments
In this section, we present the data used for our MT
experiments, and compare three different systems to
justify our joint word alignment and decipherment ap-
proach.
4.4.1 Baseline Machine Translation System
We build a state-of-the-art phrase-based MT system,
PBMT, using Moses (Koehn et al., 2007). PBMT has 3
models: a translation model, a distortion model, and
a language model. We train the other models using
half of the Global Voices parallel data (the rest is re-
served for development and testing), and build a 5-
gram language model using 834 million tokens from
AFP section of English Gigaword, 396 million tokens
from allAfrica, and the English part of the parallel cor-
pus for training. For alignment, we run 10 iterations
of Model 1, and 5 iterations of HMM. We did not run
Model 3 and Model 4 as we see no improvements in
Bleu scores from running those models. We do word
3
an online dictionary from malagasyword.org, as well as
a lexicon learned from the parallel data
562
alignment in two directions and use grow-diag-final-
and heuristic to obtain final alignment. During decod-
ing, we use 8 standard features in Moses to score a can-
didate translation: direct and inverse translation proba-
bilities, direct and inverse lexical weighting, a language
model score, a distortion score, phrase penalty, and
word penalty. The weights for the features are learned
on the tuning data using minimum error rate training
(MERT) (Och, 2003).
To compare with previous decipherment approach to
improve machine translation, we build a second base-
line system. We follow the work by Dou and Knight
(2013) to decipher Malagasy into English, and build a
translation lexicon T
decipher
from decipherment. To im-
prove machine translation, we simply use T
decipher
as
an additional parallel corpus. First, we filter T
decipher
by keeping only translation pairs ( f , e), where f is ob-
served in the Spanish part and e is observed in the En-
glish part of the parallel corpus. Then we append all
the Spanish and English words in the filtered T
decipher
to the end of Spanish part and English part of the paral-
lel corpus respectively. The training and tuning process
is the same as the baseline machine translation system
PBMT. We call this system Decipher-Pipeline.
4.4.2 Joint Word Alignment and Decipherment
for Machine Translation
When deciphering Malagasy to English, we extract
Malagasy dependency bigrams using all available
Malagasy monolingual data plus the Malagasy part of
the Global Voices parallel data, and extract English
dependency bigrams using 834 million tokens from
English Gigaword, and 396 million tokens from al-
lAfrica news to build an English dependency language
model. In the other direction, we extract English de-
pendency bigrams from English part of the entire paral-
lel corpus plus 9.7 million tokens from allAfrica news
4
, and use 17.3 million tokens Malagasy monolingual
data (15.3 million from the web and 2.0 million from
Global Voices) to build a Malagasy dependency lan-
guage model. We require that all dependency bigrams
only contain words observed in the parallel data used
to train the baseline MT system.
During learning, we run Model 1 without decipher-
ment for 5 iterations. Then we perform joint word
alignment and decipherment for another 5 iterations
with Model 1 and 5 iterations with HMM. We tune
decipherment weights (?) for Model 1 and HMM us-
ing grid search against Bleu score on a development
set. In the end, we only extract rules from one di-
rection P(English|Malagasy), where the decipherment
weights for Model 1 and HMM are 0.5 and 0.005 re-
spectively. We chose this because we did not find any
benefits to tune the weights on each direction, and then
use grow-diag-final-end heuristic to form final align-
ments. We call this system Decipher-Joint.
4
We do not find further Bleu gains by using more English
monolingual data.
Parallel
Malagasy English
Train (GV) 0.9 million 0.8 million
Tune (GV) 22.2k 20.2k
Test (GV) 23k 21k
Test (Web) 2.2k 2.1k
Non Parallel
Malagasy English
Gigaword N/A 834 million
Web 15.3 million 396 million
Table 3: Size of training, tuning, and testing data in
number of tokens (GV: Global Voices)
4.5 Results
We tune each system three times with MERT and
choose the best weights based on Bleu scores on tuning
set.
Table 4 shows that while using a translation lexicon
learnt from decipherment does not improve the quality
of machine translation significantly, the joint approach
improves Bleu score by 0.9 and 2.1 on Global Voices
test set and web news test set respectively. The results
show that the parsing quality correlates with gains in
Bleu scores. Scores in the brackets in the last row of
the table are achieved using a dependency parser with
72.4% attachment accuracy, while scores outside the
brackets are obtained using a dependency parser with
80.0% attachment accuracy.
We analyze the results and find the gain mainly
comes from two parts. First, adding expected counts
from non parallel data makes the distribution of trans-
lation probabilities sparser in word alignment models.
The probabilities of translation pairs favored by both
parallel data and decipherment becomes higher. This
gain is consistent with previous observation where a
sparse prior is applied to EM to help improve word
alignment and machine translation (Vaswani et al.,
2012). Second, expected counts from decipherment
also help discover new translation pairs in the paral-
lel data for low frequency words, where those words
are either aligned to NULL or wrong translations in the
baseline.
5 Conclusion and Future Work
We propose a new objective function for word align-
ment to combine the process of word alignment and
decipherment into a single task. In, experiments, we
find that the joint process performs better than previous
pipeline approach, and observe Bleu gains of 0.9 and
2.1 point on Global Voices and local web news test sets,
respectively. Finally, our research leads to the release
of 15.3 million tokens of monolingual Malagasy data
from the web as well as a small Malagasy dependency
tree bank containing 20k tokens.
Given the positive results we obtain by using the
joint approach to improve word alignment, we are in-
563
Decipherment System Tune (GV) Test (GV) Test (Web)
None PBMT (Baseline) 18.5 17.1 7.7
Separate Decipher-Pipeline 18.5 17.4 7.7
Joint Decipher-Joint 18.9 (18.7) 18.0 (17.7) 9.8 (8.5)
Table 4: Decipher-Pipeline does not show significant improvement over the baseline system. In contrast, Decipher-
Joint using joint word alignment and decipherment approach achieves a Bleu gain of 0.9 and 2.1 on the Global
Voices test set and the web news test set, respectively. The results in brackets are obtained using a parser trained
with only 120 sentences. (GV: Global Voices)
spired to apply this approach to help find translations
for out of vocabulary words, and to explore other pos-
sible ways to improve machine translation with deci-
pherment.
Acknowledgments
This work was supported by NSF Grant 0904684 and
ARO grant W911NF-10-1-0533. The authors would
like to thank David Chiang, Malte Nuhn, Victoria Fos-
sum, Ashish Vaswani, Ulf Hermjakob, Yang Gao, and
Hui Zhang (in no particular order) for their comments
and suggestions.
References
Shane Bergsma and Benjamin Van Durme. 2011.
Learning bilingual lexicons using the visual similar-
ity of labeled web images. In Proceedings of the
Twenty-Second International Joint Conference on
Artificial Intelligence - Volume Three. AAAI Press.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The math-
ematics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19:263?311.
Hal Daum?e, III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining
unseen words. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies. Associa-
tion for Computational Linguistics.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Computational Linguistics, 39(4):1?
38.
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning. Asso-
ciation for Computational Linguistics.
Qing Dou and Kevin Knight. 2013. Dependency-
based decipherment for resource-limited machine
translation. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 1. Association for Com-
putational Linguistics.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 1 - Volume 1. Association for Computational
Linguistics.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon in-
duction from monolingual corpora via dependency
contexts and part-of-speech equivalences. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning. Association for
Computational Linguistics.
Dan Garrette, Jason Mielens, and Jason Baldridge.
2013. Real-world semi-supervised learning of pos-
taggers for low-resource languages. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers). Association for Computational Linguistics.
Stuart Geman and Donald Geman. 1987. Stochas-
tic relaxation, Gibbs distributions, and the Bayesian
restoration of images. In Readings in computer vi-
sion: issues, problems, principles, and paradigms.
Morgan Kaufmann Publishers Inc.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT. Association for Computational Linguis-
tics.
Ann Irvine and Chris Callison-Burch. 2013a. Com-
bining bilingual and comparable corpora for low re-
source machine translation. In Proceedings of the
Eighth Workshop on Statistical Machine Transla-
tion. Association for Computational Linguistics, Au-
gust.
Ann Irvine and Chris Callison-Burch. 2013b. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In Proceedings of the 2013
564
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics.
Ann Irvine, Chris Quirk, and Hal Daume III. 2013.
Monolingual marginal matching for translation
model adaptation. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L?u.
2008. A cascaded linear model for joint Chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL-08: HLT. Association for Com-
putational Linguistics.
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch, and David Yarowsky. 2012. Toward statisti-
cal machine translation without parallel corpora. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics. Association for Computational Linguistics.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji
Yamada. 2006. Unsupervised analysis for decipher-
ment problems. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions. Association
for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 Workshop on Unsuper-
vised Lexical Acquisition. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions.
Association for Computational Linguistics.
Patrik Lambert, Adri?a De Gispert, Rafael Banchs, and
Jos?e B. Mari?no. 2005. Guidelines for word align-
ment evaluation and manual alignment. Language
Resources and Evaluation, 39(4):267?285.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2).
Andre Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the Turbo: Fast third-order non-
projective Turbo parsers. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers). Asso-
ciation for Computational Linguistics.
Radford Neal. 2000. Slice sampling. Annals of Statis-
tics, 31.
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining lan-
guage models and context vectors. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics: Long Papers - Volume
1. Association for Computational Linguistics.
Malte Nuhn, Julian Schamper, and Hermann Ney.
2013. Beam search for solving substitution ciphers.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics. Associ-
ation for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of the 33rd an-
nual meeting of Association for Computational Lin-
guistics. Association for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies. Associa-
tion for Computational Linguistics.
Sujith Ravi. 2013. Scalable decipherment for machine
translation via hash sampling. In Proceedings of the
51th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Ashish Vaswani, Liang Huang, and David Chiang.
2012. Smaller alignment models for better trans-
lations: Unsupervised word alignment with the l0-
norm. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics: Long
Papers - Volume 1. Association for Computational
Linguistics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proceedings of the 16th Conference
on Computational Linguistics - Volume 2. Associa-
tion for Computational Linguistics.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
on Language Technologies. Association for Compu-
tational Linguistics.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of ACL-08: HLT, Columbus,
Ohio. Association for Computational Linguistics.
565
Proceedings of NAACL-HLT 2013, pages 1120?1130,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Learning Whom to Trust with MACE
Dirk Hovy1 Taylor Berg-Kirkpatrick2 Ashish Vaswani1 Eduard Hovy3
(1) Information Sciences Institute, University of Southern California, Marina del Rey
(2) Computer Science Division, University of California at Berkeley
(3) Language Technology Institute, Carnegie Mellon University, Pittsburgh
{dirkh,avaswani}@isi.edu, tberg@cs.berkeley.edu, hovy@cmu.edu
Abstract
Non-expert annotation services like Amazon?s
Mechanical Turk (AMT) are cheap and fast
ways to evaluate systems and provide categor-
ical annotations for training data. Unfortu-
nately, some annotators choose bad labels in
order to maximize their pay. Manual iden-
tification is tedious, so we experiment with
an item-response model. It learns in an un-
supervised fashion to a) identify which an-
notators are trustworthy and b) predict the
correct underlying labels. We match perfor-
mance of more complex state-of-the-art sys-
tems and perform well even under adversarial
conditions. We show considerable improve-
ments over standard baselines, both for pre-
dicted label accuracy and trustworthiness es-
timates. The latter can be further improved
by introducing a prior on model parameters
and using Variational Bayes inference. Ad-
ditionally, we can achieve even higher accu-
racy by focusing on the instances our model is
most confident in (trading in some recall), and
by incorporating annotated control instances.
Our system, MACE (Multi-Annotator Compe-
tence Estimation), is available for download1.
1 Introduction
Amazon?s MechanicalTurk (AMT) is frequently
used to evaluate experiments and annotate data in
NLP (Callison-Burch et al, 2010; Callison-Burch
and Dredze, 2010; Jha et al, 2010; Zaidan and
Callison-Burch, 2011). However, some turkers try to
maximize their pay by supplying quick answers that
have nothing to do with the correct label. We refer to
1Available under http://www.isi.edu/
publications/licensed-sw/mace/index.html
this type of annotator as a spammer. In order to mit-
igate the effect of spammers, researchers typically
collect multiple annotations of the same instance so
that they can, later, use de-noising methods to infer
the best label. The simplest approach is majority
voting, which weights all answers equally. Unfor-
tunately, it is easy for majority voting to go wrong.
A common and simple spammer strategy for cate-
gorical labeling tasks is to always choose the same
(often the first) label. When multiple spammers
follow this strategy, the majority can be incorrect.
While this specific scenario might seem simple to
correct for (remove annotators that always produce
the same label), the situation grows more tricky
when spammers do not annotate consistently, but in-
stead choose labels at random. A more sophisticated
approach than simple majority voting is required.
If we knew whom to trust, and when, we could
reconstruct the correct labels. Yet, the only way
to be sure we know whom to trust is if we knew
the correct labels ahead of time. To address this
circular problem, we build a generative model of the
annotation process that treats the correct labels as
latent variables. We then use unsupervised learning
to estimate parameters directly from redundant
annotations. This is a common approach in the
class of unsupervised models called item-response
models (Dawid and Skene, 1979; Whitehill et al,
2009; Carpenter, 2008; Raykar and Yu, 2012).
While such models have been implemented in
other fields (e.g., vision), we are not aware of their
availability for NLP tasks (see also Section 6).
Our model includes a binary latent variable that
explicitly encodes if and when each annotator is
spamming, as well as parameters that model the
annotator?s specific spamming ?strategy?. Impor-
1120
tantly, the model assumes that labels produced by
an annotator when spamming are independent of
the true label (though, a spammer can still produce
the correct label by chance).
In experiments, our model effectively differenti-
ates dutiful annotators from spammers (Section 4),
and is able to reconstruct the correct label with high
accuracy (Section 5), even under extremely adver-
sarial conditions (Section 5.2). It does not require
any annotated instances, but is capable of including
varying levels of supervision via token constraints
(Section 5.2). We consistently outperform major-
ity voting, and achieve performance equal to that of
more complex state-of-the-art models. Additionally,
we find that thresholding based on the posterior la-
bel entropy can be used to trade off coverage for ac-
curacy in label reconstruction, giving considerable
gains (Section 5.1). In tasks where correct answers
are more important than answering every instance,
e.g., when constructing a new annotated corpus, this
feature is extremely valuable. Our contributions are:
? We demonstrate the effectiveness of our model
on real world AMT datasets, matching the ac-
curacy of more complex state-of-the-art sys-
tems
? We show how posterior entropy can be used to
trade some coverage for considerable gains in
accuracy
? We study how various factors affect perfor-
mance, including number of annotators, anno-
tator strategy, and available supervision
? We provide MACE (Multi-Annotator Compe-
tence Estimation), a Java-based implementa-
tion of a simple and scalable unsupervised
model that identifies malicious annotators and
predicts labels with high accuracy
2 Model
We keep our model as simple as possible so that it
can be effectively trained from data where annotator
quality is unknown. If the model has too many
parameters, unsupervised learning can easily pick
up on and exploit coincidental correlations in the
data. Thus, we make a modeling assumption that
keeps our parameterization simple. We assume that
an annotator always produces the correct label when
N
T
i
M
A
ij
S
ij
T
A
2
C
2
A
3
C
3
A
1
C
1
Figure 1: Graphical model: Annotator j produces
label Aij on instance i. Label choice depends on
instance?s true label Ti, and whether j is spam-
ming on i, modeled by binary variable Sij . N =
|instances|, M = |annotators|.
for i = 1 . . . N :
Ti ? Uniform
for j = 1 . . .M :
Sij ? Bernoulli(1? ?j)
if Sij = 0 :
Aij = Ti
else :
Aij ? Multinomial(?j)
Figure 2: Generative process: see text for descrip-
tion.
he tries to. While this assumption does not reflect
the reality of AMT, it allows us to focus the model?s
power where it?s important: explaining away labels
that are not correlated with the correct label.
Our model generates the observed annotations as
follows: First, for each instance i, we sample the
true label Ti from a uniform prior. Then, for each
annotator j we draw a binary variable Sij from a
Bernoulli distribution with parameter 1 ? ?j . Sij
represents whether or not annotator j is spamming
on instance i. We assume that when an annotator
is not spamming on an instance, i.e. Sij = 0, he
just copies the true label to produce annotation Aij .
If Sij = 1, we say that the annotator is spamming
on the current instance, and Aij is sampled from
a multinomial with parameter vector ?j . Note that
in this case the annotation Aij does not depend on
the true label Ti. The annotations Aij are observed,
1121
while the true labels Ti and the spamming indicators
Sij are unobserved. The graphical model is shown
in Figure 1 and the generative process is described
in Figure 2.
The model parameters are ?j and ?j . ?j specifies
the probability of trustworthiness for annotator j
(i.e. the probability that he is not spamming on
any given instance). The learned value of ?j will
prove useful later when we try to identify reliable
annotators (see Section 4). The vector ?j determines
how annotator j behaves when he is spamming. An
annotator can produce the correct answer even while
spamming, but this can happen only by chance since
the annotator must use the same multinomial param-
eters ?j across all instances. This means that we only
learn annotator biases that are not correlated with
the correct label, e.g., the strategy of the spammer
who always chooses a certain label. This contrasts
with previous work where additional parameters are
used to model the biases that even dutiful annotators
exhibit. Note that an annotator can also choose not
to answer, which we can naturally accommodate be-
cause the model is generative. We enhance our gen-
erative model by adding Beta and Dirichlet priors on
?j and ?j respectively which allows us to incorporate
prior beliefs about our annotators (section 2.1).
2.1 Learning
We would like to set our model parameters to
maximize the probability of the observed data, i.e.,
the marginal data likelihood:
P (A; ?, ?) =
X
T,S
h NY
i=1
P (Ti) ?
MY
j=1
P (Sij ; ?j) ? P (Aij |Sij , Ti; ?j)
i
where A is the matrix of annotations, S is the
matrix of competence indicators, and T is the vector
of true labels.
We maximize the marginal data likelihood using
Expectation Maximization (EM) (Dempster et al,
1977), which has successfully been applied to
similar problems (Dawid and Skene, 1979). We ini-
tialize EM randomly and run for 50 iterations. We
perform 100 random restarts, and keep the model
with the best marginal data likelihood. We smooth
the M-step by adding a fixed value ? to the fractional
counts before normalizing (Eisner, 2002). We find
that smoothing improves accuracy, but, overall,
learning is robust to varying ?, and set ? = 0.1num labels .
We observe, however, that the average annota-
tor proficiency is usually high, i.e., most annota-
tors answer correctly. The distribution learned by
EM, however, is fairly linear. To improve the cor-
relation between model estimates and true annotator
proficiency, we would like to add priors about the
annotator behavior into the model. A straightfor-
ward approach is to employ Bayesian inference with
Beta priors on the proficiency parameters, ?j . We
thus also implement Variational-Bayes (VB) train-
ing with symmetric Beta priors on ?j and symmet-
ric Dirichlet priors on the strategy parameters, ?j .
Setting the shape parameters of the Beta distribution
to 0.5 favors the extremes of the distribution, i.e.,
either an annotator tried to get the right answer, or
simply did not care, but (almost) nobody tried ?a lit-
tle?. With VB training, we observe improved corre-
lations over all test sets with no loss in accuracy. The
hyper-parameters of the Dirichlet distribution on ?j
were clamped to 10.0 for all our experiments with
VB training. Our implementation is similar to John-
son (2007), which the reader can refer to for details.
3 Experiments
We evaluate our method on existing annotated
datasets from various AMT tasks. However, we
also want to ensure that our model can handle
adversarial conditions. Since we have no control
over the factors in existing datasets, we create
synthetic data for this purpose.
3.1 Natural Data
In order to evaluate our model, we use the
datasets from (Snow et al, 2008) that use discrete
label values (some tasks used continuous values,
which we currently do not model). Since they
compared AMT annotations to experts, gold anno-
tations exist for these sets. We can thus evaluate
the accuracy of the model as well as the proficiency
of each annotator. We show results for word sense
disambiguation (WSD: 177 items, 34 annotators),
recognizing textual entailment (RTE: 800 items,
164 annotators), and recognizing temporal relation
(Temporal: 462 items, 76 annotators).
3.2 Synthetic Data
In addition to the datasets above, we generate
synthetic data in order to control for different
1122
factors. This also allows us to create a gold standard
to which we can compare. We generate data sets
with 100 items, using two or four possible labels.
For each item, we generate answers from 20
different annotators. The ?annotators? are functions
that return one of the available labels according
to some strategy. Better annotators have a smaller
chance of guessing at random.
For various reasons, usually not all annotators see
or answer all items. We thus remove a randomly
selected subset of answers such that each item is
only answered by 10 of the annotators. See Figure
3 for an example annotation of three items.
annotators
ite
m
s ? 0 0 1 ? 0 ? ? 0 ?
1 ? ? 0 ? 1 0 ? ? 0
? ? 0 ? 0 1 ? 0 ? 0
Figure 3: Annotations: 10 annotators on three items,
labels {1, 0}, 5 annotations/item. Missing annota-
tions marked ???
3.3 Evaluations
First, we want to know which annotators to trust.
We evaluate whether our model?s learned trustwor-
thiness parameters ?j can be used to identify these
individuals (Section 4).
We then compare the label predicted by our model
and by majority voting to the correct label. The
results are reported as accuracy (Section 5). Since
our model computes posterior entropies for each
instance, we can use this as an approximation for the
model?s confidence in the prediction. If we focus on
predictions with high confidence (i.e., low entropy),
we hope to see better accuracy, even at the price of
leaving some items unanswered. We evaluate this
trade-off in Section 5.1. In addition, we investigate
the influence of the number of spammers and their
strategy on the accuracy of our model (Section 5.2).
4 Identifying Reliable Annotators
One of the distinguishing features of the model
is that it uses a parameter for each annotator to
estimate whether or not they are spamming. Can
we use this parameter to identify trustworthy indi-
viduals, to invite them for future tasks, and block
untrustworthy ones?
RTE Temporal WSD
raw agreement 0.78 0.73 0.81
Cohen?s ? 0.70 0.80 0.13
G-index 0.76 0.73 0.81
MACE-EM 0.87 0.88 0.44
MACE-VB (0.5,0.5) 0.91 0.90 0.90
Table 1: Correlation with annotator proficiency:
Pearson ? of different methods for various data sets.
MACE-VB?s trustworthiness parameter (trained
with Variational Bayes with ? = ? = 0.5) corre-
lates best with true annotator proficiency.
It is natural to apply some form of weighting.
One approach is to assume that reliable annotators
agree more with others than random annotators.
Inter-annotator agreement is thus a good candidate
to weigh the answers. There are various measures
for inter-annotator agreement.
Tratz and Hovy (2010) compute the average
agreement of each annotator and use it as a weight
to identify reliable ones. Raw agreement can be
directly computed from the data. It is related to
majority voting, since it will produce high scores for
all members of the majority class. Raw agreement
is thus a very simple measure.
In contrast, Cohen?s ? corrects the agreement
between two annotators for chance agreement. It
is widely used for inter-annotator agreement in
annotation tasks. We also compute the ? values
for each pair of annotators, and average them for
each annotator (similar to the approach in Tratz and
Hovy (2010)). However, whenever one label is more
prevalent (a common case in NLP tasks), ? overesti-
mates the effect of chance agreement (Feinstein and
Cicchetti, 1990) and penalizes disproportionately.
The G-index (Gwet, 2008) corrects for the number
of labels rather than chance agreement.
We compare these measures to our learned trust-
worthiness parameters ?j in terms of their ability to
select reliable annotators. A better measure should
lend higher score to annotators who answer correctly
more often than others. We thus compare the ratings
of each measure to the true proficiency of each
annotator. This is the percentage of annotated items
the annotator answered correctly. Methods that can
identify reliable annotators should highly correlate
1123
to the annotator?s proficiency. Since the methods
use different scales, we compute Pearson?s ? for the
correlation coefficient, which is scale-invariant. The
correlation results are shown in Table 1.
The model?s ?j correlates much more strongly
with annotator proficiency than either ? or raw
agreement. The variant trained with VB performs
consistently better than standard EM training, and
yields the best results. This show that our model
detects reliable annotators much better than any
of the other measures, which are only loosely
correlated to annotator proficiency.
The numbers for WSD also illustrate the low ?
score resulting when all annotators (correctly) agree
on a small number of labels. However, all inter-
annotator agreement measures suffer from an even
more fundamental problem: removing/ignoring
annotators with low agreement will always improve
the overall score, irrespective of the quality of their
annotations. Worse, there is no natural stopping
point: deleting the most egregious outlier always
improves agreement, until we have only one anno-
tator with perfect agreement left (Hovy, 2010). In
contrast, MACE does not discard any annotators,
but weighs their contributions differently. We are
thus not losing information. This works well even
under adversarial conditions (see Section 5.2).
5 Recovering the Correct Answer
RTE Temporal WSD
majority 0.90 0.93 0.99
Raykar/Yu 2012 0.93 0.94 ?
Carpenter 2008 0.93 ? ?
MACE-EM/VB 0.93 0.94 0.99
MACE-EM@90 0.95 0.97 0.99
MACE-EM@75 0.95 0.97 1.0
MACE-VB@90 0.96 0.97 1.0
MACE-VB@75 0.98 0.98 1.0
Table 2: Accuracy of different methods on data sets
from (Snow et al, 2008). MACE-VB uses Varia-
tional Bayes training. Results @n use the n% items
the model is most confident in (Section 5.1). Results
below double line trade coverage for accuracy and
are thus not comparable to upper half.
The previous sections showed that our model reli-
ably identifies trustworthy annotators. However, we
also want to find the most likely correct answer. Us-
ing majority voting often fails to find the correct la-
bel. This problem worsens when there are more than
two labels. We need to take relative majorities into
account or break ties when two or more labels re-
ceive the same number of votes. This is deeply un-
satisfying.
Figure 2 shows the accuracy of our model on
various data sets from Snow et al (2008). The
model outperforms majority voting on both RTE
and Temporal recognition sets. It performs as well
as majority voting for the WSD task. This last set
is somewhat of an exception, though, since almost
all annotators are correct all the time, so majority
voting is trivially correct. Still, we need to ensure
that the model does not perform worse under such
conditions. The results for RTE and Temporal data
also rival those reported in Raykar and Yu (2012)
and Carpenter (2008), yet were achieved with a
much simpler model.
Carpenter (2008) models instance difficulty as
a parameter. While it seems intuitively useful to
model which items are harder than other, it increases
the parameter space more than our trustworthiness
variable. We achieve comparable performance with-
out modeling difficulty, which greatly simplifies
inference. The model of Raykar and Yu (2012) is
more similar to our approach, in that it does not
model item difficulty. However, it adds an extra step
that learns priors from the estimated parameters. In
our model, this is part of the training process. For
more details on both models, see Section 6.
5.1 Trading Coverage for Accuracy
Sometimes, we want to produce an answer for ev-
ery item (e.g., when evaluating a data set), and some-
times, we value good answers more than answering
all items (e.g., when developing an annotated
corpus). Jha et al (2010) have demonstrated how to
achieve better coverage (i.e., answer more items) by
relaxing the majority voting constraints. Similarly,
we can improve accuracy if we only select high qual-
ity annotations, even if this incurs lower coverage.
We provide a parameter in MACE that allows
users to set a threshold for this trade-off: the
model only returns a label for an instance if it is
sufficiently confident in its answer. We approximate
the model?s confidence by the posterior entropy of
1124
0$&((0
0$&(9%
PDMRULW\
Figure 4: Tradeoff between coverage and accuracy for RTE (left) and temporal (right). Lower thresholds
lead to less coverage, but result in higher accuracy.
each instance. However, entropy depends strongly
on the specific makeup of the dataset (number of
annotators and labels, etc.), so it is hard for the user
to set a specific threshold.
Instead of requiring an exact entropy value, we
provide a simple thresholding between 0.0 and 1.0
(setting the threshold to 1.0 will include all items).
After training, MACE orders the posterior entropies
for all instances and selects the value that covers
the selected fraction of the instances. The threshold
thus roughly corresponds to coverage. It then only
returns answers for instances whose entropy is
below the threshold. This procedure is similar to
precision/recall curves.
Jha et al (2010) showed the effect of varying the
relative majority required, i.e., requiring that at least
n out of 10 annotators have to agree to count an
item. We use that method as baseline comparison,
evaluating the effect on coverage and accuracy
when we vary n from 5 to 10.
Figure 4 shows the tradeoff between coverage
and accuracy for two data sets. Lower thresholds
produce more accurate answers, but result in lower
coverage, as some items are left blank. If we pro-
duce answers for all items, we achieve accuracies
of 0.93 for RTE and 0.94 for Temporal, but by
excluding just the 10% of items in which the model
is least confident, we achieve accuracies as high as
0.95 for RTE and 0.97 for Temporal. We omit the
results for WSD here, since there is little headroom
and they are thus not very informative. Using Varia-
tional Bayes inference consistently achieves higher
results for the same coverage than the standard im-
plementation. Increasing the required majority also
improves accuracy, although not as much, and the
loss in coverage is larger and cannot be controlled.
In contrast, our method allows us to achieve better
accuracy at a smaller, controlled loss in coverage.
5.2 Influence of Strategy, Number of
Annotators, and Supervision
Adverse Strategy We showed that our model
recovers the correct answer with high accuracy.
However, to test whether this is just a function of
the annotator pool, we experiment with varying
the trustworthiness of the pool. If most annotators
answer correctly, majority voting is trivially correct,
as is our model. What happens, however, if more
and more annotators are unreliable? While some
agreement can arise from randomness, majority
voting is bound to become worse?can our model
overcome this problem? We set up a second set of
experiments to test this, using synthetic data. We
choose 20 annotators and vary the amount of good
annotators among them from 0 to 10 (after which
the trivial case sets in). We define a good annotator
as one who answers correctly 95% of the time.2
Adverse annotators select their answers randomly or
always choose a certain value (minimal annotators).
These are two frequent strategies of spammers.
For different numbers of labels and varying
percentage of spammers, we measure the accuracy
2The best annotators on the Snow data sets actually found
the correct answer 100% of the time.
1125
a) random annotators b) minimal annotators
Figure 5: Influence of adverse annotator strategy on label accuracy (y-axis). Number of possible labels
varied between 2 (top row) and 4 (bottom row). Adverse annotators either choose at random (a) or always
select the first label (b). MACE needs fewer good annotators to recover the correct answer.
0$&((0
0$&(9%
PDMRULW\
Figure 6: Varying number of annotators: effect on prediction accuracy. Each point averaged over 10 runs.
Note different scale for WSD.
of our model and majority voting on 100 items,
averaged over 10 runs for each condition. Figure
5 shows the effect of annotator proficiency on both
majority voting and our method for both kinds of
spammers. Annotator pool strategy affects majority
voting more than our model. Even with few good
annotators, our model learns to dismiss the spam-
mers as noise. There is a noticeable point on each
graph where MACE diverges from the majority
voting line. It thus reaches good accuracy much
1126
faster than majority voting, i.e., with fewer good an-
notators. This divergence point happens earlier with
more label values when adverse annotators label
randomly. In general, random annotators are easier
to deal with than the ones always choosing the first
label. Note that in cases where we have a majority
of adversarial annotators, VB performs worse than
EM, since this condition violates the implicit as-
sumptions we encoded with the priors in VB. Under
these conditions, setting different priors to reflect
the annotator pool should improve performance.
Obviously, both of these pools are extremes: it is
unlikely to have so few good or so many malicious
annotators. Most pools will be somewhere in
between. It does show, however, that our model
can pick up on reliable annotators even under very
unfavorable conditions. The result has a practical
upshot: AMT allows us to require a minimum rating
for annotators to work on a task. Higher ratings
improve annotation quality, but delay completion,
since there are fewer annotators with high ratings.
The results in this section suggest that we can find
the correct answer even in annotator pools with low
overall proficiency. We can thus waive the rating
requirement and allow more annotators to work on
the task. This considerably speeds up completion.
Number of Annotators Figure 6 shows the effect
different numbers of annotators have on accuracy.
As we increase the number of annotators, MACE
and majority voting achieve better accuracy results.
We note that majority voting results level or even
drop when going from an odd to an even number.
In these cases, the new annotator does not improve
accuracy if it goes with the previous majority (i.e.,
going from 3:2 to 4:2), but can force an error when
going against the previous majority (i.e., from 3:2 to
3:3), by creating a tie. MACE-EM and MACE-VB
dominate majority voting for RTE and Temporal.
For WSD, the picture is less clear, where majority
voting dominates when there are fewer annotators.
Note that the differences are minute, though (within
1 percentage point). For very small pool sizes (< 3),
MACE-VB outperforms both other methods.
Amount of Supervision So far, we have treated
the task as completely unsupervised. MACE does
not require any expert annotations in order to
achieve high accuracy. However, we often have
annotations for some of the items. These annotated
data points are usually used as control items (by
removing annotators that answer them incorrectly).
If such annotated data is available, we would like
to make use of it. We include an option that lets
users supply annotations for some of the items,
and use this information as token constraints in the
E-step of training. In those cases, the model does
not need to estimate the correct value, but only has
to adjust the trust parameter. This leads to improved
performance.3
We explore for RTE and Temporal how per-
formance changes when we vary the amount of
supervision in increments of 5%.4 We average over
10 runs for each value of n, each time supplying an-
notations for a random set of n items. The baseline
uses the annotated label whenever supplied, other-
wise the majority vote, with ties split at random.
Figure 7 shows that, unsurprisingly, all methods
improve with additional supervision, ultimately
reaching perfect accuracy. However, MACE uses
the information more effectively, resulting in
higher accuracy for a given amount of supervision.
This gain is more pronounced when only little
supervision is available.
6 Related Research
Snow et al (2008) and Sorokin and Forsyth
(2008) showed that Amazon?s MechanicalTurk use
in providing non-expert annotations for NLP tasks.
Various models have been proposed for predicting
correct annotations from noisy non-expert annota-
tions and for estimating annotator trustworthiness.
These models divide naturally into two categories:
those that use expert annotations for supervised
learning (Snow et al, 2008; Bian et al, 2009), and
completely unsupervised ones. Our method falls
into the latter category because it learns from the
redundant non-expert annotations themselves, and
makes no use of expertly annotated data.
Most previous work on unsupervised models
belongs to a class called ?Item-response models?,
used in psychometrics. The approaches differ with
respect to which aspect of the annotation process
3If we had annotations for all items, accuracy would be per-
fect and require no training.
4Given the high accuracy for the WSD data set even in the
fully unsupervised case, we omit the results here.
1127
Figure 7: Varying the amount of supervision: effect on prediction accuracy. Each point averaged over 10
runs. MACE uses supervision more efficiently.
they choose to focus on, and the type of annotation
task they model. For example, many methods ex-
plicitly model annotator bias in addition to annotator
competence (Dawid and Skene, 1979; Smyth et al,
1995). Our work models annotator bias, but only
when the annotator is suspected to be spamming.
Other methods focus modeling power on instance
difficulty to learn not only which annotators are
good, but which instances are hard (Carpenter,
2008; Whitehill et al, 2009). In machine vision,
several models have taken this further by parameter-
izing difficulty in terms of complex features defined
on each pairing of annotator and annotation instance
(Welinder et al, 2010; Yan et al, 2010). While
such features prove very useful in vision, they are
more difficult to define for the categorical problems
common to NLP. In addition, several methods are
specifically tailored to annotation tasks that involve
ranking (Steyvers et al, 2009; Lee et al, 2011),
which limits their applicability in NLP.
The method of Raykar and Yu (2012) is most
similar to ours. Their goal is to identify and filter
out annotators whose annotations are not correlated
with the gold label. They define a function of the
learned parameters that is useful for identifying
these spammers, and then use this function to build
a prior. In contrast, we use simple priors, but incor-
porate a model parameter that explicitly represents
the probability that an annotator is spamming. Our
simple model achieves the same accuracy on gold
label predictions as theirs.
7 Conclusion
We provide a Java-based implementation, MACE,
that recovers correct labels with high accuracy, and
reliably identifies trustworthy annotators. In
addition, it provides a threshold to control the
accuracy/coverage trade-off and can be trained with
standard EM or Variational Bayes EM. MACE
works fully unsupervised, but can incorporate token
constraints via annotated control items. We show
that even small amounts help improve accuracy.
Our model focuses most of its modeling power
on learning trustworthiness parameters, which
are highly correlated with true annotator relia-
bility (Pearson ? 0.9). We show on real-world
and synthetic data sets that our method is more
accurate than majority voting, even under ad-
versarial conditions, and as accurate as more
complex state-of-the-art systems. Focusing on high-
confidence instances improves accuracy consider-
ably. MACE is freely available for download under
http://www.isi.edu/publications/
licensed-sw/mace/index.html.
Acknowledgements
The authors would like to thank Chris Callison-
Burch, Victoria Fossum, Stephan Gouws, Marc
Schulder, Nathan Schneider, and Noah Smith for
invaluable discussions, as well as the reviewers for
their constructive feedback.
1128
References
Jiang Bian, Yandong Liu, Ding Zhou, Eugene Agichtein,
and Hongyuan Zha. 2009. Learning to recognize re-
liable users and content in social media with coupled
mutual reinforcement. In Proceedings of the 18th in-
ternational conference on World wide web, pages 51?
60. ACM.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with amazon?s mechanical
turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
June. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July. Association for
Computational Linguistics.
Bob Carpenter. 2008. Multilevel Bayesian models of
categorical data annotation. Unpublished manuscript.
A. Philip Dawid and Allan M. Skene. 1979. Maximum
likelihood estimation of observer error-rates using the
EM algorithm. Applied Statistics, pages 20?28.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), 39(1):1?38.
Jason Eisner. 2002. An interactive spreadsheet for teach-
ing the forward-backward algorithm. In Proceed-
ings of the ACL-02 Workshop on Effective tools and
methodologies for teaching natural language process-
ing and computational linguistics-Volume 1, pages 10?
18. Association for Computational Linguistics.
Alvan R. Feinstein and Domenic V. Cicchetti. 1990.
High agreement but low kappa: I. the problems of
two paradoxes. Journal of Clinical Epidemiology,
43(6):543?549.
Kilem Li Gwet. 2008. Computing inter-rater reliabil-
ity and its variance in the presence of high agreement.
British Journal of Mathematical and Statistical Psy-
chology, 61(1):29?48.
Eduard Hovy. 2010. Annotation. A Tutorial. In 48th
Annual Meeting of the Association for Computational
Linguistics.
Mukund Jha, Jacob Andreas, Kapil Thadani, Sara Rosen-
thal, and Kathleen McKeown. 2010. Corpus creation
for new genres: A crowdsourced approach to pp at-
tachment. In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk, pages 13?20. Asso-
ciation for Computational Linguistics.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 296?305.
Michael D. Lee, Mark Steyvers, Mindy de Young, and
Brent J. Miller. 2011. A model-based approach to
measuring expertise in ranking tasks. In L. Carlson,
C. Ho?lscher, and T.F. Shipley, editors, Proceedings of
the 33rd Annual Conference of the Cognitive Science
Society, Austin, TX. Cognitive Science Society.
Vikas C. Raykar and Shipeng Yu. 2012. Eliminating
Spammers and Ranking Annotators for Crowdsourced
Labeling Tasks. Journal of Machine Learning Re-
search, 13:491?518.
Padhraic Smyth, Usama Fayyad, Mike Burl, Pietro Per-
ona, and Pierre Baldi. 1995. Inferring ground truth
from subjective labelling of Venus images. Advances
in neural information processing systems, pages 1085?
1092.
Rion Snow, Brendan O?Connor, Dan Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 254?263. Association for Computational Lin-
guistics.
Alexander Sorokin and David Forsyth. 2008. Utility
data annotation with Amazon Mechanical Turk. In
IEEE Computer Society Conference on Computer Vi-
sion and Pattern Recognition Workshops, CVPRW ?08,
pages 1?8. IEEE.
Mark Steyvers, Michael D. Lee, Brent Miller, and
Pernille Hemmer. 2009. The wisdom of crowds in the
recollection of order information. Advances in neural
information processing systems, 23.
Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 678?687. Association for Computational
Linguistics.
Peter Welinder, Steve Branson, Serge Belongie, and
Pietro Perona. 2010. The multidimensional wisdom
of crowds. In Neural Information Processing Systems
Conference (NIPS), volume 6.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels from
labelers of unknown expertise. Advances in Neural In-
formation Processing Systems, 22:2035?2043.
Yan Yan, Ro?mer Rosales, Glenn Fung, Mark Schmidt,
Gerardo Hermosillo, Luca Bogoni, Linda Moy, and
1129
Jennifer Dy. 2010. Modeling annotator expertise:
Learning when everybody knows a bit of something.
In International Conference on Artificial Intelligence
and Statistics.
Omar F. Zaidan and Chris Callison-Burch. 2011. Crowd-
sourcing translation: Professional quality from non-
professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220?
1229, Portland, Oregon, USA, June. Association for
Computational Linguistics.
1130
Proceedings of the ACL 2010 Conference Short Papers, pages 209?214,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Efficient Optimization of an MDL-Inspired Objective Function for
Unsupervised Part-of-Speech Tagging
Ashish Vaswani1 Adam Pauls2 David Chiang1
1Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
{avaswani,chiang}@isi.edu
2Computer Science Division
University of California at Berkeley
Soda Hall
Berkeley, CA 94720
adpauls@eecs.berkeley.edu
Abstract
The Minimum Description Length (MDL)
principle is a method for model selection
that trades off between the explanation of
the data by the model and the complexity
of the model itself. Inspired by the MDL
principle, we develop an objective func-
tion for generative models that captures
the description of the data by the model
(log-likelihood) and the description of the
model (model size). We also develop a ef-
ficient general search algorithm based on
the MAP-EM framework to optimize this
function. Since recent work has shown that
minimizing the model size in a Hidden
Markov Model for part-of-speech (POS)
tagging leads to higher accuracies, we test
our approach by applying it to this prob-
lem. The search algorithm involves a sim-
ple change to EM and achieves high POS
tagging accuracies on both English and
Italian data sets.
1 Introduction
The Minimum Description Length (MDL) princi-
ple is a method for model selection that provides a
generic solution to the overfitting problem (Barron
et al, 1998). A formalization of Ockham?s Razor,
it says that the parameters are to be chosen that
minimize the description length of the data given
the model plus the description length of the model
itself.
It has been successfully shown that minimizing
the model size in a Hidden Markov Model (HMM)
for part-of-speech (POS) tagging leads to higher
accuracies than simply running the Expectation-
Maximization (EM) algorithm (Dempster et al,
1977). Goldwater and Griffiths (2007) employ a
Bayesian approach to POS tagging and use sparse
Dirichlet priors to minimize model size. More re-
cently, Ravi and Knight (2009) alternately mini-
mize the model using an integer linear program
and maximize likelihood using EM to achieve the
highest accuracies on the task so far. However, in
the latter approach, because there is no single ob-
jective function to optimize, it is not entirely clear
how to generalize this technique to other prob-
lems. In this paper, inspired by the MDL princi-
ple, we develop an objective function for genera-
tive models that captures both the description of
the data by the model (log-likelihood) and the de-
scription of the model (model size). By using a
simple prior that encourages sparsity, we cast our
problem as a search for the maximum a poste-
riori (MAP) hypothesis and present a variant of
EM to approximately search for the minimum-
description-length model. Applying our approach
to the POS tagging problem, we obtain higher ac-
curacies than both EM and Bayesian inference as
reported by Goldwater and Griffiths (2007). On a
Italian POS tagging task, we obtain even larger
improvements. We find that our objective function
correlates well with accuracy, suggesting that this
technique might be useful for other problems.
2 MAP EM with Sparse Priors
2.1 Objective function
In the unsupervised POS tagging task, we are
given a word sequence w = w1, . . . ,wN and want
to find the best tagging t = t1, . . . , tN , where
ti ? T , the tag vocabulary. We adopt the problem
formulation of Merialdo (1994), in which we are
given a dictionary of possible tags for each word
type.
We define a bigram HMM
P(w, t | ?) =
N?
i=1
P(w, t | ?) ? P(ti | ti?1) (1)
In maximum likelihood estimation, the goal is to
209
find parameter estimates
?? = arg max
?
log P(w | ?) (2)
= arg max
?
log
?
t
P(w, t | ?) (3)
The EM algorithm can be used to find a solution.
However, we would like to maximize likelihood
and minimize the size of the model simultane-
ously. We define the size of a model as the number
of non-zero probabilities in its parameter vector.
Let ?1, . . . , ?n be the components of ?. We would
like to find
?? = arg min
?
(
? log P(w | ?) + ????0
)
(4)
where ???0, called the L0 norm of ?, simply counts
the number of non-zero parameters in ?. The
hyperparameter ? controls the tradeoff between
likelihood maximization and model minimization.
Note the similarity of this objective function with
MDL?s, where ? would be the space (measured
in nats) needed to describe one parameter of the
model.
Unfortunately, minimization of the L0 norm
is known to be NP-hard (Hyder and Mahata,
2009). It is not smooth, making it unamenable
to gradient-based optimization algorithms. There-
fore, we use a smoothed approximation,
???0 ?
?
i
(
1 ? e
??i
?
)
(5)
where 0 < ? ? 1 (Mohimani et al, 2007). For
smaller values of ?, this closely approximates the
desired function (Figure 1). Inverting signs and ig-
noring constant terms, our objective function is
now:
?? = arg max
?
?
??????log P(w | ?) + ?
?
i
e
??i
?
?
?????? (6)
We can think of the approximate model size as
a kind of prior:
P(?) =
exp?
?
i e
??i
?
Z
(7)
log P(?) = ? ?
?
i
e
??i
? ? log Z (8)
where Z =
?
d?
exp?
?
i e
??i
? is a normalization
constant. Then our goal is to find the maximum
 
0
 
0.2
 
0.4
 
0.6
 
0.8 1  0
 
0.2
 
0.4
 
0.6
 
0.8
 
1
Function Values
? i
?=0.005 ?=0.05 ?=0.5 1-||? i|| 0
Figure 1: Ideal model-size term and its approxima-
tions.
a posterior parameter estimate, which we find us-
ing MAP-EM (Bishop, 2006):
?? = arg max
?
log P(w, ?) (9)
= arg max
?
(
log P(w | ?) + log P(?)
)
(10)
Substituting (8) into (10) and ignoring the constant
term log Z, we get our objective function (6) again.
We can exercise finer control over the sparsity
of the tag-bigram and channel probability distri-
butions by using a different ? for each:
arg max
?
(
log P(w | ?) +
?c
?
w,t
e
?P(w|t)
? + ?t
?
t,t?
e
?P(t? |t)
?
)
(11)
In our experiments, we set ?c = 0 since previ-
ous work has shown that minimizing the number
of tag n-gram parameters is more important (Ravi
and Knight, 2009; Goldwater and Griffiths, 2007).
A common method for preferring smaller mod-
els is minimizing the L1 norm,
?
i |?i|. However,
for a model which is a product of multinomial dis-
tributions, the L1 norm is a constant.
?
i
|?i| =
?
i
?i
=
?
t
?
??????
?
w
P(w | t) +
?
t?
P(t? | t)
?
??????
= 2|T |
Therefore, we cannot use the L1 norm as part of
the size term as the result will be the same as the
EM algorithm.
210
2.2 Parameter optimization
To optimize (11), we use MAP EM, which is an it-
erative search procedure. The E step is the same as
in standard EM, which is to calculate P(t | w, ?t),
where the ?t are the parameters in the current iter-
ation t. The M step in iteration (t + 1) looks like
?t+1 = arg max
?
(
EP(t|w,?t)
[
log P(w, t | ?)
]
+
?t
?
t,t?
e
?P(t? |t)
?
) (12)
Let C(t,w; t,w) count the number of times the
word w is tagged as t in t, and C(t, t?; t) the number
of times the tag bigram (t, t?) appears in t. We can
rewrite the M step as
?t+1 = arg max
?
(?
t
?
w
E[C(t,w)] log P(w | t)+
?
t
?
t?
(
E[C(t, t?)] log P(t? | t) + ?te
?P(t? |t)
?
)??????? (13)
subject to the constraints
?
w P(w | t) = 1 and?
t? P(t
? | t) = 1. Note that we can optimize each
term of both summations over t separately. For
each t, the term
?
w
E[C(t,w)] log P(w | t) (14)
is easily optimized as in EM: just let P(w | t) ?
E[C(t,w)]. But the term
?
t?
(
E[C(t, t?)] log P(t? | t) + ?te
?P(t? |t)
?
)
(15)
is trickier. This is a non-convex optimization prob-
lem for which we invoke a publicly available
constrained optimization tool, ALGENCAN (An-
dreani et al, 2007). To carry out its optimization,
ALGENCAN requires computation of the follow-
ing in every iteration:
? Objective function, defined in equation (15).
This is calculated in polynomial time using
dynamic programming.
? Constraints: gt =
?
t? P(t
? | t) ? 1 = 0 for
each tag t ? T . Also, we constrain P(t? | t) to
the interval [, 1].1
1We must have  > 0 because of the log P(t? | t) term
in equation (15). It seems reasonable to set   1N ; in our
experiments, we set  = 10?7.
? Gradient of objective function:
?F
?P(t? | t)
=
E[C(t, t?)]
P(t? | t)
?
?t
?
e
?P(t? |t)
? (16)
? Gradient of equality constraints:
?gt
?P(t?? | t?)
=
?
???
???
1 if t = t?
0 otherwise
(17)
? Hessian of objective function, which is not
required but greatly speeds up the optimiza-
tion:
?2F
?P(t? | t)?P(t? | t)
= ?
E[C(t, t?)]
P(t? | t)2
+ ?t
e
?P(t? |t)
?
?2
(18)
The other second-order partial derivatives are
all zero, as are those of the equality con-
straints.
We perform this optimization for each instance
of (15). These optimizations could easily be per-
formed in parallel for greater scalability.
3 Experiments
We carried out POS tagging experiments on En-
glish and Italian.
3.1 English POS tagging
To set the hyperparameters ?t and ?, we prepared
three held-out sets H1,H2, and H3 from the Penn
Treebank. Each Hi comprised about 24, 000 words
annotated with POS tags. We ran MAP-EM for
100 iterations, with uniform probability initializa-
tion, for a suite of hyperparameters and averaged
their tagging accuracies over the three held-out
sets. The results are presented in Table 2. We then
picked the hyperparameter setting with the highest
average accuracy. These were ?t = 80, ? = 0.05.
We then ran MAP-EM again on the test data with
these hyperparameters and achieved a tagging ac-
curacy of 87.4% (see Table 1). This is higher than
the 85.2% that Goldwater and Griffiths (2007) ob-
tain using Bayesian methods for inferring both
POS tags and hyperparameters. It is much higher
than the 82.4% that standard EM achieves on the
test set when run for 100 iterations.
Using ?t = 80, ? = 0.05, we ran multiple ran-
dom restarts on the test set (see Figure 2). We find
that the objective function correlates well with ac-
curacy, and picking the point with the highest ob-
jective function value achieves 87.1% accuracy.
211
?t
?
0.75 0.5 0.25 0.075 0.05 0.025 0.0075 0.005 0.0025
10 82.81 82.78 83.10 83.50 83.76 83.70 84.07 83.95 83.75
20 82.78 82.82 83.26 83.60 83.89 84.88 83.74 84.12 83.46
30 82.78 83.06 83.26 83.29 84.50 84.82 84.54 83.93 83.47
40 82.81 83.13 83.50 83.98 84.23 85.31 85.05 83.84 83.46
50 82.84 83.24 83.15 84.08 82.53 84.90 84.73 83.69 82.70
60 83.05 83.14 83.26 83.30 82.08 85.23 85.06 83.26 82.96
70 83.09 83.10 82.97 82.37 83.30 86.32 83.98 83.55 82.97
80 83.13 83.15 82.71 83.00 86.47 86.24 83.94 83.26 82.93
90 83.20 83.18 82.53 84.20 86.32 84.87 83.49 83.62 82.03
100 83.19 83.51 82.84 84.60 86.13 85.94 83.26 83.67 82.06
110 83.18 83.53 83.29 84.40 86.19 85.18 80.76 83.32 82.05
120 83.08 83.65 83.71 84.11 86.03 85.39 80.66 82.98 82.20
130 83.10 83.19 83.52 84.02 85.79 85.65 80.08 82.04 81.76
140 83.11 83.17 83.34 85.26 85.86 85.84 79.09 82.51 81.64
150 83.14 83.20 83.40 85.33 85.54 85.18 78.90 81.99 81.88
Table 2: Average accuracies over three held-out sets for English.
system accuracy (%)
Standard EM 82.4
+ random restarts 84.5
(Goldwater and Griffiths, 2007) 85.2
our approach 87.4
+ random restarts 87.1
Table 1: MAP-EM with a L0 norm achieves higher
tagging accuracy on English than (2007) and much
higher than standard EM.
system zero parameters bigram types
maximum possible 1389 ?
EM, 100 iterations 444 924
MAP-EM, 100 iterations 695 648
Table 3: MAP-EM with a smoothed L0 norm
yields much smaller models than standard EM.
We also carried out the same experiment with stan-
dard EM (Figure 3), where picking the point with
the highest corpus probability achieves 84.5% ac-
curacy.
We also measured the minimization effect of the
sparse prior against that of standard EM. Since our
method lower-bounds all the parameters by , we
consider a parameter ?i as a zero if ?i ? . We
also measured the number of unique tag bigram
types in the Viterbi tagging of the word sequence.
Table 3 shows that our method produces much
smaller models than EM, and produces Viterbi
taggings with many fewer tag-bigram types.
3.2 Italian POS tagging
We also carried out POS tagging experiments on
an Italian corpus from the Italian Turin Univer-
 
0.78
 
0.79 0.8
 
0.81
 
0.82
 
0.83
 
0.84
 
0.85
 
0.86
 
0.87
 
0.88
 
0.89 -532
00-53
000-5
2800-
52600
-
52400
-
52200
-
52000
-
51800
-
51600
-
51400
Tagging accuracy
objective
 function
 value
? t=80,
?=0.05,T
est Set
 24115
 Words
Figure 2: Tagging accuracy vs. objective func-
tion for 1152 random restarts of MAP-EM with
smoothed L0 norm.
sity Treebank (Bos et al, 2009). This test set com-
prises 21, 878 words annotated with POS tags and
a dictionary for each word type. Since this is all
the available data, we could not tune the hyperpa-
rameters on a held-out data set. Using the hyper-
parameters tuned on English (?t = 80, ? = 0.05),
we obtained 89.7% tagging accuracy (see Table 4),
which was a large improvement over 81.2% that
standard EM achieved. When we tuned the hyper-
parameters on the test set, the best setting (?t =
120, ? = 0.05 gave an accuracy of 90.28%.
4 Conclusion
A variety of other techniques in the literature have
been applied to this unsupervised POS tagging
task. Smith and Eisner (2005) use conditional ran-
dom fields with contrastive estimation to achieve
212
?t
?
0.75 0.5 0.25 0.075 0.05 0.025 0.0075 0.005 0.0025
10 81.62 81.67 81.63 82.47 82.70 84.64 84.82 84.96 84.90
20 81.67 81.63 81.76 82.75 84.28 84.79 85.85 88.49 85.30
30 81.66 81.63 82.29 83.43 85.08 88.10 86.16 88.70 88.34
40 81.64 81.79 82.30 85.00 86.10 88.86 89.28 88.76 88.80
50 81.71 81.71 78.86 85.93 86.16 88.98 88.98 89.11 88.01
60 81.65 82.22 78.95 86.11 87.16 89.35 88.97 88.59 88.00
70 81.69 82.25 79.55 86.32 89.79 89.37 88.91 85.63 87.89
80 81.74 82.23 80.78 86.34 89.70 89.58 88.87 88.32 88.56
90 81.70 81.85 81.00 86.35 90.08 89.40 89.09 88.09 88.50
100 81.70 82.27 82.24 86.53 90.07 88.93 89.09 88.30 88.72
110 82.19 82.49 82.22 86.77 90.12 89.22 88.87 88.48 87.91
120 82.23 78.60 82.76 86.77 90.28 89.05 88.75 88.83 88.53
130 82.20 78.60 83.33 87.48 90.12 89.15 89.30 87.81 88.66
140 82.24 78.64 83.34 87.48 90.12 89.01 88.87 88.99 88.85
150 82.28 78.69 83.32 87.75 90.25 87.81 88.50 89.07 88.41
Table 4: Accuracies on test set for Italian.
 
0.76
 
0.78 0.8
 
0.82
 
0.84
 
0.86
 
0.88 0.9 -1475
00-14
7400-
147300
-
147200
-
147100
-
147000
-
146900
-
146800
-
146700
-
146600
-
146500
-
146400
Tagging accuracy
objective
 function
 value
EM, T
est Set
 24115
 Words
Figure 3: Tagging accuracy vs. likelihood for 1152
random restarts of standard EM.
88.6% accuracy. Goldberg et al (2008) provide
a linguistically-informed starting point for EM to
achieve 91.4% accuracy. More recently, Chiang et
al. (2010) use GIbbs sampling for Bayesian in-
ference along with automatic run selection and
achieve 90.7%.
In this paper, our goal has been to investi-
gate whether EM can be extended in a generic
way to use an MDL-like objective function that
simultaneously maximizes likelihood and mini-
mizes model size. We have presented an efficient
search procedure that optimizes this function for
generative models and demonstrated that maxi-
mizing this function leads to improvement in tag-
ging accuracy over standard EM. We infer the hy-
perparameters of our model using held out data
and achieve better accuracies than (Goldwater and
Griffiths, 2007). We have also shown that the ob-
jective function correlates well with tagging accu-
racy supporting the MDL principle. Our approach
performs quite well on POS tagging for both En-
glish and Italian. We believe that, like EM, our
method can benefit from more unlabeled data, and
there is reason to hope that the success of these
experiments will carry over to other tasks as well.
Acknowledgements
We would like to thank Sujith Ravi, Kevin Knight
and Steve DeNeefe for their valuable input, and
Jason Baldridge for directing us to the Italian
POS data. This research was supported in part by
DARPA contract HR0011-06-C-0022 under sub-
contract to BBN Technologies and DARPA con-
tract HR0011-09-1-0028.
References
R. Andreani, E. G. Birgin, J. M. Martnez, and M. L.
Schuverdt. 2007. On Augmented Lagrangian meth-
ods with general lower-level constraints. SIAM
Journal on Optimization, 18:1286?1309.
A. Barron, J. Rissanen, and B. Yu. 1998. The min-
imum description length principle in coding and
modeling. IEEE Transactions on Information The-
ory, 44(6):2743?2760.
C. Bishop. 2006. Pattern Recognition and Machine
Learning. Springer.
J. Bos, C. Bosco, and A. Mazzei. 2009. Converting a
dependency treebank to a categorical grammar tree-
bank for italian. In Eighth International Workshop
on Treebanks and Linguistic Theories (TLT8).
D. Chiang, J. Graehl, K. Knight, A. Pauls, and S. Ravi.
2010. Bayesian inference for Finite-State transduc-
ers. In Proceedings of the North American Associa-
tion of Computational Linguistics.
213
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Computational Linguistics, 39(4):1?
38.
Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM can
find pretty good HMM POS-taggers (when given a
good start). In Proceedings of the ACL.
S. Goldwater and T. L. Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the ACL.
M. Hyder and K. Mahata. 2009. An approximate L0
norm minimization algorithm for compressed sens-
ing. In Proceedings of the 2009 IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing.
B. Merialdo. 1994. Tagging English text with a
probabilistic model. Computational Linguistics,
20(2):155?171.
H. Mohimani, M. Babaie-Zadeh, and C. Jutten. 2007.
Fast sparse representation based on smoothed L0
norm. In Proceedings of the 7th International Con-
ference on Independent Component Analysis and
Signal Separation (ICA2007).
S. Ravi and K. Knight. 2009. Minimized models for
unsupervised part-of-speech tagging. In Proceed-
ings of ACL-IJCNLP.
N. Smith. and J. Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proceedings of the ACL.
214
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 856?864,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Rule Markov Models for Fast Tree-to-String Translation
Ashish Vaswani
Information Sciences Institute
University of Southern California
avaswani@isi.edu
Haitao Mi
Institute of Computing Technology
Chinese Academy of Sciences
htmi@ict.ac.cn
Liang Huang and David Chiang
Information Sciences Institute
University of Southern California
{lhuang,chiang}@isi.edu
Abstract
Most statistical machine translation systems
rely on composed rules (rules that can be
formed out of smaller rules in the grammar).
Though this practice improves translation by
weakening independence assumptions in the
translation model, it nevertheless results in
huge, redundant grammars, making both train-
ing and decoding inefficient. Here, we take the
opposite approach, where we only use min-
imal rules (those that cannot be formed out
of other rules), and instead rely on a rule
Markov model of the derivation history to
capture dependencies between minimal rules.
Large-scale experiments on a state-of-the-art
tree-to-string translation system show that our
approach leads to a slimmer model, a faster
decoder, yet the same translation quality (mea-
sured using Bleu) as composed rules.
1 Introduction
Statistical machine translation systems typically
model the translation process as a sequence of trans-
lation steps, each of which uses a translation rule,
for example, a phrase pair in phrase-based transla-
tion or a tree-to-string rule in tree-to-string transla-
tion. These rules are usually applied independently
of each other, which violates the conventional wis-
dom that translation should be done in context.
To alleviate this problem, most state-of-the-art sys-
tems rely on composed rules, which are larger rules
that can be formed out of smaller rules (includ-
ing larger phrase pairs that can be formerd out of
smaller phrase pairs), as opposed to minimal rules,
which are rules that cannot be formed out of other
rules. Although this approach does improve trans-
lation quality dramatically by weakening the inde-
pendence assumptions in the translation model, they
suffer from two main problems. First, composition
can cause a combinatorial explosion in the number
of rules. To avoid this, ad-hoc limits are placed dur-
ing composition, like upper bounds on the number
of nodes in the composed rule, or the height of the
rule. Under such limits, the grammar size is man-
ageable, but still much larger than the minimal-rule
grammar. Second, due to large grammars, the de-
coder has to consider many more hypothesis transla-
tions, which slows it down. Nevertheless, the advan-
tages outweigh the disadvantages, and to our knowl-
edge, all top-performing systems, both phrase-based
and syntax-based, use composed rules. For exam-
ple, Galley et al (2004) initially built a syntax-based
system using only minimal rules, and subsequently
reported (Galley et al, 2006) that composing rules
improves Bleu by 3.6 points, while increasing gram-
mar size 60-fold and decoding time 15-fold.
The alternative we propose is to replace composed
rules with a rule Markov model that generates rules
conditioned on their context. In this work, we re-
strict a rule?s context to the vertical chain of ances-
tors of the rule. This ancestral context would play
the same role as the context formerly provided by
rule composition. The dependency treelet model de-
veloped by Quirk and Menezes (2006) takes such
an approach within the framework of dependency
translation. However, their study leaves unanswered
whether a rule Markov model can take the place
of composed rules. In this work, we investigate the
use of rule Markov models in the context of tree-
856
to-string translation (Liu et al, 2006; Huang et al,
2006). We make three new contributions.
First, we carry out a detailed comparison of rule
Markov models with composed rules. Our experi-
ments show that, using trigram rule Markov mod-
els, we achieve an improvement of 2.2 Bleu over
a baseline of minimal rules. When we compare
against vertically composed rules, we find that our
rule Markov model has the same accuracy, but our
model is much smaller and decoding with our model
is 30% faster. When we compare against full com-
posed rules, we find that our rule Markov model still
often reaches the same level of accuracy, again with
savings in space and time.
Second, we investigate methods for pruning rule
Markov models, finding that even very simple prun-
ing criteria actually improve the accuracy of the
model, while of course decreasing its size.
Third, we present a very fast decoder for tree-to-
string grammars with rule Markov models. Huang
and Mi (2010) have recently introduced an efficient
incremental decoding algorithm for tree-to-string
translation, which operates top-down and maintains
a derivation history of translation rules encountered.
This history is exactly the vertical chain of ancestors
corresponding to the contexts in our rule Markov
model, which makes it an ideal decoder for our
model.
We start by describing our rule Markov model
(Section 2) and then how to decode using the rule
Markov model (Section 3).
2 Rule Markov models
Our model which conditions the generation of a rule
on the vertical chain of its ancestors, which allows it
to capture interactions between rules.
Consider the example Chinese-English tree-to-
string grammar in Figure 1 and the example deriva-
tion in Figure 2. Each row is a derivation step; the
tree on the left is the derivation tree (in which each
node is a rule and its children are the rules that sub-
stitute into it) and the tree pair on the right is the
source and target derived tree. For any derivation
node r, let anc1(r) be the parent of r (or  if it has no
parent), anc2(r) be the grandparent of node r (or  if
it has no grandparent), and so on. Let ancn1(r) be the
chain of ancestors anc1(r) ? ? ? ancn(r).
The derivation tree is generated as follows. With
probability P(r1 | ), we generate the rule at the root
node, r1. We then generate rule r2 with probability
P(r2 | r1), and so on, always taking the leftmost open
substitution site on the English derived tree, and gen-
erating a rule ri conditioned on its chain of ancestors
with probability P(ri | ancn1(ri)). We carry on until
no more children can be generated. Thus the proba-
bility of a derivation tree T is
P(T ) =
?
r?T
P(r | ancn1(r)) (1)
For the minimal rule derivation tree in Figure 2, the
probability is:
P(T ) = P(r1 | ) ? P(r2 | r1) ? P(r3 | r1)
? P(r4 | r1, r3) ? P(r6 | r1, r3, r4)
? P(r7 | r1, r3, r4) ? P(r5 | r1, r3) (2)
Training We run the algorithm of Galley et al
(2004) on word-aligned parallel text to obtain a sin-
gle derivation of minimal rules for each sentence
pair. (Unaligned words are handled by attaching
them to the highest node possible in the parse tree.)
The rule Markov model
can then be trained on the path set of these deriva-
tion trees.
Smoothing We use interpolation with absolute
discounting (Ney et al, 1994):
Pabs(r | ancn1(r)) =
max
{
c(r | ancn1(r)) ? Dn, 0
}
?
r? c(r? | ancn1(r?))
+ (1 ? ?n)Pabs(r | ancn?11 (r)), (3)
where c(r | ancn1(r)) is the number of times we have
seen rule r after the vertical context ancn1(r), Dn is
the discount for a context of length n, and (1 ? ?n) is
set to the value that makes the smoothed probability
distribution sum to one.
We experiment with bigram and trigram rule
Markov models. For each, we try different values of
D1 and D2, the discount for bigrams and trigrams,
respectively. Ney et al (1994) suggest using the fol-
lowing value for the discount Dn:
Dn =
n1
n1 + n2
(4)
857
rule id translation rule
r1 IP(x1:NP x2:VP) ? x1 x2
r2 NP(Bu`sh??) ? Bush
r3 VP(x1:PP x2:VP) ? x2 x1
r4 PP(x1:P x2:NP) ? x1 x2
r5 VP(VV(ju?x??ng) AS(le) NPB(hu?`ta?n)) ? held talks
r6 P(yu?) ? with
r?6 P(yu?) ? and
r7 NP(Sha?lo?ng) ? Sharon
Figure 1: Example tree-to-string grammar.
derivation tree derived tree pair
 IP@ : IP@
r1
IP@
NP@1 VP@2 : NP
@1 VP@2
r1
r2 r3
IP@
NP@1
Bu`sh??
VP@2
PP@2.1 VP@2.2
: Bush VP@2.2 PP@2.1
r1
r2 r3
r4 r5
IP@
NP@1
Bu`sh??
VP@2
PP@2.1
P@2.1.1 NP@2.1.2
VP@2.2
VV
ju?x??ng
AS
le
NP
hu?`ta?n
: Bush held talks P@2.1.1 NP@2.1.2
r1
r2 r3
r4
r6 r7
r5
IP@
NP@1
Bu`sh??
VP@2
PP@2.1
P@2.1.1
yu?
NP@2.1.2
Sha?lo?ng
VP@2.2
VV
ju?x??ng
AS
le
NP
hu?`ta?n
: Bush held talks with Sharon
Figure 2: Example tree-to-string derivation. Each row shows a rewriting step; at each step, the leftmost nonterminal
symbol is rewritten using one of the rules in Figure 1.
858
Here, n1 and n2 are the total number of n-grams with
exactly one and two counts, respectively. For our
corpus, D1 = 0.871 and D2 = 0.902. Additionally,
we experiment with 0.4 and 0.5 for Dn.
Pruning In addition to full n-gram Markov mod-
els, we experiment with three approaches to build
smaller models to investigate if pruning helps. Our
results will show that smaller models indeed give a
higher Bleu score than the full bigram and trigram
models. The approaches we use are:
? RM-A: We keep only those contexts in which
more than P unique rules were observed. By
optimizing on the development set, we set P =
12.
? RM-B: We keep only those contexts that were
observed more than P times. Note that this is a
superset of RM-A. Again, by optimizing on the
development set, we set P = 12.
? RM-C: We try a more principled approach
for learning variable-length Markov models in-
spired by that of Bejerano and Yona (1999),
who learn a Prediction Suffix Tree (PST). They
grow the PST in an iterative manner by start-
ing from the root node (no context), and then
add contexts to the tree. A context is added if
the KL divergence between its predictive distri-
bution and that of its parent is above a certain
threshold and the probability of observing the
context is above another threshold.
3 Tree-to-string decoding with rule
Markov models
In this paper, we use our rule Markov model frame-
work in the context of tree-to-string translation.
Tree-to-string translation systems (Liu et al, 2006;
Huang et al, 2006) have gained popularity in recent
years due to their speed and simplicity. The input to
the translation system is a source parse tree and the
output is the target string. Huang and Mi (2010) have
recently introduced an efficient incremental decod-
ing algorithm for tree-to-string translation. The de-
coder operates top-down and maintains a derivation
history of translation rules encountered. The history
is exactly the vertical chain of ancestors correspond-
ing to the contexts in our rule Markov model. This
IP@
NP@1
Bu`sh??
VP@2
PP@2.1
P@2.1.1
yu?
NP@2.1.2
Sha?lo?ng
VP@2.2
VV@2.2.1
ju?x??ng
AS@2.2.2
le
NP@2.2.3
hu?`ta?n
Figure 3: Example input parse tree with tree addresses.
makes incremental decoding a natural fit with our
generative story. In this section, we describe how
to integrate our rule Markov model into this in-
cremental decoding algorithm. Note that it is also
possible to integrate our rule Markov model with
other decoding algorithms, for example, the more
common non-incremental top-down/bottom-up ap-
proach (Huang et al, 2006), but it would involve
a non-trivial change to the decoding algorithms to
keep track of the vertical derivation history, which
would result in significant overhead.
Algorithm Given the input parse tree in Figure 3,
Figure 4 illustrates the search process of the incre-
mental decoder with the grammar of Figure 1. We
write X@? for a tree node with label X at tree address
? (Shieber et al, 1995). The root node has address ,
and the ith child of node ? has address ?.i. At each
step, the decoder maintains a stack of active rules,
which are rules that have not been completed yet,
and the rightmost (n ? 1) English words translated
thus far (the hypothesis), where n is the order of the
word language model (in Figure 4, n = 2). The stack
together with the translated English words comprise
a state of the decoder. The last column in the fig-
ure shows the rule Markov model probabilities with
the conditioning context. In this example, we use a
trigram rule Markov model.
After initialization, the process starts at step 1,
where we predict rule r1 (the shaded rule) with prob-
ability P(r1 | ) and push its English side onto the
stack, with variables replaced by the correspond-
ing tree nodes: x1 becomes NP@1 and x2 becomes
VP@2. This gives us the following stack:
s = [ NP@1 VP@2]
The dot () indicates the next symbol to process in
859
stack hyp. MR prob.
0 [<s>  IP@ </s>] <s>
1 [<s>  IP@ </s>] [ NP@1 VP@2] <s> P(r1 | )
2 [<s>  IP@ </s>] [ NP@1 VP@2] [ Bush] <s> P(r2 | r1)
3 [<s>  IP@ </s>] [ NP@1 VP@2] [Bush  ] . . . Bush
4 [<s>  IP@ </s>] [NP@1  VP@2] . . . Bush
5 [<s>  IP@ </s>] [NP@1  VP@2] [ VP@2.2 PP@2.1] . . . Bush P(r3 | r1)
6 [<s>  IP@ </s>] [NP@1  VP@2] [ VP@2.2 PP@2.1] [ held talks] . . . Bush P(r5 | r1, r3)
7 [<s>  IP@ </s>] [NP@1  VP@2] [ VP@2.2 PP@2.1] [ held  talks] . . . held
8 [<s>  IP@ </s>] [NP@1  VP@2] [ VP@2.2 PP@2.1] [ held talks  ] . . . talks
9 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] . . . talks
10 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] . . . talks P(r4 | r1, r3)
11 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] [ with] . . . with P(r6 | r3, r4)
12 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] [with  ] . . . with
13 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] . . . with
14 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] [ Sharon] . . . with P(r7 | r3, r4)
11? [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] [ and] . . . and P(r?6 | r3, r4)
12? [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] [and  ] . . . and
13? [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] . . . and
14? [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] [ Sharon] . . . and P(r7 | r3, r4)
15 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] [Sharon  ] . . . Sharon
16 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1 NP@2.1.2  ] . . . Sharon
17 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2 PP@2.1  ] . . . Sharon
18 [<s>  IP@ </s>] [NP@1 VP@2  ] . . . Sharon
19 [<s> IP@  </s>] . . . Sharon
20 [<s> IP@ </s>  ] . . . </s>
Figure 4: Simulation of incremental decoding with rule Markov model. The solid arrows indicate one path and the
dashed arrows indicate an alternate path.
860
VP@2
VP@2.2 PP@2.1
P@2.1.1
yu?
NP@2.1.2
Figure 5: Vertical context r3 r4 which allows the model
to correctly translate yu? as with.
the English word order. We expand node NP@1 first
with English word order. We then predict lexical rule
r2 with probability P(r2 | r1) and push rule r2 onto
the stack:
[ NP@1 VP@2 ] [ Bush]
In step 3, we perform a scan operation, in which
we append the English word just after the dot to the
current hypothesis and move the dot after the word.
Since the dot is at the end of the top rule in the stack,
we perform a complete operation in step 4 where we
pop the finished rule at the top of the stack. In the
scan and complete steps, we don?t need to compute
rule probabilities.
An interesting branch occurs after step 10 with
two competing lexical rules, r6 and r?6. The Chinese
word yu? can be translated as either a preposition with
(leading to step 11) or a conjunction and (leading
to step 11?). The word n-gram model does not have
enough information to make the correct choice, with.
As a result, good translations might be pruned be-
cause of the beam. However, our rule Markov model
has the correct preference because of the condition-
ing ancestral sequence (r3, r4), shown in Figure 5.
Since VP@2.2 has a preference for yu? translating to
with, our corpus statistics will give a higher proba-
bility to P(r6 | r3, r4) than P(r?6 | r3, r4). This helps
the decoder to score the correct translation higher.
Complexity analysis With the incremental decod-
ing algorithm, adding rule Markov models does not
change the time complexity, which is O(nc|V |g?1),
where n is the sentence length, c is the maximum
number of incoming hyperedges for each node in the
translation forest, V is the target-language vocabu-
lary, and g is the order of the n-gram language model
(Huang and Mi, 2010). However, if one were to use
rule Markov models with a conventional CKY-style
bottom-up decoder (Liu et al, 2006), the complexity
would increase to O(nCm?1|V |4(g?1)), where C is the
maximum number of outgoing hyperedges for each
node in the translation forest, and m is the order of
the rule Markov model.
4 Experiments and results
4.1 Setup
The training corpus consists of 1.5M sentence pairs
with 38M/32M words of Chinese/English, respec-
tively. Our development set is the newswire portion
of the 2006 NIST MT Evaluation test set (616 sen-
tences), and our test set is the newswire portion of
the 2008 NIST MT Evaluation test set (691 sen-
tences).
We word-aligned the training data using GIZA++
followed by link deletion (Fossum et al, 2008),
and then parsed the Chinese sentences using the
Berkeley parser (Petrov and Klein, 2007). To extract
tree-to-string translation rules, we applied the algo-
rithm of Galley et al (2004). We trained our rule
Markov model on derivations of minimal rules as
described above. Our trigram word language model
was trained on the target side of the training cor-
pus using the SRILM toolkit (Stolcke, 2002) with
modified Kneser-Ney smoothing. The base feature
set for all systems is similar to the set used in Mi et
al. (2008). The features are combined into a standard
log-linear model, which we trained using minimum
error-rate training (Och, 2003) to maximize the Bleu
score on the development set.
At decoding time, we again parse the input
sentences using the Berkeley parser, and convert
them into translation forests using rule pattern-
matching (Mi et al, 2008). We evaluate translation
quality using case-insensitive IBM Bleu-4, calcu-
lated by the script mteval-v13a.pl.
4.2 Results
Table 1 presents the main results of our paper. We
used grammars of minimal rules and composed rules
of maximum height 3 as our baselines. For decod-
ing, we used a beam size of 50. Using the best
bigram rule Markov models and the minimal rule
grammar gives us an improvement of 1.5 Bleu over
the minimal rule baseline. Using the best trigram
rule Markov model brings our gain up to 2.3 Bleu.
861
grammar rule Markov max parameters (?10
6) Bleu time
model rule height full dev+test test (sec/sent)
minimal None 3 4.9 0.3 24.2 1.2
RM-B bigram 3 4.9+4.7 0.3+0.5 25.7 1.8
RM-A trigram 3 4.9+7.6 0.3+0.6 26.5 2.0
vertical composed None 7 176.8 1.3 26.5 2.9
composed None 3 17.5 1.6 26.4 2.2
None 7 448.7 3.3 27.5 6.8
RM-A trigram 7 448.7+7.6 3.3+1.0 28.0 9.2
Table 1: Main results. Our trigram rule Markov model strongly outperforms minimal rules, and performs at the same
level as composed and vertically composed rules, but is smaller and faster. The number of parameters is shown for
both the full model and the model filtered for the concatenation of the development and test sets (dev+test).
These gains are statistically significant with p <
0.01, using bootstrap resampling with 1000 samples
(Koehn, 2004). We find that by just using bigram
context, we are able to get at least 1 Bleu point
higher than the minimal rule grammar. It is interest-
ing to see that using just bigram rule interactions can
give us a reasonable boost. We get our highest gains
from using trigram context where our best perform-
ing rule Markov model gives us 2.3 Bleu points over
minimal rules. This suggests that using longer con-
texts helps the decoder to find better translations.
We also compared rule Markov models against
composed rules. Since our models are currently lim-
ited to conditioning on vertical context, the closest
comparison is against vertically composed rules. We
find that our approach performs equally well using
much less time and space.
Comparing against full composed rules, we find
that our system matches the score of the base-
line composed rule grammar of maximum height 3,
while using many fewer parameters. (It should be
noted that a parameter in the rule Markov model is
just a floating-point number, whereas a parameter in
the composed-rule system is an entire rule; there-
fore the difference in memory usage would be even
greater.) Decoding with our model is 0.2 seconds
faster per sentence than with composed rules.
These experiments clearly show that rule Markov
models with minimal rules increase translation qual-
ity significantly and with lower memory require-
ments than composed rules. One might wonder if
the best performance can be obtained by combin-
ing composed rules with a rule Markov model. This
rule Markov D1
Bleu time
model dev (sec/sent)
RM-A 0.871 29.2 1.8
RM-B 0.4 29.9 1.8
RM-C 0.871 29.8 1.8
RM-Full 0.4 29.7 1.9
Table 2: For rule bigrams, RM-B with D1 = 0.4 gives the
best results on the development set.
rule Markov D1 D2
Bleu time
model dev (sec/sent)
RM-A 0.5 0.5 30.3 2.0
RM-B 0.5 0.5 29.9 2.0
RM-C 0.5 0.5 30.1 2.0
RM-Full 0.4 0.5 30.1 2.2
Table 3: For rule bigrams, RM-A with D1, D2 = 0.5 gives
the best results on the development set.
is straightforward to implement: the rule Markov
model is still defined over derivations of minimal
rules, but in the decoder?s prediction step, the rule
Markov model?s value on a composed rule is cal-
culated by decomposing it into minimal rules and
computing the product of their probabilities. We find
that using our best trigram rule Markov model with
composed rules gives us a 0.5 Bleu gain on top of
the composed rule grammar, statistically significant
with p < 0.05, achieving our highest score of 28.0.1
4.3 Analysis
Tables 2 and 3 show how the various types of rule
Markov models compare, for bigrams and trigrams,
1For this experiment, a beam size of 100 was used.
862
parameters (?106) Bleu dev/test time (sec/sent)
dev/test without RMM with RMM without/with RMM
2.6 31.0/27.0 31.1/27.4 4.5/7.0
2.9 31.5/27.7 31.4/27.3 5.6/8.1
3.3 31.4/27.5 31.4/28.0 6.8/9.2
Table 6: Adding rule Markov models to composed-rule grammars improves their translation performance.
D2
D1
0.4 0.5 0.871
0.4 30.0 30.0
0.5 29.3 30.3
0.902 30.0
Table 4: RM-A is robust to different settings of Dn on the
development set.
parameters (?106) Bleu time
dev+test dev test (sec/sent)
1.2 30.2 26.1 2.8
1.3 30.1 26.5 2.9
1.3 30.1 26.2 3.2
Table 5: Comparison of vertically composed rules using
various settings (maximum rule height 7).
respectively. It is interesting that the full bigram and
trigram rule Markov models do not give our high-
est Bleu scores; pruning the models not only saves
space but improves their performance. We think that
this is probably due to overfitting.
Table 4 shows that the RM-A trigram model does
fairly well under all the settings of Dn we tried. Ta-
ble 5 shows the performance of vertically composed
rules at various settings. Here we have chosen the
setting that gives the best performance on the test
set for inclusion in Table 1.
Table 6 shows the performance of fully composed
rules and fully composed rules with a rule Markov
Model at various settings.2 In the second line (2.9
million rules), the drop in Bleu score resulting from
adding the rule Markov model is not statistically sig-
nificant.
5 Related Work
Besides the Quirk and Menezes (2006) work dis-
cussed in Section 1, there are two other previous
2For these experiments, a beam size of 100 was used.
efforts both using a rule bigram model in machine
translation, that is, the probability of the current rule
only depends on the immediate previous rule in the
vertical context, whereas our rule Markov model
can condition on longer and sparser derivation his-
tories. Among them, Ding and Palmer (2005) also
use a dependency treelet model similar to Quirk and
Menezes (2006), and Liu and Gildea (2008) use a
tree-to-string model more like ours. Neither com-
pared to the scenario with composed rules.
Outside of machine translation, the idea of weak-
ening independence assumptions by modeling the
derivation history is also found in parsing (Johnson,
1998), where rule probabilities are conditioned on
parent and grand-parent nonterminals. However, be-
sides the difference between parsing and translation,
there are still two major differences. First, our work
conditions rule probabilities on parent and grandpar-
ent rules, not just nonterminals. Second, we com-
pare against a composed-rule system, which is anal-
ogous to the Data Oriented Parsing (DOP) approach
in parsing (Bod, 2003). To our knowledge, there has
been no direct comparison between a history-based
PCFG approach and DOP approach in the parsing
literature.
6 Conclusion
In this paper, we have investigated whether we can
eliminate composed rules without any loss in trans-
lation quality. We have developed a rule Markov
model that captures vertical bigrams and trigrams of
minimal rules, and tested it in the framework of tree-
to-string translation. We draw three main conclu-
sions from our experiments. First, our rule Markov
models dramatically improve a grammar of minimal
rules, giving an improvement of 2.3 Bleu. Second,
when we compare against vertically composed rules
we are able to get about the same Bleu score, but
our model is much smaller and decoding with our
863
model is faster. Finally, when we compare against
full composed rules, we find that we can reach the
same level of performance under some conditions,
but in order to do so consistently, we believe we
need to extend our model to condition on horizon-
tal context in addition to vertical context. We hope
that by modeling context in both axes, we will be
able to completely replace composed-rule grammars
with smaller minimal-rule grammars.
Acknowledgments
We would like to thank Fernando Pereira, Yoav
Goldberg, Michael Pust, Steve DeNeefe, Daniel
Marcu and Kevin Knight for their comments.
Mi?s contribution was made while he was vis-
iting USC/ISI. This work was supported in part
by DARPA under contracts HR0011-06-C-0022
(subcontract to BBN Technologies), HR0011-09-1-
0028, and DOI-NBC N10AP20031, by a Google
Faculty Research Award to Huang, and by the Na-
tional Natural Science Foundation of China under
contracts 60736014 and 90920004.
References
Gill Bejerano and Golan Yona. 1999. Modeling pro-
tein families using probabilistic suffix trees. In Proc.
RECOMB, pages 15?24. ACM Press.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proceedings of EACL, pages 19?26.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probablisitic synchronous dependency in-
sertion grammars. In Proceedings of ACL, pages 541?
548.
Victoria Fossum, Kevin Knight, and Steve Abney. 2008.
Using syntax to improve word alignment precision for
syntax-based machine translation. In Proceedings of
the Workshop on Statistical Machine Translation.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT-NAACL, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL, pages 961?968.
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of EMNLP, pages 273?283.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA, pages
66?73.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24:613?
632.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388?395.
Ding Liu and Daniel Gildea. 2008. Improved tree-to-
string transducer for machine translation. In Proceed-
ings of the Workshop on Statistical Machine Transla-
tion, pages 62?69.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL, pages 609?
616.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL: HLT, pages
192?199.
H. Ney, U. Essen, and R. Kneser. 1994. On structur-
ing probabilistic dependencies in stochastic language
modelling. Computer Speech and Language, 8:1?38.
Franz Joseph Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL, pages 404?411.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? Challenging the conventional wisdom in sta-
tistical machine translation. In Proceedings of NAACL
HLT, pages 9?16.
Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24:3?36.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP, vol-
ume 30, pages 901?904.
864
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 323?328,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Models and Training for Unsupervised Preposition Sense Disambiguation
Dirk Hovy and Ashish Vaswani and Stephen Tratz and
David Chiang and Eduard Hovy
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
{dirkh,avaswani,stratz,chiang,hovy}@isi.edu
Abstract
We present a preliminary study on unsu-
pervised preposition sense disambiguation
(PSD), comparing different models and train-
ing techniques (EM, MAP-EM with L0 norm,
Bayesian inference using Gibbs sampling). To
our knowledge, this is the first attempt at un-
supervised preposition sense disambiguation.
Our best accuracy reaches 56%, a significant
improvement (at p <.001) of 16% over the
most-frequent-sense baseline.
1 Introduction
Reliable disambiguation of words plays an impor-
tant role in many NLP applications. Prepositions
are ubiquitous?they account for more than 10% of
the 1.16m words in the Brown corpus?and highly
ambiguous. The Preposition Project (Litkowski and
Hargraves, 2005) lists an average of 9.76 senses
for each of the 34 most frequent English preposi-
tions, while nouns usually have around two (Word-
Net nouns average about 1.2 senses, 2.7 if monose-
mous nouns are excluded (Fellbaum, 1998)). Dis-
ambiguating prepositions is thus a challenging and
interesting task in itself (as exemplified by the Sem-
Eval 2007 task, (Litkowski and Hargraves, 2007)),
and holds promise for NLP applications such as
Information Extraction or Machine Translation.1
Given a sentence such as the following:
In the morning, he shopped in Rome
we ultimately want to be able to annotate it as
1See (Chan et al, 2007) for how using WSD can help MT.
in/TEMPORAL the morning/TIME he/PERSON
shopped/SOCIAL in/LOCATIVE
Rome/LOCATION
Here, the preposition in has two distinct meanings,
namely a temporal and a locative one. These mean-
ings are context-dependent. Ultimately, we want
to disambiguate prepositions not by and for them-
selves, but in the context of sequential semantic la-
beling. This should also improve disambiguation of
the words linked by the prepositions (here, morn-
ing, shopped, and Rome). We propose using un-
supervised methods in order to leverage unlabeled
data, since, to our knowledge, there are no annotated
data sets that include both preposition and argument
senses. In this paper, we present our unsupervised
framework and show results for preposition disam-
biguation. We hope to present results for the joint
disambiguation of preposition and arguments in a
future paper.
The results from this work can be incorporated
into a number of NLP problems, such as seman-
tic tagging, which tries to assign not only syntac-
tic, but also semantic categories to unlabeled text.
Knowledge about semantic constraints of preposi-
tional constructions would not only provide better
label accuracy, but also aid in resolving preposi-
tional attachment problems. Learning by Reading
approaches (Mulkar-Mehta et al, 2010) also cru-
cially depend on unsupervised techniques as the
ones described here for textual enrichment.
Our contributions are:
? we present the first unsupervised preposition
sense disambiguation (PSD) system
323
? we compare the effectiveness of various models
and unsupervised training methods
? we present ways to extend this work to prepo-
sitional arguments
2 Preliminaries
A preposition p acts as a link between two words, h
and o. The head word h (a noun, adjective, or verb)
governs the preposition. In our example above, the
head word is shopped. The object of the preposi-
tional phrase (usually a noun) is denoted o, in our
example morning and Rome. We will refer to h and
o collectively as the prepositional arguments. The
triple h, p, o forms a syntactically and semantically
constrained structure. This structure is reflected in
dependency parses as a common construction. In
our example sentence above, the respective struc-
tures would be shopped in morning and shopped in
Rome. The senses of each element are denoted by a
barred letter, i.e., p? denotes the preposition sense, h?
denotes the sense of the head word, and o? the sense
of the object.
3 Data
We use the data set for the SemEval 2007 PSD
task, which consists of a training (16k) and a test
set (8k) of sentences with sense-annotated preposi-
tions following the sense inventory of The Preposi-
tion Project, TPP (Litkowski and Hargraves, 2005).
It defines senses for each of the 34 most frequent
prepositions. There are on average 9.76 senses per
preposition. This corpus was chosen as a starting
point for our study since it allows a comparison with
the original SemEval task. We plan to use larger
amounts of additional training data.
We used an in-house dependency parser to extract
the prepositional constructions from the data (e.g.,
?shop/VB in/IN Rome/NNP?). Pronouns and num-
bers are collapsed into ?PRO? and ?NUM?, respec-
tively.
In order to constrain the argument senses, we con-
struct a dictionary that lists for each word all the
possible lexicographer senses according to Word-
Net. The set of lexicographer senses (45) is a higher
level abstraction which is sufficiently coarse to allow
for a good generalization. Unknown words are as-
sumed to have all possible senses applicable to their
respective word class (i.e. all noun senses for words
labeled as nouns, etc).
4 Graphical Model
ph o
p?h? o?
h o
p?h? o?
h o
p?h? o?
a)
b)
c)
Figure 1: Graphical Models. a) 1st order HMM. b)
variant used in experiments (one model/preposition,
thus no conditioning on p). c) incorporates further
constraints on variables
As shown by Hovy et al (2010), preposition
senses can be accurately disambiguated using only
the head word and object of the PP. We exploit this
property of prepositional constructions to represent
the constraints between h, p, and o in a graphical
model. We define a good model as one that reason-
ably constrains the choices, but is still tractable in
terms of the number of parameters being estimated.
As a starting point, we choose the standard first-
order Hidden Markov Model as depicted in Figure
1a. Since we train a separate model for each preposi-
tion, we can omit all arcs to p. This results in model
1b. The joint distribution over the network can thus
be written as
Pp(h, o, h?, p?, o?) = P (h?) ? P (h|h?) ? (1)
P (p?|h?) ? P (o?|p?) ? P (o|o?)
We want to incorporate as much information as
possible into the model to constrain the choices. In
Figure 1c, we condition p? on both h? and o?, to reflect
the fact that prepositions act as links and determine
324
their sense mainly through context. In order to con-
strain the object sense o?, we condition on h?, similar
to a second-order HMM. The actual object o is con-
ditioned on both p? and o?. The joint distribution is
equal to
Pp(h, o, h?, p?, o?) = P (h?) ? P (h|h?) ? (2)
P (o?|h?) ? P (p?|h?, o?) ? P (o|o?, p?)
Though we would like to also condition the prepo-
sition sense p? on the head word h (i.e., an arc be-
tween them in 1c) in order to capture idioms and
fixed phrases, this would increase the number of pa-
rameters prohibitively.
5 Training
The training method largely determines how well the
resulting model explains the data. Ideally, the sense
distribution found by the model matches the real
one. Since most linguistic distributions are Zipfian,
we want a training method that encourages sparsity
in the model.
We briefly introduce different unsupervised train-
ing methods and discuss their respective advantages
and disadvantages. Unless specified otherwise, we
initialized all models uniformly, and trained until the
perplexity rate stopped increasing or a predefined
number of iterations was reached. Note that MAP-
EM and Bayesian Inference require tuning of some
hyper-parameters on held-out data, and are thus not
fully unsupervised.
5.1 EM
We use the EM algorithm (Dempster et al, 1977) as
a baseline. It is relatively easy to implement with ex-
isting toolkits like Carmel (Graehl, 1997). However,
EM has a tendency to assume equal importance for
each parameter. It thus prefers ?general? solutions,
assigning part of the probability mass to unlikely
states (Johnson, 2007). We ran EM on each model
for 100 iterations, or until the perplexity stopped de-
creasing below a threshold of 10?6.
5.2 EM with Smoothing and Restarts
In addition to the baseline, we ran 100 restarts with
random initialization and smoothed the fractional
counts by adding 0.1 before normalizing (Eisner,
2002). Smoothing helps to prevent overfitting. Re-
peated random restarts help escape unfavorable ini-
tializations that lead to local maxima. Carmel pro-
vides options for both smoothing and restarts.
5.3 MAP-EM with L0 Norm
Since we want to encourage sparsity in our mod-
els, we use the MDL-inspired technique intro-
duced by Vaswani et al (2010). Here, the goal
is to increase the data likelihood while keeping
the number of parameters small. The authors use
a smoothed L0 prior, which encourages probabil-
ities to go down to 0. The prior involves hyper-
parameters ?, which rewards sparsity, and ?, which
controls how close the approximation is to the true
L0 norm.2 We perform a grid search to tune the
hyper-parameters of the smoothed L0 prior for ac-
curacy on the preposition against, since it has a
medium number of senses and instances. For HMM,
we set ?trans =100.0, ?trans =0.005, ?emit =1.0,
?emit =0.75. The subscripts trans and emit de-
note the transition and emission parameters. For
our model, we set ?trans =70.0, ?trans =0.05,
?emit =110.0, ?emit =0.0025. The latter resulted
in the best accuracy we achieved.
5.4 Bayesian Inference
Instead of EM, we can use Bayesian inference with
Gibbs sampling and Dirichlet priors (also known as
the Chinese Restaurant Process, CRP). We follow
the approach of Chiang et al (2010), running Gibbs
sampling for 10,000 iterations, with a burn-in pe-
riod of 5,000, and carry out automatic run selec-
tion over 10 random restarts.3 Again, we tuned the
hyper-parameters of our Dirichlet priors for accu-
racy via a grid search over the model for the prepo-
sition against. For both models, we set the concen-
tration parameter ?trans to 0.001, and ?emit to 0.1.
This encourages sparsity in the model and allows for
a more nuanced explanation of the data by shifting
probability mass to the few prominent classes.
2For more details, the reader is referred to Vaswani et al
(2010).
3Due to time and space constraints, we did not run the 1000
restarts used in Chiang et al (2010).
325
result table
Page 1
HMM
0.40 (0.40)
0.42 (0.42) 0.55 (0.55) 0.45 (0.45) 0.53 (0.53)
0.41 (0.41) 0.49 (0.49) 0.55 (0.56) 0.48 (0.49)
baseline Vanilla EM
EM, smoothed, 
100 random 
restarts
MAP-EM + 
smoothed L0 
norm
CRP, 10 random 
restarts
our model
Table 1: Accuracy over all prepositions w. different models and training. Best accuracy: MAP-
EM+smoothed L0 norm on our model. Italics denote significant improvement over baseline at p <.001.
Numbers in brackets include against (used to tune MAP-EM and Bayesian Inference hyper-parameters)
6 Results
Given a sequence h, p, o, we want to find the se-
quence of senses h?, p?, o? that maximizes the joint
probability. Since unsupervised methods use the
provided labels indiscriminately, we have to map the
resulting predictions to the gold labels. The pre-
dicted label sequence h?, p?, o? generated by the model
via Viterbi decoding can then be compared to the
true key. We use many-to-1 mapping as described
by Johnson (2007) and used in other unsupervised
tasks (Berg-Kirkpatrick et al, 2010), where each
predicted sense is mapped to the gold label it most
frequently occurs with in the test data. Success is
measured by the percentage of accurate predictions.
Here, we only evaluate p?.
The results presented in Table 1 were obtained
on the SemEval test set. We report results both
with and without against, since we tuned the hyper-
parameters of two training methods on this preposi-
tion. To test for significance, we use a two-tailed
t-test, comparing the number of correctly labeled
prepositions. As a baseline, we simply label all word
types with the same sense, i.e., each preposition to-
ken is labeled with its respective name. When using
many-to-1 accuracy, this technique is equivalent to a
most-frequent-sense baseline.
Vanilla EM does not improve significantly over
the baseline with either model, all other methods
do. Adding smoothing and random restarts increases
the gain considerably, illustrating how important
these techniques are for unsupervised training. We
note that EM performs better with the less complex
HMM.
CRP is somewhat surprisingly roughly equivalent
to EM with smoothing and random restarts. Accu-
racy might improve with more restarts.
MAP-EM with L0 normalization produces the
best result (56%), significantly outperforming the
baseline at p < .001. With more parameters (9.7k
vs. 3.7k), which allow for a better modeling of
the data, L0 normalization helps by zeroing out in-
frequent ones. However, the difference between
our complex model and the best HMM (EM with
smoothing and random restarts, 55%) is not signifi-
cant.
The best (supervised) system in the SemEval task
(Ye and Baldwin, 2007) reached 69% accuracy. The
best current supervised system we are aware of
(Hovy et al, 2010) reaches 84.8%.
7 Related Work
The semantics of prepositions were topic of a special
issue of Computational Linguistics (Baldwin et al,
2009). Preposition sense disambiguation was one of
the SemEval 2007 tasks (Litkowski and Hargraves,
2007), and was subsequently explored in a number
of papers using supervised approaches: O?Hara and
Wiebe (2009) present a supervised preposition sense
disambiguation approach which explores different
settings; Tratz and Hovy (2009), Hovy et al (2010)
make explicit use of the arguments for preposition
sense disambiguation, using various features. We
differ from these approaches by using unsupervised
methods and including argument labeling.
The constraints of prepositional constructions
have been explored by Rudzicz and Mokhov (2003)
and O?Hara and Wiebe (2003) to annotate the se-
mantic role of complete PPs with FrameNet and
Penn Treebank categories. Ye and Baldwin (2006)
explore the constraints of prepositional phrases for
326
semantic role labeling. We plan to use the con-
straints for argument disambiguation.
8 Conclusion and Future Work
We evaluate the influence of two different models (to
represent constraints) and three unsupervised train-
ing methods (to achieve sparse sense distributions)
on PSD. Using MAP-EM with L0 norm on our
model, we achieve an accuracy of 56%. This is a
significant improvement (at p <.001) over the base-
line and vanilla EM. We hope to shorten the gap to
supervised systems with more unlabeled data. We
also plan on training our models with EM with fea-
tures (Berg-Kirkpatrick et al, 2010).
The advantage of our approach is that the models
can be used to infer the senses of the prepositional
arguments as well as the preposition. We are cur-
rently annotating the data to produce a test set with
Amazon?s Mechanical Turk, in order to measure la-
bel accuracy for the preposition arguments.
Acknowledgements
We would like to thank Steve DeNeefe, Jonathan
Graehl, Victoria Fossum, and Kevin Knight, as well
as the anonymous reviewers for helpful comments
on how to improve the paper. We would also like
to thank Morgan from Curious Palate for letting us
write there. Research supported in part by Air Force
Contract FA8750-09-C-0172 under the DARPA Ma-
chine Reading Program and by DARPA under con-
tract DOI-NBC N10AP20031.
References
Tim Baldwin, Valia Kordoni, and Aline Villavicencio.
2009. Prepositions in applications: A survey and in-
troduction to the special issue. Computational Lin-
guistics, 35(2):119?149.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless Unsu-
pervised Learning with Features. In North American
Chapter of the Association for Computational Linguis-
tics.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Annual Meeting ? Association
For Computational Linguistics, volume 45, pages 33?
40.
David Chiang, Jonathan Graehl, Kevin Knight, Adam
Pauls, and Sujith Ravi. 2010. Bayesian inference
for Finite-State transducers. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 447?455. Association for
Computational Linguistics.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), 39(1):1?38.
Jason Eisner. 2002. An interactive spreadsheet for teach-
ing the forward-backward algorithm. In Proceed-
ings of the ACL-02 Workshop on Effective tools and
methodologies for teaching natural language process-
ing and computational linguistics-Volume 1, pages 10?
18. Association for Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database. MIT Press USA.
Jonathan Graehl. 1997. Carmel Finite-state Toolkit.
ISI/USC.
Dirk Hovy, Stephen Tratz, and Eduard Hovy. 2010.
What?s in a Preposition? Dimensions of Sense Dis-
ambiguation for an Interesting Word Class. In Coling
2010: Posters, pages 454?462, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 296?305.
Ken Litkowski and Orin Hargraves. 2005. The prepo-
sition project. ACL-SIGSEM Workshop on ?The Lin-
guistic Dimensions of Prepositions and Their Use in
Computational Linguistic Formalisms and Applica-
tions?, pages 171?179.
Ken Litkowski and Orin Hargraves. 2007. SemEval-
2007 Task 06: Word-Sense Disambiguation of Prepo-
sitions. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
Rutu Mulkar-Mehta, James Allen, Jerry Hobbs, Eduard
Hovy, Bernardo Magnini, and Christopher Manning,
editors. 2010. Proceedings of the NAACL HLT
2010 First International Workshop on Formalisms and
Methodology for Learning by Reading. Association
for Computational Linguistics, Los Angeles, Califor-
nia, June.
Tom O?Hara and Janyce Wiebe. 2003. Preposi-
tion semantic classification via Penn Treebank and
FrameNet. In Proceedings of CoNLL, pages 79?86.
Tom O?Hara and Janyce Wiebe. 2009. Exploiting se-
mantic role resources for preposition disambiguation.
Computational Linguistics, 35(2):151?184.
327
Frank Rudzicz and Serguei A. Mokhov. 2003. Towards
a heuristic categorization of prepositional phrases in
english with wordnet. Technical report, Cornell
University, arxiv1.library.cornell.edu/abs/1002.1095-
?context=cs.
Stephen Tratz and Dirk Hovy. 2009. Disambiguation of
Preposition Sense Using Linguistically Motivated Fea-
tures. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, Companion Volume: Student Research Work-
shop and Doctoral Consortium, pages 96?100, Boul-
der, Colorado, June. Association for Computational
Linguistics.
Ashish Vaswani, Adam Pauls, and David Chiang. 2010.
Efficient optimization of an MDL-inspired objective
function for unsupervised part-of-speech tagging. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, pages 209?214. Association for Computational
Linguistics.
Patrick Ye and Tim Baldwin. 2006. Semantic role la-
beling of prepositional phrases. ACM Transactions
on Asian Language Information Processing (TALIP),
5(3):228?244.
Patrick Ye and Timothy Baldwin. 2007. MELB-YB:
Preposition Sense Disambiguation Using Rich Seman-
tic Features. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
328
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 311?319,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Smaller Alignment Models for Better Translations:
Unsupervised Word Alignment with the `0-norm
Ashish Vaswani Liang Huang David Chiang
University of Southern California
Information Sciences Institute
{avaswani,lhuang,chiang}@isi.edu
Abstract
Two decades after their invention, the IBM
word-based translation models, widely avail-
able in the GIZA++ toolkit, remain the dom-
inant approach to word alignment and an in-
tegral part of many statistical translation sys-
tems. Although many models have surpassed
them in accuracy, none have supplanted them
in practice. In this paper, we propose a simple
extension to the IBM models: an `0 prior to en-
courage sparsity in the word-to-word transla-
tion model. We explain how to implement this
extension efficiently for large-scale data (also
released as a modification to GIZA++) and
demonstrate, in experiments on Czech, Ara-
bic, Chinese, and Urdu to English translation,
significant improvements over IBM Model 4
in both word alignment (up to +6.7 F1) and
translation quality (up to +1.4 Bleu).
1 Introduction
Automatic word alignment is a vital component of
nearly all current statistical translation pipelines. Al-
though state-of-the-art translation models use rules
that operate on units bigger than words (like phrases
or tree fragments), they nearly always use word
alignments to drive extraction of those translation
rules. The dominant approach to word alignment has
been the IBM models (Brown et al, 1993) together
with the HMM model (Vogel et al, 1996). These
models are unsupervised, making them applicable
to any language pair for which parallel text is avail-
able. Moreover, they are widely disseminated in the
open-source GIZA++ toolkit (Och and Ney, 2004).
These properties make them the default choice for
most statistical MT systems.
In the decades since their invention, many mod-
els have surpassed them in accuracy, but none has
supplanted them in practice. Some of these models
are partially supervised, combining unlabeled paral-
lel text with manually-aligned parallel text (Moore,
2005; Taskar et al, 2005; Riesa and Marcu, 2010).
Although manually-aligned data is very valuable, it
is only available for a small number of language
pairs. Other models are unsupervised like the IBM
models (Liang et al, 2006; Grac?a et al, 2010; Dyer
et al, 2011), but have not been as widely adopted as
GIZA++ has.
In this paper, we propose a simple extension to
the IBM/HMM models that is unsupervised like the
IBM models, is as scalable as GIZA++ because it is
implemented on top of GIZA++, and provides sig-
nificant improvements in both alignment and trans-
lation quality. It extends the IBM/HMM models by
incorporating an `0 prior, inspired by the princi-
ple of minimum description length (Barron et al,
1998), to encourage sparsity in the word-to-word
translation model (Section 2.2). This extension fol-
lows our previous work on unsupervised part-of-
speech tagging (Vaswani et al, 2010), but enables
it to scale to the large datasets typical in word
alignment, using an efficient training method based
on projected gradient descent (Section 2.3). Ex-
periments on Czech-, Arabic-, Chinese- and Urdu-
English translation (Section 3) demonstrate consis-
tent significant improvements over IBM Model 4 in
both word alignment (up to +6.7 F1) and transla-
tion quality (up to +1.4 Bleu). Our implementation
has been released as a simple modification to the
GIZA++ toolkit that can be used as a drop-in re-
placement for GIZA++ in any existing MT pipeline.
311
2 Method
We start with a brief review of the IBM and HMM
word alignment models, then describe how to extend
them with a smoothed `0 prior and how to efficiently
train them.
2.1 IBM Models and HMM
Given a French string f = f1 ? ? ? f j ? ? ? fm and an
English string e = e1 ? ? ? ei ? ? ? e`, these models de-
scribe the process by which the French string is
generated by the English string via the alignment
a = a1, . . . , a j, . . . , am. Each a j is a hidden vari-
ables, indicating which English word ea j the French
word f j is aligned to.
In IBM Model 1?2 and the HMM model, the joint
probability of the French sentence and alignment
given the English sentence is
P(f, a | e) =
m?
j=1
d(a j | a j?1, j)t( f j | ea j). (1)
The parameters of these models are the distortion
probabilities d(a j | a j?1, j) and the translation prob-
abilities t( f j | ea j). The three models differ in their
estimation of d, but the differences do not concern us
here. All three models, as well as IBM Models 3?5,
share the same t. For further details of these models,
the reader is referred to the original papers describ-
ing them (Brown et al, 1993; Vogel et al, 1996).
Let ? stand for all the parameters of the model.
The standard training procedure is to find the param-
eter values that maximize the likelihood, or, equiv-
alently, minimize the negative log-likelihood of the
observed data:
?? = arg min
?
(
? log P(f | e, ?)
)
(2)
= arg min
?
?
??????? log
?
a
P(f, a | e, ?)
?
?????? (3)
This is done using the Expectation-Maximization
(EM) algorithm (Dempster et al, 1977).
2.2 MAP-EM with the `0-norm
Maximum likelihood training is prone to overfitting,
especially in models with many parameters. In word
alignment, one well-known manifestation of overfit-
ting is that rare words can act as ?garbage collectors?
(Moore, 2004), aligning to many unrelated words.
This hurts alignment precision and rule-extraction
recall. Previous attempted remedies include early
stopping, smoothing (Moore, 2004), and posterior
regularization (Grac?a et al, 2010).
We have previously proposed another simple
remedy to overfitting in the context of unsuper-
vised part-of-speech tagging (Vaswani et al, 2010),
which is to minimize the size of the model using a
smoothed `0 prior. Applying this prior to an HMM
improves tagging accuracy for both Italian and En-
glish.
Here, our goal is to apply a similar prior in a
word-alignment model to the word-to-word transla-
tion probabilities t( f | e). We leave the distortion
models alone, since they are not very large, and there
is not much reason to believe that we can profit from
compacting them.
With the addition of the `0 prior, the MAP (maxi-
mum a posteriori) objective function is
?? = arg min
?
(
? log P(f | e, ?)P(?)
)
(4)
where
P(?) ? exp
(
??????0
)
(5)
and
????0 =
?
e, f
(
1 ? exp
?t( f | e)
?
)
(6)
is a smoothed approximation of the `0-norm. The
hyperparameter ? controls the tightness of the ap-
proximation, as illustrated in Figure 1. Substituting
back into (4) and dropping constant terms, we get
the following optimization problem: minimize
? log P(f | e, ?) ? ?
?
e, f
exp
?t( f | e)
?
(7)
subject to the constraints
?
f
t( f | e) = 1 for all e. (8)
We can carry out the optimization in (7) with the
MAP-EM algorithm (Bishop, 2006). EM and MAP-
EM share the same E-step; the difference lies in the
312
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
Figure 1: The `0-norm (top curve) and smoothed approx-
imations (below) for ? = 0.05, 0.1, 0.2.
M-step. For vanilla EM, the M-step is:
?? = arg min
?
?
????????
?
?
e, f
E[C(e, f )] log t( f | e)
?
????????
(9)
again subject to the constraints (8). The count
C(e, f ) is the number of times that f occurs aligned
to e. For MAP-EM, it is:
?? = arg min
?
(
?
?
e, f
E[C(e, f )] log t( f | e) ?
?
?
e, f
exp
?t( f | e)
?
) (10)
This optimization problem is non-convex, and we
do not know of a closed-form solution. Previously
(Vaswani et al, 2010), we used ALGENCAN, a non-
linear optimization toolkit, but this solution does not
scale well to the number of parameters involved in
word alignment models. Instead, we use a simpler
and more scalable method which we describe in the
next section.
2.3 Projected gradient descent
Following Schoenemann (2011b), we use projected
gradient descent (PGD) to solve the M-step (but
with the `0-norm instead of the `1-norm). Gradient
projection methods are attractive solutions to con-
strained optimization problems, particularly when
the constraints on the parameters are simple (Bert-
sekas, 1999). Let F(?) be the objective function in
(10); we seek to minimize this function. As in pre-
vious work (Vaswani et al, 2010), we optimize each
set of parameters {t(? | e)} separately for each En-
glish word type e. The inputs to the PGD are the
expected counts E[C(e, f )] and the current word-to-
word conditional probabilities ?. We run PGD for K
iterations, producing a sequence of intermediate pa-
rameter vectors ?1, . . . , ?k, . . . , ?K . Each iteration has
two steps, a projection step and a line search.
Projection step In this step, we compute:
?
k
=
[
?k ? s?F(?k)
]?
(11)
This moves ? in the direction of steepest descent
(?F) with step size s, and then the function [?]?
projects the resulting point onto the simplex; that
is, it finds the nearest point that satisfies the con-
straints (8).
The gradient ?F(?k) is
?F
?t( f | e)
= ?
E[C( f , e)]
t( f | e)
+
?
?
exp
?t( f | e)
?
(12)
In contrast to Schoenemann (2011b), we use an
O(n log n) algorithm for the projection step due to
Duchi et. al. (2008), shown in Pseudocode 1.
Pseudocode 1 Project input vector u ? Rn onto the
probability simplex.
v = u sorted in non-increasing order
? = 0
for i = 1 to n do
if vi ? 1i
(?i
r=1 vr ? 1
)
> 0 then
? = i
end if
end for
? = 1
?
(??
r=1 vr ? 1
)
wr = max{vr ? ?, 0} for 1 ? r ? n
return w
Line search Next, we move to a point between ?k
and ?
k
that satisfies the Armijo condition,
F(?k + ?m) ? F(?k) + ?
(
?F(?k) ? ?m
)
(13)
where ?m = ?m(?
k
? ?k) and ? and ? are both con-
stants in (0, 1). We try values m = 1, 2, . . . until the
Armijo condition (13) is satisfied or the limit m = 20
313
Pseudocode 2 Find a point between ?k and ?
k
that
satisfies the Armijo condition.
Fmin = F(?k)
?min = ?
k
for m = 1 to 20 do
?m = ?
m
(
?
k
? ?k
)
if F(?k + ?m) < Fmin then
Fmin = F(?k + ?m)
?min = ?
k + ?m
end if
if F(?k + ?m) ? F(?k) + ?
(
?F(?k) ? ?m
)
then
break
end if
end for
?k+1 = ?min
return ?k+1
is reached. (Note that we don?t allow m = 0 because
this can cause ?k + ?m to land on the boundary of
the probability simplex, where the objective func-
tion is undefined.) Then we set ?k+1 to the point in
{?k} ? {?k + ?m | 1 ? m ? 20} that minimizes F.
The line search algorithm is summarized in Pseu-
docode 2.
In our implementation, we set ? = 0.5 and ? =
0.5. We keep s fixed for all PGD iterations; we ex-
perimented with s ? {0.1, 0.5} and did not observe
significant changes in F-score. We run the projection
step and line search alternately for at most K itera-
tions, terminating early if there is no change in ?k
from one iteration to the next. We set K = 35 for the
large Arabic-English experiment; for all other con-
ditions, we set K = 50. These choices were made to
balance efficiency and accuracy. We found that val-
ues of K between 30 and 75 were generally reason-
able.
3 Experiments
To demonstrate the effect of the `0-norm on the IBM
models, we performed experiments on four trans-
lation tasks: Arabic-English, Chinese-English, and
Urdu-English from the NIST Open MT Evaluation,
and the Czech-English translation from the Work-
shop on Machine Translation (WMT) shared task.
We measured the accuracy of word alignments gen-
erated by GIZA++ with and without the `0-norm,
and also translation accuracy of systems trained us-
ing the word alignments. Across all tests, we found
strong improvements from adding the `0-norm.
3.1 Training
We have implemented our algorithm as an open-
source extension to GIZA++.1 Usage of the exten-
sion is identical to standard GIZA++, except that the
user can switch the `0 prior on or off, and adjust the
hyperparameters ? and ?.
For vanilla EM, we ran five iterations of Model 1,
five iterations of HMM, and ten iterations of
Model 4. For our approach, we first ran one iter-
ation of Model 1, followed by four iterations of
Model 1 with smoothed `0, followed by five itera-
tions of HMM with smoothed `0. Finally, we ran ten
iterations of Model 4.2
We used the following parallel data:
? Chinese-English: selected data from the con-
strained task of the NIST 2009 Open MT Eval-
uation.3
? Arabic-English: all available data for the
constrained track of NIST 2009, excluding
United Nations proceedings (LDC2004E13),
ISI Automatically Extracted Parallel Text
(LDC2007E08), and Ummah newswire text
(LDC2004T18), for a total of 5.4+4.3 mil-
lion words. We also experimented on a larger
Arabic-English parallel text of 44+37 million
words from the DARPA GALE program.
? Urdu-English: all available data for the con-
strained track of NIST 2009.
1The code can be downloaded from the first author?s website
at http://www.isi.edu/?avaswani/giza-pp-l0.html.
2GIZA++ allows changing some heuristic parameters for
efficient training. Currently, we set two of these to zero:
mincountincrease and probcutoff. In the default setting,
both are set to 10?7. We set probcutoff to 0 because we would
like the optimization to learn the parameter values. For a fair
comparison, we applied the same setting to our vanilla EM
training as well. To test, we ran GIZA++ with the default set-
ting on the smaller of our two Arabic-English datasets with the
same number of iterations and found no change in F-score.
3LDC catalog numbers LDC2003E07, LDC2003E14,
LDC2005E83, LDC2005T06, LDC2006E24, LDC2006E34,
LDC2006E85, LDC2006E86, LDC2006E92, and
LDC2006E93.
314
pr
es
id
en
t
of th
e
fo
re
ig
n
af
fa
ir
s
in
st
it
ut
e
sh
uq
in
li
u
wa
s
al
so
pr
es
en
t
at th
e
me
et
in
g
.
   u u           
wa`ijia?o
     u          
xue?hu?`
u               
hu?`zha?ng
       u        
liu?
      u u        
shu?q??ng
             u  
hu?`jia`n
               
sh??
         u u u  u  
za`izuo`
              u 
.
ov
er
40
00
gu
es
ts
fr
om
ho
me
an
d
ab
ro
ad
at
te
nd
ed
th
e
op
en
in
g
ce
re
mo
ny
.
    u  u      
zho?ngwa`i
  u          
la?ib??n
 u           
s?`qia?n
u u           
duo?
            
re?n
       u     
chu?x??
       u     
le
         u u  
ka?imu`sh?`
           u 
.
(a) (b)
it ?s ex
tr
em
el
y
tr
ou
bl
es
om
e
to ge
t
th
er
e
vi
a
la
nd
.
u          
ru?guo?
    u      
ya`o
       u u  
lu`lu`
          
zhua?n
     u u    
qu`
          
dehua`
          
ne
         u 
,
  u        
he?n
  u        
he?n
  u        
he?n
  u        
he?n
   u       
ma?fan
          
de
         u 
,
af
te
r
th
is
wa
s
ta
ke
n
ca
re
of , fo
ur
bl
oc
kh
ou
se
s
we
re
bl
ow
n
up .
 u            
zhe`ge
    u         
chu`l??
             
wa?n
u             
y??ho`u
             
ne
      u       
,
             
ha?i
          u   
zha`
           u  
le
       u      
s?`ge
        u     
dia?oba?o
            u 
.
(c) (d)
Figure 2: Smoothed-`0 alignments (red circles) correct many errors in the baseline GIZA++ alignments (black
squares), as shown in four Chinese-English examples (the red circles are almost perfect for these examples, except
for minor mistakes such as liu-shu?q??ng and meeting-za`izuo` in (a) and .-, in (c)). In particular, the baseline system
demonstrates typical ?garbage-collection? phenomena in proper name ?shuqing? in both languages in (a), number
?4000? and word ?la?ib??n? (lit. ?guest?) in (b), word ?troublesome? and ?lu`lu`? (lit. ?land-route?) in (c), and ?block-
houses? and ?dia?oba?o? (lit. ?bunker?) in (d). We found this garbage-collection behavior to be especially common with
proper names, numbers, and uncommon words in both languages. Most interestingly, in (c), our smoothed-`0 system
correctly aligns ?extremely? to ?he?n he?n he?n he?n? (lit. ?very very very very?) which is rare in the bitext.
315
task data (M) system align F1 (%) word trans (M) ??sing. Bleu (%)
2008 2009 2010
Chi-Eng 9.6+12
baseline 73.2 3.5 6.2 28.7
`0-norm 76.5 2.0 3.3 29.5
difference +3.3 ?43% ?47% +0.8
Ara-Eng 5.4+4.3
baseline 65.0 3.1 4.5 39.8 42.5
`0-norm 70.8 1.8 1.8 41.1 43.7
difference +5.9 ?39% ?60% +1.3 +1.2
Ara-Eng 44+37
baseline 66.2 15 5.0 41.6 44.9
`0-norm 71.8 7.9 1.8 42.5 45.3
difference +5.6 ?47% ?64% +0.9 +0.4
Urd-Eng 1.7+1.5
baseline 1.7 4.5 25.3? 29.8
`0-norm 1.2 2.2 25.9? 31.2
difference ?29% ?51% +0.6? +1.4
Cze-Eng 2.1+2.3
baseline 65.6 1.5 3.0 17.3 18.0
`0-norm 72.3 1.0 1.4 17.9 18.4
difference +6.7 ?33% ?53% +0.6 +0.4
Table 1: Adding the `0-norm to the IBM models improves both alignment and translation accuracy across four different
language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the
lexical weighting table) is reduced. The ??sing. column shows the average fertility of once-seen source words. For
Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open
MT Evaluation. ?Half of this test set was also used for tuning feature weights.
? Czech-English: A corpus of 4 million words of
Czech-English data from the News Commen-
tary corpus.4
We set the hyperparameters ? and ? by tuning
on gold-standard word alignments (to maximize F1)
when possible. For Arabic-English and Chinese-
English, we used 346 and 184 hand-aligned sen-
tences from LDC2006E86 and LDC2006E93. Sim-
ilarly, for Czech-English, 515 hand-aligned sen-
tences were available (Bojar and Prokopova?, 2006).
But for Urdu-English, since we did not have any
gold alignments, we used ? = 10 and ? = 0.05. We
did not choose a large ?, as the dataset was small,
and we chose a conservative value for ?.
We ran word alignment in both directions and
symmetrized using grow-diag-final (Koehn et al,
2003). For models with the smoothed `0 prior, we
tuned ? and ? separately in each direction.
3.2 Alignment
First, we evaluated alignment accuracy directly by
comparing against gold-standard word alignments.
4This data is available at http://statmt.org/wmt10.
The results are shown in the alignment F1 col-
umn of Table 1. We used balanced F-measure rather
than alignment error rate as our metric (Fraser and
Marcu, 2007).
Following Dyer et al (2011), we also measured
the average fertility, ??sing., of once-seen source
words in the symmetrized alignments. Our align-
ments show smaller fertility for once-seen words,
suggesting that they suffer from ?garbage collec-
tion? effects less than the baseline alignments do.
The fact that we had to use hand-aligned data to
tune the hyperparameters ? and ? means that our
method is no longer completely unsupervised. How-
ever, our observation is that alignment accuracy is
actually fairly robust to the choice of these hyperpa-
rameters, as shown in Table 2. As we will see below,
we still obtained strong improvements in translation
quality when hand-aligned data was unavailable.
We also tried generating 50 word classes using
the tool provided in GIZA++. We found that adding
word classes improved alignment quality a little, but
more so for the baseline system (see Table 3). We
used the alignments generated by training with word
classes for our translation experiments.
316
? model
?
0 10 25 50 75 100 250 500 750
?
HMM 47.5
M4 52.1
0.5
HMM 46.3 48.4 52.8 55.7 57.5 61.5 62.6 62.7
M4 51.7 53.7 56.4 58.6 59.8 63.3 64.4 64.8
0.1
HMM 55.6 60.4 61.6 62.1 61.9 61.8 60.2 60.1
M4 58.2 62.4 64.0 64.4 64.8 65.5 65.6 65.9
0.05
HMM 59.1 61.4 62.4 62.5 62.3 60.8 58.7 57.7
M4 61.0 63.5 64.6 65.3 65.3 65.4 65.7 65.7
0.01
HMM 59.7 61.6 60.0 59.5 58.7 56.9 55.7 54.7
M4 62.9 65.0 65.1 65.2 65.1 65.4 65.3 65.4
0.005
HMM 58.1 59.0 58.3 57.6 57.0 55.9 53.9 51.7
M4 62.0 64.1 64.5 64.5 64.5 65.0 64.8 64.6
0.001
HMM 51.7 52.1 51.4 49.3 50.4 46.8 45.4 44.0
M4 59.8 61.3 61.5 61.0 61.8 61.2 61.0 61.2
Table 2: Almost all hyperparameter settings achieve higher F-scores than the baseline IBM Model 4 and HMM model
for Arabic-English alignment (? = 0).
word classes?
direction system no yes
P( f | e)
baseline 49.0 52.1
`0-norm 63.9 65.9
difference +14.9 +13.8
P(e | f )
baseline 64.3 65.2
`0-norm 69.2 70.3
difference +4.9 +5.1
Table 3: Adding word classes improves the F-score in
both directions for Arabic-English alignment by a little,
for the baseline system more so than ours.
Figure 2 shows four examples of Chinese-
English alignment, comparing the baseline with our
smoothed-`0 method. In all four cases, the base-
line produces incorrect extra alignments that prevent
good translation rules from being extracted while
the smoothed-`0 results are correct. In particular, the
baseline system demonstrates typical ?garbage col-
lection? behavior (Moore, 2004) in all four exam-
ples.
3.3 Translation
We then tested the effect of word alignments on
translation quality using the hierarchical phrase-
based translation system Hiero (Chiang, 2007). We
used a fairly standard set of features: seven in-
herited from Pharaoh (Koehn et al, 2003), a sec-
setting align F1 (%) Bleu (%)
t( f | e) t(e | f ) 2008 2009
1st 1st 70.8 41.1 43.7
1st 2nd 70.7 41.1 43.8
2nd 1st 70.7 40.7 44.1
2nd 2nd 70.9 41.1 44.2
Table 4: Optimizing hyperparameters on alignment F1
score does not necessarily lead to optimal Bleu. The
first two columns indicate whether we used the first- or
second-best alignments in each direction (according to
F1); the third column shows the F1 of the symmetrized
alignments, whose corresponding Bleu scores are shown
in the last two columns.
ond language model, and penalties for the glue
rule, identity rules, unknown-word rules, and two
kinds of number/name rules. The feature weights
were discriminatively trained using MIRA (Chi-
ang et al, 2008). We used two 5-gram language
models, one on the combined English sides of
the NIST 2009 Arabic-English and Chinese-English
constrained tracks (385M words), and another on
2 billion words of English.
For each language pair, we extracted grammar
rules from the same data that were used for word
alignment. The development data that were used for
discriminative training were: for Chinese-English
and Arabic-English, data from the NIST 2004 and
NIST 2006 test sets, plus newsgroup data from the
317
GALE program (LDC2006E92); for Urdu-English,
half of the NIST 2008 test set; for Czech-English,
a training set of 2051 sentences provided by the
WMT10 translation workshop.
The results are shown in the Bleu column of Ta-
ble 1. We used case-insensitive IBM Bleu (closest
reference length) as our metric. Significance test-
ing was carried out using bootstrap resampling with
1000 samples (Koehn, 2004; Zhang et al, 2004).
All of the tests showed significant improvements
(p < 0.01), ranging from +0.4 Bleu to +1.4 Bleu.
For Urdu, even though we didn?t have manual align-
ments to tune hyperparameters, we got significant
gains over a good baseline. This is promising for lan-
guages that do not have any manually aligned data.
Ideally, one would want to tune ? and ? to max-
imize Bleu. However, this is prohibitively expen-
sive, especially if we must tune them separately
in each alignment direction before symmetrization.
We ran some contrastive experiments to investi-
gate the impact of hyperparameter tuning on trans-
lation quality. For the smaller Arabic-English cor-
pus, we symmetrized all combinations of the two
top-scoring alignments (according to F1) in each di-
rection, yielding four sets of alignments. Table 4
shows Bleu scores for translation models learned
from these alignments. Unfortunately, we find that
optimizing F1 is not optimal for Bleu?using the
second-best alignments yields a further improve-
ment of 0.5 Bleu on the NIST 2009 data, which is
statistically significant (p < 0.05).
4 Related Work
Schoenemann (2011a), taking inspiration from Bo-
drumlu et al (2009), uses integer linear program-
ming to optimize IBM Model 1?2 and the HMM
with the `0-norm. This method, however, does not
outperform GIZA++. In later work, Schoenemann
(2011b) used projected gradient descent for the `1-
norm. Here, we have adopted his use of projected
gradient descent, but using a smoothed `0-norm.
Liang et al (2006) show how to train IBM mod-
els in both directions simultaneously by adding a
term to the log-likelihood that measures the agree-
ment between the two directions. Grac?a et al (2010)
explore modifications to the HMM model that en-
courage bijectivity and symmetry. The modifications
take the form of constraints on the posterior dis-
tribution over alignments that is computed during
the E-step. Mermer and Sarac?lar (2011) explore a
Bayesian version of IBM Model 1, applying sparse
Dirichlet priors to t. However, because this method
requires the use of Monte Carlo methods, it is not
clear how well it can scale to larger datasets.
5 Conclusion
We have extended the IBM models and HMM model
by the addition of an `0 prior to the word-to-word
translation model, which compacts the word-to-
word translation table, reducing overfitting, and, in
particular, the ?garbage collection? effect. We have
shown how to perform MAP-EM with this prior
efficiently, even for large datasets. The method is
implemented as a modification to the open-source
toolkit GIZA++, and we have shown that it signif-
icantly improves translation quality across four dif-
ferent language pairs. Even though we have used a
small set of gold-standard alignments to tune our
hyperparameters, we found that performance was
fairly robust to variation in the hyperparameters, and
translation performance was good even when gold-
standard alignments were unavailable. We hope that
our method, due to its simplicity, generality, and ef-
fectiveness, will find wide application for training
better statistical translation systems.
Acknowledgments
We are indebted to Thomas Schoenemann for ini-
tial discussions and pilot experiments that led to
this work, and to the anonymous reviewers for
their valuable comments. We thank Jason Riesa for
providing the Arabic-English and Chinese-English
hand-aligned data and the alignment visualization
tool, and Chris Dyer for the Czech-English hand-
aligned data. This research was supported in part
by DARPA under contract DOI-NBC D11AP00244
and a Google Faculty Research Award to L. H.
318
References
Andrew Barron, Jorma Rissanen, and Bin Yu. 1998. The
minimum description length principle in coding and
modeling. IEEE Transactions on Information Theory,
44(6):2743?2760.
Dimitri P. Bertsekas. 1999. Nonlinear Programming.
Athena Scientific.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Tugba Bodrumlu, Kevin Knight, and Sujith Ravi. 2009.
A new objective function for word alignment. In Pro-
ceedings of the NAACL HLT Workshop on Integer Lin-
ear Programming for Natural Language Processing.
Ondr?ej Bojar and Magdalena Prokopova?. 2006. Czech-
English word alignment. In Proceedings of LREC.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19:263?311.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of EMNLP.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?208.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Computational Linguistics, 39(4):1?38.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and
Tushar Chandra. 2008. Efficient projections onto the
`1-ball for learning in high dimensions. In Proceed-
ings of ICML.
Chris Dyer, Jonathan H. Clark, Alon Lavie, and Noah A.
Smith. 2011. Unsupervised word alignment with ar-
bitrary features. In Proceedings of ACL.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Computational Linguistics, 33(3):293?303.
Joa?o V. Grac?a, Kuzman Ganchev, and Ben Taskar.
2010. Learning tractable word alignment models
with complex constraints. Computational Linguistics,
36(3):481?504.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL.
Cos?kun Mermer and Murat Sarac?lar. 2011. Bayesian
word alignment for statistical machine translation. In
Proceedings of ACL HLT.
Robert C. Moore. 2004. Improving IBM word-
alignment Model 1. In Proceedings of ACL.
Robert Moore. 2005. A discriminative framework for
bilingual word alignment. In Proceedings of HLT-
EMNLP.
Franz Joseph Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30:417?449.
Jason Riesa and Daniel Marcu. 2010. Hierarchical
search for word alignment. In Proceedings of ACL.
Thomas Schoenemann. 2011a. Probabilistic word align-
ment under the L0-norm. In Proceedings of CoNLL.
Thomas Schoenemann. 2011b. Regularizing mono- and
bi-word models for word alignment. In Proceedings
of IJCNLP.
Ben Taskar, Lacoste-Julien Simon, and Klein Dan. 2005.
A discriminative matching approach to word align-
ment. In Proceedings of HLT-EMNLP.
Ashish Vaswani, Adam Pauls, and David Chiang. 2010.
Efficient optimization of an MDL-inspired objective
function for unsupervised part-of-speech tagging. In
Proceedings of ACL.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of COLING.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much improve-
ment do we need to have a better system? In Proceed-
ings of LREC.
319
