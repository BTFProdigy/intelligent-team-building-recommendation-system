Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 189?192,
New York, June 2006. c?2006 Association for Computational Linguistics
BioEx: A Novel User-Interface that Accesses Images from Abstract Sentences 
  
Hong Yu Minsuk Lee 
Department of Biomedical Informatics Department of Biomedical Informatics 
Columbia University Columbia University 
New York, NY 10032 New York, NY 10032 
Hy52@columbia.edu minsuk.lee@gmail.com 
 
Abstract 
Images (i.e., figures or tables) are important ex-
perimental results that are typically reported in 
bioscience full-text articles. Biologists need to 
access the images to validate research facts and 
to formulate or to test novel research hypothe-
ses. We designed, evaluated, and implemented a 
novel user-interface, BioEx, that allows biolo-
gists to access images that appear in a full-text 
article directly from the abstract of the article.  
1 Introduction 
The rapid growth of full-text electronic publica-
tions in bioscience has made it necessary to cre-
ate information systems that allow biologists to 
navigate and search efficiently among them. Im-
ages are usually important experimental results 
that are typically reported in full-text bioscience 
articles. An image is worth a thousand words. 
Biologists need to access image data to validate 
research facts and to formulate or to test novel 
research hypotheses. Additionally, full-text arti-
cles are frequently long and typically incorpo-
rate multiple images. For example, we have 
found an average of 5.2 images per biological 
article in the journal Proceedings of the National 
Academy of Sciences (PNAS). Biologists need to 
spend significant amount of time to read the full-
text articles in order to access specific images.  
 
 
Figure 1. BioEx user-interface (as shown in A) is built upon the PubMed user-interface. Images 
are shown as thumbnails at the bottom of a PubMed abstract. Images include both Figure and Ta-
ble. When a mouse (as shown as a hand in A) moves to ?Fig x?, it shows the associated abstract 
sentence(s) that link to the original figure that appears in the full-text articles. For example, ?Fig 
1? links to image B. ?Related Text? provides links to other associated texts that correspond to the 
image besides its image caption. 
 
189
In order to facilitate biologists? access to images, 
we designed, evaluated, and implemented a 
novel user-interface, BioEx, that allows biolo-
gists to access images that appear in a full-text 
article directly from the abstract of the article. In 
the following, we will describe the BioEx user-
interface, evaluation, and the implementation.  
 
2. Data Collection 
 
We hypothesize that images reported in a full-
text article can be summarized by sentences in 
the abstract. To test this hypothesis, we ran-
domly selected a total of 329 biological articles 
that are recently published in leading journals 
Cell (104), EMBO (72), Journal of Biological 
Chemistry (92), and Proceedings of the National 
Academy of Sciences (PNAS) (61). For each arti-
cle, we e-mailed the corresponding author and 
invited him or her to identify abstract sentences 
that summarize image content in that article. In 
order to eliminate the errors that may be intro-
duced by sentence boundary ambiguity, we 
manually segmented the abstracts into sentences 
and sent the sentences as the email attachments.  
 
A total of 119 biologists from 19 countries par-
ticipated voluntarily the annotation to identify 
abstract sentences that summarize figures or ta-
bles from 114 articles (39 Cells, 29 EMBO, 30 
Journal of Biological Chemistry, and 16 PNAS), 
a collection that is 34.7% of the total articles we 
requested. The responding biologists included 
the corresponding authors to whom we had sent 
emails, as well as the first authors of the articles 
to whom the corresponding authors had for-
warded our emails. None of the biologists or 
authors were compensated.  
 
This collection of 114 full-text articles incorpo-
rates 742 images and 826 abstract sentences. 
The average number of images per document is 
6.5?1.5 and the average number of sentences per 
abstract is 7.2?1.9. Our data show that 87.9% 
images correspond to abstract sentences and 
66.5% of the abstract sentences correspond to 
images. The data empirically validate our hy-
pothesis that image content can be summarized 
by abstract sentences. Since an abstract is a sum-
mary of a full-text article, our results also em-
pirically validate that images are important 
elements in full-text articles. This collection of 
114 annotated articles was then used as the cor-
pus to evaluate automatic mapping of abstract 
sentences to images using the natural language 
processing approaches described in Section 4. 
 
3. BioEx User-Interface Evaluation 
 
In order to evaluate whether biologists would 
prefer to accessing images from abstract sen-
tence links, we designed BioEx (Figure 1) and 
two other baseline user-interfaces. BioEx is built 
upon the PubMed user-interface except that im-
ages can be accessed by the abstract sentences. 
We chose the PubMed user-interface because it 
has more than 70 million hits a month and repre-
sents the most familiar user-interface to biolo-
gists. Other information systems have also 
adapted the PubMed user-interface for similar 
reasons (Smalheiser and Swanson 1998; Hearst 
2003). The two other baseline user-interfaces 
were the original PubMed user-interface and a 
modified version of the SummaryPlus user-
interface, in which the images are listed as dis-
jointed thumbnails rather than related by abstract 
sentences.  
 
We asked the 119 biologists who linked sen-
tences to images in their publications to assign a 
label to each of the three user-interfaces to be 
?My favorite?, ?My second favorite?, or ?My 
least favorite?. We designed the evaluation so 
that a user-interface?s label is independent of the 
choices of the other two user-interfaces.  
 
A total of 41 or 34.5% of the biologists com-
pleted the evaluation in which 36 or 87.8% of 
the total 41 biologists judged BioEx as ?My fa-
vorite?. One biologist judged all three user-
interfaces to be ?My favorite?. Five other biolo-
gists considered SummaryPlus as ?My favorite?, 
two of whom (or 4.9% of the total 41 biologists) 
judged BioEx to be ?My least favorite?.  
 
4. Linking Abstract Sentences to Images 
 
We have explored hierarchical clustering algo-
rithms to cluster abstract sentences and image 
captions based on lexical similarities.  
Hierarchical clustering algorithms are well-
established algorithms that are widely used in 
190
many other research areas including biological 
sequence alignment (Corpet 1988), gene expres-
sion analyses (Herrero et al 2001), and topic 
detection (Lee et al 2006). The algorithm starts 
with a set of text (i.e., abstract sentences or im-
age captions). Each sentence or image caption 
represents a document that needs to be clustered. 
The algorithm identifies pair-wise document 
similarity based on the TF*IDF weighted cosine 
similarity. It then merges the two documents 
with the highest similarity into one cluster. It 
then re-evaluates pairs of documents/clusters; 
two clusters can be merged if the average simi-
larity across all pairs of documents within the 
two clusters exceeds a predefined threshold.  In 
presence of multiple clusters that can be merged 
at any time, the pair of clusters with the highest 
similarity is always preferred. 
In our application, if abstract sentences belong 
to the same cluster that includes images cap-
tions, the abstract sentences summarize the im-
age content of the corresponded images. The 
clustering model is advantageous over other 
models in that the flexibility of clustering meth-
ods allows ?many-to-many? mappings. That is a 
sentence in the abstract can be mapped to zero, 
one or more than one images and an image can 
be mapped to zero, one or more than one ab-
stract sentences.  
 
We explored different learning features, weights 
and clustering algorithms to link abstract sen-
tences to images. We applied the TF*IDF 
weighted cosine similarity for document cluster-
ing. We treat each sentence or image caption as 
a ?document? and the features are bag-of-words.  
 
We tested three different methods to obtain the 
IDF value for each word feature: 1) 
IDF(abstract+caption): the IDF values were 
calculated from the pool of abstract sentences 
and image captions; 2) IDF(full-text): the IDF 
values were calculated from all sentences in the 
full-text article; and 3) 
IDF(abstract)::IDF(caption): two sets of IDF 
values were obtained. For word features that 
appear in abstracts, the IDF values were calcu-
lated from the abstract sentences. For words that 
appear in image captions, the IDF values were 
calculated from the image captions.  
 
The positions of abstract sentences or images are 
important. The chance that two abstract sen-
tences link to an image decreases when the dis-
tance between two abstract sentences increases. 
For example, two consecutive abstract sentences 
have a higher probability to link to one image 
than two abstract sentences that are far apart. 
Two consecutive images have a higher chance to 
link to the same abstract sentence than two im-
ages that are separated by many other images. 
Additionally, sentence positions in an abstract 
seem to correspond to image positions. For ex-
ample, the first sentences in an abstract have 
higher probabilities than the last sentences to 
link to the first image. 
  
To integrate such ?neighboring effect? into our 
existing hierarchical clustering algorithms, we 
modified the TF*IDF weighted cosine similar-
ity. The TF*IDF weighted cosine similarity for a 
pair of documents i and j is Sim(i,j), and the final 
similarity metric W(i,j) is: 
( ) ))//(1(*),(, jjii TPTPabsjiSimjiW ??=
                                 
1. If i and j are both abstract sentences,   
Ti=Tj=total number of abstract sentences; and 
Pi and Pj represents the positions of sentences i 
and j in the abstract.   
2. If i and j are both image captions, 
Ti=Tj=total number of images that appear in a 
full-text article; and Pi and Pj represents the 
positions of images i and j in the full-text arti-
cle. 
3.  If i and j are an abstract sentence and an 
image caption, respectively, Ti=total number 
of abstract sentences and Tj=total number of 
images that appear in a full-text article; and Pi 
and Pj represent the positions of abstract sen-
tence i and image j.    
Finally, we explored three clustering strategies; 
namely, per-image, per-abstract sentence, and 
mix. 
The Per-image strategy clusters each image 
caption with all abstract sentences. The image is 
191
assigned to (an) abstract sentence(s) if it belongs 
to the same cluster. This method values features 
in abstract sentences more than image captions 
because the decision that an image belongs to (a) 
sentence(s) depends upon the features from all 
abstract sentences and the examined image cap-
tion. The features from other image captions do 
not play a role in the clustering methodology.  
The Per-abstract-sentence strategy takes each 
abstract sentence and clusters it with all image 
captions that appear in a full-text article. Images 
are assigned to the sentence if they belong to the 
same cluster. This method values features in im-
age captions higher than the features in abstract 
sentences because the decision that an abstract 
sentence belongs to image(s) depends upon the 
features from the image captions and the exam-
ined abstract sentence. Similar to per-image 
clustering, the features from other abstract sen-
tences do not play a role in the clustering meth-
odology.  
The Mix strategy clusters all image captions 
with all abstract sentences. This method treats 
features in abstract sentences and image captions 
equally. 
5. Results and Conclusions 
Figures 2 - 4 show the results from three differ-
ent combinations of features and algorithms with 
varied TF*IDF thresholds. The default parame-
ters for all these experiments were ?per image?, 
?bag-of-words?, and ?without neighboring 
weight?. 
 
Figure 2 shows that the ?global? IDFs, or the 
IDFs obtained from the full-text article, have a 
much lower performance than ?local? IDFs, or 
IDFs calculated from the abstract sentences and 
image captions. Figure 3 shows that Per-image 
out-performs the other two strategies. The re-
sults suggest that features in abstract sentences 
are more useful than features that reside within 
captions for the task of clustering. Figure 4 
shows that the ?neighboring weighted? approach 
offers significant enhancement over the TF*IDF 
weighted approach. When the recall is 33%, the 
precision of ?neighboring weighted? approach 
increases to 72% from the original 38%, which 
corresponds to a 34% increase. The results 
strongly indicate the importance of the 
?neighboring effect? or positions of additional 
features. When the precision is 100%, the recall 
is 4.6%. We believe BioEx system is applicable 
for real use because a high level of precision is 
the key to BioEx success. 
 
Acknowledgement: The authors thank Dr. Weiqing 
Wang for her contribution to this work. The authors 
also thank Michael Bales, Li Zhou and Eric Silfen, 
and three anonymous reviewers for valuable com-
ments. The authors acknowledge the support of Juve-
nile Diabetes Foundation International (JDRF 6-
2005-835).  
 
References: 
Corpet F (1988) Multiple sequence alignment with hierar-
chical clustering. Nucleic Acids Res 16:10881-10890 
Hearst M (2003) The BioText project. A powerpoint pres-
entation. 
Herrero J, Valencia A, Dopazo J (2001) A hierarchical 
unsupervised growing neural network for clustering gene 
expression patterns. Bioinformatics 17:126-136 
Lee M, Wang W, Yu H (2006) Exploring supervised and 
unsupervised methods to detect topics in Biomedical text. 
BMC Bioinformatics 7:140 
Smalheiser NR, Swanson DR (1998) Using 
ARROWSMITH: a computer-assisted approach to formu-
lating and assessing scientific hypotheses. Comput Meth-
ods Programs Biomed 57:149-153 
 
 
192
Towards Answering Opinion Questions: Separating Facts from Opinions
and Identifying the Polarity of Opinion Sentences
Hong Yu
Department of Computer Science
Columbia University
New York, NY 10027, USA
hongyu@cs.columbia.edu
Vasileios Hatzivassiloglou
Department of Computer Science
Columbia University
New York, NY 10027, USA
vh@cs.columbia.edu
Abstract
Opinion question answering is a challenging task
for natural language processing. In this paper, we
discuss a necessary component for an opinion ques-
tion answering system: separating opinions from
fact, at both the document and sentence level. We
present a Bayesian classifier for discriminating be-
tween documents with a preponderance of opinions
such as editorials from regular news stories, and
describe three unsupervised, statistical techniques
for the significantly harder task of detecting opin-
ions at the sentence level. We also present a first
model for classifying opinion sentences as positive
or negative in terms of the main perspective be-
ing expressed in the opinion. Results from a large
collection of news stories and a human evaluation
of 400 sentences are reported, indicating that we
achieve very high performance in document classi-
fication (upwards of 97% precision and recall), and
respectable performance in detecting opinions and
classifying them at the sentence level as positive,
negative, or neutral (up to 91% accuracy).
1 Introduction
Newswire articles include those that mainly present
opinions or ideas, such as editorials and letters to
the editor, and those that mainly report facts such as
daily news articles. Text materials from many other
sources also contain mixed facts and opinions. For
many natural language processing applications, the
ability to detect and classify factual and opinion sen-
tences offers distinct advantages in deciding what in-
formation to extract and how to organize and present
this information. For example, information extrac-
tion applications may target factual statements rather
than subjective opinions, and summarization sys-
tems may list separately factual information and ag-
gregate opinions according to distinct perspectives.
At the document level, information retrieval systems
can target particular types of articles and even utilize
perspectives in focusing queries (e.g., filtering or re-
trieving only editorials in favor of a particular policy
decision).
Our motivation for building the opinion detec-
tion and classification system described in this pa-
per is the need for organizing information in the
context of question answering for complex ques-
tions. Unlike questions like ?Who was the first
man on the moon?? which can be answered with
a simple phrase, more intricate questions such as
?What are the reasons for the US-Iraq war?? require
long answers that must be constructed from multi-
ple sources. In such a context, it is imperative that
the question answering system can discriminate be-
tween opinions and facts, and either use the appro-
priate type depending on the question or combine
them in a meaningful presentation. Perspective in-
formation can also help highlight contrasts and con-
tradictions between different sources?there will be
significant disparity in the material collected for the
question mentioned above between Fox News and
the Independent, for example.
Fully analyzing and classifying opinions involves
tasks that relate to some fairly deep semantic and
syntactic analysis of the text. These include not only
recognizing that the text is subjective, but also de-
termining who the holder of the opinion is, what the
opinion is about, and which of many possible posi-
tions the holder of the opinion expresses regarding
that subject. In this paper, we are presenting three
of the components of our opinion detection and or-
ganization subsystem, which have already been in-
tegrated into our larger question-answering system.
These components deal with the initial tasks of clas-
sifying articles as mostly subjective or objective,
finding opinion sentences in both kinds of articles,
and determining, in general terms and without refer-
ence to a specific subject, if the opinions are positive
or negative. The three modules of the system dis-
cussed here provide the basis for ongoing work for
further classification of opinions according to sub-
ject and opinion holder and for refining the original
positive/negative attitude determination.
We review related work in Section 2, and then
present our document-level classifier for opinion or
factual articles (Section 3), three implemented tech-
niques for detecting opinions at the sentence level
(Section 4), and our approach for rating an opinion
as positive or negative (Section 5). We have evalu-
ated these methods using a large collection of news
articles without additional annotation (Section 6)
and an evaluation corpus of 400 sentences anno-
tated for opinion classifications (Section 7). The
results, presented in Section 8, indicate that we
achieve very high performance (more than 97%) at
document-level classification and respectable per-
formance (86?91%) at detecting opinion sentences
and classifying them according to orientation.
2 Related Work
Much of the earlier research in automated opinion
detection has been performed by Wiebe and col-
leagues (Bruce and Wiebe, 1999; Wiebe et al, 1999;
Hatzivassiloglou and Wiebe, 2000; Wiebe, 2000;
Wiebe et al, 2002), who proposed methods for dis-
criminating between subjective and objective text at
the document, sentence, and phrase levels. Bruce
and Wiebe (1999) annotated 1,001 sentences as sub-
jective or objective, and Wiebe et al (1999) de-
scribed a sentence-level Naive Bayes classifier using
as features the presence or absence of particular syn-
tactic classes (pronouns, adjectives, cardinal num-
bers, modal verbs, adverbs), punctuation, and sen-
tence position. Subsequently, Hatzivassiloglou and
Wiebe (2000) showed that automatically detected
gradable adjectives are a useful feature for subjec-
tivity classification, while Wiebe (2000) introduced
lexical features in addition to the presence/absence
of syntactic categories. More recently, Wiebe et al
(2002) report on document-level subjectivity classi-
fication, using a k-nearest neighbor algorithm based
on the total count of subjective words and phrases
within each document.
Psychological studies (Bradley and Lang, 1999)
found measurable associations between words and
human emotions. Hatzivassiloglou and McKeown
(1997) described an unsupervised learning method
for obtaining positively and negatively oriented ad-
jectives with accuracy over 90%, and demonstrated
that this semantic orientation, or polarity, is a con-
sistent lexical property with high inter-rater agree-
ment. Turney (2002) showed that it is possible
to use only a few of those semantically oriented
words (namely, ?excellent? and ?poor?) to label
other phrases co-occuring with them as positive or
negative. He then used these phrases to automati-
cally separate positive and negative movie and prod-
uct reviews, with accuracy of 66?84%. Pang et al
(2002) adopted a more direct approach, using super-
vised machine learning with words and n-grams as
features to predict orientation at the document level
with up to 83% precision.
Our approach to document and sentence classi-
fication of opinions builds upon the earlier work
by using extended lexical models with additional
features. Unlike the work cited above, we do not
rely on human annotations for training but only on
weak metadata provided at the document level. Our
sentence-level classifiers introduce additional crite-
ria for detecting subjective material (opinions), in-
cluding methods based on sentence similarity within
a topic and an approach that relies on multiple clas-
sifiers. At the document level, our classifier uses the
same document labels that the method of (Wiebe et
al., 2002) does, but automatically detects the words
and phrases of importance without further analy-
sis of the text. For determining whether an opin-
ion sentence is positive or negative, we have used
seed words similar to those produced by (Hatzivas-
siloglou and McKeown, 1997) and extended them to
construct a much larger set of semantically oriented
words with a method similar to that proposed by
(Turney, 2002). Our focus is on the sentence level,
unlike (Pang et al, 2002) and (Turney, 2002); we
employ a significantly larger set of seed words, and
we explore as indicators of orientation words from
syntactic classes other than adjectives (nouns, verbs,
and adverbs).
3 Document Classification
To separate documents that contain primarily opin-
ions from documents that report mainly facts, we ap-
plied Naive Bayes1, a commonly used supervised
machine-learning algorithm. This approach pre-
supposes the availability of at least a collection of ar-
ticles with pre-assigned opinion and fact labels at the
document level; fortunately, Wall Street Journal ar-
ticles contain such metadata by identifying the type
of each article as Editorial, Letter to editor, Business
and News. These labels are used only to provide the
correct classification labels during training and eval-
uation, and are not included in the feature space. We
used as features single words, without stemming or
stopword removal. Naive Bayes assigns a document
 
to the class  that maximizes 
 	
by applying
Bayes? rule 
 	
 

and assuming con-
ditional independence of the features.
Although Naive Bayes can be outperformed in
text classification tasks by more complex methods
such as SVMs, Pang et al (2002) report similar per-
formance for Naive Bayes and other machine learn-
ing techniques for a similar task, that of distinguish-
ing between positive and negative reviews at the
document level. Further, we achieved such high per-
formance with Naive Bayes (see Section 8) that ex-
ploring additional techniques for this task seemed
unnecessary.
4 Finding Opinion Sentences
We developed three different approaches to clas-
sify opinions from facts at the sentence level. To
avoid the need for obtaining individual sentence an-
notations for training and evaluation, we rely in-
stead on the expectation that documents classified
as opinion on the whole (e.g., editorials) will tend to
have mostly opinion sentences, and conversely doc-
uments placed in the factual category will tend to
have mostly factual sentences. Wiebe et al (2002)
report that this expectation is borne out 75% of the
time for opinion documents and 56% of the time for
factual documents.
4.1 Similarity Approach
Our first approach to classifying sentences as opin-
ions or facts explores the hypothesis that, within a
given topic, opinion sentences will be more simi-
lar to other opinion sentences than to factual sen-
1Using the Rainbow implementation, available from www.
cs.cmu.edu/?mccallum/bow/rainbow.
tences. We used SIMFINDER (Hatzivassiloglou et
al., 2001), a state-of-the-art system for measuring
sentence similarity based on shared words, phrases,
and WordNet synsets. To measure the overall simi-
larity of a sentence to the opinion or fact documents,
we first select the documents that are on the same
topic as the sentence in question. We obtain topics
as the results of IR queries (for example, by search-
ing our document collection for ?welfare reform?).
We then average its SIMFINDER-provided similari-
ties with each sentence in those documents. Then
we assign the sentence to the category for which the
average is higher (we call this approach the ?score?
variant). Alternatively, for the ?frequency? variant,
we do not use the similarity scores themselves but
instead we count how many of them, for each cate-
gory, exceed a predetermined threshold (empirically
set to 0.65).
4.2 Naive Bayes Classifier
Our second method trains a Naive Bayes classifier
(see Section 3), using the sentences in opinion and
fact documents as the examples of the two cate-
gories. The features include words, bigrams, and
trigrams, as well as the parts of speech in each sen-
tence. In addition, the presence of semantically ori-
ented (positive and negative) words in a sentence is
an indicator that the sentence is subjective (Hatzi-
vassiloglou and Wiebe, 2000). Therefore, we in-
clude in our features the counts of positive and neg-
ative words in the sentence (which are obtained with
the method of Section 5.1), as well as counts of
the polarities of sequences of semantically oriented
words (e.g., ?++? for two consecutive positively ori-
ented words). We also include the counts of parts
of speech combined with polarity information (e.g.,
?JJ+? for positive adjectives), as well as features en-
coding the polarity (if any) of the head verb, the
main subject, and their immediate modifiers. Syn-
tactic structure was obtained with Charniak?s statis-
tical parser (Charniak, 2000). Finally, we used as
one of the features the average semantic orientation
score of the words in the sentence.
4.3 Multiple Naive Bayes Classifiers
Our designation of all sentences in opinion or factual
articles as opinion or fact sentences is an approxima-
tion. To address this, we apply an algorithm using
multiple classifiers, each relying on a different sub-
set of our features. The goal is to reduce the training
set to the sentences that are most likely to be cor-
rectly labeled, thus boosting classification accuracy.
Given separate sets of features   			
  ,
we train separate Naive Bayes classifiers  ,
			 corresponding to each feature set. Assum-
ing as ground truth the information provided by the
document labels and that all sentences inherit the
status of their document as opinions or facts, we
first train   on the entire training set, then use the
resulting classifier to predict labels for the training
set. The sentences that receive a label different from
the assumed truth are then removed, and we train
  on the remaining sentences. This process is re-
peated iteratively until no more sentences can be re-
moved. We report results using five feature sets,
starting from words alone and adding in bigrams, tri-
grams, part-of-speech, and polarity.
5 Identifying the Polarity of Opinion
Sentences
Having distinguished whether a sentence is a fact or
opinion, we separate positive, negative, and neutral
opinions into three classes. We base this decision
on the number and strength of semantically oriented
words (either positive or negative) in the sentence.
We first discuss how such words are automatically
found by our system, and then describe the method
by which we aggregate this information across the
sentence.
5.1 Semantically Oriented Words
To determine which words are semantically ori-
ented, in what direction, and the strength of their
orientation, we measured their co-occurrence with
words from a known seed set of semantically ori-
ented words. The approach is based on the hypothe-
sis that positive words co-occur more than expected
by chance, and so do negative words; this hypothe-
sis was validated, at least for strong positive/negative
words, in (Turney, 2002). As seed words, we used
subsets of the 1,336 adjectives that were manually
classified as positive (657) or negative (679) by
Hatzivassiloglou and McKeown (1997). In earlier
work (Turney, 2002) only singletons were used as
seed words; varying their number allows us to test
whether multiple seed words have a positive effect
in detection performance. We experimented with
seed sets containing 1, 20, 100 and over 600 positive
and negative pairs of adjectives. For a given seed set
size, we denote the set of positive seeds as ADJ 
and the set of negative seeds as ADJ  . We then cal-
culate a modified log-likelihood ratio  Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 1?8,
New York City, June 2006. c?2006 Association for Computational Linguistics
The Semantics of a Definiendum Constrains both the Lexical Semantics 
and the Lexicosyntactic Patterns in the Definiens 
 
Hong Yu Ying Wei 
Department of Health Sciences Department of Biostatistics 
University of Wisconsin-Milwaukee Columbia University 
Milwaukee, WI  53201 New York, NY 10032 
Hong.Yu@uwm.edu Ying.Wei@columbia.com 
 
Abstract 
Most current definitional question an-
swering systems apply one-size-fits-all 
lexicosyntactic patterns to identify defini-
tions. By analyzing a large set of online 
definitions, this study shows that the se-
mantic types of definienda constrain both 
lexical semantics and lexicosyntactic pat-
terns of the definientia. For example, 
?heart? has the semantic type [Body Part, 
Organ, or Organ Component] and its 
definition (e.g., ?heart locates between the 
lungs?) incorporates semantic-type-
dependent lexicosyntactic patterns (e.g., 
?TERM locates ??) and terms (e.g., 
?lung? has the same semantic type [Body 
Part, Organ, or Organ Component]). In 
contrast, ?AIDS? has a different semantic 
type [Disease or Syndrome]; its definition 
(e.g., ?An infectious disease caused by 
human immunodeficiency virus?) consists 
of different lexicosyntactic patterns (e.g., 
??causes by??) and terms (e.g., ?infec-
tious disease? has the semantic type [Dis-
ease or Syndrome]). The semantic types 
are defined in the widely used biomedical 
knowledge resource, the Unified Medical 
Language System (UMLS).  
1 Introduction 
 
Definitional questions (e.g., ?What is X??) consti-
tute an important question type and have been a 
part of the evaluation at the Text Retrieval Confer-
ence (TREC) Question Answering Track since 
2003. Most systems apply one-size-fits-all lexico-
syntactic patterns to identify definitions (Liang et 
al. 2001; Blair-Goldensohn et al 2004; 
Hildebrandt et al 2004; Cui et al 2005). For ex-
ample, the pattern ?NP, (such as|like|including) 
query term? can be used to identify the definition 
?New research in mice suggests that drugs such as 
Ritalin quiet hyperactivity? (Liang et al 2001).  
 
Few existing systems, however, have explored the 
relations between the semantic type (denoted as 
SDT) of a definiendum (i.e., a defined term (DT)) 
and the semantic types (denoted as SDef) of terms in 
its definiens (i.e., definition). Additionally, few 
existing systems have examined whether the lexi-
cosyntactic patterns of definitions correlate with 
the semantic types of the defined terms.  
 
By analyzing a large set of online definitions, this 
study shows that 1) SDef correlates with SDT, and 2) 
SDT constrains the lexicosyntactic patterns of the 
corresponding definitions. In the following, we 
will illustrate our findings with the following four 
definitions: 
 
  a. Heart[Body Part, Organ, or Organ Component]: The hol-
low[Spatial Concept] muscular[Spatial Concept] organ[Body Part, 
Organ, or Organ Component,Tissue]
 located[Spatial Concept] be-
hind[Spatial Concept] the sternum[Body Part, Organ, or Organ Com-
ponent] and between the lungs[Body Part, Organ, or Organ 
Component]
. 
   b. Kidney[Body Part, Organ, or Organ Component]: The kid-
neys are a pair of glandular organs[Body Part, Organ, or 
Organ Component]
 located[Spatial Concept] in the abdomi-
nal_cavities[Body Part, Organ, or Organ Component] of mam-
mals[Mammal] and reptiles[Reptile].    
   c. Heart attack[Disease or Syndrome]: also called myo-
cardial_infarction[Disease or Syndrome]; damage[Functional 
Concept]
 to the heart_muscle[Tissue] due to insufficient 
1
blood supply[Organ or Tissue Function] for an extended[Spatial 
Concept]
 time_period[Temporal Concept]. 
   d. AIDS[Disease or Syndrome]: An infec-
tious_disease[Disease or Syndrome] caused[Functional Concept] 
by human_immunodeficiency_virus[Virus]. 
 
In the above four definitions, the superscripts in 
[brackets] are the semantic types (e.g., [Body Part, 
Organ, or Organ Component] and [Disease or Syn-
drome]) of the preceding terms. A multiword term 
links words with the underscore ?_?. For example, 
?heart? IS-A [Body Part, Organ, or Organ Compo-
nent] and ?heart_muscle? IS-A [Tissue]. The se-
mantic types are defined in the Semantic Network 
(SN) of the Unified Medical Language System 
(UMLS), the largest biomedical knowledge re-
source. Details of the UMLS and SN will be de-
scribed in Section 2. We applied MMTx (Aronson 
et al 2004) to automatically map a string to the 
UMLS semantic types. MMTx will also be de-
scribed in Section 2.  
 
Simple analysis of the above four definitions 
shows that given a defined term (DT) with a se-
mantic type SDT (e.g., [Body Part, Organ, or Organ 
Component]), terms that appear in the definition 
tend to have the same or related semantic types 
(e.g., [Body Part, Organ, or Organ Component] 
and [Spatial Concept]). Such observations were 
first reported as ?Aristotelian definitions? 
(Bodenreider and Burgun 2002) in the limited do-
main of anatomy. (Rindflesch and Fiszman 2003) 
reported that the hyponym related to the definien-
dum must be in an IS-A relation with the hy-
pernym that is related to the definiens. However, 
neither work demonstrated statistical patterns on a 
large corpus as we report in this study. Addition-
ally, none of the work explicitly suggested the use 
of patterns to support question answering.  
 
In addition to statistical correlations among seman-
tic types, the lexicosyntactic patterns of the defini-
tions correlate with SDT. For example, as shown by 
sentences a~d, when SDT is [Body Part, Organ, or 
Organ Component], its lexicosyntactic patterns 
include ??located??. In contrast, when SDT is 
[Disease or Syndrome], the patterns include 
??due to?? and ?? caused by??.  
 
In this study, we empirically studied statistical cor-
relations between SDT and SDef and between SDT and 
the lexicosyntactic patterns in the definitions. Our 
study is a result of detailed statistical analysis of 
36,535 defined terms and their 226,089 online 
definitions. We built our semantic constraint model 
based on the widely used biomedical knowledge 
resource, the UMLS. We also adapted a robust in-
formation extraction system to generate automati-
cally a large number of lexicosyntactic patterns 
from definitions. In the following, we will first 
describe the UMLS and its semantic types. We will 
then describe our data collection and our methods 
for pattern generation. 
2 Unified Medical Language System 
The Unified Medical Language System (UMLS) is 
the largest biomedical knowledge source main-
tained by the National Library of Medicine. It pro-
vides standardized biomedical concept relations 
and synonyms (Humphreys et al 1998). The 
UMLS has been widely used in many natural lan-
guage processing tasks, including information re-
trieval (Eichmann et al 1998), extraction 
(Rindflesch et al 2000), and text summarization 
(Elhadad et al 2004; Fiszman et al 2004).  
 
The UMLS includes the Metathesaurus (MT), 
which contains over one million biomedical con-
cepts and the Semantic Network (SN), which 
represents a high-level abstraction from the UMLS 
Metathesaurus. The SN consists of 134 semantic 
types with 54 types of semantic relations (e.g., is-a 
or part-of) that relate the semantic types to each 
other. The UMLS Semantic Network provides 
broad and general world knowledge that is related 
to human health. Each UMLS concept is assigned 
one or more semantic types.  
 
The National Library of Medicine also makes 
available MMTx, a programming implementation 
of MetaMap (Aronson 2001), which maps free text 
to the UMLS concepts and associated semantic 
types. MMTx first parses text into sentences, then 
chunks the sentences into noun phrases.  Each 
noun phrase is then mapped to a set of possible 
UMLS concepts, taking into account spelling and 
morphological variations; each concept is 
weighted, with the highest weight representing the 
most likely mapped concept. One recent study has 
evaluated MMTx to have 79% (Yu and Sable 
2005) accuracy for mapping a term to the semantic 
2
type(s) in a small set of medical questions. Another 
study (Lacson and Barzilay 2005) measured 
MMTx to have a recall of 74.3% for capturing the 
semantic types in another set of medical texts. 
 
In this study, we applied MMTx to identify the 
semantic types of terms that appear in their defini-
tions. For each candidate term, MMTx ranks a list 
of UMLS concepts with confidence. In this study, 
we selected the UMLS concept that was assigned 
with the highest confidence by MMTx. The UMLS 
concepts were then used to obtain the correspond-
ing semantic types. 
3 Data Collection 
We collected a large number of online definitions 
for the purpose of our study. Specifically, we ap-
plied more than 1 million of the UMLS concepts as 
candidate definitional terms, and searched for the 
definitions from the World Wide Web using the 
Google:Definition service; this resulted in the 
downloads of a total of 226,089 definitions that 
corresponded to a total of 36,535 UMLS concepts 
(or 3.7% of the total of 1 million UMLS concepts). 
We removed from definitions the defined terms; 
this step is necessary for our statistical studies, 
which we will explain later in the following sec-
tions. We applied MMTx to obtain the correspond-
ing semantic types.   
4 Statistically Correlated Semantic Types 
We then identified statistically correlated semantic 
types between SDT and SDef based on bivariate tabu-
lar chi-square (Fleiss 1981). 
 
 
 
Specifically, given a semantic type STYi, i=1,2,3,?, 134 
of any defined term, the observed numbers of defi-
nitions that were and were not assigned the STYi 
are O(Defi) and O(Defi). All indicates the total 
226,089 definitions. The observed numbers of defi-
nitions in which the semantic type STYi, did and did 
not appear were O(Alli) and O(Alli). 134 represents 
the total number of the UMLS semantic types. We 
applied formulas (1) and (2) to calculate expected 
frequencies and then the chi-square value (the de-
gree of freedom is one). A high chi-square value 
indicates the importance of the semantic type that 
appears in the definition. We removed the defined 
terms from their definitions prior to the semantic-
type statistical analysis in order to remove the bias 
introduced by the defined terms (i.e., defined terms 
frequently appear in the definitions). 
 
      ( )iDefE = N
NN iDef *
, ( )
i
DefE = N
NN iDef *
, 
( )iAllE = N
NN iAll *
, ( )iAllE = N NN iAll *               (1) 
     
( )? ?= E
OE 22?
                                     (2) 
To determine whether the chi-square value is large 
enough for statistical significance, we calculated 
its p-value. Typically, 0.05 is the cutoff of signifi-
cance, i.e. significance is accepted if the corre-
sponding p-value is less than 0.05. This criterion 
ensures the chance of false significance (incor-
rectly detected due to chance) is 0.05 for a single 
SDT-SDef pair. However, since there are 134*134 
possible SDT-SDef pairs, the chance for obtaining at 
least one false significance could be very high. To 
have a more conservative inference, we employed 
a Bonferroni-type correction procedure (Hochberg 
1988).  
 
Specifically, let )()2()1( mppp ??? L be the or-
dered raw p-values, where m is the total number of 
SDT-SDef pairs. A SDef is significantly associated 
with a SDT if SDef?s corresponding p-value 
)1/()( +??? imp i ?  for some i. This correction 
procedure allows the probability of at-least-one-
false-significance out of the total m pairs is less 
than alpha (=0.05). 
 
The number of definitions for each SDT ranges from 
4 ([Entity]), 10 ([Event]), 17 ([Vertebrate]) to 
8,380 ([Amino Acid, Peptide, or Protein]) and 
18,461 ([Organic Chemical]) in our data collection.  
As the power of a statistical test relies on the sam-
ple size, some correlated semantic types might be 
undetected when the number of available defini-
tions is small. It is therefore worthwhile to know 
what the necessary sample size is in order to have a 
decent chance of detecting difference statistically. 
3
For this task, we assume P0 and P1 are true prob-
abilities that a STY will appear in NDef and NAll. 
Based upon that, we calculated the minimal re-
quired number of sentences n such that the prob-
ability of statistical significance will be larger than 
or equal to 0.8. This sample size is determined 
based on the following two assumptions: 1) the 
observed frequencies are approximately normally 
distributed, and 2) we use chi-square significance 
to test the hypothesis P0 = P1 at significance level 
0.05 ( 2
10 PPP
+
= ). 
2
10
2
00112.0025.0
)(
))1()1()1(2(
PP
PPPPzPPz
n
?
?+?+?
>        (3) 
5 Semantic Type Distribution  
Our null hypothesis is that given any pair of 
{SDT(X), SDT(Y)}, X ? Y, where X and Y represent 
two different semantic types of the total 134 se-
mantic types, there are no statistical differences in 
the distributions of the semantic types of the terms 
that appear in the definitions.  
 
We applied the bivariate tabular chi-square test to 
measure the semantic type distribution. Following 
similar notations to Section 4, we use OXi and OYi  
for the corresponding frequencies of not being ob-
served in SDef(X) and SDef(Y). 
 
For each semantic type STY, we calculate the ex-
pected frequencies of being observed and not being 
observed in SDef(X) and SDef(Y), respectively, and 
their corresponding chi-square value according to 
formulas (3) and (4): 
 
      
iX
E =
iYiX
NN
OON
+
+ )*(
iYiXiX
, 
iX
E =
iYiX
iX
NN
OON
+
+ )(*
iYiX
,  
iY
E =
iYiX
NN
OON
+
+ )*(
iYiXiY
,
iY
E =
iYiX
iY
NN
OON
+
+ )(*
iYiX
     (4) 
( ) ( )? ? ?+?=
iY
iY
iX
iX
iYX E
OE
E
OE 2
iY
2
iX2
,,
?
               (5)                               
where NX and NY are the numbers of sentences in 
SDef(X) and SDef(Y), respectively, and in both (4) 
and (5), 134,...,2,1=i , and (X, Y)=1,2,?, 134 and 
X ? Y. The degree of freedom is 1. The chi-square 
value measures whether the occurrences of STYi, 
are equivalent between SDef(X) and SDef(Y). The 
same multiple testing correction procedure will be 
used to determine the significance of the chi-
square value. Note that if at least one STYi has 
been detected to be statistically significant after 
multiple-testing correction, the distributions of the 
semantic types are different between SDef(X) and 
SDef(Y).  
6 Automatically Identifying Semantic-Type-
Dependent Lexicosyntactic Patterns 
Most current definitional question answering sys-
tems generate lexicosyntactic patterns either 
manually or semi-automatically. In this study, we 
automatically generated large sets of lexicosyntac-
tic patterns from our collection of online defini-
tions. We applied the information extraction 
system Autoslog-TS (Riloff and Philips 2004) to 
automatically generate lexicosyntactic patterns in 
definitions. We then identified the statistical corre-
lation between the semantic types of defined terms 
and their lexicosyntactic patterns in definitions. 
AutoSlog-TS is an information extraction system 
that is built upon AutoSlog (Riloff 1996). 
AutoSlog-TS automatically identifies extraction 
patterns for noun phrases by learning from two sets 
of un-annotated texts relevant and non-relevant. 
AutoSlog-TS first generates every possible lexico-
syntactic pattern to extract every noun phrase in 
both collections of text and then computes statis-
tics based on how often each pattern appears in the 
relevant text versus the background and outputs a 
ranked list of extraction patterns coupled with sta-
tistics indicating how strongly each pattern is asso-
ciated with relevant and non-relevant texts.  
We grouped definitions based on the semantic 
types of the defined terms. For each semantic type, 
the relevant text incorporated the definitions, and 
the non-relevant text incorporated an equal number 
of sentences that were randomly selected from the 
MEDLINE collection. For each semantic type, we 
applied AutoSlog-TS to its associated relevant and 
non-relevant sentence collections to generate lexi-
cosyntactic patterns; this resulted in a total of 134 
sets of lexicosyntactic patterns that corresponded 
to different semantic types of defined terms. Addi-
tionally, we identified the common lexicosyntactic 
patterns across the semantic types and ranked the 
lexicosyntactic patterns based on their frequencies 
across semantic types. 
 
4
We also identified statistical correlations between 
SDT and the lexicosyntactic patterns in definitions 
based on chi-square statistics that we have de-
scribed in the previous two sections. For formula 
1~4, we replaced each STY with a lexicosyntactic 
pattern. Our null hypothesis is that given any SDT, 
there are no statistical differences in the distribu-
tions of the lexicosyntactic patterns that appear in 
the definitions. 
 
 
Figure 1: A list of semantic types of de-
fined terms with the top five statistically 
correlated semantic types (P<<0.0001) that 
appear in their definitions.  
7 Results 
Our chi-square statistics show that for any pair of 
semantic types {SDT(X), SDT(Y)}, X ? Y, the distri-
butions of SDef are statistically different at al-
pha=0.05; the results show that the semantic types 
of the defined terms correlate to the semantic types 
in the definitions. Our results also show that the 
syntactic patterns are distributed differently among 
different semantic types of the defined terms (al-
pha=0.05). 
 
Our results show that many semantic types that 
appear in definitions are statistically correlated 
with the semantic types of the defined terms. The 
average number and standard deviation of statisti-
cally correlated semantic types is 80.6?35.4 at 
P<<0.0001.  
Figure 1 shows three SDT ([Body Part, Organ, or 
Organ Component], [Disease or Syndrome], and 
[Organization]) with the corresponding top five 
statistically correlated semantic types that appear 
in their definitions. Our results show that in a total 
of 112 (or 83.6%) cases, SDT appears as one of the 
top five statistically correlated semantic types in 
SDef, and that in a total of 94 (or 70.1%) cases,  SDT 
appears at the top in SDef. Our results indicate that 
if a definitional term has a semantic type SDT, then 
the terms in its definition tend to have the same or 
related semantic types. 
 
We examined the cases in which the semantic 
types of definitional terms do not appear in the top 
five semantic types in the definitions. We found 
that in all of those cases, the total numbers of defi-
nitions that were used for statistical analysis were 
too small to obtain statistical significance. For ex-
ample, when SDT is ?Entity?, the minimum size for 
a SDef  was 4.75, which is larger than the total num-
ber of the definitions (i.e., 4). As a result, some 
actually correlated semantic types might be unde-
tected due to insufficient sample size. 
 
Our results also show that the lexicosyntactic pat-
terns of definitional sentences are SDT-dependent. 
Our results show that many lexicosyntactic pat-
terns that appear in definitions are statistically cor-
related with the semantic types of defined terms. 
The average number and standard deviation of sta-
tistically correlated lexico-syntactic patterns is 
1656.7?1818.9 at P<<0.0001. We found that the 
more definitions an SDT has, the more lexicosyntac-
tic patterns. 
 
Figure 2 shows the top 10 lexicosyntactic patterns 
(based on chi-square statistics) that were captured 
by Autoslog-TS with three different SDT; namely, 
[Disease or Syndrome], [Body Part, Organ, or 
Organ Component], and [Organization]. Figure 3 
shows the top 10 lexicosyntactic patterns ranked 
by AutoSlog-TS which incorporated the frequen-
cies of the patterns (Riloff and Philips 2004). 
 
Figure 4 lists the top 30 common patterns across 
all different semantic types SDT. We found that 
many common lexicosyntactic patterns (e.g., 
??known as??, ??called?, ??include??) have 
been identified by other research groups through 
either manual or semi-automatic pattern discovery 
(Blair-Goldensohn et al 2004). 
 
5
 Figure 2: The top 10 lexicosyntactic patterns that appear in definitions based on chi-square statis-
tics. The defined terms have one of the three semantic types [Disease_or_Syndrome], [Body Part, 
Organ, or Organ Component], and [Organization].  
 
 
Figure 3: The top 10 lexicosyntactic patterns ranked by Autoslog-TS. The defined terms have 
one of the three semantic types [Disease_or_Syndrome], [Body Part, Organ, or Organ Compo-
nent], and [Organization]. 
 
 
Figure 4: The top 30 common lexicosyntactic patterns generated across patterns with different DTS . 
 
8  Discussion 
 
The statistical correlations between SDT and SDef 
may be useful to enhance the performance of a 
definition-question-answering system by at least 
two means. First, the semantic types may be useful 
for word sense disambiguation. A simple applica-
tion is to rank definitional sentences based on the 
distributions of the semantic types of terms in the 
definitions to capture the definition of a specific 
sense. For example, a biomedical definitional ques-
tion answering system may exclude the definition 
of other senses (e.g., ?feeling? as shown in the sen-
tence ?The locus of feelings and intuitions; ?in 
your heart you know it is true?; ?her story would 
melt your heart.??) if the semantic types that define 
?heart? do not include [Body Part, Organ, or Organ 
Component] of terms other than ?heart?. 
 
Secondly, the semantic-type correlations may be 
used as features to exclude non-definitional sen-
tences. For example, a biomedical definitional 
question answering system may exclude the fol-
lowing non-definitional sentence ?Heart rate was 
6
unaffected by the drug? because the semantic types 
in the sentence do not include [Body Part, Organ, 
or Organ Component] of terms other than ?heart?. 
 
SDT-dependent lexicosyntactic patterns may en-
hance both the recall and precision of a definitional 
question answering system. First, the large sets of 
lexicosyntactic patterns we generated automati-
cally may expand the smaller sets of lexicosyntac-
tic patterns that have been reported by the existing 
question answering systems. Secondly, SDT-
dependent lexicosyntactic patterns may be used to 
capture definitions.  
 
The common lexicosyntactic patterns we identified 
(in Figure 4) may be useful for a generic defini-
tional question answering system. For example, a 
definitional question answering system may im-
plement the most common patterns to detect any 
generic definitions; specific patterns may be im-
plemented to detect definitions with specific SDT.  
 
One limitation of our work is that the lexicosyntac-
tic patterns generated by Autoslog-TS are within 
clauses. This is a disadvantage because 1) lexico-
syntactic patterns can extend beyond clauses (Cui 
et al 2005) and 2) frequently a definition has mul-
tiple lexicosyntactic patterns. Many of the patterns 
might not be generalizible. For example, as shown 
in Figure 2, some of the top ranked patterns (e.g., 
?Subj_AuxVp_<dobj>_BE_ARMY>?) identified 
by AutoSlog-TS may be too specific to the text 
collection. The pattern-ranking method introduced 
by AutoSlog-TS takes into consideration the fre-
quency of a pattern and therefore is a better rank-
ing method than the chi-square ranking (shown in 
Figure 3). 
 
9  Related Work 
 
Systems have used named entities (e.g., 
?PEOPLE? and ?LOCATION?) to assist in infor-
mation extraction (Agichtein and Gravano 2000) 
and question answering (Moldovan et al 2002; 
Filatova and Prager 2005). Semantic constraints 
were first explored by (Bodenreider and Burgun 
2002; Rindflesch and Fiszman 2003) who observed 
that the principle nouns in definientia are fre-
quently semantically related (e.g., hyponyms, hy-
pernyms, siblings, and synonyms) to definiena. 
Semantic constraints have been introduced to defi-
nitional question answering (Prager et al 2000; 
Liang et al 2001). For example, an artist?s work 
must be completed between his birth and death 
(Prager et al 2000); and the hyponyms of defined 
terms might be incorporated in the definitions 
(Liang et al 2001). Semantic correlations have 
been explored in other areas of NLP. For example, 
researchers (Turney 2002; Yu and Hatzivassi-
loglou 2003) have identified semantic correlation 
between words and views: positive words tend to 
appear more frequently in positive movie and 
product reviews and newswire article sentences 
that have a positive semantic orientation and vice 
versa for negative reviews or sentences with a 
negative semantic orientation. 
10 Conclusions and Future Work 
This is the first study in definitional question an-
swering that concludes that the semantics of a de-
finiendum constrain both the lexical semantics and 
the lexicosyntactic patterns in the definition. Our 
discoveries may be useful for the building of a 
biomedical definitional question answering system.  
 
Although our discoveries (i.e., that the semantic 
types of the definitional terms determine both the 
lexicosyntactic patterns and the semantic types in 
the definitions) were evaluated with the knowledge 
framework from the biomedical, domain-specific 
knowledge resource the UMLS, the principles may 
be generalizable to any type of semantic classifica-
tion of definitions. The semantic constraints may 
enhance both recall and precision of one-size-fits-
all question answering systems, which may be 
evaluated in future work. 
 
As stated in the Discussion session, one disadvan-
tage of this study is that the lexicosyntactic pat-
terns generated by Autoslog-TS are within clauses. 
Future work needs to develop pattern-recognition 
systems that are capable of detecting patterns 
across clauses.  
 
In addition, future work needs to move beyond 
lexicosyntactic patterns to extract semantic-
lexicosyntactic patterns and to evaluate how the 
semantic-lexicosyntactic   patterns    can    enhance  
definitional question answering. 
7
Acknowledgement: The author thanks Sasha 
Blair-Goldensohn, Vijay Shanker, and especially 
the three anonymous reviewers who provide valu-
able critics and comments. The concepts ?Defini-
endum? and ?Definiens? come from one of the 
reviewers? recommendation. 
References  
 
Agichtein E, Gravano L (2000) Snowball: extracting 
relations from large plain-text collections. . Paper 
presented at Proceedings of the 5th ACM Interna-
tional Conference on Digital Libraries 
Aronson A (2001) Effective Mapping of Biomedical 
Text to the UMLS Metathesaurus: The MetaMap 
Program. Paper presented at American Medical In-
formation Association 
Aronson A, Mork J, Gay G, Humphrey S, Rogers W 
(2004) The NLM Indexing Initiative's Medical Text 
Indexer. Paper presented at MedInfo 2004 
Blair-Goldensohn S, McKeown K, Schlaikjer A (2004) 
Answering Definitional Questions: A Hybrid Ap-
proach. In: Maybury M (ed) New Directions In 
Question Answering. AAAI Press 
Bodenreider O, Burgun A (2002) Characterizing the 
definitions of anatomical concepts in WordNet and 
specialized sources. Paper presented at The First 
Global WordNet Conference 
Cui H, Kan M, Cua T (2005) Generic soft pattern mod-
els for definitional question answering. . Paper pre-
sented at The 28th Annual International ACM 
SIGIR Salvado, Brazil 
Eichmann D, Ruiz M, Srinivasan P (1998) Cross-
language information retrieval with the UMLS 
metathesaurus. Paper presented at SIGIR 
Elhadad N, Kan M, Klavans J, McKeown K (2004) 
Customization in a unified framework for summa-
rizing medical literature. Journal of Artificial Intel-
ligence in Medicine 
Filatova E, Prager J (2005) Tell me what you do and I'll 
tell you what you are: learning occupation-related 
activities for biographies. Paper presented at 
HLT/EMNLP 2005. Vancouver, Canada 
Fiszman M, Rindflesch T, Kilicoglu H (2004) Abstrac-
tion Summarization for Managing the Biomedical 
Research Literature. Paper presented at HLT-
NAACL 2004: Computational Lexical Semantic 
Workshop 
Fleiss J (1981) Statistical methods for rates and propor-
tions. 
Hildebrandt W, Katz B, Lin J (2004) Answering defini-
tion questions with multiple knowledge sources. . 
Paper presented at HLT/NAACL 
Hochberg Y (1988) A sharper Bonferroni procedure for 
multiple tests of significance. Biometrika 75:800-
802 
Humphreys BL, Lindberg DA, Schoolman HM, Barnett 
GO (1998) The Unified Medical Language System: 
an informatics research collaboration. J Am Med 
Inform Assoc 5:1-11. 
Lacson R, Barzilay R (2005) Automatic processing of 
spoken dialogue in the hemodialysis domain. Paper 
presented at Proc AMIA Symp 
Liang L, Liu C, Xu Y-Q, Guo B, Shum H-Y (2001) 
Real-time texture synthesis by patch-based sam-
pling. ACM Trans Graph 20:127--150 
Moldovan D, Harabagiu S, Girju R, Morarescu P, Laca-
tusu F, Novischi A, Badulescu A, Bolohan O 
(2002) LCC tools for question answering. Paper 
presented at The Eleventh Text REtrieval Confer-
ence (TREC 2002) 
Prager J, Brown E, Coden A, Radev D (2000) Quesiton-
answering by predictive annotation. Paper pre-
sented at Proceeding 22nd Annual International 
ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval 
Riloff E (1996) Automatically generating extraction 
patterns from untagged text. . Paper presented at 
AAAI-96  
Riloff E, Philips W (2004) An introduction to the Sun-
dance and AutoSlog Systems. Technical Report 
#UUCS-04-015. University of Utah School of 
Computing.  
Rindflesch T, Tanabe L, Weinstein J, Hunter L (2000) 
EDGAR: extraction of drugs, genes and relations 
from the biomedical literature. Pac Symp Biocom-
put:517-528. 
Rindflesch TC, Fiszman M (2003) The interaction of 
domain knowledge and linguistic structure in natu-
ral language processing: interpreting hypernymic 
propositions in biomedical text. J Biomed Inform 
36:462-477 
Turney P (2002) Thumbs up or thumbs down? Semantic 
orientation applied to unsupervised classification of 
reviews. Paper presented at ACL 2002 
Yu H, Hatzivassiloglou V (2003) Towards answering 
opinion questions: Separating facts from opinions 
and identifying the polarity of opinion sentences. 
Paper presented at Proceedings of the 2003 Confer-
ence on Empirical Methods in Natural Language 
Processing (EMNLP 2003) 
Yu H, Sable C (2005) Being Erlang Shen: Identifying 
answerable questions. Paper presented at Nine-
teenth International Joint Conference on Artificial 
Intelligence on Knowledge and Reasoning for An-
swering Questions  
 
 
8
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 73?80,
New York City, June 2006. c?2006 Association for Computational Linguistics
Exploring Text and Image Features to Classify Images in Bioscience Lit-
erature 
 
Barry Rafkind Minsuk Lee Shih-Fu Chang Hong Yu 
DVMM Group Department of Health Sci-
ences 
DVMM Group Department of Health Sci-
ences 
Columbia University University of Wisconsin-
Milwaukee 
Columbia University University of Wisconsin-
Milwaukee 
New York, NY 10027 Milwaukee, WI  53201 New York, NY 10027 Milwaukee, WI  53201 
Barryr 
@ee.columbia.edu 
Minsuk.Lee 
@gmail.com 
Sfchang 
@ee.columbia.edu 
Hong.Yu @uwm.edu 
 
 
 
Abstract 
A picture is worth a thousand words. 
Biomedical researchers tend to incorpo-
rate a significant number of images (i.e., 
figures or tables) in their publications to 
report experimental results, to present re-
search models, and to display examples of 
biomedical objects. Unfortunately, this 
wealth of information remains virtually 
inaccessible without automatic systems to 
organize these images. We explored su-
pervised machine-learning systems using 
Support Vector Machines to automatically 
classify images into six representative 
categories based on text, image, and the 
fusion of both. Our experiments show a 
significant improvement in the average F-
score of the fusion classifier (73.66%) as 
compared with classifiers just based on 
image (50.74%) or text features (68.54%). 
1 Introduction 
A picture is worth a thousand words. Biomedical 
researchers tend to incorporate a significant num-
ber of figures and tables in their publications to 
report experimental results, to present research 
models, and to display examples of biomedical 
objects (e.g., cell, tissue, organ and other images). 
For example, we have found an average of 5.2 im-
ages per biological article in the journal Proceed-
ings of the National Academy of Sciences (PNAS). 
We discovered that 43% of the articles in the 
medical journal The Lancet contain biomedical 
images. Physicians may want to access biomedical 
images reported in literature for the purpose of 
clinical education or to assist clinical diagnoses. 
For example, a physician may want to obtain im-
ages that illustrate the disease stage of infants with 
Retinopathy of Prematurity for the purpose of 
clinical diagnosis, or to request a picture of ery-
thema chronicum migrans, a spreading annular 
rash that appears at the site of tick-bite in Lyme 
disease. Biologists may want to identify the ex-
perimental results or images that support specific 
biological phenomenon. For example, Figure 1 
shows that a transplanted progeny of a single mul-
tipotent stem cell can generate sebaceous glands. 
Organizing bioscience images is not a new task. 
Related work includes the building of domain-
specific image databases. For example, the Protein 
Data Bank (PDB) 1  (Sussman et al, 1998) stores 
3-D images of macromolecular structure data. 
WebPath 2  is a medical web-based resource that 
has been created by physicians to include over 
4,700 gross and microscopic medical images. Text-
based image search systems like Google ignore 
image content. The SLIF (Subcellular Location 
Image Finder) system (Murphy et al, 2001; Kou et 
al., 2003) searches protein images reported in lit-
erature. Other work has explored joint text-image 
features in classifying protein subcellular location 
images (Murphy et al, 2004). The existing sys-
tems, however, have not explored approaches that 
automatically classify general bioscience images 
into generic categories. 
                                                          
1
 http://www.rcsb.org/pdb/ 
2
 http://www-medlib.med.utah.edu/WebPath/webpath.html 
73
Classifying images into generic categories is an 
important task that can benefit many other natural 
language processing and image processing tasks. 
For example, image retrieval and question answer-
ing systems may return ?Image-of-Thing? images 
(e.g., Figure 1), not the other types (e.g., Figure 
2~5), to illustrate erythema chronicum migrans. 
Biologists may examine ?Gel? images (e.g., Figure 
2), rather than ?Model? (e.g., Figure 4) to access 
specific biological evidence for molecular interac-
tions. Furthermore, a generic category may ease 
the task of identifying specific images that may be 
sub-categories of the generic category. For exam-
ple, a biologist may want to obtain an image of a 
protein structure prediction, which might be a sub-
category of ?Model? (Figure 4), rather than an im-
age of x-ray crystallography that can be readily 
obtained from the PDB database.  
This paper represents the first study that defines 
a generic bioscience image taxonomy, and ex-
plores automatic image classification based on the 
fusion of text and image classifiers. 
 
Gel-Image consists of gel images such as Northern 
(for DNA), Southern (for RNA), and Western (for 
protein).  Figure 2 shows an example. 
 
Graph consists of bar charts, column charts, line 
charts, plots and other graphs that are drawn either 
by authors or by a computer (e.g., results of patch 
clamping). Figure 3 shows an example. 
 
Image-of-Thing refers to images of cells, cell 
components, tissues, organs, or species. Figure 1 
shows an example. 
 
Mix refers to an image (e.g., Figure 5) that incor-
porates two or more other categories of images. 
 
Model: A model may demonstrate a biological 
process, molecular docking, or an experimental 
design. We include as Model any structure (e.g., 
chemical, molecular, or cellular) that is illustrated 
by a drawing. We also include gene or protein se-
quences and sequence alignments, as well as phy-
logenetic trees in this category. Figure 4 shows one 
example.  
 
Table refers to a set of data arranged in rows and 
columns. 
 
Table 1. Bioscience Image Taxonomy 
2 Image Taxonomy 
We downloaded from PubMed Central  a total of 
17,000 PNAS full-text articles (years 1995-2004), 
which contain a total of 88,225 images. We manu-
ally examined the images and defined an image 
taxonomy (as shown in Table 1) based on feedback 
from physicians. The categories were chosen to 
maintain balance between coherence of content in 
each category and the complexity of the taxonomy. 
For example, we keep images of biological objects 
(e.g., cells, tissues, organs etc) in one single cate-
gory in this experiment to avoid over decomposi-
tion of categories and insufficient data in 
individual categories. Therefore we stress princi-
pled approaches for feature extraction and classi-
fier design. The same fusion classification 
framework can be applied to cases where each 
category is further refined to include subclasses. 
 
               
Figure 1. Image of_Thing3  Figure 2. Gel image4 
 
              
Figure 3. Graph image5   Figure 4. Model image6  
           
                        Figure 5. Mix image7     
 
                                                          
3
 This image appears in the cover page of PNAS 102 (41): 
14477 ? 14936. 
4
 The image appears in the article (pmid=10318918) 
5
 The image appears in the article (pmid=15699337) 
6
 The image appears in the article (pmid=11504922) 
7
 The image appears in the article (pmid=15755809)  
74
3 Image Classification  
We explored supervised machine-learning methods 
to automatically classify images according to our 
image taxonomy (Table 1). Since it is straightfor-
ward to distinguish table separately by applying 
surface cues (e.g., ?Table? and ?Figure?), we have 
decided to exclude it from our experiments. 
3.1 Support Vector Machines 
We explored supervised machine-learning systems 
using Support Vector Machines (SVMs) which 
have shown to out-perform many other supervised 
machine-learning systems for text categorization 
tasks (Joachims, 1998). We applied the freely 
available machine learning MATLAB package The 
Spider to train our SVM systems (Sable and Wes-
ton, 2005; MATLAB). The Spider implements 
many learning algorithms including a multi-class 
SVM classifier which was used to learn our dis-
criminative classifiers as described below in sec-
tion 3.4. 
A fundamental concept in SVM theory is the 
projection of the original data into a high-
dimensional space in which separating hyperplanes 
can be found. Rather than actually doing this pro-
jection, kernel functions are selected that effi-
ciently compute the inner products between data in 
the high-dimensional space. Slack variables are 
introduced to handle non-separable cases and this 
requires an upper bound variable, C. 
Our experiments considered three popular ker-
nel function families over five different variants 
and five different values of C. The kernel function 
implementations are explained in the software 
documentation. We considered kernel functions in 
the forms of polynomial, radial basis function, and 
Gaussian. The adjustable parameter for polynomial 
functions is the order of the polynomial. For radial 
basis function and Gaussian functions, sigma is the 
adjustable parameter. A grid search was performed 
over the adjustable parameter for values 1 to 5 and 
for values of C equal to [10^0, 10^1, 10^2, 10^3, 
10^4]. 
3.2 Text Features 
Previous work in the context of newswire image 
classification show that text features in image cap-
tions are efficient for image categorization (Sable, 
2000, 2002, 2003). We hypothesize that image 
captions provide certain lexical cues that effi-
ciently represent image content. For example, the 
words ?diameter?, ?gene-expression?, ?histogram?, 
?lane?, ?model?, ?stained?, ?western?, etc are 
strong indicators for image classes and therefore 
can be used to classify an image into categories. 
The features we explored are bag-of-words and n-
grams from the image captions after processing the 
caption text by the Word Vector Tool (Wurst). 
3.3 Image Features  
We also investigated image features for the tasks 
of image classification. We started with four types 
of image features that include intensity histogram 
features, edge-direction histogram features, edge-
based axis features, and the number of 8-connected 
regions in the binary-valued image obtained from 
thresholding the intensity.  
The intensity histogram was created by quantiz-
ing the gray-scale intensity values into the range 0-
255 and then making a 256-bin histogram for these 
values. The histogram was then normalized by di-
viding all values by the total sum. For the purpose 
of entropy calculations, all zero values in the his-
togram are set to one. From this adjusted, normal-
ized histogram, we calculated the total entropy as 
the sum of the products of the entries with their 
logarithms. Additionally, the mean, 2nd moment, 
and 3rd moment are derived. The combination of 
the total entropy, mean, 2nd, and 3rd moments 
constitute a robust and concise representation of 
the image intensity. 
Edge-Direction Histogram (Jain and Vailaya, 
1996) features may help distinguish images with 
predominantly straight lines such as those found in 
graphs, diagrams, or charts from other images with 
more variation in edge orientation. The EDH be-
gins by convolving the gray-scale image with both 
3x3 Sobel edge operators (Jain, 1989). One opera-
tor finds vertical gradients while the other finds 
horizontal gradients. The inverse tangent of the 
ratio of the vertical to horizontal gradient yields 
continuous orientation values in the range of ?pi to 
+pi. These values are subsequently converted into 
degrees in the range of 0 to 179 degrees (we con-
sider 180 and 0 degrees to be equal). A histogram 
is counted over these 180 degrees. Zero values in 
the histogram are set to one in order to anticipate 
entropy calculations and then the modified histo-
gram is normalized to sum to one. Finally, the total 
75
entropy, mean, 2nd and 3rd moments are extracted 
to summarize the EDH. 
The edge-based axis features are meant to help 
identify images containing graphs or charts. First, 
Sobel edges are extracted above a sensitivity 
threshold of 0.10 from the gray-scale image. This 
yields a binary-valued intensity image with 1?s 
occurring in locations of all edges that exceed the 
threshold and 0?s occurring otherwise. Next, the 
vertical and horizontal sums of this intensity image 
are taken yielding two vectors, one for each axis. 
Zero values are set to one to anticipate the entropy 
calculations. Each vector is then normalized by 
dividing each element by its total sum. Finally, we 
find the total entropy, mean, 2nd , and 3rd mo-
ments to represent each axis for a total of eight axis 
features. 
The last image feature under consideration was 
the number of 8-connected regions in the binary-
valued, thresholded Sobel edge image as described 
above for the axis features. An 8-connected region 
is a group of edge pixels for which each member 
touches another member vertically, horizontally, or 
diagonally in the eight adjacent pixel positions sur-
rounding it. The justification for this feature is that 
the number of solid regions in an image may help 
separate classes. 
A preliminary comparison of various combina-
tions of these image features showed that the inten-
sity histogram features used alone yielded the best 
classification accuracy of approximately 54% with 
a quadratic kernel SVM using an upper slack limit 
of C = 10^4. 
3.4 Fusion  
We integrated both image and text features for the 
purpose of image classification. Multi-class SVM?s 
were trained separately on the image features and 
the text features. A multi-class SVM attempts to 
learn the boundaries of maximal margin in feature 
space that distinguishes each class from the rest. 
Once the optimal image and text classifiers were 
found, they were used to process a separate set of 
images in the fusion set. We extracted the margins 
from each data point to the boundary in feature 
space.  
Thus, for a five-class classifier, each data point 
would have five associated margins. To make a 
fair comparison between the image-based classifier 
and the text-based classifier, the margins for each 
data point were normalized to have unit magnitude. 
So, the set of five margins for the image classifier 
constitutes a vector that then gets normalized by 
dividing each element by its L2 norm. The same is 
done for the vector of margins taken from the text 
classifier. Finally, both normalized vectors are 
concatenated to form a 10-dimensional fusion vec-
tor. To fuse the margin results from both classifi-
ers, these normalized margins were used to train 
another multi-class SVM.  
A grid search through parameter space with 
cross validation identified near-optimal parameter 
settings for the SVM classifiers.  See Figure 6 for 
our system flowchart. 
 
 
Figure 6. System Flow-chart 
3.5 Training, Fusion, and Testing Data  
We randomly selected a subset of 554 figure im-
ages from the total downloaded image pool. One 
author of this paper is a biologist who annotated 
figures under five classes; namely, Gel_Image 
(102), Graph (179), Image_of_Thing (64), Mix 
(106), and Model (103). 
These images were split up such that for each 
category, roughly a half was used for training, a 
quarter for fusion, and a quarter for testing (see 
Figure 7). The training set was used to train classi-
76
fiers for the image-based and text-based features. 
The fusion set was used to train a classifier on top 
of the results of the image-based and text-based 
classifiers. The testing set was used to evaluate the 
final classification system. 
For each division of data, 10 folds were gener-
ated. Thus within the training and fusion data sets, 
there are 10 folds which each have a randomized 
partitioning into 90% for training and 10% for test-
ing. The testing data set did not need to be parti-
tioned into folds since all of it was used to test the 
final classification system. (See Figure 8). 
In the 10-fold cross-validation process, a classi-
fier is trained on the training partition and then 
measured for accuracy (or error rate) on the testing 
partition. Of the 10 resulting algorithms, the one 
which performs the best is chosen (or just one 
which ties for the best accuracy). 
 
 
Figure 7. Image-set Divisions 
3.6 Evaluation Metrics  
We report the widely used recall, precision, and F-
score (also known as F-measure) as the evaluation 
metrics for image classification. Recall is the total 
number of true positive predictions divided by the 
total number of true positives in the set (true pos + 
false neg). Precision is the fraction of the number 
of true positive predictions divided by the total 
number of positive predictions (true pos + false 
pos). F-score is the harmonic mean of recall and 
precision equal to (C. J. van Rijsbergen, 1979): 
( )recallprecisionrecallprecision +/**2  
 
 
Figure 8. Partitioning Method for Training and 
Fusion Datasets 
4 Experimental Results 
Table 2 shows the Confusion Matrix for the image 
feature classifier obtained from the testing part of 
the training data. The actual categories are listed 
vertically and predicted categories are listed hori-
zontally. For instance, of 26 actual GEL images, 
18 were correctly classified as GEL, 4 were mis-
classified as GRAPH, 2 as IMAGE_OF_THING, 0 
as MIX, and 2 as MODEL. 
 
Actual  Predicted Categories 
 Gel Graph Thing Mix Model 
Gel 18 4 2 0 2 
Graph 3 39 0 1 1 
Img_Thing 1 1 12 2 0 
Mix 4 17 0 3 3 
Model 8 13 0 1 3 
Table 2. Confusion Matrix for Image Feature Clas-
sifier 
 
A near-optimal parameter setting for the classi-
fier based on image features alone used a polyno-
mial kernel of order 2 and an upper slack limit of C 
= 10^4. Table 3 shows the performance of image 
classification with image features. True Positives, 
False Positives, False Negatives, Precision = 
TP/(TP+FP), Recall = TP/(TP+FN), and F-score = 
2 * Precision * Recall / (Precision + Recall). Ac-
cording to the F-score scores, this classifier does 
best on distinguishing IMAGE_OF_THING im-
ages. The overall accuracy = sum of true positives / 
total number of images = (18+39+12+3+3)/138 = 
75/138 =  54%. This can be compared with the 
baseline of (3+39+1+1)/138 = 32% if all images 
77
were classified as the most popular category, 
GRAPH. Clearly, the image-based classifier does 
best at recognizing IMAGE_OF_THING figures. 
 
Category TP FP FN Prec. Recall Fscore 
Gel 18 16 8 0.529 0.692 0.600 
Graph 39 35 5 0.527 0.886 0.661 
Img_Thing 12 2 4 0.857 0.750 0.800 
Mix 3 4 10 0.429 0.231 0.300 
Model 3 6 22 0.333 0.120 0.176 
Table 3. Precision, Recall, F-score for Image Clas-
sifier 
 
Actual  Predicted Categories 
 Gel Graph Thing Mix Model 
Gel 22 2 0 2 0 
Graph 4 36 0 4 0 
Img_Thing 0 3 11 1 1 
Mix 3 9 1 12 2 
Model 3 5 0 3 14 
Table 4. Confusion Matrix for Caption Text Clas-
sifier 
 
Category TP FP FN Prec Recall Fscore 
Gel 22 10 4 0.688 0.845 0.758 
Graph 36 19 8 0.655 0.818 0.727 
Img_Thing 11 1 5 0.917 0.688 0.786 
Mix 12 10 15 0.545 0.444 0.489 
Model 14 3 11 0.824 0.560 0.667 
Table 5. Precision, Recall, F-score for Caption 
Text Classifier 
 
The text-based classifier excels in finding GEL, 
GRAPH, and IMAGE_OF_THING images. It 
achieves an accuracy of (22+36+11+12+14)/138 = 
95/138 = 69%. 
 
A near-optimal parameter setting for the fusion 
classifier based on both image features and text 
features used a linear kernel with C = 10. The cor-
responding Confusion matrix follows in Table 6. 
 
Actual  Predicted Categories 
 Gel Graph Thing Mix Model 
Gel 23 0 0 3 0 
Graph 2 37 1 2 2 
Img_Thing 0 1 15 0 0 
Mix 2 7 1 14 3 
Model 3 5 0 4 13 
Table 6. Confusion Matrix for Fusion Classifier 
 
 
Category TP FP FN Prec. Recall Fscore 
Gel 23 7 3 0.767 0.885 0.822 
Graph 37 13 7 0.740 0.841 0.787 
Img_Thing 15 2 1 0.882 0.938 0.909 
Mix 14 9 13 0.609 0.519 0.560 
Model 13 5 12 0.722 0.520 0.605 
Table 7. Precision, Recall, F-score for Fusion 
Classifier 
 
From Table 7, it is apparent that the fusion clas-
sifier does best on IMAGE_OF_THING and also 
performs well on GEL and GRAPH. These are 
substantial improvements over the classifiers that 
were based on image or text feature alone. Average 
F-scores and accuracies are summarized below in 
Table 8. 
The overall accuracy for the fusion classifier = 
sum of true positives / total number of image = 
(23+37+15+14+13)/138 = 102/138 = 74%. This 
can be compared with the baseline of 44/138 = 
32% if all images were classified as the most popu-
lar category, GRAPH. 
 
Classifier Average F-score  Accuracy 
Image 50.74% 54% 
Caption 
Text 
68.54% 69% 
Fusion 73.66% 74% 
Table 8. Comparison of Average F-scores and Ac-
curacy among all three Classifiers 
5 Discussion 
It is not surprising that the most difficult category 
to classify is Mix. This was due to the fact that Mix 
images incorporate multiple categories of other 
image types. Frequently, one other image type that 
appears in a Mix image dominates the image fea-
tures and leads to its misclassification as the other 
image type. For example, Figure 9 shows that a 
Mix image was misclassified as Gel_Image.  
 
This mistake is forgivable because the image 
does contain sub-images of gel-images, even 
though the entire figure is actually a mix of gel-
images and diagrams. This type of result highlights 
the overlap between classifications and the diffi-
culty in defining exclusive categories. 
For both misclassifications, it is not easy to 
state exactly why they were classified wrongly 
based on their image or text features. This lack of 
78
intuitive understanding of discriminative behavior 
of SVM classifiers is a valid criticism of the tech-
nique. Although generative machine learning 
methods (such as Bayesian techniques or Graphical 
Models) offer more intuitive models for explaining 
success or failure, discriminative models like SVM 
are adopted here due to their higher performance 
and ease of use. 
Figure 10 shows an example of a MIX figure 
that was mislabeled by the image classifier as 
GRAPH and as GEL_IMAGE by the text classi-
fier. However, it was correctly labeled by the fu-
sion classifier. This example illustrates the value of 
the fusion classifier for being able to improve upon 
its component classifiers. 
6 Conclusions 
From the comparisons in Table 8, we see that fus-
ing the results of classifiers based on text and im-
age features yields approximately 5% 
improvement over the text -based classifier alone 
with respect to both average F-score and Accuracy. 
In fact, the F-score improved for all categories ex-
cept for MODEL which experienced a 6% drop. 
The natural conclusion is that the fusion classifier 
combines the classification performance from the 
text and image classifiers in a complementary fash-
ion that unites the strengths of both. 
7 Future Work 
To enhance the performance of the text features, 
one may restrict the vocabulary to functionally im-
portant biological words. For example, ?phos-
phorylation? and ?3-D? are important words that 
might sufficiently separate ?protein function? from 
?protein structure?. 
Further experimentation on a larger image set 
would give us even greater confidence in our re-
sults. It would also expand the diversity within 
each category, which would hopefully lead to bet-
ter generalization performance of our classifiers. 
Other possible extensions of this work include 
investigating different machine learning ap-
proaches besides SVMs and other fusion methods. 
Additionally, different sets of image and text fea-
tures can be explored as well as other taxonomies. 
 
 
 
 
Caption: ?The 2.6-kb HincII XhoI fragment con-
taining approximately half of exon 4 and exon 5 
and 6 was subcloned between the Neo gene and 
thymidine kinase (Fig. 1 A). The location of the 
genomic probe used to screen for homologous re-
combination is shown in Fig. 1 A. Gene Targeting 
in Embryonic Stem (ES) Cells and Generation of 
Mutant Mice. Genomic DNA of resistant clones 
was digested with SacI and hybridized with the 3 
0.9-kb KpnI SacI external probe (Fig. 1 A). Chi-
meric male offspring were bred to C57BL/6J fe-
males and the agouti F1 offspring were tested for 
transmission of the disrupted allele by Southern 
blot analysis of SacI-digested genomic DNA by 
using the 3 external probe (Fig. 1 A and B). A 360-
bp region, including the first 134 bp of the 275-bp 
exon 4, was deleted and replaced with the PGKneo 
cassette in the reverse orientation (Fig. 1 A). After 
selection with G418 and gangciclovir, doubly re-
sistant clones were screened for homologous re-
combination by Southern blotting and 
hybridization with a 3 external probe (Fig. 1 A). 
Offspring were genotyped by Southern blotting of 
genomic tail DNA and hybridized with a 3 external 
probe (Fig. 1 B). To confirm that HFE / mice do 
not express the HFE gene product, we performed 
Northern blot analyses ? 
Figure 9. Above, caption text and image of a MIX 
figure mis-classified as GEL_IMAGE by the Fu-
sion Classifier 
 
79
 ?Conductance properties of store-operated channels in 
A431 cells. (a) Store-operated channels in A431 cells, 
activated by the mixture of 100 mM BAPTA-AM and 1 
mM Tg in the bath solution, were recorded in c/a mode 
with 105 mM Ba2+ (Left), 105 mM Ca2+ (Center), and 
140 mM Na+ (Right) in the pipette solution at mem-
brane potential as indicated. (b) Fit to the unitary cur-
rent-voltage relationship of store-operated channels with 
Ba2+ (n = 46), Ca2+ (n = 4), Na+ (n = 3) yielded slope 
single-channel conductance of 1 pS for Ca2+ and Ba2+ 
and 6 pS for Na+. (c) Open channel probability of store-
operated channels (NPomax30) expressed as a function 
of membrane potential. Data from six independent ex-
periments in c/a mode with 105 mM Ba2+ as a current 
carrier were averaged at each membrane potential. (b 
and c) The average values are shown as mean ? SEM, 
unless the size of the error bars is smaller than the size 
of the symbols.? 
Figure 10.  Above, caption text and image of a 
MIX figure incorrectly labeled as GRAPH by Im-
age Classifier and GEL_IMAGE by the Text Clas-
sifier 
Acknowledgements 
We thank three anonymous reviewers for their 
valuable comments. Hong Yu and Minsuk Lee ac-
knowledge the support of JDRF 6-2005-835. 
References  
Anil K. Jain and A. Vailaya., August 1996, Image re-
trieval using color and 
shape. Pattern Recognition, 29:1233?1244 
Anil K. Jain, Fundamentals of Digital Image Processing, 
Prentice Hall, 1989 
C. J. van Rijsbergen. Information Retrieval. Butter-
worths, London, second edition, 1979. 
Joachims T, 1998, Text categorization with support vec-
tor machines: Learning with many relevant features. 
Presented at Proceedings of ECML-98, 10th Euro-
pean Conference on Machine Learning 
Kou, Z., W.W. Cohen and R.F. Murphy. 2003. Extract-
ing Information from Text and Images for Location 
Protemics, pp. 2-9. In ACM SIGKDD Workshop on 
Data Mining in Bioinformatics (BIOKDD). 
Murphy, R.F., M. Velliste, J. Yao, and P.G. 2001. 
Searching Online Journals for Fluorescence Micro-
scope Images depicting Protein Subcellular Location 
Patterns, pp. 119-128. In IEEE International Sympo-
sium on Bio-Informatics and Biomedical Engineering 
(BIBE). 
Murphy, R.F., Kou, Z., Hua, J., Joffe, M., and Cohen, 
W. 2004. Extracting and structuring subcellular lo-
cation information from on-line journal articles: the 
subcellular location image finder. In Proceedings of 
the IASTED International Conference on Knowledge 
Sharing and Collaborative Engineering (KSCE2004), 
St. Thomas, US Virgin Islands, pp. 109-114. 
Sable, C. and V. Hatzivassiloglou. 2000. Text-based 
approaches for non-tropical image categorization. 
International Journal on Digital Libraries. 3:261-275. 
Sable, C., K. McKeown and K. Church. 2002. NLP 
found helpful (at least for one text categorization 
task). In Proceedings of Empirical Methods in Natu-
ral Language Processing (EMNLP). Philadelphia, PA 
Sable, C. 2003. Robust Statistical Techniques for the 
Categorization of Images Using Associated Text. In 
Computer Science. Columbia University, New York. 
Sussman J.L., Lin D., Jiang J., Manning N.O., Prilusky 
J., Ritter O., Abola E.E. (1998) Protein Data Bank 
(PDB): Database of Three-Dimensional Structural In-
formation of Biological Macromolecules. Acta Crys-
tallogr D Biol Crystallogr 54:1078-1084 
MATLAB ?. The Mathworks Inc., 
http://www.mathworks.com/ 
Weston, J., A. Elisseeff, G. BakIr, F. Sinz. Jan. 26th, 
2005. The SPIDER: object-orientated machine learn-
ing library. Version 6. MATLAB Package. 
http://www.kyb.tuebingen.mpg.de/bs/people/spider/ 
Wurst, M., Word Vector Tool, Univerist?t Dortmund, 
http://www-ai.cs.uni-
dortmund.de/SOFTWARE/WVTOOL/index.html 
80
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 92?93,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
 
A Pilot Annotation to Investigate Discourse Connectivity in Biomedical Text  
 
 
 
Hong Yu, Nadya Frid, Susan McRoy Rashmi Prasad, Alan Lee, Aravind Joshi 
University of Wisconsin-Milwaukee University of Pennsylvania 
P.O.Box 413 3401 Walnut Street 
Milwaukee, WI 53201 Philadelphia, PA 19104, USA 
Hongyu,frid,mcroy@uwm.edu Rjprasad,aleewk,joshi@seas.upenn.edu
 
 
 
 
 
 
Abstract 
The goal of the Penn Discourse Treebank 
(PDTB) project is to develop a large-scale cor-
pus, annotated with coherence relations marked 
by discourse connectives. Currently, the primary 
application of the PDTB annotation has been to 
news articles. In this study, we tested whether 
the PDTB guidelines can be adapted to a differ-
ent genre. We annotated discourse connectives 
and their arguments in one 4,937-token full-text 
biomedical article. Two linguist annotators 
showed an agreement of 85% after simple con-
ventions were added. For the remaining 15% 
cases, we found that biomedical domain-specific 
knowledge is needed to capture the linguistic 
cues that can be used to resolve inter-annotator 
disagreement. We found that the two annotators 
were able to reach an agreement after discussion. 
Thus our experiments suggest that the PDTB an-
notation can be adapted to new domains by mini-
mally adjusting the guidelines and by adding 
some further domain-specific linguistic cues. 
1 Introduction 
Large scale annotated corpora, e.g., the Penn 
TreeBank (PTB) project (Marcus et al 1993), 
have played an important role in text-mining. 
The Penn Discourse Treebank (PDTB) 
(http://www.seas.upenn.edu/~pdtb) (Prasad et al 
2008a) annotates the argument structure, seman-
tics, and attribution of discourse connectives and 
their arguments. The current release of PDTB-
2.0 contains the annotations of 1,808 Wall Street 
Journal articles (~1 million words) from the 
Penn TreeBank (Marcus et al 1993) II distribu-
tion and a total of 40,600 discourse connective  
tokens (Prasad et al 2008b). This work exam-
ines whether the PDTB annotation guidelines 
can be adapted to a different genre, the biomedi-
cal literature.  
2 Notation 
A discourse connective can be defined as a 
word or multiword expression that signals a 
discourse relation. Discourse connectives 
can be subordinating conjunctions (e.g., be-
cause, when, although), coordinating con-
junctions (e.g., but, or, nor) and adverbials 
(e.g., however, as a result, for example). A 
discourse connective takes in two argu-
ments, Arg1 and Arg2. Arg2 is the argument 
that appears in the clause that is syntacti-
cally bound to the connective and Arg1 is 
the other argument. In the sentence ?John 
failed the exam because he was lazy? the dis-
course connective is underlined, Arg1 ap-
pears in italics and Arg2 appears in bold. 
3 A Pilot Annotation 
Following the PDTB annotation manual (Prasad 
et al 2008b), we conducted a pilot annotation of 
discourse connectivity in biomedical text. As an 
initial step, we only annotated the three most 
92
important components of a discourse relation; 
namely, a discourse connective and its two ar-
guments; we did not annotate attribution. Two 
linguist annotators independently annotated one 
full-text biomedical article (Verpy et al 1999) 
that we randomly selected. The article is 4,937 
tokens long. When the annotation work was 
completed, we measured the inter-annotator 
agreement, following the PDTB exact match 
criterion (Miltsakaki et al 2004). According to 
this criterion, a discourse relation is in dis-
agreement if there is disagreement on any text-
span (i.e., the discourse connective or any of its 
two arguments). In addition, we also measured 
the agreement in the components (i.e., discourse 
connectives and the arguments). We discussed 
the annotation results and made suggestions to 
adapt the PDTB guidelines to biomedical text.  
4 Results and Discussion 
The first annotator identified 74 discourse con-
nectives, and the second annotator identified 75, 
68 of which were the same as those identified by 
the first annotator. The combined total number 
of discourse connectives was 81. The overall 
agreement in discourse connective identification 
was 68/81=84%.  
 
Of the 68 discourse connectives that were anno-
tated by both annotators, 31 were an exact 
match, 31 had an exact match for Arg1, and 54 
had an exact match for Arg2. The overall 
agreement for the 68 discourse relations is 
45.6% for exact match, 45.6% for Arg1, and 
79.4% for Arg2. The PDTB also reported a 
higher level of agreement in annotating Arg2 
than in annotating Arg1 (Miltsakaki et al 2004). 
We manually analyzed the cases with disagree-
ment. We found the disagreements are nearly all 
related to the annotation of citation references, 
supplementary clauses, and other conventions. 
When a few conventions for these cases were 
added, the inter-annotator agreement went up to 
85%. We also found that different interpretation 
of a relation and its arguments by annotators 
plays an important role for the remaining 15% 
inconsistency, and domain-specific knowledge 
is necessary to resolve such cases.   
 
5 New Conventions 
After the completion of the pilot annotation and 
the discussion, we decided to add the following 
conventions to the PDTB annotation guidelines 
to address the characteristics of biomedical text: 
 
i. Citation references are to be annotated as 
a part of an argument because the inclu-
sion will benefit many text-mining tasks 
including identifying the semantic rela-
tions among citations. 
ii. Clausal supplements (e.g., relative or 
parenthetical constructions) that modify  
arguments but are not minimally 
necessary for the interpretation of the 
relation,  are annotated as part of the 
arguments. 
iii. We will annotate a wider variety of 
nominalizations as arguments than 
allowed by the PDTB guidelines. 
 
We anticipate that these changes will both de-
crease the amount of effort required for annota-
tion and increase the reliability of the 
annotation. 
6 References 
Marcus M, Santorini B, Marcinkiewicz M (1993) 
Building a Large Annotated Corpus of Eng-
lish: The Penn Treebank. Computational 
Linguistics 19 
Miltsakaki E, Prasad R, Joshi A, Webber B (2004) 
Annotating discourse connectives and their 
arguments. Paper presented at Proceedings 
of the NAACL/HLT Workshop: Frontiers in 
Corpus Annotation 
Prasad R, Dinesh N, Lee A, Miltsakaki E, Robaldo L, 
Joshi A, Webber B (2008a) The Penn Dis-
course Treebank 2.0. Paper presented at The 
6th International Conference on Language 
Resources and Evaluation (LREC). Marra-
kech, Morroco 
Prasad R, Miltsakaki E, Dinesh N, Lee A, Joshi A, 
Robaldo L, Webber B (2008b) The Penn 
Discourse TreeBank 2.0 Annotation Manual. 
Technical Report: IRCS-08-01 
Verpy E, Leibovici M, Petit C (1999) Characteriza-
tion of otoconin-95, the major protein of 
murine otoconia, provides insights into the 
formation of these inner ear biominerals. 
Proc Natl Acad Sci U S A 96:529-534 
 
 
93
Proceedings of the Workshop on BioNLP, pages 171?178,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Evaluation of the Clinical Question Answering Presentation 
 
Yong-Gang Cao John Ely Lamont Antieau Hong Yu 
College of Health Sci-
ences 
Carver College of 
Medicine 
College of Health Sci-
ences 
College of Health Sci-
ences 
University of Wisconsin 
Milwaukee 
University of Iowa University of Wisconsin  
Milwaukee 
University of Wisconsin 
Milwaukee 
Milwaukee, WI 
53211,USA 
Iowa, IA 52242,USA Milwaukee, WI 
53211,USA 
Milwaukee, WI 
53211,USA 
yonggang@uwm.edu john-
ely@uiowa.edu 
antieau@uwm.edu hongyu@uwm.edu 
 
 
 
Abstract 
Question answering is different from infor-
mation retrieval in that it attempts to an-
swer questions by providing summaries 
from numerous retrieved documents rather 
than by simply providing a list of docu-
ments that requires users to do additional 
work.  However, the quality of answers that 
question answering provides has not been 
investigated extensively, and the practical 
approach to presenting question answers 
still needs more study. In addition to fac-
toid answering using phrases or entities, 
most question answering systems use a sen-
tence-based approach for generating an-
swers. However, many sentences are often 
only meaningful or understandable in their 
context, and a passage-based presentation 
can often provide richer, more coherent 
context. However, passage-based presenta-
tions may introduce additional noise that 
places greater burden on users. In this 
study, we performed a quantitative evalua-
tion on the two kinds of presentation pro-
duced by our online clinical question 
answering system, AskHERMES 
(http://www.AskHERMES.org). The over-
all finding is that, although irrelevant con-
text can hurt the quality of an answer, the 
passage-based approach is generally more 
effective in that it provides richer context 
and matching across sentences. 
1 Introduction 
Question answering is different from informa-
tion retrieval in that it attempts to answer ques-
tions by providing summaries from numerous 
retrieved documents rather than by simply pro-
viding a list of documents for preparing the user 
to do even more exploration. The presentation of 
answers to questions is a key factor in its effi-
ciently meeting the information needs of infor-
mation users. 
 
While different systems have adopted a variety 
of approaches for presenting the results of ques-
tion answering, the efficacy of the use of these 
different approaches in extracting, summarizing, 
and presenting results from the biomedical lit-
erature has not been adequately investigated.  In 
this paper, we compare the sentence-based ap-
proach and the passage-based approach by using 
our own system, AskHERMES, which is de-
signed to retrieve passages of text from the bio-
medical literature in response to ad hoc clinical 
questions.   
2 Background 
2.1 Clinical Question Collection 
The National Library of Medicine (NLM) has 
published a collection of 4,653 questions that 
can be freely downloaded from the Clinical 
Questions Collection website1 and includes the 
questions below: 
 
                                                           
1
 http://clinques.nlm.nih.gov/JitSearch.html 
171
Question 1: ?The maximum dose of estradiol 
valerate is 20 milligrams every 2 weeks. We 
use 25 milligrams every month which seems to 
control her hot flashes. But is that ade-
quate for osteoporosis and cardiovascular 
disease prevention?? 
 
Question 2: ?Child has pectus carinatum. Ra-
diologist told Dr. X sometimes there are as-
sociated congenital heart problems. Dr. X 
wants to study up on this. Does the patient 
have these associated problems?? 
 
Such examples show that clinicians pose com-
plex questions of a far greater sophistication 
than the simple term searches that typical infor-
mation retrieval systems require as input. Ask-
HERMES, however, has been designed to 
handle such complexity as it encounters it. 
2.2 Result Presentation 
In recent years, there has been an emergence of 
numerous search engines ? both open domain 
and domain-specific ? as well as question an-
swering systems, and these systems have em-
ployed a variety of methods for presenting their 
results, including the use of metadata, sentences, 
snippets, and passages. PubMed (Anon 2009a) 
and EAGLi (Anon 2009b), for example, use ar-
ticle metadata to present their results, and the 
combination of title, author name and publica-
tion name that they use works like the citation at 
the end of a paper to provide users with a gen-
eral idea of what the listed article is about. On 
the other hand, AnswerBus (Anon 2009c) and 
AnswerEngine (Anon 2009d) extract sentences 
from relevant articles, then rank and list them 
one by one to answer the questions that users 
have.  In response to a query, Google and other 
general search engines provide the title of a 
work plus a snippet of text to provide metadata 
as well as multiple matching hints from articles. 
In response to user questions, Start (Anon 
2009e), Powerset(Anon 2009f) and Ask (Anon 
2009g) provide a single passage as output, mak-
ing them ideal for answering simple questions 
because they do not require users to access and 
read extra articles in order to answer the ques-
tions they have.  
 
Each of these methods of presentation has 
strengths and weaknesses.  First, a strength of 
using metadata is that it provides a way for dis-
covering the general idea of an article, but it 
does not explain to a user why the article is rele-
vant to the query or question, making it difficult 
to decide whether it is worth the time and effort 
to access the listed article to read more. An ap-
proach presenting a single sentence in response 
to a query can result in a good answer if the user 
is lucky but typically provides a limited idea of 
what the target article contains and demands that 
users access the source of the item to learn more. 
A snippet-based approach can provide a hint as 
to why the target article is relevant, but snippets 
are limited in that they are composed of seg-
ments and usually cannot be read at all; even 
presenting a snippet with metadata as Google 
does is not suitable for adequately answering 
many questions.  
 
We propose a passage-based approach in which 
each passage is constructed by coherent sen-
tences.  The approach we propose is similar to 
that used by Start and Ask, but these systems 
have limited knowledge bases and require que-
ries to be written using very specific question 
types. On the other hand, our system will be able 
to answer ad hoc questions (that is, questions not 
limited to specific types).  Furthermore, the sys-
tem we propose will be oriented toward answer-
ing questions in the biomedical community, a 
field in which automated question answering 
and information retrieval and extraction are in 
strong demand.    
3 Passage-Based Approach versus Sen-
tence-Based Approach  
We define as sentence-based approaches those 
approaches that return a list of independently 
retrieved and ranked sentences. Although all the 
sentences are assumed to be relevant to the ques-
tion, there are no assumptions of their relation-
ship with each other. On the other hand, a 
passage-based approach is defined as one that 
returns a list of independently retrieved and 
ranked passages, each of which can comprise 
multiple tightly coupled sentences. 
 
The passage-based approach has two benefits: 
 
1. It provides richer context for reading 
and understanding. 
2. It provides greater evidence for relevant 
ranking of the passage by matching 
across sentences. 
 
For example, in Figure 1, the passage-based out-
put of the top results of AskHERMES pertains 
to the question ?What is the difference between 
the Denver ii and the regular Denver develop-
mental screening test?? The first answer is a 
passage with two sentences; the first sentence in 
the passage informs users that there have been 
172
criticisms of the ?Denver Developmental 
Screening Test,? and the second sentence shows 
that ?Denver II? addressed several concerns of 
the ?Denver Developmental Screening Test.? 
The two sentences indicate that the article will 
mention several issues that answer the question. 
And the second passage directly shows the an-
swer to the question: The criteria to select Den-
ver II and the difference between the two tests.  
 
If we use the sentence-based approach (see Fig-
ure 2), the sentences in the first passage will be 
ranked very low and might not appear in the re-
sults because both of them contain only one of 
the screening tests mentioned in the question. 
The second passage will be reduced to only the 
second sentence, which is an incomplete answer 
to the question; consequently, the user may re-
main uninformed of the selection criteria be-
tween the two screening tests without further 
examination of the article. Figure 2 shows the 
sentence-based output of the same question. A 
comparison of the examples in the figure clearly 
shows how the results of the query are affected 
by the two approaches. The first result is incom-
plete, and the second and third results are irrele-
vant to the question although they have many 
matched terms. 
 
 
Figure 1. AskHERMES? passage-based output for the question ?What is the difference between the Den-
ver ii and the regular Denver developmental screening test?? 
 
 
Figure 2. AskHERMES? sentence-based output for the question ?What is the difference between 
the Denver ii and the regular Denver developmental screening test?? 
173
 While the results shown in Figures 1 and 2 suggest 
that a passage-based approach might be better than 
a sentence-based approach for question answering, 
this is not to say that passage-based approaches are 
infallible. Most importantly, a passage-based ap-
proach can introduce noisy sentences that place an 
additional burden on users as they search for the 
most informative answers to their questions. In 
Figure 3, the first sentence in the output of sen-
tence-based approach answers the question. How-
ever, the passage-based approach does not answer 
the question until the fourth passage, and when it 
does, it outputs the same core answer sentence that 
was provided in the sentence-based approach. Ad-
ditionally, the core sentence is nested within a 
group of sentences that on their own are only mar-
ginally relevant to the query and in effect bury the 
answer. 
 
 
Figure 3. An example comparing the sentence-based approach and passage-based approach 
 
4 Evaluation Design 
To evaluate whether the passage-based presenta-
tion improves question answering, we plugged two 
different approaches into our real system by mak-
ing use of either the passage-based or the sentence-
based ranking and presentation unit constructor. 
Both of them share the same document retrieval 
component, and they share the same ranking and 
clustering strategies. In our system, we used a den-
sity-based passage retrieval strategy (Tellex et al 
2003) and a sequence sensitive ranking strategy 
similar to ROUGE (F. Liu and Y. Liu 2008). An 
in-house query-oriented clustering algorithm was 
used to construct the order and structure of the fi-
nal hierarchical presentation. The difference be-
tween the two approaches is the unit for ranking 
and presentation. A passage-based approach takes 
the passage as its primary unit, with each passage 
consisting of one or more sentences. Those sen-
tences in the passage are extracted from the adja-
cent matching sentences in the original article.  
174
To evaluate the difference between the passage-
based presentation and sentence-based presenta-
tion, we randomly selected 20 questions from 
4,653 clinical questions. A physician (Dr. John 
Ely) was shown the corresponding passage-based 
and sentence-based outputs of every question and 
was then asked to judge the relevance of the output 
and which output had the higher quality answer. 
Because physicians have little time in clinical set-
tings to be sifting through data, we presented only 
the top five units (sentences or passages) of output 
for every question. 
 
 
Figure 4. A partial screenshot of AskHERMES 
illustrating hierarchical clustering based on the 
question ?What is the dose of sporanox?? 
 
For answer extraction, we built a hierarchical 
weighted-keyword grouping model (Yu and Cao 
2008;Yu and Cao 2009). More specifically, in us-
ing this model we group units based on the pres-
ence of expanded query-term categories: 
keywords, keyword synonyms, UMLS concepts, 
UMLS synonyms, and original words, and we then 
prioritize the groups based on their ranking. For 
example, units that incorporate keywords are 
grouped into the first cluster, followed by the clus-
ter of units that incorporate keyword synonyms, 
UMLS concepts, etc. The units that appear syn-
onymous are in the clusters with the same parent 
cluster. Figure 4 shows an example of the top 
branch of the clusters for the question ?What is the 
dose of sporanox?? in which the answers are or-
ganized by sporanox and dose as well as their 
synonyms. 
 
5 Evaluation Result and Discussion 
We classify physician evaluations as being of the 
following four types and plot their distribution in 
Figure 5: 
? Hard Question: The question is considered 
difficult because it is patient-specific or 
unclear (that is, it is a poorly formed ques-
tion), e.g., ?Multiple small ulcers on ankles 
and buttocks. No history of bites. I sent 
him for a complete blood count (cbc) and 
blood sugar but I don't know what these 
are.? 
? Failed Question: Neither approach can find 
any relevant information for the question. 
? Passage Better: Passage-based approach 
presents more useful information for an-
swering the question. 
? Sentence Better: Sentence-based approach 
provides the same amount of useful infor-
mation while reducing the effort required 
by the passage-based approach. 
 
Failed 
Question
25%
Passage 
Better
40%
Sentence 
Better
15%
Hard 
Question
20%
 
Figure 5. Distribution of the defined Evaluation 
categories 
 
 
175
The evaluation data is shown in Table 1.  In our 
study, the score range is set from 0 to 5 with the 
value 0 referring to answers that are totally irrele-
vant to the question and the value 5 meaning there 
is enough information to fully answer the question. 
Our results show that the passage-based approach 
is better than the sentence-based approach (p-value 
< 0.05).  
 
Table 1. Quantitative measurement of the answers 
generated by both approaches to the 20 questions 
No. Passage-based 
approach score 
Sentence-based 
approach score 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
mean 
s.deviation 
3 
2 
2 
0 
0 
1 
3 
3 
0 
0 
1 
1 
3 
0 
1 
2 
0 
1 
0 
0 
1.15 
1.18 
1 
0 
0 
0 
0 
0 
1 
0 
0 
0 
2 
2 
4 
0 
0 
1 
0 
0 
0 
0 
0.55 
1.05 
p-value 0.01 
 
Through further analysis of the results, we found 
that 70% of the sentences yielded by the sentence-
based approach did not answer the question at all 
(the score is zero), while this was true for only 
40% of the output of the passage-based approach. 
This indicates that the passage-based approach pro-
vides more evidence for answering questions by 
providing richer context and matching across sen-
tences.  
 
On the other hand, if the question was too general 
and included a plethora of detail and little focus, 
both approaches failed.  For example, in the ques-
tion ?One year and 10-month-old boy removed 
from his home because of parental neglect. Care-
taker says he often cries like he's in pain, possibly 
abdominal pain. Not eating, just drinking liquids, 
not sleeping. The big question with him: "is it 
something physical or all adjustment disorder?"? 
there is a great deal of description of the boy, and a 
variety of common symptoms are also provided. 
AskHERMES found a passage containing all of the 
following extracted words: ?availability, because, 
before, between, changes, children, decrease, dis-
order/disorders, drug, eating, going, increase, indi-
cations/reasons, intake, laboratory, level, may, 
often, one, patient/patients, physical, recom-
mended, routinely, specific, still, symp-
tom/symptoms, two, urine, used, women, 
treat/treated/treating/therapy/treatment/treatments, 
and work.? But since these words are so commonly 
used in a variety of scenarios, the output passage is 
off-topic. 
 
For very simple questions, the sentence-based ap-
proach works well for providing answers in a very 
concise form. For example, the question ?what is 
the dose of zyrtec for a 3-year-old?? can be an-
swered by the dosage amount for the target age 
group, and the query resulted in this answer: 
??children of both sexes aged between 2 to 6 
years with allergy rhinitis (AR) were included in 
this study, who were randomly selected to be 
treated with Zyrtec (Cetirizine 2 HCL) drops 5 mg 
daily for 3 weeks.? From a literal view, this looks 
like an answer to the question because it discusses 
the dosage of Zyrtec for the specific age group; 
however, it actually describes an experiment and 
does not necessarily provide the suggested dosage 
that the user is seeking. This leads to an interesting 
problem for clinical question answering: how 
should experimental data be distinguished from 
suggestion data for recommended daily usage? 
People tend to ask for the best answer instead of 
the possible answers. This is one of the main rea-
sons why in Table 1, there is no perfect score (5). 
 
Our result looks similar to the conclusion of Lin et 
al (Jimmy Lin et al 2003), whose study on open-
domain factoid question answering indicates a 
preference among users for the answer-in-
paragraph approach rather than  the three other 
types of presentation: exact-answer (that is, answer 
entity), answer-in-sentence, and answer-in-
176
document. The results of both Lin?s research and 
our own indicate the usefulness of context, but 
Lin?s work focuses on how surrounding context 
helps users to understand and become confident in 
answers retrieved by simple open-domain queries, 
while our research reveals that adjacent sentences 
can improve the quality of answers retrieved using 
complex clinical questions. Our results also indi-
cate that context is important for relevance rank-
ing, which has not been thoroughly investigated in 
previous research. Furthermore, our work places 
emphasis on proper passage extraction from the 
document or paragraph because irrelevant context 
can also be a burden to users, especially for physi-
cians who have limited time for reading through 
irrelevant text. Our continuous sentence-based pas-
sage extraction method works well for our study, 
but other approaches should be investigated to im-
prove the passage-based approach.  
 
With respect to the quality of the answer, the con-
tent of the output is not the only important issue. 
Rather, the question itself and the organization of 
content are also important issues to consider. Luo 
and Tang (Luo and Tang 2008) proposed an itera-
tive user interface to capture the information needs 
of users to form structured queries with the assis-
tance of a knowledge base, and this kind of ap-
proach guides users toward a clearer and more 
formal representation of their questions. DynaCat 
(Pratt and Fagan 2000) also uses a knowledge-
based approach to organize search results. Thus, 
applying domain-specific knowledge is promising 
for improving the quality of an answer, but the dif-
ficulty of the knowledge-based approach is that 
building and updating such knowledge bases is 
human labor intensive, and furthermore, a knowl-
edge-based approach restricts the usage of the sys-
tem.  
 
6 Conclusion and Future Work 
In this study, we performed a quantitative evalua-
tion on the two kinds of presentation produced by 
our online clinical question answering system, 
AskHERMES. Although there is some indication 
that sentence-based passages are more effective for 
some question types, the overall finding is that by 
providing richer context and matching across sen-
tences, the passage-based approach is generally a 
more effective approach for answering questions. 
Compared to Lin?s study on open-domain factoid 
questions (Jimmy Lin et al 2003), our study ad-
dresses the usefulness of context for answering 
complex clinical questions and its ability to im-
prove answer quality instead of just adding sur-
rounding context to the specific answer. 
While conducting this investigation, we noticed 
that simple continuous sentence-based passage 
constructions have limitations in that they have no 
semantic boundary and will form too long a pas-
sage if the question contains many common words. 
Therefore, we will take advantage of recent ad-
vances we have made in HTML page analysis 
components to split documents into paragraphs and 
use the paragraph as the maximum passage, that is, 
a passage will only group sentences that appear in 
the same paragraph. Furthermore, by setting the 
boundary at a single paragraph, we can loosen the 
adjacency criterion of our current approach, which 
requires that the sentences in a passage be next to 
each other in the original source, and instead adopt 
a requirement that they only be in the same para-
graph. This will enable us to build a model consist-
ing of one or more core sentences as well as 
several satellite sentences that could be used to 
make the answer more complete or understandable. 
Acknowledgments 
The authors acknowledge support from the Na-
tional Library of Medicine to Hong Yu, grant 
number 1R01LM009836-01A1. Any opinions, 
findings, or recommendations are those of the au-
thors and do not necessarily reflect the views of the 
NIH. 
References  
Anon. 2009a. PubMed Home. 
http://www.ncbi.nlm.nih.gov/pubmed/ (Ac-
cessed: 10. March 2009). 
Anon. 2009b. EAGLi: the EAGL project's biomedical 
question answering and information retrieval 
interface. http://eagl.unige.ch/EAGLi/ (Ac-
cessed: 6. March 2009). 
Anon. 2009c. AnswerBus Question Answering System. 
http://www.answerbus.com/index.shtml (Ac-
cessed: 6. March 2009). 
Anon. 2009d. Question Answering Engine. 
http://www.answers.com/bb/ (Accessed: 6. 
March 2009). 
177
Anon. 2009e. The START Natural Language Question 
Answering System. http://start.csail.mit.edu/ 
(Accessed: 6. March 2009). 
Anon. 2009f. Powerset. http://www.powerset.com/ (Ac-
cessed: 19. April 2009). 
Anon. 2009g. Ask.com Search Engine - Better Web 
Search. http://www.ask.com/ (Accessed: 6. 
March 2009). 
Lin, Jimmy, Dennis Quan, Vineet Sinha, Karun Bakshi, 
David Huynh Boris, Boris Katz and David R 
Karger. 2003. What Makes a Good Answer? 
The Role of Context in Question Answering 
Jimmy Lin, Dennis Quan, Vineet Sinha, Karun 
Bakshi, PROCEEDINGS OF INTERACT 2003: 
25--32. doi:10.1.1.4.7644, . 
Liu, F. and Y. Liu. 2008. Correlation between rouge and 
human evaluation of extractive meeting sum-
maries. In: The 46th Annual Meeting of the As-
sociation for Computational Linguistics: 
Human Language Technologies (ACL-HLT 
2008). 
Luo, Gang and Chunqiang Tang. 2008. On iterative 
intelligent medical search. In: Proceedings of 
the 31st annual international ACM SIGIR con-
ference on Research and development in in-
formation retrieval, 3-10. Singapore, 
Singapore: ACM. 
doi:10.1145/1390334.1390338, 
http://portal.acm.org/citation.cfm?id=1390338 
(Accessed: 13. March 2009). 
Pratt, Wanda and Lawrence Fagan. 2000. The Useful-
ness of Dynamically Categorizing Search Re-
sults. Journal of the American Medical 
Informatics Association 7, Nr. 6 (December): 
605?617. 
Tellex, S., B. Katz, J. Lin, A. Fernandes and G. Marton. 
2003. Quantitative evaluation of passage re-
trieval algorithms for question answering. In: 
Proceedings of the 26th annual international 
ACM SIGIR conference on Research and de-
velopment in informaion retrieval, 41-47. 
ACM New York, NY, USA. 
Yu, Hong and Yong-Gang Cao. 2008. Automatically 
extracting information needs from ad hoc clini-
cal questions. AMIA ... Annual Symposium 
Proceedings / AMIA Symposium. AMIA Sym-
posium: 96-100. 
Yu, Hong and Yong-Gang Cao. 2009. Using the 
weighted keyword models to improve informa-
tion retrieval for answering biomedical ques-
tions. In: To appear in AMIA Summit on 
Translational Bioinformatics. 
178
