Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 36?37,
Vancouver, October 2005.
POSBIOTM/W: A Development Workbench For Machine Learning
Oriented Biomedical Text Mining System ?
Kyungduk Kim, Yu Song, Gary Geunbae Lee
Department of Computer Science and Engineering
Pohang University of Science & Technology (POSTECH)
San 31, Hyoja-Dong, Pohang, 790-784, Republic of Korea
{getta, songyu, gblee}@postech.ac.kr
Abstract
The POSBIOTM/W1 is a workbench for
machine-learning oriented biomedical text
mining system. The POSTBIOTM/W is
intended to assist biologist in mining use-
ful information efficiently from biomed-
ical text resources. To do so, it pro-
vides a suit of tools for gathering, manag-
ing, analyzing and annotating texts. The
workbench is implemented in Java, which
means that it is platform-independent.
1 Introduction
Large amounts of biomedical literature exist and the
volume continues to grow exponentially. Following
the increase of literature, there is growing need for
appropriate tools in support of collecting, managing,
creating, annotating and exploiting rich biomedical
text resources.
Especially, information on interactions among bi-
ological entities is very important for understanding
the biological process in a living cell (Blascheke et.
al., 1999). In our POSBIOTM/W workbench, we
use a supervised machine learning method to gen-
erate rules automatically to extract biological events
from free texts with minimum human effort. And we
adopt the Conditional Random Fields (CRF) model
(Lafferty et. al.,2001) for the biomedical named-
entity recognition (NER) task. Finally, to reduce the
? The research was supported by Brain Neuro Informatics
Research program by MOCIE.
1POSBIOTM/W stands for POSTECH Bio-Text Mining
System Workbench
labeling effort in a larger extent we incorporate an
active learning idea into the workbench.
2 System Description
The POSBIOTM/W comprises a set of appropriate
tools to provide users a convenient environment for
gathering, managing and analyzing biomedical text
and for named-entity annotation. The workbench
consists of four components: Managing tool, NER
tool, Event Extraction Tool and Annotation Tool.
And we adopt an active learning idea into the work-
bench to improve the NER and the Event Extraction
module?s performance. The overall design is shown
in Figure 1.
POSBIOTM W client/ ( ) POSBIOTM Syst erverem s( )
NER Module
Event ExtractionModule
NER Tool
Event Extraction Tool
ManagingTool AnnotatingTool TrainingData ActiveearningL
Figure 1: Overview of POSBIOTM/W
2.1 Managing tool
Main objective of the Managing tool is to help biolo-
gists search, collect and manage literatures relevant
to their interest. Users can access to the PubMed
database of bibliographic information using quick
searching bar and incremental PubMed search en-
gine.
36
2.2 NER tool
The NER tool is a client tool of POSBIOTM-
NER module and able to automatically annotate
biomedical-related texts. The NER tool provides
access to three target-specific named entity mod-
els - GENIA-NER model, GENE-NER model and
GPCR-NER model. Each of these model is trained
based on GENIA-Corpus (Kim et. al., 2003),
BioCreative data (Blaschke et. al., 2004) and POS-
BIOTM/NE corpus2 respectively. In POSBIOTM-
NER system, we adopt the Conditional Random
Fields (CRF) model (Lafferty et. al., 2001) for the
biomedical NER task.
2.3 Event Extraction tool
The Event Extraction tool extracts several biologi-
cal events from texts using automatically generated
rules. We use a supervised machine learning method
to overcome a knowledge-engineering bottleneck by
learning event extraction rules automatically. We
modify the WHISK (Soderland, 1999) algorithm to
provide a two-level rule learning method as a divide-
and-conquer strategy. In two-level rule learning, the
system learns event extraction rules which are inside
of the noun chunk at first level, and then it learns the
rules for whole sentence.
Since the system extracts biological events using
automatically generated rules, we can not guarantee
that every extracted event is always correct because
many different rules can be applied to the same sen-
tence. Therefore we try to verify the result with a
Maximum Entropy (ME) classifier to remove incor-
rectly extracted events. For each extracted event,
we verify each component of the event with the ME
classifier model. If one component is contradicted
to the class assigned by the classification model, we
will remove the event. For detail event extraction
process, please consult our previous paper (Kim et.
al., 2004).
2.4 Annotation tool
Our workbench provides a Graphical User Interface
based Annotation tool which enables the users to
annotate and correct the result of the named-entity
recognition and the event extraction. And users can
2POSBIOTM/NE corpus, our own corpus, is used to identify
four target named entities: protein, gene, small molecule and
cellular process.
upload the revised data to the POSBIOTM system,
which would contribute to the incremental build-up
of named-entity and relation annotation corpus.
2.5 Active learning
To minimize the human labeling effort, we employ
the active learning method to select the most infor-
mative samples. We proposed a new active learning
paradigm which considers not only the uncertainty
of the classifier but also the diversity of the corpus,
which will soon be published.
References
Christian Blaschke, Andrade, M.A., Ouzouis, C., Valen-
cia, A.. 1999. Automatic extraction of biological in-
formation from scientific text : protein-protein interac-
tions. Intelligent Systems for Molecular Biology 60-
67.
Christian Blaschke, L. Hirschman, and A.
Yeh, editors. 2004. Proceedings of the
BioCreative Workshop, Granda, March.
http://www.pdg.cnb.uam.es/BioLINK/
workshop BioCreative 04/handout/
Eunju Kim, Yu Song, Gary Geunbae Lee, Byoung-Kee
Yi. 2004. Learning for interaction extraction and ver-
ification from biological full articles. Proceedings of
the ACM SIGIR 2004 workshop on search and discov-
ery in bioinformatics, July 2004, Sheffield, UK
J.-D. Kim, T. Ohta, Y. Tateisi and J. Tsujii 2003. GE-
NIA corpus - a semantically annotated corpus for bio-
textmining. Bioinformatics, Vol 19 Suppl. 1 2003,
pages i180-i182
J. Lafferty, A. McCallum and F. Pereira 2001. Con-
ditional random fields: probabilistic models for seg-
menting and labelling sequence data. International
Conference on Machine Learning.
Soderland S. 1999. Learning information extraction
rules for semi-structured and free text. Machine
Learning, volume 34, 233-272.
37
POSBIOTM-NER in the shared task of BioNLP/NLPBA 2004    
Yu Song, Eunju Kim, Gary Geunbae Lee, Byoung-kee Yi 
Department of CSE, 
Pohang University of Science and Technology (POSTECH)  
Pohang, Korea 790-784 
{songyu, hosuabi, gblee, bkyi} @postech.ac.kr 
 
Abstract 
Two classifiers -- Support Vector Machine 
(SVM) and Conditional Random Fields (CRFs) are 
applied here for the recognition of biomedical 
named entities. According to their different 
characteristics, the results of two classifiers are 
merged to achieve better performance. We propose 
an automatic corpus expansion method for SVM 
and CRF to overcome the shortage of the annotated 
training data. In addition, we incorporate a 
keyword-based post-processing step to deal with 
the remaining problems such as assigning an 
appropriate named entity tag to the word/phrase 
containing parentheses. 
1 Introduction 
Recently, with the rapid growth in the number of 
published papers in biomedical domain, many NLP 
(Natural Language Processing) researchers have 
been interested in a task of automatic extraction of 
facts from biomedical articles. The first and 
fundamental step is to extract the named entities. 
And recently several SVM-based named entity 
recognition models have been proposed. Lee et. al. 
([Lee et. al., 2003]) proposed a two-phrase SVM 
recognition model. Yamamoto et. al. ([Yamamoto 
et. al., 2003]) proposed a SVM-based recognition 
method which uses various morphological 
information and input features such as base noun 
phrase information, stemmed forms of a word, etc. 
However, notable limitation of SVM is its low 
speed both for training and recognition. 
On the other hand, conditional random fields 
(CRFs) ([Lafferty, 2001]) is a probabilistic 
framework for labelling and segmenting sequential 
data, which is much faster  comparing with SVM. 
The conditional probability of the label sequence 
can depend on arbitrary, non-independent features 
of the observation sequence without forcing the 
model to account for the distribution of those 
dependencies. Named entity recognition problem 
can be taken as assigning the named entity class 
tag sequences to the input sentences. We adopt 
CRF to be the complementary scheme of SVM. 
 In natural language processing, supervised 
machine-learning based approach is a kind of 
standard and its efficiency is proven in various task 
fields. However, the most problematic point of 
supervised learning methods is that the size of 
training data is essential to achieve good 
performance, but building a training corpus by 
human labeling is time consuming, labor intensive, 
and expensive. To overcome this problem, various 
attempts have been proposed to acquire a training 
data set in an easy and fast way. Some approaches 
focus on minimally-supervised style learning and 
some approaches try to expand or acquire the 
training data automatically or semi-automatically. 
Using virtual examples, i.e., artificially created 
examples, is a type of method to expand the 
training data in an automatic way ([Niyogi et al 
1998] [Sasano, 2003] [Scholkopf et. al., 1996]. In 
this paper, we propose an automatic corpus 
expansion method both for SVM and CRF based 
biological named entity recognition using virtual 
example idea. 
The remainder of this paper is organized as 
follows: Section 2 introduces named entity 
recognition (NER) part: two machine learning 
approaches with some justification, feature set 
used in NER and virtual examples generation.  In 
section 3, we present some keyword-based post-
processing methods. The experiment results and 
analysis will be presented in section 4. Finally, 
conclusion is provided in section 5.  
2 Named Entity Recognition 
The training corpus is provided in IOB notion. 
The IOB notation is used where named entities are 
not nested and therefore do not overlap. Words 
outside of named entities are tagged with ?O?, 
while the first word in a named entity is tagged 
with B-[entity class], and further named entity 
words receive tag I-[entity class] for inside. We 
define the named entity recognition problem as a 
classification problem, assigning an appropriate 
classification tag for each token in the input 
sentences.  
To simplify the classification problem, we assign 
each token only with I-[entity class]/O. Then we 
convert the tag of the initial token of a consecutive 
100
sequence of predicted named entity tokens to B-
[entity class]. 
2.1 SVM 
Support Vector Machine (SVM) is a well-known 
machine learning technique showing a good 
performance in several classification problems. 
However, SVM has suffered from low speed and 
unbalanced distributed data.     
Named entity token is a compound token that 
consists of the constituents of some other named 
entities, and all other un-related tokens are 
considered as outside tokens. Due to the 
characteristics of SVM, this unbalanced 
distribution of training data can cause a drop-off in 
classification coverage. 
In order to resolve this low coverage and low 
speed problem together, we filter out possible 
outside tokens in the training data through two 
steps. First, we eliminate tokens that are not 
constituents of a base noun phrase, assuming that 
every named entity token should be inside of a 
base noun phrase boundary. Second, we exclude 
some tokens according to their part-of-speech tags. 
We build a stop-part-of-speech tag list by 
collecting tags which have a small chance of being 
a named entity token, such as predeterminer, 
determiner, etc. 
2.2 CRF 
Conditional random fields (CRFs) ([Wallach, 
2004] is a probabilistic framework for labelling 
and segmenting a sequential data. Let ),( EVG be 
a graph such that VvYvY ?= )( , and there is a 
node Vv? corresponding to each of the random 
variable representing an element Yv of Y . Then 
),( YX  is a conditional random field, and when 
conditioned on X  , the random variables Yv  obey 
the Markov property with respect to the graph: 
),~,,|(),,||( vwYwXYvpvwYwXYvp =?  
where vw ~  means that w and v are neighbours in 
G. 
Let X  and Y  be jointly distributed random 
variables respectively representing observation 
sequences and corresponding label sequences. A 
CRF is an undirected graphical model, globally 
conditioned on X (the observation sequence). 
We try to use this CRF model to our NER as a 
complementary method for both speed and 
coverage. SVM predicts the named entities based 
on feature information of words collected in a 
predefined window size while CRF predicts them 
based on the information of the whole sentence. 
So, CRF can handle the named entities with 
outside tokens which SVM always tags as ?O?. 
2.3 Feature set 
As an input to the classifier, we use a bit-vector 
representation, each dimension of which indicates 
whether the input matches with the corresponding 
feature.  
The followings are the basic input features: 
 
z Surface word - only in the case that the 
previous/current/next words are in the 
surface word dictionary. 
z word feature - orthographical feature of 
the previous/current/next words. 
z prefix/suffix - prefixes/suffixes which are 
contained in the current word among the 
entries in the prefix/suffix dictionary. 
z part-of-speech tag - POS tag of the 
previous/current/next words. 
z Base noun phrase tag - base noun tag of 
the previous/current/next words. 
z previous named entity tag - named entity 
tag which is assigned for previous word. 
This feature is only for SVM. 
 
The surface word dictionary is constructed from 
the words that occur more than one time in the 
training part of the corpus.  
2.4 Automatic Corpus Expansion using 
Virtual Examples  
To achieve good results in machine learning 
based classification, it is important to use training 
data which is sufficient not only in the quality but 
also in the quantity. But making the training data 
by hand requires considerable man-power and 
takes a long time. Expanding the training data 
using virtual examples is an attempt for corpus 
expansion in the biomedical domain. 
We expand the training data by augmenting 
the set of virtual examples generated using some 
prior knowledge on the training data. We use the 
fact that the syntactic role of a named entity is a 
noun and the basic syntactic structure of a sentence 
is preserved if we replace a noun with another 
noun in the sentence. Based on this linguistic 
paradigmatic relation, we can generate a new 
sentence by replacing each named entity by 
another named entity which is in the named entity 
dictionary of the corresponding class. Then we 
augment the sentence into the original training data. 
If we apply this replacement processes n times for 
each sentence in the original corpus, then we can 
obtain a virtual corpus about n+1 times bigger than 
the original one. Since the virtual corpus 
strengthens the right information which may not be 
observed in the original corpus, it is helpful to 
extend the coverage of a recognition model and 
101
also helpful to improve the recognition 
performance. 
3 Keyword based post-processing 
We notice that some words occur more 
frequently in the specific entity class. For example, 
the word ?genes? appears in class DNA 590 times 
while in other entity class appears less than 10 
times. The information provided by these key 
words not only impacts the named entity prediction 
part but also shows great power in post-processing 
part. Once keywords appear at specific position in 
a named entity, we can surely decide the entity 
class of this named entity. 
3.1 Words containing parentheses or ?and? 
 It is difficult but significant to decide whether 
parentheses or ?and? are part of named entity or 
not. Parentheses occur in the named entity more 
than 700 times in the training data. Both SVM and 
CRF cannot work well while dealing with this 
problem. 
Once a specific keyword appears at the right side 
of ?)?, we can tell that the parentheses belong to a 
named entity.  The named entity tag information 
can also be determined by the keyword. For 
example, in Table 1, the left column is the result of 
the NER module. At post-processing stage, the 
word ?genes? is detected on the right side of ?)?, 
then this pair of parentheses and keyword ?genes? 
are included in the current named entity.  
 
Before After 
text tag text tag 
(        O (  I-DNA 
VH      I-DNA VH       I-DNA 
)        O   )         I-DNA 
genes    O genes      I-DNA 
    Table 1: An example for the usage of 
keywords. 
 
A keyword list for parentheses is collected from 
the training corpus, including the named entity 
tag information.  It not only solves the 
parentheses named entity tag problem but also 
helps to correct the wrong named entity assigned 
to the words between parentheses by the previous 
step. The word ?and? can be treated similarly as 
the parenthesis case.  
3.2 Correcting the wrong named entity tag 
Some keywords occur in one specific type of 
named entities with high frequency. We employ 
the information provided by those keywords in 
correcting the wrongly assigned named entity tag. 
First a list of high frequency keywords with 
class information is collected. Once a keyword is 
predicted as another type of named entity, all the 
words in the current named entity boundary will be 
corrected as the corresponding named entity type 
as the keyword. For example, the keywords 
?protein? and ?proteins?, in a very rare case, 
belong to other named entity class rather than the 
class ?PROTEIN?.  
4 Experiment Result and analysis 
4.1 Corpus  
The shared task BioNLP/NLPBA 2004 provides 
2000 MEDLINE abstracts from the GENIA ([Ohta 
et. al., 2002]) bio-named entity corpus version 3.02. 
There are total 5 entity classes: DNA, RNA, 
protein, cell_line and cell_type.  
4.2 Experiment results and analysis 
CLASS Recall/Precision/F-score 
Full R64.80   P67.82   F66.28
Left R69.99   P73.25   F71.58
ALL 
Right R73.25   P76.67   F74.92
Full R65.50   P73.04   F69.07
Left R71.26   P79.46   F75.13
Protein 
Right R72.23   P80.54   F76.16
Full R53.77   P61.40   F57.33
Left R56.39   P64.40   F60.13
Cell_Line 
Right R63.57   P72.60   F67.79
Full R58.60   P61.65   F60.08
Left R64.27   P67.61   F65.90
DNA 
Right R66.79   P70.27   F68.48
Full R65.49   P62.71   F64.07
Left R67.26   P64.41   F65.80
RNA 
Right R75.22   P72.03   F73.59
Full R70.45   P59.45   F64.48
Left R74.46   P62.83   F68.15
Cell_Type
Right R84.52   P71.32   F77.36
 Table 2: Final result of POSBIOTM-NER (with 
no abstract boundary information). 
 
Method Full:   Recall/Precision/F-score
SVM.base R62.01   P65.80   F63.85 
SVM+V R63.91   P66.89   F65.37 
CRF.base R64.90   P61.33   F63.06 
CRF+V R65.78   P61.06   F63.34 
Final R64.80   P67.82   F66.28 
Table 3: Step by step result 
102
From table 3, we can see that after using virtual 
samples, both the precision and recall increased, 
especially for SVM. In CRF, even though the full 
f-score did not increase the full F-score much, but 
for RNA class, after using virtual samples, the f-
score has increased 3%. 
A CRF has different characteristics from SVM, 
and is good at handling different kinds of data. So, 
we simply merge the results of two machine 
learning approaches, by using the CRF results to 
extend the boundaries of named entities predicted 
by SVM. After merging the results of the baseline 
of SVM and CRF (without using virtual samples) 
the f-score reaches to 64.58, while the f-score of 
SVM alone is 63.85. The final score in Table 3 is 
the merged results with the virtual samples. 
Although we have improved our system by 
using virtual samples, CRF and SVM as 
complementary means and post-processing, we 
still have some problems to solve, such as correct 
named entity boundary detection. It is more 
difficult to correctly predict the left boundary of 
named entities than the right boundary. From the 
analysis of the results, we usually predict ?human? 
and ?factor? as the beginning and end of a named 
entity, but, it is even difficult for human to decide 
correctly whether it is a part of a named entity or 
not. 
5 Conclusion and Future Works 
In this paper, we propose a general method for 
named entity recognition in the biomedical domain. 
Various morphological, part-of-speech and base 
noun phrase features are incorporated to recognise 
the named entities. We use two different kinds of 
machine learning techniques, SVM and CRF, with 
merged results. We also developed a virtual sample 
technique to overcome the training data shortage 
problem. Finally, we present a   keyword-based 
heuristic post-processing which increases both 
precision and recall. 
 As shown in the experiment results, more 
correct detection of the named entity boundary is 
required, especially the detection of left boundary. 
6 Acknowledgements 
This research is supported by BIT Fusion project 
(by MOST). 
References  
J.Lafferty, A.McCallum, and F.Pereira. 
Conditional random fields: probabilistic models 
for segmenting and labelling sequence data. In 
International Conference on Machine Learning, 
2001. 
Ki-Joong Lee, Young-Sook Hwang, and Hae-
Chang Rim. Two-phase biomedical NE 
recognition based on SVMs. Proceedings of 
ACL 2003 Workshop on Natural Language 
Processing in Biomedicine,2003. 
P.Niyogi, F.Girosi, and T.Poggio. Incorporating 
prior information in machine learning by 
creating virtual examples. Proceedings of 
IEEE volume 86, pages 2196-2207, 1998 
 
Manabu Sasano. Virtual examples for text 
classification with support vector machines. 
Proceedings of 2003 Conference on 
Empirical Methods in Natural Language 
Processing (EMNLP 2003), 2003. 
Bernhard Scholkopf, Chris Burges, and Vladimir 
Vapnik. Incorporating invariances in support 
vector learning machines.  Artificial Neural 
Networks- ICANN96,1112:47-52,1996. 
Hanna M.Wallach. Conditional Random Fields: 
An Introduction. 2004 
T.Ohta, Y. Tateisi, J.Kim, H. Mima and 
J.Tsujiii.2002. The GENIA corpus: An 
annotated research abstract corpus in 
molecular biology domain. In Proceedings of 
Human Language Technology Conference, 
2002. 
Kaoru Yamamoto, Taku Kudo, Akihiko Konagaya, 
and Yuji Matusmoto. Protein name tagging 
for biomedical annotation in text. Proceedings 
of ACL 2003 Workshop on Natural Language 
Processing in Biomedicine, 2003. 
 
103
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 69?72,
New York, June 2006. c?2006 Association for Computational Linguistics
MMR-based Active Machine Learning  
for Bio Named Entity Recognition 
Seokhwan Kim1 Yu Song2 Kyungduk Kim1 Jeong-Won Cha3 Gary Geunbae Lee1
1 Dept. of Computer Science and Engineering, POSTECH, Pohang, Korea 
2 AIA Information Technology Co., Ltd. Beijing, China 
3 Dept. of Computer Science, Changwon National University, Changwon, Korea 
megaup@postech.ac.kr, Song-Y.Song@AIG.com, getta@postech.ac.kr
jcha@changwon.ac.kr, gblee@postech.ac.kr
Abstract 
This paper presents a new active learning 
paradigm which considers not only the 
uncertainty of the classifier but also the 
diversity of the corpus. The two measures 
for uncertainty and diversity were com-
bined using the MMR (Maximal Marginal 
Relevance) method to give the sampling 
scores in our active learning strategy. We 
incorporated MMR-based active machine-
learning idea into the biomedical named-
entity recognition system. Our experimen-
tal results indicated that our strategies for 
active-learning based sample selection 
could significantly reduce the human ef-
fort. 
1 Introduction 
Named-entity recognition is one of the most ele-
mentary and core problems in biomedical text min-
ing. To achieve good recognition performance, we 
use a supervised machine-learning based approach 
which is a standard in the named-entity recognition 
task. The obstacle of supervised machine-learning 
methods is the lack of the annotated training data 
which is essential for achieving good performance. 
Building a training corpus manually is time con-
suming, labor intensive, and expensive. Creating 
training corpora for the biomedical domain is par-
ticularly expensive as it requires domain specific 
expert knowledge. 
One way to solve this problem is through active 
learning method to select the most informative 
samples for training. Active selection of the train-
ing examples can significantly reduce the neces-
sary number of labeled training examples without 
degrading the performance. 
Existing work for active learning explores two 
approaches: certainty or uncertainty-based methods 
(Lewis and Gale 1994; Scheffer and Wrobel 2001; 
Thompson et al 1999) and committee-based 
methods (Cohn et al 1994; Dagan and Engelson 
1995; Freund et al 1997; Liere and Tadepalli 
1997). Uncertainty-based systems begin with an 
initial classifier and the systems assign some un-
certainty scores to the un-annotated examples. The 
k examples with the highest scores will be anno-
tated by human experts and the classifier will be 
retrained. In the committee-based systems, diverse 
committees of classifiers were generated. Each 
committee member will examine the un-annotated 
examples. The degree of disagreement among the 
committee members will be evaluated and the ex-
amples with the highest disagreement will be se-
lected for manual annotation. 
Our efforts are different from the previous ac-
tive learning approaches and are devoted to two 
aspects: we propose an entropy-based measure to 
quantify the uncertainty that the current classifier 
holds. The most uncertain samples are selected for 
human annotation. However, we also assume that 
the selected training samples should give the dif-
ferent aspects of learning features to the classifica-
tion system. So, we try to catch the most 
representative sentences in each sampling. The 
divergence measures of the two sentences are for 
the novelty of the features and their representative 
levels, and are described by the minimum similar-
ity among the examples. The two measures for un-
certainty and diversity will be combined using the 
MMR (Maximal Marginal Relevance) method 
(Carbonell and Goldstein 1998) to give the sam-
pling scores in our active learning strategy. 
69
We incorporate MMR-based active machine-
learning idea into the POSBIOTM/NER (Song et 
al. 2005) system which is a trainable biomedical 
named-entity recognition system using the Condi-
tional Random Fields (Lafferty et al 2001) ma-
chine learning technique to automatically identify 
different sets of biological entities in the text. 
2 MMR-based Active Learning for Bio-
medical Named-entity Recognition 
2.1 Active Learning 
We integrate active learning methods into the 
POSBIOTM/NER (Song et al 2005) system by the 
following procedure: Given an active learning 
scoring strategy S and a threshold value th, at each 
iteration t, the learner uses training corpus TMt   to 
train the NER module Mt. Each time a user wants 
to annotate a set of un-labeled sentences U, the 
system first tags the sentences using the current 
NER module Mt. At the same time, each tagged 
sentence is assigned with a score according to our 
scoring strategy S. Sentences will be marked if its 
score is larger than the threshold value th. The tag 
result is presented to the user, and those marked 
ones are rectified by the user and added to the 
training corpus. Once the training data accumulates 
to a certain amount, the NER module Mt will be 
retrained. 
2.2 Uncertainty-based Sample Selection 
We evaluate the uncertainty degree that the current 
NER module holds for a given sentence in terms of 
the entropy of the sentence. Given an input se-
quence o, the state sequence set S is a finite set. 
And  is the probability distribu-
tion over S. By using the equation for CRF 
(Lafferty et al 2001) module, we can calculate the 
probability of any possible state sequence s given 
an input sequence o. Then the entropy of  
is defined to be: 
Sso|s ??   ),(p
)( o|s?p
? ???=
s
o|so|s )]([log)( 2 PPH  
The number of possible state sequences grows 
exponentially as the sentence length increases. In 
order to measure the uncertainty by entropy, it is 
inconvenient and unnecessary to compute the 
probability of all the possible state sequences. In-
stead we implement N-best Viterbi search to find 
the N state sequences with the highest probabilities. 
The entropy H(N) is defined as the entropy of the 
distribution of the N-best state sequences: 
? ??= = ?
?
= ?
?
???
?
???
?
?=
N
i
N
i i
i
N
i i
i
P
P
P
P
NH
1
1
2
1
)(
)(
log
)(
)(
)(
o|s
o|s
o|s
o|s .  (1) 
The range of the entropy H(N) is [0, 
N
1
log 2? ] which varies according to different N. 
We could use the equation (2) to normalize the 
H(N) to [0, 1]. 
N
NH
NH
1
log
)(
)(
2?
=? .  (2) 
2.3 Diversity-based Sample Selection 
We measure the sentence structure similarity to 
represent the diversity and catch the most represen-
tative ones in order to give more diverse features to 
the machine learning-based classification systems. 
We propose a three-level hierarchy to represent 
the structure of a sentence. The first level is NP 
chunk, the second level is Part-Of-Speech tag, and 
the third level is the word itself. Each word is rep-
resented using this hierarchy structure. For exam-
ple in the sentence "I am a boy", the word "boy" is 
represented as w
r
=[NP, NN, boy]. The similarity 
score of two words is defined as: 
)()(
),(2
)(
21
21
21 wDepthwDepth
wwDepth
wwsim rr
rrrr
+
?=?  
Where ),( 21 wwDepth
rr
 is defined from the top 
level as the number of levels that the two words are 
in common. Under our three-level hierarchy 
scheme above, each word representation has depth 
of 3. 
The structure of a sentence S is represented as 
the word representation vectors ],  ,,[ 21 Nwww
rKrr . 
We measure the similarity of two sentences by the 
standard cosine-similarity measure. The similarity 
score of two sentences is defined as: 
2211
21
21 ),(
SSSS
SS
SSsimilarity rrrr
rrrr
??
?= , 
?? ?=?
i j
ji wwsimSS )( 2121
rrrr
. 
70
2.4 MMR Combination for Sample Selection 
We would like to score the sample sentences with 
respect to both the uncertainty and the diversity. 
The following MMR (Maximal Marginal Rele-
vance) (Carbonell and Goldstein 1998) formula is 
used to calculate the active learning score: 
),(Similaritymax                   
)1(),(yUncertaint)(
jiTs
i
def
i
ss
Mssscore
Mj??
???= ??   (3) 
where si is the sentence to be selected, Uncertainty 
is the entropy of si given current NER module M, 
and Similarity indicates the divergence degree be-
tween the si and the sentence sj in the training cor-
pus TM of M. The combination rule could be 
interpreted as assigning a higher score to a sen-
tence of which the NER module is uncertain and 
whose configuration differs from the sentences in 
the existing training corpus. The value of parame-
ter ?  coordinates those two different aspects of 
the desirable sample sentences. 
After initializing a NER module M and an ap-
propriate value of the parameter? , we can assign 
each candidate sentence a score under the control 
of the uncertainty and the diversity. 
3 Experiment and Discussion 
3.1 Experiment Setup 
We conducted our active learning experiments us-
ing pool-based sample selection (Lewis and Gale 
1994). The pool-based sample selection, in which 
the learner chooses the best instances for labeling 
from a given pool of unlabelled examples, is the 
most practical approach for problems in which 
unlabelled data is relatively easily available. 
For our empirical evaluation of the active learn-
ing methods, we used the training and test data 
released by JNLPBA (Kim et al 2004). The train-
ing corpus contains 2000 MEDLINE abstracts, and 
the test data contains 404 abstracts from the 
GENIA corpus. 100 abstracts were used to train 
our initial NER module. The remaining training 
data were taken as the pool. Each time, we chose k 
examples from the given pool to train the new 
NER module and the number k varied from 1000 
to 17000 with a step size 1000. 
We test 4 different active learning methods: Ran-
dom selection, Entropy-based uncertainty selection, 
Entropy combined with Diversity, and Normalized 
Entropy (equation (2)) combined with Diversity. 
When we compute the active learning score using 
the entropy based method and the combining 
methods we set the values of parameter N (from 
equation (1)) to 3 and ?  (from equation (3)) to 0.8 
empirically. 
 
Fig1. Comparison of active learning strategies with the ran-
l in the y-axis shows the 
per
bin  
ies consistently outperform 
the
dom selection 
3.2 Results and Analyses 
The initial NER module gets an F-score of 52.54, 
while the F-score performance of the NER module 
using the whole training data set is 67.19. We plot-
ted the learning curves for the different sample 
selection strategies. The interval in the x-axis be-
tween the curves shows the number of examples 
selected and the interva
formance improved. 
We compared the entropy, entropy combined 
with sentence diversity, normalized entropy com-
ed with sentence diversity and random selection.
The curves in Figure 1 show the relative per-
formance. The F-score increases along with the 
number of selected examples and receives the best 
performance when all the examples in the pool are 
selected. The results suggest that all three kinds of 
active learning strateg
 random selection.  
The entropy-based example selection has im-
proved performance compared with the random 
selection. The entropy (N=3) curve approaches to 
the random selection around 13000 sentences se-
lected, which is reasonable since all the methods 
choose the examples from the same given pool. As 
71
the number of selected sentences approaches the 
pool size, the performance difference among the 
different methods gets small. The best performance 
of the entropy strategy is 67.31 when 17000 exam-
ple
the
 normalized combined strategy 
behaves the worst. 
4 Conclusion 
ction could significantly reduce 
the human effort. 
by Minis-
try of Commerce, Industry and Energy. 
s are selected. 
Comparing with the entropy curve, the com-
bined strategy curve shows an interesting charac-
teristic. Up to 4000 sentences, the entropy strategy 
and the combined strategy perform similarly. After 
the 11000 sentence point, the combined strategy 
surpasses the entropy strategy. It accords with our 
belief that the diversity increases the classifier's 
performance when the large amount of samples is 
selected.  The normalized combined strategy dif-
fers from the combined strategy. It exceeds the 
other strategies from the beginning and maintains 
 best performance up until 12000 sentence point. 
   The entropy strategy reaches 67.00 in F-score 
when 11000 sentences are selected. The combined 
strategy receives 67.17 in F-score while 13000 sen-
tences are selected, while the end performance is 
67.19 using the whole training data. The combined 
strategy reduces 24.64 % of training examples 
compared with the random selection. The normal-
ized combined strategy achieves 67.17 in F-score 
when 11000 sentences are selected, so 35.43% of 
the training examples do not need to be labeled to 
achieve almost the same performance as the end 
performance. The normalized combined strategy's 
performance becomes similar to the random selec-
tion strategy at around 13000 sentences, and after 
14000 sentences the
 
We incorporate active learning into the biomedical 
named-entity recognition system to enhance the 
system's performance with only small amount of 
training data. We presented the entropy-based un-
certainty sample selection and combined selection 
strategies using the corpus diversity. Experiments 
indicate that our strategies for active-learning 
based sample sele
Acknowledgement  
This research was supported as a Brain Neuroin-
formatics Research Program sponsored 
References 
Carbonell J., & Goldstein J. (1998). The Use of MMR, 
Diversity-Based Reranking for Reordering Docu-
ments and Producing Summaries. In Proceedings of 
the 21st Annual International ACM-SIGIR Confer-
ence on Research and Development in Information 
Retrieval, pages 335-336. 
Cohn, D. A., Atlas, L., & Ladner, R. E. (1994). Improv-
ing generalization with active learning, Machine 
Learning, 15(2), 201-221. 
Dagan, I., & Engelson S. (1995). Committee-based 
sampling for training probabilistic classifiers. In Pro-
ceedings of the Twelfth International Conference on 
Machine Learning, pages 150-157, San Francisco, 
CA, Morgan Kaufman. 
Freund Y., Seung H.S., Shamir E., & Tishby N. (1997). 
Selective sampling using the query by committee al-
gorithm, Machine Learning, 28, 133-168. 
Kim JD., Ohta T., Tsuruoka Y., & Tateisi Y. (2004). 
Introduction to the Bio-Entity Recognition Task at 
JNLPBA, Proceedings of the International Workshop 
on Natural Language Processing in Biomedicine and 
its Application (JNLPBA). 
Lafferty, J., McCallum, A., & Pereira, F. (2001). Condi-
tional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of the 
18th International Conf. on Machine Learning, pages 
282-289, Williamstown, MA, Morgan Kaufmann. 
Lewis D., & Gale W. (1994). A Sequential Algorithm 
for Training Text Classifiers, In: Proceedings of the 
Seventeenth Annual International ACM-SIGIR Con-
ference on Research and Development in Information 
Retrieval. pp. 3-12, Springer-Verlag. 
Liere, R., & Tadepalli, P. (1997). Active learning with 
committees for text categorization, In proceedings of 
the Fourteenth National Conference on Artificial In-
telligence, pp. 591-596 Providence, RI. 
Scheffer T., & Wrobel S. (2001). Active learning of 
partially hidden markov models. In Proceedings of 
the ECML/PKDD Workshop on Instance Selection. 
Song Y., Kim E., Lee G.G., & Yi B-k. (2005). 
POSBIOTM-NER: a trainable biomedical named-
entity recognition system. Bioinformatics, 21 (11): 
2794-2796. 
Thompson C.A., Califf M.E., & Mooney R.J. (1999). 
Active Learning for Natural Language Parsing and 
Information Extraction, In Proceedings of the Six-
teenth International Machine Learning Conference, 
pp.406-414, Bled, Slovenia. 
72
