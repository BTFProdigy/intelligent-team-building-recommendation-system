Compiling French-Japanese Terminologies from the Web 
 
Xavier Robitaille?, Yasuhiro Sasaki?, Masatsugu Tonoike?,  
Satoshi Sato? and Takehito Utsuro? 
?Graduate School of Informatics, 
Kyoto University 
Yoshida-Honmachi, Sakyo-ku, 
Kyoto 606-8501 Japan 
?Graduate School of Engineering, 
Nagoya University 
Furo-cho, Chikusa-ku, 
Nagoya 464-8603 Japan 
{xavier, sasaki, tonoike, utsuro}@pine.kuee.kyoto-u.ac.jp, 
ssato@nuee.nagoya-u.ac.jp 
 
Abstract 
We propose a method for compiling bi-
lingual terminologies of multi-word 
terms (MWTs) for given translation pairs 
of seed terms. Traditional methods for bi-
lingual terminology compilation exploit 
parallel texts, while the more recent ones 
have focused on comparable corpora. We 
use bilingual corpora collected from the 
web and tailor made for the seed terms. 
For each language, we extract from the 
corpus a set of MWTs pertaining to the 
seed?s semantic domain, and use a com-
positional method to align MWTs from 
both sets. We increase the coverage of 
our system by using thesauri and by ap-
plying a bootstrap method. Experimental 
results show high precision and indicate 
promising prospects for future develop-
ments.  
1 Introduction 
Bilingual terminologies have been the center of 
much interest in computational linguistics. Their 
applications in machine translation have proven 
quite effective, and this has fuelled research aim-
ing at automating terminology compilation. Early 
developments focused on their extraction from 
parallel corpora (Daille et al (1994), Fung 
(1995)), which works well but is limited by the 
scarcity of such resources. Recently, the focus 
has changed to utilizing comparable corpora, 
which are easier to obtain in many domains. 
Most of the proposed methods use the fact that 
words have comparable contexts across lan-
guages. Fung (1998) and Rapp (1999) use so 
called context vector methods to extract transla-
tions of general words. Chiao and Zweigenbaum 
(2002) and D?jean and Gaussier (2002) apply 
similar methods to technical domains. Daille and 
Morin (2005) use specialized comparable cor-
pora to extract translations of multi-word terms 
(MWTs).  
These methods output a few thousand terms 
and yield a precision of more or less 80% on the 
first 10-20 candidates. We argue for the need for 
systems that output fewer terms, but with a 
higher precision. Moreover, all the above were 
conducted on language pairs including English. 
It would be possible, albeit more difficult, to ob-
tain comparable corpora for pairs such as 
French-Japanese. We will try to remove the need 
to gather corpora beforehand altogether. To 
achieve this, we use the web as our only source 
of data. This idea is not new, and has already 
been tried by Cao and Li (2002) for base noun 
phrase translation. They use a compositional 
method to generate a set of translation candidates 
from which they select the most likely translation 
by using empirical evidence from the web.  
The method we propose takes a translation 
pair of seed terms in input. First, we collect 
MWTs semantically similar to the seed in each 
language. Then, we work out the alignments be-
tween the MWTs in both sets. Our intuition is 
that both seeds have the same related terms 
across languages, and we believe that this will 
simplify the alignment process. The alignment is 
done by generating a set of translation candidates 
using a compositional method, and by selecting 
the most probable translation from that set. It is 
very similar to Cao and Li?s, except in two re-
spects. First, the generation makes use of 
thesauri to account for lexical divergence be-
tween MWTs in the source and target language. 
Second, we validate candidate translations using 
a set of terms collected from the web, rather than 
using empirical evidence from the web as a 
whole. Our research further differs from Cao and 
Li?s in that they focus only on finding valid 
translations for given base noun phrases. We at-
tempt to both collect appropriate sets of related 
MWTs and to find their respective translations. 
The initial output of the system contains 9.6 
pairs on average, and has a precision of 92%.  
We use this high precision as a bootstrap to 
augment the set of Japanese related terms, and 
obtain a final output of 19.6 pairs on average, 
with a precision of 81%. 
2 Related Term Collection 
Given a translation pair of seed terms (sf, sj), we 
use a search engine to gather a set F of French 
terms related to sf, and a set J of Japanese terms 
related to sj. The methods applied for both lan-
guages use the framework proposed by Sato and 
Sasaki (2003), outlined in Figure 1. We proceed 
in three steps: corpus collection, automatic term 
recognition (ATR), and filtering.   
2.1 Corpus Collection 
For each language, we collect a corpus C from 
web pages by selecting passages that contain the 
seed. 
Web page collection 
In French, we use Google to find relevant web 
pages by entering the following three queries: 
?sf?, ?sf est? (sf is), and ?sf sont? (sf are). In Japa-
nese, we do the same with queries ?sj?, ?sj???, 
?sj??, ?sj????, and ?sj??, where ?? toha, 
? ha, ??? toiu, and ? no are Japanese func-
tional words that are often used for defining or 
explaining a term. We retrieve the top pages for 
each query, and parse those pages looking for 
hyperlinks whose anchor text contain the seed. If 
such links exist, we retrieve the linked pages as 
well. 
Sentence extraction 
From the retrieved web pages, we remove html 
tags and other noise. Then, we keep only prop-
erly structured sentences containing the seed, as 
well as the preceding and following sentences ? 
that is, we use a window of three sentences 
around the seed. 
2.2 Automatic Term Recognition 
The next step is to extract candidate related terms 
from the corpus. Because the sentences compos-
ing the corpus are related to the seed, the same 
should be true for the terms they contain. The 
process of extracting terms is highly language 
dependent. 
French ATR 
We use the C-value method (Frantzi and 
Ananiadou (2003)), which extracts compound 
terms and ranks them according to their term-
hood. It consists of a linguistic part, followed by 
a statistical part. 
The linguistic part consists in applying a lin-
guistic filter to constrain the structure of terms 
extracted. We base our filter on a morphosyntac-
tic pattern for the French language proposed by 
Daille et al It defines the structure of multi-word 
units (MWUs) that are likely to be terms. Al-
though their work focused on MWUs limited to 
two content words (nouns, adjectives, verbs or 
adverbs), we extend our filter to MWUs of 
greater length. The pattern is defined as follows: 
( ) ( )( )+NumNounDetPrepAdjNumNoun ?  
The statistical part measures the termhood of 
each compound that matches the linguistic pat-
tern. It is given by the C-value:  
( )
( )
( )
( )
( )
??
??
?
??
??
?
?
??
?
?
?
??
?
?
?
?=?
?
?
otherwise
T
b
aaa
nestednotisaif
aa
a
a
Tb a
P
f
f)f(log
,
flog
valueC
2
2
 
where a is the candidate string, f(a) is its fre-
quency of occurrence in all the web pages re-
trieved, Ta is the set of extracted candidate terms 
that contain a, and P(Ta) is the number of these 
candidate terms. 
The nature of our variable length pattern is 
such that if a long compound matches the pat-
tern, all the shorter compounds it includes also 
match. For example, consider the N-Prep-N-
 
 
 
related term sets 
(F, J)
 the  Web ATR 
Filtering 
 
 Corpus collection 
corpora 
(Cf, Cj) 
 
term sets 
(Xf, Xj) 
seed terms
(sf, sj) 
Figure 1: Related term collection 
Prep-N structure in syst?me ? base de connais-
sances (knowledge based system). The shorter 
candidate syst?me ? base (based system) also 
matches, although we would prefer not to extract 
it. 
Fortunately, the strength of the C-value is the 
way it effectively handles nested MWTs. When 
we calculate the termhood of a string, we sub-
tract from its total frequency its frequency as a 
substring of longer candidate terms. In other 
words, a shorter compound that almost always 
appears nested in a longer compound will have a 
comparatively smaller C-value, even if its total 
frequency is higher than that of the longer com-
pound. Hence, we discard MWTs whose C-value 
is smaller than that of a longer candidate term in 
which it is nested. 
Japanese ATR 
Because compound nouns represent the bulk of 
Japanese technical MWTs, we extract them as 
candidate related terms. As opposed to Sato and 
Sasaki, we ignore single nouns. Also, we do not 
limit the number of candidates output by ATR as 
they did.  
2.3 Filtering 
Finally, from the output set of ATR, we select 
only the technical terms that are part of the 
seed?s semantic domain. Numerous measures 
have been proposed to gauge the semantic simi-
larity between two words (van Rijsbergen 
(1979)). We choose the Jaccard coefficient, 
which we calculate based on search engine hit 
counts. The similarity between a seed term s and 
a candidate term x is given by: ( )
( )xsH
xsHJac ?
?=  
where H(s ? x) is the hit count of pages contain-
ing both s and x, and H(s ? x) is the hit count of 
pages containing s or x. The latter can be calcu-
lated as follows: 
( ) ( ) ( )xsHxHsHxsH ??+=? )(  
Candidates that have a high enough coefficient 
are considered related terms of the seed.  
3 Term Alignment 
Once we have collected related terms in both 
French and Japanese, we must link the terms in 
the source language to the terms in the target 
language. Our alignment procedure is twofold. 
First, we first generate Japanese translation can-
didates for each collected French term. Second, 
we select the most likely translation(s) from the 
set of candidates. This is similar to the genera-
tion and selection procedures used in the litera-
ture (Baldwin and Tanaka (2004), Cao and Li, 
Langkilde and Knight (1998)). 
3.1 Translation Candidates Generation 
Translation candidates are generated using a 
compositional method, which can be divided in 
three steps. First, we decompose the French 
MWTs into combinations of shorter MWU ele-
ments. Second, we look up the elements in bilin-
gual dictionaries. Third, we recompose transla-
tion candidates by generating different combina-
tions of translated elements. 
Decomposition 
In accordance with Daille et al, we define the 
length of a MWU as the number of content 
words it contains. Let n be the length of the 
MWT to decompose. We produce all the combi-
nations of MWU elements of length less or equal 
to n. For example, consider the French transla-
tion of ?knowledge based system?: 
It has a length of three and yields the following 
four combinations1: 
Note the treatment given to the prepositions 
and determiners: we leave them in place when 
they are interposed between content words 
within elements, otherwise we remove them. 
Dictionary Lookup 
We look up each element in bilingual dictionar-
ies. Because some words appear in their inflected 
forms, we use their lemmata. In the example 
given above, we look up connaissance (lemma) 
rather than connaissances (inflected). Note that 
we do not lemmatize MWUs such as base de 
connaissances. This is due to the complexity of 
gender and number agreements of French com-
pounds. However, only a small part of the 
MWTs are collected in their inflected forms, and 
French-Japanese bilingual dictionaries do not 
contain that many MWTs to begin with. The per-
formance hit should therefore be minor.  
Already at this stage, we can anticipate prob-
lems arising from the insufficient coverage of 
                                                 
1 A MWT of length n produces 2n-1 combinations, 
including itself. 
syst?me ? base de connaissances
Noun Prep Noun Prep Noun 
[syst?me ? [base de [connaissances]
[syst?me]  [base de [connaissances]
[syst?me ? [base]  [connaissances]
[syst?me]  [base]  [connaissances]
French-Japanese lexicon resources. Bilingual 
dictionaries may not have enough entries, and  
existing entries may not include a great variety of 
translations for every sense. The former problem 
has no easy solution, and is one of the reasons 
we are conducting this research. The latter can be 
partially remedied by using thesauri ? we aug-
ment each element?s translation set by looking 
up in thesauri all the translations obtained with 
bilingual dictionaries. 
Recomposition 
To recompose the translation candidates, we 
simply generate all suitable combinations of 
translated elements for each decomposition. The 
word order is inverted to take into account the 
different constraints in French and Japanese. In 
the example above, if the lookup phase gave {?
? chishiki}, {?? dodai, ??? besu} and {?
? taikei, ???? shisutemu} as respective 
translation sets for syst?me, base and connais-
sance, the fourth decomposition given above 
would yield the following candidates: 
connaissance base syst?me 
?? ?? ?? 
?? ?? ????
?? ??? ?? 
?? ??? ????
If we do not find any translation for one of the 
elements, the generation fails. 
3.2 Translation Selection  
Selection consists of picking the most likely 
translation from the translation candidates we 
have generated. To discern the likely from the 
unlikely, we use the empirical evidence provided 
by the set of Japanese terms related to the seed. 
We believe that if a candidate is present in that 
set, it could well be a valid translation, as the 
French MWT in consideration is also related to 
the seed. Accordingly, our selection process con-
sists of picking those candidates for which we 
find a complete match among the related terms.  
3.3 Relevance of Compositional Methods 
The automatic translation of MWTs is no simple 
task, and it is worthwhile asking if it is best tack-
led with a compositional method. Intricate prob-
lems have been reported with the translations of 
compounds (Daille and Morin, Baldwin and Ta-
naka), notably:  
? fertility: source and target MWTs can be 
of different lengths. For example, table 
de v?rit? (truth table) contains two con-
tent words and translates into ??????
shinri ? chi ? hyo (lit. truth-value-table), 
which contains three. 
? variability of forms in the transla-
tions: MWTs can appear in many forms. 
For example, champ electromagn?tique 
(electromagnetic field) translates both 
into ???? denji? ba (lit. electromag-
netic field)???? denji?kai (lit. elec-
tromagnetic ?region?). 
? constructional variability in the trans-
lations: source and target MWTs have 
different morphological structures. For 
example, in the pair apprentissage auto-
matique??? ???  kikai ? gakushu 
(machine learning) we have (N-
Adj)?(N-N). In the pair programmation 
par contraintes???????? patan?
ninshiki (pattern recognition) we have 
(N-par-N)?(N-N). 
? non-compositional compounds: some 
compounds? meaning cannot be derived 
from the meaning of their components. 
For example, the Japanese term ???
aka?ten (failing grade, lit. ?red point?) 
translates into French as note d??chec (lit. 
failing grade) or simply ?chec (lit. fail-
ure).  
? lexical divergence: source and target 
MWTs can use different lexica to ex-
press a concept. For example, traduction 
automatique (machine translation, lit. 
?automatic translation?) translates as ?
???? kikai ? honyaku (lit. machine 
translation). 
It is hard to imagine any method that could ad-
dress all these problems accurately.  
Tanaka and Baldwin (2003) found that 48.7% 
of English-Japanese Noun-Noun compounds 
translate compositionality. In a preliminary ex-
periment, we found this to be the case for as 
much as 75.1% of the collected MWTs. If we are 
to maximize the coverage of our system, it is 
sensible to start with a compositional approach. 
We will not deal with the problem of fertility and 
non-compositional compounds in this paper. 
Nonetheless, lexical divergence and variability 
issues will be partly tackled by broader transla-
tions and related words given by thesauri. 
4 Evaluation 
4.1 Linguistic Resources 
The bilingual dictionaries used in the experi-
ments are the Crown French-Japanese Dictionary 
(Ohtsuki et al (1989)), and the French-Japanese 
Scientific Dictionary (French-Japanese Scientific 
Association (1989)). The former contains about 
50,000 entries of general usage single words. 
The latter contains about 50,000 entries of both 
single and multi-word scientific terms. These 
two complement each other, and by combining 
both entries we form our base dictionary to 
which we refer as DicFJ. 
The main thesaurus used is Bunrui Goi Hyo 
(National Institute for Japanese Language 
(2004)). It contains about 96,000 words, and 
each entry is organized in two levels: a list of 
synonyms and a list of more loosely related 
words. We augment the initial translation set by 
looking up the Japanese words given by DicFJ. 
The expanded bilingual dictionary comprised of 
the words from DicFJ combined with their syno-
nyms is denoted DicFJJ. The dictionary resulting 
of DicFJJ combined with the more loosely related 
words is denoted DicFJJ2. 
Finally, we build another thesaurus from a 
Japanese-English dictionary. We use Eijiro 
(Electronic Dictionary Project (2004)), which 
contains 1,290,000 entries. For a given Japanese 
entry, we look up its English translations. The 
Japanese translations of the English intermediar-
ies are used as synonyms/related words of the 
entry. The resulting thesaurus is expected to pro-
vide even more loosely related translations (and 
also many irrelevant ones). We denote it DicFJEJ. 
4.2 Notation 
Let F and J be the two sets of related terms col-
lected in French and Japanese. F? is the subset of 
F for which Jac?0.01: { }01.0)(' ??= fJacFfF  
F?* is the subset of valid related terms in F?, as 
determined by human evaluation. P is the set of 
all potential translation pairs among the collected 
terms (P=F?J). P? is the set of pairs containing 
either a French term or a Japanese term with 
Jac?0.01: 
( ){ }01.0)(01.0)(,' ?????= jJacfJacJjFfP  
P?* is the subset of valid translation pairs in P?, 
determined by human evaluation. These pairs 
need to respect three criteria: 1) contain valid 
terms, 2) be related to the seed, and 3) constitute 
a valid translation. M is the set of all translations 
selected by our system. M? is the subset of pairs 
in M with Jac?0.01 for either the French or the 
Japanese term. It is also the output of our system: { }01.0)(01.0)(),(' ????= jJacfJacMjfM  
M?* is the intersection of M? and P?*, or in other 
words, the subset of valid translation pairs output 
by our system. 
4.3 Baseline Method 
Our starting point is the simplest possible align-
ment, which we refer to as our baseline. It is 
worked out by using each of the aforementioned 
dictionaries independently. The output set ob-
tained using DicFJ is denoted FJ, the one using 
DicFJJ is denoted FJJ, and so on. The experiment 
is made using the eight seed pairs given in Table 
1. On average, we have |F'| =74.3, |F'*|=51.0 and 
|P'*|=24.0. Table 2 gives a summary of the key 
results. The precision and the recall are given by: 
'
'*
M
M
precision =  , 
'*
'*
P
M
recall =  
DicFJ contains only Japanese translations cor-
responding to the strict sense of French elements. 
Such a dictionary generates only a few transla-
tion candidates which tend to be correct when 
present in the target set. On the other hand, the 
lookup in DicFJJ2 and DicFJEJ interprets French 
Set |M'| |M'*| Prec. Recall 
FJ 10.5 9.6  92% 40% 
FJJ 15.3 12.6  83% 53% 
FJJ2 20.5 13.4  65% 56% 
FJEJ 30.9 14.1  46% 59% 
Table 2: Results for the baseline 
Id French Japanese (English)
1 analyse vectorielle ??????? bekutoru?kaiseki (vector analysis) 
2 circuit logique ????? ronri?kairo (logic circuit) 
3   intelligence artificielle          ????? jinko?chinou (artificial intelligence) 
4 linguistique informatique ?????? keisan?gengogaku (computational linguistics) 
5 reconnaissance des formes ??????? patan?ninshiki (pattern recognition) 
6 reconnaissance vocale ????? onsei?ninshiki (speech recognition) 
7 science cognitive ????? ninchi?kagaku (cognitive science) 
8 traduction automatique ????? kikai?honyaku (machine translation) 
Table 1: Seed pairs 
MWT elements with more laxity, generating 
more translations and thus more alignments, at 
the cost of some precision. 
4.4 Incremental Selection 
The progressive increase in recall given by the 
increasingly looser translations is in inverse pro-
portion to the decrease in precision, which hints 
that we should give precedence to the alignments 
obtained with the more accurate methods. Con-
sequently, we start by adding the alignments in 
FJ to the output set. Then, we augment it with 
the alignments from FJJ whose terms are not 
already in FJ. The resulting set is denoted FJJ'. 
We then augment FJJ' with the pairs from FJJ2 
whose terms are not in FJJ', and so on, until we 
exhaust the alignments in FJEJ.  
For instance, let FJ contain (synth?se de la 
parole? ? ? ? ? ? onsei ? gousei (speech 
synthesis)) and FJJ contain this pair plus 
(synth?se de la parole?????? onsei?kaiseki 
(speech analysis)). In the first iteration, the pair 
in FJ is added to the output set. In the second 
iteration, no pair is added because the output set 
already contains an alignment with synth?se de 
la parole. 
Table 3 gives the results for each incremental 
step. We can see an increase in precision for FJJ', 
FJJ2' and FJEJ' of respectively 5%, 9% and 8%, 
compared to FJJ, FJJ2 and FJEJ. We are effec-
tively filtering output pairs and, as expected, the 
increase in precision is accompanied by a slight 
decrease in recall.  Note that, because FJEJ is 
not a superset of FJJ2, we see an increase in both 
precision and recall in FJEJ' over FJEJ. None-
theless, the precision yielded by FJEJ' is not suf-
ficient, which is why DicFJEJ is left out in the 
next experiment. 
4.5 Bootstrapping 
The coverage of the system is still shy of the 20 
pairs/seed objective we gave ourselves. One 
cause for this is the small number of valid trans-
lation pairs available in the corpora. From an 
average of 51 valid related terms in the source 
set, only 24 have their translation in the target set. 
To counter that problem, we increase the cover-
age of Japanese related terms and hope that by 
doing so, we will also increase the coverage of 
the system as a whole.  
Once again, we utilize the high precision of 
the baseline method. The average 10.5 pairs in 
FJ include 92% of Japanese terms semantically 
similar to the seed. By inputting these terms in 
the term collection system, we collect many 
more terms, some of which are probably the 
translations of our French MWTs. 
The results for the baseline method with boot-
strapping are given in Table 4. The ones using 
incremental selection and bootstrapping are 
given in Table 5. FJ+ consists of the alignments 
given by a generation process using DicFJ and a 
selection performed on the augmented set of re-
lated terms. FJJ+ and FJJ2+ are obtained in the 
same way using DicFJJ and DicFJJ2. FJ+' contains 
the alignments from FJ, augmented with those 
from FJ+ whose terms are not in FJ. FJJ+' con-
tains FJ+', incremented with terms from FJJ. 
FJJ+'' contains FJJ+', incremented with terms 
from FJJ+, and so on.  
The bootstrap mechanism grows the target 
term set tenfold, making it very laborious to 
identify all the valid translation pairs manually. 
Consequently, we only evaluate the pairs output 
by the system, making it impossible to calculate 
recall. Instead, we use the number of valid trans-
lation pairs as a makeshift measure. 
Bootstrapping successfully allows for many 
more translation pairs to be found. FJ+, FJJ+, 
and FJJ2+ respectively contain 7.6, 8.7 and 8.5 
more valid alignments on average than FJ, FJJ 
and FJJ2. The augmented target term set is nois-
ier than the initial set, and it produces many more 
invalid alignments as well. Fortunately, the in-
cremental selection effectively filters out most of 
the unwanted, restoring the precision to accept-
able levels.  
Set |M'| |M'*| Prec. Recall 
FJJ' 14.0  12.3  88% 51% 
FJJ2' 16.1  12.8  79% 53% 
FJEJ' 29.1  15.5  53% 65% 
Table 3: Results for the incremental selection 
Set |M'| |M'*| Prec. 
FJ+' 19.5 16.1  83% 
FJJ+' 22.5 18.6  83% 
FJJ +'' 24.3 19.6  81% 
FJJ2+' 25.6 20.1  79% 
FJJ2+'' 28.6 20.6  72% 
Table 5: Results for the incremental 
selection with bootstrap expansion 
Set |M'| |M'*| Prec. 
FJ+ 20.9 16.8  80% 
FJJ+ 30.9 21.3  69% 
FJJ2+ 45.8 22.6  49% 
Table 4: Results for the baseline 
method with bootstrap expansion 
4.6 Analysis 
A comparison of all the methods is illustrated in 
the precision ? valid alignments curves of Figure 
2. The points on the four curves are taken from 
Tables 2 to 5. The gap between the dotted and 
filled curves clearly shows that bootstrapping 
increases coverage. The respective positions of 
the squares and crosses show that incremental 
selection effectively filters out erroneous align-
ments. FJJ+'', with 19.6 valid alignments and a 
precision of 81%, is at the rightmost and upper-
most position in the graph. The detailed results 
for each seed are presented in Table 6, and the 
complete output for the seed ?logic circuit? is 
given in Table 7.  
From the average 4.7 erroneous pairs/seed, 3.2 
(68%) were correct translations but were judged 
unrelated to the seed. This is not surprising, con-
sidering that our set of French related terms con-
tained only 69% (51/74.3) of valid related terms. 
Also note that, of the 24.3 pairs/seed output, 5.25 
are listed in the French-Japanese Scientific Dic-
tionary. However, only 3.9 of those pairs are in-
cluded in M'*. The others were deemed unrelated 
to the seed.  
In the output set of ?machine translation?, ?
??????? shizen ?gengo ?shori (natural lan-
guage processing) is aligned to both traitement 
du language naturel and traitement des langues 
naturelles. The system captures the term?s vari-
ability around langue/language. Lexical diver-
gence is also taken into account to some extent. 
The seed computational linguistics yields the 
alignment of langue maternelle (mother tongue) 
with ?? ?? bokoku ? go (literally [[mother-
country]-language]). The usage of thesauri en-
abled the system to include the concept of coun-
try in the translated MWT, even though it is not 
present in any of the French elements. 
5 Conclusion and future work 
We have proposed a method for compiling bilin-
gual terminologies of compositionally translated 
MWTs. As opposed to previous work, we use the 
web rather than comparable corpora as a source 
of bilingual data. Our main insight is to constrain 
source and target candidate MWTs to only those 
strongly related to the seed. This allows us to 
achieve term alignment with high precision. We 
showed that coverage reaches satisfactory levels 
by using thesauri and bootstrapping.  
Due to the difference in objectives and in cor-
pora, it is very hard to compare results: our 
method produces a rather small set of highly ac-
curate alignments, whereas extraction from com-
parable corpora generates much more candidates, 
but with an inferior precision. These two ap-
proaches have very different applications. Our 
method does however eliminate the requirement 
of comparable corpora, which means that we can 
use seeds from any domain, provided we have 
reasonably rich dictionaries and thesauri.  
Let us not forget that this article describes 
only a first attempt at compiling French-Japanese 
terminology, and that various sources of im-
provement have been left untapped. In particular, 
our alignment suffers from the fact that we do 
not discriminate between different candidate 
translations. This could be achieved by using any 
of the more sophisticated selection methods pro-
posed in the literature. Currently, corpus features 
are used solely for the collection of related terms. 
These could also be utilized in the translation 
selection, which Baldwin and Tanaka have 
shown to be quite effective. We could also make 
use of bilingual dictionary features as they did. 
Lexical context is another resource we have not 
exploited. Context vectors have successfully 
been applied in translation selection by Fung  as 
well as  Daille and Morin.  
On a different level, we could also apply the 
bootstrapping to expand the French set of related 
terms. Finally, we are investigating the possibil-
seed |F'| |F'*| |P'*| |M'| |M'*| Prec. 
1 89 40 14 26 13 50% 
2 64 55 24 14 14 100% 
3 72 59 38 40 33 83% 
4 67 49 22 23 18 78% 
5 85 70 22 21 17 81% 
6 67 50 27 22 21 95% 
7 36 27 16 20 17 85% 
8 114 58 29 28 24 86% 
avg 74.3 51.0 24.0  24.3  19.6  81% 
Table 6: Detailed results for  FJJ+'' 
70% 
80% 
90% 
100% 
25
Pr
ec
is
io
n 
0% 
10% 
20% 
30% 
40% 
50% 
60% 
0 5 10 15 20 
Baseline 
Baseline with bootstrap
Incremental 
Incremental with bootstrap
Number of Valid Alignments
Figure 2: Precision - Valid Alignments curves 
ity of resolving the alignments in the opposite 
direction: from Japanese to French. Surely the 
constructional variability of French MWTs 
would present some difficulties, but we are con-
fident that this could be tackled using translation 
templates, as proposed by Baldwin and Tanaka. 
References 
T. Baldwin and T. Tanaka. 2004. Translation by Ma-
chine of Complex Nominals: Getting it Right. In 
Proc. of the ACL 2004 Workshop on Multiword 
Expressions: Integrating Processing, pp. 24?31, 
Barcelona, Spain.  
Y. Cao and H. Li. 2002. Base Noun Phrase Transla-
tion Using Web Data and the EM Algorithm. In 
Proc. of COLING -02, Taipei, Taiwan. 
Y.C. Chiao and P. Zweigenbaum. 2002. Looking for 
Candidate Translational Equivalents in Specialized, 
Comparable Corpora. In Proc. of COLING-02, pp. 
1208?1212. Taipei, Taiwan. 
B. Daille, E. Gaussier, and J.M. Lange. 1994. To-
wards Automatic Extraction of Monolingual and 
Bilingual Terminology. In Proc. of COLING-94, 
pp. 515?521, Kyoto, Japan. 
B. Daille and E. Morin. 2005. French-English Termi-
nology Extraction from Comparable Corpora, In 
IJCNLP-05, pp. 707?718, Jeju Island, Korea. 
H. D?jean., E. Gaussier and F. Sadat. An Approach 
Based on Multilingual Thesauri and Model Com-
bination for Bilingual Lexicon Extraction. In Proc. 
of COLING-02, pp. 218?224. Taipei, Taiwan. 
Electronic Dictionary Project. 2004. Eijiro Japanese-
English Dictionary: version 79. EDP. 
K.T. Frantzi, and S. Ananiadou. 2003. The C-
Value/NC-Value Domain Independent Method for 
Multi-Word Term Extraction. Journal of Natural 
Language Processing, 6(3), pp. 145?179. 
French Japanese Scientific Association. 1989. French-
Japanese Scientific Dictionary: 4th edition. Haku-
suisha. 
P. Fung. 1995. A Pattern Matching Method for Find-
ing Noun and Proper Noun from Noisy Parallel 
Corpora. In Proc of the ACL-95, pp. 236?243, 
Cambridge, USA. 
P. Fung. 1998. A Statiscal View on Bilingual Lexicon 
Extraction: From Parallel Corpora to Non-parallel 
Corpora. In D. Farwell, L. Gerber and L. Hovy 
eds.: Proceedings of the AMTA-98, Springer, pp. 
1?16. 
I. Langkilde and K. Knight. 1998. Generation that 
exploits corpus-based statistical knowledge. In 
COLLING/ACL-98, pp. 704?710, Montreal, Can-
ada. 
National Institute for Japanese Language. 2004. Bun-
rui Goi Hyo: revised and enlarged edition Dainip-
pon Tosho. 
T. Ohtsuki et al 1989. Crown French-Japanese Dic-
tionary: 4th edition. Sanseido. 
R. Rapp. 1999. Automatic Identification of Word 
Translations from Unrelated English and German 
Corpora. In Proc. of the ACL-99. pp. 1?17. Col-
lege Park, USA. 
S. Sato and Y. Sasaki. 2003. Automatic Collection of 
Related Terms from the Web. In ACL-03 Compan-
ion Volume to the Proc. of the Conference, pp. 
121?124, Sapporo, Japan. 
T. Tanaka and T. Baldwin. 2003. Noun-Noun Com-
pound Machine Translation: A Feasibility Study on 
Shallow Processing. In Proc. of the ACL-2003 
Workshop on Multiword Expressions: Analysis, 
Acquisition and Treatment, pp. 17?24. Sapporo, 
Japan. 
van Rijsbergen, C.J. 1979. Information Retrieval. 
London: Butterworths. Second Edition. 
Jac (Fr.) French term Japanese term (English) eval? 
0.100  portes logiques ?????? ronri?geeto (logic gate) 2/2/2 
0.064  fonctions logiques ????? ronri?kansuu (logic function) 2/2/2 
0.064  fonctions logiques ????? ronri?kinou (logic function) 2/2/2 
0.048  registre ? d?calage ???????? shifuto?rejisuta (shift register) 2/2/2 
0.044  simulateur de circuit ????????? kairo?shimureeta (circuit simulator) 2/2/2 
0.040  circuit combinatoire ?????? kumiawase?kairo (combinatorial circuit) 2/2/2 
0.031  nombre binaire 2??? ni?shinsuu (binary number) 2/2/2 
0.024  niveaux logiques ?????? ronri?reberu (logical level) 2/2/2 
0.020  circuit logique combinatoire ????????? kumiawase?ronri?kairo (combinatorial logic circuit) 2/2/2 
0.017  valeur logique ???? ronri?chi (logical value) 2/2/2 
0.013  tension d' alimentation ????? dengen?denatsu (supply voltage) 2/2/2 
0.011  conception de circuits ????? kairo?sekkei (circuit design) 2/2/2 
0.007  conception d' un circuit logique ???????? ronri?kairo?sekkei (logic circuit design) 2/1/2 
0.005  nombre de portes ????? geeto?suu (number of gates) 2/1/2 
? relatedness / termhood / quality of the translation, on a scale of  0 to 2 
Table 7: System output for seed pair circuit logique ????? (logic circuit) 
Effect of Domain-Specific Corpus
in Compositional Translation Estimation for Technical Terms
Masatsugu Tonoike?, Mitsuhiro Kida?,
Takehito Utsuro?
?Graduate School of Informatics,
Kyoto University
Yoshida-Honmachi, Sakyo-ku,
Kyoto 606-8501 Japan
(tonoike,kida,takagi,sasaki,
utsuro)@pine.kuee.kyoto-u.ac.jp
Toshihiro Takagi?, Yasuhiro Sasaki?,
and Satoshi Sato?
?Graduate School of Engineering,
Nagoya University
Furo-cho, Chikusa-ku,
Nagoya 464-8603 JAPAN
ssato@nuee.nagoya-u.ac.jp
Abstract
This paper studies issues on compiling
a bilingual lexicon for technical terms.
In the task of estimating bilingual term
correspondences of technical terms, it
is usually quite difficult to find an exist-
ing corpus for the domain of such tech-
nical terms. In this paper, we take an
approach of collecting a corpus for the
domain of such technical terms from
the Web. As a method of translation
estimation for technical terms, we pro-
pose a compositional translation esti-
mation technique. Through experimen-
tal evaluation, we show that the do-
main/topic specific corpus contributes
to improving the performance of the
compositional translation estimation.
1 Introduction
This paper studies issues on compiling a bilingual
lexicon for technical terms. So far, several tech-
niques of estimating bilingual term correspon-
dences from a parallel/comparable corpus have
been studied (Matsumoto and Utsuro, 2000). For
example, in the case of estimation from compa-
rable corpora, (Fung and Yee, 1998; Rapp, 1999)
proposed standard techniques of estimating bilin-
gual term correspondences from comparable cor-
pora. In their techniques, contextual similarity
between a source language term and its transla-
tion candidate is measured across the languages,
and all the translation candidates are re-ranked ac-
cording to the contextual similarities. However,
collecting terms
of specific
domain/topic
(language S )
XSU (# of translations
is one)
compiled bilingual lexicon
process data
collecting
corpus
(language T )
domain/topic
specific
corpus
(language T )
sample terms
of specific 
domain/topic
(language S )
XSTU , XSTM ,YST
estimating bilingual term
correspondences
language pair (S,T )
term set
(language S )
XTU
(lang. T )
translation set
(language T )
web
(language S )
web
(language S )
existing
bilingual lexicon
XSM (# of translations
is more than one)
YS (# of translations
is zero)
web
(language T )
web
(language T )
looking up
bilingual lexicon
validating
translation
candidates
Figure 1: Compilation of a Domain/Topic Spe-
cific Bilingual Lexicon
there are limited number of parallel/comparable
corpora that are available for the purpose of es-
timating bilingual term correspondences. There-
fore, even if one wants to apply those existing
techniques to the task of estimating bilingual term
correspondences of technical terms, it is usually
quite difficult to find an existing corpus for the
domain of such technical terms.
Considering such a situation, we take an ap-
proach of collecting a corpus for the domain of
such technical terms from the Web. In this ap-
proach, in order to compile a bilingual lexicon
for technical terms, the following two issues have
to be addressed: collecting technical terms to be
listed as the headwords of a bilingual lexicon, and
estimating translation of those technical terms.
Among those two issues, this paper focuses on the
second issue of translation estimation of technical
terms, and proposes a method for translation es-
timation for technical terms using a domain/topic
specific corpus collected from the Web.
More specifically, the overall framework of
114
compiling a bilingual lexicon from the Web can
be illustrated as in Figure 1. Suppose that we have
sample terms of a specific domain/topic, techni-
cal terms to be listed as the headwords of a bilin-
gual lexicon are collected from the Web by the re-
lated term collection method of (Sato and Sasaki,
2003). Those collected technical terms can be di-
vided into three subsets according to the number
of translation candidates they have in an existing
bilingual lexicon, i.e., the subset XUS of terms for
which the number of translations in the existing
bilingual lexicon is one, the subset XMS of terms
for which the number of translations is more than
one, and the subset YS of terms which are not
found in the existing bilingual lexicon. (Hence-
forth, the union XUS ? XMS is denoted as XS .)
The translation estimation task here is to estimate
translations for the terms of XMS and YS . For the
terms of XMS , it is required to select an appro-
priate translation from the translation candidates
found in the existing bilingual lexicon. For ex-
ample, as a translation of the Japanese technical
term ??????, which belongs to the logic cir-
cuit field, the term ?register? should be selected
but not the term ?regista? of the football field. On
the other hand, for the terms of YS , it is required
to generate and validate translation candidates. In
this paper, for the above two tasks, we use a do-
main/topic specific corpus. Each term of XUS has
the only one translation in the existing bilingual
lexicon. The set of the translations of terms of
XUS is denoted as XUT . Then, the domain/topic
specific corpus is collected from the Web using
the terms in the set XUT . A new bilingual lexicon
is compiled from the result of translation estima-
tion for the terms of XMS and YS , as well as the
translation pairs which consist of the terms of XUS
and their translations found in the existing bilin-
gual lexicon.
For each term of XMS , from the translation can-
didates found in the existing bilingual lexicon, we
select the one which appears most frequently in
the domain/topic specific corpus. The experimen-
tal result of this translation selection process is de-
scribed in Section 5.2.
As a method of translation genera-
tion/validation for technical terms, we propose a
compositional translation estimation technique.
Compositional translation estimation of a term
can be done through the process of composi-
tionally generating translation candidates of the
term by concatenating the translation of the
constituents of the term. Here, those translation
candidates are validated using the domain/topic
specific corpus.
In order to assess the applicability of the com-
positional translation estimation technique, we
randomly pick up 667 Japanese and English tech-
nical term translation pairs of 10 domains from
existing technical term bilingual lexicons. We
then manually examine their compositionality,
and find out that 88% of them are actually com-
positional, which is a very encouraging result.
Based on this assessment, this paper proposes a
method of compositional translation estimation
for technical terms, and through experimental
evaluation, shows that the domain/topic specific
corpus contributes to improving the performance
of compositional translation estimation.
2 Collecting a Domain/Topic Specific
Corpus
When collecting a domain/topic specific corpus of
the language T , for each technical term xUT in the
set XUT , we collect the top 100 pages with search
engine queries including xUT . Our search engine
queries are designed so that documents which de-
scribe the technical term xUT is to be ranked high.
For example, an online glossary is one of such
documents. Note that queries in English and those
in Japanese do not correspond. When collect-
ing a Japanese corpus, the search engine ?goo?1
is used. Specific queries used here are phrases
with topic-marking postpositional particles such
as ?xUT ???, ?xUT ????, ?xUT ??, and an ad-
nominal phrase ?xUT ??, and ?xUT ?. When col-
lecting a English corpus, the search engine ?Ya-
hoo!?2 is used. Specific queries used here are ?xUT
AND what?s?, ?xUT AND glossary?, and ?xUT ?.
3 Compositional Translation Estimation
for Technical Terms
3.1 Overview
An example of compositional translation estima-
tion for the Japanese technical term ??????
1http://www.goo.ne.jp/
2http://www.yahoo.com/
115
? application(1)
? practical(0.3)
? applied(1.6)
? action(1)
? activity(1)
? behavior(1)
? analysis(1)
? diagnosis(1)
? assay(0.3)
? behavior analysis(10)
??Compositional generation 
of translation candidate
? applied behavior analysis(17.6)
? application behavior analysis(11)
? applied behavior diagnosis(1)
??Decompose source term into constituents  
??Translate constituents into target language      process
?? ?? ??a
?? ????b
Generated translation candidates
?(1.6?1?1)+(1.6?10)
? application(1)
? practical(0.3)
? applied(1.6)
Figure 2: Compositional Translation Estimation
for the Japanese Technical Term ????????
?? is shown in Figure 2. First, the Japanese tech-
nical term ???????? is decomposed into
its constituents by consulting an existing bilin-
gual lexicon and retrieving Japanese headwords.3
In this case, the result of this decomposition can
be given as in the cases ?a? and ?b? (in Fig-
ure 2). Then, each constituent is translated into
the target language. A confidence score is as-
signed to the translation of each constituent. Fi-
nally, translation candidates are generated by con-
catenating the translation of those constituents
without changing word order. The confidence
score of translation candidates are defined as the
product of the confidence scores of each con-
stituent. Here, when validating those translation
candidates using the domain/topic specific cor-
pus, those which are not observed in the corpus
are not regarded as candidates.
3.2 Compiling Bilingual Constituents
Lexicons
This section describes how to compile bilingual
constituents lexicons from the translation pairs of
the existing bilingual lexicon Eijiro. The under-
lying idea of augmenting the existing bilingual
lexicon with bilingual constituents lexicons is il-
lustrated with the example of Figure 3. Suppose
that the existing bilingual lexicon does not in-
clude the translation pair ?applied :???, while
it includes many compound translation pairs with
the first English word as ?applied? and the first
3Here, as an existing bilingual lexicon, we use Ei-
jiro(http://www.alc.co.jp/) and bilingual constituents lexi-
cons compiled from the translation pairs of Eijiro (details
to be described in the next section).
 
applied mathematics : ?? ??
applied science : ?? ??
applied robot : ?? ????
.
.
. frequency
? ??
applied : ?? : 40
 
Figure 3: Example of Estimating Bilingual Con-
stituents Translation Pair (Prefix)
Table 1: Numbers of Entries and Translation Pairs
in the Lexicons
lexicon # of entries # of translationEnglish Japanese pairs
Eijiro 1,292,117 1,228,750 1,671,230
P
2
232,716 200,633 258,211
B
P
38,353 38,546 112,586
B
S
22,281 20,627 71,429
Eijiro : existing bilingual lexicon
P
2
: entries of Eijiro with two constituents
in both languages
B
P
: bilingual constituents lexicon (prefix)
B
S
: bilingual constituents lexicon (suffix)
Japanese word ????.4 In such a case, we align
those translation pairs and estimate a bilingual
constituent translation pair, which is to be col-
lected into a bilingual constituents lexicon.
More specifically, from the existing bilingual
lexicon, we first collect translation pairs whose
English terms and Japanese terms consist of two
constituents into another lexicon P
2
. We compile
?bilingual constituents lexicon (prefix)? from the
first constituents of the translation pairs in P
2
and
compile ?bilingual constituents lexicon (suffix)?
from their second constituents. The numbers of
entries in each language and those of translation
pairs in those lexicons are shown in Table 1.
In the result of our assessment, only 27% of the
667 translation pairs mentioned in Section 1 can
be compositionally generated using Eijiro, while
the rate increases up to 49% using both Eijiro and
?bilingual constituents lexicons?.5
4Japanese entries are supposed to be segmented into a
sequence of words by the morphological analyzer JUMAN
(http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html)
5In our rough estimation, the upper bound of this rate
is about 80%. Improvement from 49% to 80% could be
achieved by extending the bilingual constituents lexicons
and by introducing constituent reordering rules with preposi-
tions into the process of compositional translation candidate
generation.
116
3.3 Score of Translation Pairs in the
Lexicons
This section introduces a confidence score of
translation pairs in the various lexicons presented
in the previous section. Here, we suppose that
the translation pair ?s, t? of terms s and t is used
when estimating translation from the language of
the term s to that of the term t. First, in this pa-
per, we assume that translation pairs follow cer-
tain preference rules and can be ordered as below:
1. Translation pairs ?s, t? in the existing bilin-
gual lexicon Eijiro, where the term s consists
of two or more constituents.
2. Translation pairs in the bilingual constituents
lexicons whose frequencies in P
2
are high.
3. Translation pairs ?s, t? in the existing bilin-
gual lexicon Eijiro, where the term s consists
of exactly one constituent.
4. Translation pairs in the bilingual constituents
lexicons whose frequencies in P
2
are not
high.
As the definition of the confidence score
q(?s, t?) of a translation pair ?s, t?, in this paper,
we use the following:
q(?s, t?) =
?
?
?
?
?
10
(compo(s)?1) (?s, t? in Eijiro)
log
10
fp(?s, t?) (?s, t? in BP )
log
10
fs(?s, t?) (?s, t? in BS)
(1)
where compo(s) denotes the word (in English) or
morpheme (in Japanese) count of s, fp(?s, t?) the
frequency of ?s, t? as the first constituent in P
2
,
and fs(?s, t?) the frequency of ?s, t? as the second
constituent in P
2
.
6
3.4 Score of Translation Candidates
Suppose that a translation candidate yt is gener-
ated from translation pairs ?s
1
, t
1
?, ? ? ? , ?sn, tn?
by concatenating t
1
, ? ? ? , tn as yt = t1 ? ? ? tn.
Here, in this paper, we define the confidence score
of yt as the product of the confidence scores of the
6It is necessary to empirically examine whether this def-
inition of the confidence score is optimal or not. However,
according to our rough qualitative examination, the results
of the confidence scoring seem stable when without a do-
main/topic specific corpus, even with minor tuning by incor-
porating certain parameters into the score.
collecting terms
of specific
domain/topic
(language S )
XSU (# of translations
is one)
compiled bilingual lexicon
process data
collecting
corpus
(language T )
domain/topic
specific
corpus
(language T )
sample terms
of specific 
domain/topic
(language S )
XSTU , XSTM ,YST
estimating bilingual term
correspondences
language pair (S,T )
term set
(language S )
XTU
(lang. T )
translation set
(language T )
web
(language S )
web
(language S )
existing
bilingual lexicon
XSM (# of translations
is more than one)
YS (# of translations
is zero)
web
(language T )
web
(language T )
looking up
bilingual lexicon
validating
translation
candidates
Figure 4: Experimental Evaluation of Translation
Estimation for Technical Terms with/without the
Domain/Topic Specific Corpus (taken from Fig-
ure 1)
constituent translation pairs ?s
1
, t
1
?, ? ? ? , ?sn, tn?.
Q(yt) =
n
?
i=1
q(?si, ti?) (2)
If a translation candidate is generated from
more than one sequence of translation pairs, the
score of the translation candidate is defined as the
sum of the score of each sequence.
4 Translation Candidate Validation
using a Domain/Topic Specific Corpus
It is not clear whether translation candidates
which are generated by the method described in
Section 3 are valid as English or Japanese terms,
and it is not also clear whether they belong to the
domain/topic. So using a domain/topic specific
corpus collected by the method described in Sec-
tion 2, we examine whether the translation candi-
dates are valid as English or Japanese terms and
whether they belong to the domain/topic. In our
validation method, given a ranked list of trans-
lation candidates, each translation candidate is
checked whether it is observed in the corpus, and
one which is not observed in the corpus is re-
moved from the list.
5 Experiments and Evaluation
5.1 Translation Pairs for Evaluation
In our experimental evaluation, within the frame-
work of compiling a bilingual lexicon for tech-
nical terms, we evaluate the translation estima-
tion part which is indicated with bold line in Fig-
117
Table 2: Number of Translation Pairs for Evaluation
dictionaries categories |X
S
| |Y
S
|
S = English S = Japanese
|X
U
S
| |X
M
S
| C(S) |X
U
S
| |X
M
S
| C(S)
Electromagnetics 58 33 36 22 82% 32 26 76%
McGraw-Hill Electrical engineering 52 45 34 18 67% 25 27 64%
Optics 54 31 42 12 65% 22 32 65%
Iwanami Programming language 55 29 37 18 86% 38 17 100%Programming 53 29 29 24 86% 29 24 79%
Dictionary of (Computer) 100 100 91 9 46% 69 31 56%Computer
Anatomical Terms 100 100 91 9 86% 33 67 39%
Dictionary of Disease 100 100 91 9 74% 53 47 51%
250,000 Chemicals and Drugs 100 100 94 6 58% 74 26 51%
medical terms Physical Science and Statistics 100 100 88 12 64% 58 42 55%
Total 772 667 633 139 68% 433 339 57%
McGraw-Hill : Dictionary of Scientific and Technical Terms
Iwanami : Encyclopedic Dictionary of Computer Science
C(S) : for Y
S
, the rate of including correct translations within the collected domain/topic specific corpus
ure 4. In the evaluation of this paper, we sim-
ply skip the evaluation of the process of collecting
technical terms to be listed as the headwords of a
bilingual lexicon. In order to evaluate the transla-
tion estimation part, from ten categories of exist-
ing Japanese-English technical term dictionaries
listed in Table 2, terms are randomly picked up
for each of the set XUS , XMS , and YS . (Here, as
the terms of YS , these which consist of the only
one word or morpheme are excluded.) As de-
scribed in Section 1, the terms of XUT (the set
of the translations for the terms of XUS ) is used
for collecting a domain/topic specific corpus from
the Web. Translation estimation evaluation is to
be done against the set XMS and YS . For each of
the ten categories, Table 2 shows the sizes of XUS ,
XMS and YS , and for YS , the rate of including cor-
rect translation within the collected domain/topic
specific corpus, respectively.
5.2 Translation Selection from Existing
Bilingual Lexicon
For the terms of XMS , the selected translations are
judged by a human. The correct rates are 69%
from English to Japanese on the average and 75%
from Japanese to English on the average.
5.3 Compositional Translation Estimation
for Technical Terms without the
Domain/Topic Specific Corpus
Without the domain specific corpus, the cor-
rect rate of the first ranked translation candidate
is 19% on the average (both from English to
Japanese and from Japanese to English). The
rate of including correct candidate within top 10
is 40% from English to Japanese and 43% from
Japanese to English on the average. The rate of
compositionally generating correct translation us-
ing both Eijiro and the bilingual constituents lex-
icons (n = ?) is about 50% on the average (both
from English to Japanese and from Japanese to
English).
5.4 Compositional Translation Estimation
for Technical Terms with the
Domain/Topic Specific Corpus
With domain specific corpus, on the average, the
correct rate of the first ranked translation candi-
date improved by 8% from English to Japanese
and by 2% from Japanese to English. However,
the rate of including correct candidate within top
10 decreased by 7% from English to Japanese,
and by 14% from Japanese to English. This is be-
cause correct translation does not exist in the cor-
pus for 32% (from English to Japanese) or 43%
(from Japanese to English) of the 667 translation
pairs for evaluation.
For about 35% (from English to Japanese) or
30% (from Japanese to English) of the 667 trans-
lation pairs for evaluation, correct translation does
exist in the corpus and can be generated through
the compositional translation estimation process.
For those 35% or 30% translation pairs, Fig-
ure 5 compares the correct rate of the first ranked
translation pairs between with/without the do-
main/topic specific corpus. The correct rates in-
crease by 34?37% with the domain/topic specific
corpus. This result supports the claim that the do-
118
??
???
???
???
???
???
???
???
???
???
????
???
???
??
??
??
???
?
???
???
???
???
??
???
???
??
??
???
?
??
??
???
??
??
???
??
??
??
??
??
???
??
??
??
??
???
?
??
???
??
??
???
???
?
???
??
??
??
??
???
???
??
???
???
?
??
??
???
???
???
??
?
??
???
??
??
??
??
??
??
??
??
???
???
??
??
??
??
??
??
??
??
???
?
??
??
??
??
??
??
?
??????????????
???????????
(a) English to Japanese
??
???
???
???
???
???
???
???
???
???
????
???
???
??
??
??
???
?
???
???
???
???
??
???
???
??
??
???
?
??
??
???
??
??
???
??
??
??
??
??
???
??
??
??
??
???
?
??
???
??
??
???
???
?
???
??
??
??
??
???
???
??
???
???
?
??
??
???
???
???
??
?
??
???
??
??
??
??
??
??
??
??
???
???
??
??
??
??
??
??
??
??
???
?
??
??
??
??
??
??
?
??????????????
???????????
(b) Japanese to English
Figure 5: Evaluation against the Translation Pairs
whose Correct Translation Exist in the Corpus
and can be Generated Compositionally
main/topic specific corpus is effective in transla-
tion estimation of technical terms.
6 Related Works
As a related work, (Fujii and Ishikawa, 2001)
proposed a technique of compositional estima-
tion of bilingual term correspondences for the
purpose of cross-language information retrieval.
In (Fujii and Ishikawa, 2001), a bilingual con-
stituents lexicon is compiled from the translation
pairs included in an existing bilingual lexicon in
the same way as our proposed method. One of the
major differences of the technique of (Fujii and
Ishikawa, 2001) and the one proposed in this pa-
per is that in (Fujii and Ishikawa, 2001), instead of
the domain/topic specific corpus, they use a cor-
pus of the collection of the technical papers, each
of which is published by one of the 65 Japanese
associations for various technical domains. An-
other important difference is that in (Fujii and
Ishikawa, 2001), they evaluate only the perfor-
mance of cross-language information retrieval but
not that of translation estimation.
(Cao and Li, 2002) proposed a method of com-
positional translation estimation for compounds.
In the proposed method of (Cao and Li, 2002),
translation candidates of a term are composition-
ally generated by concatenating the translation
of the constituents of the term and are re-ranked
by measuring contextual similarity against the
source language term. One of the major differ-
ences of the technique of (Cao and Li, 2002) and
the one proposed in this paper is that in (Cao and
Li, 2002), they do not use the domain/topic spe-
cific corpus.
7 Conclusion
This paper proposed a method of compositional
translation estimation for technical terms using
the domain/topic specific corpus, and through
the experimental evaluation, showed that the do-
main/topic specific corpus contributes to improv-
ing the performance of compositional translation
estimation.
Future works include the followings: first, in
order to improve the proposed method with re-
spect to its coverage, for example, it is desir-
able to extend the bilingual constituents lexicons
and to introduce constituent reordering rules with
prepositions into the process of compositional
translation candidate generation. Second, we are
planning to introduce a mechanism of re-ranking
translation candidates based on the frequencies of
technical terms in the domain/topic specific cor-
pus.
References
Y. Cao and H. Li. 2002. Base noun phrase translation using
Web data and the EM algorithm. In Proc. 19th COLING,
pages 127?133.
Atsushi Fujii and Tetsuya Ishikawa. 2001. Japanese/english
cross-language information retrieval: Exploration of
query translation and transliteration. Computers and the
Humanities, 35(4):389?420.
P. Fung and L. Y. Yee. 1998. An IR approach for translating
new words from nonparallel, comparable texts. In Proc.
17th COLING and 36th ACL, pages 414?420.
Y. Matsumoto and T. Utsuro. 2000. Lexical knowledge ac-
quisition. In R. Dale, H. Moisl, and H. Somers, editors,
Handbook of Natural Language Processing, chapter 24,
pages 563?610. Marcel Dekker Inc.
R. Rapp. 1999. Automatic identification of word transla-
tions from unrelated English and German corpora. In
Proc. 37th ACL, pages 519?526.
S. Sato and Y. Sasaki. 2003. Automatic collection of related
terms from the web. In Proc. 41st ACL, pages 121?124.
119
Answer Validation by Keyword Association
Masatsugu Tonoike, Takehito Utsuro and Satoshi Sato
Graduate school of Informatics, Kyoto University
Yoshida-Honmachi, Sakyo-ku 606-8501 Kyoto, JAPAN
{tonoike,utsuro,sato}@pine.kuee.kyoto-u.ac.jp
Abstract
Answer validation is a component of question
answering system, which selects reliable answer
from answer candidates extracted by certain
methods. In this paper, we propose an approach
of answer validation based on the strengths of
lexical association between the keywords ex-
tracted from a question sentence and each an-
swer candidate. The proposed answer valida-
tion process is decomposed into two steps: the
first is to extract appropriate keywords from a
question sentence using word features and the
strength of lexical association, while the second
is to estimate the strength of the association
between the keywords and an answer candidate
based on the hits of search engines. In the re-
sult of experimental evaluation, we show that a
good proportion (79%) of a multiple-choice quiz
?Who wants to be a millionaire? can be solved
by the proposed method.
1 Introduction
The technology of searching for the answer of
a question written in natural language is called
?Question Answering?(QA), and has gotten a
lot of attention recently. Research activities of
QA have been promoted through competitions
such as TREC QA Track (Voorhees, 2004) and
NTCIR QAC (Fukumoto et al, 2004). Ques-
tion answering systems can be decomposed into
two steps: first step is to collect answer can-
didates, while the second is to validate each of
those candidates. The first step of collecting an-
swer candidates has been well studied so far. Its
standard technology is as follows: first, the an-
swer type of a question, such as LOCATION or
PERSON, is identified. Then, the documents
which may contain answer candidates are re-
trieved by querying available document set with
queries generated from the question sentence.
Finally, named entities which match the answer
type of the question sentence are collected from
the retrieved documents as answer candidates.
In this paper, we focus on the second step of
how to validate an answer candidate. Several
answer validation methods have been proposed.
One of the well-known approaches is that based
on deep understanding of text (e.g. Moldovan
et al (2003)). In the approach of answer valida-
tion based on deep understanding, first a ques-
tion and the paragraph including an answer can-
didate are parsed and transformed into logical
forms. Second, the validity of the answer candi-
date is examined through logical inference. One
drawback of this approach is that it requires a
rich set of lexical knowledge such as WordNet
and world knowledge such as the inference rule
set. Consequently, this approach is computa-
tionally expensive. In contrast, in this paper,
we propose another approach of answer vali-
dation, which is purely based on the estima-
tion of the strengths of lexical association be-
tween the keywords extracted from a question
sentence and each answer candidate. One un-
derlying motivation of this paper is to exam-
ine the effectiveness of quite low level semantic
operation such as measuring lexical association
against knowledge rich NLP tasks such as an-
swer validation of question answering. Surpris-
ingly, as we show later, given multiple-choices as
answer candidates of a question, a good propor-
tion of a certain set of questions can be solved
by our method based on lexical association.
In our framework of answer validation by key-
word association (in the remaining of this paper,
we call the notion of the lexical association in-
troduced above as ?keyword association?), the
answer validation process is decomposed into
two steps: the first step is to extract appro-
priate keywords from a question sentence, while
the second step is to estimate the strength of
the association between the keywords and an
answer candidate. We propose two methods for
the keyword selection step: one is by a small
number of hand-crafted rules for determining
word weights based on word features, while the
other is based on search engine hits. In the sec-
ond step of how to validate an answer candidate,
the web is used as a knowledge base for estimat-
ing the strength of the association between the
extracted keywords and an answer candidate.
Its basic idea is as follows: the stronger the as-
sociation between the keywords and an answer
candidate, the more frequently they co-occur on
the web. In this paper, we introduce several
measures for estimating the strength of the as-
sociation, and show their effectiveness through
experimental evaluation.
In this paper, in order to concentrate on the
issue of answer validation, but not the whole QA
processes, we use an existing multiple-choice
quiz as the material for our study. The multiple-
choice quiz we used is taken from ?Who wants to
be a millionaire?. ?Who wants to be a million-
aire? is a famous TV show, which originated in
the United Kingdom and has been localized in
more than fifty countries. We used the Japanese
version, which is produced by Fuji Television
Network, Inc.. In the experimental evaluation,
about 80% of the questions of this quiz can be
solved by the proposed method of answer vali-
dation by keyword association.
Section 2 introduces the idea of question an-
swering by keyword association. Section 3 de-
scribes how to select keywords from a question
sentence. Section 4 describes how to select the
answer of multiple-choice questions. Section 5
describes how to integrate the procedures of
keyword selection and answer selection. Sec-
tion 6 presents the results of experimental eval-
uations. Section 7 compares our work with sev-
eral related works Section 8 presents our con-
clusion and future works.
2 Answer Validation by Keyword
Association
2.1 Keyword Association
Here is an example of the multiple-choice quiz.
Q1: Who is the director of ?American Graffiti??
a: George Lucas
b: Steven Spielberg
c: Francis Ford Coppola
d: Akira Kurosawa
Suppose that you do not know the correct an-
swer and try to find it using a search engine
on the Web. The simplest way is to input the
query ?American Graffiti? to the search engine
and skim the retrieved pages. This strategy as-
sumes that the correct answer may appear on
the page that includes the keyword ?American
Graffiti?. A little cleverer way is to consider the
number of pages that contain both the keyword
and a choice. This number can be estimated
Table 1: Hits of Keywords and the Choices for
the Question Q1 (X:?American Graffiti?)
Y (choice) hits(X and Y )
?George Lucas? 15,500
?Steven Spielberg? 5,220
?Francis Ford Coppola? 4,800
?Akira Kurosawa? 836
from the hits of a search engine when you in-
put a conjunct query ?American Graffiti? and
?George Lucas?. Based on this assumption, it is
reasonable to hypothesize that the choice which
has the largest hits is the answer. For the above
question Q1, this strategy works. Table 1 shows
the hits of the conjunct queries for each of the
choices. We used ?google1? as a search engine.
Here, let X be the set of keywords, Y be the
choice. Function hits is defined as follows.
hits(X) ? hits(x
1
AND x
2
AND ? ? ?AND x
n
)
where
X = {x
1
, x
2
, . . . , x
n
}
The conjunct query with ?George Lucas?, which
is the correct answer, returns the largest hits.
Here, the question Q1 can be regarded as a
question on the strength of association between
keyword and an choice, and converted into the
following form.
Q1?: Select the one that has the strongest asso-
ciation with ?American Graffiti?.
a: George Lucas
b: Steven Spielberg
c: Francis Ford Coppola
d: Akira Kurosawa
We call this association between the keyword
and the choice as keyword association.
2.2 How to Select Keywords
It is important to select appropriate keywords
from a question sentence. Consider the follow-
ing question.
Q2: Who is the original author of the famous
movie ?Lord of the Rings??
a: Elijah Wood
b: JRR Tolkien
c: Peter Jackson
d: Liv Tyler
The numbers of hits are shown in Table 2. Here,
let X be ?Lord of the Rings?, X ? be ?Lord of the
1http://www.google.com
Table 2: Hits of Keywords and the Choices
for the Question Q2 (X:?Lord of the Rings?,
X ?:?Lord of the Rings? and ?original author?)
Y (choice) hits hits
(X and Y ) (X ? and Y )
?Elijah Wood? 682,000 213
?JRR Tolkien? 652,000 702
?Peter Jackson? 1,140,000 340
?Liv Tyler? 545,000 106
Rings? and ?original author?. When you select
the title of this movie ?Lord of the Rings? as a
keyword, the choice with the maximum hits is
?Peter Jackson?, which is not the correct an-
swer ?JRR Tolkien?. However, if you select
?Lord of the Rings? and ?original author? as
keywords, this question can be solved by select-
ing the choice with maximum hits. Therefore,
it is clear from this example that how to select
appropriate keywords is important.
2.3 Forward and Backward Association
For certain questions, it is not enough to gen-
erate a conjunct query consisting of some key-
words and a choice, and then to simply select
the choice with maximum hits. This section in-
troduces more sophisticated measures for select-
ing an appropriate answer. Consider the follow-
ing question.
Q3: Where is Pyramid?
a: Canada
b: Egypt
c: Japan
d: China
The numbers of hits are shown in Table 3. In
this case, given a conjunct query consisting of
a keyword ?Pyramid? and a choice, the choice
with the maximum hits, i.e., ?Canada? is not
the correct answer ?Egypt?. Why could not this
question be solved? Let us consider the hits of
the choices alone. The hits of the atomic query
?Canada? is about seven times larger than the
hits of the atomic query ?Egypt?. With this ob-
servation, we can hypothesize that the hits of a
conjunct query ?Pyramid? and a choice are af-
fected by the hits of the choice alone. Therefore
some normalization might be required.
Based on the analysis above, we employ the
metrics proposed by Sato and Sasaki (2003).
Sato and Sasaki (2003) has proposed two met-
rics for evaluating the strength of the relation
of two terms. Suppose that X be the set of
keywords and Y be the choice. In this paper,
we call the hits of a conjunct query consisting
of keywords X and a choice Y , which is nor-
malized by the hits of X, as forward association
FA(X, Y ). We also call the hits of a conjunct
query X and Y , which is normalized by the hits
of Y , as backward association BA(X, Y ).
FA(X, Y ) = hits(X ? {Y })/hits(X)
BA(X, Y ) = hits(X ? {Y })/hits({Y })
Note that when X is fixed, FA(X, Y ) is propor-
tional to hits(X ? {Y }).
Let?s go back to Q3. In this case, the choice
with the maximum BA is correct. Some ques-
tions may solved by referring to FA, while oth-
ers may be solved only by referring to BA.
Therefore, it is inevitable to invent a mecha-
nism which switches between FA and BA.
2.4 Summary
Based on the observation of Sections 2.1 ? 2.3,
the following three questions must be addressed
by answer validation based on keyword associ-
ation.
? How to select appropriate keywords from a
question sentence.
? How to identify the correct answer consid-
ering forward and/or backward association.
? How many questions can be solved by this
strategy based on keyword association.
3 Keyword Selection
This section describes two methods for selecting
appropriate keywords from a question sentence:
one is based on the features of each word, the
other based on hits of a search engine.
First, all the nouns are extracted from the
question sentence using a Japanese morpholog-
ical analyzer JUMAN(Kurohashi and Nagao,
1999) and a Japanese parser KNP(Kurohashi,
1998). Here, when the sequence of nouns con-
stitute a compound, only the longest compound
is extracted and their constituent nouns are not
extracted. Let N denote the set of those ex-
tracted nouns and compounds, from which key-
words are selected. In the following, the search
engine ?goo2? is used for obtaining the number
of hits.
2http://www.goo.ne.jp
Table 3: Hits of Keywords and the Choices for the Question Q3
X(keyword) hits(X)
Pyramid 3,170,000
Y(choice) hits(Y ) hits(Y and X) FA(X , Y ) BA(X , Y )
Canada 100,000,000 334,000 0.105 0.00334
Egypt 14,500,000 325,000 0.103 0.0224
Japan 63,100,000 246,000 0.0776 0.00390
China 53,600,000 225,000 0.0710 0.00420
3.1 Keyword Selection Based on Word
Features
In this method, keywords are selected by the
following procedure:
1. If the question sentence contains n quota-
tions with quotation marks ??? and ???,
those n quoted strings are selected as key-
words.
2. Otherwise:
2-1. According to the rules for word
weights in Table 4, weights are as-
signed to each element of the keyword
candidate set N .
2-2. Select the keyword candidate with the
maximum weight and that with the
second maximum weight.
2-3. i. If the hits of AND search of those
two keyword candidates are 15 or
more, both are selected as key-
words.
ii. Otherwise, select the one with the
maximum weight.
Let k denote the set of the selected keywords
(k ? N), we examine the correctness of k as
follows. Let c denote a choice, cF A
1
(k) the
choice with the maximum FA(k, c), and cBA
1
(k)
the choice with the maximum BA(k, c), respec-
tively.
cF A
1
(k) = argmax
c
FA(k, c)
c
BA
1
(k) = argmax
c
BA(k, c)
Here, we regard the selected keywords k to be
correct if either cF A
1
(k) or cBA
1
(k) is correct.
Against the development set which is to be in-
troduced in Section 6.1, the correct rate of the
keywords selected by the procedure above is
84.5%.
Table 4: Rules for Word Weights
rule weight
n-th segment (1 +
0.01 ? n)
stopword 0
quoted by quotation marks?? 3
person name 3
verbal nouns (?sahen?-verb stem) 0.5
word which expresses relation 2
Katakana 2
name of an award 2
name of an era 0.5
name of a country 0.5
number 3
hits > 1000000
and consists of one character 0.9
marked by a topic maker and
name of a job 0.1
hits > 100000 0.2
hits < 10000 1.1
number of characters = 1 0.2
number of characters = 2 0.25
number of characters = 3 0.5
number of characters = 4 1.1
number of characters ? 5 1.2
3.2 Keyword Selection Based on Hits
of Search Engine
3.2.1 Basic Methods
First, we introduce several basic methods for
selecting keywords based on hits of a search en-
gine. Let 2N denote the power set of N , where a
set of keywords k is an element of 2N (k ? 2N ).
Let k? denote the selected set of keywords and c?
the selected choice.
The first method is to simply select the pair
of ?k?, c?? which gives the maximum hits as below:
?k?, c?? = argmax
c, k?2
N
hits(k ? {c})
Against the development set, the correct rate of
the choice which is selected by this method is
35.7%.
In a similar way, another method which se-
lects the maximum FA or BA can be given as
below:
?k?, c?? = argmax
c, k?2
N
FA(k ? {c})
?k?, c?? = argmax
c, k?2
N
BA(k ? {c})
Their correct rates are 71.3% and 36.1%, respec-
tively.
3.2.2 Keyword Association Ratio
Next, we introduce more sophisticated meth-
ods which use the ratio of maximum and sec-
ond maximum associations such as FA or BA.
The underlying assumption of those methods
are that: the greater those ratios are, the more
reliable is the selected choice with the maximum
FA/BA. First, we introduce two methods: FA
ratio and BA ratio.
FA ratio This is the ratio of FA of the choice
with second maximum FA over one with maxi-
mum FA. FA ratio is calculated by the follow-
ing procedure.
1. Select the choices with maximum FA and
second maximum FA.
2. Estimate the correctness of the choice with
maximum FA by the ratio of their FAs.
The set k? of keywords and the choice c? to be
selected by FA ratio are expressed as below:
k? = argmin
k?2
N
FA(k, cF A
2
(k))
FA(k, cF A
1
(k))
c? = cF A
1
(k?)
c
F A
2
(k) = arg-secondmax
c
FA(k, c)
where arg-secondmax
c
is defined as a function
which selects c with second maximum value.
Similarly, the method based on BA ratio is
given as below:
BA ratio
k? = argmin
k?2
N
BA(k, cBA
2
(k))
BA(k, cBA
1
(k))
c? = cBA
1
(k?)
c
BA
2
(k) = arg-secondmax
c
BA(k, c)
Unlike the methods based on FA ratio and
BA ratio, the following two methods consider
both FA and BA. The motivation of those two
methods is to regard the decision by FA and
BA to be reliable if FA and BA agree on se-
lecting the choice.
Table 5: Evaluation of Keyword Association
Ratios (precision/coverage)(%)
max and second max
FA BA
ratio
FA 63.1/100 70.6/95.0
BA 75.8/93.2 67.6/100
BA ratio with maximum and second max-
imum FA
k? = argmin
k?2
N
BA(k, cF A
2
(k))
BA(k, cF A
1
(k))
c? = cF A
1
(k?)
FA ratio with maximum and second max-
imum BA
k? = argmin
k?2
N
FA(k, cBA
2
(k))
FA(k, cBA
1
(k))
c? = cBA
1
(k?)
Coverages and precisions of these four methods
against the development set are shown in Ta-
ble 5. Coverage is measured as the rate of ques-
tions for which the ratio is less than or equal to
13. Precisions are measured as the rate of ques-
tions for which the selected choice c? is the cor-
rect answer, over those covered questions. The
method having the greatest precision is BA ra-
tio with maximum and second maximum FA.
In the following sections, we use this ratio as
the keyword association ratio. Table 6 farther
examines the correlation of the range of the ra-
tio and the coverage/precision. When the ratio
is less than or equal to 0.25, about 60% of the
questions are solved with the precision close to
90%. This threshold of 0.25 is used in the Sec-
tion 5 when integrating the keyword association
ratio and word weights.
4 Answer Selection
In this section, we explain a method to identify
the correct answer considering forward and/or
backward association. After selecting keywords,
the following numbers are obtained by a search
engine.
? Hits of the keywords X: hits(X)
? Hits of the choice Y : hits({Y })
3For the ratios considering both FA and BA, the
ratio greater than 1 means that FA and BA disagree on
selecting the choice.
Table 6: Evaluation of Keyword Association
Ratio: BA ratio of FA max and second-max
ratio
# of questions
coverage precision
0 18.9% (163/888) 89.6% (146/163)
? 0.01 21.5% (191/888) 89.5% (171/191)
? 0.1 40.5% (360/888) 87.5% (315/360)
? 0.25 60.4% (536/888) 86.9% (466/536)
? 0.5 78.0% (693/888) 81.6% (566/693)
? 0.75 87.2% (774/888) 78.4% (607/774)
? 1 93.2% (828/888) 75.8% (628/828)
? Hits of the conjunct query:
hits(X ? {Y })
Then for each choice Y , FA and BA are cal-
culated. As introduced in section 3, cF A
1
(k) de-
notes the choice whose FA value is highest, and
cBA
1
(k) the choice whose BA value is highest.
What has to done here is to decide which of
cF A
1
(k) and cBA
1
(k) is correct.
After manually analyzing the search engine
hits against the development set, we hand-
crafted the following rules for switching between
cF A
1
(k) and cBA
1
(k).
1. if cF A
1
(k) = cBA
1
(k) then cF A
1
(k)
2. else if F A(k,c
BA
1
(k))
F A(k,c
FA
1
(k))
? 0.8 then cBA
1
(k)
3. else if F A(k,c
BA
1
(k))
F A(k,c
FA
1
(k))
? 0.2 then cF A
1
(k)
4. else if BA(k,c
FA
1
(k))
BA(k,c
BA
1
(k))
? 0.53 then cF A
1
(k)
5. else if hits(k) ? 1300 then cBA
1
(k)
6. else if F A(k,c
BA
1
(k))
F A(k,c
FA
1
(k))
? 0.6 then cBA
1
(k)
7. else cF A
1
(k)
Table 7 shows the results of evaluating preci-
sion of answer selection methods against the de-
velopment set, when the keywords are selected
based on word weights in Section 3.1. In the
table, in addition to the result of answer selec-
tion rules above, the results with baselines of
selecting the choice with maximum FA or BA
are also shown. It is clear from the table that
the answer selection rules described here signif-
icantly outperforms those baselines.
For each of the answer selection rules, Ta-
ble 8 shows its precision. In the development
set4, there are 541 questions (about 60%) where
4Four questions are excluded because hits of the con-
junct query hits(X ? {Y }) were 0
Table 7: Precision of Answer Selection (with
keyword selection by word weights)
method precision
max FA 70.8%
max BA 67.6%
selection rule 77.3%
Table 8: Evaluation of Each Answer Selection
Rule (with keyword selection by word weights)
rule answer precision
1 cFA
1
(k) = cBA
1
(k) 88.5% (479/541)
2 ? 6 - 60.3% (207/343)
total - 77.6% (686/884)
2 cBA
1
(k) 65.3% (32/49)
3 cFA
1
(k) 61.8% (68/110)
4 cFA
1
(k) 53.6% (37/69)
5 cBA
1
(k) 60.3% (35/58)
6 cBA
1
(k) 66.7% (12/18)
7 cFA
1
(k) 59.0% (23/39)
cF A
1
(k) and cBA
1
(k) are identical, and the 88.5%
of the selected choices are correct. This re-
sult shows that more than half of the questions
cF A
1
(k) is equal to cBA
1
(k) and about 90% of
these questions can be solved. This result shows
that whether FA and BA agree or not is very
important and is crucial for reliably selecting
the answer.
5 Total Procedure of Keyword
Selection and Answer Selection
Finally, the procedures of keyword selection and
answer selection presented in the previous sec-
tions are integrated as given below:
1. If ratio ? 0.25:
Use the set of keywords selected by BA ra-
tio with maximum and second maximum
FA. The choice to be selected is the one
with maximum BA.
2. Otherwise:
Use the set of keywords selected by word
weights. Answer selection is done by the
procedure of Section4.
6 Evaluation
6.1 Data Set
In this research, we used the card game ver-
sion of ?????????? (Who wants to be a
millionaire)?, which is sold by Tomy Company,
LTD. It has 1960 questions, which are classi-
fied into fifteen levels according to the amount
of prize money. Each question has four choices.
All questions are written in Japanese. The fol-
lowings give a few examples.
10,000 yen level
[A39]???????????????
??????
(Which continent are Egypt and
Kenya located in?)
A. ?????? (Africa)
B. ??????? (Eurasia)
C. ??????? (North America)
D. ??????? (South America)
[Correct Answer: ??????]
1,000,000 yen level
[J39] ???????????????
?????????????????
(What is the name of the ship in which
Columbus was sailing when he discov-
ered a new continent?)
A. ???????? (Atlantis)
B. ???? (Argo)
C. ??????? (Santa Maria)
D. ?????? (Nautilus)
[Correct Answer: ???????]
10,000,000 yen level
[O4] ???????????????
?????????????????
??
(In which summer Olympics did the
number of participating countries first
exceed 100?)
A. ????? (Rome Olympics)
B. ???? (Tokyo Olympics)
C. ?????? (Mexico Olympics)
D. ??????? (Munich Olympics)
[Correct Answer: ??????]
We divide questions of each level into two
halves: first of which is used as the develop-
ment set and the second as the test set. We
exclude questions with superlative expressions
(e.g., Out of the following four countries, select
the one with the maximum number of states.)
or negation (e.g., Out of the following four col-
ors, which is not used in the national flag of
France.) because they are not suitable for solv-
ing by keyword association. Consequently, the
development set comprises 888 questions, while
the test set comprises 906 questions. The num-
ber of questions per prize money amount is
shown in Table 9.
Table 9: The number of questions per prize
money amount
prize money amount # of questions
(yen) full dev test
10,000 160 71 74
20,000 160 71 77
30,000 160 67 70
50,000 160 75 71
100,000 160 73 73
150,000 160 76 72
250,000 160 71 77
500,000 160 74 77
750,000 160 78 71
1,000,000 160 73 76
1,500,000 120 53 58
2,500,000 90 38 42
5,000,000 70 30 32
7,500,000 50 24 21
10,000,000 30 14 15
total 1960 888 906
We compare the questions of ?Who wants to
be a millionaire? with those of TREC 2003 QA
track and those of NTCIR4 QAC2 task. The
questions of ?Who wants to be a millionaire?
are all classified as factoid question. They cor-
respond to TREC 2003 QA track factoid com-
ponent. The questions of NTCIR4 QAC2 are
also all classified as factoid question. We com-
pare bunsetsu 5 count of the questions of ?Who
wants to be a millionaire? with word count of
the questions of TREC 2003 QA track factoid
component and bunsetsu count of the questions
of NTCIR4 QAC2 Subtask1. The questions
of ?Who wants to be a millionaire? consist of
7.24 bunsetsu on average, while those of TREC
2003 QA track factoid component consist of 7.76
words on average, and those of NTCIR4 QAC2
Subtask1 consist of 6.19 bunsetsu on average.
Therefore, it can be concluded that the ques-
tions of ?Who wants to be a millionaire? are
not shorter than those of TREC 2003 QA track
and those of NTCIR4 QAC2 task.
6.2 Results
Against the development and the test sets,
Table 10 shows the results of evaluating the
total procedure of keyword selection and an-
swer selection presented in Section 5. The ta-
ble also shows the performance of baselines:
5A bunsetsu is one of the linguistic units in Japanese.
A bunsetsu consists of one content word possibly fol-
lowed by one or more function words.
Table 10: Total Evaluation Results (preci-
sion/coverage)(%)
method dev test
K.A.R. (r ? 1) 75.8/93.2 74.6/93.6
word weights
77.3/100 73.4/100
+ answer selection
Integration 78.6/100 75.9/100
K.A.R. (r ? 0.25) 86.9/60.4 86.0/61.5
word weights (r > 0.25)
65.9/39.6 59.9/38.5
+ answer selection
K.A.R.: keyword association ratio
?
??
??
??
??
??
??
??
??
??
???
??
???
?
??
???
?
??
???
?
??
???
?
??
???
??
??
???
??
??
???
??
??
???
??
??
???
??
???
??
???
?
???
??
???
?
???
??
???
?
???
??
???
?
???
??
???
?
??
???
???
??
????????????????????????
??
??
??
??
??
??
?
Figure 1: Precision classified by prize money
amount
i.e., keyword association ratio presented in Sec-
tion 3.2.2, and word weights of Section 3.1 +
answer selection of Section 4. Integration of
keyword association ratio and word weight out-
performs those baselines. In total, about 79%
(for the development set) and 76% (for the test
set) of the questions are solved by the proposed
answer validation method based on keyword as-
sociation.
Comparing the performance of the two data
sets, word weights + answer selection has 4%
lower precision in the test set. This result indi-
cates that rules for word weights as well as an-
swer selection rules overfit to the development
set. On the other hand, the difference of the pre-
cisions of the keyword association ratio is much
less between the two data sets, indicating that
keyword association ratio has less overfit to the
development set.
Finally, the result of the experiment where
the development set was solved by the inte-
gration method was classified by prize money
amount. The result is shown in Figure 1. The
more the prize money amount is, the lower the
precision seems to be, while their precisions are
all above 60%, and their differences are less than
20% in most cases. It can be concluded that our
system can solve questions of all the levels al-
most equally.
7 Related Work
Kwok et al (2001) proposed the first automated
question-answering system which uses the web.
First, it collects documents that are related to
the question sentence using google and picks an-
swer candidates up from them. Second, it se-
lects an answer based on the frequency of can-
didates which appear near the keywords.
In the method proposed by Brill et al (2002),
answer candidates are picked up from the sum-
mary pages returned by a search engine. Then,
each answer candidate is validated by searching
for relevant documents in the TREC QA docu-
ment collection. Both methods do not consider
the number of hits returned by the search en-
gine.
Magnini et al (2002) proposed an answer val-
idation method which uses the number of search
engine hits. They formulate search engine
queries using AltaVista?s OR and NEAR oper-
ators. Major difference between the method of
Magnini et al (2002) and ours is in keyword se-
lection. In the method of Magnini et al (2002),
the initial keywords are content words extracted
from a question sentence. If the hits of keywords
is less than a threshold, the least important key-
word is removed. This procedure is repeated un-
til the hits of the keywords is over the threshold.
On the other hand, in our method, keywords
are selected so that the strength of the associ-
ation between the keyword and an answer can-
didate is maximized. Intuitively, our method of
keyword selection is more natural than that of
Magnini et al (2002), since it considers both
the question sentence and an answer candidate.
As for measures for scoring answer candidates,
Magnini et al (2002) proposed three measures,
out of which ?Corrected Conditional Probabil-
ity? performs best. In our implementation, the
performance of ?Corrected Conditional Proba-
bility? is about 5% lower than our best result.
8 Conclusion and Future Work
In this paper, we proposed an approach of an-
swer validation based on the strengths of lexi-
cal association between the keywords extracted
from a question sentence and each answer can-
didate. The proposed answer validation process
is decomposed into two steps: the first is to
extract appropriate keywords from a question
sentence using word features and the strength
of lexical association, while the second is to es-
timate the strength of the association between
the keywords and an answer candidate based on
the hits of search engines. In the result of exper-
imental evaluation, we showed that a good pro-
portion (79%) of the multiple-choice quiz ?Who
wants to be a millionaire? can be solved by the
proposed method.
Future works include the followings: first, we
are planning to examine whether the syntactic
structures of the question sentence is useful for
selecting appropriate keywords from the ques-
tion sentence. Secondly, it is interesting to see
whether the keyword selection method proposed
in this paper is also effective for other applica-
tions such as answer candidate collection of the
whole question answering process.
References
E. Brill, J. Lin, M. Banko, S. Dumais, and A. Ng. 2002.
Data-intensive question answering. In Proc. TREC
2001.
J. Fukumoto, T. Kato, and F. Masui. 2004. Question
answering challenge for five ranked answers and list
answers -overview of ntcir4 qac2 subtask 1 and 2-. In
Proc. 4th NTCIR Workshop Meeting.
Sadao Kurohashi and Makoto Nagao, 1999. Japanese
Morphological Analysis System JUMAN version 3.62
Manual.
Sadao Kurohashi, 1998. Japanese Dependency/Case
Structure Analyzer KNP version 2.0b6 Manual.
C. C. T. Kwok, O. Etzioni, and D. S. Weld. 2001. Scal-
ing question answering to the web. In Proc. the 10th
WWW Conf., pages 150?161.
B. Magnini, M. Negri, R. Prevete, and H. Tanev. 2002.
Is it the right answer? exploiting web redundancy for
answer validation. In Proc. 40th ACL, pages 425?432.
D. Moldovan, S. Harabagiu, R. Girju, P. Morarescu, and
F. Lacatusu. 2003. Lcc tools for question answering.
In Proc. TREC 2002.
S. Sato and Y. Sasaki. 2003. Automatic collection of
related terms from the web. In Proc. 41st ACL, pages
121?124.
E. M. Voorhees. 2004. Overview of the trec 2003 ques-
tion answering track. In Proc. TREC 2003.
A Comparative Study on Compositional Translation Estimation
using a Domain/Topic-Specific Corpus collected from the Web
Masatsugu Tonoike?, Mitsuhiro Kida?, Toshihiro Takagi?, Yasuhiro Sasaki?,
Takehito Utsuro??, Satoshi Sato? ? ?
?Graduate School of Informatics, Kyoto University
Yoshida-Honmachi, Sakyo-ku, Kyoto 606-8501, Japan
??Graduate School of Systems and Information Engineering, University of Tsukuba
1-1-1, Tennodai, Tsukuba, 305-8573, Japan
? ? ?Graduate School of Engineering, Nagoya University
Furo-cho, Chikusa-ku, Nagoya 464-8603, Japan
Abstract
This paper studies issues related to the
compilation of a bilingual lexicon for tech-
nical terms. In the task of estimating bilin-
gual term correspondences of technical
terms, it is usually rather difficult to find
an existing corpus for the domain of such
technical terms. In this paper, we adopt
an approach of collecting a corpus for the
domain of such technical terms from the
Web. As a method of translation esti-
mation for technical terms, we employ a
compositional translation estimation tech-
nique. This paper focuses on quantita-
tively comparing variations of the compo-
nents in the scoring functions of composi-
tional translation estimation. Through ex-
perimental evaluation, we show that the
domain/topic-specific corpus contributes
toward improving the performance of the
compositional translation estimation.
1 Introduction
This paper studies issues related to the compilation
of a bilingual lexicon for technical terms. Thus
far, several techniques of estimating bilingual term
correspondences from a parallel/comparable cor-
pus have been studied (Matsumoto and Utsuro,
2000). For example, in the case of estimation from
comparable corpora, (Fung and Yee, 1998; Rapp,
1999) proposed standard techniques of estimating
bilingual term correspondences from comparable
corpora. In their techniques, contextual similarity
between a source language term and its translation
candidate is measured across the languages, and
all the translation candidates are re-ranked accord-
ing to their contextual similarities. However, there
are limited number of parallel/comparable corpora
that are available for the purpose of estimating
bilingual term correspondences. Therefore, even
if one wants to apply those existing techniques to
the task of estimating bilingual term correspon-
dences of technical terms, it is usually rather dif-
ficult to find an existing corpus for the domain of
such technical terms.
On the other hand, compositional translation es-
timation techniques that use a monolingual corpus
(Fujii and Ishikawa, 2001; Tanaka and Baldwin,
2003) are more practical. It is because collecting a
monolingual corpus is less expensive than collect-
ing a parallel/comparable corpus. Translation can-
didates of a term can be compositionally generated
by concatenating the translation of the constituents
of the term. Here, the generated translation candi-
dates are validated using the domain/topic-specific
corpus.
In order to assess the applicability of the com-
positional translation estimation technique, we
randomly pick up 667 Japanese and English tech-
nical term translation pairs of 10 domains from ex-
isting technical term bilingual lexicons. We then
manually examine their compositionality, and find
out that 88% of them are actually compositional,
which is a very encouraging result.
But still, it is expensive to collect a
domain/topic-specific corpus. Here, we adopt
an approach of using the Web, since documents
of various domains/topics are available on the
Web. When validating translation candidates
using the Web, roughly speaking, there exist the
following two approaches. In the first approach,
translation candidates are validated through
the search engine (Cao and Li, 2002). In the
second approach, a domain/topic-specific corpus
is collected from the Web in advance and fixed
11
collecting terms
of specific
domain/topic
(language S )
XSU (# of translations
is one)
compiled bilingual lexicon
process data
collecting
corpus
(language T )
domain/topic
specific
corpus
(language T )
sample terms
of specific 
domain/topic
(language S )
XSTU , XSTM ,YST
estimating bilingual term
correspondences
language pair (S,T )
term set
(language S )
XTU
(lang. T )
translation set
(language T )
web
(language S )
web
(language S )
existing
bilingual lexicon
XSM (# of translations
is more than one)
YS (# of translations
is zero)
web
(language T )
web
(language T )
looking up
bilingual lexicon
validating
translation
candidates
web
(language T )
web
(language T )
Figure 1: Compilation of a Domain/Topic-
Specific Bilingual Lexicon using the Web
before translation estimation, then generated
translation candidates are validated against the
domain/topic-specific corpus (Tonoike et al,
2005). The first approach is preferable in terms of
coverage, while the second is preferable in terms
of computational efficiency. This paper mainly
focuses on quantitatively comparing the two
approaches in terms of coverage and precision of
compositional translation estimation.
More specifically, in compositional translation
estimation, we decompose the scoring function
of a translation candidate into two components:
bilingual lexicon score and corpus score. In this
paper, we examine variants for those components
and define 9 types of scoring functions in total.
Regarding the above mentioned two approaches
to validating translation candidates using the Web,
the experimental result shows that the second
approach outperforms the first when the correct
translation does exist in the corpus. Furthermore,
we examine the methods that combine two scor-
ing functions based on their agreement. The ex-
perimental result shows that it is quite possible to
achieve precision much higher than those of single
scoring functions.
2 Overall framework
The overall framework of compiling a bilingual
lexicon from the Web is illustrated as in Figure 1.
Suppose that we have sample terms of a specific
domain/topic, then the technical terms that are to
be listed as the headwords of a bilingual lexicon
are collected from the Web by the related term col-
lection method of (Sato and Sasaki, 2003). These
collected technical terms can be divided into three
subsets depending on the number of translation
candidates present in an existing bilingual lexicon,
i.e., the subset XUS of terms for which the number
of translations in the existing bilingual lexicon is
one, the subset XMS of terms for which the number
of translations is more than one, and the subset YS
of terms that are not found in the existing bilingual
lexicon (henceforth, the union XUS ? XMS will be
denoted as XS). Here, the translation estimation
task here is to estimate translations for the terms
of the subsets XMS and YS . A new bilingual lex-
icon is compiled from the result of the translation
estimation for the terms of the subsets XMS and
YS as well as the translation pairs that consist of
the terms of the subset XUS and their translations
found in the existing bilingual lexicon.
For the terms of the subset XMS , it is required
that an appropriate translation is selected from
among the translation candidates found in the ex-
isting bilingual lexicon. For example, as a trans-
lation of the Japanese technical term ?????,?
which belongs to the logic circuit domain, the term
?register? should be selected but not the term ?reg-
ista? of the football domain. On the other hand, for
the terms of YS , it is required that the translation
candidates are generated and validated. In this pa-
per, out of the above two tasks, we focus on the
latter of translation candidate generation and val-
idation using the Web. As we introduced in the
previous section, here we experimentally compare
the two approaches to validating translation candi-
dates. The first approach directly uses the search
engine, while the second uses the domain/topic-
specific corpus, which is collected in advance from
the Web. Here, in the second approach, we use the
term of XUS , which has only one translation in the
existing bilingual lexicon. The set of translations
of the terms of the subset XUS is denoted as XUT .
Then, in the second approach, the domain/topic-
specific corpus is collected from the Web using the
terms of the set XUT .
3 Compositional Translation Estimation
for Technical Terms
3.1 Overview
An example of compositional translation estima-
tion for the Japanese technical term ??????
?? is illustrated in Figure 2. First, the Japanese
technical term ???????? is decomposed
into its constituents by consulting an existing
bilingual lexicon and retrieving Japanese head-
12
? application(1)
? practical(0.3)
? applied(1.6)
? action(1)
? activity(1)
? behavior(1)
? analysis(1)
? diagnosis(1)
? assay(0.3)
? behavior analysis(10)
??Compositional generation 
of translation candidate
? applied behavior analysis(17.6)
? application behavior analysis(11)
? applied behavior diagnosis(1)
??Decompose source term into constituents  
??Translate constituents into target language      process
?? ?? ??a
?? ????b
Generated translation candidates
?(1.6?1?1)+(1.6?10)
? application(1)
? practical(0.3)
? applied(1.6)
Figure 2: Compositional Translation Estimation
for the Japanese Technical Term ????????
words.1 In this case, the result of this decompo-
sition can be given as in the cases ?a? and ?b?
(in Figure 2). Then, each constituent is translated
into the target language. A confidence score is as-
signed to the translation of each constituent. Fi-
nally, translation candidates are generated by con-
catenating the translation of those constituents ac-
cording to word ordering rules considering prepo-
sitional phrase construction.
3.2 Collecting a Domain/Topic-Specific
Corpus
When collecting a domain/topic-specific corpus of
the language T , for each technical term xUT in the
set XUT , we collect the top 100 pages obtained
from search engine queries that include the term
xUT . Our search engine queries are designed such
that documents that describe the technical term xUT
are ranked high. For example, an online glossary
is one such document. When collecting a Japanese
corpus, the search engine ?goo?2 is used. The spe-
cific queries that are used in this search engine
are phrases with topic-marking postpositional par-
ticles such as ?xUT ??,? ?xUT ???,? ?xUT ?,?
and an adnominal phrase ?xUT ?,? and ?xUT .?
3.3 Translation Estimation
3.3.1 Compiling Bilingual Constituents
Lexicons
This section describes how to compile bilingual
constituents lexicons from the translation pairs of
1Here, as an existing bilingual lexicon, we use Ei-
jiro(http://www.alc.co.jp/) and bilingual constituents lexicons
compiled from the translation pairs of Eijiro (details to be de-
scribed in the next section).
2http://www.goo.ne.jp/
 
applied mathematics : ?? ??
applied science : ?? ??
applied robot : ?? ????
.
.
. frequency
? ??
applied : ?? : 40
 
Figure 3: Example of Estimating Bilingual Con-
stituents Translation Pair (Prefix)
the existing bilingual lexicon Eijiro. The under-
lying idea of augmenting the existing bilingual
lexicon with bilingual constituents lexicons is il-
lustrated in Figure 3. Suppose that the existing
bilingual lexicon does not include the translation
pair ?applied : ??,? while it includes many
compound translation pairs with the first English
word ?applied? and the first Japanese word ??
?.?3 In such a case, we align those translation
pairs and estimate a bilingual constituent transla-
tion pair which is to be collected into a bilingual
constituents lexicon.
More specifically, from the existing bilingual
lexicon, we first collect translation pairs whose
English terms and Japanese terms consist of two
constituents into another lexicon P
2
. We com-
pile the ?bilingual constituents lexicon (prefix)?
from the first constituents of the translation pairs
in P
2
and compile the ?bilingual constituents lex-
icon (suffix)? from their second constituents. The
number of entries in each language and those of
the translation pairs in these lexicons are shown in
Table 1.
The result of our assessment reveals that only
48% of the 667 translation pairs mentioned in Sec-
tion 1 can be compositionally generated by using
Eijiro, while the rate increases up to 69% using
both Eijiro and ?bilingual constituents lexicons.?4
3.3.2 Score of Translation Candidates
This section gives the definition of the scores
of a translation candidate in compositional trans-
lation estimation.
First, let ys be a technical term whose transla-
tion is to be estimated. We assume that ys is de-
3Japanese entries are supposed to be segmented into a
sequence of words by the morphological analyzer JUMAN
(http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html).
4In our rough estimation, the upper bound of this rate
is approximately 80%. An improvement from 69% to 80%
could be achieved by extending the bilingual constituents lex-
icons.
13
Table 1: Numbers of Entries and Translation Pairs
in the Lexicons
lexicon # of entries # of translationEnglish Japanese pairs
Eijiro 1,292,117 1,228,750 1,671,230
P
2
217,861 186,823 235,979
B
P
37,090 34,048 95,568
B
S
20,315 19,345 62,419
B 48,000 42,796 147,848
Eijiro : existing bilingual lexicon
P
2
: entries of Eijiro with two constituents
in both languages
B
P
: bilingual constituents lexicon (prefix)
B
S
: bilingual constituents lexicon (suffix)
B : bilingual constituents lexicon (merged)
composed into their constituents as below:
ys = s1, s2, ? ? ? , sn (1)
where each si is a single word or a sequence of
words.5 For ys, we denote a generated translation
candidate as yt.
yt = t1, t2, ? ? ? , tn (2)
where each ti is a translation of si. Then the trans-
lation pair ?ys, yt? is represented as follows.
?ys, yt? = ?s1, t1?, ?s2, t2?, ? ? ? , ?sn, tn? (3)
The score of a generated translation candidate is
defined as the product of a bilingual lexicon score
and a corpus score as follows.
Q(ys, yt) = Qdict(ys, yt) ? Qcoprus(yt) (4)
Bilingual lexicon score measures appropriateness
of correspondence of ys and yt. Corpus score
measures appropriateness of the translation candi-
date yt based on the target language corpus. If a
translation candidate is generated from more than
one sequence of translation pairs, the score of the
translation candidate is defined as the sum of the
score of each sequence.
Bilingual Lexicon Score
In this paper, we compare two types of bilin-
gual lexicon scores. Both scores are defined as the
product of scores of translation pairs included in
the lexicons presented in the previous section as
follows.
5Eijiro has both single word entries and compound word
entries.
? Frequency-Length
Qdict(ys, yt) =
n
?
i=1
q(?si, ti?) (5)
The first type of bilingual lexicon scores is re-
ferred to as ?Frequency-Length.? This score is
based on the length of translation pairs and the fre-
quencies of translation pairs in the bilingual con-
stituent lexicons (prefix,suffix) BP , BS in Table 1.
In this paper, we first assume that the translation
pairs follow certain preference rules and that they
can be ordered as below:
1. Translation pairs ?s, t? in the existing bilin-
gual lexicon Eijiro, where the term s consists
of two or more constituents.
2. Translation pairs in the bilingual constituents
lexicons whose frequencies in P
2
are high.
3. Translation pairs ?s, t? in the existing bilin-
gual lexicon Eijiro, where the term s consists
of exactly one constituent.
4. Translation pairs in the bilingual constituents
lexicons whose frequencies in P
2
are not
high.
As the definition of the confidence score
q(?s, t?) of a translation pair ?s, t?, we use the fol-
lowing:
q(?s, t?) =
?
?
?
10
(compo(s)?1) (?s, t? in Eijiro)
log
10
fp(?s, t?) (?s, t? in BP )
log
10
fs(?s, t?) (?s, t? in BS)
(6)
, where compo(s) denotes the word count of s,
fp(?s, t?) represents the frequency of ?s, t? as the
first constituent in P
2
, and fs(?s, t?) represents the
frequency of ?s, t? as the second constituent in P
2
.
? Probability
Qdict(ys, yt) =
n
?
i=1
P (si|ti) (7)
The second type of bilingual lexicon scores is re-
ferred to as ?Probability.? This score is calcu-
lated as the product of the conditional probabili-
ties P (si|ti). P (s|t) is calculated using bilingual
lexicons in Table 1.
P (s|t) =
fprob(?s, t?)
?
s
j
fprob(?sj , t?)
(8)
14
Table 2: 9 Scoring Functions of Translation Candidates and their Components
bilingual lexicon score corpus score corpus
score ID freq-length probability probability frequency occurrence off-line on-line
(search engine)
A prune/final prune/final o
B prune/final prune/final o
C prune/final prune/final o
D prune/final prune o
E prune/final
F prune/final final prune o
G prune/final prune/final o
H prune/final final o
I prune/final final o
fprob(?s, t?) denotes the frequency of the transla-
tion pair ?s, t? in the bilingual lexicons as follows:
fprob(?s, t?) =
{
10 (?s, t? in Eijiro)
fB(?s, t?) (?s, t? in B)
(9)
Note that the frequency of a translation pair in Ei-
jiro is regarded as 106 and fB(?s, t?) denotes the
frequency of the translation pair ?s, t? in the bilin-
gual constituent lexicon B.
Corpus Score
We evaluate three types of corpus scores as fol-
lows.
? Probability: the occurrence probability of yt
estimated by the following bi-gram model
Qcorpus(yt) = P (t1) ?
n
?
i=1
P (ti+1|ti) (10)
? Frequency: the frequency of a translation
candidate in a target language corpus
Qcorpus(yt) = freq(yt) (11)
? Occurrence: whether a translation candidate
occurs in a target language corpus or not
Qcorpus(yt) =
?
?
?
?
?
1 yt occurs in a corpus
0 yt does not occur
in a corpus
(12)
6It is necessary to empirically examine whether or not the
definition of the frequency of a translation pair in Eijiro is
appropriate.
Variation of the total scoring functions
As shown in Table 2, in this paper, we examine
the 9 combinations of the bilingual lexicon scores
and the corpus scores. In the table, ?prune? indi-
cates that the score is used for ranking and pruning
sub-sequences of generated translation candidates
in the course of generating translation candidates
using a dynamic programming algorithm. ?Final?
indicates that the score is used for ranking the fi-
nal outputs of generating translation candidates.
In the column ?corpus?, ?off-line? indicates that
a domain/topic-specific corpus is collected from
the Web in advance and then generated transla-
tion candidates are validated against this corpus.
?On-line? indicates that translation candidates are
directly validated through the search engine.
Roughly speaking, the scoring function ?A? cor-
responds to a variant of the model proposed by
(Fujii and Ishikawa, 2001). The scoring func-
tion ?D? is a variant of the model proposed by
(Tonoike et al, 2005) and ?E? corresponds to the
bilingual lexicon score of the scoring function ?D?.
The scoring function ?I? is intended to evaluate the
approach proposed in (Cao and Li, 2002).
3.3.3 Combining Two Scoring Functions
based on their Agreement
In this section, we examine the method that
combines two scoring functions based on their
agreement. The two scoring functions are selected
out of the 9 functions introduced in the previous
section. In this method, first, confidence of trans-
lation candidates of a technical term are measured
by the two scoring functions. Then, if the first
ranked translation candidates of both scoring func-
tions agree, this method outputs the agreed trans-
lation candidate. The purpose of introducing this
method is to prefer precision to recall.
15
collecting terms
of specific
domain/topic
(language S )
XSU (# of translations
is one)
compiled bilingual lexicon
process data
collecting
corpus
(language T )
sample terms
of specific 
domain/topic
(language S )
XSTU , XSTM ,YST
estimating bilingual term
correspondences
language pair (S,T )
term set
(language S )
XTU
(lang. T )
translation set
(language T )
web
(language S )
web
(language S )
existing
bilingual lexicon
XSM (# of translations
is more than one)
YS (# of translations
is zero)
web
(language T )
web
(language T )
looking up
bilingual lexicon
domain/topic
specific
corpus
(language T )
validating
translation
candidates
web
(language T )
web
(language T )
Figure 4: Experimental Evaluation of Translation
Estimation for Technical Terms with/without the
Domain/Topic-Specific Corpus (taken from Fig-
ure 1)
4 Experiments and Evaluation
4.1 Translation Pairs for Evaluation
In our experimental evaluation, within the frame-
work of compiling a bilingual lexicon for techni-
cal terms, we evaluate the translation estimation
portion that is indicated by the bold line in Fig-
ure 4. In this paper, we simply omit the evalua-
tion of the process of collecting technical terms to
be listed as the headwords of a bilingual lexicon.
In order to evaluate the translation estimation por-
tion, terms are randomly selected from the 10 cate-
gories of existing Japanese-English technical term
dictionaries listed in Table 3, for each of the sub-
sets XUS and YS (here, the terms of YS that consist
of only one word or morpheme are excluded). As
described in Section 1, the terms of the set XUT (the
set of translations for the terms of the subset XUS )
is used for collecting a domain/topic-specific cor-
pus from the Web. As shown in Table 3, size of the
collected corpora is 48MB on the average. Trans-
lation estimation evaluation is to be conducted for
the subset YS . For each of the 10 categories, Ta-
ble 3 shows the sizes of the subsets XUS and YS ,
and the rate of including correct translation within
the collected domain/topic-specific corpus for YS .
In the following, we show the evaluation results
with the source language S as English and the tar-
get language T as Japanese.
4.2 Evaluation of single scoring functions
This section gives the results of evaluating single
scoring functions A ? I listed in Table 2.
Table 4 shows three types of experimental re-
sults. The column ?the whole set YS? shows the
results against the whole set YS . The column
?generatable? shows the results against the trans-
lation pairs in YS that can be generated through
the compositional translation estimation process.
69% of the terms in ?the whole set YS? belongs
to the set ?generatable?. The column ?gene.-exist?
shows the result against the source terms whose
correct translations do exist in the corpus and that
can be generated through the compositional trans-
lation estimation process. 50% of the terms in ?the
whole set YS? belongs to the set ?gene.-exist?. The
column ?top 1? shows the correct rate of the first
ranked translation candidate. The column ?top 10?
shows the rate of including the correct candidate
within top 10.
First, in order to evaluate the effectiveness of
the approach of validating translation candidates
by using a target language corpus, we compare the
scoring functions ?D? and ?E?. The difference be-
tween them is whether or not they use a corpus
score. The results for the whole set YS show that
using a corpus score, the precision improves from
33.9% to 43.0%. This result supports the effec-
tiveness of the approach of validating translation
candidates using a target language corpus.
As can be seen from these results for the whole
set YS , the correct rate of the scoring function ?I?
that directly uses the web search engine in the cal-
culation of its corpus score is higher than those
of other scoring functions that use the collected
domain/topic-specific corpus. This is because,
for the whole set YS , the rate of including cor-
rect translation within the collected domain/topic-
specific corpus is 72% on the average, which is
not very high. On the other hand, the results of the
column ?gene.-exist? show that if the correct trans-
lation does exist in the corpus, most of the scor-
ing functions other than ?I? can achieve precisions
higher than that of the scoring function ?I?. This
result supports the effectiveness of the approach
of collecting a domain/topic-specific corpus from
the Web in advance and then validating generated
translation candidates against this corpus.
4.3 Evaluation of combining two scoring
functions based on their agreement
The result of evaluating the method that combines
two scoring functions based on their agreement is
shown in Table 5. This result indicates that com-
binations of scoring functions with ?off-line?/?on-
16
Table 3: Number of Translation Pairs for Evaluation (S=English)
dictionaries categories |Y
S
| |X
U
S
| corpus size C(S)
Electromagnetics 33 36 28MB 85%
McGraw-Hill Electrical engineering 45 34 21MB 71%
Optics 31 42 37MB 65%
Iwanami Programming language 29 37 34MB 93%Programming 29 29 33MB 97%
Dictionary of (Computer) 100 91 67MB 51%Computer
Anatomical Terms 100 91 73MB 86%
Dictionary of Disease 100 91 83MB 77%
250,000 Chemicals and Drugs 100 94 54MB 60%
medical terms Physical Science and Statistics 100 88 56MB 68%
Total 667 633 482MB 72%
McGraw-Hill : Dictionary of Scientific and Technical Terms
Iwanami : Encyclopedic Dictionary of Computer Science
C(S) : for Y
S
, the rate of including correct translations within the collected domain/topic-specific corpus
Table 4: Result of Evaluating single Scoring Functions
the whole set Y
S
(667 terms?100%) generatable (458 terms?69%) gene.-exist (333 terms?50%)
ID top 1 top 10 top 1 top 10 top 1 top 10
A 43.8% 52.9% 63.8% 77.1% 82.0% 98.5%
B 42.9% 50.7% 62.4% 73.8% 83.8% 99.4%
C 43.0% 58.0% 62.7% 84.5% 75.1% 94.6%
D 43.0% 47.4% 62.7% 69.0% 85.9% 94.6%
E 33.9% 57.3% 49.3% 83.4% 51.1% 84.1%
F 40.2% 47.4% 58.5% 69.0% 80.2% 94.6%
G 39.1% 46.8% 57.0% 68.1% 78.1% 93.4%
H 43.8% 57.3% 63.8% 83.4% 73.6% 84.1%
I 49.8% 57.3% 72.5% 83.4% 74.8% 84.1%
Table 5: Result of combining two scoring func-
tions based on their agreement
corpus combination precision recall F
?=1
A & I 88.0% 27.6% 0.420
off-line/ D & I 86.0% 29.5% 0.440
on-line F & I 85.1% 29.1% 0.434
H & I 58.7% 37.5% 0.457
A & H 86.0% 30.4% 0.450
F & H 80.6% 33.7% 0.476
off-line/ D & H 80.4% 32.7% 0.465
off-line A & D 79.0% 32.1% 0.456
A & F 74.6% 33.0% 0.457
D & F 68.2% 35.7% 0.469
line? corpus tend to achieve higher precisions than
those with ?off-line?/?off-line? corpus. This result
also shows that it is quite possible to achieve high
precisions even by combining scoring functions
with ?off-line?/?off-line? corpus (the pair ?A? and
?H?). Here, the two scoring functions ?A? and ?H?
are the one with frequency-based scoring func-
tions and that with probability-based scoring func-
tions, and hence, have quite different nature in the
design of their scoring functions.
5 Related Works
As a related work, (Fujii and Ishikawa, 2001) pro-
posed a technique for compositional estimation of
bilingual term correspondences for the purpose of
cross-language information retrieval. One of the
major differences between the technique of (Fu-
jii and Ishikawa, 2001) and the one proposed in
this paper is that in (Fujii and Ishikawa, 2001), in-
stead of a domain/topic-specific corpus, they use a
corpus containing the collection of technical pa-
pers, each of which is published by one of the
65 Japanese associations for various technical do-
mains. Another significant difference is that in
(Fujii and Ishikawa, 2001), they evaluate only the
performance of the cross-language information re-
trieval and not that of translation estimation.
(Cao and Li, 2002) also proposed a method
of compositional translation estimation for com-
pounds. In the method of (Cao and Li, 2002), the
translation candidates of a term are composition-
ally generated by concatenating the translation of
the constituents of the term and are validated di-
rectly through the search engine. In this paper,
we evaluate the approach proposed in (Cao and
Li, 2002) by introducing a total scoring function
17
that is based on validating translation candidates
directly through the search engine.
6 Conclusion
This paper studied issues related to the compila-
tion a bilingual lexicon for technical terms. In
the task of estimating bilingual term correspon-
dences of technical terms, it is usually rather dif-
ficult to find an existing corpus for the domain
of such technical terms. In this paper, we adopt
an approach of collecting a corpus for the do-
main of such technical terms from the Web. As
a method of translation estimation for technical
terms, we employed a compositional translation
estimation technique. This paper focused on quan-
titatively comparing variations of the components
in the scoring functions of compositional transla-
tion estimation. Through experimental evaluation,
we showed that the domain/topic specific corpus
contributes to improving the performance of the
compositional translation estimation.
Future work includes complementally integrat-
ing the proposed framework of compositional
translation estimation using the Web with other
translation estimation techniques. One of them is
that based on collecting partially bilingual texts
through the search engine (Nagata and others,
2001; Huang et al, 2005). Another technique
which seems to be useful is that of transliteration
of names (Knight and Graehl, 1998; Oh and Choi,
2005).
References
Y. Cao and H. Li. 2002. Base noun phrase translation using
Web data and the EM algorithm. In Proc. 19th COLING,
pages 127?133.
A. Fujii and T. Ishikawa. 2001. Japanese/english cross-
language information retrieval: Exploration of query
translation and transliteration. Computers and the Hu-
manities, 35(4):389?420.
P. Fung and L. Y. Yee. 1998. An IR approach for translating
new words from nonparallel, comparable texts. In Proc.
17th COLING and 36th ACL, pages 414?420.
F. Huang, Y. Zhang, and S. Vogel. 2005. Mining key phrase
translations from web corpora. In Proc. HLT/EMNLP,
pages 483?490.
K. Knight and J. Graehl. 1998. Machine transliteration.
Computational Linguistics, 24(4):599?612.
Y. Matsumoto and T. Utsuro. 2000. Lexical knowledge ac-
quisition. In R. Dale, H. Moisl, and H. Somers, editors,
Handbook of Natural Language Processing, chapter 24,
pages 563?610. Marcel Dekker Inc.
M. Nagata et al 2001. Using the Web as a bilingual dictio-
nary. In Proc. ACL-2001 Workshop on Data-driven Meth-
ods in Machine Translation, pages 95?102.
J. Oh and K. Choi. 2005. Automatic extraction of english-
korean translations for constituents of technical terms. In
Proc. 2nd IJCNLP, pages 450?461.
R. Rapp. 1999. Automatic identification of word translations
from unrelated English and German corpora. In Proc.
37th ACL, pages 519?526.
S. Sato and Y. Sasaki. 2003. Automatic collection of related
terms from the web. In Proc. 41st ACL, pages 121?124.
T. Tanaka and T. Baldwin. 2003. Translation selection for
japanese-english noun-noun compounds. In Proc. Ma-
chine Translation Summit IX, pages 378?85.
M. Tonoike, M. Kida, T. Takagi, Y. Sasaki, T. Utsuro, and
S. Sato. 2005. Effect of domain-specific corpus in com-
positional translation estimation for technical terms. In
Proc. 2nd IJCNLP, Companion Volume, pages 116?121.
18
Proceedings of the Workshop on BioNLP, pages 185?192,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
TEXT2TABLE:  
Medical Text Summarization System based on Named Entity 
Recognition and Modality Identification 
 
 
Eiji ARAMAKI Yasuhide MIURA Masatsugu TONOIKE 
The university of Tokyo Fuji Xerox Fuji Xerox 
eiji.aramaki@gmail.com Yasuhide.Miura@fujixerox.co.jp masatsugu.tonoike@fujixerox.co.jp 
 
Tomoko OHKUMA 
 
Hiroshi MASHUICHI 
 
Kazuhiko OHE 
Fuji Xerox Fuji Xerox The university of Tokyo Hospital 
ohkuma.tomoko@fujixerox.co.jp hiroshi.masuichi@fujixerox.co.jp kohe@hcc.h.u-tokyo.ac.jp 
 
 
 
Abstract 
With the rapidly growing use of electronic 
health records, the possibility of large-scale 
clinical information extraction has drawn 
much attention. It is not, however, easy to ex-
tract information because these reports are 
written in natural language. To address this 
problem, this paper presents a system that 
converts a medical text into a table structure. 
This system?s core technologies are (1) medi-
cal event recognition modules and (2) a nega-
tive event identification module that judges 
whether an event actually occurred or not. 
Regarding the latter module, this paper also 
proposes an SVM-based classifier using syn-
tactic information. Experimental results dem-
onstrate empirically that syntactic information 
can contribute to the method?s accuracy. 
1 Introduction 
The use of electronic texts in hospitals is increas-
ing rapidly everywhere. This study specifically 
examines discharge summaries, which are reports 
generated by medical personnel at the end of a pa-
tient?s hospital stay. They include massive clinical 
information about a patient?s health, such as the 
frequency of drug usage, related side-effects, and 
correlation between a disease and a patient?s ac-
tions (e.g., smoking, drinking), which enables un-
precedented large-scale research, engendering 
promising findings. 
N
A
(1
(2
(3
                                                          
evertheless, it is not easy to extract clinical in-
formation from the reports because these reports 
are written in natural language. An example of a 
discharge summary is presented in Table 1. The 
table shows records that are full of medical jargon, 
acronyms, shorthand notation, misspellings, and 
sentence fragments (Tawanda et al, 2006). 
To address this problem, this paper presents a 
proposal of a system that extracts medical events 
and date times from a text. It then converts them 
into a table structure. We designate this system 
TEXT2TABLE, which is available from a web 
site 1 . The extraction method, which achieves a 
high accuracy extraction, is based on Conditional 
Random Fields (CRFs) (Lafferty et al, 2001). 
nother problem is posed by events that do not 
actually occur, i.e., future scheduled events, events 
that are merely intended to take place, or hypo-
thetical events. As described herein, we call such 
non-actual events negative events. Negative 
events are frequently mentioned in medical re-
cords; actually, in our corpus, 12% of medical 
events are negative. Several examples of negative 
events (in italic letters) are presented below: 
 
) no headache 
) keep appointment of radiotherapy 
) .. will have intravenous fluids 
1 http://lab0.com/  
185
(4
(4'
(5
th
(6
ac
A
B
T
T
A
) .. came for radiotherapy 
) .. came for headache 
) Every week radiation therapy and chemical 
erapy are scheduled 
) Please call Dr. Smith with worsening head-
he or back pain, or any other concern. 
 
Negative events have two characteristics. First, 
various words and phrases indicate that an event is 
negative. For this study, such a word or phrase that 
makes an event negative is called a negative trig-
ger. For instance, a negation word ?no? is a nega-
tive trigger in (1). A noun ?appointment? in (2) is a 
negative trigger. Similarly, the auxiliary ?will? in 
(3) signals negation. More complex phenomena are 
presented in (4) and (4'). For instance, ?radiother-
apy? in (4) is a negative event because the therapy 
will be held in the future. In contrast, ?headache? 
in (4') is not negative because a patient actually has 
a ?headache?. These indicate that a simple rule-
based approach (such as a list of triggers) can only 
imply classification of whether an event is negative 
or not, and that information of the event category 
(e.g., a therapy or symptom) is required. 
nother characteristic is a long scope of a nega-
tive trigger. Although negative triggers are near the 
descriptive words of events in (1)?(4), there could 
alternatively be a great distance of separation, as 
portrayed in (5) and (6). In (5), a noun coordina-
tion separates a negative trigger from the event. In 
(6), the trigger ?please? renders all events in that 
sentence negative. These indicate that neighboring 
words are insufficient to determine whether an 
event is negative or not. To deal with (5), syntactic 
information is helpful because the trigger and the 
event are neighboring in the dependency structure, 
as portrayed in Fig. 2. To deal with (6), bag-of-
word (BOW) information is desired. 
ecause of the observation described above, this 
paper presents a proposal of a classifier: whether 
an event is negative or not. The proposed classifier 
uses various information, the event category, 
neighboring words, BOW, and dependent phrases. 
he point of this paper is two-fold: (1) We pro-
pose a new type of text-summarizing system 
(TEXT2TABLE) that requires a technique for a 
negative event identification. (2) We investigate 
what kind of information is helpful for negative 
event identification. 
he experiment results revealed that, in spite of 
the risk of parsing error, syntactic information can 
contribute to performance, demonstrating the fea-
sibility of the proposed approach. 
lthough experiments described in this paper are 
related to Japanese medical reports, the proposed 
method does not depend on specific languages or 
domains. 
 
Table 1: A Health Record Sample. 
BRIEF RESUME OF HOSPITAL COURSE : 57 yo with 
NSCLCa with back pain and headache . Trans-
ferred from neurosurgery for additional mgmt 
with palliative XRT to head . Pt initially 
presented with cough and hemoptysis to his 
primary MD . On CXR he was found to have a 
upper left lobe mass . He subsequently un-
derwent bronchoscopy and bx revealed non-
small cell adeno CA. STaging revealed multi-
ple bony mets including skull, spine with 
MRI revealing mild compression of vertebral 
bodies at T9, T11, T12 . T9 with encroach-
ment of spinal cord underwent urgent XRT 
with no response so he was referred to neu-
rosurgery for intervention . MRI-rt. fron-
tal, left temporal, rt cerebellar 
hemorrhagic enhancing lesions- most likely 
extensive intracranial mets? T-spine surgery 
considered second priority and plan to radi-
ate cranially immediately with steroid and 
anticonvulsant . He underwent simulation on 
3/28 to whole brain and T3-T7 fields with 
plan for rx to both sites over 2.5 weeks. 
Over the past 2 weeks he has noted frontal 
and occipital HA with left eyelid swelling, 
ptosis, and denies CP, SOB, no sig. BM in 
past 5 days, small amt of stool after sup-
pository. Neuro?He was Dilantin loaded and a 
level should be checked on 3/31 . He is to 
continue Decadron . Onc?He is to receive XRT 
on 3/31 and daily during that week . Pain 
control?Currently under control with MS con-
tin and MSIR prn. regimen . Follow HA, LBP. 
ENDO?Glucose control monitored while on de-
cadron with SSRI coverage . Will check 
HgbA1C prior to discharge . GI?Aggressive 
bowel regimen to continue at home . Pt is 
Full Code . ADDITIONAL COMMENTS: Please call 
Dr. Xellcaugh with worsening headache or 
back pain, or any other concern . Keep ap-
pointment as scheduled with XRT . Please 
check fingerstick once a day, and record, 
call MD if greater than 200 .  
 
186
 
Figure 1: Visualization result (Left), magnified (Right). 
 
 
Figure 2: Negative Triggers and Events on a Depend-
ency Structure. 
 
Table 2: Corpora and Modalities 
CORPUS MODALITY 
ACE asserted, or other 
TIMEML must, may, should, would, or 
could 
Prasad et al, 
2006 
assertion, belief, facts or eventu-
alities 
Saur? et al, 2007 certain, probable, possible, or 
other 
Inui et al, 2008 affirm, infer, doubt, hear, intend, 
ask, recommend, hypothesize, or 
other 
THIS STUDY S/O, necessity, hope, possible, 
recommend, intend  
 
Table 3: Markup Scheme (Tags and Definitions) 
Tag Definition (Examples) 
R Remedy, Medical operation 
(e.g. radiotherapy) 
T Medical test, Medical examination 
(e.g., CT, MRI) 
D Deasese, Symptom 
(e.g., Endometrial cancer, headache) 
M Medication, administration of a drug 
(e.g., Levofloxacin, Flexeril) 
A patient action 
(e.g., admitted to a hospital) 
V Other verb 
(e.g., cancer spread to ...)  
 
2 Related Works 
2.1 Previous Markup Schemes 
In the NLP field, fact identification has not been 
studied well to date. Nevertheless, similar analyses 
can be found in studies of sentence modality. 
The Automatic Content Extraction (ACE)2 in-
formation extraction program deals with event ex-
traction, by which each event is annotated with 
temporal and modal markers. 
A
S  
A
T
                                                          
 similar effort is made in the TimeML project 
(Pustejovsky et al, 2003). This project specifically 
examines temporal expressions, but several modal 
expressions are also covered. 
Prasad et al (2006) propose four factuality clas-
sifications (certain, probable...etc.) for the Penn 
Discourse TreeBank (PDTB) 3. 
aur? et al (2007) propose three modal categories
for text entailment tasks. 
mong various markup schemes, the most recent 
one is Experience Mining (Inui et al, 2008), which 
collects personal experiences from the web. They 
also distinguish whether an experience is an actual 
one or not, which is a similar problem to that con-
fronting us. 
able 2 portrays a markup scheme adopted by 
each project. Our purpose is similar to that of Ex-
perience Mining. Consequently, we fundamentally 
adopt its markup scheme. However, we modify the 
label to suit medical mannerisms. For example, 
?doubt? is modified into ?(S/O) suspicion of?. Rare 
modalities such as ?hear? are removed. 
 
2.2 Previous Algorithms 
Negation is a traditional topic in medical fields. 
Therefore, we can find many previous studies of 
the topic in the relevant literature. 
An algorithm, NegEx4 was proposed by Chap-
man et al (Chapman et al, 2001a; Chapman et al, 
2001b). It outputs an inference of whether a term is 
positive or negative. The original algorithm is 
based on a list of negation expressions. Goldin et al 
(2003) incorporate machine learning techniques 
(Na?ve Bayes and decision trees) into the algorithm. 
The extended version (ConText) was also proposed 
(Chapman et al, 2007). 
Elkin et al (2005) use a list of negation words 
and a list of negation scope-ending words to iden-
2 http://projects.ldc.upenn.edu/ace/ 
3 http://www.seas.upenn.edu/~pdtb/ 
4 http://www.dbmi.pitt.edu/chapman/NegEx.html 
187
tify negated statements and their scope. Their tech-
nique was used in The MAYO Clinic Vocabulary 
Server (MCVS)5, which encodes clinical expres-
sions into medical ontology (SNOMED-CT) and 
identifies whether the event is positive or negative. 
M
H
T
A
                                                          
utalik et al (2001) earlier developed Negfinder 
to recognize negated patterns in medical texts. 
Their system uses regular expressions to identify 
words indicating negation. Then it passes them as 
special tokens to the parser, which makes use of 
the single-token look-ahead strategy. 
uang and Lowe (2007) implemented a hybrid 
approach to automated negation detection. They 
combined regular expression matching with 
grammatical parsing: negations are classified based 
on syntactic categories. In fact, they are located in 
parse trees. Their hybrid approach can identify ne-
gated concepts in radiology reports even when they 
are located distantly from the negative term. 
he Medical Language Extraction and Encoding 
(MedLEE) system was developed as a general 
natural language processor to encode clinical doc-
uments in a structured form (Friedman et al, 
1994). Negated concepts and certainty modifiers 
are also encoded within the system. 
Veronika et al (2008) published a negation 
scope corpus6 in which both negation and uncer-
tainty are addressed. 
lthough their motivations are identical to ours, 
two important differences are apparent. (1) Previ-
ous (except for Veronika et al, 2008) methods deal 
with the two-way problem (positive or negative), 
whereas the analyses proposed herein tackle more 
fine-grained modalities. (2) Previous studies (ex-
cept for Huang et al, 2007) are based on BOW 
approaches, whereas we use syntactic information. 
3 Medical Text Summarization System: 
TEXT2TABLE 
Because the core problem of this paper is to iden-
tify negative events, this section briefly presents a 
description of the entire system, which consists of 
four steps. The detailed algorithm of negative iden-
tification is explained in Section 4. 
STEP 1: Event Identification 
First, we define the event discussed in this paper. 
We deal with events of six types, as presented in 
5 http://mayoclinproc.highwire.org/content/81/6/741.figures-
only 
6 www.inf.u-szeged.hu/rgai/bioscope 
Table 3. Two of the four are Verb Phrases (base 
VPs); the others are noun phrases (base-NPs). Be-
cause this task is similar to Named Entity Recogni-
tion (NER), we use the state-of-the art NER 
method, which is based on the IOB2 representation 
and Conditional Random Fields (CRFs). In learn-
ing, we use standard features, as shown in Table 4. 
 
Table 4: Features for Event Identification 
Lexicon 
and 
Stem 
Current target word (and its stem) and its 
surrounding words (and stem). The win-
dow size is five words (-2, -1, 0, 1, 2). 
POS Part of speech of current target word and 
its surrounding words (-2, -1, 0, 1, 2). The 
part of speech is analyzed using a POS 
tagger7. 
DIC A fragment for the target word appears in 
the medical dictionary (Ito et al, 2003).  
 
STEP 2: Normalization 
As described in Section 1, a term in a record is 
sometimes an acronym: shorthand notation. Such 
abbreviations are converted into standard notation 
through (1) date time normalization or (2) event 
normalization. 
(1) Date Time Normalization 
As for date time expressions, relative date expres-
sions are converted into YYYY/MM/DD as fol-
lows. 
  On Dec Last year ? 2007/12/XX 
  10 Dec 2008        ? 2008/12/10 
These conversions are based on heuristic rules. 
(2) Event Normalization 
Medical terms are converted into standard notation 
(dictionary entry terms) using orthographic disam-
biguation (Aramaki et al, 2008). 
STEP 3: TIME?EVENT Relation Identification 
Then, each event is tied with a date time. The cur-
rent system relies on a simple rule (i.e., an event is 
tied with the latest date time). 
STEP 4: Negative Identification 
The proposed SVM classifier distinguishes nega-
tive events from other events. The detailed algo-
rithm is described in the next section. 
4 Modality Identification Algorithm 
First, we define the negative. We classify modality 
events into eight types (Table 5). These classifica-
tions are motivated by those used in previous stud-
                                                          
7 http://chasen-legacy.sourceforge.jp/ 
188
ies (Inui et al, 2008). However, we simplify their 
scheme because several categories are rare in this 
domain. 
T
U
hese classes are not exclusive. For that reason, 
they sometimes lead to multiple class events. For 
example, given ?No chemotherapy is planned?, an 
event ?chemotherapy? belongs to two classes, 
which are ?NEGATION? and ?FUTURE?. 
Training Phase 
sing a corpus with modality annotation, we train 
a SVM classifier for each category. The training 
features come from four parts: 
(1) Current phrases: words included in a current 
event. We also regard their STEMs, POSs, and the 
current event category as features. 
(2) Surrounding phrases: words included in the 
current event phrase and its surrounding two 
phrases (p1, p2, n1, n2, as depicted in Fig. 3). The 
unit of the phrase is base-NP/VP, which is pro-
duced by the Japanese parser (Kurohashi et al, 
1994). Its window size is two in the neighboring 
phrase (p1, p2, c, n1, n2). We also deal with their 
STEMs and POSs. 
(3) Dependent phrases: words included in the 
parent phrase of the current phrase (d1 in Fig. 3), 
and grandparent phrases (d2 in Fig. 3). We also 
deal with their STEMs and POSs. 
(4) Previous Event: words (with STEMs and 
POSs) included in the previous (left side) events. 
Additionally, we deal with the previous event cate-
gory and the modality class. 
(5) Bag-of-words: all words (with STEMs and 
POSs) in the sentence. 
 
TEST Phrase 
During the test, each SVM classifier runs. 
Although this task is multiclass labeling, several 
class combinations are unnatural, such as 
FUTURE and S/O. We list up possible label com-
binations (that have at least one occurrence in the 
corpora); if such a combination appears in a text, 
we adapt a high confidence label (using a marginal 
distance). 
 
5 Experiments 
We investigate what kind of information contrib-
utes to the performance in various machine learn-
ing algorithms. 
 
Table 5: Classification of Modalities 
NEGATION An event with negation words 
such as ?not? or ?no?. 
FUTURE An event that is scheduled for 
execution in the future. 
PURPOSE An event that is planed by a doc-
tor, but its time schedule is am-
biguous (just a hope/intention).  
S/O An event (usually a disease) that 
is suspected. For example, given 
?suspected microscopic tumor in 
...?, ?microscopic tumor'' is an 
S/O event.? 
NECESSITY An event (usually a remedy or 
medical test) that is required. 
INTEND An event that is hoped for by a 
patient.  
Note that if the event is hoped by 
a doctor, we regard is a 
PURPOSE or FUTURE. For ex-
ample, given ?He hoped for 
chemical therapy?, ?chemical 
therapy? is INTEND. 
POSSIBLE An event (usually remedy) that is 
possible under the current situa-
tion. 
RECOMMEND An event (usually remedy) that is 
recommended by other doctor(s). 
 
 
5.1 Corpus and Setting 
We collected 435 Japanese discharge summaries in 
which events and the modality are annotated. For 
training, we used the CRF toolkit8 with standard 
parameters. In this experiment setting, the input is 
an event with its contexts. The output is an event 
modality class (positive of negative in two-way) 
(or more detailed modality class in nine-way). 
T
 
                                                          
he core problem addressed in this paper is mo-
dality classification. Therefore, this task setting 
assumes that all events are identified correctly. 
Table 6 presents the event identification accuracy. 
Except for the rare class V (the other verb), we got 
more than 80% F-scores. It is true that the accu-
racy is not perfect. Nevertheless, most of the re-
maining problems in this step will be solved using 
a larger corpus. 
5.2 Comparable Methods 
We conducted experiments in the 10-fold cross 
validation manner. We investigated the perform-
8 http://crfpp.sourceforge.net/ 
189
ance in various feature combinations and the fol-
lowing machine learning methods. 
 
 
Figure 3: Features 
 
Table 6: Event Identification Result. Tag precision re-
call F-score.  
 # P R F 
A (ACTION) 1,556 94.63 91.04 92.80 
V (VERB) 1,047 84.64 74.89 79.47 
D (DISEASE) 3,601 85.56 80.24 82.82 
M (MEDICINE) 1,045 86.99 81.34 84.07 
R (REMEDY) 1,699 84.50 76.36 80.22 
T (TEST) 2,077 84.74 76.68 80.51 
ALL 11,025 84.74 76.68 80.51  
 
Table 7: Various Machine Learning Method 
SVM Support Vector Machine (Vapnik, 
1999). We used TinySVM9 with a 
polynomial kernel (degree=2). 
AP Averaged Perceptron (Collins, 2002) 
PA1 Passive Aggressive I (Crammer et 
al., 2006)* 
PA2 Passive Aggressive II (Crammer et 
al., 2006)* 
CW Confidence Weighted (Dredze et al, 
2008)* 
* The online learning library10 is used for AP PA1,2 
CW . 
 
5.3 Evaluation Metrics 
We adopt evaluation of two types: 
(1) Two-way: positive or negative: 
(2) Nine-way: positive or one of eight modality 
categories. 
Recall and F-measure are investigated in both for 
evaluation precision. 
 
5.4 Results 
The results are shown in Table 8 (Two-Way) and 
in Table 9 (Nine-Way). 
Current Event Category 
The results in ID0?ID1 indicate that the current 
event category (CAT) is useful. However, events 
are sometimes misestimated in real settings. We 
                                                          
In
R
A
A
H
9 http://chasen.org/ taku/software/TinySVM/ 
10 http://code.google.com/p/oll 
must check more practical performance in the fu-
ture. 
Bag-of-words (BOW) Information 
Results in ID1?ID2 indicate that BOW is impor-
tant. 
Surrounding Phrase Contribution 
The results appearing in ID2?ID9 represent the 
contribution of each feature position. From ID3, 
ID4, and ID7 results, next phrases (n1, n2) and 
parent phrases (d1) were able to boost the accuracy. 
Despite the risk of parsing errors, parent phrases 
(d1) are helpful, which is an insight of this study. 
 contrast, we can say that the following features 
had little contribution: previous phrases (p1, p2 
from ID5 and ID6), grandparent phrases (d2 from 
ID8), and previous events (e from ID9). 
egarding p1 and p2, these modalities are rarely 
expressed in the previous parts in Japanese. 
s for d2, the grandparent phrases might be too 
removed from the target events. 
s for e, because texts in health records are frag-
mented, each event might have little relation. 
owever, the above features are also helpful in 
cases with a stronger learning algorithm. 
In fact, among ID10?ID14, the SVM-based 
classifier achieved the best accuracy with all fea-
tures (ID14). 
 
Table 8: Two-way Results 
 
? indicates the used feature. c are features from the cur-
rent phrase. p1, p2, n1, n2 are features from surrounding 
phrases. e are features from a previous event. BOW is a 
bag-of-words using features from an entire sentence. 
CAT is the category of the current event. 
 
190
Learning Methods 
Regarding the learning algorithms, all online learn-
ing methods (ID7 and ID15?17) showed lower ac-
curacies than SVM (ID11), indicating that this task 
requires heavy learning. 
 
Nine-way Results 
Table 9 presents the accuracies of each class. Fun-
damentally, we can obtain high performance in the 
frequent classes (such as NEGATION, PURPOSE, 
and S/O). In contrast, the classifier suffers from 
low frequent classes (such as FUTURE). How to 
handle such examples is a subject of future study. 
 
Table 9: Two-way Results 
 # Preci-
sion 
Re-
call 
F-
measure 
NEGATION 441 84.19 77.36 80.63 
PURPOSE 346 91.35 63.87 75.17 
S/O 242 90.74 72.39 80.53 
FUTURE 97 23.31 55.96 32.91 
POSSIBLE 36 83.33 40.55 54.55 
INTEND 32 76.66 29.35 42.44 
RECOMMEND 21 95.71 38.57 54.98 
NECESSITY 4 100 0 0  
 
4.5 Future Works 
In this section, we will discuss several remaining 
problems. First, as described, the classifier suffers 
from low frequent modality classes. To give more 
examples for such classes is an important problem. 
Our final goal is to realize precise information ex-
traction from health records. Our IE systems are 
already available at the web site (http://lab0.com). 
Comprehensive evaluation of those systems is re-
quired. 
6 Conclusions 
This paper presented a classifier that identified 
whether an event has actually occurred or not. The 
proposed SVM-based classifier uses both BOW 
information and dependency parsing results. The 
experimental results demonstrated 85.8 F-
measure% accuracy and revealed that syntactic 
information can contribute to the method?s accu-
racy. In the future, a method of handling low-
frequency events is strongly desired. 
 
 
Acknowledgments 
Part of this research is supported by Grant-in-Aid 
for Scientific Research (A) of Japan Society for the 
Promotion of Science Project Number:?20680006  
F.Y.2008-20011 and the Research Collaboration 
Project with Fuji Xerox  Co. Ltd. 
References 
Wendy Chapman, Will Bridewell, Paul Hanbury, Greg-
ory F. Cooper, and Bruce Buchanan. 2001a. Evalua-
tion of negation phrases in narrative clinical reports. 
In Proceedings of AMIA Symp, pages 105-109. 
Wendy Chapman, Will Bridewell, Paul Hanbury, Greg-
ory F. Cooper, and Bruce Buchanan. 2001b. A sim-
ple algorithm for identifying negated findings and 
diseases in discharge summaries. Journal of Bio-
medical Informatics, 5:301-310. 
Wendy Chapman, John Dowling and David Chu. 2007. 
ConText: An algorithm for identifying contextual 
features from clinical text. Biological, translational, 
and clinical language processing (BioNLP2007), pp. 
81?88. 
Eiji Aramaki, Takeshi Imai, Kengo Miyo, and Kazuhiko 
Ohe: Orthographic Disambiguation Incorporating 
Transliterated Probability International Joint Confer-
ence on Natural Language Processing (IJCNLP2008), 
pp.48-55, 2008. 
Peter L. Elkin, Steven H. Brown, Brent A. Bauer, Casey 
S. Husser, William Carruth, Larry R. Bergstrom, and 
Dietlind L. Wahner Roedler. A controlled trial of au-
tomated classification of negation from clinical notes. 
BMC Medical Informatics and Decision Making 
5:13. 
C. Friedman, P.O. Alderson, J.H. Austin, J.J. Cimino, 
and S.B. Johnson. 1994. A general natural language 
text processor for clinical radiology. Journal of the 
American Medical Informatics Association, 
1(2):161-174. 
L. Gillick and S.J. Cox. 1989. Some statistical issues in 
the comparison of speech recognition algorithms. In 
Proceedings of IEEE International Conference on 
Acoustics, Speech, and Signal Processing, pages 532-
535. 
Ilya M. Goldin and Wendy Chapman. 2003. Learning to 
detect negation with not in medical texts. In Work-
shop at the 26th ACM SIGIR Conference. 
Yang Huang and Henry J. Lowe. 2007. A novel hybrid 
approach to automated negation detection in clinical 
radiology reports. Journal of the American Medical 
Informatics Association, 14(3):304-311. 
191
Kentaro Inui, Shuya Abe, Hiraku Morita, Megumi Egu-
chi, Asuka Sumida, Chitose Sao, Kazuo Hara, Koji 
Murakami, and Suguru Matsuyoshi. 2008. Experi-
ence mining: Building a large-scale database of per-
sonal experiences and opinions from web documents. 
In Proceedings of the 2008 IEEE/WIC/ACM Interna-
tional Conference on Web Intelligence, pages 314-
321. 
M. Ito, H. Imura, and H. Takahisa. 2003. Igaku- Shoin?s 
Medical Dictionary. Igakusyoin. 
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic 
analysis method of long Japanese sentences based on 
the detection of conjunctive structures. Computa-
tional Linguistics, 20(4). 
Pradeep G. Mutalik, Aniruddha Deshpande, and Pra-
kash M. Nadkarni. 2001. Use of general purpose ne-
gation detection to augment concept indexing of 
medical documents: A quantitative study using the 
umls. Journal of the American Medical Informatics 
Association, 8(6):598-609. 
J. Lafferty, A. McCallum, and F. Pereira: Conditional 
random fields: Probabilistic models for segmenting 
and labeling sequence data, In Proceedings of the In-
ternational Conference on Machine Learning 
(ICML2001), pp.282-289, 2001. 
R. Prasad, N. Dinesh, A. Lee, A. Joshi and B. Webber: 
Annotating Attribution in the Penn Discourse Tree-
Bank, In Proceedings of the International Conference 
on Computational Linguistics and the Annual Con-
ference of the Association for Computational Lin-
guistics (COLING/ACL2006) Workshop on 
Sentiment and Subjectivity in Text, pp.31-38 (2006). 
R. Saur?, and J. Pustejovsky: Determining Modality and 
Factuality for Text Entailment, Proceedings of 
ICSC2007, pp. 509-516 (2007). 
Gaizauskas, A. Setzer, G. Katz, and D.R. Radev. 2003. 
New Directions in Question Answering: Timeml: 
Robust specification of event and temporal expres-
sions in text. AAAI Press. 
SNOMED-CT. 2002. SNOMED Clinical Terms Guide. 
College of American Pathologists.  
Sibanda Tawanda, Tian He, Peter Szolovits, and Uzuner 
Ozlem. 2006. Syntactically informed semantic cate-
gory recognizer for discharge summaries. In Proceed-
ings of the Fall Symposium of the American Medical 
Informatics Association (AMIA 2006), pages 11-15. 
Sibanda Tawanda and Uzuner Ozlem. 2006. Role of 
local context in automatic deidentification of un- 
grammatical, fragmented text. In Proceedings of the 
Human Language Technology conference and the 
North American chapter of the Association for Com-
putational Linguistics (HLT-NAACL2006), pages 
65-73. 
Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The bioscope 
corpus: biomedical texts annotated for uncertainty, 
negation and their scopes. BMC Bioinformatics, 
9(11). 
 
192
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 75?83,
Beijing, August 2010
Adverse?Effect Relations Extraction from  
Massive Clinical Records 
Yasuhide Miura a, Eiji Aramaki b, Tomoko Ohkuma a, Masatsugu Tonoike a,  
Daigo Sugihara a, Hiroshi Masuichi a and Kazuhiko Ohe c 
     a  Fuji Xerox Co., Ltd.
       b Center for Knowledge Structuring, University of Tokyo
         c University of Tokyo Hospital
yasuhide.miura@fujixerox.co.jp, eiji.aramaki@gmail.com, 
{ohkuma.tomoko,masatsugu.tonoike,daigo.sugihara, 
hiroshi.masuichi}@fujixerox.co.jp,  
kohe@hcc.h.u-tokyo.ac.jp 
 
Abstract 
The rapid spread of electronic health 
records raised an interest to large-scale 
information extraction from clinical 
texts. Considering such a background, 
we are developing a method that can 
extract adverse drug event and effect 
(adverse?effect) relations from massive 
clinical records. Adverse?effect rela-
tions share some features with relations 
proposed in previous relation extrac-
tion studies, but they also have unique 
characteristics. Adverse?effect rela-
tions are usually uncertain. Not even 
medical experts can usually determine 
whether a symptom that arises after a 
medication represents an adverse?
effect relation or not. We propose a 
method to extract adverse?effect rela-
tions using a machine-learning tech-
nique with dependency features. We 
performed experiments to extract ad-
verse?effect relations from 2,577 clini-
cal texts, and obtained F1-score of 
37.54 with an optimal parameters and 
F1-score of 34.90 with automatically 
tuned parameters. The results also 
show that dependency features increase 
the extraction F1-score by 3.59.  
1 Introduction  
The widespread use of electronic health rec-
ords (EHR) made clinical texts to be stored as 
computer processable data. EHRs contain im-
portant information about patients? health. 
However, extracting clinical information from 
EHRs is not easy because they are likely to be 
written in a natural language. 
We are working on a task to extract adverse 
drug event and effect relations from clinical 
records. Usually, the association between a 
drug and its adverse?effect relation is investi-
gated using numerous human resources, cost-
ing much time and money. The motivation of 
our task comes from this situation. An example 
of the task is presented in Figure 1. We defined 
an adverse?effect relation as a relation that 
holds between a drug entity and a symptom 
entity. The sentence illustrates the occurrence 
of the adverse?effect hepatic disorder by the 
Singulair medication.  
 
Figure 1. Example of an adverse?effect relation. 
A hepatic disorder found was suspected drug-induced and the Singulair was stopped.
adverse?effect relation
symptom drug
75
A salient characteristic of adverse?effect re-
lations is that they are usually uncertain. The 
sentence in the example states that the hepatic 
disorder is suspected drug-induced, which 
means the hepatic disorder is likely to present 
an adverse?effect relation. Figure 2 presents an 
example in which an adverse?effect relation is 
suspected, but words to indicate the suspicion 
are not stated. The two effects of the drug??the 
recovery of HbA1c and the appearance of the 
edema??are expressed merely as observation 
results in this sentence. The recovery of 
HbA1c is an expected effect of the drug and 
the appearance of the edema probably repre-
sents an adverse?effect case. The uncertain 
nature of adverse?effect relations often engen-
ders the statement of an adverse?effect rela-
tion as an observed fact. A sentence includ-
ing an adverse?effect relation occasionally be-
comes long to list all observations that ap-
peared after administration of a medication. 
Whether an interpretation that expresses an 
adverse?effect relation, such as drug-induced 
or suspected to be an adverse?effect, exists in a 
clinical record or not depends on a person who 
writes it. However, an adverse?effect relation 
is associated with an undesired effect of a 
medication. Its appearance would engender an 
extra action (e.g. stopped in the first example) 
or lead to an extra indication (e.g. but ? ap-
peared in the second example). Proper han-
dling of this extra information is likely to boost 
the extraction accuracy. 
The challenge of this study is to capture re-
lations with various certainties. To establish 
this goal, we used a dependency structure for 
the adverse?effect relation extraction method. 
Adverse?effect statements are assumed to 
share a dependency structure to a certain 
degree. For example, if we obtain the depend-
ency structures as shown in Figure 3, then we 
can easily determine that the structures are 
similar. Of course, obtaining such perfect pars-
ing results is not always possible. A statistical 
syntactic parser is known to perform badly if a 
text to be parsed belongs to a domain which 
differs from a domain on which the parser is 
trained (Gildea, 2001). A statistical parser will 
likely output incomplete results in these texts 
and will likely have a negative effect on rela-
tion extraction methods which depend on it. 
The specified research topic of this study is to 
investigate whether incomplete dependency 
structures are effective and how they behave in 
the extraction of uncertain relations.  
Figure 2. The example of an adverse-effect relation where the suspicion is not stated. 
Figure 3. The example of a similarity within dependency structures. 
ACTOS 30 recovered HbA1c to 6.5% but an edema appeared after the medication.
A suspected drug-induced hepatic disorder found and the Singulair was stopped.
conjunct
nominal subject nominal subject
nominal subject nominal subject
conjunct
was
ACTOS 30 recovered HbA1c to 6.5% but an edema appeared after the medication.
adverse-effect relation
drug symptom
76
2 Related Works 
Various studies have been done to extract se-
mantic information from texts. SemEval-2007 
Task:04 (Girju et al, 2007) is a task to extract 
semantic relations between nominals. The task 
includes ?Cause?Effect? relation extraction, 
which shares some similarity with a task that 
will be presented herein. Saeger et al (2008) 
presented a method to extract potential trou-
bles or obstacles related to the use of a given 
object. This relation can be interpreted as a 
more general relation of the adverse?effect 
relation. The protein?protein interaction (PPI) 
annotation extraction task of BioCreative II 
(Krallinger et al, 2008) is a task to extract PPI 
from PubMed abstracts. BioNLP?09 Shared 
Task on Event Extraction (Kim et al, 2009) is 
a task to extract bio-molecular events (bio-
events) from the GENIA event corpus.  
Similar characteristics to those of the ad-
verse?effect relation are described in previous 
reports in the bio-medical domain. Friedman et 
al. (1994) describes the certainty in findings of 
clinical radiology. Certainty is also known in 
scientific papers of biomedical domains as 
speculation (Light et al, 2004). Vincze et al 
(2008) are producing a freely available corpus 
including annotations of uncertainty along with 
its scope. 
Dependency structure feature which we uti-
lized to extract adverse?effect relations are 
widely used in relation extraction tasks. We 
present previous works which used syntac-
tic/dependency information as a feature of a 
statistical method. Beamer et al (2007), Giuli-
ano et al (2007), and Hendrickx et al (2007) 
all used syntactic information with machine 
learning techniques in SemEval-2007 Task:04 
and achieved good performance. Riedel et al 
(2009) used dependency path features with a 
statistical relational learning method in Bi-
oNLP?09 Shared Task on Event Extraction and 
achieved the best performance in the event en-
richment subtask. Miyao et al (2008) com-
pared syntactic information of various statisti-
cal parsers on PPI. 
3 Corpus  
We produced an annotated corpus of adverse?
effect relations to develop and test an adverse?
effect relation extraction method. This section 
presents a description of details of the corpus. 
3.1 Texts Comprising the Corpus 
We used a discharge summary among various 
documents in a hospital as the source data of 
the task. The discharge summary is a docu-
ment created by a doctor or another medical 
expert at the conclusion of a hospital stay. 
Medications performed during a stay are writ-
ten in discharge summaries. If adverse?effect 
relations were observed during the stay, they 
are likely to be expressed in free text. Texts 
written in discharge summaries tend to be writ-
ten more roughly than texts in newspaper arti-
cles or scientific papers. For example, the 
amounts of medications are often written in a 
name-value list as shown below: 
?When admitted to the hospital, Artist 6 mg1x, 
Diovan 70 mg1x, Norvasac 5 mg1x and BP 
was 145/83, but after dialysis, BP showed a 
decreasing tendency and in 5/14 Norvasac was 
reduced to 2.5 mg1x.? 
3.2 Why Adverse?Effect Relation Extrac-
tion from Discharge Summaries is 
Important 
In many countries, adverse?effects are investi-
gated through multiple phases of clinical trials, 
but unexpected adverse?effects occur in actual 
medications. One reason why this occurs is 
that drugs are often used in combination with 
others in actual medications. Clinical trials 
usually target single drug use. For that reason, 
the combinatory uses of drugs occasionally 
engender unknown effects. This situation natu-
rally motivates automatic adverse?effect rela-
tion extraction from actual patient records.  
  
77
 3.3 Corpus Size 
We collected 3,012 discharge summaries1 writ-
ten in Japanese from all departments of a hos-
pital. To reduce a cost to survey the occurrence 
of adverse?effects in the summaries, we first 
split the summaries into two sets: SET-A, 
which contains keywords related to adverse?
effects and SET-B, which do not contain the 
keywords. The keywords we used were ?stop, 
change, adverse effect?, and they were chosen 
based on a heuristic. The keyword filtering 
resulted to SET-A with 435 summaries and 
SET-B with 2,577 summaries. Regarding SET-
A, we randomly sampled 275 summaries and 
four annotators annotated adverse?effect in-
formation to these summaries to create the ad-
verse?effect relation corpus. For SET-B, the 
four annotators checked the small portion of 
the summaries. Cases of ambiguity were re-
solved through discussion, and even suspicious 
adverse?effect relations were annotated in the 
corpus as positive data. The overview of the 
summary selection is presented in Figure 4.  
                                                 
1 All private information was removed from them. 
The definition of private information was referred 
from the HIPAA guidelines. 
3.4 Quantities of Adverse?Effects in Clin-
ical Texts 
55.6% (=158/275) of the summaries in SET-A 
contained adverse?effects. 11.3% (=6/53) of 
the summaries in SET-B contained adverse?
effects. Since the ratio of SET-A:SET-B is 
14.4:85.6, we estimated that about 17.7%  
(=0.556?0.144+0.113?0.856) of the summar-
ies contain adverse?effects. Even considering 
that a summary may only include suspected 
adverse?effects, we think that discharge sum-
maries are a valuable resource to explore ad-
verse?effects. 
3.5 Annotated Information 
We annotated information of two kinds to the 
corpus: term information and relation infor-
mation. 
(1) Term Annotation  
Term annotation includes two tags: a tag to 
express a drug and a tag to express a drug ef-
fect. Table 1 presents the definition. In the 
corpus, 2,739 drugs and 12,391 effects were 
annotated. 
(2) Relation Annotation  
Adverse?effect relations are annotated as the 
?relation? attribute of the term tags. We repre-
sent the effect of a drug as a relation between a 
drug tag and a symptom tag. Table 2 presents 
Table 2. Annotation examples. 
Figure 4. The overview of the summary 
selection. 
Table 1. Markup scheme. 
The expression of a disease or 
symptom: e.g. endometrial cancer, 
headache. This tag covers not only a 
noun phrase but also a verb phrase 
such as ?<symptom>feels a pain in 
front of the head</symptom>?.
symptom
The expression of an administrated 
drug: e.g. Levofloxacin, Flexeril. 
drug
Definition and Examplestag
<drug relation=?1?>ACTOS(30)</drug> brought 
both <symptom relation=?1?>headache<symptom> 
and <symptom relation=?1?>insomnia</symptom>.
<drug relation=?1?>Ridora</drug> resumed 
because it is associated with an <symptom 
relation=?1?>eczematous rash</symptom>.
* If a drug has two or more adverse-effects, 
symptoms take a same relation ID.
3,012 
discharge 
summaries
435
summaries
w/ keywords
2,577
summaries
w/o keywords
275
summaries
53
summaries
153
summaries
w/ adverse?
effects
122
summaries
w/o adverse?
effects
6
summaries
w/ adverse?
effects
47
summaries
w/o adverse?
effects
YES NO
Contain keywords?
Random samplingRandom sampling
Contain adverse?
effects?
Contain adverse?
effects?
YES YESNO NO
SET-A (annotated corpus) SET-B
78
several examples, wherein ?relation=1? de-
notes the ID of a adverse?effect relation. In the 
corpus, 236 relations were annotated.  
4 Extraction Method 
We present a simple adverse?effect relation 
extraction method. We extract drug?symptom 
pairs from the corpus and discriminate them 
using a machine-learning technique. Features 
based on morphological analysis and depend-
ency analysis are used in discrimination. This 
approach is similar to the PPI extraction ap-
proach of Miyao et al (2008), in which we 
binary classify pairs whether they are in ad-
verse?effect relations or not. A pattern-based 
semi-supervised approach like Saeger et al 
(2008), or more generally Espresso (Pantel and 
Pennacchiotti, 2006), can also be taken, but we 
chose a pair classification approach to avoid 
the effect of seed patterns. To capture a view 
of an adverseness of a drug, a statistic of ad-
verse?effect relations is important. We do not 
want to favor certain patterns and chose a pair 
classification approach to equally treat every 
relation. Extraction steps of our method are as 
presented below. 
STEP 1: Pair Extraction   
All combinations of drug?symptom pairs that 
appear in a same sentence are extracted. Pairs 
<drug relation=?1?>Lasix</drug> for 
<symptom>hyperpiesia</symptom> has 
been suspended due to the appearance of 
a <symptom relation=?1?>headache
</symptom>.
headacheLasixpositive
hyperpiesiaLasixnegative
symptomdruglabel
ID Feature Definition and Examples
1 Character Distance The number of characters between members of a pair.
2 Morpheme Distance The number of morpheme between members of a pair.
3 Pair Order Order in which a drug and a symptom appear in a text; 
?drug?symptom? or ?symptom?drug?.
4 Symptom Type The type of symptom: ?disease name?, ?medical test name?, 
or ?medical test value?. 
5 Morpheme Chain Base?forms of morphemes that appear between a pair.
6 Dependency Chain Base?forms of morphemes included in the minimal 
dependency path of a pair.
7 Case Frame Chain Verb, case frame, and object triples that appear between a 
pair: e.g. ?examine? ??de?(case particle) ? ?inhalation?, 
?begin? ??wo?(case particle) ??medication?.
8 Case Frame 
Dependency Chain
Verb, case frame, and object triples included in the minimal 
dependency path of a pair.
Figure 6. Dependency chain example. 
 
Figure 5. Pair extraction example. 
hyperpiesia no-PP
for no-PP
Lasix wo-PP
headache no-PP 
appear niyori-PP
suspend ta-AUX
Lasix, wo-PP, headache, no-PP, 
appear, niyori-PP, suspend, ta-AUX
minimal path
Table 3. Features used in adverse-effect extraction. 
79
with the same relation ID become positive 
samples; pairs with different relation IDs be-
come negative samples. Figure 5 shows exam-
ples of positive and negative samples.  
STEP 2: Feature Extraction  
Features presented in Table 3 are extracted. 
The text in the corpus is in Japanese. Some 
features assume widely known characteristics 
of Japanese. For example, the dependency fea-
ture allows a phrase to depend on only one 
phrase that appears after a dependent phrase. 
Figure 6 portrays an example of a dependency 
chain feature. In the example, most terms were 
translated into English, excluding postpositions 
(PP) and auxiliaries (AUX), which are ex-
pressed in italic. To reduce the negative effect 
of feature sparsity, features which appeared in 
more than three summaries are used for fea-
tures with respective IDs 5?8. 
STEP 3: Machine Learning  
The support vector machine (SVM) (Vapnik, 
1995) is trained using positive/negative labels 
and features extracted in prior steps. In testing,                                          
an unlabeled pair is given a positive or nega-
tive label with the trained SVM.  
5 Experiment 
We performed two experiments to evaluate the 
extraction method. 
5.1 Experiment 1 
Experiment 1 aimed to observe the effects of 
the presented features. Five combinations of 
the features were evaluated with a five-fold 
cross validation assuming that an optimal pa-
rameter combination was obtained. The exper-
iment conditions are described below: 
A. Data  
7,690 drug?symptom pairs were extracted 
from the corpus.  Manually annotated infor-
mation was used to identify drugs and symp-
toms. Within 7,690 pairs, 149 pairs failed to 
extract the dependency chain feature. We re-
moved these 149 pairs and used the remaining 
7,541 pairs in the experiment. The 7,541 pairs 
consisted of 367 positive samples and 7,174 
negative samples.  
B. Feature Combinations  
We tested the five combinations of features in 
the experiment. Manually annotated infor-
mation was used for the symptom type feature. 
Features related to morphemes are obtained by 
processing sentences with a Japanese mor-
phology analyzer (JUMAN2 ver. 6.0). Features 
related to dependency and case are obtained by 
processing sentences using a Japanese depend-
ency parser (KNP ver. 3.0; Kurohashi and Na-
gao, 1994).  
C. Evaluations  
We evaluated the extraction method with all 
combinations of SVM parameters in certain 
                                                 
2 http://www-lab25.kuee.kyoto-u.ac.jp/nl-
resource/juman-e.html 
E
D
C
B
A
ID
35.45
35.01
34.39
33.30
26.72
Precision
41.05
40.67
43.06
42.43
46.21
Recall
log(c)=1.0, log(g)=-5.0, p=0.10
log(c)=1.0, log(g)=-5.0, p=0.10
log(c)=1.0, log(g)=-5.0, p=0.10
log(c)=1.0, log(g)=-5.0, p=0.10
log(c)=3.0, log(g)=-5.0, p=0.10
Parameters
37.181,2,3,4,5,6,7,8
36.781,2,3,4,5,6,8
37.541,2,3,4,5,6,7
36.641,2,3,4,5,6
33.051,2,3,4,5
F1-scoreFeature 
Combination
Table 4. Best F1-scores and their parameters. 
Figure 7. Precision?recall distribution. 
80
ranges. We used LIBSVM3 ver. 2.89 as an im-
plementation of SVM. The radial basis func-
tion (RBF) was used as the kernel function of 
SVM. The probability estimates option of 
LIBSVM was used to obtain the confidence 
value of discrimination.  
The gamma parameter of the RBF kernel 
was chosen from the range of [2-20, 20]. The C 
parameter of SVM was chosen from the range 
of [2-10, 210]. The SVM was trained and tested 
on 441 combinations of gamma and C. In test-
ing, the probability threshold parameter p be-
tween [0.05, 0.95] was also chosen, and the F1-
scores of all combination of gamma, C, and p 
were calculated with five-fold cross validation. 
The best F1-scores and their parameter values 
for each combination of features (optimal F1-
scores in this setting) are portrayed in Table 4. 
The precision?recall distribution of F1-scores 
with feature combination C is presented in 
Figure 7.  
5.2 Experiment 2 
Experiment 2 aimed to observe the perfor-
mance of our extraction method when SVM 
parameters were automatically tuned. In this 
experiment, we performed two cross valida-
tions: a cross validation to tune SVM parame-
ters and another cross validation to evaluate 
the extraction method. The experiment condi-
tions are described below:  
A. Data 
The same data as Experiment 1 were used. 
B. Feature Combination  
Feature combination C, which performed best 
in Experiment 1, was used.  
C. Evaluation  
                                                 
3 http://www.csie.ntu.edu.tw/~cjlin/libsvm 
Two five-fold cross validations were per-
formed. The first cross validation divided the 
data to 5 sets (A, B, C, D, and E) each consist-
ing of development set and test set with the 
ratio of 4:1.  The second cross validation train 
and test all combination of SVM parameters (C, 
gamma, and p) in certain ranges and decide the 
optimal parameter combination(s) for  the de-
velopment sets of A, B, C, D, and E. The se-
cond cross validation denotes the execution of 
Experiment 1 for each development set.  For 
each optimal parameter combination of A, B, 
C, D, and E, the corresponding development 
set was trained and the trained model was test-
ed on the corresponding test set. The average 
F1-score on five test sets marked 34.90, which 
is 2.64 lower than the F1-score of Experiment 1 
with the same feature combination. 
6 Discussion 
The result of the experiment reveals the effec-
tiveness of the dependency chain feature and 
the case-frame chain feature. This section pre-
sents a description of the effects of several fea-
tures in detail. The section also mentions re-
maining problems in our extraction method.  
6.1 Effects of the Dependency Chain Fea-
ture and Case-frame Features  
A. Dependency Chain Feature  
The dependency chain features improved the 
F1-score by 3.59 (the F1-score difference be-
tween feature combination A and B). This in-
crease was obtained using 260 improved pairs 
and 127 deproved pairs. Improved pairs con-
Figure 8. Relation between the number of 
pairs and the morpheme distance. 
Figure 9. Number of dependency errors 
in the improved pairs sentences. 
25
93
23
sentence
with no error
sentence
with 1?3
errors
sentence
with 4 or
more errors
0
10
20
30
40
50
distance less 
than 40
distance larger than
or equal to 40
fre
qu
en
cy
improved 
deproved
81
tribute to the increase of a F1-score. Deproved 
pairs have the opposite effect. 
We observed that improved pairs tend to 
have longer morpheme distance compared to 
deproved pairs. Figure 8 shows the relation 
between the number of pairs and the mor-
pheme distance of improved pairs and de-
proved pairs. The ratio between the improved 
pairs and the deproved pairs is 11:1 when the 
distance is greater than 40.  In contrast, the 
ratio is 2:1 when the distance is smaller than 
40. This observation suggests that adverse?
effect relations share dependency structures to 
a certain degree.  
We also observed that in improved pairs, 
dependency errors tended to be low. Figure 9 
presents the manually counted number of de-
pendency errors in the 141 sentences in which 
the 260 improved pairs exist: 65.96 % of the 
sentences included 1?3 errors. The result sug-
gests that the dependency structure is effective 
even if it includes small errors.  
B. Case-frame Features  
The effect of the case-frame dependency chain 
feature differed with the effect of the depend-
ency chain feature. The case-frame chain fea-
ture improved the F1-score by 0.90 (the F1-
score difference between feature combination 
B and C), but the case-frame dependency chain 
feature decreased the F1-score by 0.36 (the F1-
score difference between feature combination 
C and E). One reason for the negative effect of 
the case-frame dependency feature might be 
feature sparsity, but no clear evidence of it has 
been found.  
6.2 Remaining Problems 
A. Imbalanced Data  
The adverse?effect relation pairs we used in 
the experiment were not balanced. Low values 
of optimal probability threshold parameter p 
suggest the degree of imbalance. We are con-
sidering introduction of some kind of method-
ology to reduce negative samples or to use a 
machine learning method that can accommo-
date imbalanced data well.  
B. Use of Medical Resources  
The extraction method we propose uses no 
medical resources. Girju et al (2007) indicate 
the effect of WordNet senses in the classifica-
tion of a semantic relation between nominals. 
Krallinger et al (2008) report that top scoring 
teams in the interaction pair subtask used so-
phisticated interactor protein normalization 
strategies. If medical terms in texts can be 
mapped to a medical terminology or ontology, 
it would likely improve the extraction accuracy.  
C. Fully Automated Extraction 
In the experiments, we used the manually 
annotated information to extract pairs and fea-
tures. This setting is, of course, not real if we 
consider a situation to extract adverse?effect 
relations from massive clinical records, but we 
chose it to focus on the relation extraction 
problem. We performed an event recognition 
experiment (Aramaki et al, 2009) and 
achieved F1-score of about 80. We assume that 
drug expressions and symptom expressions to 
be automatically recognized in a similar accu-
racy.  
We are planning to perform a fully automat-
ed adverse?effect relations extraction from a 
larger set of clinical texts to see the perfor-
mance of our method on a raw corpus. The 
extraction F1-score will likely to decrease, but 
we intend to observe the other aspect of the 
extraction, like the overall tendency of extract-
ed relations.  
7 Conclusion 
We presented a method to extract adverse?
effect relations from texts. One important 
characteristic of adverse?effect relations is that 
they are uncertain in most cases. We per-
formed experiments to extract adverse?effect 
relations from 2,577 clinical texts, and ob-
tained F1-score of 37.54 with optimal SVM 
parameters and F1-score of 34.90 with auto-
matically tuned SVM parameters. Results also 
show that dependency features increase the 
extraction F1-score by 3.59. We observed that 
an increased F1-score was obtained using the 
improvement of adverse?effects with long 
morpheme distance, which suggests that ad-
verse?effect relations share dependency struc-
tures to a certain degree. We also observed that 
the increase of the F1-score was obtained with 
dependency structures that include small errors, 
which suggests that the dependency structure 
is effective even if it includes small errors. 
  
82
References 
Aramaki, Eiji, Yasuhide Miura, Masatsugu Tonoike, 
Tomoko Ohkuma, Hiroshi Masuichi, and 
Kazuhiko Ohe. 2009. TEXT2TABLE: Medical 
Text Summarization System Based on Named 
Entity Recognition and Modality Identification. 
In Proceedings of the BioNLP 2009 Workshop, 
pages 185-192. 
Beamer, Brandon, Suma Bhat, Brant Chee, Andrew 
Fister, Alla Rozovskaya, and Roxana Girju. 
2007. UIUC: A Knowledge-rich Approach to 
Identifying Semantic Relations between Nomi-
nals. In Proceedings of Fourth International 
Workshop on Semantic Evaluations, pages 386-
389. 
Friedman, Carol, Philip O. Alderson, John H. M. 
Austin, James J. Cimino, and Stephen B. John-
son. 1994. A General Natural-language Text 
Processor for Clinical Radiology. Journal of the 
American Medical Informatics Association, 1(2), 
pages 161-174. 
Gildea, Daniel. 2001. Corpus Variation and Parser 
Performance. In Proceedings of the 2001 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 1-9. 
Girju, Roxana, Preslav Nakov, Vivi Nastase,  Stan 
Szpakowicz, Peter Turney, and Deniz Yuret.  
2007. SemEval-2007 task 04: Classification of 
Semantic Relations between Nominals. In Pro-
ceedings of Fourth International Workshop on 
Semantic Evaluations, pages 13-18. 
Giuliano, Claudio, Alberto Lavelli, Daniele Pighin, 
and Lorenza Romano. 2007. FBK-IRST: Kernel 
Methods for Semantic Relation Extraction. In 
Proceedings of the 4th International Workshop 
on Semantic Evaluations, pages 141-144.  
Hendrickx , Iris, Roser Morante, Caroline Sporleder, 
and Antal van den Bosch. 2007. ILK: Machine 
learning of semantic relations with shallow fea-
tures and almost no data. In Proceedings of the 
4th International Workshop on Semantic Evalua-
tions, 187-190. 
Kim, Jin-Dong, Tomoko Ohta, Sampo Pyysalo, 
Yoshinobu Kano, and Jun?ichi Tsujii. 2009. 
Overview of  BioNLP?09 Shared Task on Event 
Extraction. In Proceedings of the BioNLP 2009 
Workshop Companion Volume for Shared Task, 
pages 1-9. 
Krallinger, Martin, Florian Leitner, Carlos  
Rodriguez-Penagos, and Alfonso Valencia. 2008.  
Overview of the protein-protein interaction an-
notation extraction task of BioCreative II. Ge-
nome Biology 2008, 9(Suppl 2):S4. 
Kurohashi, Sadao and Makoto Nagao. 1994. KN 
Parser : Japanese Dependency/Case Structure 
Analyzer. In Proceedings of The International 
Workshop on Sharable Natural Language Re-
sources, pages 22-28. Software available at 
http://www-lab25.kuee.kyoto-u.ac.jp/nl-
resource/knp-e.html. 
Light, Marc, Xin Ying Qiu, and Padmini Srinivasan. 
2004. The Language of Bioscience: Facts, Spec-
ulations, and Statements in Between. In Pro-
ceedings of HLT/NAACL 2004 Workshop: Bi-
oLINK 2004, Linking Biological Literature, On-
tologies and Databases, pages 17-24. 
Miyao, Yusuke, Rune S?tre, Kenji Sagae, Takuya 
Matsuzaki, and Jun'ichi Tsujii. 2008. Task-
oriented Evaluation of Syntactic Parsers and 
Their Representations. In Proceedings of the 
46th Annual Meeting of the Association for 
Computational Linguistics: Human Language 
Technologies, pages 46-54. 
Pantel, Patrick and Marco Pennacchiotti. 2006. Es-
presso: Leveraging Generic Patterns for Auto-
matically Harvesting Semantic Relations. In 
Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 113-120. 
Riedel, Sebastian, Hong-Woo Chun, Toshihisa 
Takagi, and Jun'ichi Tsujii. 2009. A Markov 
Logic Approach to Bio-Molecular Event Extrac-
tion. In Proceedings of the BioNLP 2009 Work-
shop Companion Volume for Shared Task, pages 
41-49. 
Saeger, Stijn De, Kentaro Torisawa, and Jun?ichi 
Kazama. 2008. Looking for Trouble. In Proceed-
ings of the 22nd International Conference on 
Computational Linguistics, pages 185-192. 
Vapnik, Vladimir N.. 1995. The Nature of Statisti-
cal Learning Theory. Springer-Verlag New York, 
Inc.. 
Vincze, Veronika, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008.  The Bio-
Scope corpus: biomedical texts annotated for un-
certainty, negation and their scopes. BMC Bioin-
formatics 2008, 9(Suppl 11):S9.  
 
83
