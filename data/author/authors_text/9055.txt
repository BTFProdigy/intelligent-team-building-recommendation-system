Using a Mixture of N-Best Lists from Multiple MT Systems
in Rank-Sum-Based Confidence Measure for MT Outputs ?
Yasuhiro Akiba?,?, Eiichiro Sumita?, Hiromi Nakaiwa?,
Seiichi Yamamoto?, and Hiroshi G. Okuno?
? ATR Spoken Language Translation Research Laboratories
2-2-2 Hikaridai, Keihana Science City, Kyoto 619-0288, Japan
? Graduate School of Informatics, Kyoto University
Yoshida-Honmachi, Sakyo-ku, Kyoto 606-8501, Japan
{yasuhiro.akiba, eiichiro.sumita, hiromi.nakaiwa seiichi.yamamoto}@atr.jp, and okuno@i.kyoto-u.ac.jp
Abstract
This paper addressees the problem of eliminat-
ing unsatisfactory outputs from machine trans-
lation (MT) systems. The authors intend to
eliminate unsatisfactory MT outputs by using
confidence measures. Confidence measures for
MT outputs include the rank-sum-based confi-
dence measure (RSCM) for statistical machine
translation (SMT) systems. RSCM can be ap-
plied to non-SMT systems but does not always
work well on them. This paper proposes an
alternative RSCM that adopts a mixture of the
N-best lists from multiple MT systems instead
of a single-system?s N-best list in the exist-
ing RSCM. In most cases, the proposed RSCM
proved to work better than the existing RSCM
on two non-SMT systems and to work as well
as the existing RSCM on an SMT system.
1 Introduction
This paper addresses the challenging problem of
eliminating unsatisfactory outputs from machine
translation (MT) systems, which are subsystems of
a speech-to-speech machine translation (S2SMT)
system. The permissible range of translation quality
by MT/S2SMT systems depends on the user. Some
users permit only perfect translations, while other
users permit even translations with flawed grammar.
Unsatisfactory MT outputs are those whose transla-
tion quality is worse than the level the user can per-
mit.
In this paper, the authors intend to eliminate un-
satisfactory outputs by using confidence measures
for MT outputs. The confidence measures1 indicate
how perfect/satisfactory the MT outputs are. In the
? This research was supported in part by the Ministry of Public
Management, Home Affairs, Posts and Telecommunications,
Japan.
1These confidence measures are a kind of automatic evalu-
ator such as mWER (Niessen et al, 2000) and BLEU (Papineni
et al, 2001). While mWER and BLEU cannot be used online,
these confidence measures can. This is because the former are
based on reference translations, while the latter is not.
discipline of MT, confidence measures for MT out-
puts have rarely been investigated.
The few existing confidence measures include
the rank-sum-based confidence measure (RSCM)
for statistical machine translation (SMT) systems,
Crank in (Ueffing et al, 2003). The basic idea
of this confidence measure is to roughly calculate
the word posterior probability by using ranks of
MT outputs in an N-best list from an SMT system.
In the discipline of non-parametric statistical test,
ranks of numerical values are commonly used in-
stead of the numerical values themselves for statis-
tical tests. In the case of the existing RSCM, the
ranks of probabilities of MT outputs in the N-best
list were used instead of the probabilities of the out-
puts themselves. The existing RSCM scores each
word in an MT output by summing the comple-
mented ranks of candidates in the N-best list that
contain the same word in a Levenshtein-aligned po-
sition (Levenshtein, 1966). When the confidence
values of all words in the MT output are larger than
a fixed threshold, the MT output is judged as cor-
rect/perfect. Otherwise, the output is judged as in-
correct/imperfect.
The existing RSCM does not always work well
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Co
rre
ct
 re
jec
tio
n r
ate
: y
Correct acceptance rate: x
Performance of existing method (A|BCD)
J2E SAT + Existing method
J2E HPAT + Existing method
J2E D3 + Existing method
Figure 1: Performance of the existing RSCM on three
different types of Japanese-to-English (J2E) MT sys-
tems: D3, HPAT, and SAT. The existing RSCM tried to
accept perfect MT outputs (grade A in Section 4) and to
reject imperfect MT outputs (grades B, C, and D in Sec-
tion 4).
on types of MT systems other than SMT systems.
Figure 1 shows the differences among the perfor-
mances, indicated by the Receiver Operating Char-
acteristics (ROC) curve (Section 4.1), of the exist-
ing RSCM on each of three MT systems (Section
4.2.1): D3, HPAT, and SAT (Doi and Sumita, 2003;
Imamura et al, 2003; Watanabe et al, 2003). Only
SAT is an SMT system; the others are not. The ideal
ROC curve is a square (0,1), (1,1), (1,0); thus, the
closer the curve is to a square, the better the perfor-
mance of the RSCM is. The performances of the
existing RSCM on the non-SMT systems, D3 and
HPAT, are much worse than that on the SMT sys-
tem, SAT.
The performance of the existing RSCM depends
on the goodness/density of MT outputs in the N-
best list from the system. However, the system?s
N-best list does not always give a good approxi-
mation of the total summation of the probability
of all candidate translations given the source sen-
tence/utterance. The N-best list is expected to ap-
proximate the total summation as closely as possi-
ble.
This paper proposes a method that eliminates
unsatisfactory top output by using an alternative
RSCM based on a mixture of N-best lists from mul-
tiple MT systems (Figure 2). The elimination sys-
tem is intended to be used in the selector architec-
ture, as in (Akiba et al, 2002). The total transla-
tion quality of the selector architecture proved to be
better than the translation quality of each element
MT system. The final output from the selection sys-
tem is the best among the satisfactory top2 outputs
from the elimination system. In the case of Fig-
ure 2, the selection system can receive zero to three
top MT outputs. When the selection system receive
fewer than two top MT outputs, the selection sys-
tem merely passes a null output or the one top MT
output.
The proposed RSCM differs from the existing
RSCM in its N-best list. The proposed RSCM re-
2To distinguish the best output from the selection system,
the MT output in the first place in each N-best list (e.g., N-best
lista in Figure 2 ) refers to the top MT output.
The best output
Elimination System
Selection System
Satisfactory top outputs
MTa MTb MTc
The top outputa
?
The M-th best outputa
Input
M-best lista
The top outputb
?
The M-th best outputb
M-best listb
The top outputc
?
The M-th best outputc
M-best listc
Figure 2: Image of our eliminator
ceives an M-best list from each element MT sys-
tem. Next, it sorts the mixture of the MT outputs in
all M-best lists in the order of the average product
(Section 3.2) of the scores of a language model and
a translation model (Akiba et al, 2002). This sorted
mixture is used instead of the system?s N-best list in
the existing RSCM.
To experimentally evaluate the proposed RSCM,
the authors applied the proposed RSCM and the ex-
isting RSCM to a test set of the Basic Travel Ex-
pression Corpus (Takezawa et al, 2002). The pro-
posed RSCM proved to work better than the exist-
ing RSCM on the non-SMT systems and to work as
well as the existing RSCM on the SMT system.
The next section outlines the existing RSCM.
Section 3 proposes our RSCM. Experimental results
are shown and discussed in Section 4. Finally, our
conclusions are presented in Section 5.
2 The Existing RSCM
The existing confidence measures include the rank-
sum-based confidence measure (RSCM) for SMT
systems (Ueffing et al, 2003). The basic idea of
this RSCM is to roughly calculate the word poste-
rior probability by using ranks of MT outputs in the
N-best list of an SMT system. That is, the ranks of
probabilities of MT outputs in the N-best list were
used instead of the probabilities of the outputs them-
selves, as in the non-parametric statistical test.
Hereafter, e?I1 and wIn1 denote the top output2
and the n-th best output in the N-best list, respec-
tively. e?i denotes the i-th word in the top MT output
e?I1. Li(e?I1, wIn1 ) denote the Levenshtein alignment3
(Levenshtein, 1966) of e?i on the n-th best output
wIn1 according to the top output e?I1. The existing
RSCM of the word e?i is the sum of the ranks of MT
outputs in an N-best list containing the word e?i in a
position that is aligned to i in the Levenshtein align-
ment, which is normalized by the total rank sum:
Crank(e?i) =
?N
n=1(N ? n) ? ?(e?i, Li(e?I1, wIn1 ))
N(N + 1)/2 ,
where ?(?, ?) is the Kronecker function, that is, if
words/morphemes x and y are the same, ?(x, y) =
1; otherwise, ?(x, y) = 0. Thus, only in the case
where e?i and Li(e?I1, wIn1 ) are the same, the rank of
the MT output wIn1 , N ? n, is summed. In the
calculation of Crank, N ? n is summed instead of
the rank n because ranks near the top of the N-best
list contribute more to the score Crank.
3This is the word on the n-th best output wIn1 , aligned with
the i-th word e?i, in the calculation of edit distance from the top
MT output e?I1 to the n-th best output wIn1 .
In this paper, the calculation of Crank is slightly
modified to sum N ? n + 1 so that the total sum-
mation is equal to N(N + 1)/2. Moreover, when
there are MT outputs that have the same score, such
MT outputs are assigned the average rank as in the
discipline of non-parametric statistical test.
As shown in Section 1, the existing RSCM does
not always work well on types of MT systems other
than SMT systems. This is because the system?s
N-best list does not always give a good approxi-
mation of the total summation of the probability
of all candidate translations given the source sen-
tence/utterance. The N-best list is expected to ap-
proximate the total summation as closely as possi-
ble.
3 Proposed Method
In this section, the authors propose a method that
eliminates unsatisfactory top output by using an al-
ternative RSCM based on a mixture of N-best lists
from multiple MT systems. The judgment that
the top output is satisfactory is based on the same
threshold comparison as the judgment that the top
output is perfect, as mentioned in Section 1. The
elimination system and the alternative RSCM are
explained in Sections 3.1 and 3.2, respectively.
3.1 Elimination system
This section proposes a method that eliminates
unsatisfactory top outputs by using an alternative
RSCM based on a mixture of N-best lists from mul-
tiple MT systems (Figure 3). This elimination sys-
tem is intended to be used in the selector architec-
ture (Figure 2). The elimination system receives
an M-best list from each element MT system and
outputs only top2 outputs whose translation quality
is better than or as good as that which the user can
permit. In the case of Figure 3, the number of MT
systems is three; thus, the elimination system can
output zero to three top MT outputs, which depends
on the number of the eliminated top outputs.
MTa MTb MTc
The top outputa
?
The M-th best outputa
Input
Satisfactory top outputs
M-best lista
The top outputb
?
The M-th best outputb
M-best listb
The top outputc
?
The M-th best outputc
M-best listc
3M outputs sorted in the higher order
Sorter based on SMT?s scoring system
Checker based on rank sum
Elimination System
Figure 3: Proposed RSCM
The proposed elimination system judges whether
a top output is satisfactory by using a threshold
comparison, as in (Ueffing et al, 2003). When
the confidence values of all words in the top out-
put, which are calculated by using the alternative
RSCM explained in Section 3.2, are larger than a
fixed threshold, the top output is judged as satisfac-
tory. Otherwise, the top output is judged as unsatis-
factory. The threshold was optimized on a develop-
ment corpus.
3.2 The proposed RSCM
The proposed RSCM is an extension of the existing
RSCM outlined in Section 2. The proposed RSCM
differs from the existing RSCM in the adopted N-
best list (Figure 3). The proposed RSCM receives
an M-best list from each element MT system. Next
the proposed RSCM sorts the mixture of all the MT
outputs in the order of the average product of the
scores of a language model and a translation model
(Akiba et al, 2002). This sorted mixture is alter-
natively used instead of the system?s N-best list in
the existing RSCM. That is, the proposed RSCM
checks whether it accepts/rejects each top MT out-
put in the original M-best lists by using the sorted
mixture; on the other hand, the existing RSCM
checks whether it accepts/rejects the top MT out-
put in the system?s N-best list by using the system?s
N-best.
For scoring MT outputs, the proposed RSCM
uses a score based on a translation model called
IBM4 (Brown et al, 1993) (TM-score) and a score
based on a language model for the translation tar-
get language (LM-score). As Akiba et al (2002)
reported, the products of TM-scores and LM-scores
are statistical variables. Even in the case where the
translation model (TM) and the language model for
the translation target language (LM) are trained on
a sub-corpus of the same size, changing the training
corpus also changes the TM-score, the LM-score,
and their product. Each pair of TM-score and LM-
score differently order the MT outputs.
For robust scoring, the authors adopt the multi-
ple scoring technique presented in (Akiba et al,
2002). The multiple scoring technique prepares
C1 Ck
C
k-fold Cross Validation
?..
TM1LM1 TMkLMk?..
C0
TM0LM0
Parallel corpus
Figure 4: Method for training multiple pairs of Lan-
guage Models (LMs) and Translation Models (TMs)
(Akiba et al, 2002).
multiple subsets of the full parallel corpus accord-
ing to k-fold cross validation (Mitchell, 1997) and
trains both TM and LM on each subset. Each
MT output is scored in k ways. For example, the
full parallel corpus C is divided into three subsets
Vi (i = 0, 1, 2). For each i, the proposed method
trains a translation model TMi on Ci (= C ? Vi)
and a language model LMi on the target-language
part of Ci (Figure 4). MT outputs in the mixture are
sorted by using the average of the product scores
by TMi and LMi for each i. In (Akiba et al, 2002),
this multiple scoring technique was shown to select
the best translation better than a single scoring tech-
nique that uses TM and LM trained from a full cor-
pus.
4 Experimental Comparison
The authors conducted an experimental compari-
son between the proposed RSCM and the existing
RSCM in the framework of the elimination system.
The task of both RSCMs was to judge whether each
top2 MT output from an MT system is satisfactory,
that is, whether the translation quality of the top MT
output is better than or as good as that which the
user can permit.
In this experiment, the translation quality of MT
outputs was assigned one of four grades: A, B,
C, or D as follows: (A) Perfect: no problems in
either information or grammar; (B) Fair: easy-to-
understand, with either some unimportant informa-
tion missing or flawed grammar; (C) Acceptable:
broken, but understandable with effort; (D) Non-
sense: important information has been translated in-
correctly. This evaluation standard was introduced
by Sumita et al (1999) to evaluate S2SMT systems.
In advance, each top MT output was evaluated by
nine native speakers of the target language, who
were also familiar with the source language, and
then assigned the median grade of the nine grades.
To conduct a fair comparison, the number of MT
outputs in the system?s N-best list and the number
of MT outputs in the mixture are expected to be
the same. Thus, the authors used either a three-
best list from each of three MT systems or a five-
best list from each of two non-SMT MT systems
for the proposed RSCM and a ten-best list for the
existing RSCM. Naturally, this setting4 is not disad-
vantageous for the existing RSCM.
4In the future, we will conduct a large-scale experiment to
investigate how both RSCMs work while increasing the size of
the system?s N-best list and the mixture of M-best lists.
Table 1: Confusion matrix
Accept Reject Subtotal
Satisfactory Vs,a Vs,r Vs (= Vs,a + Vs,r)
Unsatisfactory Vu,a Vu,r Vu (= Vu,a + Vu,r)
4.1 Evaluation metrics
The performances of both RSCMs were evaluated
by using three different metrics: ROC Curve, H-
mean, and Accuracy. For each MT system, these
metrics were separately calculated by using a con-
fusion matrix (Table 1). For example, for J2E
D3 (Section 4.2.1), the proposed RSCM checked
each top MT output from J2E D3 by using the input
mixture of three-best lists from the three J2E MT
systems (Section 4.2.1); on the other hand, the ex-
isting RSCM checked each top MT output from J2E
D3 by using the input ten-best list from J2E D3. For
J2E D3, the results were counted up into the con-
fusion matrix of each RSCM, and the metrics were
calculated as follows:
ROC Curve plots the correct acceptance rate ver-
sus the correct rejection rate for different values of
the threshold. Correct acceptance rate (CAR) is
defined as the number of satisfactory outputs that
have been accepted, divided by the total number of
satisfactory outputs, that is, Vs,a/Vs (Table 1). Cor-
rect rejection rate (CRR) is defined as the number
of unsatisfactory outputs that have been rejected, di-
vided by the total number of unsatisfactory outputs,
that is, Vu,r/Vu (Table 1).
H-mean is defined as a harmonic mean5 of
the CAR and the CRR (Table 1), 2 ? CAR ?
CRR/(CAR + CRR).
Accuracy is defined as a weighted mean6 of the
CAR and the CRR (Table 1), (Vs ? CAR + Vu ?
CRR)/(Vs + Vu) = (Vs,a + Vu,r)/(Vs + Vu).
For each performance of H-mean and Accuracy,
10-fold cross validation was conducted. The thresh-
old was fixed such that the performance was maxi-
mized on each non-held-out subset, and the perfor-
mance was calculated on the corresponding held-out
subset. To statistically test the differences in per-
formance (H-mean or Accuracy) between the confi-
dence measures, the authors conducted a pairwise t-
test (Mitchell, 1997), which was based on the results
of 10-fold cross validation. When the difference in
performance meets the following condition, the dif-
ference is statistically different at a confidence level
5This harmonic mean is used for summarizing two mea-
sures, each of which has a trade-off relationship with each
other. For example, F-measure is the harmonic mean of pre-
cision and recall, which is well used in the discipline of Infor-
mation Retrieval.
6This weighted mean is used for evaluating classification
tasks in the discipline of Machine Learning.
00.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Co
rre
ct
 re
jec
tio
n r
ate
: y
Correct acceptance rate: x
J2E-D3 (A|BCD)
0.80.7
y=x
Existing method
Proposed method (D3+HPAT+SAT)
Proposed method (D3+HPAT)
Existing method + reordering
Contours by H-mean
Figure 5: ROC Curves of both
RSCMs for J2E-D3
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Co
rre
ct
 re
jec
tio
n r
ate
: y
Correct acceptance rate: x
J2E-HPAT (A|BCD)
0.80.7
y=x
Existing method
Proposed method (D3+HPAT+SAT)
Proposed method (D3+HPAT)
Existing method + reordering
Contours by H-mean
Figure 6: ROC Curves of both
RSCMs for J2E-HPAT
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Co
rre
ct
 re
jec
tio
n r
ate
: y
Correct acceptance rate: x
J2E-SAT (A|BCD)
0.80.7
y=x
Existing method
Proposed method (D3+HPAT+SAT)
Contours by H-mean
Figure 7: ROC Curves of both
RSCMs for J2E-SAT
Table 2: Performance of MT systems: Each number
in the AB row indicates the ratio of A-or-B-graded
translation by each MT system. Each number in the
other rows similarly indicates corresponding ratios.
J2E MT systems E2J MT systems
D3 HPAT SAT D3 HPAT SAT
A 63.7 42.5 67.2 58.4 59.6 69.8
AB 72.1 63.7 74.7 72.9 75.4 81.1
ABC 78.8 79.0 82.5 83.3 86.8 88.0
of 1-?%.
|ppro ? pext| > t(?,10?1) ? S/
?
10,
where ppro and pext, respectively, denote the aver-
age performance of the proposed RSCM and the ex-
isting RSCM, t(?,10?1) denotes the upper ? point of
the Student?s t-distribution with (10 ? 1) degrees of
freedom, and S denotes the estimated standard de-
viation of the average difference in performance.
4.2 Experimental conditions
4.2.1 MT systems
Three English-to-Japanese (E2J) MT systems and
three Japanese-to-English (J2E) MT systems of the
three types described below were used. Table 2
shows the performances of these MT systems.
D3 (DP-match Driven transDucer) is an
example-based MT system using online-
generated translation patterns (Doi and Sumita,
2003).
HPAT (Hierarchical Phrase Alignment based
Translation) is a pattern-based system using au-
tomatically generated syntactic transfer (Imamura
et al, 2003).
SAT (Statistical ATR Translator) is an SMT
system using a retrieved seed translation as the
start point for decoding/translation (Watanabe et al,
2003).
4.2.2 Test set
The test set used consists of five hundred and ten
pairs of English and Japanese sentences, which
Table 3: Corpora for training TMs and LMs: Basic
Travel Expression Corpus Nos. 1-3 (Takezawa et al,
2002), Travel Reservation Corpus (Takezawa, 1999), and
MT-Aided Dialogue Corpus No. 1 (Kikui et al, 2003)
.
Japanese English
# of sentences 449,357
# of words 3,471,996 2,978,517
Vocabulary size 43,812 28,217
Ave. sent. length 7.7 6.6
were randomly selected from the Basic Travel Ex-
pression Corpus (BTEC) (Takezawa et al, 2002).
BTEC contains a variety of expressions used in a
number of situations related to overseas travel.
4.2.3 Training TMs and LMs
The corpora used for training TMs and LMs de-
scribed in Section 3.2 were merged corpora (Table
3). The number of trained TMs/LMs was three.
The translation models and language models were
learned by using GIZA++ (Och and Ney, 2000) and
the CMU-Cambridge Toolkit (Clarkson and Rosen-
feld, 1997), respectively.
4.3 Experimental results and discussion
4.3.1 ROC Curve
In order to plot the ROC Curve, the authors con-
ducted the same experiment as shown in Figure 1.
That is, in the case where the grade of satisfactory
translations is only grade A, each of the proposed
and existing RSCMs tried to accept grade A MT
outputs and to reject grade B, C, or D MT outputs.
Figures 5 to 7 show the ROC Curves for each of the
three J2E MT systems (D3, HPAT, and SAT).
The curves with diamond marks, cross marks,
triangle marks, and circle marks show the ROC
Curves for the existing RSCM, the proposed RSCM
by using the mixture of three-best lists from D3,
HPAT and SAT, the proposed RSCM by using the
mixture of five-best lists from D3 and HPAT, and
the existing RSCM with reordering, respectively. In
the existing RSCM with reordering, the system?s
Table 4: Ten-fold cross-validated pairwise t-test of H-mean: Each set of three columns corresponds to the experimen-
tal results of each of the three MT systems: D3, HPAT, and SAT. Each floating number in the first to third column of
each MT system indicates the average performance of the proposed RSCM, the average difference of the performance
of the proposed RSCM from that of the existing RSCM, and the t-value of the left-next difference, respectively. The
bold floating numbers indicate that the left-next difference is significant at a confidence level of 95%. The floating
numbers on the three rows for each MT system, whose row heads are ?A | BCD?, ?AB | CD?, or ?ABC | D?, corre-
spond to the three types of experiments in which each RSCM tried to accept/reject the MT output assigned one of the
grades left/right of ?|?, respectively.
E2J-D3 E2J-HPAT E2J-SAT
Separating point Ave. Diff. T-val. Ave. Diff. T-val. Ave. Diff. T-val.
A | BCD 76.2 15.7 4.424 73.2 14.1 5.099 65.5 0.3 0.108
AB | CD 77.3 16.5 5.154 72.6 14.3 3.865 66.9 2.8e-5 0.002
ABC | D 74.9 11.4 5.963 74.7 16.6 4.906 73.2 5.5 2.281
J2E-D3 J2E-HPAT J2E-SAT
Separating point Ave. Diff. T-val. Ave. Diff. T-val. Ave. Diff. T-val.
A | BCD 76.8 16.1 4.928 75.5 25.8 9.218 70.2 -3.3 1.618
AB | CD 79.6 15.9 4.985 70.8 28.9 6.885 66.0 -5.9 2.545
ABC | D 77.7 14.4 4.177 71.0 22.6 4.598 72.1 1.7 0.588
Table 5: Ten-fold cross-validated pairwise t-test of Accuracy: The description of this figure is the same as that of
Table 4 except that Accuracy is used instead of H-mean.
E2J-D3 E2J-HPAT E2J-SAT
Separating point Ave. Diff. T-val. Ave. Diff. T-val. Ave. Diff. T-val.
A | BCD 77.4 10.5 4.354 71.1 15.4 5.667 76.4 1.1 1.000
AB | CD 78.2 4.9 2.953 78.2 2.5 2.176 81.1 0.0 0.000
ABC | D 85.0 1.3 1.172 84.1 -2.9 2.182 88.0 0.0 0.000
J2E-D3 J2E-HPAT J2E-SAT
Separating point Ave. Diff. T-val. Ave. Diff. T-val. Ave. Diff. T-val.
A | BCD 78.8 15.8 8.243 76.2 18.2 8.118 76.4 3.1 1.041
AB | CD 77.8 4.1 3.279 72.7 8.8 3.288 77.6 -1.5 0.537
ABC | D 83.3 2.9 1.771 77.4 -1.7 1.646 82.7 0.1 0.428
original N-best list was sorted by using the aver-
age of the product scores from the multiple scor-
ing technique described in Section 3.2, and the ex-
isting RSCM with reordering used this sorted sys-
tem?s N-best instead of the system?s original N-best.
The dotted lines indicate the contours by H-mean
from 0.7 to 0.8. The ideal ROC curve is a square
(0, 1), (1, 1), (1, 0); thus, the closer the curve is to a
square, the better the performance of the RSCM is.
In Figures 5 and 6, the curves of the proposed
RSCM by using the mixture of three-best lists from
the three MT systems are much closer to a square
than that of the existing RSCM; moreover, the
curves of the proposed RSCM by using the mixture
of five-best lists from the two MT systems are much
closer to a square than that of the existing RSCM.
Note that the superiority of the proposed RSCM to
the existing RSCM is maintained even in the case
where an M-best list from the SMT system was not
used. The curves of the existing RSCM with re-
ordering are closer to a square than those of the ex-
isting RSCM. Thus the performance of the proposed
RSCM on the non-SMT systems, D3 and HPAT, are
much better than that of the existing RSCM. The
difference between the performance of the proposed
and existing RSCMs is due to both resorting the MT
outputs and using a mixture of N-best lists.
In Figure 7, the curve of the proposed RSCM is a
little closer when CRR is larger than CAR; and the
curve of the existing RSCM is a little closer when
CAR is larger than CRR. Thus, the performance
of the proposed RSCM on the SMT system, SAT,
is a little better than that of the existing RSCM in
the case where CRR is regarded as important; sim-
ilarly, the performance of the proposed RSCM on
the SMT system is a little worse than that of the ex-
isting RSCM in the case where CAR is regarded as
important.
4.3.2 H-mean and Accuracy
Tables 4 and 5 show the experimental results of ten-
fold cross-validated pairwise t-tests of the perfor-
mance of H-mean and Accuracy, respectively.
On the non-SMT systems, Table 4 shows that at
every level of translation quality that the user would
permit, the H-mean of the proposed RSCM is sig-
nificantly better than that of the existing RSCM. On
the SMT MT system, Table 4 shows that at every
permitted level of translation quality, there is no sig-
nificant difference between the H-mean of the pro-
posed RSCM and that of the existing RSCM except
for two cases: ?ABC | D? for E2J- SAT and ?AB |
CD? for J2E- SAT.
Table 5 shows almost the same tendency as Table
4. As for difference, in the case where the transla-
tion quality that the user would permit is better than
D, there is no significant difference between the Ac-
curacy of the proposed RSCM and that of the exist-
ing RSCM except in the one case of ?ABC | D? for
E2J-HPAT.
As defined in Section 4.1, Accuracy is an eval-
uation metric whose value is sensitive/inclined to
the ratio of the number of satisfactory translations
and unsatisfactory translations. H-mean is an eval-
uation metric whose value is independent/natural to
this ratio. We need to use these different evaluation
metrics according to the situations encountered. For
general purposes, the natural evaluation metric, H-
mean, is better. In the case where the test set reflects
special situations encountered, Accuracy is useful.
Regardless of whether we encounter any special
situation, in most cases on a non-SMT system, the
proposed RSCM proved to be significantly better
than the existing RSCM. In most cases on an SMT
system, the proposed RSCM proved to be as good
in performance as the existing RSCM.
This paper reports a case study in which a mixture
of N-best lists from multiple MT systems boosted
the performance of the RSCM for MT outputs. The
authors believe the proposed RSCM will work well
only when each of the element MT systems comple-
ments the others, but the authors leave the question
of the best combination of complementary MT sys-
tems open for future study.
5 Conclusions
This paper addressed the problem of eliminating un-
satisfactory outputs from MT systems. It proposed
a method that eliminates unsatisfactory outputs by
using an alternative RSCM based on a mixture of
N-best lists from multiple MT systems. The au-
thors compared the proposed and existing RSCMs
in the framework of an elimination system. When
the number of MT outputs both in the N-best list for
the existing RSCM and in the mixture of N-best lists
for the proposed RSCM is almost the same number,
i.e. ten, in most cases, the proposed RSCM proved
to work better than the existing RSCM on two non-
SMT systems and to work as well as the existing
RSCM on an SMT system.
In the future, the authors will conduct the follow-
ing experiments: (1) investigating how the proposed
RSCM works when the size of the M-best lists is
increased, and (2) seeing how the proposed RSCM
influences the performance of the selection system.
References
Yasuhiro Akiba, Taro Watanabe, and Eiichiro Sumita. 2002.
Using language and translation models to select the best
among outputs from multiple MT systems. In Proc.
COLING-2002, pages 8?14.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19(2):263?311.
Philip Clarkson and Ronald Rosenfeld. 1997. Statistical lan-
guage modeling using the CMU-Cambridge toolkit. In
Proc. EUROSPEECH-1997, pages 2707?2710.
Takao Doi and Eiichiro Sumita. 2003. Input sentence splitting
and translating. In Proc. the HLT-NAACL 2003 Workshop
on DDMT, pages 104?110.
Kenji Imamura, Eiichiro Sumita, and Yuji Matsumoto. 2003.
Feedback cleaning of machine translation rules using auto-
matic evaluation. In Proc. ACL-2003, pages 447?454.
Genichiro Kikui, Eiichiro Sumita, Toshiyuki Takezawa, and
Seiichi Yamamoto. 2003. Creating corpora for speech-
to-speech translation. In Proc. EUROSPEECH-2003, vol-
ume 1, pages 381?384.
Vladimir I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions and reversals. Soviet Physics
Doklady, 10(8):707?710.
Tom M. Mitchell. 1997. Machine Learning. The McGraw-Hill
Companies Inc., New York, USA.
Sonja Niessen, Franz J. Och, G. Leusch, and Hermann Ney.
2000. An evaluation tool for machine translation: Fast eval-
uation for machine translation research. In Proc. LREC-
2000, pages 39?45.
Franz Josef Och and Hermann Ney. 2000. Improved statistical
alignment models. In Proc. ACL-2000, pages 440?447.
Kishore A. Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2001. Bleu: a method for automatic evaluation of ma-
chine translation. In Technical Report RC22176 (W0109-
022), IBM Research Division, Thomas J. Watson Research
Center, Yorktown Heights, NY, pages 257?258.
Eiichiro Sumita, Setsuo Yamada, Kazuhiro Yamamoto,
Michael Paul, Hideki Kashioka, Kai Ishikawa, and Satoshi
Shirai. 1999. Solutions to problems inherent in spoken-
language translation: The ATR-MATRIX approach. In
Proc. MT Summit VII, pages 229?235.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya, Hiro-
fumi Yamamoto, and Seiichi Yamamoto. 2002. Toward a
broad-coverage bilingual corpus for speech translation of
travel conversations in the real world. In Proc. LREC-2002,
pages 147?152.
Toshiyuki Takezawa. 1999. Building a bilingual travel conver-
sation database for speech translation research. In Proc. the
Oriental COCOSDA Workshop-1999, pages 17?20.
Nicola Ueffing, Klaus Macherey, and Hermann Ney. 2003.
Confidence measures for statistical machine translation. In
Proc. MT Summit IX, pages 394?401.
Taro Watanabe, Eiichiro Sumita, and Hiroshi G. Okuno. 2003.
Chunk-based statistical translation. In Proc. MT Summit IX,
pages 410?417.
371
372
373
374
375
376
377
378
Quality-Sensitive Test Set Selection for a Speech Translation System 
Fumiaki Sugaya1, Keiji Yasuda2, Toshiyuki Takezawa and Seiichi Yamamoto 
ATR Spoken Language Translation Research Laboratories 
2-2-2 Hikari-dai Seika-cho, Soraku-gun, Kyoto, 619-0288, Japan 
{fumiaki.sugaya, keiji.yasuda, toshiyuki.takezawa,
seiichi.yamamoto}@atr.co.jp
 
 
                                                          
1 
2 
1Current affiliation: KDDI R&D Laboratories. Also at Graduate School of Science and Technology, Kobe University. 
2Also at Graduate School of Engineering, Doshisha University. 
  
Abstract 
We propose a test set selection method to 
sensitively evaluate the performance of a 
speech translation system. The proposed 
method chooses the most sensitive test 
sentences by removing insensitive 
sentences iteratively. Experiments are 
conducted on the ATR-MATRIX speech 
translation system, developed at ATR 
Interpreting Telecommunications 
Research Laboratories. The results show 
the effectiveness of the proposed method. 
According to the results, the proposed 
method can reduce the test set size to less 
than 40% of the original size while 
improving evaluation reliability. 
Introduction 
The translation paired comparison method 
precisely measures the capability of a speech 
translation system.  In this method, native speakers 
compare a system?s translation and the translations, 
made by examinees who have various TOEIC 
scores. The method requires two human costs: the 
data collection of examinees? translations and the 
comparison by native speakers.  In this paper, we 
propose a test set size reduction method that 
reduces the number of test set utterances.  The 
method chooses the most sensitive test utterances 
by removing the most insensitive utterances 
iteratively.    
In section 2, the translation paired comparison 
method is described. Section 3 explains the 
proposed method. In section 4, evaluation results 
for ATR-MATRIX are shown. Section 5 discusses 
the experimental results. In section 6, we state our 
conclusions. 
Translation paired comparison method 
The translation paired comparison method  
(Sugaya, 2000) is an effective evaluation method 
for precisely measuring the capability of a speech 
translation system. In this section, a description of 
the method is given. 
2.1 Methodology of the translation paired 
comparison method 
Figure 1 shows a diagram of the translation paired 
comparison method in the case of Japanese to 
English translation. The Japanese native-speaking 
examinees are asked to listen to Japanese text and 
provide an English translation on paper.  The 
Japanese text is spoken twice within one minute, 
with a pause in-between. To measure the English 
capability of the Japanese native speakers, the 
TOEIC score is used. The examinees are requested 
to present an official TOEIC score certificate 
showing that they have taken the test within the 
past six months. A questionnaire is given to them 
and the results show that the answer time is 
moderately difficult for the examinees. 
The test text is the SLTA1 test set, which 
consists of 330 utterances in 23 conversations from 
a bilingual travel conversation database (Morimoto, 
1994; Takezawa, 1999). The SLTA1 test set is 
                                            Association for Computational Linguistics.
                         Algorithms and Systems, Philadelphia, July 2002, pp. 109-116.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
open for both speech recognition and language 
translation. The answers written on paper are typed. 
In the proposed method, the typed translations 
made by the examinees and the outputs of the 
system are merged into evaluation sheets and are 
then compared by an evaluator who is a native 
English speaker. Each utterance information is 
shown on the evaluation sheets as the Japanese test 
text and the two translation results, i.e., translations 
by an examinee and by the system.  The two 
translations are presented in random order to 
eliminate bias by the evaluator.  The evaluator is 
asked to follow the procedure illustrated in Figure 
2. The four ranks in Figure 2 are the same as those 
used in Sumita (1999). The ranks A, B, C, and D 
indicate: (A) Perfect: no problems in both 
information and grammar; (B) Fair: easy-to-
understand with some unimportant information 
missing or flawed grammar; (C) Acceptable: 
broken but understandable with effort; (D) 
Nonsense: important information has been 
translated incorrectly. 
2.2 Evaluation result using the translation 
paired comparison method 
Figure 3 shows the result of a comparison between 
a language translation subsystem (TDMT) and the 
examinees. The input for TDMT included accurate 
transcriptions. The total number of examinees was 
thirty, with five people having scores in every 
hundred-point TOEIC range between the 300s and 
800s. In Figure 3, the horizontal axis represents the 
TOEIC score and the vertical axis the system 
winning rate (SWR) given by following equation: 
Translation 
Result by 
Human 
Evaluation 
Sheet 
Japanese Test 
Text Typing Paired Comparison 
Accurate Text 
 
 
 
 
where NTOTAL denotes the total number of 
utterances in the test set, NTDMT represents the 
number of  "TDMT won" utterances,  and NEVEN, 
indicates the number of  even (non-winner) 
utterances, i.e., no difference between the results of 
the TDMT and humans. The SWR ranges from 0 
to 1.0, signifying the degree of capability of the 
MT system relative to that of the examinee.  An 
SWR of 0.5 means that the TDMT has the same 
capability as the human examinee. 
Figure 3 shows that the SWR of TDMT is 
greater than 0.5 at TOEIC scores of around 300 
and 400, i.e., the TDMT system wins over humans 
with TOEIC scores of 300 and 400. Examinees, in 
contrast, win at scores of around 800. The 
capability balanced area is around a score of 600 to 
(1)                   
0.5
TOTAL
EVENTDMT
N
NNSWR
?+
=
Figure 1: Diagram of translation pair comparison method 
Japanese-to-English 
Language Translation 
(J-E TDMT) 
Japanese Recognition
(Japanese SPREC) 
Choose A, B, C, or D rank 
 
No 
Same rank?
 
Yes 
Consider naturalness 
 
YesNo 
Same? 
 
Select better result 
 
EVEN 
 
Figure 2: Procedure of comparison 
by native speaker 
 300 400 500 600 700 800 900
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
TO EIC score
SW
R
Figure 3: Evaluation results using translation 
paired comparison method 
Under the above condition, the standard deviation 
of the system's TOEIC score is calculated by 
 
(4)          
)(
)(1
2
2
0
2 ? ?
?
+=
XX
XC
n i
t ?
?
?
 
 
 
where n is the number of examinees, C0 is the 
system's TOEIC score, and X  is the average of 
the examinees' TOEIC scores. Equation (4) 
indicates that the minimum error is given when the 
system's TOEIC score equals the average of the 
examinees' TOEIC scores. 
By using a t-distribution, the confidence 
interval (CI) of the system's TOEIC score with 
confidence coefficient 1-?  is given by 
 700. To precisely determine the balanced point, we 
used regression analysis. The straight line in Figure 
3 is the regression line. The capability balanced 
point between the TDMT subsystem and the 
examinees is 0.5 of SWR. In Figure 3, the exact 
point is a TOEIC score of 708. We call this point 
the system's TOEIC score. Consequently, the 
translation capability of the language translation 
system equals that of the examinees at around a 
score of 700 points on the TOEIC.  
 
 
 
[ ]
(5)                                   )2;
2
( 
   , 00
??=
+?=
ntI
ICICCI
t
?
?
 
In the current study, we employ 0.01 for the 
value of ? .  
2.4 Costs for the translation paired comparison 
method 
The experimental result for ATR-MATRIX, 
which consists of a speech recognition subsystem 
and TDMT, has been also reported (Sugaya, 2000). 
This system?s TOEIC score is 548, where the 
number of speech recognition errors is a factor in 
the degradation of the score. 
The translation paired comparison method is an 
effective evaluation method because it can clearly 
express a system?s performance as a TOEIC (Test 
of English for International Communication)   
score. However, this method has excessive 
evaluation costs.    
Roughly speaking, one of these costs is the need 
to collect translations made by examinees of 
various TOEIC scores. As shown in Equations (4) 
and (5), n, the number of examinees, affects the 
confidence interval of the system?s TOEIC score. 
Therefore, a reduction in this number makes it 
difficult to obtain a reliable evaluation result. 
2.3 Error in the system?s TOEIC score 
The SWR (Yi) and TOEIC scores for the examinees 
(Xi) are assumed to satisfy the population 
regression equation:  
 (2)          ),...,2,1(    21 niXY iii =++= ??? 
The other cost is for the evaluation. Compared 
to a conventional evaluation method, such as a 
simple rank evaluation method, the translation 
paired comparison method uses a larger amount of 
labor because the evaluator must work on n 
evaluation sheets. Each sheet consists of 330 pairs 
of translation results to be evaluated. Even for an 
accomplished evaluator, it takes more than two 
weeks to finish the work, following the method 
explained in section 2.2. 
where 1? and 2? are population regression 
coefficients.  The error term ( i? ) is assumed to 
satisfy the following condition: 
 
 
0    (d)
   if     0),(),(    (c)
(3)                     ,...,2,1     ,)(    (b)
0)(    (a)
22
?
?==
==
=
i
jiji
i
i
jiECov
niV
E
?
????
??
?
 
 
 
 
3 Proposed method 
Yes 
No 
?
?
No
Yes
All candidates 
are calculated?
Set the number of iterations 
 
Remove worst utterances from 
candidates 
Is iteration 
achieved? 
Calculate iteration 
 
Update worst sentence, 
which causes maximum 
iteration 
Get next candidate 
 
As explained in the previous section, the 
translation paired comparison method has an 
excessive evaluation cost. Nevertheless, it is an 
effective evaluation method for measuring the 
capability of a speech translation system. 
Therefore, cost reduction for this evaluation 
method is an important subject for study. 
The proposed method reduces the evaluation 
cost by removing insensitive test utterances from 
the test set. In this section, we explain the 
optimization procedure of the proposed method.  
3.1 Optimization basis 
In the proposed method, the basis of test set 
optimization is the minimization of ? . As shown 
in Equations (4) and (5), this value has an 
influence on the confidence interval of the system's 
TOEIC score. Therefore, minimizing ?  brings 
about a reliable evaluation result.  
We introduce ? iteration, which is calculated in 
each iteration step. ? iteration is also calculated by 
using Equations (2) and (3). The difference 
between ? iteration and?  is the test set to be used 
for calculation. ? iteration is calculated using 
residual test utterances in each iteration step. 
However, the values of 1?  and 2?  are fixed, i.e., 
for the calculation of ? iteration, these 1?  and 2?  
are calculated using the original test set consisting 
of 330 test utterances. 
Optimization is conducted iteratively by 
picking up the test utterance that causes maximum 
?  iteration in each iteration step. The details of this 
procedure is explained in the next subsection. 
3.2 Methodology of the proposed method 
Figure 4 shows a diagram of the proposed method. 
In the first step, the number of iterations is set. 
This number is an actual number of removed test 
utterances. During the iterations, test utterances are 
removed one-by-one. To decide which test 
utterance to remove in each iteration, ? iteration is 
calculated for the condition of removing each test 
utterance. This calculation is done for all 
candidates, i.e., all constituents of residual test 
utterances.  
Figure 4: Procedure of proposed method 
 
At the end of each iteration step, the test 
utterance to be removed is decided. The removed 
test utterance is the one that maximizes ? iteration. 
We regard the utterance as maximizing? iteration if 
removing it from the test set gives minimum 
? iteration. 
70
720
740
760
TO
EI
C
 
sco
re
0 50 100 150 200 250 300
660
680
Iteration
   (upper)  C0 opt  +  Iopt 
  C0 opt
   (lower)  C0 opt   -  Iopt
20
30
40
?
t
 
o
p
t
      Random selection (Averaging of 10 trials)
      Optimzed  (Open)
      Optimzed  (Closed)
0 50 100 150 200 250 300
0
10
Iteration
 
Figure 6: Relationship between iteration 
and ? t opt 
Figure 5: Relationship between iteration  
and system?s TOEIC score 
 As shown in the figure, from iteration 1 to 
iteration 250, the value of C0 opt is stable and does 
not deviate from C0, which is 708. Furthermore, 
until around iteration 200, the value of Iopt 
decreases concurrently with the iteration. 
4 Experimental results 
In this section, we show experimental results of the 
proposed method. Here, we introduce the suffix 
?opt? to distinguish a variable calculated with the 
optimized test set from a variable calculated with 
the original test set. All of the above variables are 
calculated with the original test set. By joining the 
suffix ?opt? to these variables, we refer to variables 
calculated with the optimized test set, e.g., ?  opt 3, 
? t opt, Iopt, C0 opt, CI opt, and so on. 
This result suggests that the proposed may 
provide low-cost evaluation with high reliability. 
 
4.2 Experiment opened for examinees 
In the result shown in the previous subsection, the 
optimization and evaluation were conducted on the 
same examinees, i.e., the evaluation is closed for 
examinees. In this subsection, we look into the 
robustness of the proposed method against 
different examinees. We divided the group, 
consisting of 30 examinees, into two groups: a 
group of odd-numbered examinees and a group of 
even-numbered examinees. Individuals were sorted 
by TOEIC score from lowest to highest.  
4.1 Closed experiment 
This  subsection discusses an experimental result    
obtained for the same test set and examinees 
described in Section 2. Namely, the target test set 
for optimization consists of 330 utterances and the 
number of examinees is 30. 
Figure 5 shows the relationship between 
iteration and the system?s TOEIC score (C0 opt). In 
this figure, the horizontal axis represents the 
iteration number and the vertical axis the TOEIC 
score. The solid line represents C0 opt, which is the 
system?s TOEIC score using the optimized test in 
each iteration. The dotted line above the solid line 
represents the value of C0 opt + Iopt, and the dotted 
line below the solid line C0 opt - Iopt. 
One of the groups is used to optimize the test set. 
The other group is used for the translation paired 
comparison method. We use the term 
?optimization group? to refer to the first group and 
?evaluation group? to refer to the second group. 
Figure 6 shows the relationship between 
iteration and ? t opt. In this figure, the horizontal 
axis represents the iteration and the vertical axis 
shows? t opt. Three kinds of experimental results 
are shown in this figure. In each of three 
experiments, the translation paired comparison is 
conducted by the evaluation group. The differences 
                                                          
3 ? opt is different from? iteration. ? opt is calculated based on 
1?  opt and 2?  opt (not 1?  and 2? ) for the optimized test set.  
Figure 8: Relationship between iteration and 
t opt ?
0 50 100 150 200 250 300
0
5
10
15
20
25
30
Iteration
?
t
 
o
p
t
    Random selection (Averaging of 10 trials)
   Optimzed for TDM T
   Optimzed for ATR-M ATRIX
0 50 100 150 200 250 300
550
60
650
70
750
800
850
Iteration
C 0
 
opt
Random selection (Averaging of 10 trials) 
Optimized (Open)
Optimized (Closed)
Figure 7: Relationship between iteration and 
C0 opt 
among the three experiments are in the group to be 
used for optimization of the test set or the method 
used to reduce it. The double line represents the 
closed result using the test set, optimized on the 
evaluation group. The solid line represents the 
open result using the test set, optimized on the 
optimization group. The broken line represents the 
result using the test set, which is reduced by 
randomly removing test utterances one-by-one. 
The actual plotted broken line is averaged over 10 
random trials.  
0 50 100 150 200 250 300
460
480
500
520
540
560
580
Iteration
C 0
 
opt
   Optimized for TDM T
   Optimzed for ATR-M ATRIX
As shown in Figure 6, in the random selection 
result, t opt is on the rise. On the other hand, the 
open result is on the decline. 
?
Figure 7 shows the relationship between 
iteration and the system?s TOEIC score. In this 
figure, the horizontal axis represents the iteration 
and the vertical axis the TOEIC score. The 
denotation of each line is the same as that in Figure 
6. The error bar from the broken line represents 
? random, which is the standard deviation of the 
system?s TOEIC score over 10 random trials. 
Figure 9: Relationship between iteration and 
C0 opt 
In Figure 7, considering ? random, C0 opt of the 
open evaluation is more approximate to C0 than 
that of random selection, whereas C0 opt of the 
closed evaluation is much more approximate to C0. 
4.3 Experiment on ATR-MATRIX 
To be of actual use, the test set optimized for some 
system must be applicable for evaluation of other 
systems. In this subsection, we show the results of 
an experiment aimed at verifying this requirement 
is met. In this experiment, we apply the test set, 
which is optimized for TDMT, to evaluate ATR-
MATRIX. The experimental conditions are the 
same as in Section 4.1, except for the evaluation 
target. The results are shown in Figure 8 and 
Figure 9. 
Figure 8 shows the relationship between 
iteration and ? t opt. In this figure, the horizontal 
axis represents the iteration and the vertical axis 
shows ? t opt. The double line represents the result 
using the test set, optimized for ATR-MATRIX. 
The solid line represents the result using the test 
set, optimized for TDMT. The broken line 
represents the result using the test set, which is 
reduced by randomly removing test utterances one- 
6 Conclusions by-one. The actual plotted broken line is averaged 
over 10 random trials. 
We proposed a test set selection method for 
evaluating a speech translation system.  This 
method optimizes and drastically reduces the test 
set required by the translation paired comparison 
method. 
Figure 9 shows the relationship between 
iteration and the system?s TOEIC score. In this 
figure, the horizontal axis represents the iteration, 
and the vertical axis TOEIC score. The broken line 
and the solid line are plotted using the same 
denotation as that in Figure 8. Translation paired comparison is an effective 
method for measuring a system?s performance as a 
TOEIC score. However, this method has excessive 
evaluation costs. Therefore, cost reduction for this 
evaluation method is an important subject for study. 
In Figure 8, the solid line always lies on a lower 
position than the broken line. In Figure 9, from 
iteration 1 to around iteration 200, the broken line 
does not deviate from the actual system?s TOEIC 
score, which is 548. We applied the proposed method in an evaluation 
of ATR-MATRIX. Experimental results showed 
the effectiveness of the proposed method. This 
method reduced evaluation costs by more than  
60% and also improved the reliability of the 
evaluation result. 
Considering these results, the test set optimized 
for TDMT is shown to be applicable for evaluating 
ATR-MATRIX. 
5 Discussion 
Acknowledgement In this section, we discuss the experimental results 
shown in Section 4.  
The research reported here was supported in part 
by a contract with the Telecommunications 
Advancement Organization of Japan entitled, "A 
study of speech dialogue translation technology 
based on a large corpus." 
Looking at the broken lines in Figure 6 and 
Figure 8, test set reduction using random selection 
always causes an increase of ? t opt i.e., an increase 
in the scale of confidence interval. Therefore, this 
method causes the reliability of the evaluation 
result to deteriorate. Meanwhile, in the case of 
using the proposed method, looking at the solid 
lines on these figures, ? t opt is on the decline until 
around iteration 200. This means that we can 
achieve a more reliable evaluation result with a 
lower evaluation cost than when using the original 
test set. Here, looking at the solid lines in Figure 7 
and Figure 9, the Co opt system?s TOEIC score is 
nearly stable until iteration 200, and it does not 
deviate from Co. As mentioned before, Co for 
Figure 7 is 708 and Co for Figure 9 is 548. 
References 
Morimoto, T., Uratani, N., Takezawa, T., Furuse, 
O., Sobashima, Y., Iida, H., Nakamura, A., 
Sagisaka, Y., Higuchi, N. and Yamazaki, Y.  
1994. A speech and language database for 
speech translation research. In Proceedings of 
ICSLP `94, pages 1791-1794. 
Sugaya, F., Takezawa, T., Yokoo, A., Sagisaka, Y. 
and Yamamoto, S. 2000. Evaluation of the 
ATR-MATRIX Speech Translation System with 
a Pair Comparison Method between the System 
and Humans. In Proceedings of ICSLP 2000, 
pages 1105-1108. 
Considering these results, the proposed method 
can reduce the 330-utterance test set to a 130- 
utterance test set while reducing the scale of 
confidence interval. In other words, the proposed 
method both reduces evaluation costs by 60% and 
improves  reliability of the evaluation result. 
Sumita, E., Yamada, S., Yamamoto K., Paul, M., 
Kashioka, H., Ishikawa, K. and Shirai, S. 1999. 
Solutions to Problems Inherent in Spoken-
language Translation: The ATR-MATRIX 
Approach. In Proceedings of MT Summit `99, 
pages 229-235. 
Looking at Equations (4) and (5), the scale of 
confidence interval is also influenced by n.  When 
we allow the scale of confidence interval obtained 
from the original test set, we can use the proposed 
method?s reduction effect of ? t to compensate the 
? t 's increase by reducing n.  In this case, the 
actual achievable cost reduction will be more than 
60%.  
Takezawa, T. 1999. Building a bilingual travel 
conversation database for speech recognition 
research. In Proceedings of Oriental COCOSDA 
Workshop, pages 17-20. 
Takezawa, T., Morimoto, T., Sagisaka, Y., 
Campbell, N., Iida., H., Sugaya, F., Yokoo, A. 
and Yamamoto, S. 1998. A Japanese-to-English 
speech translation system: ATR-MATRIX. In 
Proceedings of ICSLP 1998, pages 2779-2782. 
Automatic Measuring of English Language Proficiency using MT
Evaluation Technology
Keiji Yasuda
ATR Spoken Language Translation
Research Laboratories
Department of SLR
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
keiji.yasuda@atr.jp
Fumiaki Sugaya
KDDI R&D Laboratories
2-1-15, Ohara, Kamifukuoka-city,
Saitama, 356-8502, Japan
fsugaya@kddilabs.jp
Eiichiro Sumita
ATR Spoken Language Translation
Research Laboratories
Department of NLR
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
eiichiro.sumita@atr.jp
Toshiyuki Takezawa
ATR Spoken Language Translation
Research Laboratories
Department of SLR
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
toshiyuki.takezawa@atr.jp
Genichiro Kikui
ATR Spoken Language Translation
Research Laboratories
Department of SLR
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
genichiro.kikui@atr.jp
Seiichi Yamamoto
ATR Spoken Language Translation
Research Laboratories
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
seiichi.yamamoto@atr.jp
Abstract
Assisting in foreign language learning is one of
the major areas in which natural language pro-
cessing technology can contribute. This paper
proposes a computerized method of measuring
communicative skill in English as a foreign lan-
guage. The proposed method consists of two
parts. The first part involves a test sentence
selection part to achieve precise measurement
with a small test set. The second part is the ac-
tual measurement, which has three steps. Step
one asks proficiency-known human subjects to
translate Japanese sentences into English. Step
two gauges the match between the translations
of the subjects and correct translations based
on the n-gram overlap or the edit distance be-
tween translations. Step three learns the rela-
tionship between proficiency and match. By re-
gression it finds a straight-line fitting for the
scatter plot representing the proficiency and
matches of the subjects. Then, it estimates pro-
ficiency of proficiency-unknown users by using
the line and the match. Based on this approach,
we conducted experiments on estimating the
Test of English for International Communica-
tion (TOEIC) score. We collected two sets of
data consisting of English sentences translated
from Japanese. The first set consists of 330 sen-
tences, each translated to English by 29 subjects
with varied English proficiency. The second set
consists of 510 sentences translated in a similar
manner by a separate group of 18 subjects. We
found that the estimated scores correlated with
the actual scores.
1 Introduction
For effective second language learning, it is ab-
solutely necessary to test proficiency in the sec-
ond language. This testing can help in selecting
educational materials before learning, checking
learners? understanding after learning, and so
on.
To make learning efficient, it is important to
achieve testing with a short turnaround time.
Computer-based testing is one solution for this,
and several kinds of tests have been developed,
including CASEC (CASEC, 2004) and TOEFL-
CBT (TOEFL, 2004). However, these tests are
mainly based on cloze testing or multiple-choice
questions. Consequently, they require labour
costs for expert examination designers to make
the questions and the alternative ?detractor?
answers.
In this paper, we propose a method for the au-
tomatic measurement of English language pro-
ficiency by applying automatic evaluation tech-
niques. The proposed method selects adequate
test sentences from an existing corpus. Then,
it automatically evaluates the translations of
test sentences done by users. The core tech-
nology of the proposed method, i.e., the auto-
matic evaluation of translations, was developed
in research aiming at the efficient development
of Machine Translation (MT) technology (Su et
al., 1992; Papineni et al, 2002; NIST, 2002).
In the proposed method, we apply these MT
evaluation technologies to the measurement of
human English language proficiency. The pro-
posed method focuses on measuring the commu-
nicative skill of structuring sentences, which is
indispensable for writing and speaking. It does
not measure elementary capabilities including
vocabulary or grammar. This method also pro-
poses a test sentence selection scheme to enable
efficient testing.
Section 2 describes several automatic evalua-
tion methods applied to the proposed method.
Section 3 introduces the proposed evaluation
scheme. Section 4 shows the evaluation results
obtained by the proposed method. Section 5
concludes the paper.
2 MT Evaluation Technologies
In this section, we briefly describe automatic
evaluation methods of translation. These meth-
ods were proposed to evaluate MT output, but
they are applicable to translation by humans.
All of these methods are based on the same
idea, that is, to compare the target transla-
tion for evaluation with high-quality reference
translations that are usually done by skilled
translators. Therefore, these methods require a
corpus of high-quality human reference transla-
tions. We call these translations as ?references?.
2.1 DP-based Method
The DP score between a translation output and
references can be calculated by DP matching
(Su et al, 1992; Takezawa et al, 1999). First,
we define the DP score between sentence (i.e.,
word array) Wa and sentence Wb by the follow-
ing formula.
SDP (Wa,Wb) = T ? S ? I ?DT (1)
where T is the total number of words in Wa, S is
the number of substitution words for comparing
Wa to Wb, I is the number of inserted words for
comparing Wa to Wb, and D is the number of
deleted words for comparing Wa to Wb.
Using Equation 1, (Si(j)), that is, the test
sentence unit DP-score of the translation of test
sentence j done by subject i, can be calculated
by the following formula.
SDPi(j) =
max
k=1 to Nref
{
SDP (Wref(k)(j),Wsub(i)(j)), 0
}
(2)
where Nref is the number of references,
Wref(k)(j) is the k-th reference of the test sen-
tence j, and Wsub(i)(j) is the translation of the
test sentence j done by subject i.
Finally, SDPi , which is the test set unit DP-
score of subject i, can be calculated by the fol-
lowing formula.
SDPi =
1
Nsent
Nsent?
j=1
SDPi(j) (3)
where Nsent is the number of test sentences.
2.2 N-gram-based Method
Papineni et al (2002) proposed BLEU, which is
an automatic method for evaluating MT qual-
ity using N -gram matching. The National Insti-
tute of Standards and Technology also proposed
an automatic evaluation method called NIST
(2002), which is a modified method of BLEU.
In this research we use two kinds of units to
apply BLEU and NIST. One is a test sentence
unit and the other is a test set unit. The unit of
utterance corresponds to the unit of ?segment?
in the original BLEU and NIST studies (Pap-
ineni et al, 2002; NIST, 2002).
Equation 4 is the test sentence unit BLEU
score formulation of the translation of test sen-
tence j done by subject i.
SBLEUi (j) =
exp
{ N?
n=1
wn log(pn)?max
(L?ref
Lsys ? 1, 0
)}
(4)
where
pn =?
C?{Candidates}
?
n?gram?{C} Countclip (n?gram)?
C?{Candidates}
?
n?gram?{C} Count(n?gram)
wn = N?1
and
L?ref = the number of words in the reference
translation that is closest in length to the
translation being scored
Lsys = the number of words in the transla-
tion being scored
Equation 5 is the test sentence unit NIST
score formulation of the translation of test sen-
tence j done by subject i.
SNISTi(j) =
?N
n=1
{?
all w1...wn in sys output
info(w1...wn)?
all w1...wn in sys output
(1)
}
?exp
{
? log2
[
min
(
Lsys
Lref , 1
)]}
(5)
where
info(w1 . . . wn) =
log2
( the number of occurence of w1...wn?1
the number of occurence of w1...wn
)
Lref = the average number of words in a ref-
erence translation, averaged over all refer-
ence translations
Lsys = the number of words in the transla-
tion being scored
and ? is chosen to make the brevity penalty fac-
tor=0.5 when the number of words in the sys-
tem translation is 2/3 of the average number
of words in the reference translation. For Equa-
tions 4 and 7, N indicates the maximum n-gram
length. In this research we set N to 4 for BLEU
and to 5 for NIST.
We may consider the unit of the test set cor-
responding to the unit of ?document? or ?sys-
tem? in BLEU and NIST. However, we use for-
mulations for the test set unit scores that are
different from those of the original BLEU and
NIST.
Calculate correlation 
between TOEIC score and 
sentence unit automatic score
References translated
by bilinguals
English writing by 
proficiency-known
human subjects
English sentences
by proficiency
Japanese test set
Automatic evaluation
(sentence unit evaluation)
Corpus
Select test sentences
based on correlation
Figure 1: Flow of Test Set Selection
The test set unit scores of BLEU and NIST
are calculated by Equations 6 and 7.
SBLEUi =
1
Nsent
Nsent?
j=1
SBLEUi(j) (6)
SNISTi =
1
Nsent
Nsent?
j=1
SNISTi(j) (7)
3 The Proposed Method
The proposed method described in this paper
consists of two parts. One is the test set selec-
tion part and the other is the actual measure-
ment part. The measurement part is divided
into two phases: a parameter-estimation phase
and a testing phase. Here, we use the term ?sub-
jects? to refer to the human subjects in the test
set selection part and the parameter-estimation
phase of the measurement part; we use ?users?
to refer to the humans in the testing phase of
the measurement part.
Regression analysis using
proficiency and automatic
scores
References translated
by bilinguals
English writing by 
proficiency-known 
human subjects
English sentences
by proficiency
Japanese test set
Regression 
coefficient
Automatic evaluation
(Test set unit evaluation)
English writing by a user
Automatic evaluation
Estimation of English
proficiency
English sentences
Automatic score
English
proficiency
?Testing phase?
Corpus
?Parameter-estimation phase?
Figure 2: Flow of English Proficiency Measurment
We employ the Test of English for Interna-
tional Communication (TOEIC, 2004) as an ob-
jective measure of English proficiency.
3.1 Test Sentence Selection Method
Figure 1 shows the flow of the test sentence se-
lection. We first calculate the test sentence
unit automatic score by using Equation 2, 4 or
5 for each test sentence and subject. Second,
for each test sentence, we calculate the correla-
tion between the automatic scores and subjects?
TOEIC scores. Finally, using the above results,
we choose the test sentences that give high cor-
relation.
3.2 Method of Measuring English
Proficiency
Figure 2 shows the flow of measuring English
proficiency. In the parameter-estimation phase,
for each subject, we first calculate the test set
unit automatic score by using Equation 3, 6 or
7. Next, we apply regression analysis using the
automatic scores and subjects? TOEIC scores.
In the testing phase, we calculate a user?s
TOEIC score using the automatic score of the
user and the regression line calculated in the
parameter-estimation phase.
4 Experiments
4.1 Experimental Conditions
4.1.1 Test sets
For the experiments, we employ two differ-
ent test sets. One is BTEC (Basic Travel
Expression Corpus) (Takezawa et al, 2002)
and the other is SLTA1 (Takezawa, 1999).
Both BTEC and SLTA1 are parts of bilingual
corpora that have been collected for research
on speech translation systems. However, they
have different features. A detailed analysis
of these corpora was done by Kikui et al
(2003). Here, we briefly explain these test sets.
In this study, we use the Japanese side as a
test set and the English side as a reference for
automatic evaluation.
BTEC
BTEC was designed to cover expressions for
every potential subject in travel conversation.
This test set was collected by investigating
?phrasebooks? that contain Japanese/English
sentence pairs that experts consider useful for
tourists traveling abroad. One sentence con-
tains 8 words on average. The test set for this
experiment consists of 510 sentences from the
BTEC corpus.
The total number of examinees is 18, and
the range of their TOEIC scores is between the
400s and 900s. Every hundred-point range has
3 examinees.
SLTA1
SLTA1 consists of 330 sentences in 23 conver-
sations from the ATR bilingual travel conver-
sation database (Takezawa, 1999). One sen-
tence contains 13 words on average. This corpus
was collected by simulated dialogues between
Japanese and English speakers through a pro-
fessional interpreter. The topics of the conver-
sations are mainly hotel conversations, such as
reservations, enquiries and so on.
The total number of examinees is 29, and the
range of their TOEIC score is between the 300s
and 800s. Excluding the 600s, every hundred-
point range has 5 examinees.
4.1.2 Reference
For the automatic evaluation, we collected 16
references for each test sentence. One of them
is from the English side of the test set, and the
remaining 15 were translated by 5 bilinguals (3
references by 1 bilingual).
4.2 Experimental Results
4.2.1 Experimental Results of Test Set
Selection
Figures 3 and 4 show the correlation between
the test sentence unit automatic score and the
subjects? TOEIC score. Here, the automatic
score is calculated using Equation 2, 4 or 5. Fig-
ure 3 shows the results on BTEC, and Fig. 4
shows the results on SLTA1. In these fig-
ures, the ordinate represents the correlation.
The filled circles indicate the results using the
DP-based automatic evaluation method. The
gray circles indicate the results using BLEU.
The empty circles indicate the results using
NIST. Looking at these figures, we find that
the three automatic evaluation methods show
a similar tendency. Comparing BTEC and
SLTA1, BTEC contains more cumbersome test
sentences. In BTEC, about 20% of the test sen-
tences give a correlation of less than 0. Mean-
while, in the SLTA1, this percentage is about
10%.
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0 30 60 90 120 150 180 210 240 270 300 330 360 390 420 450 480 510
Test sentence (sorted by correlation)
C
o
r
r
e
l
a
t
i
o
n
DP
BLEU
NIST
Figure 3: Correlation between test sentence unit
automatic scores and subjects? TOEIC scores
(BTEC)
Table 1 shows examples of low-correlated test
sentences. As shown in the table, BTEC con-
tains more short and frequently used expres-
sions than does SLTA1. This kind of expres-
sion is thought to be too easy for testing, so
this low-correlation phenomenon is thought to
occur. SLTA1 still contains a few sentences of
this kind (?Example 1? of SLTA1 in the ta-
ble). Additionally, there is another contributing
factor explaining the low correlation in SLTA1.
Looking at ?Example 2? of SLTA1 in the ta-
ble, this expression is not very easy to translate.
For this test sentence, several expressions can
be produced as an English translation. Thus,
automatic evaluation methods cannot evaluate
correctly due to the insufficient variety of ref-
erences. Considering these results, this method
can remove inadequate test sentences due not
only to the easiness of the test sentence but
also to the difficulty of the automatic evalua-
tion. Figures 5 and 6 show the relationship
between the number of test sentences and cor-
relation. This correlation is calculated between
the test set unit automatic scores and the sub-
jects? TOEIC scores. Here, the automatic score
is calculated using Equation 3, 6 or 7. Figure
5 shows the results on BTEC, and Fig. 6 shows
the results on SLTA1.
In these figures, the abscissa represents the
number of test sentences, i.e., Nsent in Equa-
tions 3, 6 and 7, and the ordinate represents
the correlation. Definitions of the circles are
the same as those in the previous figure. Here,
the test sentence selection is based on the cor-
relation shown in Figs. 3 and 4.
Comparing Fig. 5 to Fig. 6, in the case of
Table 1: Example of low-correlated test sentences
Japanese English
Example 1
???????
Good night.
Example 2
????????????
Can I see a menu, please?
Example 1
??????????????????
Yes, with my Mastercard please
Example 2
???????????????????????
????????????????????????
I wish I could take that but we have a limited budget so
how much will that cost?
S
L
T
A
1
B
T
E
C
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0 30 60 90 120 150 180 210 240 270 300 330
Test sentence (sorted by correlation)
C
o
r
r
e
l
a
t
i
o
n
DP
BLEU
NIST
Figure 4: Correlation between test sentence unit
automatic scores and subjects? TOEIC scores
(SLTA1)
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0 30 60 90 120 150 180 210 240 270 300 330 360 390 420 450 480 510
Number of test sentences
C
o
r
r
e
l
a
t
i
o
n
DP
BLEU
NIST
Figure 5: Correlation between test set unit
automatic scores and subjects? TOEIC scores
(BTEC)
using the full test set (510 test sentences for
BTEC, 330 test sentences for SLTA1), the cor-
relation of BTEC is lower than that of SLTA1.
As we mentioned above, the ratio of the low-
correlated test sentences in BTEC is higher than
that of SLTA1 (See Figs. 3 and 4). This issue
is thought to cause a decrease in the correlation
shown in Fig. 5. However, by applying the se-
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0 30 60 90 120 150 180 210 240 270 300 330
Number of test sentences
C
o
r
r
e
l
a
t
i
o
n
DP
BLEU
NIST
Figure 6: Correlation between test set unit
automatic scores and subjects? TOEIC scores
(SLTA1)
50
100
150
200
250
300
350
0 30 60 90 120 150 180 210 240 270 300 330 360 390 420 450 480 510
Number of test sentences
S
t
a
n
d
a
r
d
 
e
r
r
o
r
DP
BLEU
NIST
Figure 7: Standard error (BTEC)
lection based on sentence unit correlation, these
obstructive test sentences can be removed. This
permits the selection of high-correlated small-
sized test sets. In these figures, the highest cor-
relations are around 0.95.
4.2.2 Experimental Results of English
Proficiency Measurement
For the experiments on English proficiency mea-
surement, we carried out a leave-one-out cross
validation test. The leave-one-out cross valida-
50
100
150
200
250
300
350
0 30 60 90 120 150 180 210 240 270 300 330
Number of test sentences
S
t
a
n
d
a
r
d
 
e
r
r
o
r
DP
BLEU
NIST
Figure 8: Standard error (SLTA1)
tion test is conducted not only for the measure-
ment of the English proficiency but also for the
test set selection.
To evaluate the proficiency measurement by
the proposed method, we calculate the standard
error of the results of a leave-one-out cross val-
idation test. The following formula is the defi-
nition of the standard error.
?E =
???? 1
Nuser
Nuser?
i=1
(Ti ?Ai)2 (8)
where Nuser is the number of users, Ti is the
actual TOEIC score of user i, and Ai is user i?s
estimated TOEIC score by using the proposed
method.
Figures 7 and 8 show the relationship between
the number of test sentences and the standard
error.
In these figures, the abscissa represents the
number of test sentences, and the ordinate rep-
resents the standard error. Definitions of the
circles are the same as in the previous figure.
Here, the test sentence selection is based on the
correlation shown in Figs. 3 and 4.
Looking at Figs. 7 and 8, we can observe dif-
ferences between the standard errors of BTEC
and SLTA1. This is thought to be due to the
difference of the number of subjects in the ex-
periments (for the leave-one-out cross valida-
tion test, 17 subjects with BTEC and 28 sub-
jects with SLTA1). Even though these were
closed experiments, the results in Figs. 5 and
6 show an even higher correlation with BTEC
than with SLTA1 at the highest point. There-
fore, there is room for improvement by increas-
ing the number of subjects with BTEC.
In the test using 30 to 60 test sentences in
Figs. 7 and 8, the standard errors are much
smaller than in the test using the full test set
(510 test sentences for BTEC, 330 test sentences
for SLTA1). These results imply that the test
set selection works very well and that it enables
precise testing using a smaller size test set.
5 Conclusion
We proposed an automatic measurement
method for English language proficiency. The
proposed method applies automatic MT evalu-
ation to measure human English language pro-
ficiency. This method focuses on measuring the
communicative skill of structuring sentences,
which is indispensable in writing and speaking.
However, it does not measure elementary capa-
bilities such as vocabulary and grammar. The
method also involves a new test sentence selec-
tion scheme to enable efficient testing.
In the experiments, we used TOEIC as an ob-
jective measure of English language proficiency.
We then applied some currently available auto-
matic evaluation methods: BLEU, NIST and a
DP-based method. We carried out experiments
on two test sets: BTEC and SLTA1. Accord-
ing to the experimental results, the proposed
method gave a good measurement result on a
small-sized test set. The standard error of mea-
surement is around 120 points on the TOEIC
score with BTEC and less than 100 TOEIC
points score with SLTA1. In both cases, the
optimum size of the test set is 30 to 60 test sen-
tences.
The proposed method still needs human
labour to make the references. To obtain higher
portability, we will apply an automatic para-
phrase scheme (Finch et al, 2002; Shimohata
and Sumita, 2002) to make the references auto-
matically.
6 Acknowledgements
The research reported here was supported in
part by a contract with the National Institute
of Information and Communications Technol-
ogy entitled ?A study of speech dialogue trans-
lation technology based on a large corpus?.
References
CASEC. 2004. Computer Assessment
System for English Communication.
http://www.ets.org/toefl/.
A. Finch, T. Watanabe, and E. Sumita. 2002.
?Paraphrasing by Statistical Machine Trans-
lation?. In Proceedings of the 1st Forum on
Information Technology (FIT2002), volume
E-53, pages 187?188.
G. Kikui, E. Sumita, T. Takezawa, and
S. Yamamoto. 2003. ?Creating Corpora for
Speech-to-Speech Translation?. In Proceed-
ings of EUROSPEECH, pages 381?384.
NIST. 2002. Automatic Evaluation
of Machine Translation Quality Us-
ing N-gram Co-Occurence Statistics.
http://www.nist.gov/speech/tests/mt
/mt2001/resource/.
K. Papineni, S. Roukos, T. Ward, and W.-
J. Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In
Proceedings of the 40th Annual Meeting of
the Association for Computational Linguis-
tics (ACL), pages 311?318.
M. Shimohata and E. Sumita. 2002. ?Auto-
matic Paraphrasing Based on Parallel Corpus
for Normalization?. In Proceedings of Inter-
national Conference on Language Resources
and Evaluation (LREC), pages 453?457.
K.-Y. Su, M.-W. Wu, and J.-S. Chang. 1992.
A new quantitative quality measure for ma-
chine translation systems. In Proceedings of
the 14th International Conference on Com-
putational Linguistics(COLING), pages 433?
439.
T. Takezawa, F. Sugaya, A. Yokoo, and S. Ya-
mamoto. 1999. A new evaluation method for
speech translation systems and a case study
on ATR-MATRIX from Japanese to English.
In Proceeding of Machine Translation Summit
(MT Summit), pages 299?307.
T. Takezawa, E. Sumita, F. Sugaya, H. Ya-
mamoto, and S. Yamamoto. 2002. ?Toward a
Broad-Coverage Bilingual Corpus for Speech
Translation of Travel Conversations in the
Real World?. In Proceedings of International
Conference on Language Resources and Eval-
uation (LREC), pages 147?152.
T. Takezawa. 1999. Building a bilingual travel
conversation database for speech translation
research. In Proceedings of the 2nd Inter-
national Workshop on East-Asian Language
Resources and Evaluation ? Oriental CO-
COSDA Workshop ?99 ?, pages 17?20.
TOEFL. 2004. Test of English as a Foreign
Language. http://www.ets.org/toefl/.
TOEIC. 2004. Test of English
for International Communication.
http://www.ets.org/toeic/.
 
		
Example-based Rescoring of Statistical Machine Translation Output
Michael Paul??, Eiichiro Sumita?
?ATR Spoken Language Translation Labs
Keihanna Science City
619-0288 Kyoto, Japan
{ Michael.Paul , Eiichiro.Sumita , Seiichi.Yamamoto }@atr.jp
and Seiichi Yamamoto??
?Kobe University
Graduate School of Science and Technology
657-8501 Kobe, Japan
Abstract
Conventional statistical machine translation
(SMT) approaches might not be able to find
a good translation due to problems in its sta-
tistical models (due to data sparseness dur-
ing the estimation of the model parameters) as
well as search errors during the decoding pro-
cess. This paper1 presents an example-based
rescoring method that validates SMT transla-
tion candidates and judges whether the selected
decoder output is good or not. Given such
a validation filter, defective translations can
be rejected. The experiments show a dras-
tic improvement in the overall system perfor-
mance compared to translation selection meth-
ods based on statistical scores only.
1 Introduction
The statistical machine translation framework (SMT) for-
mulates the problem of translating a sentence from a
source language S into a target language T as the maxi-
mization problem of the conditional probability:
TM?LM = argmaxT p(S|T ) ? p(T ), (1)
where p(S|T ) is called a translation model (TM ), rep-
resenting the generation probability from T into S, p(T )
is called a language model (LM ) and represents the like-
lihood of the target language (Brown et al, 1993). The
TM and LM probabilities are trained automatically from
a parallel text corpus (parameter estimation). They rep-
resent the general translation knowledge used to map a
sequence of words from the source language into the tar-
get language. During the translation process (decoding) a
statistical score based on the probabilities of the transla-
tion and the language models is assigned to each transla-
tion candidate and the one with the highest TM?LM score
is selected as the translation output.
However, the system might not be able to find a good
translation due to parameter estimation problems of the
statistical models (due to data sparseness during the es-
timation of the model probabilities) and search errors
1The research reported here was supported in part by a con-
tract with the Telecommunications Advancement Organization
of Japan entitled, ?A study of speech dialogue translation tech-
nology based on a large corpus?.
during the translation process. Moreover, conventional
SMT approaches use words as the translation unit. There-
fore, the optimization is carried out locally generating the
translation word-by-word.
In the framework of example-based machine transla-
tion (EBMT), however, a parallel text corpus is used di-
rectly to obtain the translation (Nagao, 1984). Given an
input sentence, translation examples from the corpus that
are best matched to the input are retrieved and adjusted
to obtain the translation. Thus the translation unit used
in EBMT approaches is a complete sentence, providing a
larger context for the generation of an appropriate transla-
tion. However, this approach requires appropriate trans-
lation examples to achieve an accurate translation.
A combination of statistical and example-based MT
approaches shows some promising perspectives for over-
coming the shortcomes of each approach. In this paper,
we propose an example-based rescoring method (EBRS)
for selecting translation candidates generated by a statis-
tical decoder, as illustrated in Figure 1.
Translation
Exanples
TM LM
Parallel Text Corpus
Translation
CandidatesDecoderSeed"Input"
 SMT 
 EBRS 
"Output"
"Output"
(of convential
method)
(of proposed
method)
RescoreEditDistance
Figure 1: Outline
It retrieves translation examples that are similar to the
input from a parallel text corpus (cf. Section 2). The
target parts of these examples (seed) paired with the in-
put form the input of a statistical decoder (cf. Section 3).
The statistical scores of each generated translation candi-
date are rescored using information about how much the
seed sentence is modified during decoding. It measures
the distance between the word sequences of the decoder
output and its seed sentence based on the costs of edit dis-
tance operations (cf. Section 4). We combine the distance
measure with the statistical scores of the SMT engine, re-
sulting in a reliability measure to identify modeling prob-
lems in statistically optimized translation candidates and
to reject inappropriate solutions (cf. Section 5).
2 Translation Example Retrieval
Translation examples consist of pairs of pre-translated
sentences, either by humans (high quality) or automati-
cally using MT systems (reduced quality). A collection
of translation examples can be used directly to obtain a
translation of a given input sentence. The similarity of
the input to the source part of the translation examples
enables us to identify translation candidates that might be
close to the actual translation.
A common approach to measure the distance between
sequences of words is the edit distance criteria (Wagner,
1974). The distance is defined as the sum of the costs
of insertion (INS), deletion (DEL), and substitution (SUB)
operations required to map one word sequence into the
other. The edit distance can be calculated by a standard
dynamic programming technique.
ED(s1,s2) = |INS| + |DEL|+ |SUB|
An extension of the edit-distance-based retrieval
method is presented in (Watanabe and Sumita, 2003). It
incorporates the tf?idf criteria as seen in the information
retrieval framework by treating each translation example
as a document. For each word of the input, its term fre-
quency tfi,j is combined with its document frequency dfi
into a single weight wi,j , which is used to select the most
relevant ones out of N documents (= example targets).
Another possibility for obtaining translation examples
is simply to utilize available (off-the-shelf) MT systems
by pairing the input sentence with the obtained MT out-
put. However, the quality of those translation examples
might be much lower than manually created translations.
3 Statistical Decoding
(Germann et al, 2001) presents a greedy approach to
search for the translation that is most likely according to
previously learned statitistical models. An extension of
this approach that can take advantage of translation ex-
amples provided for a given input sentence is proposed in
(Watanabe and Sumita, 2003). Instead of decoding and
generating an output string word-by-word as is done in
the basic concept, this greedy approach slightly modifies
the target part of the translation examples so that the pair
becomes the actual translation.
The advantage of the example-based approach is that
the search for a good translation starts from the retrieved
translation example, not a guessed translation resulting
in fewer search errors. However, since it uses the same
greedy search algorithm as the basic method, search er-
rors cannot be avoided completely. Furthermore, the pa-
rameter estimation problem still remains.
The experiment discussed in Section 5.1 indeed shows
a large degradation in the system performance when
the greedy decoder is applied to already perfect transla-
tions, indicating that the decoder may modify translations
wrongly based on its statistical models (IBM model 4).
4 Example-based Rescoring
Therefore we have to validate the quality of translation
candidates selected by the decoder and judge whether
problems in the SMT models or search errors resulted in
an inaccurate translation or not.
Our approach extends the example-based concept of
(Watanabe and Sumita, 2003). It compares the decoder
output with the seed sentence, i.e., the target part of the
translation example that forms the input of the decoder.
Given a translation example whose source part is quite
similar to the input, we can assume that the fewer the
modifications that are necessary to alter the correspond-
ing example target to the translation candidate during de-
coding, the less likely it is that there will be a problem in
the statistical models.
The decision on translation quality is based on the edit
distance criteria, as introduced in Section 2. For each
translation candidate, we measure the edit distance be-
tween the word sequence of the decoder output and the
seed sentence. The proposed method rescores the transla-
tion candidates of the SMT decoder by combining the sta-
tistical probabilities of the translation and language mod-
els with the example-based translation quality hypothesis
and selects the translation candidate with the highest re-
vised score as the translation output.
The rescoring function rescore has to be designed in
such a way that almost unaltered translation candidates
with good translation and language model scores are pre-
ferred over those with the highest statistical scores that
required lots of modifications to the seed sentence.
For the experiments described below we defined two
different rescoring functions. First, the edit distance of
the seed sentence sd and the decoder output d is used as
a weight to decrease the statistical scores. The larger the
edit distance score, the smaller the revised score of the
respective translation candidate. The scaling factor scale
depends on the utilized corpus and can be optimized on a
development set reserved for parameter tuning.
TM?LM?EDW(d) = TM?LM(d)exp( scale ? ED(sd,d) ) (2)
The second rescoring function assigns a probability to
each decoder output that combines the exponential of the
sum of log probabilities of TM and LM and the scaled
negative ED scores of all translation candidates TC as
follows.
TM?LM?EDP(d) = (3)
exp(log TM(d)+log LM(d)?scale ? ED(sd,d))
?
(stc,tc)?T C
exp(log TM(tc)+log LM(tc)?scale ? ED(stc,tc))
5 Evaluation
The evaluation of our approach is carried out using a col-
lection of Japanese sentences and their English transla-
tions that are commonly found in phrasebooks for tourists
going abroad (Takezawa et al, 2002). The Basic Travel
Expression Corpus (BTEC) contains 157K sentence pairs
and the average lengths in words of Japanese and En-
glish sentences are 7.7 and 5.5, respectively. The corpus
was split randomly into three parts for training (155K),
parameter tuning (10K), and evaluation (10K) purposes.
The experiments described below were carried out on 510
sentences selected randomly as the test set.
For the evaluation, we used the following automatic
scoring measures and human assessment.
? Word Error Rate (WER), which penalizes the edit dis-
tance against reference translations (Su et al, 1992)
? BLEU: the geometric mean of n-gram precision for
the translation results found in reference translations
(Papineni et al, 2002)
? Translation Accuracy (ACC): subjective evaluation
ranks ranging from A to D (A: perfect, B: fair, C:
acceptable and D: nonsense), judged blindly by a
native speaker (Sumita et al, 1999)
In contrast to WER, higher BLEU and ACC scores indicate
better translations. For the automatic scoring measures
we utilized up to 16 human reference translations.
5.1 Downgrading Effects During Decoding
In order to get an idea about how much degradation is
to be expected in the translation candidates modified by
the statistical decoder, we conducted an experiment us-
ing the reference translations of the test set as the input of
the example-based decoder. These seed sentences are al-
ready accurate translations, thus simulating the ?optimal?
translation example retrieval case resulting in an upper
boundary of the statistical decoder performance.
Table 1: Downgrading Effects During Decoding
scoring automatic subjective (ACC)
scheme WER BLEU A A+B A+B+C gain
TM?LM 0.255 0.744 0.660 0.790 0.854 ?
TM?LM?EDP 0.179 0.814 0.745 0.854 0.898 0.044
TM?LM?EDW 0.010 0.984 0.903 0.968 0.982 0.128
The results summarized in Table 1 show a large
degradation (WER=25.5%, BLEU=0.744) in the refer-
ence translations when modified by the statistical decoder
(TM?LM). Only 66.0% of the decoder output are still per-
fect and 14.6% even result in unacceptable translations.
The rescoring function TM?LM?EDP enables us to recover
some of the decoder problems gaining 4.4% in accuracy
compared to the statistical decoder. The best perfor-
mance is achieved by the weight-based rescoring func-
tion TM?LM?EDW. However, around 10% of the selected
translations are not yet perfect.
5.2 Baseline Comparison
In the second experiment, we used two types of retrieval
methods (tf?idf-based, MT -based), as introduced in Sec-
tion 2, and compared the results with the baseline sys-
tem TM?LM, i.e., the example-based decoding approach
of (Watanabe and Sumita, 2003) using the tf?idf criteria
for the retrieval of translation examples and only the sta-
tistical scores for the selection of the translation.
For the MT-based retrieval method we used eight ma-
chine translation systems for Japanese-to-English. Three
of them were in-house EBMT systems which differ in the
translation unit (sentence-based vs. phrase-based). They
were trained on the same corpus as the statistical decoder.
The remaining five systems were (off-the-shelf) general-
purpose translation engines with quite different levels of
performance (cf. Table 2).
Table 2: MT System Performance
MT1 MT2 MT3 MT4 MT5 MT6 MT7 MT8
WER 0.320 0.408 0.419 0.580 0.584 0.588 0.600 0.646
BLEU 0.604 0.489 0.424 0.222 0.252 0.237 0.205 0.200
The results of our experiments are summarized in
Table 3. The baseline system TM?LM seems to work
best when used in combination with the tf?idf-based re-
trieval method, achieving around 80% translation accu-
racy. Moderate improvements of around 2% can be seen
when the proposed rescoring functions are used together
with the seed sentences obtained for the baseline system.
However, the largest gain in performance is achieved
when the decoder is applied to the output of multiple ma-
chine translation systems and the translation is selected
using the weight-based rescoring function.
Table 3: Baseline Comparison
tf?idf-based automatic subjective (ACC)
retrieval WER BLEU A A+B A+B+C gain
TM?LM 0.313 0.655 0.629 0.743 0.808 ?
TM?LM?EDP 0.297 0.668 0.668 0.766 0.823 0.015
TM?LM?EDW 0.289 0.639 0.676 0.749 0.815 0.007
MT -based automatic subjective (ACC)
retrieval WER BLEU A A+B A+B+C gain
TM?LM 0.338 0.630 0.627 0.731 0.796 -0.012
TM?LM?EDP 0.292 0.673 0.719 0.811 0.854 0.046
TM?LM?EDW 0.272 0.661 0.809 0.890 0.927 0.119
Table 4 compares the evaluation results of the baseline
and the TM?LM?EDW system. 67.5% of the translations
are assigned to the same rank, out of which 29.2% of the
translations are identical. TM?LM?EDW achieves higher
grades for 27% of the sentences, whereas 5.5% of the
baseline system translations are better. In total, the trans-
lation accuracy improved by 11.9% to 92.7%. Examples
of differing translation ratings are given in Table 5.
One of the reasons for the improved performance is
Table 4: Change in Translation Accuracy
TM?LM?EDW
A B C D ?
A 0.592 0.012 0.015 0.010 0.629
TM?LM B 0.080 0.024 0.004 0.006 0.114
C 0.035 0.012 0.010 0.008 0.065
D 0.102 0.033 0.008 0.049 0.192
? 0.809 0.081 0.037 0.073
Table 5: Translation Examples
input: Zutsuu ga shimasu asupirin wa arimasu ka
TM?LM [D] aspirin do i have a headache
TM?LM?EDW [A] i have a headache do you have any aspirin
input: kore wa nani de dekiteimasu ka
TM?LM [C] what is this made
TM?LM?EDW [A] what is this made of
input: nanjikan no okure ni narimasu ka
TM?LM [B] how many hours are we behind schedule
TM?LM?EDW [A] how many hours are we delayed
input: watashi wa waruku arimasen
TM?LM [A] it ?s not my fault
TM?LM?EDW [B] I ?m not bad
input: omedetou onnanoko ga umareta sou desu ne
TM?LM [A] i hear you had a baby girl congratulations
TM?LM?EDW [C] congratulations i heard you were born a boy
or a girl
input: ima me o akete mo ii desu ka
TM?LM [A] is it all right to open my eyes now
TM?LM?EDW [D] do you mind opening the eye
that the seed sentences obtained by the tf?idf-based re-
trieval method are not translations of the input sentence.
Moreover, the translations of the MT-based retrieval
method cover a large variation of expressions due to dif-
ferent MT output styles, whereby the reduced quality of
these seed sentences seems to be successfully compen-
sated by the statistical models. In contrast, the translation
examples retrieved by the tf?idf-based method are quite
similar to each other. Thus, local optimization might re-
sult in the same decoder output.
In addition, the statistical decoder has the tendency to
select shorter translations (4.8 words/sentence for TM?LM
and 5.5 words/sentence for TM?LM?EDW, which might in-
dicate some problems in the utilized translation models as
well as the language model.
(Watanabe and Sumita, 2003) try to overcome these
problems by skipping the decoding process of seed
sentences whose tf?idf-score indicates an exact match
and output the obtained seed sentence instead. How-
ever, this shortcut method (WER=0.295, BLEU=0.641,
ACC=0.898) is out-performed by the proposed rescor-
ing method by 2.9% in translation accuracy, because our
method takes advantage of translations successfully mod-
ified by the decoder and is able to identify and reject
wrongly modified ones.
Moreover, the rescoring function is language-
independent and thus can be easily applied to other
language-pairs as well.
6 Conclusion
In this paper, we proposed an example-based method for
selecting translation candidates generated by a statistical
decoder. It utilizes translation examples that are similar
to the source sentence as the input and validates the de-
coder output against its seed sentences in order to iden-
tify defective translations. The revised scoring scheme
achieved a translation accuracy of 92.7%, an improve-
ment of 11.9% over the baseline system.
So far, we treated the statistical decoder as a black-
box. However, further investigations will have to sepa-
rate modeling errors and search errors during decoding
and compare our findings to advanced statistical model-
ing approaches (phrase-based) and other search strate-
gies. Future work will also focus on the integration of the
proposed rescoring formula in the decoding process.
References
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguis-
tics, 19(2):263?311.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-
mada. 2001. Fast decoding and optimal decoding for
machine translation. In Proc. of ACL 2001, Toulouse,
France.
M. Nagao. 1984. A Framework of a Mechanical Trans-
lation between Japanese and English by Analogy Prin-
ciple. A. Elithorn and R. Banerji (eds), Artificial and
Human Intelligence, Amsterdam, North-Holland.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of the 40th ACL, pages 311?318,
Philadelphia, USA.
K. Su, M. Wu, and J. Chang. 1992. A new quantita-
tive quality measure for machine translation systems.
In Proc. of the 14th COLING, pages 433?439, Nantes,
France.
E. Sumita, S. Yamada, K. Yamamoto, M. Paul, H. Kash-
ioka, K. Ishikawa, and S. Shirai. 1999. Solutions
to problems inherent in spoken-language translation:
The ATR-MATRIX approach. In Proc. of the Machine
Translation Summit VII, pages 229?235, Singapore.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel conversa-
tions in the real world. In Proc. of the 3rd LREC, pages
147?152, Las Palmas, Spain.
R.W. Wagner. 1974. The string-to-string correction
problem. Journal of the ACM, 21(1):169?173.
T. Watanabe and E. Sumita. 2003. Example-based de-
coding for statistical machine translation. In Proc. of
the Machine Translation Summit IX, pages 410?417,
New Orleans, USA.
Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,
pages 61?68, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
 
Measuring Non-native Speakers? Proficiency of English by Using a Test 
with Automatically-Generated Fill-in-the-Blank Questions 
Eiichiro SUMITA 
Spoken Language Communication 
Research Laboratories 
ATR 
Kyoto 619-0288 Japan 
eiichiro.sumita@atr.jp 
Fumiaki SUGAYA 
Text Information Processing Labo-
ratory 
KDDI R&D Laboratories Inc. 
Saitama 356-8502 Japan 
fsugaya@kddilabs.jp 
Seiichi Yamamoto 
Department of Information Systems 
Design 
Doshisha University 
Kyoto 610-0321 Japan 
seyamamo@mail.doshisha.ac.jp
&  
Spoken Language Communication 
Research Laboratories 
ATR 
  
Abstract  
This paper proposes the automatic generation 
of Fill-in-the-Blank Questions (FBQs) together 
with testing based on Item Response Theory 
(IRT) to measure English proficiency. First, the 
proposal generates an FBQ from a given sen-
tence in English. The position of a blank in the 
sentence is determined, and the word at that 
position is considered as the correct choice. 
The candidates for incorrect choices for the 
blank are hypothesized through a thesaurus. 
Then, each of the candidates is verified by us-
ing the Web. Finally, the blanked sentence, the 
correct choice and the incorrect choices surviv-
ing the verification are together laid out to 
form the FBQ. Second, the proficiency of non-
native speakers who took the test consisting of 
such FBQs is estimated through IRT.  
Our experimental results suggest that: 
(1) the generated questions plus IRT estimate 
the non-native speakers? English proficiency; 
(2) while on the other hand, the test can be 
completed almost perfectly by English native 
speakers; and (3) the number of questions can 
be reduced by using item information in IRT.  
The proposed method provides teach-
ers and testers with a tool that reduces time 
and expenditure for testing English profi-
ciency. 
1 Introduction 
                                                          
English has spread so widely that 1,500 million 
people, about a quarter of the world?s population, 
speak it, though at most about 400 million speak it 
as their native language (Crystal, 2003). Thus, 
English education for non-native speakers both 
now and in the near future is of great importance.  
The progress of computer technology is ad-
vancing an electronic tool for language learning 
called Computer-Assisted Language Learning 
(CALL) and for language testing called Computer-
Based Testing (CBT) or Computer-Adaptive Test-
ing (CAT). However, no computerized support for 
producing a test, a collection of questions for 
evaluating language proficiency, has emerged to 
date. * 
Fill-in-the-Blank Questions (FBQs) are widely 
used from the classroom level to far larger scales 
to measure peoples? proficiency at English as a 
second language. Examples of such tests include 
TOEFL (Test Of English as a Foreign Language, 
http://www.ets.org/toefl/) and TOEIC (Test Of 
English for International Communication, 
http://www.ets.org/toeic/).  
A test comprising FBQs has merits in that (1) it 
is easy for test-takers to input answers, (2) com-
puters can mark them, thus marking is invariable 
and objective, and (3) they are suitable for the 
modern testing theory, Item Response Theory 
(IRT).  
Because it is regarded that writing incorrect 
choices that distract only the non-proficient test-
taker is a highly skilled business (Alderson, 1996), 
FBQs have been written by human experts. Thus, 
test construction is time-consuming and expensive. 
As a result, utilizing up-to-date texts for question 
writing is not practical, nor is tuning in to individ-
ual students. 
 
* See the detailed discussion in Section 6. 
61
  To solve the problems of time and expenditure, 
this paper proposes a method for generating FBQs 
using a corpus, a thesaurus, and the Web. Experi-
ments have shown that the proficiency estimated 
through IRT with generated FBQs highly corre-
lates with non-native speakers? real proficiency. 
This system not only provides us with a quick and 
inexpensive testing method, but it also features the 
following advantages:  
(I) It provides ?anyone? individually with 
up-to-date and interesting questions for 
self-teaching. We have implemented a 
program that downloads any Web page 
such as a news site and generates ques-
tions from it.  
(II) It also enables on-demand testing at 
?anytime and anyplace.? We have im-
plemented a system that operates on a 
mobile phone. Questions are generated 
and pooled in the server, and upon a 
user?s request, questions are 
downloaded. CAT (Wainer, 2000) is 
then conducted on the phone. The sys-
tem for mobile phone is scheduled to be 
deployed in May of 2005 in Japan. 
 
The remainder of this paper is organized as fol-
lows. Section 2 introduces a method for making 
FBQ, Section 3 explains how to estimate test-
takers? proficiency, and Section 4 presents the ex-
periments that demonstrate the effectiveness of the 
proposal. Section 5 provides some discussion, and 
Section 6 explains the differences between our 
proposal and related work, followed by concluding 
remarks. 
2 
2.1 
Question Generation Method 
We will review an FBQ, and then explain our 
method for producing it. 
Fill-in-the-Blank Question (FBQ) 
FBQs are the one of the most popular types of 
questions in testing. Figure 1 shows a typical sam-
ple consisting of a partially blanked English sen-
tence and four choices for filling the blank. The 
tester ordinarily assumes that exactly one choice is 
correct (in this case, b)) and the other three choices 
are incorrect. The latter are often called distracters, 
because they fulfill a role to distract the less profi-
cient test-takers. 
Figure 1: A sample Fill-in-the-Blank Question 
(FBQ) 
Question 1 (FBQ)          
I only have to _______ my head above water one more 
week? 
a) reserve b) keep c) guarantee d) promise 
N.B. the correct choice is b) keep.  
2.2 
                                                          
Flow of generation 
Using question 1 above, the outline of generation 
is presented below (Figure 2). 
A seed sentence (in this case, ?I only have to 
keep my head above water one more week.?) is 
input from the designated source, e.g., a corpus or 
a Web page such as well-known news site. *  
 
 
Figure 2: Flow generating Fill-In-The-Blank Ques-
tion (FBQ) 
Seed Sentence Corpus
Testing 
knowledge
[a] Determine the blank position
[b] Generate distracter candidatesLexicon
[c] Verify the incorrectness 
[d] Form the question  
Question 
 
[a] The seed sentence is a correct English sen-
tence that is decomposed into a sentence 
with a blank (blanked sentence) and the 
correct choice for the blank. After the seed 
 
*  Selection of the seed sentence (source text) is an important 
open problem because the difficulty of the seed (text) should 
influence the difficulty of the generated question. As for text 
difficulty, several measures such as Lexile by MetaMetrics 
(http://www.Lexile.com) have been proposed. They are known 
as readability and are usually defined as a function of sentence 
length and word frequency. 
In this paper, we used corpora of business and travel con-
versations, because TOEIC itself is oriented toward business 
and daily conversation. 
62
 sentence is analyzed morphologically by a 
computer, according to the testing knowl-
edge* the blank position of the sentence is 
determined. In this paper?s experiment, the 
verb of the seed is selected, and we obtain 
the blanked sentence ?I only have to 
______ my head above water one more 
week.? and the correct choice ?keep.? 
[b] To be a good distracter, the candidates must 
maintain the grammatical characteristics of 
the correct choice, and these should be 
similar in meaning? . Using a thesaurus? , 
words similar to the correct choice are 
listed up as candidates, e.g., ?clear,? ?guar-
antee,? ?promise,? ?reserve,? and ?share? 
for the above ?keep.?  
[c] Verify (see Section 2.3 for details) the in-
correctness of the sentence restored by each 
candidate, and if it is not incorrect (in this 
case, ?clear? and ?share?), the candidate is 
given up. 
[d] If a sufficient number (in this paper, three) 
of candidates remain, form a question by 
randomizing the order of all the choices 
(?keep,? ?guarantee,? ?promise,? and ?re-
serve?); otherwise, another seed sentence is 
input and restart from step [a]. 
                                                           
2.3 Incorrectness Verification  
In FBQs, by definition, (1) the blanked sentence 
restored with the correct choice is correct, and (2) 
the blanked sentence restored with the distracter 
must be incorrect. 
In order to generate an FBQ, the incorrectness 
of the sentence restored by each distracter candi-
date must be verified and if the combination is not 
incorrect, the candidate is rejected. 
Zero-Hit Sentence 
The Web includes all manners of language data 
in vast quantities, which are for everyone easy to 
access through a networked computer. Recently, 
exploitation of the Web for various natural lan-
guage applications is rising (Grefenstette, 1999; 
Turney, 2001; Kilgarriff and Grefenstette, 2003; 
Tonoike et al, 2004).  
We also propose a Web-based approach. We 
dare to assume that if there is a sentence on the 
Web, that sentence is considered correct; other-
wise, the sentence is unlikely to be correct in that 
there is no sentence written on the Web despite the 
variety and quantity of data on it.  *  Testing knowledge tells us what part of the seed sentence 
should be blanked. For example, we selected the verb of the 
seed because it is one of the basic types of blanked words in 
popular FBQs such as in TOEIC. 
Figure 3 illustrates verification based on the re-
trieval from the Web. Here, s (x) is the blanked 
sentence, s (w) denotes the sentence restored by the 
word w, and hits (y) represents the number of 
documents retrieved from the Web for the key y. 
This can be a word of another POS (Part-Of-Speech). For 
this, we can use knowledge in the field of second-language 
education. Previous studies on errors in English usage by 
Japanese native speakers such as (Izumi and Isahara, 2004) 
unveiled patterns of errors specific to Japanese, e.g., (1) article 
selection error, which results from the fact there are no articles 
in Japanese; (2) preposition selection error, which results from 
the fact some Japanese counterparts have broader meaning; (3) 
adjective selection error, which results from mismatch of 
meaning between Japanese words and their counterpart. Such 
knowledge may generate questions harder for Japanese who 
study English. 
 
?  There are various aspects other than meaning, for example, 
spelling, pronunciation, and translation and so on. Depending 
on the aspect, lexical information sources other than a thesau-
rus should be consulted.  Figure 3: Incorrectness and Hits on the Web 
Blanked sentence:
s (x)= ?I only have to ____ my head above water 
one more week?? 
 
Hits of incorrect choice candidates: 
hits (s (?clear?)) = 176 ; correct 
hits (s (?guarantee?)) = 0 ; incorrect 
hits (s (?promise?)) = 0 ; incorrect 
hits (s (?reserve?)) = 0 ; incorrect  
hits (s (?share?)) = 3 ; correct 
?  We used an in-house English thesaurus whose hierarchy is 
based on one of the off-the-shelf thesauruses for Japanese, 
called Ruigo-Shin-Jiten (Ohno and Hamanishi, 1984). In the 
above examples, the original word ?keep? expresses two dif-
ferent concepts: (1) possession-or-disposal, which is shared by 
the words ?clear? and ?share,? and (2) promise, which is 
shared by the words ?guarantee,? ?promise,? and ?reserve.? 
Since this depends on the thesaurus used, some may sense a 
slight discomfort at these concepts. If a different thesaurus is 
used, the distracter candidates may differ. 
 
If hits (s (w)), is small, then the sentence re-
stored with the word w is unlikely, thus the word w 
should be a good distracter. If hits (s (w)), is large 
then the sentence restored with the word w is likely, 
then the word w is unlikely to be a good distracter 
and is given up.  
63
 We used the strongest condition. If hits (s (w)) 
is zero, then the sentence restored with the word w 
is unlikely, thus the word w should be a good dis-
tracter. If hits (s (w)), is not zero, then the sentence 
restored with the word w is likely, thus the word w 
is unlikely to be a good distracter and is given up.  
3 
3.1 
Estimating Proficiency 
Item Response Theory (IRT) 
Item Response Theory (IRT) is the basis of modern 
language tests such as TOEIC, and enables Com-
puterized Adaptive Testing (CAT). Here, we 
briefly introduce IRT. IRT, in which a question is 
called an item, calculates the test-takers? profi-
ciency based on the answers for items of the given 
test (Embretson, 2000).  
Retrieval NOT By Sentence  
It is often the case that retrieval by sentence does 
not work. Instead of a sentence, a sequence of 
words around a blank position, beginning with a 
content word (or sentence head) and ending with a 
content word (or sentence tail) is passed to a search 
engine automatically. For the abovementioned 
sample, the sequence of words passed to the engine 
is ?I only have to clear my head? and so on. 
The basic idea is the item response function, 
which relates the probability of test-takers answer-
ing particular items correctly to their proficiency. 
The item response functions are modeled as logis-
tic curves making an S-shape, which take the form 
(1) for item i.  
Web Search  
 
))(exp(1
1)(
ii
i ba
P ??+= ??   (1) We can use any search engine, though we have 
been using Google since February 2004. At that 
point in time, Google covered an enormous four 
billion pages. 
The test-taker parameter, ?, shows the profi-
ciency of the test-taker, with higher values indicat-
ing higher performance. The ?correct? hits may come from non-native 
speakers? websites and contain invalid language 
usage. To increase reliability, we could restrict 
Google searches to Websites with URLs based in 
English-speaking countries, although we have not 
done so yet. There is another concern: even if 
sentence fragments cannot be located on the Web, 
it does not necessarily mean they are illegitimate. 
Thus, the proposed verification based on the Web 
is not perfect; the point, however, is that with such 
limitations, the generated questions are useful for 
estimating proficiency as demonstrated in a later 
section. 
Each of the item parameters, ai and bi, controls 
the shape of the item response function. The a pa-
rameter, called discrimination, indexes how 
steeply the item response function rises. The b pa-
rameter is called difficulty. Difficult items feature 
larger b values and the item response functions are 
shifted to the right. These item parameters are usu-
ally estimated by a maximal likelihood method. 
For computations including the estimation, there 
are many commercial programs such as BILOG 
(http://www.assess.com/) available.  
3.2 Reducing test size by selection of effective 
items 
Setting aside the convenience provided by the 
off-the-shelf search engine, another search special-
ized for this application is possible, although the 
current implementation is fast enough to automate 
generation of FBQs, and the demand to accelerate 
the search is not strong. Rather, the problem of 
time needed for test construction has been reduced 
by our proposal. 
It is important to estimate the proficiency of the 
test-taker by using as few items as possible. For 
this, we have proposed a method based on item 
information. 
Expression (2) is the item information of item i 
at ?j, the proficiency of the test-taker j, which indi-
cates how much measurement discrimination an 
item provides. 
The throughput depends on the text from which 
a seed sentence comes and the network traffic 
when the Web is accessed. Empirically, one FBQ 
is obtained in 20 seconds on average and the total 
number of FBQs in a day adds up to over 4,000 on 
a single computer.  
The procedure is as follows.  
 
1. Initialize I by the set of all generated FBQs. 
 
64
 2. According to Equation (3), we select the item 
whose contribution to test information is 
maximal.  
3. We eliminate the selected item from I accord-
ing to Equation (4).  
4. If I is empty, we obtain the ordered list of ef-
fective items; otherwise, go back to step 2. 
 
))(1)(()( 2 jijiiji PPaI ??? ?=  (2) 
( )???
?
???
?= ??
?j Ii
ji
i
Ii ?maxarg?  (3) 
iII ??=  (4) 
4 
4.1 
                                                          
Experiment 
The FBQs for the experiment were generated in 
February of 2004. Seed sentences were obtained 
from ATR?s corpus (Kikui et al, 2003) of the 
business and travel domains. The vocabulary of the 
corpus comprises about 30,000 words. Sentences 
are relatively short, with the average length being 
6.47 words. For each domain 5,000 questions were 
generated automatically and each question consists 
of an English sentence with one blank and four 
choices. 
 Experiment with non-native speakers 
We used the TOEIC score as the experiment?s pro-
ficiency measure, and collected 100 Japanese sub-
jects whose TOEIC scores were scattered from 400 
to less than 900. The actual range for TOEIC 
scores is 10 to 990. Our subjects covered the 
dominant portion* of test-takers for TOEIC in Ja-
pan, excluding the highest and lowest extremes.? 
We had the subjects answer 320 randomly se-
lected questions from the 10,000 mentioned above. 
The raw marks were as follows: the average? mark 
was 235.2 (73.5%); the highest mark was 290 
(90.6%); and the lowest was 158 (49.4%)?This 
suggests that our FBQs are sensitive to test-takers? 
proficiency. In Figure 4, the y-axis represents es-
timated proficiency according to IRT (Section 3.1) 
and generated questions, while the x-axis is the 
real TOEIC score of each subject.  
As the graph illustrates, the IRT-estimated pro-
ficiency (?) and real TOEIC scores of subjects cor-
relate highly with a co-efficiency of about 80%.  
For comparison we refer to CASEC 
(http://casec.evidus.com/), an off-the-shelf test 
consisting of human-made questions and IRT. Its 
co-efficiency with real TOEIC scores is reported to 
be 86%. 
This means the proposed automatically gener-
ated questions are promising for measuring English 
proficiency, achieving a nearly competitive level 
with human-made questions but with a few reser-
vations: (1) whether the difference of 6% is large 
depends on the standpoint of possible users; (2) as 
for the number of questions to be answered, our 
proposal uses 320 questions in the experiments, 
while TOEIC uses 200 questions and CASEC uses 
only about 60 questions; (3) the proposed method 
uses FBQs only whereas CASEC and TOEIC use 
various types of questions.  
 
 
Figure 4: IRT-Estimated Proficiency (?) vs. Real 
TOEIC Score  
 ?
TOEIC Score 
9008070060050
3
2.5
2
1.5
1
0.5
0
-0.5
-1
400
 
4.2 
                                                          
Experiment with a native speaker 
To examine the quality of the generated questions, 
we asked a single subject? who is a native speaker 
of English to answer 4,000 questions (Table 1). * Over 70% of all test-takers are covered 
(http://www.toeic.or.jp/toeic/data/data02.html). The native speaker largely agreed with our gen-
eration, determining correct choices (type I). The 
?  We have covered only the range of TOEIC scores from 400 
to 900 due to expense of the experiment. In this restricted 
experiment, we do not claim that our proficiency estimation 
method covers the full range of TOEIC scores.  
 
?  Please note that the analysis is based on a single native-
speaker, thus we need further analysis by multiple subjects. ?  The standard deviation was 29.8 (9.3%). 
65
 rate was 93.50%, better than 90.6%, the highest 
mark among the non-native speakers. 
 
We present the problematic cases here.  
z Type II is caused by the seed sentence being 
incorrect for the native speaker, and a distracter is 
bad because it is correct. Or like type III, it con-
sists of ambiguous choices? 
z Type III is caused by some generated distracters 
being correct; therefore, the choices are ambiguous.  Figure 5 Correlation coefficient and Test size 
R
Test Size in Items 
350300250200150100500
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
z Type IV is caused by the seed sentence being 
incorrect and the generated distracters also being 
incorrect; therefore, the question cannot be an-
swered.  
z Type V is caused by the seed sentence being 
nonsense to the native speaker; the question, there-
fore, cannot be answered. 
 
Table 1 Responses of a Native speaker 
Type Explanation Count %
I Match 3,740 93.50
II 
Single 
Selection No match 55 1.38
III Ambiguous Choices 70 1.75
IV No Correct Choice 45 1.13
V 
No 
Selection 
Nonsense 90 2.25
 
Cases with bad seed sentences (portions of II, 
IV, and V) require cleaning of the corpus by a na-
tive speaker, and cases with bad distracters (por-
tions of II and III) require refinement of the 
proposed generation algorithm.  
Since the questions produced by this method 
can be flawed in ways which make them unan-
swerable even by native speakers (about 6.5% of 
the time) due to the above-mentioned reasons, it is 
difficult to use this method for high-stakes testing 
applications although it is useful for estimating 
proficiency as explained in the previous section.  
4.3 
5 Discussion 
5.1 
5.2 
 
This section explains the on-demand generation of 
FBQs according to individual preference, an im-
mediate extension and a limitation of our proposed 
method, and finally touches on free-format Q&A. 
Effects of Automatic FBQ Construction 
The method provides teachers and testers with a 
tool that reduces time and expenditure. Further-
more, the method can deal with any text. For ex-
ample, up-to-date and interesting materials such as 
news articles of the day can be a source of seed 
sentences (Figure 6 is a sample generated from an 
article (http://www.japantimes.co.jp/) on an earth-
quake that occurred in Japan), which enables reali-
zation of a personalized learning environment.  
 
 
Figure 6: On-demand construction ? a sample 
question from a Web news article in The Japan 
Times on ?an earthquake? 
N.B. The correct answer is c) originated. 
Question 2 (FBQ)
The second quake            10 km below the seabed some 
130 km east of Cape Shiono. 
 
a) put  b) came  c) originated d) opened 
 
We have generated questions from over 100 docu-
ments on various genres such as novels, speeches, 
academic papers and so on found in the enormous 
collection of e-Books provided by Project Guten-
berg (http://www.gutenberg.org/). 
Proficiency ? estimated with the reduced 
test and its relation to TOEIC Scores  
Figure 5 shows the relationship between reduction 
of the test size according to the method explained 
in Section 3.2 and the estimated proficiency based 
on the reduced test. The x-axis represents the size 
of the reduced test in number of items, while the y-
axis represents the correlation coefficient (R) be-
tween estimated proficiency and real TOEIC score. 
A Variation of Fill-in-the-Blank Ques-
tions for Grammar Checking 
In Section 2.2, we mentioned a constraint that a 
good distracter should maintain the grammatical 
characteristics of the correct choice originating in 
66
 the seed sentence. The question checks not the 
grammaticality but the semantic/pragmatic cor-
rectness.  
We can generate another type of FBQ by 
slightly modifying step [b] of the procedure in Sec-
tion 2.2 to retain the stem of the original word w 
and vary the surface form of the word w. This 
modified procedure generates a question that 
checks the grammatical ability of the test takers. 
Figure 7 shows a sample of this kind of question 
taken from a TOEIC-test textbook (Educational 
Testing Service, 2002). 
 
 
Figure 7: A variation on fill-in-the-blank questions 
5.3 
5.4 
6 
                                                          
Limitation of the Addressed FBQs  
The questions dealt with in this paper concern test-
ing reading ability, but these questions are not suit-
able for testing listening ability because they are 
presented visually and cannot be pronounced. To 
test listening ability, like in TOIEC, other types of 
questions should be used, and automated genera-
tion of them is yet to be developed.  
Free-Format Q&A 
Besides measuring one?s ability to receive infor-
mation in a foreign language, which has been ad-
dressed so far in this paper, it is important to 
measure a person?s ability to transmit information 
in a foreign language. For that purpose, tests for 
translating, writing, or speaking in a free format 
have been actively studied by many researchers 
(Shermis, 2003; Yasuda, 2004). 
Related Work*  
Here, we explain other studies on the generation of 
multiple-choice questions for language learning. 
There are a few previous studies on computer-
based generation such as Mitkov (2003) and Wil-
son (1997). 
 
6.1 
6.2 
7 Conclusion 
                                                          
Cloze Test 
A computer can generate questions by deleting 
words or parts of words randomly or at every N-th 
word from text. Test-takers are requested to restore 
the word that has been deleted. This is called a 
?cloze test.? The effectiveness of a ?cloze test? or 
its derivatives is a matter of controversy among 
researchers of language testing such as Brown 
(1993) and Alderson (1996). 
N.B. The correct answer is c) care.  
Question 3 (FBQ) 
                   
Because the equipment is very delicate, it must be han-
dled with ______? 
 
a) caring b) careful  c) care  d) carefully 
Tests on Facts  
Mitkov (2003) proposed a computer-aided proce-
dure for generating multiple-choice questions from 
textbooks. The differences from our proposal are 
that (1) Mitkov?s method generates questions not 
about language usage but about facts explicitly 
stated in a text?; (2) Mitkov uses techniques such 
as term extraction, parsing, transformation of trees, 
which are different from our proposal; and (3) Mit-
kov does not use IRT while we use it. 
This paper proposed the automatic construction of 
Fill-in-the-Blank Questions (FBQs). The proposed 
method generates FBQs using a corpus, a thesaurus, 
and the Web. The generated questions and Item 
Response Theory (IRT) then estimate second-
language proficiency.  
Experiments have shown that the proposed 
method is effective in that the estimated profi-
ciency highly correlates with non-native speakers? 
real proficiency as represented by TOEIC scores; 
native-speakers can achieve higher scores than 
non-native speakers. It is possible to reduce the 
size of the test by removing non-discriminative 
questions with item information in IRT. 
 
? Based on a fact stated in a textbook like, ?A prepositional 
phrase at the beginning of a sentence constitutes an introduc-
tory modifier,? Mitkov generates a question such as, ?What 
does a prepositional phrase at the beginning of a sentence 
constitute? i. a modifier that accompanies a noun; ii. an asso-
ciated modifier; iii. an introductory modifier; iv. a misplaced 
modifier.? 
* There are many works on item generation theory (ITG) such 
as Irvine and Kyllonen (2002), although we do not go any 
further into the area. We focus only on multiple-choice ques-
tions for language learning in this paper. 
67
 The method provides teachers, testers, and test 
takers with novel merits that enable low-cost test-
ing of second-language proficiency and provides 
learners with up-to-date and interesting materials 
suitable for individuals. 
Further research should be done on (1) large-
scale evaluation of the proposal, (2) application to 
different languages such as Chinese and Korean, 
and (3) generation of different types of questions. 
Acknowledgements 
The authors? heartfelt thanks go to anonymous review-
ers for providing valuable suggestions and Kadokawa-
Shoten for providing the thesaurus named Ruigo-Shin-
Jiten. The research reported here was supported in part 
by a contract with the NiCT entitled, ?A study of speech 
dialogue translation technology based on a large cor-
pus.? It was also supported in part by the Grants-in-Aid 
for Scientific Research (KAKENHI), contract with 
MEXT numbered 16300048. The study was conducted 
in part as a cooperative research project by KDDI and 
ATR.  
References 
Alderson, Charles. 1996. Do corpora have a role in 
language assessment? Using Corpora for Language 
Research, eds. Thomas, J. and Short, M., Longman: 
248?259. 
Brown, J. D. 1993. What are the characteristics of natu-
ral cloze tests? Language Testing 10: 93?116. 
Crystal, David. 2003. English as a Global Language, 
(Second Edition). Cambridge University Press: 212. 
Educational Testing Service 2002. TOEIC koushiki 
gaido & mondaishu. IIBC: 249. 
Embretson, Susan et al 2000. Item Response Theory for 
Psychologists. LEA: 371. 
Grefenstette, G. 1999. The WWW as a resource for ex-
ample-based MT tasks. ASLIB ?Translating and the 
Computer? conference. 
Irvine, H. S., and Kyllonen, P. C. (2002). Item genera-
tion for test development. LEA: 412. 
Izumi, E., and Isahara, H. (2004). Investigation into 
language learners' acquisition order based on the er-
ror analysis of the learner corpus. In Proceedings of 
Pacific-Asia Conference on Language, Information 
and Computation (PACLIC) 18 Satellite Workshop 
on E-Learning, Japan. (in printing) 
Kikui, G., Sumita, E., Takaezawa, T. and Yamamoto, S., 
?Creating Corpora for Speech-to-Speech Transla-
tion,? Special Session ?Multilingual Speech-to-
Speech Translation? of EuroSpeech, 2003. 
Kilgarriff, A. and Grefenstette, G. 2003. Special Issue 
on the WEB as Corpus. Computational Linguistics 29 
(3): 333?502. 
Mitkov, Ruslan and Ha, Le An. 2003. Computer-Aided 
Generation of Multiple-Choice Tests. HLT-NAACL 
2003 Workshop: Building Educational Applications 
Using Natural Language Processing: 17?22. 
Ohno, S. and Hamanishi, M. 1984. Ruigo-Shin-Jiten, 
Kadokawa, Tokyo (in Japanese) 
Shermis, M. D. and Burstein. J. C. 2003. Automated 
Essay Scoring. LEA: 238. 
Tonoike, M., Sato, S., and Utsuro, T. 2004. Answer 
Validation by Keyword Association. IPSJ, SIGNL, 
161: 53?60, (in Japanese). 
Turney, P.D. 2001. Mining the Web for synonyms: PMI-
IR vs. LSA on TOEFL. ECML 2001: 491?502. 
Wainer, Howard et al 2000. Conputerized Adaptive 
Testing: A Primer, (Second Edition). LEA: 335. 
Wilson, E. 1997. The Automatic Generation of CALL 
exercises from general corpora, in eds. Wichmann, 
A., Fligelstone, S., McEnery, T., Knowles, G., 
Teaching and Language Corpora, Harlow: Long-
man:116-130. 
Yasuda, K., Sugaya, F., Sumita, E., Takezawa, T., Kikui, 
G. and Yamamoto, S. 2004. Automatic Measuring of 
English Language Proficiency using MT Evaluation 
Technology, COLING 2004 eLearning for Computa-
tional Linguistics and Computational Linguistics for 
eLearning: 53-60.  
68
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 439?446,
Beijing, August 2010
Hierarchical Phrase-based Machine Translation with Word-based
Reordering Model
Katsuhiko Hayashi*, Hajime Tsukada**
Katsuhito Sudoh**, Kevin Duh**, Seiichi Yamamoto*
*Doshisha University
katsuhiko-h@is.naist.jp, seyamamo@mail.doshisha.ac.jp
**NTT Communication Science Laboratories
tsukada, sudoh, kevinduh@cslab.kecl.ntt.co.jp
Abstract
Hierarchical phrase-based machine trans-
lation can capture global reordering with
synchronous context-free grammar, but
has little ability to evaluate the correctness
of word orderings during decoding. We
propose a method to integrate word-based
reordering model into hierarchical phrase-
based machine translation to overcome
this weakness. Our approach extends the
synchronous context-free grammar rules
of hierarchical phrase-based model to in-
clude reordered source strings, allowing
efficient calculation of reordering model
scores during decoding. Our experimen-
tal results on Japanese-to-English basic
travel expression corpus showed that the
BLEU scores obtained by our proposed
system were better than those obtained by
a standard hierarchical phrase-based ma-
chine translation system.
1 Introduction
Hierarchical phrase-based machine translation
(Chiang, 2007; Watanabe et al, 2006) is one of
the promising statistical machine translation ap-
proaches (Brown et al, 1993). Its model is for-
mulated by a synchronous context-free grammar
(SCFG) which captures the syntactic information
between source and target languages. Although
the model captures global reordering by SCFG,
it does not explicitly introduce reordering model
to constrain word order. In contrast, lexicalized
reordering models (Tillman, 2004; Koehn et al,
2005; Nagata et al, 2006) are extensively used
for phrase-based translation. These lexicalized re-
ordering models cannot be directly applied to hi-
erarchical phrased-based translation since the hi-
erarchical phrase representation uses nonterminal
symbols.
To handle global reordering in phrase-based
translation, various preprocessing approaches
have been proposed, where the source sentence
is reordered to target language order beforehand
(Xia and McCord, 2004; Collins et al, 2005; Li et
al., 2007; Tromble and Eisner, 2009). However,
preprocessing approaches cannot utilize other in-
formation in the translation model and target lan-
guage model, which has been proven helpful in
decoding.
This paper proposes a method that incorpo-
rates word-based reordering model into hierarchi-
cal phrase-based translation to constrain word or-
der. In this paper, we adopt the reordering model
originally proposed by Tromble and Eisner (2009)
for the preprocessing approach in phrase-based
translation. To integrate the word-based reorder-
ing model, we added a reordered source string
into the right-hand-side of SCFG?s rules. By this
extension, our system can generate the reordered
source sentence as well as target sentence and is
able to efficiently calculate the score of the re-
ordering model. Our method utilizes the transla-
tion model and target language model as well as
the reordering model during decoding. This is an
advantage of our method over the preprocessing
approach.
The remainder of this paper is organized as
follows. Section 2 describes the concept of our
approach. Section 3 briefly reviews our pro-
posed method on hierarchical phrase-based ma-
439
Standard SCFG X ?< X1 wa jinsei no X2 da , X1 is X2 of life>
SCFG (move-to-front) X ?< X1 wa jinsei no X2 da , wa X1 da X2 no jinsei , X1 is X2 of life>
SCFG (attach) X ?< X1 wa jinsei no X2 da , X1 wa da X2 no jinsei , X1 is X2 of life>
Table 1: A Japanese-to-English example of various SCFG?s rule representations. Japanese words are
romanized. Our proposed representation of rules has reordered source string to generate reordered
source sentence S? as well as target sentence T . The ?move-to-front? means Tromble and Eisner (2009)
?s algorithm and the ?attach? means Al-Onaizan and Papineni (2006) ?s algorithm.
chine translation model. We experimentally com-
pare our proposed system to a standard hierarchi-
cal phrase-based system on Japanese-to-English
translation task in Section 4. Then we discuss on
related work in Section 5 and conclude this paper
in Section 6.
2 The Concept of Our Approach
The preprocessing approach (Xia and McCord,
2004; Collins et al, 2005; Li et al, 2007; Tromble
and Eisner, 2009) splits translation procedure into
two stages:
S ? S? ? T (1)
where S is a source sentence, S? is a reordered
source sentence with respect to the word order of
target sentence T . Preprocessing approach has the
very deterministic and hard decision in reorder-
ing. To overcome the problem, Li et al (2007)
proposed k-best appoach. However, even with a
k-best approach, it is difficult to generate good hy-
potheses S? by using only a reordering model.
In this paper, we directly integrated the reorder-
ing model into the decoder in order to use the
reordering model together with other information
in the hierarchical phrase-based translation model
and target language model. Our approach is ex-
pressed as the following equation.
S ? (S? , T ). (2)
Our proposed method generates the reordered
source sentence S? by SCFG and evaluates the
correctness of the reorderings using a word-based
reordering model of S? which will be introduced
in section 3.4.
Figure 1: A derivation tree for Japanse-to-English
translation.
3 Hierarchical Phrase-based Model
Extension
3.1 Hierarchical Phrase-based Model
Hierarchical phrase-based model (Chiang, 2007)
induces rules of the form
X ?< ?, ?,?, w > (3)
where X is a non-terminal symbol, ? is a se-
quence string of non-terminals and source termi-
nals, ? is a sequence string of non-terminals and
target terminals. ? is a one-to-one correspon-
dence for the non-terminals appeared in ? and ?.
Given a source sentence S, the translation task
under this model can be expressed as
T? = T
(
argmax
D:S(D)=S
w(D)
)
(4)
where D is a derivation and w(D) is a score of
the derivation. Decoder seeks a target sentence
440
Figure 2: Reordered source sentence generated by
our proposed system.
T (D) which has the highest score w(D). S(D)
is a source sentence under a derivation D. Fig-
ure 1 shows the example of Japanese-to-English
translation by hierarchical phrase-based machine
translation model.
3.2 Rule Extension
To generate reordered source sentence S? as well
as target sentence T , we extend hierarchical
phrase rule expressed in Equation 3 to
X ?< ?, ?? , ?,?, w > (5)
where ?? is a sequence string of non-terminals and
source terminals, which is reordered ? with re-
spect to the word order of target string ?. The
reason why we add ?? to rules is to efficiently cal-
culate the reordering model scores. If each rule
does not have ?? , the decoder need to keep word
alignments because we cannot know word order
of S? without them. The calculation of reorder-
ing model scores using word alignments is very
wasteful when decoding.
The translation task under our model extends
Equation 4 to the following equation:
T? = (S?? , T? ) = (S? , T )
(
argmax
D:S(D)=S
w(D)
)
. (6)
Our system generates the reordered source sen-
tence S? as well as target sentence T . Figure 2
shows the generated reordered source sentence S?
Uni-gram Features
sr, s-posr
sr
s-posr
sl, s-posl
sl
s-posl
Bi-gram Features
sr, s-posr, sl, s-posl
s-posr, sl, s-posl
sr, sl, s-posl
sr, s-posr, s-posl
sr, s-posr, sl
sr, sl
s-posr, s-posl
Table 2: Features used by Word-based Reordering
Model. pos means part-of-speech tag.
when translating the example of Figure 1. Note
that the structure of S? is the same as that of target
sentence T . The decoder generates both Figure 2
and the right hand side of Figure 1, allowing us to
score both global and local word reorderings.
To add ?? to rules, we permuted ? into ?? after
rule extraction based on Grow-diag-final (Koehn
et al, 2005) alignment by GIZA++ (Och and Ney,
2003). To do this permutation on rules, we ap-
plied two methods. One is the same algorithm
as Tromble and Eisner (2009), which reorders
aligned source terminals and nonterminals in the
same order as that of target side and moves un-
aligned source terminals to the front of aligned
terminals or nonterminals (move-to-front). The
other is the same algorithm as AI-Onaizan and
Papineni (2006), which differs from Tromble and
Eisner?s approach in attaching unaligned source
terminals to the closest prealigned source termi-
nals or nonterminals (attach). This extension of
adding ?? does not increase the number of rules.
Table 1 shows a Japanese-to-English example
of the representation of rules for our proposed sys-
tem. Japanese words are romanized. Suppose that
source-side string is (X1 wa jinsei no X2 da) and
target-side string is (X1 is X2 of life) and their
word alignments are a=((jinsei , life) , (no , of)
, (da , is)). Source-side aligned words and non-
terminal symbols are sorted into the same order of
target string. Source-side unaligned word (wa) is
moved to the front or right of the prealigned sym-
bol (X1).
441
Surrounding Word Pos Features
s-posr, s-posr + 1, s-posl ? 1, s-posl
s-posr ? 1, s-posr, s-posl ? 1, s-posl
s-posr, s-posr + 1, s-posl, s-posl + 1
s-posr ? 1, s-posr, s-posl, s-posl + 1
Table 3: The Example of Context Features
3.3 Word-based Reordering Model
We utilize the following score(S?) as a feature for
the word-based reordering model. This is incor-
polated into the log-linear model (Och and Ney,
2002) of statistical machine translation.
score(S?) =
?
i,j:1?i<j?n
B[s?i, s
?
j ] (7)
B[s?l, s
?
r] = ? ? ?(s
?
l, s
?
r) (8)
where n is the length of reordered source sen-
tence S? (= (s?1 . . . s
?
n)), ? is a weight vector and
? is a vector of features. This reordering model,
which is originally proposed by Tromble and Eis-
ner (2009), can assign a score to any possible per-
mutation of source sentences. Intuitively B[s?l, s
?
r]
represents the score of ordering s?l before s
?
r; the
higher the value, the more we prefer word s?l oc-
curs before s?r. Whether S
?
l should occur before S
?
r
depends on how often this reordering occurs when
we reorder the source to target sentence order.
To train B, we used binary feature functions
? as used in (Tromble and Eisner, 2009), which
were introduced for dependency parsing by Mc-
Donald et al (2005). Table 2 shows the kind
of features we used in our experiments. We did
not use context features like surrounding word pos
features in Table 3 because they were not useful in
our preliminary experiments and propose an effi-
cient implementation described in the next section
in order to calculate this reordering model when
decoding. To train the parameter ?, we used the
perceptron algorithm following Tromble and Eis-
ner (2009).
3.4 Integration to Cube Pruning
CKY parsing and cube-pruning are used for de-
coding of hierarchical phrase-based model (Chi-
ang, 2007). Figure 3 displays that hierarchical
phrase-based decoder seeks new span [1,7] items
Figure 3: Creating new items from subitems and
rules, that have a span [1,7] in source sentence.
with rules, utilizing subspan [1,3] items and sub-
span [4,7] items. In this example, we use 2-gram
language model and +LM decoding. uni(?) means
1-gram language model cost for heuristics and in-
teraction usually means language model cost that
cannot be calculated offline. Here, we introduce
our two implementations to calculate word-based
reordering model scores in this decoding algo-
rithm.
First, we explain a naive implementation shown
in the left side of Figure 4. This algorithm per-
forms the same calculation of reordering model as
that of language model. Each item keeps a part of
reordered source sentence. The reordering score
of new item can be calculated as interaction cost
when combining subitems with the rule.
The right side of Figure 4 shows our pro-
posed implementation. This implementation can
be adopted to decoding only when we do not use
context features like surrounding word pos fea-
tures in Table 3 (and consider a distance between
words in features). If a span is given, the reorder-
ing scores of new item can be calculated for each
rule, being independent from the word order of
reordered source segment of a subitem. So, the
reordering model scores can be calculated for all
rules with spans by using a part of the input source
sentence before sorting them for cube pruning.
We expect this sorting of rules with reordering
442
Figure 4: The ?naive? and ?proposed? implementation to calculate the reordering cost of new items.
model scores will have good influence on cube
pruning. The right hand side of Figure 4 shows
the diffrence between naive and proposed imple-
mentation (S? is not shown to allow for a clear pre-
sentation). Note the difference is in where/when
the reordering scores are inserted: together with
the N -gram scores in the case of naive implemen-
tation; incorpolated into sorted rules for the pro-
posed implementation.
4 Experiment
4.1 Purpose
To reveal the effectiveness of integrating the re-
ordering model into decoder, we compared the
following setups:
? baseline: a standard hierarchical phrase-
based machine translation (Hiero) system.
? preprocessing: applied Tromble and Eisner?s
approach, then translate by Hiero system.
? Hiero system + reordering model: integrated
reordering model into Hiero system.
We used the Joshua Decoder (Li and Khudanpur,
2008) as the baseline Hiero system. This decoder
uses a log-linear model with seven features, which
consist of N -gram language model PLM (T ), lex-
ical translation model Pw(?|?), Pw(?|?), rule
translation model P (?|?), P (?|?), word penalty
and arity penalty.
The ?Hiero + Reordering model? system has
word-based reordering model as an additional fea-
ture to baseline features. For this approach, we
use two systems. One has ?move-to-front? sys-
tem and the other is ?attach? system explained in
Section 3.2. We implemented our proposed algo-
rithm in Section 3.4 to both ?Hiero + Reordering
model? systems. As for beam width, we use the
same setups for each system.
4.2 Data Set
Data Sent. Word. Avg. leng
Training ja 200.8K 2.4M 12.0
en 200.8K 2.3M 11.5
Development ja 1.0K 10.3K 10.3
en 1.0K 9.8K 9.8
Test ja 1.0K 14.2K 14.2
en 1.0K 13.5K 13.5
Table 4: The Data statistics
For experiments we used a Japanese-English
basic travel expression corpus (BTEC). Japanese
word order is linguistically very different from
English and we think Japanese-English pair is
a very good test bed for evaluating reordering
model.
443
XXXXXXXXXXXSystem
Metrics BLEU PER
Baseline (Hiero) 28.09 39.68
Preprocessing 17.32 45.27
Hiero + move-to-front 28.85 39.89
Hiero + attach 29.25 39.43
Table 5: BLEU and PER scores on the test set.
Our training corpus contains about 200.8k sen-
tences. Using the training corpus, we extracted
hierarchical phrase rules and trained 4-gram lan-
guage model and word-based reordering model.
Parameters were tuned over 1.0k sentences (devel-
opment data) with single reference by minimum
error rate training (MERT) (Och, 2003). Test data
consisted of 1.0k sentences with single reference.
Table 4 shows the condition of corpus in detail.
4.3 Results
Table 5 shows the BLEU (Papineni et al, 2001)
and PER (Niesen et al, 2000) scores obtained by
each system. The results clearly indicated that
our proposed system with word-based reorder-
ing model (move-to-front or attach) outperformed
baseline system on BLEU scores. In contrast,
there is no significant improvement from baseline
on PER. This suggests that the improvement of
BLEU mainly comes from reordering. In our ex-
periment, preprocessing approach resulted in very
poor scores.
4.4 Discussion
Table 6 displays examples showing the cause of
the improvements of our system with reordering
model (attach) comparing to baseline system. We
can see that the outputs of our system are more
fluent than those of baseline system because of re-
ordering model.
As a further analysis, we calculated the BLEU
scores of Japanese S? predicted from reorder-
ing model against true Japanese S? made from
GIZA++ alignments, were only 26.2 points on de-
velopment data. We think the poorness mainly
comes from unaligned words since they are un-
tractable for the word-based reordering model.
Actually, Japanese sentences in our training data
include 34.7% unaligned words. In spite of the
poorness, our proposed method effectively utilize
this reordering model in contrast to preprocessing
approach.
5 Related Work
Our approach is similar to preprocessing approach
(Xia and McCord, 2004; Collins et al, 2005; Li
et al, 2007; Tromble and Eisner, 2009) in that it
reorders source sentence in target order. The dif-
ference is this sentence reordering is done in de-
coding rather than in preprocessing.
A lot of studies on lexicalized reordering (Till-
man, 2004; Koehn et al, 2005; Nagata et al,
2006) focus on the phrase-based model. These
works cannnot be directly applied to hierarchi-
cal phrase-based model because of the difference
between normal phrases and hierarchical phrases
that includes nonterminal symbols.
Shen et al (2008,2009) proposed a way to inte-
grate dependency structure into target and source
side string on hierarchical phrase rules. This ap-
proach is similar to our approach in extending the
formalism of rules on hierarchical phrase-based
model in order to consider the constraint of word
order. But, our approach differs from (Shen et al,
2008; Shen et al, 2009) in that syntax annotation
is not necessary.
6 Conclusion and Future Work
We proposed a method to integrate word-based
reordering model into hierarchical phrase-based
machine translation system. We add ?? into the
hiero rules, but this does not increase the num-
ber of rules. So, this extension itself does not af-
fect the search space of decoding. In this paper
we used Tromble and Eisner?s reordering model
for our method, but various reordering model can
be incorporated to our method, for example S?
N -gram language model. Our experimental re-
sults on Japanese-to-English task showed that our
system outperformed baseline system and prepro-
cessing approach.
In this paper we utilize ?? only for reorder-
ing model. However, it is possible to use ?? for
other modeling, for example we can use it for
rule translation probabilities P (?? |?), P (?|??) for
additional feature functions. Of course, we can
444
S america de seihin no hanbai wo hajimeru keikaku ga ari masu ka . kono tegami wa koukuubin de nihon made ikura kakari masu ka .
TB sales of product in america are you planning to start ? this letter by airmail to japan . how much is it ?
TP are you planning to start products in the u.s. ? how much does it cost to this letter by airmail to japan ?
R do you plan to begin selling your products in the u.s. ? how much will it cost to send this letter by air mail to japan ?
Table 6: Examples of outputs for input sentence S from baseline system TB and our proposed sys-
tem (attach) TP . R is a reference. The underlined portions have equivalent meanings and show the
reordering differences.
also utilize reordered target sentence T ? for vari-
ous modeling as well. Addtionally we plan to use
S? for MERT because we hypothesize the fluent
S? leads to fluent T .
References
AI-Onaizan, Y. and K. Papineni. 2006. Distortion
models for statistical machine translation. In Proc.
the 44th ACL, pages 529?536.
Brown, P. F., S. A. D. Pietra, V. D. J. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical ma-
chine translation: Parameter estimation. Computa-
tional Linguitics, 19:263?312.
Chiang, D., K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
Proc. NAACL, pages 216?226.
Chiang, D. 2007. Hierachical phrase-based transla-
tion. Computational Linguitics, 33:201?228.
Collins, M., P. Koehn, and I. Kucerova. 2005. Clause
restructuring for statistical machine translation. In
Proc. the 43th ACL, pages 531?540.
Collins, M. 2002. Discriminative training methods for
hidden markov models. In Proc. of EMNLP.
Freund, Y. and R. E. Schapire. 1996. Experiments
with a new boosting algorithm. In Proc. of the 13th
ICML, pages 148?156.
Koehn, P., A. Axelrod, A-B. Mayne, C. Callison-
Burch, M. Osborne, and D. Talbot. 2005. Ed-
inburgh system description for 2005 iwslt speech
translation evaluation. In Proc. the 2nd IWSLT.
Li, Z. and S. Khudanpur. 2008. A scalable decoder
for parsing-based machine translation with equiv-
alent language model state maintenance. In Proc.
ACL SSST.
Li, C-H., D. Zhang, M. Li, M. Zhou, K. Li, and
Y. Guan. 2007. A probabilistic approach to syntax-
based reordering for statistical machine translation.
In Proc. the 45th ACL, pages 720?727.
McDonald, R., K. Crammer, and F. Pereira. 2005.
Spanning tree methods for discriminative training of
dependency parsers. In Thechnical Report MS-CIS-
05-11, UPenn CIS.
Nagata, M., K. Saito, K. Yamamoto, and K. Ohashi.
2006. A clustered global phrase reordering model
for statistical machine translation. In COLING-
ACL, pages 713?720.
Niesen, S., F.J. Och, G. Leusch, and H. Ney. 2000.
An evaluation tool for machine translation: Fast
evaluation for mt research. In Proc. the 2nd In-
ternational Conference on Language Resources and
Evaluation.
Och, F. J. and H. Ney. 2002. Discriminative train-
ing and maximum entropy models for statistical ma-
chine translation. In Proc. the 40th ACL, pages 295?
302.
Och, F. and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29:19?51.
Och, F. J. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. the 41th ACL,
pages 160?167.
Papineni, K. A., S. Roukos, T. Ward, and W-J. Zhu.
2001. Bleu: a method for automatic evaluation of
machine translation. In Proc. the 39th ACL, pages
311?318.
Shen, L., J. Xu, and R. Weischedel. 2008. A new
string-to-dependency machine translation algorithm
with a target dependency language model. In Proc.
ACL, pages 577?585.
Shen, L., J. Xu, B. Zhang, S. Matsoukas, and
R. Weischedel. 2009. Effective use of linguistic and
contextual information for statistical machine trans-
lation. In Proc. EMNLP, pages 72?80.
Tillman, C. 2004. A unigram orientation model
for statistical machine translation. In Proc. HLT-
NAACL, pages 101?104.
Tromble, R. and J. Eisner. 2009. Learning linear
ordering problems for better translation. In Proc.
EMNLP, pages 1007?1016.
445
Watanabe, T., H. Tsukada, and H. Isozaki. 2006. Left-
to-right target generation for hierarchical phrase-
based translation. In Proc. COLING-ACL, pages
777?784.
Xia, F. and M. McCord. 2004. Improving a statis-
tical mt system with automatically learned rewrite
patterns. In Proc. the 18th ICON, pages 508?514.
446
