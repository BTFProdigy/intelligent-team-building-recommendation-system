Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 305?310,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Towards Tracking Semantic Change by Visual Analytics
Christian Rohrdantz1 Annette Hautli2 Thomas Mayer2
Miriam Butt2 Daniel A. Keim1 Frans Plank2
Department of Computer Science1 Department of Linguistics2
University of Konstanz
Abstract
This paper presents a new approach to detect-
ing and tracking changes in word meaning by
visually modeling and representing diachronic
development in word contexts. Previous stud-
ies have shown that computational models
are capable of clustering and disambiguat-
ing senses, a more recent trend investigates
whether changes in word meaning can be
tracked by automatic methods. The aim of our
study is to offer a new instrument for inves-
tigating the diachronic development of word
senses in a way that allows for a better under-
standing of the nature of semantic change in
general. For this purpose we combine tech-
niques from the field of Visual Analytics with
unsupervised methods from Natural Language
Processing, allowing for an interactive visual
exploration of semantic change.
1 Introduction
The problem of determining and inferring the sense
of a word on the basis of its context has been the
subject of quite a bit of research. Earlier investiga-
tions have mainly focused on the disambiguation of
word senses from information contained in the con-
text, e.g. Schu?tze (1998) or on the induction of word
senses (Yarowsky, 1995). Only recently, the field
has added a diachronic dimension to its investiga-
tions and has moved towards the computational de-
tection of sense development over time (Sagi et al,
2009; Cook and Stevenson, 2010), thereby comple-
menting theoretical investigations in historical lin-
guistics with information gained from large corpora.
These approaches have concentrated on measuring
general changes in the meaning of a word (e.g., nar-
rowing or pejoration), whereas in this paper we deal
with cases where words acquire a new sense by ex-
tending their contexts to other domains.
For the scope of this investigation we restrict our-
selves to cases of semantic change in English even
though the methodology is generally language in-
dependent. Our choice is on the one hand moti-
vated by the extensive knowledge available on se-
mantic change in English. On the other hand, our
choice was driven by the availability of large cor-
pora for English. In particular, we used the New
York Times Annotated Corpus.1 Given the variety
and the amount of text available, we are able to track
changes from 1987 until 2007 in 1.8 million news-
paper articles.
In order to be able to explore our approach in a
fruitful manner, we decided to concentrate on words
which have acquired a new dimension of use due
to the introduction of computing and the internet,
e.g., to browse, to surf, bookmark. In particular,
the Netscape Navigator was introduced in 1994 and
our data show that this does indeed correlate with a
change in use of these words.
Our approach combines methods from the fields
of Information Visualization and Visual Analyt-
ics (Thomas and Cook, 2005; Keim et al, 2010)
with unsupervised techniques from Natural Lan-
guage Processing (NLP). This combination provides
a novel instrument which allows for tracking the di-
achronic development of word meaning by visual-
izing the contexts in which the words occur. Our
overall aim is not to replace linguistic analysis in
1http://http://www.ldc.upenn.edu/
305
this field with an automatic method, but to guide re-
search by generating new hypotheses about the de-
velopment of semantic change.
2 Related work
The computational modeling of word senses is based
on the assumption that the meaning of a word can
be inferred from the words in its immediate con-
text (?context words?). Research in this area mainly
focuses on two related tasks: Word Sense Disam-
biguation (WSD) and Word Sense Induction (WSI).
The goal of WSD is to classify occurrences of pol-
ysemous words according to manually predefined
senses. One popular method for performing such
a classification is Latent Semantic Analysis (LSA)
(Deerwester et al, 1990), with other methods also
suitable for the task (see Navigli (2009) for an ex-
tensive survey).
The aim of WSI is to learn word senses from
text corpora without having a predefined number of
senses. This goal is more difficult to achieve, as it
is not clear beforehand how many senses should be
extracted and how a sense could be described in an
abstract way. Recently, however, Brody and Lapata
(2009) have shown that Latent Dirichlet Allocation
(LDA) (Blei et al, 2003) can be successfully applied
to perform word sense induction from small word
contexts.
The original idea of LSA and LDA is to learn ?top-
ics? from documents, whereas in our scenario word
contexts rather than documents are used, i.e., a small
number of words before and after the word under
investigation (bag of words). Sagi et al (2009)
have demonstrated that broadening and narrowing
of word senses can be tracked over time by applying
LSA to small word contexts in diachronic corpora.
In addition, we will use LDA, which has proven even
more reliable in the course of our investigations.
In general, the aim of our paper is to go beyond
the approach of Sagi et al (2009) and analyze se-
mantic change in more detail. Ideally, a starting
point of change is found and the development over
time can be tracked, paired with a quantitative com-
parison of prevailing senses. We therefore suggest
to visualize word contexts in order to gain a better
understanding of diachronic developments and also
generate hypotheses for further investigations.
3 An interactive visualization approach to
semantic change
In order to test our approach, we opted for a large
corpus with a high temporal resolution. The New
York Times Annotated Corpus with 1.8 million
newspaper articles from 1987 to 2007 has a rather
small time depth of 20 years but provides a time
stamp for the exact publication date. Therefore,
changes can be tracked on a daily basis.
The data processing involved context extraction,
vector space creation, and sense modeling. As
Schu?tze (1998) showed, looking at a context win-
dow of 25 words before and after a key word pro-
vides enough information in order to disambiguate
word senses. Each extracted context is comple-
mented with the time stamp from the corpus. To
reduce the dimensionality, all context words were
lemmatized and stop words were filtered out.
For the set of all contexts of a key word, a global
LDA model was trained using the MALLET toolkit2
(McCallum, 2002). Each context is assigned to its
most probable topic/sense, complemented by a spe-
cific point on the time scale according to its time
stamp from the corpus. Contexts for which the high-
est probability was less than 40% were omitted be-
cause they could not be assigned to a certain sense
unambiguously. The distribution of senses over time
was then visualized.
3.1 Visualization
Different visualizations provide multidimensional
views on the data and yield a better understanding
of the developments. While plotting every word oc-
currence individually offers the opportunity to detect
and inspect outliers, aggregated views on the data
are able to provide insights on overall developments.
Figure 1 provides a view where the percentages of
word contexts belonging to different senses are plot-
ted over time. For the verbs to browse and to surf
seven senses are learned with LDA. Each sense cor-
responds to one row and is described by the top five
terms identified by LDA. The higher the gray area
at a certain x-axis point, the more of the contexts of
the corresponding year belong to the specific sense.
Each shade of gray represents 10% of the overall
data, i.e., three shades of gray mean that between
2http://mallet.cs.umass.edu/
306
to browse to surf
time, library, 
student, music, 
people
shop, street, 
book, store, art
book, read, 
bookstore, find, 
year
deer, plant, 
tree, garden, 
animal
software, microsoft, 
internet, netscape, 
windows
web, internet, 
site, mail , 
computer
store, shop, 
buy, day, 
customer
sport, wind, 
water, ski, offer
wave, surfer, 
board, year, 
sport
channel,  
television, 
show, watch, tv
web, internet, 
site, computer, 
company
film, boy, 
movie, show, 
ride
year, day, time, 
school, friend
beach, wave, 
surfer, long, 
coast
a
b
c
d
e
f
g
h
i
j
k
l
m
n
Figure 1: Temporal development of different senses concerning the verbs to browse (left) and to surf (right)
20% and 30% of the contexts can be attributed to
that sense. For each year one value has been gener-
ated and values between two years are linearly inter-
polated.
Figure 2 shows the development of contexts over
time, with each context plotted individually. The
more recent the context, the darker the color.3 Each
axis represents one sense of to browse, in each sub-
figure different combinations of senses are plotted.
A random jitter has been introduced to avoid over-
laps. Contexts in the middle (not the lower left cor-
ner, but the middle of the graph, e.g., see e vs. f)
belong to both senses with at least 40% probabil-
ity. Senses that share many ambiguous contexts are
usually similar. By mousing over a colored dot, its
context is shown, allowing for an in depth analysis.
3.2 Case studies
In order to be able to judge the effectiveness of our
new approach, we chose key words that are likely
candidates for a change in use in the time from 1987
to 2007. That is, we concentrated on terms relat-
ing to the relatively recent introduction of the inter-
net. The advantage of these terms is that the cause
of change can be located precisely in time.
Figure 1 shows the temporal sense development
of the verbs to browse and to surf, together with
the descriptive terms for each sense. Sense e for to
3The pdf version of this paper contains a bipolar color map.
browse and sense k for to surf pattern quite similarly.
Inspecting their contexts reveals that both senses ap-
pear with the invention of web browsers, peaking
shortly after the introduction of Netscape Navigator
(1994). For to browse, another broader sense (sense
f) concerning browsing in both the internet and dig-
ital media collections shows a continuous increase
over time, dominating in 2007.
The first occurrences assigned to sense f in 1987
are ?browse data bases?, ?word-by-word brows-
ing? in databases and ?browsing files in the cen-
ter?s library?, referring to physical files, namely pho-
tographs. We speculate that the sense of browsing
physical media might haven given rise to the sense
which refers to browsing electronic media, which in
turn becomes the dominating sense with the advent
of the web.
Figure 2 shows pairwise comparisons of word
senses with respect to the contexts they share, i.e.,
contexts that cannot unambiguously be assigned to
one or the other. Each context is represented by
one dot colored according to its time stamp. It can
be seen that senses d (animals that browse) and e
(browsing the web) share no contexts at all. Senses
d (animals that browse) and f (browsing files) share
only few contexts. In turn, senses e and f share a
fair number of contexts, which is to be expected, as
they are closely related. Single contexts, each rep-
resented by a colored dot, can be inspected via a
307
Figure 2: Pairwise comparisons of different senses for the verb ?to browse?. In each subfigure different combinations
of LDA dimensions are mapped on the axes.
LSA dimensions
1 web 0.40, internet 0.38, software 0.36, microsoft 0.28, win-
dows 0.18
2 microsoft 0.24, software 0.23, windows 0.13, internet 0.13,
netscape 0.12
3 microsoft 0.27, store 0.22, shop 0.20, windows 0.19, software
0.16
4 shop 0.32, netscape 0.23, web 0.23, store 0.19, software 0.19
5 book 0.48, netscape 0.26, software 0.17, world 0.13, commu-
nication 0.12
6 internet 0.58, shop 0.25, service 0.16, computer 0.13, people
0.11
7 make 0.39, shop 0.34, site 0.16, windows 0.13, art 0.08
... ...
15 find 0.30, people 0.22, year 0.19, deer 0.16, day 0.15
Table 1: Descriptive terms for the top LSA dimensions for
the contexts of to browse. For each dimension the top 5
positively associated terms were extracted, together with
their value in the corresponding dimension.
mouse roll over. This allows for an in-depth look at
specific data points and a better understanding how
the data points relate to a sense.
3.3 LSA vs. LDA
In comparison, Table 1 shows the LSA dimensions
learned from the contexts of the verb to browse. The
top five associated terms for each dimension have
been extracted as descriptor. The dimensions are
heavily dominated by senses strongly represented
in the corpus (e.g., browsing the web). Infrequent
senses (e.g., animals that browse) only occur in very
low-ranked dimensions and are mixed with other
senses (see the bold term deer in dimension 15).
4 Evaluation
We compared the findings provided by our visual-
ization with word sense information coming from
various resources, namely the 2007 Collins dictio-
nary (COLL), the English WordNet4 (WN) (Fell-
baum, 1998) and the Longman Dictionary (LONG)
from 1987. Senses that evolved later than 1987
should not appear in LONG, but should appear in
later dictionaries.
However, we are well aware that dictionaries are
by no means good gold standards as lexicogra-
phers themselves vary greatly when assigning word
senses. Nevertheless, this comparison can provide a
first indication as to whether the results of our tool
is in line with other methods of identifying senses.
In the case of to browse, COLL and WordNet
suggest the senses ?shopping around; not necessar-
ily buying?, ?feed as in a meadow or pasture? and
?browse a computer directory, surf the internet or the
world wide web.? These senses are also identified in
our visualizations, which even additionally differen-
tiate between the senses of ?browsing the web? and
?browsing a computer directory.? A WordNet sense
that cannot be detected in the data is the meaning ?to
eat lightly and try different dishes.?
Table 2 shows the results of comparing dictionary
word senses (DIC) with the results from our visual-
ization (VIS). What can be seen is that our method
is able to track semantic change diachronically and
4http://wordnetweb.princeton.edu
308
to browse to surf messenger bug bookmark
# of word senses # of word senses # of word senses # of word senses # of word senses
DIC VIS DIC VIS DIC VIS DIC VIS DIC VIS
1987 (LONG) 2 3 1 1 1 2 6 3 1 1
1998 (WN) 5 4 3 3 1 3 5 3 1 2
2007 (COLL) 3 4 3 2 1 3 5 3 2 2
Table 2: A comparison of different word senses as given in dictionaries with the visualization results across time
in the majority of cases, the number of our senses
correspond to the information coming from the dic-
tionaries. In some cases we are even more accurate
in discriminating them. In the case of ?messenger?,
the visualizations suggest another sense related to
?instant messaging? that arises with the advent of
the AOL instant messenger in 1997. This leads us to
the conclusion that our method is appropriate from a
historical linguistic point of view.
5 Discussion and conclusions
When dealing with a complex phenomenon such as
semantic change, one has to be aware of the limita-
tions of an automatic approach in order to be able
to draw the right conclusions from its results. The
first results of the case studies presented in this pa-
per show that LDA is useful for distinguishing dif-
ferent word senses on the basis of word contexts and
performs better than LSA for this task. Further, it
has been demonstrated by exemplary cases that the
emergence of a new word sense can be detected by
our new methodology
One of the main reasons for an interactive visu-
alization approach is the possibility of being able to
detect conspicuous patterns at-a-glance, yet at the
same time being able to delve into the details of the
data by zooming in on the occurrences of particu-
lar words in their contexts. This makes it possible
to compensate for one of the major disadvantages
of generative and vector space models, namely their
functioning as ?black boxes? whose results cannot
be tracked easily.
The biggest problem in dealing with a corpus-
based method of detecting meaning change is the
availability of suitable corpora. First, computing se-
mantic information on the basis of contexts requires
a large amount of data in order to be able to infer re-
liable results. Second, the words in the context from
which the meanings will be distinguished should be
both semantically and orthographically stable over
time so that comparisons between different stages in
the development of the language can be made. Un-
fortunately, both requirements are not always met.
On the one hand words do change their meaning,
after all this is what the present study is all about.
However, we assume that the meanings in a certain
context window are stable enough to infer reliable
results provided it is possible that the forms of the
same words in different periods can be linked. This
of course limits the applicability of the approach to
smaller time ranges due to changes in the phonetic
form of words. Moreover, in particular for older pe-
riods of the language, different variants for the same
word, either due to sound changes or different (or
rather no) spelling conventions, abound. For now,
we circumvent this problem by testing our tool on
corpora where the drawbacks of historical texts are
less severe but at the same time interesting develop-
ments can be detected to prove our approach correct.
For future research, we want to test our methodol-
ogy on a broader range of terms, texts and languages
and develop novel interactive visualizations to aid
investigations in two ways. As a first aim, the user
should be allowed to check the validity and quality
of the visualizations by experimenting with param-
eter settings and inspecting their outcome. Second,
the user is supposed to gain a better understanding of
semantic change by interactively exploring a corpus.
Acknowledgments
This work has partly been funded by the Research
Initiative ?Computational Analysis of Linguistic
Development? at the University of Konstanz and by
the German Research Society (DFG) under the grant
GK-1042, Explorative Analysis and Visualization of
Large Information Spaces, Konstanz. The authors
would like to thank Zdravko Monov for his program-
ming support.
309
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Samuel Brody and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL ?09, pages 103?
111, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Paul Cook and Suzanne Stevenson. 2010. Automati-
cally Identifying Changes in the Semantic Orientation
of Words. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), pages 28?34, Valletta, Malta.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41:391?
407.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Daniel A. Keim, Joern Kohlhammer, Geoffrey Ellis, and
Florian Mansmann, editors. 2010. Mastering The In-
formation Age - Solving Problems with Visual Analyt-
ics. Goslar: Eurographics.
Andrew Kachites McCallum. 2002. MALLET:
A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACMComputing Surveys (CSUR), 41(2):1?69.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009.
Semantic Density Analysis: Comparing Word Mean-
ing across Time and Phonetic Space. In Proceedings
of the EACL 2009 Workshop on GEMS: GEometical
Models of Natural Language Semantics, pages 104?
111, Athens, Greece.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
James J. Thomas and Kristin A. Cook. 2005. Illuminat-
ing the Path The Research and Development Agenda
for Visual Analytics. National Visualization and Ana-
lytics Center.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd annual meeting on Association for
Computational Linguistics (ACL ?95), pages 189?196,
Cambridge, Massachusetts.
310
Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 70?78,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Consonant Co-occurrence in Stems Across Languages: Automatic
Analysis and Visualization of a Phonotactic Constraint
Thomas Mayer1, Christian Rohrdantz2, Frans Plank1,
Peter Bak2, Miriam Butt1, Daniel A. Keim2
1Department of Linguistics, 2Department of Computer Science
University of Konstanz, Germany
{thomas.mayer,christian.rohrdantz}@uni-konstanz.de
Abstract
In this paper, we explore the phenomenon
of Similar Place Avoidance (SPA), ac-
cording to which successive consonants
within stems sharing the same place of
articulation are avoided. This principle
has recently been hypothesized as a uni-
versal tendency although evidence from
only a few languages scattered across the
world has been considered. Using meth-
ods taken from the field of Visual Analyt-
ics, which have demonstrably been shown
to help with understanding complex in-
teractions across large data sets, we in-
vestigated a large crosslinguistic lexical
database (comprising data on more than
4,500 languages) and found that a univer-
sal tendency can indeed be maintained.
1 Introduction
Linguistic knowledge has traditionally been ac-
quired by analyzing a manageable set of data, on
the basis of which generalizations are posited that
can then be tested on an extended set of data from
the same language or comparative data from other
languages. Tendencies, rather than absolute prin-
ciples, are difficult to detect under this approach.
This is true especially when they are obscured by
counterexamples that happen to occur with high
frequency, but that may be restricted to just a
small minority of the overall pattern. This may
prompt a researcher to discard a valid generaliza-
tion from the outset. In recent years, a plethora of
statistical and stochastic methods have therefore
been pursued within linguistic research, leading to
approaches such as stochastic Optimality Theory
(Boersma and Hayes, 2001) or the use of statis-
tics to detect crosslinguistic tendencies (Bickel, in
press).
However, although the various statistical meth-
ods deal with data which exhibit very complex and
often ill-understood interactions, analyses have
not to date availed themselves of methodology
currently being developed in the field of Visual
Analytics, which allows us to use our powerful vi-
sual processing ability to understand and evaluate
complex data sets (Keim et al, 2008; Thomas and
Cook, 2005).
In this paper, we present an interdisciplinary
effort whereby linguistically interesting patterns
are automatically extracted, analyzed and visually
presented so that an at-a-glance evaluation of lin-
guistically significant patterns is made possible. In
order to demonstrate that this technique is espe-
cially useful with phenomena that do not mani-
fest themselves in absolute principles, but rather
in statistical tendencies, we investigated a phe-
nomenon that, on the basis of a comparatively
sparse and unrepresentative data set, has recently
been claimed to be a universal tendency (Pozdni-
akov and Segerer, 2007): Similar Place Avoidance
(SPA). In this paper, we conduct a more represen-
tative study of about 4,500 languages. Our results
allow an at-a-glance evaluation which shows that
SPA indeed seems to be a valid language universal
tendency.
Our work on SPA is part of a more widespread
effort currently being conducted with respect to vi-
sually representing crosslinguistic sound patterns.
In Rohrdantz et al (2010), we already showed that
phonological patterns in languages can be auto-
matically extracted and visualized from corpora.
Figure 1 displays the vowel harmony patterns that
were extracted for Turkish in comparison with the
lack of such patterns in a non-harmonic language
like Spanish.
The remainder of this article is organized as fol-
lows. Section 2 introduces SPA. Section 3 pro-
vides an overview of the material that was used. A
description of the calculations and statistical anal-
yses is given in Section 4. Section 5 presents
the results of the geo-spatial visualizations, partly
70
Figure 1: Turkish vowel harmony patterns (left).
The matrix visualizaton was generated on the
basis of the Turkish Bible text and shows the
palatal (front/back) and labial (rounding) harmony
blocks. Rows and columns are automatically
sorted according to the similarity of vowels. For
non-harmonic languages, such as Spanish (right),
no such patterns can be detected.
with respect to a WALS map (Haspelmath et al,
2005). In the final section, we consider some im-
plications of our findings and raise some questions
for future research.
2 Similar Place Avoidance (SPA)
It has long been noted in studies on Semitic lan-
guages, especially Arabic, that there are con-
straints on the structure of triliteral consonant
roots (
?
CCC) with respect to the phonological
features of the individual consonants (Greenberg,
1950). The basic observation is that consonants
with a similar place of articulation are avoided
in non-derived forms. A similar observation has
also been made with respect to the Proto-Indo-
European (PIE) roots. Among other things, Iver-
son and Salmons (1992) note that Stop-V-Stop
roots were very rare in PIE, representing only
3.5% of a lexicon of more than 2,000 items. Plank
(1981:221f) observes that Modern German tends
to avoid verbal stems with identical consonants
in initial and final positions (allowing for differ-
ences in voicing), and that those verbs with iden-
tical initial and final consonants which do exist
are all morphologically regular. This indicates that
they are not basic verbs, but represent a technique
of word formation, perhaps derivative of redupli-
cation as especially common in child or child-
directed language.1
1Note that the early speech of children is characterized by
the opposite effect of SPA: both consonants and vowels tend
to share the same place of articulation (Fikkert and Levelt,
2010), with greater and greater differentiation being achieved
in the course of language acquisition. The reasons for this
remain to be investigated.
Looking at suprasegmental features, Leben
(1973) argued that a similar restriction holds for
the co-occurrence of tones in underlying repre-
sentations. In the framework of Autosegmental
Phonology this has become known as the Oblig-
atory Contour Principle (OCP), which precludes
sequences of identical tones from underlying rep-
resentations. This principle has since been under-
stood more generally as a prohibition on similar
items and has thus also been used in relation with
the SPA bias in Semitic radicals.
More recently, the application of SPA with
respect to stem-internal consonants has been
claimed for other non-Semitic languages as well.
Pozdniakov and Segerer (2007) found impres-
sive support for it in their sample of Atlantic
and Bantu languages of Niger-Congo and fur-
ther tested its crosslinguistic validity for some
more languages or language groups (Mande, Kwa,
Ubangi, Sara-Bongo-Bagirmi, Chadic, Malagasy,
Indo-European, Nostratic, Mongolian, Basque,
Quechua, Kamilaroi, Port Moresby Pidgin En-
glish) with similar results. Table 1 shows their
findings across all 31 languages in their sample.
It can be seen that the highest negative numbers
are in the main diagonal of the matrix, which is
exactly what SPA would predict.
P T C K
P ?15 +11 +5 ?5
T +12 ?10 ?5 +13
C +8 ?5 ?6 +8
K ?3 +8 +5 ?15
Table 1: Results in Pozdniakov and Segerer
(2007). The numbers indicate the overall sum of
cells with negative vs. positive values with regard
to successions of places of articulation (see Sec-
tion 3 for a description of the labels P, T, C and K)
for all languages in their sample. Positive and neg-
ative values have been assigned if the observed ab-
solute value was at least 15% above (respectively
below) the expected value. Compare their results
with the left matrix in Figure 3.
3 Database and methodology
The data that underlies all the subsequent work
presented in this paper have been taken from the
Automated Similarity Judgment Program (ASJP;
Wichmann et al, 2010), which aims at achiev-
71
ing a computerized lexicostatistical analysis of the
world?s languages. To this end, Wichmann and his
collaborators have collected Swadesh list items for
over 4,500 languages. The so-called Swadesh list
was developed by Morris Swadesh in the 1940?
50s with the aim of having a basic set of vocabu-
lary items which are culturally neutral and which
one would expect to be stable over time. The orig-
inal idea of a Swadesh list was to be able to com-
pare and test languages with respect to genealogi-
cal relations.
The Swadesh items in the Wichmann et al
database are transcribed in the ASJP orthogra-
phy, which uses standard ASCII characters to en-
code the sounds of the world?s languages, but does
merge some of the distinctions made by the IPA.
Furthermore, stress, tone and vowel length are not
recorded in the database. However, for the pur-
pose of our investigation the transcription is suit-
able because place of articulation is sufficiently
distinguished.
We decided to experiment with two different ap-
proaches for dividing up the place of articulation
features. One approach (PTCK) is based on the ar-
rangement in Pozdniakov and Segerer (2007) and
distinguishes four places of articulation for labial
(P), dental (and alveolar) (T), (alveo-)palatal (C)
and velar (K) consonants. A second grouping
(LCD) only distinguishes three places of articula-
tion: labial (L), coronal (C) and dorsal (D).2 Ac-
cording to this classification the consonants of all
the items in the database can be assigned to one of
these symbols, as shown in Table 2.
LCD PTCK ASJP IPA
L P
p, b, m, f, v, w p, F, b, B, m,
f, v, w
C
T
8, 4, t, d, s, z,
c, n, S, Z
T, D, n
?
, t, d, s,
z, ts, dz, n, S,
Z
C
C, j, T, l, L, r,
y
?, ?, c, ?, l, L,
?, L, r, R, j
D K
5, k, g, x, N,
q, G, X, 7, h
?, k, g, x, G, N,
q, G, X, K, ?,
Q, P, h, H,
Table 2: Assignment of consonants to symbols.
All varieties of ?click?-sounds have been ignored.
2Radical and laryngeal, which are commonly employed
in the phonological literature as yet another place distinction,
are subsumed under dorsal.
Experiments with using the four-way distinc-
tion vs. the three-way distinction showed that T
and C in the four-way grouping behave very simi-
larly with respect to the transitions to other places
of articulation (see Section 4.2). We therefore de-
cided to use the three-way distinction for the bulk
of our calculations and visualizations and only
sporadically resort to the four-way grouping when
a more fine-grained distinction is needed.
Furthermore, we decided to only include those
cases where the first and second consonants are
preceded (or followed, respectively) by another
vowel or a word boundary and are therefore not
part of a consonant cluster. We mainly did this in
order to minimize the noise caused by consonants
of inflectional markers that tend to assimilate in
such clusters.
In the literature on root morphemes in Semitic,
it has been noted that the consonants within trilit-
eral radicals behave differently with respect to
OCP. Greenberg (1950:162) remarks that while
the first and second consonants are usually not
identical, the same does not hold for the sec-
ond and third consonants, which frequently consti-
tute the well-known geminate subtype of Semitic
verbs. However, for our work we understand OCP
as it was later formulated within the framework
of autosegmental phonology (Leben, 1973; Mc-
Carthy, 1986; Goldsmith, 1976) in that adjacent
identical elements (here in the sense of identical
with respect to place of articulation) are prohib-
ited, under the assumption that consonants are ad-
jacent to each other (on the C tier) even when they
are separated by vowels in the linear sequence of
phonemes within the word.
For the purposes of our experiment, we con-
sidered the relevant context for adjacency to be
one where consonants are separated by exactly one
vowel.3 Note that since the basis for our calcula-
tions were not stems in the language but the cita-
tion forms that are used in the Swadesh lists, we
also get noise from inflectional markers that are
attached to these forms and might have the same
place of articulation irrespective of the stem to
which they attach.4
Finally, there are several shortcomings of the
3Since vowel length is not marked in the ASJP database,
long vowels are also included.
4Assimilation processes are far more frequent than dis-
similation processes in this context so that it is more likely
that the same place of articulation features are to be expected
when an inflectional marker is present.
72
material in the database with respect to our investi-
gation which must be kept in mind. OCP/SPA has
been claimed to apply with respect to underlying
or non-derived representations. Previous work has
been done on the basis of stem (or root) lists. De-
pending on the language, Swadesh list items are
not always stems, but whole words in their cita-
tion forms. For instance, while both English and
German use the infinitive as the citation form for
verbal stems, in English the infinitive is identical
to the stem whereas in German it is marked with
the suffix -en. In other languages, verbs can also
be cited by inflected forms other than the infinitive
(e.g., the 3rd person singular perfective in Arabic,
or the first person singular indicative present in
Latin). The same holds for nouns or other word
classes that are included in the Swadesh list. An-
other problematic aspect is the fact that it also con-
tains items (such as personal pronouns) that are
not lexical in the strict sense of the meaning and
are realized as bound forms in many languages.
Apart from that, the number of items for each
language in the ASJP database varied greatly from
only a few to one hundred. Moreover, the num-
ber of CVC sequences within the items differed
greatly from one language to another, depending
on the phonotactic properties of the languages.
Previous statistical studies have relied on a much
larger number of stems and consonant sequences.
Pozdniakov and Segerer?s (2007) statistics, for ex-
ample, were calculated on the basis of 495 to
17,944 CVC successions for the languages in their
sample.5 In contrast, our statistics are based on
much fewer CVC successions, ranging from 21 to
246 per language. Nevertheless, our results actu-
ally correspond to the main findings of their study
so that we think that the data are good enough for
our purposes.
4 Automated statistical analysis
4.1 Methodology
In a first step, for each language in the sample
an elementary statistical processing is performed.
Thereby, all successions of places of articulation
occurring in the Swadesh list items are identified
and counted. To do so, we define a succession of
5Note that they also included cases where the first and
second consonant are part of a consonant cluster, which we
ignored for our calculations. Furthermore, those languages
where the number of consonant successions in the data was
20 or below were not included in our visualizations, thereby
reducing the number of languages from about 4,500 to 3,200.
places of articulation as a binary sequence of con-
sonants (C-C). These consonants have to appear
within a word and have to be separated by exactly
one vowel (V). Before and after the succession ei-
ther word boundaries (#) or vowels have to ap-
pear. Hence, the following regular expression is
used to extract C-C successions (marked in bold):
[#|V ]CV C[#|V ]. Next, each consonant is as-
signed to one of the three major articulation place
categories labial, coronal and dorsal. The succes-
sion counts are summarized in a quadratic matrix
where the rows represent the preceding place of ar-
ticulation and the columns the following place of
articulation. Each matrix cell contains the number
of times the respective place of articulation suc-
cession could be observed in the corpus. Subse-
quently, for each of the 9 possible successions a
contingency table was created (Table 3).
P2 ?P2
P1 A : n(P1 ? P2) B: n(P1 ? ?P2)
?P1 C : n(?P1 ? P2) D : n(?P1 ? ?P2)
Table 3: Contingency table for the articulation
place (P) succession from P1 to P2.
The succession counts were used to calculate ?
coefficients, where A,B,C and D correspond to
the four cells in Table 3.
? =
?
?2
(A+B + C +D)
(1)
The ? coefficient is a measure for the degree
of association between two variables which can
be derived from the fourfold ?2 statistical signif-
icance test (see Rummel, 1970:298f for details).
Sample ? values for the place of articulation suc-
cessions of Egyptian Arabic can be seen in Table
4. A visual representation of the same matrix is
provided in Figure 2. Note the at-a-glance analy-
sis made possible by Figure 2 vs. Table 4.
labial coronal dorsal
labial ?0.360 +0.187 +0.183
coronal +0.259 ?0.243 ?0.068
dorsal ?0.010 +0.097 ?0.121
Table 4: Matrix of ? values for Egyptian Arabic.
Figure 2 shows an example in which all diag-
onal values (self-successions of places of articu-
lation) have negative associations. This tendency
73
Figure 2: Visualization of the ? matrix from Ta-
ble 4 (Egyptian Arabic), L stands for labial, C for
coronal and D for dorsal. It can be seen that all di-
agonal values (successions of the same place of ar-
ticulation) have negative associations (red color).
to alternate places of articulation can be observed
in most languages and in the overall matrix visu-
alizations including all data from all languages in
the database (Figure 4).
4.2 General relations among places of
articulation
As already mentioned, we tested whether it is use-
ful to distinguish the two different subcategories
dental (and alveolar) (T), and (alveo-)palatal (C).
Figure 3 shows the resulting association values ?
of place successions.
It can clearly be seen that T and C behave very
similarly. A further interesting observation is that
places of articulation tend to alternate (negative di-
agonal values for self-successions). As revealed in
the succession graph of Figure 3, the places of ar-
ticulation do not remain the same, but change to
the closest alternative(s). In the case of P and K
the closest distinct places of articulation (T and C)
are preferred. In the case of T and C, however, this
is somewhat different. Apparently, direct alterna-
tions between both are less probable. One plau-
sible explanation could be that they are not dis-
tinct enough and thus either K or P are preferred
as a following place of articulation, both having
roughly the same distance. These observations
led us to merge the places T and C in our further
analyses and distinguish labial, coronal and dorsal
consonants only, as in Figure 4.
Note that the cross pattern on the left in Figure
4, which now emerges very clearly, reinforces the
hypothesis that the closest distinct place of articu-
lation is preferred as successor.
Figure 4: The ? matrix considering only the three
main categories for all the data across languages.
In the left figure, the categories are sorted accord-
ing to their position in the oral cavity. In the
right figure, the categories are sorted automati-
cally, which shows that D and L are more similar
to each other than D and C.
4.3 Distribution across languages
Next, we examined the distribution of ? values for
self-successions of places of articulation in about
3,200 languages. Self-successions correspond to
the diagonal values of the ? matrices from the up-
per left to the lower right. As can be seen in the
histogram in Figure 6, the peak of the distribution
is clearly located in the area of negative associa-
tion values. In the box-plots of Figure 5, which
show the distributions for all three places of ar-
ticulation separately, it is clearly visible that for
each of the three places of articulation at least 75%
of the languages included show negative associa-
tions. Furthermore, it can be seen that most out-
liers disappear when taking only the languages for
which most data is available and thus statistics are
more reliable. The same can be seen in the scat-
ter plot in Figure 6, where the average ? value is
always negative if the number of successions ex-
ceeds a certain threshold. For all three categories,
the figures demonstrate that the same place of ar-
ticulation is generally less frequently maintained
than expected if there were no interdependencies
between consonant co-occurrences.
5 Visualization of geo-spatial patterns
The most common approach to visually represent
crosslinguistic information on areal (or genealog-
ical) patterns is to put each language as a single
pixel or a small icon to its location on a map.
For instance, the WALS database (Haspelmath et
al., 2005) includes 141 maps on diverse structural
(phonological, grammatical, lexical) properties of
languages. We transformed the results of our SPA
statistics for each language in the ASJP database
74
P
T C
K
Figure 3: Successions of P, T, C and K in all languages. The ?+? and ??? signs indicate the polarity
of a succession (going from row to column category). The color saturation of the background indicates
the strength of association. In the left figure, places of articulation are sorted according to their position
in the oral cavity, in the middle figure an automatic similarity sorting of matrix rows and columns was
applied. The right part of the figure shows an alternative view only on those successions that have a
positive association.
l
l
l
ll
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
lll
l
l
l
l
l
l
ll
l
l
l
l
l
l
Labial?Labial Coronal?Coronal Dorsal?Dorsal
?
1.0
?
0.5
0.0
0.5
1.0
Distribution of association values across languages (all)
ll
l
l
ll
Labial?Labial Coronal?Coronal Dorsal?Dorsal
?
1.0
?
0.5
0.0
0.5
1.0
Distribution of association values across languages (top)
Figure 5: Boxplots showing the distribution of association strength values (?) for self-successions of
places of articulation. For the left boxplots about 3,200 languages were considered for which the
Swadesh lists contained more than 20 successions. For the right boxplots only the top 99 languages
were considered for which the Swadesh lists contained at least 100 successions, thereby removing most
outliers and reducing the variance.
that is also included in the WALS database into a
WALS map (Figure 7). The matrix visualization
has been simplified in that the color of the icon
represents the number of cells in the diagonal of
the matrix whose value was below zero, i.e., the
higher the number (0-3) the better the language
conforms to SPA.
Some of the drawbacks of these maps include a
high degree of overlap of data points in densely
populated areas and the lack of correlation be-
tween information content and area size. In Figure
7, the fact that those languages with fewer negative
diagonal cells are plotted on top of those with a
higher number slightly distorts the overall picture
that most languages adhere to the principle.6 Be-
sides that, the overall pattern in the densely popu-
lated areas is hardly visible, while sparsely popu-
lated areas waste space and hide the informational
6Likewise, the visualization would suggest to much ad-
herence to the principle if those languages with more nega-
tive diagonal cells were plotted on top of those with fewer
negative cells.
75
ll
lll
l
ll
l
l
l
ll
l
l
l
ll
lll
l
l
l
l
l
l
l
l
l
l
l
l
lll
ll
l
l
l
l
lll
l
l
l
l
l
l
l
l
ll
l
l
ll
l
l
l
l
lll
ll
l
l
l
l
llll
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
ll
l
ll
ll
lll
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
ll
l
ll
l
l
l
l
l
l
l
ll
l
ll
l
lll
l
l
lll
l
l
ll
ll
l
l
l
l
l
l
l
l
l
l
l
l
lll
ll
lll
l
l
l
l
l
ll
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
ll
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
l
llll
l
l
lll
l
l
l
l
l
l
l
ll
ll
l
l
ll
l
lll
l
l
l
l
l
l
l
l
l
l
l
l
l
llll
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
lll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
ll
l
ll
l
l
l
ll
l
l
l
l
l
l
l
l
ll
l
l
l
ll
ll
l
l
l
ll
l
lll
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
llll
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
ll
l
l
l
l
l
l
l
ll
l
l
ll
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
ll
l
l
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
l
ll
l
ll
l
l
l
l
l
l
l
l
l
l
ll
lll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
ll
l
l
l
ll
l
l
l
l
l
l
l
l
ll
l
l
l
ll
l
l
l
l
l
ll
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
ll
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
lll
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
ll
l
l
ll
l
l
ll
l
lll
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
ll
l
l
l
l
l
l
l
l
l
lll
l
l
l
l
ll
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
ll
l
ll
l
l
lll
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
llll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
ll
ll
l
l
l
ll
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
ll
l
l
50 100 150 200
?
0.4
?
0.2
0.0
0.2
0.4
Average phi values in dependence of data amount
Number of consonant successions in dataset
Ave
rage
 phi 
valu
e
Labial, Coronal and Dorsal
Distribution of association values for all self?successions across languages
Freq
uen
cy
?1.0 ?0.5 0.0 0.5 1.0
0
100
200
300
400
500
600
Figure 6: The scatter plot on the left displays the average ? values for self-successions of all places of
articulation depending on the number of consonant successions (CVC) for each language in the sample.
The histogram on the right shows the distribution of association strength values (?) for self-successions
of places of articulation in more than 3200 languages.
details. Finally, small clusters are difficult to find
? they are not noticeable, and are sometimes even
obscured by large clusters.
In order to avoid overlapping pixels we used
a circular arrangement around the original loca-
tion in the current analysis, taking the given order-
ing of elements into account (Bak et al, 2009a).
The ordering usually corresponds to the coloring
attribute starting with colors that occur least fre-
quently. With this arrangement a natural looking
visualization without artifacts is generated.
A way to obtain more space for regions with a
high point density are Cartograms, which distort
regions such that their size corresponds to a statis-
tical attribute (Bak et al, 2009b; Tobler, 2004), in
this case the number of languages in the database.
The advantage is that more space is reserved to
plot all important information on the map. In Fig-
ure 8, we show the density equalized distortion by
cartograms and the overlap-free representation of
the data points using pixel placement. Neighbor-
hood relations and region shapes are at the same
time maintained as accurately as possible in order
to guarantee recognizability despite of distortion.
The visualization reveals several clusters of non-
conforming languages (marked with boxes). It re-
mains for future work to investigate whether these
clusters are an artifact of the database that we used
or if they manifest an areal feature. Figure 8, in
contrast to Figure 7, shows the 3,200 languages
we investigated more closely and not just the ones
included in WALS.
The representation thereby enables investigat-
ing spatial patterns free of hidden data and distri-
butional biases.
6 Conclusions and future work
Our crosslinguistic investigation of SPA has con-
firmed the hypothesis that the phenomenon of
Similar Place Avoidance is not a particular trait
of Semitic languages, for which it was previously
described, but is a linguistic universal tendency
which can be observed in languages which are
both genealogically and geographically unrelated.
This can clearly be seen in the visualizations that
display the conformity of each language in the
database with respect to SPA. The overall pic-
ture for all languages not only shows that succes-
sive consonants with the same place of articulation
tend to be avoided, but also that there is a tendency
to avoid places of articulation that are too far away
from the preceding place (cf. Figures 3 and 4).
We combine methods from statistics, NLP and
Visual Analytics to provide a novel way of auto-
matically assessing and visualizing linguistic fea-
tures across a wide range of languages, thus al-
76
Figure 7: WALS map of the languages and their behavior with respect to SPA. The color indicates the
number of self-succession ? values which are negative, i.e., which adhere to the SPA principle. Color
mapping is from blue (conforming to SPA) to red. The numbers in square brackets indicate the number
of languages in this group.
Figure 8: Density equalized distribution of the languages with respect to SPA. The area of the geographic
regions corresponds to the number of languages in that location ? represented by dots. Overlap is avoided
using pixel-placement. The color mapping corresponds to the one used in the WALS map (Figure 7). Lo-
cations of nonconforming languages are highlighted with red boxes. Note that the number of languages
in this map is about twice the number in the WALS map (7).
77
lowing for a gain of new insights and raising fur-
ther interesting research questions that otherwise
might easily go unrecognized.
With respect to SPA a more detailed exploration
of the intricacies of phonological interdepencies is
needed as part of our more widespread study of
visually representing sound patterns in languages.
As already hinted at in Pozdniakov and Segerer
(2007), there are various other fascinating phe-
nomena that are worth looking at, especially in re-
gard to the interaction of vowels and consonants or
vowel dependencies (such as vowel harmony) and
consonant dependencies (such as SPA or conso-
nant harmony). In particular, one could investigate
why some languages apparently do not conform to
SPA and if there is any co-variation to be uncov-
ered between the adherence to the principle and
other factors that might be interesting to explore
and possibly reveal new insights into the structure
of languages.
Acknowledgments
This work has been funded by the research ini-
tiative ?Computational Analysis of Linguistic De-
velopment? at the University of Konstanz. The
authors would like to thank Aditi Lahiri and two
anonymous reviewers for valuable comments and
suggestions.
References
Peter Bak, Florian Mansmann, Halldor Janetzko, and
Daniel Keim. 2009a. Spatiotemporal analysis of
sensor logs using growth ring maps. IEEE Trans-
actions on Visualization and Computer Graphics,
15(6):913?920.
Peter Bak, Matthias Schaefer, Andreas Stoffel, Daniel
Keim, and Itzhak Omer. 2009b. Density equalizing
distortion of large geographic point sets. Journal of
Cartographic and Geographic Information Science
(CaGIS), 36(3):237?250.
Balthasar Bickel. in press. Absolute and statistical uni-
versals. In Patrick C. Hogan, editor, The Cambridge
Encyclopedia of the Language Sciences. Cambridge:
Cambridge University Press.
Paul Boersma and Bruce Hayes. 2001. Empirical tests
of the gradual learning algorithm. Linguistic In-
quiry, 32:45?86.
Paula Fikkert and Clara C. Levelt. 2010. How does
place fall into place? The lexicon and emergent con-
straints in the developing phonological grammar. In
Peter Avery, B. Elan Dresher, and Keren Rice, edi-
tors, Contrast in Phonology: Perception and Acqui-
sition. Berlin: Mouton de Gruyter.
John Goldsmith. 1976. Autosegmental phonology.
Ph.D. thesis, Massachusetts Institute of Technology.
Joseph H. Greenberg. 1950. The patterning of root
morphemes in Semitic. Word, 6:161?182.
Martin Haspelmath, Matthew S. Dryer, David Gil, and
Bernard Comrie. 2005. The World Atlas of Lan-
guage Structures Online. URL: http://wals.
info/.
Gregory K. Iverson and Joseph C. Salmonts. 1992.
The phonology of the Proto-Indo-European root
structure constraint. Lingua, 87:293?320.
Daniel A. Keim, Florian Mansmann, Joern Schnei-
dewind, Jim Thomas, and Hartmut Ziegler. 2008.
Visual analytics: Scope and challenges. In Visual
Data Mining: Theory, Techniques and Tools for Vi-
sual Analytics, Lecture Notes in Computer Science,
pages 76?91. Springer.
Wiliam R. Leben. 1973. Suprasegmental phonology.
Ph.D. thesis, Massachusetts Institute of Technology.
John J. McCarthy. 1986. OCP effects: Gemination and
antigemination. Linguistic Inquiry, 17:207?263.
Frans Plank. 1981. Morphologische (Ir-)Regularita?-
ten: Aspekte der Wortstrukturtheorie. Tu?bingen:
Gunter Narr Verlag.
Konstantin Pozdniakov and Guillaume Segerer. 2007.
Similar Place Avoidance: A statistical universal.
Linguistic Typology, 11(2):307?348.
Christian Rohrdantz, Thomas Mayer, Miriam Butt,
Frans Plank, and Daniel A. Keim. 2010. Compar-
ative visual analysis of cross-linguistic features. In
Proceedings of the International Symposium on Vi-
sual Analytics Science and Technology (EuroVAST
2010), pages 27?32.
Rudolph J. Rummel. 1970. Applied Factor Analysis.
Evanston, IL: Nortwestern University Press.
James J. Thomas and Kristin A. Cook. 2005. Illu-
minating the Path: The Research and Development
Agenda for Visual Analytics. National Visualization
and Analytics Ctr.
Waldo Tobler. 2004. Thirty five years of computer
cartograms. Association of American Geographer,
94(1):58?73.
78
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 61?69,
Beijing, August 2010
Towards multi-lingual summarization: A comparative analysis of
sentence extraction methods on English and Hebrew corpora
Marina Litvak and Hagay Lipman and Assaf Ben Gur and Mark Last
Ben Gurion University of the Negev
{litvakm, lipmanh, bengura, mlast}@bgu.ac.il
Slava Kisilevich and Daniel Keim
University of Konstanz
slaks@dbvis.inf.uni-konstanz.de
Daniel.Keim@uni-konstanz.de
Abstract
The trend toward the growing multi-
linguality of the Internet requires text
summarization techniques that work
equally well in multiple languages. Only
some of the automated summarization
methods proposed in the literature, how-
ever, can be defined as ?language-
independent?, as they are not based on
any morphological analysis of the sum-
marized text. In this paper, we per-
form an in-depth comparative analysis of
language-independent sentence scoring
methods for extractive single-document
summarization. We evaluate 15 pub-
lished summarization methods proposed
in the literature and 16 methods intro-
duced in (Litvak et al, 2010). The eval-
uation is performed on English and He-
brew corpora. The results suggest that
the performance ranking of the com-
pared methods is quite similar in both
languages. The top ten bilingual scoring
methods include six methods introduced
in (Litvak et al, 2010).
1 Introduction
Automatically generated summaries can signif-
icantly reduce the information overload on pro-
fessionals in a variety of fields, could prove ben-
eficial for the automated classification and fil-
tering of documents, the search for information
over the Internet and applications that utilize
large textual databases.
Document summarization methodologies in-
clude statistic-based, using either the classic vec-
tor space model or a graph representation, and
semantic-based, using ontologies and language-
specific knowledge (Mani & Maybury, 1999).
Although the use of language-specific knowl-
edge can potentially improve the quality of auto-
mated summaries generated in a particular lan-
guage, its language specificity ultimately re-
stricts the use of such a summarizer to a sin-
gle language. Only systems that perform equally
well on different languages in the absence of any
language-specific knowledge can be considered
language-independent summarizers.
As the number of languages used on the In-
ternet increases continiously (there are at least
75 different languages according to a estimate
performed by A. Gulli and A. Signorini1 in the
end of January 2005), there is a growing need
for language-independent statistical summariza-
tion techniques that can be readily applied to text
in any language without using language-specific
morphological tools.
In this work, we perform an in-depth com-
parative analysis of 16 methods for language-
independent extractive summarization intro-
duced in (Litvak et al, 2010) that utilize ei-
ther vector or graph-based representations of text
documents computed from word segmentation
and 15 state-of-the art language-independent
scoring methods. The main goal of the eval-
uation experiments, which focused on English
and Hebrew corpora, is to find the most efficient
language-independent sentence scoring methods
1http://www.cs.uiowa.edu/ asignori/web-size/
61
in terms of summarization accuracy and com-
putational complexity across two different lan-
guages.
This paper is organized as follows. The
next section describes related work in extrac-
tive summarization. Section 3 reviews the evalu-
ated language-independent sentence scoring ap-
proaches. Section 4 contains our experimental
results on English and Hebrew corpora. The last
section comprises conclusions and future work.
2 Related Work
Extractive summarization is aimed at the selec-
tion of a subset of the most relevant fragments,
which can be paragraphs, sentences, keyphrases,
or keywords from a given source text. The ex-
tractive summarization process usually involves
ranking, such that each fragment of a summa-
rized text gets a relevance score, and extraction,
during which the top-ranked fragments are ex-
tracted and arranged in a summary in the same
order they appeared in the original text. Statisti-
cal methods for calculating the relevance score
of each fragment can rely on such informa-
tion as: fragment position inside the document,
its length, whether it contains keywords or title
words.
Research by Luhn (1958), in which the sig-
nificance factor of a sentence is based on the
frequency and the relative position of significant
words within that sentence, is considered the first
on automated text summarization. Luhn?s work
was followed shortly thereafter by that of Ed-
mundson (1969) and some time later by stud-
ies from Radev et al (2001) and Saggion et al
(2003), all of who applied linear combinations
of multiple statistical methods to rank sentences
using the vector space model as a text representa-
tion. In (Litvak et al, 2010) we improve the sum-
marization quality by identifying the best linear
combination of the metrics evaluated in this pa-
per.
Several information retrieval and machine
learning techniques have been proposed for de-
termining sentence importance (Kupiec et al,
1995; Wong et al, 2008). Gong and Liu (2001)
and Steinberger and Jezek (2004) showed that
singular value decomposition (SVD) can be ap-
plied to generate extracts.
Among text representation models, graph-
based text representations have gained popular-
ity in automated summarization, as they enable
the model to be enriched with syntactic and se-
mantic relations. Salton et al (1997) were
among the first to attempt graph-based ranking
methods for single document extractive summa-
rization by generating similarity links between
document paragraphs. The important paragraphs
of a text were extracted using degree scores.
Erkan and Radev (2004) and Mihalcea (2005) in-
troduced approaches for unsupervised extractive
summarization that rely on the application of it-
erative graph based ranking algorithms. In their
approaches, each document is represented as a
graph of sentences interconnected by similarity
relations.
3 Language-Independent Scoring
Methods for Sentence Extraction
Various language dependent and independent
sentence scoring methods have been introduced
in the literature. We selected the 15 most promi-
nent language independent methods for evalua-
tion. Most of them can be categorized as fre-
quency, position, length, or title-based, and they
utilize vector representation. TextRank (ML TR)
is the only method that is based on graph repre-
sentation, but there are also position and length-
based methods that calculate scores using the
overall structure of a document. We have also
considered 16 methods proposed in (Litvak et al,
2010), including 13 based on the graph-theoretic
representation (Section 3.1).
Figure 1 (Litvak et al, 2010) shows the taxon-
omy of the 31 methods considered in our work.
All methods introduced in (Litvak et al, 2010)
are denoted by an asterisk (*). Methods requir-
ing a threshold value t ? [0, 1] that specifies the
portion of the top rated terms considered signifi-
cant are marked by a cross in Figure 1 and listed
in Table 1 along with the optimal average thresh-
old values obtained after evaluating the methods
62
Table 1: Selected thresholds for threshold-based
scoring methods
Method Threshold
LUHN 0.9
LUHN DEG 0.9
LUHN PR 0.0
KEY [0.8, 1.0]
KEY DEG [0.8, 1.0]
KEY PR [0.1, 1.0]
COV 0.9
COV DEG [0.7, 0.9]
COV PR 0.1
on English and Hebrew documents (Litvak et al,
2010).
The methods are divided into three main cat-
egories: structure-, vector-, and graph-based
methods, and each category also contains an
internal taxonomy. Sections 3.2, 3.3, and
3.4 present structure-, vector-, and graph-based
methods, respectively. With each description, a
reference to the original work where the method
was proposed for extractive summarization is in-
cluded. We denote sentence by S and text docu-
ment by D.
3.1 Text Representation Models
The vector-based scoring methods listed below
use tf or tf-idf term weights to evaluate sen-
tence importance while that used by the graph-
based methods (except for TextRank) is based
on the word-based graph representation model
presented in Schenker et al (2004). We repre-
sent each document by a directed, labeled, un-
weighted graph in which nodes represent unique
terms (distinct normalized words) and edges rep-
resent order-relationships between two terms.
Each edge is labeled with the IDs of sentences
that contain both words in the specified order.
3.2 Structure-based Scoring Methods
In this section, we describe the existing
structure-based methods for multilingual sen-
tence scoring. These methods do not require any
text representation and are based on its structure.
? Position (Baxendale, 1958):
POS L Closeness to the end of the document:
score(Si) = i, where i is a sequential number of
a sentence in a document;
POS F Closeness to the beginning of the docu-
ment: score(Si) = 1i ;
POS B Closeness to the borders of the docu-
ment: score(Si) = max(1i , 1n?i+1), where n isthe total number of sentences in D.
? Length (Satoshi et al, 2001):
LEN W Number of words in a sentence;
LEN CH Number of characters in a sentence.
3.3 Vector-based Scoring Methods
In this section, we describe the vector-based
methods for multilingual sentence scoring, that
are based on the vector space model for text rep-
resentation.
? Frequency-based:
LUHN (Luhn, 1958)
score(S) = maxci?{clusters(S)}{csi}, where
clusters are portions of a sentence brack-
eted by keywords2 and csi = |keywords(ci)|
2
|ci| .KEY (Edmundson, 1969) Sum of the keyword
frequencies: score(S) = ?i?{keywords(S)} tfi,
where tfi is term in-document frequency of
keyword i.
COV (Kallel et al, 2004) Ratio of keyword
numbers (Coverage): score(S) = |keywords(S)||keywords(D)|TF (Vanderwende et al, 2007) Average term
frequency for all sentence words:
score(S) =
?
i?{words(S)} tfi
|S| .TFISF (Neto et al, 2000) Average term
frequency inverted sentence frequency
for all sentence words: score(S) =?
i?{words(S)} tfi ? isfi,
where isfi = 1? log(ni)log(n) , where n is the numberof sentences in a document and ni is the number
of sentences containing word i.
SVD (Steinberger & Jezek, 2004) score(S)
is equal to the length of a sentence vector
in ?2V T after computing the Singular Value
Decomposition of a term by sentence matrix
A = U?V T
? Title (Edmundson, 1969) similarity3 to the
title, score(S) = sim(S, T ):
TITLE O using overlap similarity: |S?T |min{|S|,|T |}
TITLE J using Jaccard similarity: |S?T ||S?T |
2Luhn?s experiments suggest an optimal limit of 4 or 5
non-significant words between keywords.
3Due to multilingual focus of our work, exact word
matching was used in all similarity-based methods.
63
Multilingual sentencescoringmethods
Structure-based Vector-based Graph-based
Position Length Frequency Similarity Degree SimilarityPagerank
Title DocumentPOS_FPOS_LPOS_B
LEN_WLEN_CH
LUHN?KEY ?COV ?TFTFIISFSVD
TITLE_OTITLE_JTITLE_C
D_COV_O*D_COV_J*D_COV_C*
LUHN_DEG ? *KEY_DEG ? *COV_DEG ? *DEG*GRASE*
LUHN_PR ? *KEY_PR ? *COV_PR ? *PR*ML_TR
Title Document
TITLE_E_O*TITLE_E_J* D_COV_E_O*D_COV_E_J*
Figure 1: Taxonomy of statistical language-independent sentence scoring methods (Litvak et al,
2010)
TITLE C using cosine similarity:
sim(~S, ~T ) = cos(~S, ~T ) = ~S?~T|~S|?|~T |
? Document Coverage (Litvak et al, 2010).
These methods score a sentence according to
its similarity to the rest of the sentences in
the document (D ? S) based on the following
intuition: the more document content is covered
by a sentence, the more important the sentence is
to a summary. Redundant sentences containing
repetitive information are removed using a
similarity filter. score(S) = sim(S,D ? S):
D COV O using Overlap similarity:
|S?T |
min{|S|,|D?S|}
D COV J using Jaccard similarity: |S?T ||S?D?S|D COV C using Cosine similarity:
cos(~S, ~D ? S) = ~S? ~D?S|~S|?| ~D?S|
3.4 Graph-based Scoring Methods
In this section, we describe the methods for mul-
tilingual sentence scoring using the graph text
representation based on sentence (ML TR) or
word (all except ML TR) segmentation.
ML TR Multilingual version of Tex-
tRank (Mihalcea, 2005) without morphological
analysis. Each document is represented as a
directed graph of nodes that stand for sen-
tences interconnected by similarity (overlap)
relationship. To each edge connecting two
vertices the weight is assigned and equal to
the similarity value between the corresponding
sentences. We used backward links, as it was
the most successful according to the reported
results in (Mihalcea, 2005). score(S) is equal
to PageRank (Brin & Page, 1998) of its node,
according to the formula adapted to the weights
assigned to edges.
? Degree-based (Litvak et al, 2010):4
LUHN DEG A graph-based extension of the
LUHN measure, in which a node degree is
used instead of a word frequency: words are
considered significant if they are represented
by nodes of a higher degree than a predefined
threshold (see Table 1).
KEY DEG Graph-based extension of KEY
measure.
COV DEG Graph-based extension of COV
measure.
DEG Average degree for all sentence nodes:
score(S) =
?
i?{words(S)}Degi
|S| .GRASE(GRaph-based Automated Sentence
Extractor) Modification of Salton?s algo-
rithm (Salton et al, 1997) using the graph
4All proposed here degree-based methods, except for
GRASE, use undirected graphs and degree of nodes as a
predictive feature. The methods based on the directed word
graphs and distinguishing between in- and out-links were
outperformed in our preliminary experiments by the undi-
rected approach.
64
representation defined in Section 3.1 above.
In our graph representation, all sentences are
represented by paths, completely or partially.
To identify the relevant sentences, we search
for the bushy paths and extract from them the
sentences that appear the most frequently. Each
sentence in the bushy path gets a domination
score that is the number of edges with its label
in the path normalized by the sentence length.
The relevance score for a sentence is calculated
as a sum of its domination scores over all paths.
? PageRank-based:5
LUHN PR A graph-based extension of the
LUHN measure in which the node PageRank
value is used instead of the word frequency:
keywords are those words represented by nodes
with a PageRank score higher than a predefined
threshold (see Table 1).
KEY PR Graph-based extension of KEY mea-
sure.
COV PR Graph-based extension of COV mea-
sure.
PR Average PageRank for all sentence nodes:
score(S) =
?
i?{words(S)} PRi
|S| .? Similarity-based. Edge matching techniques
similar to those of Nastase and Szpakowicz
(2006) are used. Edge matching is an alternative
approach to measure the similarity between
graphs based on the number of common edges:
TITLE E O Graph-based extension of TI-
TLE O ? Overlap-based edge matching between
title and sentence graphs.
TITLE E J Graph-based extension of TITLE J
? Jaccard-based edge matching between title
and sentence graphs.
D COV E O Graph-based extension of
D COV O ? Overlap-based edge matching
between sentence and document complement
(the rest of a document sentences) graphs.
D COV E J Graph-based extension of
D COV J ? Jaccard-based edge matching
5Using undirected word graphs with PageRank does not
make sense, since for an undirected graph a node pagerank
score is known to be proportional to its degree. Revers-
ing links will result in hub scores instead authority. The
methods distinguishing between authority and hub scores
were outperformed in our preliminary experiments by the
degree-based approach.
between sentence and document complement
graphs.
4 Experiments
4.1 Overview
The quality of the above-mentioned sentence
ranking methods was evaluated through a com-
parative experiment on corpora of English and
Hebrew texts. These two languages, which
belong to different language families (Indo-
European and Semitic languages, respectively),
were intentionally chosen for this experiment to
increase the generality of our evaluation. The
main difference between these languages, is that
Hebrew morphology allows morphemes to be
combined systematically into complex word-
forms. In different contexts, the same morpheme
can appear as a separate word-form, while in oth-
ers it appears agglutinated as a suffix or prefix to
another word-form (Adler, 2009).
The goals of the experiment were as follows:
- To evaluate the performance of different ap-
proaches for extractive single-document summa-
rization using graph and vector representations.
- To compare the quality of the multilingual sum-
marization methods proposed in our previous
work (Litvak et al, 2010) to the state-of-the-art
approaches.
- To identify sentence ranking methods that work
equally well on both languages.
4.2 Text Preprocessing
Extractive summarization relies critically on
proper sentence segmentation to insure the qual-
ity of the summarization results. We used a sen-
tence splitter provided with the MEAD summa-
rizer (Radev et al, 2001) for English and a sim-
ple splitter for Hebrew splitting the text at every
period, exclamation point, or question mark.6
4.3 Experimental Data
For English texts, we used the corpus of sum-
marized documents provided for the single doc-
6Although the same set of splitting rules may be used
for both languages, separate splitters were used since the
MEAD splitter is restricted to European languages.
65
ument summarization task at the Document
Understanding Conference 2002 (DUC, 2002).
This benchmark dataset contains 533 news arti-
cles, each of which is at least ten sentences long
and has two to three human-generated abstracts
of approximately 100 words apiece.
However, to the best of our knowledge, no
summarization benchmarks exist for the Hebrew
language texts. To collect summarized texts in
Hebrew, we set up an experiment7 in which 50
news articles of 250 to 830 words each from the
Haaretz8 newspaper internet site were summa-
rized by human assessors by extracting the most
salient sentences. In total, 70 undergraduate stu-
dents from the Department of Information Sys-
tems Engineering, Ben Gurion University of the
Negev participated in the experiment. Ten doc-
uments were randomly assigned to each of the
70 study participants who were instructed (1)
To dedicate at least five minutes to each doc-
ument, (2) To ignore dialogs and citations, (3)
To read the whole document before starting sen-
tence extraction, (4) To ignore redundant, repet-
itive, or overly detailed information, (5) To obey
the minimal and maximal summary constraints
of 95 and 100 words, respectively. Summaries
were assessed for quality by procedure described
in (Litvak et al, 2010).
4.4 Experimental Results
We evaluated English and Hebrew summaries
using the ROUGE-1, 2, 3, 4, L, SU and W met-
rics9, described in Lin (2004). Our results were
not statistically distinguishable and matched the
conclusion of Lin (2004). However, because
ROUGE-1 showed the largest variation across
the methods, all results in the following com-
parisons are presented in terms of ROUGE-1
metric. Similar to the approach described
in Dang (2006), we performed multiple com-
parisons between the sentence scoring methods.
The Friedman test was used to reject the null hy-
7The software enabling easy selection and storage of
sentences to be included in the document extract, can be
provided upon request.
8http://www.haaretz.co.il
9ROUGE toolkit was adapted to Hebrew by specifying
?token? using Hebrew alphabet
Table 2: English: Multiple comparisons of sen-
tence ranking approaches using the Bonferroni-
Dunn test of ROUGE-1 Recall
Approach ROUGE-1
COV DEG? 0.436 A
KEY DEG? 0.433 A B
KEY 0.429 A B C
COV PR? 0.428 A B C D
COV 0.428 A B C D
D COV C? 0.428 A B C D
D COV J? 0.425 B C D E
KEY PR? 0.424 B C D E
LUHN DEG? 0.422 C D E F
POS F 0.419 E F G
LEN CH 0.418 C D E F G
LUHN 0.418 D E F G
LUHN PR? 0.418 E F G H
LEN W 0.416 D E F G H
ML TR 0.414 E F G H
TITLE E J? 0.413 F G H I
TITLE E O? 0.413 F G H I
D COV E J? 0.410 F G H I
D COV O? 0.405 G H I J
TFISF 0.405 G H I J
DEG? 0.403 G H I J
D COV E O? 0.401 H I J K
PR? 0.400 G H I J K
TITLE J 0.399 I J K
TF 0.397 I J K
TITLE O 0.396 J K
SVD 0.395 I J K
TITLE C 0.395 J K
POS B 0.392 K L
GRASE? 0.372 L
POS L 0.339 M
pothesis (all methods perform the same) at the
0.0001 significance level, after which we ran the
Bonferroni-Dunn test (Demsar, 2006) for pair-
wise comparisons. Tables 2 and 3 show the re-
sults of multiple comparisons and are arranged
in descending order with the best approaches
on top. Methods not sharing any common let-
ter were significantly different at the 95% confi-
dence level.
The Pearson correlation between methods
ranking in English and Hebrew was 0.775, which
was larger than zero at a significance level of
0.0001. In other words, most of the methods
were ranked in nearly the same relative positions
in both corpora, and the top ranked methods per-
formed equally well in both languages. The dif-
ferences in ranking were caused by morphologi-
cal differences between two languages.
To determine which approaches performed
best in both languages, we analyzed the cluster-
ing results of the methods in both corpora and
found the intersection of the top clusters from
the two clustering results. For each language,
a document-method matrix of ROUGE scores
was created with methods represented by vec-
tors of their ROUGE scores for each document
in a corpora. Since most scores are not normally
66
Table 3: Hebrew: Multiple comparisons of sen-
tence ranking approaches using the Bonferroni-
Dunn test of ROUGE-1 Recall
Approach ROUGE-1
D COV J? 0.574 A
KEY 0.570 A B
COV DEG? 0.568 A B
POS F 0.567 A B
COV 0.567 A B
TITLE J 0.567 A B
POS B 0.565 A B
LUHN PR? 0.560 A B C
LUHN DEG? 0.560 A B C
D COV E J? 0.559 A B C
LUHN 0.559 A B C
TITLE E J? 0.556 A B C
TITLE E O? 0.556 A B C
KEY DEG? 0.555 A B C
LEN W 0.555 A B C
LEN CH 0.553 A B C
KEY PR? 0.546 A B C
COV PR? 0.546 A B C
TITLE O 0.545 A B C
D COV C? 0.543 A B C
TITLE C 0.541 A B C
ML TR 0.519 A B C D
TFISF 0.514 A B C D
D COV E O? 0.498 A B C D
SVD 0.498 A B C D
D COV O? 0.466 B C D
TF 0.427 C D E
DEG? 0.399 D E F
PR? 0.331 E F
GRASE? 0.243 F
POS L 0.237 F
Table 4: English: Correlation between sentence
ranking approaches using Pearson
Approach Correlated With
POS F (LUHN PR, 0.973), (TITLE E J, 0.902), (TITLE E O, 0.902)
TITLE O (TITLE J, 0.950)
LEN W (LEN CH, 0.909)
KEY PR (COV PR, 0.944)
TITLE E O (TITLE E J, 0.997)
distributed, we chose the K-means algorithm,
which does not assume normal distribution of
data, for clustering. We ran the algorithm with
different numbers of clusters (2 ? K ? 10),
and for each K, we measured two parameters:
the minimal distance between neighboring clus-
ters in the clustered data for each language and
the level of similarity between the clustering re-
sults for the two languages. For both param-
eters, we used the regular Euclidean distance.
For K ? 6, the clusters were highly similar
for each language, and the distance between En-
glish and Hebrew clustering data was maximal.
Based on the obtained results, we left results
only for 2 ? K ? 5 for each corpus. Then,
we ordered the clusters by the average ROUGE
score of each cluster?s instances (methods) and
identified the methods appearing in the top clus-
ters for all K values in both corpora. Table 6
shows the resulting top ten scoring methods with
their rank in each corpus. Six methods intro-
Table 5: Hebrew: Correlation between sentence
ranking approaches using Pearson
Approach Correlated With
KEY (KEY DEG, 0.930)
COV (D COV J, 0.911)
POS F (POS B, 0.945), (LUHN DEG, 0.959), (LUHN PR, 0.958)
POS B (LUHN DEG, 0.927), (LUHN PR, 0.925)
TITLE O (TITLE E J, 0.920), (TITLE E O, 0.920)
TITLE J (TITLE E J, 0.942), (TITLE E O, 0.942)
LEN W (LEN CH, 0.954), (KEY PR, 0.912)
LEN CH (KEY PR, 0.936), (KEY DEG, 0.915), (COV DEG, 0.901)
LUHN DEG (LUHN PR, 0.998)
KEY DEG (COV DEG, 0.904)
Table 6: Ranking of the best bilingual scores
Scoring Rank in Rank in Text
method English corpus Hebrew corpus Representation
KEY 3 2 vector
COV 4 4 vector
KEY DEG 2 10 graph
COV DEG 1 3 graph
KEY PR 6 12 graph
COV PR 4 12 graph
D COV C 4 14 vector
D COV J 5 1 vector
LEN W 10 10 structure
LEN CH 9 11 structure
duced in this paper, such as Document Cover-
age (D COV C/J) and graph adaptations of Cov-
erage (COV DEG/PR) and Key (KEY DEG/PR),
are among these top ten bilingual methods.
Neither vector- nor graph-based text represen-
tation models, however, can claim ultimate supe-
riority, as methods based on both models promi-
nently in the top-evaluated cluster. Moreover,
highly-correlated methods (see Tables 4 and 5
for highly-correlated pairs of methods in English
and Hebrew corpora, respectively) appear in the
same cluster in most cases. As a result, some
pairs from among the top ten methods are highly-
correlated in at least one language, and only one
from each pair can be considered. For example,
LEN W and LEN CH have high correlation coef-
ficients (0.909 and 0.954 in English and Hebrew,
respectively). Since LEN CH is more appropri-
ate for multilingual processing due to variations
in the rules of tokenization between languages
(e.g., English vs. German), it may be considered
a preferable multilingual metric.
In terms of summarization quality and com-
putational complexity, all scoring functions pre-
sented in Table 6 can be considered to perform
equally well for bilingual extractive summariza-
tion. Assuming their efficient implementation,
all methods have a linear computational com-
plexity, O(n), relative to the total number of
words in a document. KEY PR and COV PR re-
67
quire additional O(c(|E|+|V |)) time for running
PageRank, where c is the number of iterations it
needs to converge, |E| is the number of edges,
and |V | is the number of nodes (distinct words)
in a document graph. Since neither |E| nor |V | in
our graph representation can be as large as n, the
total computation time for KEY PR and COV PR
metrics is also linear relative to the document
size.
In terms of implementation complexity,
LEN W and LEN CH are simpliest, since they
even do not require any preprocessing and repre-
sentation building; KEY and COV require key-
words identification; D COV C, and D COV J
require vector space model building; KEY DEG
and COV DEG need graphs building (order of
words); whereas KEY PR and COV PR, in ad-
dition, require PageRank implementation.
5 Conclusion and Future Research
In this paper, we conducted in-depth, compar-
ative evaluations of 31 existing (16 of which
are mostly graph-based modifications of exist-
ing state-of-the-art methods, introduced in (Lit-
vak et al, 2010)) scoring methods10 using En-
glish and Hebrew language texts.
The experimental results suggest that the rel-
ative ranking of methods performance is quite
similar in both languages. We identified meth-
ods that performed significantly better in only
one of the languages and those that performed
equally well in both languages. Moreover, al-
though vector and graph-based approaches were
among the top ranked methods for bilingual ap-
plication, no text representation model presented
itself as markedly superior to the other.
Our future research will extend the evaluations
of language-independent sentence ranking met-
rics to a range of other languages such as Ger-
man, Arabic, Greek, and Russian. We will adapt
similarity-based metrics to multilingual applica-
tion by implementing them via n-gram matching
instead of exact word matching. We will fur-
ther improve the summarization quality by ap-
10We will provide the code for our summarizer upon re-
quest.
plying machine learning on described features.
We will use additional techniques for summary
evaluation and study the impact of morpholog-
ical analysis on the top ranked bilingual scores
using part-of-speech (POS) tagging11, anaphora
resolution, named entity recognition, and taking
word sense into account.
Acknowledgments
We are grateful to Michael Elhadad and Galina
Volk for providing the ROUGE toolkit adapted
to Hebrew alphabet.
References
Adler, M. (2009). Hebrew morphological
disambiguation: An unsupervised stochas-
tic word-based approach. Dissertation.
http://www.cs.bgu.ac.il/ adlerm/dat/thesis.pdf.
Baxendale, P. (1958). Machine-made index for
technical literature-an experiment. IBM Jour-
nal of Research and Development, 2, 354?361.
Brin, S., & Page, L. (1998). The anatomy of
a large-scale hypertextual web search engine.
Computer networks and ISDN systems, 30,
107?117.
Dang, H. T. (2006). Overview of DUC 2006.
Proceedings of the Document Understanding
Conference.
Demsar, J. (2006). Statistical comparisons of
classifiers over multiple data sets. Journal of
Machine Learning Research, 7, 1?30.
DUC (2002). Document understanding confer-
ence. http://duc.nist.gov.
Edmundson, H. P. (1969). New methods in auto-
matic extracting. J. ACM, 16.
Erkan, G., & Radev, D. R. (2004). LexRank:
Graph-based lexical centrality as salience in
text summarization. Journal of Artificial In-
telligence Research, 22, 457?479.
11Our experiments have shown that syntactic filters,
which select only lexical units of a certain part of speech,
do not significantly improve the performance of the evalu-
ated bilingual scoring methods.
68
Gong, Y., & Liu, X. (2001). Generic text summa-
rization using relevance measure and latent se-
mantic analysis. Proceedings of the 24th ACM
SIGIR conference on Research and develop-
ment in information retrieval (pp. 19?25).
Kallel, F. J., Jaoua, M., Hadrich, L. B., &
Hamadou, A. B. (2004). Summarization at
LARIS laboratory. Proceedings of the Doc-
ument Understanding Conference.
Kupiec, J., Pedersen, J., & Chen, F. (1995). A
trainable document summarizer. Proceedings
of the 18th annual international ACM SIGIR
conference (pp. 68?73).
Lin, C.-Y. (2004). ROUGE: A package for au-
tomatic evaluation of summaries. Proceedings
of the ACL?04 Workshop: Text Summarization
Branches Out (pp. 74?81).
Litvak, M., Last, M., & Friedman, M. (2010). A
new approach to improving multilingual sum-
marization using a genetic algorithm. Pro-
ceedings of the Association for Computational
Linguistics (ACL) 2010. Uppsala, Sweden.
Luhn, H. P. (1958). The automatic creation of
literature abstracts. IBM Journal of Research
and Development, 2, 159?165.
Mani, I., & Maybury, M. (1999). Advances in
automatic text summarization.
Mihalcea, R. (2005). Language independent ex-
tractive summarization. AAAI?05: Proceed-
ings of the 20th national conference on Artifi-
cial intelligence (pp. 1688?1689).
Nastase, V., & Szpakowicz, S. (2006). A study
of two graph algorithms in topic-driven sum-
marization. Proceedings of the Workshop
on Graph-based Algorithms for Natural Lan-
guage.
Neto, J., Santos, A., Kaestner, C., & Freitas, A.
(2000). Generating text summaries through
the relative importance of topics. Lecture
Notes in Computer Science, 300?309.
Radev, D., Blair-Goldensohn, S., & Zhang, Z.
(2001). Experiments in single and multidocu-
ment summarization using MEAD. First Doc-
ument Understanding Conference.
Saggion, H., Bontcheva, K., & Cunningham, H.
(2003). Robust generic and query-based sum-
marisation. EACL ?03: Proceedings of the
tenth conference on European chapter of the
Association for Computational Linguistics.
Salton, G., Singhal, A., Mitra, M., & Buckley, C.
(1997). Automatic text structuring and sum-
marization. Information Processing and Man-
agement, 33, 193?207.
Satoshi, C. N., Satoshi, S., Murata, M., Uchi-
moto, K., Utiyama, M., & Isahara, H. (2001).
Sentence extraction system assembling mul-
tiple evidence. Proceedings of 2nd NTCIR
Workshop (pp. 319?324).
Schenker, A., Bunke, H., Last, M., & Kandel,
A. (2004). Classification of web documents
using graph matching. International Journal
of Pattern Recognition and Artificial Intelli-
gence, 18, 475?496.
Steinberger, J., & Jezek, K. (2004). Text sum-
marization and singular value decomposition.
Lecture Notes in Computer Science, 245?254.
Vanderwende, L., Suzuki, H., Brockett, C., &
Nenkova, A. (2007). Beyond SumBasic: Task-
focused summarization with sentence simplifi-
cation and lexical expansion. Information pro-
cessing and management, 43, 1606?1618.
Wong, K., Wu, M., & Li, W. (2008). Ex-
tractive summarization using supervised and
semi-supervised learning. Proceedings of the
22nd International Conference on Computa-
tional Linguistics (pp. 985?992).
69
Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 7?15,
Avignon, France, April 23 - 24 2012. c?2012 Association for Computational Linguistics
Lexical Semantics and Distribution of Suffixes ? A Visual Analysis
Christian Rohrdantz1 Andreas Niekler2 Annette Hautli1 Miriam Butt1 Daniel A. Keim1
1 University of Konstanz
first.last@uni-konstanz.de
2Leipzig University of Applied Sciences
aniekler@fbm.htwk-leipzig.de
Abstract
We present a quantitative investigation of
the cross-linguistic usage of some (rel-
atively) newly minted derivational mor-
phemes. In particular, we examine the lexi-
cal semantic content expressed by three suf-
fixes originating in English: -gate, -geddon
and -athon. Using data from newspa-
pers, we look at the distribution and lex-
ical semantic usage of these morphemes
not only within English, but across sev-
eral languages and also across time, with
a time-depth of 20 years. The occurrence
of these suffixes in available corpora are
comparatively rare, however, by investigat-
ing huge amounts of data, we are able to
arrive at interesting insights into the dis-
tribution, meaning and spread of the suf-
fixes. Processing and understanding the
huge amounts of data is accomplished via
visualization methods that allow the pre-
sentation of an overall distributional pic-
ture, with further details and different types
of perspectives available on demand.
1 Introduction
It is well-known that parts of a compound can be-
gin to lead an additional life as derivational suf-
fixes, or even as stand-alone items. A famous
example is burger, which is now used to denote
a food-item (e.g., burger, cheese burger, veggie
burger) and is originally from the word Ham-
burger, which designates a person from the Ger-
man city of Hamburg. These morphemes are gen-
erally known as cranberry morphemes (because
of the prolific use of cran). Some other examples
are -(o)nomics, -(o)mat or (o)rama.
While it is well-known that this morpholog-
ical process exists, it is less clear what condi-
tions trigger it and how the coinage ?catches? on
to become a regular part of a language. Given
the current availability of huge amounts of dig-
ital data, we decided to investigate whether we
could gain an insight into the use and spread of
some of these morphemes via quantitative meth-
ods, thereby confirming our intuitions.
Furthermore, we decided to focus not just on
the use of the cranberry morphemes in their lan-
guage of origin, but also on their use and spread in
other languages. In particular, we want to model
the contexts in which these suffixes are used to
coin new words and how these neologisms trans-
port to other languages. We chose to look at the
following three morphemes: -gate, -geddon and
-athon because they tend to be used in ?newswor-
thy? contexts and are therefore likely to appear
in newswire and newspaper corpora, which are
available to us in large amounts.
This paper describes work in progress, where
we visually analyze the lexical semantics and use
of the three suffixes -gate, -geddon and -athon.
We were able to add some time-depth to our in-
vestigation via an analysis of the New York Times
corpus from 1987?2007. This means that while
we cannot pin-point the first occurrence and fur-
ther spread of the morpheme uses, we can gain
some idea as to their historical development.
Given that the amount of data we analyze is
huge, we use methods from Visual Analytics in
order to make the vast amount of information gen-
erated from the computational models easily ac-
cessible to the human eye and mind.
We proceed as follows: After a review of re-
lated work in Section 2, we describe our study in
Section 3 and discuss the visual analysis in Sec-
tion 4. In a case study we compare the meaning of
7
words with the suffix -gate to other semantically
related words (4.1) based on an optimized topic
model. We also develop, customize and apply vi-
sualizations to investigate the productivity of new
suffixes and their spread across news sources and
languages (4.2). We conclude with Section 5.
2 Related Work
As already mentioned, the coinage and spread of
new suffixes is well-known in theoretical linguis-
tics. However, linguists are generally not sure
what effects exactly are involved in the process
(Baayen, 1992; Plag, 1999). We are not aware of
any other computational work on cranberry mor-
phemes. Work by Lu?deling and Evert (2005) on
the German non-medical suffix -itis is closest to
this paper; however, the type of the morpheme in-
vestigated is different and their focus is mainly on
productivity. We concentrate more on the lexi-
cal semantic content of the suffixes, look at them
across languages in bigger corpora to investigate
their distribution and use and provide a layer of
visual analysis.
One question we asked ourselves is whether
we could predict from the context the likelihood
of the suffixes -gate, -geddon and -athon and
whether one can identify the lexical semantic con-
tent of the suffixes more precisely. This task can
be formulated as a topic modeling problem for
which we chose to employ Latent Dirichlet Al-
location (LDA) (Blei et al, 2003). It has recently
been used to perform word sense induction from
small word contexts (e.g. Brody (2009)) and has
also proven successful when detecting changes in
word meanings over time on small word contexts
in diachronic corpora (Rohrdantz et al, 2011).
We applied an optimized topic model and com-
bined the statistical results with methods from
Visual Analytics. Visual Analytics is based on
the tight coupling of algorithms for automatic
data analysis and interactive visual components
(Thomas and Cook, 2005; Keim et al, 2010). The
idea is to exploit human perceptive abilities to
support the detection of interesting patterns (see
Card et al (1999) for details). Examples for visu-
alizations used previously to investigate linguis-
tic questions are Mayer et al (2010a) on vowel
harmony, Mayer et al (2010b) on consonant pat-
terns, Honkela et al (1995) on syntactic cate-
gories, Rohrdantz et al (2011) on lexical seman-
tics across time.
We also used visualizations to look at cross-
linguistic use and productivity of the suffixes.
Prominent theoretical work on the productivity of
morphemes has been done by Baayen (1992) and
Plag (1999), most computational approaches have
worked on English due to the availability of large
enough corpora (Nishimoto, 2004). To the best of
our knowledge, no large-scale quantitative study
has been performed which takes into account both
the diachronic as well as the cross-linguistic di-
mension of the development.
3 Our Approach
3.1 Research Questions & Analysis Tasks
The object of research are three productive suf-
fixes, namely -gate, geddon and -athon. What
these suffixes have in common is that they trig-
ger neologisms in various languages and all of
them seem to carry some lexical semantic infor-
mation. Whereas -gate, which was coined by the
Watergate affair, is used for scandalous events or
affairs, -geddon seems to denote a similar con-
cept but more of a disastrous event, building on its
original use in the bible. Usually, -athon, coming
from marathon, denotes a long-lasting event. We
assume that the lexical semantic content of these
suffixes can be modeled with standard topic mod-
els.
3.2 Data & Statistics
Our investigations are based on two different data
sets, one is a diachronic news corpus, the New
York Times Annotated Corpus1 containing 1.8
million newspaper articles from 1987 to 2007. To
generate the second data set, we performed an on-
line scan of the EMM news service,2 which links
to multilingual news articles from all over the
world and enriches them with metada (Atkinson
and der Goot, 2009; Krstajic et al, 2010). Be-
tween May 2009 and January 2012, we scanned
about eleven million news articles in English,
German and French.
For both data sources, we extract a context of
25 words before and after the word under inves-
tigation, together with its timestamp. In the case
of the EMM data, we also save information on the
news source, the source country and the language
of the article. In a manual postprocessing step, we
1http://www.ldc.upenn.edu/
2http://emm.newsexplorer.eu/
8
clean the dataset from words ending in the suffixes
by coincidence, many of which are proper names
of persons and locations.
From the EMM metadata, we can attribute the
employment of the suffixes to the countries they
were used in. Table 1 shows the figures for the
-gate suffix, what language it was used in, and
its country of origin. We can see that the suffix
was used in many countries and different world
regions between May 2009 and January 2012.
Lang. Country
English GB (1142), USA (840), Ireland
(364), Pakistan (275), South Africa
(190), India (131), Australia (129),
Canada (117), Zimbabwe (73)
French France (2089), Switzerland (429),
Belgium (108), Senegal (30)
German Germany (493), Switzerland (151),
Austria (151)
Table 1: Usage of the suffix -gate in different lan-
guages/countries. For each language only the coun-
tries with the most occurrences are listed.
Among the total 7,500 -gate appearances,
Rubygate ? the affair of Italian?s ex prime min-
ister Silvio Berlusconi with an under-aged girl
from Morocco ? was the most frequent word with
1558 matches, followed by Angolagate with 1025
matches and Climategate with 752 matches. The
NYT corpus has 1,000 matches of -gate words,
the top ones were Iraqgate with 148, Travelgate
with 122, and Irangate with 105 matches. The
frequency of -geddon and -athon was much lower.
3.3 Topic Modeling
The task of the topic modeling in this paper is to
discover meaning relationships between our the
suffixes and semantically related words, i.e. we
want to determine from the word contexts whether
-gate words share context features with words
such as scandal or affair. For this task, we use
LDA, which describes a generative hierarchical
Bayesian model that relates the words and doc-
uments within a corpus through a latent variable.
The interpretation of this latent variable could be
seen as topics that are responsible for the usage
of words within the documents. Within the LDA
framework we can describe the generation of a
document by the following process
1. draw K multinomials ?k ? Dir(?k), one for
each topic k
2. for each document d, d = 1, . . . , D
(a) draw multinomial ?d ? Dir(?d)
(b) for each word wdn in document d, n =
1, . . . , Nd
i. draw a topic zdn ?
Multinomial(?d)
ii. draw a wordwdn from p(wdn|?zdn),
the multinomial probability condi-
tioned on topic zdn
Following this generative process we identify the
hidden variables for every document in a corpus
by computing the posterior distribution:
p(?, ?, z|w, ?, ?) =
p(?, ?, z,w|?, ?)
p(w|?, ?)
. (1)
Exact inference for this posterior distribution
is not tractable and we use collapsed Gibbs sam-
pling as in Griffiths and Steyver (2004). We com-
pute the posterior distribution over all variables
and model parameters instead of inferring ? and
? directly. The Gibbs sampling procedure sam-
ples a topic zdn for each word in all documents
of the corpus. This procedure is iterated until
the approximated posterior distribution does not
change the likelihood of the model with more it-
erations. As a result we get a sampled topic zdn
for each word in the corpus and can trace ? and
?. For our problem we can use the counts of zdn,
the count of words belonging to a topic, for each
document in combination with the timestamps to
see which word in question appears how often in
a specific topic in which time slice. This allows
us to observe the usage of a word within a cer-
tain timespan. The hidden variable ? can be in-
terpreted as a matrix having the conditional prob-
ability p(wi|zk) at the matrix position ?i,k. This
means that every column vector in ? is a probabil-
ity distribution over the whole vocabulary. These
distributions can be seen as topics since they de-
scribe a mixture of words with exact probabilities.
Having those distributions at hand we can analyze
which words occur significantly often in the same
topic or semantic context.
The purpose of the LDA model is to analyze the
latent structure of the passages extracted from the
NYT corpus. We decided to use the contexts of
Watergate, scandal, affair, crisis, controversy in
combination with the suffix -gate. We can then
9
0 = society, art, culture 
Society, Art, 
and Culture 
Watergate 
Economy 
Foreign 
Policy 
Domestic 
Policy 
Sports 
Society, Art, 
and Culture 
Watergate 
Economy 
Foreign 
Policy 
Domestic 
Policy 
Sports 
1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 
1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 
Figure 1: The diachronic distribution of the words under investigation over the 6 topics learned from the New
York Times Corpus.
10
see where these terms co-occur and hence what
the semantic context is. We infer a model which
consists of six topics under the assumption that if
the word senses of the six words given above do
not overlap at all, there should not be more than
six senses to analyze. The fixed parameter K in
the model leads us to an optimization problem of
the hyper-parameter ?. The hyper-parameter ?
is not as important as ? since it scales the topic
per document mixture. For that reason we do not
optimize ? explicitly. We rather estimate the opti-
mal value after optimizing the value for ?. Since
the ? parameter is of crucial impact to the gener-
ation of the hidden variable ? and thus the topics,
we need to find the optimal hyper-parameter that
generalizes the model to the given data. Most ap-
proaches show that one can optimize the model
for fixed parameters ? and ? when testing mod-
els with different values for K as in (Griffiths and
Steyver, 2004). Since we are fixing K we must
test the dataset for an optimal model given differ-
ent values for ?. This can be done by utilizing
the model perplexity (Blei et al, 2003) and thus
maximizing the likelihood of a test dataset from
the same corpus.
In our experiment we used a relatively small
number of topics and we expected a large number
of words aligned to a topic.
4 Visual Analytics
4.1 Topic Modeling
The topics extracted from the NYT corpus by the
model described in Section 3.3 was further inves-
tigated with respect to the correlation between the
lexical semantic content of the suffixed words and
a development over time. For this purpose we de-
signed a pixel visualization (see Figure 1), map-
ping the data facets to the visual variables as fol-
lows: The data is divided according to the topics
mapping each topic to one horizontal band. The
descriptive words of a topic as found by LDA are
listed above its band. In addition, each topic is
manually assigned an interpretive label. These la-
bels are at the far left of a topic band.
Each topic band is further subdivided according
to the words under investigation. Under the label
?gate-aggregated?, all words with -gate suffixes
(except Watergate) are summarized. The bands
are aligned with a time axis and vertically divided
into cells, each cell representing one week of data.
The cell color indicates whether the correspond-
ing word under investigation occurred within the
corresponding topic in the corresponding week.
The black color means that there was no such oc-
currence, whereas the brightest white is assigned
to the cell of the week where most occurrences
(max) of a word under investigation are found,
independent from the topic. Other occurrence
counts are colored in grey tones according to a lin-
ear mapping into the normalized color range from
black=0 to white=max. Note that the normaliza-
tion depends on the word under investigation, i.e.
is relative to its maximal occurrence.
In Figure 1, the data has to be split into two
chunks to fit the page. The upper part shows the
years from 1987 to 1997 and the lower part from
1997 to 2007. There are several possibilities for
user interaction: A semantic zoom allows the data
to be displayed in different levels of time granu-
larity, e.g. day, week, month, year. By mousing
over a cell, the underlying text passages are dis-
played in a tooltip.
Findings Figure 1 shows that the topics are
dominated by different words under investiga-
tion, i.e. the words under investigation cannot be
clearly separated into self-contained meanings.
This mixture indicates that the words under
investigation have similar meanings, but that
in different contexts they are used in different
combinations:
1. Society, Art, and Culture: This seems to be
the most general topic with the broadest usage of
the words under investigation. The descriptive
terms show that it is a lot about interpersonal re-
lations and dominated by ?affair?. In 1989/1990
the play Mastergate becomes visible in the
?gate-aggregated? band.
2. Economy: This topic is strongly related to
?crisis? and apart from the moderate frequency
of ?scandal?, other words are rarely used in this
context. Apparently, financial scandals were
usually not described attaching the suffix ?-gate?
in the years between 1987 and 2007.
3. Foreign Policy: This is another topic domi-
nated by ?crisis?, with moderate occurrences of
?controversy?. Some ?gate-words? also appear.
4. Sports: Here, ?controversy? is the dominating
element, with a raised frequency of ?affair? and
small frequency of ?scandal?. Again, ?gate-
words? appear from time to time, with a slightly
11
increased frequency towards the end.
5. Domestic Politics: The dominant words are
?controversy? and ?crisis?. It?s noteworthy that
?controversy? is a lot more frequent here than
for Foreign Policy. Especially in the last years
?gate-words? appeared from time to time.
In sum, we find that there are preferred contexts
in which -gate is used, namely mainly in topics to
do with society, art and culture and that topics to
do with the economy, -gate is hardly used. The
lexical semantic content of -gate seems to be most
closely linked to the word affair.
4.2 Productivity
The cases of suffixation presented above should
also be considered from the standpoint of mor-
phological productivity. For Baayen (1992), mor-
phological productivity is a complex phenomenon
in which factors like the structure of the lan-
guage, its processing complexities and social con-
ventions mingle. Whereas he focuses on the the
correlation between productivity and frequency,
we can take into account another variable for pro-
ductivity. In particular, we can consider the num-
ber of newspapers that use a certain term. This
will normalize the measures usually taken in that
a term like ?Watergate?, which is highly frequent
and mentioned in a variety of sources is more
productive than a term that occurs frequently, but
only in one source. Using this methodology we
can at least partly circumvent the problem of pro-
ductivity effects that are merely based on the spe-
cific style of one particular newspaper.
First, we visually evaluate the productivity of
the different suffixes plotting the sum of different
coinages against time, see Figure 2. As can be ex-
pected, in all three cases there is a steeper slope in
the beginning of the monitored period. This is an
artifact because all older coinages that had been
around before the monitoring started will be ob-
served for the first time. As more time passes all
plots show a linear overall trend, indicating that
the rate with which new coinages appear remains
somewhat constant. Yet, there are some local os-
cillations in the rate that become more visible in
the plots of -geddon- and -athon-coinages, which
are in general much more infrequent than -gate-
coinages. It can be concluded that over the last
two and a half years the suffixes kept their rate
of productivity in English, German, and French
newswire texts fairly constant.
To investigate the cross-linguistic productivity
of the new coinages we customized a visualiza-
tion with the Tableau software.3 Figure 3 shows
the appearances of the 15 most frequent -gate-
coinages across the three languages over time.
Along the y-axis the data is divided according to
-gate-coinages and languages, whereas the x-axis
encodes the time. Whenever a certain coinage ap-
pears in a certain language at a certain point in
time, a colored triangle is plotted to the corre-
sponding position. The color redundantly encodes
the language for easier interpretation.
Figure 3 shows many interesting patterns. The
most salient patterns can be summarized as:
1. No language barrier: The top -gate-coinages
belong to scandals that are of international
interest and once they are coined in English they
immediately spread to the other languages, see
Rubygate, Climategate, Cablegate, Antennagate,
and Crashgate. Only in the case of Angolagate
and Karachigate there is a certain delay in the
spread, possibly due to the fact that it was coined
in French first and initially did not achieve the
same attention as coinages in English.
2. Pertinacity partly depends on language:
Some -gate-coinages re-appear over and over
again only in individual languages. This espe-
cially holds for words that were coined before
the monitoring started, e.g. Sachsgate, Oilgate,
Troopergate, and Travelgate which all persist in
English. Examples can be found for other lan-
guages, e.g. Angolagate for French. Interestingly,
in German Nipplegate persists over the whole
monitored period, but only in German, and even
outperforms its German spelling Nippelgate.
3. Some coinages are special: Some of the
recent coinages such as Memogate, Asiagate, and
Weinergate reach an extremely high frequency
within very short time ranges, but can be found
almost exclusively in English. These will be
subject of further investigation in Section 4.2.1.
It has to be noted that many of the infrequent
coinages appear only once and are never adopted.
4.2.1 Spread across News Sources and
Countries
Figure 3 clearly shows that Memogate is heav-
ily mentioned within English speaking news
3http://www.tableausoftware.com/
12
Su
m
 of
 di
ffe
re
nt
 co
in
ag
es
 
Su
m
 of
 di
ffe
re
nt
 co
in
ag
es
 
Su
m
 of
 di
ffe
re
nt
 co
in
ag
es
 
days days days 
Different geddon-coinages over time Different athon-coinages over time Different gate-coinages over time 
Figure 2: The number of different coinages containing the suffixes under investigation (on the y-axis) plotted
against the number of days passed during the monitoring process (on the x-axis)
Data  used 
in Figure  4  
PDWFK ODQJXDJH
$SU $XJ 'H] $SU $XJ 'H] $SU $XJ 'H]
7DJYRQHPPBSXEOLFDWLRQGDWH
:XOIJDWH H
<DFKWJDWH HQ
<DFKWVJDWH HQ
<HRQJSRJDWH HQ
<HRQJSRJDWH HQ
<RXQJSRJDWH HQ
<XQXVJDWH HQ
=DKLDJDWH IU
=LIDJDWH HQ
=LPEDEZHJDWH HQ
=LQHEJDWH IU
=LSSHUJDWH HQ
=LVFRJDWH HQ
=RUEDJDWH HQ
=XPDJDWH HQ
%ODW
ODQJXDJH
GH
HQ
IU
HPPBSXEOLFDWLRQGDWH7DJI?UMHGHODQJXDJHXQWHUWHLOWQDFKPDWFK)DUEH]HLJW'HWDLOV]XODQJXDJHDQ'HWDLOVZHUGHQI?UFRQWH[WXQGHPPBVRXUFHBFRXQWU\DQJH]HLJW'LH$QVLFKW
ZLUGXQWHUODQJXDJHXQGPDWFKJHILOWHUW'HU)LOWHUODQJXDJHVFKOLH?W,/XQG]KDXV'HU)LOWHUPDWFKVFKOLH?W1RWIDOVWURPDJUHJDWH1RWVURPDJJUHJDWH1RWVWURPDJJHJDWHXQG
1RWVWURPDJUHJDWHDXV
Figure 3: The appearances of the 15 most frequent -gate coinages over time and across the different languages
13
Figure 4: Detailed analysis of the Memogate cluster highlighted in Figure 3 using alternative visual mappings:
Sequence of spread over different countries and news sources.
sources within a short time range. We developed
a further visualization that shows how these men-
tions sequentially distribute over different news
sources and countries. In Figure 4 each article
mentioning Memogate is represented by a col-
ored icon. The y-axis position encodes the news
source, the x-axis position encodes the temporal
order of the occurrences. Note that exact time
differences are omitted to make the display more
compact. The shape of an icon indicates the lan-
guage of the article; Circles (English) heavily
dominate. The color encodes the country of origin
of the news source, here green (Pakistan), yellow
(India), and purple (USA) dominate.
Findings: While the first three mentions of
Memogate could be found in British and Amer-
ican Newspapers, early on it was adopted by
http://tribune.com.pk/ in Pakistan (fourth line
from the top) and used so heavily that it kept being
adopted and became constantly used by further
sources from Pakistan and also India. Apparently,
individual sources may have a huge influence on
the spread of a new coinage.
5 Future work and conclusion
We have presented initial experiments with re-
spect to the application of topic modeling and vi-
sualization to gain a better understanding of de-
velopments in morphological coinage and lexical
semantics. We investigated three relatively new
productive suffixes, namely -gate, -geddon, and
-athon based on their occurrences in newswire
data. Even though our data set was huge, the oc-
currences of the suffixes are comparatively rare
and so we only had enough data for -gate to inves-
tigate the contexts it occurs in with an optimized
topic modeling. The results indicate that it is used
in broader contexts than affair, with which it is
most related. Different domains of usage could be
distinguished, even though a clear development
over time could not be detected based the NYT
corpus. Investigating the multilingual newswire
data it became evident that all three suffixes un-
der investigation have a relatively stable rate of
appearance. Many more different -gate-coinages
could be found, though. We could observe that
-gate was usually attached to one specific single
event, and especially in many of the less frequent
coinages the suffix was combined with proper
names of persons, institutions, or locations. In
contrast, -athon and -mageddon coinages seem to
be easier to generalize. For example, the two most
widely spread coinages Snowmageddon and Car-
mageddon, while initially referring to a certain
snow storm and a certain traffic jam, have been
applied to further such events and can be found
listed in resources such as the Urban Dictionary.4
In conclusion, we demonstrated that visual
analyses can help to gain insight and generate new
hypotheses about the behavior of the distribution
and use of new morphemes. In our future research
we aim to investigate how much the success of a
certain coinage depends on the event as such and
its news dynamics, and what role linguistic fea-
tures like e.g. phonology (two vs. three syllables,
etc.) might play.
4http://www.urbandictionary.com/define.php?term=Carmageddon
14
Acknowledgments
This work has partly been funded by the Research
Initiative ?Computational Analysis of Linguistic
Development? at the University of Konstanz and
by the German Research Society (DFG) under the
grant GK-1042, Explorative Analysis and Visu-
alization of Large Information Spaces, Konstanz.
The authors would like to thank Volker Rehberg
for his programming support and Thomas Mayer
for comments on previous versions of the paper.
References
Martin Atkinson and Erik Van der Goot. 2009. Near
real time information mining in multilingual news.
In Juan Quemada, Gonzalo Leo?n, Yoe?lle S. Maarek,
and Wolfgang Nejdl, editors, Proceedings of the
18th International Conference on World Wide Web,
WWW 2009, Madrid, Spain, April 20-24, 2009,
pages 1153?1154.
R. Harald Baayen. 1992. On frequency, transparency,
and productivity. Yearbook of Morphology, pages
181?208.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In Proceedings of the 12th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, EACL ?09,
pages 103?111, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Stuart K. Card, Jock D. Mackinlay, and Ben Shneider-
man, editors. 1999. Readings in information visu-
alization: using vision to think. Morgan Kaufmann
Publishers Inc., San Francisco, CA, USA.
Thomas L. Griffiths and Mark Steyver. 2004. Find-
ing scientific topics. In Proceedings of the National
Academy of Sciences 101, pages 5228?5235.
Timo Honkela, Ville Pulkki, and Teuvo Kohonen.
1995. Contextual relations of words in grimm tales,
analyzed by self-organizing map. In Proceedings of
International Conference on Artificial Neural Net-
works (ICANN-95), pages 3?7.
Daniel A. Keim, Joern Kohlhammer, Geoffrey Ellis,
and Florian Mansmann, editors. 2010. Mastering
The Information Age - Solving Problems with Visual
Analytics. Goslar: Eurographics.
Milos Krstajic, Florian Mansmann, Andreas Stoffel,
Martin Atkinson, and Daniel A. Keim. 2010. Pro-
cessing Online News Streams for Large-Scale Se-
mantic Analysis. In Proceedings of the 1st Inter-
national Workshop on Data Engineering meets the
Semantic Web (DESWeb 2010).
Anke Lu?deling and Stefan Evert. 2005. The emer-
gence of productive non-medical -itis. corpus ev-
idence and qualitative analysis. In S. Kepser
and M. Reis, editors, Linguistic Evidence. Empir-
ical, Theoretical, and Computational Perspectives,
pages 351?370. Berlin: Mouton de Gruyter.
Thomas Mayer, Christian Rohrdantz, Miriam Butt,
Frans Plank, and Daniel Keim. 2010a. Visualiz-
ing vowel harmony. Journal of Linguistic Issues in
Language Technology (LiLT), 4(2).
Thomas Mayer, Christian Rohrdantz, Frans Plank,
Peter Bak, Miriam Butt, and Daniel A. Keim.
2010b. Consonant co-occurrence in stems across
languages: Automatic analysis and visualization of
a phonotactic constraint. In Proceedings of the ACL
2010 Workshop on NLP and Linguistics: Finding
the Common Ground (NLPLING 2010), pages 67?
75.
Eiji Nishimoto. 2004. Defining new words in corpus
data: Productivity of english suffixes in the british
national corpus. In 26th Annual Meeting of the
Cognitive Science Society.
Ingo Plag. 1999. Morphological productivity. Struc-
tural constraints in English derivation. Berlin/New
York: Mouton de Gruyter.
Christian Rohrdantz, Annette Hautli, Thomas Mayer,
Miriam Butt, Daniel A. Keim, and Frans Plank.
2011. Towards tracking semantic change by vi-
sual analytics. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Langauge Technologies (ACL-
HLT ?11): shortpapers, pages 305?310, Portland,
Oregon. Association for Computational Linguistics.
James J. Thomas and Kristin A. Cook. 2005. Illu-
minating the Path The Research and Development
Agenda for Visual Analytics. National Visualization
and Analytics Center.
15
