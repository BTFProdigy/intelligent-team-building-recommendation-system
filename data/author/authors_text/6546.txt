Coling 2008: Companion volume ? Posters and Demonstrations, pages 91?94
Manchester, August 2008
Rank Distance as a Stylistic Similarity
Marius Popescu
University of Bucharest
Department of Computer Science
Academiei 14, Bucharest, Romania
mpopescu@phobos.cs.unibuc.ro
Liviu P. Dinu
University of Bucharest
Department of Computer Science
Academiei 14, Bucharest, Romania
ldinu@funinf.cs.unibuc.ro
Abstract
In this paper we propose a new distance
function (rank distance) designed to reflect
stylistic similarity between texts. To assess
the ability of this distance measure to cap-
ture stylistic similarity between texts, we
tested it in two different machine learning
settings: clustering and binary classifica-
tion.
1 Introduction
Computational stylistics investigates texts from the
standpoint of individual style (author identifica-
tion) or functional style (genres, registers). Be-
cause in all computational stylistic studies / ap-
proaches, a process of comparison of two or more
texts is involved, in a way or another, there was
always a need for a distance function to measure
similarity (more precisely dissimilarity) of texts
from the stylistic point of view. Such distance
measures were proposed and used for example in
authorship identification (Labb?e and Labb?e, 2001;
Burrows, 2002) or clustering texts by genre (Luy-
ckx et al, 2006).
In this paper we propose a new distance mea-
sure designed to reflect stylistic similarity between
texts. As style markers we used the function word
frequencies. Function words are generally con-
sidered good indicators of style because their use
is very unlikely to be under the conscious con-
trol of the author and because of their psycholog-
ical and cognitive role (Chung and Pennebaker,
2007). Also function words prove to be very effec-
tive in many author attribution studies. The nov-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
elty of our approach resides in the way we use
information given by the function word frequen-
cies. Given a fixed set of function words (usually
the most frequent ones), a ranking of these func-
tion words according to their frequencies is built
for each text; the obtained ranked lists are subse-
quently used to compute the distance between two
texts. To calculate the distance between two rank-
ings we used Rank distance (Dinu, 2003), an ordi-
nal distance tightly related to the so-called Spear-
man?s footrule (Diaconis and Graham, 1977).
Usage of the ranking of function words in the
calculation of the distance instead of the actual val-
ues of the frequencies may seem as a loss of infor-
mation, but we consider that the process of rank-
ing makes the distance measure more robust acting
as a filter, eliminating the noise contained in the
values of the frequencies. The fact that a specific
function word has the rank 2 (is the second most
frequent word) in one text and has the rank 4 (is
the fourth most frequent word) in another text can
be more relevant than the fact that the respective
word appears 349 times in the first text and only
299 times in the second.
To assess the ability of this distance function to
capture stylistic similarity between texts, we tested
it in two different machine learning settings: clus-
tering and binary classification.
Compared with other machine learning and sta-
tistical approaches, clustering was relatively rarely
used in stylistic investigations. However, few re-
searchers (Labb?e and Labb?e, 2001; Luyckx et al,
2006) have recently proved that clustering can be
a useful tool in computational stylistic studies.
Apart of this, clustering is a very good test bed for
a distance measure behavior. We plugged our dis-
tance function into a standard hierarchical cluster-
ing algorithm and test it on a collection of 21 nine-
91
teenth century English books (Koppel et al, 2007).
The results are very encouraging. The family trees
produced grouped together texts according to their
author, genre, even gender.
Also a distance measure can be used to solve
classification problems if it is coupled with proper
learning algorithm. One of the simplest such algo-
rithms is nearest neighbor classification algorithm.
We chose nearest neighbor algorithm because its
performance is entirely based on the appropriate-
ness to the data of the distance function on which it
relies. In this way the accuracy of the classification
will reflect the adequacy of the distance measure to
data and domain on which the method was applied.
We used the new distance function in conjunction
with nearest neighbor classification algorithm and
tested it on the well known case of authorship of
disputed Federalist papers. The method attributed
all disputed papers to Madison, the result being
consistent with that of Mosteller and Wallace.
To check if the usage of ranks of function words
is better suited for capturing stylistic differences
than the usage of actual frequencies of the function
words, we repeated the above experiments on clus-
tering and binary classification with the standard
euclidean distance between the vectors of frequen-
cies of the same function words that were used in
computing the rank distance. The comparison is in
favor of rank distance.
2 Rank Distance and Its Use as a Stylistic
Distance Between Texts
Rank distance (Dinu, 2003) is an ordinal met-
ric able to compare different rankings of a set of
objects. It is tightly related to the Spearman?s
footrule (Diaconis and Graham, 1977), and it had
already been successfully used in computational
linguistics, in such problems as the similarity of
Romance languages (Dinu and Dinu, 2005).
A ranking of a set of n objects can be repre-
sented as a permutation of the integers 1, 2, . . . , n,
? ? S
n
. ?(i) will represent the place (rank) of the
object i in the ranking. The Rank distance in this
case is simply the distance induced by L
1
norm:
D(?
1
, ?
2
) =
n
?
i=1
|?
1
(i)? ?
2
(i)| (1)
This is a distance between what is called full rank-
ings. However, in real situations, the problem of
tying arises, when two or more objects claim the
same rank (are ranked equally). For example, two
a been had its one that was
all but has may only the were
also by have more or their what
an can her must our then when
and do his my shall there which
any down if no should things who
are even in not so this will
as every into now some to with
at for is of such up would
be from it on than upon your
Table 1: Function words used in computing the
distance
or more function words can have the same fre-
quency in a text and any ordering of them would
be arbitrary.
The Rank distance allocates to tied objects a
number which is the average of the ranks the tied
objects share. For instance, if two objects claim the
rank 2, then they will share the ranks 2 and 3 and
both will receive the rank number (2+3)/2 = 2.5.
In general, if k objects will claim the same rank
and the first x ranks are already used by other ob-
jects, then they will share the ranks x + 1, x +
2, . . . , x + k and all of them will receive as rank
the number:
(x+1)+(x+2)+...+(x+k)
k
= x+
k+1
2
. In
this case, a ranking will be no longer a permutation
(?(i) can be a non integer value), but the formula
(1) will remain a distance (Dinu, 2003).
Rank distance can be used as a stylistic distance
between texts in the following way:
First a set of function word must be fixed. The
most frequent function words may be selected or
other criteria may be used for selection. In all our
experiments we used the set of 70 function words
identified by Mosteller and Wallace (Mosteller and
Wallace, 1964) as good candidates for author-
attribution studies. The set is given in Table 1.
Once the set of function words is established,
for each text a ranking of these function words is
computed. The ranking is done according to the
function word frequencies in the text. Rank 1 will
be assigned to the most frequent function word,
rank 2 will be assigned to the second most frequent
function word, and so on. The ties are resolved as
we discussed above. If some function words from
the set don?t appear in the text, they will share the
last places (ranks) of the ranking.
The distance between two texts will be the Rank
distance between the two rankings of the function
words corresponding to the respective texts.
3 Clustering Experiments
One good way to test the virtues of a distance mea-
sure is to use it as a base for a hierarchical cluster-
92
Group Author Book
American Novelists Hawthorne Dr. Grimshawe?s Secret
House of Seven Gables
Melville Redburn
Moby Dick
Cooper The Last of the Mohicans
The Spy
Water Witch
American Essayists Thoreau Walden
A Week on Concord
Emerson Conduct Of Life
English Traits
British Playwrights Shaw Pygmalion
Misalliance
Getting Married
Wilde An Ideal Husband
Woman of No Importance
Bronte Sisters Anne Agnes Grey
Tenant Of Wildfell Hall
Charlotte The Professor
Jane Eyre
Emily Wuthering Heights
Table 2: The list of books used in the experiment
ing algorithm. The family trees (dendogram) thus
obtained can reveal a lot about the distance mea-
sure behavior.
In our experiments we used an agglomerative hi-
erarchical clustering algorithm (Duda et al, 2001)
with average linkage.
In the first experiment we cluster a collection
of 21 nineteenth century English books written by
10 different authors and spanning a variety of gen-
res (Table 2). The books were used by Koppel et
al. (Koppel et al, 2007) in their authorship verifi-
cation experiments.
The resulted dendogram is shown in Figure 1.
As can be seen, the family tree produced is a very
good one, accurately reflecting the stylistic rela-
tions between books. The books were grouped
in three big clusters (the first three branches of
the tree) corresponding to the three genre: dramas
(lower branch), essays (middle branch) and novels
(upper branch). Inside each branch the works were
first clustered according to their author. The only
exceptions are the two essays of Emerson which
instead of being first cluster together and after that
merged in the cluster of essays, they were added
one by one to this cluster. Apart of this, the family
tree is perfect. Even more, in the cluster of novels
one may distinguished two branches clearly sepa-
rated that can correspond to the gender or national-
ity of the authors: female English (lower part) and
male American (upper part).
For comparison, the dendogram in Figure 2
show the same books clustered with the same al-
gorithm, but using the standard euclidean distance
instead of the rank distance as measure of stylis-
tic similarity. The same set of function words as
in the case of rank distance was used. This time
though, each text was represented as a vector of
Figure 1: Dendogram of 21 nineteenth century En-
glish books (Rank Distance)
Figure 2: Dendogram of 21 nineteenth century En-
glish books (Euclidean Distance)
93
relative frequencies of these function words in the
text. The relative frequency of a particular func-
tion word in a text is calculated as the number of
appearances of the respective function word in the
text divided by the length (in tokens) of the text.
The distance between two texts is given by the eu-
clidean distance between the corresponding vec-
tors of relative frequencies of function words. In
the family tree obtained using euclidean distance,
most of the books are still grouped according to
their author, but the distinct clusters corresponding
to genre and gender disappeared and the novels of
Melville were separated: one being clustered with
the essays of Thoreau (Moby Dick) and the other
with the novels of Hawthorne.
4 Binary Classification Experiments
When a distance measure is available, the most
natural choice of a classification algorithm is the
nearest neighbor algorithm (Duda et al, 2001).
We tested the nearest neighbor classification al-
gorithm combined with both rank distance and eu-
clidean distance on the case of the 12 disputed fed-
eralist papers (Mosteller and Wallace, 1964). In
our experiments we followed the Mosteller and
Wallace setting, treating the problem as a binary
classification problem. Each one of the 12 dis-
puted papers has to be classified as being written
by Hamilton or Madison. For training are used the
51 papers written by Hamilton and the 14 papers
written by Madison.
Tested on disputed papers, the nearest neighbor
classification algorithm combined with rank dis-
tance attributed all the 12 papers to Madison. This
matches the results obtained by Mosteller and Wal-
lace and is in agreement with today accepted thesis
that the disputed papers belong to Madison. When
the nearest neighbor classification algorithm was
combined with euclidean distance only 11 papers
were attributed to Madison, the paper 56 was at-
tributed to Hamilton.
5 Discussion
In this paper we have proposed a new distance
measure based on the ranking of function words,
designed to capture stylistic similarity between
texts. We have tested it in two different machine
learning settings: clustering and binary classifica-
tion; we have compared its performance with that
of standard euclidean distance on vectors of fre-
quencies of the function words. Though testing on
more data is needed, the initial experiments shown
that the new distance measure is indeed a good in-
dicator of stylistic similarity and better suited for
capturing stylistic differences between texts than
the standard euclidean distance.
In future work it would be useful to test this dis-
tance measure on other data sets and especially in
other machine learning paradigms like one-class
classification to solve authorship verification prob-
lems (Koppel et al, 2007).
Acknowledgments Research supported by
MEdC-ANCS, PNII-Idei, project 228 and Univer-
sity of Bucharest.
References
Burrows, John. 2002. ?delta?: a measure of stylistic
difference and a guide to likely authorship. Literary
and Linguistic Computing, 17(3):267?287.
Chung, Cindy K. and James W. Pennebaker. 2007. The
psychological function of function words. In Fiedler,
K., editor, Social communication: Frontiers of social
psychology, pages 343?359. Psychology Press, New
York.
Diaconis, P. and R.L. Graham. 1977. Spearman?s
footrule as a measure of disarray. Journal of the
Royal Statistical Society, Series B (Methodological),
39(2):262?268.
Dinu, Anca and Liviu Petrisor Dinu. 2005. On the syl-
labic similarities of romance languages. In CICLing-
2005, pages 785?788.
Dinu, Liviu Petrisor. 2003. On the classification and
aggregation of hierarchies with different constitutive
elements. Fundamenta Informaticae, 55(1):39?50.
Duda, R. O., P. E. Hart, and D. G. Stork. 2001. Pattern
Classification (2nd ed.). Wiley-Interscience Publi-
cation.
Koppel, Moshe, Jonathan Schler, and Elisheva
Bonchek-Dokow. 2007. Measuring differentiabil-
ity: Unmasking pseudonymous authors. Journal of
Machine Learning Research, 8:1261?1276.
Labb?e, Cyril and Dominique Labb?e. 2001. Inter-
textual distance and authorship attribution corneille
and moliere. Journal of Quantitative Linguistics,
8(3):213?231.
Luyckx, Kim, Walter Daelemans, and Edward Van-
houtte. 2006. Stylogenetics: Clustering-based
stylistic analysis of literary corpora. In Proceedings
of LREC-2006, the fifth International Language Re-
sources and Evaluation Conference, pages 30?35.
Mosteller, Frederick and David L. Wallace. 1964. In-
ference and Disputed Authorship: The Federalist.
Addison-Wesley, Massachusetts.
94
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1368?1377,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
What?s in a name? In some languages, grammatical gender
Vivi Nastase
EML Research gGmbH
Heidelberg, Germany
nastase@eml-research.de
Marius Popescu
Department of Mathematics and Computer Science
University of Bucharest, Bucharest, Romania
mpopescu@phobos.cs.unibuc.ro
Abstract
This paper presents an investigation of the
relation between words and their gender in
two gendered languages: German and Ro-
manian. Gender is an issue that has long
preoccupied linguists and baffled language
learners. We verify the hypothesis that
gender is dictated by the general sound
patterns of a language, and that it goes
beyond suffixes or word endings. Exper-
imental results on German and Romanian
nouns show strong support for this hypoth-
esis, as gender prediction can be done with
high accuracy based on the form of the
words.
1 Introduction
For speakers of a language whose nouns have no
gender (such as modern English), making the leap
to a language that does (such as German), does
not come easy. With no or few rules or heuris-
tics to guide him, the language learner will try to
draw on the ?obvious? parallel between grammat-
ical and natural gender, and will be immediately
baffled to learn that girl ? Ma?dchen ? is neuter in
German. Furthermore, one may refer to the same
object using words with different gender: car can
be called (das) Auto (neuter) or (der) Wagen (mas-
culine). Imagine that after hard work, the speaker
has mastered gender in German, and now wishes
to proceed with a Romance language, for example
Italian or Spanish. He is now confronted with the
task of relearning to assign gender in these new
languages, made more complex by the fact that
gender does not match across languages: e.g. sun
? feminine in German (die Sonne), but masculine
in Spanish (el sol), Italian (il sole) and French (le
soleil); moon ? masculine in German (der Mond),
but feminine in Spanish (la luna), Italian (la luna)
and French (la lune). Gender doesn?t even match
within a single language family: travel ? mascu-
line in Spanish (el viage) and Italian (il viaggio),
but feminine in Portuguese (a viagem).
Grammatical gender groups nouns in a lan-
guage into distinct classes. There are languages
whose nouns are grouped into more or less than
three classes. English for example has none, and
makes no distinction based on gender, although
Old English did have three genders and some
traces remain (e.g. blonde, blond).
Linguists assume several sources for gender: (i)
a first set of nouns which have natural gender and
which have associated matching grammatical gen-
der; (ii) nouns that resemble (somehow) the nouns
in the first set, and acquire their grammatical gen-
der through this resemblance. Italian and Roma-
nian, for example, have strong and reliable phono-
logical correlates (Vigliocco et al, 2004b, for Ital-
ian). (Doca, 2000, for Romanian). In Romanian
the majority of feminine nouns end in a? or e. Some
rules exists for German as well (Schumann, 2006),
for example nouns ending in -ta?t, -ung, -e, -enz,
-ur, -keit, -in tend to be feminine. Also, when
specific morphological processes apply, there are
rules that dictate the gender of the newly formed
word. This process explains why Frau (woman) is
feminine in German, while Fra?ulein (little woman,
miss) is neuter ? Fra?ulein = Frau + lein. The ex-
isting rules have exceptions, and there are numer-
ous nouns in the language which are not derived,
and such suffixes do not apply.
Words are names used to refer to concepts. The
fact that the same concept can be referred to using
names that have different gender ? as is the case
for car in German ? indicates that at least in some
cases, grammatical gender is in the name and not
the concept. We test this hypothesis ? that the gen-
der of a noun is in its word form, and that this goes
beyond word endings ? using noun gender data
for German and Romanian. Both Romanian and
German have 3 genders: masculine, feminine and
1368
neuter. The models built using machine learning
algorithms classify test nouns into gender classes
based on their form with high accuracy. These re-
sults support the hypothesis that in gendered lan-
guages, the word form is a strong clue for gender.
This supplements the situation when some con-
cepts have natural gender that matches their gram-
matical gender: it allows for an explanation where
there is no such match, either directly perceived,
or induced through literary devices.
The present research has both theoretical and
practical benefits. From a theoretical point of
view, it contributes to research on phonology and
gender, in particular in going a step further in un-
derstating the link between the two. From a practi-
cal perspective, such a connection between gender
and sounds could be exploited in advertising, in
particular in product naming, to build names that
fit a product, and which are appealing to the de-
sired customers. Studies have shown that espe-
cially in the absence of meaning, the form of a
word can be used to generate specific associations
and stimulate the imagination of prospective cus-
tomers (Sells and Gonzales, 2003; Bedgley, 2002;
Botton et al, 2002).
2 Gender
What is the origin of grammatical gender and how
does it relate to natural gender? Opinions are split.
Historically, there were two main, opposite, views:
(i) there is a semantic explanation, and natural
gender motivated the category (ii) the relationship
between natural and grammatical gender is arbi-
trary.
Grimm (1890) considered that grammatical
gender is an extension of natural gender brought
on by imagination. Each gender is associated
with particular adjectives or other attributes, and in
some cases (such as for sun and moon) the assign-
ment of gender is based on personification. Brug-
mann (1889) and Bloomfield (1933) took the po-
sition that the mapping of nouns into genders is
arbitrary, and other phenomena ? such as deriva-
tions, personification ? are secondary to the estab-
lished agreement. Support for this second view
comes also from language acquisition: children
who learn a gendered language do not have a nat-
ural gender attribute that they try to match onto
the newly acquired words, but learn these in a sep-
arate process. Any match or mapping between
natural and grammatical gender is done after the
natural gender ?feature? is acquired itself. Ki-
larski (2007) presents a more detailed overview
of currents and ideas about the origin of gender.
Unterbeck (1999) contains a collection of papers
that investigate grammatical gender in several lan-
guages, aspects of gender acquisition and its rela-
tion with grammatical number and agreement.
There may be several reasons for the polemic
between these two sides. One may come from the
categorization process, the other from the relation
between word form and its meaning. Let us take
them each in turn, and see how they influenced
gender.
Grammatical gender separates the nouns in a
language into disjoint classes. As such, it is a cat-
egorization process. The traditional ? classical ?
theory of categorization and concepts viewed cat-
egories and concepts as defined in terms of a set
of common properties that all its members should
share. Recent theories of concepts have changed,
and view concepts (and categories) not necessar-
ily as ?monolithic? and defined through rules, but
rather as clusters of members that may resemble
each other along different dimensions (Margolis
and Laurence, 1999).
In most linguistic circles, the principle of ar-
bitrariness of the association between form and
meaning, formalized by de Saussure (1916) has
been largely taken for granted. It seems however,
that it is hard to accept such an arbitrary relation,
as there have always been contestants of this prin-
ciple, some more categorical than others (Jakob-
son, 1937; Jespersen, 1922; Firth, 1951). It is pos-
sible that the correlation we perceive between the
word form and the meaning is something that has
arisen after the word was coined in a language, be-
ing the result of what Firth called ?phonetic habit?
through ?an attunement of the nervous system?,
and that we have come to prefer, or select, cer-
tain word forms as more appropriate to the con-
cept they name ? ?There is no denying that there
are words which we feel instinctively to be ade-
quate to express the ideas they stand for. ... Sound
symbolism, we may say, makes some words more
fit to survive? (Jespersen, 1922).
These two principles relate to the discussion
on gender in the following manner: First of all,
the categories determined by grammatical gen-
der need not be homogeneous, and their mem-
bers need not all respect the same member-
ship criterion. This frees us from imposing a
matching between natural and grammatical gen-
der where no such relation is obvious or pos-
1369
sible through literary devices (personification,
metaphor, metonymy). Nouns belonging to the
same gender category may resemble each other
because of semantic considerations, lexical deriva-
tions, internal structure, perceived associations
and so on. Second, the fact that we allow for the
possibility that the surface form of a word may
encode certain word characteristics or attributes,
allows us to hypothesize that there is a surface,
phonological, similarity between words grouped
within the same gender category, that can supple-
ment other resemblance criteria in the gender cat-
egory (Zubin and Ko?pcke, 1986).
Zubin and Ko?pcke (1981), Zubin and Ko?pcke
(1986) have studied the relation between seman-
tic characteristics and word form with respect to
gender for German nouns. Their study was mo-
tivated by two observations: Zipf (1935) showed
that word length is inversely correlated with fre-
quency of usage, and Brown (1958) proposed that
in choosing a name for a given object we are more
likely to use a term corresponding to a ?basic?
level concept. For example, chair, dog, apple
would correspond to the basic level, while furni-
ture, animal, fruit and recliner, collie, braeburn
apple correspond to a more general or a more
specific level, respectively. Their study of gen-
der relative to these levels have shown that basic
level terms have masculine, feminine, and rarely
neuter genders, while the more undifferentiated
categories at the superordinate level are almost ex-
clusively neuter.
In psycholinguistic research, Friederici and Ja-
cobsen (1999) adopt the position that a lexical
entry consists of two levels: form and seman-
tic and grammatical properties to study the influ-
ence of gender priming ? both from a form and
semantic perspective ? on language comprehen-
sion. Vigliocco et al (2004a) study gender prim-
ing for German word production. While this re-
search studies the influence of the word form on
the production of nouns with the same or different
grammatical gender, there is no study of the rela-
tion between word forms and their corresponding
gender.
In recent studies we have found on the rela-
tion between word form and its associated gender,
the only phonological component of a word that
is considered indicative is the ending. Spalek et
al. (2008) experiment on French nouns, and test
whether a noun?s ending is a strong clue for gen-
der for native speakers of French. Vigliocco et al
(2004b) test cognitive aspects of grammatical gen-
der of Italian nouns referring to animals.
Cucerzan and Yarowsky (2003) present a boot-
strapping process to predict gender for nouns in
context. They show that context gives accurate
clues to gender (in particular through determiners,
quantifiers, adjectives), but when the context is not
useful, the algorithm can fall back successfully on
the word form. Cucerzan and Yarowsky model
the word form for predicting gender using suffix
trie models. When a new word is encountered, the
word is mapped onto the trie starting from the last
letter, and it is assigned the gender that has the
highest probability based on the path it matches in
the trie. In context nouns appear with various in-
flections ? for number and case in particular. Such
morphological derivations are gender specific, and
as such are strong indicators for gender.
The hypothesis tested here is that gender comes
from the general sound of the language, and is dis-
tributed throughout the word. For this, the data
used should not contain nouns with ?tell tale? in-
flections. The data will therefore consist of nouns
in the singular form, nominative case. Some nouns
are derived from verbs, adverbs or adjectives, or
other nouns through morphological derivations.
These derivations are regular and are identifiable
through a rather small number of regular suffixes.
These suffixes (when they are indicative of gen-
der) and word endings will be used as baselines
to compare the accuracy of prediction on the full
word with the ending fragment.
3 Data
We test our gender-language sounds connection
through two languages from different language
families. German will be the representative of the
Germanic languages, and Romanian for the Ro-
mance ones. We first collect data in the two lan-
guages, and then represent them through various
features ? letters, pronunciation, phonetic features.
3.1 Noun collections
German data For German we collect nouns and
their grammatical gender from a German-English
dictionary, part of the BEOLINGUS multi-lingual
dictionary1. In the first step we collected the Ger-
man nouns and their gender from this dictionary.
In step 2, we filter out compounds. The reason
for this step is that a German noun compound will
1http://dict.tu-chemnitz.de/
1370
have the gender of its head, regardless of its nom-
inal modifiers. For the lack of a freely available
tool to detect and split noun compounds, we resort
to the following algorithm:
1. initialize the list of nouns L
N
to the empty
list;
2. take each noun n in the dictionary D, and
(a) if ?n
i
? L
N
such that n is an end sub-
string of n
i
, then add n to L
N
and re-
move n
i
from L
N
;
(b) if ?n
i
? L
N
such that n
i
is a end sub-
string of n, skip n;
Essentially, we remove from the data all nouns
that include another noun as the end part (which
is the head position in German noun compounds).
This does not filter examples that have suffixes
added to form the feminine version of a masculine
noun, for example: (der) Lehrer ? (die) Lehrerin
(teacher). The suffixes are used in one of the base-
lines for comparison with our learning method.
We obtain noun pronunciation information from
the Bavarian Archive for Speech Signals2. We fil-
ter again our list L
N
to keep nouns for which we
have pronunciation information. This allows us to
compare the learning results when letter or pro-
nunciation information is used.
After collecting the nouns and their pronunci-
ation, we map the pronunciation onto lower level
phonetic features, following the IPA encoding of
sounds for the German language. The mapping
between sounds and IPA features was manually
encoded following IPA tables.
Romanian data We extract singular nomina-
tive forms of nouns from the Romanian lexical
database (Barbu, 2008). The resource contains
the proper word spelling, including diacritics and
special characters. Because of this and the fact
that there is a straightforward mapping between
spelling and pronunciation in Romanian, we can
use the entire data extracted from the dictionary
in our experiments, without special pronunciation
dictionaries. Following the example for the Ger-
man language, we encode each sound through
lower level phonological features using IPA guide-
lines.
As in Italian, in Romanian there are strong
phonological cues for nouns, especially those hav-
ing the feminine gender: they end in a? and e.
2http://www.phonetik.uni-muenchen.de/
Bas/
To determine whether the connection between a
word form and gender goes beyond this superfi-
cial rule, we generate a dataset in which the nouns
are stripped of their final letter, and their represen-
tation is built based on this reduced form.
Table 1 shows the data collected and the distri-
bution in the three classes.
German Romanian
masc. 565 32.64% 7338 15.14%
fem. 665 38.42% 27187 56.08%
neut. 501 28.94% 13952 28.78%
total 1731 48477
Table 1: Data statistics
Because for Romanian the dataset is rather
large, we can afford to perform undersampling
to balance our classes, and have a more straight-
forward evaluation. We generate a perfectly bal-
anced dataset by undersampling the feminine and
the neuter classes down to the level of the mascu-
line class. We work then with a dataset of 22014
instances, equally distributed among the three gen-
ders.
3.2 Data representation
For each word in our collections we produce three
types of representation: letters, phonemes and
phonological features. Table 2 shows examples
for each of these representations. The letter and
phoneme representations are self-explanatory. We
obtain the pronunciation corresponding to each
word from a pronunciation dictionary, as men-
tioned in Section 3.1, which maps a word onto a
sequence of phonemes (phones). For Romanian
we have no such resource, but me make without
since in most part the pronunciation matches the
letter representation3.
German
letter abend (m) a b e n d
phoneme a: b @ n d
Romanian
letter seara? (f) s e a r a?
Table 2: Data representation in terms of letters and
phonemes for the German and Romanian forms of
the word evening. For Romanian, the letter and
phoneme representation is the same.
3The exceptions are the diphthongs and a few groups of
letters: ce, ci, che, chi, oa, and the letter x.
1371
Phonemes, the building blocks of the phonetic
representation, can be further described in terms
of phonological features ? ?configurations? of
the vocal tract (e.g tongue and lips position),
and acoustic characteristics (e.g. manner of
air flow). We use IPA standards for mapping
phones in German and Romanian onto these
phonological features. We manually construct
a map between phones and features, and then
automatically binarize this representation and
use it to generate a representation for each
phone in each word in the data. For the word
abend (de) / seara (ro) (evening) in Figure 2, the
phonological feature representation for German is:
0000100000001000010000000001
0001000100000000000010000000
0000100000000100000000010001
1000000100000010000000000000
1000000100000000000010000000,
with the feature base:
< alveolar, approximant, back, bilabial, cen-
tral, close, closemid, consonant, fricative, front,
glottal, labiodental, long, mid, nasal, nearclose,
nearopen, open, openmid, palatal, plosive,
postalveolar, rounded, short, unrounded, uvular,
velar, vowel >.
For Romanian, the phonological feature base
is:
< accented, affricate, approximant, back, bi-
labial, central, close, consonant, dental, fricative,
front, glottal, labiodental, mid, nasal, open,
plosive, postalveolar, rounded, trill, unrounded,
velar, voiced, voiceless, vowel >,
and the phonological feature representation
of the word changes accordingly.
4 Kernel Methods and String Kernels
Our hypothesis that the gender is in the name is
equivalent to proposing that there are sequences of
letters/sounds/phonological features that are more
common among nouns that share the same gender
or that can distinguish between nouns under differ-
ent genders. To determine whether that is the case,
we use a string kernel, which for a given string (se-
quence) generates a representation that consists of
all its substrings of length less than a parameter l.
The words are represented as strings with bound-
aries marked with a special character (?#?). The
high dimensional representation generated by the
string kernel is used to find a hyperplane that sep-
arates instances of different classes. In this section
we present in detail the kernel we use.
Kernel-based learning algorithms work by em-
bedding the data into a feature space (a Hilbert
space), and searching for linear relations in that
space. The embedding is performed implicitly,
that is by specifying the inner product between
each pair of points rather than by giving their co-
ordinates explicitly.
Given an input set X (the space of examples),
and an embedding vector space F (feature space),
let ? : X ? F be an embedding map called fea-
ture map.
A kernel is a function k, such that for all x, z ?
X , k(x, z) =< ?(x), ?(z) >, where < ., . >
denotes the inner product in F .
In the case of binary classification problems,
kernel-based learning algorithms look for a dis-
criminant function, a function that assigns +1 to
examples belonging to one class and ?1 to exam-
ples belonging to the other class. This function
will be a linear function in the space F , that means
it will have the form:
f(x) = sign(< w, ?(x) > +b),
for some weight vector w. The kernel can be
exploited whenever the weight vector can be ex-
pressed as a linear combination of the training
points,
n
?
i=1
?
i
?(x
i
), implying that f can be ex-
pressed as follows:
f(x) = sign(
n
?
i=1
?
i
k(x
i
, x) + b)
.
Various kernel methods differ in the way in
which they find the vector w (or equivalently the
vector ?). Support Vector Machines (SVM) try to
find the vector w that define the hyperplane that
maximum separate the images in F of the train-
ing examples belonging to the two classes. Math-
ematically SVMs choose the w and b that satisfy
the following optimization criterion:
min
w,b
1
n
n
?
i=1
[1? y
i
(< w, ?(x
i
) > +b)]
+
+ ?||w||
2
where y
i
is the label (+1/?1) of the training ex-
ample x
i
, ? a regularization parameter and [x]
+
=
max(x, 0).
1372
Kernel Ridge Regression (KRR) selects the vec-
tor w that simultaneously has small empirical er-
ror and small norm in Reproducing Kernel Hilbert
Space generated by kernel k. The resulting mini-
mization problem is:
min
w
1
n
n
?
i=1
(y
i
? < w, ?(x
i
) >)
2
+ ?||w||
2
where again y
i
is the label (+1/?1) of the training
example x
i
, and ? a regularization parameter. De-
tails about SVM and KRR can be found in (Taylor
and Cristianini, 2004). What is important is that
above optimization problems are solved in such a
way that the coordinates of the embedded points
are not needed, only their pairwise inner products
which in turn are given by the kernel function k.
SVM and KRR produce binary classifiers and
gender classification is a multi-class classification
problem. There are a lot of approaches for com-
bining binary classifiers to solve multi-class prob-
lems. We used one-vs-all scheme. For arguments
in favor of one-vs-all see (Rifkin and Klautau,
2004).
The kernel function offers to the kernel methods
the power to naturally handle input data that are
not in the form of numerical vectors, for example
strings. The kernel function captures the intuitive
notion of similarity between objects in a specific
domain and can be any function defined on the
respective domain that is symmetric and positive
definite. For strings, a lot of such kernel functions
exist with many applications in computational bi-
ology and computational linguistics (Taylor and
Cristianini, 2004).
Perhaps one of the most natural ways to mea-
sure the similarity of two strings is to count how
many substrings of length p the two strings have
in common. This give rise to the p-spectrum ker-
nel. Formally, for two strings over an alphabet ?,
s, t ? ?
?
, the p-spectrum kernel is defined as:
k
p
(s, t) =
?
v??
p
num
v
(s)num
v
(t)
where num
v
(s) is the number of occurrences of
string v as a substring in s 4 The feature map de-
fined by this kernel associate to each string a vec-
tor of dimension |?|p containing the histogram of
frequencies of all its substrings of length p. Taking
4Note that the notion of substring requires contiguity. See
(Taylor and Cristianini, 2004) for discussion about the am-
biguity between the terms ?substring? and ?subsequence?
across different traditions: biology, computer science.
into account all substrings of length less than p it
will be obtained a kernel that is called the blended
spectrum kernel:
k
p
1
(s, t) =
p
?
q=1
k
q
(s, t)
The blended spectrum kernel will be the ker-
nel that we will use in conjunction with SVM and
KRR. More precisely we will use a normalized
version of the kernel to allow a fair comparison
of strings of different length:
?
k
p
1
(s, t) =
k
p
1
(s, t)
?
k
p
1
(s, s)k
p
1
(t, t)
5 Experiments and Results
We performed 10-fold cross-validation learning
experiments with kernel ridge regression and the
string kernel (KRR-SK) presented in Section 4.
We used several baselines to compare the results
of the experiments against:
BL-R Gender is assigned following the distribu-
tion of genders in the data.
BL-M Gender is assigned following the majority
class (only for German, for Romanian we use
balanced data).
BL-S Gender is assigned based on suffix-gender
relation found in the literature. We use the
following mappings:
? German (Schumann, 2006):
feminine -ade, -age, -anz, -e, -ei, -enz,
-ette, -heit, -keit, -ik, -in, -ine, -ion, -
itis, -ive, -schaft, -ta?t, -tur, -ung, -ur;
masculine -ant, -er, -ich, -ismus, -ling;
neuter -chen, -ist, -lein, -ment, -nis, -o,
-tel, -um.
In our data set the most dominant gen-
der is feminine, therefore we assign this
gender to all nouns that do not match
any of the previous suffixes. Table 4
shows a few suffixes for each gender,
and an example noun.
? Romanian: in Romanian the word end-
ing is a strong clue for gender, especially
for feminine nouns: the vast majority
end in either -e or -a? (Doca, 2000). We
design a heuristic that assigns the gen-
der ?preferred? by the last letter ? the
1373
Method Accuracy masc. F-score fem. F-score neut. F-score
German
BL-R 33.79
BL-M 38.42
BL-S 51.35 40.83 62.42 26.69
KRR-SK 72.36 ? 3 64.88 ? 5 84.34 ? 4 64.44 ? 7
KRR-SK
noWB
66.91 58.77 79.19 58.26
Romanian
BL-R 33.3
BL-S 74.38 60.65 97.96 63.93
KRR-SK 78.83 ? 0.8 68.74 ? 0.9 98.05 ? 0.2 69.38 ? 2
KRR-SK no last letter 65.73 ? 0.6 56.11 ? 1 85.00 ? 0.5 55.05 ? 1
KRR-SK
noWB
77.36 67.54 96.75 67.39
Table 3: 10-fold cross-validation results ? accuracy and f-scores percentages (? variation over the 10
runs) ? for gender learning using string kernels
German
gender suffix example
fem. -e Ecke (corner)
-heit Freiheit (freedom)
-ie Komo?die (comedy)
masc. -er Fahrer (driver)
-ich Rettich (radish)
-ling Fru?hling (spring - season)
neut. -chen Ma?dchen (girl)
-nis Versta?ndnis (understanding)
-o Auto (car)
Table 4: Gender assigning rules and examples for
German
majority gender of all nouns ending in
the respective letter ? based on analy-
sis of our data. In Table 5 we include
some of the letter endings with an exam-
ple noun, and a percentage that shows
the precision of the ending in classify-
ing the noun in the gender indicated in
the table.
The results of our experiments are presented
in Table 3, in terms of overall accuracy, and f-
score for each gender. The performance presented
corresponds to the letter-based representation of
words. It is interesting to note that this represen-
tation performed overall better than the phoneme
or phonological feature-based ones. An explana-
Romanian
gender ending example Prec.
fem. -a? masa? (table) 98.04
-e pa?ine (bread) 97.89
masc. -g sociolog (sociologist) 72.77
-r nor (cloud) 66.89
-n domn (gentleman) 58.45
neut. -m algoritm (algorithm) 90.95
-s vers (verse) 66.97
-t eveniment (event) 51.02
Table 5: Word-ending precision on classifying
gender and examples for Romanian
tion may be that in both the languages we consid-
ered, there is an (almost) one-to-one mapping be-
tween letters and their pronunciation, making thus
the pronunciation-based representation unneces-
sary. As such, the letter level captures the interest-
ing commonalities, without the need to go down
to the phoneme-level.
We performed experiments for Romanian when
the last letter of the word is removed. The reason
for this batch of experiments is to further test the
hypothesis that gender is more deeply encoded in a
word form than just the word ending. For both lan-
guages we observe statistically significant higher
performance than all baselines. For Romanian,
the last letter heuristic gives a very high baseline,
confirming that Romanian has strong phonologi-
cal cues for gender in the ending. Had the word
ending been the only clue to the word?s gender,
1374
 0
 20
 40
 60
 80
 100
 0  5  10  15  20  25
# of letters considered from end
Romanian
classification accuracy
word length percentages
 30
 35
 40
 45
 50
 55
 60
 65
 70
 75
 80
 0  1  2  3  4  5  6  7
a
cc
u
ra
cy
# of letters cut from end
Romanian
word without last letters
baseline
 0
 20
 40
 60
 80
 100
 0  2  4  6  8  10  12  14  16  18  20
# of letters considered from end
German
classification accuracy
word length percentages
 35
 40
 45
 50
 55
 60
 65
 70
 75
 0  1  2  3  4  5  6
a
cc
u
ra
cy
# of letters cut from end
German
word without last letters
baseline
Figure 1: Gender prediction based on the last N letters, and based on the word minus the last N letters
once it is removed the performance on recogniz-
ing gender should be close to the random assign-
ment. This is not the case, and the improvement
over the random baseline is 32% points. It is inter-
esting to notice that when cutting off the last letter
the class for which the gender assignment heuris-
tic was clearest ? the feminine class with -a? and
-e endings ? the performance remains very high ?
85% F-score.
To further test where the gender indicators are
located, we performed two more sets of experi-
ments: (i) classify words in their corresponding
gender class using the word minus the last N let-
ters; (ii) classify words based on the last N let-
ters. The results of these experiments in terms of
accuracy are presented in Figure 1. When con-
sidering only the last N letters the performance is
high for both German and Romanian, as expected
if the gender indicators are concentrated at the end
of the word. It is interesting though to notice the
results of classification based on the word without
the last N letters. The prediction accuracy mono-
tonically decreases, but remains above the base-
line until more than 6 letters are cut. Because as
letters are cut some words completely disappear,
the baseline changes accordingly. 94.07% of the
words have a length of at most 12 letters in the
Romanian dataset, and 96.07% in the German one.
Because gender prediction can be done with accu-
racy higher than the random baseline even after 6
letters are cut from the ending of the word indicate
that for more than 94% of the words considered,
gender clues are spread over more than the second
half of the word. Again, we remind the reader that
the word forms are in nominative case, with no
case or number inflections (which are strong indi-
cators of gender in both Romanian and German).
Except for lines KRR ? SK
noWB
, the results
in Table 3 are obtained through experiments con-
ducted on words containing word boundary mark-
ers, as indicated in Section 4. Because of these
markers, word starting or word ending substrings
are distinct from all the others, and information
about their position in the original word is thus
preserved. To further explore the idea that gender
indicators are not located only in word endings,
we ran classification experiments for German and
Romanian when the word representation does not
contain word boundary markers. This means that
the substrings generated by the string kernel have
1375
no position information. The results of these ex-
periments are presented in rows KRR?SK
noWB
in Table 3. The accuracy is slightly lower than the
best results obtained when word boundaries are
marked and the entire word form is used. How-
ever, they are well above all the baselines consid-
ered, without no information about word endings.
For both German and Romanian, the gender that
was learned best was feminine. For German part
of this effect is due to the fact that the feminine
class is more numerous in the data. For Roma-
nian the data was perfectly balanced, so there is no
such bias. Neuter and masculine nouns have lower
learning performance. For Romanian, a contri-
bution to this effect is the fact that neuter nouns
behave as masculine nouns in their singular form
(take the same articles, inflections, derivations),
but as feminine in the plural, and our data consists
of nouns in singular form. It would seem that from
an orthographic point of view, neuter and mascu-
line nouns are closer to each other than to feminine
nouns.
From the reviewed related work, the one that
uses the word form to determine gender is
Cucerzan and Yarowsky (2003) for Romanian.
There are two important differences with respect
to the approach presented here. First, they con-
sider words in context, which are inflected for
number and case. Number and case inflections
are reflected in suffixes that are gender specific.
The words considered here are in singular form,
nominative case ? as such, with no inflections.
Second, Cucerzan and Yarowsky consider two
classes: feminine vs. masculine and neuter. Mas-
culine and neuter nouns are harder to distinguish,
as in singular form neuter nouns behave like mas-
culine nouns in Romanian. While the datasets and
word forms used by Cucerzan and Yarowsky are
different than the one used here, the reader may
be curious how well the word form distinguishes
between feminine and the other two classes in
the experimental set-up used here. On the full5
Romanian dataset described in Section 3, a two
class classification gives 99.17% accuracy. When
predicting gender for all words in their dataset,
Cucerzan and Yarowsky obtain 98.25% accuracy.
6 Conclusion
When a speaker of a genderless language tries to
learn a language with grammatical gender, it is
5By ?full? we mean the dataset before balancing the
classes 48,477 instances (see Table 1).
very tempting to try to assign grammatical gen-
der based on perceived or guessed natural gender
types. This does not work out well, and it only
serves to confuse the learner even more, when he
finds out that nouns expressing concepts with clear
feminine or masculine natural gender will have the
opposite or a neutral grammatical gender, or that
one concept can be referred to through names that
have different grammatical genders. Going with
the flow of the language seems to be a better idea,
and allow the sound of a word to dictate the gen-
der.
In this paper we have investigated the hypothe-
sis that gender is encoded in the word form, and
this encoding is more than just the word endings
as it is commonly believed. The results obtained
show that gender assignment based on word form
analysis can be done with high accuracy ? 72.36%
for German, and 78.83% for Romanian. Existing
gender assignment rules based on word endings
have lower accuracy. We have further strength-
ened the point by conducting experiments on Ro-
manian nouns without tell-tale word endings. The
accuracy remains high, with remarkably high per-
formance in terms of F-score for the feminine
class (85%). This leads us to believe that gen-
der information is somehow redundantly coded in
a word. We plan to look closer at cases where
we obtain different predictions based on the word
ending and the full form of the word, and use
boosting to learn weights for classifiers based on
different parts of the word to see whether we can
further improve the results.
As we have underlined before, word form simi-
larity between words under the same gender is one
criterion for gender assignment. It would be in-
teresting to verify whether gender recognition can
be boosted by using lexical resources that capture
the semantics of the words, such as WordNets or
knowledge extracted from Wikipedia, and verify
whether similarities from a semantic point of view
are also responsible for gender assignments in var-
ious languages.
References
Ana-Maria Barbu. 2008. Romanian lexical data
bases: Inflected and syllabic forms dictionar-
ies. In Proceedings of the Sixth International
Language Resources and Evaluation (LREC?08).
http://www.lrec-conf.org/proceedings/lrec2008/.
Sharon Bedgley. 2002. Strawberry is
no blackberry: Building brands us-
1376
ing sound. http://online.wsj.com/article/
0,,SB1030310730179474675.djm,00.html.
Leonard Bloomfield. 1933. Language. Holt, Reinhart
& Winston, New York.
Marcel Botton, Jean-Jack Cegarra, and Beatrice Fer-
rari. 2002. Il nome della marca: creazione e strate-
gia di naming, 3rd edition. Guerini e Associati.
Roger Brown. 1958. Words and Things. The Free
Press, New York.
Karl Brugmann. 1889. Das Nominalgeschlecht
in den indogermanischen Sprachen. In Inter-
nationale Zeitschrift fu?r allgemenine Sprachwis-
senschaft, pages 100?109.
S. Cucerzan and D. Yarowsky. 2003. Minimally super-
vised induction of grammatical gender. In Proceed-
ings of HLT-NAACL 2003, pages 40?47.
Ferdinand de Saussure. 1916. Cours de linguistique
ge?ne?rale. Harrassowitz, Wiesbaden.
Gheorghe Doca Doca. 2000. Romanian language. Vol.
II: Morpho-Syntactic and Lexical Structures. Ars
Docendi, Bucharest, Romania.
John Rupert Firth. 1951. Modes and meaning. In
Papers in linguistics 1934-1951. Oxford University
Press, London.
Angela Friederici and Thomas Jacobsen. 1999. Pro-
cessing grammatical gender during language com-
prehension. Journal of Psychological Research,
28(5):467?484.
Jacob Grimm. 1890. Deutsche Grammatik.
Roman Jakobson. 1937. Lectures on Sound and Mean-
ing. MIT Press, Cambridge, MA.
Otto Jespersen. 1922. Language - its Nature, Devel-
opment and Origin. George Allen & Unwim Ltd.,
London.
Marcin Kilarski. 2007. On grammatical gender as an
arbitrary and redundant category. In Douglas Kil-
bee, editor, History of Linguistics 2005: Selected pa-
pers from the 10th International Conference on the
History of Language Sciences (ICHOLS X), pages
24?36. John Benjamins, Amsterdam.
Eric Margolis and Stephen Laurence, editors. 1999.
Concepts: Core Readings. MIT Press.
Ryan Rifkin and Aldebaro Klautau. 2004. In de-
fense of one-vs-all classification. Journal of Ma-
chine Learning Research, 5(January):101?141.
Johannes Schumann. 2006. Mittelstufe Deutsch. Max
Hueber Verlag.
Peter Sells and Sierra Gonzales.
2003. The language of advertising.
http://www.stanford.edu/class/linguist34/; in
particular unit 8: ?/Unit 08/blackberry.htm.
Katharina Spalek, Julie Franck, Herbert Schriefers, and
Ulrich Frauenfelder. 2008. Phonological regulari-
ties and grammatical gender retrieval in spoken word
recognition and word production. Journal of Psy-
cholinguistic Research, 37(6):419?442.
John S. Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge Univer-
sity Press, New York, NY, USA.
Barbara Unterbeck, editor. 1999. Gender in Grammar
and Cognition. Approaches to Gender. Trends in
Linguistics. Studies and Monographs. 124. Mouton
de Gruyter.
Gabriela Vigliocco, David Vinson, Peter Indefrey,
Willem Levelt, and Frauke Hellwig. 2004a. Role of
grammatical gender and semantics in german word
production. Journal of Experimental Psychology:
Learning, Memory and Cognition, 30(2):483?497.
Gabriela Vigliocco, David Vinson, and Federica Pa-
ganelli. 2004b. Grammatical gender and meaning.
In Proc. of the 26th Meeting of the Cognitive Science
Society.
George Zipf. 1935. The Psychobiology of Language.
Addison-Wesley.
David Zubin and Klaus-Michael Ko?pcke. 1981. Gen-
der: A less than arbitrary grammatical category. In
R. Hendrick, C. Masek, and M. F. Miller, editors,
Papers from the seventh regional meeting, pages
439?449. Chicago Linguistic Society, Chicago.
David Zubin and Klaus-Michael Ko?pcke. 1986. Gen-
der and folk taxonomy: The indexical relation be-
tween grammatical and lexical categorization. In
C. Craig, editor, Noun classes and categorization,
pages 139?180. Benjamins, Philadelphia.
1377
Regularized Least-Squares Classification for Word Sense
Disambiguation
Marius Popescu
Department of Computer Science, University of Bucharest
Str. Academiei 14
70109 Bucharest,
Romania,
mpopescu@phobos.cs.unibuc.ro
Abstract
The paper describes RLSC-LIN and RLSC-
COMB systems which participated in the
Senseval-3 English lexical sample task. These
systems are based on Regularized Least-Squares
Classification (RLSC) learning method. We
describe the reasons of choosing this method,
how we applied it to word sense disambigua-
tion, what results we obtained on Senseval-
1, Senseval-2 and Senseval-3 data and discuss
some possible improvements.
1 Introduction
Word sense disambiguation can be viewed as
a classification problem and one way to ob-
tain a classifier is by machine learning methods.
Unfortunately, there is no single one universal
good learning procedure. The No Free Lunch
Theorem assures us that we can not design a
good learning algorithm without any assump-
tions about the structure of the problem. So,
we start by trying to find out what are the par-
ticular characteristics of the learning problem
posed by the word sense disambiguation.
In our opinion, one of the most important
particularities of the word sense disambiguation
learning problem, seems to be the dimensional-
ity problem, more specifically the fact that the
number of features is much greater than the
number of training examples. This is clearly
true about data in Senseval-1, Senseval-2 and
Senseval-3. One can argue that this happens
because of the small number of training exam-
ples in these data sets, but we think that this
is an intrinsic propriety of learning task in the
case of word sense disambiguation.
In word sense disambiguation one important
knowledge source is the words that co-occur (in
local or broad context) with the word that had
to be disambiguated, and every different word
that appears in the training examples will be-
come a feature. Increasing the number of train-
ing examples will increase also the number of
different words that appear in the training ex-
amples, and so will increase the number of fea-
tures. Obviously, the rate of growth will not be
the same, but we consider that for any reason-
able number of training examples (reasonable as
the possibility of obtaining these training exam-
ples and as the capacity of processing, learning
from these examples) the dimension of the fea-
ture space will be greater.
Actually, the high dimensionality of the fea-
ture space with respect to the number of exam-
ples is a general scenario of learning in the case
of Natural Language Processing tasks and word
sense disambiguation is one of these examples.
In such situations, when the dimension of
the feature space is greater than the number
of training examples, the potential for over-
fitting is huge and some form of regulariza-
tion is needed. This is the reason why we
chose to use Regularized Least-Squares Classifi-
cation (RLSC) (Rifkin, 2002; Poggio and Smale,
2003), a method of learning based on kernels
and Tikhonov regularization.
In the next section we explain what source
of information we used and how this informa-
tion is transformed into features. In section 3
we briefly describe the RLSC learning algorithm
and in section 4, how we applied this algorithm
for word sense disambiguation and what results
we have obtained. Finally, in section 5, we dis-
cuss some possible improvements.
2 Knowledge Sources and Feature
Space
We follow the common practice (Yarowsky,
1993; Florian and Yarowsky, 2002; Lee and Ng,
2002) to represent the training instances as fea-
ture vectors. This features are derived from var-
ious knowledge sources. We used the following
knowledge sources:
? Local information:
? the word form of words that appear
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
near the target word in a window of
size 3
? the part-of-speech (POS) tags that ap-
pear near the target word in a window
of size 3
? the lexical form of the target word
? the POS tag of the target word
? Broad context information:
? the lemmas of all words that appear
in the provided context of target word
(stop words are removed)
In the case of broad context we use the
bag-of-words representation with two weighting
schema. Binary weighting for RLSC-LIN and
term frequency weighting1 for RLSC-COMB.
For stemming we used Porter stem-
mer (Porter, 1980) and for tagging we used
Brill tagger (Brill, 1995).
3 RLSC
RLSC (Rifkin, 2002; Poggio and Smale, 2003)
is a learning method that obtains solutions for
binary classification problems via Tikhonov reg-
ularization in a Reproducing Kernel Hilbert
Space using the square loss.
Let S = (x1, y1), . . . , (xn, yn) be a training
sample with xi ? Rd and yi ? {?1, 1} for all i.
The hypothesis space H of RLSC is the set of
functions f : Rd ? R of the form:
f(x) =
n
?
i=1
cik(x,xi)
with ci ? R for all i and k : Rd ? Rd ? R
a kernel function (a symmetric positive definite
function) that measures the similarity between
two instances.
RLSC tries to find a function from this hy-
pothesis space that simultaneously has small
empirical error and small norm in Reproduc-
ing Kernel Hilbert Space generated by kernel k.
The resulting minimization problem is:
min
f?H
1
n
n
?
i=1
(yi ? f(xi))2 + ??f?2K
In spite of the complex mathematical tools
used, the resulted learning algorithm is a very
1We didn?t use any kind of smoothing. The weight of
a term is simply the number of time the term appears in
the context divided by the length of the context.
simple one (for details of how this algorithm is
derived see Rifkin, 2002):
? From the training set S =
(x1, y1), . . . , (xn, yn) construct the
kernel matrix K
K = (kij)1?i,j?n kij = k(xi,xj)
? Compute the vector of coefficients c =
(c1, . . . , cn)? by solving the system of lin-
ear equations:
(K + n?I)c = y
c = (K + n?I)?1y
where y = (y1, . . . , yn)? and I is the iden-
tity matrix of dimension n
? Form the classifier:
f(x) =
n
?
i=1
cik(x,xi)
The sign(f(x)) will be interpreted as the pre-
dicted label (?1 or +1) to be assigned to in-
stance x, and the magnitude |f(x)| as the con-
fidence in this prediction.
4 Applying RLSC to Word Sense
Disambiguation
To apply the RLSC learning method we must
take care about some details.
First, RLSC produces a binary classifier and
word sense disambiguation is a multi-class clas-
sification problem. There are a lot of ap-
proaches for combining binary classifiers to
solve multi-class problems. We used one-vs-all
scheme. We trained a different binary classi-
fier for each sense. For a word with m senses
we train m different binary classifiers, each one
being trained to distinguish the examples in a
single class from the examples in all remaining
classes. When a new example had to be classi-
fied, the m classifiers are run, and the classifier
with the highest confidence, which outputs the
largest (most positive) value, is chosen. If more
than one such classifiers exists, than from the
senses output by these classifiers we chose the
one that appears most frequently in the train-
ing set. One advantage of one-vs-all combining
scheme is the fact that it exploits the confidence
(real value) of classifiers produced by RLSC. For
more arguments in favor of one-vs-all see (Rifkin
and Klautau, 2004).
Second, RLSC needs a kernel function.
Preliminary experiments with Senseval-1 and
Senseval-2 data show us that the best perfor-
mance is obtained by linear kernel. This obser-
vation agrees with the Lee and Ng results (Lee
and Ng, 2002), that in the case of SVM also
have obtained the best performance with linear
kernel. One-vs-all combining scheme requires
comparison of confidences output by different
classifiers, and for an unbiased comparison the
real values produced by classifiers correspond-
ing to different senses of the target word must
be on the same scale. To achieve this goal we
need a normalized version of linear kernel.
Our first system RLSC-LIN used the follow-
ing kernel:
k(x,y) = < x,y >?x??y?
where x and y are two instances (feature vec-
tors), < ?, ? > is the dot product on Rd and ? ? ?
is the L2 norm on Rd.
In the case of RLSC-LIN we used a binary
weighting scheme for coding broad context. In
the RLSC-COMB we tried to obtain more in-
formation from broad context and we used a
term frequency weighting scheme. Now, the fea-
ture vectors will have apart form 0 two kind of
values: 1 for features that encode local informa-
tion and much small values (of order of 10?2)
for features encoding broad context. A simple
linear kernel will not work in this case because
its value will be dominated by the similarity of
local contexts. To solve this problem we split
the kernel in two parts:
k(x,y) = 12kl(x,y) +
1
2kb(x,y)
where kl is a linear normalized kernel that uses
only the components of the feature vectors that
encode local information (and have 0/1 values)
and kb is a normalized kernel that uses only the
components of the feature vectors that encode
broad context.
The last detail concerning application of
RLSC is the value of regularization parameter
?. Experimenting on Senseval-1 and Senseval-
2 data sets we establish that small values of ?
achieve best performance. In all reported re-
sults we used ? = 10?9.
The results2 of RLSC-LIN and RLSC-
2The coarse-grained score on Senseval-3 for both
RLSC-LIN and RLSC-COMB was 0.784
COMB on Senseval-1, Senseval-2 and Senseval-
3 data are summarized in Table 1.
RLSC-LIN RLSC-COMB
Senseval-1 0.772 0.775
Senseval-2 0.652 0.656
Senseval-3 0.718 0.722
Table 1: Fine-grained score forRLSC-LIN and
RLSC-COMB on Senseval data sets
Because RLSC has many points in common
with the well-known Support Vector Machine
(SVM), we list in Table 2 for comparison the
results obtained by SVM with the same kernels.
SVM-LIN SVM-COMB
Senseval-1 0.771 0.773
Senseval-2 0.644 0.642
Senseval-3 0.714 0.708
Table 2: Fine-grained score for SVM-LIN and
SVM-COMB on Senseval data sets
The results are competitive with the state of
the art results reported until now. For exam-
ple the best two results reported until now on
Senseval-2 data are 0.654 (Lee and Ng, 2002)
obtained with SVM and 0.665 (Florian and
Yarowsky, 2002) obtained by classifiers combi-
nation.
The results are especially good if we take into
account the fact that our systems do not use
syntactic information3 while the others do. Lee
and Ng (Lee and Ng, 2002) report a fine-grained
score for SVM of only 0.648 if they do not use
syntactic knowledge source.
These results encourage us to participate
with RLSC-LIN and RLSC-COMB to the
Senseval-3 competition.
5 Possible Improvements
First evident improvement is to incorporate
syntactic information as knowledge source into
our systems.
It is quite possible to substantially improve
the results of RLSC-COMB using a combina-
tion of more adequate kernels (each kernel in the
combination being adequate to the source of in-
formation represented by the part of the feature
3It takes too long to adapt to our systems a parser
(to prepare the data for parsing, parse it with a free
statistical parser and extract useful features from the
parser output)
vector that the kernel uses). For example, we
can use a combination of a linear kernel for local
information a string kernel (Lodhi et al, 2002)
for broad context and a tree kernel (Collins and
Duffy, 2002) for syntactic relations.
Also, instead of using an equal weight for each
kernel in the combination we can use weights4
that reflect the importance for disambiguation
of knowledge source that the kernel uses, or we
can establish the weight of each kernel experi-
mentally by kernel-target algnment (Cristianini
et al, 2002).
References
Eric Brill. 1995. Transformation-based error-
driven learning and natural language process-
ing: A case study in part of speech tagging.
Computational Linguistics, 21(4):543?565.
M. Collins and N. Duffy. 2002. Convolution
kernels for natural language. In T. G. Di-
etterich, S. Becker, and Z. Ghahramani, ed-
itors, Advances in Neural Information Pro-
cessing Systems 14, pages 625?632, Cam-
bridge, MA. MIT Press.
N. Cristianini, J. Shawe-Taylor, A. Elisseeff,
and J. Kandola. 2002. On kernel-target
alignment. In T. G. Dietterich, S. Becker, and
Z. Ghahramani, editors, Advances in Neu-
ral Information Processing Systems 14, pages
367?373, Cambridge, MA. MIT Press.
Radu Florian and David Yarowsky. 2002. Mod-
eling consensus: Classifier combination for
word sense disambiguation. In Proceedings of
EMNLP?02, pages 25?32, Philadelphia, PA,
USA.
Yoong Lee and Hwee Ng. 2002. An empirical
evaluation of knowledge sources and learn-
ing algorithms for word sense disambiguation.
In Proceedings of EMNLP?02, pages 41?48,
Philadelphia, PA, USA.
Huma Lodhi, Craig Saunders, John Shawe-
Taylor, Nello Cristianini, and Chris Watkins.
2002. Text classification using string ker-
nels. Journal of Machine Learning Research,
2(February):419?444.
Tomaso Poggio and Steve Smale. 2003. The
mathematics of learning: Dealing with data.
Notices of the American Mathematical Soci-
ety (AMS), 50(5):537?544.
Martin Porter. 1980. An algorithm for suffix
stripping. Program, 14(3):130?137.
Ryan Rifkin and Aldebaro Klautau. 2004. In
4The weights must sum to one
defense of one-vs-all classification. Journal of
Machine Learning Research, 5(January):101?
141.
Ryan Rifkin. 2002. Everything Old Is New
Again: A Fresh Look at Historical Approaches
to Machine Learning. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
David Yarowsky. 1993. One sense per colloca-
tion. In ARPA Human Language Technology
Workshop, pages 266?271, Princeton, USA.
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1363?1373,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Can characters reveal your native language? A language-independent
approach to native language identification
Radu Tudor Ionescu

, Marius Popescu

, Aoife Cahill
?

University of Bucharest
Department of Computer Science
14 Academiei, Bucharest, Romania
raducu.ionescu@gmail.com
popescunmarius@gmail.com
?
Educational Testing Service
660 Rosedale Rd
Princeton, NJ 08541, USA
acahill@ets.org
Abstract
A common approach in text mining tasks
such as text categorization, authorship
identification or plagiarism detection is to
rely on features like words, part-of-speech
tags, stems, or some other high-level lin-
guistic features. In this work, an approach
that uses character n-grams as features is
proposed for the task of native language
identification. Instead of doing standard
feature selection, the proposed approach
combines several string kernels using mul-
tiple kernel learning. Kernel Ridge Re-
gression and Kernel Discriminant Analy-
sis are independently used in the learning
stage. The empirical results obtained in all
the experiments conducted in this work in-
dicate that the proposed approach achieves
state of the art performance in native lan-
guage identification, reaching an accuracy
that is 1.7% above the top scoring system
of the 2013 NLI Shared Task. Further-
more, the proposed approach has an im-
portant advantage in that it is language in-
dependent and linguistic theory neutral. In
the cross-corpus experiment, the proposed
approach shows that it can also be topic
independent, improving the state of the art
system by 32.3%.
1 Introduction
Using words as basic units is natural in textual
analysis tasks such as text categorization, author-
ship identification or plagiarism detection. Per-
haps surprisingly, recent results indicate that meth-
ods handling the text at the character level can
also be very effective (Lodhi et al., 2002; Sander-
son and Guenter, 2006; Popescu and Dinu, 2007;
Grozea et al., 2009; Popescu, 2011; Popescu and
Grozea, 2012). By disregarding features of natu-
ral language such as words, phrases, or meaning,
an approach that works at the character level has
an important advantage in that it is language inde-
pendent and linguistic theory neutral. This paper
presents a state of the art machine learning system
for native language identification that works at the
character level. The proposed system is inspired
by the system of Popescu and Ionescu (2013), but
includes some variations and improvements. A
major improvement is that several string kernels
are combined via multiple kernel learning (Shawe-
Taylor and Cristianini, 2004). Despite the fact that
the (histogram) intersection kernel is very popular
in computer vision (Maji et al., 2008; Vedaldi and
Zisserman, 2010), it has never been used before in
text mining. In this work, the intersection kernel is
used for the first time in a text categorization task,
alone and in combination with other kernels. The
intersection kernel lies somewhere in the middle
between the kernel that takes into account only the
presence of n-grams and the kernel based on the
frequency of n-grams (p-spectrum string kernel).
Two kernel classifiers are proposed for the
learning task, namely Kernel Ridge Regression
(KRR) and Kernel Discriminant Analysis (KDA).
The KDA classifier is able to avoid the class-
masking problem (Hastie and Tibshirani, 2003),
which may often arise in the context of native
language identification. Several experiments are
conducted to evaluate the performance of the ap-
proach proposed in this work. While multiple ker-
nel learning seems to produce a more robust sys-
tem, the two kernel classifiers obtained mixed re-
sults in the experiments. Overall, the empirical re-
sults indicate that the approach proposed in this
paper achieves state of the art performance in na-
tive language identification, while being both lan-
1363
guage independent and linguistic theory neutral.
Furthermore, the approach based on string kernels
does not need any expert knowledge of words or
phrases in the language.
The paper is organized as follows. Related
work is presented in Section 2. Section 3 presents
several similarity measures for strings, including
string kernels and Local Rank Distance. The
learning methods used in the experiments are de-
scribed in Section 4. Section 5 presents details
about the experiments. Finally, the conclusions are
drawn in Section 6.
2 Related Work
2.1 Native Language Identification
The goal of automatic native language identifica-
tion (NLI) is to determine the native language of
a language learner, based on a piece of writing in
a foreign language. This can provide useful in-
formation in forensic linguistic tasks (Estival et
al., 2007) or could be used in an educational set-
ting to provide contrastive feedback to language
learners. Most research has focused on identify-
ing the native language of English language learn-
ers, though there have been some efforts recently
to identify the native language of writing in other
languages (Malmasi and Dras, 2014).
In general most approaches to NLI have used
multi-way classification with SVMs or similar
models along with a range of linguistic features.
The seminal paper by Koppel et al. (2005) intro-
duced some of the best-performing features: char-
acter, word and part-of-speech n-grams along with
features inspired by the work in the area of second-
language acquisition such as spelling and gram-
matical errors. In 2013, Tetreault et al. (2013) or-
ganized the first shared task in the field. This al-
lowed researchers to compare approaches for the
first time on a specifically designed NLI corpus
that was much larger than previously available
data sets. In the shared task, 29 teams submit-
ted results for the test set, and one of the most
successful aspects of the competition was that it
drew submissions from teams working in a variety
of research fields. The submitted systems utilized
a wide range of machine learning approaches,
combined with several innovative feature contri-
butions. The best performing system achieved an
overall accuracy of 83.6% on the 11-way classifi-
cation of the test set, although there was no signif-
icant difference between the top teams.
2.2 Methods that Work at the Character
Level
In recent years, methods of handling text at
the character level have demonstrated impres-
sive performance levels in various text analy-
sis tasks (Lodhi et al., 2002; Sanderson and
Guenter, 2006; Popescu and Dinu, 2007; Grozea
et al., 2009; Popescu, 2011; Popescu and Grozea,
2012). Lodhi et al. (2002) used string kernels
for document categorization with very good re-
sults. String kernels were also successfully used in
authorship identification (Sanderson and Guenter,
2006; Popescu and Dinu, 2007; Popescu and
Grozea, 2012). For example, the system described
in (Popescu and Grozea, 2012) ranked first in most
problems and overall in the PAN 2012 Traditional
Authorship Attribution tasks.
Using string kernels makes the corresponding
learning method completely language indepen-
dent, because the texts will be treated as sequences
of symbols (strings). Methods working at the
word level or above very often restrict their feature
space according to theoretical or empirical princi-
ples. For instance, they select only features that re-
flect various types of spelling errors or only some
type of words, such as function words. These fea-
tures prove to be very effective for specific tasks,
but it is possible that other good features also ex-
ist. String kernels embed the texts in a very large
feature space, given by all the substrings of length
p, and leave it to the learning algorithm to select
important features for the specific task, by highly
weighting these features. It is important to note
that this approach is also linguistic theory neutral,
since it disregards any features of natural language
such as words, phrases, or meaning. On the other
hand, a method that considers words as features
cannot be completely language independent, since
the definition of a word is necessarily language-
specific. For example, a method that uses only
function words as features is not completely lan-
guage independent because it needs a list of func-
tion words which is specific to a language. When
features such as part-of-speech tags are used, as
in the work of Jarvis et al. (2013), the method re-
lies on a part-of-speech tagger which might not be
available (yet) for some languages. Furthermore,
a way to segment a text into words is not an easy
task for some languages, such as Chinese.
Character n-grams are used by some of the sys-
tems developed for native language identification.
1364
In work where feature ablation results have been
reported, the performance with only character n-
gram features was modest compared to other types
of features (Tetreault et al., 2012). Initially, most
work limited the character features to unigrams,
bigrams and trigrams, perhaps because longer n-
grams were considered too expensive to compute
or unlikely to improve performance. However,
some of the top systems in the 2013 NLI Shared
Task were based on longer character n-grams,
up to 9-grams (Jarvis et al., 2013; Popescu and
Ionescu, 2013). The results presented in this work
are obtained using a range of 5?8 n-grams. Com-
bining all 5?8 n-grams would generate millions
of features, which are indeed expensive to com-
pute and represent. The key to avoiding the com-
putation of such a large number of features lies
in using the dual representation provided by the
string kernel. String kernel similarity matrices can
be computed much faster and are extremely useful
when the number of samples is much lower than
the number of features.
3 Similarity Measures for Strings
3.1 String Kernels
The kernel function gives kernel methods the
power to naturally handle input data that is not
in the form of numerical vectors, e.g. strings.
The kernel function captures the intuitive notion
of similarity between objects in a specific domain
and can be any function defined on the respec-
tive domain that is symmetric and positive definite.
For strings, many such kernel functions exist with
various applications in computational biology and
computational linguistics (Shawe-Taylor and Cris-
tianini, 2004).
Perhaps one of the most natural ways to mea-
sure the similarity of two strings is to count how
many substrings of length p the two strings have
in common. This gives rise to the p-spectrum ker-
nel. Formally, for two strings over an alphabet ?,
s, t ? ?
?
, the p-spectrum kernel is defined as:
k
p
(s, t) =
?
v??
p
num
v
(s) ? num
v
(t),
where num
v
(s) is the number of occurrences of
string v as a substring in s.
1
The feature map de-
1
Note that the notion of substring requires contiguity.
Shawe-Taylor and Cristianini (2004) discuss the ambiguity
between the terms substring and subsequence across differ-
ent domains: biology, computer science.
fined by this kernel associates a vector of dimen-
sion |?|
p
containing the histogram of frequencies
of all its substrings of length p (p-grams) with each
string.
A variant of this kernel can be obtained if the
embedding feature map is modified to associate a
vector of dimension |?|
p
containing the presence
bits (instead of frequencies) of all its substrings of
length p with each string. Thus, the character p-
grams presence bits kernel is obtained:
k
0/1
p
(s, t) =
?
v??
p
in
v
(s) ? in
v
(t),
where in
v
(s) is 1 if string v occurs as a substring
in s, and 0 otherwise.
In computer vision, the (histogram) intersec-
tion kernel has successfully been used for object
class recognition from images (Maji et al., 2008;
Vedaldi and Zisserman, 2010). In this paper, the
intersection kernel is used for the first time as a
kernel for strings. The intersection string kernel is
defined as follows:
k
?
p
(s, t) =
?
v??
p
min{num
v
(s), num
v
(t)},
where num
v
(s) is the number of occurrences of
string v as a substring in s.
For the p-spectrum kernel, the frequency of a p-
gram has a very significant contribution to the ker-
nel, since it considers the product of such frequen-
cies. On the other hand, the frequency of a p-gram
is completely disregarded in the p-grams presence
bits kernel. The intersection kernel lies some-
where in the middle between the p-grams presence
bits kernel and p-spectrum kernel, in the sense that
the frequency of a p-gram has a moderate contri-
bution to the intersection kernel. More precisely,
the following inequality that describes the relation
between the three kernels holds:
k
0/1
p
(s, t) ? k
?
p
(s, t) ? k
p
(s, t).
What is actually more interesting is that the inter-
section kernel assigns a high score to a p-gram if it
has a high frequency in both strings, since it con-
siders the minimum of the two frequencies. The
p-spectrum kernel assigns a high score even when
the p-gram has a high frequency in only one of
the two strings. Thus, the intersection kernel cap-
tures something about the correlation between the
p-gram frequencies in the two strings, which may
lead to a more sensitive similarity between strings.
1365
Normalized versions of these kernels ensure a
fair comparison of strings of different lengths:
?
k
p
(s, t) =
k
p
(s, t)
?
k
p
(s, s) ? k
p
(t, t)
,
?
k
0/1
p
(s, t) =
k
0/1
p
(s, t)
?
k
0/1
p
(s, s) ? k
0/1
p
(t, t)
,
?
k
?
p
(s, t) =
k
?
p
(s, t)
?
k
?
p
(s, s) ? k
?
p
(t, t)
.
Taking into account p-grams of different length
and summing up the corresponding kernels, new
kernels, termed blended spectrum kernels, can be
obtained.
The string kernel implicitly embeds the texts
in a high dimensional feature space. Then, a
kernel-based learning algorithm implicitly assigns
a weight to each feature, thus selecting the fea-
tures that are important for the discrimination task.
For example, in the case of text categorization
the learning algorithm enhances the features rep-
resenting stems of content words (Lodhi et al.,
2002), while in the case of authorship identifica-
tion the same learning algorithm enhances the fea-
tures representing function words (Popescu and
Dinu, 2007).
3.2 Local Rank Distance
A recently introduced distance measure, termed
Local Rank Distance (Ionescu, 2013), comes from
the idea of better adapting rank distance (Dinu,
2003) to string data, in order to capture a bet-
ter similarity between strings, such as DNA se-
quences or text. Local Rank Distance (LRD) has
already shown promising results in computational
biology (Ionescu, 2013) and native language iden-
tification (Popescu and Ionescu, 2013).
In order to describe LRD, the following nota-
tions are defined. Given a string x over an al-
phabet ?, and a character a ? ?, the length of
x is denoted by |x|. Strings are considered to
be indexed starting from position 1, that is x =
x[1]x[2] ? ? ?x[|x|]. Moreover, x[i : j] denotes its
substring x[i]x[i+ 1] ? ? ?x[j ? 1].
Local Rank Distance is inspired by rank dis-
tance (Dinu, 2003), the main differences being
that it uses p-grams instead of single charac-
ters, and that it matches each p-gram in the first
string with the nearest equal p-gram in the second
string. Given a fixed integer p ? 1, a thresh-
old m ? 1, and two strings x and y over ?,
the Local Rank Distance between x and y, de-
noted by ?
LRD
(x, y), is defined through the fol-
lowing algorithmic process. For each position i in
x (1 ? i ? |x|?p+1), the algorithm searches for
that position j in y (1 ? j ? |y|? p+ 1) such that
x[i : i+p] = y[j : j+p] and |i? j| is minimized.
If j exists and |i ? j| < m, then the offset |i ? j|
is added to the Local Rank Distance. Otherwise,
the maximal offset m is added to the Local Rank
Distance. An important remark is that LRD does
not impose any mathematically developed global
constraints, such as matching the i-th occurrence
of a p-gram in x with the i-th occurrence of that
same p-gram in y. Instead, it is focused on the lo-
cal phenomenon, and tries to pair equal p-grams at
a minimum offset. To ensure that LRD is a (sym-
metric) distance function, the algorithm also has
to sum up the offsets obtained from the above pro-
cess by exchanging x and y. LRD can be formally
defined as follows.
Definition 1 Let x, y ? ?
?
be two strings, and let
p ? 1 and m ? 1 be two fixed integer values. The
Local Rank Distance between x and y is defined
as:
?
LRD
(x, y) = ?
left
(x, y) + ?
right
(x, y),
where ?
left
(x, y) and ?
right
(x, y) are defined as
follows:
?
left
(x, y) =
|x|?p+1
?
i=1
min{|i? j| such that
1 ? j ? |y| ? p+ 1 and
x[i : i+ p] = y[j : j + p]} ? {m},
?
right
(x, y) =
|y|?p+1
?
j=1
min{|j ? i| such that
1 ? i ? |x| ? p+ 1 and
y[j : j + p] = x[i : i+ p]} ? {m}.
Interestingly, the search for matching p-grams is
limited within a window of fixed size. The size of
this window is determined by the maximum offset
parameter m. This parameter must be set a priori
and should be proportional to the size of the alpha-
bet, the p-grams, and to the lengths of the strings.
The following example offers a better under-
standing of how LRD actually works. LRD is
computed between two strings using 2-grams.
Example 1 Given two strings x = abcaa and
y = cabca, a fixed maximal offset m = 3, and
1366
a fixed size of p-grams p = 2, ?
left
and ?
right
are computed as follows:
?
left
(x, y) = |1? 2|+ |2? 3|
+ |3? 4|+ 3 = 6,
?
right
(x, y) = |1? 3|+ |2? 1|
+ |3? 2|+ |4? 3| = 5.
By summing up the two partial sums, Local Rank
Distance is obtained
?
LRD
(x, y) = ?
left
(x, y) + ?
right
(x, y) = 11.
The maximum LRD value between two strings
can be computed as the product between the max-
imum offset m and the number of pairs of com-
pared p-grams. Thus, LRD can be normalized
to a value in the [0, 1] interval. By normalizing,
LRD becomes a dissimilarity measure. LRD can
be also used as a kernel, since kernel methods are
based on similarity. The classical way to transform
a distance or dissimilarity measure into a simi-
larity measure is by using the Gaussian-like ker-
nel (Shawe-Taylor and Cristianini, 2004):
?
k
LRD
p
(s, t) = e
?
?
LRD
(s, t)
2?
2
,
where s and t are two strings and p is the p-grams
length. The parameter ? is usually chosen so that
values of
?
k(s, t) are well scaled. In the above
equation, ?
LRD
is already normalized to a value
in the [0, 1] interval to ensure a fair comparison of
strings of different length.
4 Learning Methods
Kernel-based learning algorithms work by embed-
ding the data into a Hilbert feature space, and
searching for linear relations in that space. The
embedding is performed implicitly, that is by spec-
ifying the inner product between each pair of
points rather than by giving their coordinates ex-
plicitly. More precisely, a kernel matrix that con-
tains the pairwise similarities between every pair
of training samples is used in the learning stage
to assign a vector of weights to the training sam-
ples. Let ? denote this weight vector. In the test
stage, the pairwise similarities between a test sam-
ple x and all the training samples are computed.
Then, the following binary classification function
assigns a positive or a negative label to the test
sample:
g(x) =
n
?
i=1
?
i
? k(x, x
i
),
where x is the test sample, n is the number of
training samples, X = {x
1
, x
2
, ..., x
n
} is the set
of training samples, k is a kernel function, and ?
i
is the weight assigned to the training sample x
i
.
In the primal form, the same binary classification
function can be expressed as:
g(x) = ?w, x?,
where ??, ?? denotes the scalar product, x ? R
m
is
the test sample represented as a vector of features,
and w ? R
m
is a vector of feature weights that can
be computed as follows:
w =
n
?
i=1
?
i
? x
i
,
given that the kernel function k can be expressed
as a scalar product between samples.
The advantage of using the dual representation
induced by the kernel function becomes clear if
the dimension of the feature space m is taken
into consideration. Since string kernels are based
on character n-grams, the feature space is indeed
very high. For instance, using 5-grams based only
on the 26 letters of the English alphabet will re-
sult in a feature space of 26
5
= 11, 881, 376 fea-
tures. However, in the experiments presented in
this work the feature space includes 5-grams along
with 6-grams, 7-grams and 8-grams. As long as
the number of samples n is not greater than the
number of features m, it is more efficient to use
the dual representation given by the kernel matrix.
This fact is also known as the kernel trick (Shawe-
Taylor and Cristianini, 2004).
Various kernel methods differ in the way they
learn to separate the samples. In the case of binary
classification problems, kernel-based learning al-
gorithms look for a discriminant function, a func-
tion that assigns +1 to examples belonging to one
class and ?1 to examples belonging to the other
class. For the NLI experiments, two binary kernel
classifiers are used, namely the SVM (Cortes and
Vapnik, 1995), and the KRR. Support Vector Ma-
chines try to find the vector of weights that defines
the hyperplane that maximally separates the im-
ages in the Hilbert space of the training examples
1367
belonging to the two classes. Kernel Ridge Re-
gression selects the vector of weights that simulta-
neously has small empirical error and small norm
in the Reproducing Kernel Hilbert Space gener-
ated by the kernel function. More details about
SVM and KRR can be found in (Shawe-Taylor and
Cristianini, 2004). The important fact is that the
above optimization problems are solved in such a
way that the coordinates of the embedded points
are not needed, only their pairwise inner products
which in turn are given by the kernel function.
SVM and KRR produce binary classifiers, but
native language identification is usually a multi-
class classification problem. There are many ap-
proaches for combining binary classifiers to solve
multi-class problems. Typically, the multi-class
problem is broken down into multiple binary clas-
sification problems using common decomposing
schemes such as: one-versus-all and one-versus-
one. There are also kernel methods that take the
multi-class nature of the problem directly into ac-
count, e.g. Kernel Discriminant Analysis. The
KDA classifier is able to improve accuracy by
avoiding the masking problem (Hastie and Tib-
shirani, 2003). In the case of multi-class native
language identification, the masking problem may
appear when non-native English speakers have ac-
quired, as the second language, a different lan-
guage to English. For example, an essay written in
English produced by a French native speaker that
is also proficient in German, could be identified as
either French or German.
5 Experiments
5.1 Data Sets Description
In this paper, experiments are carried out on three
datasets: a modified version of the ICLEv2 cor-
pus (Granger et al., 2009), the ETS Corpus of
Non-Native Written English, or TOEFL11 (Blan-
chard et al., 2013), and the TOEFL11-Big corpus
as used by Tetreault et al. (2012). A summary of
the corpora is given in Table 1.
Corpus Languages Documents
ICLE 7 770
TOEFL11 11 12, 100
TOEFL11-Big 11 87, 502
Table 1: Summary of corpora used in the experi-
ments.
The ICLEv2 is a corpus of essays written by
highly-proficient non-native college-level students
of English. For many years this was the standard
corpus used in the task of native language identi-
fication. However, the corpus was originally col-
lected for the purpose of corpus linguistic inves-
tigations, and because of this contains some id-
iosyncrasies that make it problematic for the task
of NLI (Brooke and Hirst, 2012). Therefore, a
modified version of the corpus that has been nor-
malized as much as possible for topic and charac-
ter encoding (Tetreault et al., 2012) is used. This
version of the corpus contains 110 essays each for
7 native languages: Bulgarian, Chinese, Czech,
French, Japanese, Russian and Spanish.
The ETS Corpus of Non-Native Written English
(TOEFL11) was first introduced by Tetreault et al.
(2012) and extended for the 2013 Native Language
Identification Shared Task (Tetreault et al., 2013).
It was designed to overcome many of the short-
comings identified with using the ICLEv2 corpus
for this task. The TOEFL11 corpus contains a
balanced distribution of essays per prompt (topic)
per native language. It also contains information
about the language proficiency of each writer. The
corpus contains essays written by speakers of the
following 11 languages: Arabic, Chinese, French,
German, Hindi, Italian, Japanese, Korean, Span-
ish, Telugu and Turkish. For the shared task, the
12, 100 essays were split into 9, 900 for training,
1, 100 for development and 1, 100 for testing.
Tetreault et al. (2012) present a corpus,
TOEFL11-Big, to investigate the performance of
their NLI system on a very large data set. This
data set contains the same languages as TOEFL11,
but with no overlap in content. It contains a total
of over 87 thousand essays written to a total of
76 different prompts. The distribution of L1 per
prompt is not as even as for TOEFL11, though all
topics are represented for all L1s.
5.2 Parameter Tuning and Implementation
Choices
In the string kernels approach proposed in this
work, documents or essays from this corpus are
treated as strings. Therefore, the notions of string
or document is used interchangeably throughout
this work. Because the approach works at the char-
acter level, there is no need to split the texts into
words, or to do any NLP-specific preprocessing.
The only editing done to the texts was the replac-
ing of sequences of consecutive space characters
1368
(space, tab, new line, and so on) with a single
space character. This normalization was needed in
order to prevent the artificial increase or decrease
of the similarity between texts, as a result of differ-
ent spacing. All uppercase letters were converted
to the corresponding lowercase ones.
A series of preliminary experiments were con-
ducted in order to select the best-performing learn-
ing method. In these experiments the string ker-
nel was fixed to the p-spectrum normalized ker-
nel of length 5 (
?
k
5
), because the goal was to se-
lect the best learning method, and not to find the
best kernel. The following learning methods were
evaluated: one-versus-one SVM, one-versus-all
SVM, one-versus-one KRR, one-versus-all KRR,
and KDA. A 10-fold cross-validation procedure
was carried out on the TOEFL11 training set to
evaluate the classifiers. The preliminary results in-
dicate that the one-versus-all KRR and the KDA
classifiers produce the best results. Therefore,
they are selected for the remaining experiments.
Another set of preliminary experiments were
performed to determine the range of n-grams that
gives the most accurate results on a 10-fold cross-
validation procedure carried out on the TOEFL11
training set. All the n-grams in the range 2-10
were evaluated. Furthermore, experiments with
different blended kernels were conducted to see
whether combining n-grams of different lengths
could improve the accuracy. The best results were
obtained when all the n-grams with the length in
the range 5-8 were used. Other authors (Bykh
and Meurers, 2012; Popescu and Ionescu, 2013)
also report better results by using n-grams with
the length in a range, rather than using n-grams
of fixed length. Consequently, the results reported
in this work are based on blended string kernels
based on 5-8 n-grams.
Some preliminary experiments were also per-
formed to establish the type of kernel to be used,
namely the blended p-spectrum kernel (
?
k
5?8
), the
blended p-grams presence bits kernel (
?
k
0/1
5?8
), the
blended p-grams intersection kernel (
?
k
?
5?8
), or the
kernel based on LRD (
?
k
LRD
5?8
.). These different
kernel representations are obtained from the same
data. The idea of combining all these kernels is
natural when one wants to improve the perfor-
mance of a classifier. When multiple kernels are
combined, the features are actually embedded in
a higher-dimensional space. As a consequence,
the search space of linear patterns grows, which
helps the classifier to select a better discriminant
function. The most natural way of combining two
kernels is to sum them up. Summing up kernels
or kernel matrices is equivalent to feature vector
concatenation. Another option is to combine ker-
nels by kernel alignment (Cristianini et al., 2001).
Instead of simply summing kernels, kernel align-
ment assigns weights for each of the two kernels
based on how well they are aligned with the ideal
kernel Y Y
?
obtained from training labels. The ker-
nels were evaluated alone and in various combina-
tions. The best kernels are the blended p-grams
presence bits kernel and the blended p-grams in-
tersection kernel. The best kernel combinations
include the blended p-grams presence bits kernel,
the blended p-grams intersection kernel and the
kernel based on LRD. Since the kernel based on
LRD is slightly slower than the other string ker-
nels, the kernel combinations that include it were
only evaluated on the TOEFL11 corpus and on the
ICLE corpus.
5.3 Experiment on TOEFL11 Corpus
This section describes the results on the TOEFL11
corpus. Thus, results for the 2013 Closed NLI
Shared Task are also included. In the closed shared
task the goal is to predict the native language of
testing examples, restricted to learning only from
the training and the development data. The ad-
ditional information from prompts or the English
language proficiency level were not used in the
proposed approach.
The regularization parameters were tuned on the
development set. In this case, the systems were
trained on the entire training set. A 10-fold cross-
validation (CV) procedure was done on the train-
ing and the development sets. The folds were pro-
vided along with the TOEFL11 corpus. Finally,
the results of the proposed systems are also re-
ported on the NLI Shared Task test set. For test-
ing, the systems were trained on both the training
set and the development set. The results are sum-
marized in Table 2.
The results presented in Table 2 show that string
kernels can reach state of the art accuracy levels
for this task. Overall, it seems that KDA is able
to obtain better results than KRR. The intersection
kernel alone is able to obtain slightly better results
than the presence bits kernel. The kernel based on
LRD gives significantly lower accuracy rates, but
it is able to improve the performance when it is
1369
Method Development 10-fold CV Test
Ensemble model (Tetreault et al., 2012) - 80.9% -
KRR and string kernels (Popescu and Ionescu, 2013) - 82.6% 82.7%
SVM and word features (Jarvis et al., 2013) - 84.5% 83.6%
KRR and
?
k
0/1
5?8
85.4% 82.5% 82.0%
KRR and
?
k
?
5?8
84.9% 82.2% 82.6%
KRR and
?
k
LRD
5?8
78.7% 77.1% 77.5%
KRR and
?
k
0/1
5?8
+
?
k
LRD
5?8
85.7% 82.6% 82.7%
KRR and
?
k
?
5?8
+
?
k
LRD
5?8
84.9% 82.2% 82.0%
KRR and
?
k
0/1
5?8
+
?
k
?
5?8
85.5% 82.6% 82.5%
KRR and a
1
?
k
0/1
5?8
+ a
2
?
k
?
5?8
85.5% 82.6% 82.5%
KDA and
?
k
0/1
5?8
86.2% 83.6% 83.6%
KDA and
?
k
?
5?8
85.2% 83.5% 84.6%
KDA and
?
k
LRD
5?8
79.7% 78.5% 79.2%
KDA and
?
k
0/1
5?8
+
?
k
LRD
5?8
87.1% 84.0% 84.7%
KDA and
?
k
?
5?8
+
?
k
LRD
5?8
85.8% 83.4% 83.9%
KDA and
?
k
0/1
5?8
+
?
k
?
5?8
86.4% 84.1% 85.0%
KDA and a
1
?
k
0/1
5?8
+ a
2
?
k
?
5?8
86.5% 84.1% 85.3%
KDA and
?
k
0/1
5?8
+
?
k
?
5?8
+
?
k
LRD
5?8
87.0% 84.1% 84.8%
Table 2: Accuracy rates on TOEFL11 corpus of various classification systems based on string kernels
compared with other state of the art approaches. The best accuracy rates on each set of experiments are
highlighted in bold. The weights a
1
and a
2
from the weighted sums of kernels are computed by kernel
alignment.
combined with the blended p-grams presence bits
kernel. In fact, most of the kernel combinations
give better results than each of their components.
The best kernel combination is that of the pres-
ence bits kernel and the intersection kernel. Re-
sults are quite similar when they are combined ei-
ther by summing them up or by kernel alignment.
The best performance on the test set (85.3%) is ob-
tained by the system that combines these two ker-
nels via kernel alignment and learns using KDA.
This system is 1.7% better than the state of the art
system of Jarvis et al. (2013) based on SVM and
word features, this being the top scoring system in
the NLI 2013 Shared Task. It is also 2.6% better
than the state of the art system based on string ker-
nels of Popescu and Ionescu (2013). On the cross
validation procedure, there are three systems that
reach the accuracy rate of 84.1%. All of them are
based on KDA and various kernel combinations.
The greatest accuracy rate of 84.1% reported for
the cross validation procedure is 3.2% above the
state of the art system of Tetreault et al. (2012) and
0.4% below the top scoring system of Jarvis et al.
(2013). The empirical results obtained in this ex-
periment demonstrate that the approach proposed
in this paper can reach state of the art accuracy
levels. It is worth mentioning that a significance
test performed by the organizers of the NLI 2013
Shared Task showed that the top systems that par-
ticipated in the competition are not essentially dif-
ferent. Further experiments on the ICLE corpus
and on the TOEFL11-Big corpus are conducted to
determine whether the approach proposed in this
paper is significantly better than other state of the
art approaches.
5.4 Experiment on ICLE Corpus
The results on the ICLE corpus using a 5-fold
cross validation procedure are summarized in Ta-
ble 3. To adequately compare the results with a
state of the art system, the same 5-fold cross val-
idation procedure used by Tetreault et al. (2012)
was also used in this experiment. Table 3 shows
that the results obtained by the presence bits kernel
and by the intersection kernel are systematically
better than the state of the art system of Tetreault
et al. (2012). While both KRR and KDA produce
accuracy rates that are better than the state of the
art accuracy rate, it seems that KRR is slightly bet-
ter in this experiment. Again, the idea of com-
bining kernels seems to produce more robust sys-
tems. The best systems are based on combin-
ing the presence bits kernel either with the kernel
based on LRD or the intersection kernel. Over-
all, the reported accuracy rates are higher than the
state of the art accuracy rate. The best perfor-
mance (91.3%) is achieved by the KRR classifier
based on combining the presence bits kernel with
1370
Method 5-fold CV
Ensemble model (Tetreault et al., 2012) 90.1%
KRR and
?
k
0/1
5?8
91.2%
KRR and
?
k
?
5?8
90.5%
KRR and
?
k
LRD
5?8
81.8%
KRR and
?
k
0/1
5?8
+
?
k
LRD
5?8
91.3%
KRR and
?
k
?
5?8
+
?
k
LRD
5?8
90.1%
KRR and
?
k
0/1
5?8
+
?
k
?
5?8
90.9%
KRR and
?
k
0/1
5?8
+
?
k
?
5?8
+
?
k
LRD
5?8
90.6%
KDA and
?
k
0/1
5?8
90.5%
KDA and
?
k
?
5?8
90.5%
KDA and
?
k
LRD
5?8
82.3%
KDA and
?
k
0/1
5?8
+
?
k
LRD
5?8
90.8%
KDA and
?
k
?
5?8
+
?
k
LRD
5?8
90.4%
KDA and
?
k
0/1
5?8
+
?
k
?
5?8
91.0%
KDA and
?
k
0/1
5?8
+
?
k
?
5?8
+
?
k
LRD
5?8
90.8%
Table 3: Accuracy rates on ICLE corpus of vari-
ous classification systems based on string kernels
compared with a state of the art approach. The ac-
curacy rates are reported for the same 5-fold CV
procedure as in (Tetreault et al., 2012). The best
accuracy rate is highlighted in bold.
the kernel based on LRD. This represents an 1.2%
improvement over the state of the art accuracy rate
of Tetreault et al. (2012). Two more systems are
able to obtain accuracy rates greater than 91.0%.
These are the KRR classifier based on the presence
bits kernel (91.2%) and the KDA classifier based
on the sum of the presence bits kernel and the in-
tersection kernel (91.0%). The overall results on
the ICLE corpus show that the string kernels ap-
proach can reach state of the art accuracy levels.
It is worth mentioning the purpose of this experi-
ment was to use the same approach determined to
work well in the TOEFL11 corpus. To serve this
purpose, the range of n-grams was not tuned on
this data set. Furthermore, other classifiers were
not tested in this experiment. Nevertheless, better
results can probably be obtained by adding these
aspects into the equation.
5.5 Cross-corpus Experiment
In this experiment, various systems based on KRR
or KDA are trained on the TOEFL11 corpus and
tested on the TOEFL11-Big corpus. The kernel
based on LRD was not included in this experiment
since it is more computationally expensive. There-
fore, only the presence bits kernel and the intersec-
tion kernel were evaluated on the TOEFL11-Big
corpus. The results are summarized in Table 4.
The same regularization parameters determined to
Method Test
Ensemble model (Tetreault et al., 2012) 35.4%
KRR and
?
k
0/1
5?8
66.7%
KRR and
?
k
?
5?8
67.2%
KRR and
?
k
0/1
5?8
+
?
k
?
5?8
67.7%
KRR and a
1
?
k
0/1
5?8
+ a
2
?
k
?
5?8
67.7%
KDA and
?
k
0/1
5?8
65.6%
KDA and
?
k
?
5?8
65.7%
KDA and
?
k
0/1
5?8
+
?
k
?
5?8
66.2%
KDA and a
1
?
k
0/1
5?8
+ a
2
?
k
?
5?8
66.2%
Table 4: Accuracy rates on TOEFL11-Big corpus
of various classification systems based on string
kernels compared with a state of the art approach.
The systems are trained on the TOEFL11 corpus
and tested on the TOEFL11-Big corpus. The best
accuracy rate is highlighted in bold. The weights
a
1
and a
2
from the weighted sums of kernels are
computed by kernel alignment.
work well on the TOEFL11 development set were
used.
The most interesting fact is that all the proposed
systems are at least 30% better than the state of the
art system. Considering that the TOEFL11-Big
corpus contains 87 thousand samples, the 30% im-
provement is significant without any doubt. Div-
ing into details, it can be observed that the results
obtained by KRR are higher than those obtained
by KDA. However, both methods perform very
well compared to the state of the art. Again, kernel
combinations are better than each of their individ-
ual kernels alone.
It is important to mention that the significant
performance increase is not due to the learning
method (KRR or KDA), but rather due to the string
kernels that work at the character level. It is not
only the case that string kernels are language in-
dependent, but for the same reasons they can also
be topic independent. Since the topics (prompts)
from TOEFL11 are different from the topics from
TOEFL11-Big, it becomes clear that a method
that uses words as features is strongly affected,
since the distribution of words per topic can be
completely different. But mistakes that reveal the
native language can be captured by character n-
grams that can appear more often even in differ-
ent topics. The results indicate that this is also
the case of the approach based on string kernels,
which seems to be more robust to such topic vari-
ations of the data set. The best system has an ac-
curacy rate that is 32.3% better than the state of
1371
the art system of Tetreault et al. (2012). Overall,
the empirical results indicate that the string ker-
nels approach can achieve significantly better re-
sults than other state of the art approaches.
6 Conclusions
A language-independent approach to native lan-
guage identification was presented in this paper.
The system works at the character level, mak-
ing the approach completely language indepen-
dent and linguistic theory neutral. The results ob-
tained in all the three experiments were very good.
The best system presented in this work is based on
combining the intersection and the presence string
kernels by kernel alignment and on deciding the
class label either with KDA or KRR. The best sys-
tem is 1.7% above the top scoring system of the
2013 NLI Shared Task. Furthermore, it has an im-
pressive generalization capacity, achieving results
that are 30% higher than the state of the art method
in the cross-corpus experiment.
Despite the fact that the approach based on
string kernels performed so well, it remains to be
further investigated why this is the case and why
such a simple approach can compete with far more
complex approaches that take words, lemmas,
syntactic information, or even semantics into ac-
count. It seems that there are generalizations to the
kinds of mistakes that certain non-native English
speakers make that can be captured by n-grams
of different lengths. Interestingly, using a range
of n-grams generates a large number of features
including (but not limited to) stop words, stems
of content words, word suffixes, entire words, and
even n-grams of short words. Rather than doing
feature selection before the training step, which
is the usual NLP approach, the kernel classifier
selects the most relevant features during training.
With enough training samples, the kernel classi-
fier does a better job of selecting the right features
from a very high feature space. This may be one
reason for why the string kernel approach works
so well. To gain additional insights into why this
technique is working well, the features selected
by the classifier as being more discriminating can
be analyzed in future work. This analysis would
also offer some information about localized lan-
guage transfer effects, since the features used by
the proposed model are n-grams of lengths 5 to
8. As mentioned before, the features captured by
the model typically include stems, function words,
word prefixes and suffixes, which have the poten-
tial to generalize over purely word-based features.
These features would offer insights into two kinds
of language transfer effects, namely word choice
(lexical transfer) and morphological differences.
Acknowledgments
The authors would like to thank Beata Beigman
Klebanov, Nitin Madnani and Xinhao Wang from
ETS for their helpful comments and suggestions.
The author also thank the anonymous reviewers
for their valuable insights which lead to improve-
ments in the presentation of this work.
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins,
Aoife Cahill, and Martin Chodorow. 2013.
TOEFL11: A Corpus of Non-Native English. Tech-
nical report, Educational Testing Service Research
Report No. RR?13?24.
Julian Brooke and Graeme Hirst. 2012. Robust, Lex-
icalized Native Language Identification. Proceed-
ings of COLING 2012, pages 391?408, December.
Serhiy Bykh and Detmar Meurers. 2012. Native Lan-
guage Identification using Recurring n-grams ? In-
vestigating Abstraction and Domain Dependence.
Proceedings of COLING 2012, pages 425?440, De-
cember.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
Vector Networks. Machine Learning, 20(3):273?
297.
Nello Cristianini, John Shawe-Taylor, Andr?e Elisseeff,
and Jaz S. Kandola. 2001. On kernel-target align-
ment. Proceedings of NIPS, pages 367?373, De-
cember.
Liviu P. Dinu. 2003. On the classification and aggre-
gation of hierarchies with different constitutive ele-
ments. Fundamenta Informaticae, 55(1):39?50.
Dominique Estival, Tanja Gaustad, Son-Bao Pham,
Will Radford, and Ben Hutchinson. 2007. Author
profiling for English emails. Proceedings of PA-
CLING, pages 263?272.
Sylviane Granger, Estelle Dagneaux, and Fanny Me-
unier. 2009. The International Corpus of
Learner English: Handbook and CD-ROM, version
2. Presses Universitaires de Louvain, Louvain-la-
Neuve, Belgium.
Cristian Grozea, Christian Gehl, and Marius Popescu.
2009. ENCOPLOT: Pairwise Sequence Matching
in Linear Time Applied to Plagiarism Detection. In
3rd PAN Workshop. Uncovering Plagiarism, Author-
ship, and Social Software Misuse, page 10.
1372
Trevor Hastie and Robert Tibshirani. 2003. The El-
ements of Statistical Learning. Springer, corrected
edition, July.
Radu Tudor Ionescu. 2013. Local Rank Distance.
Proceedings of SYNASC, pages 221?228.
Scott Jarvis, Yves Bestgen, and Steve Pepper. 2013.
Maximizing classification accuracy in native lan-
guage identification. Proceedings of the Eighth
Workshop on Innovative Use of NLP for Building
Educational Applications, pages 111?118, June.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005. Automatically Determining an Anonymous
Author?s Native Language. Proceedings of ISI,
pages 209?217.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Christopher J. C. H. Watkins.
2002. Text classification using string kernels. Jour-
nal of Machine Learning Research, 2:419?444.
Subhransu Maji, Alexander C. Berg, and Jitendra Ma-
lik. 2008. Classification using intersection kernel
support vector machines is efficient. Proceedings of
CVPR.
Shervin Malmasi and Mark Dras. 2014. Chinese Na-
tive Language Identification. Proceedings of EACL,
2:95?99, April.
Marius Popescu and Liviu P. Dinu. 2007. Kernel meth-
ods and string kernels for authorship identification:
The federalist papers case. Proceedings of RANLP,
September.
Marius Popescu and Cristian Grozea. 2012. Ker-
nel methods and string kernels for authorship analy-
sis. CLEF (Online Working Notes/Labs/Workshop),
September.
Marius Popescu and Radu Tudor Ionescu. 2013. The
Story of the Characters, the DNA and the Native
Language. Proceedings of the Eighth Workshop on
Innovative Use of NLP for Building Educational Ap-
plications, pages 270?278, June.
Marius Popescu. 2011. Studying translationese at the
character level. Proceedings of RANLP, pages 634?
639, September.
Conrad Sanderson and Simon Guenter. 2006. Short
text authorship attribution via sequence kernels,
markov chains and author unmasking: An investiga-
tion. Proceedings of EMNLP, pages 482?491, July.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and
Martin Chodorow. 2012. Native Tongues, Lost and
Found: Resources and Empirical Evaluations in Na-
tive Language Identification. Proceedings of COL-
ING 2012, pages 2585?2602, December.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identifi-
cation shared task. Proceedings of the Eighth Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications, pages 48?57, June.
Andrea Vedaldi and Andrew Zisserman. 2010. Effi-
cient additive kernels via explicit feature maps. Pro-
ceedings of CVPR, pages 3539?3546.
1373
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 270?278,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
The Story of the Characters, the DNA and the Native Language
Marius Popescu
University of Bucharest
Department of Computer Science
Academiei 14, Bucharest, Romania
popescunmarius@gmail.com
Radu Tudor Ionescu
University of Bucharest
Department of Computer Science
Academiei 14, Bucharest, Romania
raducu.ionescu@gmail.com
Abstract
This paper presents our approach to the 2013
Native Language Identification shared task,
which is based on machine learning methods
that work at the character level. More pre-
cisely, we used several string kernels and a
kernel based on Local Rank Distance (LRD).
Actually, our best system was a kernel combi-
nation of string kernel and LRD. While string
kernels have been used before in text analysis
tasks, LRD is a distance measure designed to
work on DNA sequences. In this work, LRD is
applied with success in native language iden-
tification.
Finally, the Unibuc team ranked third in the
closed NLI Shared Task. This result is more
impressive if we consider that our approach
is language independent and linguistic theory
neutral.
1 Introduction
This paper presents our approach to the shared task
on Native Language Identification, NLI 2013. We
approached this task with machine learning methods
that work at the character level. More precisely, we
treated texts just as sequences of symbols (strings)
and used different string kernels in conjunction with
different kernel-based learning methods in a series
of experiments to assess the best performance level
that can be achieved. Our aim was to investigate if
identifying native language is possible with machine
learning methods that work at the character level.
By disregarding features of natural language such as
words, phrases, or meaning, our approach has an im-
portant advantage in that it is language independent.
Using words is natural in text analysis tasks like
text categorization (by topic), authorship identifi-
cation and plagiarism detection. Perhaps surpris-
ingly, recent results have proved that methods han-
dling the text at character level can also be very
effective in text analysis tasks (Lodhi et al, 2002;
Sanderson and Guenter, 2006; Popescu and Dinu,
2007; Grozea et al, 2009; Popescu, 2011; Popescu
and Grozea, 2012). In (Lodhi et al, 2002) string
kernels were used for document categorization with
very good results. Trying to explain why treating
documents as symbol sequences and using string
kernels led to such good results the authors sup-
pose that: ?the [string] kernel is performing some-
thing similar to stemming, hence providing seman-
tic links between words that the word kernel must
view as distinct?. String kernels were also suc-
cessfully used in authorship identification (Sander-
son and Guenter, 2006; Popescu and Dinu, 2007;
Popescu and Grozea, 2012). For example, the sys-
tem described in (Popescu and Grozea, 2012) ranked
first in most problems and overall in the PAN 2012
Traditional Authorship Attribution tasks. A possible
reason for the success of string kernels in authorship
identification is given in (Popescu and Dinu, 2007):
?the similarity of two strings as it is measured by
string kernels reflects the similarity of the two texts
as it is given by the short words (2-5 characters)
which usually are function words, but also takes into
account other morphemes like suffixes (?ing? for ex-
ample) which also can be good indicators of the au-
thor?s style?.
Even more interesting is the fact that two meth-
ods, that are essentially the same, obtained very
270
good results for text categorization (by topic) (Lodhi
et al, 2002) and authorship identification (Popescu
and Dinu, 2007). Both are based on SVM and a
string kernel of length 5. How is this possible?
Traditionally, the two tasks, text categorization (by
topic) and authorship identification are viewed as
opposite. When words are considered as features,
for text categorization the (stemmed) content words
are used (the stop words being eliminated), while for
authorship identification the function words (stop
words) are used as features, the others words (con-
tent words) being eliminated. Then, why did the
same string kernel (of length 5) work well in both
cases? In our opinion the key factor is the kernel-
based learning algorithm. The string kernel im-
plicitly embeds the texts in a high dimensional fea-
ture space, in our case the space of all (sub)strings
of length 5. The kernel-based learning algorithm
(SVM or another kernel method), aided by regu-
larization, implicitly assigns a weight to each fea-
ture, thus selecting the features that are important
for the discrimination task. In this way, in the
case of text categorization the learning algorithm
(SVM) enhances the features (substrings) represent-
ing stems of content words, while in the case of au-
thorship identification the same learning algorithm
enhances the features (substrings) representing func-
tion words.
Using string kernels will make the correspond-
ing learning method completely language indepen-
dent, because the texts will be treated as sequences
of symbols (strings). Methods working at the word
level or above very often restrict their feature space
according to theoretical or empirical principles. For
example, they select only features that reflect var-
ious types of spelling errors or only some type of
words, such as function words, for example. These
features prove to be very effective for specific tasks,
but other, possibly good features, depending on the
particular task, may exist. String kernels embed the
texts in a very large feature space (all substrings
of length k) and leave it to the learning algorithm
(SVM or others) to select important features for the
specific task, by highly weighting these features.
A method that considers words as features can not
be language independent. Even a method that uses
only function words as features is not completely
language independent because it needs a list of func-
tion words (specific to a language) and a way to seg-
ment a text into words which is not an easy task for
some languages, like Chinese.
Character n-grams were already used in native
language identification (Brooke and Hirst, 2012;
Tetreault et al, 2012). The reported performance
when only character n-grams were used as features
was modest compared with other type of features.
But, in the above mentioned works, the authors in-
vestigated only the bigrams and trigrams and not
longer n-grams. Particularly, we have obtained sim-
ilar results with (Tetreault et al, 2012) when using
character bigrams, but we have achieved the best
performance using a range of 5 to 8 n-grams (see
section 4.3). We have used with success a similar
approach for the related task of identifying transla-
tionese (Popescu, 2011).
The first application of string kernel ideas came in
the field of text categorization, with the paper (Lodhi
et al, 2002), followed by applications in bioinfor-
matics (Leslie et al, 2002). Computer science re-
searchers have developed a wide variety of methods
that can be applied with success in computational
biology. Such methods range from clustering tech-
niques used to analyze the phylogenetic trees of dif-
ferent organisms (Dinu and Sgarro, 2006; Dinu and
Ionescu, 2012b), to genetic algorithms used to find
motifs or common patterns in a set of given DNA
sequences (Dinu and Ionescu, 2012a). Most of these
methods are based on a distance measure for strings,
such as Hamming (Chimani et al, 2011; Vezzi et
al., 2012), edit (Shapira and Storer, 2003), Kendall-
tau (Popov, 2007), or rank distance (Dinu, 2003). A
similar idea to character n-grams was introduced in
the early years of bioinformatics, where k-mers are
used instead of single characters 1. There are recent
studies that use k-mers for the phylogenetic analy-
sis of organisms (Li et al, 2004), or for sequence
alignment (Melsted and Pritchard, 2011). Analyz-
ing DNA at substring level is also more suited from
a biological point of view, because DNA substrings
may contain meaningful information. For example,
genes are encoded by a number close to 100 base
pairs, or codons that encode the twenty standard
amino acids are formed of 3-mers. Local Rank Dis-
1In biology, single DNA characters are also referred to as
nucleotides or monomers. Polymers are also known as k-mers.
271
tance (LRD) (Ionescu, 2013) has been recently pro-
posed as an extension of rank distance. LRD drops
the annotation step of rank distance, and uses k-mers
instead of single characters. The work (Ionescu,
2013) shows that LRD is a distance function and that
it has very good results in phylogenetic analysis and
DNA sequence comparison. But, LRD can be ap-
plied to any kind of string sequences, not only to
DNA. Thus, LRD was transformed into a kernel and
used for native language identification. Despite the
fact it has no linguistic motivation, LRD gives sur-
prisingly good results for this task. Its performance
level is lower than string kernel, but LRD can con-
tribute to the improvement of string kernel when the
two methods are combined.
The paper is organized as follows. In the next
section, the kernel methods we used are briefly de-
scribed. Section 3 presents the string kernels and
the LRD, and shows how to transform LRD into a
kernel. Section 4 presents details about the experi-
ments. It gives details about choosing the learning
method, parameter tuning, combining kernels and
results of submitted systems. Finally, conclusions
are given in section 5.
2 Kernel Methods and String Kernels
Kernel-based learning algorithms work by embed-
ding the data into a feature space (a Hilbert space),
and searching for linear relations in that space. The
embedding is performed implicitly, that is by speci-
fying the inner product between each pair of points
rather than by giving their coordinates explicitly.
Given an input set X (the space of examples), and
an embedding vector space F (feature space), let ? :
X ? F be an embedding map called feature map.
A kernel is a function k, such that for all x, z ?
X , k(x, z) =< ?(x), ?(z) >, where < ?, ? > de-
notes the inner product in F .
In the case of binary classification problems,
kernel-based learning algorithms look for a discrim-
inant function, a function that assigns +1 to exam-
ples belonging to one class and ?1 to examples be-
longing to the other class. This function will be a lin-
ear function in the space F , that means it will have
the form:
f(x) = sign(< w,?(x) > +b),
for some weight vector w. The kernel can be
exploited whenever the weight vector can be ex-
pressed as a linear combination of the training
points,
n?
i=1
?i?(xi), implying that f can be ex-
pressed as follows:
f(x) = sign(
n?
i=1
?ik(xi, x) + b).
Various kernel methods differ by the way in which
they find the vector w (or equivalently the vector
?). Support Vector Machines (SVM) try to find the
vector w that defines the hyperplane that maximally
separates the images in F of the training examples
belonging to the two classes. Mathematically, SVMs
choose the w and the b that satisfy the following op-
timization criterion:
min
w,b
1
n
n?
i=1
[1? yi(< w,?(xi) > +b)]+ + ?||w||
2
where yi is the label (+1/?1) of the training ex-
ample xi, ? a regularization parameter and [x]+ =
max(x, 0).
Kernel Ridge Regression (KRR) selects the vec-
tor w that simultaneously has small empirical er-
ror and small norm in Reproducing Kernel Hilbert
Space generated by kernel k. The resulting mini-
mization problem is:
min
w
1
n
n?
i=1
(yi? < w,?(xi) >)
2 + ?||w||2
where again yi is the label (+1/?1) of the training
example xi, and ? a regularization parameter.
Details about SVM and KRR can be found
in (Taylor and Cristianini, 2004). The important fact
is that the above optimization problems are solved
in such a way that the coordinates of the embedded
points are not needed, only their pairwise inner prod-
ucts which in turn are given by the kernel function
k.
3 String Kernels and Local Rank Distance
The kernel function offers to the kernel methods the
power to naturally handle input data that are not in
the form of numerical vectors, for example strings.
The kernel function captures the intuitive notion of
272
similarity between objects in a specific domain and
can be any function defined on the respective do-
main that is symmetric and positive definite. For
strings, many such kernel functions exist with vari-
ous applications in computational biology and com-
putational linguistics (Taylor and Cristianini, 2004).
3.1 String Kernels
Perhaps one of the most natural ways to measure the
similarity of two strings is to count how many sub-
strings of length p the two strings have in common.
This gives rise to the p-spectrum kernel. Formally,
for two strings over an alphabet ?, s, t ? ??, the
p-spectrum kernel is defined as:
kp(s, t) =
?
v??p
numv(s) ? numv(t)
where numv(s) is the number of occurrences of
string v as a substring in s 2. The feature map de-
fined by this kernel associates to each string a vector
of dimension |?|p containing the histogram of fre-
quencies of all its substrings of length p (p-grams).
A variant of this kernel can be obtained if the
embedding feature map is modified to associate to
each string a vector of dimension |?|p containing
the presence bits (instead of frequencies) of all its
substrings of length p. Thus the character p-grams
presence bits kernel is obtained:
k0/1p (s, t) =
?
v??p
inv(s) ? inv(t)
where inv(s) is 1 if string v occurs as a substring in
s and 0 otherwise.
Normalized versions of these kernels ensure a fair
comparison of strings of different lengths:
k?p(s, t) =
kp(s, t)
?
kp(s, s) ? kp(t, t)
k?0/1p (s, t) =
k0/1p (s, t)
?
k0/1p (s, s) ? k
0/1
p (t, t)
.
Taking into account p-grams of different length
and summing up the corresponding kernels, new
kernels (called blended spectrum kernels) can be ob-
tained.
2Note that the notion of substring requires contiguity. See
(Taylor and Cristianini, 2004) for a discussion about the ambi-
guity between the terms substring and subsequence across dif-
ferent traditions: biology, computer science.
3.2 Local Rank Distance
Local Rank Distance is an extension of rank distance
that drops the annotation step and uses n-grams in-
stead of single characters. Thus, characters in one
string are simply matched with the nearest similar
characters in the other string. To compute the LRD
between two strings, the idea is to sum up all the off-
sets of similar n-grams between the two strings. For
every n-gram in one string, we search for a similar
n-gram in the other string. First, look for similar n-
grams in the same position in both strings. If those
n-grams are similar, sum up 0 since there is no offset
between them. If the n-grams are not similar, start
looking around the initial n-gram position in the sec-
ond string to find an n-gram similar to the one in the
first string. If a similar n-gram is found during this
process, sum up the offset between the two n-grams.
The search goes on until a similar n-gram is found or
until a maximum offset is reached. LRD is formally
defined next.
Definition 1 Let S1, S2 ? ?? be two strings with
symbols (n-grams) from the alphabet ?. Local Rank
Distance between S1 and S2 is defined as:
?LRD(S1, S2) = ?left + ?right
=
?
xs?S1
min
xs?S2
{|posS1(xs)? posS2(xs)|,m}+
+
?
ys?S2
min
ys?S1
{|posS1(ys)? posS2(ys)|,m},
where xs and ys are occurrences of symbol s ? ? in
strings S1 and S2, posS(xs) represents the position
(or the index) of the occurrence xs of symbol s ? ?
in string S, and m ? 1 is the maximum offset.
A string may contain multiple occurrences of a
symbol s ? ?. LRD matches each occurrence xs
of symbol s ? ? from a string, with the nearest oc-
currence of symbol s in the other string. A sym-
bol can be defined either as a single character, or
as a sequence of characters (n-grams). Overlapping
n-grams are also permitted in the computation of
LRD. Notice that in order to be a symmetric distance
measure, LRD must consider every n-gram in both
strings. The complexity of an algorithm to compute
LRD can be reduced to O(l ? m) using advanced
string searching algorithms, where l is the maximum
length of the two strings involved in the computation
of LRD, and m is the maximum offset.
273
To understand how LRD actually works, consider
example 1 where LRD is computed between strings
s1 and s2 using 1-grams (single characters).
Example 1 Let s1 = CCBAADACB, s2 =
DBACDCA, and m = 10 be the maximum offset.
The LRD between s1 and s2 is given by:
?LRD(s1, s2) = ?left + ?right
where the two sums ?left and ?right are computed
as follows:
?left =
?
xs?s1
min
xs?s2
{|poss1(xs)? poss2(xs)|, 10}
= |1? 4|+ |2? 4|+ |3? 2|+ |4? 3|+ |5? 3|+
+ |6? 5|+ |7? 7|+ |8? 6|+ |9? 2| = 19
?right =
?
ys?s2
min
ys?s1
{|poss1(ys)? poss2(ys)|, 10}
= |1? 6|+ |2? 3|+ |3? 4|+ |4? 2|+ |5? 6|+
+ |6? 8|+ |7? 7| = 12.
In other words, ?left considers every symbol
from s1, while ?right considers every symbol from
s2. Observe that ?LRD(s1, s2) = ?LRD(s2, s1).
LRD measures the distance between two strings.
Knowing the maximum offset (used to stop sim-
ilar n-gram searching), the maximum LRD value
between two strings can be computed as the prod-
uct between the maximum offset and the number of
pairs of compared n-grams. Thus, LRD can be nor-
malized to a value in the [0, 1] interval. By normal-
izing, LRD is transformed into a dissimilarity mea-
sure. LRD can be also used as a kernel, since kernel
methods are based on similarity. The classical way
to transform a distance or dissimilarity measure into
a similarity measure is by using the Gaussian-like
kernel (Taylor and Cristianini, 2004):
k(s1, s2) = e
?
LRD(s1, s2)
2?2
where s1 and s2 are two strings. The parameter
? is usually chosen to match the number of fea-
tures (characters) so that values of k(s1, s2) are well
scaled.
4 Experiments
4.1 Dataset
The dataset for the NLI shared task is the TOEFL11
corpus (Blanchard et al, 2013). This corpus con-
tains 9900 examples for training, 1100 examples for
development (or validation) and another 1100 ex-
amples for testing. Each example is an essay writ-
ten in English by a person that is a non-native En-
glish speaker. The people that produced the essays
have one of the following native languages: German,
French, Spanish, Italian, Chinese, Korean, Japanese,
Turkish, Arabic, Telugu, Hindi. For more details
see (Blanchard et al, 2013).
We participated only in the closed NLI shared
task, where the goal of the task is to predict the
native language of testing examples, only by us-
ing the training and development data. In our ap-
proach, documents or essays from this corpus are
treated as strings. Thus, when we refer to strings
throughout this paper, we really mean documents
or essays. Because we work at the character level,
we didn?t need to split the texts into words, or to do
any NLP-specific preprocessing. The only editing
done to the texts was the replacing of sequences of
consecutive space characters (space, tab, new line,
etc.) with a single space character. This normaliza-
tion was needed in order to not artificially increase
or decrease the similarity between texts as a result
of different spacing. Also all uppercase letters were
converted to the corresponding lowercase ones. We
didn?t use the additional information from prompts
and English language proficiency level.
4.2 Choosing the Learning Method
SVM and KRR produce binary classifiers and native
language identification is a multi-class classification
problem. There are a lot of approaches for com-
bining binary classifiers to solve multi-class prob-
lems. Typically, the multiclass problem is broken
down into multiple binary classification problems
using common decomposing schemes such as: one-
versus-all (OVA) and one-versus-one (OVO). There
are also kernel methods that directly take into ac-
count the multiclass nature of the problem such as
the kernel partial least squares regression (KPLS).
We conducted a series of preliminary experiments
in order to select the learning method. In these ex-
274
0 1 2 3 4 5 6 7 8 9 10 110
10
20
30
40
50
60
70
90
100
n?grams
ac
cu
rac
y
 
 
n?grams frequencies
n?grams presence 
Figure 1: 10-fold cross-validation accuracy on the train set for different n-grams.
Method Accuracy
OVO SVM 72.72%
OVA SVM 74.94%
OVO KRR 73.99%
OVA KRR 77.74%
KPLS 74.99%
Table 1: Accuracy rates using 10-fold cross-validation on
the train set for different kernel methods with k?5 kernel.
periments we fixed the kernel to the p-spectrum nor-
malized kernel of length 5 (k?5) and plugged it in
the following learning methods: OVO SVM, OVA
SVM, OVO KRR, OVA KRR and KPLS. Note that
in this stage we were interested only in selecting the
learning method and not in finding the best kernel.
We chose the k?5 because it was reported to work
well in the case of the related task of identifying
translationese (Popescu, 2011).
We carried out a 10-fold cross-validation on the
training set and the result obtained (with the best pa-
rameters setting) are shown in Table 1.
The results show that for native language identi-
fication the one-vs-all scheme performs better than
the one-versus-one scheme. The same fact was re-
ported in (Brooke and Hirst, 2012). See also (Rifkin
and Klautau, 2004) for arguments in favor of one-
vs-all. The best result was obtained by one-vs-all
Kernel Ridge Regression and we selected it as our
learning method.
4.3 Parameter Tuning for String Kernel
To establish the type of kernel, (blended) p-spectrum
kernel or (blended) p-grams presence bits kernel,
and the length(s) of of n-grams that must be used,
we performed another set of experiments. For both
p-spectrum normalized kernel and p-grams presence
bits normalized kernel, and for each value of p from
2 to 10, we carried out a 10-fold cross-validation on
the train set. The results are summarized in Figure 1.
As can be seen, both curves have similar shapes,
both achieve their maximum at 8, but the accuracy of
the p-grams presence bits normalized kernel is gen-
erally better than the accuracy of the p-spectrum nor-
malized kernel. It seem that in native language iden-
tification the information provided by the presence
of an n-gram is more important than the the infor-
mation provided by the frequency of occurrence of
the respective n-gram. This phenomenon was also
noticed in the context of sexual predator identifica-
tion (Popescu and Grozea, 2012).
We also experimented with different blended ker-
nels to see if combining n-grams of different lengths
can improve the accuracy. The best result was ob-
tained when all the n-grams with the length in the
range 5-8 were used, that is the 5-8-grams presence
bits normalized kernel (k?0/15?8). The 10-fold cross-
validation accuracy on the train set for this kernel
275
Method Accuracy
KRR + KLRD6 42.1%
KRR + KnLRD4 70.8%
KRR + KnLRD6 74.4%
KRR + KnLRD8 74.8%
Table 2: Accuracy rates, using 10-fold cross-validation
on the training set, of LRD with different n-grams, with
and without normalization. Normalized LRD is much
better.
was 80.94% and was obtained for the KRR param-
eter ? set to 10?5. The authors of (Bykh and Meur-
ers, 2012) also obtained better results using n-grams
with the length in a range than using n-grams of a
fixed length.
4.4 Parameter Tuning for LRD Kernel
Parameter tuning for LRD kernel (KLRD) was also
done by using 10-fold cross validation on the train-
ing data. First, we observed that the KRR based on
LRD works much better with the normalized version
of LRD (KnLRD). Another concern was to choose
the right length of n-grams. We tested with several
n-grams such as 4-grams, 6-grams and 8-grams that
are near the mean English word length of 5-6 let-
ters. The tests show that the LRD kernels based on
6-grams (KnLRD6) and 8-grams (KnLRD8) give the
best results. In the end, the LRD kernels based on 6-
grams and 8-grams are combined to obtain even bet-
ter results (see section 4.5). Finally, the maximum
offset parameter m involved in the computation of
LRD was chosen so that it generates search window
size close to the average number of letters per docu-
ment from the TOEFL 11 set. There are 1802 char-
acters per document on average, and m was chosen
to be 700. This parameter was also chosen with re-
spect to the computational time of LRD, which is
proportional to the parameter value. Table 2 shows
the results of the LRD kernel with different parame-
ters cross validated on the training set. For KnLRD,
the ? parameter of the Gaussian-like kernel was set
to 1. The reported accuracy rates were obtained with
the KRR parameter ? set to 10?5.
Regarding the length of strings, we observed that
LRD is affected by the variation of string lengths.
When comparing two documents with LRD, we
tried to cut the longer one to match the length of
Method Accuracy
KRR + KnLRD6+8 75.4%
KRR + k?0/15?8 + KnLRD6+8 81.6%
KRR + (k?0/1 +KnLRD)6+8 80.9%
Table 3: Accuracy rates of different kernel combinations
using 10-fold cross-validation on the training set.
the shorter. This made the accuracy even worse. It
seems that the parts cut out from longer documents
contain valuable information for LRD. We decided
to use the entire strings for LRD, despite the noise
brought by the variation of string lengths.
4.5 Combining Kernels
To improve results, we thought of combining the
kernels in different ways. First, notice that the
blended string kernels presented in section 4.3 are
essentially a sum of the string kernels with different
n-grams. This combination improves the accuracy,
being more stable and robust. In the same manner,
the LRD kernels based on 6-grams and 8-grams, re-
spectively, were summed up to obtain the kernel de-
noted by KnLRD6+8 . Indeed, the KnLRD6+8 kernel
works better (see Table 3).
There are other options to combine the string ker-
nels with LRD kernels, besides summing them up.
One option is by kernel alignment (Cristianini et al,
2001). Instead of simply summing kernels, kernel
alignment assigns weights for each to the two ker-
nels based on how well they are aligned with the
ideal kernel Y Y ? obtained from labels. Thus, the 5-
8-grams presence bits normalized kernel (k?0/15?8) was
combined with the LRD kernel based on sum of 6,8-
grams (KnLRD6+8), by kernel alignment. From our
experiments, kernel alignment worked slightly bet-
ter than the sum of the two kernels. This also sug-
gests that kernels can be combined only by kernel
alignment. The string kernel of length 6 was aligned
with the LRD kernel based on 6-grams. In the same
way, the string kernel of length 8 was aligned with
the LRD kernel based on 8-grams. The two kernels
obtained by alignment are combined together, again
by kernel alignment, to obtain the kernel denoted by
(k?0/1 +KnLRD)6+8. The results of all kernel com-
binations are presented in Table 3. The reported ac-
curacy rates were obtained with the KRR parameter
276
Method Submission CV Tr. Dev. CV Tr.+Dev. Test
KRR + k?0/15?8 Unibuc-1 80.9% 85.4% 82.5% 82.0%
KRR + KnLRD6+8 Unibuc-2 75.4% 76.3% 75.7% 75.8%
KRR + k?0/15?8 + KnLRD6+8 Unibuc-3 81.6% 85.7% 82.6% 82.5%
KRR + (k?0/1 +KnLRD)6+8 Unibuc-4 80.9% 85.6% 82.0% 81.4%
KRR + k?0/15?8 + KnLRD6+8 + heuristic Unibuc-5 - - - 82.7%
Table 4: Accuracy rates of submitted systems on different evaluation sets. The Unibuc team ranked third in the closed
NLI Shared Task with the kernel combination improved by the heuristic to level the predicted class distribution.
? set to 10?5.
4.6 Results and Discussion
For the closed NLI Shared Task we submitted the
two main systems, namely the 5-8-grams presence
bits normalized kernel and the LRD kernel based on
sum of 6,8-grams, separately. Another two submis-
sions are the kernel combinations discussed in sec-
tion 4.5. These four systems were tested using sev-
eral evaluation procedures, with results shown in Ta-
ble 4. First, they were tested using 10-fold cross val-
idation on the training set. Next, the systems were
tested on the development set. In this case, the sys-
tems were trained on the entire training corpus. An-
other 10-fold cross validation procedure was done
on the corpus obtained by combining the training
and the development sets. The folds were provided
by the organizers. Finally, the results of our systems
on the NLI Shared Task test set are given in the last
column of Table 4. For testing, the systems were
trained on the entire training and development set,
with the KRR parameter ? set to 2 ? 10?5.
We didn?t expect KnLRD6+8 kernel to perform
very well on the test set. This system was submitted
just to be compared with systems submitted by other
participants. Considering that LRD is inspired from
biology and that it has no ground in computational
linguistics, it performed very well, by standing in the
top half of the ranking of all submitted systems.
The kernel obtained by aligning the k?0/15?8 and
KnLRD6+8 kernels gives the best results, no matter
the evaluation procedure. It is followed closely by
the other two submitted systems.
We thought of exploiting the distribution of the
testing set in our last submitted system. We knew
that there should be exactly 100 examples per class
for testing. We took the kernel obtained by com-
bining the k?0/15?8 and KnLRD6+8 kernels, and tried to
adjust its output to level the predicted class distribu-
tion. We took all the classes with more than 100 ex-
amples and ranked the examples by their confidence
score (returned by regression) to be part of the pre-
dicted class. The examples ranked below 100 were
chosen to be redistributed to the classes that had less
than 100 examples per class. Examples were redis-
tributed only if their second most confident class had
less than 100 examples. This heuristic improved the
results on the test set by 0.2%, enough to put us on
third place in the closed NLI Shared Task.
5 Conclusion
In this paper, we have presented our approach to
the 2013 NLI Shared Task. What makes our sys-
tem stand out is that it works at the character level,
making the approach completely language indepen-
dent and linguistic theory neutral. The results ob-
tained were very good. A standard approach based
on string kernels, that proved to work well in many
text analysis tasks, obtained an accuracy of 82% on
test data with a difference of only 1.6% between it
and the top performing system. A second system
based on a new kernelKLRD, inspired from biology
with no ground in computational linguistics, per-
formed also unexpectedly well, by standing in the
top half of the ranking of all submitted systems. The
combination of the two kernels obtained an accuracy
of 82.5% making it to the top ten, while an heuristic
improvement of this combination ranked third with
an accuracy of 82.7%. Obviously, an explanation
for these results was needed. It will be adressed in
future work.
277
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Julian Brooke and Graeme Hirst. 2012. Robust, Lexical-
ized Native Language Identification. In Proceedings
of COLING 2012, pages 391?408, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Serhiy Bykh and Detmar Meurers. 2012. Native Lan-
guage Identification using Recurring n-grams ? In-
vestigating Abstraction and Domain Dependence. In
Proceedings of COLING 2012, pages 425?440, Mum-
bai, India, December. The COLING 2012 Organizing
Committee.
Markus Chimani, Matthias Woste, and Sebastian Bocker.
2011. A Closer Look at the Closest String and Closest
Substring Problem. Proceedings of ALENEX, pages
13?24.
Nello Cristianini, John Shawe-Taylor, Andre? Elisseeff,
and Jaz S. Kandola. 2001. On kernel-target algn-
ment. In Thomas G. Dietterich, Suzanna Becker, and
Zoubin Ghahramani, editors, NIPS, pages 367?373.
MIT Press.
Liviu P. Dinu and Radu Tudor Ionescu. 2012a. An Ef-
ficient Rank Based Approach for Closest String and
Closest Substring. PLoS ONE, 7(6):e37576, 06.
Liviu P. Dinu and Radu Tudor Ionescu. 2012b. Clus-
tering based on Rank Distance with Applications on
DNA. Proceedings of ICONIP, 7667:722?729.
Liviu P. Dinu and Andrea Sgarro. 2006. A Low-
complexity Distance for DNA Strings. Fundamenta
Informaticae, 73(3):361?372.
Liviu P. Dinu. 2003. On the classification and aggrega-
tion of hierarchies with different constitutive elements.
Fundamenta Informaticae, 55(1):39?50.
C. Grozea, C. Gehl, and M. Popescu. 2009. EN-
COPLOT: Pairwise Sequence Matching in Linear
Time Applied to Plagiarism Detection. In 3rd
PAN WORKSHOP. UNCOVERING PLAGIARISM,
AUTHORSHIP AND SOCIAL SOFTWARE MISUSE,
page 10.
Radu Tudor Ionescu. 2013. Local Rank Distance and its
Applications on DNA. Submitted to PKDD.
Christina S. Leslie, Eleazar Eskin, and William Stafford
Noble. 2002. The spectrum kernel: A string kernel
for svm protein classification. In Pacific Symposium
on Biocomputing, pages 566?575.
Ming Li, Xin Chen, Xin Li, Bin Ma, and Paul M. B. Vi-
tanyi. 2004. The similarity metric. IEEE Transac-
tions on Information Theory, 50(12):3250?3264.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Christopher J. C. H. Watkins. 2002.
Text classification using string kernels. Journal of Ma-
chine Learning Research, 2:419?444.
Pall Melsted and Jonathan Pritchard. 2011. Efficient
counting of k-mers in DNA sequences using a bloom
filter. BMC Bioinformatics, 12(1):333.
Marius Popescu and Liviu P. Dinu. 2007. Kernel meth-
ods and string kernels for authorship identification:
The federalist papers case. In Proceedings of the In-
ternational Conference on Recent Advances in Natu-
ral Language Processing (RANLP-07), Borovets, Bul-
garia, September.
Marius Popescu and Cristian Grozea. 2012. Ker-
nel methods and string kernels for authorship analy-
sis. In Pamela Forner, Jussi Karlgren, and Christa
Womser-Hacker, editors, CLEF (Online Working
Notes/Labs/Workshop).
Marius Popescu. 2011. Studying translationese at the
character level. In Proceedings of the International
Conference Recent Advances in Natural Language
Processing 2011, pages 634?639, Hissar, Bulgaria,
September. RANLP 2011 Organising Committee.
V. Yu. Popov. 2007. Multiple genome rearrangement by
swaps and by element duplications. Theoretical Com-
puter Science, 385(1-3):115?126.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense of
one-vs-all classification. Journal of Machine Learning
Research, 5(January):101?141.
Conrad Sanderson and Simon Guenter. 2006. Short text
authorship attribution via sequence kernels, markov
chains and author unmasking: An investigation. In
Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 482?
491, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Dana Shapira and James A. Storer. 2003. Large Edit Dis-
tance with Multiple Block Operations. Proceedings of
SPIRE, 2857:369?377.
J. S. Taylor and N. Cristianini. 2004. Kernel Methods for
Pattern Analysis. Cambridge University Press, New
York, NY, USA.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native Tongues, Lost and
Found: Resources and Empirical Evaluations in Na-
tive Language Identification. In Proceedings of COL-
ING 2012, pages 2585?2602, Mumbai, India, Decem-
ber. The COLING 2012 Organizing Committee.
Francesco Vezzi, Cristian Del Fabbro, Alexandru I.
Tomescu, and Alberto Policriti. 2012. rNA: a fast and
accurate short reads numerical aligner. Bioinformat-
ics, 28(1):123?124.
278
