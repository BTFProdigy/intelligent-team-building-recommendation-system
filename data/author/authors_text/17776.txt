Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 913?922,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Query-Chain Focused Summarization 
 
Tal Baumel 
Dept. of Computer Science  
Ben-Gurion University 
Beer-Sheva, Israel 
talbau@cs.bgu.ac.il 
Raphael Cohen 
Dept. of Computer Science  
Ben-Gurion University 
Beer-Sheva, Israel 
cohenrap@cs.bgu.ac.il 
Michael Elhadad 
Dept. of Computer Science  
Ben-Gurion University 
Beer-Sheva, Israel 
elhadad@cs.bgu.ac.il 
 
 
Abstract 
Update summarization is a form of multi-
document summarization where a document 
set must be summarized in the context of other 
documents assumed to be known. Efficient 
update summarization must focus on identify-
ing new information and avoiding repetition of 
known information. In Query-focused summa-
rization, the task is to produce a summary as 
an answer to a given query.  We introduce a 
new task, Query-Chain Summarization, which 
combines aspects of the two previous tasks: 
starting from a given document set, increas-
ingly specific queries are considered, and a 
new summary is produced at each step. This 
process models exploratory search: a user ex-
plores a new topic by submitting a sequence of 
queries, inspecting a summary of the result set 
and phrasing a new query at each step. We 
present a novel dataset comprising 22 query-
chains sessions of length up to 3 with 3 match-
ing human summaries each in the consumer-
health domain. Our analysis demonstrates that 
summaries produced in the context of such 
exploratory process are different from in-
formative summaries. We present an algorithm 
for Query-Chain Summarization based on a 
new LDA topic model variant.  Evaluation in-
dicates the algorithm improves on strong base-
lines. 
1 Introduction 
In the past 10 years, the general objective of 
text summarization has been refined into more 
specific tasks. Such summarization tasks include: 
(i) Generic Multi Document Summarization: 
aims at summarizing a cluster of topically related 
documents, such as the top results of a search 
engine query; (ii) in Update Summarization, a set 
of documents is summarized while assuming the 
user has already read a summary of earlier doc-
uments on the same topic; (iii) in Query-Focused 
Summarization, the summary of a documents set 
is produced to convey an informative answer in 
the context of a specific query. The importance 
of these specialized tasks is that they help us dis-
tinguish criteria that lead to the selection of con-
tent in a summary: centrality, novelty, relevance, 
and techniques to avoid redundancy. 
We present in this paper a variant summariza-
tion task which combines the two aspects of up-
date and query-focused summarization.  The task 
is related to exploratory search (Marchionini, 
2006). In contrast to classical information seek-
ing, in exploratory search, the user is uncertain 
about the information available, and aims at 
learning and understanding a new topic (White 
and Roth, 2009).  In typical exploratory search 
behavior, a user posts a series of queries, and 
based on information gathered at each step, de-
cides how to further explore a set of documents. 
The metaphor of berrypicking introduced in 
(Bates, 1989) captures this interactive process. 
At each step, the user may zoom in to a more 
specific information need, zoom out to a more 
general query, or pan sideways, in order to inves-
tigate a new aspect of the topic.  
We define Query-Chain Focused Summariza-
tion as follows: for each query in an exploratory 
search session, we aim to extract a summary that 
answers the information need of the user, in a 
manner similar to Query-Focused Summariza-
tion, while not repeating information already 
provided in previous steps, in a manner similar to 
Update Summarization. In contrast to query-
focused summarization, the context of a sum-
913
mary is not a single query, but the set of queries 
that led to the current step, their result sets and 
the corresponding summaries. 
We have constructed a novel dataset of Query-
Sets with matching manual summarizations in 
the consumer health domain (Cline and Haynes, 
2001). Queries are extracted from PubMed 
search logs (Dogan et al, 2009). We have ana-
lyzed this manual dataset and confirm that sum-
maries written in the context of berry-picking are 
markedly different from those written for similar 
queries on the same document set, but without 
the query-chain context. 
We have adapted well-known multi-document 
algorithms to the task, and present baseline algo-
rithms based on LexRank (Erkan and Radev, 
2004), KLSum and TopicSum (Haghighi and 
Vanderwende, 2009). We introduce a new algo-
rithm to address the task of Query-Chain Fo-
cused Summarization, based on a new LDA topic 
model variant, and present an evaluation which 
demonstrates it improves on these baselines. 
The paper is structured as follows. Section 2 
formulates the task of Query-Chain Focused 
Summarization. Section 3 reviews related work. 
In Section 4, we describe the data collection pro-
cess and the resulting dataset. We then present 
our algorithm, as well as the baseline algorithms 
used for evaluation. We conclude with evalua-
tion and discussion. 
2 Query- Chain Summarization 
In this work, we focus on the zoom in aspect 
of the exploratory search process described 
above. We formulate the Query-Chain Focused 
Summarization (QCFS) task as follows: 
Given an ordered chain of queries Q and a set 
of documents D , for each query Qqi?  a sum-
mary Si is generated from D answering 
iq  under 
the assumption that the user has already read the 
summaries Si-1 for queries
10... ?iqq . 
A typical example of query chain in the con-
sumer health domain we investigate includes the 
following 3 successive queries: (Causes of asth-
ma, Asthma and Allergy, Asthma and Mold Al-
lergy).   We consider a single set of documents 
relevant to the domain of Asthma as the refer-
ence set D.  The QCFS task consists of generat-
ing one summary of D as an answer to each que-
ry, so that the successive answers do not repeat 
information already provided in a previous an-
swer. 
3 Previous Work 
We first review the closely related tasks of 
Update Summarization and Query-Focused 
Summarization. We also review key summariza-
tion algorithms that we have selected as baseline 
and adapted to the QCFS task. 
Update Summarization focuses on identifying 
new information relative to a previous body of 
information, modeled as a set of documents. It 
has been introduced in shared tasks in DUC 2007 
and TAC 2008.  This task consists of producing a 
multi-document summary for a document set on 
a specific topic, and then a multi-document 
summary for a different set of articles on the 
same topic published at later dates. This task 
helps us understand how update summaries iden-
tified and focused on new information while re-
ducing redundancy compared to the original 
summaries.  
The TAC 2008 dataset includes 48 sets of 20 
documents, each cluster split in two subsets of 10 
documents (called A and B). Subset B docu-
ments were more recent. Original summaries 
were generated for the A subsets and update 
summaries were then produced for the B subsets. 
Human summaries and candidate systems are 
evaluated using the Pyramid method (Nenkova 
and Passonneau, 2004). For automatic evaluation, 
ROUGE (Lin, 2004) variants have been pro-
posed (Conroy et al, 2011).  In contrast to this 
setup, QCFS distinguishes the subsets of docu-
ments considered at each step of the process by 
facets of the underlying topic, and not by chro-
nology. In addition, the document subsets are not 
identified as part of the task in QCFS (as op-
posed to the explicit split in A and B subsets in 
Update Summarization). 
Most systems working on Update Summariza-
tion have focused on removing redundancy. Du-
alSum (Delort and Alfonseca, 2012) is notable in 
attempting to directly model novelty using a spe-
cialized topic-model to distinguish words ex-
pressing background information and those in-
troducing new information in each document. 
In Query-Focused Summarization (QFS), the 
task consists of identifying information in a doc-
ument set that is most relevant to a given query.  
914
This differs from generic summarization, where 
one attempts to identify central information.  
QFS helps us distinguish models of relevance 
and centrality.  Unfortunately, detailed analysis 
of the datasets produced for QFS indicates that 
these two notions are not strongly distinguished 
in practice: (Gupta et al, 2007) observed that in 
QFS datasets, up to 57% of the words in the doc-
ument sets were closely related to the query 
(through simple query expansion).  They note 
that as a consequence, a generic summarizer 
forms a strong baseline for such biased QFS 
tasks. 
We address this limitation of existing QFS da-
tasets in our definition of QCFS: we identify a 
chain of at least 3 related queries which focus on 
different facets of the same central topic and re-
quire the generation of distinct summaries for 
each query, with little repetition across the steps. 
A specific evaluation aspect of QFS measures 
responsiveness (how well the summary answers 
the specific query).  QFS must rely on Infor-
mation Retrieval techniques to overcome the 
scarceness of the query to establish relevance.  
As evidenced since (Daume and Marcu, 2006), 
Bayesian techniques have proven effective at this 
task: we construct a latent topic model on the 
basis of the document set and the query. This 
topic model effectively serves as a query expan-
sion mechanism, which helps assess the rele-
vance of individual sentences to the original que-
ry. 
In recent years, three major techniques have 
emerged to perform multi-document summariza-
tion: graph-based methods such as LexRank (Er-
kan and Radev, 2004) for multi document sum-
marization and Biased-LexRank (Otterbacher et 
al., 2008) for query focused summarization, lan-
guage model methods such as KLSum (Haghighi 
and Vanderwende, 2009) and variants of KLSum 
based on topic models such as BayesSum (Dau-
me and Marcu, 2006) and TopicSum (Haghighi 
and Vanderwende, 2009).   
LexRank is a stochastic graph-based method 
for computing the relative importance of textual 
units in a natural text. The LexRank algorithm 
builds a weighted graph ? = (?, ?) where each 
vertex in ? is a linguistic unit (in our case sen-
tences) and each weighted edge in ? is a measure 
of similarity between the nodes. In our imple-
mentation, we model similarity by computing the 
cosine distance between the ?? ? ???  vectors 
representing each node. After the graph is gener-
ated, the PageRank algorithm (Page et al, 1999) 
is used to determine the most central linguistic 
units in the graph. To generate a summary we 
use the ?  most central lexical units, until the 
length of the target summary is reached. This 
method has no explicit control to avoid redun-
dancy among the selected sentences, and the 
original algorithm does not address update or 
query-focused variants. Biased-LexRank (Otter-
bacher et al, 2008) makes LexRank sensitive to 
the query by introducing a prior belief about the 
ranking of the nodes in the graph, which reflects 
the similarity of sentences to the query. Pag-
eRank spreads the query similarity of a vertex to 
its close neighbors, so that we rank higher sen-
tences that are similar to other sentences which 
are similar to the query. As a result, Biased-
LexRank overcomes the lexical sparseness of the 
query and obtained state of the art results on the 
DUC 2005 dataset. 
KLSum adopts a language model approach to 
compute relevance: the documents in the input 
set are modeled as a distribution over words (the 
original algorithm uses a unigram distribution 
over the bag of words in documents D). KLSum 
is a sentence extraction algorithm: it searches for 
a subset of the sentences in D with a unigram 
distribution as similar as possible to that of the 
overall collection D, but with a limited length. 
The algorithm uses Kullback-Lieber (KL) diver-
gence ??(?||?) = ? log? (
?(?)
?(?)
)?(?)  to com-
pute the similarity of the distributions. It searches 
for ?? = argmin|?|<???(??||??). This search is 
performed in a greedy manner, adding sentences 
one by one to S until the length L is reached, and 
choosing the best sentence as measured by KL-
divergence at each step. The original method has 
no update or query focusing capability, but as a 
general modeling framework it is easy to adapt to 
a wide range of specific tasks. 
TopicSum uses an LDA-like topic model (Blei 
et al 2003) to classify words from a number of 
document sets (each set discussing a different 
topic) as either general non-content words, topic 
specific words and document specific word (this 
category refers to words that are specific to the 
writer and not shared across the document set). 
After the words are classified, the algorithm uses 
a KLSum variant to find the summary that best 
matches the unigram distribution of topic specif-
ic words. This method improves the results of 
915
KLSum but it also has no update summary or 
query answering capabilities.  
4 Dataset Collection 
We now describe how we have constructed a 
dataset to evaluate QCFS algorithms, which we 
are publishing freely. We selected to build our 
dataset in the Consumer Health domain, a popu-
lar domain in the web (Cline and Haynes 2001) 
providing medical information at various levels 
of complexity, ranging from layman and up to 
expert information, because consumer health il-
lustrates the need for exploratory search.   
The PubMed repository, while primarily serving 
the academic community, is also used by laymen 
to ask health related questions. The PubMed que-
ry logs (Dogan et al, 2009) provide user queries 
with timestamps and anonymized user identifica-
tion. They are publically available and include 
over 600K queries per day. In this dataset, Dogan 
and Murray found that query reformulation (typ-
ical of exploratory search) is quite frequent: "In 
our dataset, 47% of all queries are followed by a 
new subsequent query. These users did not select 
any abstract or full text views from the result set. 
We make an operational assumption that these 
users? intent was to modify their search by re-
formulating their query." We used these logs to 
extract laymen queries relating to four topics: 
Asthma, Lung Cancer 2EHVLW\ DQG $O]KHLPHU?V 
disease. We extracted a single day query log. 
From these, we extracted sessions which con-
WDLQHG WKH WHUPV ?Asthma? ?Lung Cancer ,? 
?Obesity? RU ?Alzheimer .? Sessions containing 
VHDUFK WDJV VXFK DV ?>$XWKRU@? ZHUH removed 
to reduce the number of academic searches. The 
sessions were then manually examined and used 
to create zoom-in query chains of length 3 at 
most. The queries appear below: 
Asthma: 
Asthma causes? asthma allergy? asthma mold allergy; 
Asthma treatment?asthma medication?corticosteroids; 
Exercise induced asthma? exercise for asthmatic; 
Atopic dermatitis? atopic dermatitis medications? atopic 
dermatitis side effects; 
Atopic dermatitis? atopic dermatitis children? atopic der-
matitis treatment; 
Atopic dermatitis? atopic dermatitis exercise activity?
 atopic dermatitis treatment; 
Cancer: 
Lung cancer? lung cancer causes? lung cancer symptoms; 
Lung cancer diagnosis? lung cancer treatment?lung cancer 
treatment side effects; 
Stage of lung cancer? lung cancer staging tests? lung can-
cer TNM staging system; 
Types of lung cancer?non-small cell lung cancer treat-
ment?non-small cell lung cancer surgery; 
Lung cancer in women? risk factors for lung cancer in 
women? treatment of lung cancer in women; 
Lung cancer chemotherapy? goals of lung cancer chemo-
therapy? palliative care for lung cancer; 
Obesity: 
Salt obesity?retaining fluid; 
Obesity screening?body mass index?BMI Validity; 
Childhood obesity?childhood obesity low income?chil-
dren diet and exercise; 
Causes of childhood obesity?obesity and nutrition?school 
lunch; 
Obesity and lifestyle change?obesity metabolism?super-
foods antioxidant; 
Obesity and diabetes?emergence of type 2 diabetes?type 2 
diabetes and obesity in children; 
Alzheimer?s disease: 
Alzheimer memory?helping retrieve memory alzheimer 
?alzheimer memory impairment nursing; 
Cognitive impairment?Vascular Dementia?Vascular De-
mentia difference alzheimer; 
$O]KHLPHU?V symptoms?alzheimer diagnosis?alzheimer 
medications; 
Semantic dementia?first symptoms dementia?first symp-
toms alzheimer; 
Figure 1: Queries Used to Construct Dataset 
We asked medical experts to construct four 
document collections from well-known and reli-
able consumer health websites relating to the 
four subjects (Wikipedia, WebMD, and the 
NHS), so that they would provide general infor-
mation relevant to the queries. 
We then asked medical students to manually 
produce summaries of these four document col-
lections for each query-chain. The medical stu-
dents were instructed construct a text of up to 
250 words that provides a good answer to each 
query in the chain. For each query in a chain the 
summarizers should assume that the person read-
ing the summaries is familiar with the previous 
916
summaries in the chain so they should avoid re-
dundancy. 
Three distinct human summaries were pro-
duced for each chain.  For each chain, one sum-
mary was produced for each of the three queries, 
where the person producing the summary was 
not shown the next steps in the chain when an-
swering the first query. 
To simulate the exploratory search of the user 
we provided the annotators with a Solr1  query 
interface for each document collection. The in-
terface allowed querying the document set, read-
ing the documents and choosing sentences which 
answer the query. After choosing the sentences, 
annotators can copy and edit the resulting sum-
mary in order to create an answer of up to 250 
words. After processing the first two query chain 
summaries, the annotators held a post-hoc dis-
cussion about the different summaries in order to 
adjust their conception of the task. 
The statistics on the collected dataset appear in 
the Tables below: 
Document sets # Docs # Sentences #Tokens / 
Unique 
Asthma  125 1,924 19,662 / 2,284 
Lung-Cancer 135 1,450 17,842 / 2,228 
Obesity 289 1,615 21,561 / 2,907 
$O]KHLPHU?V 'LVHDVH 191 1,163 14,813 / 2,508 
 
Queries # Sessions # Sentences #Tokens / 
Unique 
Asthma  5 15 36 / 14 
Lung-Cancer 6 18 71 / 25 
Obesity 6 17 45 / 29 
$O]KHLPHU?V 'LVHDVH 4 12 33 / 16 
 
Manual Summaries # Docs # Sentences #Tokens / 
Unique 
Asthma  45 543 6,349  / 1,011 
Lung-Cancer 54 669 8,287  / 1,130 
Obesity 51 538 7,079  / 1,270 
$O]KHLPHU?V 'LVHDVH 36 385 5,031  /    966  
Table 1: Collected Dataset Size Statistics 
A key aspect of the dataset is that the same 
documents are summarized for each step of the 
chains, and we expect the summaries for each 
step to be different (that is, each answer is indeed 
responsive to the specific query it addresses). In 
addition, each answer is produced in the context 
of the previous steps, and only provides updated 
                                                 
1 http://lucene.apache.org/solr/ 
information with respect to previous answers. To 
ensure that the dataset indeed reflects these two 
aspects (responsiveness and freshness), we em-
pirically verified that summaries created for ad-
vanced queries are different from the summaries 
created for the same queries by summarizers who 
did not see the previous summaries in the chain. 
We asked from additional annotators to create 
manual summaries of advanced queries from the 
query chain without ever seeing the queries from 
the beginning of the chain. For example, given 
the chain (asthma causes? asthma allergy?
 asthma mold allergy), we asked summarizers to 
produce an answer for the second query (asthma 
allergy) without seeing the first step, on the same 
input documents. 
We used ROUGE to perform this validation: 
ROUGE compares a summary with a set of ref-
erence summaries and source documents. We 
first computed the mean ROUGE score of the 
second query summaries. The mean ROUGE 
score is the mean score of each manual summary 
vs. all other summaries about the same query. 
We got (?1 = 0.52, ?2 = 0.22, ??4 = 0.13). The 
mean ROUGE scores of the same second query 
summaries by people who did not see the previ-
ous query were markedly lower: ( ?1 =
0.40, ?2 = 0.22, ??4 = 0.01).  We only verified 
the asthma dataset in this manner. The results, 
except for the R2 test, had statistically significant 
difference with 95%  confidence interval. 
All the data, code and an annotated example can 
be found in our site2. 
We show below slightly shortened manual 
summaries created in answer to the following 
query chain: "obesity screening -> body mass 
index -> BMI Validity". We manually annotated 
the summaries to highlight how the exploration 
process develops and indicate some of the infor-
mation retrieval challenges faced when address-
ing the task: underlined terms are directly rele-
vant to the current query; bold italic terms have 
likely triggered the curiosity of the reader and led 
to asking the next query.  In this example, a user 
asks about obesity screening, and reformulates 
his query using the technical term of BMI, which 
is prevalent in the produced answer. 
 
Q1:  obesity screening 
Body mass index (BMI) is a measure used to determine 
                                                 
2 http://www.cs.bgu.ac.il/~nlpproj/QCFS/dataset.html 
917
childhood overweight and obesity. A child's weight status 
is determined using an age- and sex-specific percentile 
for BMI rather than the BMI categories used for adults 
because children's body composition varies as they age 
and varies between boys and girls. For children and ado-
lescents (aged 2-19 years): Overweight is defined as 
a BMI at or above the 85th percentile and lower than the 
95th percentile for children of the same age and 
sex.  Obesity is defined as a BMI at or above the 95th 
percentile for children of the same age and sex. BMI is not 
a perfect measure, but rather a helpful indicator. It doesn't 
consider things like being muscular or having a larger than 
average body frame and because growth patterns vary 
greatly among children.  As a result, doctors also factor a 
child's growth and development into consideration 
when screening for overweight and obesity. Obesity 
screening of adults using BMI may be less accurate for 
adults, due to the coarseness of the measurement: Heavily 
muscled athletes may have a high BMI, as many women 
who enter menopause, who tend to gain weight due to 
increased water retention. 
Q2: body mass index 
The body mass index (BMI), or Quetelet index, is a meas-
ure for human body shape based on an individu-
al's mass and height. Devised in the mid-1800s by Adolphe 
Quetelet during the course of developing "social physics", 
it is defined as an individual's body mass divided by the 
square of their height, the units being kg/m^2. BMI was 
explicitly cited as being appropriate for population studies, 
and inappropriate for individual diagnosis. BMI provides a 
simple measure of a person's thickness, allowing health 
professionals to discuss over-weight and underweight  
problems more objectively with their patients. Howev-
er, BMI has become controversial because many people, 
including physicians, have come to rely on its appar-
ent authority for medical diagnosis. However, it was origi-
nally meant to be used as a simple means of classifying 
sedentary individuals, or rather, populations, with an aver-
age body composition. For these individuals, the current 
value settings are as follows: (...). Nick Korevaar (a mathe-
matics lecturer from the University of Utah) suggests that 
instead of squaring the body height or cubing 
the body height, it would be more appropriate to use an 
exponent of between 2.3 and 2.7 (as originally noted by 
Quetelet). 
Q3: BMI Validity 
BMI has become controversial because many people, in-
cluding physicians, have come to rely on its apparent nu-
merical authority for medical diagnosis, but that was never 
the BMI's purpose; it is meant to be used as a simple 
means of classifying sedentary populations with an average 
body composition. In an article published in the July edi-
tion of 1972 of the Journal of Chronic Diseases, Ancel Keys 
explicitly cited BMI as being appropriate for population 
studies, but inappropriate for individual diagnosis. These 
ranges of BMI values are valid only as statistical categories 
While BMI is a simple, inexpensive method of screening for 
weight categories, it is not a good diagnostic tool: It does 
not take into account age, gender, or muscle mass. (...). 
Figure 2: Query Chain Summary Annotated Example 
5 Algorithms  
In this section, we first explain how we 
adapted the previously mentioned methods to the 
QCFS task, thus producing 3 strong baselines. 
We then describe our new algorithm for QCFS. 
5.1 Focused KLSum 
We adapted KLSum to QCFS by introducing 
a simple document selection step in the algo-
rithm.  The method is: given a query step ?, we 
first select a focused subset of documents from 
?,?(?).  We then apply the usual KLSum algo-
rithm over ?(?). This approach does not make 
any effort to reduce redundancy from step to step 
in the query chain.  In our implementation, we 
compute ?(?) by selecting the top-10 documents 
in ? ranked by ?? ? ??? scores to the query, as 
implemented in SolR. 
5.2 KL-Chain-Update 
KL-Chain-Update is a slightly more sophisti-
cated variation of KLSum that answers a query 
chain (instead a single query). When construct-
ing a summary, we update the unigram distribu-
tion of the constructed summary so that it in-
cludes a smoothed distribution of the previous 
summaries in order to eliminate redundancy be-
tween the successive steps in the chain. For ex-
ample, when we summarize the documents that 
were retrieved as a result to the first query, we 
calculate the unigram distribution in the same 
manner as we did in Focused KLSum; but for the 
second query, we calculate the unigram distribu-
tion as if all the sentences we selected for the 
previous summary were selected for the current 
query too, with a damping factor. In this variant, 
the Unigram Distribution estimate of word X is 
computed as: 
918
(Count(?,??????????) +
Count(?, ???????????)
??????????????? )
Length(??????????) +
Length(PreviousSum ? ??????????)
???????????????
 
5.3 ChainSum 
ChainSum is our adaptation of TopicSum to 
the QCFS task. We developed a novel Topic 
Model to identify words that are associated to the 
current query and not shared with the previous 
queries. We achieved this with the following 
model. For each query in a chain, we consider 
the documents ??which are "good answers" to 
the query; and ?? which are the documents used 
to answer the previous steps of the chain.  We 
assume in this model that these document subsets 
are observable (in our implementation, we select 
these subsets by ranking the documents for the 
query based on TFxIDF similarity). 
1. ? is the general words topic, it is intended 
to capture stop words and non-topic spe-
cific vocabulary. Its distribution ??  is 
drawn for all the documents from 
?????????(?, ??). 
2. ?? is the document specific topic; it repre-
sents words which are local for a specific 
document.  ???  is drawn for each docu-
ment from ?????????(?, ???). 
3. ? is the new content topic, which should 
capture words that are characteristic for 
??. ?? is drawn for all the documents in 
?? from ?????????(?, ??). 
4. ?  captures old content from ?? , ??  is 
drawn for all the documents in ??  from 
?????????(?, ??). 
5. ? captures redundant information between 
??  and ??, ??  is drawn for all the docu-
ments in ?? ? ?? from ?????????(?, ??). 
6. For documents from ?? we draw from the 
distribution ??1  over topics (?, ?, ?, ??) 
from a Dirichlet prior with pseudo-
counts (10.0,15.0,15.0,1.0)3 . For each 
word in the document, we draw a topic ? 
from ??, and a word ? from the topic in-
dicated by ?. 
                                                 
3 All pseudo-counts were selected empirically  
7. For documents from ??, we draw from the 
distribution ??2  over topics (?, ?, ?, ??) 
from a Dirichlet prior with pseudo-
counts  (10.0,15.0,15.0,1.0) . The words 
are drawn in the same manner as in ?1. 
8. For documents in ? ? (?? ? ??) we draw 
from the distribution ??3  over topics 
(?, ??) from a Dirichlet prior with pseudo-
counts (10.0,1.0) . The words are also 
drawn in the same manner as in ?1. 
The plate diagram of this generative model is 
shown in Fig.3. 
 
Figure 3 Plate Model for Our Topic Model 
We implemented inference over this topic 
model using Gibbs Sampling (we distribute the 
code of the sampler together with our dataset).  
After the topic model is applied to the current 
query, we apply KLSum only on words that are 
assigned to the new content topic. Fig.4 summa-
rizes the algorithm data flow. 
When running this topic model on our dataset, 
we observe: ??  mean size was 978 words and 
375 unique words. ??   mean size was 1374 
words and 436 unique words. ??  and ??  mean 
on average 159 words. These figures show there 
is high lexical overlap between the summaries 
answering query qi and qi+1 and highlight the 
need to distinguish new and previously exposed 
content. 
In the ChainSum model, the topic R aims at 
modeling redundant information between the 
previous summaries and the new summary.  We 
intend in the future to exploit this information to 
construct a contrastive model of content selec-
tion.  In the current version, R does not play an 
active role in content selection.  We, therefore, 
tested a variant of ChainSum that did not in-
clude ??  and obtained results extremely similar 
to the full model, which we report below. 
919
 Figure 4 ChainSum Architecture 
5.4 Adapted LexRank 
In LexRank, the algorithm creates a graph 
where nodes represent the sentences from the 
text and weighted edges represent the cosine-
distance of each sentence's TFxIDF vec-
tors. After creating the graph, PageRank is run to 
rank sentences. We adapted LexRank to QCFS in 
two main ways: we extend the sentence represen-
tation scheme to capture semantic information 
and refine the model of sentences similarity so 
that it captures query answering instead of cen-
trality. We tagged each sentence with Wikipedia 
terms using the Illinois Wikifier (Ratinov et al, 
2011) and with UMLS (Bodenreider, 2004) 
terms using HealthTermFinder (Lipsky-Gorman 
and Elhadad, 2011). UMLS is a rich medical on-
tology, which is appropriate to the consumer 
health domain. 
We changed the edges scoring formula to use 
the sum of Lexical Semantic Similarity (LSS) 
functions (Li et al, 2007) on lexical terms, Wik-
ipedia terms and UMLS terms: 
?????(?, ?) = ??????????(?, ?) + ?
? ???????(?, ?) + ?
? ???????(?, ?) 
Where: 
???(?1, ?2) =
? (????(
???(??
1,??
2)
???(??
1,??
1)
)???(??
1))?
? ???(??
1)?
 
Instead of using the cosine distance, in order to 
incorporate advanced word/term similarity func-
tions. For lexical terms, we used the identity 
function, for Wikipedia term we used Wikiminer 
(Milne, 2007), and for UMLS we used Ted 
Pedersen UMLS similarity function (McInnes et 
al., 2009).  Finally, instead of PageRank, we 
used SimRank (Haveliwala, 2002) to identify the 
nodes most similar to the query node and not 
only the central sentences in the graph.  
6 Evaluation 
6.1 Evaluation Dataset 
We worked on the dataset we created for 
QCFS and added semantic tags: 10% of the to-
kens had Wikipedia annotations and 33% had a 
UMLS annotation. 
6.2 Results 
 
Figure 5: ROUGE Recall Scores (with stemming and 
stop-words) 
For Focused KLSum we received ROUGE 
scores of (r1 = 0.281, r2 = 0.061, su4 = 0.100), 
KL-Chain-Update (r1 = 0.424, r2 = 0.149, su4 = 
0.193), ChainSum (r1 = 0.44988, r2 = 0.1587, 
su4 = 0.20594), ChainSum with t Simplified 
Topic model (r1 = 0.44992, r2 = 0.15814, su4 = 
0.20507) and for Modified-LexRank (r1 = 0.444, 
r2 = 0.151, su4 = 0.201). All of the modified ver-
sions of our algorithm performed better than Fo-
cused KLSum with more than 95% confidence.  
7 Conclusions 
We presented a new summarization task tai-
lored for the needs of exploratory search system. 
This task combines elements of question answer-
ing by sentence extraction with those of update 
summarization. 
The main contribution of this paper is the def-
inition of a new summarization task that corre-
sponds to exploratory search behavior and the 
contribution of a novel dataset containing human 
summaries. This dataset is annotated with Wik-
ipedia and UMLS terms for over 30% of the to-
kens. We controlled that the summaries cover 
only part of the input document sets (and are, 
therefore, properly focused) and sensitive to the 
position of the queries in the chain. 
Four methods were evaluated for the task. The 
baseline methods based on KL-Sum show a sig-
0
0.5
R1 R2 R3 R4 SU4
Focused-KLSum KLSum-Update LexRank-U
QC-LDA QC-simplified
920
nificant improvement when penalizing redun-
dancy with the previous summarization. 
7KLV SDSHU FRQFHQWUDWHG RQ ?]RRP LQ? TXHU\ 
FKDLQV RWKHU XVHU DFWLRQV VXFK DV ?]RRP RXW? RU 
?VZLWFK WRSLF? ZHUH OHIW WR IXWXUH ZRUN This pa-
SHU FRQFHQWUDWHG RQ ?]RRP LQ? TXHU\ FKDLQV RWK
HU XVHU DFWLRQV VXFK DV ?]RRP RXW? RU ?VZLWFK 
WRSLF? ZHUH OHIW WR IXWXUH ZRUN  The task remains 
extremely challenging, and we hope the dataset 
availability will allow further research to refine 
our understanding of topic-sensitive summariza-
tion and redundancy control. 
In future work, we will attempt to derive a 
task-specific evaluation metric that exploits the 
structure of the chains to better assess relevance, 
redundancy and contrast. 
Acknowledgments 
This work was supported by the Israeli Minis-
ter of Science (Grant #3-8705) and by the Lynn 
and William Frankel Center for Computer Sci-
ences, Ben-Gurion University.  We thank the 
reviewers for extremely helpful advice. 
References  
Marcia J. Bates. 1989. The design of browsing and 
berrypicking techniques for the online search 
interface, Online Information Review, 13(5), 407-
424.  
 
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 
2003. Latent dirichlet alocation, the Journal of 
machine Learning research, 3, 993-1022. 
 
Olivier Bodenreider. 2004. The unified medical 
language system (UMLS): integrating biomedical 
terminology, Nucleic acids research, 32(suppl 1), 
D267-D270.  
 
John M. Conroy, Judith D. Schlesinger, and Dianne P. 
O'Leary. 2011. Nouveau-rouge: A novelty metric 
for update summarization, Computational 
Linguistics, 37(1), 1-8. 
 
Rebecca JW Cline, and Katie M. Haynes. 2001. 
Consumer health information seeking on the 
Internet: the state of the art, Health education 
research, 16(6), 671-692.  
 
Daume Hal and Daniel Marcu. 2006. Bayesian query-
focused summarization, In Proceedings of the 21st 
International Conference on Computational 
Linguistics and the 44th annual meeting of the 
Association for Computational Linguistics (pp. 
305-312). Association for Computational 
Linguistics. 
 
Jean-Yves Delort, and Enrique Alfonseca. 2012. 
DualSum: a Topic-Model based approach for 
update summarization, In Proceedings of the 13th 
Conference of the European Chapter of the 
Association for Computational Linguistics (pp. 
214-223). Association for Computational 
Linguistics.  
 
Rezarta Islamaj Dogan, G. Craig Murray, Aur?lie 
N?v?ol, and Zhiyong Lu. 2009. Understanding 
PubMed? user search behavior through log 
analysis, Database: The Journal of Biological 
Databases & Curation, 2009. 
 
G?nes Erkan, and Dragomir R. Radev. 2004. 
LexRank: Graph-based lexical centrality as 
salience in text summarization, J. Artif. Intell. 
Res.(JAIR), 22(1), 457-479.  
 
Surabhi Gupta, Ani Nenkova, and Dan Jurafsky. 
2007. Measuring importance and query relevance 
in topic-focused multi-document summarization, In 
Proceedings of the 45th Annual Meeting of the 
ACL on Interactive Poster and Demonstration 
Sessions (pp. 193-196). Association for 
Computational Linguistics. 
 
Aria Haghighi, and Lucy Vanderwende. 2009. 
Exploring content models for multi-document 
summarization, In Proceedings of Human 
Language Technologies: The 2009 Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics (pp. 
362-370). Association for Computational 
Linguistics.  
 
Glen Jeh, and Jennifer Widom. 2002. SimRank: a 
measure of structural-context similarity, In 
Proceedings of the eighth ACM SIGKDD 
international conference on Knowledge discovery 
and data mining (pp. 538-543). ACM.  
 
Baoli Li, Joseph Irwin, Ernest V. Garcia, and Ashwin 
Ram. 2007. Machine learning based semantic 
inference: Experiments and Observations at RTE-
3, In Proceedings of the ACL-PASCAL Workshop 
on Textual Entailment and Paraphrasing (pp. 159-
164). Association for Computational Linguistics. 
 
Chin-Yew Lin. 2004. Rouge: A package for automatic 
evaluation of summaries, In Text Summarization 
Branches Out: Proceedings of the ACL-04 
Workshop (pp. 74-81). 
 
Sharon Lipsky-Gorman, and No?mie Elhadad 2011. 
ClinNote and HealthTermFinder: a pipeline for 
921
processing clinical notes, Columbia University 
Technical Report, Columbia University. 
 
Gary Marchionini. 2006. Exploratory search: from 
finding to understanding, Communications of the 
ACM, 49(4), 41-46.  
 
Bridget T. McInnes, Ted Pedersen, and Serguei VS 
Pakhomov. (2009). UMLS-Interface and UMLS-
Similarity: open source software for measuring 
paths and semantic similarity, AMIA Annual 
Symposium Proceedings, American Medical 
Informatics Association. 
 
David Milne. 2007. Computing semantic relatedness 
using wikipedia link structure, In Proceedings of 
the new zealand computer science research student 
conference. 
 
Ani Nenkova, and Rebecca J. Passonneau. 2004. 
Evaluating Content Selection in Summarization: 
The Pyramid Method, In HLT-NAACL (pp. 145-
152). 
 
Jahna Otterbacher, Gunes Erkan, and Dragomir R. 
Radev. 2009. Biased LexRank: Passage retrieval 
using random walks with question-based priors, 
Information Processing & Management, 45(1), 42-
54. 
 
Lawrence Page, Sergey Brin, Rajeev Motwani, and 
Terry Winograd. 1999. The PageRank citation 
ranking: bringing order to the web, 
Lev Ratinov, Dan Roth, Doug Downey, and Mike 
Anderson. 2011. Local and Global Algorithms for 
Disambiguation to Wikipedia, In ACL (Vol. 11, 
pp. 1375-1384). 
 
Ryen W. White, and Resa A. Roth. 2009. Exploratory 
search: Beyond the query-response paradigm. 
Synthesis Lectures on Information Concepts, 
Retrieval, and Services, 1(1), 1-98.  
 
 
 
922
Proceedings of the 2012 Student Research Workshop, pages 43?48,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Domain Adaptation of a Dependency Parser with a Class-Class Selectional Preference Model  
 Abstract When porting parsers to a new domain, many of the errors are related to wrong attachment of out-of-vocabulary words. Since there is no available annotated data to learn the attachment preferences of the target domain words, we attack this problem using a model of selectional preferences based on domain-specific word classes. Our method uses Latent Dirichlet Allocations (LDA) to learn a domain-specific Selectional Preference model in the target domain using un-annotated data. The model provides features that model the affinities among pairs of words in the domain.  To incorporate these new features in the parsing model, we adopt the co-training approach and retrain the parser with the selectional preferences features. We apply this method for adapting Easy First, a fast non-directional parser trained on WSJ, to the biomedical domain (Genia Treebank). The Selectional Preference features reduce error by 4.5% over the co-training baseline. 1 Introduction Dependency parsing captures a useful representation of syntactic structure for information extraction. For example, the Stanford Dependency representation has been used extensively in domain-specific relation extraction tasks such as BioNLP09 (Kim, Ohta et al 2009) and BioNLP11 (Pyysalo, Ohta et al 2011). One obstacle to widespread adoption of such syntactic representations is that parsers are generally trained on a specific domain (typically WSJ news data) and it has often been observed that the accuracy of dependency parsers drops significantly when used in a domain other than the training domain.  
Domain adaptation for dependency parsing has been explored extensively in the CoNLL 2007 Shared Task (Nivre, Hall et al 2007). The objective in this task is to adapt an existing parser from a source domain in order to achieve high parsing accuracy on a target domain in which no annotated data is available. Common approaches include self-training (McClosky, Charniak et al 2006), using word distribution features (Koo, Carreras et al 2008) and co-training (Sagae and Tsujii 2007) . Dredze et al (Dredze, Blitzer et al 2007) explored a variety of methods for domain adaptation, which consistently showed little improvement and concluded that domain adaptation for dependency parsing is indeed a hard task. Typically, parsing accuracy drops from 90+% in-domain to 80-84% in the target domain. When porting parsers to the target domain, many of the errors are related to wrong attachment of out-of-vocabulary words, i.e., words which were not observed when training on the source domain. Since there is not sufficient annotated data to learn the attachment preferences of the target domain words, we attack this problem using a model of selectional preferences based on domain-specific word classes.  Selectional preferences (SP) describe the relative affinity of arguments and head of a syntactic relation. For example, in the sentence: ?D3 activates receptors in blood cells from patients?, the preposition ?from? may be attached to either ?cells? or ?receptors?. However, the head word ?cells? has greater affinity to ?patients? than the candidate ?receptors? would have towards "patients". Note that this preference is highly context-specific. Several methods for learning SP (not in the context of domain adaptation) have been proposed. Commonly, these methods rely on learning semantic classes for arguments and learning the preference of a predicate to a semantic class. These semantic classes may be derived from manual knowledge bases such as WordNet or FrameNet, or semantic classes learned from large corpora. Recently, Ritter et al (2010) and 
Raphael Cohen* Yoav Goldberg** Michael Elhadad Ben Gurion University of the Negev Department of Computer Science POB 653 Be?er Sheva, 84105, Israel {cohenrap,yoavg,elhadad}@cs.bgu.ac.il 
?Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University **Current affiliation: Google Inc. 
43
S?aghdha (2010) both present induction methods of SP of verb-arguments using LDA (Blei, Ng et al 2003). Hartung and Frank (2011) extended the LDA-based approach to learning preference for adjective-noun phrases.  In this work, we tackle the task of domain adaptation by developing a domain-specific SP model. Our initial observation is that parsers fail on the target domain when trying to attach domain-specific words not seen during training. We observe as many as 15% of the words are unknown when applying a WSJ-trained parser on Genia and PennBioIE data, compared to only 2.5% in-domain. Parsers trained on the source domain cannot learn attachment preferences for such words. Our motivation is, therefore, to attempt to learn attachment preferences for domain specific words using un-annotated data. Specifically, we focus on acquiring a domain-specific SP model.  Our approach consists of using the low-accuracy source-domain parser on large quantities of in-domain sentences. We extract from the resulting parse trees a collection of syntactically related pairs of words. We then train an LDA model over these pairs of words and derive a domain-specific model of lexical affinities between pairs of words.  We finally re-train a parser model to exploit this domain-specific data.  To this end, we use the approach of co-training, which consists of identifying reliable parse trees in the target domain in an unsupervised manner using an ensemble of two distinct parsers, and extending the annotated training set with these reliable parse trees. Co-training alone significantly reduces the proportion of unknown words in the re-trained parser ? in the extended co-training dataset, we observe that the unknown words rate drops from 15% to 4.5%. Data sparseness, however, remains an issue: 1/3 of the domain-specific words added to the model by co-training appear only once in the extended training set, and we observe that many of the attachment errors are concentrated in a few syntactic configurations (e.g., head(V or N)-prep-pobj, N-N or head(N)-Adj).  We extend co-training by introducing our SP model, which is class-based and specific to these difficult syntactic configurations. Our method reduces error in the Genia Treebank (Tateisi, Yakushiji et al 2005) by 3.5% over co-training. Introducing additional distributional lexical features (Brown clusters learned in-domain), further reduces error to a total 4.5% reduction. Overall, our parser achieves an accuracy of 83.6% UAS on the Genia domain without annotated data in this domain. 
2 Our Approach To understand the difficulty of domain adaptation, we applied our parser trained on the WSJ news domain to the Genia and measured observed errors.  Most of the errors were found in a small set of syntactic configurations: verb-prep-noun, noun-adjective, noun-noun (together these relations make up 32 % of the errors).  For example: in ?nuclear factor-kappa-B DNA-binding activity? the parser chooses ?factor-kappa-B? as the head of ?nuclear? instead of ?activity?. We observe that these errors involve domain-specific vocabulary, and are difficult to disambiguate for non-expert humans as well. Accordingly, we try to acquire a domain-specific model of word-pairs affinities.  Our parsing model (EasyFirst) allows us to use such bi-lexical features in an efficient manner.  Because of data sparseness, however, we aim to acquire class-based features, and decide to model these lexical preferences using the LDA approach. Our method proceeds in two stages: 1. Learn selectional preferences from an automatically parsed corpus using LDA on selected syntactic configurations 2. Integrate the preferences into the parsing model as new features using co-training. 2.1 Learning Selectional Preferences Following (Ritter, Mausam et al 2010) and (S?aghdha 2010), we model lexical affinity between words in specific syntactic configurations using LDA.  Traditionally, LDA learns a set of "topics" from observed documents, based on observed word co-occurrences. In our case, we form artificial documents, which we call syntactic contexts, by collecting head-daughter pairs from parse trees. A syntactic context is constructed for each head word, which contains the related words to which it was found attached.  In the collection process, we identify two syntactic configurations that yield high error rates: head-prep-noun and noun-adj. We collect two types of syntactic contexts: the preposition contexts contain the set of nouns related to the head through any preposition and the adjective contexts contain the set of adjectives directly related to the head noun. We then learn an LDA model on each of these contexts collections.  We use Mallet (McCallum 2002) to learn topic models with hyper-parameter optimization(Wallach, Mimno et al 2009). The optimal number of topics is selected empirically based on model fit to held-out data. 
44
The resulting topics represent latent semantic classes of the daughter words. We define a measure of shared affinity between a head word h and a candidate daughter word d (in a given configuration) s: ??????? ?, ? =  ? ?(?|?) ? ?(?|?) ? ??? ? ???  where P(c|h) is the predicted probability of topic c given the syntactic context associated to head word h. That is, when we apply the LDA model on the syntactic context of h, we assign topics to each of the associated daughter words and count their proportion. Note that this affinity measure may predict a non-zero affinity to a pair (h, d) even though this word pair has never been observed. The result is a class-class SP model with reduced dimensionality compared to word-word models based for example on PMI.  Table 1 lists examples of learned topics. Note that these topics are high-quality semantic clusters that reflect domain semantics, with marked differences between the news and bio-medical domains. 2.2  Co-training to exploit domain features At this stage, we have acquired a domain-specific model of word affinity that exploits semantic classes and depends on specific syntactic configurations (head-prep-obj and noun-adj).  We now attempt to exploit this model to adapt our source parser to the target domain.  To this end, we want to re-train the parser using new features based on the SP model in addition to the original features.  We use the framework of co-training to achieve this goal (Sagae 
and Tsujii 2007): we use two different parsers: Easy-First (Goldberg and Elhadad 2010) and MALT (Nivre, Hall et al 2006) trained on the same WSJ source domain. We apply these two parsers on a large set of target-domain sentences. We select those sentences where the 2 parsers agree (produce identical trees) and add them to the original source-domain training set. We thus obtain an extended training set with many in-domain samples. We can now re-train the parser using the new SP features. 2.3 SP as features for the Easy First parser We use the deterministic non-directional Easy-First parser for re-training. This parser incrementally adds edges between words starting with the easier decisions before continuing to difficult ones. Simple structures are first created and their information is available when deciding how to connect complex ones. Easy-First operates in ?(?????) time compared to ?(??) of graph-based parsers such as MST (McDonald, Pereira et al 2005). As a baseline we use the features provided in the Easy-First distribution. We extend these features with pair-wise affinity measures based on our SP model. The affinity measure ranges from 0 to 1. We bin this measure into (low, medium, high, very-high) binary features. When attaching a preposition to its parent, we add one more feature: the affinity of the head candidate with the preposition's daughter (the pobj).  In addition to these pair-wise features, we also 
Source Relation Type Semantic Class Arguments Predicates BLLIP Arg?? Prep ?? Predicate Show Business   actors clips soundtrack genre taping characters roles immortalized starred costumes premise screening featured performances poster trumpeted star retrospective clip script  
film show movie films movies shows television series stage theater program production version music hollywood broadway 
BLLIP Arg?? Prep ?? Predicate Sports quarterbacks starters pitcher pitchers quarterback coaching receiver linebackers cornerback outfielder baseman fullback  team game league teams games time field players years baseball year rules nfl seasons level player leagues nba club history school state BLLIP Arg?? Prep ?? Predicate Work Position jockeying groom groomed relegate relieved unwinding jockeyed selected selecting appointing disqualify named  job post position draft positions candidate team one jobs which role posts successor Genia Arg?? Prep ?? Predicate Cell-cycle process stages stage process steps committed block regulator acquire switch points needed directs determinant il-21 proceeds arrest regulators relate d3  
differentiation development activation  maturation cycle hematopoiesis infection commitment lymphopoiesis stage lineage selection erythropoiesis cascade Genia Arg?? Prep ?? Predicate Cells and growing conditions supernatants co-culture co-cultured replication medium surface chemotaxis supernatant beta migration cocultured cultures hyporesponsiveness  
cell monocyte lymphocyte pbmc macrophage line blood neutrophil cd dc leukocyte t eosinophil fibroblast platelet keratinocyte Genia Adjective?? Noun Protein activity and regulation factor-induced tnfalpha-induced agonist-induced thrombin-induced il-2-induced factor-alpha-induced il-1beta-induced cd40-induced rankl-induced augmented il-4-induced  
expression activation production phosphorylation response proliferation activity binding secretion apoptosis differentiation translocation release signaling adhesion synthesis generation Table 1 High affinity classes in the Class-Class Selectional Preferences model extracted with LDA. Classes 1-5 are from preposition head/object pairs (e.g ?groomed for position? fits the third topic) and class 6 are adjective modifier pairs. Classes 1-3 are from Bllip (un-annotated WSJ corpus) (Charniak, Blaheta et al 2000) while classes 4-6 are from a corpus composed of Medline abstracts from the Genia (see section 5.1). Class 4 contains arguments and predicates concerning cell-cycle process. In class 5 arguments are cell growing conditions and predicates are types of cells. 
45
introduce features that correspond to the latent topic class of the words according to each of the 2 acquired LDA models (this introduces one binary feature for each topic).  These latent semantic class features are similar in nature to distributional lexical features as used in (Koo, Carreras et al 2008). The EasyFirst parser combines partial trees bottom-up. When deciding whether to attach the partial tree "from patients" to either "cells" or "receptors", we compute the affinities of "cells/patients" and "receptors/patients". Our model produces features indicating medium affinity for ?receptors from patients? and a high affinity for cells from patients?. 3 Experiments and Evaluation 3.1 Genia Treebank The Genia Treebank (Tateisi, Yakushiji et al 2005) contains 18K sentences from the biomedical domain, transformed into dependency trees 1  using (De Marneffe, MacCartney et al 2006) 2 . The corpus contains 2.3K sentences longer than 40 tokens that were excluded from the evaluation. The treebank was divided into test and development sets of equal size.  We created an un-annotated corpus of 200K sentences by querying Medline with the same query terms used to create Genia. We used the Genia POS Tagger on this dataset (Tsuruoka, Tateishi et al 2005). The corpus was parsed with Easy-First and MALT (arc-eager, polynomial) to create co-training data, yielding 21K sentences with 100% agreement. The parsed corpus of 200K sentences was used to produce selectional preference models for adjective-nouns, with 200 topics, and for head-prep-object with 300 topics. We used word lemmas for each pair when preparing syntactic contexts for LDA training (see Table 2).  Relation #  Pairs # Daughter # Heads preposition 360,041 1,727 2,391 adjective 384,347 1,570 2,003  Table 2. Statistics for the training data of the SP model. 3.2 Coverage Many of the features learned in training a parser are lexicalized; this is an important factor in the drop in accuracy when parsing in a new domain.  To understand the nature of the contribution of the features learned by our SP model, we calculated the coverage of the features acquired in two unsupervised methods: Brown clustering and our SP classes. We                                                       1 We use the PTB version of Genia created by Illes Solt. 2 We convert using the Stanford Parser bundle. 
count the number of tokens in the Treebank which gain a feature at training time (we ignore punctuation, coordination and preposition tokens). Our SP model covers 53% of the tokens in the test set. Brown clusters calculated with the implementation of Liang (2005) achieve coverage of 73%.  Brown clusters features are also class-based distributional features based on n-gram language models, but do not take into account syntactic configurations. 3.3 Adaptation Evaluation We use a number of baselines for the adaptation task. Three parsers were evaluated on the target domain: Easy-First, MST second order and MALT arc-eager with a polynomial kernel. We report UAS scores of trees of length < 40 without punctuation. The first baseline setting for each parser is the model trained on WSJ sections 2-21.  The second baseline we report is co-training using WSJ 2-21 combined with the 21K full agreement parse trees extracted from Medline, but without new features. Parser Training Data Features UAS (Exact Match)  MST WSJ 2-21  79.6 (10)  MALT WSJ 2-21  81.1 (16.6)  Easy-First WSJ 2-21  80.5 (12.3)  MST Co-Training  81.3 (14.1)  MALT Co-Training  82.1 (16.5)  Easy-First Co-Training  82.8 (16.2)  Easy-First Co-Training +Brown Clusters 83.1 (17) +0.3 Easy-First Co-Training +SP-Lexicalized 83.0 (16.9) +0.2 Easy-First Co-Training +SP-Lexicalized +SP-Classes 83.4 (16.6) +0.6 Easy-First Co-Training +SP-Lexicalized +SP-Classes +Brown Clusters 83.6 (17.2) +0.8 Easy-First GeniaTB Dev  89.8 (28.6)  Table 3. Accuracy for different parser settings on Genia test set.  The best performing adapted model trains with co-training data and combines SP and Brown clusters as features.  In Table 3, we see that the combined SP-Features improved the co-training baseline by 0.6%, a significant error reduction of 3.5% (p-value < 0.01).  We list improvement when introducing only pair-wise SP features, and when adding SP-based semantic classes. The effect is also additive with the Brown clusters features, producing an improvement of 0.8% when combined (error reduction of 4.5%). To evaluate the model adapted for Genia on the general biomedical domain, we used the PennBioIE Treebank . This dataset contains 6K sentences from different biomedical domains. We compared 3 models (see Table 4):  1. Easy-First, MALT and MST trained on WSJ. 2. Easy-First with co-training on Genia. 
46
3. Easy-First with co-training on Genia with Selectional Preference features. Domain adaptation to Genia carried over to the closely related PennBioIE dataset, demonstrating the generalization capability of the method. Parser Training Data Features UAS  MALT WSJ 2-21  78.8  MST WSJ 2-21  81.4  Easy-First WSJ 2-21  79.8  Easy-First Co-Training  81.9  Easy-First Co-Training +SP-Lexicalized +SP-Classes +Brown Clusters 82.2 +0.3 Table 4. Accuracy of parsers on PennBioIE Treebank.  3.4 Error Analysis We compare the parser using the SP pair-wise features for preposition attachment to the co-trained baseline on Genia. The overall accuracy of the parser is improved by 0.2%. However, the two models agree only on 90% of the edges, indicating the new SP features play a very active role when parsing. For ?E3330 inhibited this induced promoter activity in a dose-dependent manner?, the co-trained parser chose ?activity? as the head of ?in? instead of ?inhibited?. The affinity feature in our model for (?inhibited?, ?manner?) shows affinity of high (40-60%) compared to low (5-20%) for the wrong pair ("activity", "manner"). The same change occurs for ?LysoPC attenuates activation during inflammation and athero-sclerosis?, where the improved model prefers the pair (?attenuates?, ?inflammation?) to the pair (?activation?, ?inflammation?) which was chosen by the co-trained model. The modest overall improvement is due to errors introduced by the new model. In ?Tissue obtained from ectopic pregnancies may identify the mechanism of trophoblast invasion in ectopic pregnancies?, the correct governor of ?in? is ?invasion?. However, the SP model ranks the affinity of (?invasion?, ?pregnancies?) lower than that of (?mechanism?, ?pregnancies?). Most of the improvement of the full SP model (+0.6%) comes from an improvement in the N-N relation from 83% to 84.9% (11% error reduction), this improvement is due to semantic classes features learned on the relations of noun-adjective and head-prep-pobj. 3.5 Effect on NER Since most of the improvement comes from the N-N relation, we expect improvement for downstream applications such as Named Entity Recognition, a basic task frequently used in the biomedical domain. 
We use the portion of the Genia Treebank covered by the Genia NER corpus (Kim, Ohta et al 2004). We expect the inner tokens of a named entity to be connected by relation of N-N or N-Adj. We evaluate the accuracy of these two relations for NE tokens. The Easy-First with co-training baseline produces accuracy of 82.9% on this specific set of relations, improved by the SP model to 84.4%, a reduction in error of 8.7%. 4 Related Work 4.1 Learning of Selectional Preference Preference of predicate-argument pairs has been studied in depth with a number of approaches. Resnik (1993) suggested a class-based model for preference of predicates combining WordNet classes with mutual information techniques for associating an argument with a predicate class from WordNet.  Another approach models words in a corpus as context vectors (Erk and Pado 2008; Turney and Pantel 2010) for discovering predicate or argument classes using large corpora or the Web. Recently, semantic classes were successfully induced using LDA topic modeling. These methods have shown success in modeling verb argument relationship to a single predicate (Ritter, Mausam et al 2010) or a predicate pair (S?aghdha 2010), as well as for adjective-noun preference (Hartung and Frank 2011).  4.2 Learning SP for improving dependency parsing  The argument-predicate choice learned in SP is directly related to the decision of creating an edge between them in a parse tree. Van Noord (2007) modeled verb-noun preferences using pointwise mutual information (PMI) using an automatically parsed corpus in Dutch. Association scores of pairs were added as features improving the accuracy significantly from 87.4% to 87.9%.  Nakov and Hearst (Nakov and Hearst 2005) focused on resolving PP attachments and coordination. They used co-occurrence counts from web queries in order to estimate selectional restrictions. Zhou et al (2011) used N-gram counts from Google search and Google V1 to deduce word-word attachment preferences. They used these counts in a pair-wise mutual information (PMI) scheme as features for improving parsing in the News domain (WSJ) and adaptation for biomedical domain. Their evaluation showed improvement of 1% on WSJ 
47
section 23 over the vanilla MST parser and a significant increase in the domain adaptation problem.  4.3 Domain adaptation of dependency parsing Domain adaptation for dependency parsing has been studied mostly in regard to the CoNLL 2007 shared task (Nivre, Hall et al 2007). Both of the leading methods included learning from a parser ensemble. Attardi et al?s (2007) used a weak parser in order to identify common parsing errors and overcome those in the training of a stronger parser. Sagae and Tsujii (2007) used two different parsers to parse un-annotated in-domain data and used the trees where the two parsers agreed to augment the training corpus.  Dredze et al (2007) approached the ?closed? problem, i.e., without using additional un-annotated data. They used the PennBioIE Treebank and applied a number of adaptation techniques: (1) features concerning NPs such as chunking information and frequency; (2) word distribution features; (3) features encoding information from diverse parsers; (4) target focused learning ? giving greater weight in training to sentences which are more likely in a target domain language model.  These methods have not improved accuracy over the baseline of the MST parser (McDonald, Pereira et al 2005) trained on WSJ.  5 Conclusion Learning class-class selectional preferences from a large in-domain corpus assists dependency parsing significantly. We have suggested a method for learning selectional preference classes for a specific domain using an existing parser and a standard implementation of LDA topic modeling. The SP model can be used for estimating the affinity between a pair of tokens or simply as a feature of semantic class association. This approach is faster when querying the model for the affinity of a pair of words than a PMI model suggested by Zhou et al(2011). While covering fewer tokens in the target test set than Brown clusters, the method achieved a higher improvement of parsing performance. Furthermore, some of the improvement was additive and reduced UAS error by 4.5% compared to a strong co-training baseline. 6 References  Attardi, G., F. Dell?Orletta, et al (2007). Multilingual dependency parsing and domain adaptation using DeSR. ACL. Blei, D. M., A. Y. Ng, et al (2003). "Latent dirichlet alocation." JMLR 3: 993-1022. Charniak, E., D. Blaheta, et al (2000). "Bllip 1987-89 wsj corpus release 1." LDC. 
De Marneffe, M. C., B. MacCartney, et al (2006). Generating typed dependency parses from phrase structure parses. LREC. Dredze, M., J. Blitzer, et al (2007). Frustratingly hard domain adaptation for dependency parsing. CoNLL 2007. Erk, K. and S. Pado (2008). A structured vector space model for word meaning in context. EMNLP 2008: 897-906. Goldberg, Y. and M. Elhadad (2010). An efficient algorithm for easy-first non-directional dependency parsing. NAACL 2010: 742-750. Hartung, M. and A. Frank (2011). Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases. ACL. Kim, J.-D., T. Ohta, et al (2009). Overview of BioNLP'09 shared task on event extraction. Current Trends in Biomedical NLP, ACL: 1-9. Kim, J.-D., T. Ohta, et al (2004). Introduction to the bio-entity recognition task at JNLPBA. Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications. Geneva, Switzerland, Association for Computational Linguistics: 70-75. Koo, T., X. Carreras, et al (2008). Simple semi-supervised dependency parsing. ACL 2008: 595-603. Liang, P. (2005). Semi-supervised learning for natural language, Massachusetts Institute of Technology. McCallum, A. K. (2002). "Mallet: A machine learning for language toolkit." McClosky, D., E. Charniak, et al (2006). Effective self-training for parsing, ACL. McDonald, R., F. Pereira, et al (2005). Non-projective dependency parsing using spanning tree algorithms. EMNLP: 523-530. Nakov, P. and M. Hearst (2005). Using the web as an implicit training set: application to structural ambiguity resolution. EMNLP, Association for Computational Linguistics: 835-842. Nivre, J., J. Hall, et al (2007). The CoNLL 2007 Shared Task on Dependency Parsing, CoNLL 2007. s. 915-932. Nivre, J., J. Hall, et al (2006). Maltparser: A data-driven parser-generator for dependency parsing. Noord, G. v. (2007). Using self-trained bilexical preferences to improve disambiguation accuracy. 10th International Conference on Parsing Technologies, ACL: 1-10. Pyysalo, S., T. Ohta, et al (2011). "Overview of the Entity Relations (REL) supporting task of BioNLP 2011." ACL HLT 2011 1(480): 83. Ritter, A., Mausam, et al (2010). A latent dirichlet alocation method for selectional preferences. ACL 2010: 424-434. Sagae, K. and J.-i. Tsujii (2007). Dependency parsing and domain adaptation with LR models and parser ensembles. EMNLP-CoNLL 2007: 1044-1050. S?aghdha, D. (2010). Latent variable models of selectional preference. ACL 2010: 435-444. Tateisi, Y., A. Yakushiji, et al (2005). Syntax Annotation for the GENIA corpus. ACL. Tsuruoka, Y., Y. Tateishi, et al (2005). "Developing a robust part-of-speech tagger for biomedical text." AII: 382-392. Turney, P. D. and P. Pantel (2010). "From frequency to meaning: Vector space models of semantics." JAIR 37(1): 141-188. Wallach, H., D. Mimno, et al (2009). "Rethinking LDA: Why priors matter." NIPS 22: 1973?1981. Zhou, G., J. Zhao, et al (2011). Exploiting web-derived selectional preference to improve statistical dependency parsing. ACL. 
48
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 116?119,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Effect of Out Of Vocabulary terms on inferring eligibility criteria for a 
retrospective study in Hebrew EHR 
 
 
Raphael Cohen* 
Computer Science Dept. 
Ben-Gurion University in the Negev 
cohenrap@bgu.ac.il 
Michael Elhadad 
Computer Science Dept. 
Ben-Gurion University in the Negev 
elhadad@cs.bgu.ac.il 
 
  
 
1 Background 
The Electronic Health Record (EHR) contains 
information useful for clinical, epidemiological 
and genetic studies. This information of patient 
symptoms, history, medication and treatment is 
not completely captured in the structured part of 
the EHR but is often found in the form of free-
text narrative. 
A major obstacle for clinical studies is finding 
patients that fit the eligibility criteria of the 
study. Using EHR in order to automatically iden-
tify relevant cohorts can help speed up both clin-
ical trials and retrospective studies (Restificar, 
Korkontzelos et al 2013).  
While the clinical criteria for inclusion and 
exclusion from the study are explicitly stated in 
most studies, automating the process using the 
EHR database of the hospital is often impossible 
as the structured part of the database (age, gen-
der, ICD9/10 medical codes, etc.?) rarely covers 
all of the criteria. 
Many resources such as UMLS (Bodenreider 
2004), cTakes (Savova, Masanz et al 2010), 
MetaMap (Aronson and Lang 2010) and recently 
richly annotated corpora and treebanks (Albright, 
Lanfranchi et al 2013) are available for pro-
cessing and representing medical texts in Eng-
lish. Resource poor languages, however, suffer 
from lack in NLP tools and medical resources. 
Dictionaries exhaustively mapping medical terms 
to the UMLS medical meta-thesaurus are only 
available in a limited number of languages be-
sides English. NLP annotation tools, when they 
exist for resource poor languages, suffer from 
heavy loss of accuracy when used outside the 
domain on which they were trained, as is well 
documented for English (Tsuruoka, Tateishi et 
al. 2005; Tateisi, Tsuruoka et al 2006). 
In this work we focus on the problem of clas-
sifying patient eligibility for inclusion in retro-
spective study of the epidemiology of epilepsy in 
Southern Israel. Israel has a centralized structure 
of medical services which include advanced 
EHR systems. However, the free text sections of 
these EHR are written in Hebrew, a resource 
poor language in both NLP tools and hand-
crafted medical vocabularies. 
Epilepsy is a common chronic neurologic dis-
order characterized by seizures. These seizures 
are transient signs and/or symptoms of abnormal, 
excessive, or hyper synchronous neuronal activi-
ty in the brain. Epilepsy is one of the most com-
mon of the serious neurological disorders (Hirtz, 
Thurman et al 2007).  
2 Corpus 
We collected a corpus of patient notes from 
the Pediatric Epilepsy Unit, an outpatient clinic 
for neurology problems, not limited to epilepsy, 
in Soroka Hospital. This clinic is the only availa-
ble pediatric neurology clinic in southern Israel 
and at the time of the study was staffed by a sin-
gle expert serving approximately 225,000 chil-
dren. The clinical corpus spans 894 visits to the 
Children Epilepsy Unit which occurred in 2009 
by 516 unique patients. The corpus contains 
226K tokens / 12K unique tokens. 
?Supported by the Lynn and William Frankel Center for 
Computer Sciences, Ben Gurion University 
116
The patients were marked by the attending 
physician as positive or negative for epilepsy. In 
the study year, 2009, 208 patients were marked 
as positive examples and 292 as negative. The 
inclusion criteria were defined as history of more 
than one convulsive episode excluding febrile 
seizures. In practice, the decision for inclusion 
was more complex as some types of febrile sei-
zure syndromes are considered a type of epilepsy 
while some patients with convulsion were ex-
cluded from the study for various reasons. 
3 Method 
We developed a system to classify EHR notes in 
Hebrew into ?epilepsy? / ?non-epilepsy? classes, 
so that they can later be reviewed by a physician 
as eligible candidates into a cohort. The system 
analyzes the Hebrew text into relevant tokens by 
applying morphological analysis and word seg-
mentations, Hebrew words are then semi-
automatically aligned to the UMLS vocabulary. 
The most important tagged Hebrew words are 
then used as features fed to a statistical document 
classification system.  We evaluate the perfor-
mance of the system on our corpus, and measure 
the impact of Hebrew text analysis in improving 
the performance for patient classification. 
4 Out-Of-Vocabulary Terms 
The complex rules of Hebrew word formation 
make word segmentation the first challenge of 
any NLP pipeline in Hebrew. Agglutination of 
function words leads to high ambiguity in He-
brew (Adler and Elhadad 2006). To perform 
word segmentation, Adler and Elhadad (Adler 
and Elhadad 2006) combine segmentation and 
morpheme tagging using an HMM model over a 
lattice of possible segmentations. This learning 
method uses a lexicon to find all possible seg-
mentations for all tokens and chooses the most 
likely one according to POS sequences. Un-
known words, a class to which most borrowed 
medical terms belong, are segmented in all pos-
sible ways (there are over 150 possible prefixes 
and suffixes in Hebrew) and the most likely form 
is chosen using the context within the same sen-
tence. Beyond word segmentation, the rich mor-
phological nature of Hebrew makes POS tagging 
more complex with 2.4 possible tags per token 
on average, compared to 1.4 for English. 
Out of 12K token types in the corpus 3.9K 
(30%) were not found in the lexicon used by the 
Morphological Disambiguator compared to only 
7.5% in the Newswire domain. A sample of 2K 
unknown token was manually annotated as: 
transliteration, misspelling and Hebrew words 
missing in the lexicon. Transliterated terms made 
up most of the unknown tokens (71.5%) while 
the rest were misspelled words (16%) and words 
missing from the lexicon (13.5%). 
Error analysis of the Morphological Disam-
biguator in the medical domain corpora shows 
that in the medical domain, Adler et als un-
known model still performs well: 80% of the 
unknown tokens were still analyzed correctly. 
However, 88.5% of the segmentation errors were 
found in unknown tokens. Moreover, the translit-
erated words are mostly medical terms important 
for understanding the text. 
5 Acquiring a Transliterations Lexicon 
As transliterations account for a substantial 
amount of the errors and are usually medical 
terms, therefore of interest, we aim to automati-
cally create a dictionary mapping transliterations 
in our target corpus to a terminology or vocabu-
lary in the source language. In our case, the 
source language is medical English which is a 
mix of English and medical terms from Latin as 
represented by the UMLS vocabulary. 
The dictionary construction algorithm is based 
on two methods: noisy transliteration of the med-
ical English terms from the UMLS to Hebrew 
forms (producing all the forms an English terms 
may be written in Hebrew, see (Kirschenbaum 
and Wintner 2009)) and matching the generated 
Figure 1 ? Decision Tree for inclusion/exclusion. Sodium Valproate (dplpt) is a key term which is 
often segmented incorrectly. 
117
transliterations to the unknown Hebrew forms 
found in our target corpus. After creating a list of 
candidate pairs (Hebrew form found in the cor-
pus and transliterated UMLS concept), we filter 
the results to create an accurate dictionary using 
various heuristic measures.  
The produced lexicon contained 2,507 trans-
literated lemmas with precision of 75%. The ac-
quired lexicon reduced segmentation errors by 
50%. 
6 Experiments 
6.1 Experimental Settings 
An SVM classifier was trained using the 200 
most common nouns as features. The noun lem-
mas were extracted with the morphological dis-
ambiguator in two settings: na?ve setting using 
the newswire lexicon and an adapted setting us-
ing the acquired lexicon.  
We divided the corpus into training and testing 
sets of equal size, we report on the average re-
sults or 10 different divisions of the data. 
6.2 Results 
The classifier using the baseline lexicon achieved 
an average F-Score of 83.6%. With the extended 
in-domain transliterations lexicon the classifier 
achieves F-Score of 87%, an error reduction of 
20%. 
We repeated the experiment with decision 
trees for visualization for error analysis. With 
decision trees we see an improvement from 
76.8% to 82.6% F-score. In Figure 1, we see in 
the resulting decision tree the most commonly 
prescribed medication for epilepsy patients, So-
dium Valproate ?depalept? (???????). This word 
appears in three forms: ?depalept?, ?b+deplapet? 
and ?h+depalept?. The acquired lexicon allows 
better segmentation of this word thus removing 
noise for documents containing the agglutinated 
forms. 
7 Conclusions 
We presented the task of classifying patients? 
Hebrew free text EHR for inclusion/exclusion 
from a prospective study. Transliterated tokens 
are an important feature in medical texts. In lan-
guages with compound tokens this is likely to 
lead to segmentation errors. 
Using a lexicon adapted for the domain im-
pacts the number of segmentation errors, this 
error reduction translates into further improve-
ments when using these data for down the line 
applications such as classification. 
Creating domain adaptation methods for re-
source-poor languages can positively impact the 
use of clinical records in these languages. 
 
 
Acknowledgments 
 
 
Adler, M. and M. Elhadad (2006). An 
unsupervised morpheme-based hmm for 
hebrew morphological disambiguation. 
Proceedings of the 21st International 
Conference on Computational Linguistics 
and the 44th annual meeting of the 
Association for Computational 
Linguistics, Association for 
Computational Linguistics. 
Albright, D., A. Lanfranchi, et al (2013). 
"Towards comprehensive syntactic and 
semantic annotations of the clinical 
narrative." Journal of the American 
Medical Informatics Association. 
Aronson, A. R. and F. M. Lang (2010). "An 
overview of MetaMap: historical 
perspective and recent advances." Journal 
of the American Medical Informatics 
Association 17(3): 229-236. 
Bodenreider, O. (2004). "The unified medical 
language system (UMLS): integrating 
biomedical terminology." Nucleic Acids 
Research 32(Database Issue): D267. 
Hirtz, D., D. Thurman, et al (2007). "How 
FRPPRQ DUH WKH ?FRPPRQ? QHXURORJLF
disorders?" Neurology 68(5): 326-337. 
Kirschenbaum, A. and S. Wintner (2009). Lightly 
supervised transliteration for machine 
translation. Proceedings of the 12th 
Conference of the European Chapter of 
the Association for Computational 
Linguistics, Association for 
Computational Linguistics. 
Restificar, A., I. Korkontzelos, et al (2013). "A 
method for discovering and inferring 
appropriate eligibility criteria in clinical 
trial protocols without labeled data." 
BMC Medical Informatics and Decision 
Making 13(Suppl 1): S6. 
Savova, G. K., J. J. Masanz, et al (2010). "Mayo 
clinical Text Analysis and Knowledge 
Extraction System (cTAKES): 
architecture, component evaluation and 
applications." Journal of the American 
Medical Informatics Association 17(5): 
507-513. 
Tateisi, Y., Y. Tsuruoka, et al (2006). Subdomain 
adaptation of a POS tagger with a small 
corpus. Proceedings of the Workshop on 
118
Linking Natural Language Processing and 
Biology: Towards Deeper Biological 
Literature Analysis, Association for 
Computational Linguistics. 
Tsuruoka, Y., Y. Tateishi, et al (2005). 
"Developing a robust part-of-speech 
tagger for biomedical text." Advances in 
informatics: 382-392. 
 
 
119
