115
116
117
118
119
120
121
122
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 668?675, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Analyzing models for semantic role assignment using confusability
Katrin Erk and Sebastian Pad?
Computational Linguistics
Saarland University
Saarbr?cken, Germany
{erk,pado}@coli.uni-sb.de
Abstract
We analyze models for semantic role
assignment by defining a meta-model
that abstracts over features and learning
paradigms. This meta-model is based on
the concept of role confusability, is de-
fined in information-theoretic terms, and
predicts that roles realized by less specific
grammatical functions are more difficult
to assign. We find that confusability is
strongly correlated with the performance
of classifiers based on syntactic features,
but not for classifiers including semantic
features. This indicates that syntactic fea-
tures approximate a description of gram-
matical functions, and that semantic fea-
tures provide an independent second view
on the data.
1 Introduction
Semantic roles have become a focus of research in
computational linguistics during the recent years.
The driving force behind this interest is the prospect
that semantic roles, as a shallow meaning represen-
tation, can improve many NLP applications, while
still being amenable to automatic analysis. The
benefit of semantic roles has already been demon-
strated for a number of tasks, among others for ma-
chine translation (Boas, 2002), information extrac-
tion (Surdeanu et al, 2003), and question answer-
ing (Narayanan and Harabagiu, 2004).
Robust and accurate automatic semantic role as-
signment, a prerequisite for the wide-range use of
semantic roles in NLP, has been investigated in a
number of studies and shared tasks. Typically, role
assignment has been modeled as a classification
task, with models being estimated from large cor-
pora (Gildea and Jurafsky, 2002; Moschitti, 2004;
Xue and Palmer, 2004; Surdeanu et al, 2003; Prad-
han et al, 2004; Litkowski, 2004; Carreras and
M?rquez, 2005).
Within this framework, there is a number of archi-
tectural parameters which lend themselves to opti-
mization: the machine learning framework, the fea-
ture set, pre- and postprocessing, each of which has
been investigated in the context of semantic role as-
signment. The current paper concentrates on feature
engineering, since the feature set is a pivotal com-
ponent of any kind of machine learning system, and
allows us to incorporate and test linguistic intuitions
on the role assignment task.
We approach feature engineering not by directly
optimizing system performance. Instead, we pro-
ceed by error analysis, like Pado and Boleda (2004).
Our aim is to form a global hypothesis that explains
the distribution of errors across classes. Insofar as
the model does not contain model-specific infor-
mation, following this methodology can provide a
meta-model of a model family which abstracts over
concrete features and over the learning paradigm.
The concrete global hypothesis we test is: (1) All
features of current models approximate a descrip-
tion of grammatical functions, and the complete sys-
tems approximate an assignment based on grammat-
ical functions. (2) System performance for a given
role depends on how easily it is confused with other
roles. We will give this concept of role confusability
a formal, information-theoretic definition.
The present study specifically analyzes mod-
els for semantic role assignment in the FrameNet
668
paradigm (Fillmore et al, 2003). We are going to
show that our hypothesis indeed holds for a variety
of models ? but only models that comprise exclu-
sively syntactic features. We conclude that syntactic
features approximate a description of grammatical
functions, but that semantic features model a dif-
ferent aspect of the role assignment mapping. To-
gether with the reasonable performance of a solely
semantics-based system, this leads us to suggest a
closer investigation of semantic features ? and in
particular, a co-training approach with syntactic and
semantic features as different views on the role as-
signment data.
Plan of the paper. In Section 2, we give a
brief introduction to FrameNet, the semantic role
paradigm and corpus we are using in this study. Our
first experiment, described in Section 3, establishes
that there is a high variance in performance across
roles, and that this variance is itself stable across
models and learners. In Section 4, we state our hy-
pothesis, namely that this variance can be explained
through role confusability, and formalize the con-
cept . In Section 5, we perform detailed correlation
tests to verify our hypothesis and discuss our find-
ings. Section 6 concludes the paper.
2 FrameNet
This section presents the semantic role paradigm and
the role-annotated corpus on which the present study
is based. FrameNet1 is a lexical resource based on
Fillmore?s Frame Semantics (Fillmore, 1985). It de-
scribes frames, representations of prototypical situa-
tions. Each frame provides its set of semantic roles,
the entities or concepts pertaining to the prototypi-
cal situation. Each frame is further associated with a
set of target predicates (nouns, verbs or adjectives),
occurrences of which can introduce the frame.
FrameNet provides manually annotated examples
for each predicate, sampled from the British Na-
tional Corpus (Burnard, 1995). The size of this cor-
pus exceeds 135,000 sentences. The following sen-
tences are examples for verbs in the IMPACT frame,
which describes a situation in which typically ?an
IMPACTOR makes sudden, forcible contact with the
IMPACTEE, or two IMPACTORS both ... [make]
forcible contact?:
1http://www.icsi.berkeley.edu/~framenet/
(1) [Impactee His car] was struck [Impactor by a
third vehicle].
(2) [Impactor The door] slammed [Result shut].
(3) [Impactors Their vehicles] collided [Place at
Pond Hill].
FrameNet manual annotation also comprises a layer
of grammatical functions: For example, the subject
of finite verbs is labeled Ext, and Mod is a label
used for modifiers of heads, e.g. an adjective mod-
ifying a noun. The grammatical functions used in
FrameNet are listed in Fillmore and Petruck (2003).
Note that the frame-specificity of semantic roles
in FrameNet has important consequences for seman-
tic role assignment, since there is no direct way
to generalize role assignments across frames, and
learning has to proceed frame-wise. This com-
pounds the data sparseness problem, and automatic
assignment for frames with no training data is very
difficult (Gildea and Jurafsky, 2002).
3 Experiment 1: Variance in role
assignment
Several studies have established that there is con-
siderable variance in semantic role assignment per-
formance across different semantic roles within sys-
tems (Carreras and M?rquez, 2004; Carreras and
M?rquez, 2005; Pado and Boleda Torrent, 2004).
However, these studies used either the PropBank
semantic role paradigm (Carreras and M?rquez)
or a limited of experimental conditions (Pado and
Boleda). For this reason, we perform a first experi-
ment to replicate this phenomenon in our setting.
Note that the vast majority of participant sys-
tems in recent shared tasks divides semantic role as-
signment into multiple sequential steps. The max-
imal decomposition is as follows: preprocessing,
e.g. removal of unlikely argument candidates; ar-
gument recognition, the distinction between role-
bearing and non-role-bearing instances; argument
labeling, the actual classification of role-bearing in-
stances; and postprocessing, e.g. by inference over
probable role sequences.
Following this distinction, we concentrate in this
study on the argument labeling step, i.e. distinguish-
ing between roles, rather than distinguishing roles
669
from non-roles. This is justified by earlier empiri-
cal results, namely that the argument labeling step
requires more training data than argument recogni-
tion (Fleischmann and Hovy, 2003), and that it calls
for more sophisticated feature construction (Xue and
Palmer, 2004). We take this as evidence that the
quality of the argument labeling step is central to a
good semantic role assignment system.
In order to isolate the effects of argument label-
ing, we assume perfect argument recognition by us-
ing gold standard role boundaries; however, we do
not use gold standard parse trees, but rather automat-
ically computed ones, which realistically introduces
some noise (see the following paragraph).
Data and preprocessing. As experimental mate-
rial, we used the same data that was used in the
Senseval-3 semantic role assignment task: 40 frames
from FrameNet version 1.1, comprising 66,777 in-
stances. The number of roles per frame ranged from
2 to 22, and the number of role instances ranged
from 593 to 8,378. The data was randomly split into
training (90%) and test instances (10%).
The data was parsed with the Collins
model 3 (1996) parser; in addition, all tokens
were lemmatized with TreeTagger (Schmid, 1994).
Modeling. We model role assignment as a clas-
sification task, with parse tree constituents as in-
stances to be classified. We repeated the classifica-
tion with two different learners: The first learner,
TiMBL (Daelemans et al, 2003) is an implementa-
tion of nearest-neighbor classification algorithms in
the memory-based learning paradigm2. The second
learner, Malouf?s probabilistic maximum entropy
(Maxent) system (Malouf, 2002), uses the LMVM
algorithm to estimate log-linear models. We did not
perform smoothing.
Table 5 shows the features we use. Here as in the
system setup, we keep close to current existing mod-
els for semantic role assignment in order to make our
results as representative as possible. We investigate
different feature sets in order to verify our results. In
Exp. 1, we limit ourselves to two feature sets, Syn
(syntactic features) and Sem (lexical features) from
the bottom of Table 5. The feature sets were exactly
the same for both learners.
2TiMBL was set to k-NN classification, using the MVDM
distance metric and 5 neighbors.
Syn/Sem Syn
MBL 87.1 ? 12.7 82.2 ? 17.8
Maxent 87.5 ? 13.4 82.4 ? 18. 2
Table 1: Exp. 1: Overall results (F-scores and stan-
dard deviation across roles).
Syn/Sem Syn
Role FMBL FMaxent FMBL FMaxent
Frame: CHANGE_POSITION_ON_A_SCALE
ATTR 79.0 80.7 57.6 66.1
CO_VAR 55.6 64.0 22.2 31.6
DIFF 87.1 84.9 75.0 66.7
ITEM 68.6 70.3 48.0 61.3
VALUE_1 88.0 91.7 78.3 72.7
VALUE_2 93.3 90.9 89.3 85.2
Frame: KINSHIP
ALTER 87.0 89.2 87.8 87.4
EGO 96.7 98.8 96.7 95.5
Frame: PART_ORIENTATIONAL
PART 98.2 96.4 97.6 97.0
WHOLE 100 100 98.2 100
Frame: TRAVEL
AREA 31.6 52.6 25.0 45.5
GOAL 74.4 71.4 68.3 62.2
MODE 46.2 72.7 12.5 15.4
PATH 66.7 53.3 50.0 40.0
SOURCE 66.7 72.7 66.7 66.7
TIME 77.8 66.7 15.4 40.0
TRAVELER 90.9 90.6 90.9 90.6
Table 2: Exp. 1: Role-specific figures of system per-
formance for four example frames.
Results. Table 1 shows the systems? overall F-
scores and standard deviation across roles. Table 2
illustrates the differences in performance across
roles on four frames: It lists all roles with ? 5 oc-
currences for each frame. PART_ORIENTATIONAL
shows very little variance, while the roles of
CHANGE_POSITION_ON_A_SCALE and especially
TRAVEL differ widely. For KINSHIP, the system
shows good performance for both roles, but the F-
scores still differ by around 9 points.
Discussion. Table 1 shows that there is consider-
able variance across roles, with a standard devia-
tion in the range of 18% for the syntax-only model.
We note that the deviation decreases to 13% for the
combined syntax-semantics model. Table 2 con-
firms that this is not purely between-frames, but
also within-frames variance. This confirms the phe-
nomenon described at the beginning of this section.
670
fr frame
fe role (frame element)
fes(fr) roles of a frame
gfs(fr) gramm. functions of a frame
gfsfr (fe) gramm. functions realizing a role in
a frame
Table 3: Notation summary
4 A meta-model for role assignment:
Confusability
The experiment of the previous section has shown a
considerable variance in system performance across
roles. The aim of this section is to develop a meta-
model which can explain this variance.
The models we have explored in Exp. 1 rely
mainly on syntactic features: Even in the combined
syntax-semantics model, 24 of the 31 features de-
scribe syntactic structure. This predominance of
syntactic features can be observed in many current
models for semantic role assignment. Accordingly,
our meta-model focuses on the uniformity of the
mapping from syntactic structure to semantic roles.
We formalize the variance in this mapping by the
confusability of a semantic role. It implements the
following hypothesis:
(1) The semantic role assignment systems we study
approximate role assignment through gram-
matical functions.
(2) System performance for a given role depends on
the role?s confusability: A role is highly con-
fusable if the grammatical functions that in-
stantiate it often also instantiate other roles.
By using the ideal, manually assigned grammat-
ical functions that are available from the FrameNet
data ? and which are not passed on to the learner ?
our meta-model abstracts over concrete feature sets.
Our definition of confusability proceeds in two
steps. First we model the informativity of a gram-
matical function by the entropy of semantic roles
that it maps to. Then we compute the confusabil-
ity of a role as a weighted average of the entropies
of the grammatical functions that realize it.
Grammatical function entropy. Viewing a gram-
matical function as a random variable with semantic
Grammatical function entropy
GF DEG THM DEP LOC H
Mod 69 43 24 0 1.46
Comp 18 491 12 41 0.72
Ext 0 17 0 561 0.16
Head 0 0 0 273 0.0
Obj 0 0 0 3 0.0
Role Confusability
Role Mod Comp Ext Head Obj Conf
DEG 69 18 0 0 0 1.31
THM 43 491 17 0 0 0.76
DEP 24 12 0 0 0 1.22
LOC 0 41 561 273 3 0.16
Table 4: Grammatical function entropy and role con-
fusability for the frame ABUNDANCE
roles as values, we define the entropy of a grammat-
ical function gf within the frame fr as
Hfr (gf ) =
?
fe?fes(fr)
?p(fe|gf ) log p(fe|gf )
where p(fe|gf ) = f(gf ,fe)f(gf ) is the conditional proba-
bility of roles fe given gf (cf. the notation in Table 3).
Role confusability. The confusability of a role
is the sum of its grammatical function entropies,
weighted by the conditional probabilities p(gf |fe) =
f(gf ,fe)
f(fe) of grammatical functions gf given fe.
cfr (fe) =
?
gf ?gfs(fr)
p(gf |fe)Hfr (gf )
An example. Table 4 shows the grammatical func-
tion entropies and role confusabilities for the frame
ABUNDANCE, both computed on the training data.
The upper part of Table 4 lists the entropies of
the grammatical functions Mod, Comp, Ext,
Head and Obj3 and the counts f(gf, fe) of occur-
rences of the grammatical functions together with
the roles DEGREE (DEG), THEME (THM), DEPIC-
TIVE (DEP) and LOCATION (LOC). The entropy of
Mod, with similar numbers of occurrences for three
different roles, is relatively high, while Ext occurs
almost exclusively for one role and has a much lower
entropy. The lower part of Table 4 shows the confus-
ability for the same set of roles. The confusability of
3See Fillmore and Petruck (2003) for a glossary of
FrameNet?s grammatical functions.
671
DEGREE is relatively high even though it is mostly
realized by Mod because Mod has a high entropy, i.e.
it indicates multiple roles; LOCATION on the other
hand is not very confusable even though it occurs
frequently as both Ext and Head, since both gram-
matical functions indicate this role.
Related work. Our approach is similar to Pado
and Boleda (2004) in that they also use the unifor-
mity of linking as an explanation for performance
variations in semantic role assignment. However,
their analysis is located at the frame level. We ex-
amine individual roles, which allows us to derive a
simpler and more intuitive formalization of linking
uniformity. Also, our model will ultimately lead us
to a different conclusion: the uniformity of linking
is a good predictor of the performance of role as-
signment systems, but only for exclusively syntactic
models (see Section 5).
5 Experiment 2: Relating confusability
and system performance
In this section, we test the validity of our meta-
model. We assess whether confusability, defined in
Section 4, can explain the variance in role assign-
ment that we have found in Section 3, by testing the
correlation between the two variables.
Experimental setup. We use the same data set
(Senseval-3) and the same two classifiers (memory-
based and maximum entropy classification) as in
Exp. 1. To cover a wider range of models and thus
increase the validity of our analysis, we split up the
Syn feature set from Exp. 1 into the four smaller
sets described in the upper part of Table 5. We use
these sets individually, combined, and together with
the lexical features in the Sem set. This results in a
total of 20 different models (10 for each classifier),
for which we computed role-specific F-scores.
In parallel, we estimated the confusability as de-
scribed in Section 4, with FrameNet?s manually as-
signed grammatical functions as a basis, using only
the training portion of our data. We did not smooth,
but omitted roles occurring less than 5 times to
avoid sparse and thus unreliable data points. Re-
call that confusability does not vary with the feature
set, since its central asset is to abstract over concrete
model parameters and feature sets.
Feature set FMBL FMaxent
Path0 70.9 71.3
Path 73.3 72.6
Pt 78.8 79.0
Path/Pt 80.8 79.8
Path/Sibling 76.7 76.6
Pt/Sibling 78.8 79.1
Syn 82.2 82.4
Sem 80.3 80.7
Syn/Sem 87.1 87.5
Table 6: Exp. 2: Results for different feature sets
Results. The F-scores for the subdivided Syn fea-
ture set are shown in the upper part of Table 6, with
the complete Syn and Sem sets and their combina-
tion below. There is a clear relationship between
features and F-score: additional features are consis-
tently rewarded with higher performance. Interest-
ingly, phrase type information appears to be a better
role predictor than path (compare models Path and
Pt). Also, the semantic feature set alne (Sem) per-
forms at over 80% F-Score, slightly better any of the
individual syntactic feature groups.
The high F-score variance between individual
roles which we have shown for the feature sets Syn
and Syn/Sem in Exp. 1 generalizes to the other fea-
ture sets; all individual syntactic feature sets exhibit
a higher variance than Syn, and Sem shows a higher
variance than the Syn/Sem combination. This does
not come as a surprise, since the two models of
Exp. 1 use the two richest feature sets, and we would
expect less robust behavior for weaker models. An-
other point to note is that the performance of the two
learners is remarkably similar.
The high variance in the F-scores is mirrored in
the confusability figures; we obtain an average con-
fusability for our semantic roles of 1.79 with a high
standard deviation of 0.84. A scatter plot of F-scores
against confusability figures (Fig.1) suggests a linear
correlation analysis.
Analysis 1: Correlating confusability and F-
score. Since the data does not appear to be nor-
mally distributed, we apply Kendall?s nonparamet-
ric rank test. The results, which are listed in Table 7,
show an extremely significant negative correlation
between confusability and F-score: higher confus-
672
Path0 These are features centered around the path from the target lemma to the constituent: the path
itself, its length, partial path up to the lowest common ancestor, the grammatical rule that
expands the target predicate?s parent, relative position of constituent to target
Path Feature set Path, plus target lemma
Pt These are features related to phrase type and part of speech: the phrase type of the constituent
and its parent, the POS of the constituent first word, last word and head as well as the POS of
an informative content word of the constituent (for PP and SBar constituents only: the head of
the head?s complement), as well as the target lemma
Sibling Phrase type and POS of the head of the left and right sibling constituent, and the Collins parser?s
judgment on the argumenthood of the constituent
Syn This set combines Path, Sibling and Pt. Additional features are: target voice; the constituent?s
preposition; a feature combining path with target voice and target POS; and two rule-based
features judging argumenthood and grammatical function of the constituent
Sem These are lexical features: Head words of the constituent and of its left and right siblings;
leftmost and rightmost word of the constituent; informative content word lemma (see set Pt for
details); and the governing verb of the target predicate
Table 5: Feature groups used in the experiments
Figure 1: Scatter plot: F-score against confusability
(Feature set Syn).
ability appears to be related to lower F-score.
However, note that the correlation is extremely
significant even for the model which only uses se-
mantic features. This is unexpected at best and
makes a strong interpretation of this correlation
doubtful: it is rather likely that there is a third vari-
able with which both F-score and confusability are
correlated. The most obvious candidate for such a
confounding variable is the size of the training set ?
clearly, we expect our models to perform better with
larger training sets. In order to get a more realistic
MBL MaxEnt
Feature set z p z p
Path0 -11.72 10?15 -11.76 10?15
Path -12.29 10?15 -11.23 10?15
Pt -10.64 10?15 -11.12 10?15
Path/Pt -11.19 10?15 -10.45 10?15
Path/Sibling -12.65 10?15 -11.76 10?15
Pt/Sibling -10.58 10?15 -9.90 10?15
Syn -9.47 10?15 -9.38 10?15
Sem -6.90 10?11 -8.23 10?15
Syn/Sem -8.30 10?15 -8.29 10?15
Table 7: Exp. 2, Analysis 1: Correlation between F-
Score and confusability. z: Kendall?s tau coefficient,
p: significance level
assessment of the relationship between confusabil-
ity and F-score, we perform an additional analysis
to disconfound confusability and frequency.
Analysis 2: Disconfounding confusability and
frequency. One way of factoring out the influ-
ence of a confounding variable is to perform a par-
tial correlation analysis, which explicitly removes
the effects of a third variable when determining the
strength of a correlation between two variables. Like
a normal correlation analysis, it yields a partial cor-
relation coefficient.
673
MBL MaxEnt
Features rc rf rc rf
Path0 -.29??? -.03 -.29??? -.03
Path -.30??? -.02 -.27??? -.07??
Pt -.19??? -.11?? -.21??? -.12??
Path/Pt -.22??? -.07? -.19??? -.16???
Path/Sibl -.31??? +.01 -.28??? -.06?
Pt/Sibl -.20??? -.10?? -.18??? -.16???
Syn -.10? -.17??? -.12? -.19???
Sem +.01 -.27??? -.02 -.24???
Syn/Sem +.02 -.25??? -.01 -.25???
Table 8: Exp. 2, Analysis 2: Partial correlation
coefficients. rc: correlation between F-score and
confusability, controlling for training set size. rf :
correlation between F-score and training set size,
controlling for confusability. Significance levels:
???: p<0.001; ??: p<0.01; ?: p<0.05.
We first compute partial correlation coefficients
between F-score and confusability, controlling for
training set size. The results, which indicate the
?true? relationship between performance and con-
fusability, are shown in the rc columns of Table 8.
For both learners, confusability is significantly cor-
related with F-score for all syntactic feature sets, but
not for the semantic feature set and for the combined
set Syn/Sem.
We also compute the partial correlation coeffi-
cients between F-score and training set size, control-
ling for confusability. These figures are reported in
the rf columns of Table 8 and show the ?true? rela-
tionship between performance and training set size.
There is no significant correlation between training
set size and performance for simple syntax based-
models, but the correlation is highly significant for
complex syntactic models and all semantic models.
Discussion. The partial correlation analysis con-
firms that confusability is a meta-model that can ex-
plain the performance of a range of different models
for semantic role assignment, namely those models
which rely exclusively on syntactic features. Since
we used the gold standard features provided by
FrameNet and did not introduce implementation- or
feature-specific knowledge, this points to a general
limitation of syntax-based models. In contrast, se-
mantic features behave completely differently; their
contribution is not limited by a role?s confusabil-
ity. At the very least, it cannot be captured by
our current meta-model, but the absolute increase in
performance indicates that integrating semantics is
the way forward, which is surprising given that the
purely lexical features we use the present study are
usually extremely sparse.
The analysis of the partial correlation between F-
score and training set size also allows interesting
conclusions. The correlation is not significant for
small syntactic feature sets like Path, indicating that
models for such features can be learned satisfacto-
rily from relatively small training sets (but which are
also limited in expressivity). This is markedly dif-
ferent for richer feature sets. Arguably, these feature
sets are sparser and can therefore profit more from
an increased amount of training data. Again, the ef-
fect is most pronounced for the semantic feature set.
6 Conclusion
In this paper, we have formulated a meta-model for
semantic role assignment. We have used the confus-
ability of roles to predict classification performance
independently of the classification framework and
feature sets used. We have defined role confusability
in two steps: First, we have formalized the certainty
with which we can predict a semantic role from a
given grammatical function with grammatical func-
tion entropy. Then, we have defined the confusabil-
ity of a role as a weighted sum of grammatical func-
tion entropies.
We have found that role confusability is highly
significantly correlated with system performance for
models based solely on syntactic features. We con-
clude that syntactic features approximate a descrip-
tion of grammatical functions, but that semantic fea-
tures model a different aspect of the world.
Much of current research in semantic role assign-
ment is centered on the refinement of syntactic fea-
tures. Our study suggests that it may be worth-
while to explore the refinement of semantic fea-
tures as well. The most obvious choice is to in-
vestigate features related to selectional preferences.
Possible features include goodness of fit relative to
pre-computed preferences (Baldewein et al, 2004),
named entities (Pradhan et al, 2004), or broad on-
tological classes like ?animate? or ?artifact?. Fol-
674
lowing up on this idea, a natural continuation of the
present study would be to create a meta-model that
subsumes semantic features. Such a model could
use optimal selectional restrictions as a predictor.
The next step would then be to construct a combined
meta-model that describes the behavior of systems
with both syntactic and semantic features.
Another interesting research direction that our
study suggests is the combination of syntactic and
semantic models in co-training. Co-training can
be sensibly applied only when conditional indepen-
dence holds for the two target functions and the dis-
tribution (Blum and Mitchell, 1998), i.e. when it
uses two independent views on the instance set. By
pointing out a highly significant distinction between
syntactic and semantic features with respect to role
confusability, our study provides empirical evidence
that syntactic and semantic features model different
aspects of the role assignment mapping, and that co-
training may be feasible by using syntactic and se-
mantic features as views.
Acknowledgments. We are grateful to the
Deutsche Forschungsgemeinschaft (DFG) for
funding the SALSA-II project (grant PI-154/9-2).
References
U. Baldewein, K. Erk, S. Pado, D. Prescher. 2004. Se-
mantic role labelling with similarity-based generali-
sation using em-based clustering. In Proceedings of
SENSEVAL-3.
A. Blum, T. Mitchell. 1998. Combining labeled and un-
labeled data with co-training. In COLT: Proceedings
of the Workshop on Computational Learning Theory,
Morgan Kaufmann Publishers.
H. C. Boas. 2002. Bilingual framenet dictionaries for
machine translation. In Proceedings of LREC 2002,
1364?1371, Las Palmas, Canary Islands.
L. Burnard, 1995. User?s guide for the British National
Corpus. British National Corpus Consortium, Oxford
University Computing Services, 1995.
X. Carreras, L. M?rquez. 2004. Introduction to the
CoNLL-2004 shared task: semantic role labeling. In
Proceedings of CoNLL 2004, Boston, MA.
X. Carreras, L. M?rquez. 2005. Introduction to the
CoNLL-2005 shared task: semantic role labeling. In
Proceedings of CoNLL 2005, Ann Arbor, MI.
M. J. Collins. 1996. A new statistical parser based on
bigram lexical dependencies. In A. Joshi, M. Palmer,
eds., Proceedings of the Thirty-Fourth Annual Meeting
of the Association for Computational Linguistics, 184?
191, San Francisco. Morgan Kaufmann Publishers.
W. Daelemans, J. Zavrel, K. van der Sloot, A. van den
Bosch. 2003. Timbl: Tilburg memory based
learner, version 5.0, reference guide. Technical Re-
port ILK 03-10, Tilburg University, 2003. Available
from http://ilk.uvt.nl/downloads/pub/
papers/ilk0310.ps.gz.
C. J. Fillmore, M. R. Petruck. 2003. FrameNet glossary.
International Journal of Lexicography, 16:359?361.
C. J. Fillmore, C. R. Johnson, M. R. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16:235?250.
C. J. Fillmore. 1985. Frames and the semantics of under-
standing. Quaderni di Semantica, IV(2).
M. Fleischmann, E. Hovy. 2003. A maximum en-
tropy approach to framenet tagging. In Proceedings
of HLT/NAACL 2003, Edmonton, Canada.
D. Gildea, D. Jurafsky. 2002. Automatic labeling of se-
mantic roles. Computational Linguistics, 28(3):245?
288.
K. Litkowski. 2004. Senseval-3 task: Automatic label-
ing of semantic roles. In R. Mihalcea, P. Edmonds,
eds., Proceedings of Senseval-3: The Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, Barcelona, Spain.
R. Malouf. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proceedings of
CoNLL 2002, Taipei, Taiwan.
A. Moschitti. 2004. A study on convolution kernel for
shallow semantic parsing. In Proceedings of the ACL
2004, Barcelona, Spain.
S. Narayanan, S. Harabagiu. 2004. Question answering
based on semantic structures. In Proceedings of COL-
ING 2004, Geneva, Switzerland.
S. Pado, G. Boleda Torrent. 2004. The influence of ar-
gument structure on semantic role assignment. In Pro-
ceedings of EMNLP 2004, Barcelona, Spain.
S. Pradhan, W. Ward, K. Hacioglu, J. H. Martin, D. Ju-
rafsky. 2004. Shallow semantic parsing using sup-
port vector machines. In Proceedings of HLT/NAACL
2004, Boston, MA.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of NeMLaP 1994.
M. Surdeanu, S. Harabagiu, J. Williams, P. Aarseth.
2003. Using predicate-argument structures for infor-
mation extraction. In Proceedings of ACL 2003, Sap-
poro, Japan.
N. Xue, M. Palmer. 2004. Calibrating features for se-
mantic role labeling. In Proceedings of EMNLP 2004,
Barcelona, Spain.
675
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 128?135,
New York, June 2006. c?2006 Association for Computational Linguistics
Unknown word sense detection as outlier detection
Katrin Erk
Computational Linguistics
Saarland University
Saarbru?cken, Germany
erk@coli.uni-sb.de
Abstract
We address the problem of unknown word
sense detection: the identification of cor-
pus occurrences that are not covered by
a given sense inventory. We model this
as an instance of outlier detection, using
a simple nearest neighbor-based approach
to measuring the resemblance of a new
item to a training set. In combination with
a method that alleviates data sparseness by
sharing training data across lemmas, the
approach achieves a precision of 0.77 and
recall of 0.82.
1 Introduction
If a system has seen only positive examples, how
does it recognize a negative example? This is
the problem addressed by outlier detection, also
called novelty detection1 (Markou and Singh, 2003a;
Markou and Singh, 2003b; Marsland, 2003): to de-
tect novel or unknown items that differ from all the
seen training data. Outlier detection approaches typ-
ically derive some model of ?normal? objects from
the training set and use a distance measure and a
threshold to detect abnormal items.
In this paper, we apply outlier detection tech-
niques to the task of unknown sense detection: the
identification of corpus occurrences that are not cov-
ered by a given sense inventory. The training set
1The term novelty detection is also used for the distinction
of novel and repeated information in information retrieval, a
different if related topic.
Figure 1: Wrong assignment due to missing sense:
from the Hound of the Baskervilles, Ch. 14
against which new occurrences are compared will
consist of sense-annotated text.
Unknown sense detection is related to word sense
disambiguation (WSD) and to word sense discrim-
ination (Schu?tze, 1998), but differs from both. In
WSD all senses are assumed known, and the task is
to select one of them, while in unknown sense detec-
tion the task is to decide whether a given occurrence
matches any of the known senses or none of them,
and all training instances, regardless of the sense to
which they belong, are modeled as one group of
known data. Unknown sense detection also differs
from word sense discrimination, where no sense in-
ventory is given and the task is to group occurrences
into senses. In unknown sense detection the model
respects the given word senses.
The main motivation for this study comes from
shallow semantic parsing, by which we mean a com-
bination of WSD and the automatic assignment of
128
semantic roles to free text. In cases where a sense
is missing from the inventory, WSD will wrongly
assign one of the existing senses. Figure 1 shows
an example, a sentence from the Hound of the
Baskervilles, analyzed by the SHALMANESER (Erk
and Pado, 2006) shallow semantic parser. The anal-
ysis is based on FrameNet (Baker et al, 1998), a
resource that lists senses and semantic roles for En-
glish expressions. FrameNet is lacking a sense of
?expectation? or ?being mentally prepared? for the
verb prepare, so prepared has been assigned the
sense COOKING CREATION, a possible but improb-
able analysis2. Such erroneous labels can be fa-
tal when further processing builds on the results of
shallow semantic parsing, e.g. for drawing infer-
ences. Unknown sense detection can prevent such
mistakes.
All sense inventories face the problem of missing
senses, either because of their small overall size (as
is the case for some non-English WordNets) or when
they encounter domain-specific senses. Our study
will be evaluated on FrameNet because of our main
aim of improving shallow semantic parsing, but the
method we propose is applicable to any sense inven-
tory that has annotated data; in particular, it is also
applicable to WordNet.
In this paper we model unknown sense detec-
tion as outlier detection, using a simple Nearest
Neighbor-based method (Tax and Duin, 2000) that
compares the local probability density at each test
item with that of its nearest training item.
To our knowledge, there exists no other approach
to date to the problem of detecting unknown senses.
There are, however, approaches to the complemen-
tary problem of determining the closest known sense
for unknown words (Widdows, 2003; Curran, 2005;
Burchardt et al, 2005), which can be viewed as the
logical next step after unknown sense detection.
Plan of the paper. After a brief sketch of
FrameNet in Section 2, we describe the experimen-
tal setup used throughout this paper in Section 3.
Section 4 tests whether a very simple model suffices
for detecting unknown senses: a threshold on confi-
dence scores returned by the SHALMANESER WSD
2Unfortunately, the semantic roles have been mis-assigned
by the system. The word I should fill the FOOD role, while for
a hound could be assigned the optional RECEIVER role.
system. The result is that recall is much too low.
Section 5 introduces the NN-based outlier detection
approach that we use in section 6 for unknown sense
detection, with better results than in the first experi-
ment but still low recall. Section 7 repeats the exper-
iment of section 6 with added training data, making
use of the fact that one semantic class in FrameNet
typically pertains to several lemmas and achieving a
marked improvement in results.
2 FrameNet
Frame Semantics (Fillmore, 1982) models the mean-
ings of a word or expression by reference to
frames which describe the background and situa-
tional knowledge necessary for understanding what
the predicate is ?about?. Each frame provides its
specific set of semantic roles.
The Berkeley FrameNet project (Baker et al,
1998) is building a semantic lexicon for English de-
scribing the frames and linking them to the words
and expressions that can evoke them. These can
be verbs as well as nouns, adjectives, preposi-
tions, adverbs, and multiword expressions. Frames
are linked by IS-A and other relations. Currently,
FrameNet contains 609 frames with 8,755 lemma-
frame pairs, of which 5,308 are exemplified in an-
notated sentences from the British National Corpus.
The annotation comprises 133,846 sentences.
As FrameNet is a growing resource, many lem-
mas are still lacking senses, and many senses are still
lacking annotation. This is problematic for the use
of FrameNet analyses as a basis for inferences over
text, as e.g. in Tatu and Moldovan (2005).
For example, the verb prepare from Figure 1 is
associated with the frames
COOKING CREATION: prepare food
ACTIVITY PREPARE: get ready for an activity
ACTIVITY READY STATE: be ready for an activity
WILLINGNESS: be willing
of which only the COOKING CREATION sense has
been annotated. The sense in Figure 1 is not cov-
ered yet: ACTIVITY READY STATE would be more
appropriate than COOKING CREATION, but still not
optimal, since the sentence refers to a mental state
rather than the preparation of an activity.
129
3 Experimental setup and data
Experimental setup. To evaluate an unknown
sense detection system, we need occurrences that are
guaranteed not to belong to any of the seen senses.
To that end we use sense-annotated data, in our case
the FrameNet annotated sentences, simulating un-
known senses by designating one sense of each am-
biguous lemma as unknown. All occurrences of that
sense are placed in the test set, while occurrences
of all other senses are split randomly between train-
ing and test set, using 5-fold cross-validation. We
repeat the experiment with each of the senses of an
ambiguous lemma playing the part of the unknown
sense once. Viewing each cross-validation run for
each unknown sense as a separate experiment, we
then report precision and recall averaged over un-
known senses and cross-validation runs.
It may seem questionable that in this experimen-
tal setup, the unknown sense occurrences of each
lemma all belong to the same sense. However, this
does not bias the experiment since none of the mod-
els we study take advantage of the shape of the test
set in any way. Rather, each test item is classified in-
dividually, without recourse to the other test items.
Data. All experiments in this paper were per-
formed on the FrameNet 1.2 annotated data per-
taining to ambiguous lemmas. After removal of
instances that were annotated with more than one
sense, we obtain 26,496 annotated sentences for the
1,031 ambiguous lemmas. They were parsed with
Minipar (Lin, 1993); named entities were computed
using Heart of Gold (Callmeier et al, 2004).
4 Experiment 1: WSD confidence scores
for unknown sense detection
In this section we test a very simple model of un-
known sense detection: Classifiers often return a
confidence score along with the assigned label. We
will try to detect unknown senses by a threshold
on confidence scores, declaring anything below the
threshold as unknown. Note that this method can
only be applied to lemmas that have more than one
sense, since for single-sense lemmas the system will
always return the maximum confidence score.
Data. While the approach that we follow in this
section is applicable to all lemmas with at least two
her and upwards
She
She
wave
hand outwards
s subj obj mod
gen punc conj
(1): subj, obj, mod (since s and subj corefer,
we use only one of them)
(2): she, hand, outwards
(3): subj-she, obj-hand, mod-outwards
(4): mod-obj-subj
Figure 2: Sample Minipar parse and extracted gram-
matical function features
senses, we need lemmas with at least three senses
to evaluate it: One of the senses of each lemma is
treated as unknown, which for lemmas with three or
more senses leaves at least two senses for the train-
ing set. This reduces our data set to 125 lemmas
with 7,435 annotated sentences.
Modeling. We test whether the WSD system built
into SHALMANESER (Erk, 2005) can distinguish
known sense items from unknown sense items reli-
ably by its confidence scores. The system extracts
a rich feature set, which forms the basis of all three
experiments in this paper:
? a bag-of-words context, with a window size of
one sentence;
? bi- and trigrams centered on the target word;
? grammatical function information: for each de-
pendent of the target, (1) its function label, (2)
its headword, and (3) a combination of both are
used as features. (4) The concatenation of all
function labels constitutes another feature. For
PPs, function labels are extended by the prepo-
sition. As an example, Figure 2 shows a BNC
sentence and its grammatical function features.
? for verb targets, the target voice.
The feature set is based on Florian et al (2002) but
contains additional syntax-related features. Each
word-related feature is represented as four features
for word, lemma, part of speech, and named entity.
SHALMANESER trains one Naive Bayes classifier
per lemma to be disambiguated. For this experiment,
130
? Precision Recall
0.5 0.6524 (? 0.115) 0.0011 (? 0.0004)
0.75 0.7855 (? 0.0086) 0.0527 (? 0.0013)
0.9 0.7855 (? 0.0093) 0.1006 (? 0.0021)
0.98 0.7847 (? 0.0073) 0.1744 (? 0.0025)
Table 1: Experiment 1: Results for label unknown
sense, WSD confidence level approach. ?: confi-
dence threshold. ?: std. dev.
all system parameters were set to their default set-
tings. To detect unknown senses building on this
WSD system, we use a fixed confidence threshold
and label all items below the threshold as unknown.
Results and discussion. Table 1 shows precision
and recall for labeling instances as unknown using
different confidence thresholds ?, averaged over un-
known senses and 5-fold cross-validation3. We see
that while the precision of this method is acceptable
at 0.74 to 0.765, recall is extremely low, i.e. almost
no items were labeled unknown, even at a threshold
of 0.98. However, SHALMANESER has very high
confidence values overall: Only 14.5% of all in-
stances in this study had a confidence value of 0.98
or below (7,697 of 53,206).
We conclude that with the given WSD system and
(rather standard) features, this simple method cannot
detect items with an unknown sense reliably. This
may be due to the indiscriminately high confidence
scores; or it could indicate that classifiers, which
are geared at distinguishing between known classes
rather than detecting objects that differ from all seen
data, are not optimally suited to the task. However,
one further disadvantage of this approach is that, as
mentioned above, it can only be applied to lemmas
with more than one annotated sense. For FrameNet
1.2, this comprises only 19% of the lemmas.
5 A nearest neighbor-based method for
outlier detection
In the previous section we have tested a simple ap-
proach to unknown sense detection using WSD con-
fidence scores. Our conclusion was that it was not a
viable approach, given its low recall and given that
3Note that the minimum confidence score is 0.5 if 2 senses
are present in the training set, 0.33 for 3 present senses etc.
t t?
dtt?x dxt
Figure 3: Outlier detection by comparing distances
between nearest neighbors
it is only applicable to lemmas with more than one
known sense. In this section we introduce an al-
ternative approach, which uses distances to nearest
neighbors to detect outliers.
In general, the task of outlier detection is to de-
cide whether a new object belongs to a given training
set or not. Typically, outlier detection approaches
derive some boundary around the training set, or
they derive from the set some model of ?normal-
ity? to which new objects are compared (Markou
and Singh, 2003a; Markou and Singh, 2003b; Mars-
land, 2003). Applications of outlier detection in-
clude fault detection (Hickinbotham and Austin,
2000), hand writing deciphering (Tax and Duin,
1998; Scho?lkopf et al, 2000), and network intru-
sion detection (Yeung and Chow, 2002; Dasgupta
and Forrest, 1999). One standard approach to out-
lier detection estimates the probability density of the
training set, such that a test object can be classified
as an outlier or non-outlier according to its probabil-
ity of belonging to the set.
Rather than estimating the complete density func-
tion, Tax and Duin (2000) approximate local density
at the test object by comparing distances between
nearest neighbors. Given a test object x, the ap-
proach considers the training object t nearest to x
and compares the distance dxt between x and t to the
distance dtt? between t and its own nearest training
data neighbor t?. Then the quotient between the dis-
tances is used as an indicator of the (ab-)normality
of the test object x:
pNN (x) =
dxt
dtt?
When the distance dxt is much larger than dtt? , x is
considered an outlier. Figure 3 illustrates the idea.
The normality or abnormality of test objects is de-
cided by a fixed threshold ? on pNN . The lowest
131
threshold that makes sense is 1.0, which rejects any
x that is further apart from its nearest training neigh-
bor t than t is from its neighbor. Tax and Duin use
Euclidean distance, i.e.
dxt =
?
?
i
(xi ? ti)2
Applied to feature vectors with entries either 0 or 1,
this corresponds to the size of the symmetric differ-
ence of the two feature sets.
6 Experiment 2: NN-based outlier
detection
In this section we use the NN-based outlier detection
approach of the previous section for an experiment
in unknown sense detection. Experimental setup and
data are as described in Section 3.
Modeling. We model unknown sense detection as
an outlier detection task, using Tax and Duin?s out-
lier detection approach that we have outlined in
the previous section. Nearest neighbors (by Eu-
clidean distance) were computed using the ANN
tool (Mount and Arya, 2005). We compute one out-
lier detection model per lemma. With training and
test sets constructed as described in Section 3, the
average training set comprises 22.5 sentences.
We use the same features as in Section 4, with fea-
ture vector entries of 1 for present and 0 for absent
features. For a more detailed analysis of the contri-
bution of different feature types, we test on reduced
as well as full feature vectors:
All: full feature vectors
Cx: only bag-of-word context features (words, lem-
mas, POS, NE)
Syn: function labels of dependents
Syn-hw: Syn plus headwords of dependents
We compare the NN-based model to that of
Experiment 1, but not to any simpler baseline.
While for WSD it is possible to formulate simple
frequency-based methods that can serve as a base-
line, this is not so in unknown sense detection be-
cause the frequency of unknown senses is, by def-
inition, unknown. Furthermore, the number of an-
notated sentences per sense in FrameNet depends
Features Precision Recall
All 0.7072 (? 0.0088) 0.2683 (? 0.0043)
Cx 0.7016 (? 0.0041) 0.3511 (? 0.0035)
Syn 0.8333 (? 0.0085) 0.2099 (? 0.0042)
Syn-hw 0.7784 (? 0.0029) 0.2368 (? 0.0022)
Table 2: Experiment 2: Results for label unknown
sense, NN-based outlier detection, ? = 1.0. ?: stan-
dard deviation
Precision Recall
Features all ? 10 ? 20 all ? 10 ? 20
All 0.71 0.70 0.67 0.27 0.35 0.45
Cx 0.70 0.70 0.67 0.35 0.47 0.58
Syn 0.83 0.81 0.77 0.21 0.22 0.21
Syn-hw 0.78 0.76 0.73 0.24 0.28 0.31
Table 3: Experiment 2: Results by training set size,
? = 1.0
on the number of subcategorization frames of the
lemma rather than the frequency of the sense, which
makes frequency calculations meaningless.
Results. Table 2 shows precision and recall for la-
beling instances as unknown using a distance quo-
tient threshold of ?=1.0, averaged over unknown
senses and over 5-fold cross-validation. We see that
recall is markedly higher than in Experiment 1, es-
pecially for the two conditions that include context
words, All and Cx. The syntax-based conditions
Syn and Syn-hw show a higher precision, with a
less pronounced increase in recall.
Raising the distance quotient threshold results in
little change in precision, but a large drop in recall.
For example, All vectors with a threshold of ? =
1.1 achieve a recall of 0.14 in comparison to 0.27
for ? = 1.0 .
Training set size is an important factor in sys-
tem results. Table 3 lists precision and recall for all
training sets, for training sets of size ? 10, and for
training sets of size ? 20. Especially in conditions
All and Cx, recall rises steeply when we only con-
sider cases with larger training sets. However note
that precision does not rise with larger training sets,
rather it shows a slight decline.
Another important factor is the number of senses
that a lemma has, as the upper part of Table 7 shows.
For lemmas with a higher number of senses, preci-
132
Figure 4: ?Acceptance radius? of an outlier within
the training set (left) and a more ?normal? training
set object (right)
sion is much lower, while recall is much higher.
Discussion. While results in this experiment are
better than in Experiment 1 ? in particular recall has
risen by 19 points for Cx ?, system performance is
still not high enough to be usable in practice.
The uniformity of the training set has a large in-
fluence on performance, as Table 7 shows. The more
senses a lemma has, the harder it seems to be for the
model to identify known sense occurrences. Preci-
sion for the assignment of the unknown label drops,
while recall rises. We see a tradeoff between preci-
sion and recall, in this table as well as in Table 3.
There, we see that many more unknown test objects
are identified when training sets are larger, but a
larger training set does not translate into universally
higher results.
One possible explanation for this lies in a prop-
erty of Tax and Duin?s approach. If a training item t
is situated at distance d from its nearest neighbor in
the training set, then any test item within a radius of
d around t will be considered known. Thus we could
term d the ?acceptance radius? of t. Now if t is an
outlier within the training set, then d will be large, as
illustrated in Figure 4. The sparser the training set is,
the more training outliers we are likely to find, with
large acceptance radii that assign a label of known
even to more distanced test items. Thus a sparse
training set could lead to lower recall of unknown
sense assignment and at the same time higher pre-
cision, as the items labeled unknown would be the
ones at great distance from any items on the training
set ? conforming to the pattern in Tables 3 and 7.
7 Experiment 3: NN-based outlier
detection with added training data
While the NN-based outlier detection model we
used in the previous experiment showed better re-
Target lemma: put
Senses: ENCODING, PLACING
Sense currently treated as unknown: PLACING
Extend training set by: all annotated sentences for
lemmas other than put in the sense ENCODING:
couch.v, expression.n, formulate.v, formulation.n,
frame.v, phrase.v, word.v, wording.n
Table 4: Extending training sets: an example
Features Precision Recall
All 0.7709 (? 0.001) 0.7243 (? 0.0018)
Cx 0.7727 (? 0.0027) 0.8172 (? 0.0035)
Syn 0.8571 (? 0.0045) 0.1694 (? 0.0012)
Syn-hw 0.8025 (? 0.0041) 0.3383 (? 0.0025)
Syn 0.8587 (? 0.0081) 0.1748 (? 0.0015)
Syn-hw 0.8055 (? 0.0056) 0.3516 (? 0.0015)
Table 5: Experiment 3: Results for label unknown
sense, NN-based outlier detection, ? = 1.0. ?: stan-
dard deviation
sults than the WSD confidence model, its recall is
still low. We have suggested that data sparseness
may be responsible for the low performance. Con-
sequently, we repeat the experiment of the previous
section with more, but less specific, training data.
Like WordNet synsets, FrameNet frames are se-
mantic classes that typically comprise several lem-
mas or expressions. So, assuming that words with
similar meaning occur in similar contexts, the con-
text features for lemmas in the same frame should
be similar. Following this idea, we supplement the
training data for a lemma by all the other annotated
data for the senses that are present in the training
set, where by ?other data? we mean data with other
target lemmas. Table 4 shows an example4.
Modeling. Again, we use Tax and Duin?s outlier
detection approach for unknown sense detection.
The experimental design and evaluation are the same
as in Experiment 2, the only difference being the
training set extension. Training set extension raises
the average training set size from 22.5 to 374.
Results. Table 5 shows precision and recall for la-
beling instances as unknown, with a distance quo-
tient threshold of 1.0, averaged over unknown senses
4Conditions Syn and Syn-hw were also tested using only
other target lemmas with the same part of speech. Results were
virtually unchanged.
133
Precision Recall
Features all ? 50 ? 200 all ? 50 ? 200
All 0.77 0.77 0.73 0.72 0.80 0.87
Cx 0.77 0.77 0.73 0.82 0.89 0.94
Syn 0.86 0.85 0.82 0.17 0.16 0.13
Syn-hw 0.80 0.79 0.76 0.38 0.36 0.38
Syn 0.86 0.85 0.82 0.17 0.17 0.14
Syn-hw 0.81 0.80 0.76 0.35 0.37 0.38
Table 6: Experiment 3: Results by training set size,
? = 1.0
Number of senses
2 3 4 5
Exp. 2 Prec. 0.78 0.68 0.59 0.55
Rec. 0.21 0.38 0.47 0.59
Exp. 3 Prec. 0.83 0.71 0.63 0.56
Rec. 0.68 0.81 0.89 0.88
Table 7: Experiments 2 and 3: Results by the num-
ber of senses of a lemma, condition All, ? = 1.0
and 5-fold cross-validation. In comparison to Exper-
iment 2, precision has risen slightly, and for condi-
tions All, Cx and Syn-hw, recall has risen steeply;
the maximum recall is achieved by Cx at 0.82.
As before, increasing the distance quotient thresh-
old leads to little change in precision but a sharp
drop in recall. For All vectors, recall is 0.72 for
threshold 1.0, 0.56 for ? = 1.1, and 0.41 for ? = 1.2.
Table 6 shows system performance by training set
size. As the average training set in this experiment
is much larger than in Experiment 2, we are now
inspecting sets of minimum size 50 and 200 rather
than 10 and 20. We find the same effect as in Ex-
periment 2, with noticeably higher recall for lemmas
with larger training sets, but slightly lower precision.
Table 7 breaks down system performance by the
degree of ambiguity of a lemma. Here, too, we see
the same effect as in Experiment 2: the more senses
a lemma has, the lower the precision and the higher
the recall of unknown label assignment.
Discussion. In comparison to Experiment 2, Ex-
periment 3 shows a dramatic increase in recall, and
even some increase in precision. Precision and re-
call for conditions All and Cx are good enough for
the system to be usable in practice.
Of the four conditions, the three that involve con-
text words, All, Cx and Syn-hw, show consid-
erably higher recall than Syn. Furthermore, the
two conditions that do not involve syntactic fea-
tures, All and Cx, have markedly higher results
than Syn-hw. This could mean that syntactic fea-
tures are not as helpful as context features in detect-
ing unknown senses; however in Experiment 2 the
performance difference between Syn and the other
conditions was not by far as large as in this experi-
ment. It could also mean that frames are not as uni-
form in their syntactic structure as they are in their
context words. This seems plausible as FrameNet
frames are constructed mostly on semantic grounds,
without recourse to similarity in syntactic structure.
Table 6 points to a sparse data problem, even with
training sets extended by additional items. It also
shows that the more a test condition relies on context
word information, the more it profits from additional
data. So it may be worthwhile to explore methods
for a further alleviation of data sparseness, e.g. by
generalizing over context words.
Table 7 underscores the large influence of train-
ing set uniformity: the more senses a lemma has, the
more likely the model is to classify a test instance as
unknown. This is the case even for extended training
sets. One possible way of addressing this problem
would be to take into account more than a single
nearest neighbor in NN-based outlier detection in
order to compute more precise boundaries between
known and unknown instances.
8 Conclusion and outlook
We have defined and addressed the problem of
unknown word sense detection: the identification
of corpus occurrences that are not covered by a
given sense inventory, using a training set of sense-
annotated data as a basis. We have modeled this
problem as an instance of outlier detection, using
the simple nearest neighbor-based approach of Tax
and Duin to measure the resemblance of a new oc-
currence to the training data. In combination with
a method that alleviates data sparseness by sharing
training data across lemmas, the approach achieves
good results that make it usable in practice: With
items represented as vectors of context words (in-
cluding lemma, POS and NE), the system achieves
0.77 precision and 0.82 recall in an evaluation on
FrameNet 1.2. The training set extension method,
134
which proved crucial to our approach, relies solely
on a grouping of annotated data by semantic simi-
larity. As such, the method is applicable to any re-
source that groups words into semantic classes, for
example WordNet.
For this first study on unknown sense detection,
we have chosen a maximally simple outlier detec-
tion method; many extensions are possible. One ob-
vious possibility is the extension of Tax and Duin?s
method to more than one nearest training neigh-
bor for a more accurate estimate of local density.
Furthermore, more sophisticated feature vectors can
be employed to generalize over context words, and
other outlier detection approaches (Markou and
Singh, 2003a; Markou and Singh, 2003b; Marsland,
2003) can be tested on this task.
Our immediate goal is to use unknown sense de-
tection in combination with WSD, to filter out items
that the WSD system cannot handle due to missing
senses. Once items have been identified as unknown,
they are available for further processing: If possible
one would like to assign some measure of sense in-
formation even to these items. Possibilities include
associating items with similar existing senses (Wid-
dows, 2003; Curran, 2005; Burchardt et al, 2005) or
clustering them into approximate senses.
References
C. Baker, C. Fillmore, and J. Lowe. 1998. The Berkeley
FrameNet Project. In Proc. ACL-98, Montreal.
A. Burchardt, K. Erk, and A. Frank. 2005. A WordNet
detour to FrameNet. In Proc. GLDV 2005 Workshop
GermaNet II, Bonn.
U. Callmeier, A. Eisele, U. Scha?fer, and M. Siegel. 2004.
The DeepThought core architecture framework. In
Proc. LREC-04, Lisbon.
James Curran. 2005. Supersense tagging of unknown
nouns using semantic similarity. In Proc. ACL-05,
Ann Arbor.
D. Dasgupta and S. Forrest. 1999. Novelty detection
in time series data using ideas from immunology. In
Proc. International Conference on Intelligent Systems.
Katrin Erk and Sebastian Pado. 2006. Shalmaneser -
a toolchain for shallow semantic parsing. In Proc.
LREC-06, Genoa.
K. Erk. 2005. Frame assignment as word sense disam-
biguation. In Proc. IWCS 2005, Tilburg.
C. Fillmore. 1982. Frame Semantics. Linguistics in the
Morning Calm.
R. Florian, S. Cucerzan, C. Schafer, and D. Yarowsky.
2002. Combining classifiers for word sense disam-
biguation. Journal of Natural Language Engineering,
8(4):327?431.
S. Hickinbotham and J. Austin. 2000. Neural networks
for novelty detection in airframe strain data. In Proc.
International Joint Conference on Neural Networks.
D. Lin. 1993. Principle-based parsing without overgen-
eration. In Proc. ACL-93, Columbus, OH.
M. Markou and S. Singh. 2003a. Novelty detection:
A review. part 1: Statistical approaches. ACM Signal
Processing, 83(12):2481 ? 2497.
M. Markou and S. Singh. 2003b. Novelty detection:
A review. part 2: Neural network based approaches.
ACM Signal Processing, 83(12):2499 ? 2521.
S. Marsland. 2003. Novelty detection in learning sys-
tems. Neural computing surveys, 3:157?195.
D. Mount and S. Arya. 2005. ANN: A library for approx-
imate nearest neighbor searching. Download from
http://www.cs.umd.edu/?mount/ANN/.
B. Scho?lkopf, R. Williamson, A. Smola, J. Shawe-Taylor,
and J. Platt. 2000. Support vector method for novelty
detection. Advances in neural information processing
systems, 12.
H. Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97 ? 123.
M. Tatu and D. Moldovan. 2005. A semantic approach to
recognizing textual entailment. In Proc. HLT/EMNLP
2005, Vancouver.
D. Tax and R. Duin. 1998. Outlier detection using clas-
sifier instability. In Advances in Pattern Recognition:
the Joint IAPR International Workshops.
D. Tax and R. Duin. 2000. Data description in sub-
spaces. In International Conference on Pattern recog-
nition, volume 2, Barcelona.
Dominic Widdows. 2003. Unsupervised methods for de-
veloping taxonomies by combining syntactic and sta-
tistical information. In Proc. HLT/NAACL-03, Ed-
monton.
D. Yeung and C. Chow. 2002. Parzen-window network
intrusion detectors. In Proc. International Conference
on Pattern Recognition.
135
Underspecified Beta Reduction
Manuel Bodirsky
Katrin Erk
Joachim Niehren
Programming Systems Lab
Saarland University
D-66041 Saarbru?cken, Germany
{bodirsky|erk|niehren}@ps.uni-sb.de
Alexander Koller
Department of Computational Linguistics
Saarland University
D-66041 Saarbru?cken, Germany
koller@coli.uni-sb.de
Abstract
For ambiguous sentences, traditional
semantics construction produces large
numbers of higher-order formulas,
which must then be
 
-reduced individ-
ually. Underspecified versions can pro-
duce compact descriptions of all read-
ings, but it is not known how to perform
 
-reduction on these descriptions. We
show how to do this using
 
-reduction
constraints in the constraint language
for  -structures (CLLS).
1 Introduction
Traditional approaches to semantics construction
(Montague, 1974; Cooper, 1983) employ formu-
las of higher-order logic to derive semantic rep-
resentations compositionally; then
 
-reduction is
applied to simplify these representations. When
the input sentence is ambiguous, these approaches
require all readings to be enumerated and
 
-
reduced individually. For large numbers of read-
ings, this is both inefficient and unelegant.
Existing underspecification approaches (Reyle,
1993; van Deemter and Peters, 1996; Pinkal,
1996; Bos, 1996) provide a partial solution to this
problem. They delay the enumeration of the read-
ings and represent them all at once in a single,
compact description. An underspecification for-
malism that is particularly well suited for describ-
ing higher-order formulas is the Constraint Lan-
guage for Lambda Structures, CLLS (Egg et al,
2001; Erk et al, 2001). CLLS descriptions can
be derived compositionally and have been used
to deal with a rich class of linguistic phenomena
(Koller et al, 2000; Koller and Niehren, 2000).
They are based on dominance constraints (Mar-
cus et al, 1983; Rambow et al, 1995) and extend
them with parallelism (Erk and Niehren, 2000)
and binding constraints.
However, lifting
 
-reduction to an operation on
underspecified descriptions is not trivial, and to
our knowledge it is not known how this can be
done. Such an operation ? which we will call un-
derspecified   -reduction ? would essentially   -
reduce all described formulas at once by deriv-
ing a description of the reduced formulas. In this
paper, we show how underspecified
 
-reductions
can be performed in the framework of CLLS.
Our approach extends the work presented in
(Bodirsky et al, 2001), which defines   -reduction
constraints and shows how to obtain a complete
solution procedure by reducing them to paral-
lelism constraints in CLLS. The problem with
this previous work is that it is often necessary to
perform local disambiguations. Here we add a
new mechanism which, for a large class of de-
scriptions, permits us to perform underspecified
 
-reduction steps without disambiguating, and is
still complete for the general problem.
Plan. We start with a few examples to show
what underspecified
 
-reduction should do, and
why it is not trivial. We then introduce CLLS
and
 
-reduction constraints. In the core of the
paper we present a procedure for underspecified
 
-reduction and apply it to illustrative examples.
2 Examples
In this section, we show what underspecified
 
-
reduction should do, and why the task is nontriv-
ial. Consider first the ambiguous sentence Every
student didn?t pay attention. In first-order logic,
the two readings can be represented as
 



	
 






 





 



ffTowards a Resource for Lexical Semantics:
A Large German Corpus with Extensive Semantic Annotation
Katrin Erk and Andrea Kowalski and Sebastian Pado? and Manfred Pinkal
Department of Computational Linguistics
Saarland University
Saarbru?cken, Germany
{erk, kowalski, pado, pinkal}@coli.uni-sb.de
Abstract
We describe the ongoing construction of
a large, semantically annotated corpus
resource as reliable basis for the large-
scale acquisition of word-semantic infor-
mation, e.g. the construction of domain-
independent lexica. The backbone of the
annotation are semantic roles in the frame
semantics paradigm. We report expe-
riences and evaluate the annotated data
from the first project stage. On this ba-
sis, we discuss the problems of vagueness
and ambiguity in semantic annotation.
1 Introduction
Corpus-based methods for syntactic learning and
processing are well-established in computational
linguistics. There are comprehensive and carefully
worked-out corpus resources available for a num-
ber of languages, e.g. the Penn Treebank (Marcus et
al., 1994) for English or the NEGRA corpus (Skut
et al, 1998) for German. In semantics, the sit-
uation is different: Semantic corpus annotation is
only in its initial stages, and currently only a few,
mostly small, corpora are available. Semantic an-
notation has predominantly concentrated on word
senses, e.g. in the SENSEVAL initiative (Kilgarriff,
2001), a notable exception being the Prague Tree-
bank (Hajic?ova?, 1998) . As a consequence, most
recent work in corpus-based semantics has taken an
unsupervised approach, relying on statistical meth-
ods to extract semantic regularities from raw cor-
pora, often using information from ontologies like
WordNet (Miller et al, 1990).
Meanwhile, the lack of large, domain-
independent lexica providing word-semantic
information is one of the most serious bottlenecks
for language technology. To train tools for the
acquisition of semantic information for such lexica,
large, extensively annotated resources are necessary.
In this paper, we present current work of the
SALSA (SAarbru?cken Lexical Semantics Annota-
tion and analysis) project, whose aim is to provide
such a resource and to investigate efficient methods
for its utilisation. In the current project phase, the
focus of our research and the backbone of the an-
notation are semantic role relations. More specif-
ically, our role annotation is based on the Berke-
ley FrameNet project (Baker et al, 1998; Johnson
et al, 2002). In addition, we selectively annotate
word senses and anaphoric links. The TIGER corpus
(Brants et al, 2002), a 1.5M word German newspa-
per corpus, serves as sound syntactic basis.
Besides the sparse data problem, the most seri-
ous problem for corpus-based lexical semantics is
the lack of specificity of the data: Word meaning is
notoriously ambiguous, vague, and subject to con-
textual variance. The problem has been recognised
and discussed in connection with the SENSEVAL
task (Kilgarriff and Rosenzweig, 2000). Annotation
of frame semantic roles compounds the problem as
it combines word sense assignment with the assign-
ment of semantic roles, a task that introduces vague-
ness and ambiguity problems of its own.
The problem can be alleviated by choosing a suit-
able resource as annotation basis. FrameNet roles,
which are local to particular frames (abstract sit-
uations), may be better suited for the annotation
task than the ?classical? thematic roles concept with
a small, universal and exhaustive set of roles like
agent, patient, theme: The exact extension of the
role concepts has never been agreed upon (Fillmore,
1968). Furthermore, the more concrete frame se-
mantic roles may make the annotators? task easier.
The FrameNet database itself, however, cannot be
taken as evidence that reliable annotation is pos-
sible: The aim of the FrameNet project is essen-
tially lexicographic and its annotation not exhaus-
tive; it comprises representative examples for the use
of each frame and its frame elements in the BNC.
While the vagueness and ambiguity problem may
be mitigated by the using of a ?good? resource, it
will not disappear entirely, and an annotation format
is needed that can cope with the inherent vagueness
of word sense and semantic role assignment.
Plan of the paper. In Section 2 we briefly intro-
duce FrameNet and the TIGER corpus that we use
as a basis for semantic annotation. Section 3 gives
an overview of the aims of the SALSA project, and
Section 4 describes the annotation with frame se-
mantic roles. Section 5 evaluates the first annotation
results and the suitability of FrameNet as an anno-
tation resource, and Section 6 discusses the effects
of vagueness and ambiguity on frame semantic role
annotation. Although the current amount of anno-
tated data does not allow for definitive judgements,
we can discuss tendencies.
2 Resources
SALSA currently extends the TIGER corpus by se-
mantic role annotation, using FrameNet as a re-
source. In the following, we will give a short
overview of both resources.
FrameNet. The FrameNet project (Johnson et al,
2002) is based on Fillmore?s Frame Semantics. A
frame is a conceptual structure that describes a situ-
ation. It is introduced by a target or frame-evoking
element (FEE). The roles, called frame elements
(FEs), are local to particular frames and are the par-
ticipants and props of the described situations.
The aim of FrameNet is to provide a comprehen-
sive frame-semantic description of the core lexicon
of English. A database of frames contains the
frames? basic conceptual structure, and names and
descriptions for the available frame elements. A
lexicon database associates lemmas with the frames
they evoke, lists possible syntactic realizations of
FEs and provides annotated examples from the
BNC. The current on-line version of the frame
database (Johnson et al, 2002) consists of almost
400 frames, and covers about 6,900 lexical entries.
Frame: REQUEST
FE Example
SPEAKER Pat urged me to apply for the job.
ADDRESSEE Pat urged me to apply for the job.
MESSAGE Pat urged me to apply for the job.
TOPIC Kim made a request about changing the title.
MEDIUM Kim made a request in her letter.
Frame: COMMERCIAL TRANSACTION (C T)
BUYER Jess bought a coat.
GOODS Jess bought a coat.
SELLER Kim sold the sweater.
MONEY Kim paid 14 dollars for the ticket.
PURPOSE Kim bought peppers to cook them.
REASON Bob bought peppers because he was hungry.
Figure 1: Example frame descriptions.
Figure 1 shows two frames. The frame REQUEST
involves a FE SPEAKER who voices the request,
an ADDRESSEE who is asked to do something, the
MESSAGE, the request that is made, the TOPIC that
the request is about, and the MEDIUM that is used to
convey the request. Among the FEEs for this frame
are the verb ask and the noun request. In the frame
COMMERCIAL TRANSACTION (henceforth C T), a
BUYER gives MONEY to a SELLER and receives
GOODS in exchange. This frame is evoked e.g. by
the verb pay and the noun money.
The TIGER Corpus. We are using the TIGER
Corpus (Brants et al, 2002), a manually syntacti-
cally annotated German corpus, as a basis for our
annotation. It is the largest available such cor-
pus (80,000 sentences in its final release compared
to 20,000 sentences in its predecessor NEGRA)
and uses a rich annotation format. The annotation
scheme is surface oriented and comparably theory-
neutral. Individual words are labelled with POS
information. The syntactic structures of sentences
are described by relatively flat trees providing in-
formation about grammatical functions (on edge la-
bels), syntactic categories (on node labels), and ar-
gument structure of syntactic heads (through the
use of dependency-oriented constituent structures,
which are close to the syntactic surface). An exam-
ple for a syntactic structure is given in Figure 2.
3 Project overview
The aim of the SALSA project is to construct a large
semantically annotated corpus and to provide meth-
ods for its utilisation.
Corpus construction. In the first phase of the
project, we annotate the TIGER corpus in part man-
Figure 2: A sentence and its syntactic structure.
ually, in part semi-automatically, having tools pro-
pose tags which are verified by human annotators.
In the second phase, we will extend these tools for
the weakly supervised annotation of a much larger
corpus, using the TIGER corpus as training data.
Utilisation. The SALSA corpus is designed to
be utilisable for many purposes, like improving sta-
tistical parsers, and extending methods for informa-
tion extraction and access. The focus in the SALSA
project itself is on lexical semantics, and our first
use of the corpus will be to extract selectional pref-
erences for frame elements.
The SALSA corpus will be tagged with the fol-
lowing types of semantic information:
FrameNet frames. We tag all FEEs that oc-
cur in the corpus with their appropriate frames, and
specify their frame elements. Thus, our focus is
different from the lexicographic orientation of the
FrameNet project mentioned above. As we tag all
corpus instances of each FEE, we expect to en-
counter a wider range of phenomena. which Cur-
rently, FrameNet only exists for English and is still
under development. We will produce a ?light ver-
sion? of a FrameNet for German as a by-product
of the annotation, reusing as many as possible of
the semantic frame descriptions from the English
FrameNet database. Our first results indicate that
the frame structure assumed for the description of
the English lexicon can be reused for German, with
minor changes and extensions.
Word sense. The additional value of word sense
disambiguation in a corpus is obvious. However,
exhaustive word sense annotation is a highly time-
consuming task. Therefore we decided for a selec-
tive annotation policy, annotating only the heads of
frame elements. GermaNet, the German WordNet
version, will be used as a basis for the annotation.
            request         conversation
SPKR
FEE
ADD
MSG
FEE FEE
TOPIC
INTLC_1
Figure 3: Frame annotation.
Coreference. Similarly, we will selectively anno-
tate coreference. If a lexical head of a frame element
is an anaphor, we specify the antecedent to make the
meaning of the frame element accessible.
4 Frame Annotation
Annotation schema. To give a first impression of
frame annotation, we turn to the sentence in Fig. 2:
(1) SPD fordert Koalition zu Gespra?ch u?ber Re-
form auf.
(SPD requests that coalition talk about reform.)
Fig. 3 shows the frame annotation associated with
(1). Frames are drawn as flat trees. The root node is
labelled with the frame name. The edges are labelled
with abbreviated FE names, like SPKR for SPEAKER,
plus the tag FEE for the frame-evoking element. The
terminal nodes of the frame trees are always nodes
of the syntactic tree. Cases where a semantic unit
(FE or FEE) does not form one syntactic constituent,
like fordert . . . auf in the example, are represented
by assignment of the same label to several edges.
Sentence (1), a newspaper headline, contains at
least two FEEs: auffordern and Gespra?ch. auf-
fordern belongs to the frame REQUEST (see Fig. 1).
In our example the SPEAKER is the subject NP SPD,
the ADDRESSEE is the direct object NP Koalition,
and the MESSAGE is the complex PP zu Gespra?ch
u?ber Reform. So far, the frame structure follows the
syntactic structure, except for that fact that the FEE,
as a separable prefix verb, is realized by two syntac-
tic nodes. However, it is not always the case that
frame structure parallels syntactic structure. The
second FEE Gespra?ch introduces the frame CON-
VERSATION. In this frame two (or more) groups
talk to one another and no participant is construed
as only a SPEAKER or only an ADDRESSEE. In
our example the only NP-internal frame element is
the TOPIC (?what the message is about?) u?ber Re-
form, whereas the INTERLOCUTOR-1 (?the promi-
nent participant in the conversation?) is realized by
the direct object of auffordern.
As shown in Fig. 3, frames are annotated as trees
of depth one. Although it might seem semantically
more adequate to admit deeper frame trees, e.g. to
allow the MSG edge of the REQUEST frame in Fig.
3 to be the root node of the CONVERSATION tree,
as its ?real? semantic argument, the representation
of frame structure in terms of flat and independent
semantic trees seems to be preferable for a number
of practical reasons: It makes the annotation process
more modular and flexible ? this way, no frame an-
notation relies on previous frame annotation. The
closeness to the syntactic structure makes the an-
notators? task easier. Finally, it facilitates statistical
evaluation by providing small units of semantic in-
formation that are locally related to syntax.
Difficult cases. Because frame elements may
span more than one sentence, like in the case of
direct speech, we cannot restrict ourselves to an-
notation at sentence level. Also, compound nouns
require annotation below word level. For ex-
ample, the word ?Gagenforderung? (demand for
wages) consists of ?-forderung? (demand), which in-
vokes the frame REQUEST, and a MESSAGE element
?Gagen-?. Another interesting point is that one word
may introduce more than one frame in cases of co-
ordination and ellipsis. An example is shown in (2).
In the elliptical clause only one fifth for daughters,
the elided bought introduces a C T frame. So we let
the bought in the antecedent introduce two frames,
one for the antecedent and one for the ellipsis.
(2) Ein Viertel aller Spielwaren wu?rden fu?r So?hne
erworben, nur ein Fu?nftel fu?r To?chter.
(One quarter of all toys are bought for sons, only one fifth
for daughters.)
Annotation process. Frame annotation proceeds
one frame-evoking lemma at a time, using subcor-
pora containing all instances of the lemma with
some surrounding context. Since most FEEs are
polysemous, there will usually be several frames rel-
evant to a subcorpus. Annotators first select a frame
for an instance of the target lemma. Then they assign
frame elements.
At the moment the annotation uses XML tags on
bare text. The syntactic structure of the TIGER-
sentences can be accessed in a separate viewer. An
annotation tool is being implemented that will pro-
vide a graphical interface for the annotation. It will
display the syntactic structure and allow for a graph-
ical manipulation of semantic frame trees, in a simi-
lar way as shown in Fig. 3.
Extending FrameNet. Since FrameNet is far
from being complete, there are many word senses
not yet covered. For example the verb fordern,
which belongs to the REQUEST frame, additionally
has the reading challenge, for which the current ver-
sion of FrameNet does not supply a frame.
5 Evaluation of Annotated Data
Materials. Compared to the pilot study we previ-
ously reported (Erk et al, 2003), in which 3 annota-
tors tagged 440 corpus instances of a single frame,
resulting in 1,320 annotation instances, we now dis-
pose of a considerably larger body of data. It con-
sists of 703 corpus instances for the two frames
shown in Figure 1, making up a total of 4,653 an-
notation instances. For the frame REQUEST, we
obtained 421 instances with 8-fold and 114 with
7-fold annotation. The annotated lemmas com-
prise auffordern (to request), fordern, verlangen (to
demand), zuru?ckfordern (demand back), the noun
Forderung (demand), and compound nouns ending
with -forderung. For the frame C T we have 30, 40
and 98 instances with 5-, 3-, and 2-fold annotation
respectively. The annotated lemmas are kaufen (to
buy), erwerben (to acquire), verbrauchen (to con-
sume), and verkaufen (to sell).
Note that the corpora we are evaluating do not
constitute a random sample: At the moment, we
cover only two frames, and REQUEST seems to be
relatively easy to annotate. Also, the annotation re-
sults may not be entirely predictive for larger sam-
ple sizes: While the annotation guidelines were be-
ing developed, we used REQUEST as a ?calibration?
frame to be annotated by everybody. As a result, in
some cases reliability may be too low because de-
tailed guidelines were not available, and in others
it may be too high because controversial instances
were discussed in project meetings.
Results. The results in this section refer solely to
the assignment of fully specified frames and frame
elements. Underspecification is discussed at length
frames average best worst
REQUEST 96.83% 100% 90.73%
COMM. 97.11% 98.96% 88.71%
elements average best worst
REQUEST 88.86% 95.69% 66.57%
COMM. 74.25% 90.30% 69.33%
Table 1: Inter-annotator agreement on frames (top)
and frame elements (below).
in Section 6. Due to the limited space in this pa-
per, we only address the question of inter-annotator
agreement or annotation reliability, since a reliable
annotation is necessary for all further corpus uses.
Table 1 shows the inter-annotator agreement on
frame assignment and on frame element assignment,
computed for pairs of annotators. The ?average?
column shows the total agreement for all annotation
instances, while ?best? and ?worst? show the fig-
ures for the (lemma-specific) subcorpora with high-
est and lowest agreement, respectively. The upper
half of the table shows agreement on the assignment
of frames to FEEs, for which we performed 14,410
pairwise comparisons, and the lower half shows
agreement on assigned frame elements (29,889 pair-
wise comparisons). Agreement on frame elements is
?exact match?: both annotators have to tag exactly
the same sequence of words. In sum, we found that
annotators agreed very well on frames. Disagree-
ment on frame elements was higher, in the range of
12-25%. Generally, the numbers indicated consider-
able differences between the subcorpora.
To investigate this matter further, we computed
the Alpha statistic (Krippendorff, 1980) for our an-
notation. Like the widely used Kappa, ? is a chance-
corrected measure of reliability. It is defined as
? = 1 ? observed disagreement
expected disagreement
We chose Alpha over Kappa because it also indi-
cates unreliabilities due to unequal coder preference
for categories. With an ? value of 1 signifying total
agreement and 0 chance agreement, ? values above
0.8 are usually interpreted as reliable annotation.
Figure 4 shows single category reliabilities for
the assignment of frame elements. The graphs
shows that not only did target lemmas vary in
their difficulty, but that reliability of frame ele-
ment assignment was also a matter of high varia-
tion. Firstly, frames introduced by nouns (Forderung
and -forderung) were more difficult to annotate than
verbs. Secondly, frame elements could be assigned
to three groups: frame elements which were al-
ways annotated reliably, those whose reliability was
highly dependent on the FEE, and the third group
whose members were impossible to annotate reli-
ably (these are not shown in the graphs). In the
REQUEST frames, SPEAKER, MESSAGE and AD-
DRESSEE belong to the first group, at least for verbal
FEEs. MEDIUM is a member of the second group,
and TOPIC was annotated at chance level (? ? 0).
In the COMMERCE frame, only BUYER and GOODS
always show high reliability. SELLER can only be re-
liably annotated for the target verkaufen. PURPOSE
and REASON fall into the third group.
5.1 Discussion
Interpretation of the data. Inter-annotator agree-
ment on the frames shown in Table 1 is very high.
However, the lemmas we considered so far were
only moderately ambiguous, and we might see lower
figures for frame agreement for highly polysemous
FEEs like laufen (to run).
For frame elements, inter-annotator agreement
is not that high. Can we expect improvement?
The Prague Treebank reported a disagreement of
about 10% for manual thematic role assignment
( ?Zabokrtsky?, 2000). However, in contrast to our
study, they also annotated temporal and local modi-
fiers, which are easier to mark than other roles.
One factor that may improve frame element
agreement in the future is the display of syntactic
structure directly in the annotation tool. Annotators
were instructed to assign each frame element to a
single syntactic constituent whenever possible, but
could only access syntactic structure in a separate
viewer. We found that in 35% of pairwise frame ele-
ment disagreements, one annotator assigned a single
syntactic constituent and the other did not. Since a
total of 95.6% of frame elements were assigned to
single constituents, we expect an increase in agree-
ment when a dedicated annotation tool is available.
As to the pronounced differences in reliability be-
tween frame elements, we found that while most
central frame elements like SPEAKER or BUYER
were easy to identify, annotators found it harder to
agree on less frequent frame elements like MEDIUM,
PURPOSE and REASON. The latter two with their
 0.6
 0.8
 1
auffordern fordern verlangen Forderung -forderung
a
lp
ha
 v
al
ue
addressee
medium
message
speaker
 0.6
 0.8
 1
erwerben kaufen verkaufen
a
lp
ha
 v
al
ue
buyer
seller
money
goods
Figure 4: Alpha values for frame elements. Left: REQUEST. Right: COMMERCIAL TRANSACTION.
particularly low agreement (? < 0.8) contribute to-
wards the low overall inter-annotator agreement of
the C T frame. We suspect that annotators saw too
few instances of these elements to build up a reli-
able intuition. However, the elements may also be
inherently difficult to distinguish.
How can we interpret the differences in frame el-
ement agreement across target lemmas, especially
between verb and noun targets? While frame ele-
ments for verbal targets are usually easy to identify
based on syntactic factors, this is not the case for
nouns. Figure 3 shows an example: Should SPD
be tagged as INTERLOCUTOR-2 in the CONVERSA-
TION frame? This appears to be a question of prag-
matics. Here it seems that clearer annotation guide-
lines would be desirable.
FrameNet as a resource for semantic role an-
notation. Above, we have asked about the suitabil-
ity of FrameNet for semantic role annotation, and
our data allow a first, though tentative, assessment.
Concerning the portability of FrameNet to other
languages than English, the English frames worked
well for the German lemmas we have seen so far.
For C T a number of frame elements seem to be
missing, but these are not language-specific, like
CREDIT (for on commission and in installments).
The FrameNet frame database is not yet complete.
How often do annotators encounter missing frames?
The frame UNKNOWN was assigned in 6.3% of the
instances of REQUEST, and in 17.6% of the C T in-
stances. The last figure is due to the overwhelm-
ing number of UNKNOWN cases in verbrauchen, for
which the main sense we encountered is ?to use up
a resource?, which FrameNet does not offer.
Is the choice of frame always clear? And can
frame elements always be assigned unambiguously?
Above we have already seen that frame element as-
signment is problematic for nouns. In the next sec-
tion we will discuss problematic cases of frame as-
signment as well as frame element assignment.
6 Vagueness, Ambiguity and
Underspecification
Annotation Challenges. It is a well-known prob-
lem from word sense annotation that it is often im-
possible to make a safe choice among the set of pos-
sible semantic correlates for a linguistic item. In
frame annotation, this problem appears on two lev-
els: The choice of a frame for a target is a choice
of word sense. The assignment of frame elements to
phrases poses a second disambiguation problem.
An example of the first problem is the Ger-
man verb verlangen, which associates with both the
frame REQUEST and the frame C T. We found sev-
eral cases where both readings seem to be equally
present, e.g. sentence (3). Sentences (4) and (5) ex-
emplify the second problem. The italicised phrase in
(4) may be either a SPEAKER or a MEDIUM and the
one in (5) either a MEDIUM or not a frame element
at all. In our exhaustive annotation, these problems
are much more virulent than in the FrameNet corpus,
which consists mostly of prototypical examples.
(3) Gleichwohl versuchen offenbar Assekuranzen,
[das Gesetz] zu umgehen, indem sie von Nicht-
deutschen mehr Geld verlangen.
(Nonetheless insurance companies evidently try to cir-
cumvent [the law] by asking/demanding more money
from non-Germans.)
(4) Die nachhaltigste Korrektur der Programmatik
fordert ein Antrag. . .
(The most fundamental policy correction is requested by
a motion. . . )
(5) Der Parteitag billigte ein Wirtschaftskonzept, in
dem der Umbau gefordert wird.
(The party congress approved of an economic concept in
which a change is demanded.)
Following Kilgarriff and Rosenzweig (2000), we
distinguish three cases where the assignment of a
single semantic tag is problematic: (1), cases in
which, judging from the available context informa-
tion, several tags are equally possible for an ambigu-
ous utterance; (2), cases in which more than one tag
applies at the same time, because the sense distinc-
tion is neutralised in the context; and (3), cases in
which the distinction between two tags is systemati-
cally vague or unclear.
In SALSA, we use the concept of underspecifica-
tion to handle all three cases: Annotators may assign
underspecified frame and frame element tags. While
the cases have different semantic-pragmatic status,
we tag all three of them as underspecified. This is in
accordance with the general view on underspecifica-
tion in semantic theory (Pinkal, 1996). Furthermore,
Kilgarriff and Rosenzweig (2000) argue that it is im-
possible to distinguish those cases
Allowing underspecified tags has several advan-
tages. First, it avoids (sometimes dubious) decisions
for a unique tag during annotation. Second, it is use-
ful to know if annotators systematically found it hard
to distinguish between two frames or two frame ele-
ments. This diagnostic information can be used for
improving the annotation scheme (e.g. by removing
vague distinctions). Third, underspecified tags may
indicate frame relations beyond an inheritance hier-
archy, horizontal rather than vertical connections. In
(3), the use of underspecification can indicate that
the frames REQUEST and C T are used in the same
situation, which in turn can serve to infer relations
between their respective frame elements.
Evaluating underspecified annotation. In the
previous section, we disregarded annotation cases
involving underspecification. In order to evalu-
ate underspecified tags, we present a method of
computing inter-annotator agreement in the pres-
ence of underspecified annotations. Represent-
ing frames and frame elements as predicates that
each take a sequence of word indices as their
argument, a frame annotation can be seen as a
pair (CF,CE) of two formulae, describing the
frame and the frame elements, respectively. With-
out underspecification, CF is a single predicate
and CE is a conjunction of predicates. For the
CONVERSATION frame of sentence (1), CF has
the form CONVERSATION(Gespra?ch)1 , and CE is
INTLC 1(Koalition) ? TOPIC(u?ber Reform). Un-
derspecification is expressed by conjuncts that are
disjunctions instead of single predicates. Table 2
shows the admissible cases. For example, the CE
of (4) contains the conjunct SPKR(ein Antrag) ?
MEDIUM(ein Antrag). Our annotation scheme guar-
antees that every FE name appears in at most one
conjunct of CE. Exact agreement means that ev-
ery conjunct of annotator A must correspond to a
conjunct by annotator B, and vice versa. For partial
agreement, it suffices that for each conjunct of A,
one disjunct matches a disjunct in a conjunct of B,
and conversely.
frame annotation
F(t) single frame: F is assigned to t
(F1(t)?F2(t)) frame disjunction: F1 or F2 is
assigned to t
frame element annotation
E(s) single frame element: E is as-
signed to s
(E1(s)?E2(s)) frame element disjunction: E1
or E2 is assigned to s
(E(s)?NOFE(s)) optional element: E1 or no
frame element is assigned to s
(E(s)?E(s1ss2)) underspecified length: frame
element E is assigned to s
or the longer sequence s1ss2,
which includes s
Table 2: Types of conjuncts. F is a frame name, E
a frame element name, and t and s are sequences of
word indices (t is for the target (FEE))
Using this measure of partial agreement, we now
evaluate underspecified annotation. The most strik-
ing result is that annotators made little use of under-
specification. Frame underspecification was used in
0.4% of all frames, and frame element underspecifi-
cation for 0.9% of all frame elements. The frame el-
ement MEDIUM, which was rarely assigned outside
1We use words instead of indices for readability.
underspecification, accounted for roughly half of all
underspecification in the REQUEST frame. 63% of
the frame element underspecifications are cases of
optional elements, the third class in the lower half of
Table 2. (Partial) agreement on underspecified tags
was considerably lower than on non-underspecified
tags, both in the case of frames (86%) and in the
case of frame elements (54%). This was to be ex-
pected, since the cases with underspecified tags are
the more difficult and controversial ones. Since un-
derspecified annotation is so rare, overall frame and
frame element agreement including underspecified
annotation is virtually the same as in Table 1.
It is unfortunate that annotators use underspecifi-
cation only infrequently, since it can indicate inter-
esting cases of relatedness between different frames
and frame elements. However, underspecification
may well find its main use during the merging of
independent annotations of the same corpus. Not
only underspecified annotation, also disagreement
between annotators can point out vague and ambigu-
ous cases. If, for example, one annotator has as-
signed SPEAKER and the other MEDIUM in sentence
(4), the best course is probably to use an underspec-
ified tag in the merged corpus.
7 Conclusion
We presented the SALSA project, the aim of which
is to construct and utilize a large corpus reliably
annotated with semantic information. While the
SALSA corpus is designed to be utilizable for many
purposes, our focus is on lexical semantics, in or-
der to address one of the most serious bottlenecks
for language technology today: the lack of large,
domain-independent lexica.
In this paper we have focused on the annotation
with frame semantic roles. We have presented the
annotation scheme, and we have evaluated first an-
notation results, which show encouraging figures for
inter-annotator agreement. We have discussed the
problem of vagueness and ambiguity of the data and
proposed a representation for underspecified tags,
which are to be used both for the annotation and the
merging of individual annotations.
Important next steps are: the design of a tool for
semi-automatic annotation, and the extraction of se-
lectional preferences from the annotated data.
Acknowledgments. We would like to thank the
following people, who helped us with their sugges-
tions and discussions: Sue Atkins, Collin Baker,
Ulrike Baldewein, Hans Boas, Daniel Bobbert,
Sabine Brants, Paul Buitelaar, Ann Copestake,
Christiane Fellbaum, Charles Fillmore, Gerd Flied-
ner, Silvia Hansen, Ulrich Heid, Katja Markert and
Oliver Plaehn. We are especially indebted to Maria
Lapata, whose suggestions have contributed to the
current shape of the project in an essential way. Any
errors are, of course, entirely our own.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceedings of
COLING-ACL, Montreal, Canada.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lez-
ius, and George Smith. 2002. The TIGER treebank. In
Proceedings of the Workshop on Treebanks and Linguistic
Theories, Sozopol, Bulgaria.
Katrin Erk, Andrea Kowalski, and Manfred Pinkal. 2003. A
corpus resource for lexical semantics. In Proceedings of
IWCS5, pages 106?121, Tilburg, The Netherlands.
Charles J. Fillmore. 1968. The case for case. In Bach and
Harms, editors, Universals in Linguistic Theory, pages 1?88.
Holt, Rinehart, and Winston, New York.
Eva Hajic?ov a?. 1998. Prague Dependency Treebank: From An-
alytic to Tectogrammatical Annotation. In Proceedings of
TSD?98, pages 45?50, Brno, Czech Republic.
C. R. Johnson, C. J. Fillmore, M. R. L. Petruck, C. F. Baker,
M. Ellsworth, J. Ruppenhofer, and E. J. Wood. 2002.
FrameNet: Theory and Practice. http://www.icsi.
berkeley.edu/?framenet/book/book.html.
Adam Kilgarriff and Joseph Rosenzweig. 2000. Framework
and results for English Senseval. Computers and the Hu-
manities, 34(1-2).
Adam Kilgarriff, editor. 2001. SENSEVAL-2, Toulouse.
Klaus Krippendorff. 1980. Content Analysis. Sage.
M. Marcus, G. Kim, M.A. Marcinkiewicz, R. MacIntyre,
A. Bies, M. Gerguson, K. Katz, and B. Schasberger. 1994.
The Penn Treebank: Annotating predicate argument struc-
ture. In Proceedings of the ARPA HLT Workshop.
G. Miller, R. Beckwith, C. Fellbaum, D. Gros, and K. Miller.
1990. Introduction to WordNet: An on-line lexical database.
International Journal of Lexicography, 3(4):235?44.
Manfred Pinkal. 1996. Vagueness, ambiguity, and underspeci-
fication. In Proceedings of SALT?96, pages 185?201.
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and Hans
Uszkoreit. 1998. A linguistically interpreted corpus of Ger-
man newspaper text. In Proceedings of LREC?98, Granada.
Zdene?k ?Zabokrtsk y?. 2000. Automatic functor assignment
in the Prague Dependency Treebank. In Proceedings of
TSD?00, Brno, Czech Republic.
Semantic Role Labelling with Similarity-Based Generalization Using
EM-based Clustering
Ulrike Baldewein, Katrin Erk, Sebastian Pad?
Saarland University
Saarbr?cken, Germany
{ulrike,erk,pado}@coli.uni-sb.de
Detlef Prescher
University of Amsterdam
Amsterdam, The Netherlands
prescher@science.uva.nl
Abstract
We describe a system for semantic role assignment
built as part of the Senseval III task, based on an
off-the-shelf parser and Maxent and Memory-Based
learners. We focus on generalisation using several
similarity measures to increase the amount of train-
ing data available and on the use of EM-based clus-
tering to improve role assignment. Our final score
is Precision=73.6%, Recall=59.4% (F=65.7).
1 Introduction
This paper describes a study in semantic role la-
belling in the context of the Senseval III task, for
which the training and test data were both drawn
from the current FrameNet release (Johnson et al,
2002). We concentrated on two questions: first,
whether role assignment can be improved by gener-
alisation over training instances using different sim-
ilarity measures; and second, the impact of EM-
based clustering, both in deriving more informative
selectional preference features and in the generali-
sations mentioned above. The basis of our experi-
ments was formed by off-the-shelf statistical tools
for data processing and modelling.
After listing our data preparation steps (Sec. 2)
and features (Sec. 3), we describe our classification
procedure and the learners we used (Sec. 4). Sec. 5
outlines our experiments in similarity-based gener-
alisations, and Section 6 discusses our results.
2 Data and Instances
Parsing. To tag and parse the data, we used
LoPar (Schmid, 2000), a probabilistic context-
free parser, which comes with a Head-Lexicalised
Grammar for English (Carroll and Rooth, 1998).
We considered only the most probable parse for
each sentence and simplified parse trees by elim-
inating unary nodes. The resulting nodes form
the instances of our classification. We used the
Stuttgart TreeTagger (Schmid, 1994) to lemmatise
constituent heads.
Projection of role labels. FrameNet provides se-
mantic roles as character offsets. We labelled
those instances (i.e. nodes in the parse tree) with
gold standard semantic roles which corresponded to
roles? maximal projections. 13.95% of roles in the
training corpus spanned more than one parse tree
node. Figure 1 shows an example sentence for the
AWARENESS frame. The nodes? respective seman-
tic role labels are given in small caps, and the target
predicate is marked in boldface.
S (NONE)
NP (COGNIZER)
Peter
VP (NONE)
V (NONE)
does not VP (NONE)
know NP (CONTENT)
the answer
Figure 1: Example parse tree with role labels
Semantic clustering. We used clustering to gen-
eralise over possible fillers of roles. In a first model,
we derived a probability distribution   for pairs
	

, where  is a target:role combination
and   is the head lemma of a role filler. The key
idea is that   and   are mutually independent, but
conditioned on an unobserved class  . In this
manner, we define the probability of 
     
Semantic Role Labelling With Chunk Sequences
Ulrike Baldewein, Katrin Erk, Sebastian Pad?
Saarland University
Saarbr?cken, Germany
{ulrike,erk,pado}@coli.uni-sb.de
Detlef Prescher
University of Amsterdam
Amsterdam, The Netherlands
prescher@science.uva.nl
Abstract
We describe a statistical approach to semantic
role labelling that employs only shallow infor-
mation. We use a Maximum Entropy learner,
augmented by EM-based clustering to model
the fit between a verb and its argument can-
didate. The instances to be classified are se-
quences of chunks that occur frequently as ar-
guments in the training corpus. Our best model
obtains an F score of 51.70 on the test set.
1 Introduction
This paper describes a statistical approach to semantic
role labelling addressing the CoNLL shared task 2004,
which is based on the the current release of the English
PropBank data (Kingsbury et al, 2002). For further de-
tails of the task, see (Carreras and M?rquez, 2004).
We address the main challenge of the task, the absence
of deep syntactic information, with three main ideas:
  Proper constituents being unavailable, we use chunk
sequences as instances for classification.
  The classification is performed by a maximum en-
tropy model, which can integrate features from het-
erogeneous data sources.
  We model the fit between verb and argument can-
didate by clusters induced with EM on the training
data, which we use as features during classification.
Sections 2 through 4 describe the systems? architec-
ture. First, we compute chunk sequences for all sentences
(Sec. 2). Then, we classify these sequences with max-
imum entropy models (Sec. 3). Finally, we determine
the most probable chain of sequences covering the whole
sentence (Sec. 4). Section 5 discusses the impact of dif-
ferent parameters and gives final results.
2 Chunk Sequences as Instances
All studies of semantic role labelling we are aware of
have used constituents as instances for classification.
However, constituents are not available in the shallow
syntactic information provided by this task. Two other
levels of granularity are available in the data: words and
chunks. In a pretest, we found that words are too fine
grained, such that learners find it very difficult to identify
argument boundaries on the word level. Chunks, too, are
problematic, since one third of the arguments span more
than one chunk, and for one tenth of the arguments the
boundaries do not coincide with any chunk boundaries.
We decided to use chunk sequences as instances for
classification. They can describe multi-chunk and part-
chunk arguments, and by approximating constituents,
they allow the use of linguistically informed features. In
the sentence in Figure 1, Britain?s manufacturing indus-
try forms a sequence of type NP_NP. To make sequences
more distinctive, we conflate whole clauses embedded
deeper than the target to S: For the target transform-
ing, we characterise the sequence for to boost exports
as S rather than VP_NP. An argument boundary inside
a chunk is indicated by the part of speech of the last in-
cluded word: For boost the sequence is VP(NN).
To determine ?good? sequences, we collected argu-
ment realisations from the training corpus, generalising
them by simple heuristics (e.g. removing anything en-
closed in brackets). The generalised argument sequences
exhibit a Zipfian distribution (see Fig. 2). NP is by
far the most frequent sequence, followed by S. An ex-
ample of a very infrequent argument chunk sequence
is NP_PP_NP_PP_NP_VP_PP_NP_NP (in words: a
bonus in the form of charitable donations made from an
employer ?s treasury).
The chunk sequence approach also allows us to con-
sider the divider chunk sequences that separate arguments
and targets. For example, A0s are usually divided from
the target by the empty divider, while A2 arguments are
Britain ?s manufacturing industry is transforming itself to boost exports
NNP POS VBG NN VBZ VBG PRP TO NN NNS
[NP ] [NP ] [VP ] [NP] [VP ] [NP ]
[S ]
Figure 1: Part of a sentence with part of speech, chunk and clause information
 0
 5000
 10000
 15000
 20000
 25000
Fr
eq
ue
nc
y 
in
 tr
ai
ni
ng
 d
at
a
Sequence frequencies
Divider frequencies
Figure 2: Frequency distribution for the 20 most frequent
sequences and dividers in the training data
separated from it by e.g. a typical A1 sequence. Gen-
eralised divider chunk sequences separating actual argu-
ments and targets in the training set show a Zipfian distri-
bution similar to the chunk sequences (see Fig. 2).
As instances to be classified, we consider all sequences
whose generalised sequence and divider each appear at
least 10 times for an argument in the training corpus, and
whose generalised sequence and divider appear together
at least 5 times. The first cutoff reduces the number of
sequences from 1089 to 87, and the number of dividers
from 999 to 120, giving us 581,813 sequences as training
data (about twice as many as words), of which 45,707
are actual argument labels. The additional filter for se-
quence/divider pairs reduces the training data to 354,916
sequences, of which 43,622 are actual arguments. We pay
for the filtering by retaining only 87.49% of arguments on
the training set (83.32% on the development set).
3 Classification
3.1 Maximum Entropy Modelling
We use a log-linear model as classifier, which defines the
probability of a class  given an feature vector

 as




	
ffProceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 400?409, Prague, June 2007. c?2007 Association for Computational Linguistics
Flexible, Corpus-Based Modelling of Human Plausibility Judgements
Sebastian Pad? and Ulrike Pad?
Computational Linguistics
Saarland University
Saarbr?cken, Germany
{pado,ulrike}@coli.uni-sb.de
Katrin Erk
Dept. of Linguistics
University of Texas at Austin
Austin, Texas
katrin.erk@mail.utexas.edu
Abstract
In this paper, we consider the computational
modelling of human plausibility judgements
for verb-relation-argument triples, a task
equivalent to the computation of selectional
preferences. Such models have applications
both in psycholinguistics and in computa-
tional linguistics.
By extending a recent model, we obtain
a completely corpus-driven model for this
task which achieves significant correlations
with human judgements. It rivals or exceeds
deeper, resource-driven models while exhibit-
ing higher coverage. Moreover, we show that
our model can be combined with deeper mod-
els to obtain better predictions than from ei-
ther model alone.
1 Introduction
One fundamental and intuitive finding in experimen-
tal psycholinguistics is that humans judge the plau-
sibility of a verb-argument pair vastly differently de-
pending on the semantic relation in the pair. Table 1
lists example human judgements which McRae et
al. (1998) elicited by asking about the plausibility of,
e.g., a hunter shooting (relation agent) or being shot
(relation patient). McRae et al found that ?hunter? is
judged to be a very plausible agent of ?shoot? and
an implausible patient, while the reverse is true for
?deer?. In linguistics, this phenomenon is explained
by selectional preferences on verbs? argument po-
sitions; we use plausibility and fit with selectional
preferences interchangeably.
Verb Relation Noun Plausibility
shoot agent hunter 6.9
shoot patient hunter 2.8
shoot agent deer 1.0
shoot patient deer 6.4
Table 1: Verb-relation-noun triples with plausibility
judgements on a 7-point scale (McRae et al, 1998)
In this paper, we consider computational mod-
els that predict human plausibility ratings, or the
fit of selectional preferences and argument, for
such (verb, relation, argument), in short, (v, r, a),
triples. Being able to model this type of data is rel-
evant in a number of ways. From the point of view
of psycholinguistics, selectional preferences have an
important effect in human sentence processing (e.g.,
McRae et al (1998), Trueswell et al (1994)), and
models of selectional preferences are therefore nec-
essary to inform models of this process (Pad? et al,
2006). In computational linguistics, a multitude of
tasks is sensitive to selectional preferences, such as
the resolution of ambiguous attachments (Hindle and
Rooth, 1993), word sense disambiguation (McCarthy
and Carroll, 2003), semantic role labelling (Gildea
and Jurafsky, 2002), or testing the applicability of
inference rules (Pantel et al, 2007).
A number of approaches has been proposed to
model selectional preference data (Pad? et al, 2006;
Resnik, 1996; Clark and Weir, 2002; Abe and Li,
1996). These models generally operate by general-
ising from seen (v, r, a) triples to unseen ones. By
relying on resources like corpora with semantic role
annotation or the WordNet ontology, these models
400
generally share two problems: (a), limited coverage;
and (b), the resource (at least partially) predetermines
the generalisations that they can make.
In this paper, we investigate whether it is possi-
ble to predict the plausibility of (v, r, a) triples in
a completely corpus-driven way. We build on a re-
cent selectional preference model (Erk, 2007) that
bases its generalisations on word similarity in a vec-
tor space. While that model relies on corpora with
semantic role annotation, we show that it is possible
to predict plausibility ratings solely on the basis of a
parsed corpus, by using shallow cues and a suitable
vector space specification.
For evaluation, we use two balanced data sets of
human plausibility judgements, i.e., datasets where
each verb is paired both with a good agent and a good
patient, and where both nouns are presented in either
semantic relation (as in Table 1). Using balanced test
data is a particularly difficult task, since it forces
the models to account reliably both for the influence
of the semantic relation (agent/patient) and of the
argument head (?hunter?/?deer?).
We obtain three main results: (a), our model is able
to match the superior performance of the model pro-
posed by Pad? et al (2006), while retaining the high
coverage of the model proposed by Resnik (1996);
(b), using parsing as a preprocessing step improves
the model?s performance significantly; and (c), a com-
bination of our model with the Pad? model exceeds
both individual models in accuracy.
Plan of the paper. In Section 2, we give an
overview of existing selectional preferences and vec-
tor space models. Section 3 introduces our model and
discusses its parameters. Sections 4 and 5 present our
experimental setup and results. Section 6 concludes.
2 Related Work
Modelling Selectional Preferences with Gram-
matical Functions. The idea of inducing selec-
tional preferences from corpora was introduced by
Resnik (1996). He approximated the semantic verb-
argument relations in (v, r, a) triples by grammatical
functions, which are readily available for large train-
ing corpora. His basic two-step procedure was fol-
lowed by all later approaches: (1), extract argument
headwords for a given predicate and relation from
a corpus; (2), generalise to other, similar words us-
ing the WordNet noun hierarchy. Other models also
relying on the WordNet resource include Abe and
Li (1996) and Clark and Weir (2002).
We present Resnik?s model in some detail, since
we will use it for comparison below. Resnik first
computes the overall selectional preference strength
for each verb-relation pair, i.e. the degree of ?con-
strainedness? of each relation. This quantity is esti-
mated as the difference (in terms of the Kullback-
Leibler divergence D) between the distribution over
WordNet argument classes given the relation, p(c|r),
and the distribution of argument classes given the
current verb-relation combination, p(c|v, r). The in-
tuition is that a verb-relation pair that only allows
for a limited range of argument heads will have a
probability distribution over argument classes that
strongly diverges from the prior distribution.
Next, the selectional association of the triple,
A(v, r, c), is computed as the ratio of the selectional
preference strength for this particular class, divided
by the overall selectional preference strength of the
verb-relation pair. This is shown in Equation 1.
A(v, r, c) =
p(c|v, r)log p(c|v,r)p(c|r)
D(p(c|r)||p(c|v, r))
(1)
Finally, the selectional preference between a verb,
a relation, and an argument head is taken to be the
selectional association of the verb and relation with
the most strongly associated WordNet ancestor class
of the argument.
WordNet-based approaches however face two
problems. One is a coverage problem due to the lim-
ited size of the resource (see the task-based evalu-
ation in Gildea and Jurafsky (2002)). The other is
that the shape of the WordNet hierarchy determines
the generalisations that the models make. These are
not always intuitive. For example, Resnik (1996) ob-
serves that (answer, obj, tragedy) receives a high
preference because ?tragedy? in WordNet is a type
of written communication, which is a preferred argu-
ment class of ?answer?.
Rooth et al (1999) present a fundamentally dif-
ferent approach to selectional preference induction
which uses soft clustering to form classes for general-
isation and does not take recourse to any hand-crafted
resource. We will argue in Section 6 that our model
allows more control over the generalisations made.
401
Modelling Selectional Preferences with Thematic
Roles. Pad? et al (2006) present a deeper model
for the plausibility of (v, r, a) triples that approxi-
mates the relations with thematic roles. It estimates
the selectional preferences of a verb-role pair with
a generative probability model that equates the plau-
sibility of a (v, r, a) triple with the joint probability
of seeing the thematic role with the verb-argument
pair. In addition, the model also considers the verb?s
sense s and the grammatical function gf of the ar-
gument; however, since the model is generative, it
can make predictions even when not all variables are
instantiated. The final model is shown in Equation 2.
Plausibilityv,r,a = P (v, s, r, a, gf ) (2)
The induction of this model from the FrameNet cor-
pus of semantically annotated training data (Fillmore
et al, 2003) encounters a serious sparse data prob-
lem, which is approached by the application of word-
class-based and Good-Turing re-estimation smooth-
ing. The resulting model?s plausibility predictions
are significantly correlated to human judgements, but
because of the use of verb-specific thematic roles,
the model?s coverage is still restricted by the verb
coverage of the training corpus.
Vector Space Models. Another class of models
that has found wide application in lexical semantics
is the family of vector space models. In a vector space
model, each target word is represented as a vector,
typically constructed from co-occurrence counts with
context words in a large corpus (the so-called basis
elements). The underlying assumption is that words
with similar meanings occur in similar contexts, and
will be assigned similar vectors. Thus, the distance
between the vectors of two target words, as given by
some distance measure (e.g., Cosine or Jaccard), is a
measure of their semantic similarity.
Vector space models are simple to construct, and
the semantic similarity they provide has found a wide
range of applications. Examples in NLP include in-
formation retrieval (Salton et al, 1975), automatic
thesaurus extraction (Grefenstette, 1994), and pre-
dominant sense identification (McCarthy et al, 2004).
In cognitive science, they have been used to account
for the influence of context on human lexical pro-
cessing (McDonald and Brew, 2004), and to model
lexical priming (Lowe and McDonald, 2000).
A drawback of vector space models is the diffi-
culty of interpreting what some degree of ?generic
semantic similarity? between two target words means
in linguistic terms. In particular, this similarity is
not sensitive to selectional preferences over specific
semantic relations, and thus cannot model the plau-
sibility data we are interested in. The next section
demonstrates how the integration of ideas from se-
lectional preference induction makes this distinction
possible.
3 The Vector Similarity Model:
Corpus-Based Modelling of Plausibility
3.1 Model Architecture
Our model builds on the architecture of Erk (2007). It
combines the idea underlying the selectional prefer-
ence models from Section 2, namely to predict plau-
sibility by generalising over head words, with vector
space similarity. The fundamental idea of our model
is to model the plausibility of the triple (v, r, a) by
comparing the argument head a to other headwords
a? which we have already seen in a corpus for the
same verb-relation pair (v, r), and which we there-
fore assume to be plausible. We write Seenr(v) for
the set of seen headwords. Our intuition is that if a
is similar to the words in Seenr(v), then the triple
(v, r, a) is plausible; conversely, if it is very dissimi-
lar, then the triple is implausible.
Concretely, we judge the plausibility of the triple
by averaging over the similarity of the vector for a to
all vectors for the seen headwords in Seenr(v):
Pl(v, r, a) =
?
a??Seenr(v)
w(a?) ? sim(a, a?)
|Seenr(v)|
(3)
where w is a weight factor specific to each a?. w can
be used to implement different weighting schemes
that encode prior knowledge, e.g., about the reliabil-
ity of different words in Seenr(v). In this paper, we
only consider a very simple weighting factor, namely
the frequency of the seen headwords. This encodes
the assumption that similarity to frequent head words
is more important than similarity to infrequent ones:
Pl(v, r, a) =
?
a??Seenr(v)
f(a?) ? sim(a, a?)
|Seenr(v)|
(4)
402
deer
lion
hunter
poacher
director
seen patients
of "shoot"
seen agents
of "shoot"
Figure 1: A vector space for estimating the
plausibilities of (shoot, agent, hunter) and
(shoot, patient, hunter).
This model can be seen as a straightforward imple-
mentation of the selectional preference induction pro-
cess of generalising from seen headwords to other,
similar words. By using vector space representations
to judge the similarity of words, we obtain a com-
pletely corpus-driven model that does not require any
additional resources and is very flexible. A comple-
mentary view on this model is as a generalisation of
traditional vector space models that computes simi-
larity not between two vectors, but between a vector
and a set of other vectors. By using the vectors for
seen headwords of a given relation as this set, the
similarity we compute is specific to this relation.
Example. Figure 1 shows an example vector space.
Consider v = ?shoot?, r = agent, and a = ?hunter?.
In order to judge whether a hunter is a plausible agent
of ?shoot?, the vector space representation of ?hunter?
is compared to all representations of known agents
of "shoot?, namely ?poacher? and ?director?. Due
to the nearness of the vector for ?hunter? to these
two vectors, ?hunter? will be judged a fairly good
agent of ?shoot?. Compare this with the result for the
role patient : ?hunter? is further away from ?lion? and
?deer?, and will therefore be found to be a rather bad
patient of ?shoot?. However, ?hunter? is still more
plausible as a patient of ?shoot? than e.g., ?director?.
3.2 Instantiating the Model: Unparsed
vs. Parsed Corpora
The two major tasks which need to be addressed to
obtain an instance of this model are (a), determining
the sets of seen head words Seenr(v), and (b), the
construction of a vector space. Erk (2007) extracted
the set of seen head words from corpora with se-
mantic role annotation, and used only a single vector
space representation. In this paper, we eliminate the
reliance on special annotation by considering shallow
approximations of the semantic relations in question.
In addition, we discuss in detail which properties of
the vector space are crucial for the prediction of plau-
sibility ratings, a much more fine-grained task than
the pseudo-word disambiguation task presented in
Erk (2007) that is more closely related to semantic
role labelling. The goal of our exposition is thus to
develop a model that can use more training data, and
represent the corpus information optimally in order
to obtain superior coverage.
In fact, tasks (a) and (b) can be solved on the basis
of unparsed corpora, but we would expect the results
to be rather noisy. Fortunately, the state of the art in
broad-coverage (Lin, 1993) and unsupervised (Klein
and Manning, 2004) dependency parsing allows us to
treat dependency parsing merely as a preprocessing
step. We therefore describe two instantiations of our
model: one based on an unprocessed corpus, and one
based on a dependency-based parsed corpus. By com-
paring the models, we can gauge whether syntactic
preprocessing improves model performance. In the
following, we describe the strategies the two models
adopt for (a) and (b).
Identifying seen head words for relations. Re-
call that the set Seenr(v) is supposed to contain
known head words a that are observed in the corpus
as triples (v, r, a). In a parsed corpus, we can approx-
imate the relation agent by the dependency relation
of subject provided by the parser, and the relation
patient by the dependency relation of object. In
an unparsed corpus, these grammatical relations are
unavailable, and the only straightforward evidence
we can use is word order. In this case, we assume
that words directly adjacent to the left of a predicate
are subjects, and therefore agents, whereas words
directly to its right are objects, and thus patients.
Vector space topology. The success of our method
depends directly on the topology of the vector space.
More specifically, two words should only be assigned
similar vectors if they are in fact of similar plausibil-
ity. If this is not the case, there is no guarantee that a
word a that is similar to the words in Seenr(v) forms
403
``````````````Basis elements
Target
deer hunter
shoot 10 10
escape 12 12
``````````````Basis elements
Target
deer hunter
shoot-SUBJ 0 8
shoot-OBJ 10 2
escape-SUBJ 10 5
escape-OBJ 2 7
Figure 2: Two vector spaces, using as basis elements
either context words (above) or words paired with
grammatical functions (below)
a plausible triple (v, r, a) itself (cf. Figure 1).
The topology, in turn, is related to the choice of
basis elements. Traditional vector space models use
context words as basis elements of the space. The
top table in Figure 2 illustrates our intuition that such
spaces are problematic: ?deer? and ?hunter? receive
identical vectors, even though they show complemen-
tary plausibility ratings (cf. Table 1). The reason is
that ?deer? and ?hunter? often co-occur quite closely
to one another (e.g., in the vicinity of ?shoot?), and
thus show a very similar profile in terms of context
words. In preliminary experiments, we found that vec-
tor spaces with context words as basis elements are
in fact unable to distinguish such word pairs reliably.
In contrast, the bottom table in Figure 2 indicates
that this problem can be alleviated by using context
words combined with the grammatical relation to
the target word as basis elements. Target words now
receive different representations, depending on the
grammatical function in which they occur with con-
text words. In consequence, resulting spaces can dis-
tinguish, for example, between ?hunter? and ?deer?.
We adopt word-function pairs as basis elements for
the vector spaces in all our models. In a dependency-
parsed corpus, the basis elements can be directly read
off the syntactic structure. In an unparsed corpus, we
again fall back on word order, appending to each
context word its relative position to the target word.
4 Experimental Setup
Experimental Materials. In order to make our
evaluation comparable to the earlier modelling study
by Pad? et al (2006), we present evaluations on the
two plausibility judgement datasets used there.1
The first dataset consists of 100 data points from
McRae et al (1998). Our example in Table 1, which
is taken from this dataset, demonstrates its balanced
structure: 25 verbs are paired with two arguments
and two relations each, such that each argument is
highly plausible in one relation, but implausible in
the other. The resulting distribution of ratings is thus
highly bimodal. Models can only reliably predict the
human ratings in this data set if they can capture the
difference between verb argument slots as well as as
between individual fillers.
The second, larger dataset is less strictly balanced,
since its triples are constructed on the basis of corpus
co-occurrences (Pad? et al, 2006). 18 verbs are com-
bined with the three most frequent subjects and ob-
jects from both the Penn Treebank and the FrameNet
corpus. Each verb-argument pair was rated both as
an agent and as a patient, which leads to a total of
24 rated triples per verb. The dataset contains ratings
for a total of 414 triples, due to overlap between cor-
pora. The resulting judgements show a more even
distribution of ratings than the McRae data.
Vector Similarity Models. Following our exposi-
tion in the last section, we construct two instantia-
tions of our vector similarity model, one using un-
parsed and one parsed data. Both are trained on the
complete British National Corpus (Burnard, 1995,
BNC) with more than six million sentences.
The unparsed model (Unparsed) uses the BNC
without any pre-processing. We first construct the
set of known headwords, Seenr(v), as follows: All
words up to 2 words to the left of instances of v
are assumed to be subjects, and thus agents; vice
versa for patients to the right. Then, we construct
semantic space representations for the experimental
arguments and known headwords, adopting optimal
parameter settings from the literature (Pad? and Lap-
ata, 2007). This means a context window of 5 words
to either side and 2,000 basis elements (dimensions),
which are formed by the most frequent 1,000 words
1We are grateful to Ken McRae for his dataset.
404
in the BNC, combined with each of the relations
agent and patient. All counts are log-likelihood trans-
formed (Lowe, 2001).
To construct the parsed model (Parsed), we
dependency-parsed the BNC with Minipar (Lin,
1993). We first obtain the seen headwords Seenr(v)
by using all subjects and objects of v as agents and pa-
tients, respectively. We then construct a vector space
for the experimental arguments and known head-
words.2 We use 2,000 dimensions again, but adopt the
most frequent (head , grammatical function) pairs
in the BNC as basis elements. The context window
is formed by subject and object dependencies.
All counts are log-likelihood transformed.
We experiment with two distance measures to com-
pute vector similarity, namely the Jaccard Coefficient
and Cosine Distance, both of which have been shown
to yield good performance in NLP tasks (Lee, 1999;
McDonald and Lowe, 1998).
Evaluation Procedure. We evaluate our models
by correlating the predicted plausibility values with
the human judgements, which range between 1 and
7. Since the human judgement data is not normally
distributed, we use Spearman?s ?, a non-parametric
rank-order test. We determine the statistical signif-
icance of differences in correlation strength using
the method described in Raghunathan (2003). This
method can deal with missing values and thus allows
us to compare models with different coverage.
It is difficult to specify a straightforward baseline
for our correlation-based evaluation. In contrast to
classification tasks, where models choose one out of
a fixed number of classes, our model predicts contin-
uous data. This task is more difficult to approximate,
e.g., using frequency information.
With respect to upper bounds, we hold that au-
tomatic models of plausibility cannot be expected
to surpass the typical agreement on the plausibility
judgement task between human participants. Thus,
we assume an upper bound of ? ? 0.7.
Comparison against Other Models. We compare
our performance to two models from the literature dis-
cussed in Section 2. The first model (Pado) is the the-
2This space was computed using the
DependencyVectors software described in Pad? and
Lapata (2007). This software can be downloaded from http:
//www.coli.uni-saarland.de/~pado/dv.html.
Model Coverage Spearman?s ?
Unparsed Cosine 90% 0.023, ns
Unparsed Jaccard 90% 0.044, ns
Parsed Cosine 91% 0.218, *
Parsed Jaccard 91% 0.129, ns
Resnik 94% 0.028, ns
Pado 56% 0.415, **
Table 2: Model performance on McRae data.
*: p < 0.05, **: p < 0.01
matic role-based model by Pad? et al (2006) trained
on the FrameNet (Fillmore et al, 2003) release 1.2 ex-
ample sentences, a subset of the BNC annotated with
semantic roles. This corpus contains about 57,000
sentences, which corresponds to roughly 1% of the
BNC data.
The second model (Resnik) is the WordNet-based
selectional preference model by Resnik (1996),
trained on the dependency-parsed BNC (see above).
5 Experimental Evaluation
The McRae Dataset. Table 2 summarises our re-
sults on the McRae dataset. The upper part shows
the results for our two vector similarity models
(Parsed/Unparsed), combined with the two distance
measures (Cosine/Jaccard). The lower part shows the
two resource-based models we use for comparison.
We find that all vector similarity models exhibit
high coverage (above 90%), and one model (Parsed
Cosine) can predict human judgements with a signifi-
cant correlation. The instantiation of the model has
a significant impact on the performance: The Parsed
models clearly outperform the Unparsed models. The
effect of the distance measure is less clear-cut, since
the Unparsed models perform better with Jaccard,
while the Parsed models prefer Cosine.
The deep semantic plausibility model (Pado)
makes predictions only for slightly more than half of
the data. This low coverage is a direct result of the
small overlap in verbs between the McRae dataset
and the FrameNet corpus. However, on the data
points it covers, it achieves a significant correlation
to human judgements. The correlation coefficient is
numerically much higher than that of the Parsed Co-
sine model, but due to the large coverage difference,
the two models are not statistically distinguishable.
405
Model Coverage Spearman?s ?
Unparsed Cosine 98% 0.117, *
Unparsed Jaccard 98% 0.149, **
Parsed Cosine 98% 0.479, ***
Parsed Jaccard 98% 0.120, *
Resnik 98% 0.237, ***
Pado 97% 0.515, ***
Table 3: Model performance on Pado data.
*: p < 0.05, **: p < 0.01, ***: p < 0.001
Resnik?s WordNet-based model shows a coverage
that is comparable to the vector similarity models,
but does not achieve a significant correlation to the
human judgements.
The Pado Dataset. Table 3 summarises the results
for the Pado dataset. Since all verbs in this dataset are
covered in FrameNet, the deep Pado model shows a
coverage comparable to all other models, at >95%.
The main difference to the McRae dataset lies in
the models? performance. We find that all models,
including the Unparsed vector models and Resnik,
manage to achieve significant correlations with the
human judgements. Within the vector similarity mod-
els, the same trends hold as for the McRae dataset:
Parsed outperforms Unparsed, and the best combina-
tion is Parsed Cosine. The models fall into two clearly
separated groups: The Pado and Parsed Cosine mod-
els achieve a highly significant correlation, and are
statistically indistinguishable. They significantly out-
perform the second group (p < 0.001), formed by
all other models. Within this second group, Resnik is
numerically the best model and shows a significant
correlation with human data; nevertheless, the differ-
ence to the first group is evident from its substantially
lower correlation coefficient.
The construction of the Pado dataset alows a fur-
ther analysis. As mentioned in Section 4, the dataset
consists of verb-argument pairs drawn from two dif-
ferent corpora. Therefore, each verb is combined
both with some arguments that are seen in FrameNet,
and some that are not. Our hypothesis is that the
FrameNet-trained Pado model performs consider-
ably better on the 216 ?FN-Seen? data points (verb-
argument pairs observed in FrameNet in at least one
relation) than on the 198 ?FN-Unseen? data points
(verb-argument pairs unseen in both relations).
Table 4 shows the results of this analysis for the
best-performing models. We observe a pattern corre-
sponding to our expectations: The performance of the
Pado model is clearly worse for FN-Unseen than for
FN-Seen, while the Resnik and Parsed Cosine mod-
els perform more evenly across both datasets. While
the Pado model is significantly better on the FN-Seen
dataset, it is numerically outperformed by the Parsed
Cosine model for the FN-Unseen data points. We
conclude that the deep model is more accurate within
the coverage of its resources, but loses its advantage
when it has to resort to smoothing.
Model combination. Our last analysis indicates
that the models have complementary strengths: the
thematic role-based Pado model is the best plausi-
bility predictor on the data points it has seen, while
the Parsed cosine model overall predicts human data
only numerically worse, and with better coverage.
We therefore suggest to combine the predictions of
the two models to combine their respective strengths.
For the moment, we only consider a naive backoff
scheme: For each data point, we use the prediction
of the Pado model if the data point is ?FN-Seen? (cf.
the last paragraph), and the prediction of the Parsed
Cosine model otherwise. Note that this criterion does
not consider the predictions of the models themselves,
only properties of the underlying training set.
The actual combination requires a normalisation
of the respective predictions, since one of the models
(Pado) is probabilistic, while the other one (Parsed
Cosine) is similarity-based, and their predictions are
not directly comparable. We perform a simple nor-
malisation by z-transforming the complete predic-
tions of each model.3 The combination of the scaled
predictions in fact results in an improved correlation
with the human data. The correlation coefficient of
?=0.552 numerically exceeds either base model, and
the coverage of 98% corresponds to the coverage of
the more robust Parsed Cosine model.
We take this result as evidence that even a simple
combination technique can lead to improved predic-
tions. Unfortunately, our naive backoff scheme does
not directly carry over to the McRae dataset, where
only 2 out of 100 data points are ?FN-Seen?, and the
Pado model would thus hardly contribute.
3The z transformation scales a dataset to a mean of 0 and a
standard deviation of 1.
406
Model FN-Seen Data FN-Unseen Data
Parsed Cosine 94% 0.426, *** 100% 0.461, ***
Resnik 96% 0.217, ** 100% 0.263, ***
Pado 97% 0.569, *** 96% 0.383, ***
Table 4: Performance on data points seen and unseen in FrameNet (Pado dataset). **: p < 0.01 ***: p < 0.001
Discussion. We have verified experimentally that
our vector similarity model is able to match the per-
formance of a deep plausibility model, exceeding it
in coverage, and to outperform a WordNet-based se-
lectional preference model. We conclude that a com-
pletely corpus-driven approach constitutes a viable
alternative to resource-based models.
One insight from our experiments is that vec-
tor similarity models constructed from dependency-
parsed corpora perform significantly better than un-
parsed models. This indicates that dependency rela-
tions like subject and object are reliable syntac-
tic correlates of semantic relations like agent and pa-
tient, but that their approximation in terms of word or-
der introduces considerable noise. The Parsed models
are best combined with Cosine Distance. We surmise
that Cosine, which tends to consider low-frequency
words more than Jaccard, is more susceptible to the
additional noise in unparsed corpora.
Furthermore, the choice of basis elements for the
vector space is vital: Plausibilities could only be pre-
dicted successfully with word-relation pairs as basis
elements. This is in contrast to recent results on pre-
dominant sense acquisition, the task of identifying
the most frequent sense for a given word in an un-
supervised manner (McCarthy et al, 2004). On that
task, Pad? and Lapata (2007) found vector spaces
with words as basis elements are in fact competitive
with models using word-relation pairs. This diver-
gence underlines an interesting difference between
the two tasks. Evidently, predominant senses identi-
fication, as a WSD-related task, can succeed on the
basis of topical information, which is represented
well in word-based spaces. In contrast, plausibility
judgments can only be predicted by a space based
on word-relation pairs which can represent the finer-
grained distinctions arising from different relations
between verb and noun.
A second important finding is that the relative per-
formance of the different models is the same on the
McRae and Pado datasets. The Pado model performs
best, followed by our Parsed Cosine vector similarity
model, followed by the Unparsed and Resnik models.
The McRae dataset, however, is much more diffi-
cult to account for than the Pado data, independent of
the model. This effect was already noted by Pad? et
al. (2006), who attributed it to the very limited over-
lap between the McRae dataset and FrameNet. While
this explanation can account for the difference for the
Pado model, we observe the same pattern across all
models. This suggests that a more general frequency
effect is at work here: The median frequency of the
hand-selected McRae nouns is 1,356 in the BNC, as
opposed to 8,184 for the corpus-derived Pado nouns.
The resulting sparseness affects all model families,
since all ultimately rely on co-occurrences.
The performance difference between the two
datasets is particularly large for the WordNet-based
selectional preference model (Resnik). A further
analysis of the model?s predictions shows that
the model has difficulty in distinguishing between
verb-relation-argument triples that differ only in
the argument, such as (shoot, agent, hunter) and
(shoot, agent, deer). Recall that it is crucial for the
prediction of the McRae data to make this distinc-
tion, since the arguments for each relation are cho-
sen to differ widely in plausibility. The reason for
the Resnik model?s difficulty is that arguments are
mapped onto WordNet synsets, and whenever two
arguments are mapped onto closely related synsets,
their plausibility ratings are similar. This problem is
graver for the McRae test set, where all arguments are
animates, and thus more similar in terms of WordNet,
than for the Pado set, which also contains a portion of
inanimate arguments with animate counterparts. This
analysis highlights again the fundamental problem
of resource-based models, where design decisions of
the underlying resource may limit, or even mislead,
the models? generalisations.
Finally, we have shown in a first experiment that
407
the syntax-based vector similarity model can be com-
bined with the role-base model to obtain a combined
model that performs superior to both. In this com-
bined model, the shallowmodel?s better coverage sup-
plements the accurate predictions of the deep model.
6 Conclusions
In this paper, we have considered the computational
modelling of human plausibility judgements for verb-
relation-argument triples, a task equivalent to the
computation of selectional preferences. We have ex-
tended a recent proposal (Erk, 2007) which com-
bines ideas from selectional preference induction and
vector space models. Our model can be constructed
from a large corpus with partial syntactic information
(specifically, subject and object relations) from which
it builds an optimally informative vector space.
We have demonstrated that the successful evalua-
tion of the model in Erk (2007) on the coarse-grained
pseudo-word disambiguation task carries over to the
prediction of human plausibility judgments which re-
quires relatively fine-grained, relation-based distinc-
tions. Our model is competitive with existing ?deep?
models while exhibiting a higher coverage. We have
also shown that our vector similarity model can be
combined with a ?deep? model so that the combined
model outperforms both base models. A thorough
investigation of strategies for prediction combination
and scaling remains future work.
The strategy of our model to derive generalisations
directly from corpus data, without recourse to re-
sources, is similar to another family of corpus-driven
selectional preference models, namely EM-based
clustering models (Rooth et al, 1999). However, we
believe that our model has a number of advantages.
(1), It is conceptually simple and implements the
intuition behind selectional preference models, ?gen-
eralise from known headwords to unknown ones?,
particularly directly through the comparison of new
headwords to known ones according to a given defini-
tion of similarity. (2), The separation of the similarity
computation and the acquisition of seen headwords
gives the experimenter fine-grained control over the
types and sources of information which inform the
construction of the model. (3), The instantiation of
the similarity computation with a vector space makes
it possible to integrate additional linguistic informa-
tion beyond verb-argument co-occurrences into the
model, building on a large body of work in vector
space construction. In sum, our modular model pro-
vides a higher degree of control than one-step models
like the EM-based proposal.
An important avenue of further research is the
ability of the vector plausibility model to model finer-
grained distinctions between semantic relations be-
yond the agent/patient dichotomy, as thematic role-
based models are able to. Excluding the direct use of
role-annotated corpora like FrameNet for coverage
reasons, the most promising strategy is to extend our
present scheme of approximating semantic relations
by grammatical realisations. How much noise this
approximation introduces when finer role sets are
used is an open research question.
Acknowledgments. The work presented in this pa-
per was supported by the financial support of DFG
(grants Pi-154/9-2 and IRTG ?Language Technology
and Cognitive Systems?).
References
Naoki Abe and Hang Li. 1996. Learning word associa-
tion norms using tree cut pair models. In Proceedings
of ICML 1996, pages 3?11.
Lou Burnard, 1995. User?s guide for the British National
Corpus. British National Corpus Consortium, Oxford
University Computing Services.
Stephen Clark and David Weir. 2002. Class-based prob-
ability estimation using a semantic hierarchy. Compu-
tational Linguistics, 28(2):187?206.
Katrin Erk. 2007. A simple, similarity-based model for
selectional preferences. In Proceedings of the 45th
ACL, Prague, Czech Republic.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to FrameNet.
International Journal of Lexicography, 16:235?250.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers.
Donald Hindle and Mats Rooth. 1993. Structural ambi-
guity and lexical relations. Computational Linguistics,
19(1):103?120.
408
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the 42th
ACL, pages 478?485, Barcelona, Spain.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th ACL, pages 25?32, College
Park, MA.
Dekang Lin. 1993. Principle-based parsing without over-
generation. In Proceedings of the 31st ACL, pages
112?120, Columbus, OH.
Will Lowe and Scott McDonald. 2000. The direct route:
Mediated priming in semantic space. In Proceedings
of the 22nd CogSci, pages 675?680, Philadelphia, PA.
Will Lowe. 2001. Towards a theory of semantic space.
In Proceedings of the 23rd CogSci, pages 576?581, Ed-
inburgh, UK.
Diana McCarthy and John Carroll. 2003. Disambiguat-
ing nouns, verbs and adjectives using automatically ac-
quired selectional preferences. Computatinal Linguis-
tics, 29(4):639?654.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42th ACL, pages
279?286, Barcelona, Spain.
Scott McDonald and Chris Brew. 2004. A distributional
model of semantic context effects in lexical process-
ing. In Proceedings of the 42th ACL, pages 17?24,
Barcelona, Spain.
Scott McDonald and Will Lowe. 1998. Modelling func-
tional priming and the associative boost. In Proceed-
ings of the 20th CogSci, pages 675?680, Madison, WI.
Ken McRae, Michael Spivey-Knowlton, and Michael
Tanenhaus. 1998. Modeling the influence of thematic
fit (and other constraints) in on-line sentence compre-
hension. Journal of Memory and Language, 38:283?
312.
Sebastian Pad? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2).
Ulrike Pad?, Frank Keller, and Matthew W. Crocker.
2006. Combining syntax and thematic fit in a proba-
bilistic model of sentence processing. In Proceedings
of the 28th CogSci, pages 657?662, Vancouver, BC.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Tim-
othy Chklovski, and Eduard Hovy. 2007. ISP: Learn-
ing inferential selectional preferences. In Proceedings
of NAACL 2007, Rochester, NY.
Trivellore Raghunathan. 2003. An approximate test for
homogeneity of correlated correlations. Quality and
Quantity, 37:99?110.
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational re-
alization. Cognition, 61:127?159.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing an semanti-
cally annotated lexicon via EM-based clustering. In
Proceedings of the 37th ACL, pages 104?111, College
Park, MA.
Gerard Salton, Anita Wong, and Chung-Shu Yang. 1975.
A vector-space model for information retrieval. Jour-
nal of the American Society for Information Science,
18:613?620.
John Trueswell, Michael Tanenhaus, and Susan Garnsey.
1994. Semantic influences on parsing: Use of the-
matic role information in syntactic ambiguity resolu-
tion. Journal of Memory and Language, 33:285?318.
409
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 897?906,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
A Structured Vector Space Model for Word Meaning in Context
Katrin Erk
Department of Linguistics
University of Texas at Austin
katrin.erk@mail.utexas.edu
Sebastian Pado?
Department of Linguistics
Stanford University
pado@stanford.edu
Abstract
We address the task of computing vector space
representations for the meaning of word oc-
currences, which can vary widely according to
context. This task is a crucial step towards a
robust, vector-based compositional account of
sentence meaning. We argue that existing mod-
els for this task do not take syntactic structure
sufficiently into account.
We present a novel structured vector space
model that addresses these issues by incorpo-
rating the selectional preferences for words?
argument positions. This makes it possible to
integrate syntax into the computation of word
meaning in context. In addition, the model per-
forms at and above the state of the art for mod-
eling the contextual adequacy of paraphrases.
1 Introduction
Semantic spaces are a popular framework for the rep-
resentation of word meaning, encoding the meaning
of lemmas as high-dimensional vectors. In the de-
fault case, the components of these vectors measure
the co-occurrence of the lemma with context features
over a large corpus. These vectors are able to pro-
vide a robust model of semantic similarity that has
been used in NLP (Salton et al, 1975; McCarthy and
Carroll, 2003; Manning et al, 2008) and to model
experimental results in cognitive science (Landauer
and Dumais, 1997; McDonald and Ramscar, 2001).
Semantic spaces are attractive because they provide a
model of word meaning that is independent of dictio-
nary senses and their much-discussed problems (Kil-
garriff, 1997; McCarthy and Navigli, 2007).
In a default semantic space as described above,
each vector represents one lemma, averaging over
all its possible usages (Landauer and Dumais, 1997;
Lund and Burgess, 1996). Since the meaning of
words can vary substantially between occurrences
(e.g., for polysemous words), the next necessary step
is to characterize the meaning of individual words in
context.
There have been several approaches in the liter-
ature (Smolensky, 1990; Schu?tze, 1998; Kintsch,
2001; McDonald and Brew, 2004; Mitchell and La-
pata, 2008) that compute meaning in context from
lemma vectors. Most of these studies phrase the prob-
lem as one of vector composition: The meaning of a
target occurrence a in context b is a single new vector
c that is a function (for example, the centroid) of the
vectors: c = a b.
The context b can consist of as little as one word,
as shown in Example (1). In (1a), the meaning of
catch combined with ball is similar to grab, while in
(1b), combined with disease, it can be paraphrased
by contract. Conversely, verbs can influence the in-
terpretation of nouns: In (1a), ball is understood as a
spherical object, and in (1c) as a dancing event.
(1) a. catch a ball
b. catch a disease
c. attend a ball
In this paper, we argue that models of word mean-
ing relying on this procedure of vector composition
are limited both in their scope and scalability. The
underlying shortcoming is a failure to consider syntax
in two important ways.
The syntactic relation is ignored. The first problem
concerns the manner of vector composition, which
ignores the relation between the target a and its con-
text b. This relation can have a decisive influence on
their interpretation, as Example (2) shows:
897
(2) a. a horse draws
b. draw a horse
In (2a), the meaning of the verb draw can be para-
phrased as pull, while in (2b) it is similar to sketch.
This difference in meaning is due to the difference in
relation: in (2a), horse is the subject, while in (2b)
it is the object. On the modeling side, however, a
vector combination function that ignores the relation
will assign the same representation to (2a) and (2b).
Thus, existing models are systematically unable to
capture this class of phenomena.
Single vectors are too weak to represent phrases.
The second problem arises in the context of the im-
portant open question of how semantic spaces can
?scale up? to provide interesting meaning representa-
tions for entire sentences. We believe that the current
vector composition methods, which result in a single
vector c, are not informative enough for this purpose.
One proposal for ?scaling up? is to straightforwardly
interpret c = a  b as the meaning of the phrase
a + b (Kintsch, 2001; Mitchell and Lapata, 2008).
The problem is that the vector c can only encode a
fixed amount of structural information if its dimen-
sionality is fixed, but there is no upper limit on sen-
tence length, and hence on the amount of structure
to be encoded. It is difficult to conceive how c could
encode deeper semantic properties, like predicate-
argument structure (distinguishing ?dog bites man?
and ?man bites dog?), that are crucial for sentence-
level semantic tasks such as the recognition of textual
entailment (Dagan et al, 2006). An alternative ap-
proach to sentence meaning would be to use the vec-
tor space representation only for representing word
meaning, and to represent sentence structure sepa-
rately. Unfortunately, present models cannot provide
this grounding either, since they compute a single
vector c that provides the same representations for
both the meanings of a and b in context.
In this paper, we propose a new, structured vector
space model for word meaning (SVS) that addresses
these problems. A SVS representation of a lemma
comprises several vectors representing the word?s
lexical meaning as well as the selectional preferences
that it has for its argument positions. The meaning
of word a in context b is computed by combining a
with b?s selectional preference vector specific to the
relation between a and b, addressing the first problem
above. In an expression a + b, the meanings of a
and b in this context are computed as two separate
vectors a? and b?. These vectors can then be combined
with a representation of the structure?s expression
(e.g., a parse tree), to address the second problem
discussed above. We test the SVS model on the task
of recognizing contextually appropriate paraphrases,
finding that SVS performs at and above the state-of-
the-art.
Plan of the paper. Section 2 reviews related work.
Section 3 presents the SVS model for word meaning
in context. Sections 4 to 6 relate experiments on the
paraphrase appropriateness task.
2 Related Work
In this section we give a short overview over existing
vector space based approaches to computing word
meaning in context.
General context effects. The first category of
models aims at integrating the widest possible range
of context information without recourse to linguistic
structure. The best-known work in this category is
Schu?tze (1998). He first computes ?first-order? vec-
tor representations for word meaning by collecting
co-occurrence counts from the entire corpus. Then,
he determines ?second-order? vectors for individual
word instances in their context, which is taken to be a
simple surface window, by summing up all first-order
vectors of the words in this context. The resulting
vectors form sense clusters.
McDonald and Brew (2004) present a similar
model. They compute the expectation for a word
wi in a sequence by summing the first-order vectors
for the words w1 to wi?1 and showed that the dis-
tance between expectation and first-order vector for
wi correlates with human reading times.
Predicate-argument combination. The second
category of prior studies concentrates on contexts
consisting of a single word only, typically modeling
the combination of a predicate p and an argument a.
Kintsch (2001) uses vector representations of p and
a to identify the set of words that are similar to both
p and a. After this set has been narrowed down in a
self-inhibitory network, the meaning of the predicate-
argument combination is obtained by computing the
898
centroid of its members? vectors. The procedure does
not take the relation between p and a into account.
Mitchell and Lapata (2008) propose a framework
to represent the meaning of the combination p+ a as
a function f operating on four components:
c = f(p, a,R,K) (3)
R is the relation holding between p and a, and K
additional knowledge. This framework allows sen-
sitivity to the relation. However, the concrete in-
stantiations that Mitchell and Lapata consider disre-
gards K and R, thus sharing the other models? limi-
tations. They focus instead on methods for the direct
combination of p and a: In a comparison between
component-wise addition and multiplication of p and
a, they find far superior results for the multiplication
approach.
Tensor product-basedmodels. Smolensky (1990)
uses tensor product to combine two word vectors a
and b into a vector c representing the expression a+b.
The vector c is located in a very high-dimensional
space and is thus capable of encoding the structure
of the expression; however, this makes the model
infeasible in practice, as dimensionality rises with
every word added to the representation. Jones and
Mewhort (2007) represent lemma meaning by using
circular convolution to encode n-gram co-occurrence
information into vectors of fixed dimensionality. Sim-
ilar to Brew and McDonald (2004), they predict most
likely next words in a sequence, without taking syn-
tax into account.
Kernel methods. One of the main tests for the
quality of models of word meaning in context is the
ability to predict the appropriateness of paraphrases
in given a context. Typically, a paraphrase applies
only to some senses of a word, not all, as can be seen
in the paraphrases ?grab? and ?contract? of ?catch?.
Vector space models generally predict paraphrase ap-
propriateness based on the similarity between vectors.
This task can also be addressed with kernel methods,
which project items into an implicit feature space
for efficient similarity computation. Consequently,
vector space methods and kernel methods have both
been used for NLP tasks based on similarity, no-
tably Information Retrieval and Textual Entailment.
Nevertheless, they place their emphasis on different
types of information. Current kernels are mostly tree
kernels that compare syntactic structure, and use se-
mantic information mostly for smoothing syntactic
similarity (Moschitti and Quarteroni, 2008). In con-
trast, vector-space models focus on the interaction
between the lexical meaning of words in composi-
tion.
3 A structured vector space model for
word meaning in context
In this section, we define the structured vector space
(SVS) model of word meaning.
The main intuition behind our model is to view
the interpretation of a word in context as guided by
expectations about typical events. For example, in
(1a), we assume that upon hearing the phrase ?catch a
ball?, the hearer will interpret the meaning of ?catch?
to match typical actions that can be performed with a
ball. Similarly, the interpretation of ?ball? will reflect
the hearer?s expectations about typical things that can
be caught. This move to include typical arguments
and predicates into a model of word meaning can be
motivated both on cognitive and linguistic grounds.
In cognitive science, the central role of expecta-
tions about typical events for human language pro-
cessing is well-established. Expectations affect read-
ing times (McRae et al, 1998), the interpretation of
participles (Ferretti et al, 2003), and sentence pro-
cessing generally (Narayanan and Jurafsky, 2002;
Pado? et al, 2006). Expectations exist both for verbs
and nouns (McRae et al, 1998; McRae et al, 2005).
In linguistics, expectations, in the form of selec-
tional restrictions and selectional preferences, have
long been used in semantic theories (Katz and Fodor,
1964; Wilks, 1975), and more recently induced
from corpora (Resnik, 1996; Brockmann and Lapata,
2003). Attention has mostly been limited to selec-
tional preferences of verbs, which have been used
for example for syntactic disambiguation (Hindle
and Rooth, 1993), word sense disambiguation (Mc-
Carthy and Carroll, 2003) and semantic role label-
ing (Gildea and Jurafsky, 2002). Recently, a vector-
spaced model of selectional preferences has been
proposed that computes the typicality of an argument
simply through similarity to previously seen argu-
ments (Erk, 2007; Pado? et al, 2007).
We first present the SVS model of word meaning
899
catch
he
fielder
dog
cold
baseball
drift
objsubj
accuse
say
claim
comp
-1
ball
whirl
fly
provide
throw
catch
organise
obj
-1
subj
-1
mod
red
golf
elegant
Figure 1: Structured meaning representations for noun
ball and verb catch : lexical information plus expectations
that integrates lexical information with selectional
preferences. Then, we show how the SVS model pro-
vides a new way of computing meaning in context.
Representing lemma meaning. We abandon the
traditional choice of representing word meaning as
a single vector. Instead, we encode each word as
a combination of (a) one vector that models the
lexical meaning of the word, and (b) a set of vec-
tors, each of which represents the semantic expecta-
tions/selectional preferences for one particular rela-
tion that the word supports.1
The idea is illustrated in Fig. 1. In the representa-
tion of the verb catch, the central square stands for
the lexical vector of catch itself. The three arrows
link it to catch ?s preferences for its subjects (subj),
its objects (obj), and for verbs for which it appears
as a complement (comp?1). The figure shows the se-
lectional preferences as word lists for readability; in
practice, each selectional preference is a single vector
(cf. Section 4). Likewise, ball is represented by one
vector for ball itself, one for ball ?s preferences for its
modifiers (mod), one vector for the verbs of which it
is a subject (subj?1), and one for the verbs of which
is an object (obj?1).
This representation includes selectional prefer-
ences (like subj, obj, mod) exactly parallel to
inverse selectional preferences (subj?1, obj?1,
comp?1). To our knowledge, preferences of the lat-
ter kind have not been studied in computational lin-
guistics. However, their existence is supported in
psycholinguistics by priming effects from nouns to
typical verbs (McRae et al, 2005).
Formally, let D be a vector space (the set of possi-
1We do not commit to a particular set of relations; see the
discussion at the end of this section.
catch
...
cold
baseball
drift
obj
subj
...
comp
-1
ball
...
throw
catch
organise
obj
-1
subj
-1
mod
...
!
!
Figure 2: Combining predicate and argument via relation-
specific semantic expectations
ble vectors), and let R be some set of relation labels.
In the structured vector space (SVS) model, we rep-
resent the meaning of a lemma w as a triple
w = (v,R,R?1)
where v ? D is a lexical vector describing the word
w itself, R : R ? D maps each relation label onto
a vector that describes w?s selectional preferences,
and R?1 : R ? D maps from role labels to vec-
tors describing inverse selectional preferences of w.
Both R and R?1 are partial functions. For example,
the direct object preference would be undefined for
intransitive verbs.
Computing meaning in context. The SVS model
of lemma meaning permits us to compute the mean-
ing of a word a in the context of another word b
in a new way, via their selectional preferences. Let
(va, Ra, R?1a ) and (vb, Rb, R
?1
b ) be the representa-
tions of the two words, and let r ? R be the relation
linking a to b. Then, we define the meaning of a and
b in this context as a pair (a?, b?) of vectors, where
a? is the meaning of a in the context of b, and b? the
meaning of b in the context of a:
a? =
(
va R
?1
b (r), Ra ? {r}, R
?1
a
)
b? =
(
vb Ra(r), Rb, R
?1
b ? {r}
) (4)
where v1 v2 is a direct vector combination function
as in traditional models, e.g. addition or component-
wise multiplication. If either Ra(r) or R
?1
b (r) are
not defined, the combination fails. Afterwards, the ar-
gument position r is considered filled, and is deleted
from Ra and R
?1
b .
900
Figure 2 illustrates this procedure on the represen-
tations from Figure 1. The dotted lines indicate that
the lexical vector for catch is combined with the in-
verse object preference of ball. Likewise, the lexical
vector for ball is combined with the object preference
vector of catch.
Note that our procedure for computing meaning
in context can be expressed within the framework of
Mitchell and Lapata (Eq. (3)). We can encode the
expectations of a and b as additional knowledge K.
The combined representation c is the pair (a?, b?) that
is computed according to our model (Eq. (4)).
The SVS scheme we have proposed incorporates
syntactic information in a more general manner than
previous models, and thus addresses the issues we
have discussed in Section 1. Since the representation
retains individual selectional preferences for all rela-
tions, combining the same words through different
relations can (and will in general) result in different
adapted representations. For instance, in the case of
Example (2), we would expect the inverse subject
preference of horse (?things that a horse typically
does?) to push the lexical vector of draw into the di-
rection of pulling, while its inverse object preference
(?things that are done to horses?) suggest a different
interpretation.
Rather than yielding a single, joint vector for the
whole expression, our procedure for computing mean-
ing in context results in one context-adapted meaning
representation per word, similar to the output of a
WSD system. As a consequence, our model can
be combined with any formalism representing the
structure of an expression. (The formalism used then
determines the set R of relations.) For example, com-
bining SVS with a dependency tree would yield a tree
in which each node is labeled by a SVS tuple that
represents the word?s meaning in context.
4 Experimental setup
This section provides the background to the following
experimental evaluation of SVS, including parameters
used for computing the SVS representations that will
be used in the experiments.
4.1 Experimental rationale
In this paper, we evaluate the SVS model against the
task of predicting, given a predicate-argument pair,
how appropriate a paraphrase (of either the predicate
or the argument) is in that context. We perform two
experiments that both use the paraphrase task, but
differ in their emphasis. Experiment 1 replicates an
existing evaluation against human judgments. This
evaluation uses synthetic dataset, limited to one par-
ticular construction, and constructed to provide max-
imally distinct paraphrase candidates. Experiment 2
considers a broader class of constructions along with
annotator-generated paraphrase candidates that are
not screened for distinctness. In both experiments,
we compare the SVS model against the state-of-the-
art model by Mitchell and Lapata 2008 (henceforth
M&L; cf. Sec. 2 for model details).
4.2 Parameter choices
Vector space. In our parameterization of the vector
space, we largely follow M&L because their model
has been rigorously evaluated and found to outper-
form a range of other models.
Our first space is a traditional ?bag-of-words? vec-
tor space (BOW, (Lund and Burgess, 1996)). For
each pair of a target word and context word, the BOW
space records a function of their co-occurrence fre-
quency within a surface window of size 10. The
space is constructed from the British National Cor-
pus (BNC), and uses the 2,000 most frequent context
words as dimensions.
We also consider a ?dependency-based? vector
space (SYN, (Pado? and Lapata, 2007)). In this space,
target and context words have to be linked by a ?valid?
dependency path in a dependency graph to count as
co-occurring.2 This space was built from BNC de-
pendency parses obtained from Minipar (Lin, 1993).
For both spaces, we used pre-experiments to com-
pare two methods for the computation of vector com-
ponents, namely raw co-occurrence counts, the stan-
dard model, and the pointwise mutual information
(PMI) definition employed by M&L.
Selectional preferences. We use a simple,
knowledge-lean representation for selectional
preferences inspired by Erk (2007), who models
selectional preference through similarity to seen filler
vectors ~va: We compute the selectional preference
vector for word b and relation r as the weighted
2More specifically, we used the minimal context specification
and plain weight function. See Pado? and Lapata (2007).
901
centroid of seen filler vectors ~va. We collect seen
fillers from the Minipar-parse of the BNC.
Let f(a, r, b) denote the frequency of a occurring
in relation r to b in the parsed BNC, then
Rb(r)SELPREF =
?
a:f(a,r,b)>0
f(a, r, b) ? ~va (5)
We call this base model SELPREF. We will also
study two variants of SELPREF, based on two dif-
ferent hypotheses about what properties of the se-
lectional preferences are particularly important for
meaning adaption. The first model aims specifically
at alleviating noise introduced by infrequent fillers, a
common problem in data-driven approaches. It only
uses fillers seen more often than a threshold ?. We
call this model SELPREF-CUT:
Rb(r)SELPREF-CUT =
?
a:f(a,r,b)>?
f(a, r, b) ? ~va (6)
Our second variant again aims at alleviating noise,
but noise introduced by low-valued dimensions rather
than infrequent fillers. It achieves this by taking each
component of the selectional preference vector to
the nth power. In this manner, dimensions with high
counts are further inflated, while dimensions with low
counts are depressed.3 This model, SELPREF-POW, is
defined as follows: If Rb(r)SELPREF = ?v1, . . . , vm?,
Rb(r)SELPREF-POW = ?v
n
1 , . . . , v
n
m? (7)
The inverse selectional preferences R?1b are de-
fined analogously for all three model variants. We
instantiate the vector combination function  as
component-wise multiplication, following M&L.
Baselines and significance testing. All tasks that
we consider below involve judgments for the mean-
ing of a word a in the context of a word b. A first
baseline that every model must beat is simply using
the original vector for a. We call this baseline ?target
only?. Since we assume that the selectional prefer-
ences of b model the expectations for a, we use b?s
selectional preference vector for the given relation as
a second baseline, ?selpref only?.
3Since we focus on the size-invariant cosine similarity, the
use of this model does not require normalization.
verb subject landmark sim judgment
slump shoulder slouch high 7
slump shoulder decline low 2
slump value slouch low 3
slump value decline high 7
Figure 3: Experiment 1: Human similarity judgements for
subject-verb pair with high- and low-similarity landmarks
Differences between the performance of mod-
els were tested for significance using a stratified
shuffling-based randomization test (Yeh, 2000).4.
5 Exp. 1: Predicting similarity ratings
In our first experiment, we attempt to predict human
similarity judgments. This experiment is a replication
of the evaluation of M&L on their dataset5.
Dataset. The M&L dataset comprises a total of
3,600 human similarity judgements for 120 experi-
mental items. Each item, as shown in Figure 3, con-
sists of an intransitive verb and a subject noun that
are combined with a ?landmark?, a synonym of the
verb that is chosen to be either similar or dissimilar
to the verb in the context of the given subject.
The dataset was constructed by extracting pairs
of subjects and intransitive verbs from a parsed ver-
sion of the BNC. Each item was paired with two
landmarks, chosen to be as dissimilar as possible ac-
cording to a WordNet similarity measure. All nouns
and verbs were subjected to a pretest, where only
those with highly significant variations in human
judgments across landmarks were retained.
For each item of the final dataset, judgements on
a 7-point scale were elicited. For example, judges
considered the compatible landmark ?slouch? to be
much more similar to ?shoulder slumps? than the
incompatible landmark ?decline?. In Figure 3, the
column sim shows whether the experiment designers
considered the respective landmark to have high or
low similarity to the verb, and the column judgment
shows a participant?s judgments.
Experimental procedure. We used cosine to com-
pute similarity to the lexical vector of the landmark.
4The software is available at http://www.nlpado.de/
?sebastian/sigf.html.
5We thank J. Mitchell and M. Lapata for providing their data.
902
Model high low ?
BOW space
Target only 0.32 0.32 0.0
Selpref only 0.46 0.4 0.06**
M&L 0.25 0.15 0.20**
SELPREF 0.32 0.26 0.12**
SELPREF-CUT, ?=10 0.31 0.24 0.11**
SELPREF-POW, n=20 0.11 0.03 0.27**
Upper bound ? ? 0.4
SYN space
Target only 0.2 0.2 0.08**
Selpref only 0.27 0.21 0.16**
M&L 0.13 0.06 0.24**
SELPREF 0.22 0.16 0.13**
SELPREF-CUT, ?=10 0.2 0.13 0.13**
SELPREF-POW, n=30 0.08 0.04 0.22**
Upper bound ? ? 0.4
Table 1: Experiment 1: Mean cosine similarity for items
with high- and low-similarity landmarks; correlation with
human judgements (?). (**: p < 0.01)
?Target only? compares the landmark against the lexi-
cal vector of the verb, and ?selpref only? compares
it to the noun?s subj?1 preference. For the M&L
model, the comparison is to the combined lexical
vectors of verb and noun. For our models SELPREF,
SELPREF-CUT and SELPREF-POW, we combine the
verb?s lexical vector with the subj?1 preference of
the noun. We used a held-out dataset of 10% of the
data to optimize the parameters of ? of SELPREF-CUT
and n of SELPREF-POW. Vectors with PMI compo-
nents could model the data, while raw frequency
components could not; we report only the former.
We use the same two evaluation scores as M&L:
The first score is the average similarity to compatible
landmarks (high) and incompatible landmarks (low).
The second is Spearman?s ?, a nonparametric corre-
lation coefficient. We compute ? between individual
human similarity scores and our predictions. Based
on agreement between human judges, M&L estimate
an upper bound ? of 0.4 for the dataset.
Results and discussion. Table 1 shows the results
of Exp. 1 on the test set. In the upper half (BOW), we
replicate M&L?s main finding that simple component-
wise multiplication of the predicate and argument
vectors results in a highly significant correlation of
Model lex. vector obj?1 selpref
SELPREF 0.23 (0.09) 0.88 (0.07)
SELPREF-CUT (10) 0.20 (0.10) 0.72 (0.18)
SELPREF-POW (30) 0.03 (0.08) 0.52 (0.48)
Table 2: Experiment 1: Average similarity (and standard
deviation) between the inverse subject preferences of a
noun and (left) its lexical vector and (right) inverse object
preferences vector (cosine similarity in SYN space)
? = 0.2, significantly outperforming both baselines.
It is interesting, though, that the subj?1 preference
itself (?Selpref only?) is already highly significantly
correlated with the human judgments.
A comparison of the upper half (BOW) with the
lower half (SYN) shows that the dependency-based
space generally shows better correlation with human
judgements. This corresponds to a beneficial effect of
syntactic information found for other applications of
semantic spaces (Lin, 1998; Pado? and Lapata, 2007).
All instances of the SELPREF model show highly
significant correlations. SELPREF and SELPREF-CUT
show very similar performance. They do better than
both baselines in the BOW space; however, in the
cleaner SYN space, their performance is numerically
lower than using selectional preferences only (? =
0.13 vs. 0.16). SELPREF-POW is always significantly
better than SELPREF and SELPREF-CUT, and shows
the best result of all tested models (? = 0.27, BOW
space). The performance is somewhat lower in the
SYN space (? = 0.22). However, this difference, and
the difference to the best M&L model at ? = 0.24,
are not statistically significant.
The SVS model computes meaning in context by
combining a word?s lexical representation with the
preference vector of its context. In this, it differs from
previous models, including that by M&L, which used
what we have been calling ?direct combination?. So
it is important to ask to what extent this difference
in method translate to a difference in predictions.
We analyzed this by measuring the similarity by the
nouns? lexical vectors, used by direct combination
methods, and their inverse subject preferences, which
SVS uses. The result is shown in the first column
in Table 2, computed as mean cosine similarities
and standard deviations between noun vectors and
selectional preferences. The table shows that these
vectors have generally low similarity, which is further
903
reduced by applying cutoff and potentiation. Thus,
the predictions of SVS will differ from those of direct
combination models like M&L.
A related question is whether syntax-aware vec-
tor combination makes a difference: Does the model
encode different expectations for different syntactic
relations (cf. Example 2)? The second column of Ta-
ble 2 explores this question by comparing inverse se-
lectional preferences for the subject and object slots.
We observe that the similarity is very high for raw
preferences, but becomes lower when noise is elim-
inated. Since the SELPREF-POW model performed
best in our evaluation, we read this as evidence that
potentiation helps to suppress noise introduced by
mis-identified subject and object fillers.
In Experiment 1, all experimental items were
verbs, which means that all disambiguation was done
through inverse selectional preferences. As inverse
selectional preferences are currently largely unex-
plored, it is interesting to note that the evidence that
they provide for the paraphrase task is as strong as
that of the context nouns themselves.
6 Exp. 2: Ranking paraphrases
This section reports on a second, more NLP-oriented
experiment whose task is to distinguish between ap-
propriate and inappropriate paraphrases on a broader
range of constructions.
Dataset. For this experiment, we use the SemEval-
1 lexical substitution (lexsub) dataset (McCarthy and
Navigli, 2007), which contains 10 instances each of
200 target words in sentential contexts, drawn from
Sharoff?s (2006) English Internet Corpus. Contex-
tually appropriate paraphrases for each instance of
each target word were elicited from up to 6 partic-
ipants. Fig. 4 shows two instances for the verb to
work. The distribution over paraphrases can be seen
as a characterization of the target word?s meaning in
each context.
Experimental procedure. In this paper, we pre-
dict appropriate paraphrases solely on the basis of a
single context word that stands in a direct predicate-
argument relation to the target word. We extracted
all instances from the lexsub test data with such a
relation. After parsing all sentences with verbal and
nominal targets with Minipar, this resulted in three
Sentence Substitutes
By asking people who work
there, I have since determined
that he didn?t. (# 2002)
be employed 4;
labour 1
Remember how hard your an-
cestors worked. (# 2005)
toil 4; labour 3;
task 1
Figure 4: Lexical substitution example items for ?work?
sets of sentences: (a), target intransitive verbs with
noun subjects (V-SUBJ, 48 sentences); (b), target tran-
sitive verbs with noun objects (V-OBJ, 213 sent.); and
(c), target nouns occurring as objects of verbs (N-OBJ,
102 sent.).6 Note that since we use only part of the
lexical substitution dataset in this experiment, a di-
rect comparison with results from the SemEval task
is not possible.
As in the original SemEval task, we phrase the
task as a ranking problem. For each target word, the
paraphrases given for all 10 instances are pooled. The
task is to rank the list for each item so that appropriate
paraphrases (such as ?be employed? for # 2002) rank
higher than paraphrases not given (e.g., ?toil?).
Our model ranks paraphrases by their similarity
to the following combinations (Eq. (4)): for V-SUBJ,
verb plus the noun?s subj?1 preferences; for V-OBJ,
verb plus the noun?s obj?1 preferences; and for N-
OBJ, the noun plus the verb?s obj preferences. Our
comparison model, M&L, ranks all paraphrases by
their similarity to the direct noun-verb combination.
To avoid overfitting, we consider only the two mod-
els that performed optimally in in the SYN space in
Experiment 1 (SELPREF-POW with n=30 and M&L).
However, since we found that vectors with raw fre-
quency components could model the data, while PMI
components could not, we only report the former.
For evaluation, we adopt the SemEval ?out of
ten? precision metric POOT. It uses the model?s ten
top-ranked paraphrases as its guesses for appropri-
ate paraphrases. Let Gi be the gold paraphrases for
item i, Mi the model?s top ten paraphrases for i, and
f(s, i) the frequency of s as paraphrase for i:
POOT = 1/|I|
?
i
?
s?Mi?Gi
f(s, i)
?
s?Gi
f(s, i)
(8)
McCarthy and Navigli propose this metric for the
6The specification of this dataset will be made available.
904
Model V-SUBJ V-OBJ N-OBJ
Target only 47.9 47.4 49.6
Selpref only 54.8 51.4 55.0
M&L 50.3 52.0 53.4
SELPREF-POW, n=30 63.1 55.8 56.9
Table 3: Experiment 2: Mean ?out of ten? precision (POOT)
dataset for robustness. Due to the sparsity of para-
phrases, a metric that considers fewer guesses leads
to artificially low results when a ?good? paraphrase
was not mentioned by the annotators by chance but
is ranked highly by a model.
Results and discussion. Table 6 shows the mean
out-of-ten precision for all models. The behavior is
fairly uniform across all three datasets. Unsurpris-
ingly, ?target only?, which uses the same ranking for
all instances of a target, yields the worst results.7
M&L?s direct combination model outperforms ?tar-
get only? significantly (p < 0.05). However, on both
the V-SUBJ and the N-OBJ the ?selpref only? baseline
does better than direct combination. The best results
on all datasets are obtained by SELPREF-POW. The
difference between SELPREF-POW and the ?target
only? baseline is highly significant (p < 0.01). The
difference to M&L?s model is significant at p = 0.05.
We interpret these results as encouraging evidence
for the usefulness of selectional preferences for judg-
ing substitutability in context. Knowledge about the
selectional preferences of a single context word can
already lead to a significant improvement in precision.
We find this overall effect even though the word is
not informative in all cases. For instance, the subject
of item 2002 in Fig. 4, ?who?, presumably helps little
in determining the verb?s context-adapted meaning.
It is interesting that the improvement of SELPREF-
POW over ?selpref only? is smallest for the N-OBJ
dataset (1.9% POOT). N-OBJ uses selectional prefer-
ences for nouns that may fill the direct object position,
, while V-SUBJ and V-OBJ use inverse selectional
preferences for verbs (cf. the two graphs in Fig. 1).
7?Target only? still does very much better than a random
baseline, which performs at 22% POOT.
7 Conclusion
In this paper, we have considered semantic space
models that can account for the meaning of word
occurrences in context. Arguing that existing models
do not sufficiently take syntax into account, we have
introduced the new structured vector space (SVS)
model of word meaning. In addition to a vector rep-
resenting a word?s lexical meaning, it contains vec-
tors representing the word?s selectional preferences.
These selectional preferences play a central role in
the computation of meaning in context.
We have evaluated the SVS model on two datasets
on the task of predicting the felicitousness of para-
phrases in given contexts. On the M&L dataset,
SVS outperforms the state-of-the-art model of M&L,
though the difference is not significant. On the Lex-
ical Substitution dataset, SVS significantly outper-
forms the state-of-the-art. This is especially interest-
ing as the Lexical Substitution dataset, in contrast to
the M&L data, uses ?realistic? paraphrase candidates
that are not necessarily maximally distinct.
The most important limitation of the evaluation
that we have given in this paper is that we have only
considered single words as context. Our next step
will be to integrate information from multiple rela-
tions (such as both the subject and object positions
of a verb) into the computation of context-specific
meaning. Our eventual aim is a model that can give
a compositional account of a word?s meaning in con-
text, where all words in an expression disambiguate
one another according to the relations between them.
We will explore the usability of vector space mod-
els of word meaning in NLP applications, formulated
as the question of how to perform inferences on them
in the context of the Textual Entailment task (Dagan
et al, 2006). Paraphrase-based inference rules play
a large role in several recent approaches to Textual
Entailment (e.g. Szpektor et al(2008)); appropriate-
ness judgments of paraphrases in context, the task of
Experiments 1 and 2 above, can be viewed as testing
the applicability of these inferences rules.
Acknowledgments. Many thanks for helpful dis-
cussion to Jason Baldridge, David Beaver, Dedre
Gentner, James Hampton, Dan Jurafsky, Alexander
Koller, Brad Love, and Ray Mooney.
905
References
C. Brockmann, M. Lapata. 2003. Evaluating and combin-
ing approaches to selectional preference acquisition. In
Proceedings of EACL, 27?34.
I. Dagan, O. Glickman, B. Magnini. 2006. The PASCAL
Recognising Textual Entailment Challenge. In Ma-
chine Learning Challenges, Lecture Notes in Computer
Science, 177?190. Springer.
K. Erk. 2007. A simple, similarity-based model for selec-
tional preferences. In Proceedings of ACL, 216?223.
T. Ferretti, C. Gagne?, K. McRae. 2003. Thematic role fo-
cusing by participle inflections: evidence form concep-
tual combination. Journal of Experimental Psychology,
29(1):118?127.
D. Gildea, D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3):245?
288.
D. Hindle, M. Rooth. 1993. Structural ambiguity and
lexical relations. Computational Linguistics, 19(1):103?
120.
M. Jones, D. Mewhort. 2007. Representing word mean-
ing and order information in a composite holographic
lexicon. Psychological review, 114:1?37.
J. J. Katz, J. A. Fodor. 1964. The structure of a semantic
theory. In The Structure of Language. Prentice-Hall.
A. Kilgarriff. 1997. I don?t believe in word senses. Com-
puters and the Humanities, 31(2):91?113.
W. Kintsch. 2001. Predication. Cognitive Science,
25:173?202.
T. Landauer, S. Dumais. 1997. A solution to Platos prob-
lem: the latent semantic analysis theory of acquisition,
induction, and representation of knowledge. Psycho-
logical Review, 104(2):211?240.
D. Lin. 1993. Principle-based parsing without overgener-
ation. In Proceedings of ACL, 112?120.
D. Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of COLING-ACL, 768?774.
K. Lund, C. Burgess. 1996. Producing high-dimensional
semantic spaces from lexical co-occurrence. Behav-
ior Research Methods, Instruments, and Computers,
28:203?208.
C. D. Manning, P. Raghavan, H. Schu?tze. 2008. Introduc-
tion to Information Retrieval. CUP.
D. McCarthy, J. Carroll. 2003. Disambiguating nouns,
verbs, and adjectives using automatically acquired
selectional preferences. Computational Linguistics,
29(4):639?654.
D. McCarthy, R. Navigli. 2007. SemEval-2007 Task 10:
English Lexical Substitution Task. In Proceedings of
SemEval, 48?53.
S. McDonald, C. Brew. 2004. A distributional model
of semantic context effects in lexical processing. In
Proceedings of ACL, 17?24.
S. McDonald, M. Ramscar. 2001. Testing the distribu-
tional hypothesis: The influence of context on judge-
ments of semantic similarity. In Proceedings of CogSci,
611?616.
K. McRae, M. Spivey-Knowlton, M. Tanenhaus. 1998.
Modeling the influence of thematic fit (and other con-
straints) in on-line sentence comprehension. Journal of
Memory and Language, 38:283?312.
K. McRae, M. Hare, J. Elman, T. Ferretti. 2005. A
basis for generating expectancies for verbs from nouns.
Memory and Cognition, 33(7):1174?1184.
J. Mitchell, M. Lapata. 2008. Vector-based models of
semantic composition. In Proceedings of ACL, 236?
244.
A. Moschitti, S. Quarteroni. 2008. Kernels on linguistic
structures for answer extraction. In Proceedings of
ACL, 113?116, Columbus, OH.
S. Narayanan, D. Jurafsky. 2002. A Bayesian model
predicts human parse preference and reading time in
sentence processing. In Proceedings of NIPS, 59?65.
S. Pado?, M. Lapata. 2007. Dependency-based construc-
tion of semantic space models. Computational Linguis-
tics, 33(2):161?199.
U. Pado?, F. Keller, M. W. Crocker. 2006. Combining syn-
tax and thematic fit in a probabilistic model of sentence
processing. In Proceedings of CogSci, 657?662.
S. Pado?, U. Pado?, K. Erk. 2007. Flexible, corpus-based
modelling of human plausibility judgements. In Pro-
ceedings of EMNLP/CoNLL, 400?409.
P. Resnik. 1996. Selectional constraints: An information-
theoretic model and its computational realization. Cog-
nition, 61:127?159.
G. Salton, A. Wang, C. Yang. 1975. A vector-space model
for information retrieval. Journal of the American So-
ciety for Information Science, 18:613?620.
H. Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?124.
S. Sharoff. 2006. Open-source corpora: Using the net to
fish for linguistic data. International Journal of Corpus
Linguistics, 11(4):435?462.
P. Smolensky. 1990. Tensor product variable binding and
the representation of symbolic structures in connection-
ist systems. Artificial Intelligence, 46:159?216.
I. Szpektor, I. Dagan, R. Bar-Haim, J. Goldberger. 2008.
Contextual preferences. In Proceedings of ACL, 683?
691, Columbus, OH.
Y. Wilks. 1975. Preference semantics. In Formal Seman-
tics of Natural Language. CUP.
A. Yeh. 2000. More accurate tests for the statistical
significance of result differences. In Proceeedings of
COLING, 947?953.
906
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 440?449,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Graded Word Sense Assignment
Katrin Erk
University of Texas at Austin
katrin.erk@mail.utexas.edu
Diana McCarthy
University of Sussex
dianam@sussex.ac.uk
Abstract
Word sense disambiguation is typically
phrased as the task of labeling a word in
context with the best-fitting sense from a
sense inventory such as WordNet. While
questions have often been raised over the
choice of sense inventory, computational
linguists have readily accepted the best-
fitting sense methodology despite the fact
that the case for discrete sense bound-
aries is widely disputed by lexical seman-
tics researchers. This paper studies graded
word sense assignment, based on a recent
dataset of graded word sense annotation.
1 Introduction
The task of automatically characterizing word
meaning in text is typically modeled as word sense
disambiguation (WSD): given a list of senses for
target lemma w, the task is to pick the best-fitting
sense for a given occurrence of w. The list of
senses is usually taken from an online dictionary
or thesaurus. However, clear cut sense boundaries
are sometimes hard to define, and the meaning of
words depends strongly on the context in which
they are used (Cruse, 2000; Hanks, 2000). Some
researchers in lexical semantics have suggested
that word meanings lie on a continuum between
i) clear cut cases of ambiguity and ii) vagueness
where clear cut boundaries do not hold (Tuggy,
1993). Certainly, it seems that a more complex
representation of word sense is needed with a
softer, graded representation of meaning rather
than a fixed listing of senses (Cruse, 2000).
A recent annotation study ((Erk et al, 2009),
hereafter GWS) marked a target word in context
with graded ratings (on a scale of 1-5) on senses
from WordNet (Fellbaum, 1998). Table 1 shows
an example of a sentence with the target word
in bold, and with the annotator judgments given
to each sense. The study found that annotators
made ample use of the intermediate ratings on the
scale, and often gave high ratings to more than one
WordNet sense for the same occurrence. It was
found that the annotator ratings could not easily
be transformed to categorial judgments by making
more coarse-grained senses. If human word sense
judgments are best viewed as graded, it makes
sense to explore models of word sense that can
predict graded sense assignments.
In this paper we look at the issue of graded ap-
plicability of word sense from the point of view
of automatic graded word sense assignment, us-
ing the GWS graded word sense dataset. We make
three primary contributions. Firstly, we propose
evaluation metrics that can be used on graded
word sense judgments. Some of these metrics, like
Spearman?s ?, have been used previously (Mc-
Carthy et al, 2003; Mitchell and Lapata, 2008),
but we also introduce new metrics based on the
traditional precision and recall. Secondly, we in-
vestigate how two classes of models perform on
the task of graded word sense assignment: on
the one hand classical WSD models, on the other
hand prototype-based vector space models that
can be viewed as simple one-class classifiers. We
study supervised models, training on traditional
WSD data and evaluating against a graded scale.
Thirdly, the evaluation metrics we use also pro-
vides a novel analysis of annotator performance
on the GWS dataset.
2 Related Work
WSD has to date been a task where word senses are
viewed as having clear cut boundaries. However,
there are indications that word meanings do not
behave in this way (Kilgarriff, 2006). Researchers
in the field of WSD have acknowledged these prob-
lems but have used existing lexical resources in
the hope that useful applications can be built with
them. However, there is no consensus on which
440
Senses
Sentence 1 2 3 4 5 6 7 Annotator
This can be justified thermodynamically in this case, and
this will be done in a separate paper which is being
prepared.
2 3 3 5 5 2 3 Ann. 1
1 3 1 3 5 1 1 Ann. 2
1 5 2 1 5 1 1 Ann. 3
1.3 3.7 2 3 5 1.3 1.7 Avg
Table 1: A sample annotation in the GWS experiment. The senses are: 1 material from cellulose 2 report
3 publication 4 medium for writing 5 scientific 6 publishing firm 7 physical object
inventory is suitable for which application, other
than cross-lingual applications where the inven-
tory can be determined from parallel data (Carpuat
and Wu, 2007; Chan et al, 2007). For monolin-
gual applications however it is less clear whether
current state-of-the-art WSD systems for tagging
text with dictionary senses are able to have an im-
pact on applications.
One way of addressing the problem of low inter-
annotator agreement and system performance is to
create an inventory that is coarse-grained enough
for humans and computers to do the job reli-
ably (Ide and Wilks, 2006; Hovy et al, 2006;
Palmer et al, 2007). Such coarse-grained invento-
ries can be produced manually from scratch (Hovy
et al, 2006) or by automatically relating (Mc-
Carthy, 2006) or clustering (Navigli, 2006; Nav-
igli et al, 2007) existing word senses. While the
reduction in polysemy makes the task easier, we
do not know which are the right distinctions to re-
tain. In fact, fine-grained distinctions may be more
useful than coarse-grained ones for some applica-
tions (Stokoe, 2005). Furthermore, Hanks (2000)
goes further and argues that while the ability to
distinguish coarse-grained senses is indeed desir-
able, subtler and more complex representations of
word meaning are necessary for text understand-
ing.
In this paper, instead of focusing on issues of
granularity we try to predict graded judgments of
word sense applicability, using a recent dataset
with graded annotation (Erk et al, 2009). Our
hope is that models which can mimic graded hu-
man judgments on the same task should better re-
flect the underlying phenomena of word mean-
ing compared to a system that focuses on mak-
ing clear cut distinctions. Also, we hope that such
models might prove more useful in applications.
There is one existing study of graded sense as-
signment (Ramakrishnan et al, 2004). It tries to
estimate a probability distribution over senses by
converting all of WordNet into a huge Bayesian
Network, and reports improvements in a Question
Answering task. However, it does not test its pre-
diction against human annotator data.
We concentrate on supervised models in this
paper since they generally perform better than
their unsupervised or knowledge-based counter-
parts (Navigli, 2009). We compare them against
a baseline model which simply uses the train-
ing data to obtain a probability distribution over
senses regardless of context, since marginal distri-
butions are highly skewed making a prior distribu-
tion very informative (Chan and Ng, 2005; Lapata
and Brew, 2004).
Along with standard WSD models, we evalu-
ate vector space models that use the training data
to locate a word sense in semantic space. Word
sense and vector space models have been related in
two ways. On the one hand, vector space models
have been used for inducing word senses (Sch?utze,
1998; Pantel and Lin, 2002). The different mean-
ings of a word are obtained by clustering vectors.
The clusters must then be mapped to an inven-
tory if a standard WSD dataset is used for eval-
uation. In contrast, we use sense tagged train-
ing data with the aim of building models of given
word senses, rather than clustering occurrences
into word senses. The second way in which word
sense and vector space models have been related is
to assign disambiguated feature vectors to Word-
Net concepts (Pantel, 2005; Patwardhan and Ped-
ersen, 2006). However those works do not use
sense-tagged data and are not aimed at WSD, rather
the applications are to insert new concepts into an
ontology and to measure the relatedness of con-
cepts.
We are not concerned in this paper with argu-
ing for or against any particular sense inventory.
WordNet has been criticized for being overly fine-
grained (Navigli et al, 2007; Ide and Wilks, 2006),
we are using it here because it is the sense inven-
tory used by Erk et al (2009). That annotation
study used it because it is sufficiently fine-grained
to allow for the examination of subtle distinctions
between usages and because it is publicly available
441
lemma # # training
(PoS) senses SemCor SE-3
add (v) 6 171 238
argument (n) 7 14 195
ask (v) 7 386 236
different (a) 5 106 73
important (a) 5 125 11
interest (n) 7 111 160
paper (n) 7 46 207
win (v) 4 88 53
total training sentences 1047 1173
Table 2: Lemmas used in this study
with various sense-tagged datasets (e.g. (Miller et
al., 1993; Mihalcea et al, 2004)) for comparison.
3 Data
In this paper, we use a subset of the GWS
dataset (Erk et al, 2009) where three annotators
supplied ordinal judgments of the applicability of
WordNet (v3.0) senses on a 5 point scale: 1 ?
completely different, 2 ? mostly different, 3 ? sim-
ilar, 4 ? very similar and 5 ? identical. Table 1
shows a sample annotation. The sentences that
we use from the GWS dataset were originally ex-
tracted from the English SENSEVAL-3 lexical sam-
ple task (Mihalcea et al, 2004) (hereafter SE-3)
and SemCor (Miller et al, 1993).
1
For 8 lem-
mas, 25 sentences were randomly sampled from
SemCor and 25 randomly sampled from SE-3, giv-
ing a total of 50 sentences per lemma. The lem-
mas, their PoS and number of senses from Word-
Net are shown in table 2.
The annotation study found that annotators
made ample use of the intermediate levels of ap-
plicability (2-4), and they often gave positive rat-
ings (3-5) to more than one sense for a single oc-
currence. The example in Table 1 is one such
case. An analysis of the annotator ratings found
that they could not easily be explained in catego-
rial terms by making more coarse-grained senses
because senses that were not positively correlated
often had high ratings for the same instance.
The GWS dataset contains a sequence of judg-
ments for each occurrence of a target word in a
sentence context: one judgment for each Word-
Net sense of the target word. To obtain a sin-
gle judgment for each sense in each sentence we
use the average judgment from the three annota-
tors. As models typically assign values between
1
The GWS data also contains data from the English Lex-
ical Substitution Task (McCarthy and Navigli, 2007) but we
do not use that portion of the data for these experiments.
0 and 1, we normalize the annotator judgments
from the GWS dataset to fall into the same range by
using normalized judgment = (judgment ?
1.0)/4.0. This maps an original judgment of 5 to
a normalized judgment of 1.0, it maps an original
1 to 0.0, and intermediate judgments are mapped
accordingly.
As the GWS dataset is too small to accommodate
both training and testing of a supervised model, we
use all the data from GWS for testing our models,
and train our models on traditional word sense an-
notation data. We use as training data all sentences
from SemCor and the training portion of SE-3 that
are not included in GWS. The quantity of training
data available is shown in the last two columns of
table 2.
4 Evaluating Graded Word Sense
Assignment
This section discusses measures for evaluating
system performance for the case where gold judg-
ments are graded rather than categorial.
Correlation. The standard method for compar-
ing a list of graded gold judgments to a list of
graded predicted judgments is by testing for corre-
lation. In our case, as we cannot assume a normal
distribution of the judgments, a non-parametric
test such as Spearman?s ? will be appropriate.
Spearman?s ? uses the formula of Pearson?s coef-
ficient, defined as
?(X,Y ) =
cov(X,Y )
?
X
?
Y
Pearson?s coefficient computes the correlation of
two random variables X and Y as their covari-
ance divided by the product of their standard devi-
ations. In the computation of Spearman?s ?, values
are transformed to rankings before the formula is
applied.
2
As Spearman?s ? compares the rank-
ings of two sets of judgments, it abstracts from the
absolute values of the judgments. It is useful to
have a measure that abstracts from absolute values
of judgments and magnitude of difference because
the GWS dataset contains annotator judgments on
a fixed scale, and it is quite possible that human
judges will differ in how they use such a scale.
Each judgment in the gold-standard can be
represented as a 4-tuple ?lemma, sense no, sen-
tence no, gold judgment?. For example, ?add.v,
2
Mitchell and Lapata (2008) note that Spearman?s ? tends
to yield smaller coefficients than its parametric counterparts
such as Pearson?s coefficient.
442
1, 1, 0.8? is the first sentence for target add.v, first
WordNet sense, with a (normalized) judgment of
0.8. Likewise, each prediction by the model can
be represented as a 4-tuple ?lemma, sense no, sen-
tence no, predicted judgment?. We writeG for the
set of gold tuples, A for the set of assigned tuples,
L for the set of lemmas, S
`
for the set of sense
numbers that exist for lemma `, and T for the set
of sentence numbers (there are 50 sentences for
each lemma). We writeG|
lemma=`
for the gold set
restricted to those tuples with lemma `, and anal-
ogously for other set restrictions and for A. There
are several possibilities for measuring correlation:
by lemma: for each lemma ` ? L, compute cor-
relation between G|
lemma=`
and A|
lemma=`
by lemma+sense: for each lemma ` and each
sense number i ? S
`
, compute cor-
relation between G|
lemma=`,senseno=i
and
A|
lemma=`,senseno=i
by lemma+sentence: for each lemma ` and sen-
tence number t ? T , compute cor-
relation between G|
lemma=`,sentence=t
and
A|
lemma=`,sentence=t
Comparison by lemma tests for the consis-
tent use of judgments for the same target lemma.
A comparison by lemma+sense ranks all occur-
rences of the same target lemma by how strongly
they evoke a given word sense. A comparison
by lemma+sentence ranks different senses by how
strongly they apply to a given target lemma oc-
currence. In reporting correlation by lemma (by
lemma+sense, by lemma+sentence), we average
over all lemmas (lemma+sense, lemma+sentence
combinations), and we report the percentage of
lemmas (combinations) for which the correlation
was significant. We report averaged correlation by
lemma rather than one overall correlation over all
judgments in order not to give more weight to lem-
mas with more senses.
Divergence. Another possibility for measuring
the performance of a graded sense assignment
model is to use Jensen/Shannon divergence (J/S),
which is a symmetric version of Kullback/Leibler
divergence. Given two probability distributions
p, q, the Kullback/Leibler divergence of q from p
is
D(p||q) =
?
x
p(x) log
p(x)
q(x)
and their J/S is
JS(p, q) =
1
2
(
D(p||
p+ q
2
) +D(q||
p+ q
2
)
We will use J/S for an evaluation by
lemma+sentence: for each lemma ` ? L
and sentence number t ? T , we normalize
G|
lemma=`,sentence=t
, the set of judgments for
senses of ` in t, by the sum of sense judgments for
` and t. We do the same for A|
lemma=`,sentence=t
.
Then we compute J/S. In doing so, we are not
trying to interpret G|
lemma=`,sentence=t
as some
kind of probability distribution over senses, rather
we use J/S as a measure that abstracts from
absolute judgments but not from the magnitude of
differences between judgments.
Precision and Recall. We have discussed a
measure that abstracts from both absolute judg-
ments and magnitude of differences (Spearman?s
?), and a measure that abstracts from absolute
judgments but not the magnitude of differences
(J/S). What is still missing is a measure that tests
to what degree a model conforms to the absolute
judgments given by the human annotators.
To obtain a measure for performance in predict-
ing absolute gold judgments, we generalize preci-
sion and recall. In the categorial case, precision is
defined as P =
true positives
true positives+false positives
, true pos-
itives divided by system-assigned positives, and
recall is R =
true positives
true positives+false negatives
, true posi-
tives divided by gold positives. Writing gold
`,i,t
for the judgment j associated with lemma ` and
sense number i for sentence t in the gold data (i.e.,
?`, i, t, j? ? G), and analogously assigned
`,i,t
, we
extend precision and recall to the graded case as
follows:
P
`
=
?
i?S
`
,t?T
min(gold
`,i,t
, assigned
`,i,t
)
?
i?S
`
,t?T
assigned
`,i,t
and
R
`
=
?
i?S
`
,t?T
min(gold
`,i,t
, assigned
`,i,t
)
?
i?S
`
,t?T
gold
`,i,t
where ` is a lemma. We compute precision and re-
call by lemma, then macro-average them in order
not to give more weight to lemmas that have more
senses. The formula for F-score as the harmonic
mean of precision and recall remains unchanged:
F = 2 P R/(P +R).
If the data is categorial, the graded precision and
recall measures coincide with ?classical? precision
443
Cx/2 until, IN, soft, JJ, remaining, VBG, ingredient,
NNS
Cx/50 for, IN, sweet-sour, NN, sauce, NN, . . . , to, TO,
a, DT, boil, NN
Ch OA, OA/ingredient/NNS
Table 3: Sample features for add in BNC occur-
rence For sweet-sour sauce, cook onion in oil un-
til soft. Add remaining ingredients and bring to
a boil. Cx/2 (Cx/50): context of size 2 (size 50)
either side of the target. Ch: children of target.
and recall, which can be seen as follows. Graded
sense assignment is represented by assigning each
sense a score between 0.0 and 1.0. The categorial
case can be represented in the same way, the dif-
ference being that one single sense will receive a
score of 1.0 while all other senses get a score of
0.0. With this representation for categorial sense
assignment, consider a fixed token t of lemma `.
?
i?S
`
min(assigned
`,i,t
, gold
`,i,t
) will be 1 if the
assigned sense is the gold sense, and 0 otherwise.
5 Models for Graded Word Sense
Assignment
In this section we discuss the computational mod-
els for graded word sense that are tested in this
paper.
Single-best-sense WSD. The first model that we
test is a standard WSD model that assigns, to each
test occurrence of a target word, a single best-
fitting word sense. The system thus attributes a
confidence score of 1 to the assigned sense and a
confidence score of 0 for all other senses for that
sentence. We refer to it as WSD/single. The model
uses standard features: lemma and part of speech
in a narrow context window (2 words either side)
and a wide context window (50 words either side),
as well as dependency labels leading to parent,
children, and siblings of the target word, and lem-
mas and part of speech of parent, child, and sibling
nodes. Table 3 shows sample model features for an
occurrence of add in the British National Corpus
(BNC) (Leech, 1992). The model uses a maxi-
mum entropy learner
3
, training one binary classi-
fier per sense. (With n-ary classifiers, the model?s
performance is slightly worse.) The model is thus
not highly optimized, but fairly standard.
WSD confidence level as judgment. Our second
model is the same WSD system as above, but we
3
http://maxent.sourceforge.net/
use it to predict a judgment for each sense of a
target occurrence, taking the confidence level re-
turned by each sense-specific binary classifier as
the predicted judgment. We refer to this model as
WSD/conf .
Word senses as points in semantic space. The
results of the GWS annotation study raise the ques-
tion of how word senses are best conceptualized,
given that annotators assigned graded judgments
of applicability of word senses, and given that they
often combined high judgments for multiple word
senses. One way of modeling these findings is
to view word senses as prototypes, where some
uses of a word will be typical examples of a given
sense, for some uses the sense will clearly not ap-
ply, and to some uses the sense will be borderline
applicable.
We use a very simple model of word senses as
prototypes, representing them as points in a se-
mantic space. Graded sense applicability judg-
ments can then be modeled using vector similarity.
The dimensions of the vector space are the features
of the WSD system above (including dimensions
like Cx2/until, Cx2/IN, Ch/OA/ingredient/NNS for
the example in Table 3), and the coordinates are
raw feature counts. We compute a single vector
for each sense s, the centroid of all training oc-
currences that have been labeled with s. The pre-
dicted judgment for a test sentence and sense s
is then the similarity of the sentence?s vector to
the centroid vector for s, computed using cosine.
We call this model Prototype. Like instance-based
learners (Daelemans and den Bosch, 2005), the
Prototype model measures the distance between
feature vectors in space. Unlike instance-based
learners, it only uses data from a single category
for training.
As it is to be expected that the vectors in this
space will be very sparse, we also test a variant
of the Prototype model with Sch?utze-style second-
order vectors (Sch?utze, 1998), called Prototype/2.
Given a (first-order) feature vector, we compute
a second-order vector as the centroid of vectors
for all lemma features (omitting stopwords) in the
first-order vector. For the feature vector in Table 3,
this is the centroid of vectors
~
sweet-sour,
~
sauce,
. . . ,
~
boil. We compute the vectors
~
sweet-sour etc.
as dependency vectors (Pad?o and Lapata, 2007)
4
over a Minipar parse (Lin, 1993) of the BNC.
4
We use the DV package, http://www.nlpado.de/
?
sebastian/dv.html, to compute the vector space.
444
We transform raw co-occurrence counts in the
BNC-based vectors using pointwise mutual in-
formation (PMI), a common transformation func-
tion (Mitchell and Lapata, 2008).
5
Another way of motivating the use of vector
space models of word sense is by noting that we
are trying to predict graded sense assignment by
training on traditional word sense annotated data,
where each target word occurrence is typically
marked with a single word sense. Traditional word
sense annotation, when used to predict GWS judg-
ments, will contain spurious negative data: sup-
pose a human annotator is annotating an occur-
rence of target word t and views senses s
1
, s
2
and
s
3
as somewhat applicable, with sense s
1
applying
most clearly. Then if the annotation guidelines ask
for the best-fitting sense, the annotator should only
assign s
1
. The occurrence is recorded as having
sense s
1
, but not senses s
2
and s
3
. This, then, con-
stitutes spurious negative data for senses s
2
and s
3
.
The simple vector space model of word sense that
we use implements a radical solution to this prob-
lem of spurious negative data: it only uses positive
data for a single sense, thus forgoing competition
between categories. It is to be expected that not
using competition between categories will hurt the
vector space model?s performance, but this design
gives us the chance to compare two model classes
that use opposing strategies with respect to spuri-
ous negative data: the WSD models fully trust the
negative data, while the vector space models ig-
nore it.
6 Experiments
This section reports on experiments for the task
of graded word sense assignment. As data, we
use the GWS dataset described in Sec. 3. We test
the models discussed in Sec. 5, evaluating with the
methods described in Sec. 4.
To put the models? performance into perspec-
tive, we first consider the human performance on
the task, shown in Table 4. The first three lines
of the table show the performance of each annota-
tor evaluated against the average of the other two.
The fourth line averages over the previous three
lines to provide an average human ceiling for the
task. In the correlation of rankings by lemma, cor-
relation is statistically significant for all lemmas at
5
We also tested PMI transformation for the first-order vec-
tors, but will not report the results here as they were worse
across the board than without PMI.
p ? 0.01. For correlation by lemma+sense and by
lemma+sentence, the percentage of pairs with sig-
nificant correlation is lower: 73.6 of lemma/sense
pairs and 29.0 of lemma/sentence pairs reach sig-
nificance at p ? 0.05. For p ? 0.01, the per-
centage is 58.3 and 12.2, respectively. The higher
? but lower proportion of significant values for
lemma+sentence pairs compared to lemma+sense
is due to the fact that there are far fewer dat-
apoints (sample size) for each calculation of ?
(#senses for lemma+sentence vs 50 sentences for
lemma+sense).
At 0.131, J/S for Annotator 1 is considerably
lower than for Annotators 2 and 3.
6
In terms
of precision and recall, Annotator 1 again differs
from the other two. At 87.5, her recall is higher
than her precision (50.6), while the other annota-
tors have considerably higher precision (75.5 and
82.4) than recall (62.4 and 52.3). This indicates
that Annotator 1 tended to assign higher ratings
throughout, an impression that is confirmed by Ta-
ble 6. The left two columns show average rat-
ings for each annotator over all senses of all to-
kens (normalized to values between 0.0 and 1.0 as
described in Sec. 3). The three annotators differ
widely in their average ratings, which range from
0.285 for Ann.3 to 0.540 for Ann.1.
Standard WSD. We tested the performance of
the WSD/single model on a standard WSD task,
using the same training and testing data as in
our subsequent experiments, as described in sec-
tion 3.
7
The model?s accuracy when trained and
tested on SemCor was A=77.0%, with a most fre-
quent sense baseline of 63.5%. When trained
and tested on SE-3, the model achieved A=53.0%
against a baseline of 44.0%. When trained and
tested on SemCor plus SE-3, the model reached an
accuracy 58.2%, with a baseline of 56.0%. So on
the combined dataset, the baseline is the average
of the baselines on the individual datasets, while
the model?s performance falls below the average
performance on the individual datasets.
WSD models for graded sense assignment.
Table 5 shows the performance of different mod-
els in the task of graded word sense assignment.
The first line in Table 5 lists results for the maxi-
mum entropy model when used to assign a single
best sense. The second line lists the results for
6
Low J/S implies a closer agreement between two sets of
judgments.
7
Note that this constitutes less training data than in the
SE-3 task.
445
by lemma by lemma+sense by lemma+sentence
Ann ? * ** ? * ** ? * ** J/S P R F
Ann.1 0.517 100.0 100.0 0.407 75.0 58.3 0.482 27.3 11.5 0.131 50.6 87.5 64.1
Ann.2 0.587 100.0 100.0 0.403 68.8 58.3 0.612 38.1 17.2 0.153 75.5 62.4 68.3
Ann.3 0.528 100.0 100.0 0.41 77.1 58.3 0.51 21.8 7.8 0.165 82.4 52.3 64.0
Avg 0.544 100.0 100.0 0.407 73.6 58.3 0.535 29.0 12.2 0.149 69.5 67.4 65.5
Table 4: Human ceiling: one annotator vs. average of the other two annotators. ?, ??: percentage
significant at p ? 0.05, p ? 0.01. Avg: average annotator performance
by lemma by lemma+sense by lemma+sentence
Model ? * ** ? * ** ? * ** J/S P R F
WSD/single 0.267 87.5 75.0 0.053 6.3 4.2 0.28 2.8 1.8 0.39 58.7 25.5 35.5
WSD/conf 0.396 87.5 87.5 0.177 33.3 18.8 0.401 10.8 3.0 0.164 81.8 37.1 51.0
Prototype 0.245 62.5 62.5 0.053 20.8 8.3 0.396 15.3 2.5 0.173 58.4 78.3 66.9
Prototype/2 0.292 87.5 87.5 0.086 14.6 4.2 0.478 22.8 7.5 0.164 68.2 63.3 65.7
Prototype/N 0.396 100.0 100.0 0.137 22.9 14.6 0.396 15.3 2.5 0.173 82.2 29.9 43.9
Prototype/2N 0.465 100.0 100.0 0.168 29.8 23.4 0.478 22.8 7.5 0.164 82.6 30.9 45.0
baseline 0.338 87.5 87.5 0.0 0.0 0.0 0.355 10.3 3.0 0.167 79.9 34.5 48.2
Table 5: Evaluation: computational models, and baseline. ?, ??: percentage significant at p ? 0.05,
p ? 0.01
the same maximum entropy model when classifier
confidence is used as predicted judgment. The last
line shows the baseline, an adaptation of the most
frequent sense baseline to the graded case. For
this baseline, we computed the relative frequency
of each sense in the training corpus and used this
relative frequency as the prediction for each test
sentence and sense combination. The WSD/single
model remains below the baseline in all evalua-
tions except correlation by lemma+sense, where
no rank-based correlation could be computed for
the baseline because it always assigns the same
judgment for a given sense. WSD/conf shows a
performance slightly above the baseline in all eval-
uation measures. Table 6 lists average ratings, av-
eraged over all lemmas, senses, and occurrences,
for each model in the two right-hand columns.
Prototype models. Lines 3-6 in Tables 5 and
6 show results for Prototype variants. While each
Prototype and Prototype/2 model only sees pos-
itive data annotated for a single sense, the vari-
ants with /N (lines 5 and 6) make very limited use
of information coming from all senses of a given
lemma. They normalize judgments for each sen-
tence, with
assigned
norm
`,i,t
=
assigned
`,i,t
?
j?S
`
assigned
`,j,t
Line 3 evaluates the Prototype model with first-
order vectors. Its correlation with the gold data is
somewhat lower than that of WSD/conf in almost
all cases.
8
The Prototype model deviates strongly
8
The reason why the average ? for correlation by
from both WSD/conf and baseline in having a very
good recall, at 78.3, with lower precision at 58.4,
for an overall F-score that is 16 points higher than
that of WSD/conf . Both Prototype and Prototype/2
have average ratings (Table 6) far above those
of the WSD models and of the /N variants. The
second-order vector model Prototype/2 has rela-
tively low correlation by lemma+sense, while cor-
relation by lemma+sentence shows the best per-
formance of all models (along with Prototype/2N).
Its correlation by lemma+sentence is similar to the
lowest correlation by lemma+sentence achieved
by a human annotator. In terms of J/S, this
model also shows the best performance along with
WSD/conf and Prototype/2N. Both /N variants
achieve very high correlation by lemma. Corre-
lation by lemma+sense for the /N models is be-
tween those of Prototype and WSD/conf . The cor-
relation by lemma+sentence is the same with or
without normalization, as normalization does not
change the ranking of senses of an individual sen-
tence. While Prototype has higher recall than pre-
cision, normalization turns it into a model with
even higher precision than WSD/conf but even
lower recall.
Discussion
Human performance. The evaluation of human
annotators in Table 4 provides a novel analysis of
the GWS dataset over and above that by Erk et al
lemma+sense is the same for Prototype and WSD/single
while the significance percentage differs greatly is that the
Prototype shows negative correlation for some of the senses.
446
Ann. avg Model avg
Ann.1 0.540 WSD/single 0.163
Ann.2 0.345 WSD/conf 0.173
Ann.3 0.285 Prototype 0.558
Prototype/N 0.143
Prototype/2 0.375
Prototype/2N 0.143
baseline 0.167
Table 6: Average judgment for individual annota-
tors (transformed) and average rating for models
(2009). Human annotators show very strong cor-
relation of their rankings by lemma. They also had
strong agreement on rankings by lemma+sense,
which ranks occurrences of a lemma by how
strongly they evoke a given sense. The relatively
low precision and recall in Table 4 confirm that
different annotators use the 5-point scale in differ-
ent ways. A comparison of precision and recall
between the annotators reflects the fact that An-
notator 1 tended to give considerably higher rat-
ings than the other two, which is also apparent
in the average ratings in Table 6. Given the rela-
tively low F-score achieved by human annotators,
judgments by additional annotators could make
the GWS dataset more useful, in that the average
judgments would not be influenced so strongly by
idiosyncrasies in the use of the 5-point scale. (Psy-
cholinguistic experiments using fixed scales typi-
cally elicit judgments from 10 or more participants
per item.)
Evaluation measures. Given the degree of dif-
ferences in the absolute values of the human an-
notator judgments (Table 4), a rank-based evalu-
ation of graded sense assignment models, com-
plemented by J/S to evaluate the magnitude of
differences between ratings, seems most appro-
priate to the data. Rankings by lemma+sense
and by lemma+sentence are especially interest-
ing for their potential use in systems that might
use graded sense assignment as part of a larger
pipeline. Still, the new graded precision and re-
call measures allow for a more fine-grained anal-
ysis of the performance of models, showing fun-
damental differences in the behavior of WSD/conf
and the Prototype model. Graded precision and
recall could become even more informative mea-
sures with a gold set containing judgments of more
annotators, since then the absolute gold judgments
would be more reliable.
Standard WSD models and vector space mod-
els. The results in Table 5 reflect the compromise
between the advantage of having competition be-
tween categories and the disadvantage of spurious
negative data: WSD/conf , Prototype/N and Proto-
type/2N achieve the highest correlation by lemma,
and high precision, while Prototype has much bet-
ter recall for an overall higher F-score. However,
as Table 6 shows, Prototype tends to assign high
ratings across the board, leading to high recall.
The much lower average ratings of the /N mod-
els explain their higher precision and lower recall:
they overshoot less and undershoot more. The im-
provement in correlation for the /N models also
indicates that Prototype assigns some sentences
high ratings for all senses, impacting rankings by
lemma and by lemma+sense.
The comparison of Prototype and Prototype/2
gives us a chance to study effects of feature sparse-
ness. Prototype/2, using second-order vectors that
should be much less sparse, yields better rankings
than Prototype. The average ratings of model Pro-
totype/2 (Table 6) are lower than those of Pro-
totype (and closer to human average ratings), re-
sulting in higher precision and lower recall. One
possible reason for the high average ratings of
Prototype is that in sparser (and shorter) vectors,
matches in dimensions for high-frequency, rela-
tively uninformative context items have greater
impact.
It is interesting to see that WSD/conf performs
slightly above the sense frequency baseline in all
evaluations, since this is a very familiar picture
from standard WSD.
Prototype/2N shows the overall most favorable
performance in terms of correlation as it i) pays
minimal attention to the negative data ii) uses nor-
malization to avoid overshooting and iii) compen-
sates for sparse data by using second order vectors.
For J/S, WSD/conf , Prototype/2, Prototype/2N
and the sense frequency baseline just outperform
the score of the lowest-scoring of the three anno-
tators. In terms of F-score, Prototype shows re-
sults very close to human performance. Interest-
ingly, the Prototype model resembles Annotator 1
in its precision and recall, while WSD/conf more
resembles Annotators 2 and 3. None of the mod-
els come close to human performance in ranking
by lemma+sense, which requires an identification
of the ?typical? occurrence of a given sense. The
low ratings in correlation by lemma+sense indi-
cate that the models might be limited by the lack
of training data for many of the rarer senses. In fu-
447
ture work, we will test how the frequency of senses
in the training data affects the different models.
7 Conclusion
In this paper we have done a first study on mod-
eling graded annotator judgments on sense appli-
cability. We have discussed evaluation measures
for models of graded sense assignment, includ-
ing new extensions of precision and recall to the
graded case. A combination of rank-based correla-
tion at the level of lemmas, senses, and sentences,
Jensen/Shannon divergence, and precision and re-
call provided a nuanced picture of the strengths
and weaknesses of different models. We have
tested two types of models: on the one hand a
standard binary WSD model using classifier con-
fidence as predicted judgments, and on the other
hand several vector space models which compute a
prototype vector for each sense in semantic space.
These two types of model differ strongly in their
behavior. The WSD model shows a similar behav-
ior as the baseline, with high precision but low re-
call, while the unnormalized version of the vector
space model has higher recall at lower precision.
The results show both the benefits of having com-
petition between categories, for improved rank-
based correlation and precision, and the problem
of spurious negative data in the training set arising
from the best-sense methodology.
The last two correlation measures, by
lemma+sense and by lemma+sentence, yield
maybe the most insight into the question of the
usability of a computational model for graded
word sense assignment: a graded word sense
assignment model that is a component of a larger
system could provide useful sense information
either by ranking occurrences by how strongly
they evoke a sense, or by ranking senses by how
strongly they apply to a given occurrence. There
is room for improvement however as system
performance is well below that of humans. In
the future we plan to investigate features that are
more informative for making graded judgments.
Second, the vector space model we used was
very simple; it might be worthwhile to test more
sophisticated one-class classifiers (Marsland,
2003; Sch?olkopf et al, 2000).
Acknowledgments. We acknowledge support
from the UK Royal Society for a Dorothy Hodgkin
Fellowship to the second author.
References
M. Carpuat and D. Wu. 2007. Improving statistical
machine translation using word sense disambigua-
tion. In Proceedings of EMNLP-CoNLL 2007, pages
61?72, Prague, Czech Republic, June. Association
for Computational Linguistics.
Y. S. Chan and H. T. Ng. 2005. Word sense disam-
biguation with distribution estimation. In Proceed-
ings of IJCAI 2005, pages 1010?1015, Edinburgh,
Scotland.
Y. S. Chan, H. T. Ng, and D. Chiang. 2007. Word sense
disambiguation improves statistical machine transla-
tion. In Proceedings of ACL?07, Prague, Czech Re-
public, June.
D. A. Cruse. 2000. Aspects of the microstructure of
word meanings. In Y. Ravin and C. Leacock, edi-
tors, Polysemy: Theoretical and Computational Ap-
proaches, pages 30?51. OUP, Oxford, UK.
W. Daelemans and A. Van den Bosch. 2005. Memory-
Based Language Processing. Cambridge University
Press, Cambridge, UK.
K. Erk, D. McCarthy, and N. Gaylord. 2009. Inves-
tigations on word senses and word usages. In Pro-
ceedings of ACL-09, Singapore.
C. Fellbaum, editor. 1998. WordNet, An Electronic
Lexical Database. The MIT Press, Cambridge, MA.
P. Hanks. 2000. Do word meanings exist? Computers
and the Humanities, 34(1-2):205?215(11).
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90% solu-
tion. In Proceedings of the HLT-NAACL 2006 work-
shop on Learning word meaning from non-linguistic
data, New York City, USA. Association for Compu-
tational Linguistics.
N. Ide and Y. Wilks. 2006. Making sense about
sense. In E. Agirre and P. Edmonds, editors,
Word Sense Disambiguation, Algorithms and Appli-
cations, pages 47?73. Springer.
A. Kilgarriff. 2006. Word senses. In E. Agirre
and P. Edmonds, editors, Word Sense Disambigua-
tion, Algorithms and Applications, pages 29?46.
Springer.
M. Lapata and C. Brew. 2004. Verb class disambigua-
tion using informative priors. Computational Lin-
guistics, 30(1):45?75.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
D. Lin. 1993. Principle-based parsing without over-
generation. In Proceedings of ACL?93, Columbus,
Ohio, USA.
448
S. Marsland. 2003. Novelty detection in learning sys-
tems. Neural computing surveys, 3:157?195.
D. McCarthy and R. Navigli. 2007. SemEval-2007
task 10: English lexical substitution task. In Pro-
ceedings of SemEval-2007, pages 48?53, Prague,
Czech Republic.
D. McCarthy, B. Keller, and J. Carroll. 2003. De-
tecting a continuum of compositionality in phrasal
verbs. In Proceedings of the ACL 03 Workshop:
Multiword expressions: analysis, acquisition and
treatment, pages 73?80.
D. McCarthy. 2006. Relating WordNet senses for
word sense disambiguation. In Proceedings of the
ACL Workshop on Making Sense of Sense: Bring-
ing Psycholinguistics and Computational Linguis-
tics Together, pages 17?24, Trento, Italy.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004.
The Senseval-3 English lexical sample task. In Pro-
ceedings of SensEval-3, Barcelona, Spain.
G. A. Miller, C. Leacock, R. Tengi, and R. T Bunker.
1993. A semantic concordance. In Proceedings of
the ARPA Workshop on Human Language Technol-
ogy, pages 303?308. Morgan Kaufman.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL?08
- HLT, pages 236?244, Columbus, Ohio.
R. Navigli, K. C. Litkowski, and O. Hargraves. 2007.
SemEval-2007 task 7: Coarse-grained English all-
words task. In Proceedings of SemEval-2007, pages
30?35, Prague, Czech Republic.
R. Navigli. 2006. Meaningful clustering of senses
helps boost word sense disambiguation perfor-
mance. In Proceedings of COLING-ACL 2006,
pages 105?112, Sydney, Australia.
R. Navigli. 2009. Word sense disambiguation: a sur-
vey. ACM Computing Surveys, 41(2):1?69.
S. Pad?o and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
M. Palmer, H. Trang Dang, and C. Fellbaum. 2007.
Making fine-grained and coarse-grained sense dis-
tinctions, both manually and automatically. Natural
Language Engineering, 13:137?163.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proceedings of KDD?02.
P. Pantel. 2005. Inducing ontological co-occurrence
vectors. In Proceedings of ACL?05, Ann Arbor,
Michigan.
S. Patwardhan and T. Pedersen. 2006. Using wordnet-
based context vectors to estimate the semantic relat-
edness of concepts. In Proceedings of the EACL 06
Workshop: Making Sense of Sense: Bringing Psy-
cholinguistics and Computational Linguistics To-
gether, Trento, Italy.
G. Ramakrishnan, B.P. Prithviraj, A. Deepa, P. Bhat-
tacharyya, and S. Chakrabarti. 2004. Soft word
sense disambiguation. In Proceedings of GWC 04,
Brno, Czech Republic.
B. Sch?olkopf, R. Williamson, A. Smola, J. Shawe-
Taylor, and J. Platt. 2000. Support vector method
for novelty detection. Advances in neural informa-
tion processing systems, 12.
H. Sch?utze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1).
C. Stokoe. 2005. Differentiating homonymy and pol-
ysemy in information retrieval. In Proceedings of
HLT/EMNLP-05, pages 403?410, Vancouver, B.C.,
Canada.
D. H. Tuggy. 1993. Ambiguity, polysemy and vague-
ness. Cognitive linguistics, 4(2):273?290.
449
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 668?677,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Unsupervised morphological segmentation and clustering with document
boundaries
Taesun Moon, Katrin Erk, and Jason Baldridge
Department of Linguistics
University of Texas at Austin
1 University Station B5100
Austin, TX 78712-0198 USA
{tsmoon,katrin.erk,jbaldrid}@mail.utexas.edu
Abstract
Many approaches to unsupervised mor-
phology acquisition incorporate the fre-
quency of character sequences with re-
spect to each other to identify word stems
and affixes. This typically involves heuris-
tic search procedures and calibrating mul-
tiple arbitrary thresholds. We present a
simple approach that uses no thresholds
other than those involved in standard ap-
plication of ?2 significance testing. A
key part of our approach is using docu-
ment boundaries to constrain generation of
candidate stems and affixes and clustering
morphological variants of a given word
stem. We evaluate our model on English
and the Mayan language Uspanteko; it
compares favorably to two benchmark sys-
tems which use considerably more com-
plex strategies and rely more on experi-
mentally chosen threshold values.
1 Introduction
Unsupervised morphology acquisition attempts to
learn from raw corpora one or more of the follow-
ing about the written morphology of a language:
(1) the segmentation of the set of word types in a
corpus (Creutz and Lagus, 2007), (2) the cluster-
ing of word types in a corpus based on some notion
of morphological relatedness (Schone and Juraf-
sky, 2000), (3) the generation of out-of-vocabulary
items which are morphologically related to other
word types in the corpus (Yarowsky et al, 2001).
We take a novel approach to segmenting words
and clustering morphologically related words.
The approach uses no parameters that need to
be tuned on data. The two main ideas of the
approach are (a) the filtering of affixes by sig-
nificant co-occurrence, and (b) the integration of
knowledge of document boundaries when gener-
ating candidate stems and affixes and when clus-
tering morphologically related words. The main
application that we envision for our approach is
to produce interlinearized glossed texts for under-
resourced/endangered languages (Palmer et al,
2009). Thus, we strive to eliminate hand-tuned
parameters to enable documentary linguists to use
our model as a preprocessing step for their manual
analysis of stems and affixes. To require a docu-
mentary linguist?who is likely to have little to no
knowledge of NLP methods?to tune parameters is
unfeasible. Additionally, data-driven exploration
of parameter settings is unlikely to be reliable in
language documentation since datasets typically
are quite small. To be relevant in this context, a
model needs to produce useful results out of the
box.
Constraining learning by using document
boundaries has been used quite effectively in un-
supervised word sense disambiguation (Yarowsky,
1995). Many applications in information retrieval
are built on the statistical correlation between doc-
uments and terms. However, we are unaware of
cases where knowledge of document boundaries
has been used for unsupervised learning for mor-
phology. The intuition behind our approach is very
simple: if two words in a single document are
very similar in terms of orthography, then the two
words are likely to be related morphologically. We
measure how integrating these assumptions into
our model at different stages affects performance.
We define a simple pipeline model. After gen-
erating candidate stems and affixes (possibly con-
strained by document boundaries), a ?2 test based
on global corpus counts filters out unlikely affixes.
Mutually consistent affix pairs are then clustered
to form affix groups. These in turn are used to
build morphologically related word clusters, pos-
sibly constrained by evidence from co-occurence
of word forms in documents. Following Schone
and Jurafsky (2000), clusters are evaluated for
668
whether they capture inflectional paradigms using
CELEX (Baayen et al, 1993).
We are unaware of other work on morphology
using ?2 tests despite its wide application across
many disciplines.1 This may be due to the large
degree of noise found in the candidate affix sets
induced through other candidate generation meth-
ods. The ?2 test has two standard thresholds?a
significance threshold and a lower bound on ob-
served counts. These are the only manually set
parameters we require?and we in fact use the
widely accepted standard values for these thresh-
olds without varying them in our experiments.
This is a significant improvement over other ap-
proaches that typically require a number of arbi-
trary thresholds and parameters yet provide little
intuitive justification for them. (We give examples
of these in ?3.)
We evaluate our approach on two languages,
English and Uspanteko, and compare its per-
formance to two benchmark systems, Morfessor
(Creutz and Lagus, 2007) and Linguistica (Gold-
smith, 2001). English is commonly used in other
studies and permits the use of CELEX as a gold
standard for evaluation. Uspanteko is an endan-
gered Mayan language for which we have a set of
interlinearized glossed texts (IGT) (Pixabaj et al,
2007; Palmer et al, 2009). IGT provides word-
by-word morpheme segmenation, which we use
to create a synthetic gold standard. In addition
to evaluation against this standard, Telma Kaan
Pixabaj?a Mayan linguist who helped create the
annotated corpus?reviewed by hand 100 word
clusters produced by our system, Morfessor and
Linguistica. Note that because English is suffixal
and Uspanteko is both prefixal and suffixal, we use
a slightly modified model for Uspanteko.
The approach introduced in this paper compares
favorably to Linguistica and Morfessor, two mod-
els that employ much more complex strategies and
rely on experimentally-tuned language/corpus-
specific parameters. In our evaluation, document
boundary awareness greatly benefits precision for
small datasets, blocking acquisition of spurious af-
fixes. For large datasets, global candidate genera-
tion outperforms document-aware candidate gen-
eration at the task of filtering out spurious stems,
but document-aware clustering improves preci-
sion. These findings are promising for the applica-
tion of this approach to under-resourced languages
1Monson (2004) suggests, but does not actually use, ?2.
like Uspanteko.
2 Unsupervised morphology acquisition
Unsupervised morphology acquisition aims to
model one or more of three properties of writ-
ten morphology: segmentation, clustering around
a common stem, and generation of new word
forms with productive affixes. Intuitively, there are
straightforward, but non-trivial, challenges that
arise when evaluating a model. One large chal-
lenge is distinguishing derivational from inflec-
tional morphology. Most approaches deal with to-
kens without considering context. Since inflec-
tional morphology is virtually always driven by
syntax and word context, such approaches are un-
able to learn only inflectional morphology or only
derivational morphology. Even approaches which
take context into consideration (Schone and Juraf-
sky, 2000; Baroni et al, 2002; Freitag, 2005) can-
not learn specifically for one or the other.
In addition, the evaluation of both segmentation
and clustering involves arbitrary judgment calls.
Concerning segmentation, should altimeter and
altitude be one morpheme or two? (The sam-
ple English gold standard for MorphoChallenge
2009 provides alti+meter but altitude.) Similar is-
sues arise when evaluating clusters of related word
forms if inflection and derivation are not distin-
guished. Does atheism belong to the same cluster
as theism? Where is the frequency cutoff point be-
tween a productive derivational morpheme and an
unproductive one? Yet, many studies have eval-
uated their segmentations and clusters by going
over their results word by word, cluster by cluster
and judging by sight whether some segmentation
or clustering is good (e.g., Goldsmith (2001)).
Like Schone and Jurafsky (2001), we build clus-
ters that will have both inflectionally and deriva-
tionally related stems and evaluate them with re-
spect to a gold standard of only inflectionally re-
lated stems.
3 Related work
There is a diverse body of existing work on unsu-
pervised morphology acquisition. We summarize
previous work, emphasizing some of its more ar-
bitrary and ad hoc aspects.
Letter successor variety. Letter successor va-
riety (LSV) models (Hafer and Weiss, 1974;
Gaussier, 1999; Bernhard, 2005; Bordag, 2005;
669
Keshava and Pitler, 2005; Hammarstro?m, 2006;
Dasgupta and Ng, 2007; Demberg, 2007) use the
hypothesis that there is less certainty when pre-
dicting the next character at morpheme bound-
aries. LSV has several issues that require fine pa-
rameter tuning. For example, Hafer and Weiss
(1974) counts how many types of characters ap-
pear after some initial string (the successor count)
and how many types of characters appear before
some final string (the predecessor count). A suc-
cessful criterion for segmenting a word was if the
predecessor count for the second part was greater
than 17 and the successor count for the first part
was greater than 5. Other studies have similar data
specific parameters and restrictions.
MDL and Bayesian models. Minimum descrip-
tion length (MDL) models (Goldsmith, 2001;
Creutz and Lagus, 2002; Creutz and Lagus, 2004;
Goldsmith, 2006; Creutz and Lagus, 2007) try to
segment words by maximizing the probability of
a training corpus subject to a penalty based on
the size of hypothesized morpheme lexicons they
build on the basis of the segmentations. While the-
oretically elegant, a pure implementation on real
data results in descriptions that do not reflect ac-
tual morphology. Creutz and Lagus (2005) re-
port that, ?frequent word forms remain unsplit,
whereas rare word forms are excessively split.? In
the end, every MDL approach uses probabilisti-
cally motivated refinements that restrict the ten-
dency of raw MDL to generate descriptions that
do not fit linguistic notions of morphology. De-
spite the sophistication of the models in this group,
there are many parameters that need to be set, and
heuristic search procedures are crucial for their
success (Goldwater, 2007). Snover et al (2002)
present a Bayesian model that uses a prior distribu-
tion to refine disjoint clusters of morphologically
related words. It disposes with parameter setting
by selecting the highest ranking hypothesis.
Context aware approaches. A word?s mor-
phology is strongly influenced by its syntactic and
semantic context. Schone and Jurafsky (2000) at-
tempts to cluster morphologically related words
starting with an unrefined trie search (but with a
parameter of minimum possible stem length and
an upper bound on potential affix candidates) that
is constrained by semantic similarity in a word
context vector space. Schone and Jurafsky (2001)
builds on this approach, but adds more ad hoc
parameters to handle circumfixation. Baroni et
al. (2002) takes a similar approach but uses edit
distance to cluster words that are similar but do
not necessarily share a long, contiguous substring.
They remove noise by constraining cluster mem-
bership with mutual information derived semantic
similarity. Freitag (2005) uses a mutual informa-
tion derived measure to learn the syntactic simi-
larity between words and clusters them. Then he
derives finite state machines across words in dif-
ferent clusters and refines them through a graph
walk algorithm. This group is the only one to eval-
uate against CELEX (Schone and Jurafsky, 2000;
Schone and Jurafsky, 2001; Freitag, 2005).
Others. Some other models require input such
as POS tables and lexicons and use a wider range
of information about the corpus (Yarowsky and
Wicentowski, 2000; Yarowsky et al, 2001; Chan,
2006). Because of the knowledge dependence of
these models, they are able to properly induce
inflectional morphology, as opposed to the stud-
ies cited above. Snyder and Barzilay (2008) uses
a set of aligned phrases across related languages
to learn how to segment words with a Bayesian
model and is otherwise fully unsupervised.
4 Model2
Our goal is to generate conflation sets: sets of
word types that are related through either inflec-
tional or derivational morphology (Schone and Ju-
rafsky, 2000). Solving this task requires learning
how individual types are segmented (though the
segmentation itself is not evaluated). For present
purposes, we assume that the affixal pattern of the
language is known: whether it is prefixal, suffixal,
or both. To simplify presentation, we discuss a
model that captures suffixes only. Our approach is
a four stage process:
1. Candidate Generation: generate candidate
stems and affixes using an orthographically
defined data structure (a trie)
2. Candidate Filtering: filter candidate affixes
using the statistical significance for pairs of
affixes based on their co-occurence counts
with shared stems
3. Affix Clustering: cluster significant affix pairs
into affix groups
2The code implementing the model is available from
http://comp.ling.utexas.edu/earl
670
4. Word Clustering: form conflation sets based
on affix clusters
The first and last stages are particularly prone to
noise, which has necessitated many of the thresh-
olds and heuristics employed in previous work.
We hypothesize that naturally occuring document
boundaries provide a strong constraint that should
reduce this noise, and we test that hypothesis by
using it in those stages.
Our intuition comes from an observation by
Yarowsky (1995) regarding multiple tokens of
words in documents. He tabulates the applicabil-
ity of using document boundaries to disambiguate
word senses, which measures how often a given
word occurs more than twice in the same docu-
ment. For ten potentially ambiguous words, he
counts how often they occur more than once in
some document and finds that if the words do oc-
cur, they do so multiple times in 50.1% of these
documents, on average. His counts ignored mor-
phological variation, and it is likely the applica-
bility measure would have increased considerably:
if a content word is used more than once in some
text, it is likely to be repeated in different syntactic
contexts, requiring the word to be inflected or to be
derived for a different part-of-speech category. 3
For stage one, we build separate tries for each
document rather than a trie for the entire corpus.
This should reduce the chance that orthographi-
cally similar but morphologically unrelated word
pairs lead to bad candidates by reducing the search
space for words which share a stem to a local doc-
ument. For example, assuage and assume are both
likely to occur in a large corpus and suggest that
there is a stem assu with affixes -age and -me.
They are less likely to occur together in many dif-
ferent documents that form the corpus, whereas
assume, assumed, and assuming are. We refer to
this document constrained candidate generation as
CandGen-D, and to the unconstrained generation
(a single trie for all documents) as CandGen-G.
For stage four, documents are used to constrain
potential membership of words in clusters: all
pairs of words in a cluster must have occured to-
gether in some document. We refer to document-
constrained clustering as Clust-D and the uncon-
strained global clustering as Clust-G.
3For example, in just this one paragraph we have
{document,documents}, {measure, measures}, {occur, oc-
curs, occuring}, and {word, words}.
4.1 Candidate generation
Given a document or collection of documents, we
use tries (prefix trees) to identify potential stems
and affixes and collect statistics for co-occurrences
between affixes and between affixes and stems.
a
b c
d $
Figure 1
A trie G, like the example
on the right, can be iden-
tified with the set of all
words on paths from the
root to any leaf, in the case
of the example figure the
set G = {abd, ab$, ac}.
(We use $ to denote an
empty affix.) Given a trie
G over alphabet L, we de-
fine the set of trunks of G
as all paths from the root to a branching point:
Tr(G) = {w ? L+ |?a, b ? L, x
1
, x
2
? L
?
:
a 6= b ? wax
1
, wbx
2
? G}
Also, we define the set of branches of a trunk t ?
Tr(G) as the paths from its branching points to the
leaves:
Br(t,G) = {x ? L+ | tx ? G}
In our example, {a, ab} are the trunks, with
Br(a, G) = {bd, b$, c} and Br(ab, G) = {d, $}.
When we use a trie to induce stems and affixes,
all induced stems will be trunks, and all induced
affixes will be branches.
From a given trie, we induce a set of stem can-
didates and affix candidates. A simple criterion is
used: if a trunk is longer than all of its branches,
the trunk is a stem candidate and its branches are
affix candidates. So, the set of stem candidates for
a trie G, CStem(G), is the set of trunks t ? Tr(G)
such that |t| > |b| for all b ? Br(t,G).
Given a stem candidate s ? CStem(G), its set of
affix candidates CAff(s,G) is identical to its set of
branches. (To talk about the sets of stem and affix
candidates for a whole trie G or a set of tries, we
write CAff(G), StC(G), CAff, and CStem.) The
count of an affix candidate b ? CAff is the number
of stem candidates with which it occurs:
count(b) =
?
G
|{s ? CStem(G) | b ? CAff(s,G)}|
For Fig. 1, the set of stem candidates is {ab} (since
some branches of the trunk a are longer than the
671
trunk itself). The matching set of affix candidates
is CAff(ab, G) = {d, $}, each with a count of one.
An affix rule candidate is an unordered pair of
affix candidates {b
1
, b
2
}. It states that any stem
occurring with b
1
can also occur with b
2
. Affix
rules implement the assumption that all produc-
tive affixes will cooccur with other productive af-
fixes and that these will form a coherent group.
The rule candidates for a given stem candidate
s ? CStem(G) are:
CRule(s,G) =
{
{b
1
, b
2
} ? CAff(s,G) | b
1
6= b
2
}
For example, the single stem candidate ab in
Fig. 1 has one rule candidate, {d, $}. We also use
CRule(G) for the rule candidates of a trie G across
all stems, and CRule for the union of rule candi-
dates in a set of tries.
The count of a rule candidate r={b
1
, b
2
} in a
trie is the number of stem candidates it appears
with:
count(r) =
?
G
|{s ? CStem(G) | r ? CRule(s,G)}|
We also use CAff(s) for the set of affix candidates
of stem s across several tries, and CRule(s) for the
set of rule candidates of a stem s across several
tries.
Document-specific versus global candidate gen-
eration. CandGen-D defines separate tries for
every document in the corpus and induces stem,
affix and rule candidates for each document.
CandGen-G instead induces these candidates for
a global trie over all the words in the corpus.
From the perspective of the formalism laid out
above, the only difference is that CandGen-D
has as many tries G
i
as there are documents i
and CandGen-G has only one G. This simple
difference leads to different candidate sets and
counts over their occurrences. For example, say
two documents contain the pair putt/putts and
another contains bogey/bogeys. With CandGen-
D, count($)=3, count(s)=3, and count($, s)=2.
For the same documents, CandGen-G would pro-
duce count($)=2 and count(s)=2 since putt/putts
would have occurred only once in the global trie.
Also, consider a rare pair such as aard-
vark/aardvarks where each word is found in a dif-
ferent document. The pair would be identified
by CandGen-G but not by CandGen-D. The pair
would contribute a count of one to count($, s) in
CandGen-G but not in CandGen-D. So, CandGen-
G can provide better coverage, but it is also more
likely to identify noisy candidates, such as as-
suage/assumed, than CandGen-D.
4.2 Candidate filtering
The sets of candidates CStem,CAff,CRule is ex-
pected to be noisy since the only basis for gener-
ating them was strings that share a large portion of
their substrings. One way of filtering candidates is
to find affix candidates whose co-occurence with
other candidates is not statistically significant.
We measure correlation between candidate af-
fixes b
1
, b
2
in a candidate rule with the paired
?
2 test. By using ?2, we only consider pairwise
correlation between affixes, rather than attempting
global inference. Global consistency of affix sets
is not ensured, and as such the approach is sus-
ceptible to the multiple comparisons problem. We
still opt for this approach for its simplicity and be-
cause global inference is problematic due to data
sparseness.
Correlation between b
1
and b
2
is determined by
the following contingency table:4
b
1
? b
1
b
2
O
11
O
12
? b
2
O
21
O
22
Based on the significance testing, we define the set
of valid rules PairRule as those for which the ?2
test is significant at p < 0.05. Thus, affix can-
didates not significantly correlated with any other
affix in CAff are discarded.
4.3 Affix clustering
The previous stage produces a set of pairs of af-
fixes that are significantly correlated. However,
inflectional paradigms rarely contain just two af-
fixes, so we would like to group together affix
pairs into larger affix sets to improve generaliza-
tion. We use a bottom up, minimum distance clus-
tering for valid affix pairs (rules). We do not as-
sume that cluster membership is exclusive. For
example, it would not make sense to determine
that the null affix -$ can belong to only one cluster.
Therefore, we produce non-disjoint affix clusters.
A valid cluster of affixes is a maximal set of af-
fixes forming pairwise valid rules: Aff ? CAff is a
valid cluster of affixes iff
4where O
11
= count({b
1
, b
2
}), O
12
= count(b
2
) ?
O
11
, O
21
= count(b
1
)?O
11
, O
22
= N?O
11
?O
12
?O
21
and N =
P
b?CAff count(b). See table (1) for examples.
672
ed ?ed
ing 10273 21853
?ing 27120 4119332
(a) ?2 = 352678
le ?le
s 122 132945
?s 936 4044575
(b) ?2 = 239.132
ed ?ed
ing 2651 1310
?ing 1490 150848
(c) ?2 = 65101.6
le ?le
s 20 12073
?s 198 144008
(d) ?2 = 0.631, p = 0.427
Table 1: Affix counts in contingency tables for the valid pair ed/ing and spurious pair le/s according to
CandGen-D in (a) and (b) and according to CandGen-G in (c) and (d). ?2 test values are given under
each table. Data is from NYT. Total affix token counts induced through CandGen-D and CandGen-G
are N=4178578 and N=156299, respectively. A total of 2054 and 3739 affix types were induced for
CandGen-D and CandGen-G, respectively showing that CandGen-G does have better coverage though
it might have more noise.
1. ?b
1
, b
2
? Aff : {b
1
, b
2
} ? PairRule, and
2. If b ? CAff with ?b? ? Aff : {b, b?} ?
PairRule, then b ? Aff.
The set of all valid affix clusters is GroupRule.
This formulation does not rule out the existence
of clusters with affixes in common.
4.4 Word clustering
We next cluster word forms into morphologically
related groups. Our model assumes two word
forms to be morphologically related iff (1) they oc-
curred in the same trie G, (2) they have a trunk s in
common that is a stem in Stem(G), and (3) their af-
fixes under this stem s are members in a common
valid affix cluster in GroupRule. Hence a single
stem s can be involved in at most |GroupRule| con-
flation sets, one for each valid affix cluster. Again,
the only distinction between clustering with a
global trie (Clust-G) and clustering with several
tries from the documents in a corpus (Clust-D) is
that the former has only one trie.
We define the conflation set for a given stem s ?
Stem and valid affix cluster Aff ? GroupRule as
Wd(s,Aff) = {sb
1
, sb
2
| b
1
, b
2
? Aff ?
?G.s ? Stem(G) ? b
1
, b
2
? CAff(s,G)}
One issue that needs clarification is when the
candidate generation and clustering stages use dif-
ferent strategies, i.e. the models CandGen-D
+Clust-G and CandGen-G +Clust-D. This sim-
ply means that the statistics, and thus the valid
GroupRule, are derived from either CandGen-D or
CandGen-G.
4.5 Induction for languages that are both
prefixal and affixal
The above approach would not fit a language that
is prefixal and suffixal. Assuming we have in-
duced separate conflation sets over a prefix trie and
a suffix trie, we merge clusters between the two if
they have at least one word form in common. For-
mally, given a set of prefix conflation sets PCS and
a set of suffix conflation sets SCS, the final set of
conflation sets CS is:
CS = {p ? s |p ? PCS, s ? SCS ? p ? s 6= ?}
5 Data
We apply our method on English and Uspanteko,
an endangered Mayan language.
Learning corpora. For English, we use two
subsets of the NYTimes portion in the Gigaword
corpus which we will call NYT and MINI-NYT.
NYT in the current study is the complete collec-
tion of articles in the New York Times from June,
2002. NYT has 10K articles, 88K types and 9M
tokens. MINI-NYT is a subset of NYT with 190
articles, 15K types and 187K tokens.
The Uspanteko text, USP has 29 distinct texts,
7K types, and 50K tokens. The texts are from
OKMA (Pixabaj et al, 2007) and the segmenta-
tion and labels of the interlinear glossed text anno-
tations were checked for consistency and cleaned
up (Palmer et al, 2009). All counts are for lower-
cased, punctuation-removed word forms.
CELEX. The CELEX lexical database (Baayen
et al, 1993) has been built for Dutch, English and
German and provides detailed entries that list and
analyze the morphological properties of words,
among other information. Using CELEX, we eval-
uate on types rather than tokens. The performance
of the model is based on how many of the words it
judges to be morphologically related overlap with
the entries in CELEX. Following previous work
(Schone and Jurafsky, 2000; Schone and Jurafsky,
673
2001; Freitag, 2005), we evaluate on inflectional
clusters only, using the CELEX file listing clusters
of inflectional variants. 5
6 Experiments and evaluation
We outline our evaluation methodology, baselines,
benchmarks and results, and discuss the results.
6.1 Evaluation metric
Schone and Jurafsky (2000) give definitions for
correct (C), inserted (I), and deleted (D) words
in model-derived conflation sets in relation to a
gold standard. Their formulation does not allow
for multiple cluster membership of words. We ex-
tend the definition to incorporate this fact about the
data. Let w be a word form. We write X
w
for the
clusters induced by the model that contain w, and
Y
w
for gold standard clusters containing w. X
w
and Y
w
only count words which occurred in both
model and gold standard clusters. Then
C =
?
w
?
X
w
?
Y
w
(|X
w
? Y
w
|/|Y
w
|)
I =
?
w
?
X
w
?
Y
w
(|X
w
? (X
w
? Y
w
)|/|Y
w
|)
D =
?
w
?
X
w
?
Y
w
(|Y
w
? (X
w
? Y
w
)|/|Y
w
|)
Based on these definitions, we formulate preci-
sion (P ), recall (R), and the f -score (F ) as: P =
C/(C+I), R = C/(C+D), F = (2PR)/(P+R).
USP evaluation We use two different means to
evaluate the performance on USP. One is the
f -score derived from the above section with re-
spect to a standard that was automatically gen-
erated from the morpheme segment tiers of the
OKMA IGT. We generated the standard by taking
non-hyphenated segments as the stem and cluster-
ing words with shared stems.
We also had an expert in Uspanteko manually
evaluate a random subset (N = 100) of the model
output to compensate for any failings in the stan-
dard. The evaluator determined a dominant stem
for a cluster and identified words which were not
related to that stem. We measured accuracy and
5CELEX does have a second file listing words and their
breakup into constituent morphemes for both derivation and
inflection, but its use would have required additional process-
ing that could introduce errors.
0 10 20 30 40 50 60 70 80 90 100
40
50
60
70
80
90
100
precision
re
ca
ll
 
 
mini?NYT
NYT
Usp?S
Usp?P
Figure 2: Precision/recall graph for baseline ex-
periments on English, prefix USP (Usp-P) and suf-
fix USP (Usp-S).
full cluster accuracy6 for the expert evaluations
(table 4).
We experimented on Uspanteko with three dif-
ferent assumptions: (1) it is only prefixal; (2) it is
only suffixal; (3) it is both prefixal and suffixal.
We applied the assumptions of only prefixal or
only suffixal to LINGUISTICA as well. The rele-
vant results are given row headers in tables with a
corresponding +P(prefix) or +S(suffix).
6.2 Baselines and benchmarks
In a set of baselines, we put words which share
the first k characters into the same cluster. We
do this for NYT, MINI-NYT, and USP in a pre-
fix tree, and for USP in suffix tree (using the last k
characters). We set the values of 0 < k < max,
where max is the length of the longest string, and
plot the results in a precision-recall graph (Fig. 2).
Low k corresponds to high recall and low preci-
sion while high k shows the opposite. The contrast
in morphological patterns for each language can
also be seen. Because Uspanteko is morpholog-
ically complex with suffixes and prefixes, a very
simple strategy cannot achieve high recall as op-
posed to English where it is possible to retrieve all
variants with a simple prefix tree.
We use Linguistica (Goldsmith, 2001) and Mor-
fessor (Creutz and Lagus, 2007) as benchmarks.
We used the default settings for these programs.
Note that comparison with these tools is not com-
6Given a model cluster C
i
and the ?misses? for each clus-
ter M
i
, accuracy is measured as 1/N
P
i
(|C
i
|?|M
i
|)/(|C
i
|)
where N is the sample size. Full cluster accuracy is the num-
ber of clusters that did not have any misses over N .
674
MINI-NYT NYT
P R F P R F
LINGUISTICA 64.30 93.34 76.15 47.50 88.33 61.77
MORFESSOR 45.2 87.8 59.7 63.6 69.2 66.3
CandGen-D + Clust-G 69.41 91.42 78.91 46.00 79.81 58.36
CandGen-D + Clust-D 83.47 80.36 81.89 59.02 74.50 65.86
CandGen-G + Clust-G 73.44 88.72 80.36 61.81 82.98 70.85
CandGen-G + Clust-D 88.34 77.95 82.82 77.71 70.24 73.79
Table 2: Results on English for all models in precision(P), recall(R), f -score(F) for each data set.
pletely fair. Morfessor only generates segmenta-
tions. We therefore processed Morfessor output
by clustering words by assuming that the longest
segment in any segmentation is the stem and eval-
uated this instead. Linguistica produces stems and
associated suffixes so the clusters naturally follow
from this output. However, Linguistica only infers
either prefix or suffix patterns.
6.3 Results and discussion
The results on English are in table 2 with ?2 test
criteria of p<0.05 and each cell in the contingency
table >5. CandGen-G +Clust-D had the best f -
score, and easily beats the benchmarks.
This is different from our expectation that
awareness of document boundaries at all stages
(i.e., CandGen-D +Clust-D) would show the best
results. The discrepancy is especially marked for
the larger NYT. One important reason for this is
the affix criterion itself: trunks must be longer than
branches. Consider again the sample contingency
tables in Table 1 that were derived from NYT
through CandGen-D and CandGen-G. We had as-
sumed at the outset that CandGen-D would be bet-
ter able to filter out noise and would be sparser, but
results show the opposite. The reason is that that
short words in a global lexicon are more likely to
share trunks with longer, unrelated words. This
ensures that short word forms rarely generate can-
didate affixes. Longer words which are less likely
to have spurious long branches generate the bulk
of candidate suffixes and stems. This is born out
by the stems that were associated with the spuri-
ous suffix pair le/s: CandGen-G has cliente, cripp,
crumb, daniel, ender, label, mccord, nag, oval,
sear, stubb, whipp. CandGen-D has crumb, hand,
need, sing, tab, trick, trip. The word forms that
are associated with le/s through the CandGen-D
strategy are crumble/crumbs, handle/hands, . . . .
Compare this with the word forms associated with
the search strategy CandGen-G such as clien-
tele/clientes, cripple/crips, . . . . The majority of
them are not common English words; they are
most probably proper names such as LaBelle and
Searle. Furthermore, there is no item among the
stems from the CandGen-G search where concate-
nating the stems le and s would result in both word
forms being a common noun or verb as is the
case with the stems from the CandGen-D search
where all concatenated word forms are common
English words. Though CandGen-G finds spuri-
ous stems, the counts for the spurious affix pair are
suppressed (see table 1) because it is a type count
rather than a token count. This results in le/s be-
ing properly excluded as a rule. This explains why
CandGen-D has worse precision in general than
CandGen-G.
The affix criterion has other minor issues. One
is that it ignores the few cases where stems are
shorter than affixes, such as the very common
words be, do, go.7 Assuming that the longest
productive inflectional suffix in English is -ing8,
the criterion would correctly find stem candidates
for -ing only when the stem is longer than 3 or
4 letters. Another is that the criterion, when
combined with CandGen-D, generates candidates
from the/them/then/their/these which cooccur fre-
quently in documents. This is not an issue when
the criterion is applied in CandGen-G.
Nonetheless, results show that when data sizes
are small, as with USP (Table 3) and MINI-NYT,
awareness of document boundaries at the candi-
date generation stage is beneficial to precision.
7The exclusion of such words in a token based evaluation
as opposed to a type based evaluation would heavily penalize
our approach. We are not aware, however, of any prior work
in unsupervised morphology that evaluates over tokens.
8with occasional gemination of final consonant such as
occur ? occurring
675
P R F
Ca-D + Cl-D 70.51 44.35 54.45
Ca-G + Cl-G 70.00 46.87 56.15
Ca-D + Cl-D + S 88.58 45.21 59.86
Ca-D + Cl-G + S 85.03 44.75 58.64
Ca-G + Cl-D + S 90.34 45.48 60.50
Ca-G + Cl-G + S 84.54 46.03 59.60
Ca-D + Cl-D + P 93.84 47.90 63.42
Ca-D + Cl-G + P 89.94 47.38 62.06
Ca-G + Cl-D + P 95.42 47.89 63.78
Ca-G + Cl-G + P 92.03 50.01 64.80
LINGUISTICA + S 81.14 47.60 60.00
LINGUISTICA + P 84.15 52.00 64.28
MORFESSOR 28.12 62.28 38.75
Table 3: Performance of models on automatically
generated USP evaluation set. P: Prefix only, S:
Suffix only. If there is no indication of S or P, it
means model attempted to learn both
Acc. FAcc. Avg. Sz.
Ca-G + Cl-G 98.5 79.0 2.94
LINGUISTICA 96.0 85.0 2.64
MORFESSOR 85.3 55.0 4.8
Table 4: Human expert evaluated accuracy (Acc.)
and full cluster accuracy (FAcc.) of models on
USP and average cluster size in words (Avg. Sz.)
However, it seems that CandGen-G has better cov-
erage no matter the size of the corpus, which
explains why coupling it with Clust-D produces
overall better scores. Clust-D does provide a use-
ful added constraint to mere orthographic similar-
ity (i.e. shared trunks in a trie).
A worrisome aspect of the results is that perfor-
mance degrades for large data sets (this is also true
for Linguistica). However, it also hints that this
method might work well for under-resourced lan-
guages. We surmise that since productive suffixes
do not suffer from sparsity, even a small data set
provides sufficient evidence to reach reliable con-
clusions about the productive morphology of some
language. Increasing the size of the data merely
increases the counts of spurious affixes and poses
problems for a relative simple measure such as
the ?2 test. A similar result was shown in Creutz
and Lagus (2005) where f -score performance of
their segmentation method improved as more data
was provided then decreased as the input exceeded
250K tokens in English. Their method showed
continued improvement with increased data for
Finnish. This hints that more data is beneficial
for morphologically complex languages but not
for morphologically impoverished languages.
Finally, it is also encouraging that the manual
evaluation (Table 4) shows very high accuracy, as
judged by a documentary linguist. Both our model
and Linguistica perform very well under this eval-
uation.
7 Conclusion
We have presented a novel approach to unsuper-
vised morphology acquisition that uses a very
simple pipeline and does not use any thresholds
other than standard ones associated with the ?2
test. The model relies on document boundaries
and correlation tests for filtering spurious stems
and affixes. The model compares favorably to
Linguistica and Morfessor, two models that em-
ploy much more complex strategies and rely on
fine-tuned parameters. We found that the use of
document boundaries is especially beneficial with
small datasets, which is promising for the applica-
tion of this model to under-resourced languages.
For large datasets, global candidate generation
outperformed document-aware candidate genera-
tion at the task of filtering out spurious stems,
but document-aware clustering does improve pre-
cision and overall performance.
In this paper we have addressed one aspect of
morphology acquisition, segmentation and clus-
tering. Extending the approach is straightforward,
for example, substituting more sophisticated data
structures or statistical tests for the current ones.
In particular, we will move from the use of doc-
ument boundaries to a flexible notion of textual
distance to estimate likelihood of morphological
relatedness.
Acknowledgments
This work is funded by NSF grant BCS 06651988
?Reducing Annotation Effort in the Documenta-
tion of Languages using Machine Learning and
Active Learning.? Thanks to Alexis Palmer, Telma
Kaan Pixabaj, Elias Ponvert, and the anonymous
reviewers.
676
References
R. H. Baayen, R. Piepenbrock, and H. van Rijn. 1993.
The CELEX lexical database on CD-ROM. Linguis-
tic Data Consortium, Philadelphia, PA.
M. Baroni, J. Matiasek, and H. Trost. 2002. Unsu-
pervised discovery of morphologically related words
based on orthographic and semantic similarity. In
ACL ?02 workshop on Morphological and phonolog-
ical learning, pages 48?57.
D. Bernhard. 2005. Unsupervised morphological seg-
mentation based on segment predictability and word
segments alignment. In Proceedings of Morpho
Challenge 2005, pages 18?27.
S. Bordag. 2005. Two-step approach to unsupervised
morpheme segmentation. In Proceedings of Morpho
Challenge 2005, pages 23?27.
E. Chan. 2006. Learning Probabilistic Paradigms for
Morphology in a Latent Class Model. In ACL SIG-
PHON ?06, pages 69?78.
M. Creutz and K. Lagus. 2002. Unsupervised dis-
covery of morphemes. In ACL ?02 workshop on
Morphological and phonological learning-Volume
6, pages 21?30.
M. Creutz and K. Lagus. 2004. Induction of a simple
morphology for highly-inflecting languages. In ACL
SIGPHON ?04, pages 43?51.
M. Creutz and K. Lagus. 2005. Inducing the morpho-
logical lexicon of a natural language from unanno-
tated text. In AKRR ?05, pages 106?113.
M. Creutz and K. Lagus. 2007. Unsupervised models
for morpheme segmentation and morphology learn-
ing. ACM Trans. Speech Lang. Process., 4(1):3.
S. Dasgupta and V. Ng. 2007. High-performance,
language-independent morphological segmentation.
In NAACL-HLT, pages 155?163.
V. Demberg. 2007. A language-independent unsu-
pervised model for morphological segmentation. In
ACL ?07, volume 45, page 920.
D. Freitag. 2005. Morphology induction from term
clusters. In CoNLL ?05.
E. Gaussier. 1999. Unsupervised learning of deriva-
tional morphology from inflectional lexicons. In
ACL workshop on Unsupervised Methods in Natu-
ral Language Learning.
J. Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Comp. Ling.,
27(2):153?198.
J. Goldsmith. 2006. An algorithm for the unsupervised
learning of morphology. Natural Language Engi-
neering, 12(04):353?371.
S.J. Goldwater. 2007. Nonparametric Bayesian mod-
els of lexical acquisition. Ph.D. thesis, Brown Uni-
versity.
M.A. Hafer and S.F. Weiss. 1974. Word Segmentation
by Letter Successor Varieties. Information Storage
and Retrieval, 10:371?385.
H. Hammarstro?m. 2006. A naive theory of affixation
and an algorithm for extraction. In ACL SIGPHON
?06, pages 79?88, June.
S. Keshava and E. Pitler. 2005. A simpler, intuitive
approach to morpheme induction. In Proceedings of
Morpho Challenge 2005, pages 28?32.
C. Monson. 2004. A framework for unsupervised nat-
ural language morphology induction. In Proceed-
ings of the Student Workshop at ACL, volume 4.
Alexis Palmer, Taesun Moon, and Jason Baldridge.
2009. Evaluating automation strategies in language
documentation. In Proceedings of the NAACL HLT
2009 Workshop on Active Learning for Natural Lan-
guage Processing, pages 36?44, Boulder, CO.
T.C. Pixabaj, M.A. Vicente Me?ndez, M. Vicente
Me?ndez, and O.A. Damia?n. 2007. Text collections
in Four Mayan Languages. Archived in The Archive
of the Indigenous Languages of Latin America.
P. Schone and D. Jurafsky. 2000. Knowledge-free in-
duction of morphology using latent sematic analysis.
In CoNLL-2000 and LLL-2000.
P. Schone and D. Jurafsky. 2001. Knowledge-free
induction of inflectional morphologies. In NAACL
?01, pages 1?9.
M.G. Snover, G.E. Jarosz, and M.R. Brent. 2002. Un-
supervised learning of morphology using a novel di-
rected search algorithm: taking the first step. In ACL
?02 workshop on Morphological and phonological
learning, pages 11?20.
B. Snyder and R. Barzilay. 2008. Unsupervised multi-
lingual learning for morphological segmentation. In
ACL ?08.
D. Yarowsky and R. Wicentowski. 2000. Minimally
supervised morphological analysis by multimodal
alignment. In ACL ?00, pages 207?216.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001.
Inducing multilingual text analysis tools via robust
projection across aligned corpora. In HLT ?01.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In ACL ?95,
pages 189?196.
677
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 216?223,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Simple, Similarity-based Model for Selectional Preferences
Katrin Erk
University of Texas at Austin
katrin.erk@mail.utexas.edu
Abstract
We propose a new, simple model for the auto-
matic induction of selectional preferences, using
corpus-based semantic similarity metrics. Fo-
cusing on the task of semantic role labeling,
we compute selectional preferences for seman-
tic roles. In evaluations the similarity-based
model shows lower error rates than both Resnik?s
WordNet-based model and the EM-based clus-
tering model, but has coverage problems.
1 Introduction
Selectional preferences, which characterize typ-
ical arguments of predicates, are a very use-
ful and versatile knowledge source. They have
been used for example for syntactic disambigua-
tion (Hindle and Rooth, 1993), word sense dis-
ambiguation (WSD) (McCarthy and Carroll,
2003) and semantic role labeling (SRL) (Gildea
and Jurafsky, 2002).
The corpus-based induction of selectional
preferences was first proposed by Resnik (1996).
All later approaches have followed the same two-
step procedure, first collecting argument head-
words from a corpus, then generalizing to other,
similar words. Some approaches have used
WordNet for the generalization step (Resnik,
1996; Clark and Weir, 2001; Abe and Li, 1993),
others EM-based clustering (Rooth et al, 1999).
In this paper we propose a new, simple model
for selectional preference induction that uses
corpus-based semantic similarity metrics, such
as Cosine or Lin?s (1998) mutual information-
based metric, for the generalization step. This
model does not require any manually created
lexical resources. In addition, the corpus for
computing the similarity metrics can be freely
chosen, allowing greater variation in the domain
of generalization than a fixed lexical resource.
We focus on one application of selectional
preferences: semantic role labeling. The ar-
gument positions for which we compute selec-
tional preferences will be semantic roles in the
FrameNet (Baker et al, 1998) paradigm, and
the predicates we consider will be semantic
classes of words rather than individual words
(which means that different preferences will be
learned for different senses of a predicate word).
In SRL, the two most pressing issues today are
(1) the development of strong semantic features
to complement the current mostly syntactically-
based systems, and (2) the problem of the do-
main dependence (Carreras and Marquez, 2005).
In the CoNLL-05 shared task, participating sys-
tems showed about 10 points F-score difference
between in-domain and out-of-domain test data.
Concerning (1), we focus on selectional prefer-
ences as the strongest candidate for informative
semantic features. Concerning (2), the corpus-
based similarity metrics that we use for selec-
tional preference induction open up interesting
possibilities of mixing domains.
We evaluate the similarity-based model
against Resnik?s WordNet-based model as well
as the EM-based clustering approach. In the
evaluation, the similarity-model shows lower er-
ror rates than both Resnik?s WordNet-based
model and the EM-based clustering model.
However, the EM-based clustering model has
higher coverage than both other paradigms.
Plan of the paper. After discussing previ-
216
ous approaches to selectional preference induc-
tion in Section 2, we introduce the similarity-
based model in Section 3. Section 4 describes
the data used for the experiments reported in
Section 5, and Section 6 concludes.
2 Related Work
Selectional restrictions and selectional prefer-
ences that predicates impose on their arguments
have long been used in semantic theories, (see
e.g. (Katz and Fodor, 1963; Wilks, 1975)). The
induction of selectional preferences from corpus
data was pioneered by Resnik (1996). All sub-
sequent approaches have followed the same two-
step procedure, first collecting argument head-
words from a corpus, then generalizing over the
seen headwords to similar words. Resnik uses
the WordNet noun hierarchy for generalization.
His information-theoretic approach models the
selectional preference strength of an argument
position1 rp of a predicate p as
S(rp) =
?
c
P (c|rp) log
P (c|rp)
P (c)
where the c are WordNet synsets. The prefer-
ence that rp has for a given synset c0, the selec-
tional association between the two, is then de-
fined as the contribution of c0 to rp?s selectional
preference strength:
A(rp, c0) =
P (c0|rp) log
P (c0|rp)
P (c0)
S(rp)
Further WordNet-based approaches to selec-
tional preference induction include Clark and
Weir (2001), and Abe and Li (1993). Brock-
mann and Lapata (2003) perform a comparison
of WordNet-based models.
Rooth et al (1999) generalize over seen head-
words using EM-based clustering rather than
WordNet. They model the probability of a word
w occurring as the argument rp of a predicate p
as being independently conditioned on a set of
classes C:
P (rp, w) =
?
c?C
P (c, rp, w) =
?
c?C
P (c)P (rp|c)P (w|c)
1We write rp to indicate predicate-specific roles, like
?the direct object of catch?, rather than just ?obj?.
The parameters P (c), P (rp|c) and P (w|c) are
estimated using the EM algorithm.
While there have been no isolated compar-
isons of the two generalization paradigms that
we are aware of, Gildea and Jurafsky?s (2002)
task-based evaluation has found clustering-
based approaches to have better coverage than
WordNet generalization, that is, for a given role
there are more words for which they can state a
preference.
3 Model
The approach we are proposing makes use of
two corpora, a primary corpus and a gener-
alization corpus (which may, but need not, be
identical). The primary corpus is used to extract
tuples (p, rp, w) of a predicate, an argument
position and a seen headword. The general-
ization corpus is used to compute a corpus-based
semantic similarity metric.
Let Seen(rp) be the set of seen headwords for
an argument rp of a predicate p. Then we model
the selectional preference S of rp for a possible
headword w0 as a weighted sum of the similari-
ties between w0 and the seen headwords:
Srp(w0) =
?
w?Seen(rp)
sim(w0, w) ? wtrp(w)
sim(w0, w) is the similarity between the seen
and the potential headword, and wtrp(w) is the
weight of seen headword w.
Similarity sim(w0, w) will be computed on
the generalization corpus, again on the ba-
sis of extracted tuples (p, rp, w). We will
be using the similarity metrics shown in Ta-
ble 1: Cosine, the Dice and Jaccard coefficients,
and Hindle?s (1990) and Lin?s (1998) mutual
information-based metrics. We write f for fre-
quency, I for mutual information, and R(w) for
the set of arguments rp for which w occurs as a
headword.
In this paper we only study corpus-based met-
rics. The sim function can equally well be in-
stantiated with a WordNet-based metric (for
an overview see Budanitsky and Hirst (2006)),
but we restrict our experiments to corpus-based
metrics (a) in the interest of greatest possible
217
simcosine(w,w?) =
P
rp
f(w,rp)?f(w?,rp)
qP
rp
f(w,rp)2?
qP
rp
f(w?,rp)2
simDice(w,w?) =
2?|R(w)?R(w?)|
|R(w)|+|R(w?)|
simLin(w,w?) =
P
rp?R(w)?R(w?)
I(w,r,p)I(w?,r,p)
P
rp?R(w)
I(w,r,p)
P
rp?R(w)
I(w?,r,p) simJaccard(w,w
?) = |R(w)?R(w
?)|
|R(w)?R(w?)|
simHindle(w,w?) =
?
rp simHindle(w,w
?, rp) where
simHindle(w,w?, rp) =
?
?
?
min(I(w,rp),I(w?,rp) if I(w, rp) > 0 and I(w?, rp) > 0
abs(max(I(w,rp),I(w?,rp))) if I(w, rp) < 0 and I(w?, rp) < 0
0 else
Table 1: Similarity measures used
resource-independence and (b) in order to be
able to shape the similarity metric by the choice
of generalization corpus.
For the headword weights wtrp(w), the sim-
plest possibility is to assume a uniform weight
distribution, i.e. wtrp(w) = 1. In addition, we
test a frequency-based weight, i.e. wtrp(w) =
f(w, rp), and inverse document frequency, which
weighs a word according to its discriminativity:
wtrp(w) = log
num. words
num. words to whose context w belongs .
This similarity-based model of selectional
preferences is a straightforward implementa-
tion of the idea of generalization from seen
headwords to other, similar words. Like the
clustering-based model, it is not tied to the
availability of WordNet or any other manually
created resource. The model uses two corpora,
a primary corpus for the extraction of seen head-
words and a generalization corpus for the com-
putation of semantic similarity metrics. This
gives the model flexibility to influence the simi-
larity metric through the choice of text domain
of the generalization corpus.
Instantiation used in this paper. Our aim
is to compute selectional preferences for seman-
tic roles. So we choose a particular instantia-
tion of the similarity-based model that makes
use of the fact that the two-corpora approach
allows us to use different notions of ?predicate?
and ?argument? in the primary and general-
ization corpus. Our primary corpus will con-
sist of manually semantically annotated data,
and we will use semantic verb classes as pred-
icates and semantic roles as arguments. Ex-
amples of extracted (p, rp, w) tuples are (Moral-
ity evaluation, Evaluee, gamblers) and (Placing,
Goal, briefcase). Semantic similarity, on the
other hand, will be computed on automatically
syntactically parsed corpus, where the predi-
cates are words and the arguments are syntac-
tic dependents. Examples of extracted (p, rp, w)
tuples from the generalization corpus include
(catch, obj, frogs) and (intervene, in, deal).2
This instantiation of the similarity-based
model allows us to compute word sense specific
selectional preferences, generalizing over manu-
ally semantically annotated data using automat-
ically syntactically annotated data.
4 Data
We use FrameNet (Baker et al, 1998), a se-
mantic lexicon for English that groups words
in semantic classes called frames and lists se-
mantic roles for each frame. The FrameNet
1.3 annotated data comprises 139,439 sentences
from the British National Corpus (BNC). For
our experiments, we chose 100 frame-specific se-
mantic roles at random, 20 each from five fre-
quency bands: 50-100 annotated occurrences
of the role, 100-200 occurrences, 200-500, 500-
1000, and more than 1000 occurrences. The
annotated data for these 100 roles comprised
59,608 sentences, our primary corpus. To deter-
mine headwords of the semantic roles, the cor-
pus was parsed using the Collins (1997) parser.
Our generalization corpus is the BNC. It was
parsed using Minipar (Lin, 1993), which is con-
siderably faster than the Collins parser but
failed to parse about a third of all sentences.
2For details about the syntactic and semantic analyses
used, see Section 4.
218
Accordingly, the arguments r extracted from
the generalization corpus are Minipar depen-
dencies, except that paths through preposition
nodes were collapsed, using the preposition as
the dependency relation. We obtained parses for
5,941,811 sentences of the generalization corpus.
The EM-based clustering model was com-
puted with all of the FrameNet 1.3 data (139,439
sentences) as input. Resnik?s model was trained
on the primary corpus (59,608 sentences).
5 Experiments
In this section we describe experiments com-
paring the similarity-based model for selectional
preferences to Resnik?s WordNet-based model
and to an EM-based clustering model3. For the
similarity-based model we test the five similar-
ity metrics and three weighting schemes listed
in section 3.
Experimental design
Like Rooth et al (1999) we evaluate selectional
preference induction approaches in a pseudo-
disambiguation task. In a test set of pairs
(rp, w), each headword w is paired with a con-
founder w? chosen randomly from the BNC ac-
cording to its frequency4. Noun headwords are
paired with noun confounders in order not to
disadvantage Resnik?s model, which only works
with nouns. The headword/confounder pairs
are only computed once and reused in all cross-
validation runs. The task is to choose the more
likely role headword from the pair (w,w?).
In the main part of the experiment, we count
a pair as covered if both w and w? are assigned
some level of preference by a model (?full cover-
age?). We contrast this with another condition,
where we count a pair as covered if at least one
of the two words w,w? is assigned a level of pref-
erence by a model (?half coverage?). If only one
is assigned a preference, that word is counted as
chosen.
To test the performance difference between
models for significance, we use Dietterich?s
3We are grateful to Carsten Brockmann and Detlef
Prescher for the use of their software.
4We exclude potential confounders that occur less
than 30 or more than 3,000 times.
Error Rate Coverage
Cosine 0.2667 0.3284
Dice 0.1951 0.3506
Hindle 0.2059 0.3530
Jaccard 0.1858 0.3506
Lin 0.1635 0.2214
EM 30/20 0.3115 0.5460
EM 40/20 0.3470 0.9846
Resnik 0.3953 0.3084
Table 2: Error rate and coverage (micro-
average), similarity-based models with uniform
weights.
5x2cv (Dietterich, 1998). The test involves
five 2-fold cross-validation runs. Let di,j (i ?
{1, 2}, j ? {1, . . . , 5}) be the difference in error
rates between the two models when using split
i of cross-validation run j as training data. Let
s2j = (d1,j? d?j)
2+(d2,j? d?j)2 be the variance for
cross-validation run j, with d?j =
d1,j+d2,j
2 . Then
the 5x2cv t? statistic is defined as
t? =
d1,1
?
1
5
?5
j=1 s
2
j
Under the null hypothesis, the t? statistic has
approximately a t distribution with 5 degrees of
freedom.5
Results and discussion
Error rates. Table 2 shows error rates and
coverage for the different selectional prefer-
ence induction methods. The first five mod-
els are similarity-based, computed with uniform
weights. The name in the first column is the
name of the similarity metric used. Next come
EM-based clustering models, using 30 (40) clus-
ters and 20 re-estimation steps6, and the last
row lists the results for Resnik?s WordNet-based
method. Results are micro-averaged.
The table shows very low error rates for the
similarity-based models, up to 15 points lower
than the EM-based models. The error rates
5Since the 5x2cv test fails when the error rates vary
wildly, we excluded cases where error rates differ by 0.8
or more across the 10 runs, using the threshold recom-
mended by Dietterich.
6The EM-based clustering software determines good
values for these two parameters through pseudo-
disambiguation tests on the training data.
219
Cos Dic Hin Jac Lin EM 40/20 Resnik
Cos -16 (73) -12 (73) -18 (74) -22 (57) 11 (67) 11 (74)
Dic 16 (73) 2 (74) -8 (85) -10 (64) 39 (47) 27 (62)
Hin 12 (73) -2 (74) -8 (75) -11 (63) 33 (57) 16 (67)
Jac 18 (74) 8 (85) 8 (75) -7 (68) 42 (45) 30 (62)
Lin 22 (57) 10 (64) 11 (63) 7 ( 68) 29 (41) 28 (51)
EM 40/20 -11 ( 67 ) -39 ( 47 ) -33 ( 57 ) -42 ( 45 ) -29 ( 41 ) 3 ( 72 )
Resnik -11 (74) -27 (62) -16 (67) -30 (62) -28 (51) -3 (72)
Table 3: Comparing similarity measures: number of wins minus losses (in brackets non-significant
cases) using Dietterich?s 5x2cv; uniform weights; condition (1): both members of a pair must be
covered
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0  100  200  300  400  500
e
r
r
o
r
_
r
a
t
e
numhw
Learning curve: num. headwords, sim_based-Jaccard-Plain, error_rate, all
Mon Apr 09 02:30:47 2007
1000-100-200500-1000200-50050-100
Figure 1: Learning curve: seen headwords ver-
sus error rate by frequency band, Jaccard, uni-
form weights
50-100 100-200 200-500 500-1000 1000-
Cos 0.3167 0.3203 0.2700 0.2534 0.2606
Jac 0.1802 0.2040 0.1761 0.1706 0.1927
Table 4: Error rates for similarity-based mod-
els, by semantic role frequency band. Micro-
averages, uniform weights
of Resnik?s model are considerably higher than
both the EM-based and the similarity-based
models, which is unexpected. While EM-based
models have been shown to work better in SRL
tasks (Gildea and Jurafsky, 2002), this has been
attributed to the difference in coverage.
In addition to the full coverage condition, we
also computed error rate and coverage for the
half coverage case. In this condition, the error
rates of the EM-based models are unchanged,
while the error rates for all similarity-based
models as well as Resnik?s model rise to values
between 0.4 and 0.6. So the EM-based model
tends to have preferences only for the ?right?
words. Why this is so is not clear. It may be a
genuine property, or an artifact of the FrameNet
data, which only contains chosen, illustrative
sentences for each frame. It is possible that
these sentences have fewer occurrences of highly
frequent but semantically less informative role
headwords like ?it? or ?that? exactly because of
their illustrative purpose.
Table 3 inspects differences between error
rates using Dietterich?s 5x2cv, basically confirm-
ing Table 2. Each cell shows the wins minus
losses for the method listed in the row when
compared against the method in the column.
The number of cases that did not reach signifi-
cance is given in brackets.
Coverage. The coverage rates of the
similarity-based models, while comparable
to Resnik?s model, are considerably lower than
for EM-based clustering, which achieves good
coverage with 30 and almost perfect coverage
with 40 clusters (Table 2). While peculiarities
of the FrameNet data may have influenced the
results in the EM-based model?s favor (see the
discussion of the half coverage condition above),
the low coverage of the similarity-based models
is still surprising. After all, the generalization
corpus of the similarity-based models is far
larger than the corpus used for clustering.
Given the learning curve in Figure 1 it is
unlikely that the reason for the lower cover-
age is data sparseness. However, EM-based
clustering is a soft clustering method, which
relates every predicate and every headword to
every cluster, if only with a very low probabil-
220
ity. In similarity-based models, on the other
hand, two words that have never been seen in
the same argument slot in the generalization
corpus will have zero similarity. That is, a
similarity-based model can assign a level of
preference for an argument rp and word w0 only
if R(w0) ? R(Seen(rp)) is nonempty. Since the
flexibility of similarity-based models extends to
the vector space for computing similarities, one
obvious remedy to the coverage problem would
be the use of a less sparse vector space. Given
the low error rates of similarity-based models,
it may even be advisable to use two vector
spaces, backing off to the denser one for words
not covered by the sparse but highly accurate
space used in this paper.
Parameters of similarity-based models.
Besides the similarity metric itself, which we dis-
cuss below, parameters of the similarity-based
models include the number of seen headwords,
the weighting scheme, and the number of similar
words for each headword.
Table 4 breaks down error rates by semantic
role frequency band for two of the similarity-
based models, micro-averaging over roles of the
same frequency band and over cross-validation
runs. As the table shows, there was some vari-
ation across frequency bands, but not as much
as between models.
The question of the number of seen headwords
necessary to compute selectional preferences is
further explored in Figure 1. The figure charts
the number of seen headwords against error rate
for a Jaccard similarity-based model (uniform
weights). As can be seen, error rates reach a
plateau at about 25 seen headwords for Jaccard.
For other similarity metrics the result is similar.
The weighting schemes wtrp had surprisingly
little influence on results. For Jaccard similar-
ity, the model had an error rate of 0.1858 for
uniform weights, 0.1874 for frequency weight-
ing, and 0.1806 for discriminativity. For other
similarity metrics the results were similar.
A cutoff was used in the similarity-based
model: For each seen headword, only the 500
most similar words (according to a given sim-
ilarity measure) were included in the computa-
Cos Dic Hin Jac Lin
(a) Freq. sim. 1889 3167 2959 3167 860
(b) Freq. wins 65% 73% 79% 72% 58%
(c) Num. sim. 81 60 67 60 66
(d) Intersec. 7.3 2.3 7.2 2.1 0.5
Table 5: Comparing sim. metrics: (a) avg. freq.
of similar words; (b) % of times the more fre-
quent word won; (c) number of distinct similar
words per seen headword; (d) avg. size of inter-
section between roles
tion; for all others, a similarity of 0 was assumed.
Experiments testing a range of values for this
parameter show that error rates stay stable for
parameter values ? 200.
So similarity-based models seem not overly
sensitive to the weighting scheme used, the num-
ber of seen headwords, or the number of similar
words per seen headword. The difference be-
tween similarity metrics, however, is striking.
Differences between similarity metrics.
As Table 2 shows, Lin and Jaccard worked best
(though Lin has very low coverage), Dice and
Hindle not as good, and Cosine showed the worst
performance. To determine possible reasons for
the difference, Table 5 explores properties of the
five similarity measures.
Given a set S = Seen(rp) of seen headwords
for some role rp, each similarity metric produces
a set like(S) of words that have nonzero simi-
larity to S, that is, to at least one word in S.
Line (a) shows the average frequency of words
in like(S). The results confirm that the Lin
and Cosine metrics tend to propose less frequent
words as similar.
Line (b) pursues the question of the frequency
bias further, showing the percentage of head-
word/confounder pairs for which the more fre-
quent of the two words ?won? in the pseudo-
disambiguation task (using uniform weights).
This it is an indirect estimate of the frequency
bias of a similarity metric. Note that the head-
word actually was more frequent than the con-
founder in only 36% of all pairs.
These first two tests do not yield any expla-
nation for the low performance of Cosine, as the
results they show do not separate Cosine from
221
Jaccard Cosine
Ride vehicle:Vehicle truck 0.05 boat 0.05
coach 0.04 van 0.04 ship 0.04 lorry 0.04 crea-
ture 0.04 flight 0.04 guy 0.04 carriage 0.04 he-
licopter 0.04 lad 0.04
Ingest substance:Substance loaf 0.04 ice
cream 0.03 you 0.03 some 0.03 that 0.03 er
0.03 photo 0.03 kind 0.03 he 0.03 type 0.03
thing 0.03 milk 0.03
Ride vehicle:Vehicle it 1.18 there 0.88 they
0.43 that 0.34 i 0.23 ship 0.19 second one 0.19
machine 0.19 e 0.19 other one 0.19 response
0.19 second 0.19
Ingest substance:Substance there 1.23
that 0.50 object 0.27 argument 0.27 theme
0.27 version 0.27 machine 0.26 result 0.26
response 0.25 item 0.25 concept 0.25 s 0.24
Table 6: Highest-ranked induced headwords (seen headwords omitted) for two semantic classes of
the verb ?take?: similarity-based models, Jaccard and Cosine, uniform weights.
all other metrics. Lines (c) and (d), however, do
just that. Line (c) looks at the size of like(S).
Since we are using a cutoff of 500 similar words
computed per word in S, the size of like(S) can
only vary if the same word is suggested as similar
for several seen headwords in S. This way, the
size of like(S) functions as an indicator of the
degree of uniformity or similarity that a sim-
ilarity metric ?perceives? among the members
of S. To facilitate comparison across frequency
bands, line (c) normalizes by the size of S, show-
ing |like(S)||S| micro-averaged over all roles. Here
we see that Cosine seems to ?perceive? consid-
erably less similarity among the seen headwords
than any of the other metrics.
Line (d) looks at the sets s25(r) of the 25 most
preferred potential headwords of roles r, show-
ing the average size of the intersection s25(r) ?
s25(r?) between two roles (preferences computed
with uniform weights). It indicates another pos-
sible reason for Cosine?s problem: Cosine seems
to keep proposing the same words as similar for
different roles. We will see this tendency also in
the sample results we discuss next.
Sample results. Table 6 shows samples of
headwords induced by the similarity-based
model for two FrameNet senses of the verb
?take?: Ride vehicle (?take the bus?) and In-
gest substance (?take drugs?), a semantic class
that is exclusively about ingesting controlled
substances. The semantic role Vehicle of the
Ride vehicle frame and the role Substance of In-
gest substance are both typically realized as the
direct object of ?take?. The table only shows
new induced headwords; seen headwords were
omitted from the list.
The particular implementation of the
similarity-based model we have chosen, using
frames and roles as predicates and arguments
in the primary corpus, should enable the model
to compute preferences specific to word senses.
The sample in Table 6 shows that this is indeed
the case: The preferences differ considerably
for the two senses (frames) of ?take?, at least
for the Jaccard metric, which shows a clear
preference for vehicles for the Vehicle role. The
Substance role of Ingest substance is harder to
characterize, with very diverse seen headwords
such as ?crack?, ?lines?, ?fluid?, ?speed?.
While the highest-ranked induced words for
Jaccard do include three food items, there is
no word, with the possible exception of ?ice
cream?, that could be construed as a controlled
substance. The induced headwords for the
Cosine metric are considerably less pertinent
for both roles and show the above-mentioned
tendency to repeat some high-frequency words.
The inspection of ?take? anecdotally con-
firms that different selectional preferences are
learned for different senses. This point (which
comes down to the usability of selectional pref-
erences for WSD) should be verified in an em-
pirical evaluation, possibly in another pseudo-
disambiguation task, choosing as confounders
seen headwords for other senses of a predicate
word.
6 Conclusion
We have introduced the similarity-based model
for inducing selectional preferences. Comput-
ing selectional preference as a weighted sum of
similarities to seen headwords, it is a straight-
222
forward implementation of the idea of general-
ization from seen headwords to other, similar
words. The similarity-based model is particu-
larly simple and easy to compute, and seems not
very sensitive to parameters. Like the EM-based
clustering model, it is not dependent on lexical
resources. It is, however, more flexible in that it
induces similarities from a separate generaliza-
tion corpus, which allows us to control the simi-
larities we compute by the choice of text domain
for the generalization corpus. In this paper we
have used the model to compute sense-specific
selectional preferences for semantic roles.
In a pseudo-disambiguation task the simila-
rity-based model showed error rates down to
0.16, far lower than both EM-based clustering
and Resnik?s WordNet model. However its cov-
erage is considerably lower than that of EM-
based clustering, comparable to Resnik?s model.
The most probable reason for this is the spar-
sity of the underlying vector space. The choice
of similarity metric is critical in similarity-based
models, with Jaccard and Lin achieving the best
performance, and Cosine surprisingly bringing
up the rear.
Next steps will be to test the similarity-based
model ?in vivo?, in an SRL task; to test the
model in a WSD task; to evaluate the model on
a primary corpus that is not semantically ana-
lyzed, for greater comparability to previous ap-
proaches; to explore other vector spaces to ad-
dress the coverage issue; and to experiment on
domain transfer, using an appropriate general-
ization corpus to induce selectional preferences
for a domain different from that of the primary
corpus. This is especially relevant in view of the
domain-dependence problem that SRL faces.
Acknowledgements Many thanks to Jason
Baldridge, Razvan Bunescu, Stefan Evert, Ray
Mooney, Ulrike and Sebastian Pado?, and Sabine
Schulte im Walde for helpful discussions.
References
N. Abe and H. Li. 1993. Learning word association
norms using tree cut pair models. In Proceedings of
ICML 1993.
C. Baker, C. Fillmore, and J. Lowe. 1998. The Berkeley
FrameNet project. In Proceedings of COLING-ACL
1998, Montreal, Canada.
C. Brockmann and M. Lapata. 2003. Evaluating and
combining approaches to selectional preference acqui-
sition. In Proceedings of EACL 2003, Budapest.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of semantic distance. Computational
Linguistics, 32(1).
X. Carreras and L. Marquez. 2005. Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of CoNLL-05, Ann Arbor, MI.
S. Clark and D. Weir. 2001. Class-based probability
estimation using a semantic hierarchy. In Proceedings
of NAACL 2001, Pittsburgh, PA.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings of ACL 1997,
Madrid, Spain.
T. Dietterich. 1998. Approximate statistical tests
for comparing supervised classification learning algo-
rithms. Neural Computation, 10:1895?1923.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3):245?
288.
D. Hindle and M. Rooth. 1993. Structural ambiguity and
lexical relations. Computational Linguistics, 19(1).
D. Hindle. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL 1990,
Pittsburg, Pennsylvania.
J. Katz and J. Fodor. 1963. The structure of a semantic
theory. Language, 39(2).
D. Lin. 1993. Principle-based parsing without overgen-
eration. In Proceedings of ACL 1993, Columbus, OH.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL 1998,
Montreal, Canada.
D. McCarthy and J. Carroll. 2003. Disambiguating
nouns, verbs and adjectives using automatically ac-
quired selectional preferences. Computatinal Linguis-
tics, 29(4).
P. Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational re-
alization. Cognition, 61:127?159.
M. Rooth, S. Riezler, D. Prescher, G. Carroll, and F. Beil.
1999. Inducing an semantically annotated lexicon via
EM-based clustering. In Proceedings of ACL 1999,
Maryland.
Y. Wilks. 1975. Preference semantics. In E. Keenan,
editor, Formal Semantics of Natural Language. Cam-
bridge University Press.
223
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 10?18,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Investigations on Word Senses and Word Usages
Katrin Erk
University of Texas at Austin
katrin.erk@mail.utexas.edu
Diana McCarthy
University of Sussex
dianam@sussex.ac.uk
Nicholas Gaylord
University of Texas at Austin
nlgaylord@mail.utexas.edu
Abstract
The vast majority of work on word senses
has relied on predefined sense invento-
ries and an annotation schema where each
word instance is tagged with the best fit-
ting sense. This paper examines the case
for a graded notion of word meaning in
two experiments, one which uses WordNet
senses in a graded fashion, contrasted with
the ?winner takes all? annotation, and one
which asks annotators to judge the similar-
ity of two usages. We find that the graded
responses correlate with annotations from
previous datasets, but sense assignments
are used in a way that weakens the case for
clear cut sense boundaries. The responses
from both experiments correlate with the
overlap of paraphrases from the English
lexical substitution task which bodes well
for the use of substitutes as a proxy for
word sense. This paper also provides two
novel datasets which can be used for eval-
uating computational systems.
1 Introduction
The vast majority of work on word sense tag-
ging has assumed that predefined word senses
from a dictionary are an adequate proxy for the
task, although of course there are issues with
this enterprise both in terms of cognitive valid-
ity (Hanks, 2000; Kilgarriff, 1997; Kilgarriff,
2006) and adequacy for computational linguis-
tics applications (Kilgarriff, 2006). Furthermore,
given a predefined list of senses, annotation efforts
and computational approaches to word sense dis-
ambiguation (WSD) have usually assumed that one
best fitting sense should be selected for each us-
age. While there is usually some allowance made
for multiple senses, this is typically not adopted by
annotators or computational systems.
Research on the psychology of concepts (Mur-
phy, 2002; Hampton, 2007) shows that categories
in the human mind are not simply sets with clear-
cut boundaries: Some items are perceived as
more typical than others (Rosch, 1975; Rosch and
Mervis, 1975), and there are borderline cases on
which people disagree more often, and on whose
categorization they are more likely to change their
minds (Hampton, 1979; McCloskey and Glucks-
berg, 1978). Word meanings are certainly related
to mental concepts (Murphy, 2002). This raises
the question of whether there is any such thing as
the one appropriate sense for a given occurrence.
In this paper we will explore using graded re-
sponses for sense tagging within a novel annota-
tion paradigm. Modeling the annotation frame-
work after psycholinguistic experiments, we do
not train annotators to conform to sense distinc-
tions; rather we assess individual differences by
asking annotators to produce graded ratings in-
stead of making a binary choice. We perform two
annotation studies. In the first one, referred to
as WSsim (Word Sense Similarity), annotators
give graded ratings on the applicability of Word-
Net senses. In the second one, Usim (Usage Sim-
ilarity), annotators rate the similarity of pairs of
occurrences (usages) of a common target word.
Both studies explore whether users make use of
a graded scale or persist in making binary deci-
sions even when there is the option for a graded
response. The first study additionally tests to what
extent the judgments on WordNet senses fall into
clear-cut clusters, while the second study allows
us to explore meaning similarity independently of
any lexicon resource.
10
2 Related Work
Manual word sense assignment is difficult for
human annotators (Krishnamurthy and Nicholls,
2000). Reported inter-annotator agreement (ITA)
for fine-grained word sense assignment tasks has
ranged between 69% (Kilgarriff and Rosenzweig,
2000) for a lexical sample using the HECTOR dic-
tionary and 78.6.% using WordNet (Landes et al,
1998) in all-words annotation. The use of more
coarse-grained senses alleviates the problem: In
OntoNotes (Hovy et al, 2006), an ITA of 90% is
used as the criterion for the construction of coarse-
grained sense distinctions. However, intriguingly,
for some high-frequency lemmas such as leave
this ITA threshold is not reached even after mul-
tiple re-partitionings of the semantic space (Chen
and Palmer, 2009). Similarly, the performance
of WSD systems clearly indicates that WSD is not
easy unless one adopts a coarse-grained approach,
and then systems tagging all words at best perform
a few percentage points above the most frequent
sense heuristic (Navigli et al, 2007). Good perfor-
mance on coarse-grained sense distinctions may
be more useful in applications than poor perfor-
mance on fine-grained distinctions (Ide and Wilks,
2006) but we do not know this yet and there is
some evidence to the contrary (Stokoe, 2005).
Rather than focus on the granularity of clus-
ters, the approach we will take in this paper
is to examine the phenomenon of word mean-
ing both with and without recourse to predefined
senses by focusing on the similarity of uses of a
word. Human subjects show excellent agreement
on judging word similarity out of context (Ruben-
stein and Goodenough, 1965; Miller and Charles,
1991), and human judgments have previously been
used successfully to study synonymy and near-
synonymy (Miller and Charles, 1991; Bybee and
Eddington, 2006). We focus on polysemy rather
than synonymy. Our aim will be to use WSsim
to determine to what extent annotations form co-
hesive clusters. In principle, it should be possi-
ble to use existing sense-annotated data to explore
this question: almost all sense annotation efforts
have allowed annotators to assign multiple senses
to a single occurrence, and the distribution of these
sense labels should indicate whether annotators
viewed the senses as disjoint or not. However,
the percentage of markables that received multi-
ple sense labels in existing corpora is small, and it
varies massively between corpora: In the SemCor
corpus (Landes et al, 1998), only 0.3% of all
markables received multiple sense labels. In the
SENSEVAL-3 English lexical task corpus (Mihal-
cea et al, 2004) (hereafter referred to as SE-3), the
ratio is much higher at 8% of all markables1. This
could mean annotators feel that there is usually a
single applicable sense, or it could point to a bias
towards single-sense assignment in the annotation
guidelines and/or the annotation tool. The WSsim
experiment that we report in this paper is designed
to eliminate such bias as far as possible and we
conduct it on data taken from SemCor and SE-3 so
that we can compare the annotations. Although we
use WordNet for the annotation, our study is not a
study of WordNet per se. We choose WordNet be-
cause it is sufficiently fine-grained to examine sub-
tle differences in usage, and because traditionally
annotated datasets exist to which we can compare
our results.
Predefined dictionaries and lexical resources are
not the only possibilities for annotating lexical
items with meaning. In cross-lingual settings, the
actual translations of a word can be taken as the
sense labels (Resnik and Yarowsky, 2000). Re-
cently, McCarthy and Navigli (2007) proposed
the English Lexical Substitution task (hereafter
referred to as LEXSUB) under the auspices of
SemEval-2007. It uses paraphrases for words in
context as a way of annotating meaning. The task
was proposed following a background of discus-
sions in the WSD community as to the adequacy
of predefined word senses. The LEXSUB dataset
comprises open class words (nouns, verbs, adjec-
tives and adverbs) with token instances of each
word appearing in the context of one sentence
taken from the English Internet Corpus (Sharoff,
2006). The methodology can only work where
there are paraphrases, so the dataset only contains
words with more than one meaning where at least
two different meanings have near synonyms. For
meanings without obvious substitutes the annota-
tors were allowed to use multiword paraphrases or
words with slightly more general meanings. This
dataset has been used to evaluate automatic sys-
tems which can find substitutes appropriate for the
context. To the best of our knowledge there has
been no study of how the data collected relates to
word sense annotations or judgments of semantic
similarity. In this paper we examine these relation-
1This is even though both annotation efforts use balanced
corpora, the Brown corpus in the case of SemCor, the British
National Corpus for SE-3.
11
ships by re-using data from LEXSUB in both new
annotation experiments and testing the results for
correlation.
3 Annotation
We conducted two experiments through an on-
line annotation interface. Three annotators partic-
ipated in each experiment; all were native British
English speakers. The first experiment, WSsim,
collected annotator judgments about the applica-
bility of dictionary senses using a 5-point rating
scale. The second, Usim, also utilized a 5-point
scale but collected judgments on the similarity in
meaning between two uses of a word. 2 The scale
was 1 ? completely different, 2 ? mostly different,
3 ? similar, 4 ? very similar and 5 ? identical. In
Usim, this scale rated the similarity of the two uses
of the common target word; in WSsim it rated the
similarity between the use of the target word and
the sense description. In both experiments, the an-
notation interface allowed annotators to revisit and
change previously supplied judgments, and a com-
ment box was provided alongside each item.
WSsim. This experiment contained a total of
430 sentences spanning 11 lemmas (nouns, verbs
and adjectives). For 8 of these lemmas, 50 sen-
tences were included, 25 of them randomly sam-
pled from SemCor 3 and 25 randomly sampled
from SE-3.4 The remaining 3 lemmas in the ex-
periment each had 10 sentences taken from the
LEXSUB data.
WSsim is a word sense annotation task using
WordNet senses.5 Unlike previous word sense an-
notation projects, we asked annotators to provide
judgments on the applicability of every WordNet
sense of the target lemma with the instruction: 6
2Throughout this paper, a target word is assumed to be a
word in a given PoS.
3The SemCor dataset was produced alongside WordNet,
so it can be expected to support the WordNet sense distinc-
tions. The same cannot be said for SE-3.
4Sentence fragments and sentences with 5 or fewer words
were excluded from the sampling. Annotators were given
the sentences, but not the original annotation from these re-
sources.
5WordNet 1.7.1 was used in the annotation of both SE-3
and SemCor; we used the more current WordNet 3.0 after
verifying that the lemmas included in this experiment had the
same senses listed in both versions. Care was taken addition-
ally to ensure that senses were not presented in an order that
reflected their frequency of occurrence.
6The guidelines for both experiments are avail-
able at http://comp.ling.utexas.edu/
people/katrin erk/graded sense and usage
annotation
Your task is to rate, for each of these descriptions,
how well they reflect the meaning of the boldfaced
word in the sentence.
Applicability judgments were not binary, but were
instead collected using the five-point scale given
above which allowed annotators to indicate not
only whether a given sense applied, but to what
degree. Each annotator annotated each of the 430
items. By having multiple annotators per item and
a graded, non-binary annotation scheme we al-
low for and measure differences between annota-
tors, rather than training annotators to conform to
a common sense distinction guideline. By asking
annotators to provide ratings for each individual
sense, we strive to eliminate all bias towards either
single-sense or multiple-sense assignment. In tra-
ditional word sense annotation, such bias could be
introduced directly through annotation guidelines
or indirectly, through tools that make it easier to
assign fewer senses. We focus not on finding the
best fitting sense but collect judgments on the ap-
plicability of all senses.
Usim. This experiment used data from LEXSUB.
For more information on LEXSUB, see McCarthy
and Navigli (2007). 34 lemmas (nouns, verbs, ad-
jectives and adverbs) were manually selected, in-
cluding the 3 lemmas also used in WSsim. We se-
lected lemmas which exhibited a range of mean-
ings and substitutes in the LEXSUB data, with
as few multiword substitutes as possible. Each
lemma is the target in 10 LEXSUB sentences. For
our experiment, we took every possible pairwise
comparison of these 10 sentences for a lemma. We
refer to each such pair of sentences as an SPAIR.
The resulting dataset comprised 45 SPAIRs per
lemma, adding up to 1530 comparisons per anno-
tator overall.
In this annotation experiment, annotators saw
SPAIRs with a common target word and rated the
similarity in meaning between the two uses of the
target word with the instruction:
Your task is to rate, for each pair of sentences, how
similar in meaning the two boldfaced words are on
a five-point scale.
In addition annotators had the ability to respond
with ?Cannot Decide?, indicating that they were
unable to make an effective comparison between
the two contexts, for example because the mean-
ing of one usage was unclear. This occurred in
9 paired occurrences during the course of anno-
tation, and these items (paired occurrences) were
12
excluded from further analysis.
The purpose of Usim was to collect judgments
about degrees of similarity between a word?s
meaning in different contexts. Unlike WSsim,
Usim does not rely upon any dictionary resource
as a basis for the judgments.
4 Analyses
This section reports on analyses on the annotated
data. In all the analyses we use Spearman?s rank
correlation coefficient (?), a nonparametric test,
because the data does not seem to be normally
distributed. We used two-tailed tests in all cases,
rather than assume the direction of the relation-
ship. As noted above, we have three annotators
per task, and each annotator gave judgments for
every sentence (WSsim) or sentence pair (Usim).
Since the annotators may vary as to how they use
the ordinal scale, we do not use the mean of judg-
ments7 but report all individual correlations. All
analyses were done using the R package.8
4.1 WSsim analysis
In the WSsim experiment, annotators rated the ap-
plicability of each WordNet 3.0 sense for a given
target word occurrence. Table 1 shows a sample
annotation for the target argument.n. 9
Pattern of annotation and annotator agree-
ment. Figure 1 shows how often each of the five
judgments on the scale was used, individually and
summed over all annotators. (The y-axis shows
raw counts of each judgment.) We can see from
this figure that the extreme ratings 1 and 5 are used
more often than the intermediate ones, but annota-
tors make use of the full ordinal scale when judg-
ing the applicability of a sense. Also, the figure
shows that annotator 1 used the extreme negative
rating 1 much less than the other two annotators.
Figure 2 shows the percentage of times each judg-
ment was used on senses of three lemmas, differ-
ent.a, interest.n, and win.v. In WordNet, they have
5, 7, and 4 senses, respectively. The pattern for
win.v resembles the overall distribution of judg-
ments, with peaks at the extreme ratings 1 and 5.
The lemma interest.n has a single peak at rating
1, partly due to the fact that senses 5 (financial
7We have also performed several of our calculations us-
ing the mean judgment, and they also gave highly significant
results in all the cases we tested.
8http://www.r-project.org/
9We use word.PoS to denote a target word (lemma).
Annotator 1 Annotator 2 Annotator 3 overall
1
2
3
4
5
0
500
1000
1500
2000
2500
3000
Figure 1: WSsim experiment: number of times
each judgment was used, by annotator and
summed over all annotators. The y-axis shows raw
counts of each judgment.
different.a interest.n win.v
1
2
3
4
5
0.0
0.1
0.2
0.3
0.4
0.5
Figure 2: WSsim experiment: percentage of times
each judgment was used for the lemmas differ-
ent.a, interest.n and win.v. Judgment counts were
summed over all three annotators.
involvement) and 6 (interest group) were rarely
judged to apply. For the lemma different.a, all
judgments have been used with approximately the
same frequency.
We measured the level of agreement between
annotators using Spearman?s ? between the judg-
ments of every pair of annotators. The pairwise
correlations were ? = 0.506, ? = 0.466 and ? =
0.540, all highly significant with p < 2.2e-16.
Agreement with previous annotation in
SemCor and SE-3. 200 of the items in WSsim
had been previously annotated in SemCor, and
200 in SE-3. This lets us compare the annotation
results across annotation efforts. Table 2 shows
the percentage of items where more than one
sense was assigned in the subset of WSsim from
SemCor (first row), from SE-3 (second row), and
13
Senses
Sentence 1 2 3 4 5 6 7 Annotator
This question provoked arguments in America about the
Norton Anthology of Literature by Women, some of the
contents of which were said to have had little value as
literature.
1 4 4 2 1 1 3 Ann. 1
4 5 4 2 1 1 4 Ann. 2
1 4 5 1 1 1 1 Ann. 3
Table 1: A sample annotation in the WSsim experiment. The senses are: 1:statement, 2:controversy,
3:debate, 4:literary argument, 5:parameter, 6:variable, 7:line of reasoning
WSsim judgment
Data Orig. ? 3 ? 4 5
WSsim/SemCor 0.0 80.2 57.5 28.3
WSsim/SE-3 24.0 78.0 58.3 27.1
All WSsim 78.8 57.4 27.7
Table 2: Percentage of items with multiple senses
assigned. Orig: in the original SemCor/SE-3 data.
WSsim judgment: items with judgments at or
above the specified threshold. The percentages for
WSsim are averaged over the three annotators.
all of WSsim (third row). The Orig. column
indicates how many items had multiple labels in
the original annotation (SemCor or SE-3) 10. Note
that no item had more than one sense label in
SemCor. The columns under WSsim judgment
show the percentage of items (averaged over
the three annotators) that had judgments at or
above the specified threshold, starting from rating
3 ? similar. Within WSsim, the percentage of
multiple assignments in the three rows is fairly
constant. WSsim avoids the bias to one sense
by deliberately asking for judgments on the
applicability of each sense rather than asking
annotators to find the best one.
To compute the Spearman?s correlation between
the original sense labels and those given in the
WSsim annotation, we converted SemCor and
SE-3 labels to the format used within WSsim: As-
signed senses were converted to a judgment of 5,
and unassigned senses to a judgment of 1. For the
WSsim/SemCor dataset, the correlation between
original and WSsim annotation was ? = 0.234,
? = 0.448, and ? = 0.390 for the three anno-
tators, each highly significant with p < 2.2e-16.
For the WSsim/SE-3 dataset, the correlations were
? = 0.346, ? = 0.449 and ? = 0.338, each of them
again highly significant at p < 2.2e-16.
Degree of sense grouping. Next we test to what
extent the sense applicability judgments in the
10Overall, 0.3% of tokens in SemCor have multiple labels,
and 8% of tokens in SE-3, so the multiple label assignment in
our sample is not an underestimate.
p < 0.05 p < 0.01
pos neg pos neg
Ann. 1 30.8 11.4 23.2 5.9
Ann. 2 22.2 24.1 19.6 19.6
Ann. 3 12.7 12.0 10.0 6.0
Table 3: Percentage of sense pairs that were sig-
nificantly positively (pos) or negatively (neg) cor-
related at p < 0.05 and p < 0.01, shown by anno-
tator.
j ? 3 j ? 4 j = 5
Ann. 1 71.9 49.1 8.1
Ann. 2 55.3 24.7 8.1
Ann. 3 42.8 24.0 4.9
Table 4: Percentage of sentences in which at least
two uncorrelated (p > 0.05) or negatively corre-
lated senses have been annotated with judgments
at the specified threshold.
WSsim task could be explained by more coarse-
grained, categorial sense assignments. We first
test how many pairs of senses for a given lemma
show similar patterns in the ratings that they re-
ceive. Table 3 shows the percentage of sense pairs
that were significantly correlated for each anno-
tator.11 Significantly positively correlated senses
can possibly be reduced to more coarse-grained
senses. Would annotators have been able to des-
ignate a single appropriate sense given these more
coarse-grained senses? Call two senses groupable
if they are significantly positively correlated; in or-
der not to overlook correlations that are relatively
weak but existent, we use a cutoff of p = 0.05 for
significant correlation. We tested how often anno-
tators gave ratings of at least similar, i.e. ratings
? 3, to senses that were not groupable. Table 4
shows the percentages of items where at least two
non-groupable senses received ratings at or above
the specified threshold. The table shows that re-
gardless of which annotator we look at, over 40%
of all items had two or more non-groupable senses
receive judgments of at least 3 (similar). There
11We exclude senses that received a uniform rating of 1 on
all items. This concerned 4 senses for annotator 2 and 6 for
annotator 3.
14
1) We study the methods and concepts that each writer uses to
defend the cogency of legal, deliberative, or more generally
political prudence against explicit or implicit charges that
practical thinking is merely a knack or form of cleverness.
2) Eleven CIRA members have been convicted of criminal
charges and others are awaiting trial.
Figure 3: An SPAIR for charge.n. Annotator judg-
ments: 2,3,4
were even several items where two or more non-
groupable senses each got a judgment of 5. The
sentence in table 1 is a case where several non-
groupable senses got ratings ? 3. This is most
pronounced for Annotator 2, who along with sense
2 (controversy) assigned senses 1 (statement), 7
(line of reasoning), and 3 (debate), none of which
are groupable with sense 2.
4.2 Usim analysis
In this experiment, ratings between 1 and 5 were
given for every pairwise combination of sentences
for each target lemma. An example of an SPAIR
for charge.n is shown in figure 3. In this case the
verdicts from the annotators were 2, 3 and 4.
Pattern of Annotations and Annotator Agree-
ment Figure 4 gives a bar chart of the judgments
for each annotator and summed over annotators.
We can see from this figure that the annotators
use the full ordinal scale when judging the simi-
larity of a word?s usages, rather than sticking to
the extremes. There is variation across words, de-
pending on the relatedness of each word?s usages.
Figure 5 shows the judgments for the words bar.n,
work.v and raw.a. We see that bar.n has predom-
inantly different usages with a peak for category
1, work.v has more similar judgments (category 5)
compared to any other category and raw.a has a
peak in the middle category (3). 12 There are other
words, like for example fresh.a, where the spread
is more uniform.
To gauge the level of agreement between anno-
tators, we calculated Spearman?s ? between the
judgments of every pair of annotators as in sec-
tion 4.1. The pairwise correlations are all highly
significant (p < 2.2e-16) with Spearman?s ? =
0.502, 0.641 and 0.501 giving an average corre-
lation of 0.548. We also perform leave-one-out re-
sampling following Lapata (2006) which gave us
a Spearman?s correlation of 0.630.
12For figure 5 we sum the judgments over annotators.
Annotator 4 Annotator 5 Annotator 6 overall
12345
0
500
1000
1500
Figure 4: Usim experiment: number of times each
judgment was used, by annotator and summed
over all annotators
bar.n raw.a work.v
12345
0
10
20
30
40
50
60
Figure 5: Usim experiment: number of times each
judgment was used for bar.n, work.v and raw.a
Comparison with LEXSUB substitutions Next
we look at whether the Usim judgments on sen-
tence pairs (SPAIRs) correlate with LEXSUB sub-
stitutes. To do this we use the overlap of substi-
tutes provided by the five LEXSUB annotators be-
tween two sentences in an SPAIR. In LEXSUB the
annotators had to replace each item (a target word
within the context of a sentence) with a substitute
that fitted the context. Each annotator was permit-
ted to supply up to three substitutes provided that
they all fitted the context equally. There were 10
sentences per lemma. For our analyses we take
every SPAIR for a given lemma and calculate the
overlap (inter) of the substitutes provided by the
annotators for the two usages under scrutiny. Let
s1 and s2 be a pair of sentences in an SPAIR and
15
x1 and x2 be the multisets of substitutes for the
respective sentences. Let f req(w,x) be the fre-
quency of a substitute w in a multiset x of sub-
stitutes for a given sentence. 13 INTER(s1,s2) =
?w?x1?x2 min( f req(w,x1), f req(w,x2))
max(|x1|, |x2|)
Using this calculation for each SPAIR we can
now compute the correlation between the Usim
judgments for each annotator and the INTER val-
ues, again using Spearman?s. The figures are
shown in the leftmost block of table 5. The av-
erage correlation for the 3 annotators was 0.488
and the p-values were all < 2.2e-16. This shows
a highly significant correlation of the Usim judg-
ments and the overlap of substitutes.
We also compare the WSsim judgments against
the LEXSUB substitutes, again using the INTER
measure of substitute overlap. For this analysis,
we only use those WSsim sentences that are origi-
nally from LEXSUB. In WSsim, the judgments for
a sentence comprise judgments for each WordNet
sense of that sentence. In order to compare against
INTER, we need to transform these sentence-wise
ratings in WSsim to a WSsim-based judgment of
sentence similarity. To this end, we compute the
Euclidean Distance14 (ED) between two vectors J1
and J2 of judgments for two sentences s1,s2 for the
same lemma `. Each of the n indexes of the vector
represent one of the n different WordNet senses
for `. The value at entry i of the vector J1 is the
judgment that the annotator in question (we do not
average over annotators here) provided for sense i
of ` for sentence s1.
ED(J1,J2) =
?
(
n
?
i=1
(J1[i]? J2[i])
2) (1)
We correlate the Euclidean distances with
INTER. We can only test correlation for the subset
of WSsim that overlaps with the LEXSUB data: the
30 sentences for investigator.n, function.n and or-
der.v, which together give 135 unique SPAIRs. We
refer to this subset as W?U. The results are given
in the third block of table 5. Note that since we are
measuring distance between SPAIRs for WSsim
13The frequency of a substitute in a multiset depends on
the number of LEXSUB annotators that picked the substitute
for this item.
14We use Euclidean Distance rather than a normalizing
measure like Cosine because a sentence where all ratings are
5 should be very different from a sentence where all senses
received a rating of 1.
Usim All Usim W?U WSsim W?U
ann. ? ? ann. ?
4 0.383 0.330 1 -0.520
5 0.498 0.635 2 -0.503
6 0.584 0.631 3 -0.463
Table 5: Annotator correlation with LEXSUB sub-
stitute overlap (inter)
whereas INTER is a measure of similarity, the cor-
relation is negative. The results are highly signif-
icant with individual p-values from < 1.067e-10
to < 1.551e-08 and a mean correlation of -0.495.
The results in the first and third block of table 5 are
not directly comparable, as the results in the first
block are for all Usim data and not the subset of
LEXSUB with WSsim annotations. We therefore
repeated the analysis for Usim on the subset of
data in WSsim and provide the correlation in the
middle section of table 5. The mean correlation
for Usim on this subset of the data is 0.532, which
is a stronger relationship compared to WSsim, al-
though there is more discrepancy between individ-
ual annotators, with the result for annotator 4 giv-
ing a p-value = 9.139e-05 while the other two an-
notators had p-values < 2.2e-16.
The LEXSUB substitute overlaps between dif-
ferent usages correlate well with both Usim and
WSsim judgments, with a slightly stronger rela-
tionship to Usim, perhaps due to the more compli-
cated representation of word meaning in WSsim
which uses the full set of WordNet senses.
4.3 Correlation between WSsim and Usim
As we showed in section 4.1, WSsim correlates
with previous word sense annotations in SemCor
and SE-3 while allowing the user a more graded
response to sense tagging. As we saw in sec-
tion 4.2, Usim and WSsim judgments both have a
highly significant correlation with similarity of us-
ages as measured using the overlap of substitutes
from LEXSUB. Here, we look at the correlation
of WSsim and Usim, considering again the sub-
set of data that is common to both experiments.
We again transform WSsim sense judgments for
individual sentences to distances between SPAIRs
using Euclidean Distance (ED). The Spearman?s
? range between ?0.307 and ?0.671, and all re-
sults are highly significant with p-values between
0.0003 and < 2.2e-16. As above, the correla-
tion is negative because ED is a distance measure
between sentences in an SPAIR, whereas the judg-
16
ments for Usim are similarity judgments. We see
that there is highly significant correlation for every
pairing of annotators from the two experiments.
5 Discussion
Validity of annotation scheme. Annotator rat-
ings show highly significant correlation on both
tasks. This shows that the tasks are well-defined.
In addition, there is a strong correlation between
WSsim and Usim, which indicates that the poten-
tial bias introduced by the use of dictionary senses
in WSsim is not too prominent. However, we note
that WSsim only contained a small portion of 3
lemmas (30 sentences and 135 SPAIRs) in com-
mon with Usim, so more annotation is needed to
be certain of this relationship. Given the differ-
ences between annotator 1 and the other annota-
tors in Fig. 1, it would be interesting to collect
judgments for additional annotators.
Graded judgments of use similarity and sense
applicability. The annotators made use of the
full spectrum of ratings, as shown in Figures 1 and
4. This may be because of a graded perception of
the similarity of uses as well as senses, or because
some uses and senses are very similar. Table 4
shows that for a large number of WSsim items,
multiple senses that were not significantly posi-
tively correlated got high ratings. This seems to
indicate that the ratings we obtained cannot sim-
ply be explained by more coarse-grained senses. It
may hence be reasonable to pursue computational
models of word meaning that are graded, maybe
even models that do not rely on dictionary senses
at all (Erk and Pado, 2008).
Comparison to previous word sense annotation.
Our graded WSsim annotations do correlate with
traditional ?best fitting sense? annotations from
SemCor and SE-3; however, if annotators perceive
similarity between uses and senses as graded, tra-
ditional word sense annotation runs the risk of in-
troducing bias into the annotation.
Comparison to lexical substitutions. There is a
strong correlation between both Usim and WSsim
and the overlap in paraphrases that annotators gen-
erated for LEXSUB. This is very encouraging, and
especially interesting because LEXSUB annotators
freely generated paraphrases rather than selecting
them from a list.
6 Conclusions
We have introduced a novel annotation paradigm
for word sense annotation that allows for graded
judgments and for some variation between anno-
tators. We have used this annotation paradigm
in two experiments, WSsim and Usim, that shed
some light on the question of whether differences
between word usages are perceived as categorial
or graded. Both datasets will be made publicly
available. There was a high correlation between
annotator judgments within and across tasks, as
well as with previous word sense annotation and
with paraphrases proposed in the English Lex-
ical Substitution task. Annotators made ample
use of graded judgments in a way that cannot
be explained through more coarse-grained senses.
These results suggest that it may make sense to
evaluate WSD systems on a task of graded rather
than categorial meaning characterization, either
through dictionary senses or similarity between
uses. In that case, it would be useful to have more
extensive datasets with graded annotation, even
though this annotation paradigm is more time con-
suming and thus more expensive than traditional
word sense annotation.
As a next step, we will automatically cluster the
judgments we obtained in the WSsim and Usim
experiments to further explore the degree to which
the annotation gives rise to sense grouping. We
will also use the ratings in both experiments to
evaluate automatically induced models of word
meaning. The SemEval-2007 word sense induc-
tion task (Agirre and Soroa, 2007) already allows
for evaluation of automatic sense induction sys-
tems, but compares output to gold-standard senses
from OntoNotes. We hope that the Usim dataset
will be particularly useful for evaluating methods
which relate usages without necessarily producing
hard clusters. Also, we will extend the current
dataset using more annotators and exploring ad-
ditional lexicon resources.
Acknowledgments. We acknowledge support
from the UK Royal Society for a Dorothy Hodkin
Fellowship to the second author. We thank Sebas-
tian Pado for many helpful discussions, and An-
drew Young for help with the interface.
References
E. Agirre and A. Soroa. 2007. SemEval-2007
task 2: Evaluating word sense induction and dis-
17
crimination systems. In Proceedings of the 4th
International Workshop on Semantic Evaluations
(SemEval-2007), pages 7?12, Prague, Czech Repub-
lic.
J. Bybee and D. Eddington. 2006. A usage-based ap-
proach to Spanish verbs of ?becoming?. Language,
82(2):323?355.
J. Chen and M. Palmer. 2009. Improving English
verb sense disambiguation performance with lin-
guistically motivated features and clear sense dis-
tinction boundaries. Journal of Language Resources
and Evaluation, Special Issue on SemEval-2007. in
press.
K. Erk and S. Pado. 2008. A structured vector space
model for word meaning in context. In Proceedings
of EMNLP-08, Waikiki, Hawaii.
J. A. Hampton. 1979. Polymorphous concepts in se-
mantic memory. Journal of Verbal Learning and
Verbal Behavior, 18:441?461.
J. A. Hampton. 2007. Typicality, graded membership,
and vagueness. Cognitive Science, 31:355?384.
P. Hanks. 2000. Do word meanings exist? Computers
and the Humanities, 34(1-2):205?215(11).
E. H. Hovy, M. Marcus, M. Palmer, S. Pradhan,
L. Ramshaw, and R. Weischedel. 2006. OntoNotes:
The 90% solution. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL (NAACL-2006), pages
57?60, New York.
N. Ide and Y. Wilks. 2006. Making sense about
sense. In E. Agirre and P. Edmonds, editors,
Word Sense Disambiguation, Algorithms and Appli-
cations, pages 47?73. Springer.
A. Kilgarriff and J. Rosenzweig. 2000. Framework
and results for English Senseval. Computers and the
Humanities, 34(1-2):15?48.
A. Kilgarriff. 1997. I don?t believe in word senses.
Computers and the Humanities, 31(2):91?113.
A. Kilgarriff. 2006. Word senses. In E. Agirre
and P. Edmonds, editors, Word Sense Disambigua-
tion, Algorithms and Applications, pages 29?46.
Springer.
R. Krishnamurthy and D. Nicholls. 2000. Peeling
an onion: the lexicographers? experience of man-
ual sense-tagging. Computers and the Humanities,
34(1-2).
S. Landes, C. Leacock, and R. Tengi. 1998. Build-
ing semantic concordances. In C. Fellbaum, editor,
WordNet: An Electronic Lexical Database. The MIT
Press, Cambridge, MA.
M. Lapata. 2006. Automatic evaluation of information
ordering. Computational Linguistics, 32(4):471?
484.
D. McCarthy and R. Navigli. 2007. SemEval-2007
task 10: English lexical substitution task. In Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 48?53,
Prague, Czech Republic.
M. McCloskey and S. Glucksberg. 1978. Natural cat-
egories: Well defined or fuzzy sets? Memory &
Cognition, 6:462?472.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004.
The Senseval-3 English lexical sample task. In
3rd International Workshop on Semantic Evalua-
tions (SensEval-3) at ACL-2004, Barcelona, Spain.
G. Miller and W. Charles. 1991. Contextual correlates
of semantic similarity. Language and cognitive pro-
cesses, 6(1):1?28.
G. L. Murphy. 2002. The Big Book of Concepts. MIT
Press.
R. Navigli, K. C. Litkowski, and O. Hargraves.
2007. SemEval-2007 task 7: Coarse-grained En-
glish all-words task. In Proceedings of the 4th
International Workshop on Semantic Evaluations
(SemEval-2007), pages 30?35, Prague, Czech Re-
public.
P. Resnik and D. Yarowsky. 2000. Distinguishing
systems and distinguishing senses: New evaluation
methods for word sense disambiguation. Natural
Language Engineering, 5(3):113?133.
E. Rosch and C. B. Mervis. 1975. Family resem-
blance: Studies in the internal structure of cate-
gories. Cognitive Psychology, 7:573?605.
E. Rosch. 1975. Cognitive representations of seman-
tic categories. Journal of Experimental Psychology:
General, 104:192?233.
H. Rubenstein and J. Goodenough. 1965. Contextual
correlates of synonymy. Computational Linguistics,
8:627?633.
S. Sharoff. 2006. Open-source corpora: Using the net
to fish for linguistic data. International Journal of
Corpus Linguistics, 11(4):435?462.
C. Stokoe. 2005. Differentiating homonymy and pol-
ysemy in information retrieval. In Proceedings of
HLT/EMNLP-05, pages 403?410, Vancouver, B.C.,
Canada.
18
Proceedings of the Linguistic Annotation Workshop, pages 176?183,
Prague, June 2007. c?2007 Association for Computational Linguistics
IGT-XML: an XML format for interlinearized glossed texts
Alexis Palmer, Katrin Erk
Department of Linguistics
University of Texas at Austin
{alexispalmer,katrin.erk}@mail.utexas.edu
Abstract
We propose a new XML format for repre-
senting interlinearized glossed text (IGT),
particularly in the context of the documen-
tation and description of endangered lan-
guages. The proposed representation, which
we call IGT-XML, builds on previous mod-
els but provides a more loosely coupled and
flexible representation of different annota-
tion layers. Designed to accommodate both
selective manual reannotation of individual
layers and semi-automatic extension of an-
notation, IGT-XML is a first step toward par-
tial automation of the production of IGT.
1 Introduction
Much previous work on linguistic annotation has
necessarily focused on resource-rich languages, as it
is these languages for which we have large corpora
in need of linguistic annotation. In contrast, devel-
opment of annotation schemata and methodologies
to be used with language data from endangered lan-
guages has been left largely to individual documen-
tary and/or descriptive linguists working with partic-
ular languages.
This paper addresses linguistic annotation in the
context of the documentation and description of
endangered languages. One interesting feature of
language documentation projects is that, while the
languages studied differ widely, there is a quasi-
standard for presenting the material, in the form of
interlinearized glossed text (IGT). IGT typically
comprises at least four levels: (1) the original text,
(2) a separation of the original text into individual
morphemes, (3) a detailed morpheme-by-morpheme
gloss, and (4) a free translation of each sentence.
Another characteristic of language documentation
projects is the tentative nature of many analyses,
given that linguistic analysis is often occurring in
tandem with the annotation process, sometimes for
the first time in the recorded history of the language.
Furthermore, language documentation projects re-
quire long-term accessibility of the collected lan-
guage data as well as easy accessibility to commu-
nity members as well as to linguists.
In this paper we propose a new XML format for
representing IGT, which we call IGT-XML. We
build on the model of Hughes et al(2003) (the BHB
model from now on), who first proposed using the
IGT structure directly as a basis for an XML format.
While their format shows closely integrated annota-
tion layers using XML embedding, our model has
a more loosely coupled and flexible representation
of different annotation layers, to accommodate (a)
selective manual reannotation of individual layers,
and (b) the (semi-)automatic extension of annota-
tion, without the format posing an a priori restriction
on the annotation levels that can be added. The IGT-
XML representation is thus a first step toward par-
tial automation of the production of IGT, which in
turn is part of a larger project using techniques from
machine learning and natural language processing to
significantly reduce the time and money required to
produce annotated texts.
Besides the BHB model, we build on the Open
Languages Archiving Community (OLAC)1 meta-
data standard. OLAC is developing best practice
guidelines for archiving language resources digi-
tally, including a list of metadata entries to record
1http://www.language-archives.org
176
with language data.
Plan of the paper. After discussing interlin-
earized glosses in Section 2, we show the BHB
model and corresponding XML format in Section 3.
Section 4 presents the IGT-XML format that we pro-
pose. Section 5 demonstrates the applicability of
IGT-XML to data from different languages and dif-
ferent documentation projects, and Section 6 con-
cludes.
2 Interlinearized glossed text
IGT is a way of encoding linguistic data commonly
used to present linguistic examples. The example
below is a segment of IGT taken from Kuhn and
Mateo-Toledo (2004). The language is Q?anjob?al,
a Mayan language of Guatemala.
(1) Maxab? ek?elteq ix unin yet
sq?inib?alil tu.
(2) max-ab?
COM-EV
ek?-el-teq
pass-DIR-DIR
ix
CL
unin
child
y-et
E3S-when
s-q?inib?-al-il
E3S-early-ABS-ABS
tu
DEM
?The child came out early that morning (they say)? 2
The format of the IGT in this example is typical of
the presentation of individual examples in the lin-
guistics literature. The raw, unannotated text (1) is
associated with three layers of annotation, shown in
(2). The first annotation layer shows the same text
with each word segmented into its constituent mor-
phemes. The next layer, the gloss layer, is a combi-
nation of English translations of the Q?anjob?al lem-
mas and tags representing the linguistic information
encoded by affixes on the lemmas. The third layer is
an English translation.
IGT formats vary more widely in language doc-
umentation, where IGT is typically the product of
linguistic analysis of texts transcribed from audio or
audiovisual recordings. A broad survey of formats
for interlinear texts (Bow et al, 2003) found vari-
ation in the number of rows, the type of analysis
found in each row, as well as the level of granularity
of analysis in each row.3
2KEY: ABS=abstract, COM=completive, CL=classifier,
DEM=demonstrative, E=ergative, EV=evidential, S=singular,
3=third person
3Hughes et al(2003) also discuss variation in presentational
factors, which we choose not to encode in our XML format.
Tools using IGT Shoebox/Toolbox4 (Shoebox in
following text) is a system that is widely used in doc-
umentary linguistics for storing and managing lan-
guage data. It provides facilities for lexicon man-
agement as well as text interlinearization.
Figure 1 shows one sentence of Q?anjob?al IGT in
the Shoebox output format.5 Shoebox exports texts
as plain text files. The different annotation layers
are marked by labels at the beginning of the line.
For example, in Figure 1 the label \tx marks the
original text and the line starting with \dm contains
its morphological segmentation.
One important test case for any XML format for
IGT is whether it can represent existing IGT data.
As Shoebox is a widely used tool, we take the
Shoebox data format as a representative case study.
Specifically, in Section 5 we show how texts from
two different languages, interlinearized using Shoe-
box and represented in the Shoebox output format,
can be encoded in IGT-XML.
In this paper we focus on the question of repre-
sentation rather than format transformation. Each
system managing IGT data will have different out-
put formats, requiring different techniques for trans-
forming the data to XML. The aim of this paper is
simply to describe and demonstrate the IGT-XML
format; a detailed automatic transformation method
mapping other formats to IGT-XML is beyond the
scope of this paper and will be addressed separately.
3 Previous work
This section discusses previous work on representa-
tion formats and specifically XML formats for inter-
linear text.
The BHB model: four levels of interlinear text.
Building on Bow et al?s (2003) analysis of differ-
ent IGT formats used in the literature, Hughes et
al. (2003) propose a four-level hierarchical model
for representing interlinear text. The four levels en-
code elements common to most instances of IGT:
text, phrase, word, and morpheme. One text may
consist of several individual phrases. A phrase con-
sists of one or more words, each of which consists
4http://www.sil.org/computing/catalog/
show software.asp?id=79
5Data from B?alam Mateo-Toledo, p.c.
177
\ref txt080_p2.002
\tx Exx a yet junxa tyempohal, ayin ti? xiwil+
\dm exxx a y- et jun - xa tyempo -al, ayin ti xiwil+
\ge INTJ ENF E3- de/cuando ART/uno - ya tiempo -ABS yo DEM muchos
\cp intj part pref- sr num - adv s -suf pro part adv
\tes Eee en otro tiempo yo vi
Figure 1: Shoebox output: Q?anjob?al
<resource>
<interlinear_text>
<item type="title">Example</item>
<phrases>
<phrase>
<item type="gls">The child came out
early that morning (they say)</item>
<words>
<word>
<item type="txt">ek?elteq</item>
<morphemes>
<morph>
<item type="txt">ek?</item>
<item type="gls">pass</item>
</morph>
<morph>
<item type="txt">el</item>
<item type="gls">DIR</item>
</morph>
<morph>
<item type="txt">teq</item>
<item type="gls">DIR</item>
</morph>
</morphemes>
</word>
</words>
</phrase>
</phrases>
</interlinear_text>
</resource>
Figure 2: BHB IGT representation format:
Q?anjob?al
of one or more morphemes. To make this more con-
crete, the example in (1) shows a single phrase (or a
one-phrase text). The three annotation layers in (2)
are situated at different levels in the hierarchy: The
first and second annotation layers are both situated
at the morpheme level, showing a separation of the
original phrase into its constituent morphemes and
a morpheme-by-morpheme gloss, respectively. The
third annotation layer, the translation, is again situ-
ated at the phrase level, like the original text in (1).
The BHB model was originally developed in the
context of the EMELD project,6 which has focused
on advancing the state of technologies, data repre-
sentation formats, and methodologies for digital lan-
guage documentation.
The BHB XML format. Figure 2 shows an exam-
ple of the BHB XML format, which articulates the
four nested levels of structure of the BHB model.
It directly expresses the hierarchy of annotation lev-
els in a nested XML structure, in which, for exam-
ple, <morph> elements representing morphemes
are embedded in <word> elements representing
the corresponding words. The model maintains
the link between the source text morpheme and
the morpheme-level gloss annotation by embedding
both as <item> elements within the <morph>
and distinguishing the two by an attribute called
type.
While this representation provides the needed link
between morphemes and their glosses, it is rather in-
flexible because it is not modular: To add an addi-
tional annotation layer at the word level, one would
need to access and change the representation of each
word of each phrase. In this way, the BHBXML for-
mat is not ideally suited for an extensible annotation
that would need to add additional layers of linguistic
information in a flexible way.
6http://linguistlist.org/emeld
178
4 IGT-XML
In this section we propose a new XML representa-
tion for IGT, IGT-XML. Like the BHB XML for-
mat, it is based on the BHB four-level model, but
it modularizes annotation levels. Linking between
annotation levels is achieved via unique IDs.
The IGT-XML format.
Figure 3 illustrates the new IGT-XML format, show-
ing a representation of the Q?anjob?al example of
Figure 1, mostly restricted to a single word, tyem-
pohal, for simplicity.
The IGT-XML format contains (at least) three
main components:
? a plaintext component comprising phrases as
well as the individual words making up each
phrase, encased in the <phrases> XML el-
ement,
? a morpheme component giving a morphologi-
cal analysis of the source text, encased in the
<morphemes> XML element, and
? a gloss component including glosses at both the
phrase and the word level.
Further annotation layers can be added by extend-
ing the format with additional components beyond
these three, which describe the core four levels of
interlinear text.
Within the <phrases> block, each individual
phrase is encased in a <phrase> element, which
includes the plain text within the <plaintext>
element as well as each individual word of the plain
text in a <word> element. Each <phrase> and
each <word> has a globally unique ID, assigned
in an id attribute. We choose to give explicit IDs
to words, rather than rely on character offsets, to
avoid possible problems with character encodings
and mis-represented special characters.
The morphemes in the<morphemes> block are
again organized by<phrase>. Each<phrase>
in the <morphemes> block refers to the corre-
sponding phrase in the <phrases> block by that
phrase?s unique ID.
Each individual morpheme, represented by a
<morph> element, refers to the unique ID corre-
sponding to the word of which it is a part. The lin-
ear order of morphemes belonging to the same word
is reflected in the order in which <morph> ele-
ments appear, as well as in the running id of the
morphemes. Morphemes have id attributes of their
own such that further annotation levels can refer to
the morphological segmentation of the source text,
as is the case for the morpheme-by-morpheme gloss
in the example in (2).
Whole-sentence glosses are collected in the
<translations> block, while word-by-word
glosses reside in the <gloss> block. Again,
glosses are organized by <phrase>, linked to the
original phrases by idref attributes. The glosses
in <gloss> refer to individual morphemes, hence
their idref attributes point to id attributes of the
<morphemes> block.
Metadata information in the file header
As suggested in Figure 3, IGT-XML is easily ex-
tended with metadata for each text. We adopt the
OLAC metadata set which uses the fifteen elements
defined in the Dublin Core metadata standard (Bird
and Simons, 2003a; Bird and Simons, 2001). These
elements provide a framework for specifying key in-
formation such as annotators, format, and language
of the text. In addition, the OLAC standard incorpo-
rates a number of qualifiers specific to the language-
resource community, such as discourse types (story,
conversation, etc.) and linguistic data types (lexi-
con, language description, primary text, etc.), and a
process for adopting further extensions.
In addition to the metadata block at the head of the
document, it would be possible to intersperse addi-
tional metadata blocks throughout the document, if
for example we wanted to indicate change of speaker
from one phrase to another in recorded conversation.
Discussion
Feature overview. The IGT-XML format we have
presented groups annotation into blocks in a mod-
ular fashion. Each block represents an annotation
layer. The format uses globally unique IDs (via
id and idref attributes) rather than XML em-
bedding for linking annotation layers. In particular,
<morph> and <word> annotation is kept sepa-
rate, such that additional layers of annotation at the
word and morpheme levels can be added modularly
without interfering with each other.
In its minimal form, the format has three blocks,
179
<text id="T1" lg="kjb" source_id="txt080_p2" title="Pixanej">
<metadata idref="T1">
<!-- incorporate OLAC metadata standard -->
</metadata>
<body>
<phrases>
<phrase id="T1.P2" source_id="txt080_p2.002">
<plaintext>Exx a yet junxa tyempohal, ayin ti? xiwil+</plaintext>
<word id="T1.P2.W5" text="tyempohal"/>
</phrase>
</phrases>
<morphemes source_layer="\dm">
<phrase idref="T1.P2">
<morph idref="T1.P2.W5" id="T1.P2.W5.M1" text="tyempo"/>
<morph idref="T1.P2.W5" id="T1.P2.W5.M2" text="al">
<type l="suf"/>
</morph>
</phrase>
</morphemes>
<gloss source_layer="\ge">
<phrase idref="T1.P2">
<gls idref="T1.P2.W5.M1" text="tiempo"/>
<gls idref="T1.P2.W5.M2" text="ABS"/>
</phrase>
</gloss>
<translations>
<phrase idref="T1.P2">
<trans id="T1.P2.Tr1" lg="en">Eee en otro tiempo yo vi</trans>
</phrase>
</translations>
</body>
</text>
Figure 3: IGT-XML representation format: Q?anjob?al
180
for phrases, morphemes, and glosses, but it is exten-
sible by further blocks, for example for POS-tags. It
is also possible to have different types of annotation
at the same linguistic level, for example manually
created as well as automatically assigned POS-tags.
Mildly standoff annotation. The IGT-XML for-
mat keeps the plain text separate from all levels of
annotation. However, it is not standoff in the strict
sense of having all annotation levels refer to the
plain text only and never to one another. The rea-
son for this is that there is no clear ?basic? level to
which all other annotation could refer.
One obvious candidate is the plain text, but the
morpheme-by-morpheme gloss refers not to words,
but to the morpheme segmentation of the source
text, as can be seen in example (2). This makes the
morpheme-segmented source text another candidate
for the basic level, but it is not guaranteed that this
level of annotation will always be available. At the
start of the annotation process the documentary lin-
guist likely has a transcription and a translation, but
he or she may or may not have determined the mor-
photactics of the language or even how to identify
word boundaries.
So, in order (a) not to commit the annotator to one
single order of annotation, or the presence of any
particular annotation level besides the plain text, and
(b) to allow annotation to refer to each of the levels
identified in the BHB model ? text, phrase, word,
and morpheme ?, we allow annotation levels to refer
to each other via unique IDs.
Requirements for IGT formats. Given the nature
of language documentation projects and IGT data,
an IGT representation format should (1) support
long term archiving of language data (Bird and Si-
mons, 2003b), which requires platform-independent
encoding, and it should (2) support a range of for-
mats. IGT data from different sources may show
differences in format and in what is annotated (Bow
et al, 2003), and may be produced using different
software systems. (3) It should be possible to add
or exchange layers of annotation in a modular fash-
ion. This is important because linguistic analysis
in language documentation, which typically targets
languages that are not well-studied, is often tenta-
tive and subject to change. This will also become
increasingly important with the use of automation
to aid and speed up language documentation: Au-
tomation techniques will typically target individual
annotation layers, and it is desirable to be able to
exchange automatic analysis tools freely.
Point (1), platform independence, is achieved by
almost any XML format, since XML formats are
plain text-based and mostly human-readable. Point
(2), the coverage of IGT formats in all variants, can
be achieved by adoption of the BHB model. Flexi-
bility and modularity (point (3)) are the main moti-
vations in the introduction of IGT-XML.
Beyond word-level annotation. For now the an-
notation focus in language documentation projects
is mostly on the word level, especially on morphol-
ogy and POS-tags. For annotation at the syntactic
level, it is an open question what the features of a
universally applicable annotation format should be.
At the moment, TIGER XML (Mengel and Lezius,
2000), with its capability to represent discontinuous
constituents, and constituent as well as dependency
information, seems like a good candidate. Syntac-
tic information could be represented in a separate
top-level XML element, linking tree terminals to
<word> elements by their ID attributes.
5 Data
An important goal of this research is to develop an
XML format which will be viable for use in the
broadest possible range of language documentation
contexts. To that end, the format needs to stretch
and morph with the needs and desires of the individ-
ual user. This section discusses some issues arising
from actual use of the format. The points are illus-
trated with pieces of the XML representation rather
than complete XML documents.
IGT-XML has been used to encode portions of
texts from the Mayan language Q?anjob?al and the
Mixe-Zoquean language Soteapanec (more com-
monly known as Sierra Popoluca). Q?anjob?al is
spoken primarily in the northwestern regions of
Guatemala, and Soteapanec is spoken in the south-
ern part of the state of Veracruz, Mexico. Both texts
come from ongoing documentation efforts, and both
were first interlinearized using Shoebox.
181
5.1 Q?anjob?al
Figure 1 shows a Q?anjob?al sentence in the Shoe-
box export format. The annotation comprises origi-
nal text (\tx level), morphological analysis (\dm),
morpheme gloss (\ge), and parts of speech (\cp).
The Q?anjob?al texts we received preserve links be-
tween Shoebox annotation layers only through typo-
graphical alignment. The IGT-XML representation
makes these links explicit through global IDs using
id and idref attributes. It also splits off punctua-
tion, treating punctuation marks as separate words:
<word id="T1.P2.W5" text="tyempohal"/>
<word id="T1.P2.W6" text=","/>
<word id="T1.P2.W7" text="ayin"/>
In the part of speech annotation level (line \cp),
the annotator has additionally marked prefixes and
suffixes, using the labels pref- and -suf, respec-
tively. In the IGT-XML, we have incorporated this
information in the<morphemes> level as type in-
formation on a morpheme. Figure 3 shows an exam-
ple of this, extended below:
<morph idref="T1.P2.W5" id="T1.P2.W5.M1"
text="tyempo"/>
<morph idref="T1.P2.W5" id="T1.P2.W5.M2"
text="al">
<type l="suf"/>
</morph>
<morph idref="T1.P2.W6" id="T1.P2.W6.M1"
text=",">
<type l="punct"/>
</morph>
By encoding morpheme type as a <type> ele-
ment embedded in the <morph>, we can allow a
single morpheme to bear more than one type label.
For example, an annotator may want to mark a single
morpheme as being an inflectional morpheme which
appears in a suffixal position. This would be in-
dicated by associating multiple <type> elements
with a single<morph> element, differentiating the
<type> elements through use of the label (l) at-
tribute, as shown in the constructed example below.
<morph idref="T3.P1.W3" id="T3.P1.W3.M2"
text="al">
<type l="suf"/>
<type l="infl"/>
</morph>
Furthermore, as the type label is specified in an at-
tribute value, each documentation project can spec-
ify its own list of possible labels.
\ref Jovenes 002
\t Weenyi woony=jaych@@x+tyam
\mb weenyi woonyi=jay.ty@@xi+tam
\gs algunos varon+HPL
\t yo7om@7yyajpa+m
\mb 0+yoomo.7@7y-yaj-pa+m
\gs 3ABS+casar con mujer-3PL-INC+ALR
\f Algunos nin*os se casan.
Figure 4: Shoebox output: Soteapanec
5.2 Soteapanec
Figure 4 shows the Shoebox output for a Soteapanec
phrase.7 In the notation chosen in this project, the
characters ?7? and ?@? refer to phonemes (glottal
stop and mid high unrounded vowel, respectively),
while ?-?, ?+?, ?>?, ?=? and ?.? all mark morpheme
boundaries. Clitic boundaries are marked by ?+?, in-
flectional boundaries by ?-?, derivational boundaries
by ?>? or ?.?, and compounds are indicated with ?=?.
The four different morpheme boundaries translate
to morpheme types in the IGT-XML, which are en-
coded as in the Q?anjob?al case:
<morph idref="T1.P2.W1" id="T1.P2.W1.M1"
text="weenyi"/>
<morph idref="T1.P2.W2" id="T1.P2.W2.M1"
text="woonyi=jay">
<type l="compound"/>
</morph>
<morph idref="T1.P2.W2" id="T1.P2.W2.M2"
text="ty@@xi"/>
<morph idref="T1.P2.W2" id="T1.P2.W2.M3"
text="tam">
<type l="suf"/>
</morph>
The encoding of the compound represents one of
many choices to be made by users of IGT-XML. We
have chosen to present the compound woonyi=jay as
a single morpheme, in line with the linguist?s choice
to notate the compounds this way in the text. An al-
ternative would be to break the compound into two
separate morphemes, each marked as a compound
via the l attribute of the <type> element.
A similar choice exists with respect to the repre-
sentation of other derivational morphology, both at
the level of morphological segmentation and at the
level of the plaintext. In this case, the plaintext of the
Soteapanec includes boundary markers. IGT-XML
7Data from (Franco and de Jong Boudreault, 2005).
182
can accommodate this type of text as well as it can a
truly plain text.
In this Shoebox output, there is no typograph-
ical alignment between annotation levels. So the
manual transformation to IGT-XML had to rely on
counting morphemes. However there are frequent
mismatches between the number of morphemes in
the morphological level (\mb) and the gloss level
(\gs). The second group of lines in Figure 4 shows
an example: There are six morphemes on the \mb
level, but seven on the \gs level. We envision that
automatic transformation to IGT-XMLwill flag such
cases as mismatched, thus functioning as error de-
tection for the annotation. Even in the manual trans-
formation process, we have marked mismatches at
the gloss level to facilitate adjudication by the anno-
tator.
<morph idref="T1.P2.W2.M4"><gls text="HPL"
flag="mismatch" flagsrc="amp"
flagdate="031507"/>
</morph>
We also include the source and date of the flag, at-
tributes which could easily be obtained automati-
cally.
This section provides only a sample of the is-
sues encountered using IGT-XML. One of our next
steps is to work on automatic transformations from
Shoebox data formats to IGT-XML, a stage at which
many of these challenges will necessarily be ad-
dressed.
6 Conclusion
In this paper we have introduced a new XML for-
mat for representing language documentation data,
IGT-XML. At the heart of the model is a represen-
tation of interlinearized glossed text (IGT). Building
on the BHB model (Hughes et al, 2003), IGT-XML
represents original text, its translation, a morpholog-
ical analysis of the original text, and a morpheme-
by-morpheme gloss. Different annotation layers are
represented separately in a modular fashion, allow-
ing for flexible annotation of individual layers as
well as the extension by further annotation layers.
Layers are linked explicitly via globally unique IDs,
using id and idref attributes.
One main aim in the design of the IGT-XML for-
mat is to facilitate the (semi-)automatic annotation
of language documentation data. In fact, our next
step will be to explore the use of computational tools
for speeding up and extending the annotation of less-
studied languages. This connection of documentary
and computational linguistics has the potential to be
very useful to documentary linguists. It also repre-
sents an interesting opportunity for the use of semi-
supervised machine learning techniques like active
learning on a novel application.
Acknowledgments
We would like to thank Lynda de Jong Boudreault
and B?alam Mateo-Toledo for sharing with us data
collected in their documentation efforts.
References
Steven Bird and Gary Simons. 2001. The OLAC meta-
data set and controlled vocabularies. In Proceedings
of ACL Workshop on Sharing Tools and Resources for
Research and Education, pages 7?18, Toulouse.
Steven Bird and Gary Simons. 2003a. Extending Dublin
Core Metadata to support the description and discov-
ery of language resources. Computing and the Hu-
manities, 37:375?388.
Steven Bird and Gary Simons. 2003b. Seven dimensions
of portability for language documentation and descrip-
tion. Language, 79(3):557?582.
Catherine Bow, Baden Hughes, and Steven Bird. 2003.
Towards a general model of interlinear text. In Pro-
ceedings of EMELD Workshop 2003: Digitizing and
Annotating Texts and Field Recordings, LSA Institute:
Lansing MI, USA.
Julia Albino Franco and Lynda de Jong Boudreault.
2005. Jovenes. Unpublished annotated text. Univer-
sity of Texas at Austin.
Baden Hughes, Steven Bird, and Catherine Bow. 2003.
Encoding and presenting interlinear text using XML
technologies. In Alistair Knott and Dominique Esti-
val, editors, Proceedings of the Australasian Language
Technology Workshop, pages 105?113.
Jonas Kuhn and B?alam Mateo-Toledo. 2004. Applying
computational linguistic techniques in a documentary
project for Q?anjob?al (Mayan, Guatemala). In Pro-
ceedings of LREC 2004, Lisbon, Portugal.
Andreas Mengel and Wolfgang Lezius. 2000. An XML-
based encoding format for syntactically annotated cor-
pora. In Proceedings of LREC 2000, Athens, Greece.
183
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 99?104,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval?07 Task 19: Frame Semantic Structure Extraction
Collin Baker, Michael Ellsworth
International Computer Science Institute
Berkeley, California
{collinb,infinity}
@icsi.berkeley.edu
Katrin Erk
Computer Science Dept.
University of Texas
Austin
katrin.erk@mail.utexas.edu
Abstract
This task consists of recognizing words
and phrases that evoke semantic frames as
defined in the FrameNet project (http:
//framenet.icsi.berkeley.edu),
and their semantic dependents, which are
usually, but not always, their syntactic
dependents (including subjects). The train-
ing data was FN annotated sentences. In
testing, participants automatically annotated
three previously unseen texts to match gold
standard (human) annotation, including pre-
dicting previously unseen frames and roles.
Precision and recall were measured both for
matching of labels of frames and FEs and
for matching of semantic dependency trees
based on the annotation.
1 Introduction
The task of labeling frame-evoking words with ap-
propriate frames is similar to WSD, while the task of
assigning frame elements is called Semantic Role
Labeling (SRL), and has been the subject of several
shared tasks at ACL and CoNLL. For example, in
the sentence ?Matilde said, ?I rarely eat rutabaga,??
said evokes the Statement frame, and eat evokes
the Ingestion frame. The role of SPEAKER in the
Statement frame is filled by Matilda, and the role
of MESSAGE, by the whole quotation. In the Inges-
tion frame, I is the INGESTOR and rutabaga fills the
INGESTIBLES role. Since the ingestion event is con-
tained within the MESSAGE of the Statement event,
we can represent the fact that the message conveyed
was about ingestion, just by annotating the sentence
with respect to these two frames.
After training on FN annotations, the participants?
systems labeled three new texts automatically. The
evaluation measured precision and recall for frames
and frame elements, with partial credit for incorrect
but closely related frames. Two types of evaluation
were carried out: Label matching evaluation, in
which the participant?s labeled data was compared
directly with the gold standard labeled data, and Se-
mantic dependency evaluation, in which both the
gold standard and the submitted data were first con-
verted to semantic dependency graphs in XML for-
mat, and then these graphs were compared.
There are three points that make this task harder
and more interesting than earlier SRL tasks: (1)
while previous tasks focused on role assignment, the
current task also comprises the identification of the
appropriate FrameNet frame, similar to WSD, (2)
the task comprises not only the labeling of individ-
ual predicates and their arguments, but also the inte-
gration of all labels into an overall semantic depen-
dency graph, a partial semantic representation of
the overall sentence meaning based on frames and
roles, and (3) the test data includes occurrences of
frames that are not seen in the training data. For
these cases, participant systems have to identify the
closest known frame. This is a very realistic sce-
nario, encouraging the development of robust sys-
tems showing graceful degradation in the face of un-
known events.
99
2 Frame semantics and FrameNet
The basic concept of Frame Semantics is that many
words are best understood as part of a group of
terms that are related to a particular type of event
and the participants and ?props? involved in it (Fill-
more, 1976; Fillmore, 1982). The classes of events
are the semantic frames of the lexical units (LUs)
that evoke them, and the roles associated with the
event are referred to as frame elements (FEs). The
same type of analysis applies not only to events but
also to relations and states; the frame-evoking ex-
pressions may be single words or multi-word ex-
pressions, which may be of any syntactic category.
Note that these FE names are quite frame-specific;
generalizations over them are expressed via explicit
FE-FE relations.
The Berkeley FrameNet project (hereafter FN)
(Fillmore et al, 2003) is creating a computer- and
human-readable lexical resource for English, based
on the theory of frame semantics and supported by
corpus evidence. The current release (1.3) of the
FrameNet data, which has been freely available for
instructional and research purposes since the fall
of 2006, includes roughly 780 frames with roughly
10,000 word senses (lexical units). It also contains
roughly 150,000 annotation sets, of which 139,000
are lexicographic examples, with each sentence an-
notated for a single predicator. The remainder are
from full-text annotation in which each sentence is
annotated for all predicators; 1,700 sentences are an-
notated in the full-text portion of the database, ac-
counting for roughly 11,700 annotation sets, or 6.8
predicators (=annotation sets) per sentence. Nearly
all of the frames are connected into a single graph
by frame-to-frame relations, almost all of which
have associated FE-to-FE relations (Fillmore et al,
2004a)
2.1 Frame Semantics of texts
The ultimate goal is to represent the lexical se-
mantics of all the sentences in a text, based on
the relations between predicators and their depen-
dents, including both phrases and clauses, which
may, in turn, include other predicators; although this
has been a long-standing goal of FN (Fillmore and
Baker, 2001), automatic means of doing this are only
now becoming available.
Consider a sentence from one of the testing texts:
(1) This geography is important in understanding
Dublin.
In the frame semantic analysis of this sentence,
there are two predicators which FN has analyzed:
important and understanding, as well as one which
we have not yet analyzed, geography. In addition,
Dublin is recognized by the NER system as a loca-
tion. In the gold standard annotation, we have the
annotation shown in (2) for the Importance frame,
evoked by the target important, and the annotation
shown in (3) for the Grasp frame, evoked by under-
standing.
(2) [FACTOR This geography] [COP is] IMPOR-TANT [UNDERTAKING in understanding Dublin].[INTERESTED PARTY INI](3) This geography is important in UNDER-
STANDING [PHENOMENON Dublin]. [COGNIZERCNI]
The definitions of the two frames begin like this:
Importance: A FACTOR affects the outcome of an
UNDERTAKING, which can be a goal-oriented activ-
ity or the maintenance of a desirable state, the work
in a FIELD, or something portrayed as affecting an
INTERESTED PARTY. . .
Grasp: A COGNIZER possesses knowledge about
the workings, significance, or meaning of an idea or
object, which we call PHENOMENON, and is able to
make predictions about the behavior or occurrence
of the PHENOMENON. . .
Using these definitions and the labels, and the fact
that the target and FEs of one frame are subsumed
by an FE of the other, we can compose the mean-
ings of the two frames to produce a detailed para-
phrase of the meaning of the sentence: Something
denoted by this geography is a factor which affects
the outcome of the undertaking of understanding the
location called ?Dublin? by any interested party. We
have not dealt with geography as a frame-evoking
expression, although we would eventually like to.
(The preposition in serves only as a marker of the
frame element UNDERTAKING.)
In (2), the INTERESTED PARTY is not a label on
any part of the text; rather, it is marked INI, for ?in-
definite null instantiation?, meaning that it is con-
ceptually required as part of the frame definition,
absent from the sentence, and not recoverable from
the context as being a particular individual?meaning
100
that this geography is important for anyone in gen-
eral?s understanding of Dublin. In (3), the COG-
NIZER is ?constructionally null instantiated?, as the
gerund understanding licenses omission of its sub-
ject. The marking of null instantiations is important
in handling text coherence and was part of the gold
standard, but as far as we know, none of the partici-
pants attempted it, and it was ignored in the evalua-
tion.
Note that we have collapsed the two null instan-
tiated FEs, the INTERESTED PARTY of the impor-
tance frame and the COGNIZER in the Grasp frame,
since they are not constrained to be distinct.
2.2 Semantic dependency graphs
Since the role fillers are dependents (broadly speak-
ing) of the predicators, the full FrameNet annotation
of a sentence is roughly equivalent to a dependency
parse, in which some of the arcs are labeled with role
names; and a dependency graph can be derived algo-
rithmically from FrameNet annotation; an early ver-
sion of this was proposed by (Fillmore et al, 2004b)
Fig. 1 shows the semantic dependency graph de-
rived from sentence (1); this graphical representa-
tion was derived from a semantic dependency XML
file (see Sec. 5). It shows that the top frame in this
sentence is evoked by the word important, although
the syntactic head is the copula is (here given the
more general label ?Support?). The labels on the
arcs are either the names of frame elements or indi-
cations of which of the daughter nodes are seman-
tic heads, which is important in some versions of
the evaluation. The labels on nodes are either frame
names (also colored gray), syntactic phrases types
(e.g. NP), or the names of certain other syntactic
?connectors?, in this case, Marker and Support.
3 Definition of the task
3.1 Training data
The major part of the training data for the task con-
sisted of the current data release from FrameNet
(Release 1.3), described in Sec.2 This was supple-
mented by additional training data made available
through SemEval to participants in this task. In ad-
dition to updated versions of some of the full-text an-
notation from Release 1.3, three files from the ANC
were included: from Slate.com, ?Stephanopoulos
Importance:
important 
Marker: in 
Undertaking
 NP 
Factor
Grasp:
understanding 
SemHead
This geography
Head
NE:location:
Dublin 
DenotedFE: location
Phenomenon
 <s> 
Supp: is 
Head
.
SemHead
Figure 1: Sample Semantic Dependency Graph
Crimes? and ?Entrepreneur as Madonna?, and from
the Berlitz travel guides, ?History of Jerusalem?.
3.2 Testing data
The testing data was made up of three texts, none
of which had been seen before; the gold standard
consisted of manual annotations (by the FrameNet
team) of these texts for all frame evoking expres-
sions and the fillers of the associated frame ele-
ments. All annotation of the testing data was care-
fully reviewed by the FN staff to insure its cor-
rectness. Since most of the texts annotated in the
FN database are from the NTI website (www.nti.
org), we decided to take two of the three test-
ing texts from there also. One, ?China Overview?,
was very similar to other annotated texts such
as ?Taiwan Introduction?, ?Russia Overview?, etc.
available in Release 1.3. The other NTI text,
?Work Advances?, while in the same domain, was
shorter and closer to newspaper style than the rest
of the NTI texts. Finally, the ?Introduction to
101
Sents NEs Frames
Tokens Types
Work 14 31 174 77
China 39 90 405 125
Dublin 67 86 480 165
Totals 120 207 1059 272
Table 1: Summary of Testing Data
Dublin?, taken from the American National Cor-
pus (ANC, www.americannationalcorpus.
org) Berlitz travel guides, is of quite a different
genre, although the ?History of Jerusalem? text in
the training data was somewhat similar. Table 1
gives some statistics on the three testing files. To
give a flavor of the texts, here are two sentences;
frame evoking words are in boldface:
From ?Work Advances?: ?The Iranians are now
willing to accept the installation of cameras only
outside the cascade halls, which will not enable the
IAEA to monitor the entire uranium enrichment
process,? the diplomat said.
From ?Introduction to Dublin?: And in this
city, where literature and theater have historically
dominated the scene, visual arts are finally com-
ing into their own with the new Museum of Modern
Art and the many galleries that display the work of
modern Irish artists.
4 Participants
A number of groups downloaded the training or test-
ing data, but in the end, only three groups submitted
results: the UTD-SRL group and the LTH group,
who submitted full results, and the CLR group who
submitted results for frames only. It should also be
noted that the LTH group had the testing data for
longer than the 10 days allowed by the rules of the
exercise, which means that the results of the two
teams are not exactly comparable. Also, the results
from the CLR group were initially formatted slightly
differently from the gold standard with regard to
character spacing; a later reformatting allowed their
results to be scored with the other groups?.
The LTH system used only SVM classifiers, while
the UTD-SRL system used a combination of SVM
and ME classifiers, determined experimentally. The
CLR system did not use classifiers, but hand-written
symbolic rules. Please consult the separate system
papers for details about the features used.
5 Evaluation
The labels-only matching was similar to previous
shared tasks, but the dependency structure evalua-
tion deserves further explanation: The XML seman-
tic dependency structure was produced by a program
called fttosem, implemented in Perl, which goes
sentence by sentence through a FrameNet full-text
XML file, taking LU, FE, and other labels and using
them to structure a syntactically unparsed piece of a
sentence into a syntactic-semantic tree. Two basic
principles allow us to produce this tree: (1) LUs are
the sole syntactic head of a phrase whose semantics
is expressed by their frame and (2) each label span
is interpreted as the boundaries of a syntactic phrase,
so that when a larger label span subsumes a smaller
one, the larger span can be interpreted as a the higher
node in a hierarchical tree. There are a fair num-
ber of complications, largely involving identifying
mismatches between syntactic and semantic headed-
ness. Some of these (support verbs, copulas, mod-
ifiers, transparent nouns, relative clauses) are anno-
tated in the data with their own labels, while oth-
ers (syntactic markers, e.g. prepositions, and auxil-
iary verbs) must be identified using simple syntactic
heuristics and part-of-speech tags.
For this evaluation, a non-frame node counts as
matching provided that it includes the head of the
gold standard, whether or not non-head children of
that node are included. For frame nodes, the partici-
pants got full credit if the frame of the node matched
the gold standard.
5.1 Partial credit for related frames
One of the problems inherent in testing against un-
seen data is that it will inevitably contain lexical
units that have not previously been annotated in
FrameNet, so that systems which do not generalize
well cannot get them right. In principle, the deci-
sion as to what frame to add a new LU to should be
helped by the same criteria that are used to assign
polysemous lemmas to existing frames. However,
in practice this assignment is difficult, precisely be-
cause, unlike WSD, there is no assumption that all
the senses of each lemma are defined in advance; if
102
the system can?t be sure that a new use of a lemma
is in one of the frames listed for that lemma, then
it must consider all the 800+ frames as possibili-
ties. This amounts to the automatic induction of
fine-grained semantic similarity from corpus data, a
notoriously difficult problem (Stevenson and Joanis,
2003; Schulte im Walde, 2003).
For LUs which clearly do not fit into any exist-
ing frames, the problem is still more difficult. In the
course of creating the gold standard annotation of
the three testing texts, the FN team created almost 40
new frames. We cannot ask that participants hit upon
the new frame name, but the new frames are not cre-
ated in a vacuum; as mentioned above, they are al-
most always added to the existing structure of frame-
to-frame relations; this allows us to give credit for
assignment to frames which are not the precise one
in the gold standard, but are close in terms of frame-
to-frame relations. Whenever participants? proposed
frames were wrong but connected to the right frame
by frame relations, partial credit was given, decreas-
ing by 20% for each link in the frame-frame relation
graph between the proposed frame and the gold stan-
dard. For FEs, each frame element had to match the
gold standard frame element and contain at least the
same head word in order to gain full credit; again,
partial credit was given for frame elements related
via FE-to-FE relations.
6 Results
Text Group Recall Prec. F1
Dublin UTD-SRL 0.4188 0.7716 0.5430
China UTD-SRL 0.5498 0.8009 0.6520
Work UTD-SRL 0.5251 0.8382 0.6457
Dublin LTH 0.5184 0.7156 0.6012
China LTH 0.6261 0.7731 0.6918
Work LTH 0.6606 0.8642 0.7488
Dublin CLR 0.3984 0.6469 0.4931
China CLR 0.4621 0.6302 0.5332
Work CLR 0.5054 0.7452 0.6023
Table 2: Frame Recognition only
The strictness of the requirement of exact bound-
ary matching (which depends on an accurate syntac-
tic parse) is compounded by the cascading effect of
semantic classification errors, as seen by comparing
Text Group Recall Prec. F1
Label matching only
Dublin UTD-SRL 0.27699 0.55663 0.36991
China UTD-SRL 0.31639 0.51715 0.39260
Work UTD-SRL 0.31098 0.62408 0.41511
Dublin LTH 0.36536 0.55065 0.43926
China LTH 0.39370 0.54958 0.45876
Work LTH 0.41521 0.61069 0.49433
Semantic dependency matching
Dublin UTD-SRL 0.26238 0.53432 0.35194
China UTD-SRL 0.31489 0.53145 0.39546
Work UTD-SRL 0.30641 0.61842 0.40978
Dublin LTH 0.36345 0.54857 0.43722
China LTH 0.40995 0.57410 0.47833
Work LTH 0.45970 0.67352 0.54644
Table 3: Results for combined Frame and FE recog-
nition
the F-scores in Table 3 with those in Table 2. The
difficulty of the task is reflected in the F-scores of
around 35% for the most difficult text in the most
difficult condition, but participants still managed to
reach F-scores as high as 75% for the more limited
task of Frame Identification (Table 2), which more
closely matches traditional Senseval tasks, despite
the lack of a full sense inventory. The difficulty
posed by having such an unconstrained task led to
understandably low recall scores in all participants
(between 25 and 50%). The systems submitted by
the teams differed in their sensitivity to differences
in the texts: UTD-SRL?s system varied by around
10% across texts, while LTH?s varied by 15%.
There are some rather encouraging results also.
The participants rather consistently performed bet-
ter with our more complex, but also more useful and
realistic scoring, including partial credit and grad-
ing on semantic dependency rather than exact span
match (compare the top and bottom halves of Table
3). The participants all performed relatively well on
the frame-recognition task, with precision scores av-
eraging 63% and topping 85%.
7 Discussion
The testing data for this task turned out to be espe-
cially challenging with regard to new frames, since,
in an effort to annotate especially thoroughly, almost
103
40 new frames were created in the process of an-
notating these three specific passages. One result
of this was that the test passages had more unseen
frames than a random unseen passage, which prob-
ably lowered the recall on frames. It appears that
this was not entirely compensated by giving partial
credit for related frames.
This task is a more advanced and realistic version
of the Automatic Semantic Role Labeling task of
Senseval-3 (Litkowski, 2004). Unlike that task, the
testing data was previously unseen, participants had
to determine the correct frames as a first step, and
participants also had to determine FE boundaries,
which were given in the Senseval-3.
A crucial difference from similar approaches,
such as SRL with PropBank roles (Pradhan et al,
2004) is that by identifying relations as part of a
frame, you have identified a gestalt of relations that
enables far more inference, and sentences from the
same passage that use other words from the same
frame will be easier to link together. Thus, the
FN SRL results are translatable fairly directly into
formal representations which can be used for rea-
soning, question answering, etc. (Scheffczyk et
al., 2006; Frank and Semecky, 2004; Sinha and
Narayanan, 2005).
Despite the problems with recall, the participants
have expressed a determination to work to improve
these results, and the FN staff are eager to collabo-
rate in this effort. A project is now underway at ICSI
to speed up frame and LU definition, and another to
speed up the training of SRL systems is just begin-
ning, so the prospects for improvement seem good.
This material is based in part upon work sup-
ported by the National Science Foundation under
Grant No. IIS-0535297.
References
Charles J. Fillmore and Collin F. Baker. 2001. Framesemantics for text understanding. In Proceedingsof WordNet and Other Lexical Resources Workshop,Pittsburgh, June. NAACL.
Charles J. Fillmore, Christopher R. Johnson, andMiriam R.L Petruck. 2003. Background to FrameNet.International Journal of Lexicography, 16.3:235?250.
Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato.
2004a. FrameNet as a ?Net?. In Proceedings ofLREC, volume 4, pages 1091?1094, Lisbon. ELRA.
Charles J. Fillmore, Josef Ruppenhofer, and Collin F.Baker. 2004b. FrameNet and representing the linkbetween semantic and syntactic relations. In Chu-
ren Huang and Winfried Lenders, editors, Frontiersin Linguistics, volume I of Language and LinguisitcsMonograph Series B, pages 19?59. Inst. of Linguistics,
Acadmia Sinica, Taipei.
Charles J. Fillmore. 1976. Frame semantics and the na-ture of language. Annals of the New York Academy ofSciences, 280:20?32.
Charles J. Fillmore. 1982. Frame semantics. In Lin-guistics in the Morning Calm, pages 111?137. Han-shin Publishing Co., Seoul, South Korea.
Anette Frank and Jiri Semecky. 2004. Corpus-basedinduction of an LFG syntax-semantics interface for
frame semantic processing. In Proceedings of the 5thInternational Workshop on Linguistically InterpretedCorpora (LINC 2004), Geneva, Switzerland.
Ken Litkowski. 2004. Senseval-3 task: Automatic label-
ing of semantic roles. In Rada Mihalcea and Phil Ed-monds, editors, Senseval-3: Third International Work-shop on the Evaluation of Systems for the SemanticAnalysis of Text, pages 9?12, Barcelona, Spain, July.Association for Computational Linguistics.
Sameer S. Pradhan, Wayne H. Ward, Kadri Hacioglu,James H. Martin, and Dan Jurafsky. 2004. Shallow
semantic parsing using support vector machines. InDaniel Marcu Susan Dumais and Salim Roukos, ed-itors, HLT-NAACL 2004: Main Proceedings, pages
233?240, Boston, Massachusetts, USA, May 2 - May7. Association for Computational Linguistics.
Jan Scheffczyk, Collin F. Baker, and Srini Narayanan.2006. Ontology-based reasoning about lexical re-
sources. In Alessandro Oltramari, editor, Proceedingsof ONTOLEX 2006, pages 1?8, Genoa. LREC.
Sabine Schulte im Walde. 2003. Experiments on thechoice of features for learning verb classes. In Pro-ceedings of the 10th Conference of the EACL (EACL-03).
Steve Sinha and Srini Narayanan. 2005. Model basedanswer selection. In Proceedings of the Workshop onTextual Inference, 18th National Conference on Artifi-cial Intelligence, PA, Pittsburgh. AAAI.
Suzanne Stevenson and Eric Joanis. 2003. Semi-supervised verb class discovery using noisy features.
In Proceedings of the 7th Conference on Natural Lan-guage Learning (CoNLL-03), pages 71?78.
104
Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 1?9,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Teaching computational linguistics to a large, diverse student body:
courses, tools, and interdepartmental interaction
Jason Baldridge and Katrin Erk
Department of Linguistics
The University of Texas at Austin
{jbaldrid,erk}@mail.utexas.edu
Abstract
We describe course adaptation and develop-
ment for teaching computational linguistics
for the diverse body of undergraduate and
graduate students the Department of Linguis-
tics at the University of Texas at Austin. We
also discuss classroom tools and teaching aids
we have used and created, and we mention
our efforts to develop a campus-wide compu-
tational linguistics program.
1 Introduction
We teach computational linguistics courses in the
linguistics department of the University of Texas
at Austin, one of the largest American universi-
ties. This presents many challenges and opportu-
nities; in this paper, we discuss issues and strate-
gies for designing courses in our context and build-
ing a campus-wide program.1 The main theme of
our experience is that courses should be targeted to-
ward specific groups of students whenever possible.
This means identifying specific needs and design-
ing the course around them rather than trying to sat-
isfy a diverse set of students in each course. To this
end, we have split general computational linguistics
courses into more specific ones, e.g., working with
corpora, a non-technical overview of language tech-
nology applications, and natural language process-
ing. In section 2, we outline how we have stratified
our courses at both the graduate and undergraduate
levels.
1Links to the courses, tools, and resources described in this
paper can be found on our main website:
http://comp.ling.utexas.edu
As part of this strategy, it is crucial to ensure that
the appropriate student populations are reached and
that the courses fulfill degree requirements. For ex-
ample, our Language and Computers course fulfills
a Liberal Arts science requirement and our Natural
Language Processing is cross-listed with computer
science. This is an excellent way to get students in
the door and ensure that courses meet or exceed min-
imum enrollments. We find that many get hooked
and go on to specialize in computational linguistics.
Even for targeted CL courses, there is still usually
significant diversity of backgrounds among the stu-
dents taking them. Thus, it is still important to care-
fully consider the teaching tools that are used; in sec-
tion 3, we discuss our experience with several stan-
dard tools and two of our own. Finally, we describe
our efforts to build a campus-wide CL program that
provides visibility for CL across the university and
provides coherence in our course offerings.
2 Courses
Our courses are based on those initiated by Jonas
Kuhn between 2002 and 2005. Since 2005, we have
created several spin-off courses for students with
different backgrounds. Our broad goals for these
courses are to communicate both the practical util-
ity of computational linguistics and its promise for
improving our understanding of human languages.
2.1 Graduate
We started with two primary graduate courses, Com-
putational Linguistics I and II. The first introduces
central algorithms and data structures in computa-
tional linguistics, while the second focuses on learn-
1
Figure 1: Flow for non-seminar courses. Left: graduate
courses, right: undergraduate courses.
ing and disambiguation. This served a computation-
ally savvy segment of the student population quite
well. However, we view one of our key teaching
contributions as computational linguists in a linguis-
tics department to be providing non-computational
students with technical and formal skills useful for
their research. We discovered quickly that our first
computational linguistics course did not fill these
needs, and the second is not even accessible to most
students. The graduate linguistics students did put
in the effort to learn Python for Computational Lin-
guistics I, but many would have preferred a (much)
gentler introduction and also more coverage of is-
sues connected to their linguistic concerns. This led
us to create a new course, Working with Corpora.
Still, there is a need for the primary courses,
which receive interest from many students in com-
puter science and also graduate students from other
departments such as German and English. One of
the great surprises for us in our graduate courses has
been the interest from excellent linguistics and com-
puter science undergraduates.
We have sought to encourage our students to be
active in the academic community outside of UT
Austin. One way we do this is to have a final
project for each course (and most seminars) that has
four distinct stages: (i) a project proposal halfway
through the semester, (ii) a progress report three-
quarters of the way through, (iii) a 10-minute pre-
sentation during the last week of class, and (iv) a
final report at the end. We have found that having
course projects done in this staged manner ensures
that students think very thoroughly about what their
topic is early on, receive significant feedback from
us, and then still have enough time to do significant
implementation for their project, rather than rushing
everything in last minute. Also, by having students
do presentations on their work before they hand in
the final report, they can incorporate feedback from
other students. A useful strategy we have found for
scoring these projects is to use standard conference
reviews in Computational Linguistics II. The final
projects have led to several workshops and confer-
ence publications for the students so far, as well
as honors theses. The topics have been quite var-
ied (in line with our varied student body), including
lexicon induction using genetic algorithms (Ponvert,
2007), alignment-and-transfer for bootstrapping tag-
gers (Moon and Baldridge, 2007), lemmatization us-
ing parallel corpora (Moon and Erk, 2008), graphi-
cal visualization of articles using syntactic depen-
dencies (Jeff Rego, CS honors thesis), and feature
extraction for semantic role labeling (Trevor Foun-
tain, CS honors thesis).
Working with corpora. Computational linguis-
tics skills and techniques are tremendously valuable
for linguists using corpora. Ideally, a linguist should
be able to extract the relevant data, count occur-
rences of phenomena, and do statistical analyses.
The intersection of these skills and needs is the core
of this course, which covers corpus formats (XML,
bracket formats for syntax, ?word/POS? formats for
part-of-speech information), query languages and
tools (regular expressions, cqp, tregex), and some
statistical analysis techniques. It also teaches Python
gently for liberal arts students who have never pro-
grammed and have only limited or no knowledge of
text processing. Other main topics are the compi-
lation of corpora and corpus annotation, with issues
like representativity and what meta-data to include.
At the end of this course, students are prepared for
our primary computational courses.
We observed the tremendous teaching potential
of effective visualization in this course with the R
statistics package. It was used for statistical anal-
yses: students loved it because they could produce
meaningful results immediately and visualize them.
The course includes only a very short two-session
introduction to working with R. We were worried
that this would overtax students because R is its own
2
programming language. But interestingly they had
no problems with learning this second programming
language (after Python). This is particularly striking
as most of the students had no programming experi-
ence prior to the class.
We have not yet used the Natural Language
Toolkit (Loper and Bird, 2002) (see Section 3.1) in
this course. But as it, too, offers visualization and
rapid access to meaningful results, we intend to use
it in the future. In particular, the NLTK allows very
easy access to Toolbox data (Robinson et al, 2007),
which we feel will greatly improve the utility and
appeal of the course for the significant number of
documentary linguistics students in the department.
Seminars. We also offer several seminars in
our areas of interest. These include Categorial
Grammar, Computational Syntax, and Lexical Ac-
quisition. These courses have attracted ?non-
computational? linguistics students with related in-
terests, and have served as the launching point for
several qualifying papers and masters theses. It
is important to offer these courses so that these
students gain a view into computational linguistics
from the standpoint of a topic with which they al-
ready have some mastery; it also ensures healthier
enrollments from students in our own department.
We are currently co-teaching a seminar called
Spinning Straw into Gold: Automated Syntax-
Semantics Analysis, that is designed to overlap with
the CoNLL-2008 shared task on joint dependency
parsing and semantic role labeling. The entire class
is participating in the actual competition, and we
have been particularly pleased with how this exter-
nal facet of the course motivates students to consider
the topics we cover very carefully ? the papers truly
matter for the system we are building. It provides an
excellent framework with which to judge the contri-
butions of recent research in both areas and compu-
tational linguistics more generally.
2.2 Undergraduate
Our first undergraduate course was Introduction to
Computational Linguistics in Fall 2006. Our expe-
rience with this course, which had to deal with the
classic divide in computational linguistics courses
between students with liberal arts versus computer
science backgrounds, led us to split it into two
courses. We briefly outline some of the missteps
with this first course (and what worked well) and
how we are addressing them with new courses.
Introduction to Computational Linguistics.
This course is a boiled-down version of the graduate
Computational Linguistics I taught in Fall 2006.
Topics included Python programming, regular
expressions, finite-state transducers, part-of-speech
tagging, context-free grammar, categorial grammar,
meaning representations, and machine translation.
Overall, the course went well, but enrollment
dropped after the mid-term. As many have found
teaching such courses, some students truly struggled
with the course material while others were ready for
it to go much faster. Several students had interpreted
?introduction? to mean that it was going to be about
computational linguistics, but that they would not
actually have to do computational linguistics. Many
stayed with it, but there were still others who could
have gone much further if it had not been necessary
to slow down to cover basic material like for loops.
Note that several linguistics majors were among the
compationally savvy students.
In fairness to the students who struggled, it was
certainly ill-advised to ask students with no previous
background to learn Python and XFST in a single
semester. One of the key points of confusion was
regular expression syntax. The syntax used in the
textbook (Jurafsky and Martin, 2000) transfers eas-
ily to regular expressions in Python, but is radically
different from that of XFST. For students who had
never coded anything in their life, this proved ex-
tremely frustrating. On the other hand, for computa-
tionally savvy students, XFST was great fun, and it
was an interesting new challenge after having to sit
through very basic Python lectures.
On the other hand, the use of NLTK to drive learn-
ing about Python and NLP tasks (like building POS-
taggers) significantly eased the burden for new pro-
grammers. Many of them were highly satisfied that
they could build interesting programs and experi-
ment with their behavior so easily.
Language and Computers. We had fortunately
already planned the first replacement course: Lan-
guage and Computers, based on the course designed
at the Department of Linguistics at the Ohio State
University (Brew et al, 2005). This course intro-
3
duces computational linguistics to a general audi-
ence and is ideal for students who want exposure
to computational methods without having to learn
to program. We designed and taught it jointly, and
added several new aspects to the course. Whereas
OSU?s course fulfills a Mathematical and Logical
Analysis requirement, our course fulfills a Science
requirement for liberal arts majors. These require-
ments were met by course content that requires un-
derstanding and thinking about formal methods.
The topics we added to our course were question
answering, cryptography,2 and world knowledge.
The course provides ample opportunity to discuss
high-level issues in language technology with low-
level aspects such as understanding particular algo-
rithms (e.g., computing edit distance with dynamic
programming) and fundamental concepts (such as
regular languages and frequency distributions).
In addition to its target audience, the course
nonetheless attracts students who are already well-
versed in many of the low-level concepts. The high-
level material plays an important role for such stu-
dents: while they find the low-level problems quite
easy, many find a new challenge in thinking about
and communicating clearly the wider role that such
technologies play. The high-level material is even
more crucial for holding the interest of less formally
minded students. It gives them the motivation to
work through and understand calculations and com-
putations that might otherwise bore them. Finally,
it provides an excellent way to encourage class dis-
cussion. For example, this year?s class became very
animated on the question of ?Can a machine think??
that we discussed with respect to dialogue systems.
Though the course does not require students to
do any programming, we do show them short pro-
grams that accomplish (simplified versions of) some
of the tasks discussed in the course; for example,
short programs for document retrieval and creating
a list of email address from US census data. The
goal is to give students a glimpse into such applica-
tions, demonstrate that they are not hugely compli-
cated magical systems, and hopefully entice some of
them to learn how to do it for themselves.
The 2007 course was quite successful: it filled
2The idea to cover cryptography came from a discussion
with Chris Brew; he now teaches an entire course on it at OSU.
up (40 students) and received very positive feedback
from the students. It filled up again for this year?s
Spring 2008 offering. The major challenge is the
lack of a textbook, which means that students must
rely heavily on lecture slides and notes.
Words in a Haystack: Methods and Tools for
Working with Corpora. This advanced under-
graduate version of Working with corpora was of-
fered because we felt that graduate and undergrad-
uate linguistics students were actually on an equal
footing in their prior knowledge, and could profit
equally from a gentle introduction to programming.
Although the undergraduate students were active
and engaged in the class, they did not benefit as
much from it as the graduate students. This is likely
because graduate students had already experienced
the need for extracting information from corpora for
their research and the consequent frustration when
they did not have the skills to do so.
Natural Language Processing. This is an de-
manding course that will be taught in Fall 2008. It
is cross-listed with computer science and assumes
knowledge of programming and formal methods in
computer science, mathematics, or linguistics. It is
designed for the significant number of students who
wish to carry on further from the courses described
previously. It is also an appropriate course for un-
dergraduates who have ended up taking our graduate
courses for lack of such an option.
Much of the material from Introduction to Com-
putational Linguistics will be covered in this course,
but it will be done at a faster pace and in greater
detail since programming and appropriate thinking
skills are assumed. A significant portion of the grad-
uate course Computational Linguistics II also forms
part of the syllabus, including machine learning
methods for classification tasks, language modeling,
hidden Markov models, and probabilistic parsing.
We see cross-listing the course with computer sci-
ence as key to its success. Though there are many
computationally savvy students in our liberal arts
college, we expect cross-listing to encourage signif-
icantly more computer science students to try out a
course that they would otherwise overlook or be un-
able to use for fulfilling degree requirements.
4
3 Teaching Tools and Tutorials
We have used a range of external tools and have
adapted tools from our own research for various as-
pects of our courses. In this section, we describe our
experience using these as part of our courses.
We have used Python as the common language in
our courses. We are pleased with it: it is straight-
forward for beginning programmers to learn, its in-
teractive prompt facilitates in-class instruction, it is
text-processing friendly, and it is useful for gluing
together other (e.g., Java and C++) applications.
3.1 External tools and resources
NLTK. We use the Natural Language Toolkit
(NLTK) (Loper and Bird, 2002; Bird et al, 2008) in
both undergraduate and graduate courses for in-class
demos, tutorials, and homework assignments. We
use the toolkit and tutorials for several course com-
ponents, including introductory Python program-
ming, text processing, rule-based part-of-speech tag-
ging and chunking, and grammars and parsing.
NLTK is ideal for both novice and advanced pro-
grammers. The tutorials and extensive documenta-
tion provide novices with plenty of support outside
of the classroom, and the toolkit is powerful enough
to give plenty of room for advanced students to play.
The demos are also very useful in classes and serve
to make many of the concepts, e.g. parsing algo-
rithms, much more concrete and apparent. Some
students also use NLTK for course projects. In all,
NLTK has made course development and execution
significantly easier and more effective.
XFST. A core part of several courses is finite-state
transducers. FSTs have unique qualities for courses
about computational linguistics that are taught in
linguistics department. They are an elegant exten-
sion of finite-state automata and are simple enough
that their core aspects and capabilities can be ex-
pressed in just a few lectures. Computer science stu-
dents immediately get excited about being able to
relate string languages rather than just recognizing
them. More importantly, they can be used to ele-
gantly solve problems in phonology and morphol-
ogy that linguistics students can readily appreciate.
We use the Xerox Finite State Toolkit (XFST)
(Beesley and Karttunen, 2003) for in-class demon-
strations and homeworks for FSTs. A great aspect of
using XFST is that it can be used to show that differ-
ent representations (e.g., two-level rules versus cas-
caded rules) can be used to define the same regular
relation. This exercise injects some healthy skepti-
cism into linguistics students who may have to deal
with formalism wars in their own linguistic subfield.
Also, XFST allows one to use lenient composition to
encode Optimality Theory constraints and in so do-
ing show interesting and direct contrasts and com-
parisons between paper-and-pencil linguistics and
rigorous computational implementations.
As with other implementation-oriented activities
in our classes, we created a wiki page for XFST tu-
torials.3 These were adapted and expanded fromXe-
rox PARC materials and Mark Gawron?s examples.
Eisner?s HMM Materials. Simply put: the
spreadsheet designed by Jason Eisner (Eisner, 2002)
for teaching hidden Markov models is fantastic. We
used that plus Eisner?s HMM homework assignment
for Computational Linguistics II in Fall 2007. The
spreadsheet is great for interactive classroom explo-
ration of HMMs?students were very engaged. The
homework allows students to implement an HMM
from scratch, giving enough detail to alleviate much
of the needless frustration that could occur with this
task while ensuring that students need to put in sig-
nificant effort and understand the concepts in order
to make it work. It also helps that the new edition
of Jurafsky and Martin?s textbook discusses Eisner?s
ice cream scenario as part of its much improved
explanation of HMMs. Students had very positive
feedback on the use of all these materials.
Unix command line. We feel it is important to
make sure students are well aware of the mighty
Unix command line and the tools that are available
for it. We usually have at least one homework as-
signment per course that involves doing the same
task with a Python script versus a pipeline using
command line tools like tr, sort, grep and awk.
This gives students students an appreciation for the
power of these tools and for the fact that they are at
times preferable to writing scripts that handle every-
thing, and they can see how scripts they write can
form part of such pipelines. As part of this module,
3http://comp.ling.utexas.edu/wiki/doku.
php/xfst
5
we have students work through the exercises in the
draft chapter on command line tools in Chris Brew
and Marc Moens? Data-Intensive Linguistics course
notes or Ken Church?s Unix for Poets tutorial.4
3.2 Internal tools
Grammar engineering with OpenCCG. The
grammar engineering component of Computational
Syntax in Spring 2006 used OpenCCG,5 a catego-
rial grammar parsing system that Baldridge created
with Gann Bierner and Michael White. The prob-
lem with using OpenCCG is that its native grammar
specification format is XML designed for machines,
not people. Students in the course persevered and
managed to complete the assignments; nonetheless,
it became glaringly apparent that the non-intuitive
XML specification language was a major stumbling
block that held students back from more interesting
aspects of grammar engineering.
One student, Ben Wing, was unhappy enough us-
ing the XML format that he devised a new specifica-
tion language, DotCCG, and a converter to generate
the XML from it. DotCCG is not only simpler?it
also uses several interesting devices, including sup-
port for regular expressions and string expansions.
This expressivity makes it possible to encode a sig-
nificant amount of morphology in the specification
language and reduce redundancy in the grammar.
The DotCCG specification language and con-
verter became the core of a project funded by UT
Austin?s Liberal Arts Instructional Technology Ser-
vices to create a web and graphical user interface,
VisCCG, and develop instructional materials for
grammar engineering. The goal was to provide suit-
able interfaces and a graduated series of activities
and assignments that would allow students to learn
very basic grammar engineering and then grow into
the full capabilities of an established parsing system.
A web interface provided an initial stage that al-
lowed students in the undergraduate Introduction to
Computational Linguistics course (Fall 2006) to test
their grammars in a grammar writing assignment.
This simple interface allows students to first write
out a grammar on paper and then implement it and
test it on a set of sentences. Students grasped the
4http://research.microsoft.com/users/church/
wwwfiles/tutorials/unix for poets.ps
5http://openccg.sf.net
concepts and seemed to enjoy seeing the grammar?s
coverage improve as they added more lexical entries
or added features to constrain them appropriately. A
major advantage of this interface, of course, is that
it was not necessary for students to come to the lab
or install any software on their own computers.
The second major development was VisCCG,
a graphical user interface for writing full-fledged
OpenCCG grammars. It has special support for
DotCCG, including error checking, and it displays
grammatical information at various levels of granu-
larity while still allowing direct source text editing
of the grammar.
The third component was several online
tutorials?written on as publicly available wiki
pages?for writing grammars with VisCCG and
DotCCG. A pleasant discovery was the tremendous
utility of the wiki-based tutorials. It was very easy
to quickly create tutorial drafts and improve them
with the graduate assistant employed for creating
instructional materials for the project, regardless of
where we were. More importantly, it was possible
to fix bugs or add clarifications while students were
following the tutorials in the lab. Furthermore,
students could add their own tips for other students
and share their grammars on the wiki.
These tools and tutorials were used for two grad-
uate courses in Spring 2007, Categorial Grammar
and Computational Linguistics I. Students caught on
quickly to using VisCCG and DotCCG, which was a
huge contrast over the previous year. Students were
able to create and test grammars of reasonable com-
plexity very quickly and with much greater ease. We
are continuing to develop and improve these materi-
als for current courses.
The resources we created have been not only ef-
fective for classroom instruction: they are also be-
ing used by researchers that use OpenCCG for pars-
ing and realization. The work we did produced sev-
eral innovations for grammar engineering that we
reported at the workshop on Grammar Engineering
Across the Frameworks (Baldridge et al, 2007).
3.3 A lexical semantics workbench:
Shalmaneser
In the lexical semantics sections of our classes, word
sense and predicate-argument structure are core top-
ics. Until now, we had only discussed word sense
6
disambiguation and semantic role labeling theoret-
ically. However, it would be preferable to give the
students hands-on experience with the tasks, as well
as a sense of what does and does not work, and why
the tasks are difficult. So, we are now extending
Shalmaneser (Erk and Pado, 2006), a SHALlow se-
MANtic parSER that does word sense and semantic
role assignment using FrameNet frames and roles,
to be a teaching tool. Shalmaneser already offers a
graphical representation of the assigned predicate-
argument structure. Supported by an instructional
technology grant from UT Austin, we are extend-
ing the system with two graphical interfaces that
will allow students to experiment with a variety of
features, settings and machine learning paradigms.
Courses that only do a short segment on lexical se-
mantic analysis will be able to use the web inter-
face, which does not offer the full functionality of
Shalmaneser (in particular, no training of new clas-
sifiers), but does not require any setup. In addition,
there will be a stand-alone graphical user interface
for a more in-depth treatment of lexical semantic
analysis. We plan to have the new platform ready
for use for Fall 2008.
Besides a GUI and tutorial documents, there is
one more component to the new Shalmaneser sys-
tem, an adaptation of the idea of grammar engi-
neering workbenches to predicate-argument struc-
ture. Grammar engineering workbenches allow stu-
dents to specify grammars declaratively. For seman-
tic role labeling, the only possibility that has been
available so far for experimenting with new features
is to program. But, since semantic role labeling fea-
tures typically refer to parts of the syntactic struc-
ture, it should be possible to describe them declar-
atively using a tree description language. We are
now developing such a language and workbench as
part of Shalmaneser. We aim for a system that will
be usable not only in the classroom but also by re-
searchers who develop semantic role labeling sys-
tems or who need an automatic predicate-argument
structure analysis system.
4 University-wide program
The University of Texas at Austin has a long tra-
dition in the field of computational linguistics that
goes back to 1961, when a major machine transla-
tion project was undertaken at the university?s Lin-
guistics Research Center under the direction of Win-
fred Lehman. Lauri Karttunen, Stan Peters, and
Bob Wall were all on the faculty of the linguistics
department in the late 1960?s, and Bob Simmons
was in the computer science department during this
time. Overall activity was quite strong throughout
the 1970?s and 1980?s. After Bob Wall retired in the
mid-1990?s, there was virtually no computational
work in the linguistics department, but Ray Mooney
and his students in computer science remained very
active during this period.6
The linguistics department decided in 2000 to
revive computational linguistics in the department,
and consequently hired Jonas Kuhn in 2002. His
efforts, along with those of Hans Boas in the Ger-
man department, succeeded in producing a com-
putational linguistics curriculum, funding research,
(re)establishing links with computer science, and at-
tracting an enthusiastic group of linguistics students.
Nonetheless, there is still no formal interdepart-
mental program in computational linguistics at UT
Austin. Altogether, we have a sizable group of
faculty and students working on topics related to
computational linguistics, including many other lin-
guists, computer scientists, psychologists and oth-
ers who have interests directly related to issues in
computational linguistics, including our strong arti-
ficial intelligence group. Despite this, it was easy
to overlook if one was considering only an individ-
ual department. We thus set up a site7 to improve
the visibility of our CL-related faculty and research
across the university. There are plans to create an ac-
tual program spanning the various departments and
drawing on the strengths of UT Austin?s language
departments. For now, the web site is a low-cost and
low-effort but effective starting point.
As part of these efforts, we are working to in-
tegrate our course offerings, including the cross-
listing of the undergraduate NLP course. Our stu-
dents regularly take Machine Learning and other
courses from the computer science department. Ray
Mooney will teach a graduate NLP course in Fall
2008 that will offer students a different perspective
and we hope that it will drum up further interest
6For a detailed account, see: http://comp.ling.
utexas.edu/wiki/doku.php/austin compling history
7http://comp.ling.utexas.edu
7
in CL in the computer science department and thus
lead to further interest in our other courses.
As part of the web page, we also created a wiki.8
We have already mentioned its use in teaching and
tutorials. Other uses include lab information, a
repository of programming tips and tricks, list of im-
portant NLP papers, collaboration areas for projects,
and general information about computational lin-
guistics. We see the wiki as an important reposi-
tory of knowledge that will accumulate over time
and continue to benefit us and our students as it
grows. It simplifies our job since we answer many
student questions on the wiki: when questions get
asked again, we just point to the relevant page.
5 Conclusion
Our experience as computational linguists teaching
and doing research in a linguistics department at a
large university has given us ample opportunity to
learn a number of general lessons for teaching com-
putational linguistics to a diverse audience.
The main lesson is to stratify courses according
to the backgrounds different populations of students
have with respect to programming and formal think-
ing. A key component of this is to make expec-
tations about the level of technical difficulty of a
course clear before the start of classes and restate
this information on the first day of class. This is im-
portant not only to ensure students do not take too
challenging a course: other reasons include (a) re-
assuring programming-wary students that a course
will introduce them to programming gently, (b) en-
suring that programming-savvy students know when
there will be little programming involved or formal
problem solving they are likely to have already ac-
quired, and (c) providing awareness of other courses
students may be more interested in right away or af-
ter they have completed the current course.
Another key lesson we have learned is that the for-
mal categorization of a course within a university
course schedule and departmental degree program
are massive factors in enrollment, both at the under-
graduate and graduate level. Computational linguis-
tics is rarely a required course, but when taught in a
liberal arts college it can easily satisify undergradu-
ate math and/or science requirements (as Language
8http://comp.ling.utexas.edu/wiki/doku.php
and Computers does at OSU and UT Austin, respec-
tively). However, for highly technical courses taught
in a liberal arts college (e.g., Natural Language Pro-
cessing) it is useful to cross-list them with computer
science or related areas in order to ensure that the ap-
propriate student population is reached. At the grad-
uate level, it is also important to provide structure
and context for each course. We are now coordinat-
ing with Ray Mooney to define a core set of com-
putational linguistics courses that we offer regularly
and can suggest to incoming graduate students. This
will not be part of a formal degree program per se,
but will provide necessary structure for students to
progress through either the linguistics or computer
science program in a timely fashion while taking
courses relevant to their research interests.
One of the big questions that hovers over nearly
all discussions of teaching computational linguistics
is: how do we teach the computer science to the
linguistics students and teach the linguistics to the
computer science students? Or, rather, the question
is how to teach both groups computational linguis-
tics. This involves getting students to understand the
importance of a strong formal basis, ranging from
understanding what a tight syntax-semantics inter-
face really means to how machine learning mod-
els relate to questions of actual language acquisi-
tion to how corpus data can or should inform lin-
guistic analyses. It also involves revealing the cre-
ativity and complexity of language to students who
think it should be easy to deal with. And it involves
showing linguistics students how familiar concepts
from linguistics translate to technical questions (for
example, addressing agreement using feature log-
ics), and showing computer science students how
familiar friends like finite-state automata and dy-
namic programming are crucial for analyzing nat-
ural language phenomena and managing complexity
and ambiguity. The key is to target the courses so
that the background needs of each type of student
can be met appropriately without needing to skimp
on linguistic or computational complexity for those
who are ready to learn about it.
Acknowledgments. We would like to thank Hans
Boas, Bob Harms, Ray Mooney, Elias Ponvert, Tony
Woodbury, and the anonymous reviewers for their
help and feedback.
8
References
Jason Baldridge, Sudipta Chatterjee, Alexis Palmer, and
Ben Wing. 2007. DotCCG and VisCCG: Wiki and
programming paradigms for improved grammar engi-
neering with OpenCCG. In Proceeings of the GEAF
2007 Workshop.
Kenneth R. Beesley and Lauri Karttunen. 2003. Finite
State Morphology. CSLI Publications.
Steven Bird, Ewan Klein, Edward Loper, and Jason
Baldridge. 2008. Multidisciplinary instruction with
the Natural Language Toolkit. In Proceedings of the
Third Workshop on Issues in Teaching Computational
Linguistics. Association for Computational Linguis-
tics.
C. Brew, M. Dickinson, and W. D. Meurers. 2005. Lan-
guage and computers: Creating an introduction for a
general undergraduate audience. In Proceedings of the
Workshop on Effective Tools and Methodologies for
Teaching Natural Language Processing And Compu-
tational Linguistics, Ann Arbor, Michigan.
Jason Eisner. 2002. An interactive spreadsheet for teach-
ing the forward-backward algorithm. In Dragomir
Radev and Chris Brew, editors, Proceedings of the
ACL Workshop on Effective Tools and Methodologies
for Teaching NLP and CL, pages 10?18.
Katrin Erk and Sebastian Pado. 2006. Shalmaneser ? a
flexible toolbox for semantic role assignment. In Pro-
ceedings of LREC-2006, Genoa, Italy.
D. Jurafsky and J. H. Martin. 2000. Speech and language
processing: An Introduction to Natural Language
Processing, Computational Linguistics, and Speech
Recognition. Prentice-Hall, Upper Saddle River, NJ.
Edward Loper and Steven Bird. 2002. NLTK: The natu-
ral language toolkit. In Proceedings of the ACL Work-
shop on Effective Tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics, pages 62?69. Somerset, NJ: Association
for Computational Linguistics.
Taesun Moon and Jason Baldridge. 2007. Part-of-speech
tagging for middle English through alignment and pro-
jection of parallel diachronic texts. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
390?399.
Taesun Moon and Katrin Erk. 2008. Minimally super-
vised lemmatization scheme induction through bilin-
gual parallel corpora. In Proceedings of the Interna-
tional Conference on Global Interoperability for Lan-
guage Resources.
Elias Ponvert. 2007. Inducing Combinatory Categorial
Grammars with genetic algorithms. In Proceedings
of the ACL 2007 Student Research Workshop, pages
7?12, Prague, Czech Republic, June. Association for
Computational Linguistics.
Stuart Robinson, Greg Aumann, and Steven Bird. 2007.
Managing fieldwork data with Toolbox and the Natu-
ral Language Toolkit. Language Documentation and
Conservation, 1:44?57.
9
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 57?65,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Paraphrase assessment in structured vector space:
Exploring parameters and datasets
Katrin Erk
Department of Linguistics
University of Texas at Austin
katrin.erk@mail.utexas.edu
Sebastian Pad?
Department of Linguistics
Stanford University
pado@stanford.edu
Abstract
The appropriateness of paraphrases for words de-
pends often on context: ?grab? can replace ?catch?
in ?catch a ball?, but not in ?catch a cold?. Struc-
tured Vector Space (SVS) (Erk and Pad?, 2008) is
a model that computes word meaning in context
in order to assess the appropriateness of such para-
phrases. This paper investigates ?best-practice? pa-
rameter settings for SVS, and it presents a method to
obtain large datasets for paraphrase assessment from
corpora with WSD annotation.
1 Introduction
The meaning of individual occurrences or tokens of
a word can change vastly according to its context. A
central challenge for computational lexical semantics
is describe these token meanings and how they can be
computed for new occurrences.
One prominent approach to this question is the
dictionary-based model of token meaning: The differ-
ent meanings of a word are a set of distinct, disjoint
senses enumerated in a lexicon or ontology, such as
WordNet. For each new occurrence, determining token
meaning means choosing one of the senses, a classifica-
tion task known as Word Sense Disambiguation (WSD).
Unfortunately, this task has turned out to be very hard
both for human annotators and for machines (Kilgarriff
and Rosenzweig, 2000), not at least due to granularity
problems with available resources (Palmer et al, 2007;
McCarthy, 2006). Some researchers have gone so far
as to suggest fundamental problems with the concept of
categorical word senses (Kilgarriff, 1997; Hanks, 2000).
An interesting alternative is offered by vector space
models of word meaning (Lund and Burgess, 1996; Mc-
Donald and Brew, 2004) which characterize the mean-
ing of a word entirely without reference to word senses.
Word meaning is described in terms of a vector in a high-
dimensional vector space that is constructed with dis-
tributional methods. Semantic similarity is then simply
distance to vectors of other words. Vector space models
have been most successful in modeling the meaning of
word types (i.e. in constructing type vectors). The char-
acterization of token meaning by corresponding token
vectors would represent a very interesting alternative to
dictionary-based methods by providing a direct, graded,
unsupervised measure of (dis-)similarity between words
in context that completely avoids reference to dictionary
senses. However, there are still considerable theoretical
and practical problems, even though there is a substan-
tial body of work (Landauer and Dumais, 1997; Sch?tze,
1998; Kintsch, 2001; Mitchell and Lapata, 2008).
In a recent paper (Erk and Pad?, 2008), we have intro-
duced the structured vector space ( SVS) model which
addresses this challenge. It yields one token vector per
input word. Token vectors are not computed by com-
bining the lexical meaning of the surrounding words ?
which risks resulting in a ?topicality? vector ? but by
modifying the type meaning of a word with the semantic
expectations of syntactically related words, which can
be thought of as selectional preferences. For example,
in catch a ball, the token vector for ball is computed by
combining the type vector of ball with a vector for the
selectional preferences of catch for its object. The to-
ken vector for catch, conversely, is constructed from the
type vector of catch and the inverse object preference
vector of ball. The resulting token vectors describe the
meaning of a word in a particular sentence not through a
sense label, but through the distance of the token vector
to other vectors.
A natural question that arises is how vector-based
models of token meaning can be evaluated. It is of
course possible to apply them to a traditional WSD
task. However, this strategy remains vulnerable to all
criticism concerning the annotation of categorical word
senses, and also does not take advantage of the vec-
tor models? central asset, namely gradedness. Thus,
paraphrase-based assessment for models of token mean-
ing was proposed as a representation-neutral disam-
biguation task that can replace WSD (McCarthy and
Navigli, 2007; Mitchell and Lapata, 2008). Given a
word token in context and a set of potential paraphrases,
the task consists of identifying the subset of valid para-
phrases. For example, in the following example, the
first paraphrase is appropriate, but the second is not:
(1) Google acquired YouTube ?
Google bought YouTube
(2) How children acquire skills 6?
How children buy skills
This task is graded in the sense that there is no dis-
joint set of labels from which exactly one is picked for
each token; rather, the paraphrases form a set of labels
of which a subset is appropriate for each word token,
57
and the appropriate sets for two tokens may overlap to
varying degrees. In an ideal vector-based model, valid
paraphrases such as (1) should possess similar vectors,
and invalid ones such as (2) dissimilar ones.
In Erk and Pad? (2008), we evaluated SVS on two
variants of the paraphrase assessment test: first, the pre-
diction of human judgments on a seven-point scale for
paraphrases for verb-subject pairs (Mitchell and Lap-
ata, 2008); and second, the original Lexical Substitution
task by McCarthy and Navigli (2007). To avoid overfit-
ting, we optimized our parameters on the first dataset
and evaluated only the best model on the second dataset.
However, given evidence for substantial inter-task differ-
ences, it is unclear to what extent these parameters are
optimal beyond the Mitchell and Lapata dataset. This
paper addresses this question with two experiments:
Impact of parameters. We re-examine three central
parameters of SVS. The first one is the choice of vector
combination function. Following Mitchell and Lap-
ata (2008), we previously used componentwise multi-
plication, whose interpretation in vector space is not
straightforward. The second one is reweighting. We
obtained the best performance when the context expec-
tations were reweighted by taking each component to
a (high) n-th power, which is counterintuitive. Finally,
we found subjects to be more informative in judging
the appropriateness of paraphrases than objects. This
appears to contradict work in theoretical syntax (Levin
and Rappaport Hovav, 2005).
To reassess the role of these parameters, we construct
a controlled dataset of transitive instances from the Lex-
ical Substitution corpus to reexamine and investigate
these issues, with the aim of providing ?best practice?
settings for SVS. This turns out to be more difficult than
expected, leading us to suspect that a globally optimal
parameter setting across tasks may simply not exist. We
also test a simple extension of SVS that uses a richer
context (both subject and object) to construct the token
vector, with first positive results.
Dataset creation. The Lexical Substitution dataset
used in Erk and Pad? (2008) was very small, which lim-
its the conclusions that can be drawn from it. This points
towards a more general problem of paraphrase-based
assessment for models of token meaning: Until now, all
datasets for this task were specifically created by hand.
It would provide a strong boost for paraphrase assess-
ment if the large annotated corpora that are available for
WSD could be reused.
We present an experiment on converting the WordNet-
annotated SemCor corpus into a set of ?pseudo-
paraphrases? for paraphrase-based assessment. We use
the synonyms and direct hypernyms of an annotated
synset as these ?pseudo-paraphrases?. While the syn-
onyms and hypernyms are not guaranteed to work as
direct replacements of the target word in the given con-
text, they are semantically similar to the target word.
The result is a dataset ten times larger than the Lex-
Sub dataset. As we describe in this paper, we find that
this method is nevertheless problematic: The resulting
dataset is considerably more difficult to model than the
existing hand-built paraphrase corpora, and its proper-
ties differ considerably from the manually constructed
Lexical Substitution dataset.
2 The structured vector space model
The main intuition behind the SVS model is to treat the
interpretation of a word in context as guided by expecta-
tions about typical events. This move to include typical
arguments and predicates into a model of word meaning
is motivated both on cognitive and linguistic grounds.
In cognitive science, the central role of expectations
about typical events on almost all aspects of human
language processing is well-established (McRae et al,
1998; Narayanan and Jurafsky, 2002). In linguistics, ex-
pectations have long been used in semantic theories in
the form of selectional restrictions and selectional pref-
erences (Wilks, 1975), and more recently induced from
corpora (Resnik, 1996). Attention has mostly been lim-
ited to selectional preferences of verbs, which have been
used for for a variety of tasks (Hindle and Rooth, 1993;
Gildea and Jurafsky, 2002). A recent result that the SVS
model builds on is that selectional preferences can be
represented as prototype vectors constructed from seen
arguments (Erk, 2007; Pad? et al, 2007).
Representing lemma meaning. To accommodate in-
formation about semantic expectations, the SVS model
extends the traditional representation of word meaning
as a single vector by a set of vectors, each of which
represents the word?s selectional preferences for each
relation that the word can assume in its linguistic con-
text. While we ultimately think of these relations as
?properly semantic? in the sense of semantic roles, the
instantiation of SVS we consider in this paper makes
use of dependency relations as a level of representation
that generalizes over a substantial amount of surface
variation but that can be obtained automatically with
high accuracy using current NLP tools.
The idea is illustrated in Figure 1. In the representa-
tion of the verb catch, the central square stands for the
lexical vector of catch itself. The three arrows link it to
catch ?s preferences for dependency relations it can par-
ticipate in, such as for its subjects, its objects, and for
verbs for which it appears as a complement (comp?1).
The figure shows the head words that enter into the com-
putation of the selectional preference vector. Likewise,
ball is represented by one vector for ball itself, one for
ball ?s preferences for its modifiers (mod), and two for
the verbs of which it can occur as a subject (subj?1)
and an object (obj?1), respectively.
This representation includes selectional preferences
(like subj, obj, mod) exactly parallel to inverse selec-
tional preferences (subj?1, obj?1, comp?1). The SVS
model is then formalized as follows. Let D be a vector
space, and let R be some set of relation labels. We then
58
catch
he
fielder
dog
cold
baseball
drift
objsubj
accuse
say
claim
comp
-1
ball
whirl
fly
provide
throw
catch
organise
obj
-1
subj
-1
mod
red
golf
elegant
Figure 1: Structured Vector Space representations for
noun ball and verb catch : Each box represents one
vector (lexical information or expectations)
represent the meaning of a lemma w as a triple
m(w) = (vw, R,R
?1)
where vw ? D is the type vector of the word w itself,
R : R ? D maps each relation label onto a vector
that describes w?s selectional preferences, and R?1 :
R ? D maps from role labels to vectors describing
inverse selectional preferences of w. Both R and R?1
are partial functions. For example, the direct object
preference is undefined for intransitive verbs.1
Computing meaning in context. SVS computes the
meaning of a word a in the context of another word
b via their selectional preferences as follows: Let
m(a) = (va, Ra, R?1a ) and m(b) = (vb, Rb, R
?1
b ) be
the representations of the two words, and let r ? R
be the relation linking a to b. Then, the meaning of a
and b in this context is defined as a pair of structured
vector triples: m(a
r
? b) is the meaning of a with b as
its r-argument, and m(b
r?1
? a) the meaning of b as the
r-argument of a:
m(a
r
? b) =
(
va R
?1
b (r), Ra ? {r}, R
?1
a
)
m(b
r?1
? a) =
(
vb Ra(r), Rb, R
?1
b ? {r}
)
(3)
where v1  v2 is a direct vector combination function
as in traditional models, e.g. addition or component-
wise multiplication. If either Ra(r) or R
?1
b (r) are not
defined, the combination fails. Afterward, the filled
argument position r is deleted from Ra and R
?1
b .
Figure 2 illustrates the procedure on the representa-
tions from Figure 1. The dotted lines indicate that the
lexical vector for catch is combined with the inverse
object preference of ball. Likewise, the lexical vector
for ball combines with the object preference vector of
catch.
Recursive application. In Erk and Pad? (2008), we
considered only one combination step; however, the
1We use separate functions R, R?1 rather than a joint
syntactic context preference function because (a) this sepa-
ration models the conceptual difference between predicates
and arguments, and (b) it allows for a simpler, more elegant
formulation of the computation of meaning in context in Eq. 3.
catch
...
cold
baseball
drift
obj
subj
...
comp
-1
ball
...
throw
catch
organise
obj
-1
subj
-1
mod
...
!
!
Figure 2: Combining predicate and argument via
relation-specific semantic expectations
syntactic context of a word in a dependency tree often
consists of more than one word. It seems intuitively
plausible that disambiguation should profit from more
context information. Thus, we extend SVS with recur-
sive application. Let a stand in relation r to b. As
defined above, the result of combining m(a) and m(b)
by relation r are two structured vector triples m(a
r
? b)
and m(b
r?1
? a). If a also stands in relation s 6= r to
a word c with m(c) = (va, Ra, R?1a ), we define the
meaning of a in the context of b and c canonically as
m(m(a
r
? b)
s
? c) =
(
(va R
?1
b (r)) R
?1
c (s),
Ra ? {r, s}, R?1a
)
(4)
If  is associative and commutative, then m(m(a
r
?
b)
s
? c) = m(m(a
s
? c)
r
? b). This will be the case
for all the combination functions we use in this paper.
Note that this is a simplistic model of the influence
of multiple context words: it computes only lexical
meaning recursively, but does not model the influence
of context on the selectional preferences. For example,
the subject selectional preferences of catch are identical
to those of catch the ball, even though one would ex-
pect that the outfielder corresponds much better to the
expectations of catch the ball than of just catch.
3 Experimental Setup
The task that we are considering is paraphrase assess-
ment in context. Given a predicate-argument pair and
a paraphrase candidate, the models have to decide how
appropriate the paraphrase is for the predicate-argument
combination. This is the main task against which token
vector models have been evaluated in the past (Mitchell
and Lapata, 2008; Erk and Pad?, 2008). In Experi-
ment 1, we use manually created paraphrases. In Exper-
iment 2, we replaces human-generated paraphrases with
?pseudo-paraphrases?, contextually similar words that
may not be completely appropriate as paraphrases in the
given context, but can be collected automatically. Our
parameter choices for SVS are as similar as possible to
the second experiment of our earlier paper.
Vector space. We use a dependency-based vector
space that counts a target word and a context word
59
as co-occurring in a sentence if they are connected by
an ?informative? path in the dependency graph for the
sentence.2 We build the space from a Minipar-parsed
version of the British National Corpus with dependency
parses obtained from Minipar (Lin, 1993). It uses raw
co-occurrence counts and 2000 dimensions.
Selectional preferences and reweighting. We use
a prototype-based selectional preference model (Erk,
2007). It models the selectional preferences of a predi-
cate for an argument position as the weighted centroid
of the vectors for all head words seen for this position
in a large corpus. Let f(a, r, b) denote the frequency of
a occurring in relation r to b in the parsed BNC. Then,
we compute the selectional preferences as:
R?b(r) =
1
N
?
a:f(a,r,b)>0
f(a, r, b) ? ~va (5)
where N is the number of fillers a with f(a, r, b) > 0.
In Erk and Pad? (2008), we found that applying a
reweighting step to the selectional preference vector by
taking each component of the centroid vector R?b(r) to
the n-th power lead to substantial improvements. The
motivation for this technique is to alleviate noise aris-
ing from the use of unfiltered head words for the con-
struction. The reweighted selectional preference vector
Rb(r) is defined as:
Rb(r) = ?v
n
1 , . . . , v
n
m? for R
?
b(r) = ?v1, . . . , vm? (6)
where we write ?v1, . . . , vm? for the sequence of values
that make up a vector R?b(r). Inverse selectional pref-
erences R?1b (r) of nouns are defined analogously, by
computing the centroid of the verbs seen as governors
of the noun in relation r.
In this paper, we test reweighting parameters of n be-
tween 0.5 and 30. Generally, small ns will decrease the
influence of the selectional preference vector. The result
can be thought of as a ?word type vector modified by
context expectations?, while large ns increase the role
of context, until we arrive at a ?contextual expectation
vector modified by the word type vector?. 3
Vector combination. We test three vector combina-
tion functions , which have different interpretations
in vector space. The simplest one is componentwise
addition, abbreviated as add, i.e., simple vector addi-
tion.4 With addition, context dimensions receive a high
count whenever either of the two vectors has a high
co-occurrence count for the context.
2We used the minimal context specification and plain
weight of the DependencyVectors software package.
3For the component-wise minimum combination (see be-
low), where we normalize the vectors before the combination,
the reweighting has a different effect. It shifts most of the mass
onto the largest-value dimensions and sets smaller dimensions
to values close to zero.
4Since we subsequently focus on cosine similarity, which
is length-invariant, vector addition can also be interpreted as
centroid computation.
Next, we test component-wise multiplication (mult).
This operation is more difficult to interpret in terms of
vector space, since it does not correspond to the standard
inner or outer vector products. The most straightforward
interpretation is to reinterpret the second vector as a di-
agonal matrix, i.e., as a linear transformation of the first
vector. Large entries in the second vector increase the
weight of the corresponding contexts; small entries de-
crease it. Mitchell and Lapata (2008) found this method
to yield the best results.
The third vector combination function we consider
is component-wise minimum (min). This combination
function results in a vector with high counts only for
contexts which co-occur frequently with both input vec-
tors and can thus be understood as an intersection be-
tween the two context sets. Since the entries of two
vectors need to be on the same order to magnitude for
this method to yield meaningful results, we normalize
vectors before the combination for min.
Assessing models of token meaning. Given a transi-
tive verb v with subject a and direct object b, we test
three variants of computing a token vector for v. The
first two involve only one combination step. In the subj
condition, v?s type vector is combined with the inverse
subject preference vector of a. In the obj condition, v?s
type vector is combined with the inverse object pref-
erence vector of b. The third variant is the recursive
application of the SVS combination procedure described
in Section 2 (condition both). Specifically, we combine
v?s type vector with both a?s inverse subject preference
and with b?s inverse object preference to obtain a ?richer?
token vector.
In all three cases, the resulting token vector is com-
pared to the type vector of the paraphrase (in Experi-
ment 1) or the semantically related word (in Experiment
2). We use Cosine Similarity, a standard choice as vector
space similarity measure.
4 Experiments
4.1 Experiment 1: The impact of parameters
In our 2008 paper, we tested the LexSub data only with
the parameters that showed best results on the Mitchell
and Lapata data: vector combination using component-
wise multiplication (mult), and the computation of (in-
verse) selectional preference vectors with high powers
of n = 20 or n = 30. However, there were indications
that the two datasets showed fundamental differences.
In particular, the Mitchell and Lapata data could only be
modeled using a PMI-transformed vector space, while
the LexSub data could only be modeled using raw co-
occurrence count vectors.
Another one of our findings that warrants further in-
quiry stems from our comparison of different context
choices (verb plus subject, verb plus object, noun plus
embedding verb). We found that subjects are better dis-
ambiguators than objects. This seems counterintuitive
both on theoretical and empirical grounds. Theoretically,
60
Sentence Substitutes
By asking people who work
there, I have since determined
that he didn?t. (# 2002)
be employed 4;
labour 1
Remember how hard your ances-
tors worked. (# 2005)
toil 4; labour 3;
task 1
Figure 3: Lexical substitution example items for ?work?
the notion of verb phrase has been motivated, among
other things, with the claim that direct objects contribute
more to a verb?s disambiguation than subjects (Levin
and Rappaport Hovav, 2005). Empirically, subjects
are known to be realized more often as pronouns than
objects, which makes their vector representations less
semantically specific. However, we used two different
datasets ? the subject results on a set of intransitive
verbs, and the object results on a set of transitive verbs,
so the results are not comparable.
In this experiment, we construct a new, more con-
trolled dataset from the Lexical Substitution corpus to
systematically assess the importance of the three main
parameters: the relation used for disambiguation, the
combination function, and the reweighting parameter.
Construction of the LEXSUB-PARA dataset. The
original Lexical Substitution corpus, constructed for the
SemEval-1 lexical substitution task (McCarthy and Nav-
igli, 2007), consists of 10 instances each of 200 target
words in sentential contexts, drawn from a large inter-
net corpus (Sharoff, 2006). Contextually appropriate
paraphrases for each instance of each target word were
elicited from up to 6 participants. Figure 3 shows two in-
stances for the verb to work. The frequency distribution
over paraphrases can be understood as a characterization
of the target word?s meaning in each context.
For the current paper, we constructed a new subset of
LexSub we call LEXSUB-PARA by parsing LexSub with
Minipar (Lin, 1993) and extracting all 177 sentences
with transitive verbs that had overtly realized subjects
and objects, regardless of voice. We did not manually
verify the correctness of the parses, but discarded 17
sentences where we were not able to compute inverse
selectional preferences for the subject or object head
word (these were mostly rare proper names). This left
160 transitive instances of 42 verbs.
Evaluation For evaluation, we use a variant of the Se-
mEval ?out of ten? (OOT) evaluation metrics defined by
McCarthy and Navigli (2007). They developed two met-
rics, OOT Precision and Recall, which compare where a
predicted set of appropriate paraphrases must be evalu-
ated against a gold standard set. Their metrics are called
?out of ten? because they are measure the accuracy of the
first ten paraphrases predicted by the system. Since they
allow systems to abstain from predictions for any num-
ber of tokens, their two variants average this accuracy
(a), over the tokens with a prediction (OOT Precision),
and (b), over all tokens (OOT Recall). Since our system
0.5 1 2 5 10 20
add obj 61.5 59.7 58.9 56.1 56.0 55.7
add subj 61.7 61.7 59.5 58.4 57.3 57.0
add both 61.3 60.0 60.2 57.7 57.1 56.7
mult obj 59.8 59.7 57.8 55.7 55.7 55.4
mult subj 60.3 59.7 59.3 57.3 57.7 56.7
mult both 59.9 58.8 57.1 55.8 55.3 <1Pr
min obj 60.2 60.0 59.5 57.3 55.7 55.8
min subj 62.2 60.5 59.1 58.5 57.8 57.0
min both 62.3 60.2 59.8 57.3 55.8 55.1
Table 1: OOT accuracy on the LEXSUB-PARA dataset
across models and reweighting values (best results for
each model boldfaced). Random baseline: 53.7. Target
type vector baseline: 57.1. Pr: Numerical problem.
produces predictions for all tokens, OOT Precision and
Recall become identical.
Formally, let Gi be the gold paraphrases for occur-
rence i, and let f(s, i) be the frequency with which s
has been named as paraphrase for i. Let Mi be the ten
paraphrase candidates top-ranked by the SVS model for
i. We write out-of-ten accuracy (OOT) as:
OOT = 1/|I|
?
i
?
s?Mi?Gi
f(s, i)
?
s?Gi
f(s, i)
(7)
We compute two baselines. The first one is random
baseline that guesses whether paraphrases are appropri-
ate. The second baseline uses the original type vector
of the target verb without any combination, i.e., its ?out
of context meaning?, as representation for the token.
Results. Table 1 shows the results on the LEXSUB-
PARA dataset. Recall that the task is to decide the ap-
propriateness of paraphrases for verb instances, disam-
biguated by the inverse selectional preferences of their
subjects (subj), their objects (obj), and both. The ran-
dom baseline attains an OOT accuracy of 53.7, and the
type vector of the target vector performs at 57.1.
SVS is able to outperform both baselines for all val-
ues of the reweighting parameter n <2, and we find the
best results for the lowest value, n = 0.5. As for the
influence of the vector combination function, the best
result is yielded by min (OOT=62.3), followed by add
(OOT=61.7), while mult shows generally worse results
(OOT=60.3). For both add andmult, using only the sub-
ject as context only is optimal. The overall best result,
using min, is seen for both; however, the improvement
over subj is very small.
In the model mult-both-20, where target vectors were
multiplied with two very large expectation vectors, al-
most all instances failed due to overflow errors.
Discussion. Our results indicate that our parameter
optimization strategy in Erk and Pad? (2008) was in fact
flawed. The parameters that were best for the Mitchell
and Lapata (2008) data (mult, n = 20) are suboptimal
for LEXSUB-PARA data.5 The good results for low val-
5We assume that our results hold for the Pad? & Erk (2008)
lexical substitution dataset as well, due to its similar nature.
61
ues of n indicate that good discrimination between valid
and invalid paraphrases can be obtained by relatively
small modifications of the target vector in the direction
indicated by the context. Surprisingly, we still find that
the results in the subj condition are almost always better
than those in the obj condition, even though the dataset
consists only of transitive verbs, where we would have
expected the inverse result. We have two partial ex-
planations. First, we find that pronouns, which occur
frequently in subject position (I, he), are still informa-
tive enough to distinguish ?animate? from ?inanimate?
paraphrases of verbs such as touch. Second, we see
a higher number of Minipar errors in for object posi-
tions than for subject positions, and consequently more
data both for object fillers and for object selectional
preferences.
The overall best result was yielded by a condition that
used both (subject plus object) for disambiguation, using
the recursive modification from Eq. (4). While we see
this as a promising result, the difference to the second-
best result is very small, in almost all other conditions
the performance of both is close to the average of obj
and subj and thus a suboptimal choice.
4.2 Experiment 2: Creating larger datasets with
pseudo-paraphrases
With a size of 2,000 sentences, even the complete
LexSub dataset is tiny in comparison to many other
resources in NLP. Limiting attention to successfully
parsed transitive instances results in an even smaller
dataset on which it is difficult to distinguish noise from
genuine differences between models. This is a large
problem for the use of paraphrase appropriateness as
evaluation task for models of word meaning in context.
In consequence, the automatic creation of larger
datasets is an important task. While unsupervised meth-
ods for paraphrase induction are becoming available
(e.g., Callison-Burch (2008)), they are still so noisy
that the created datasets cannot serve as gold standards.
However, there is an alternative strategy: there is a
considerable amount of data in different languages an-
notated with categorical word sense, created (e.g.) for
Word Sense Disambiguation exercises such as Senseval.
We suggest to convert these data for use in a task similar
to paraphrase assessment, interpreting available infor-
mation about the word sense as pseudo-paraphrases.
Of course, the caveat is that these pseudo-paraphrases
may behave differently than genuine paraphrases. To
investigate this issue, we repeat Experiment 1 on this
dataset.
Construction of the SEMCOR-PARA dataset The
SemCor corpus is a subset of the Brown corpus that
contains 23,346 lemmas annotated with senses accord-
ing to WordNet 1.6. Fortunately, WordNet provides a
rich characterization of word senses. This allows us
to use the WordNet synonyms of a given word sense
as pseudo-paraphrases. Since it can be the case that
the target word is the only word in a synset, we also
0.5 1 2 5 10 20
add obj 21.7 20.7 23.2 24.3 24.2 21.8
add subj 20.6 20.1 22.9 24.4 23.3 19.7
add both 21.1 20.3 23.2 24.4 23.3 18.9
mult obj 22.6 24.8 25.0 24.4 24.2 21.4
mult subj 21.1 23.9 24.4 24.4 23.5 19.8
mult both 24.5 24.5 25.6 24.3 20.0 17.4
min obj 20.9 19.5 23.6 24.4 24.3 21.9
min subj 20.1 19.6 22.5 24.2 23.9 19.6
min both 20.1 19.8 25.2 24.5 24.3 19.0
Table 2: OOT accuracy on the SEMCOR-PARA dataset
across models and reweighting values (best results for
each line boldfaced). Random baseline: 19.6. Target
type vector baseline: 20.8
need to add direct hypernyms. Direct hypernyms have
been used in annotation tasks to characterize WordNet
senses (Mihalcea and Chklovski, 2003), an indicator
that they are usually close enough in meaning to func-
tion as pseudo-paraphrases.
Again, we parsed the corpus with Minipar and iden-
tified all sense-tagged instances of the verbs from
LEXSUB-PARA, to keep the two corpora as compa-
rable as possible. For each instance wi of word w, we
collected all synonyms and direct hypernyms of the
synset as the set of appropriate paraphrases. The list
of synonyms and direct hypernyms of all other senses
of w, whether they occur in SemCor or not, were con-
sidered inappropriate paraphrases for the instance wi.
This method does not provide us with frequencies for
the pseudo-paraphrases; we thus assumed a uniform fre-
quency of 1. This does not do away with the gradedness
of the meaning representation, though, since each token
is still associated with a set of appropriate paraphrases.
Out of 2242 transitive verb instances, we further re-
moved 153 since we could not compute selectional pref-
erences for at least one of the fillers. 484 instances were
removed because WordNet did not list any verbal para-
phrases for the annotated synset or its direct hypernym.
This resulted in 1605 instances for 40 verbs, a dataset
an order of magnitude larger than LEXSUB-PARA. (See
Section 4.3 for an example verb with paraphrases.)
Results and Discussion. We again use the OOT ac-
curacy measure. The results for paraphrase assessment
on SEMCOR-PARA are shown in Table 2. The numbers
are substantially lower than for LEXSUB-PARA. This
is first and foremost a consequence of the higher ?poly-
semy? of the pseudo-paraphrases. In LEXSUB-PARA,
the average numbers of possible paraphrases per tar-
get word is 20; in SEMCOR-PARA, 54. This is to be
expected and also reflected in the much lower random
baseline (19.6% OOT). However, we also observe that
the reduction in error rate over the baseline is consider-
ably lower for SEMCOR-PARA than for LEXSUB-PARA
(10% vs. 20% reduction).
Among the parameters of the model, we find the
largest impact for the reweighting parameter. The best
results occur in the middle range(n = 2 and n = 5),
62
with both lower and higher weights yielding consid-
erably lower scores. Apparently, it is more difficult
to strike the right balance between the target and the
expectations on this dataset. This is also mirrored in
the smaller improvement of the target type vector base-
line over the random baseline. As for vector combi-
nation functions, we find the best results for the more
?intersection?-like mult and min combinations, with
somewhat lower results for add; however, the differ-
ences are rather small. Finally, combination with obj
works better than combination with subj. At least among
the best results, both is able to improve over the use of ei-
ther individual relation. The best result uses mult-both,
with an OOT accuracy of 25.6.
4.3 Further analysis
In our two experiments, we have found systematic rela-
tionships between the SVS model parameters and their
performance within the LEXSUB-PARA and SEMCOR-
PARA datasets. Unfortunately, few of the parameter set-
tings we found to work well appear to generalize across
the two datasets; neither do they correspond to the op-
timal parameter values we established for the Mitchell
and Lapata dataset in our 2008 paper. Variables that
vary particularly strikingly are the reweighting parame-
ter and the performance of different relations. To better
understand these differences, we perform a further vali-
dation analysis that attempts to link model performance
to a variable that (a) behaves consistently across the two
datasets used in this paper and (b) sheds light onto the
patterns we have observed for the parameters.
The quantity we will use for this purpose is the aver-
age discriminativity of the model. We define discrimina-
tivity as the degree to which the token vector computed
by the model is on average more similar to the valid than
to the invalid paraphrases. For a paraphrase ordering
task such as the one we are considering, we want this
quantity to be as large as possible; very small quantities
indicate that the model is basically ?guessing? an order.
Figure 4 plots disciminativity against model perfor-
mance. As can be expected, it is indeed a very strong
correlation between discriminativity and OOT accu-
racy across all models. A Pearson?s correlation test
confirms that the correlation is highly significant for
both datasets (LEXSUB-PARA: r=0.65, p < 0.0001;
SEMCOR-PARA: r=0.76, p < 0.0001).
Next, we considered the relationship between the
mean discriminativity for different combinations and
reweighting values n. Figure 5 shows the resulting plots,
which reveal two main differences between the datasets.
The first one is the influence of the reweighting parame-
ter. For LEXSUB-PARA, the highest discriminativity is
found for small values of n, with decreasing values for
higher parameter values. In contrast, SEMCOR-PARA
shows the highest discriminativity for middle values of
n (on the order of 5?10), with lowest values on either
side. The second difference is the relative discrimina-
tivity of obj and subj. On LEXSUB-PARA, the subj
predictions are more discriminative than obj predictions
for all values of n. On SEMCOR-PARA, this picture is
reversed, with more discriminative obj predictions for
the best (and thus relevant) values of n.
We interpret these patterns, which fit the observed
OOT accuracy numbers well, as additional evidence that
the variations we see between the datasets are not noise
or artifacts of the setup, but arise due to the different
makeup of the two datasets. This ties in with our intu-
itions about the differences between human-generated
paraphrases and WordNet ?pseudo-paraphrases?. Com-
pare the following paraphrase lists:
dismiss (LexSub): banish, deride, discard, discharge, dis-
patch, excuse, fire, ignore, reject, release, remove, sack
dismiss (SemCor/WordNet): alter, axe, brush, can, change,
discount, displace, disregard, dissolve, drop, farewell,
fire, force, ignore, modify, notice, packing, push, reject,
remove, sack, send, terminate, throw, usher
The SEMCOR-PARA list contains a larger number of
unspecific pseudo-paraphrases such as change, push,
send, which stem from direct WordNet hypernyms of
the more specific dismiss senses. Presumably, these
terms are assigned rather general vectors which the SVS
finds difficult to rule out as paraphrases. This lowers
the discriminativity of the models, in particular for subj,
and results in the smaller relative improvement over
the baseline we observe for SEMCOR-PARA. This sug-
gests that the usability of word sense-derived datasets
in evaluations could be improved by taking depth in the
WordNet hierarchy into account when including direct
hypernyms among the pseudo-paraphrases.
5 Conclusions
In this paper, we have explored the parameter space
for the computation of vector-based representations of
token meaning with the SVS model.
Our evaluation scenario was paraphrase assessment.
To systematically assess the impact of parameter choice,
we created two new controlled datasets. The first one,
the LEXSUB-PARA dataset, is a small subset of the Lex-
ical Substitution corpus (McCarthy and Navigli, 2007)
that was specifically created for this task. The second
dataset, SEMCOR-PARA, which is considerably larger,
consists in instances from the SemCor corpus whose
WordNet annotation was automatically converted into
?pseudo-paraphrase? annotation.6
We found a small number of regularities that hold for
both datasets: namely, that the reweighting parameter
is the most important choice for a SVS model, followed
by the relation used as context, while the influence of
the vector combination function is comparatively small.
Unfortunately, the actual settings of these parameters
appeared not to generalize well from one dataset to
the other. We have collected evidence that these diver-
gences are not due to noise, but to genuine differences
6Both datasets can be obtained from the authors.
63
ll l
l l
l
l
l
l
l
l
l
l
l
l
l l l
l
l
l
l
l
l
l
l
l
l
l
l l l
l
l
l
l
l
l
l
l
l
l
0.018 0.020 0.022 0.024 0.026 0.028 0.030
0.56
0.57
0.58
0.59
0.60
0.61
0.62
mean sim(val)?sim(inval)
out 
of te
n pre
cisio
n
l
l
l
l
l
l
l l
l
l
l
l
l l
l
l
l
l
l
l
l l
l
l
l
l
l l
l
l
ll
l
ll
ll l
l
l
l
0.005 0.010 0.015 0.020 0.025 0.030
0.18
0.20
0.22
0.24
mean sim(val)?sim(inval)
o
ut o
f ten
 prec
ision
Figure 4: Scatterplot of "out of ten" accuracy against model discriminativity between valid and invalid paraphrases.
Left: LEXSUB-PARA, right: SEMCOR-PARA.
0 5 10 15 20 25 30
0.02
0
0.02
2
0.02
4
0.02
6
0.02
8
exponent
me
an 
sim(v
al)?si
m(inv
al)
target + object selpreftarget + subject selpref
0 5 10 15 20 25 30
0.00
5
0.01
0
0.01
5
0.02
0
0.02
5
exponent
me
an 
sim(v
al)?si
m(inv
al)
target + object selpreftarget + subject selpref
Figure 5: Average amount to which predictions are more similar to valid than to invalid paraphrases, for different
reweighting values. Left: LEXSUB-PARA, right: SEMCOR-PARA.
in the datasets. We describe an auxiliary quantity, dis-
criminativity, that measures the ability of the model?s
predictions to distinguish between valid and invalid para-
phrases.
The consequence we draw from this study is that it
is surprisingly difficult to establish generalizable ?best
practice? parameter setting for SVS. Good parameter
values appear to be sensitive to the properties of datasets.
For example, we have attributed the observation that
subjects are more informative on LEXSUB-PARA, while
objects work better on SEMCOR-PARA, to differences
in the set of paraphrase competitors. In this regard,
the conversion of the WSD corpus can be considered a
partial success. We have constructed the largest existing
paraphrase assessment corpus. However, the use of
WordNet information to create paraphrases results in a
very difficult corpus. We will investigate methods that
exclude overly general hypernyms of the target words as
paraphrases to alleviate the problems we see currently.
Discriminativity further suggests that paraphrase as-
sessment can be improved by selectional preference
representations that are trained to maximize the dis-
tance between valid and invalid paraphrases. Such a
representation could be provided by discriminative for-
mulations (Bergsma et al, 2008), or by exemplar-based
models that are able to deal better with the ambiguity
present in the preferences of very general words.
Another important topic for further research is the
computation of token vectors that incorporate more than
one context word. The current results we obtain for
?both? are promising but limited; it appears that the suc-
cessful integration of multiple context words requires
strategies that go beyond simplistic addition or intersec-
tion of observed contexts.
References
S. Bergsma, D. Lin, and R. Goebel. 2008. Discrimina-
tive learning of selectional preference from unlabeled
text. In Proceedings of EMNLP, pages 59?68.
C. Callison-Burch. 2008. Syntactic constraints on para-
phrases extracted from parallel corpora. In Proceed-
ings of EMNLP, pages 196?205.
K. Erk and S. Pad?. 2008. A structured vector space
model for word meaning in context. In Proceedings
of EMNLP.
64
K. Erk. 2007. A simple, similarity-based model for se-
lectional preferences. In Proceedings of ACL, pages
216?223.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
P. Hanks. 2000. Do word meanings exist? Computers
and the Humanities, 34(1-2):205?215.
D. Hindle and M. Rooth. 1993. Structural ambigu-
ity and lexical relations. Computational Linguistics,
19(1):103?120.
A. Kilgarriff and J. Rosenzweig. 2000. Framework
and results for English Senseval. Computers and the
Humanities, 34(1-2).
A. Kilgarriff. 1997. I don?t believe in word senses.
Computers and the Humanities, 31(2):91?113.
W. Kintsch. 2001. Predication. Cognitive Science,
25:173?202.
T. Landauer and S. Dumais. 1997. A solution to Platos
problem: the latent semantic analysis theory of ac-
quisition, induction, and representation of knowledge.
Psychological Review, 104(2):211?240.
B. Levin and M. Rappaport Hovav. 2005. Argument
Realization. Research Surveys in Linguistics Series.
CUP.
D. Lin. 1993. Principle-based parsing without overgen-
eration. In Proceedings of ACL, pages 112?120.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers, 28.
D. McCarthy and R. Navigli. 2007. SemEval-2007
Task 10: English Lexical Substitution Task. In Pro-
ceedings of SemEval, pages 48?53.
D. McCarthy. 2006. Relating WordNet senses for word
sense disambiguation. In Proceedings of the ACL
Workshop on Making Sense of Sense, pages 17?24.
S. McDonald and C. Brew. 2004. A distributional
model of semantic context effects in lexical process-
ing. In Proceedings of ACL, pages 17?24.
K. McRae, M. Spivey-Knowlton, and M. Tanenhaus.
1998. Modeling the influence of thematic fit (and
other constraints) in on-line sentence comprehension.
Journal of Memory and Language, 38:283?312.
R. Mihalcea and T. Chklovski. 2003. Open Mind
Word Expert: Creating large annotated data collec-
tions with web users? help. In Proceedings of the
EACL 2003 Workshop on Linguistically Annotated
Corpora (LINC 2003), Budapest, Hungary.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL,
pages 236?244.
S. Narayanan and D. Jurafsky. 2002. A Bayesian model
predicts human parse preference and reading time in
sentence processing. In Proceedings of NIPS, pages
59?65.
S. Pad?, U. Pad?, and K. Erk. 2007. Flexible, corpus-
based modelling of human plausibility judgements.
In Proceedings of EMNLP/CoNLL, pages 400?409.
M. Palmer, H. Dang, and C. Fellbaum. 2007. Making
fine-grained and coarse-grained sense distinctions,
both manually and automatically. Journal of Natural
Language Engineering. To appear.
P. Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational
realization. Cognition, 61:127?159.
H. Sch?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?124.
Serge Sharoff. 2006. Open-source corpora: Using the
net to fish for linguistic data. International Journal
of Corpus Linguistics, 11(4):435?462.
Y. Wilks. 1975. Preference semantics. In Formal
Semantics of Natural Language. CUP.
65
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 57?65,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Representing words as regions in vector space
Katrin Erk
University of Texas at Austin
katrin.erk@mail.utexas.edu
Abstract
Vector space models of word meaning typi-
cally represent the meaning of a word as a vec-
tor computed by summing over all its corpus
occurrences. Words close to this point in space
can be assumed to be similar to it in meaning.
But how far around this point does the region
of similar meaning extend? In this paper we
discuss two models that represent word mean-
ing as regions in vector space. Both represen-
tations can be computed from traditional point
representations in vector space. We find that
both models perform at over 95% F-score on
a token classification task.
1 Introduction
Vector space models of word meaning (Lund and
Burgess, 1996; Landauer and Dumais, 1997; Lowe,
2001; Jones and Mewhort, 2007; Sahlgren and Karl-
gren, 2005) represent words as points in a high-
dimensional semantic space. The dimensions of the
space represent the contexts in which each target
word has been observed. Distance between vec-
tors in semantic space predicts the degree of seman-
tic similarity between the corresponding words, as
words with similar meaning tend to occur in simi-
lar contexts. Because of this property, vector space
models have been used successfully both in com-
putational linguistics (Manning et al, 2008; Snow
et al, 2006; Gorman and Curran, 2006; Schu?tze,
1998) and in cognitive science (Landauer and Du-
mais, 1997; Lowe and McDonald, 2000; McDon-
ald and Ramscar, 2001). Given the known problems
with defining globally appropriate senses (Kilgarriff,
1997; Hanks, 2000), vector space models are espe-
cially interesting for their ability to represent word
meaning without relying on dictionary senses.
Vector space models typically compute one vec-
tor per target word (what we will call word type vec-
tors), summing co-occurrence counts over all corpus
tokens of the target. If the target word is polyse-
mous, the representation will constitute a union over
the uses or senses of the word. Such a model does
not provide information on the amount of variance
in each dimension: Do values on each dimension
vary a lot across occurrences of the target? Also, it
does not provide information on co-occurrences of
feature values in occurrences of the target. To en-
code these two types of information, we study richer
models of word meaning in vector space beyond sin-
gle point representations.
Many models of categorization in psychology
represent a concept as a region, characterized by
feature vectors with dimension weights (Smith et
al., 1988; Hampton, 1991; Nosofsky, 1986). Tak-
ing our cue from these approaches, we study two
models that represent a word as a region in vector
space rather than a point. The first model is one
that we have recently introduced for representing hy-
ponymy in vector space (Erk, 2009). We now test
its suitability as a general region model for word
meaning. This model can be viewed as a prototype-
style model that induces a region surrounding a cen-
tral vector. As it does not record co-occurrences of
feature values, we contrast it with a second model,
an exemplar-style model using a k-nearest neighbor
analysis, which can represent both degree of vari-
ance in each dimension and value co-occurrences.
Both models induce regions representations with-
out labeled data. The idea on which both models
are based is to use word token vectors to estimate a
57
region representation. We evaluate the two region
models on a task of token classification: Given a
point in vector space, the task is predict the word
of which it is a token vector.
By representing the meaning of words as regions
in vector space, we can describe areas in which
points encode similar meanings. This description is
flexible, depending on the target word in question,
rather than uniform for all words through a fixed
distance threshold from the target?s type vector. One
possible application of region models of word mean-
ing is in the task of determining the appropriateness
of a paraphrase in a given context (Connor and Roth,
2007). This task is highly relevant for textual entail-
ment (Szpektor et al, 2008). Current vector space
approaches typically compare the target word?s to-
ken vector to the type vector of the potential para-
phrase (Mitchell and Lapata, 2008; Erk and Pado,
2008). A region model could instead test the tar-
get?s token vector for inclusion in the potential para-
phrase?s region.
2 Related work
This section discusses existing vector space models
and compares vector space models in computational
linguistics to feature-based models of human con-
cept representation in psychology.
Vector space models. Vector space models rep-
resent the meaning of a target word as a vector in a
high-dimensional space (Lund and Burgess, 1996;
Landauer and Dumais, 1997; Sahlgren and Karl-
gren, 2005; Pado? and Lapata, 2007; Jones and Me-
whort, 2007). Dimensions stand for context items
which which the target word has been observed
to co-occur, for example other words (Lund and
Burgess, 1996) or syntactic paths (Pado? and Lapata,
2007). In the simplest case, the value on a dimension
is the raw co-occurrence count between the target
word and the context item for which the dimension
stands. Raw counts are often transformed, for ex-
ample using a log-likelihood transformation (Lowe,
2001). Sometimes the vector space as a whole is
transformed using dimensionality reduction (Lan-
dauer and Dumais, 1997).
In NLP, vector space models have featured most
prominently in information retrieval (Manning et
al., 2008), but have also been used for ontology
learning (Lin, 1998; Snow et al, 2006; Gorman
and Curran, 2006) and word sense-related tasks
(McCarthy et al, 2004; Schu?tze, 1998). In psy-
chology, vector space models have been used to
model synonymy (Landauer and Dumais, 1997;
Pado? and Lapata, 2007), lexical priming phenom-
ena (Lowe and McDonald, 2000), and similarity
judgments (McDonald and Ramscar, 2001). There
have also been studies on inducing hyponymy in-
formation from vector space representations. Gef-
fet and Dagan (2005) use a dimension re-weighting
scheme, then predict entailment when the most
highly weighted dimensions of two verbs stand in
a subset relation. However, they find that while re-
call of this method is good (whenever some senses
of two words stand in an entailment relation, top-
weighted dimensions of their vectors stand in a sub-
set relation), precision is problematic. Weeds, Weir
and McCarthy (2004) introduce the notion of distri-
butional generality (x is more distributionally gen-
eral than y if x occurs in more contexts than y) and
find that for hyponym-hypernym pairs from Word-
Net, hyponyms are typically more distributionally
general. (As they study only word pairs that are
known to be related by hyponymy, they test for recall
but not precision.) Erk (2009) suggests that while it
may not be possible to induce hyponymy informa-
tion from a vector space representation, it is possible
to encode it in a vector space representation after it
has been obtained through some other means.
Vector space models of word tokens. Vector
space models have mostly been used to represent
the meaning of a word type by summing its co-
occurrence counts over a complete corpus. There
are several approaches to computing vectors for in-
dividual word tokens. All of them compute word
type vectors first, then combine them into token vec-
tors. Kintsch (2001) and Mitchell and Lapata (2008)
combine the target?s type vector with that of a sin-
gle word in the target?s syntactic context. Lan-
dauer and Dumais (Landauer and Dumais, 1997)
and Schu?tze (1998) combine the type vectors of
all the words surrounding the target token. Erk
and Pado? (2008) combine the target?s type vector
with a vector representing the selectional preference
of a single word in the target?s syntactic context.
Smolensky (1990) focuses on integrating syntactic
information in the vector representation rather than
58
on representing the lexical meaning of the target.
Feature-based models of human concept rep-
resentation. Many models of human concept rep-
resentation in psychology are based on vectors of
features (e.g. (Smith et al, 1988; Hampton, 1991;
Nosofsky, 1986)). Features in these models are
typically weighted to represent their importance to
the concept in question. Similarity to a given fea-
ture vector is usually taken to decrease exponentially
with distance from that vector, following Shepard?s
law (Shepard, 1987). Categorization involves com-
petition between categories. Feature-based models
of human concept representation can be broadly cat-
egorized into prototype models, which represent a
concept by a single summary representation, and ex-
emplar models, which assume that categorization is
by comparison to remembered exemplars. As an ex-
ample of a feature-based model of concept represen-
tation, we show the definition of Nosofsky?s (1986)
Generalized Context Model (GCM). This exemplar
model estimates the probability of categorizing an
exemplar ~e as a member of a concept C as
P (C|~e) =
?
~??C w~?sim(~?,~e)?
concept C?
?
~??C? w~?sim(~?,~e)
(1)
where the concept C is a set of remembered exem-
plars, w~? is an exemplar weight, and the similarity
sim(~?,~e) between ~? and ~e is defined as
sim(~?,~e) = exp(z ? ?
dimension i
wi(?i ? ei)2) (2)
Here, z is a general sensitivity parameter, wi is a
weight for dimension i, and ?i, ei are the values
of ~? and ~e on dimension i. This model shows all
the properties listed above: It has weighted dimen-
sions through the wi. It incorporates Shepard?s law
through the exponential relation between sim and
the sum of squared value distances wi(?i ? ei)2.
Competition between categories arises through the
normalization of ~e?s similarity to C by the similar-
ity to all other categories in Eq. (1). While feature-
based models of concept representation talk about
concepts rather than word meaning, Murphy (2002)
argues that there is ?overwhelming empirical evi-
dence for the conceptual basis of word meaning?
through experimental results on conceptual phenom-
ena that have also been shown to hold for words.
Ga?rdenfors (2004) proposes a model that repre-
sents concepts as convex regions in a conceptual
space. Feature structures play no central role in this
model, but Ga?rdenfors suggests that concepts may
be represented by a central point, such that cate-
gorization could simply be determining the nearest
central point (without positing an exponential rela-
tion between distance and similarity).
3 Models
In this section, we present two models for represent-
ing word meaning as regions in vector space.
The centered model. The first model that we de-
fine, which we call the centered model, is prototype-
like. As the representation for a target word, it in-
duces a region surrounding the target?s type vec-
tor (Erk, 2009). Let w be the target word and
~w its type vector. Let ~x be a point in the same
vector space. To predict whether ~x represents the
same meaning as ~w, we estimate the probability
P (IN(~x, ~w)) that ~x is in the region around ~w, using
a log-linear model:
P (IN(~x, ~w)) = 1Z exp(
?
i
? INi fi(~x, ~w)) (3)
where the fi are features that characterize the point
~x, and the ? INi are weights identifying the impor-
tance of the different features for the class IN. Z is a
normalizing factor that ensures that P is a probability
distribution: If P (OUT(~x, ~w)) = 1?P (IN(~x, ~w)) is
the probability that ~x is not in the region around ~w,
with associated weights ?OUTi for the same features
fi, then Z =?`=IN,OUT exp(
?
i ?`i fi(~x, ~w)).
We define the features fi as follows: If ~w =
?w1, . . . , wn?, we define the feature fi(~x, ~w), for
1 ? i ? n, as the squared distance between ~w and ~x
on dimension i:
fi(~x) = (wi ? xi)2 (4)
This model, like feature-based models of catego-
rization from psychology, has weighted dimensions
through the ?i. It follows Shepard?s law ? the ex-
ponential relation between similarity and distance ?
through the exponential function in Eq. (3). Compe-
tition between categories is implicit in the estimation
of P (OUT(~x, ~w)).
59
Most of the weights ? INi can reasonably be ex-
pected to be negative, since a negative ? INi indicates
that membership of a point ~x in the w-region gets
less likely as the distance (wi?xi)2 increases. If ? INi
has a large negative value, categorization is highly
sensitive to changes in the ith dimension. If on the
other hand, ? INi is negative but close to zero, this
means that vector entries in dimension i can vary
greatly without much influence on categorization.
The parameters ? INi and ?OUTi need to be estimated
from training data. Although the log-likelihood
model is a supervised learning scheme, we do not
need to take recourse to labeled data. Instead, we
use token vectors: Token vectors of w will serve
as positive training data for estimating P (IN(~x, ~w)),
and token vectors of other words than w will con-
stitute negative training data. The amount of pre-
processing needed depends on the approach to com-
puting token vectors that we use. We will use an
approach that combines w?s type vector with that
of a single word in its syntactic context. This pre-
supposes a syntactic parse of the corpus. Note that
we could just as well have used a Schu?tze-style ap-
proach, which does not rely on parsing.
The distributed model. The second model that
we consider is an exemplar-style, instance-based
model. The simplest instance-based models are k-
nearest neighbor classifiers, which assign to a test
item the majority label of its k nearest neighbors
among the training items. We will here use a very
simple model, doing k nearest neighbor classifica-
tion where the distance between two vectors ~w and
~x is the sum of dimension distances ?i with
?i = ?i|wi ? xi|maxi ?mini
maxi and mini are the maximum and minimum
values observed for dimension i, and ?i is a fea-
ture weight. We use a standard feature weighting
method, gain ratio, which is information gain nor-
malized by the entropy of feature values. Informa-
tion gain on its own has a bias towards features with
many values, which gain ratio attenuates in favor of
features with lower entropy:
?i =
H(C) ??y?val(i) P (y)H(C|y)
??y?val(i) P (y) log2 P (y)
(5)
for the set C = {IN, OUT} of classes and sets val(i)
of values seen for dimension i. We call this the dis-
tributed model. As with the centered model, we
compare it to models of concept representation: It
has weighted dimensions (Eq. (5)), and it incorpo-
rates competition between categories by storing both
positive and negative exemplars and categorizing ac-
cording to the majority among the k nearest neigh-
bors. However, it does not implement Shepard?s law.
It additionally differs from the GCM (Eq. (1)) in bas-
ing categorization on the k nearest neighbors rather
than summed similarity to all neighbors.
Like the centered model, the distributed model
needs both positive and negative training data.
Again, labeled data is not necessary as we can use
word token vectors. Positive training data consists of
tokens of the target word, and tokens of other words
are negative training data. This model does not make
use of the target?s type vector.
Above we have discussed two pieces of informa-
tion that region models can encode and that are hard
to encode in single-point models of word meaning:
variance in each dimension and co-occurrence of
feature values. The centered model encodes the vari-
ance in the values of each dimension through the
weights ? INi , but it does not retain information on
feature values of different dimensions that tend to
co-occur. The distributed model encodes both vari-
ance in each dimension and co-occurrence of fea-
ture values through the remembered exemplars. So
the centered model should do well for monosemous
words, since it seems reasonable that their token
vectors should form a single region around the type
vector. For polysemous words, token vectors could
be more scattered in semantic space, in which case
the distributed model should do better.
Note that neither the centered nor the distributed
model is a clustering model: Both are supervised
models learning the distinctions between tokens of
the target word and other vectors. Neither of them
groups vectors in an unsupervised fashion.
Hard versus soft region boundaries. In the
current paper, we consider only regions with sharp
boundaries. In the centered model, a point ~x
will be considered a member of the w-region if
P (IN(~x, ~w)) ? 0.5. In the distributed model, ~x
will be considered a member if the majority of its
k nearest neighbors are members. However, it is im-
60
portant that both models can also be used to repre-
sent regions with soft boundaries. In the centered
model, we can use P (IN(~x, ~w)) without a thresh-
old. In the distributed model, we can use the fraction
of k that are positive instances, or we can compute
summed similarity to the positive instances like the
GCM does. So both models can be used to estimate
degrees of membership in a target word?s region.
4 Task, Data, and Implementation
This section describes the task used for evaluation,
the data, and the implementation of the models.
Task. The main task will be for a model trained
on a target word w to predict, for a given point ~x in
semantic space, whether ~x is a token vector of w or
not. This task is a direct test of whether the region
induced for w succeeds in characterizing the region
in semantic space in which tokens of w will occur.
As an example, consider the target word super-
sede: Region models of supersede will be trained
on tokens of supersede in a training dataset. One
such token is supersede knowledge (i.e., knowledge
as the direct object of supersede). We compute a to-
ken vector for this occurrence by combining the type
vectors of supersede and knowledge. After train-
ing a model, we test it on tokens occurring in a test
dataset. Positive test items are tokens of supersede,
and negative test items are tokens of other words, for
example guard. An example of a positive test item
is supersede collection. The test items will consist
solely of tokens that do not occur in the training data.
Data. We focus on verbs in this paper since para-
phrase appropriateness for verbs is an important task
in the context of textual entailment. Since we sus-
pect that the centered model will be better suited to
modeling monosemous words while the distributed
model should do equally well on monosemous and
polysemous words, we first test a group of monose-
mous verbs, then a mixed group. We use WordNet
3.0 to form the two groups. The first group consists
of all verbs listed in WordNet 3.0 as being monose-
mous. We refer to this set as Mon. Since we also
want to compare the two region models on the task
of hyponymy encoding (Erk, 2009), we use as our
set of mixed monosemous and polysemous verbs the
verbs used there to test hyponymy encoding: the set
of all verbs that are hypernyms of the Mon verbs ac-
cording to WordNet 3.0. We call this set Hyp.
We use the British National Corpus (BNC) to
compute the vector space and as our source of tar-
get word tokens. We need token vectors for training
the two region models, and we need separate, previ-
ously unseen token vectors as test data. So we split
the written portion of the BNC in half at random,
leaving files intact. This yielded a training and a test
set. We computed word type vectors from the train-
ing half of the BNC, using a syntax-based vector
space (Pado? and Lapata, 2007) of 500 dimensions,
with raw co-occurrence counts as dimension values.
We used the dv package1 to compute type vectors
from a Minipar (Lin, 1993) parse of the BNC.
We computed token vectors by combining the tar-
get verb?s type vector with the type vector of the
word occurring as the target?s direct object. We
test three methods for combining type vectors: First,
component-wise multiplication (below called mult),
which showed best results in Mitchell and Lapata?s
(2008) analysis. Second, component-wise averag-
ing (below called avg), a variant of type vector addi-
tion, a method often used for computing token vec-
tors. Third, we consider component-wise minimum
(min), which can be viewed as a kind of intersection
of the contexts with which the two words have been
observed. We used the training half of the BNC to
extract training tokens of the target verbs, and the
test half for extracting test tokens. We used only
those verb/object pairs as test tokens that did not also
occur in the training data.
We restricted the set of verbs to avoid data sparse-
ness issues, using only verbs that occurred with at
least 50 different direct objects in the training part of
the BNC. The direct objects, in turn, were restricted
to exclude overly rare and overly frequent (and thus
potentially uninformative) items. We restricted the
direct objects to those with no more than 6,500 and
no less than 270 occurrences in Mon ? Hyp. The
resulting set Mon consisted of 120 verbs, and Hyp
consisted of 430 verbs.
Model implementation. We implemented the
centered model using the OpenNLP maxent pack-
age2, and the distributed model using TiMBL3 in the
IB1 setting with k = 5 nearest neighbors. We use bi-
1http://www.nlpado.de/?sebastian/dv.html
2http://maxent.sourceforge.net/
3http://ilk.uvt.nl/timbl/
61
centered distributed
Prec Rec F Prec Rec F
mult 100 73.2 84.5 29.4 47.5 36.3
avg 99.6 91.3 95.3 71.1 99.9 83.1
min 97.9 85.4 91.2 21.0 90.3 34.1
Table 1: Results: token classification for monosemous
verbs. Random baseline: Prec 0.8, Rec 49.8, F 1.6.
nary models throughout, such that the classification
task is always between IN and OUT. In training and
testing, each token vector was presented to a model
only once, ignoring the frequency of direct objects.
5 Experiments
This section reports on experiments that test the per-
formance of the two region models of word meaning
in vector space that we have presented in Sec. 3, the
centered and the distributed model.
Experiment 1: Token classification for
monosemous verbs
In the first experiment, we test whether the two
region models can identify novel tokens of the
monosemous verbs in Mon. The task is the one de-
scribed in Sec. 4. We focus on monosemous verbs
first because we suspect that the centered model
should do better here than on polysemous verbs.
Both models were trained using token vectors com-
puted from the training half of the BNC. Token vec-
tors of the target verb were treated as positive data,
and token vectors of other verbs as negative data.4
We used resampling to restrict the number of nega-
tive items used during training, using 3% of the neg-
ative items, randomly sampled.5 We use for test-
ing only those direct objects that do not also ap-
pear in the training data, yielding 6,339 positive and
1,396,552 negative test items summed over all tar-
get verbs. The case of supersede discussed in Sec. 4
is an example of a monosemous verb according to
WordNet 3.0.
Table 1 summarizes precision, recall and F-score
results. Both models easily beat the random base-
4This simplification breaks down for 6 of the 120 verbs
(5%), which are in fact synonyms. We consider this an accept-
able level of noise.
5The number of 3% was determined on a development set
constructed by further splitting the training set into training and
development portion.
centered distributed
freq. Prec Rec F Prec Rec F
mult 50-100 100 59.3 74.5 20.8 47.2 28.9
100-200 100 89.4 94.4 57.4 49.7 53.2
200-500 100 97.4 98.7 92.1 41.1 56.9
avg 50 - 100 99.5 86.6 92.6 61.6 99.8 76.2
100-200 99.7 96.6 98.1 86.3 100 92.6
200-500 100 100 100 99.1 100 99.6
min 50-100 100 82.9 90.6 17.9 92.6 30.1
100-200 98.2 88.2 93.0 25.4 89.2 39.6
200-500 86.4 90.3 88.3 42.9 80.0 55.9
Table 2: Results: token classification for monosemous
verbs, by target frequency
centered distributed
# senses Prec Rec F Prec Rec F
all 100 92.9 96.3 99.6 99.8 99.7
1 100 86.1 92.5 99.0 99.5 99.2
2-5 100 90.8 95.2 99.4 99.6 99.5
6-10 100 93.5 96.7 99.9 99.9 99.9
11-20 100 96.6 98.3 100 100 100
? 21 100 99.5 99.7 100 100 100
Table 3: Results: Token classification for polysemous
verbs, avg token computation. Random baseline: Prec
8.2, Rec 50.4, F 14.0.
line. The centered model shows better performance
overall than the distributed one, and the avg method
of computing token vector worked best for both
models. The centered model has extremely high
precision throughout, while the distributed model
has better recall for conditions avg and min. Ta-
ble 2 breaks down the results by the frequency of
the target verb, measured in the number of different
verb/object tokens in the training data.
Experiment 2: Token classification for
polysemous verbs
We now test how the centered and distributed mod-
els fare on the same task, but with a mixture of
monosemous and polysemous verbs. We use the
verbs in Hyp, which in WordNet 3.0 have on aver-
age 6.79 senses. For example, follow is a WordNet
hypernym of the monosemous supersede. It has 24
senses, among them comply and postdate. Among
its training tokens are follow instruction and follow
dinner. The first is probably the comply sense of fol-
low, the second the postdate sense. An example of a
test token (i.e., occurring in the test but not the train-
62
ing data) is follow tea. (If tea is tea time, this is also
the postdate sense.)
We computed type vectors for the Hyp verbs and
their objects from the training half of the BNC, and
computed token vectors using the best method from
Exp. 1, avg. Again, we use for testing only those to-
kens that do not also appear in the training data. Due
to the larger amount of data, we used resampling in
the training as well as the test data, using only a ran-
dom 3% of negative tokens for testing. This yielded
25,736 positive and 670,630 negative test items.
Table 3 shows the results: The first line has the
overall results, and the following lines break down
the results by the number of senses each lemma has
in WordNet 3.0.6 Both models, centered and dis-
tributed, easily beat the random baseline. The cen-
tered model has comparable results for the Hyp as
for the Mon verbs (cf. Table 1), while the distributed
model has better results for this dataset, and better
results than the centered model. The centered model
shows a marked improvement in recall as the num-
ber of senses increases.
Experiment 3: Encoding hyponymy
We first proposed the centered model as a method
for encoding hyponymy information in a vector
space representation (Erk, 2009). Hyponymy infor-
mation from another source, in this case WordNet,
was encoded in a centered region representation of
a target verb by using tokens of the verb itself as
well as tokens from its direct hyponyms in training
the model. Negative data consisted of training data
tokens that were not occurrences of the target verb
or its direct hyponyms. In the example of the verb
follow, the positive training data would contain to-
kens of follow along with tokens of supersede and
guard, another direct hyponym of follow. Negative
training tokens would include, for example, tokens
of the word destroy. The resulting centered model,
in this case of follow, was then tested on previously
unseen tokens, for example guard purpose (a token
of a hyponym) and destroy lawn (a token of a non-
hyponym), with the task of predicting whether they
were tokens of direct hyponyms of follow or not.
6The one-sense items in Table 3 are a 43 verb subset of Mon.
The reason for the difference in performance in comparison to
Table 1 is unclear, as the two sets have similar distributions of
lemma frequencies.
centered distributed
Prec Rec F Prec Rec F
95.2 43.4 59.6 68.3 58.6 63.1
Table 4: Results: Identifying hyponyms based on ex-
tended hypernym representations, avg token computa-
tion. Random baseline: Prec 11.0, Rec 50.2, F 18.0
We now repeat this experiment with the dis-
tributed model. We use the direct hypernyms of the
verbs in Mon, with the same frequency restrictions
as above. We refer to this set of 273 verbs as DHyp.
We train one centered and one distributed model for
each verb w in DHyp. Positive training tokens for
training a model for a verb w ? DHyp are tokens
of w and of all sufficiently frequent children of w
in WordNet 3.0. Negative training tokens are to-
kens of other verbs in DHyp and their children. We
again sample a random 3% of the negative data dur-
ing both training and testing.
Table 4 shows the results. Both models again beat
the baseline. The distributed model shows slightly
better results overall, while the centered model has
by far the highest precision.
Discussion
Performance on monosemous verbs. For the
monosemous verbs in Exp. 1, both models succeed
in inducing regions that characterize tokens of a tar-
get word with high precision as well as high recall.
The extremely high precision of the centered model
shows that in general the region surrounding the type
vector does not contain any tokens of other verbs
than the target. Concerning the distributed model, it
is to be expected that in min, and even more so in
mult, dimension values will vary more than in avg;
this could explain the huge difference between avg
and the other two conditions for this model. It is
interesting to note that the centered model achieves
better precision, while the distributed model reaches
higher recall. Maybe it will be possible in later mod-
els to combine their strengths. The breakdown by
frequency bands in Table 2 shows that in mult and
avg, the models get strictly better with more data,
while min has a precision/recall tradeoff.
Performance on polysemous verbs. For the pol-
ysemous verbs in Exp. 2, like for the monosemous
verbs in Exp. 1, both models show excellent per-
63
formance in distinguishing tokens of the target verb
from tokens of other verbs.7 The distributed model
surpasses the centered one on this dataset. However,
it is not clear that this is because the contiguous re-
gion that the centered model infers is inappropriate
for polysemous verbs. After all the centered model,
too, achieves better performance on this dataset than
on Mon. The fact that results get better with the de-
gree of polysemy, at first surprising, may indicate
that the centered model draws an overly tight bound-
ary around the type vector and that this boundary
improves when token vectors differ more, and are
at greater distance from the type vector, as should
be the case for more polysemous lemmas. Another
possible reason for the better performance of both
models is that this dataset is larger and in particular
provides a larger set of negative data.
Encoding external information in a region
model. In the hyponymy encoding task in Exp. 3,
both models successfully encode hyponymy infor-
mation in vector space representations. The cen-
tered model manages to derive a high-precision re-
gion around the type vector, while the distributed
model makes use of outliers in the training data to
achieve higher recall.
Comparing region representations to point
representations. We now compare the two region
models to existing variants of point-based vector
space models. Both region models have dimen-
sion weights, whose function is somewhat similar to
that of log-likelihood or mutual information trans-
formations of raw co-occurrence counts: to estimate
the importance of each dimension for characteriz-
ing the target word in question. However, dimension
weights in region models are computed based on to-
ken vectors, while all co-occurrence count transfor-
mations work on type vectors.
The distributed model additionally has the ability
to represent typical co-occurrences of feature values
because the training tokens are remembered in their
entirety. The most similar mechanism in point-based
vector space models is probably dimensionality re-
duction, which strives to find latent dimensions that
explain most of the variance in the data. But again,
dimensionality reduction uses type vectors while the
7The near-perfect performance in particular of the dis-
tributed model has been confirmed on a separate noun dataset.
distributed model stores token vectors, which can
show more variance than the type vectors alone.
Applications of region models. Region models
of word meaning are interesting for the task of test-
ing the appropriateness of paraphrases in context.
Previous models either used competition between
paraphrase candidates or a global similarity thresh-
old to decide whether to accept a paraphrase can-
didate (Mitchell and Lapata, 2008; Szpektor et al,
2008). A region model of word meaning used for
the same task would still require a threshold, in this
case a threshold on membership probability, but the
regions for which membership is tested could dif-
fer in their size, and the extent of each region would
be learned individually from the data. To use the
model, for example to test whether trickle is a good
paraphrase for run in the color ran, we would test
whether the sentence-specific token vector for run
falls into the region of trickle.
6 Conclusion and outlook
In this paper, we have proposed using region models
for word meaning in vector space, predicting regions
in space in which points can be assumed to carry the
same meaning. We have studied two models, the
prototype-like centered models and the exemplar-
like distributed model, both of which are learned
without labeled data by making use of token vectors
of the target word in question. Both models show
excellent performance, with F-scores of 83%-99%,
on the task of identifying previously unseen occur-
rences of the target word.
Our aim is to to test the usability of region mod-
els for predicting paraphrase appropriateness in con-
text. The next step towards that will be to test region
models on the task of identifying synonym tokens.
Acknowledgements. Many thanks to Jason
Baldridge, David Beaver, Graham Katz, Alexander
Koller, Ray Mooney, and Manfred Pinkal for helpful
discussions. This work was supported by the Mor-
ris Memorial Grant from the New York Community
Trust.
References
M. Connor and D. Roth. 2007. Context sensitive para-
phrasing with a single unsupervised classifier. In Pro-
ceedings of ECML-07, Warsaw, Poland.
64
K. Erk and S. Pado. 2008. A structured vector space
model for word meaning in context. In Proceedings of
EMNLP-08, Hawaii.
K. Erk. 2009. Supporting inferences in semantic space:
representing words as regions. In Proceedings of
IWCS-8, Tilburg, Netherlands.
P. Ga?rdenfors. 2004. Conceptual spaces. MIT press,
Cambridge, MA.
M. Geffet and I. Dagan. 2005. The distributional inclu-
sion hypotheses and lexical entailment. In Proceed-
ings of ACL-05, Ann Arbor, MI.
J. Gorman and J. R. Curran. 2006. Scaling distributional
similarity to large corpora. In Proceedings of ACL ?06,
Sydney.
J. A. Hampton. 1991. The combination of prototype con-
cepts. In P. Schwanenflugel, editor, The psychology of
word meanings. Lawrence Erlbaum Associates.
P. Hanks. 2000. Do word meanings exist? Computers
and the Humanities, 34(1-2):205?215(11).
M. Jones and D. Mewhort. 2007. Representing word
menaing and order information in a composite holo-
graphic lexicon. Psychological Review, 114:1?37.
A. Kilgarriff. 1997. I don?t believe in word senses. Com-
puters and the Humanities, 31(2):91?113.
W. Kintsch. 2001. Predication. Cognitive Science,
25:173?202.
T. Landauer and S. Dumais. 1997. A solution to Platos
problem: the latent semantic analysis theory of ac-
quisition, induction, and representation of knowledge.
Psychological Review, 104(2):211?240.
D. Lin. 1993. Principle-based parsing without overgen-
eration. In Proceedings of ACL?93, Columbus, Ohio.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In COLING-ACL98, Montreal, Canada.
W. Lowe and S. McDonald. 2000. The direct route: Me-
diated priming in semantic space. In Proceedings of
the Cognitive Science Society.
W. Lowe. 2001. Towards a theory of semantic space. In
Proceedings of the Cognitive Science Society.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments,
and Computers, 28:203?208.
C. D. Manning, P. Raghavan, and H. Schu?tze. 2008. In-
troduction to Information Retrieval. Cambridge Uni-
versity Press.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding predominant senses in untagged text. In Pro-
ceedings of ACL?04, Barcelona, Spain.
S. McDonald and M. Ramscar. 2001. Testing the dis-
tributional hypothesis: The influence of context on
judgements of semantic similarity. In Proceedings of
the Cognitive Science Society.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL-08,
Columbus, OH.
G. L. Murphy. 2002. The Big Book of Concepts. MIT
Press.
R. M. Nosofsky. 1986. Attention, similarity, and the
identification-categorization relationship. Journal of
Experimental Psychology: General, 115:39?57.
S. Pado? and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
M. Sahlgren and J. Karlgren. 2005. Automatic bilingual
lexicon acquisition using random indexing of parallel
corpora. Journal of Natural Language Engineering,
Special Issue on Parallel Texts, 11(3).
H. Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1).
R. Shepard. 1987. Towards a universal law of
generalization for psychological science. Science,
237(4820):1317?1323.
E. E. Smith, D. Osherson, L. J. Rips, and M. Keane.
1988. Combining prototypes: A selective modifica-
tion model. Cognitive Science, 12(4):485?527.
P. Smolensky. 1990. Tensor product variable binding and
the representation of symbolic structures in connec-
tionist systems. Artificial Intelligence, 46:159?216.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
Proceedings of COLING/ACL?06.
I. Szpektor, I. Dagan, R. Bar-Haim, and J. Goldberger.
2008. Contextual preferences. In Proceedings of
ACL-08, Columbus, OH.
J. Weeds, D. Weir, and D. McCarthy. 2004. Character-
ising measures of lexical distributional similarity. In
Proceedings of COLING-04, Geneva, Switzerland.
65
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 50?53,
Suntec, Singapore, 7 August 2009.
c
?2009 ACL and AFNLP
Measuring semantic relatedness with vector space models and random
walks
Amac? Herda
?
gdelen
Center for Mind/Brain Sciences
University of Trento
amac@herdagdelen.com
Katrin Erk
Linguistics Department
University of Texas at Austin
katrin.erk@mail.utexas.edu
Marco Baroni
Center for Mind/Brain Sciences
University of Trento
marco.baroni@unitn.it
Abstract
Both vector space models and graph ran-
domwalk models can be used to determine
similarity between concepts. Noting that
vectors can be regarded as local views of
a graph, we directly compare vector space
models and graph random walk models on
standard tasks of predicting human simi-
larity ratings, concept categorization, and
semantic priming, varying the size of the
dataset from which vector space and graph
are extracted.
1 Introduction
Vector space models, representing word mean-
ings as points in high-dimensional space, have
been used in a variety of semantic relatedness
tasks (Sahlgren, 2006; Pad?o and Lapata, 2007).
Graphs are another way of representing relations
between linguistic entities, and they have been
used to capture semantic relatedness by using both
corpus-based evidence and the graph structure of
WordNet and Wikipedia (Pedersen et al, 2004;
Widdows and Dorow, 2002; Minkov and Cohen,
2008). We study the relationship between vec-
tor space models and graph random walk mod-
els by embedding vector space models in graphs.
The flexibility offered by graph randomwalk mod-
els allows us to compare the vector space-based
similarity measures to extended notions of relat-
edness and similarity. In particular, a random
walk model can be viewed as smoothing direct
similarity between two vectors using second-order
and even higher-order vectors. This view leads
to the second focal point of this paper: We in-
vestigate whether random walk models can sim-
ulate the smoothing effects obtained by methods
like Singular Value Decomposition (SVD). To an-
swer this question, we compute models on reduced
(downsampled) versions of our dataset and evalu-
ate the robustness of random walk models, a clas-
sic vector-based model, and SVD-based models
against data sparseness.
2 Model definition and implementation
We use directed graphs with weighted edges, G =
(V,E,w) where V is a set of nodes, E = V ? V
is a set of edges and w : E ? R is the weight-
ing function on edges. For simplicity, we assume
that G is fully connected, edges with zero weights
can be considered as non-existing in the graph. On
these graphs, we perform random walks with an
initial probability distribution q over the nodes (a
1 ? |V | vector). We then follow edges with prob-
ability proportional to their weights, so that the
probability of walking from node v
1
to node v
2
is w(v
1
, v
2
)/
?
v
w(v
1
, v). A fixed length random
walk ends after a predetermined number of steps.
In flexible walks, there is a constant probability ?
of stopping at each step. Thus, walk length fol-
lows a geometric distribution with parameter ?,
the probability of a walk of length k is ?(1??)
k?1
and the expected walk length is 1/?. For example,
a flexible walk with ? = 1/2 will produce 1-step,
2-step, and higher-step walks while the expected
average length is 2.
Relating vectors and graphs. Corpus co-
occurrence (e
1
, e
2
, a
12
) of two entities e
1
and e
2
that co-occur with (potentially transformed) count
a
12
can be represented in either a vector or a graph.
In a vector, it corresponds to a dimension value of
a
12
for the dimension e
2
of entity e
1
. In a graph,
it corresponds to two nodes labeled e
1
and e
2
con-
nected by an edge with weight a
12
.
Similarity measures. Let R(q) = p denote a
specific random walk process which transforms an
50
initial probability distribution q to a final prob-
ability distribution p over the nodes. We write
q(m) for the probability assigned to the node m
under q. If the initial distribution q concentrates
all probability on a single node n, i.e., q(n) = 1
and q(x) = 0 for all nodes x 6= n, we write
pr(n ? m) for the probability p(m) of ending
up at node m.
The simplest way of measuring relatedness
through random walks is to consider the probabil-
ity p(m) of a single node m as an endpoint for a
walk starting with start probability distribution q,
that is, p = R(q). We call this a direct, one-
direction measure of relatedness between q and
m. Direct, one-direction measures are typically
asymmetric. In case all start probability is con-
centrated on a single node n, we can also consider
direct, two-direction measures, which will be a
combination of pr(m ? n) and pr(n ? m). The
point of using two-direction measures is that these
can be made symmetric, which is an advantage
when we are modeling undirected semantic sim-
ilarity. In the experiments below we focus on the
average of the two probabilities.
In addition to direct measures, we will use in-
direct measures, in which we compute the relat-
edness of endpoint probability distributions p
1
=
R(q
1
) and p
2
= R(q
2
). As endpoint distribu-
tions can be viewed both as probability distribu-
tions and as vectors, we used three indirect mea-
sures: 1) Jensen/Shannon divergence, a symmet-
ric variant of the Kullback/Leibler divergence be-
tween probability distributions. 2) cosine similar-
ity, and 3) dot product. Dot product is a natural
choice in a graph setting because we can view it as
the probability of a pair of walks, one starting at a
node determined by q
1
and the other starting at a
node governed by q
2
, ending at the same node.
Discussion. Direct and indirect relatedness mea-
sures together with variation in walk length give us
a simple, powerful and flexible way to capture dif-
ferent kinds of similarity (with traditional vector-
based approach as a special case). Longer walks
or flexible walks will capture higher order effects
that may help coping with data sparseness, similar
to the use of second-order vectors. Dimensionality
reduction techniques like Singular Value Decom-
position (SVD) also capture these higher-order ef-
fects, and it has been argued that that makes them
more resistant against sparseness (Sch?utze, 1997).
To our knowledge, no systematic comparison of
SVD and classical vector-based methods has been
done on different corpus sizes. In our experiments,
we will compare the performance of SVD and
flexible-walk smoothing at different corpus sizes
and for a variety of tasks.
Implementation: We extract tuples from the 2-
billion word ukWaC corpus,
1
dependency-parsed
with MINIPAR.
2
Following Pad?o and Lapata
(2007), we only consider co-occurrences where
two target words are connected by certain de-
pendency paths, namely: the top 30 most fre-
quent preposition-mediated noun-to-noun paths
(soldier+with+gun), the top 50 transitive-verb-
mediated noun-to-noun paths (soldier+use+gun),
the top 30 direct or preposition-mediated verb-
noun paths (kill+obj+victim, kill+in+school), and
the modifying and predicative adjective-to-noun
paths. Pairs (w
1
, w
2
) that account for 0.01%
or less of the marginal frequency of w
1
were
trimmed. The resulting tuple list, with raw counts
converted to mutual information scores, contains
about 25 million tuples.
To test how well graph-based and alternative
methods ?scale down? to smaller corpora, we sam-
pled random subsets of tuples corresponding to
0.1%, 1%, 10%, and 100% of the full list. To put
things into perspective, the full list was extracted
from a corpus of about 2 billion words; so, the
10% list is on the order of magnitude of the BNC,
and the 0.1% list is on the order of magnitude of
the Brown corpus. From each of the 4 resulting
datasets, we built one graph and two vector space
models: one space with full dimensionality, and
one space reduced to 300 dimensions using singu-
lar value decomposition.
3 Experiments
First, we report the results for all tasks obtained on
the full data-set and then proceed with the compar-
ison of different models on differing graph sizes
to see the robustness of the models against data
sparseness.
Human similarity ratings: We use the dataset
of Rubenstein and Goodenough (1965), consist-
ing of averages of subject similarity ratings for
65 noun pairs. We use the Pearson?s coefficient
between estimates and human judgments as our
performance measure. The results obtained for
1
http://wacky.sslmit.unibo.it
2
http://www.cs.ualberta.ca/
?
lindek/
minipar.htm
51
Direct (average) Vector (cosine) Indirect (dot product) Previous
0.5 1 2 svd vector 0.5 1 2
RG 0.409 0.326 0.571 0.798 0.689 0.634 0.673 0.400 BL: 0.70
CLW: 0.849
AAMP Purity 0.480 0.418 0.669 0.701 0.704 0.664 0.667 0.612 AP: 0.709
RS: 0.791
Hodgson
synonym 2, 563 1.289 5, 408
??
10.015
??
6, 623
??
5, 462
??
5, 954
??
5, 537
??
coord 4, 275
??
3, 969
??
6, 319
??
11.157
??
7, 593
??
8, 466
??
8, 477
??
4, 854
??
antonym 2, 853? 2, 237 5, 319
??
7, 724
??
5, 455
??
4, 589
??
4, 859
??
6, 810
??
conass 9, 209
??
10.016
??
5, 889
??
9, 299
??
6, 950
??
5, 993
??
5, 455
??
4, 994
??
supersub 4, 038
??
4, 113
??
6, 773
??
10.422
??
7, 901
??
6, 792
??
7, 165
??
4, 828
??
phrasacc 4, 577
??
4, 718
??
2, 911
?
3, 532
?
3, 023
?
3, 506
?
3, 612
?
1.038
Table 1: All datasets. * (**) indicates significance level p < 0.01 (p < 0.001). BL: (Baroni and Lenci,
2009), CLW: (Chen et al, 2006), AP: (Almuhareb, 2006), RS: (Rothenh?ausler and Sch?utze, 2009)
0.1% 1% 10%
cos svd cos vector dot 2 cos svd cos vector dot 2 cos svd cos vector dot 2
RG 0.219 0.244 0.669 0.676 0.700 1.159 0.911 0.829 1.068
AAMP 0.379 0.339 0.366 0.723 0.622 0.634 0.923 0.886 0.948
Synonym 0.369 0.464 0.610 0.493 0.590 0.833 0.857 0.770 1.081
Antonym 0.449 0.493 0.231 0.768 0.585 0.730 1.044 0.849 0.977
Conass 0.187 0.260 0.261 0.451 0.498 0.942 0.857 0.704 1.062
Coord 0.282 0.362 0.456 0.527 0.570 1.050 0.927 0.810 1.187
Phrasacc 0.268 0.132 0.761 0.849 0.610 1.215 0.920 0.868 1.049
Supersub 0.313 0.353 0.285 0.645 0.601 1.029 0.936 0.752 1.060
Table 2: Each cell contains the ratio of the performance of the corresponding model for the corresponding
downsampling ratio to the performance of the same model on the full graph. The higher ratio means the
less deterioration due to data sparseness.
the full graph are in Table 1, line 1. The SVD
model clearly outperforms the pure-vector based
approach and the graph-based approaches. Its per-
formance is above that of previous models trained
on the same corpus (Baroni and Lenci, 2009). The
best model that we report is based on web search
engine results (Chen et al, 2006). Among the
graph-based random walk models, flexible walk
with parameter 0.5 and fixed 1-step walk with in-
direct relatedness measures using dot product sim-
ilarity achieve the highest performance.
Concept categorization: Almuhareb (2006) pro-
posed a set of 402 nouns to be categorized into
21 classes of both concrete (animals, fruit. . . ) and
abstract (feelings, times. . . ) concepts. Our results
on this clustering task are given in Table 1 (line
2). The difference between SVD and pure-vector
models is negligible and they both obtain the best
performance in terms of both cluster entropy (not
shown in the table) and purity. Both models? per-
formances are comparable with the previously re-
ported studies, and above that of random walks.
Semantic priming: The next dataset comes
from Hodgson (1991) and it is of interest since
it requires capturing different forms of seman-
tic relatedness between prime-target pairs: syn-
onyms (synonym), coordinates (coord), antonyms
(antonym), free association pairs (conass), super-
and subordinate pairs (supersub) and phrasal as-
sociates (phrasacc). Following previous simula-
tions of this data-set (Pad?o and Lapata, 2007), we
measure the similarity of each related target-prime
pair, and we compare it to the average similar-
ity of the target to all the other primes instanti-
ating the same relation, treating the latter quan-
tity as our surrogate of an unrelated target-prime
pair. We report results in terms of differences be-
tween unrelated and related pairs, normalized to
t-scores, marking significance according to two-
tailed paired t-tests for the relevant degrees of free-
dom. Even though the SVD-based and pure-vector
models are among the top achievers in general, we
see that in different tasks different random walk
models achieve comparable or even better perfor-
mances. In particular, for phrasal associates and
conceptual associates, the best results are obtained
by random walks based on direct measures.
3.1 Robustness against data sparseness
So far, we reported only the results obtained on
the full graph. However, in order to see the re-
sponse of the models to using smaller corpora
52
we ran another set of experiments on artificially
down-sampled graphs as explained above. In this
case, we are not interested in the absolute perfor-
mance of the models per se but the relative per-
formance. Thus, for ease of comparison we fixed
each model?s performance on the full graph to 1
for each task and linearly scaled its performance
on smaller graphs. For example saying that the
SVD-based model achieves a score of 0.911 on
10% graph for the Rubenstein and Goodenough
dataset means that the ratio of the performance
of SVD-based model on 10% graph to the per-
formance of the same model on the full graph is
0.911. The results are given in Table 2, where the
only random walk model we report is dot 2, i.e., a
2-step random walk coupled with the dot product-
based indirect measure. This is by far the random
walk model most robust to downsampling. In the
10% graph, we see that on all tasks but one, dot 2
is the model least affected by the data reduction.
On the contrary, down-sampling has a positive ef-
fect on this model because on 6 tasks, it actually
performs better than it does on the full graph! The
same behavior is also observed on the 1% graph
- as an example, for phrasal associates relations,
dot 2 performance increases by a factor of around
1.2 when we use one hundredth of the graph in-
stead of the full one. For the smallest graph we
used, 0.1%, still dot 2 provides the highest relative
performance in 5 out of the 8 tasks.
4 Conclusion
We compared graph-based random walk models
and vector models. For this purpose, we showed
how corpus co-occurrences could be represented
both as a graph and a vector, and we identified
two different ways to calculate relatedness based
on the outcomes of random walks, by direct and
indirect measures. The experiments carried out
on 8 different tasks by using the full graph re-
vealed that SVD-based model performs very well
across all types of semantic relatedness. How-
ever, there is also evidence that -depending on
the particular relation- some random walk models
can achieve results as good as or even better than
those of SVD-based models. Our second ques-
tion was whether the random walk models would
be able to simulate the smoothing effects obtained
by SVD. While answering this question, we also
carried out a systematic comparison of plain and
SVD-based models on different tasks with differ-
ent sizes of data. One interesting result is that an
SVD-based model is not necessarily more robust
to data sparseness than the plain vector model.
The more interesting result is that a 2-step ran-
dom walk model, based on indirect measures with
dot product, consistently outperforms both SVD-
based and plain vector models in terms of relative
performance, thus it is able to achieve compara-
ble results on very small datasets. Actually, the
improvement on absolute performance measures
of this random walk by making the dataset even
smaller calls for future research.
References
A. Almuhareb. 2006. Attributes in lexical acquisition.
Dissertation, University of Essex.
M. Baroni and A. Lenci. 2009. One distributional
memory, many semantic spaces. In Proceedings of
GEMS, Athens, Greece.
Hsin-Hsi Chen, Ming-Shun Lin, and Yu-Chuan Wei.
2006. Novel association measures using web search
with double checking. In Proceedings of ACL, pages
1009?16.
J. Hodgson. 1991. Informational constraints on pre-
lexical priming. Language and Cognitive Processes,
6:169?205.
Einat Minkov and William W. Cohen. 2008. Learn-
ing graph walk based similarity measures for parsed
text. In Proceedings of EMNLP?08.
S. Pad?o and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::similarity: Measuring the relatedness of
concepts. In Proceedings of NAACL, pages 38?41.
Klaus Rothenh?ausler and Hinrich Sch?utze. 2009.
Unsupervised classification with dependency based
word spaces. In Proceedings of GEMS, pages 17?
24.
H. Rubenstein and J.B. Goodenough. 1965. Contex-
tual correlates of synonymy. Communications of the
ACM, 8(10):627?633.
M. Sahlgren. 2006. The Word-Space Model. Disserta-
tion, Stockholm University.
H. Sch?utze. 1997. Ambiguity Resolution in Natural
Language Learning. CSLI, Stanford.
Dominic Widdows and Beate Dorow. 2002. A
graph model for unsupervised lexical acquisition.
In 19th International Conference on Computational
Linguistics, pages 1093?1099.
53
Proceedings of the 8th International Conference on Computational Semantics, pages 104?115,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Supporting inferences in semantic space:
representing words as regions
Katrin Erk
University of Texas at Austin
katrin.erk@mail.utexas.edu
Abstract
Semantic space models represent the meaning of a word as a vector
in high-dimensional space. They offer a framework in which the mean-
ing representation of a word can be computed from its context, but the
question remains how they support inferences. While there has been
some work on paraphrase-based inferences in semantic space, it is not
clear how semantic space models would support inferences involving
hyponymy, like horse ran ? animal moved. In this paper, we first dis-
cuss what a point in semantic space stands for, contrasting semantic
space with Ga?rdenforsian conceptual space. Building on this, we pro-
pose an extension of the semantic space representation from a point
to a region. We present a model for learning a region representation
for word meaning in semantic space, based on the fact that points at
close distance tend to represent similar meanings. We show that this
model can be used to predict, with high precision, when a hyponymy-
based inference rule is applicable. Moving beyond paraphrase-based
and hyponymy-based inference rules, we last discuss in what way se-
mantic space models can support inferences.
1 Introduction
Semantic space models represent the meaning of a word as a vector in a high-
dimensional space, where the dimensions stand for contexts in which the
word occurs [14, 10, 21, 20]. They have been used successfully in NLP [15],
as well as in psychology [10, 13, 16]. Semantic space models, which are
induced automatically from corpus data, can be used to characterize the
meaning of an occurrence of a word in a specific sentence [17, 3] without
recourse to dictionary senses. This is interesting especially in the light of
the recent debate about the problems of dictionary senses [9, 7]. However,
104
??
?
?
horse
dim
1
29
dim
2
0
. . .
?
?
?
?
v
?
?
?
?
animal
dim
1
1003
dim
2
5
. . .
?
?
?
? horse
animal
Figure 1: Modeling hyponymy in semantic space: as subsumption between
feature structures (left) or as subregion inclusion (right)
it makes sense to characterize the meaning of words through semantic space
representations only if these representations allow for inferences.
(1) Google acquired YouTube =? Google bought YouTube
(2) A horse ran =? An animal moved
Ex. (1) is an example of an inference involving a paraphrase: acquire can be
substituted for buy in some contexts, but not all, for example not in con-
texts involving acquiring skills. Ex. (2) is an inference based on hyponymy :
run implies move in some contexts, but not all, for example not in the con-
text computer runs. In this paper, we concentrate on these two important
types of inferences, but return to the broader question of how inferences are
supported by semantic space models towards the end.
Semantic space models support paraphrase inferences: Lists of potential
paraphrases (for example buy and gain for acquire) and the applicability
of a paraphrase rule in context [17] can be read off semantic space repre-
sentations. The same cannot be said for hyponymy-based inferences. The
most obvious conceptualization of hyponymy in semantic space, illustrated
in Fig. 1 (left), is to view the vectors as feature structures, and hyponymy as
subsumption. However, it seems unlikely that horse would occur in a subset
of the contexts in which animal is found (though see Cimiano et al[1]).
There is another possible conceptualization of hyponymy in semantic space,
illustrated in Fig. 1 (right): If the representation of a word?s meaning in
semantic space were a region rather than a point, hyponymy could be mod-
eled as the sub-region relation. This is also the model that Ga?rdenfors [5]
proposes within his framework of conceptual spaces, however it is not
clear that the notion of a point in space is the same in conceptual space as
in semantic space. To better contrast the two frameworks, we will refer to
semantic space as co-occurrence space in the rest of this paper.
This paper makes two contributions. First, it discusses the notion of
a point in space in both conceptual and co-occurrence space, arguing that
they are fundamentally different, with points in co-occurrence space not
representing potential entities but mixtures of uses. Second, it introduces
a computational model for extending the representation of word meaning
105
in co-occurrence space from a point to a region. In doing so, it makes use
of the property that points in co-occurrence space that are close together
represent similar meanings.
We do not assume that the subregion relation will hold between induced
hyponym and hypernym representations, no more than that the subsump-
tion relation would hold between them. Instead, we will argue that the
region representations make it possible to encode hyponymy information
collected from another source, for example WordNet [4] or a hyponymy in-
duction scheme [8, 25].
Plan of the paper. Sec. 2 gives a short overview of existing geometric mod-
els of meaning. In Sec. 3 we discuss the significance of a point in conceptual
space and in co-occurrence space, finding that the two frameworks differ
fundamentally in this respect, but that we can still represent word mean-
ings as regions in co-occurrence space. Building on this, Sec. 4 introduces
a region model of word meaning in co-occurrence space that can be learned
automatically from corpus data. Sec. 5 reports on experiments testing the
model on the task of predicting hyponymy relations between occurrences of
words. Sec. 6 looks at both paraphrase-based and hyponymy-based infer-
ences to see how their applicability can be tested in co-occurrence space and
how this generalizes to other types of inference rules. Sec. 7 concludes.
2 Related work
In this section we give a short overview of three types of geometric models of
meaning: co-occurrence space models, conceptual space models, and models
of human concept representation.
Co-occurrence space models. Co-occurrence space models (vector space
models) represent the meaning of a word as a vector in high-dimensional
space [14, 10, 21, 20]. In the simplest case, the vector for a target word
w is constructed by counting how often each other word co-occurs with w
in a given corpus, in a context window of n words around the occurrences
of w. Each potentially co-occurring word d then becomes a dimension, and
the co-occurrence counts of w with d become the value of w?s vector on
dimension d. As an example, Table 1 shows some co-occurrence counts for
the target words letter and surprise in Austen?s Pride and Prejudice. There
are many variations on co-occurrence space representations, for example
using syntactic context rather than word co-occurrence [20]. The most im-
portant property of co-occurrence space models is that similarity between
target words can be estimated as distance in space, using measures such as
106
admirer all allow almost am and angry . . .
letter 1 8 1 2 2 56 1 . . .
surprise 0 7 0 0 4 22 0 . . .
Table 1: Some co-occurrence counts for letter, surprise in Austen?s Pride
and Prejudice
Euclidean distance or cosine similarity. Co-occurrence space models have
been used both in NLP [15, 25, 22] and in psychology [10, 13, 16]. They
have mostly been used to represent the meaning of a word by summing
over all its occurrences. We will call these vectors summation vectors.
A few studies have developed models that represent the meaning of an oc-
currence of a word in a specific sentence [10, 22, 17, 3]. The occurrence
vector for a word in a specific sentence is typically computed by combining
its summation vector with that of a single context word in the same sen-
tence, for example the direct object of a target verb. For Ex. (1) this would
mean computing the meaning representation of this occurrence of acquire
by combining the summation vectors of acquire and YouTube. The simplest
mechanism that has been used for the vector combination is vector addi-
tion. Models for occurrence meaning have typically been tested on a task
of judging paraphrase appropriateness: A model is given an occurrence, for
example acquire in Ex. (1), and a potential paraphrase, for example buy.
The model then estimates the appropriateness of the paraphrase in the cur-
rent context as the similarity of the occurrence vector of acquire with the
summation vector of buy.
Conceptual space. Ga?rdenfors [5] proposes representing concepts as re-
gions in a conceptual space, whose quality dimensions correspond to inter-
pretable features. In the simplest case, those are types of sensory perception
(color, temperature). Ga?rdenfors defines natural properties to be properties
that occupy convex regions in conceptual space, proposing that all proper-
ties used in human cognition are natural. Natural properties offer a solution
to the problem of induction if ?undesirable? properties such as grue [6] do
not form convex regions.
Human concept representation. In psychology, feature vector based
models of human concept representation (e.g. [24, 19]) are used to model
categorization. Since many experiments on human concept representation
have been performed using verbal cues, these models represent aspects of
word meaning [18], possibly along with other types of knowledge. Nosofsky?s
Generalized Context Model (GCM) [19] models the probability of catego-
107
rizing an exemplar ~e as a member of a concept C as
P (C|~e) =
?
~??C
w
~?
sim(~?,~e)
?
concept C
?
?
~??C
?
w
~?
sim(~?,~e)
where the concept C is a set of remembered exemplars, w
~?
is an exemplar
weight, and the similarity sim(~?,~e) between ~? and ~e is defined as sim(~?,~e) =
exp(z ?
?
dimension i
w
i
(?
i
? e
i
)
2
). z is a general sensitivity parameter, w
i
is a
weight for dimension i, and ?
i
, e
i
are the values of ~? and ~e on dimension i.
3 Points in co-occurrence space
Since we need to be clear about the entities about which we perform in-
ferences, it is important to understand what a point in conceptual and co-
occurrence space stands for. This is the topic of this section.
In conceptual space, a point is a potential entity, quality, or event. In
the region occupied by the concept yellow, each point denotes a hue of
yellow. In co-occurrence space, on the other hand, the representation of
yellow is a point, the summation vector. corpus occurrences of yellow, yellow
door as well as yellow pages. The summation vector is thus not a potential
percept, but a sum or mixture of uses. As vectors are computed entirely
from observed contexts, summation vectors can be computed for words like
yellow just as well as tomorrow or idea. Furthermore, the summation vector
is a representation of the word?s meaning, rather than a meaning.
An occurrence vector is also a point in co-occurrence space. It, too, does
not represent a potential entity. It is computed from two summation vec-
tors: Summation vectors are primary, and occurrence vectors are derived,
in all current co-occurrence space approaches. Computing the meaning rep-
resentation of acquire in the context of YouTube by combining
~
acquire and
~
Y ouTube amounts to constructing a pseudo-summation vector for a word
acquire-in-the-context-of-YouTube, making pseudo-counts for the dimensions
based on the context counts of acquire and YouTube. If the occurrence vector
is computed through addition, as
~
acquire+
~
Y ouTube, we are basically taking
the contexts in which acquire-in-the-context-of-YouTube has been observed
to be the union of the contexts of acquire and YouTube. So both summation
and occurrence vectors are, in fact, summation vectors representing mixtures
of uses. They do not describe potential entities, like points in conceptual
space, but are representations of potential meanings of words.
The regions in co-occurrence space we want to identify will thus not be
regions of similar entities, but regions of similar mixtures of uses. Encoding
108
external hyponymy information in a co-occurrence space, as we will do be-
low, thus means stating that any mixture of uses in which the hyponym can
occur is also a mixture of uses where the hypernym could be found. This is
plausible for pairs like horse and animal, though it stretches corpus reality
somewhat for other hypernyms of horse like vertebrate.
4 A model for regions in co-occurrence space
In this section, we develop a model for automatically inducing region repre-
sentations for word meaning in co-occurrence space. Our aim is to induce a
region representation from existing summation vectors and occurrence vec-
tors. There is existing work on inducing regions from points in a geometric
model, in psychological models of human concept representation (Sec. 2).
These models use either a single point (prototype models)
1
or a set of points
(exemplar models), and induce regions of points that are sufficiently close
to the prototype or exemplars. They share two central properties, both of
which can be observed in the GCM similarity formula (Sec. 2): (P1) Di-
mensions differ in how strongly they influence classification. (P2) Similarity
decreases exponentially with distance (Shepard?s law, [23]). We adopt (P1)
and (P2) for our model in co-occurrence space. As co-occurrence space can
model conceptual phenomena like lexical priming [13], it is reasonable to
assume that its notion of similarity matches that of conceptual models. We
construct a prototype-style model, with the summation vector as the proto-
type, using the following additional assumptions: (P3) The representation
of a word?s meaning in co-occurrence space is a contiguous region surround-
ing the word?s summation vector. (P4) The region includes the occurrence
vectors of the word. Property (P4) builds on the argument from Sec. 3 that
occurrence vectors are pseudo-summation vectors. It also matches previous
work on judging paraphrase appropriateness (Sec. 2), since those studies
successfully rely on the assumption that occurrence vectors will be close to
summation vectors that represent similar meanings.
We define a model for region representations of word meaning that is
based on distance from the summation vector, and that uses the occurrence
vectors to determine the distance from the summation vector at which points
should still be considered as part of the region. For a given target word w,
we construct a log-linear model that estimates the probability P (in|~x) that
a point ~x is inside the meaning region of w, as follows:
1
Some prototype models use a prototype that has a weighted list of possible values for
each feature.
109
P (in|~x) =
1
Z
exp(
?
i
?
in
i
f
i
(~x)) (1)
where the f
i
are features that characterize the point ~x, and the ?
in
i
are
weights identifying the importance of the different features for the class
in. Z is a normalizing factor ensuring that the result is a probability. Let
~w = ?w
1
, . . . , w
n
? be the summation vector for the word w, in a space of
n dimensions. Then we define the features f
i
, for 1 ? i ? n, to encode
distance from ~w in each dimension, with
f
i
(~x) = (w
i
? x
i
)
2
(2)
This log-linear model has property (P1) through the weights ?
in
i
. It has
property (P2) through the exponential relation between the estimated prob-
ability and the distances f
i
. We will use occurrence vectors of w (as positive
data) and occurrence vectors of other words (as negative data) to estimate
the ?
i
during training, thus calibrating the weights by the distances between
the summation vector and known members of the region. In the current pa-
per, we will consider only regions with sharp boundaries, which we obtain by
placing a threshold of 0.5 on the probability P (in|~x). However, we consider
it important that this model can also be used to represent regions with soft
boundaries by using P (in|~x) without a threshold. It may thus be able to
model borderline uses of a word, and unclear boundaries between senses [2].
5 Experiments on hyponymy
In this section, we report on experiments on hyponymy in co-occurrence
space. We test whether different co-occurrence space models can predict,
given meaning representations (summation vectors, occurrence vectors, or
regions) of two words, whether one of the two words is a hypernym of the
other. In all tests, the models do not see the words, just the co-occurrence
space representations.
Experimental setting. We used a Minipar [11] dependency parse of the
British National Corpus (BNC) as the source of data for all experiments be-
low. The written portion of the BNC was split at random into two halves:
a training half and a test half. We used WordNet 3.0 as the ?ground truth?
against which to evaluate models. We work with two main sets of lemmas:
first, the set of monosemous verbs according to WordNet (we refer to this
set as Mon), and second, the set of hypernyms of the verbs in Mon (we
call this set Hyp). We concentrate on monosemous words in the current
110
paper since they will allow us to evaluate property (P3) most directly. Since
the model from Sec. 4 needs substantive amounts of occurrence vectors for
training, we restricted both sets Mon and Hyp to verbs that occur with at
least 50 different direct objects in the training half of the BNC. The direct
objects, in turn, were restricted to those that occurred no more than 6,500
and no less than 270 times with verbs in the BNC, to remove both uninfor-
mative and sparse objects. (The boundaries were determined heuristically
by inspection of the direct objects for this pilot study.) This resulted in a set
Mon consisting of 120 verbs, and Hyp consisting of 430 verbs. Summation
vectors for all words were computed with the dv package
2
from the training
half of the BNC, using vectors of 500 dimensions with raw co-occurrence
counts as dimension values.
Experiment 1: Subsumption. Above, we have hypothesized that co-
occurrence space representations of hyponyms and hypernyms, in the form
in which they are induced from corpus data, cannot in general be assumed to
be in either a subsumption or a subregion relation. We test this hypothesis,
starting with subsumption. We define subsumption as ~x v ~y ?? ?i(y
i
>
0? x
i
> 0). Now, any given verb in Mon will be the hyponym of some verbs
in Hyp and unrelated to others. So we test, for each summation vector ~v
1
of
a verb in Mon and summation vector ~v
2
of a verb in Hyp, whether ~v
1
v ~v
2
.
The result is that Mon verbs subsume 5% of the Hyp verbs of which
they are hyponyms, and 1% of the Hyp verbs that are unrelated.
We conclude that subsumption between summation vectors in co-occurrence
space is not a reliable indicator of the hyponymy relation between words.
Experiment 2: Subregion relation. Next, we test whether, when we
represent a Hyp-verb as a region in co-occurrence space, occurrences of its
Mon-hyponyms fall inside that region, and occurrences of non-hypernyms
are outside. First, we compute occurrence vectors for each Hyp or Mon verb
v as described in Sec. 2: Given an occurrence of a verb v, we compute its
occurrence vector by combining the summation vector of v with the sum-
mation vector of the direct object of v in the given sentence
3
. We combine
two summation vectors by computing their average. In this experiment, we
use occurrences from both halves of the BNC. With those summation and
occurrence vectors in hand, we then learn a region representation for each
Hyp verb using the model from Sec. 4. We implemented the region model
using the OpenNLP maxent package
4
. Last, we test, for each Mon verb
2
http://www.nlpado.de/
~
sebastian/dv.html
3
Occurrences without a direct object were not used in the experiments.
4
http://maxent.sourceforge.net/
111
occurrence vector and each Hyp region, whether the occurrence vector is
classified as being inside the region. The result is that the region models
classified zero hyponym occurrences as being inside, resulting in precision
and recall of 0.0. These results show clearly that our earlier hypothe-
sis was correct: The co-occurrence representations that we have induced
from corpus data do not lend themselves to reading off hyponymy relations
through either subsumption or the subregion relation.
Experiment 3: Encoding hyponymy. These findings do not mean that
it is impossible to test the applicability of hyponymy-based inferences in
co-occurrence space. If we cannot induce hyponymy relations from existing
vector representations, we may still be able to encode hyponymy information
from a separate source such as WordNet. Note that this would be difficult
in a words-as-points representation: The only possibility there would be
to modify summation vectors. With a words-as-regions representation, we
can keep the summation vectors constant and modify the regions. Our aim
in this experiment is to produce a region representation for a Hyp verb v
such that occurrence vectors of v?s hyponyms will fall into the region. We
use only direct hypernyms of Mon verbs in this experiment, a 273-verb
subset of Hyp we call DHyp. For each DHyp verb v, we learn a region
representation centered on v?s summation vector, using as positive training
data all occurrences of v and v?s direct hyponyms in the training half of
the BNC. (Negative training data are occurrences of other DHyp verbs and
their children.) We then test, for each occurrence of a Mon verb in the
test half of the BNC that does not occur in the training half with the same
direct object, whether it is classified as being inside v?s region. The result
of this experiment is a precision of 95.2, recall of 43.4, and F-score of
59.6 (against a random baseline of prec=11.0, rec=50.2, and F=18.0). This
shows that it is possible to encode hyponymy information in a co-occurrence
space representation: The region model identifies hyponym occurrences with
very high precision. If anything, the region is too narrow, classifying many
actual hyponyms as negatives.
6 Inference in co-occurrence space
In this section we take a step back to ask what it means for co-occurrence
space to support inferences, taking the inferences in Ex. (1) and (2) as an
example. The inference in Ex. (1), which involves a paraphrase, is supported
in two ways: (I1) Paraphrase candidates ? words that may be substituted
for acquire in some contexts ? can be read off a co-occurrence space represen-
112
tation [12]. They are the words whose summation vectors are closest to the
summation vector of acquire in space. In this way, co-occurrence space can
be used for the construction of context-dependent paraphrase rules. (I2)
Given an occurrence ~o of acquire, the appropriateness of applying the para-
phrase rule substituting buy for acquire is estimated based on the distance
between ~o and the summation vector
~
buy of buy [17, 3]. This can be used
to select the single best paraphrase candidate for the given occurrence of
acquire, or to produce a ranking of all paraphrase candidates [3].
Concerning the hyponymy-based inference in Ex. (2), we have established
in Experiments 1 and 2 (Sec. 5) that it is at least not straightforward to
construct hyponymy-based rules from co-occurrence space representations
in analogy to (I1). However, (H2) Experiment 3 has shown that, given a
set of attested occurrences of hyponyms of move, we can construct a region
representation for move in co-occurrence space that can be used to test
applicability of hyponymy-based rules: The appropriateness of applying the
hyponymy-based rule substituting move for this specific occurrence of run
can be estimated based on whether the occurrence vector of run is located
inside the move region.
In both (I2) and (H2), an inference rule is ?attached? to a point or a
region in co-occurrence space: the summation vector of buy in (I2), the
region representation of move in (H2). The inference rule is considered ap-
plicable to an occurrence if its occurrence vector is close enough in space to
the attachment point or inside the attachment region. Co-occurrence space
thus offers a natural way of determining the applicability of a (paraphrase
or hyponymy-based) inference rule to a particular occurrence, via distance
to the attachment point or inclusion in the attachment region. Applicabil-
ity can be treated as a yes/no decision, or it can be expressed through a
graded degree of confidence. In the case of attachment point, this degree
of confidence would simply be the similarity between occurrence vector and
attachment point. Concerning attachment regions, note that the model of
Sec. 4 actually estimates a probability of region inclusion for a given point
in space. In this paper, we have placed a threshold of 0.5 on the probability
to derive hard judgments, but the probability can also be used directly as a
degree of confidence.
The general principle of using co-occurrence space representation to rate
inference rule applicability, and to do this by linking rules to attachment
points or regions, could maybe be used for other kinds of inference rules as
well. The prerequisite is, of course, that it must make sense to judge rule
applicability through a single attachment point or region.
113
7 Conclusion and outlook
In this paper, we have studied how semantic space representations support
inferences, focusing on hyponymy. To encode hyponymy through the sub-
region relation, we have considered word meaning representations through
regions in semantic space. We have argued that a point in semantic space
represents a mixture of uses, not a potential entity, and that the regions
in semantic space we want to identify are those that represent the same or
similar meanings. We have introduced a computational model that learns
region representations, and we have shown that this model can predict hy-
ponymy with high precision. Finally, we have suggested that semantic space
supports inferences by attaching inference rules to points or regions in space
and licensing rule application depending on distance in space. It is an open
question how far the idea of attachment points and attachment regions can
be extended beyond the paraphrase and hyponymy rules we have considered
here; this is the question we will consider next.
Acknowledgements. Many thanks to Manfred Pinkal, Jason Baldridge,
David Beaver, Graham Katz, Alexander Koller, and Ray Mooney for very
helpful discussions. (All errors are, of course, my own.)
References
[1] P. Cimiano, A. Hotho, and S. Staab. Learning concept hierarchies from text
corpora using formal concept anaylsis. Journal of Artificial Intelligence Re-
search, 24:305?339, 2005.
[2] K. Erk and S. Pado?. Towards a computational model of gradience in word
sense. In Proceedings of IWCS-7, Tilburg, The Netherlands, 2007.
[3] K. Erk and S. Pado. A structured vector space model for word meaning in
context. In Proceedings of EMNLP-08, Hawaii, 2008.
[4] C. Fellbaum, editor. WordNet: An electronic lexical database. MIT Press,
Cambridge, MA, 1998.
[5] P. Ga?rdenfors. Conceptual spaces. MIT press, Cambridge, MA, 2004.
[6] N. Goodman. Fact, Fiction, and Forecast. Harvard University Press, Camb-
dridge, MA, 1955.
[7] P. Hanks. Do word meanings exist? Computers and the Humanities, 34(1-
2):205?215(11), 2000.
[8] M. Hearst. Automatic acquisition of hyponyms from large text corpora. In
Proceedings of COLING 1992, Nantes, France, 1992.
114
[9] A. Kilgarriff. I don?t believe in word senses. Computers and the Humanities,
31(2):91?113, 1997.
[10] T. Landauer and S. Dumais. A solution to Platos problem: the latent seman-
tic analysis theory of acquisition, induction, and representation of knowledge.
Psychological Review, 104(2):211?240, 1997.
[11] D. Lin. Principle-based parsing without overgeneration. In Proceedings of
ACL?93, Columbus, Ohio, USA, 1993.
[12] D. Lin. Automatic retrieval and clustering of similar words. In COLING-
ACL98, Montreal, Canada, 1998.
[13] W. Lowe and S. McDonald. The direct route: Mediated priming in semantic
space. In Proceedings of the Cognitive Science Society, 2000.
[14] K. Lund and C. Burgess. Producing high-dimensional semantic spaces from
lexical co-occurrence. Behavior Research Methods, Instruments, and Comput-
ers, 28:203?208, 1996.
[15] C. D. Manning, P. Raghavan, and H. Schu?tze. Introduction to Information
Retrieval. Cambridge University Press, 2008.
[16] S. McDonald and M. Ramscar. Testing the distributional hypothesis: The
influence of context on judgements of semantic similarity. In Proceedings of
the Cognitive Science Society, 2001.
[17] J. Mitchell and M. Lapata. Vector-based models of semantic composition. In
Proceedings of ACL-08, Columbus, OH, 2008.
[18] G. L. Murphy. The Big Book of Concepts. MIT Press, 2002.
[19] R. M. Nosofsky. Attention, similarity, and the identification-categorization
relationship. Journal of Experimental Psychology: General, 115:39?57, 1986.
[20] S. Pado? and M. Lapata. Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199, 2007.
[21] M. Sahlgren and J. Karlgren. Automatic bilingual lexicon acquisition using
random indexing of parallel corpora. Journal of Natural Language Engineering,
Special Issue on Parallel Texts, 11(3), 2005.
[22] H. Schu?tze. Automatic word sense discrimination. Computational Linguistics,
24(1), 1998.
[23] R. Shepard. Towards a universal law of generalization for psychological science.
Science, 237(4820):1317?1323, 1987.
[24] E. E. Smith, D. Osherson, L. J. Rips, and M. Keane. Combining prototypes:
A selective modification model. Cognitive Science, 12(4):485?527, 1988.
[25] R. Snow, D. Jurafsky, and A. Y. Ng. Semantic taxonomy induction from
heterogenous evidence. In Proceedings of COLING/ACL?06, 2006.
115
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1025?1036, Dublin, Ireland, August 23-29 2014.
Inclusive yet Selective: Supervised Distributional Hypernymy Detection
Stephen Roller
?
, Katrin Erk
?
, Gemma Boleda
?
?
Department of Computer Science
?
Department of Linguistics
The University of Texas at Austin
roller@cs.utexas.edu, katrin.erk@mail.utexas.edu,
gemma.boleda@upf.edu
Abstract
We test the Distributional Inclusion Hypothesis, which states that hypernyms tend to occur in
a superset of contexts in which their hyponyms are found. We find that this hypothesis only
holds when it is applied to relevant dimensions. We propose a robust supervised approach that
achieves accuracies of .84 and .85 on two existing datasets and that can be interpreted as selecting
the dimensions that are relevant for distributional inclusion.
1 Introduction
One of the main criticisms of distributional models has been that they fail to distinguish between semantic
relations: Typical nearest neighbors of dog are words like cat, animal, puppy, tail, or owner, all obviously
related to dog, but through very different types of semantic relations. On these grounds, Murphy (2002)
argues that distributional models cannot be a valid model of conceptual representation. Distinguishing
semantic relations are also crucial for drawing inferences from distributional data, as different semantic
relations lead to different inference rules (Lenci, 2008). This is of practical import for tasks such as
Recognizing Textual Entailment or RTE (Geffet and Dagan, 2004).
For these reasons, research has in recent years started to attempt the detection of specific semantic
relationships, and current results suggest that distributional models can, in fact, distinguish between
semantic relations, given the right similarity measures (Weeds et al., 2004; Kotlerman et al., 2010; Lenci
and Benotto, 2012; Herbelot and Ganesalingam, 2013; Santus, 2013). Because of its relevance for RTE
and other tasks, much of this work has focused on hypernymy. Hypernymy is the semantic relation
between a superordinate term in a taxonomy (e.g. animal) and a subordinate term (e.g. dog).
Distributional approaches to date for detecting hypernymy, and the related but broader relation of
lexical entailment, have been unsupervised (except for Baroni et al. (2012)) and have mostly been based
on the Distributional Inclusion Hypothesis (Zhitomirsky-Geffet and Dagan, 2005; Zhitomirsky-Geffet
and Dagan, 2009), which states that more specific terms appear in a subset of the distributional contexts
in which more general terms appear. So, animal can occur in all the contexts in which dog can occur,
plus some contexts in which dog cannot ? for instance, rights can be a typical cooccurrence for animal
(e.g. ?animal rights?), but not so much for dog (e.g. #?dog rights?).
This paper takes a closer look at the Distributional Inclusion Hypothesis for hypernymy detection. We
show that the current best unsupervised approach is brittle in that their performance depends on the space
they are applied to. This raises the question of whether the Distributional Inclusion Hypothesis is correct,
and if so, under what circumstances it holds. We use a simple supervised approach to relation detection
that has good performance (accuracy .84 on BLESS, .85 on the lexical entailment dataset of Baroni et
al. (2012)) and works well across different spaces.
1
Furthermore, we show that it can be interpreted
as selecting dimensions for which the Distributional Inclusion Hypothesis does hold. So, our answer is
to propose the Selective Distributional Inclusion Hypothesis: The Distributional Inclusion Hypothesis
holds, but only for relevant dimensions.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Code and data are available at http://stephenroller.com/research/coling14.
1025
2 Background
Distributional models. Distributional models represent a word through the contexts in which it has
been observed, usually in the form of a vector representation (Turney and Pantel, 2010). A target word
is represented as a vector in a high-dimensional space in which the dimensions are context items (for
example, other words) and the coordinates of the vector indicate the target?s degree of association with
each context item. In this paper, we also use dimensionality reduced spaces in which dimensions do not
stand for individual context items anymore.
Pattern-based approaches to inducing semantic relations. Early work on automatically inducing se-
mantic relations between words, starting with Hearst (1992), uses textual patterns. For example, ?[NP
1
]
and other [NP
2
]? implies that NP
2
is a hypernym of NP
1
. Pattern-based approaches have been applied
to meronymy (Berland and Charniak, 1999; Girju et al., 2003; Girju et al., 2006), synonymy (Lin et al.,
2003), co-hyponymy (Snow et al., 2005), hypernymy (Cimiano et al., 2005), and several relations be-
tween verbs (Chklovski and Pantel, 2004). Pantel and Pennachiotti (2006) generalize the idea to a wide
variety of relations. Turney (2006) uses vectors of patterns to determine similarity of semantic relations.
A task related to semantic relation induction is the extension of an existing taxonomy (Buitelaar et al.,
2005). Snow et al. (2006) do this by using hypernymy and co-hyponymy detectors.
Lexical entailment, hypernymy, and the Distributional Inclusion Hypothesis. Weeds et al. (2004)
introduce the notion of distributional generality, where v is distributionally more general than u if u
appears in a subset of the contexts in which v is found, and speculate that hypernyms (v) should be more
distributionally general than hyponyms (u). Zhitomirsky-Geffet and Dagan (2005; 2009) introduce the
term Distributional Inclusion Hypothesis for the idea that distributional generality encodes hypernymy
or the more loosely defined relation of lexical entailment.
Weeds and Weir (2003) measure distributional generality using a notion of precision (eq. 1). Here and
in all equations below, u is the narrower term, and v the more general one. Abusing notation, we write u
for both a word and its associated vector ?u
1
, . . . , u
n
?. Kotlerman et al. (2010) predict lexical entailment
with the balAPinc measure, a modification of the Average Precision (AP) measure (eq. 2). The general
notion is that scores should increase with the number of dimensions of v that u shares, and also give more
weight to the highly ranked dimensions (i.e. largest magnitude) of the narrower term u. This is captured
in APinc by computing precision P (r) at every rank r among u?s dimensions ? where precision is the
fraction of dimensions shared with v ?, and weighting by the rank of the same dimension in the broader
term, rel
?
(v, r, u). The final measure, balAPinc, smooths using the LIN similarity measure (Lin, 1998).
(We only sketch this measure here due to its complexity; details are given in Kotlerman et al. (2010).)
1(x) =
{
1 if x > 0;
0 otherwise
WeedsPrec(u, v) =
?
n
i=1
u
i
? 1(v
i
)
?
n
i=1
u
i
(1)
APinc(u, v) =
?
|1(u)|
r=1
P (r) ? rel
?
(v, r, u))
|1(u)|
(2)
balAPinc(u, v) =
?
APinc(u, v) ? LIN(u, v)
The ClarkeDE measure (Clarke, 2009) computes degree of entailment as the degree to which the nar-
rower term u has lower values than v across all dimensions (eq. 3). Lenci and Benotto (2012) introduce
the invCL measure, which uses ClarkeDE to measure both distributional inclusion of u in v and distri-
butional non-inclusion of v in u (eq. 4). While all other measures interpret the Distributional Inclusion
Hypothesis as the degree to which a ? relation holds, Lenci and Benotto test the degree to which proper
inclusion ( holds. They consider not only the degree to which the contexts of the narrower terms are
included in the contexts of the wider term, but also determine the degree to which the wider term has
contexts that the narrower term does not have.
1026
CL(u, v) =
?
n
i=1
min(u
i
, v
i
)
?
n
i=1
u
i
(3)
invCL(u, v) =
?
CL(u, v) ? (1? CL(v, u)) (4)
Like Lenci and Benotto, we focus on the stricter hypernymy relation, rather than lexical entailment.
We believe that the different relations that make up lexical entailment have different distributional indi-
cations and that, for that reason, it will be easier to detect the relations separately than together.
Baroni et al. (2012) proposes a supervised approach to hypernymy detection that represents two words
as the concatenation of their vectors. They also mention in passing another supervised approach that
represents two words as the component-wise difference of their vectors. These are broadly the two
approaches that we test, though we introduce significant modifications.
3 Data
3.1 Distributional Vector Spaces
We use three standard types of distributional spaces.
U+W2: This space is based on a concatenation of the Gigaword, BNC, English Wackypedia and
ukWaC corpora (Baroni et al., 2009). The corpora are POS-tagged and lemmatized. We keep only
content words (nouns, proper nouns, adjectives and verbs) with a corpus frequency of 500 or larger. The
resulting U+ corpus has roughly 133K word types and 2.8B word tokens. We created a vector space by
counting co-occurrences of these word types within a window of two words on the left and the right,
using the top 20k most frequent content words as dimensions. The space was transformed using Positive
Pointwise Mutual Information (PPMI).
U+Sent: The U+Sent space is constructed the same way as U+W2, but uses full sentence contexts
instead of 2-word windows.
TypeDM: This space is extracted from the TypeDM tensors (Baroni and Lenci, 2011). TypeDM con-
tains a list of weighted tuples, ??w
1
, l, w
2
?, ??, where w
1
and w
2
are content words, l is a corpus-derived
syntagmatic relationship between the words, and ? is a weight estimating saliency of the relationship. We
construct vectors for every unique w
1
using the set of ?l, w
2
? pairs as dimensions and corresponding ?
values as dimension weights. We select TypeDM for its excellent performance in previous comparisons
of distributional hypernymy measures (Lenci and Benotto, 2012).
Reduced Spaces: In some experiments, we use dimensionality reduced spaces. We reduce all three
spaces to 300 dimensions using Singular Value Decomposition. We use a subscript to denote reduced
spaces, e.g. U+W2
300
. When necessary, we use the term original dimensions to refer to the vector
dimensions from the original, non-reduced spaces (e.g. U+W2); the term latent dimensions refers to the
dimensions in the reduced spaces (e.g. U+W2
300
).
3.2 Evaluation Data Sets
BLESS: The BLESS data set (Baroni and Lenci, 2011) covers 200 concepts, or concrete and unambigu-
ous terms (divided into 17 different general concept classes, including vehicle and ground mammal), and
their relationships to other nouns, called relata. Example concepts include van and horse. Each concept
is related to several relata through different semantic relations. Following Lenci and Benotto (2012), we
focus on the four semantic relations where both concepts and relata are nouns, for a total 14K data points:
Hypernymy, denoting a superset relationship (e.g. animal-dog); Co-hyponymy, denoting words that share
a common hypernym (e.g. dog-cat); Meronymy, denoting a part-whole relationship (e.g. tail-dog); and
Random, denoting no relationship between the words (e.g. dog-computer).
1027
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
?1.5
?1.0
?0.5
0.0
0.5
1.0
1.5
Co?hyp Hyper Mero Random
z
U+W2, invCL
l
l
ll
l
l
l
l
l
ll
l
l
?1.5
?1.0
?0.5
0.0
0.5
1.0
1.5
Co?hyp Hyper Mero Random
z
U+Sent, invCL
ll
l
l
l
llll
l
l
l
l
l
l
ll
l
ll
l
ll
l
l
l
l
?1.5
?1.0
?0.5
0.0
0.5
1.0
1.5
Co?hyp Hyper Mero Random
z
TypeDM, invCL
Figure 1: Distributions of relata invCL scores for the U+W2, U+Sent, and TypeDM spaces for each of
the semantic relations, after per-concept z-normalization.
ENTAILMENT: (Baroni et al., 2012): The ENTAILMENT data set consists of 2,770 word pairs, bal-
anced between positive (house-building) and negative (leader-rider) examples of hypernymy, with 1376
unique hyponyms and 1016 unique hypernyms. The positive examples were generated by selecting direct
hypernym relationships from WordNet, the negative examples by randomly permuting the hypernyms of
the positive examples, and then manually checking correctness.
4 Distributional Inclusion across Spaces
We test several unsupervised distributional approaches to hypernymy detection from the literature, fo-
cusing on the underlying vector space representation as the main parameter that we vary. We use the
three spaces described in Section 3. We test four hypernymy detection approaches, all of them similarity
measures based on the Distributional Inclusion Hypothesis: WeedsPrec, balAPinc, ClarkeDE, and invCL.
Our baseline is the standard cosine measure. We evaluate on the BLESS dataset.
To evaluate on BLESS, we follow the evaluation scheme laid out in Baroni and Lenci (2011). Given a
space and similarity measure, we compute similarity for each concept and relatum. For each concept, we
select its nearest neighbors (according to the given similarity measure) in each of the four relations (CO-
HYP, HYPER, MERO, RANDOM), and transform the corresponding four similarities to z-scores. Across
all concepts, this yields four sets of z-normalized similarity scores, one for each relation. These four sets
describe the relative similarity of concepts to their nearest neighbors in different relations. Tukey?s Hon-
estly Significant Difference test is used for testing whether scores differ significantly between relations
(threshold: p < 0.05).
Figure 1 shows the distributions of z-scores for invCL for the four relations, with one graph for each
of the three spaces we consider. For this illustration, we focus on invCL because it shows the overall best
performance at identifying hypernymy. The rightmost plot in Figure 1 replicates the analysis of Lenci
and Benotto (2012), who used the TypeDM space. It confirms their finding that invCL gives significantly
higher values to hypernyms than co-hyponyms ? at least on this space. However, in the U+W2 and
U+Sent spaces (leftmost and middle plot), invCL clearly loses any ability to rank hypernyms the highest;
indeed, in both spaces, co-hyponymy and meronymy both have significantly higher z-scores than hyper-
nymy. Concerning the other measures, we found that they patterned with invCL. On TypeDM, ClarkeDE
and WeedsPrec had significantly higher nearest-neighbor values for hypernyms than co-hyponyms.
2
On
U+W2 and U+Sent, all measures ranked co-hyponyms significantly higher than hypernyms. With the
baseline measure, cosine, the similarity ratings for the CO-HYP relation are always the highest, no matter
the space, followed by HYPER, MERO, RANDOM in this order.
Following Kotlerman et al. (2010) and Lenci and Benotto (2012), we also report the performance of
the measures using Mean Average Precision (MAP). Average Precision (AP) is a measure often used in
2
balAPinc could not be evaluated on TypeDM due to computational issues.
1028
Measure CO-HYP HYPER MERO RANDOM
U+W2
cosine .68 .20 .27 .27
ClarkeDE .66 .19 .28 .28
invCL .60 .18 .31 .28
U+Sent
cosine .66 .18 .28 .28
ClarkeDE .66 .15 .29 .28
invCL .59 .13 .34 .29
TypeDM
cosine .78 .19 .20 .29
ClarkeDE .45 .35 .25 .32
invCL .38 .36 .27 .33
Table 1: Mean Average Precision for the unsupervised measures on three spaces.
the Information Retrieval community with a maximal AP score of 1 when all relevant documents (relata
with the right relationship, in our case) are ranked at the top. We compute AP on a per-concept basis and
report the mean over all 200 AP values. An advantage of MAP is that, while the BLESS analysis method
focuses on nearest neighbors, MAP evaluates the ranking of all relata. A disadvantage of MAP is that it
does not test the degree to which a similarity measure separates different semantic relations, like Tukey
does, so it may overstate the discriminative power of a particular measure. However, it provides a more
intuitive accuracy-like number compared to the BLESS evaluation.
Table 1 shows the Mean Average Precision values for cosine, ClarkeDE, and invCL on all three spaces.
We also computed WeedsPrec and balAPinc results, obtaining the same picture; we focus on ClarkeDE
and invCL because ClarkeDE is a component of invCL, and invCL is the current best measure. The results
corresponding to Lenci and Benotto?s are shown in the lowest part of Table 1, where we report numbers
for TypeDM. Like Lenci and Benotto, we find that unsupervised measures other than invCL rank co-
hyponyms the highest, and obtain relatively low results for hypernyms. For invCL in TypeDM, Lenci
and Benotto obtain 0.38 MAP for co-hyponyms and a slightly higher 0.40 for hypernyms, though they
do not report significance testing results. We obtain 0.38 for co-hyponyms and 0.36 for hypernyms, and
the difference is not significant.
3
Even though our results are slightly different from those in Lenci and
Benotto (2012), both our results and theirs point to at most a weak preference of invCL for hypernyms
over co-hyponyms. Moreover, in the U+W2 and U+Sent spaces we see that all three measures are very
poor at identifying hypernyms, and the co-hyponymy relation stubbornly persists as most relevant to all
three measures, by a large margin.
Our results thus constitute a puzzle for the Distributional Inclusion Hypothesis. It seems that there
must be some merit to the hypothesis: On one particular space, namely TypeDM, the nearest neighbors
in the hypernymy relation had higher similarity scores than any other relation by a significant margin.
This was true for all the hypernymy detectors we studied. But even on TypeDM, the MAP evaluation
showed at most a weak hypernymy signal, and when spaces other than TypeDM were used, the effect
vanished altogether. So how strong an indication for hypernymy can we expect from distributional
inclusion measures in general? We will return to this question below, where our answer will be: The
Distributional Inclusion Hypothesis seems to hold after all, but it needs to be applied to the right kind of
dimensions ? and a supervised approach can help in picking the right dimensions.
As the unsupervised approaches struggle to detect hypernymy and do not seem robust to changes in
standard space parameters, we think it is time to consider supervised approaches. In the next section, we
explore two simple supervised approaches that show good performance and are robust to changes in the
underlying space.
3
Wilcoxon signed-rank test.
1029
5 Supervised Hypernymy Detection
We use two simple, supervised models for predicting BLESS and ENTAILMENT relations. The first
(Concat) is a model previously proposed by Baroni et al. (2012). The second (Diff) takes up an idea
from a footnote in Baroni et al. (2012), but while that footnote stated that the approach in question did
not work, we find that, with a few modifications, it obtains the best performance ? and can be interpreted
as a supervised version of the Distributional Inclusion Hypothesis. Note that while we used unreduced
spaces in the previous section, we now use reduced spaces throughout (these are the spaces with the
300
subscript), in order not to have more features than data points.
5.1 Models, Features, and Method
Concat: We use a standard Support Vector Machine (SVM) classifier with a concatenation of vectors as
input features. SVMs are binary classifiers which learn the maximum margin hyperplane separating the
two classes. SVMs employ kernel functions to find the hyperplanes in higher dimensional spaces which
are nonlinear in the original space. As feature vectors for the classifier, we follow Baroni et al. (2012)
and use the concatenation of the latent dimension vectors representing words. For the ENTAILMENT
dataset, we use the concatenation of the hyponym latent vector and the hypernym latent vector for each
word pair as training features, and the entails/doesn?t entail annotations as binary targets. For BLESS,
we use the concatenation of the concept latent vector and the relatum latent vector as training features,
and the four relationship classes as targets. We choose the four-way task rather than a ?hypernymy vs.
other? classification because BLESS contains many more co-hyponymy and random than hypernymy
pairs, which would give a very high baseline in the two-way task. Additionally, the other relations in
BLESS, in particular meronymy, may be interesting in their own right.
Since SVMs are binary classifiers, we use SciKit-Learn?s default setting to train 6 pairwise-relation
one-vs-one classifiers which vote on the final answer. We use a polynomial kernel with a degree of 3
and a penalty term of C = 1.0, and all other hyperparameters are chosen using the SciKit-Learn default
values (Pedregosa et al., 2011). No hyperparameters are tuned in any experiment.
Diff: Our second classifier is a Logistic Regression (aka MaxEnt) model trained on difference vectors.
Logistic Regression is a statistical model for binary classification. It learns a linear hyperplane sepa-
rating the classes and estimates a probability for classes using a logistic function. We selected Logistic
Regression over other possible linear classifiers for its natural ability to give likelihood estimates, which
we believe will be useful in future work in an application of hypernymy classification to RTE.
As feature vectors, we use a Mikolov-inspired method of representing word pairs as the difference
vectors between the two words.
4
Baroni et al. (2012) suggested the use of difference vectors as input
to a classifier, but reported them as unsuccessful. We found difference vectors to be excellent features,
with three important modifications: a linear classifier is better than a nonlinear one; vectors must be
normalized to have a magnitude of 1 before taking the difference; and squared difference vectors must
also be included as features. So, we represent each word pair with latent vectors (u, v) as a two part
vector ?f ; g?, where
f
i
=
u
i
?u?
?
v
i
?v?
,
g
i
= f
2
i
.
These differences features
5
are analogous to a supervised distributional inclusion measure. The dif-
ference between two words on a particular dimension captures the degree of distributional inclusion on
that dimension. The primary distinction between the difference features and the unsupervised measures
is that the supervised classifier learns to weight the importance of different dimensions. The f features
encode directional aspects of distributional inclusion: that the hyponym contexts should be included in
4
After recent work using subtraction to represent analogy in certain neural-network spaces (Mikolov et al., 2013).
5
We also tried variations, such as not normalizing vectors and removing the difference squared vector, but found this setting
the best. We also tried the Diff features with an SVM and other nonlinear classifiers, but they performed worse.
1030
Data set BLESS ENTAILMENT
Baseline .46 .50
Classifier Concat Diff Concat Diff
U+W2
300
.76 .84 .81 .85
U+Sent
300
.73 .80 .78 .82
TypeDM
300
- .82 .65 .85
Table 2: Average accuracy of Concat and Diff on BLESS and ENTAILMENT using different spaces for
feature generation.
those of the hypernym (the weight learned is positive), and the hypernym contexts should not be in-
cluded in those of the hyponym (the weight learned is negative). So like invCL, this model uses a ?proper
subset? interpretation of the Distributional Inclusion Hypothesis, but only considers selected dimensions
(i.e. those that the model assigns nonzero weights).
The difference-squared features (g), on the other hand, typically identify dimensions that are not in-
dicative of hypernymy, by learning negative weights on them (more about this in Section 6). Thus, rather
than helping identify hypernyms, they help separate random relations from the rest.
We use a L1 regularizer with a strength of C = 1.0. All other hyperparameters are chosen using
the SciKit-Learn defaults. Since Diff is also a binary classifier, we use SciKit-Learn?s default setting of
training 4 one-vs-all classifiers for BLESS, with the most confident classifier choosing the final answer.
Method: For evaluation on BLESS, we hold out one concept and train on the remaining 199 concepts.
We also exclude from the training set any pair containing a relatum which appears in the test set. This
way, no word that appears in the test set has been seen in training. We report the average accuracy across
all concepts. We use the most frequent relation type (random) as our baseline. For the ENTAILMENT data
set, we hold out one hyponym and train on all remaining hyponyms. Again, we exclude from training
any pair containing a hypernym which appears in the test set. We report average accuracy across all
hyponyms. The data set is balanced, so the baseline is 0.5.
5.2 Results
Table 2 shows the performance of the two classifiers, Concat and Diff, on both the BLESS and ENTAIL-
MENT datasets, using three underlying spaces. We use the reduced versions of the three spaces, indicated
by the subscript
300
. Note that the Concat classifier could not converge using features from TypeDM
300
,
so we omit the result. With both methods, we obtain a high accuracy on the two datasets, with results
around .8 against baselines around .5. Our best result is .84 on BLESS and .85 on ENTAILMENT. More-
over, both approaches are in general robust to changes in space parameters (with TypeDM/Concat an
outlier). Still, the U+W2
300
space seems to be the best for this task: Its scores are significantly
6
higher
than the rest, except for TypeDM on ENTAILMENT, which achieves the same score as U+W2
300
. Diff
achieves significantly higher results than Concat.
When provided more information, Concat outperforms Diff. For instance, if cross-validation is done
over all pairs in BLESS in the U+W2
300
space, Concat achieves .98 accuracy, while Diff obtains .90.
However, in this setting the same words appear in the training and test sets (albeit in different pairs).
We take this to mean that Concat is memorizing, rather than learning the hypernymy relation. This
emphasizes the need for our stricter evaluation that prevents repetition between training and test sets.
Clearly, both classifiers do fairly well at predicting hypernymy relations between words, regardless
of space. Naturally, one should ask what are the classifiers capturing that the unsupervised measures
are missing? We propose that the supervised classifiers perform essentially the same operation as the
unsupervised measures, but are learning to determine the relevance of dimensions. In particular, Diff
is learning weights on vector difference features. This is equivalent to doing selective distributional
inclusion. In the next section, we test this Selective Distributional Inclusion Hypothesis.
6
Wilcoxon signed-rank test, p < .001.
1031
ll
l
l
l
l
ll
l
l
l
?1.5
?1.0
?0.5
0.0
0.5
1.0
1.5
Co?hyp Hyper Mero Random
z
U+W2 proj, cosine
ll
l
l
l
l
l
l
l
l
l
l
ll
ll
l
l
l
l
ll
l
l
ll
ll
l
l
?1.5
?1.0
?0.5
0.0
0.5
1.0
1.5
Co?hyp Hyper Mero Random
z
U+W2 proj, ClarkeDE
llll
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
?1.5
?1.0
?0.5
0.0
0.5
1.0
1.5
Co?hyp Hyper Mero Random
z
U+W2 proj, invCL
Figure 2: Distributions of relata scores across concepts using the cosine, ClarkeDE, and invCL measures
(after per-concept z-normalization). Here we use the selected dimensions of the U+W2
proj
space.
6 Selective Distributional Inclusion
In order to test how well our supervised model is capturing the notion of selective distributional inclusion,
we test each of the unsupervised measures on a smaller space, limited only to the dimensions preferred
by the classifier. We emphasize that we do not aim to show that our supervised method outperforms
unsupervised methods, but rather that the unsupervised methods benefit greatly from feature selection.
Additionally, we analyze which dimensions are selected by the classifier to facilitate understanding of
why these dimensions are important.
6.1 Experiment
We train the Diff classifier using the dimensionality-reduced U+W2
300
space with the same method we
use in Section 5. We take the classifier?s learned hyperplane separating hypernyms from other relations,
and project the hyperplane back into the original U+W2 space.
7
We select the 500 dimensions in the orig-
inal space that are most relevant according to the classifier weights, and test the unsupervised measures
on this new space, which we denote as U+W2
proj
.
8
The 500 most relevant dimensions are selected as follows: We select the 250 most negatively weighted
original dimensions using the difference features f . These are the features that have smaller values for
hyponyms (e.g. dog) than for hypernyms (e.g. animal), so they characterize hypernymy. We further select
the 250 most positively weighted original dimensions using the squared-differences features g. These
are the ones where a large difference does not indicate hypernymy.
Figure 2 shows the boxplots for the BLESS analysis: the distributions of nearest-neighbor similarity
scores for the four different semantic relations, for the measures cosine, ClarkeDE, and invCL. We see
that invCL now easily discriminates hypernymy from the other relations in the backprojected space. (The
difference of HYPER and CO-HYP is significant.) This is even though the space is based on U+W2, where
invCL failed to rate hypernyms higher than co-hypernyms in Section 4. Unsurprisingly, cosine, which
does not measure distributional inclusion, still prefers CO-HYP.
Table 3 shows the MAP scores for three of the measures in the new U+W2
proj
space. (The results
for balAPinc and WeedsPrec are slightly worse than ClarkeDE.) All measures except for cosine assign
higher scores to hypernyms than they did in the original space (compare to U+W2 part of Table 1). But
it is only invCL that ranks hypernyms significantly higher than co-hyponyms.
9
7
Ideally we would train on the original space to inspect the relevant dimensions. However, there are more dimensions than
examples, so we train on the SVD space and backproject.
8
Note that U+W2
proj
varies slightly from concept to concept, since the hyperplane is learned on a per-concept basis. It is
important that we use the linear Diff classifier for this reverse-projection procedure, as the separating hyperplane must be linear
in order to complete the projection. In particular, the hyperplane in the Concat classifier cannot be easily backprojected, since
it exists in a higher dimensional space than the projection matrix. Furthermore, it is important that we use a classifier trained
using the difference features because of its analogy to the Distributional Inclusion Hypothesis.
9
Wilcoxon signed-rank test, p < .001. To check that the measures are being improved by the dimension selection and not
1032
Measure CO-HYP HYPER MERO RANDOM
U+W2
proj
cosine .69 .20 .24 .28
ClarkeDE .55 .39 .24 .29
invCL .42 .58 .24 .29
Table 3: Mean Average Precision for the unsupervised measures after selecting the top dimensions from
a supervised model.
For this experiment, we train on all of BLESS except for one concept and then evaluate the unsuper-
vised models on the held-out concept ? that is a setting that could, in principle, be used as a hypernymy
detector. If we instead train the supervised model on all of BLESS to determine an upper bound of how
well dimension selection can do on this dataset, MAP for invCL rises to .67.
Overall, these experiments provide strong evidence for the Selective Distributional Inclusion Hypoth-
esis: The Distributional Inclusion Hypothesis holds, but only for relevant dimensions. In addition, hy-
pernymy detectors need to test for ?proper inclusion? of distributional contexts in order to really find
hypernyms.
Analysis of Selected Dimensions. We examine the 500 dimensions selected by the above procedure,
in order to see what the classifier is learning. As this is for analysis only, the dimensions were selected
by training on all data.
Recall that the difference-squared g features can be interpreted as dimensions that the classifier deems
not indicative of hypernymy. 200 out of the 250 most relevant dimensions by g are Computer Science
related terms like software, configure, or Linux. Since ukWaC, the largest corpus we use, is web-based,
it makes sense that it has many CS-related terms, which are noise when it comes to hypernymy detection
for BLESS concepts. Also, we find that while the supervised approach needs the negative information
from the g features (for Diff in the U+W2
300
space, omitting g features yields a drop from .84 to .8),
the unsupervised measures cannot use it. Dropping g features improves invCL results from .58 to .61.
The g-based dimensions are explicitly those for which distributional inclusion should not hold, so they
constitute noise to the unsupervised approaches.
The f features can be interpreted as dimensions that characterize hypernyms. An inspection reveals
two clear patterns. First, the features are topically relevant for the BLESS dataset. The 17 concept classes
in the dataset belong to three broader groups: animals, plants, and artifacts. An annotation of the 250
dimensions by one of the authors showed that 58 dimensions are typical of animals (parasite, extinct), 14
typical of vegetables (flora, nutrient), 80 typical of artifacts (repair, mechanical), 49 are general terms
(find, worthy), and 49 have no clear interpretation (thee, enigmatic). Second, the features are general
terms. For instance, for animals we find terms like animal, insect, creature, fauna, species, evolutionary,
pathogen, nature, ecology. We also find many hypernyms, including many concept class names.
Clearly, the selected features are domain dependent; most are directly related to the concepts and
concept classes of BLESS. We expect that our method should work well for other data sets, given its high
accuracy and the strict training procedure. However, these features are unlikely to be global indicators of
hypernymy. This emphasizes the need, in future work, to find a way to automatically determine relevance
on a per-word basis.
7 Conclusion
In this paper, we have tested the Distributional Inclusion Hypothesis, the basis for distributional ap-
proaches to hypernymy. We have found that the hypothesis only works if inclusion is selectively applied
to a set of relevant dimensions.
just by restricting to a smaller space, we evaluated the similarity measures on a variation of the U+W2 space which uses 500
randomly selected dimensions from the original space. The results are approximately unchanged from those on the original
U+W2 space.
1033
We have tested two simple supervised approaches to distributional hypernymy detection and have
found that they show good performance, and are robust to changes in the underlying space. Our best
classifier achieves .84 accuracy on BLESS and .85 on the ENTAILMENT dataset of Baroni et al. (2012). It
uses features that encode dimension-wise difference between vectors. This classifier can be interpreted
as selecting the dimensions necessary for the Distributional Inclusion Hypothesis to work, thus as an
effective way to implement selective distributional inclusion.
The next natural step is to use the supervised features to guide development of an unsupervised mea-
sure for hypernymy detection: Now that we have examples, we hope to propose a method which selects
relevant features automatically. We also would like to explore detection of other relationships, such
as meronymy. Finally, we would like to perform an extrinsic evaluation of our hypernymy detection
approach in an actual RTE system.
Acknowledgements
This research was supported by the DARPA DEFT program under AFRL grant FA8750-13-2-0026. The
authors acknowledge the Texas Advanced Computing Center (TACC)
10
for providing grid resources that
have contributed to these results. We thank the anonymous reviewers and the UTexas NLP group for
their helpful comments and suggestions.
References
Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. In Proceedings
of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, pages 1?10, Edinburgh,
UK, July. Association for Computational Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky wide web: A
collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation,
43(3):209?226.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan. 2012. Entailment above the word
level in distributional semantics. In Proceedings of the 13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 23?32, Avignon, France, April. Association for Computational
Linguistics.
Matthew Berland and Eugene Charniak. 1999. Finding parts in very large corpora. In Proceedings of the 37th
Annual Meeting of the Association for Computational Linguistics, pages 57?64, College Park, Maryland, USA,
June. Association for Computational Linguistics.
Paul Buitelaar, Philipp Cimiano, and Bernardo Magnini. 2005. Ontology Learning from Text: Methods, Evaluation
and Applications. Frontiers in Artificial Intelligence and Applications Series. IOS Press, Amsterdam.
Timothy Chklovski and Patrick Pantel. 2004. Verbocean: Mining the web for fine-grained semantic verb relations.
In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 33?40.
Philipp Cimiano, Aleksander Pivk, Lars Schmidt-Thieme, and Steffen Staab. 2005. Learning taxonomic relations
from heterogeneous sources of evidence. Ontology Learning from Text: Methods, evaluation and applications.
Daoud Clarke. 2009. Context-theoretic semantics for natural language: an overview. In Proceedings of the
Workshop on Geometrical Models of Natural Language Semantics, pages 112?119, Athens, Greece, March.
Association for Computational Linguistics.
Maayan Geffet and Ido Dagan. 2004. Feature vector quality and distributional similarity. In Proceedings of the
20th International Conference on Computational Linguistics, page 247. Association for Computational Linguis-
tics.
Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2003. Learning semantic constraints for the automatic
discovery of part-whole relations. In Proceedings of the 2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human Language Technology-Volume 1, pages 1?8. Association
for Computational Linguistics.
10
http://www.tacc.utexas.edu
1034
Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2006. Automatic discovery of part-whole relations. Com-
putational Linguistics, 32(1):83?135.
Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th
Conference on Computational Linguistics, pages 539?545, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Aur?elie Herbelot and Mohan Ganesalingam. 2013. Measuring semantic content in distributional vectors. In
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short
Papers), pages 440?445, Sofia, Bulgaria, August. Association for Computational Linguistics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language Engineering, 16:359?389, 10.
Alessandro Lenci and Giulia Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In *SEM
2012: The First Joint Conference on Lexical and Computational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 75?79, Montr?eal, Canada, 7-8 June. Association for Computational
Linguistics.
Alessandro Lenci. 2008. Distributional approaches in linguistic and cognitive research. Italian Journal of Lin-
guistics, 20(1):1?31.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou. 2003. Identifying synonyms among distributionally
similar words. In Proceedings of the 18th international Joint Conference on Artificial intelligence, pages 1492?
1493.
Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th International
Conference on Machine Learning, volume 98, pages 296?304.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word rep-
resentations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 746?751, Atlanta, Georgia, June. Associa-
tion for Computational Linguistics.
Gregory L. Murphy. 2002. The Big Book of Concepts. MIT Press, Boston, MA.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting
semantic relations. In Proceedings of the 21st International Conference on Computational Linguistics and the
44th annual meeting of the Association for Computational Linguistics.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertran Thirion, Olivier Grisel, Mathieu
Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Courna-
peau, Matthieu Brucher, MMatthieu Perrot, and
?
Edouard Duchesnay. 2011. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research, 12:2825?2830.
Enrico Santus. 2013. SLQS: An entropy measure. Master?s thesis, University of Pisa.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. Learning syntactic patterns for automatic hypernym dis-
covery. In Lawrence K. Saul, Yair Weiss, and L?eon Bottou, editors, Advances in Neural Information Processing
Systems 17, pages 1297?1304, Cambridge, MA. MIT Press.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence.
In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting
of the Association for Computational Linguistics, ACL-44, pages 801?808, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
Peter D. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379?416.
Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Michael Collins and Mark
Steedman, editors, Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,
pages 81?88.
1035
Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity.
In Proceedings of the 20th International Conference on Computational Linguistics, pages 1015?1021, Geneva,
Switzerland, Aug 23?Aug 27. Association for Computational Linguistics, COLING.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2005. The distributional inclusion hypotheses and lexical entailment.
In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 107?114,
Ann Arbor, Michigan, June. Association for Computational Linguistics.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2009. Bootstrapping distributional feature vector quality. Computa-
tional linguistics, 35(3):435?461.
1036
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 196?206,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Crouching Dirichlet, Hidden Markov Model:
Unsupervised POS Tagging with Context Local Tag Generation
Taesun Moon, Katrin Erk, and Jason Baldridge
Department of Linguistics
University of Texas at Austin
1 University Station B5100
Austin, TX 78712-0198 USA
{tsmoon,katrin.erk,jbaldrid}@mail.utexas.edu
Abstract
We define the crouching Dirichlet, hidden
Markov model (CDHMM), an HMM for part-
of-speech tagging which draws state prior dis-
tributions for each local document context.
This simple modification of the HMM takes
advantage of the dichotomy in natural lan-
guage between content and function words. In
contrast, a standard HMM draws all prior dis-
tributions once over all states and it is known
to perform poorly in unsupervised and semi-
supervised POS tagging. This modification
significantly improves unsupervised POS tag-
ging performance across several measures on
five data sets for four languages. We also show
that simply using different hyperparameter
values for content and function word states in
a standard HMM (which we call HMM+) is
surprisingly effective.
1 Introduction
Hidden Markov Models (HMMs) are simple, ver-
satile, and widely-used generative sequence models.
They have been applied to part-of-speech (POS) tag-
ging in supervised (Brants, 2000), semi-supervised
(Goldwater and Griffiths, 2007; Ravi and Knight,
2009) and unsupervised (Johnson, 2007) training
scenarios. Though discriminative models achieve
better performance in both semi-supervised (Smith
and Eisner, 2005) and supervised (Toutanova et al,
2003) learning, there has been only limited work on
unsupervised discriminative sequence models (e.g.,
on synthetic data and protein sequences (Xu et al,
2006)), and none to POS tagging.
The tagging accuracy of purely unsupervised
HMMs is far below that of supervised and semi-
supervised HMMs; this is unsurprising as it is still
not well understood what kind of structure is being
found by an unconstrained HMM (Headden III et al,
2008). However, HMMs are fairly simple directed
graphical models, and it is straightforward to ex-
tend them to define alternative generative processes.
This also applies to linguistically motivated HMMs
for recovering states and sequences that correspond
more closely to those implicitly defined by linguists
when they label sentences with parts-of-speech.
One way in which a basic HMM?s structure is a
poor model for POS tagging is that there is no inher-
ent distinction between (open-class) content words
and (closed-class) function words. Here, we propose
two extensions to the HMM. The first, HMM+, is a
very simple modification where two different hyper-
parameters are posited for content states and func-
tion states, respectively. The other is the crouch-
ing Dirichlet, hidden Markov model (CDHMM), an
extended HMM that captures this dichotomy based
on the statistical evidence that comes from context.
Content states display greater variance across lo-
cal context (e.g. sentences, paragraphs, documents),
and we capture this variance by adding a component
to the model for content states that is based on la-
tent Dirichlet alocation (Blei et al, 2003). This ex-
tension is in some ways similar to the LDAHMM
of Griffiths et al (2005). Both models are compos-
ite in that two distributions do not mix with each
other. Unlike the LDAHMM, the generation of con-
tent states is folded into the CDHMM process.
We compare the HMM+ and CDHMM against a
basic HMM and LDAHMM on POS tagging on a
more extensive and diverse set of languages than
previous work in monolingual unsupervised POS
tagging: four languages from three families (Ger-
manic: English and German; Romance: Portuguese;
196
and Mayan: Uspanteko). The CDHMM easily out-
performs all other models, including HMM+, across
three measures (accuracy, F-score, and variation
of information) for unsupervised POS tagging on
most data sets. However, the HMM+ is surpris-
ingly competitive, outperforming the basic HMM
and LDAHMM, and rivaling or even passing the
CDHMM on some measures and data sets.
2 Background
The Bayesian formulation for a basic HMM (Gold-
water and Griffiths, 2007) is:
?t|? ? Dir(?)
?t|? ? Dir(?)
wi|ti = t ? Mult(?t)
ti|ti?1 = t ? Mult(?t)
Dir is the conjugate Dirichlet prior to Mult (a multi-
nomial distribution). The state transitions are gen-
erated by Mult(?t) whose prior ?t is generated by
Dir(?) with a symmetric (i.e. uniform) hyperparam-
eter ?. Emissions are generated by Mult(?t) with
a prior ?t generated by Dir(?) with a symmetric
hyperparameter ?. Hyperparameter values smaller
than one encourage posteriors that are peaked, with
smaller values increasing this concentration. It is
not necessary that the hyperparameters be symmet-
ric, but this is a common approach when one wants
to be na??ve about the data. This is particularly ap-
propriate in unsupervised POS tagging with regard
to novel data since there won?t be a priori grounds
for favoring certain distributions over others.
There is considerable work on extensions to
HMM-based unsupervised POS tagging (see ?6),
but here we concentrate on the LDAHMM (Grif-
fiths et al, 2005), which models topics and state
sequences jointly. The model is a composite of a
probabilistic topic model and an HMM in which a
single state is allocated for words generated from
the topic model. A strength of this model is that it
is able to use less supervision than previous topic
models since it does not require a stopword list.
While the topic model component still uses the bags-
of-words assumption, the joint model infers which
words are more likely to carry topical content and
which words are more likely to contribute to the
local sequence. This model is competitive with a
standard topic model, and its output is also compet-
itive when compared with a standard HMM. How-
ever, Griffiths et al (2005) note that the topic model
component inevitably loses some finer distinctions
with respect to parts-of-speech. Though many con-
tent states such as adjectives, verbs, and nouns can
vary a great deal across documents, the topic state
groups these words together. This leads to assign-
ment of word tokens to clusters that are a poorer fit
for POS tagging. This paper shows that a model that
conflates the LDAHMM topics with content states
can significantly improve POS tagging.
3 Models
We aim to model the fact that in many languages
words can generally be grouped into function words
and content words and that these groups often
have significantly different distributions. There are
few function words and they appear frequently,
while there are many content words appearing infre-
quently. Another difference in distribution is often
implied in information retrieval by the use of stop-
word filters and tf-idf values to remove or reduce the
influence of words which occur frequently but have
low variance (i.e. their global probability is similar
to their local probability in a document).
A difference in distribution is also revealed when
the parts-of-speech are known. When no smoothing
parameters are added, the joint probability of a word
that is not ?the? or ?a? occurring with a DT tag (in
the Penn Treebank) is almost always zero. Similarly
peaked distributions are observed for other function
categories such as MD and CC. On the other hand,
the joint probability of any word occurring with NN
is much less likely to be zero and the distribution is
much less likely to be peaked.
We attempt to account for these two distributional
properties?that certain words have higher variance
across contexts (e.g. a document) and that certain
tags have more peaked emission distributions?in a
sequence model. To do this, we define the crouching
Dirichlet, hidden Markov model1 (CDHMM). This
model, like LDAHMM, captures items of high vari-
ance across contexts, but it does so without losing
1We call our model a ?crouching Dirichlet? model since it
involves a Dirichlet prior that generates distributions for certain
states as if it were ?crouching? on the side.
197
wi
?
?
?
?
ti? ? ? ? ? ?
?
?
?
?
Figure 1: Graphical representation of relevant vari-
ables and dependencies at a given time step i. Ob-
served word wi is dependent on hidden state ti.
Edges to priors ?, ?, ? may or may not be activated
depending on the value of ti. The edge to transition
prior ? is always activated. Hyperparameters to pri-
ors are represented by dots. See ?3.1 for details.
sequence distinctions, namely, a given word?s lo-
cal function via its part-of-speech. We also define
the HMM+, a simple adaptation of a basic HMM
which accounts for the latter property by using dif-
ferent priors for emissions from content and function
states.
3.1 CDHMM
The CDHMM incorporates an LDA-like module to
its graphical structure in order to capture words
and tags which have high variance across contexts.
Such tags correspond to content states. Like the
LDAHMM, the model is composite in that distribu-
tions over a single random variable are composed
of several different distribution functions which de-
pend on the value of the underlying variable.
We posit the following model (see fig. 1 for a dia-
gram of dependencies and all variables involved at a
single time step). We observe a sequence of tokens
w=(w1, . . . , wN ) that we assume is generated by
an underlying state sequence t=(t1, . . . , tN ) over a
state alphabet T with first order Markov dependen-
cies. T is a union of disjoint content states C and
function states F . In this composite model, the pri-
ors for the emission and transition for each step in
the sequence depend on whether state t at step i is
t?C or t?F . If t?C , the word emission is depen-
dent on ? (the content word prior) and the state tran-
sition is dependent on ? (the ?topic? prior) and ? (the
transition prior). If t?F , the word emission proba-
bility is dependent on ? (the function word prior)
and the state transition on ? (again, the transition
prior). Therefore, if t?F , the transition and emis-
sion structure is identical to the standard Bayesian
HMM.
To elaborate, three prior distributions are defined
globally for this model: (1) ?t, the transition prior
such that p(t?|t, ?t) = ?t?|t (2) ?t, the function word
prior such that p(w|t, ?t) = ?w|t (3) ?t, the content
word prior such that p(w|t, ?t) = ?w|t. Locally for
each context d (documents in our case), we define
?d, the topic prior such that p(t|?d) = ?t|d for t?C .
The generative story is as follows:
1. For each state t?T
(a) Draw a distribution over states ?t ?
Dir(?)
(b) If t?C , draw a distribution over words
?t ? Dir(?)
(c) If t?F , draw a distribution over words
?t ? Dir(?)
2. For each context d
(a) Draw a distribution ?d ? Dir(?) over
states t?C
(b) For each word wi in d
i. draw ti from ?ti?1 ? ?d
ii. if ti?C , then draw wi from ?ti , else
draw wi from ?ti
For each context d, we draw a prior distribution
?d?formally identical to the LDA topic prior?that
is defined only for the states t?C . This prior is then
used to weight the draws for states at each word,
from ?ti?1 ? ?d, where we have defined the vector
valued operation ? as follows:
(?ti?1 ? ?d)ti =
{
1
Z ?ti|ti?1 ? ?ti|d ti?C
1
Z ?ti|ti?1 ti?F
where (?ti?1 ? ?d)ti is the element corresponding to
state ti in the vector ?ti?1 ? ?d. Z is a normalization
constant such that the probability mass sums to one.
198
p(ti|t?i,w) ?
?
?
?
?
?
Nwi|ti+?
Nti+W?
Nti|di+?
Ndi+C?
?
Nti|ti?1+?
??
Nti+1|ti+I[ti?1=ti=ti+1]+?
?
Nti+T?+I[ti=ti?1]
ti ? C
Nwi|ti+?
Nti+W?
?
Nti|ti?1+?
??
Nti+1|ti+I[ti?1=ti=ti+1]+?
?
Nti+T?+I[ti=ti?1]
ti ? F
Figure 2: Conditional distribution for ti in the CDHMM.
The important thing to note is that the draw for
states at each word is proportional to a composite
of (a) the product of the individual elements of the
topic and transition priors when ti?C and (b) the
transition priors when ti?F . The draw is propor-
tional to the product of topic and transition priors
when ti?C because we have made a product of ex-
perts (PoE) factorization assumption (Hinton, 2002)
for tractability and to reduce the size of our model.
Without such an assumption, the transition parame-
ters would lie in a partitioned space of size O(|C|4)
as opposed to O(|T |2) for the current model. Fur-
thermore, this combination of a composite hidden
state space with a product of experts assumption al-
lows us to capture high variance for certain states.
To summarize, the CDHMM is a composite
model where both the observed token and the hidden
state variable are composite distributions. For the
hidden state, this means that there is a ?topical? ele-
ment with high variance across contexts that is em-
bedded in the state sequence for a subset of events.
We embed this element through a PoE assumption
where transitions into content states are modeled as
a product of the transition probability and the local
probability of the content state.
Inference. We use a Gibbs sampler (Gao and
Johnson, 2008) to learn the parameters of this and
all other models under consideration. In this infer-
ence regime, two distributions are of particular in-
terest. One is the posterior density and the other is
the conditional distribution, neither of which can be
learned in closed form.
Letting ? = (?, ?, ?, ?) and h = (?, ?, ?, ?), the
posterior density is given as
p(?|w, t;h) ? p(w, t|?)p(?;h)
Note that p(w, t|?) is equal to
D
?
d
Nd
?
i
(
?wi|ti?ti|d?ti|ti?1
)I[ti?C]
(
?wi|ti?ti|ti?1
)I[ti?F ] (1)
where I[?] is the indicator function, D is the number
of documents in the corpus and Nd is the number of
tokens in document d.
Another important measure is the conditional dis-
tribution which is conditioned on all the random
variables except the hidden state variable of interest
and which is derived by integrating out the priors:
p(ti|t?i,w;h) ? p(ti|t?i;h)p(wi|t,w?i;h) (2)
where t?i is the joint random variable t without ti
and w?i is w without wi.
There are two well-known approaches to conduct-
ing Gibbs sampling for HMMs. The default method
is to sample ? based on the posterior, then sample
each ti based on the conditional distribution. An-
other approach is to sample directly from the con-
ditional distribution without sampling from the pos-
terior since the conditional distribution incorporates
the posterior through integration. This is called a
collapsed Gibbs sampler, which is the method em-
ployed for the models in this study.
The full conditional distribution for tag transitions
for the Gibbs sampler is given in Figure 2. At each
time step, we decrement all counts for the current
value of ti, sample a new value for ti from a multino-
mial proportional to the conditional distribution and
assign that value to ti. ?, ? are the hyperparameters
for the word emission priors of the content states and
function states, respectively. ? is the hyperparame-
ter for the state transition priors. ? is the hyperpa-
rameter for the state prior given that it is in some
context d. Note that we have overridden notation so
199
that C and T here refer to the size of the alphabet.
W is the size of the vocabulary. Notation such as
Nti|ti?1 refers to the counts of the events indicated
by the subscript, minus the current token and tag un-
der consideration. Nti|ti?1 is the number of times ti
has occurred after ti?1 minus the tag for wi. Nwi|ti
is the number of times wi has occurred with ti minus
the current value. Nti and Ndi are the counts for the
given tag and document minus the current value.
In its broad outline, the CDHMM is not much
more complicated than an HMM since the decompo-
sition (eqn. 1) is nearly identical to that of an HMM
with the exception that conditional probabilities for
a subset of the states?the content states?are local.
An inference algorithm can be derived that involves
no more than adding a single term to the standard
MCMC algorithm for HMMs (see Figure 2).
3.2 HMM+
The CDHMM explicitly posits two different types
of states: function states and content states. Hav-
ing made this distinction, there is a very simple way
to capture the difference in emission distributions
for function and content states within an otherwise
standard HMM: posit different hyperparameters for
the two types. One type has a small hyperparame-
ter to model a sparse distribution for function words
and the other has a relatively large hyperparameter
to model a distribution with broader support. This
extension, which we refer to as HMM+, provides an
important benchmark to compare with the CDHMM
to see how much is gained by its additional ability to
model the fact that function words occur frequently
but have low variance across contexts.
As with the CDHMM, we use Gibbs sampling to
estimate the model parameters while holding the two
different hyperparameters fixed. The conditional
distribution for tag transitions for this model is iden-
tical to that in fig. 2 except that it does not have the
second term Nti|di+?Ndi+C? in the first case where ti?C .
We are not aware of a published instance of such
an extension to the HMM?which our results show
to be surprisingly effective. Goldwater and Griffiths
(2007) posits different hyperparameters for individ-
ual states, but not for different groups of states.
corpus tokens docs avg. tags
WSJ 974254 1801 541 43
Brown 797328 343 2325 80
Tiger 447079 1090 410 58
Floresta 197422 1956 101 19
Uspanteko 70125 29 2418 83
Table 2: Number of tokens, documents, average to-
kens per document and total tag types for each cor-
pus.
4 Data and Experiments
Data. We use five datasets from four languages
(English, German, Portuguese, Uspanteko) for eval-
uating POS tagging performance.
? English: the Brown corpus (Francis et al, 1982)
and the Wall Street Journal portion of the Penn
Treebank (Marcus et al, 1994).
? German: the Tiger corpus (Brants et al, 2002).
? Portuguese: the full Bosque subset of the Floresta
corpus (Afonso et al, 2002).
? Uspanteko (an endangered Mayan language of
Guatemala): morpheme-segmented and POS-
tagged texts collected and annotated by the
OKMA language documentation project (Pixabaj
et al, 2007); we use the cleaned-up version de-
scribed in Palmer et al (2009).
Table 2 provides the statistics for these corpora.
We lowercase all words, do not remove any punc-
tuation or hapax legomena, and we do not replace
numerals with a single identifier. Due to the nature
of the models, document boundaries are retained.
Evaluation We report values for three evaluation
metrics on all five corpora, using their full tagsets.
? Accuracy: We use a greedy search algorithm to
map each unsupervised tag to a gold label such
that accuracy is maximized. We evaluate on a
1-to-1 mapping between unsupervised tags and
gold labels, as well as many-to-1 (M-to-1), cor-
responding to the evaluation mappings used in
Johnson (2007). The 1-to-1 mapping provides a
stricter evaluation. The many-to-one mapping, on
the other hand, may be more adequate as unsu-
pervised tags tend to be more fine-grained than
200
Model Accuracy Pairwise P/R Scores VI1-to-1 M-to-1 P R F
W
SJ
(50
) HMM 0.34 (0.01) 0.49 (0.03) 0.51 (0.03) 0.19 (0.01) 0.28 (0.01) 3.72 (0.08)
LDAHMM 0.30 (0.04) 0.45 (0.04) 0.25 (0.07) 0.27 (0.03) 0.26 (0.04) 3.64 (0.14)
HMM+ 0.42 (0.04) 0.46 (0.05) 0.24 (0.03) 0.49 (0.03) 0.32 (0.03) 2.65 (0.15)
CDHMM 0.44 (0.01) 0.58 (0.02) 0.31 (0.01) 0.43 (0.03) 0.36 (0.02) 2.73 (0.08)
B
ro
w
n
(50
) HMM 0.32 (0.01) 0.50 (0.02) 0.60 (0.02) 0.18 (0.00) 0.28 (0.01) 3.82 (0.05)
LDAHMM 0.28 (0.06) 0.41 (0.08) 0.25 (0.10) 0.28 (0.05) 0.25 (0.05) 3.71 (0.21)
HMM+ 0.43 (0.06) 0.48 (0.07) 0.29 (0.05) 0.50 (0.04) 0.37 (0.05) 2.63 (0.19)
CDHMM 0.48 (0.02) 0.62 (0.02) 0.32 (0.03) 0.54 (0.04) 0.40 (0.03) 2.48 (0.06)
Ti
ge
r
(50
) HMM 0.29 (0.02) 0.49 (0.02) 0.49 (0.04) 0.14 (0.01) 0.22 (0.02) 3.91 (0.06)
LDAHMM 0.31 (0.04) 0.50 (0.04) 0.26 (0.07) 0.24 (0.02) 0.25 (0.04) 3.51 (0.11)
HMM+ 0.41 (0.08) 0.44 (0.05) 0.25 (0.05) 0.58 (0.10) 0.35 (0.06) 2.70 (0.25)
CDHMM 0.47 (0.01) 0.61 (0.02) 0.45 (0.01) 0.58 (0.03) 0.50 (0.02) 2.72 (0.04)
U
sp
.
(50
) HMM 0.36 (0.01) 0.49 (0.02) 0.39 (0.01) 0.18 (0.00) 0.25 (0.00) 3.63 (0.04)
LDAHMM 0.35 (0.02) 0.47 (0.02) 0.26 (0.04) 0.23 (0.03) 0.24 (0.02) 3.52 (0.09)
HMM+ 0.32 (0.02) 0.35 (0.03) 0.12 (0.02) 0.52 (0.05) 0.20 (0.02) 3.13 (0.06)
CDHMM 0.39 (0.02) 0.50 (0.02) 0.16 (0.02) 0.39 (0.03) 0.23 (0.02) 3.00 (0.06)
Fl
o
r.
(50
) HMM 0.30 (0.01) 0.58 (0.03) 0.62 (0.05) 0.18 (0.01) 0.28 (0.01) 3.51 (0.06)
LDAHMM 0.36 (0.06) 0.59 (0.04) 0.55 (0.10) 0.29 (0.07) 0.38 (0.08) 3.22 (0.15)
HMM+ 0.35 (0.04) 0.52 (0.02) 0.28 (0.04) 0.43 (0.06) 0.34 (0.04) 2.58 (0.07)
CDHMM 0.36 (0.01) 0.64 (0.02) 0.37 (0.02) 0.27 (0.01) 0.31 (0.01) 2.73 (0.05)
Table 1: Evaluation on WSJ, Brown, Tiger, Floresta and Uspanteko for models with 50 states. For VI, lower
is better
gold part-of-speech tags. In particular, they tend
to form semantically coherent sub-classes of gold
parts of speech.
? Pairwise Precision and Recall: Viewing tagging
as a clustering task over tokens, we evaluate pair-
wise precision (P ) and recall (R) between the
model tag sequence (M ) and gold tag sequence
(G) by counting the true positives (tp), false pos-
itives (fp) and false negatives (fn) between the
two and setting P = tp/(tp + fp) and R =
tp/(tp+ fn). tp is the number of token pairs that
share a tag in M as well as in G, fp is the number
token pairs that share the same tag in M but have
different tags in G, and fn is the number token
pairs assigned a different tag in M but the same
in G (Meila, 2007). We also provide the f -score
which is the harmonic mean of P and R.
? Variation of Information (VI): The variation of
information is an information theoretic metric
that measures the amount of information lost and
gained in going from tag sequenceM toG (Meila,
2007). It is defined as V I(M,G) = H(M) +
H(G) ? 2I(M,G) where H denotes entropy and
I mutual information. Goldwater and Griffiths
(2007) noted that this measure can point out mod-
els that have more consistent errors in the form
of lower VI, even when accuracy figures are the
same.
We also report learning curves on M-to-1 with ge-
ometrically increasing training set sizes of 8, 16, 32,
64, 128, 256, 512, 1024, and all documents, or as
many as possible given the corpus.
5 Experiments
In this section we discuss our parameter settings and
experimental results.
5.1 Models and Parameters
We compare four different models:
? HMM: a standard HMM
? HMM+: an HMM in which the hyperparameters
for the word emissions are asymmetric, such that
content states have different word emission priors
compared to function states.
? LDAHMM: an HMM with a distinguished state
that generates words from a topic model (Griffiths
et al, 2005)
201
WSJ Brown Tiger Floresta Uspanteko
20
30
40
50
HMM+
a
cc
0.
0
0.
1
0.
2
0.
3
0.
4
0.
5
0.
6
WSJ Brown Tiger Floresta Uspanteko
LDAHMM
a
cc
0.
0
0.
1
0.
2
0.
3
0.
4
0.
5
0.
6
WSJ Brown Tiger Floresta Uspanteko
CDHMM
a
cc
0.
0
0.
1
0.
2
0.
3
0.
4
0.
5
0.
6
Figure 3: Averaged many-to-one accuracy on the full tagset for the models HMM+, LDAHMM, CDHMM
when the number of states is set at 20, 30, 40 and 50 states.
? CDHMM: our HMM with context-based emis-
sions, where the context used is the document
We implemented all of these models, ensuring per-
formance differences are due to the models them-
selves rather than implementation details.
For all models, the transition hyperparameters ?
are set to 0.1. For the LDAHMM and HMM all emis-
sion hyperparameters are set to 0.0001. These fig-
ures are the MCMC settings that provided the best
results in Johnson (2007). For the models that distin-
guish content and function states (HMM+, CDHMM),
we fixed the number of content states at 5 and set the
function state emission hyperparameters ? = 0.0001
and the content state emission hyperparameters ? =
0.1. For the models with an LDA or LDA-like com-
ponent (LDAHMM, CDHMM), we set the topic or
content-state hyperparameter ? = 1.
For decoding, we use maximum posterior decod-
ing to obtain a single sample after the required burn-
in, as has been done in other unsupervised HMM
experiments. We use this sample for evaluation.
5.2 Results
Results for all models on the full tagset are provided
in table 1.2 Each number is the mean accuracy of
ten randomly initialized samples after a single chain
burn-in of 1000 iterations. The model with a sta-
tistically significant (p < 0.05) best score for each
measure and data set is given in plain bold. In cases
2Similar results are obtained with reduced tagsets, as is com-
monly done in other work on unsupervised POS-tagging.
where the differences for the best models are not sig-
nificantly different from each other, but are signifi-
cantly better from the others, the top model scores
are given in bold italic.
CDHMM is extremely strong on the accuracy met-
ric: it wins or ties for all datasets for both 1-to-1 and
M-to-1 measures. For pairwise f -score, it obtains
the best score for two datasets (WSJ and Tiger), and
ties with HMM+ on Brown (we return to Uspanteko
and Floresta below in an experiment that varies the
number of states). For VI, HMM+ and CDHMM both
easily outperform the other models, with CDHMM
winning Brown and Uspanteko and HMM+ winning
Floresta.
In the case of Uspanteko, the absolute difference
in mean performance between models is smaller
overall but still significant. This is due to the reduced
variance between samples for all models. This is
striking because the non-CDHMM models have much
higher standard deviation on other corpora but have
sharply reduced standard deviation only for Uspan-
teko. The most likely explanation is that the Uspan-
teko corpus is much smaller than the other corpora.3
Nonetheless, CDHMM comes out strongest on most
measures.
A simple baseline for accuracy is to choose the
most frequent tag for all tokens; this gives accura-
cies of 0.14 (WSJ), 0.14 (Brown), 0.21 (Tiger), 0.20
3which is interesting in itself since the weak law of large
numbers implies that sample standard deviation decreases with
sample size, which in our case is the number of tokens rather
than the 10 samples under discussion
202
Model Accuracy P/R Scores VI1-to-1 M-to-1 P R F
U
sp
.
(10
0) HMM 0.36 (0.01) 0.58 (0.01) 0.56 (0.02) 0.16 (0.00) 0.25 (0.01) 3.53 (0.04)
LDAHMM 0.35 (0.01) 0.58 (0.02) 0.45 (0.04) 0.17 (0.01) 0.24 (0.01) 3.46 (0.06)
HMM+ 0.35 (0.02) 0.41 (0.02) 0.18 (0.01) 0.36 (0.03) 0.24 (0.01) 3.25 (0.08)
CDHMM 0.40 (0.01) 0.59 (0.01) 0.25 (0.02) 0.27 (0.02) 0.26 (0.01) 3.05 (0.03)
Fl
o
r.
(20
) HMM 0.31 (0.02) 0.48 (0.03) 0.40 (0.03) 0.21 (0.01) 0.28 (0.02) 3.54 (0.10)
LDAHMM 0.35 (0.06) 0.46 (0.06) 0.27 (0.07) 0.45 (0.08) 0.33 (0.05) 3.10 (0.10)
HMM+ 0.37 (0.04) 0.50 (0.03) 0.30 (0.02) 0.45 (0.06) 0.36 (0.03) 2.62 (0.06)
CDHMM 0.44 (0.02) 0.55 (0.02) 0.30 (0.01) 0.53 (0.03) 0.39 (0.02) 2.39 (0.07)
Table 3: Evaluation for Uspanteko and Floresta. Experiments in this table use state sizes that correspond
more closely to the size of the tag sets in the respective corpora.
(Floresta), and 0.11 (Uspanteko). Clearly, all of the
models easily outperform this baseline.
Number of states. Figure 3 shows the change in
accuracy for the different models for different cor-
pora when the overall number of states is varied
between 20 and 50. The figure shows results for
M-to-1. All models with the exception of HMM+
show improvements as the number of states is in-
creased. This brings up the valid concern (Clark,
2003; Johnson, 2007) that a model could posit a
very large number of states and obtain high M-to-
1 scores. However, it is neither the case here nor
in any of the studies we cite. Furthermore, as is
strongly suggested with HMM+, it does not seem as
if all models will benefit from assuming a large num-
ber of states.
Looking at the results by number of states on VI
and f -score for CDHMM(Figure 5), it is clear that
Floresta displays the reverse pattern of all other data
sets where performance monotonically deteriorates
as state sizes are increased. Though the exact reason
is unknown, we believe it is partially due to the fact
that Floresta has 19 tags. We therefore wondered
whether positing a state size that more closely ap-
proximated the size of the gold tag set performs bet-
ter. Since the discrepancy is greatest for Uspanteko
and Floresta, we present tabulated results for exper-
iments with state settings of 100 and 20 states re-
spectively (table 3). With the exception of VI (where
lower is better) for Uspanteko, the scores generally
improve when the model state size is closer to the
gold size. M-to-1 goes down for Floresta when 20
states are posited, but this is to be expected since this
score is defined, to a certain extent, to do better with
WSJ Brown Tiger Floresta Uspanteko
20
30
40
50
F?SCORE
f?
sc
or
e
0.
0
0.
1
0.
2
0.
3
0.
4
0.
5
WSJ Brown Tiger Floresta Uspanteko
VI
vi
0
1
2
3
4
Figure 5: f -score and VI for CDHMM by number of
states
larger models.
Variance. As we average performance figures
over ten runs for each model, it is also instructive
to consider standard deviation across runs. Standard
deviation is lowest for the CDHMM models and the
vanilla HMM. Standard deviation is high for HMM+
and LDAHMM. This is not surprising for LDAHMM,
since it has fifty topic parameters in addition to the
number of states posited, and random initial condi-
tions would have greater effect on the outcome than
for the other models. It is unexpected, however, that
HMM+ has high variance over different chains. The
model shares the large content emission hyperpa-
rameter ? = 0.1 with CDHMM. At this point, it can
only be assumed that the additional LDA component
acts as a regularization factor for CDHMM and re-
duced the volatility in having a large emission hy-
perparameter.
203
0 1 2 3 4 5 6
0.
3
0.
4
0.
5
0.
6
Brown WSJ Tiger
UspantekoFloresta
0 1 2 3 4 5 6 7 8
0.
3
0.
4
0.
5
0.
6
0 1 2 3 4 5 6 7 8
0.
3
0.
4
0.
5
0.
6
0 1 2 3 4 5 6 7 8
0.
3
0.
4
0.
5
0.
6
0 1 2
0.
3
0.
4
0.
5
0.
6
hmm
hmm+
ldahmm
Figure 4: Learning curves on M-to-1 evaluation. The staples at each point represent two standard deviations.
Learning curves We present learning curves on
different sizes of subcorpora in Figure 4. The graphs
are box plots of the full M-1 accuracy figures on
10 randomly initialized training runs for seven sub-
corpora in Brown, nine in WSJ, Tiger, Floresta and
three in Uspanteko.
Comparing the graphs, the performance of HMM+
shows the strongest improvement for English and
German data as the amount of training data in-
creases. Also, it is evident that CDHMM posts con-
sistent performance gains across data sets as it trains
on more data. This stands in opposition to HMM and
LDAHMM which do not seem able to take advantage
of more information for WSJ and Floresta. This
suggests that performance for CDHMM and HMM+
could improve if the training corpora were aug-
mented with out-of-corpus raw data. One exception
to the consistent improvement over increased data is
the performance of the models on Uspanteko, which
uniformly flatline. One reason might be that the tags
are labeled over segmented morphemes instead of
words like the other corpora. Another could be that
Uspanteko has a relatively large number of tags in a
very small corpus.
6 Related work
Unsupervised POS tagging is an active area of re-
search. Most recent work has involved HMMs.
Given that an unconstrained HMM is not well under-
stood in POS tagging, much work has been done on
examining the mechanism and the properties of the
HMM as applied to natural language data (Johnson,
2007; Gao and Johnson, 2008; Headden III et al,
2008). Conversely, there has also been work focused
on improving the HMM as an inference procedure
that looked at POS tagging as an example (Graca et
al., 2009; Liang and Klein, 2009). Nonparametric
HMMs for unsupervised POS tag induction (Snyder
et al, 2008; Van Gael et al, 2009) have seen partic-
ular activity due to the fact that model size assump-
tions are unnecessary and it lets the data ?speak for
itself.?
There is also work on alternative unsupervised
models that are not HMMs (Schu?tze, 1993; Abend
et al, 2010; Reichart et al, 2010b) as well as re-
search on improving evaluation of unsupervised tag-
gers (Frank et al, 2009; Reichart et al, 2010a).
Though they did not concentrate on unsupervised
methods, Haghighi and Klein (2006) conducted an
unsupervised experiment that utilized certain to-
ken features (e.g. character suffixes of 3 or less,
204
has initial capital, etc.; the features themselves are
from Smith and Eisner (2005)) to learn parameters
in an undirected graphical model which was the
equivalent of an HMM in directed models. It was
also the first study to posit the one-to-one evalua-
tion criterion which has been repeated extensively
since (Johnson, 2007; Headden III et al, 2008;
Graca et al, 2009).
Finkel et al (2007) is an interesting variant of un-
supervised POS tagging where a parse tree is as-
sumed and POS tags are induced from this structure
non-parametrically. It is the converse of unsuper-
vised parsing which assumes access to a tagged cor-
pus and induces a parsing model.
Other models more directly influenced or closely
parallel our work. Griffiths et al (2005) is the work
that inspired the current approach where a set of
states is designated to capture variance across con-
texts. The primary goal of that model was to induce
a topic model given data that had not been filtered
of noise in the form of function words. As such,
distinguishing between topic states such that they
model different syntactic states was not attempted,
and we have seen in sec. 3 that such an extension is
not entirely straightforward.4 Boyd-Graber and Blei
(2009) has some parallels to our model in that a hid-
den variable over topics is distributed according to
a normalized product between a context prior and a
syntactic prior. However, it assumes a much greater
amount of information than we do in that a parse tree
as well as (possibly) POS tags are taken as observed.
The model has a very different goal from ours as
well, which is to infer a syntactically informed topic
model. Teichert and Daume? III (2010) is another
study with close similarities to our own. This study
models distinctions between closed class words and
open class words within a modified HMM. It is un-
clear from their formulation how the distinction be-
tween open class and closed class words is learned.
There is also extensive literature on learning se-
quence structure from unlabeled text (Smith and
Eisner, 2005; Goldberg et al, 2008; Ravi and
Knight, 2009) which assume access to a tag dic-
tionary. Goldwater and Griffiths (2007) deserves
mention for examining a semi-supervised model
4We tested a variant of LDAHMM in which more than one
state can generate topics. It did not achieve good results.
that sampled emission hyperparameters for each
state rather than a single symmetric hyperparame-
ter. They showed that this outperformed a symmet-
ric model. An interesting heuristic model is Zhao
and Marcus (2009) that uses a seed set of closed
class words to classify open class words.
7 Conclusion
We have shown that a hidden Markov model that
allocates a subset of the states to have distribu-
tions conditioned on localized domains can signif-
icantly improve performance in unsupervised part-
of-speech tagging. We have also demonstrated that
significant performance gains are possible simply
by setting a different emission hyperparameter for
a subgroup of the states. It is encouraging that these
results hold for both models not just on the WSJ but
across a diverse set of languages and measures.
We believe our proposed extensions to the HMM
are a significant contribution to the general HMM
and unsupervised POS tagging literature in that both
can be implemented with minimum modification
of existing MCMC inferred HMMs, have (nearly)
equivalent run times, produce output that is easy to
interpret since they are based on a generative frame-
work, and bring about considerable performance im-
provements at the same time.
Acknowledgments
The authors would like to thank Elias Ponvert and
the anonymous reviewers. This work was supported
by a grant from the Morris Memorial Trust Fund of
the New York Community Trust.
References
O. Abend, R. Reichart, and A. Rappoport. 2010. Im-
proved unsupervised POS induction through prototype
discovery. In Proceedings of ACL, pages 1298?1307.
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. Flo-
resta sinta?(c)tica?: a treebank for Portuguese. In Pro-
ceedings of LREC, pages 1698?1703.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet alocation. The Journal of Machine Learning
Research, 3:993?1022.
J. L. Boyd-Graber and D. Blei. 2009. Syntactic topic
models. In Proceedings of NIPS, pages 185?192.
205
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER treebank. In Proceedings of the
Workshop on Treebanks and Linguistic Theories.
T. Brants. 2000. TnT: a statistical part-of-speech tag-
ger. In Proceedings of conference on Applied natural
language processing, pages 224?231.
A. Clark. 2003. Combining distributional and morpho-
logical information for part of speech induction. In
Proceedings of EACL, pages 59?66.
J. R. Finkel, T. Grenager, and C. D. Manning. 2007. The
infinite tree. In Proceedings of ACL, pages 272?279.
W.N. Francis, H. Kuc?era, and A.W. Mackie. 1982. Fre-
quency analysis of English usage: Lexicon and gram-
mar. Houghton Mifflin Harcourt.
S. Frank, S. Goldwater, and F. Keller. 2009. Evaluating
models of syntactic category acquisition without using
a gold standard. In Proceedings of CogSci.
J. Gao and M. Johnson. 2008. A comparison of Bayesian
estimators for unsupervised Hidden Markov Model
POS taggers. In Proceedings of EMNLP, pages 344?
352.
Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM
can find pretty good HMM POS-taggers (when given
a good start). In Proceedings of ACL, pages 746?754.
S. Goldwater and T. L. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Proceedings of ACL, pages 744?751.
J. Graca, K. Ganchev, B. Taskar, and F. Pereira. 2009.
Posterior vs parameter sparsity in latent variable mod-
els. In Proceedings of NIPS, pages 664?672.
T. L. Griffiths, M. Steyvers, D. M. Blei, and J. M. Tenen-
baum. 2005. Integrating topics and syntax. In Pro-
ceedings of NIPS, pages 537?544.
A. Haghighi and D. Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
HLT/NAACL, pages 320?327.
W. P. Headden III, D. McClosky, and E. Charniak.
2008. Evaluating unsupervised part-of-speech tagging
for grammar induction. In Proceedings of COLING,
pages 329?336.
G.E. Hinton. 2002. Training products of experts by min-
imizing contrastive divergence. Neural Computation,
14(8):1771?1800.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers. In Proceedings of EMNLP-CoNLL,
pages 296?305.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Proceedings of HLT/NAACL, pages
611?619.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of English:
The Penn Treebank. Comp. ling., 19(2):313?330.
M. Meila. 2007. Comparing clusterings?an informa-
tion based distance. Journal of Multivariate Analysis,
98(5):873?895.
A. Palmer, T. Moon, and J. Baldridge. 2009. Evaluat-
ing automation strategies in language documentation.
In Proceedings of the NAACL-HLT 2009 Workshop
on Active Learning for Natural Language Processing,
pages 36?44.
T. C. Pixabaj, M. A. Vicente Me?ndez, M. Vicente
Me?ndez, and O. A. Damia?n. 2007. Text Collections in
Four Mayan Languages. Archived in The Archive of
the Indigenous Languages of Latin America.
S. Ravi and K. Knight. 2009. Minimized models for
unsupervised part-of-speech tagging. In Proceedings
of ACL and AFNLP, pages 504?512.
R. Reichart, O. Abend, and A. Rappoport. 2010a. Type
level clustering evaluation: New measures and a POS
induction case study. In Proceedings of CoNLL, pages
77?87.
R. Reichart, R. Fattal, and A. Rappoport. 2010b. Im-
proved unsupervised POS induction using intrinsic
clustering quality and a Zipfian constraint. In Proceed-
ings of CoNLL, pages 57?66.
H. Schu?tze. 1993. Part-of-speech induction from scratch.
In Proceedings of ACL, pages 251?258.
N.A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Pro-
ceedings of ACL, pages 354?362.
B. Snyder, T. Naseem, J. Eisenstein, and R. Barzilay.
2008. Unsupervised multilingual learning for POS
tagging. In Proceedings of EMNLP, pages 1041?
1050.
A.R. Teichert and H. Daume? III. 2010. Unsupervised
Part of Speech Tagging Without a Lexicon. In NIPS
Workshop on Grammar Induction, Representation of
Language and Language Learning 2010.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceedings of
NAACL, pages 173?180.
J. Van Gael, A. Vlachos, and Z. Ghahramani. 2009. The
infinite HMM for unsupervised PoS tagging. In Pro-
ceedings of EMNLP, pages 678?687.
L. Xu, D. Wilkinson, F. Southey, and D. Schuurmans.
2006. Discriminative unsupervised learning of struc-
tured predictors. In Proceedings of ICML, pages
1057?1064.
Q. Zhao and M. Marcus. 2009. A simple unsuper-
vised learner for POS disambiguation rules given only
a minimal lexicon. In Proceedings of EMNLP, pages
688?697.
206
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 540?549,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
What Substitutes Tell Us ?
Analysis of an ?All-Words? Lexical Substitution Corpus
Gerhard Kremer
Institute for Computational Linguistics
University of Heidelberg, Germany
kremer@cl.uni-heidelberg.de
Katrin Erk
Dept. of Linguistics
University of Texas, Austin, U.S.A.
katrin.erk@utexas.edu
Sebastian Pad?
Institute for Natural Language Processing
University of Stuttgart, Germany
pado@ims.uni-stuttgart.de
Stefan Thater
Dept. of Computational Linguistics
Saarland University, Saarbr?cken, Germany
stth@coli.uni-sb.de
Abstract
We present the first large-scale English ?all-
words lexical substitution? corpus. The
size of the corpus provides a rich resource
for investigations into word meaning. We
investigate the nature of lexical substitute
sets, comparing them to WordNet synsets.
We find them to be consistent with, but
more fine-grained than, synsets. We also
identify significant differences to results
for paraphrase ranking in context reported
for the SEMEVAL lexical substitution data.
This highlights the influence of corpus con-
struction approaches on evaluation results.
1 Introduction
Many, if not most, words have multiple meanings;
for example, the word ?bank? has a financial and
a geographical sense. One common approach to
deal with this lexical ambiguity is supervised word
sense disambiguation, or WSD (McCarthy, 2008;
Navigli, 2009), which frames the task as a lemma-
level classification problem, to be solved by train-
ing classifiers on samples of lemma instances that
are labelled with their correct senses.
This approach has its problems, however. First,
it assumes a complete and consistent set of labels.
WordNet, used in the majority of studies, does
cover several 10,000 lemmas, but has been criti-
cised for both its coverage and granularity. Second,
WSD requires annotation for each sense and lemma,
leading to an ?annotation bottleneck?. A number
of technical solutions have been suggested regard-
ing the second problem (Ando and Zhang, 2005;
Navigli and Ponzetto, 2012), but not for the first.
In 2009, McCarthy and Navigli address both
problems by proposing a fundamentally different
approach, called Lexical Substitution (McCarthy
and Navigli, 2009) which avoids capturing a word?s
meaning by a single label. Instead, annotators are
asked to list, for each instance of a word, one or
more alternative words or phrases to be substituted
for the target in this particular context. This setup
provides a number of benefits over WSD. It al-
lows characterising word meaning without using
an ontology and can be obtained easily from native
speakers through crowdsourcing. Work on mod-
elling Lexical Substitution data has also assumed a
different focus from WSD. It tends to see the predic-
tion of substitutes along the lines of compositional
lexical semantics, concentrating on explaining how
word meaning is modulated in context (Mitchell
and Lapata, 2010).
There are, however, important shortcomings of
the work in the Lexical Substitution paradigm. All
existing datasets (McCarthy and Navigli, 2009;
Sinha and Mihalcea, 2014; Biemann, 2013; Mc-
Carthy et al., 2013) are either comparatively small,
are ?lexical sample? datasets, or both. ?Lexical
sample? datasets consist of sample sentences for
each target word drawn from large corpora, with
just one target word substituted in each sentence. In
WSD, ?lexical sample? datasets contrast with ?all-
words? annotation, in which all content words in a
text are annotated for sense (Palmer et al., 2001).
540
In this paper, we present the first large ?all-
words? Lexical Substitution dataset for English. It
provides substitutions for more than 30,000 words
of running text from two domains of MASC (Ide et
al., 2008; Ide et al., 2010), a subset of the Ameri-
can National Corpus (http://www.anc.org)
that is freely available and has (partial) manual
annotation. The main advantage of the all-words
setting is that it provides a realistic frequency distri-
bution of target words and their senses. We use this
to empirically investigate (a) the nature of lexical
substitution and (b) the nature of the corpus, seen
through the lens of word meaning in context.
2 Related Work
2.1 Lexical Substitution: Data
The original ?English Lexical Substitution? dataset
(McCarthy and Navigli, 2009) comprises 200 target
content words (balanced numbers of nouns, verbs,
adjectives and adverbs). Targets were explicitly se-
lected to exhibit interesting ambiguities. For each
target, 10 sentences were chosen (mostly at ran-
dom, but in part by hand) from the English Internet
Corpus (Sharoff, 2006) and presented to 5 anno-
tators to collect substitutes. Its total size is 2,000
target instances. Sinha and Mihalcea (2014) pro-
duced a small pilot dataset (500 target instances) for
all-words substitution, asking three annotators to
substitute all content words in presented sentences.
Biemann (2013) first investigated the use of
crowdsourcing, developing a three-task bootstrap-
ping design to control for noise. His study covers
over 50,000 instances, but these correspond only to
397 targets, all of which are high-frequency nouns.
Biemann clusters the resulting substitutes into word
senses. McCarthy et al. (2013) applied lexical sub-
stitution in a cross-lingual setting, annotating 130
of the original McCarthy and Navigli targets with
Spanish substitutions (i. e., translations).
2.2 Lexical Substitution: Models
The LexSub task at SEMEVAL 2007 (McCarthy
and Navigli, 2009) required systems to both de-
termine substitution candidates and choose con-
textual substitutions in each case. Erk and Pad?
(2008) treated the gold substitution candidates as
given and focused on the context-specific ranking
of those candidates. In this form, the task has been
addressed through three types of (mostly unsuper-
vised) approaches. The first group computes a sin-
gle type representation and modifies it according
to sentence context (Erk and Pad?, 2008; Thater et
al., 2010; Thater et al., 2011; Van de Cruys et al.,
2011). The second group of approaches clusters
instance representations (Reisinger and Mooney,
2010; Dinu and Lapata, 2010; Erk and Pad?, 2010;
O?S?aghdha and Korhonen, 2011). The third op-
tion is to use a language model (Moon and Erk,
2013). Recently, supervised models have emerged
(Biemann 2013; Szarvas et al., 2013a,b).
3 COINCO ? The MASC All-Words
Lexical Substitution Corpus
1
Compared to, e. g., WSD, there still is little gold-
annotated data for lexical substitution. With the
exception of the dataset created by Biemann (2013),
all existing lexical substitution datasets are fairly
small, covering at most several thousand instances
and few targets which are manually selected. We
aim to fill this gap, providing a dataset that mirrors
the actual corpus distribution of targets in sentence
context and is sufficiently large to enable a detailed,
lexically specific analysis of substitution patterns.
3.1 Source Corpus Choice
For annotation, we chose a subset of the ?Manually
Annotated Sub-Corpus? MASC (Ide et al., 2008;
Ide et al., 2010) which is ?equally distributed across
19 genres, with manually produced or validated
annotations for several layers of linguistic phenom-
ena?, created with the purpose of being ?free of
usage and redistribution restrictions?. We chose
this corpus because (a) our analyses can profit from
the preexisting annotations and (b) we can release
our annotations as part of MASC.
Since we could not annotate the complete MASC,
we selected (complete) text documents from two
prominent genres: news (18,942 tokens) and fiction
(16,605 tokens). These two genres are both rele-
vant for NLP and provide long, coherent documents
that are appropriate for all-words annotation. We
used the MASC part-of-speech annotation to iden-
tify all content words (verbs, nouns, adjectives, and
adverbs), which resulted in a total of over 15,000
targets for annotation. This method differs from
Navigli and McCarthy?s (2009) in two crucial re-
spects: we annotate all instances of each target, and
include all targets regardless of frequency or level
of lexical ambiguity. We believe that our corpus is
considerably more representative of running text.
1
Available as XML-formatted corpus ?Concepts in Con-
text? (COINCO) from http://goo.gl/5C0jBH. Also
scheduled for release as part of MASC.
541
3.2 Crowdsourcing
We used the Amazon Mechanical Turk (AMT) plat-
form to obtain substitutes by crowdsourcing. Inter-
annotator variability and quality issues due to non-
expert annotators are well-known difficulties (see,
e. g., Fossati et al. (2013)). Our design choices
were shaped by ?best practices in AMT?, including
Mason and Suri (2012) and Biemann (2013).
Defining HITs. An AMT task consists of Human
Intelligence Tasks (HITs), each of which is sup-
posed to represent a minimal, self-contained task.
In our case, potential HITs were annotations of
(all target words in) one sentence, or just one tar-
get word. The two main advantages of annotating
a complete sentence at a time are (a) less over-
head, because the sentence has only to be read
once; (b) higher reliability, since all words within a
sentence will be annotated by the same person.
Unfortunately, presenting individual sentences
as HITs also means that all sentences pay the same
amount irrespective of their length. Since long sen-
tences require more effort, they are likely to receive
less attention. We therefore decided to generally
present two random target words per HIT, and one
word in the case of ?leftover? singleton targets.
In the HITs, AMT workers (?turkers?) saw the
highlighted target word in context. Since one sen-
tence was often insufficient to understand the target
fully, we also showed the preceding and the follow-
ing sentence. The task description asked turkers to
provide (preferably single-word) substitutes for the
target that ?would not change the meaning?. They
were explicitly allowed to use a ?more general term?
in case a substitute was hard to find (e. g., dog for
the target dachshund, cf. basic level effects: Rosch
et al. (1976)). Turkers were encouraged to produce
as many replacements as possible (up to 5). If they
could not find a substitute, they had to check one of
the following radio buttons: ?proper name?, ?part
of a fixed expression?, ?no replacement possible?,
?other problem (with description)?.
Improving Reliability. Another major problem
is reliability. Ideally, the complete dataset should
be annotated by the same group of annotators, but
turkers tend to work only on a few HITs before
switching to other AMT jobs. Following an idea
of Biemann and Nygaard (2010), we introduced a
two-tier system of jobs aimed at boosting turker
loyalty. A tier of ?open tasks? served to identify
reliable turkers by manually checking their given
substitutes for plausibility. Such turkers were then
invited to the second, ?closed task? tier, with a
higher payment. In both tiers, bonus payments
were offered to those completing full HIT sets.
For each target, we asked 6 turkers to provide
substitutions. In total, 847 turkers participated suc-
cessfully. In the open tasks, 839 turkers submitted
12,158 HITs (an average of 14.5 HITs). In the
closed tasks, 25 turkers submitted 42,827 HITs (an
average of 1,713 HITs), indicating the substantial
success of our turker retention scheme.
Cost. In the open task, each HIT was paid for
with $ 0.03, in the closed task the wage was $ 0.05
per HIT. The bonus payment for completing a HIT
set amounted to $ 2 ($ 1) in the open (closed) tasks.
The average cost for annotations was $ 0.22 for one
target word instance and $ 0.02 for one substitute.
The total cost with fees was ~$ 3,400.
3.3 COINCO: Corpus and Paraset Statistics
We POS-tagged and lemmatised targets and substi-
tutes in sentence context with TreeTagger (Schmid,
1994). We manually lemmatised unknown words.
Our annotated dataset comprises a total of 167,336
responses by turkers for 15,629 target instances in
2,474 sentences (7,117 nouns, 4,617 verbs, 2,470
adjectives, and 1,425 adverbs). As outlined above,
targets are roughly balanced across the two gen-
res (news: 8,030 instances in 984 sentences; fic-
tion: 7,599 instances in 1,490 sentences). There are
3,874 unique target lemmas; 1,963 of these occur
more than once. On this subset, there is a mean of
6.99 instances per target lemma. To our knowledge,
our corpus is the largest lexical substitution dataset
in terms of lemma coverage.
Each target instance is associated with a paraset
(i. e., the set of substitutions or paraphrases pro-
duced for a target in its context) with an average
size of 10.71. Turkers produced an average of
1.68 substitutions per target instance.
2
Despite
our instructions to provide single-word substitutes,
11,337 substitutions contain more than one word.
3.4 Inter-Annotator Agreement
McCarthy and Navigli (2009) introduced two inter-
annotator agreement (IAA) measures for their
dataset. The first one is pairwise agreement (PA),
2
Note that a small portion of the corpus was annotated by
more than 6 annotators.
542
dataset # targets PA mode-% PA
m
MN09 1,703 27.7 73.9 50.7
SM13 550 15.5 N/A N/A
COINCO (complete) 15,400 19.3 70.9 44.7
COINCO (subset) 2,828 24.6 76.4 50.9
Table 1: Pairwise turker agreement (mode-%: per-
centage of target instances with a mode)
measuring the overlap of produced substitutions:
PA =
?
t?T
?
?s
t
,s
?
t
? ?C
t
|s
t
? s
?
t
|
|s
t
? s
?
t
|
?
1
|C
t
| ? |T |
where t is a target in our target set T , s
t
is the
paraset provided by one turker for t, and C
t
is the
set comprising all pairs of turker-specific parasets
for t. Only targets with non-empty parasets (i. e.,
not marked by turkers as a problematic target) from
at least two turkers are included. The second one
is mode agreement (PA
m
), the agreement of an-
notators? parasets with the mode (the unique most
frequent substitute) for all targets where one exists:
PA
m
=
?
t?T
m
?
s
t
?S
t
[m ? s
t
] ?
1
|s
t
| ? |T
m
|
where T
m
is the set of all targets with some mode
m and S
t
is the set of all parasets for target t. The
Iverson bracket notation [m ? s
t
] denotes 1 if
mode m is included in s
t
(otherwise 0).
Table 1 compares our dataset to the results by
McCarthy and Navigli (2009, MN09) and Sinha
and Mihalcea (2014, SM13). The scores for
our complete dataset (row 3) are lower than Mc-
Carthy and Navigli?s both for PA (?8 %) and PA
m
(?6 %), but higher than Sinha and Mihalcea?s, who
also note the apparent drop in agreement.
3
We believe that this is a result of differences in
the setup rather than an indicator of low quality:
Note that PA will tend to decrease both in the face
of more annotators and of more substitutes. Both
of these factors are present in our setup. To test this
interpretation, we extracted a subset of our data that
is comparable to McCarthy and Navigli?s regard-
ing these factors. It comprises all target instances
where (a) exactly 6 turkers gave responses (9,521
targets), and (b) every turker produced between one
and three substitutes (5,734 targets). The results for
this subset (row 4) are much more similar to those
of McCarthy and Navigli: the pairwise agreement
3
Please see McCarthy and Navigli (2009) for a possible
explanation of the generally low IAA numbers in this field.
relation all verb noun adj adv
syn 9.4 12.5 7.7 8.0 10.4
direct-hyper 6.6 9.3 7.6 N/A N/A
direct-hypo 7.5 11.6 8.0 N/A N/A
trans-hyper 3.2 2.8 4.7 N/A N/A
trans-hypo 3.0 3.7 3.8 N/A N/A
wn-other 68.9 60.7 66.5 88.5 85.4
not-in-wn 2.1 0.9 2.2 3.4 4.2
Table 2: Target?substitute relations in percentages,
overall (all) and by POS. Note: WordNet contains
no hypo-/hypernyms for adjectives and adverbs.
differs only by 3 %, and the mode agreement is
almost identical. We take these figures as indica-
tion that crowdsourcing can serve as a sufficiently
reliable way to create substitution data; note that
Sinha and Mihalcea?s annotation was carried out
?traditionally? by three annotators.
Investigating IAA numbers by target POS and by
genre, we found only small differences (? 2.6 %)
among the various subsets, and no patterns.
4 Characterising Lexical Substitutions
This section examines the collected lexical substi-
tutions, both quantitatively and qualitatively. We
explore three questions: (a) What lexical relations
hold between targets and their substitutes? (b) Do
parasets resemble word senses? (c) How similar
are the parasets that correspond to the same word
sense of a target? These questions have not been
addressed before, and we would argue that they
could not be addressed before, because previous
corpora were either too small or were sampled in a
way that was not conducive to this analysis.
We use WordNet (Fellbaum, 1998), release 3.1,
as a source for both lexical relations and word
senses. WordNet is the de facto standard in NLP
and is used for both WSD and broader investiga-
tions of word meaning (Navigli and Ponzetto, 2012;
Erk and McCarthy, 2009). Multi-word substitutes
are excluded from all analyses.
4
4.1 Relating Targets and Substitutes
We first look at the most canonical lexical relations
between a target and its substitutes. Table 2 lists the
percentage of substitutes that are synonyms (syn),
direct/transitive (direct-/trans-) hypernyms (hyper)
4
All automatic lexical substitution approaches, including
Section 5, omit multi-word expressions. Also, they can be
expected to have WordNet coverage and normalisation issues,
which would constitute a source of noise for this analysis.
543
sentence substitutes
Now, how can I help the elegantly mannered friend of
my Nepthys and his surprising young charge ?
dependent, person, task, lass, prot?g?, effort, companion
The distinctive whuffle of pleasure rippled through the
betas on the bridge, and Rakal let loose a small growl,
as if to caution his charges against false hope.
dependent, command, accusation, private, companion, follower,
subordinate, prisoner, teammate, ward, junior, underling, enemy,
group, crew, squad, troop, team, kid
Table 3: Context effects below the sense level: target noun ?charge? (wn-other shown in italics)
and hyponyms (hypo) of the target. If a substitute
had multiple relations to the target, the shortest path
from any of its senses to any sense of the target
was chosen. The table also lists the percentage of
substitutes that are elsewhere in WordNet but not
related to the target (wn-other) and substitutes that
are not covered by WordNet (not-in-wn).
We make three main observations. First, Word-
Net shows very high coverage throughout ? there
are very few not-in-wn substitutes. Second, the per-
centages of synonyms, hypernyms and hyponyms
are relatively similar (even though the annotation
guidelines encouraged the annotation of hyponyms
over hypernyms), but relatively small. Finally, and
most surprisingly, the vast majority of substitutes
across all parts of speech are wn-other.
A full analysis of wn-other is beyond the cur-
rent paper. But a manual analysis of wn-other
substitutes for 10 lemmas
5
showed that most of
them were context-specific substitutes that can dif-
fer even when the sense of the target is the same.
This is illustrated in Table 3, which features two
occurrences of the noun ?charge? in the sense of
?person committed to your care?. But because of
the sentence context, the first occurrence got sub-
stitutes like ?prot?g??, while the second one was
paraphrased by words like ?underling?. We also
see evidence of annotator error (e. g., ?command?
and ?accusation? in the second sentence).
6
Dis-
counting such instances still leaves a prominent
role for correct wn-other cases.
But are these indeed contextual modulation ef-
fects below the sense level, or are parasets funda-
mentally different from word senses? We perform
two quantitative analyses to explore this question.
4.2 Comparing Parasets to Synsets
To what extent do parasets follow the boundaries
of WordNet senses? To address this question, we
5
We used the nouns business, charge, place, way and the
verbs call, feel, keep, leave, show, stand.
6
A manual analysis of the same 10 lemmas showed only
38 out of 1,398 (0.027) of the substitutes to be erroneous.
paraset?sense mapping class verb noun adj adv
mappable 90.3 73.5 33.0 49.6
uniquely mappable 63.1 57.5 24.3 41.3
Table 4: Ratios of (uniquely) mappable parasets
establish a mapping between parasets and synsets.
Since gold standard word senses in MASC are lim-
ited to high-frequency lemmas and cover only a
small part of our data, we create a heuristic map-
ping that assigns each paraset to that synset of its
target with which it has the largest intersection. We
use extended WordNet synsets that include direct
hypo- and hypernyms to achieve better matches
with parasets. We call a paraset uniquely mappable
if it has a unique best WordNet match, and map-
pable if one or more best matches exist. Table 4
shows that most parasets are mappable for nouns
and verbs, but not for adjectives or adverbs.
We now focus on mappable parasets for nouns
and verbs. To ensure that this does not lead to a
confounding bias, we performed a small manual
study on the 10 noun and verb targets mentioned
above (247 parasets). We found 25 non-mappable
parasets, which were due to several roughly equally
important reasons: gaps in WordNet, multi-word
expressions, metaphor, problems of sense granular-
ity, and annotator error. We also found 66 parasets
with multiple best matches. The two dominant
sources were target occurrences that evoked more
than one sense and WordNet synset pairs with very
close meanings. We conclude that excluding non-
mappable parasets does not invalidate our analysis.
To test whether parasets tend to map to a single
synset, we use a cluster purity test that compares
a set of clusters C to a set of gold standard classes
C
?
. Purity measures the accuracy of each cluster
with respect to its best matching gold class:
purity(C,C
?
) =
1
N
K
?
k=1
max
k
?
|C
k
? C
?
k
?
|
where N is the total number of data points, K is the
544
measure verbs nouns
cluster purity (%) 75.1 81.2
common core size within sense 1.84 2.21
common core size across senses 0.39 0.41
paraset size 6.89 6.29
Table 5: Comparing uniquely mappable parasets to
senses: overlap with best WordNet match as cluster
purity (top), and intersection size of parasets with
and without the same WordNet match (bottom)
number of clusters, and C
?
k
?
is the gold class that
has the largest overlap with cluster C
k
. In our case,
C is the set of mappable parasets
7
, C
?
the set of
extended WordNet synsets, and we only consider
substitutes that occur in one of the target?s extended
synsets (these are the data points). This makes the
current analysis complementary to the relational
analysis in Table 2.
8
The result, listed in the first row of Table 5,
shows that parasets for both verbs and nouns have
a high purity, that is, substitutes tend to focus on a
single sense. This can be interpreted as saying that
annotators tend to agree on the general sense of a
target. Roughly 20?25 % of substitutes, however,
tend to stem from a synset of the target that is not
the best WordNet match. This result comes with
the caveat that it only applies to substitutes that
are synonyms or direct hypo- and hypernyms of
the target. So in the next section, we perform an
analysis that also includes wn-other substitutes.
4.3 Similarity Between Same-Sense Parasets
We now use the WordNet mappings from the pre-
vious section to ask how (dis-)similar parasets are
that represent the same word sense. We also try to
identify the major sources for dissimilarity.
We quantify paraset similarity as the common
core, that is, the intersection of all parasets for
the same target that map onto the same extended
WordNet synset. Surprisingly, the common core
is mostly non-empty (in 85.6 % of all cases), and
contains on average around two elements, as the
second row in Table 5 shows. For this analysis, we
only use uniquely mappable parasets. In relation
to the average paraset size (see row 4), this means
that one quarter to one third of the substitutes are
7
For non-uniquely mappable parasets, the purity is the
same for all best-matching synsets.
8
Including wn-other substitutes would obscure whether
low purity means substitutes from a mixture of senses (which
we are currently interested in) or simply a large number of
wn-other substitutes (which we have explored above).
set elements
synset \ core feel, perceive, comprehend
synset ? core sense
core \ synset notice
non-core substitutes detect, recall, perceive, experi-
ence, note, realize, discern
Table 6: Target feel.v.03: synset and common core
shared among all instances of the same target?sense
combination. In contrast, the common core for
all parasets of targets that map onto two or more
synsets contains only around 0.4 substitutes (see
row 3) ? that is, it is empty more often than not.
At the same time, if about one quarter to one
third of the substitutes are shared, this means that
there are more non-shared than shared substitutes
even for same-sense parasets. Some of these cases
result from small samples: Even 6 annotators can-
not always exhaust all possible substitutes. For
example, the phrase ?I?m starting to see more busi-
ness transactions? occurs twice in the corpus. The
two parasets for ?business? share the same best
WordNet sense match, but they have only 3 shared
and 7 non-shared substitutes. This is even though
the substitutes are all valid and apply to both in-
stances. Other cases are instances of the context
sensitivity of the Lexical Substitution task as dis-
cussed above. Table 6 illustrates on an example
how the common core of a target sense relates to
the corresponding synset; note the many context-
specific substitutes outside the common core.
5 Ranking Paraphrases
While there are several studies on modelling lexi-
cal substitutes, almost all reported results use Mc-
Carthy and Navigli?s SEMEVAL 2007 dataset. We
now compare the results of three recent computa-
tional models on COINCO (our work) and on the
SEMEVAL 2007 dataset to highlight similarities
and differences between the two datasets.
Models. We consider the paraphrase ranking
models of Erk and Pad? (2008, EP08), Thater et
al. (2010, TFP10) and Thater et al. (2011, TFP11).
These models have been analysed by Dinu et al.
(2012) as instances of the same general framework
and have been shown to deliver state-of-the-art per-
formance on the SEMEVAL 2007 dataset, with best
results for Thater et al. (2011).
The three models share the idea to represent the
meaning of a target word in a specific context by
545
corpus syntactically structured syntactically filtered bag of words random
TFP11 TFP10 EP08 TFP11/EP08 TFP10 TFP11/EP08 TFP10
COINCO
context 47.8 46.0 47.4 47.4 41.9 46.2 40.8
33.0
baseline 46.2 44.6 46.2 45.8 38.8 44.7 37.5
SEMEVAL 2007
context 52.5 48.6 49.4 50.1 44.7 48.0 42.6
30.0
baseline 43.7 42.7 43.7 44.4 38.0 42.7 35.8
COINCO Subset
context 40.3 37.7 39.0 39.2 34.1 37.7 32.5
23.7
baseline 36.7 35.7 36.7 36.4 30.6 35.4 28.0
Table 7: Corpus comparison in terms of paraphrase ranking quality (GAP percentage). SEMEVAL results
from Thater et al. (2011). ?Context?: full models, ?baseline?: uncontextualised target-substitute similarity.
modifying the target?s basic meaning vector with
information from the vectors of the words in the
target?s direct syntactic context. For instance, the
vector of ?coach? in the phrase ?the coach derailed?
is obtained by modifying the basic vector represen-
tation of ?coach? through the vector of ?derail?, so
that the resulting contextualised vector reflects the
train car sense of ?coach?.
We replicate the setup of Thater et al. (2011)
to make our numbers directly comparable. We
consider three versions of each model: (a) syntacti-
cally structured models use vectors which record
co-occurrences based on dependency triples, ex-
plicitly recording syntactic role information within
the vectors; (b) syntactically filtered models also
use dependency-based co-occurrence information,
but the syntactic role is not explicitly represented in
the vector representations; (c) bag-of-words mod-
els use a window of ? 5 words. All co-occur-
rence counts are extracted from the English Giga-
word corpus (http://catalog.ldc.upenn.
edu/LDC2003T05), analysed with Stanford de-
pendencies (de Marneffe et al., 2006).
We apply the models to our dataset as follows:
We first collect all substitutes for all occurrences of
a target word in the corpus. The task of our models
for each target instance is then to rank the candi-
dates so that the actual substitutes are ranked higher
than the rest. We rank candidates according to the
cosine similarity between the contextualised vec-
tor of the target and the vectors of the candidates.
Like most previous approaches, we compare the
resulting ranked list with the gold standard annota-
tion (the paraset of the target instance), using gen-
eralised average precision (Kishida, 2005, GAP),
and using substitution frequency as weights. GAP
scores range between 0 and 1; a score of 1 indicates
a perfect ranking in which all correct substitutes
precede all incorrect ones, and correct high-weight
substitutes precede low-weight substitutes.
Results. The upper part of Table 7 shows results
for our COINCO corpus and the previous stan-
dard dataset, SEMEVAL 2007. ?Context? refers to
the full models, and ?baseline? to global, context-
unaware ranking based on the semantic similarity
between target and substitute. Baselines are model-
specific since they re-use the models? vector repre-
sentations. Note that EP08 and TFP11 are identical
unless syntactically structured vectors are used, and
their baselines are identical.
The behaviour of the baselines on the two cor-
pora is quite similar: random baselines have GAPs
around 0.3, and uncontextualised baselines have
GAPs between 0.35 and 0.46. The order of the
models is also highly parallel: the syntactically
structured TFP11 is the best model, followed by
its syntactically filtered version and syntactically
structured EP08. All differences between these
models are significant (p< 0.01) for both corpora,
as computed with bootstrap resampling (Efron and
Tibshirani, 1993). That is, the model ranking on
SEMEVAL is replicated on COINCO.
There are also substantial differences between
the two corpora, though. Most notably, all models
perform substantially worse on COINCO. This
is true in absolute terms (we observe a loss of 2?
5 % GAP) but even more dramatic expressed as the
gain over the uninformed baselines (almost 9 % for
TFP11 on SEMEVAL but only 1.2 % on COINCO).
All differences between COINCO and SEMEVAL
are again significant (p< 0.01).
We see three major possible reasons for these
differences: variations in (a) the annotation setup
(crowdsourcing, multiple substitutes); (b) the sense
distribution; (c) frequency and POS distributions
between the two corpora. We focus on (c) since it
can be manipulated most easily. SEMEVAL con-
tains exactly 10 instances for all targets, while CO-
INCO reflects the Zipf distribution of ?natural? cor-
pora, with many targets occurring only once. Such
546
corpora are easier to model in terms of absolute
performance, because the paraphrase lists for rare
targets contain less false positives for each instance.
For hapax legomena, the set of substitution candi-
dates is identical to the gold standard, and the only
way to receive a GAP score lower than 1 for such
targets is to rank low-weight substitutes ahead of
high-weight substitutes. Not surprisingly, the mean
GAP score of the syntactically structured TFP11
for hapax legomena is 0.863. At the same time,
such corpora make it harder for full models to out-
perform uncontextualised baselines; the best model
(TFP11) only outperforms the baseline by 1.6 %.
To neutralise this structural bias, we created
?SEMEVAL-like? subsets of COINCO (collectively
referred to as the COINCO Subset) by extracting
all COINCO targets with at least 10 instances (141
nouns, 101 verbs, 50 adjectives, 36 adverbs) and
building 5 random samples by drawing 10 instances
for each target. These samples match SEMEVAL in
the frequency distribution of its targets. To account
for the unequal distribution of POS in the samples,
we compute GAP scores for each POS separately
and calculate these GAP scores? average.
The results for the various models on the CO-
INCO Subset in the bottom part of Table 7 show
that the differences between COINCO and SE-
MEVAL are not primarily due to the differences
in target frequencies and POS distribution ? the
COINCO Subset is actually more different to SE-
MEVAL than the complete COINCO. Strikingly,
the COINCO Subset is very difficult, with a ran-
dom baseline of 24 % and model performances be-
low 37 % (baselines) and up to 40 % (full models),
which indicates that the set of substitutes in CO-
INCO is more varied than in SEMEVAL as an effect
of the annotation setup. Encouragingly, the margin
between full models and baselines is larger than on
the complete COINCO and generally amounts to
2?4 % (3.6 % for TFP11). That is, the full models
are more useful on the COINCO corpus than they
appeared at first glance; however, their effect still
remains much smaller than on SEMEVAL.
6 Conclusion
This paper describes COINCO, the first large-scale
?all-words? lexical substitution corpus for English.
It was constructed through crowdsourcing on the
basis of MASC, a corpus of American English.
The corpus has two major advantages over previ-
ous lexical substitution corpora. First, it covers con-
tiguous documents rather than selected instances.
We believe that analyses on our corpus generalise
better to the application domain of lexical substitu-
tion models, namely random unseen text. In fact,
we find substantial differences between the perfor-
mances of paraphrase ranking models for COINCO
and the original SEMEVAL 2007 LexSub dataset:
the margin of informed methods over the baselines
are much smaller, even when controlling for target
frequencies and POS distribution. We attribute this
divergence at least in part to the partially manual se-
lection strategy of SEMEVAL 2007 (cf. Section 2.1)
which favours a more uniform distribution across
senses, while our whole-document annotation faces
the ?natural? distribution skewed towards predom-
inant senses. This favours the non-contextualised
baseline models, consistent with our observations.
At the very least, our findings demonstrate the sen-
sitivity of evaluation results on corpus properties.
The second benefit of our corpus is that its size
enables more detailed analyses of lexical substi-
tution data than previously possible. We are able
to investigate the nature of the paraset, i. e., the
set of lexical substitutes given for one target in-
stance, finding that lexical substitution sets corre-
spond fairly well to WordNet sense distinctions
(parasets for the same synset show high similarity,
while those for different senses do not). In addition,
however, we observe a striking degree of context-
dependent variation below the sense level: the ma-
jority of lexical substitutions picks up fine-grained,
situation-specific meaning components that do not
qualify as sense distinctions in WordNet.
Avenues for future work include a more detailed
analysis of the substitution data to uncover genre-
and domain-specific patterns and the development
of lexical substitution models that take advantage
of the all-words substitutes for global optimisation.
Acknowledgements
We are grateful to Jan Pawellek for implementing
the AMT task, extracting MASC data, and preparing
HITs. Furthermore, we thank Georgiana Dinu for
her support with the word meaning models.
References
Rie Kubota Ando and Tong Zhang. 2005. A frame-
work for learning predictive structures from multiple
tasks and unlabeled data. Journal of Machine Learn-
ing Research, 6:1817?1853.
547
Chris Biemann and Valerie Nygaard. 2010. Crowd-
sourcing WordNet. In Proceedings of the 5th Global
WordNet conference, Mumbai, India.
Chris Biemann. 2013. Creating a system for lexi-
cal substitutions from scratch using crowdsourcing.
Language Resources and Evaluation, 47(1):97?122.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, pages 449?454, Genoa, Italy.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of EMNLP, pages 1162?1172, Cambridge, MA.
Georgiana Dinu, Stefan Thater, and S?ren Laue. 2012.
A comparison of models of word meaning in con-
text. In Proceedings of NAACL, pages 611?615,
Montr?al, Canada.
Bradley Efron and Robert J. Tibshirani. 1993. An
Introduction to the Bootstrap. Chapman and Hall,
New York.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of EMNLP, pages
440?449, Singapore.
Katrin Erk and Sebastian Pad?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP, pages 897?906, Honolulu,
HI.
Katrin Erk and Sebastian Pad?. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of ACL, pages 92?97, Uppsala, Sweden.
Christiane Fellbaum, editor. 1998. WordNet: An
electronic lexical database. MIT Press, Cambridge,
MA.
Marco Fossati, Claudio Giuliano, and Sara Tonelli.
2013. Outsourcing FrameNet to the crowd. In Pro-
ceedings of ACL, pages 742?747, Sofia, Bulgaria.
Nancy Ide, Collin F. Baker, Christiane Fellbaum,
Charles Fillmore, and Rebecca Passonneau. 2008.
MASC: The manually annotated sub-corpus of
American English. In Proceedings of LREC, pages
2455?2461, Marrakech, Morocco.
Nancy Ide, Christiane Fellbaum, Collin Baker, and Re-
becca Passonneau. 2010. The manually annotated
sub-corpus: A community resource for and by the
people. In Proceedings of ACL, pages 68?73, Upp-
sala, Sweden.
Kazuaki Kishida. 2005. Property of average precision
and its generalization: An examination of evalua-
tion indicator for information retrieval experiments.
Technical Report NII-2005-014E, Japanese National
Institute of Informatics.
Winter Mason and Siddharth Suri. 2012. Conducting
behavioral research on Amazon?s Mechanical Turk.
Behavior Research Methods, 44(1):1?23.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139?159.
Diana McCarthy, Ravi Sinha, and Rada Mihalcea.
2013. The cross-lingual lexical substitution task.
Language Resources and Evaluation, 47(3):607?
638.
Diana McCarthy. 2008. Word sense disambiguation.
In Linguistics and Language Compass. Blackwell.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Taesun Moon and Katrin Erk. 2013. An inference-
based model of word meaning in context as a para-
phrase distribution. ACM Transactions on Intelli-
gent Systems and Technology, 4(3).
Roberto Navigli and Simone Paolo Ponzetto. 2012.
Babelnet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41:1?69.
Diarmuid O?S?aghdha and Anna Korhonen. 2011.
Probabilistic models of similarity in syntactic con-
text. In Proceedings of EMNLP, pages 1047?1057,
Edinburgh, UK.
Martha Palmer, Christiane Fellbaum, Scott Cotton,
Lauren Delfs, and Hoa Trang Dang. 2001. English
tasks: All-words and verb lexical sample. In Pro-
ceedings of the SENSEVAL-2 workshop, pages 21?
24, Toulouse, France.
Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Proceeding of NAACL, pages 109?117, Los
Angeles, CA.
Eleanor Rosch, Carolyn B. Mervis, Wayne D. Gray,
David M. Johnson, and Penny Boyes-Braem. 1976.
Basic objects in natural categories. Cognitive Psy-
chology, 8(3):382?439.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
NEMLAP, pages 44?49, Manchester, UK.
Serge Sharoff. 2006. Open-source corpora: Using the
net to fish for linguistic data. International Journal
of Corpus Linguistics, 11(4):435?462.
Ravi Sinha and Rada Mihalcea. 2014. Explorations
in lexical sample and all-words lexical substitution.
Natural Language Engineering, 20(1):99?129.
548
Gy?rgy Szarvas, Chris Biemann, and Iryna Gurevych.
2013a. Supervised all-words lexical substitution
using delexicalized features. In Proceedings of
NAACL-HLT, pages 1131?1141, Atlanta, GA.
Gy?rgy Szarvas, R?bert Busa-Fekete, and Eyke H?ller-
meier. 2013b. Learning to rank lexical substitutions.
In Proceedings of EMLNP, pages 1926?1932, Seat-
tle, WA.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of ACL, pages 948?957, Uppsala, Sweden.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and effec-
tive vector model. In Proceedings of IJCNLP, pages
1134?1143, Chiang Mai, Thailand.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2011. Latent vector weighting for word mean-
ing in context. In Proceedings of EMNLP, pages
1012?1022, Edinburgh, Scotland.
549
A Flexible, Corpus-Driven Model of Regular
and Inverse Selectional Preferences
Katrin Erk?
University of Texas at Austin
Sebastian Pad???
Heidelberg University
Ulrike Pad??
Vico Research and Consulting GmbH
We present a vector space?based model for selectional preferences that predicts plausibility
scores for argument headwords. It does not require any lexical resources (such as WordNet). It
can be trained either on one corpus with syntactic annotation, or on a combination of a small
semantically annotated primary corpus and a large, syntactically analyzed generalization cor-
pus. Our model is able to predict inverse selectional preferences, that is, plausibility scores for
predicates given argument heads.
We evaluate our model on one NLP task (pseudo-disambiguation) and one cognitive task
(prediction of human plausibility judgments), gauging the influence of different parameters and
comparing our model against other model classes. We obtain consistent benefits from using the
disambiguation and semantic role information provided by a semantically tagged primary cor-
pus. As for parameters, we identify settings that yield good performance across a range of experi-
mental conditions. However, frequency remains a major influence of prediction quality, and
we also identify more robust parameter settings suitable for applications with many infrequent
items.
1. Introduction
Selectional preferences or selectional constraints describe knowledge about possible
and plausible fillers for a predicate?s argument positions. They model the fact that there
is often a semantically coherent set of concepts that can fill a given argument posi-
tion. Selectional preferences can help for many text analysis tasks which involve com-
paring different attachment decisions. Examples include syntactic disambiguation
(Hindle and Rooth 1993; Toutanova et al 2005), word sense disambiguation (WSD,
? Department of Linguistics, Calhoun Hall 512, 1 University Station B 5100, Austin, TX 78712.
E-mail: katrin.erk@mail.utexas.edu.
?? E-mail: pado@cl.uni-heidelberg.de.
? E-mail: ulrike.pado@vico-research.com.
Submission received: 26 November 2009; revised submission received: 6 May 2010; accepted for publication:
29 June 2010. Part of the work reported in this article was done while S. P. was a postdoctoral scholar and U. P.
a visiting scholar at Stanford University.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 4
McCarthy and Carroll 2003), semantic role labeling (SRL, Gildea and Jurafsky 2002), and
characterizing the conditions under which entailment holds between two predicates
(Zanzotto, Pennacchiotti, and Pazienza 2006; Pantel et al 2007). Furthermore, selec-
tional preferences are also helpful for determining linguistic properties of predicates
and predicate?argument combinations, for example in compositionality assessment
(McCarthy, Venkatapathy, and Joshi 2007) or the detection of diathesis alternations
(McCarthy 2000). In psycholinguistics, selectional preferences predict human plausibil-
ity judgments for predicate?argument combinations (Resnik 1996) and effects in human
sentence reading times (Pad?, Crocker, and Keller 2009).
All these applications rely on the availability of broad-coverage, reliable selectional
preferences for predicates and their argument positions. Given the immense effort nec-
essary for manual semantic lexicon building and its associated reliability problems (see,
e.g., Briscoe and Boguraev 1989), all contemporary models of selectional preferences
acquire selectional preferences automatically from large corpora.
The simplest strategy is to extract triples (v, r, a) of a predicate, role, and argument
headword (or filler) from a corpus, and then to compute selectional preference as
relative frequencies. However, due to the Zipfian nature of word frequencies, the first
step on its own results in a very sparse list of headwords, in particular for less frequent
predicates. As an example, the verb anglicize only appears with nine direct objects in
the 100-million word British National Corpus (BNC, Burnard 1995). Only one of them,
name, appears more than once. Many highly plausible fillers are missing from the list,
such as word or spelling.
In order to make sensible predictions for triples that are unseen at training time,
it is crucial to add a generalization step that infers a degree of preference for new,
unseen headwords for a given predicate and role.1 The result is, in the ideal case, an
assignment to every possible headword of some degree of compatibility (or plausibil-
ity) with the predicate?s preferences. In the case of anglicize, the desired result would be
a high plausibility for words like the (previously seen) wordlist and surname as well as
the (unseen) word and spelling, and a low plausibility for (likewise unseen) words like
cow and machine.
The predominant approach to generalizing over headwords, first introduced by
Resnik (1996), is based on semantic hierarchies such as WordNet (Miller et al 1990). The
idea is to map all observed headwords onto synsets, and then generalize to a characteri-
zation of the selectional preference in terms of the WordNet noun hierarchy. This can be
achieved in many different ways (Abe and Li 1996; Resnik 1996; Ciaramita and Johnson
2000; Clark and Weir 2001). The performance of these models relies on the coverage
of the lexical resources, which can be a problem even for English (Gildea and Jurafsky
2002). An alternative approach to generalization uses co-occurrence information, either
in the form of distributional models or through a clustering approach. These models,
which avoid dependence on lexical resources, use corpus data for generalization
(Dagan, Lee, and Pereira 1999; Rooth et al 1999; Bergsma, Lin, and Goebel 2008).
In this article, we present a lightweight model for the acquisition and representa-
tion of selectional preferences. Our model is fully distributional and does not require
any knowledge sources beyond a large corpus where subjects and objects can be iden-
tified with reasonable accuracy. Its key point is to use vector space similarity (Lund
and Burgess 1996; Laundauer and Dumais 1997) to generalize from seen to unseen
1 Some approaches also fix a role and headword list and generalize from seen predicates to other, similar
predicates.
724
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
headwords. The vector space representations which serve as a basis for computing
similarity can in principle be computed from any arbitrary corpus, given that it is large
enough. In particular, this need not be the same corpus as the one on which we observe
predicate?headword co-occurrences. Our model thus distinguishes between a primary
corpus, from which the predicate?role?headword triples are extracted, and a generali-
zation corpus for computing the vector space representations. This distinction makes
it possible to apply our model to primary corpora with rich information that are too
small for efficient generalization, such as domain-specific corpora or corpora with
deeper linguistic analysis, as long as a larger, even if potentially noisier, generalization
corpus is available. We empirically demonstrate the benefit of this distinction. We use
FrameNet (Fillmore, Johnson, and Petruck 2003) as primary corpus and the BNC as
generalization corpus, modeling selectional preferences for semantic roles with near-
perfect coverage and low error rate.2
We evaluate our model on two tasks. The first task is pseudo-disambiguation
(Yarowsky 1993), where the model decides which of two randomly chosen words is a
better filler for the given argument position. This task tests model properties that are
needed for concrete semantic analysis tasks, most notably word sense disambiguation,
but also for semantic role labeling. The second task is the prediction of human
plausibility ratings, which is a standard task-independent benchmark for the quality
of selectional preferences. We test our model across a range of parameter settings to
identify best-practice values and show that it robustly outperforms both WordNet-
based and other distributional models on both tasks.
Finally, we investigate inverse preferences, that is, preferences that arguments
have for their predicates. Although there is ample cognitive evidence for the existence
of such preferences (e.g., McRae et al 2005), to our knowledge, they have not been in-
vestigated systematically in linguistics. However, statistics about inverse preferences
have been used implicitly in computational linguistics (e.g., Hindle 1990; Rooth et al
1999). We investigate the properties of inverse selectional preferences in comparison to
regular selectional preferences, and show that it is possible to predict inverse prefer-
ences with our selectional preference model as well.
The model that we discuss in this article, EPP, was first introduced in Erk (2007)
(using a pseudo-disambiguation task for evaluation) and further studied by Pad?, Pad?,
and Erk (2007) (evaluating against human plausibility judgments). In the current text,
we perform a more extensive evaluation and analysis, including the new evaluation on
inverse preferences, and we introduce a new similarity measure, nGCM, which achieves
excellent performance in many settings.
2. Computational Models of Selectional Preferences
In this section, we provide an overview of corpus-based models of selectional prefer-
ences. See Table 1 for a summary of the notation that we use.
2 As descriptions of semantic classes of participants in events, selectional preferences are most naturally
applied to semantic argument positions, that is, semantic roles (such as agent or patient). In contrast,
syntactic argument positions (like subject and object) can comprise several semantic argument positions,
due to the presence of diathesis alternations, and thus show less consistent selectional preferences.
Nevertheless, work in computational linguistics also makes use of selectional preferences for syntactic
argument positions, considering them noisy approximations of semantic argument positions.
725
Computational Linguistics Volume 36, Number 4
Table 1
Notation used throughout the article.
w ? Lemmas Word. We assume lemmatization throughout.
v ? Preds Predicate. Preds may be a subset of Lemmas, or a set of
semantic classes.
r ? Roles Role/Argument slot. Roles may be a set of grammatical
functions, or of semantic roles.
a ? Args ? Lemmas (Potential) argument headword.
c ? C Semantic class on which selectional preferences are
conditioned, for example, WordNet sense, FrameNet frame,
or latent semantic class.
VS = (DTrans,
Basis, sim, STrans)
Vector space. Basis is a set of basis elements, sima similarity
measure, DTrans a transformation of raw counts, and STrans
a transformation of the space.
We write w = ?wb1 , . . . , wbn? for the representation of w ?
Lemmas in a vector space with Basis = {b1, . . . , bn}.
wtr,v(a) Weight of argument headword a for predicate v and role r.
2.1 Historical Models
In formal linguistics, selectional restrictions were employed as strict Boolean restrictions
by Katz and Postal (Katz and Fodor 1963; Katz and Postal 1964) as input to a mutual dis-
ambiguation process between predicates and their modifiers. Sentences are semantically
anomalous if there are no mutually consistent readings for the two words. Semantically
anomalous sentences would receive no reading, whereas ambiguous sentences would
receive several readings.
The strict dismissal as meaningless of sentences that violate selectional restrictions
was later criticized. A case in point is metaphors, which often combine predicates and
arguments from different domains (Lakoff and Johnson 1980). Wilks (1975:329) stated
that ?rejecting utterances is just what humans do not. They try to understand them.?
He proposes to reconceptualize selectional restrictions as preferences whose violation
is dispreferred, but not fatal. His proposal for a semantic interpretation mechanism still
uses semantic primitives, but always produces a single most plausible interpretation by
choosing the senses of each word that maximize the compatibility between selectional
preferences and semantic types. In this manner, he is able to compute semantic repre-
sentations for sentences that violate selectional restrictions, including metaphors such
as ?my car drinks gasoline.?
2.2 Semantic Hierarchy?Based Models
The first broad-coverage computational model of selectional preferences, and still one
of the best-known ones, namely that of Resnik (1996), belongs to the class of semantic
hierarchy?based models. These models generalize over observed headwords using a
semantic hierarchy or ontology for nouns. The two main advantages of such models are
that (a) they can make predictions for all words covered by the hierarchy, even for very
infrequent ones for which distributional representations tend to be unreliable; and (b)
the hierarchy robustly guides generalization even for few observed headwords.
Resnik?s model instantiates the set of relations Roles with grammatical functions
which can be observed in syntactically analyzed corpora. More specifically, it concen-
726
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
trates on selectional preferences for subjects and objects. For the generalization step,
Resnik?s model maps all headwords onto WordNet synsets (or classes) c. Resnik first
computes the overall selectional preference strength for each verb?relation pair (v, r),
that is, the degree to which the pair constrains possible fillers. To estimate this quantity,
the distribution of WordNet synsets for this particular verb?relation pair is compared
to the distribution of synsets over all verbs, given the relation r. Technically, this is
achieved using Kullback?Leibler divergence:
SelStr(v, r) = D(P(c|v, r)||P(c|r)) =
?
c?C
P(c|v, r)log(P(c|v, r)
P(c|r) ) (1)
The parameters P(c|v, r) and P(c|r) are estimated from the corpus frequencies of tuples
(v, r, a) and the membership of nouns a in WordNet classes c: The observed frequency
of (v, r, a) is split equally among all WordNet classes for a. This avoids word sense
disambiguration, but incurs a certain share of wrong attributions. The intuition of
SelStr(v, r) is that a verb?relation pair that only allows a limited range of argument heads
will have a posterior distribution over classes that strongly diverges from the prior.
Next, the selectional association of the triple, SelAssoc(v, r, c), is computed as the
ratio of the selectional preference strength for this particular class c to the overall selec-
tional preference strength of the verb?relation pair (v, r). This is shown in Equation (2).
SelAssoc(v, r, c) =
P(c|v, r)logP(c|v,r)P(c|r)
SelStr(v, r)
(2)
Finally, the selectional preference between a verb, a relation, and an argument head
is defined as the maximal selectional association of the verb, the relation, and any
WordNet class c that the argument can instantiate. We will refer to this model as
RESNIK herein.
In subsequent years, a number of WordNet-based models were developed that
differ from Resnik?s model in the details of how the generalization in the WordNet
hierarchy is performed. Abe and Li (1996) characterize selectional preferences by a
tree cut through the WordNet noun hierarchy that minimizes tree cut length while
maximizing accuracy of prediction. Clark and Weir (2001) perform generalization by
ascending the WordNet noun hierarchy as long as the degree of selectional preference
among siblings is not significantly different. Ciaramita and Johnson (2000) encode
WordNet in a Bayesian Network to take advantage of the Bayes nets? ability to ?ex-
plain away? ambiguity. Grishman and Sterling (1992) perform generalization on the
basis of a manually constructed semantic hierarchy specifically developed on the same
corpus.
2.3 Distributional Models
Distributional models do not make use of any lexicon resource for the generalization
step. Instead, they use word co-occurrence?typically obtained from the same corpus
as the observed headwords?for generalization. This independence from manually
constructed resources gives distributional models a good cost?benefit ratio and makes
them especially attractive for domain-specific applications. These models, like the
727
Computational Linguistics Volume 36, Number 4
semantic hierarchy?based models, usually use grammatical functions as the set Roles
for which selectional preferences are predicted.
Pereira, Tishby, and Lee (1993) and Rooth et al (1999) generalize by discovering
latent classes of noun?verb pairs with soft clustering. They model the probability of
a word a as the argument of a predicate v as the probability of generating v and a
independently from the latent classes c:
P(v, a) =
?
c?C
P(c, v, a) =
?
c?C
P(c)P(v|c)P(a|c) (3)
Pereira, Tishby, and Lee (1993) develop a task-specific procedure to optimize P(c),
P(v|c), and P(a|c). Their procedure supports hierarchical clustering and can optimize
the number of clusters. Rooth et al (1999) present a simpler Expectation Maximization?
based estimation procedure which takes the number of clusters as input parameter. We
refer to this model as ROOTH ET AL. herein.
Dagan, Lee, and Pereira (1999) introduce a general model for computing co-
occurrence probabilities with similarity-based smoothing. Although not intended as a
model of selectional preferences, it can also be interpreted as such. Given a similarity
measure sim defined on word pairs, they compute the smoothed occurrence probability
of a word w2 given w1 as
Psim(w2|w1) =
?
w?Simset(w1)
sim(w1, w)
Z(w1)
P(w2|w) (4)
where Simset(w) is the set of words most similar to w according to sim, and Z(w1) =
?
w?Simset(w1) sim(w1, w) is a normalizing factor. This model predicts w2 given w1 by
backing off from w1 to words w similar to w1. The contribution of each w in predicting
P(w2|w1) is weighted by sim(w1, w). The similarity sim(w1, w) is computed on vector
space representations.
Recently, Bergsma, Lin, and Goebel (2008) have adopted a discriminative ap-
proach to the prediction of selectional preferences. The features they use are mainly co-
occurrence statistics, enriched with morphological context features to alleviate sparse
data problems for low-frequency argument heads. They train one SVM per verb?
argument position pair, using unobserved verb?argument combinations as negative
examples, which makes their approach independent of manually annotated training
data. Schulte im Walde et al (2008) present a model that combines features of the
semantic hierarchy?based and the distributional approaches by integrating WordNet
into an EM-based clustering model; Schulte im Walde (2010) shows that integrating
noun?modifier relations improves the prediction of human plausibility judgments.
2.4 Semantic Role?Based Models
The third class of models takes advantage of semantic resources beyond simple seman-
tic hierarchies, notably of corpora with semantic role annotation. Such corpora allow the
prediction of selectional preferences for semantic roles rather than grammatical func-
tions. From a linguistic perspective, semantic roles represent a more appropriate level
for defining selectional preferences. For that reason, the role annotation provides cleaner
and more specific training data than even a manually syntactically annotated corpus
728
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
would. These advantages, however, come at the cost of considerably greater sparsity
issues.
Pad?, Crocker, and Keller (2009) present a model based on FrameNet (Fillmore,
Johnson, and Petruck 2003). This model estimates selectional preferences with a gen-
erative probability model that equates the plausibility of a (v, r, a) triple with the joint
probability of observing the thematic role r, the verb v, and the argument a, plus the
verb?s FrameNet sense c and the grammatical function gf of the argument. This joint
probability can be decomposed using the chain rule:
P(v, c, r, gf , a) = P(v)P(c|v)P(r|v, c)P(gf |r, v, c)P(a|gf , r, v, c) (5)
The model does not make any independence assumptions. To counteract sparse data
issues for the more complex terms, the model applies WordNet-based generalization
(for nouns), distributional clustering (for verbs), and Good?Turing smoothing. We refer
to this model as PADO ET AL. Another semantic role?based model was proposed by
Vandekerckhove, Sandra, and Daelemans (2009). It acquires selectional preferences for
PropBank roles from a PropBank-labeled corpus, generalizing to unseen headwords
with memory-based learning.
3. A Distributional Exemplar-Based Model of Selectional Preferences: EPP
We now present the EPP model of selectional preferences. It falls into the category of
distributional models. More specifically, it is an exemplar model that remembers all
seen headwords for a given argument position and computes the degree of plausibility
for a new headword candidate through its similarity to the stored exemplars. Exemplars
are modeled as vectors in a semantic space.
Exemplar models are a well-known modeling framework that is used in psychol-
ogy (Nosofsky 1986), in computational linguistics (under the name of memory-based
learning [Daelemans and van der Bosch 2005]), and in linguistics, particularly phonet-
ics (Hay, Nolan, and Drager 2006). The appeal of exemplar models is that they provide
a cognitively plausible process of learning as storing exemplars, and categorization as
similarity computation that is grounded in features of the exemplars (e.g., formants in
phonetics, and contexts in lexical semantics).
The representation of selectional preferences through feature vectors also fits in well
with work in psycholinguistics by McRae, Ferretti, and Amyote (1997), who studied the
characterization of verb selectional preferences through features elicited from human
subjects. They found high overlap between features used to characterize the selectional
preferences on the one hand, and features listed for typical role fillers on the other hand.
For example, features generated for the agent role of frighten include mean, scary, and
ugly, features that were also highly relevant for the typical filler noun monster.
As briefly mentioned in Section 1, we consider selectional preferences to be charac-
terizations of typical fillers for the semantic roles of a predicate. Still, we keep our model
modular to different notions of argumenthood, such that it is also applicable to the
computation of selectional preferences for syntactic dependents of a predicate, as this is
an important case for computational applications. When we compute selectional prefer-
ences for syntactic dependents rather than semantic roles, we view syntactic argument
positions as noisy approximations of semantic roles.
729
Computational Linguistics Volume 36, Number 4
3.1 The Model
As stated previously, we assume that we have two corpora which assume different func-
tions in the model: the primary corpus, which provides information about predicate?
argument co-occurrences but may be too sparse for generalization; and the large, but
potentially noisy, generalization corpus, from which we obtain reliable semantic simi-
larity estimates.
Thus, the first step is the extraction of triples (v, r, a) of a predicate v ? Preds, a
relation r ? Roles, and a headword a ? Args from the primary corpus. Let Seenargs(r, v)
be the set of argument headwords seen with an argument position r of a predicate v
in the primary corpus. Given these triples, we predict the plausibility for an arbitrary
noun a0 in position (v, r) through the semantic similarity of a0 to all the members
of Seenargs(r, v). We obtain these similarity ratings by first computing vector space
representations for both and the members of seen(r, v) from the generalization corpus,
and then using a standard vector space similarity measure. We compute the plausibility
for a0 as
SelprefEPPr,v(a0) =
?
a?Seenargs(r,v)
wtr,v(a)
Zr,v
? sim(a0, a) (6)
where sim(a0, a) is the similarity between the vector space representations of a0 and
a, wtr,v(a) a weight for the seen headword a, and Zr,v a normalization constant, Zr,v =
?
a?Seenargs(r,v) wtr,v(a), so that the number of observed exemplars for each (v, r) pair does
not matter. Because SelprefEPP is basically a weighted average over similarity values, the
range of SelprefEPP is identical to the range of the employed similarity function sim. For
example, the range is [?1, 1] for cosine similarity, or [0, 1] for the Jaccard coefficient (cf.
Section 3.3). We discuss possible choices of both the similarity sim and the weight wtr,v
in Section 3.3.
3.2 Vector Space Representations
We use vector space representations for generalization. In a vector space model, each
target word is represented as a vector, typically constructed from co-occurrence counts
with context words in a large corpus (the so-called basis elements). The underlying
assumption, which goes back to Firth (1957) and Harris (1968), is that words with similar
meanings occur in similar contexts and will be assigned similar vectors. Thus, the
distance between the vectors of two target words, as given by some distance measure
(e.g., Cosine or Jaccard), reflects their semantic similarity.
Vector space models are simple to construct, and the semantic similarity they pro-
vide has found a wide range of applications. Examples in NLP include information
retrieval (Salton, Wong, and Yang 1975), automatic thesaurus extraction (Grefenstette
1994), and predominant sense identification (McCarthy et al 2004). Lexical resources
based on distributional similarity (e.g., Lin [1998]?s thesaurus) are used in a wide range
of applications that profit from knowledge about word similarity. In cognitive science,
they have been used, for example, to account for the influence of context on human
lexical processing (McDonald and Brew 2004) and lexical priming (Lowe and McDonald
2000).
An idealized example for a semantic space representation of selectional preferences
is shown in Figure 1(a). The two ellipses represent the exemplar clouds formed by the
730
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
Figure 1
An idealized vector space for the plausibilities of (shoot, agent, hunter) and (shoot, patient, hunter).
fillers of the agent and patient position of shoot, respectively. In order to judge whether a
hunter is a plausible agent of shoot, the vector space representation of hunter is compared
to the members of the exemplar cloud for the agent position?namely, poacher, policeman,
and director. Due to the high average similarity of the hunter vector to these vectors,
hunter will be judged a fairly good agent of shoot. Compare this with the result for the
patient role: hunter is rather distant from roe, deer, and buck, and is therefore predicted
to be a bad patient of shoot. However, note that hunter is still more plausible as a patient
of shoot than, for example, director.
3.3 Formalization and Parameter Choice
Vector space models have been formalized by Lowe (2001) as tuples VS = (DTrans,
Basis, sim, STrans), where Basis is a set of basis elements or dimensions, DTrans is a
transformation of raw co-occurrence counts, sim is a similarity measure, and STrans is
a transformation of the whole space, typically dimensionality reduction. An additional
parameter that becomes relevant for our use of vector spaces (cf. Equation [6]) is the
weighting function wt that determines the contribution of each exemplar to the overall
similarity. We discuss the parameters in turn and discuss our reasons for either explor-
ing them or fixing them.
Basis elements Basis. Traditionally, context words are used as basis elements, and co-
occurrence is defined in terms of a surface window. Such bag-of-words spaces tend to
group words by topics. They ignore the syntactic relation between context items and
the target, which is a problem for selectional preference modeling. The top table in
Figure 1(b) illustrates the problem: deer and hunter receive identical vectors, even though
they show complementary plausibility ratings. The reason is that deer and hunter often
co-occur in similar lexical bag-of-words contexts (namely, hunting-related activities).
The bottom table in Figure 1(b) indicates a way out of this problem, namely the use
of word-relation pairs as basis elements (Grefenstette 1994; Pad? and Lapata 2007).
This space splits the co-occurrences with context words such as shoot based on the
grammatical relation between target and context word, and this split looks different
for different words: whereas deer occurs exclusively as the object of shoot, hunter pre-
dominantly occurs as the subject. We find the reverse pattern for escape. In consequence,
731
Computational Linguistics Volume 36, Number 4
Table 2
Similarity measures explored in this article. Notation: We assume Basis = {b1, . . . , bn}. We write I
for mutual information, and BE(a) for the set of basis elements that co-occur at least once with a.
simLin(a, a?) =
?
(r,v)?BE(a)?BE(a? ) I(a,r,v)+I(a
?,r,v)
?
(r,v)?BE(a) I(a,r,v)
?
(r,v)?BE(a? ) I(a
? ,r,v) simcosine(a, a
?) =
?n
i=1 abi
?a?bi
||a||?||a?||
simDice(a, a?) =
2?|BE(a)?BE(a? )|
|BE(a)|+|BE(a? )| simJaccard(a, a
?) = |BE(a)?BE(a
? )|
|BE(a)?BE(a? )|
simnGCM(a, a?) = exp
(
?
?
?n
i=1 (
abi
||a|| ?
a?bi
||a?||
)2
)
where ||a|| =
?
?n
i=1 a
2
bi
simHindle(a, a?) =
?n
i=1 simHindle(a, a
?, i) where
simHindle(a, a?, i) =
{
min(I(a,bi ),I(a
?,bi )) if I(a, bi) > 0 and I(a?, bi) > 0
abs(max(I(a,bi ),I(a
? ,bi ))) if I(a, bi) < 0 and I(a?, bi) < 0
0 else
the resulting spaces gain the ability to distinguish between words like hunter and deer,
based on differences in typical occurrences in argument positions.
On the downside, dependency-based spaces are more expensive to compute than
word-based spaces because they require a corpus with syntactic analysis. Thus, we
explore both options. The word-based space records co-occurrences within a surface
window of 10 (lemmatized) words.3 We refer to it as WORDSPACE. The dependency-
based space, called DEPSPACE, has basis elements consisting of a grammatical function
concatenated with a word, as in the bottom example in Figure 1(b) (Pad? and Lapata
2007). Following earlier experiments on the representation of selectional preferences
in word-dependency-relation spaces (Pad?, Pad?, and Erk 2007), we use a subject?
object context specification that only considers co-occurrences between verbs and their
subjects and direct objects.4 In each case, we adopt the 2,000 most frequent context items
as basis elements.
Similarity measure sim. In principle, any similarity measure for vectors can be plugged
into our model. Previous studies that compared similarity measures came to various
conclusions about the usefulness of different measures. Cosine similarity is very popu-
lar in Information Retrieval. Lee (1999) obtains good results for the Jaccard coefficient
in pseudo-disambiguation. In the synonymy prediction task of Curran (2004), Dice
emerged in first place. Pad? and Lapata (2007) found good results with Lin?s measure
for predominant word sense identification.
Because it is unclear whether the findings about best similarity measures general-
ize to new tasks, we will investigate a range of similarity measures shown in Table 2:
Cosine, the Dice and Jaccard coefficients, Hindle?s (1990) and Lin?s (1998) mutual
information-based metrics, and an adaptation of Nosofsky?s (1986) Generalized Context
Model (GCM), a model for exemplar-based similarity from psychology. The original
GCM includes normalization by summed similarity over all classes of exemplars, which
introduces competition between categories. Our version, which we call nGCM, instead
normalizes by vector length to alleviate the influence of overall target frequency, but
3 We do not remove stop words for reasons of simplicity, as there is no unequivocal definition of this set,
and we do not wish to remove potentially informative contexts.
4 This context specification is available as soonly in the DependencyVectors software package
(http://www.nlpado.de/?sebastian/dv.html) starting from Release 2.5.
732
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
preserves the central idea that similarity decreases exponentially with distance (Shepard
1987).
All similarity measures from Table 2 are applicable to semantic spaces with arbitrary
basis elements, with the exception of the Lin measure, whose definition applies only
to dependency-based spaces. The reason is that it decomposes the basis elements into
relation?word pairs (r, v). For semantic spaces with words as basis elements, the Lin
measure can be adapted by omitting the random variable r (cf. Pad? and Lapata 2007).
Transformations DTrans and STrans. Next, we come to transformations on counts and vec-
tor spaces. Concerning the count transformations DTrans, all counts are log-likelihood
transformed (Dunning 1993), a standard procedure for word-based semantic space
models which alleviates the problematic effects of the Zipfian distribution of lexical
items, as proposed by Lowe (2001). As for transformations on the complete space STrans,
many studies do not perform dimensionality reduction at all. Others, like the LSA fam-
ily of vector spaces (Landauer and Dumais 1997), regard it as a crucial ingredient. To
gauge the impact of STrans, we compare unreduced spaces (2,000 dimensions) to 500-
dimensional spaces created using Principal Component Analysis (PCA), a standard
method for dimensionality reduction that identifies the directions of highest variance
in a high-dimensional space.
Weight functions wt. Exemplar-based models are usually applied in conjunction with a
function that can assign each exemplar an individual weight, which can be interpreted
cognitively as degree of activation (Nosofsky 1986). We assess a small number of
weight functions to investigate their importance within the EPP model. The first one,
UNI, assumes a uniform distribution, wtr,v(a) = 1. The second one, FREQ, uses the co-
occurrence frequency as weight, wtr,v(a) = freq(a, r, v), with the intuition that more fre-
quent exemplars should be both more activated and more reliable. Finally, we consider
a weight function that is an analogue of inverse document frequency in Information
Retrieval. It weights words higher that occur with a smaller number of verb?role pairs:
wtr,v(a) = log
|
?
a? Seenrv (a
? )|
|Seenrv (a)| , where we write Seenrv(a) for the set of verb?role pairs (r, v)
for which a occurs as a headword.5 We abbreviate this weight function by DISCR for
?discrimination?.
3.4 Discussion
Our EPP model can be seen as a straightforward implementation of the intuition to
model selectional preference by generalizing from seen headwords to other, similar,
words. We use vector space representations to judge the similarity of words, obtaining
a completely corpus-driven model that does not require any additional resources and is
very flexible. A complementary view on this model is as a generalization of traditional
vector space models that represent semantic similarities between pairs of words. The
EPP model goes beyond this by computing similarity between a vector and a set of other
vectors. By instantiating the set with the vectors for seen headwords of some relation r,
the similarity turns into a plausibility prediction that is specific to this relation.
Like other distributional models, the EPP model is applicable whenever corpus
data are available; no lexical resource is required. Additionally, it does not require the
headword observation step and the generalization step (cf. Section 1) to use the same
5 By keeping the constant |
?
a? Seenrv(a
? )|, we guarantee that the fraction remains larger than one, and
wtr,v(a) remains positive. This is to ensure that the weighted average in Equation (6) yields correct results.
733
Computational Linguistics Volume 36, Number 4
corpus.6 This allows us to work with a relatively small and deeply linguistically ana-
lyzed corpus of seen headwords, the FrameNet corpus, while using a much bigger data
set to generalize over seen headwords. It also allows us to make predictions for the
potentially deeper relations annotated in the primary corpus, for example, semantic
roles. We will investigate the potential of this setup in our Experiments 1 and 2.
As a distributional model, EPP avoids the two pitfalls of resource-based models.
One is a coverage problem due to the limited size of the resource (see the task-based
evaluation in Gildea and Jurafsky [2002]). For example, the semantic role?based PADO
ET AL. model resorts to class-based smoothing methods to improve coverage, which
EPP does not need. The other problem of resource-based models is that the shape of the
WordNet hierarchy determines the generalizations that the models make. These are not
always intuitive. For example, Resnik (1996) observes that (answer, obj, tragedy) receives
a high preference because tragedy in WordNet is a type of written communication, which
is a preferred argument class of answer.
The ROOTH ET AL. model (Rooth et al 1999) shares the resource independence of
EPP, but has complementary benefits and problems. Querying the probabilistic ROOTH
ET AL. model takes only constant time, whereas querying the exemplar-based EPP
model takes time linear in the number of seen arguments for the argument position.
However, the ROOTH ET AL. model requires a dedicated training phase with a space
complexity linear in the total number of verbs and nouns, which can lead to practical
problems for large corpora (cf. Section 5.1). The separation of similarity computation
and headword observation in EPP also gives the experimenter more fine-grained control
over the types and sources of information in the model.
The EPP model looks superficially similar to the model of Dagan, Lee, and Pereira
(1999). However, they differ in the role of the similarity measure: The Dagan, Lee, and
Pereira model computes a co-occurrence probability, and it uses similarity as a weight-
ing scheme. The EPP model computes similarity (of a word to the typical fillers of an
argument position), and its weighting schemes are separate from the similarity measure.
The two models also differ in the kinds of items they consider as a basis for generaliza-
tion (or smoothing): In computing the probability of seeing a word w2 after w1, the sum
in the Dagan, Lee, and Pereira model runs over all words that are similar to w1, whereas
the sum in the EPP model runs over all words that have been seen as headwords in the
argument position in question. Given that occurrence in an argument position is a form
of co-occurrence, and similarity (in both models) is computed on the basis of vectors
derived from co-occurrence counts, one could say that the sum in the EPP model runs
over words determined by first-order co-occurrence, whereas the sum in Dagan, Lee,
and Pereira runs over words chosen through second-order co-occurrence (where w1 and
w2 are second-order co-occurring if they both tend to occur with the same words w3).
4. Design of the Experimental Evaluation
In this section, we give a high-level overview over the experiments and experimental
settings we will use subsequently. Details will be provided in the following sections.
We evaluate the EPP model in three ways: We test the prediction of verbal
selectional preference models with a pseudo-disambiguation task (Experiment 1).
Then, we address the task of predicting human verb?argument plausibility ratings
(Experiment 2). Finally, we investigate inverse selectional preferences?preferences of
6 Dagan, Lee, and Pereira (1999) could in principle do the same, but do not explore this option.
734
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
nouns for the predicates that they co-occur with?again using pseudo-disambiguation
(Experiment 3).
We compare the EPP model to models from the three model categories presented in
Section 2: RESNIK as a hierarchical model; ROOTH ET AL. as a distributional model; and
PADO ET AL. as a semantic role?based model. As both Brockmann and Lapata (2003)
and Pad? (2007) have argued, no WordNet-based model systematically outperforms
the others, and the RESNIK model shows the most consistent behavior across different
scenarios. Among the distributional models, we choose ROOTH ET AL. as a model that
performs soft clustering and thus shows a marked difference to the EPP model. To our
knowledge, this is the first comparison of all three generalization paradigms: semantic
hierarchy?based, distributional, and semantic role?based.7
As mentioned earlier, we employ two tasks to evaluate the four models: pseudo-
disambiguation and the prediction of human plausibility ratings. The pseudo-
disambiguation task (Yarowsky 1993) has become a standard evaluation measure for
selectional preference models (Dagan, Lee, and Pereira 1999; Rooth et al 1999). Given
a choice of two potential headwords, the task of a selectional preference model is to
pick the more plausible one to fill a particular argument position of a given predicate.
Pseudo-disambiguation can be viewed as a word sense disambiguation task in which
the two potential headwords together form a ?pseudo-word,? for example herb/struggle
from the original words herb and struggle. The task is to ?disambiguate? the pseudo-
word to the word that fits better in the given context. It can also be viewed as an in vitro
version of semantic role labeling and dependency parsing (depending on whether the
relations are semantic roles or grammatical functions) (Zapirain, Agirre, and M?rquez
2009). In this case, the scenario is that of a sentence containing a predicate and two
words that could potentially fill an argument position of that predicate, for example, the
predicate recommend with the potential headwords herb and struggle for the grammatical
relation of direct object. The task is to decide which of the two potential headwords is
better suited to fill the argument position.
Human plausibility ratings, on the other hand, make considerably more fine-
grained distinctions than those occurring in pseudo-disambiguation tasks. Here, mod-
els predict the exact human ratings for verb?argument?role triples. Ratings are collected
to further control carefully selected experimental items for psycholinguistic studies
(Trueswell, Tanenhaus, and Garnsey 1994; McRae, Spivey-Knowlton, and Tanenhaus
1998), or are solicited for corpus-derived triples specifically to create evaluation data for
plausibility models (Brockmann and Lapata 2003; Pad? 2007).
We contrast two different levels of semantic analysis for the predicates and argu-
ment positions. In the SEM PRIMARY setting, the predicates are FrameNet frames, each
of them potentially instantiated by multiple different verbs. The argument positions in
these settings are frame-semantic roles. This setting most closely matches the notion
of selectional preferences as characterizations of semantic arguments of an event. In
addition, we study the SYN PRIMARY setting, where predicates are verbs, and argument
positions are grammatical functions (subject and direct object). Viewing grammatical
functions as shallow approximations of semantic roles, we can expect the selectional
preference models for this setting to yield noisier estimates than in the SEM PRIMARY
setting. The two settings will differ only in the choice of primary corpus, but will use
the same generalization corpus.
7 Erk (2007) has a comparison between hierarchy-based and distributional models, but does not include a
semantic role?based model.
735
Computational Linguistics Volume 36, Number 4
Table 3 illustrates the difference between the SEM PRIMARY setting and the SYN
PRIMARY setting on an example from a pseudo-disambiguation task: The SEM PRIMARY
setting has predicates like the FrameNet frame (predicate sense) ADORNING, with the
semantic role THEME as argument position. In contrast, the SYN PRIMARY setting has
predicates that are verb lemmas, such as cause, and argument positions that are gram-
matical functions (subj). In both settings, the two potential headwords (here called
headword and confounder, to be explained in more detail in the next section) to be
distinguished in the pseudo-disambiguation task are noun lemmas.
The verb?dependency?headword tuples of the SYN PRIMARY setting yield much
more coarse-grained and noisy characterizations of selectional preferences; however,
they can be extracted from corpora with only syntactic annotation. We are therefore
able to use the 100-million word BNC (Burnard 1995) as the primary corpus for this
setting by parsing it with the Minipar dependency parser (Lin 1993). Minipar could
parse almost all of the corpus, resulting in 6,005,130 parsed sentences.
For the SEM PRIMARY setting, we require a primary corpus with role-semantic
annotation. We use the much smaller FrameNet corpus (Fillmore, Johnson, and Petruck
2003). FrameNet is a semantic lexicon for English that groups words in semantic classes
called frames and lists fine-grained semantic argument roles for each frame. Ambiguity
is expressed by membership of a word in multiple frames. Each frame is exemplified
with annotated example sentences extracted from the BNC. The FrameNet release 1.2
comprises 131,582 annotated sentences (roughly three million words). To determine
headwords of the semantic roles, the corpus was parsed using the Collins (1997) parser.
As generalization corpus, we use the Minipar-parsed BNC in both settings. The ex-
perimentation with two different primary corpora allows us to directly study the influ-
ence of the disambiguation of predicates and the semantic characterization of argument
positions on the performance of selectional preference models. Note, however, that the
comparison is complicated by differences between the two corpora: The primary corpus
for the SYN PRIMARY setting is parsed automatically, which can introduce noise in the
determination of predicates, grammatical functions, and headwords. The primary cor-
pus for the SEM PRIMARY setting is manually annotated for semantics but is parsed
automatically to determine headwords. This can introduce noise in the headwords, but
not in the determination of predicates and semantic roles. Also, the primary corpus for
the SYN PRIMARY setting is much larger than the one used in the SEM PRIMARY setting.
5. Experiment 1: Pseudo-Disambiguation
The first experiment uses a pseudo-disambiguation task to evaluate the models? perfor-
mance on modeling the plausibility of nouns as headwords of argument positions of
verbal predicates.
Table 3
Pseudo-disambiguation items for the SYN PRIMARY setting and the SEM PRIMARY setting.
Setting Predicate (v) Arg. pos. (r) Headword (a) Confounder (a?)
SYN cause subj succession island
appear subj feasibility desire
SEM ADORNING THEME illustration axe
ROPE_MANIPULATION ROPE cord literature
736
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
Require: Some corpus T: a list of triples (v, r, a) of seen predicates, roles, and arguments.
Require: Some corpus N: a list of noun lemmas, along with a function freqN : N ?  
that associates each noun n ? N with its corpus frequency.
1: Nmid = {n ? N | freqN(n) ? 30 and freqN(n) ? 3, 000}
2: We define a probability distribution pN over the n ? Nmid by pN(n) = freqN (n)?
m freqN (m)
3: conf = { } # set of headword/confounder mappings, starts empty
4: AT = {a | (v, r, a) ? T} # set of seen headwords
5: for every a in AT do
6: choose a confounder a? ? Nmid according to pN
7: conf = conf ? { a ? a? }
8: end for
9: Return: conf
Figure 2
Algorithm for choosing confounders.
5.1 Setup
Task and data. In a data set of tuples (v, r, a) of a predicate v, argument position r, and
headword a, each tuple is paired with a confounder a?. The task is to pick the original
headword by comparing the tuples (v, r, a) and (v, r, a?). Table 3 shows some examples.
We begin by collecting all triples (v, r, a) observed in the respective primary corpus.
In the SYN PRIMARY setting, this corresponds to all headwords observed in subject or
direct object position of a verbal predicate in the BNC, and in the SEM PRIMARY setting,
to all nouns observed as headword of some semantic role in a frame introduced by a
verb. From this set of triples (v, r, a) for a given primary corpus, we draw an evaluation
sample that is balanced by the corpus frequency of predicates and argument position.
As test set, we choose 100 (v, r) pairs at random, drawing 20 pairs each from five fre-
quency bands: 50?100 occurrences; 100?200 occurrences; 200?500; 500?1,000; and more
than 1,000 occurrences. For any chosen predicate?relation pair, we sample triples (v, r, a)
equally from six frequency bands of arguments a: 1?50 occurrences; 50?100; 100?200;
200?500; 500-1,000; and more than 1,000 occurrences. These evaluation samples contain
a total of 213,929 (SYN) and 65,902 (SEM) tuples.
Next, we pair each headword with a confounder sampled from the primary corpus
as described in Figure 2.8 In the literature, there have been two different approaches to
choosing confounders for pseudo-disambiguation tasks: The first approach, used by
Dagan, Lee, and Pereira (1999), chooses confounders to match the headword a in
frequency. The second approach, used in Rooth et al (1999), sets the probability that
a word is drawn as a confounder to its relative frequency. The advantage and dis-
advantage of the first approach is that it largely eliminates the frequency bias that is
a general problem of vector space-based approaches. This is an advantage in that it
allows the generalization achieved by the model to be evaluated without any distortion
from frequency bias. It is a disadvantage in that in any practical application making
use of selectional preferences, the data will not be frequency-balanced. For example,
selectional preferences could be used by a dependency parser to decide which word in
the sentence to link to a given verb via a subject edge, or selectional preferences could
8 The confounder is the same for all instances of the headword a in the evaluation sample, regardless of the
values for r and v. As confounder candidates, we only use words with between 30 and 3,000 occurrences
in the BNC, following Rooth et al (1999).
737
Computational Linguistics Volume 36, Number 4
be used by a semantic role labeler to decide which constituent is the overall best filler
for the AGENT role for a given predicate. In such cases, it does not appear warranted to
assume that the frequencies of different headword candidates are balanced. We choose
the second option for our experiments, using relative corpus frequency to approximate
the probability of encountering different headword candidates.
Training of models. As stated earlier, we evaluate all models in the SYN PRIMARY setting
and the SEM PRIMARY setting. In all experiments herein, we perform two 2-fold cross-
validations runs. In each run, we randomly split the respective (SYN or SEM) evaluation
sample into a training and a test set at the token level. Figure 3 describes the experimen-
tal procedure in pseudo-code.
The EPP, RESNIK, and PADO ET AL. models are trained on the training split of the
evaluation sample. The EPP model additionally uses the BNC as generalization corpus
in both the SYN PRIMARY setting and the SEM PRIMARY setting. This generalization
corpus is used to compute either a WORDSPACE or a DEPSPACE vector space, as
discussed in Section 3.3. For the ROOTH ET AL. model, we had to employ a frequency
Require: A set Formalisms of formalisms to test
Require: A primary corpus T: a list of triples (v, r, a) of seen predicates, argument
positions, and arguments, along with a function freqT : T ?   that associates each
triple (v, r, a) ? T with its corpus frequency
Require: A mapping conf : Lemmas ? Lemmas of headwords to confounders such that
{a | (v, r, a) ? T} ? Domain(conf )
1: eval_results = { }
2: for splitno in 1:2 do
3: # prepare two independent splits
4: half1 = { }, half2 = { } # mappings from headwords to counts
5: for each tuple t in T do
6: # decide how many occurrences of t to put in half1, half2 by drawing from the binomial
distribution
7: Sample k ? B( freqT(t), 0.5)
8: half1 = half1 ? { t ? k }, half2 = half2 ? { t ? freqT(t) ? k }
9: end for
10: splits = { (half1, half2), (half2, half1) }
11: for ( ftrain, ftest) in splits do
12: for each formalism F in Formalisms do
13: train a model mF according to formalism F using the training set defined by
the frequency function ftrain.
14: for each tuple (v, r, a) in T do
15: for i in 1:ftest(v, r, a) do
16: Evaluate the performance of mF on the tuple (v, r, a, conf (a)) and add the
result to eval_results
17: end for
18: end for
19: end for
20: end for
21: end for
22: Return: eval_results
Figure 3
Algorithm for running a pseudo-disambiguation experiment
738
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
cutoff of five in the SYN PRIMARY setting to reduce the amount of training data due to
memory limitations. The PADO ET AL. model is only used in the SEM PRIMARY setting:
FrameNet is an integral part of this model, and it cannot be used in a syntax-only
setting without major changes. For details on training, see Section 2.4. Note that no
verb classes had to be induced from the data, because the predicates v are already
instantiated by verb classes, namely, FrameNet frames (see Table 3).
Finally, we report three baselines. The first baseline, headword frequency (HW), is
very simple. It decides between the headword a and the confounder a? by comparing
the frequencies f (a) and f (a?). The second, more informed, baseline is triple frequency
(TRIPLE). It votes for a if f (v, r, a) > f (v, r, a?), and vice versa. The third baseline, a bigram
language model (LM), was constructed by training a 2-gram language model from the
large English ukWAC Web corpus (Baroni et al 2009) using the SRILM toolkit (Stolcke
2002) with default Good?Turing smoothing. We retained only verbs, nouns, adjectives,
and adverbs in order to maximize the proximity between verbs and their subjects and
objects. We defined the preference score for verb?subject triples as the probability of the
sequence av, that is, Pref (v, subj, a) = P(v|a). Conversely, the preference score for verb?
object triples was defined as the probability of the sequence va, that is, Pref (v, obj, a) =
P(a|v). Again, the model compares Pref (v, r, a) and Pref (v, r, a?) to make its decision.
Evaluation. For all models, we report two evaluation figures. One is coverage: A tuple
is covered if the model assigns some preference to both a and a?, and the preferences are
not equal. The second is error rate, which is the relative frequency, among all covered
tuples, of instances where the confounder was at least equally preferred. Both coverage
and error rate are averages over the 2 x 2 cross-validation runs in each setting.
We determine the statistical significance of differences between error rates using
bootstrap resampling (Efron and Tibshirani 1994). This procedure samples correspond-
ing model predictions with replacement from the set of predictions made by the models
to be compared and computes the difference in error rates. On the basis of n such
samples (n = 1,000), the empirical 95% confidence interval for the difference in strength
on the basis of all observed differences is computed. If the interval includes 0, the
difference is not statistically significant.
5.2 SYN PRIMARY Setting: Results
Table 4 shows the results for the SYN PRIMARY setting. The overall best error rate is
achieved by a variant of the EPP model, with the RESNIK model coming in second
(the performance difference is significant at the 0.05 level). The EPP variants also show
near-perfect coverage, whereas the RESNIK model delivers results only for 63% of the
data points. We found a very high error rate and a comparatively low coverage for
ROOTH ET AL., which most likely stems from the data pruning necessary to reduce the
training data (compare the subsequent results in the SEM PRIMARY setting). The PADO
ET AL. model was not tested in the SEM PRIMARY setting, because it requires semantic
role annotation. The HW baseline is somewhat below chance (50%), which is an effect of
our by-token sampling procedure, according to which confounders often have higher
corpus frequencies than the real arguments. The TRIPLE baseline has a better error rate
than the LM baseline, but has very low coverage. Both the RESNIK and the EPP models
outperform the baselines in terms of error rate. That they outperform the TRIPLE
baseline in terms of error rate indicates that we sometimes have confounders that have
actually been seen more often with the verb?argument pair than the headword, but that
739
Computational Linguistics Volume 36, Number 4
Table 4
SYN PRIMARY setting: Pseudo-disambiguation results for different weighting schemes.
Model Similarity Error rate (%) Coverage (%)
UNI FREQ DISCR
EPP:DEPSPACE
Cosine 32.8 30.3 31.2 98.5
Dice 49.4 48.2 47.5 97.1
nGCM 27.6 27.5 25.7 98.5
Hindle 53.7 52.3 52.8 96.6
Jaccard 49.5 48.2 47.6 97.1
Lin 35.5 34.3 33.2 98.8
EPP:DEPSPACE, PCA
Cosine 30.2 28.7 28.8 98.1
Dice 29.9 30.8 28.6 98.2
nGCM 26.4 26.4 25.6 98.1
Hindle 45.0 44.4 44.2 95.7
Jaccard 29.7 30.7 28.5 98.2
Lin 28.7 29.1 26.7 97.7
EPP:WORDSPACE
Cosine 35.3 35.8 34.0 97.4
Dice 51.0 50.7 50.3 96.0
nGCM 33.2 34.7 31.8 97.4
Hindle 52.7 52.8 52.4 96.0
Jaccard 51.8 52.0 51.3 96.0
Lin 32.0 31.8 31.4 98.2
EPP:WORDSPACE, PCA
Cosine 30.3 31.3 29.4 97.1
Dice 31.3 32.4 30.5 97.8
nGCM 30.0 30.9 29.0 97.1
Hindle 40.2 41.0 40.4 95.3
Jaccard 31.0 32.1 30.2 97.8
Lin 27.8 29.8 26.9 97.3
RESNIK 28.1 63.4
ROOTH ET AL. 58.1 61.5
PADO ET AL. ? ?
HW 60.0 100.0
TRIPLE 32.0 4.0
LM 37.0 86.0
are dissimilar from other seen headwords, which allows RESNIK and EPP to identify
them as confounders in spite of their higher co-occurrence frequency.
We now turn to a comparison of the EPP variants. The coverage of all EPP models is
very high (0.95 or higher), independent of space, similarity measure, and dimensionality
reduction. We generally observe that error rates are lower when word meaning is
represented in DEPSPACE, and when discrimination weighting is used. In DEPSPACE,
nGCM works best, yielding the overall best result with an error rate of 25.6?25.7%.
In WORDSPACE, the Lin measure shows the best error rates with an error rate of just
below 27%. These results hold both for the unreduced and the reduced spaces and are
highly significant (p ? 0.01). Hindle is clearly the worst measure at around random
performance.
740
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
The difference between UNI and DISCR is significant throughout; the difference
between FREQ and DISCR is less uniform. In DEPSPACE, the difference between the best
measure with and without PCA (nGCM in both cases) is not significant; in WORDSPACE,
the difference between the best measure with and without PCA (Lin in both cases) is
significant (p ? 0.01).
For both WORDSPACEs and DEPSPACEs without PCA, the similarity measures divide
into two distinct groups: Lin, nGCM, and Cosine on the one hand and Jaccard, Dice, and
Hindle on the other, with a significant difference in performance between the groups
(p ? 0.01). The use of dimensionality reduction through PCA improves performance
for all similarity measures, in WORDSPACE as well as DEPSPACE. The improvement is
especially marked for the Dice and Jaccard measures, which perform at the level of
a random baseline for unreduced spaces. We assume that these set intersection-based
measures benefit from the independent dimensions that PCA produces. For the simi-
larity measures with best performance, the improvement through PCA is less marked.
Thus, PCA-reduced spaces show more similar error rates across similarity measures.
After PCA, only nGCM and Lin still significantly (p ? 0.01) outperform the others
in DEPSPACE, and in WORDSPACE, Lin is the only measure that performs significantly
differently from the rest (p ? 0.01).
As arguments are sampled from six frequency bins, we can inspect the effect of
argument frequency on error rate. Figure 4 examines the performance of the EPP model
with different similarity measures and weighting schemes by argument frequency bins
(cf. the subsection Task and Data in Section 5). We find that the overall best weighting
scheme, DISCR, also works best for all except the highest argument frequency bin. In
the DEPSPACE setting (upper row), all similarity measures show a frequency bias in that
Figure 4
SYN PRIMARY setting: Error rate by argument frequency bin. Bins: 1 = 1?50; 2 = 50?100;
3 = 100?200; 4 = 200?500; 5 = 500?1,000; 6 > 1,000.
741
Computational Linguistics Volume 36, Number 4
Figure 5
SYN PRIMARY setting: Error rate by predicate frequency bin: DISCR weighting. Bins: 1 = 50?100;
2 = 100?200; 3 = 200?500; 4 = 500?1,000; 5 > 1,000.
error rate is lower for more frequent arguments, but this bias is much less pronounced
in Cosine and nGCM than in the other measures, with error rates varying between 45%
and 25% rather than 80% and 20%. (Dice and Hindle, not shown here, exhibit similar
behavior to Jaccard.) In PCA-transformed DEPSPACE (middle row), this frequency bias
largely disappears for all similarity measures. In WORDSPACE (bottom row), although
there is again a frequency bias in all similarity measures, Lin now joins Cosine and
nGCM in being much less biased than Jaccard, Dice, and Hindle. For WORDSPACE
with PCA-transformation, not shown here, the curves resemble those of DEPSPACE with
PCA-transformation.
Figure 5 examines the effect of (predicate, argument position) pair frequency
on error rate. Predicate?argument position pairs were sampled from five frequency
bins. The figure shows DISCR weighting only. In the spaces without dimensionality
reduction, there is a clear division between Cosine, nGCM, and Lin on the one hand,
and Jaccard, Dice, and Hindle on the other. In PCA spaces, all measures except for
Hindle are similar in their performance. In both DEPSPACE conditions, error rate
decreases towards the higher frequency predicate bins, although this is not so in
742
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
WORDSPACE. It seems that in the sparser DEPSPACE, models can still profit from the
additional seen headwords in the highest predicate frequency bins, whereas in the less
sparse but noisier WORDSPACE, the added noise is stronger than the added signal in
the highest predicate frequency bins. For the lowest predicate frequency bins, the best
results in WORDSPACE are better than those in DEPSPACE.
5.3 SEM PRIMARY Setting: Results
Table 5 shows the results for the SEM PRIMARY setting, where we predict head words for
pairs of a frame (predicate sense) and semantic role. In comparison to the SYN PRIMARY
setting (Table 4), error rates are lower across the board. The difference for the EPP models
is on average around 10%.
Table 5
SEM PRIMARY setting: Pseudo-disambiguation results for different weighting schemes.
Model Similarity Error rate (%) Coverage (%)
UNI FREQ DISCR
EPP:DEPSPACE
Cosine 19.8 16.9 19.0 97.1
Dice 42.3 32.4 39.5 96.3
nGCM 20.2 16.3 20.8 97.1
Hindle 48.3 46.8 47.8 93.4
Jaccard 41.5 31.5 38.5 96.3
Lin 31.1 20.2 29.0 97.7
EPP:DEPSPACE, PCA
Cosine 18.5 17.0 17.8 96.9
Dice 19.3 19.2 18.0 97.6
nGCM 16.9 15.7 16.4 96.9
Hindle 44.9 45.6 44.7 89.3
Jaccard 18.2 18.5 17.5 97.6
Lin 18.8 19.5 18.3 98.9
EPP:WORDSPACE
Cosine 24.1 20.4 23.4 93.1
Dice 23.7 24.5 22.5 89.6
nGCM 21.1 17.8 19.4 93.1
Hindle 31.8 33.1 31.8 83.1
Jaccard 24.8 26.5 24.2 89.6
Lin 22.3 18.4 21.9 92.8
EPP:WORDSPACE, PCA
Cosine 21.0 17.6 20.5 93.1
Dice 18.5 16.4 17.8 96.8
nGCM 19.7 16.4 19.3 93.1
Hindle 41.0 39.8 40.7 90.6
Jaccard 18.1 16.2 17.6 96.8
Lin 21.3 17.1 20.7 98.3
RESNIK 16.5 62.8
ROOTH ET AL. 24.9 100.0
PADO ET AL. 7.1 59.0
HW 65.0 100.0
TRIPLE 44.0 2.0
LM NA NA
743
Computational Linguistics Volume 36, Number 4
The error rate of the PADO ET AL. model, at 7%, is the best by a large margin. We
attribute this to the extensive generalization mechanisms that the model uses, which
draw on an array of lexical?semantic resources. However, with a coverage of 59%,
the model is still unable to make predictions for many of the test items. Error rates
for the RESNIK and the EPP models are comparable, at 16.5% for RESNIK and 15.7% for
the best EPP variant. The two models differ sharply in coverage, however: 62.8% for
RESNIK, consistent with the findings of Gildea and Jurafsky (2002), and between 90%
and 98% for EPP variants. The RESNIK model also profits from the presence of semantic
disambiguation in the SEM PRIMARY setting (in the SYN PRIMARY setting its error
rate was 28%), which underlines the substantial impact that properties of the training
data have on semantic hierarchy?based models of selectional preferences. ROOTH
ET AL. now has perfect coverage, affirming our assumption that the very bad results
of the ROOTH ET AL. model in the SYN PRIMARY setting were an artifact of the data
sampling necessary for that data set. Although its error rate of 24.9% is a substantial
improvement over all baselines, the EPP model achieves error rates that are up to
9 points lower at a comparable coverage. Among the baselines, HW shows that here, as
in the SYN PRIMARY setting, arguments have some tendency of having lower frequency
than the confounders. The TRIPLE baseline shows near-random performance, at very
low coverage, a result of the very small size of the corpus. Because there is no large
corpus with frame-semantic roles, nor is the annotation easily linearizable, we could
not compute a LM baseline in the SEM PRIMARY setting.
Among EPP models, the DEPSPACEs and WORDSPACEs perform comparably, with a
non-significant advantage for DEPSPACE among the best models. Overall error rates
show the same clear divide between the three high-performing similarity measures
(Cosine, nGCM, and Lin) and the three weaker ones (Dice, Jaccard, and Hindle). Di-
mensionality reduction again dramatically improves the weaker models, with Jaccard
yielding the best result for the PCA-reduced WORDSPACE.9 Whereas all best parame-
trizations in the SYN PRIMARY setting used DISCR weighting, it is now FREQ weighting
that yields the best results.
Figure 6 again analyzes the influence of argument frequency on performance by
showing the performance of different variants of the EPP model over six argument
frequency bins. The upper row shows DEPSPACE without dimensionality reduction.
Note that FREQ weighting now works especially well for the lowest argument frequency
bin, much better than DISCR and PLAIN. This is the opposite of what we saw for the
SYN PRIMARY setting in Figure 4. With DISCR and PLAIN weighting, Jaccard and Lin
again have noticeable problems with the lowest argument frequency bins?as in the SYN
PRIMARY setting?but not with FREQ weighting. With DEPSPACE and dimensionality
reduction (middle row), we get error rates of ? 26% for all settings and all frequency
bins. On the lowest frequency bin, we again see a large advantage of FREQ weighting
over the two other weighting schemes. The bottom row shows WORDSPACE without
dimensionality reduction. Note that there is much less variation in error rates across
frequency bins here than in unreduced DEPSPACE.
Figure 7 charts error rate by predicate frequency bin, showing FREQ weighting
only, as this showed the best results on this data set. The figure clearly illustrates the
divide between the top and the bottom three similarity measures in DEPSPACE, as well
as the disappearance of this divide for both PCA settings. In unreduced WORDSPACE,
9 The differences to other similarity metrics in the FREQ setting are insignificant, with the exception
of Hindle.
744
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
the divide is not as clearly visible. The figure also indicates a slight tendency for error
rates to rise for the lowest-frequency as well as the highest-frequency predicates, across
all spaces.
5.4 Discussion
The resource-based approaches that we tested, RESNIK and PADO ET AL., show superior
performance when they have coverage (which coincides with findings in other lexical
semantics tasks that supervised data, when available, always increases performance),
but showed low coverage, at most 63% (RESNIK, SYN PRIMARY setting). The EPP model
achieves near-perfect coverage at good error rates: In the SYN PRIMARY setting, the
RESNIK model achieved an error rate of 28%, and the best EPP variant was at 26%. In
the SEM PRIMARY setting, error rates were 7% for the PADO ET AL. model, 16.5% for
the RESNIK model, and 16% for the best EPP variant. Comparing the EPP and ROOTH
ET AL. models in the SEM PRIMARY setting, we find that the use of an additional gen-
eralization corpus in the EPP model seems to offset any advantages introduced by the
joint clustering of predicates and arguments.
The difference in model performance on the two primary corpora (SYN and SEM)
is striking. Even though the FrameNet corpus is smaller and a sparse data prob-
lem might be expected, models perform at considerably lower error rates in the SEM
PRIMARY setting than when the primary corpus is the larger BNC. This underscores
the point that selectional preferences belong to a predicate sense rather than a predicate
lemma, and that they describe the semantics of fillers of semantic roles rather than of
Figure 6
SEM PRIMARY setting: Error rate by argument frequency bin. Bins: 1 = 1?50; 2 = 50?100;
3 = 100?200; 4 = 200?500; 5 = 500?1,000; 6 > 1,000.
745
Computational Linguistics Volume 36, Number 4
Figure 7
SEM PRIMARY setting: Error rate by predicate frequency bin: FREQ weighting. Bins: 1 = 50?100;
2 = 100?200; 3 = 200?500; 4 = 500?1,000; 5 > 1,000.
syntactic dependents (recall that in this setting, we predict head words for pairs of a
predicate sense and semantic role). In the SEM PRIMARY setting, the data is cleaner, so
it is expected that seen headwords of an argument position will be more semantically
uniform. This has a strong influence on model performance. Another factor contributing
to the difference in performance between the two data sets may be that the primary
corpus in the SYN PRIMARY setting is parsed automatically, whereas manual annotation
is available in the FrameNet corpus. However, although this manual annotation iden-
tifies predicate senses, role headwords are still determined through automatic parsing.
The division of the training data into a primary and a secondary corpus allows us to
successfully use FrameNet as the basis for semantic space?based similarity estimates
despite the fact that this corpus alone would be too small to sustain the construction of a
robust space.
In terms of model parameters for EPP, the following patterns stand out. Cosine,
Lin, and nGCM show good performance across all spaces and parameter settings; Dice
and Jaccard work comparably only on spaces that use dimensionality reduction. The
Hindle measure is an underperformer in all conditions. With Lin, Jaccard, Dice, and
Hindle, error rates rise sharply for less frequent arguments in many spaces. Although
Cosine and nGCM also have some frequency bias, it is much less pronounced. nGCM
seems to work well with sparse data sets that are not too noisy, as evidenced by the
fact that it has the best performance among all EPP variants on both DEPSPACEs in
746
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
Table 6
Verb?argument position?noun triples with plausibility judgments on a 7-point scale (McRae
et al, 1998).
Verb Argument position Noun Plausibility
shoot agent hunter 6.9
shoot patient hunter 2.8
shoot agent deer 1.0
shoot patient deer 6.4
the SYN PRIMARY setting, as well as in all SEM conditions except reduced WORDSPACE.
The Lin measure seems to work well with noisier data: It is the best EPP model when
using WORDSPACE in the SYN PRIMARY setting. Cosine, although never showing the
top performance, is among the best models in any setting. Although dimensionality
reduction only improves the overall error rates of the best models by a few points, it
has two important consequences: First, dimensionality reduction reduces dependence
of the results on the exact similarity measure chosen, as all measures except Hindle
show nearly indistinguishable error rates on reduced spaces (Figures 5 and 7). Second,
low-frequency arguments profit by a huge margin when PCA is used (Figures 4 and 6).
Among weighting schemes, DISCR weighting seems to be most useful when the data
is sparse but somewhat noisy (as is the case in the lower argument frequency bins in
the SYN PRIMARY setting). Frequency weighting seems to work best when the data is
either not sparse (as in the highest argument frequency bin in the SYN PRIMARY setting)
or very clean but sparse (as in the lowest argument frequency bin in the SEM PRIMARY
setting). A comparison of the two vector spaces, DEPSPACE and WORDSPACE, shows
no clear winner. When the collections of seen headwords are noisier, as they are in the
SYN PRIMARY setting, DEPSPACE, with its more aggressive filtering, yields the better
results. Sets of headwords collected by predicate sense, as in the SEM PRIMARY setting,
are sparser but cleaner, and WORDSPACE shows lower error rates.
6. Experiment 2: Human Plausibility Judgments
Experimental psycholinguistics affords a second perspective on selectional preferences:
The plausibility of verb?argument pairs has been shown to have an important effect
on human sentence processing (e.g., Trueswell, Tanenhaus, and Gransey 1994; Garnsey
et al 1997; McRae, Spivey-Knowlton, and Tanenhaus 1998). In these studies, plausibility
was operationalized as the thematic fit or selectional preference between a verb and its
argument in a specific argument position. Models of human sentence processing there-
fore need selectional preference models (Pad?, Crocker, and Keller 2009). Conversely,
psycholinguistic plausibility judgments can be used to evaluate computational models
of selectional preferences.
6.1 Experimental Materials
We present evaluations on two plausibility judgment data sets used in recent studies.
747
Computational Linguistics Volume 36, Number 4
The first data set consists of 100 data points10 from McRae, Spivey-Knowlton, and
Tanenhaus (1998). Our example in Table 6, which is taken from this data set, was
elicited by asking study participants to rate the plausibility of, for example, a hunter
shooting (AGENT) or being shot (PATIENT). The data point demonstrates the McRae set?s
balanced structure: 25 verbs are paired with two argument headwords in two argument
positions each, such that each argument is highly plausible in one argument position
but implausible in the other (hunters shoot, but are seldom shot, and vice versa for deer).
The resulting distribution of ratings is thus highly bimodal. Models can only reliably
predict the human ratings in this data set if they can capture the difference between
verb argument positions as well as between individual fillers. However, because the
verb?argument pairs were created by hand and with strict requirements, many of the
arguments are infrequent in standard corpora (e.g., wimp, bellboy, or knight). When
FrameNet is used to annotate senses for the verbs, no appropriate senses are available
for 28 of the 100 verb?argument pairs, reducing the test set to 72 data points.
The second, larger data set addresses this sparseness issue. Its triples are con-
structed on the basis of corpus co-occurrences (Pad? 2007). Eighteen verbs are combined
with their three most frequent subjects and objects found in the Penn Treebank and
FrameNet corpora, respectively, up to a total of 12 arguments. Each verb?argument pair
was rated both as an agent and as a patient (i.e., both in the observed and an unobserved
argument position), which leads to a total of 24 rated triples per verb. The data set
contains ratings for 414 triples. The resulting judgments show a more even distribution
of data. With FrameNet annotation for the verbs, appropriate senses are not attested for
six verb?argument pairs, reducing the test set to 408 data points.
6.2 Setup
We evaluate the same four models as in Experiment 1: EPP, the WordNet-based RESNIK
model, the distributional ROOTH ET AL. model, and the semantic role?based PADO
ET AL. model. We again compare a SYN PRIMARY setting, where the models make pre-
dictions for pairs of a verb and a grammatical function, with a SEM PRIMARY setting,
for which the two test data sets were annotated with verb sense and semantic roles in
the FrameNet paradigm (Pad? 2007) and where models make predictions for pairs of a
frame and a semantic role. As before, the PADO ET AL. model is only tested in the SEM
PRIMARY setting.11 For the EPP model, we focus on parsed, dimensionally unreduced
spaces and DISCR weighting, following earlier results (Pad?, Pad?, and Erk 2007). We
provide results for the best WORDSPACE models from Experiment 1 for comparison.
The primary corpora for training selectional preference models were prepared as in
Experiment 1 (cf. Section 5.1). The generalization corpus for EPP was again the BNC.
For the ROOTH ET AL. model in the SYN PRIMARY setting, we again used a frequency
cutoff. We found the RESNIK model to perform better when using just a subset of the
BNC (namely, all the triples for verbs present in the test set).
6.3 Evaluation Procedure
We evaluate our models by correlating the predicted plausibility values with the human
judgments, which range between 1 and 7. Because we do not assume a priori that there
10 The original data set has 60 data points more, which were used as the development set for the PADO
ET AL. model.
11 The PADO ET AL. model now uses automatically induced verb clusters instead of FrameNet frames.
748
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
Table 7
Comparison of EPP DEPSPACE models on McRae data. Unreduced spaces, DISCR weighting.
***p < 0.001.
SEM SYN
Sim Coverage Spearman?s ? Coverage Spearman?s ?
Dice 100% 0.038 ns 98% 0.148 ns
Jaccard 100% 0.045 ns 98% 0.153 ns
Cosine 100% 0.162 ns 98% 0.197 ns
Hindle 100% 0.060 ns 98% 0.108 ns
Lin 100% 0.085 ns 98% 0.094 ns
nGCM 100% 0.154 ns 98% 0.325 ***
is a linear correlation between the two variables, we do not use Pearson?s product-
moment correlation, but instead Spearman?s ?, a non-parametric rank-order correlation
coefficient.12 Note that significance is harder to reach the smaller the number of data
points is.
In line with Experiment 1, we include a simple frequency baseline FREQ, which
predicts the plausibility of each item as its frequency in the BNC (SYN) and in FrameNet
(SEM), respectively. With regard to an upper bound, we assume that automatic models
of plausibility should not be expected to surpass the typical human agreement on the
plausibility judgment. This is roughly ? ? 0.7 for the Pado data set.
6.4 McRae Data Set: Results and Discussion
Table 7 focuses on EPP variants with unreduced DEPSPACE for the McRae data set. We
see that this data set is rather difficult to model. None of the models trained in the SEM
PRIMARY setting achieves a significant correlation.13 Apparently, the FrameNet corpus
is too small to acquire selectional preferences that generalize well to the infrequent
items that make up the McRae data set. In the SYN PRIMARY setting, the nGCM model?s
predictions reach significance.
Table 8 shows results on the McRae data set for all selectional preference models
that we are considering. For EPP, we only show nGCM as the best-performing similarity
measure from the pseudo-disambiguation task, and Cosine as a widely used vanilla
measure. The results for the SEM PRIMARY setting (left-hand side) mirror the results
for the SEM PRIMARY setting in Experiment 1: The deep PADO ET AL. model shows the
best correlation (it is the only model to predict human judgments significantly). It
overcomes the sparseness in the FrameNet corpus by using semantic verb classes that
are particularly geared towards grouping the existing verb occurrences in the way
that is most meaningful for this task. It covers about 80% of the test data. EPP has
full coverage, and although it does not make statistically significant predictions, it
shows substantially higher correlation coefficients than ROOTH ET AL. and RESNIK.
12 A second concern is the computation of significance values: The methods most widely used for the
Pearson coefficient (Student?s t-distribution, Fisher transformation) assume that the variables are
normally distributed, which is not the case in our data set. For Spearman?s ?, we use the algorithm by
Best and Roberts (1975), which does not make this assumption.
13 Significance here refers to significance of correlation with the human data, not significance of differences
between models.
749
Computational Linguistics Volume 36, Number 4
Table 8
Comparison across models on McRae data. **p < 0.01, ***p < 0.001.
SEM SYN
Model Coverage Spearman?s ? Coverage Spearman?s ?
EPP (DEPSPACE nGCM) 100% 0.154 ns 98% 0.325 ***
EPP (DEPSPACE Cosine) 100% 0.162 ns 98% 0.197 ns
RESNIK 100% ?0.041 ns 100% 0.123 ns
ROOTH ET AL. 67% 0.078 ns 48% 0.465 ***
PADO ET AL. 78% 0.415 ** ? ?
EPP (WORDSPACE Lin) 100% 0.138 ns 98% 0.062 ns
EPP (WORDSPACE nGCM) 100% 0.167 ns 98% 0.110 ns
FREQ 18% 0.087 ns 36% 0.103 ns
The DEPSPACE and WORDSPACE variants of EPP perform similarly here, and the simple
frequency baseline has very low coverage and correlation.
As the right-hand side of Table 8 shows, both ROOTH ET AL. and EPP achieve
better results in the SYN PRIMARY setting than in the SEM PRIMARY setting. The ROOTH
ET AL. model obtains a highly significant correlation. The combination of infrequent
headwords in the McRae data set and the large primary corpus brings out the benefits
that the ROOTH ET AL. model can derive from generalizing from verbs and nouns to
the latent classes via soft clustering. Unfortunately, its coverage is still quite low (48%),
and for this reason, the difference from the best EPP model is not significant.14 In the
SYN PRIMARY setting, the EPP DEPSPACE models clearly outperform the WORDSPACE
because of the DEPSPACE models? more aggressive filtering. Interestingly, RESNIK
still performs poorly in the SYN PRIMARY setting: WordNet does not make the right
generalizations to capture the selectional preferences at play in the McRae data, no
matter how much training data is available. This is underscored by an analysis of which
WordNet classes were most frequently determined as the strongest association with
the target verbs: The classes entity, person, and physical object are assigned in 60 out of
100 test cases for the McRae data (SYN PRIMARY setting), a data set where plausibility
is determined by factors much more fine-grained than animacy. (In the SEM PRIMARY
setting, the picture is similar with classes person, organism, and entity assigned in 48 out
of 72 test cases.) The frequency baseline again performs badly.
6.5 Pado Data Set: Results and Discussion
We now turn to the Pado data set. Again, we first focus on the performance of differ-
ent similarity measures in EPP using unreduced DEPSPACE (Table 9). Correlation with
human judgments is much better than for the McRae data set, and highly significant
for all SEM PRIMARY setting models and three of the SYN PRIMARY setting models. In
both settings, Cosine and Lin are the best measures (difference not significant), followed
by nGCM. Hindle comes out worst once more. The difference between the strong and
14 As in Experiment 1, we apply bootstrap resampling to determine the significance of differences between
models. This procedure also takes differences in coverage into account?specifically, a significant
difference becomes harder to achieve as the number of data points shared between the models shrinks.
750
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
Table 9
Comparison of EPP DEPSPACE parametrizations on Pad? data. Unreduced spaces, DISCR
weighting. **p < 0.01; ***p < 0.001.
SEM SYN
Sim Coverage Spearman?s ? Coverage Spearman?s ?
Dice 100% 0.289 *** 100% 0.026 ns
Jaccard 100% 0.285 *** 100% 0.023 ns
Cosine 100% 0.508 *** 100% 0.403 ***
Hindle 100% 0.160 ** 100% ?0.004 ns
Lin 100% 0.498 *** 100% 0.229 ***
nGCM 100% 0.384 *** 100% 0.156 **
weak measures is more pronounced for the SYN PRIMARY setting, compared with the
SEM PRIMARY setting. Coverage is at or close to 100% throughout.
Table 10 shows results on the Pado data set for all selectional preference models that
we consider. In the SEM PRIMARY setting (where both the data and the primary corpus
have FrameNet annotation), EPP and the deep PADO ET AL. model predict the human
judgments similarly well (difference not significant). Because all verbs in this data set
are covered by FrameNet, the PADO ET AL. model also shows a nearly perfect cover-
age. EPP and PADO ET AL. do much better than ROOTH ET AL. (differences significant
at p ? 0.01). ROOTH ET AL. has the lowest coverage at 88%, but this is still higher than
its coverage of the McRae data. As with the McRae data, ROOTH ET AL. achieves better
correlation in the SYN PRIMARY setting than the SEM PRIMARY setting, indicating that
the frequency cutoff does not harm performance as much in Experiment 2 as it did in
Experiment 1. However, the coverage of ROOTH ET AL. is lower in the SYN PRIMARY
setting, perhaps because the SEM PRIMARY setting smoothes rare verbs by grouping
them in frames with other verbs. RESNIK also achieves better correlation in the SYN
PRIMARY setting, but recall that it was trained on a subset of the BNC only to reduce
noise in the training data?when trained on the whole BNC set, performance degrades
to ? = 0.060. The difference from the best EPP model remains numerically large. As for
Table 10
Comparison across models on Pad? data. ***p < 0.001.
SEM SYN
Model Coverage Spearman?s ? Coverage Spearman?s ?
EPP (DEPSPACE Cosine) 100% 0.489 *** 98% 0.470 ***
EPP (DEPSPACE nGCM) 100% 0.393 *** 98% 0.328 ***
RESNIK 98% 0.230 *** 97% 0.317 ***
ROOTH ET AL. 88% 0.060 ns 58% 0.200 ***
PADO ET AL. 97% 0.515 *** ? ?
EPP (WORDSPACE Lin) 100% 0.254 *** 100% 0.056 ns
EPP (WORDSPACE nGCM) 100% 0.192 *** 100% 0.078 ns
FREQ 32% ?0.041 ns 69% 0.090 ns
751
Computational Linguistics Volume 36, Number 4
the McRae data set, the EPP WORDSPACE models show much worse performance than
the DEPSPACE models, and do not significantly predict the human plausibility ratings.
The frequency baseline shows a considerably better coverage for this data set, but
its correlations hover around zero, which underlines our intuition that verb?argument
combinations can be plausible without being frequent in corpora. An example is the
combination (to) embarrass (an) official, which is rated as highly plausible, but occurs
only once each in the BNC and FrameNet.
6.6 Discussion
The McRae data set seems in general more difficult to account for than the Pado data
set, as noted by Pad?, Pad?, and Erk (2007). They explain it by a general frequency effect
in the BNC data (which are a superset of the FrameNet data): The median frequency of
the hand-selected McRae nouns in the BNC is 1,356, as opposed to 8,184 for the corpus-
derived Pado nouns.
Comparing all selectional preference models, we find that the RESNIK and the
ROOTH ET AL. models generally do worse than EPP both in terms of coverage and
quality of predictions. One notable exception is the excellent performance of the ROOTH
ET AL. model on the McRae data in the SYN PRIMARY setting, which comes, however,
with a low coverage of less than 50%. A closer inspection of the predictions showed
that ROOTH ET AL. makes many predictions for verb?object pairs but abstains from
subjects, thus reducing the complexity of the task. For only 20% of verbs, predictions
are made for subjects and objects. As noted in Pad?, Pad?, and Erk (2007), the relatively
poor performance of the RESNIK model may be explained by the fact that its ability to
generalize is limited to the structure of WordNet, where some semantic distinctions are
easier to make than others. For example, a fairly easy distinction to make for WordNet-
based models is animate vs. inanimate. Because the Pado set contains a portion of
inanimate arguments with animate counterparts, the RESNIK model does well on those.
In contrast, in the McRae test set, all arguments are animates, and thus similar to one
another in terms of WordNet.
The deep PADO ET AL. model achieves the best correlation with the human judg-
ments on both data sets, but it is limited to the SEM PRIMARY setting. Although the
best model is not always among the EPP DEPSPACE models, they consistently show a
coverage of close to 100%, and are generally statistically indistinguishable from the best
model. Unlike ROOTH ET AL. and RESNIK, whose performance varies widely between
the SEM PRIMARY setting and the SYN PRIMARY setting, the correlation coefficients for
the EPP models are generally similar across settings. We take this as evidence that EPP
models can extract relevant information from deeper annotation on small corpora as
well as from large, but noisy and shallow, training data.
Finally, we consider the different similarity measures for the EPP model evaluated
on unreduced DEPSPACE. The picture differs somewhat between the two data sets, but
the Cosine measure performs well overall, with Lin and nGCM generally in second
and third place. So, the group of the three best similarity measures is the same as in
Experiment 1, but Cosine shows better performance. One possible reason for this lies
in the verb frequency, which is relatively high in both data sets: 68% of the McRae
verbs and 83% of the Pado verbs have BNC frequencies of 1,000 and more, whereas
Experiment 1 used an equal number of predicates from five frequency bins, the highest
being 1,000 and more occurrences. In that highest predicate frequency bin, Cosine
consistently performed as well as Lin or better in Experiment 1 (Figures 5 and 7).
752
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
7. Experiment 3: Inverse Selectional Preferences
The term selectional preference is typically used to describe the semantic constraints
that predicates place on their arguments. In this section, we will investigate how nominal
arguments place semantic constraints or expectations on the predicates with which they
occur. Such expectations can be thought of as typical events that involve the given
object. For example, a noun like apple could be said to have preferences about its inverse
subject position, that is, the verbs that can take it as a plausible subject. Examples might
be verbs like grow or fall; for its inverse object position, apple probably prefers verbs
like eat, cut, or plant. We will use the term inverse selectional preference to refer to
preferences of nouns for their predicates, distinguishing them from regular selectional
preferences.
It is clear that not all verbs will be equally likely to occur with a given noun?
role pair. Still, inverse selectional preferences warrant a closer look: To what extent do
inverse selectional preferences differ from regular ones? And are the tasks of predicting
regular and inverse selectional preferences equally difficult? We start in Section 7.2 with
an exploratory data analysis of inverse selectional preferences, which shows that inverse
selectional preferences show semantically coherent patterns like regular selectional
preferences, but that, in contrast to most verbs, nouns tend to occur with multiple
semantic groups of verbs. In Sections 7.3?7.5, we test the EPP model on a pseudo-
disambiguation task for inverse selectional preferences.
7.1 Related Work
In computational linguistics, some approaches to characterizing selectional preferences
have used the symmetric nature of their models to characterize nouns in terms of the
verbs that they use (Hindle 1990; Rooth et al 1999). However, they do not explicitly
compare the two types of preferences. Also, there are approaches using selectional
preference information, in particular for word sense disambiguation and related tasks,
that could be characterized as using regular along with inverse selectional preferences
(Dligach and Palmer 2008; Erk and Pad? 2008; Nastase 2008). By comparing selectional
preference model performance on the tasks of predicting inverse and regular selectional
preferences in Sections 7.3?7.5, we hope to contribute to an understanding of what can
be achieved by using inverse preferences in word sense analysis tasks.
At the same time, inverse selectional preferences have been the object of fruitful
research in both psycholinguistics and theoretical linguistics. In psycholinguistics, a
particularly plausible argument for the existence of expectations of nouns for their
predicates in human language processing is head-final word order (as in Japanese or
in German subordinate clauses), where hearers may encounter all objects before the
head. It is likely that these objects are immediately integrated into a preliminary event
structure with an assumed predicate instead of being stored in short-term memory until
the predicate is encountered (Konieczny and D?ring 2003; Nakatani and Gibson 2009).
Another strand of work is McRae et al (2001, 2005), who have studied priming of verbs
from nouns. They found that a noun engenders priming of verbs for which it is a typical
agent, patient, instrument, or location.
In theoretical linguistics, the idea of event knowledge being encoded in the lex-
ical entries of nouns has been formulated in the context of Pustejovsky?s generative
lexicon (Pustejovsky 1995), where the qualia roles TELIC and AGENTIVE provide infor-
mation about the typical use of an object (book: read) and its construction (book: write),
753
Computational Linguistics Volume 36, Number 4
respectively. Pustejovsky uses this knowledge to account, for example, for the interpre-
tation of logical metonymy (begin a book). Although qualia roles are instantiated with
individual predicates rather than characterizations of all possible events, construction
and use are arguably two very salient events for an object. Through the data exploration
in Section 7.2, we hope to contribute to a linguistic characterization of inverse selectional
preferences.
7.2 Empirical Analysis of Inverse Selectional Preferences
The first question we ask concerns the selectional preference strength of regular and
inverse selectional preferences, using the measure introduced by Resnik (1996) to de-
termine the degree to which verbs select for nouns, and vice versa. As verb?role pairs,
we re-use the same 100 pairs that were used for the pseudo-disambiguation task in
Experiment 1. For the comparison, we randomly sample a total of 100 noun/inverse-
role pairs from the BNC, using the same five frequency bands as for the verbs (50?
100, 100?200, 200?500, 500?1,000, >1,000). The sample contains approximately the same
number of (inverse) subject and object roles.
We adapt the selectional preference strength measure from Equation (1) to our
case: Unlike Resnik, we compute KL divergence not on a distribution across WordNet
synsets, but on a distribution across lemmas.
SelStr(w1, r) = D(P(w2|w1, r)||P(w2|r)) (7)
For regular selectional preferences, w1 is a verb lemma, w2 a noun lemma, and r a role.
For inverse preferences, w1 is a noun lemma, w2 a verb lemma, and r an inverse role.
SelStr(w1, r) can be interpreted as a measure of the degree to which w1 has selectional
preferences concerning the role r. We induce the probability distributions through
maximum likelihood estimation on the BNC.
We can expect to see the same overall tendency in regular and inverse selec-
tional preference strength. It is not possible that inverse selectional preference strength
would be uniform throughout if regular selectional preference strength varied between
verbs. After all, if we fix the relation r for the time being, P(v|n) and P(n|v) are re-
lated through Bayes? formula. Instead, the questions we will ask are more specific.
Are regular and inverse preference strengths similar in size? Are regular and inverse
preference strengths similar by frequency band?that is, do frequent nouns behave
similarly to frequent verbs? And what effects do we see of the prior distributions P(n|r)
and P(v|r)?
Table 11 shows the range of selectional preference strengths found in each frequency
band for verbs and nouns. As expected, we see substantial strengths in both regular
and inverse preferences. Both parts of speech show the same pattern of decreasing KL
divergences for higher-frequency words, presumably because frequent words tend to be
polysemous, and can combine with many different words. However, the strengths for
inverse selectional preferences are in general lower than those for regular preferences.
One possible reason for this is that the number of nouns seen with a verb?role
pair might differ, in general, from the number of verbs seen with each noun?role
pair. However, we find that verbs and nouns occur with roughly the same number of
associates in the frequency bands up to the 200?500 band. In the band 500?1,000, verbs
appear with roughly one third more nouns than nouns appear with verbs, and in the
band of 1,000 occurrences or more, verbs appear with twice as many nouns (on average)
754
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
Table 11
Minimal, median, and maximal selectional preference strength (measured in terms of KL
divergence) in a sample of 100 verbs and 100 nouns (20 lemmas each per frequency band).
Band Verbs Nouns
min median max min median max
50?100 4.5 7.4 8.8 3.7 4.8 6.3
100?200 3.9 5.8 7.6 2.7 3.8 5.0
200?500 3.3 5.2 6.9 2.4 3.3 4.7
500?1,000 2.4 4.4 5.9 1.9 2.9 4.1
1,000? 1.8 3.6 6.2 1.4 2.3 3.7
as nouns appear with verbs in this band (1,189 vs. 636). Incidentally, the fact that the
highest-frequency verbs (which also tend to be the most ambiguous) appear in a much
larger number of contexts than the highest-frequency nouns could be a contributing
factor to the well-known problem that verbs are harder to disambiguate than nouns.
For the lower frequency bands, number of associates is unlikely to be the reason for
the weaker inverse preferences. Instead, a more likely reason for the overall weaker
inverse preferences lies in the overall distributions of nouns and verbs in the BNC.
Both show a Zipfian distribution, but there are 15,570 verbs as opposed to 455,173
nouns. Recall that KL divergence will be high whenever the individual terms p(w2|w1,r)p(w2|r)
to be summed are large. This, in turn, is the case when p(w2|r) is small. And p(w2|r) may
be small when the distribution p(w2|r) ranges over a larger number of words w2. For
regular selectional preferences, the w2 are nouns, and for inverse preferences the w2 are
verbs. Because there are many more nouns than verbs, the denominator p(w2|r) tends to
be smaller for regular preferences.
To get a clearer understanding of how inverse selectional preferences compare to
regular selectional preferences, we next do a qualitative analysis, looking at association
strength SelAssoc for individual triples verb?role?noun and noun?inverse-role?verb.
We adapt Equation (2) to the lexicon-free case and obtain
SelAssoc(w1, r, w2) =
1
SelStr(w1, r)
P(w2|w1, r) log
P(w2|w1, r)
P(w2|r)
(8)
Table 12 shows the five strongest associates for one verb?role pair and one noun?role
pair from each frequency band. The associates on both sides of the table generally
are semantically coherent and make intuitive sense. However, there is an interesting
difference between the verbs and nouns: We find that the nouns? preferred verbs can
often be grouped loosely into several meaning clusters, whereas the verbs? associates
tend to group into one cluster per grammatical function. For example, predicates taking
wheat as objects fall into those describing production (grow, sow) and those describing
processing (shred, grind, mill). Similarly, the predicates found for pill either concern
ingestion (take, swallow, pop), prescription, or idiomatic usage. In contrast, the objects of
rebut describe different kinds of statements, and the objects of celebrate are anniversaries
and other special events. Another observation that we can make in Table 12 is that
the nouns? most preferred associates have a similarly large share in the nouns? overall
selectional preference strength as the verbs? most preferred associates have in the verbs?
selectional preference strength. This indicates that the distribution of selectional prefer-
ences is similarly skewed towards the most preferred associate for verbs and nouns.
755
Computational Linguistics Volume 36, Number 4
Table 12
Examples of regular and inverse selectional preferences from different frequency bands for
argument positions of nouns and verbs: overall selectional preference strength SelStr and most
highly associated fillers with association strengths SelAssoc.
Band Verbs Nouns
50?100
rebut?obj, SelStr(w)= 7.43 wreckage?obj?1, SelStr(w)= 5.91
presumption 0.283 survey 0.126
allegation 0.088 examine 0.089
charge 0.082 sift 0.075
criticism 0.049 clear 0.056
claim 0.041 sight 0.051
100?200
enunciate?obj, SelStr(w)= 6.89 wheat?obj?1, SelStr(w)= 5.00
principle 0.242 grow 0.184
word 0.085 shred 0.049
theory 0.034 grind 0.049
philosophy 0.034 mill 0.042
policy 0.029 sow 0.040
200?500
break_with?obj, SelStr(w)= 6.92 pill?obj?1, SelStr(w)= 4.15
tradition 0.237 take 0.290
past 0.054 swallow 0.165
precedent 0.035 sweeten 0.070
convention 0.035 prescribe 0.049
Rome 0.022 pop 0.028
500?1,000
commence?obj, SelStr(w)= 5.92 dividend?obj?1, SelStr(w)= 4.10
proceedings 0.185 pay 0.508
action 0.051 declare 0.064
work 0.043 receive 0.064
proceeding 0.041 recommend 0.054
operation 0.033 raise 0.023
1,000?
celebrate?obj, SelStr(w)= 6.23 requirement?obj?1, SelStr(w)= 3.24
anniversary 0.177 meet 0.332
birthday 0.170 satisfy 0.015
centenary 0.046 comply_with 0.093
victory 0.033 fulfill 0.061
mass 0.028 impose 0.028
In sum, we find that inverse selectional preferences have weaker overall selectional
preference strength than regular preferences, but that may be due more to specifics of
the formula used rather than the skewness towards preferred role fillers. Two differ-
ences do emerge, though. First, noun selectional preferences show more semantic filler
sets than verb preferences. Second, the highest frequency verbs appear with many more
different associates than the highest frequency nouns.
7.3 Modeling Inverse Selectional Preferences
In the rest of this section, we test selectional preference models on the task of pre-
dicting inverse selectional preferences in a pseudo-disambiguation task, and compare
the results to the performance on predicting regular preferences in Experiment 1. We
do not repeat Experiment 2 even though it would have been technically possible to
756
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
re-use the McRae and Pado data sets and predict plausibility judgments through inverse
preferences. However, the data sets combine each verb with both plausible and im-
plausible nouns, but they do not combine each noun with different verbs in a balanced
fashion, so a repetition of Experiment 2 with inverse preferences would not be very
informative.
For the pseudo-disambiguation experiment, we focus on the EPP model. Distri-
butional models can, in general, be used straightforwardly to model both regular
and inverse selectional preferences. This is different for models like RESNIK that use
the WordNet noun hierarchy to represent regular selectional preferences. To model
inverse preferences, it would be necessary to use the WordNet verb hierarchy. However,
WordNet organizes verbs in a comparatively flat, unconnected hierarchy with a high
branching factor formed by the hypernymy/troponymy (?type of?) relation. This makes
effective generalization difficult, in particular in conjunction with the marked variation
in the set of preferred predicates that we observed for inverse selectional preferences in
Section 7.2.
We adapt the formulation of the EPP model to the inverse selectional preference
case as follows. Let a stand for a noun, r for an inverse argument position of this
noun, and Seenpreds(r, a) for the set of predicates seen with noun a and role r. Then the
selectional preference SelprefEPP of (r, a) for a verb v0 is defined in parallel to Equation (6)
as weighted average similarity to seen verbs:
SelprefEPPr,a(v0) =
?
v?Seenpreds(r,a)
wtr,a(v)
Zr,a
sim(v, v0) (9)
with Zr,a =
?
v?Seenpreds(r,a) wtr,a(v) as the normalization constant.
7.4 Pseudo-Disambiguation: Experimental Setup
We evaluate inverse selectional preferences on a pseudo-disambiguation task that is
set up completely analogously to our experiments on regular preferences in Section 5:
given a noun, an inverse argument position, one verb observed in this position, and a
confounder verb, distinguish between the two verbs. We use the 100 nouns sampled
across five frequency bands that we already used in Section 7.2. We experiment with
both WORDSPACE and DEPSPACE models, but restrict our attention to DISCR weighting,
which showed good results in Experiment 1.
In Section 5, we experimented on two different primary corpora, the BNC (SYN
PRIMARY setting) and FrameNet (SEM PRIMARY setting). Subsequently, we will use the
SYN PRIMARY setting again, but not the SEM PRIMARY setting. In the SEM PRIMARY
setting, the roles are FrameNet frame elements (semantic roles). However, frame ele-
ments are specific to a single frame, for example, the frame element ROPE belongs to
the frame ROPE_MANIPULATION.15 It would thus be pointless to predict a verb frame
given a noun and a frame element name, as the frame element already gives away the
frame.
15 It is possible for multiple frame elements to share a name, for example there are multiple frames with a
frame element named THEME. However, conceptually, this is only a shared name, not a shared role across
frames.
757
Computational Linguistics Volume 36, Number 4
7.5 Pseudo-Disambiguation: Results and Discussion
Table 13 shows the results of testing the EPP model for inverse selectional preferences
on pseudo-disambiguation. Coverage is very good for all model variants, similarly to
Experiment 1. The error rates, as well, are close to those for the regular preferences in
the SYN PRIMARY setting (cf. Table 4). The best model there (DEPSPACE, PCA, nGCM
with DISCR weighting) achieved an error rate of 25.6%, and the best model for inverse
preferences (WORDSPACE, Lin with DISCR weighting) reaches an error rate of 27.2%
here. Lin shows the best error rates in all conditions, closely followed by nGCM (the
difference is significant in WORDSPACE and the reduced DEPSPACE, but not significant
in the unreduced DEPSPACE). The Hindle similarity measure again brings up the rear.
In PCA-transformed spaces, the error rates are similar across all similarity measures
except for Hindle, as in Experiment 1.
WORDSPACEs yield better results than DEPSPACEs here, in contrast to Experiment 1.
The best WORDSPACE model (Lin without PCA) reaches significantly better error rates
(p ? 0.01) than the best DEPSPACE model (Lin with PCA). We think that the reason for
this lies in the fact that for inverse selectional preferences, the true associate and the
confounder that need to be distinguished in the pseudo-disambiguation task are verbs
rather than nouns. A noun will probably have more other nouns in a bag-of-words
context window than a verb would other verbs, which will make it easier to distinguish
verbs in a WORDSPACE than to distinguish nouns. A DEPSPACE, in contrast, will bring
out differences in the immediate syntactic neighborhood of nouns even if they occur in
the same sentence.
8. Conclusion
In this article, we have presented a similarity-based model of selectional preferences,
EPP. It computes the selectional fit of a candidate role filler as a weighted sum of seman-
tic similarities to headwords observed in a corpus, in a straightforward implementation
Table 13
Pseudo-disambiguation results for inverse selectional preferences (BNC as primary and
secondary corpus, DISCR weighting). ER = Error rate; Cov = Coverage.
Dimensions Similarity DEPSPACE WORDSPACE
ER (%) Cov (%) ER (%) Cov (%)
Original
2,000 dimensions
Cosine 37.4 99.0 34.0 99.1
Dice 42.4 98.8 43.4 98.7
nGCM 33.7 99.3 31.5 99.3
Hindle 48.8 96.0 52.2 94.6
Jaccard 36.7 99.4 44.9 98.7
Lin 32.8 98.9 27.2 98.9
PCA
500 dimensions
Cosine 35.2 99.0 31.3 99.4
Dice 35.0 99.6 32.9 99.8
nGCM 32.4 99.2 30.3 99.6
Hindle 44.2 99.0 48.7 99.1
Jaccard 34.8 99.6 32.6 99.8
Lin 30.6 99.8 28.8 99.8
758
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
of the intuition that plausibility judgments should generalize to fillers with similar
meaning. Our model is simple and easy to compute. In common with other distri-
butional models like Rooth et al (1999), it does not depend on lexical resources. Our
model derives additional flexibility from distinguishing between a primary corpus (for
observing headwords) and a generalization corpus (for inducing semantic similarities).
This allows it to use primary corpora with deeper semantic annotation that are too small
as a basis for computing vector space representations.
We have evaluated the EPP model on two tasks, a pseudo-disambiguation task
that can be viewed as an abstraction of both word sense disambiguation and semantic
role labeling, as well as on the prediction of human plausibility judgments. The model
achieves similar error rates to the semantic hierarchy?based RESNIK model, at consid-
erably higher coverage, and it achieves lower error rates than the ROOTH ET AL. soft
clustering model. The semantic role?based PADO ET AL. model, although highly accu-
rate in its predictions, has much lower coverage and needs a semantically annotated
corpus as a basis. We have also demonstrated that our model is able to meaningfully
model inverse selectional preferences, that is, expectations of nouns about verbs for
which they appear as arguments.
With respect to parameter settings of the EPP model, we find consistent patterns
across the three tasks we have considered. nGCM, Lin, and Cosine are the best-
performing similarity measures throughout. The good performance of the nGCM mea-
sure, an exponential similarity measure, is particularly noteworthy. We found it to work
well on data sets that are sparse and not too noisy, whereas the Lin similarity measure
achieved better performance when the data was noisy (see Section 5.4 for details). Di-
mensionality reduction (PCA) on the vector space raises the performance of the Jaccard
and Dice similarity measures to a similar level as the best three. More importantly, PCA
neutralizes a strong frequency bias that otherwise leads to a large performance drop
on rare arguments. Concerning weighting schemes, we found that frequency-based
weighting works well when the data is either clean or not too sparse. In the face of sparse
noisy data, DISCR weighting (a variant of tf/idf) is helpful. Comparing bag-of-words?
based and dependency-based vector spaces, DEPSPACEs are sparser but cleaner than
WORDSPACEs. Accordingly, DEPSPACEs are at an advantage when many headwords are
available, making efficient use of this information, whereas WORDSPACEs work better
for predicates with few seen headwords because they are less affected by sparseness.
We conclude with two open questions. The first question concerns the appropriate
representation of selectional preferences for polysemous verbs such as address, whose
direct object can either be a person, or a problem. Polysemy leads to headwords with
lower similarity among them than for non-polysemous verbs, which in turn can lead
to artificially low plausibilities for all fillers. In the SEM PRIMARY setting, occurrences
of polysemous verbs are separated into different frames. In future work, we hope to
improve our SYN PRIMARY setting models by clustering the seen headwords, and then
computing plausibility of new headwords relative to the nearest cluster.
A second question is the usefulness of inverse selectional preferences for the ac-
quisition of fine-grained information about nouns. As we discussed in Section 7, the
preferred verbs for a noun can often be grouped into meaning clusters. In future work,
we plan to investigate whether there are groups of predicates that recur across similar
nouns, and how they can be characterized. We expect some groups to correspond to
Pustejovsky?s qualia (Pustejovsky 1995), which constitute particularly salient events
for an object, namely, their creation and typical use. However, we expect corpus data
to yield a more complex picture of the events connected to a noun, which manifest
themselves in the form of additional, more specific meaning clusters.
759
Computational Linguistics Volume 36, Number 4
Acknowledgments
We would like to thank Detlef Prescher
and Carsten Brockmann for their
implementations of the ROOTH ET AL.
and RESNIK models, respectively. We
are also grateful to Nate Chambers and
Yves Peirsman, as well as to the
anonymous reviewers, for their
comments and suggestions.
References
Abe, Naoki and Hang Li. 1996. Learning
word association norms using tree cut pair
models. In Proceedings of the 10th
International Conference on Machine
Learning, pages 3?11, Bari.
Baroni, Marco, Silvia Bernardini, Adriano
Ferraresi, and Eros Zanchetta. 2009. The
wacky wide Web: A collection of very
large linguistically processed Web-crawled
corpora. Language Resources and Evaluation,
43(3):209?222.
Bergsma, Shane, Dekang Lin, and Randy
Goebel. 2008. Discriminative learning of
selectional preference from unlabeled text.
In Proceedings of the 13th Conference on
Empirical Methods in Natural Language
Processing, pages 59?68, Honolulu, HI.
Best, John and D. E. Roberts. 1975. Algorithm
AS 89: The upper tail probabilities of
Spearman?s Rho. Applied Statistics,
24:377?379.
Briscoe, Ted and Bran Boguraev, editors.
1989. Computational Lexicography for
Natural Language Processing. Longman
Publishing Group, New York.
Brockmann, Carsten and Mirella Lapata.
2003. Evaluating and combining
approaches to selectional preference
acquisition. In Proceedings of the 16th
Meeting of the European Chapter of the
Association for Computational Linguistics,
pages 27?34, Budapest.
Burnard, Lou, 1995. User?s Guide for the
British National Corpus. British National
Corpus Consortium, Oxford University
Computing Services.
Ciaramita, Massimiliano and Mark Johnson.
2000. Explaining away ambiguity:
Learning verb selectional preference
with Bayesian networks. In Proceedings
of the 18th International Conference on
Computational Linguistics, pages 187?193,
Saarbr?cken.
Clark, Stephen and David Weir. 2001.
Class-based probability estimation using a
semantic hierarchy. In Proceedings of the
2nd Annual Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 95?102, Pittsburgh, PA.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics,
pages 16?23, Madrid.
Curran, James. 2004. From Distributional to
Semantic Similarity. Ph.D. thesis, University
of Edinburgh.
Daelemans, Walter and Antal van den
Bosch. 2005. Memory-Based Language
Processing. Cambridge University Press,
Cambridge, UK.
Dagan, Ido, Lillian Lee, and Fernando C. N.
Pereira. 1999. Similarity-based models
of word cooccurrence probabilities.
Machine Learning, 34(1):34?69.
Dligach, Dmitriy and Martha Palmer. 2008.
Novel semantic features for verb sense
disambiguation. In Proceedings of the
46th Annual Meeting of the Association for
Computational Linguistics:Human Language
Technologies, Short Papers, pages 29?32,
Columbus, OH.
Dunning, Ted. 1993. Accurate methods for
the statistics of surprise and coincidence.
Computational Linguistics, 19(1):61?74.
Efron, Bradley and Robert Tibshirani. 1994.
An Introduction to the Bootstrap.
Monographs on Statistics and Applied
Probability 57. Chapman & Hall, London.
Erk, Katrin. 2007. A simple, similarity-based
model for selectional preferences. In
Proceedings of the Annual Meeting of the
Association for Computational Linguistics,
pages 216?223, Prague.
Erk, Katrin and Sebastian Pad?. 2008.
A structured vector space model for
word meaning in context. In Proceedings
of the 13th Conference on Empirical
Methods in Natural Language Processing,
pages 897?906, Honolulu, HI.
Fillmore, Charles J., Christopher R. Johnson,
and Miriam R. L. Petruck. 2003.
Background to FrameNet. International
Journal of Lexicography, 16:235?250.
Firth, John Rupert. 1957. A synopsis of
linguistic theory, 1930?1955. In Philological
Society, editor, Studies in Linguistic
Analysis. Blackwell, Oxford, pages 1?32.
Garnsey, Susan, Neal Pearlmutter, Elizabeth
Myers, and Melanie Lotocky. 1997. The
contributions of verb bias and plausibility
to the comprehension of temporarily
ambiguous sentences. Journal of Memory
and Language, 37:58?93.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
760
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
Grefenstette, Gregory. 1994. Explorations in
Automatic Thesaurus Discovery. Kluwer
Academic Publishers, Dordrecht.
Grishman, Ralph and John Sterling. 1992.
Acquisition of selectional patterns. In
Proceedings of the 14th International
Conference on Computational Linguistics,
pages 658?664, Nantes.
Harris, Zellig. 1968. Mathematical Structure of
Language. Wiley, New York.
Hay, Jennifer, Aaron Nolan, and Katie
Drager. 2006. From fush to feesh: Exemplar
priming in speech perception. The
Linguistic Review, 23(3):351?379.
Hindle, Donald. 1990. Noun classification
from predicate-argument structures. In
Proceedings of the 28th Annual Meeting of the
Association for Computational Linguistics,
pages 268?275, Pittsburgh, PA.
Hindle, Donald and Mats Rooth. 1993.
Structural ambiguity and lexical relations.
Computational Linguistics, 19(1):103?120.
Katz, Jerrold J. and Jerry A. Fodor. 1963. The
structure of a semantic theory. Language,
39(2):170?210.
Katz, Jerrold J. and Paul M. Postal. 1964. An
Integrated Theory of Linguistic Descriptions.
Research Monograph No. 26. MIT Press,
Cambridge, MA.
Konieczny, Lars and Philipp D?ring. 2003.
Anticipation of clause-final heads.
Evidence from eye-tracking and SRNs.
In Proceedings of the 4th International
Conference on Cognitive Science,
pages 330?335, Sydney.
Lakoff, George and Mark Johnson. 1980.
Metaphors We Live By. University of
Chicago Press, Chicago, IL.
Landauer, Thomas and Susan Dumais. 1997.
A solution to Plato?s problem: The latent
semantic analysis theory of acquisition,
induction, and representation of
knowledge. Psychological Review,
104(2):211?240.
Lee, Lillian. 1999. Measures of distributional
similarity. In Proceedings of the 31st Annual
Meeting of the Association for Computational
Linguistics, pages 25?32, College Park, MA.
Lin, Dekang. 1993. Principle-based parsing
without overgeneration. In Proceedings of
the 31st Annual Meeting of the Association for
Computational Linguistics, pages 112?120,
Columbus, OH.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
of the Joint Annual Meeting of the Association
for Computational Linguistics and
International Conference on Computational
Linguistics, pages 768?774, Montreal.
Lowe, Will. 2001. Towards a theory of
semantic space. In Proceedings of the 23rd
Annual Conference of the Cognitive Science
Society, pages 576?581, Edinburgh.
Lowe, Will and Scott McDonald. 2000. The
direct route: Mediated priming in semantic
space. In Proceedings of the 22nd Annual
Conference of the Cognitive Science Society,
pages 675?680, Philadelphia, PA.
Lund, Kevin and Curt Burgess. 1996.
Producing high-dimensional semantic
spaces from lexical co-occurrence. Behavior
Research Methods, Instruments, and
Computers, 28:203?208.
McCarthy, Diana. 2000. Using semantic
preferences to identify verbal participation
in role switching alternations. In
Proceedings of the 1st Annual Meeting of the
North American Chapter of the Association for
Computational Linguistics, pages 256?263,
Seattle, WA.
McCarthy, Diana and John Carroll. 2003.
Disambiguating nouns, verbs, and
adjectives using automatically acquired
selectional preferences. Computational
Linguistics, 29(4):639?654.
McCarthy, Diana, Rob Koeling, Julie Weeds,
and John Carroll. 2004. Finding
predominant word senses in untagged
text. In Proceedings of the 42th Annual
Meeting of the Association for Computational
Linguistics, pages 279?286, Barcelona.
McCarthy, Diana, Sriram Venkatapathy, and
Aravind K. Joshi. 2007. Detecting
compositionality of verb-object
combinations using selectional
preferences. In Proceedings of the 12th Joint
Conference on Empirical Methods in Natural
Language Processing and Conference on
Natural Language Learning, pages 369?379,
Prague.
McDonald, Scott and Chris Brew. 2004. A
distributional model of semantic context
effects in lexical processing. In Proceedings
of the 42th Annual Meeting of the Association
for Computational Linguistics, pages 17?24,
Barcelona.
McRae, Ken, Todd Ferretti, and Liane
Amyote. 1997. Thematic roles as
verb-specific concepts. Language and
Cognitive Processes, 12(2/3):137?176.
McRae, Ken, Mary Hare, Jeffrey Elman, and
Todd Ferretti. 2005. A basis for generating
expectancies for verbs from nouns.
Memory and Cognition, 33(7):1174?1184.
McRae, Ken, Mary Hare, Todd Ferretti, and
Jeffrey Elman. 2001. Activating verbs
typical agents, patients, instruments, and
locations via event schemas. In Proceedings
761
Computational Linguistics Volume 36, Number 4
of the Twenty-Third Annual Conference of the
Cognitive Science Society, pages 617?622,
Mahwah, NJ.
McRae, Ken, Michael Spivey-Knowlton, and
Michael Tanenhaus. 1998. Modeling the
influence of thematic fit (and other
constraints) in on-line sentence
comprehension. Journal of Memory and
Language, 38:283?312.
Miller, George, Richard Beckwith, Christiane
Fellbaum, Derek Gross, and Katherine
Miller. 1990. Five papers on WordNet.
International Journal of Lexicography,
3(4):235?312.
Nakatani, Kentaro and Edward Gibson. 2009.
An on-line study of Japanese nesting
complexity. Cognitive Science, 1(34):94?112.
Nastase, Vivi. 2008. Unsupervised all-words
word sense disambiguation with
grammatical dependencies. In Proceedings
of the 3rd International Joint Conference
on Natural Language Processing,
pages 757?762, Honolulu, HI.
Nosofsky, Robert. 1986. Attention, similarity,
and the identification-categorization
relationship. Journal of Experimental
Psychology: General, 115(1):39?57.
Pad?, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of
semantic space models. Computational
Linguistics, 33(2):161?199.
Pad?, Sebastian, Ulrike Pad?, and Katrin Erk.
2007. Flexible, corpus-based modelling of
human plausibility judgements. In
Proceedings of the 12th Joint Conference on
Empirical Methods in Natural Language
Processing and Conference on Natural
Language Learning, pages 400?409, Prague.
Pad?, Ulrike. 2007. The Integration of Syntax
and Semantic Plausibility in a Wide-Coverage
Model of Human Sentence Processing. Ph.D.
thesis, Saarland University, Saarbr?cken,
Germany.
Pad?, Ulrike, Matthew W. Crocker, and
Frank Keller. 2009. A probabilistic model
of semantic plausibility in sentence
processing. Cognitive Science, 33:794?838.
Pantel, Patrick, Rahul Bhagat, Bonaventura
Coppola, Timothy Chklovski, and
Eduard Hovy. 2007. ISP: Learning
inferential selectional preferences. In
Proceedings of the Joint Human Language
Technology Conference and Annual Meeting
of the North American Chapter of the
Association for Computational Linguistics,
pages 564?571, Rochester, NY.
Pereira, Fernando, Naftali Tishby, and Lillian
Lee. 1993. Distributional clustering of
English words. In Proceedings of the 31st
Annual Meeting of the Association for
Computational Linguistics, pages 183?190,
Columbus, OH.
Pustejovsky, James. 1995. The Generative
Lexicon. MIT Press, Cambridge, MA.
Resnik, Philip. 1996. Selectional constraints:
An information-theoretic model and its
computational realization. Cognition,
61:127?159.
Rooth, Mats, Stefan Riezler, Detlef Prescher,
Glenn Carroll, and Franz Beil. 1999.
Inducing a semantically annotated lexicon
via EM-based clustering. In Proceedings of
the 37th Annual Meeting of the Association for
Computational Linguistics, pages 104?111,
College Park, MA.
Salton, Gerard, Anita Wong, and Chung-Shu
Yang. 1975. A vector-space model for
information retrieval. Journal of the
American Society for Information Science,
18:613?620.
Schulte im Walde, Sabine. 2010. Comparing
computational approaches to selectional
preferences: Second-order co-occurrence
vs. latent semantic clusters. In Proceedings
of the 7th International Conference on
Language Resources and Evaluation,
pages 1381?1388, Valleta.
Schulte im Walde, Sabine, Christian Hying,
Christian Scheible, and Helmut Schmid.
2008. Combining EM training and the
MDL principle for an automatic verb
classification incorporating selectional
preferences. In Proceedings of the 46th
Annual Meeting of the Association for
Computational Linguistics, pages 496?504,
Columbus, OH.
Shepard, Roger. 1987. Towards a universal
law of generalization for psychological
science. Science, 237(4820):1317?1323.
Stolcke, Andreas. 2002. SRILM?an
extensible language modeling toolkit. In
Proceedings of the International Conference on
Spoken Language Processing, pages 901?904,
Denver, CO.
Toutanova, Kristina, Christoper D. Manning,
Dan Flickinger, and Stephan Oepen. 2005.
Stochastic HPSG parse selection using the
Redwoods corpus. Journal of Research on
Language and Computation, 3(1):83?105.
Trueswell, John, Michael Tanenhaus, and
Susan Garnsey. 1994. Semantic influences
on parsing: Use of thematic role
information in syntactic ambiguity
resolution. Journal of Memory and Language,
33:285?318.
Vandekerckhove, Bram, Dominiek Sandra,
and Walter Daelemans. 2009. A robust and
extensible exemplar-based model of
762
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
thematic fit. In Proceedings of the 12th
Meeting of the European Chapter of the
Association for Computational Linguistics,
pages 826?834, Athens.
Wilks, Yorick. 1975. Preference semantics.
In E. Keenan, editor, Formal Semantics of
Natural Language. Cambridge University
Press, Cambridge, UK, pages 329?350.
Yarowsky, David. 1993. One sense per
collocation. In Proceedings of the ARPA
Human Language Technology Workshop,
pages 266?271, Princeton, NJ.
Zanzotto, Fabio Massimo, Marco
Pennacchiotti, and Maria Teresa Pazienza.
2006. Discovering asymmetric entailment
relations between verbs using selectional
preferences. In Proceedings of the Joint
Annual Meeting of the Association for
Computational Linguistics and International
Conference on Computational Linguistics,
pages 849?856, Sydney.
Zapirain, Be?at, Eneko Agirre, and Llu?s
M?rquez. 2009. Generalizing over lexical
features: Selectional preferences for
semantic role classification. In Proceedings
of the 47th Annual Meeting of the Association
for Computational Linguistics, pages 73?76,
Singapore.
763

Measuring Word Meaning in Context
Katrin Erk?
University of Texas at Austin
Diana McCarthy??
University of Cambridge
Nicholas Gaylord?
University of Texas at Austin
Word sense disambiguation (WSD) is an old and important task in computational linguistics
that still remains challenging, to machines as well as to human annotators. Recently there have
been several proposals for representing word meaning in context that diverge from the traditional
use of a single best sense for each occurrence. They represent word meaning in context through
multiple paraphrases, as points in vector space, or as distributions over latent senses. New
methods of evaluating and comparing these different representations are needed.
In this paper we propose two novel annotation schemes that characterize word meaning in
context in a graded fashion. In WSsim annotation, the applicability of each dictionary sense
is rated on an ordinal scale. Usim annotation directly rates the similarity of pairs of usages of
the same lemma, again on a scale. We find that the novel annotation schemes show good inter-
annotator agreement, as well as a strong correlation with traditional single-sense annotation and
with annotation of multiple lexical paraphrases. Annotators make use of the whole ordinal scale,
and give very fine-grained judgments that ?mix and match? senses for each individual usage.
We also find that the Usim ratings obey the triangle inequality, justifying models that treat usage
similarity as metric.
There has recently been much work on grouping senses into coarse-grained groups. We
demonstrate that graded WSsim and Usim ratings can be used to analyze existing coarse-grained
sense groupings to identify sense groups that may not match intuitions of untrained native
speakers. In the course of the comparison, we also show that the WSsim ratings are not subsumed
by any static sense grouping.
? Linguistics Department. CLA Liberal Arts Building, 305 E. 23rd St. B5100, Austin, TX, USA 78712.
E-mail: katrin.erk@mail.utexas.edu, nlgaylord@utexas.edu.
?? Visiting Scholar, Department of Theoretical and Applied Linguistics, University of Cambridge,
Sidgwick Avenue, Cambridge, CB3 9DA, UK. E-mail: diana@dianamccarthy.co.uk.
Submission received: 3 November 2011; revised version received: 30 April 2012; accepted for publication:
25 June 2012.
doi:10.1162/COLI a 000142
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
1. Introduction
Word sense disambiguation (WSD) is a task that has attracted much work in computa-
tional linguistics (see Agirre and Edmonds [2007] and Navigli [2009] for an overview),
including a series of workshops, SENSEVAL (Kilgarriff and Palmer 2000; Preiss and
Yarowsky 2001; Mihalcea and Edmonds 2004) and SemEval (Agirre, Ma`rquez, and
Wicentowski 2007; Erk and Strapparava 2010), which were originally organized
expressly as a forum for shared tasks in WSD. In WSD, polysemy is typically modeled
through a dictionary, where the senses of a word are understood to be mutually disjoint.
The meaning of an occurrence of a word is then characterized through the best-fitting
among its dictionary senses.
The assumption of senses that are mutually disjoint and that have clear bound-
aries has been drawn into doubt by lexicographers (Kilgarriff 1997; Hanks 2000), lin-
guists (Tuggy 1993; Cruse 1995), and psychologists (Kintsch 2007). Hanks (2000) argues
that word senses have uses where they clearly fit, and borderline uses where only a
few of a sense?s identifying features apply. This notion matches results in psychol-
ogy on human concept representation: Mental categories show ?fuzzy boundaries,?
and category members differ in typicality and degree of membership (Rosch 1975;
Rosch and Mervis 1975; Hampton 2007). This raises the question of annotation: Is it
possible to collect word meaning annotation that captures degrees to which a sense
applies?
Recently, there have been several proposals for modeling word meaning in context
that can represent different degrees of similarity to a word sense, as well as different
degrees of similarity between occurrences of a word. The SemEval Lexical Substitu-
tion task (McCarthy and Navigli 2009) represents each occurrence through multiple
weighted paraphrases. Other approaches represent meaning in context through a vector
space model (Erk and Pado 2008; Mitchell and Lapata 2008; Thater, Fu?rstenau, and
Pinkal 2010) or through a distribution over latent senses (Dinu and Lapata 2010). Again,
this raises the question of annotation: Can human annotators give fine-grained judg-
ments about degrees of similarity between word occurrences, like these computational
models predict?
The question that we explore in this paper is: Can word meaning be described
through annotation in the form of graded judgments? We want to know whether an-
notators can provide graded meaning annotation in a consistent fashion. Also, we
want to know whether annotators will use the whole graded scale, or whether
they will fall back on binary ratings of either ?identical? or ?different.? Our ques-
tion, however, is not whether annotators can be trained to do this. Rather, our
aim is to describe word meaning as language users perceive it. We want to tap into
the annotators? intuitive notions of word meaning. As a consequence, we use un-
trained annotators. We view it as an important aim on its own to capture lan-
guage users? intuitions on word meaning, but it is also instrumental in answering
our first question, of whether word meaning can be described through graded
annotator judgments: Training annotators in depth on how to distinguish pre-
defined hand-crafted senses could influence them to assign those senses in a binary
fashion.
We introduce two novel annotation tasks in which human annotators characterize
word meaning in context. In the first task, they rate the applicability of dictionary
senses on a graded scale. In the second task, they rate the similarity between pairs of
usages of the same word, also on a graded scale. In designing the annotation tasks, we
utilize techniques from psycholinguistic experimentation: Annotators give ratings on a
512
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
scale, rather than selecting a single label; we also use multiple annotators for each item,
retaining all annotator judgments.1
The result of this graded annotation can then be used to evaluate computational
models of word meaning: either to evaluate graded models of word meaning, or to
evaluate traditional WSD systems in a graded fashion. They can also be used to ana-
lyze existing word sense inventories, in particular to identify sense distinctions worth
revisiting?we say more on this latter use subsequently.
Our aim is not to improve inter-annotator agreement over traditional sense an-
notation. It is highly unlikely that ratings on a scale would ever achieve higher exact
agreement than binary annotation. Our aim is also not to maximize exact agreement, as
we expect to see individual differences in perceived meaning, and want to capture those
differences. Still it is desirable to have an end product of the annotation that is robust
against such individual differences. In order to achieve this, we average judgments over
multiple annotators after first inspecting pairwise correlations between annotators to
ensure that they are all doing their work diligently and with similar outcomes.
Analyzing the annotation results, we find that the annotators make use of inter-
mediate points on the graded scale and do not treat the task as inherently binary. We
find that there is good inter-annotator agreement, measured as correlation. There is
also a highly significant correlation across tasks and with traditional WSD and lexical
substitution tasks. This indicates that the annotators performed these tasks in a con-
sistent fashion. It also indicates that diverse ways of representing word meaning in
context?single best sense, weighted senses, multiple paraphrases, usage similarity?
yield similar characterizations. We find that annotators frequently give high scores to
more than one sense, in a way that is not remedied by a more coarse-grained sense
inventory. In fact, the annotations are often inconsistent with disjoint sense partitions.
The work reported here is based on our earlier work reported in Erk, McCarthy, and
Gaylord (2009). The current paper extends the previous work in three ways.
1. We add extensive new annotation to corroborate our findings from
the previous, smaller study. In this new, second round of annotation,
annotators do the two graded ratings tasks as well as traditional
single-sense annotation and annotation with paraphrases (lexical
substitutes), all on the same data. Each item is rated by eight annotators
in parallel. This setting, with four different types of word meaning
annotation on the same data, allows us to compare annotation results
across tasks more directly than before.2
2. We test whether the similarity ratings on pairs of usages obey the triangle
inequality, and find that they do. This point is interesting for psychological
reasons. Tversky and Gati (Tversky 1977; Tversky and Gati 1982) found
that similarity ratings on words did not obey the triangle inequality?
although, unlike our study, they were dealing with words out of context.
The fact that usage similarity ratings obey the triangle inequality is also
important for modeling and annotation purposes.
1 We do not use as many raters per item as is usual in psycholinguistics, however, as our aim is to cover a
sizeable amount of corpus data.
2 The annotation data from this second round are available at http://www.dianamccarthy.co.uk/
downloads/WordMeaningAnno2012/.
513
Computational Linguistics Volume 39, Number 3
3. We examine the extent to which our graded annotation accords with two
existing coarse-grained sense groupings, and we demonstrate that our
graded annotations can be used to double-check on sense groupings and
find potentially problematic groupings.
2. Background
In this section, we offer an overview of previous word sense annotation efforts, and then
discuss alternative approaches to the annotation and modeling of word meaning.
2.1 Word Sense Annotation
Inter-annotator agreement (also called inter-tagger agreement, or ITA) is one indicator
of the difficulty of the task of manually assigning word senses (Krishnamurthy and
Nicholls 2000). With WordNet, the sense inventory currently most widely used in
word sense annotation, ITA ranges from 67% to 78% (Landes, Leacock, and Tengi 1998;
Mihalcea, Chklovski, and Kilgarriff 2004; Snyder and Palmer 2004), depending on
factors such as degree of polysemy and inter-relatedness of the senses. This issue is
not specific to WordNet. Annotation efforts based on other dictionaries have achieved
similar ITA levels, as shown in Table 1. The first group in that table shows two corpora
in which all open-class words are annotated for word sense, in both cases using
WordNet. The second group consists of two English lexical sample corpora, in which
only some target words are annotated. One of them uses WordSmyth senses for verbs
and WordNet for all other parts of speech, and the other uses HECTOR, with similar
ITA, so the choice of dictionary does not seem to make much difference in this case.3
Next is SALSA, a German corpus using FrameNet frames as senses, then OntoNotes,
again an English lexical sample corpus. Inter-annotator agreement is listed in the last
column of the table; agreement is in general relatively low for the first four corpora,
which use fine-grained sense distinctions, and higher for SALSA and OntoNotes, which
have more coarse-grained senses.
Sense granularity has a clear impact upon levels of inter-annotator agreement
(Palmer, Dang, and Fellbaum 2007). ITA is substantially improved by using coarser-
grained senses, as seen in OntoNotes (Hovy et al 2006), which uses an ITA of 90% as the
criterion for constructing coarse-grained sense distinctions. Although this strategy does
improve ITA, it does not eliminate the issues seen with more fine-grained annotation
efforts: For some lemmas, such as leave, 90% ITA is not reached even after multiple
re-partitionings of the semantic space (Chen and Palmer 2009). This suggests that the
meanings of at least some words may not be separable into senses distinct enough
for consistent annotation.4 Moreover, sense granularity does not appear to be the only
question influencing ITA differences between lemmas. Passonneau et al (2010) found
three main factors: sense concreteness, specificity of the context in which the target word
occurs, and similarity between senses. It is worth noting that of these factors, only the
third can be directly addressed by a change in the dictionary.
3 HECTOR senses are described in richer detail than WordNet senses and the resource is strongly
corpus-based. We use WordNet in our work due to its high popularity and free availability.
4 Examples such as this indicate that there is at times a problem with clearly defining consistently
separable senses of a word. There is no clear measure of exactly how frequent such cases are, however.
This is due in part to the fact that this question depends so heavily on the data being considered and the
distinctions being posited.
514
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Table 1
Word sense-annotated data, with inter-annotator agreement (ITA).
Corpus Dictionary Corpus reference ITA
SemCor WordNet Landes, Leacock, and Tengi
(1998)
78.6%
SensEval-3 WordNet Snyder and Palmer (2004) 72.5%
SensEval-1 lex. sample HECTOR Kilgarriff and Rosenzweig
(2000)
66.5%
SensEval-3 lex. sample WordNet, WordSmyth Mihalcea, Chklovski, and
Kilgarriff (2004)
67.3%
SALSA FrameNet Burchardt et al (2006) 86%
OntoNotes OntoNotes Hovy et al (2006) most > 90%
Table 2
Best word sense disambiguation performance in SensEval/SemEval English lexical sample
tasks.
Shared task Shared task overview Best precision Baseline
SensEval-1 Kilgarriff and Rosenzweig (2000) 77% 69%
SensEval-2 Senseval-2 (2001) 64% 51%
SensEval-3 Mihalcea, Chklovski, and Kilgarriff (2004) 73% 55%
SemEval-1 Pradhan et al (2007) 89% (not given)
ITA levels in word sense annotation tasks are mirrored in the performance of WSD
systems trained on the annotated data. Table 2 shows results for the best systems that
participated at four English lexical sample tasks. With fine-grained sense inventories,
the top-ranking WSD systems participating in the event achieved precision scores of 73%
to 77% (Edmonds and Cotton 2001; Mihalcea, Chklovski, and Kilgarriff 2004). Current
state-of-the-art systems have made modest improvements on this; for example, the
system described by Zhong and Ng (2010) achieves 65.3% on the English lexical sample
at SENSEVAL-2, though the same system obtains 72.6%, just below Mihalcea, Chklovski,
and Kilgarriff (2004), on the English lexical sample at SENSEVAL-3. Nevertheless, the pic-
ture remains the same with systems getting around three out of four word occurrences
correct. Under a coarse-grained approach, system performance improves considerably
(Palmer, Dang, and Fellbaum 2007; Pradhan et al 2007), with the best participating
system achieving a precision close to 90%.5 The merits of a coarser-grained approach
are still a matter of debate (Stokoe 2005; Ide and Wilks 2006; Navigli, Litkowski, and
Hargraves 2007; Brown 2010), however.
Although identifying the proper level of granularity for sense repositories has im-
portant implications for improving WSD, we do not focus on this question here. Rather,
we propose novel annotation tasks that allow us to probe the relatedness between
dictionary senses in a flexible fashion, and to explore word meaning in context without
presupposing hard boundaries between usages. The resulting data sets can be used
to compare different inventories, coarse or otherwise. In addition, we hope that they
will prove useful for the evaluation of alternative representations of ambiguity in word
5 Zhong, Ng, and Chan (2008) report similar results (89.1%) with their state-of-the-art system when
evaluating on the OntoNotes corpus, which is larger than the SENSEVAL data sets.
515
Computational Linguistics Volume 39, Number 3
meaning (Erk and Pado 2008; Mitchell and Lapata 2008; Reisinger and Mooney 2010;
Thater, Fu?rstenau, and Pinkal 2010; Reddy et al 2011; Van de Cruys, Poibeau, and
Korhonen 2011).
2.2 Representation of Word Meaning in Word Sense Inventories
One possible factor contributing to the difficulty of manual and automatic word sense
assignment is the design of word sense inventories themselves. As we have seen, such
difficulties are encountered across dictionaries, and it has been argued that there are
problems with the characterization of word meanings as sets of discrete and mutually
exclusive senses (Tuggy 1993; Cruse 1995; Kilgarriff 1997; Hanks 2000; Kintsch 2007).
2.2.1 Criticisms of Enumerative Approaches to Meaning. Dictionaries are practical resources
and the nature of the finished product depends upon the needs of the target audience, as
well as budgetary and related constraints (cf. Hanks 2000). Consequently, dictionaries
differ in the words that they cover, and also in the word meanings that they distinguish.
Dictionary senses are generalizations over the meanings that a word can take, and these
generalizations themselves are abstractions over collected occurrences of the word in
different contexts (Kilgarriff 1992, 1997, 2006). Regardless of a dictionary?s granularity,
the possibility exists for some amount of detail to be lost as a result of this process.
Kilgarriff (1997) calls into question the possibility of general, all-purpose senses of
a word and argues that sense distinction only makes sense with respect to a given task.
For example, in machine translation, the senses to be distinguished should be those
that lead to different translations in the target language. It has since been demonstrated
that this is in fact the case (Carpuat and Wu 2007a, 2007b). Hanks (2000) questions the
view of senses as disjoint classes defined by necessary and sufficient conditions. He
shows that even with a classic homonym like ?bank,? some occurrences are more typical
examples of a particular sense than others. This notion of typicality is also important in
theories of concept representation in psychology (Murphy 2002). Theoretical treatments
of word meaning such as the Generative Lexicon (Pustejovsky 1991) also draw attention
to the subtle, yet reliable, fluctuations of meaning-in-context, and work in this paradigm
also provides evidence that two senses which may appear to be quite distinct can in
fact be quite difficult to distinguish in certain contexts (Copestake and Briscoe 1995,
page 53).
2.2.2 Psychological Research on Lexical and Conceptual Knowledge. Not all members of a
mental category are equal. Some are perceived as more typical than others (Rosch 1975;
Rosch and Mervis 1975; and many others), and even category membership itself is
clearer in some cases than in others (Hampton 1979). These results are about mental
concepts, however, rather than word meanings per se, which raises the question of
the relation between word meanings and conceptual knowledge. Murphy (1991, 2002)
argues that although not every concept is associated with a word, word meanings show
many of the same phenomena as concepts in general?word meaning is ?made up of
pieces of conceptual structure? (Murphy 2002, page 391). A body of work in cognitive
linguistics also discusses the relation between word meaning and conceptual structure
(Coleman and Kay 1981; Taylor 2003).
Psycholinguistic studies on word meaning offer insight into the question of the
mental representation of word senses. Unlike homonym meanings, the senses of a
polysemous word are thought to be related, suggesting that the mental representations
of these senses may overlap as well. The psycholinguistic literature on this question
516
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
is not wholly clear-cut, but by and large does support the position that polysemous
senses are not entirely discrete in the mental lexicon. Whereas Klein and Murphy (2001,
2002) do provide evidence for discreteness of mental sense representations, it appears
as though these findings may be due in part to the particular senses included in their
studies (Klepousniotou, Titone, and Romero 2008).
Moreover, many psycholinguistic studies have indeed found evidence for process-
ing differences between homonyms and polysemous words, using a variety of experi-
mental designs, including eye movements and reading times (Frazier and Rayner 1990;
Pickering and Frisson 2001) as well as response times in sensicality and lexical decision
tasks (Williams 1992; Klepousniotou 2002). Brown (2008, 2010) takes the question of
shared vs. separate meaning representations one step further in a semantic priming
study6 in which she shows that intuitive meaning-in-context similarity judgments have
a processing correlate in on-line sentence comprehension. Response time to the target
is a negative linear function of its similarity in meaning to the prime, and response
accuracy is a positive linear function of this similarity. In other words, the more similar
in meaning a prime?target pair was judged to be, the faster and more accurately sub-
jects responded. This provides empirical support for a processing correlate of graded
similarity-in-meaning judgments.
In our work reported here, we take inspiration from work in psychology and look
at ways to model word meaning more continuously. Even though there is still some
controversy, the majority of studies support the view that senses of polysemous words
are linked in their mental representations. In our work we do not make an explicit
distinction between homonymy and polysemy, but the data sets we have produced may
be useful for a future exploration of this distinction.
2.3 Alternative Approaches to Word Meaning
Earlier we suggested that word meaning may be better described without positing
disjoint senses. We now describe some alternatives to word sense inventory approaches
to word meaning, most of which do not rely on disjoint senses.
2.3.1 Substitution-Based Approaches. McCarthy and Navigli (2007) explore the use of
synonym or near-synonym lexical substitutions to characterize the meaning of word
occurrences. In contrast to dictionary senses, substitutes are not taken to partition
a word?s meaning into distinct senses. McCarthy and Navigli gathered their lexical
substitution data using multiple annotators. Annotators were allowed to provide up to
three paraphrases for each item. Data were gathered for 10 sentences per lemma for 210
lemmas, spanning verbs, nouns, adjectives, and adverbs. The annotation took the form
of each occurrence being associated with a multiset of supplied paraphrases, weighted
by the frequency with which each paraphrase was supplied. We make extensive use of
the LEXSUB dataset in our work reported here. An example sentence with substitutes
from the LEXSUB dataset (sentence 451) is given in Table 3.
A related approach also characterizes meaning through equivalent terms, but terms
in another language. Resnik and Yarowsky (2000, page 10) suggest ?to restrict a word
sense inventory to distinctions that are typically lexicalized cross-linguistically? [emphasis
in original]. They argue that such an approach will avoid being too fine-grained, and
that the distinctions that are made will be independently motivated by crosslinguistic
6 See McNamara (2005) for more information on priming studies.
517
Computational Linguistics Volume 39, Number 3
Table 3
An example of annotation from the lexical substitution data set: sentence 451.
Sentence: My interest in Europe?s defence policy is nothing new.
Annotation: original 2; recent 2; novel 2; different 1; additional 1
trends. Although substitution and translation methods are not without their own issues
(Kilgarriff 1992, page 48), they constitute an approach to word meaning that avoids
many of the drawbacks of more traditional sense distinction and annotation. Some
cross-linguistic approaches group translations into disjoint senses (Lefever and Hoste
2010), whereas others do not (Mihalcea, Sinha, and McCarthy 2010).
2.3.2 Distributional Approaches. Recently there have been a growing number of distri-
butional approaches to representing word meaning in context. These models offer an
opportunity to model subtle distinctions in meaning between two occurrences of a word
in different contexts. In particular, they allow comparisons between two occurrences of
a word without having to classify them as having the same sense or different senses.
Some of these approaches compute a distributional representation for a word across all
its meanings, and then adapt this to a given sentence context (Landauer and Dumais
1997; Erk and Pado 2008; Mitchell and Lapata 2008; Thater, Fu?rstenau, and Pinkal 2010;
Van de Cruys, Poibeau, and Korhonen 2011). Others group distributional contexts into
senses. This can be done on the fly for a given occurrence (Erk and Pado 2010; Reddy
et al 2011), or beforehand (Dinu and Lapata 2010; Reisinger and Mooney 2010). The
latter two approaches then represent an occurrence through weights over those senses.
A third group of approaches is based on language models (Deschacht and Moens 2009;
Washtell 2010; Moon and Erk 2012): They infer other words that could be used in the
position of the target word.7
3. Two Novel Annotation Tasks
In this section we introduce two novel annotation schemes that draw on methods
common in psycholinguistic experiments, but uncommon in corpus annotation. Tra-
ditional word sense annotation usually assumes that there is a single correct label
for each markable. Annotators are trained to identify the correct labels consistently,
often with highly specific a priori guidelines. Multiple annotators are often used, but
despite the frequently low ITA in word sense annotation, differences between annotator
responses are often treated as the result of annotator error and are not retained in the
final annotation data.
In these respects, traditional word sense annotation tasks differ in design from
many psycholinguistic experiments, such as the ones discussed in the previous section.
Psycholinguistic experiments frequently do not make strong assumptions about how
participants will respond, and in fact are designed to gather data on that very ques-
tion. Participants are given general guidelines for completing the experiment but these
7 Distributional models for phrases have recently received much attention, even more so than models for
word meaning in context (Baroni and Zamparelli 2010; Coecke, Sadrzadeh, and Clark 2010; Mitchell and
Lapata 2010; Grefenstette and Sadrzadeh 2011; Socher et al 2011). They are less directly relevant to the
current paper, however, as we focus on eliciting judgments for individual words in sentence contexts,
rather than whole phrases.
518
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Table 4
Interpretation of the five-point scale given to the annotators. This interpretation is the same for
the Usim and WSsim tasks.
1 completely different
2 mostly different
3 similar
4 very similar
5 identical
guidelines generally stop short of precise procedural detail, to avoid undue influence
over participant responses. All of the psycholinguistic studies discussed earlier used
participants na??ve as to the purpose of the experiment, and who were minimally trained.
Responses are often graded in nature, involving ratings on an ordinal scale or in some
cases even a continuously valued dimension (e.g., as in Magnitude Estimation). Mul-
tiple participants respond to each stimulus, but all participant responses are typically
retained, as there are often meaningful discrepancies in participant responses that are
not ascribable to error. All of the psycholinguistic studies discussed previously collected
data from multiple participants (up to 80 in the case of one experiment by Williams
[1992]).
The annotation tasks we present subsequently draw upon these principles of exper-
imental design. We collected responses using a scale, rather than binary judgments; we
designed the annotation tasks to be accomplishable without prior training and with
minimal guidelines, and we used multiple annotators (up to eight) and retained all
responses in an effort to capture individual differences. In the following, we describe
two different annotation tasks, one with and one without the use of dictionary senses.
Graded Ratings for Dictionary Senses. In our first annotation task, dubbed WSsim (for
Word Sense Similarity), annotators rated the applicability of WordNet dictionary senses,
using a five-point ordinal scale.8 Annotators rated the applicability of every single
WordNet sense for the target lemma, where a rating of 1 indicated that the sense
in question did not apply at all, and a rating of 5 indicated that the sense applied
completely to that occurrence of the lemma. Table 4 shows the descriptions of the five
points on the scale that the annotators were given. By asking annotators to provide
ratings for each individual sense, we strive to eliminate all bias toward either single-
sense or multiple-sense annotation. By asking annotators to provide ratings on a scale,
we allow for the fact that senses may not be perceived in a binary fashion.
Graded Ratings for Usage Similarity. In our second annotation task, dubbed Usim (for
Usage Similarity), we collected annotations of word usages without recourse to dic-
tionary senses, by asking annotators to judge the similarity in meaning of one usage
of a lemma to other usages. Annotators were presented with pairs of contexts that
share a word in common, and were asked to rate how similar in meaning they perceive
those two occurrences to be. Ratings are again on a five-point ordinal scale; a rating of
1 indicated that the two occurrences of the target lemma were completely dissimilar in
meaning, and a rating of 5 indicated that the two occurrences of the target lemma were
identical in meaning. The descriptions of the five points on the scale, shown in Table 4,
8 The use of a five-point scale is a common choice when collecting ordinal ratings, as it allows more
detailed responses than the ?yes/no/maybe? provided by a three-point scale.
519
Computational Linguistics Volume 39, Number 3
were identical to those used in the WSsim task. Annotators were able to respond ?I don?t
know? if they were unable to gauge the similarity in meaning of the two occurrences.9
Annotation Procedure. All annotation for this project was conducted over the Internet
in specially designed interfaces. In both tasks, all annotator responses were retained,
without resolution of disagreement between annotators. We do not focus on obtaining
a single ?correct? annotation, but rather view all responses as valuable sources of
information, even when they diverge.
For each item presented, annotators additionally were provided a comment field
should they desire to include a more detailed response regarding the item in question.
They could use this, for example, to comment on problems understanding the sentence.
The annotators were able to revisit previous items in the task. Annotators were not able
to skip forward in the task without rating the current item. If an annotator attempted to
submit an incomplete annotation they were prompted to provide a complete response
before proceeding. They were free to log out and resume later at any point, however,
and also could access the instructions whenever they wanted.
Two Rounds of Annotation. We performed two rounds of the annotation experiments,
hereafter referred to as R1 and R2.10 Both annotation rounds included both a WSsim and
a Usim task, labeled in the subsequent discussion as WSsim-1 and Usim-1 for R1, and
WSsim-2 and Usim-2 for R2. An important part of the data analysis is to compare the
new, graded annotation to other types of annotation. We compare it to both traditional
word sense annotation, with a single best sense for each occurrence, and lexical
substitution, which characterizes each occurrence through paraphrases. In R1, we chose
annotation data that had previously been labeled with either traditional single sense
annotation or with lexical substitutions. R2 included two additional annotation tasks,
one involving traditional WSD methodology (WSbest) and a lexical substitution task
(SYNbest). In the SYNbest task, annotators provided a single best lexical substitution,
in contrast to the multiple substitutes annotators provided in the original LEXSUB data.11
Three annotators participated in each task in the R1, and eight annotators partici-
pated in R2. In R1, separate groups of annotators participated in WSsim and Usim an-
notation, whereas in R2 the same group of annotators was used for all annotation, so as
to allow comparison across tasks for the same annotator as well as across annotators. In
R2, therefore, the same annotators did both traditional word sense annotation (WSbest)
and the graded word sense annotation of the WSsim task. This raises the question of
whether their experience on one task will influence their annotation choice on the other
task. We tested this by varying the order in which annotators did WSsim and WSbest.
R2 annotators were divided into two groups of four annotators with the order of tasks
as follows:
group 1: Usim-2 SYNbest WSsim-2 WSbest
group 2: Usim-2 SYNbest WSbest WSsim-2
Another difference between the two rounds of annotation was that in R2 we per-
mitted the annotators to see one more sentence of context on either side of the target
9 The ?I don?t know? option was present only in the Usim interface, and was not available in WSsim.
10 The annotation was conducted in two separate rounds due to funding.
11 Annotation guidelines for R1 are at http://www.katrinerk.com/graded-sense-and-usage-annotation
and guidelines for R2 tasks are at http://www.dianamccarthy.co.uk/downloads/
WordMeaningAnno2012/.
520
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Table 5
Abbreviations used in the text for annotation tasks and rounds.
WSsim Task: graded annotation of WordNet senses on a five-point scale
Usim Task: graded annotation of usage similarity on a five-point scale
WSbest Task: traditional single-sense annotation
SYNbest Task: lexical substitution
R1 Annotation round 1
R2 Annotation round 2
sentence. In R1 each item was given only one sentence as context. We added more
context in order to reduce the chance that the sentence would be unclear. Table 5
summarizes up the annotation tasks and annotation rounds on which we report.
Data Annotated. The data to be annotated in WSsim-1 were taken primarily from
Semcor (Miller et al 1993) and the Senseval-3 English lexical sample (SE-3) (Mihalcea,
Chklovski, and Kilgarriff 2004). This experiment contained a total of 430 sentences span-
ning 11 lemmas (nouns, verbs, and adjectives). For eight of these lemmas, 50 sentences
were included, 25 randomly sampled from Semcor and 25 randomly sampled from SE-3.
The remaining three lemmas in the experiment had 10 sentences each, from the LEXSUB
data. Each of the three annotators annotated each of the 430 items, providing a response
for each WordNet sense for that lemma. Usim-1 used data from LEXSUB. Thirty-four
lemmas were manually selected, including the three lemmas also used in WSsim-1. We
selected lemmas which exhibited a range of meanings and substitutes in the LEXSUB
data, with as few multiword substitutes as possible. Each lemma is the target in 10
LEXSUB sentences except there were only nine sentences for the lemma bar.n because
of a part-of-speech tagging error in the LEXSUB trial data. For each lemma, annotators
were presented with every pairwise comparison of these 10 sentences. We refer to each
such pair as an SPAIR. There were 45 SPAIRs per lemma (36 for bar.n), adding up to 1,521
comparisons per annotator in Usim-1.
In R1, only 30 sentences were included in both WSsim and Usim. Because compar-
ison of annotator responses on this subset of the two tasks yielded promising results,
R2 used the same set of sentences for both Usim and WSsim so as to better compare
these tasks. All data in the second round were taken from LEXSUB, and contained 26
lemmas with 10 sentences for each. We produced the SYNbest annotation, rather than
use the existing LEXSUB annotation, so that we could ensure the same conditions as
with the other annotation tasks, that is, using the same annotators and providing the
extra sentence of context on either side of the original LEXSUB context. We also only
required that the annotators provide one substitute. As such, there were 260 target
lemma occurrences that received graded word sense applicability ratings in WSsim-2,
and 1,170 SPAIRs (pairs of occurrences) to be annotated in Usim-2.
4. Analysis of the Annotation
In this section we present our analysis of the annotated data. We test inter-annotator
agreement, and we test to what extent annotators make use of the added flexibility
of the graded annotation. We also compare the outcome of our graded annotation to
traditional word sense annotation and lexical substitutions for the same data.
521
Computational Linguistics Volume 39, Number 3
4.1 Evaluation Measures
Because both graded annotation tasks, WSsim and Usim, use ratings on five-point
scales rather than binary ratings, we measure agreement in terms of correlation. Because
ratings were not normally distributed, we choose a non-parametric test which uses
ranks rather than absolute values: We use Spearmans rank correlation coefficient (rho),
following Mitchell and Lapata (2008). For assessing inter-tagger agreement on the R2
WSbest task we adopt the standard WSD measure of average pairwise agreement, and
for R2 SYNbest, we use the same pairwise agreement calculation used in LEXSUB.
When comparing graded ratings with single-sense or lexical substitution annota-
tion, we use the mean of all annotator ratings in the WSsim or Usim annotation. This
is justified because the inter-annotator agreement is highly significant, with respectable
rho compared with previous work (Mitchell and Lapata 2008).
As the annotation schemes differ between R1 and R2 (as mentioned previously, the
number of annotators and the amount of visible context are different, and R2 annotators
did traditional word sense annotation in the WSbest task in addition to the graded
tasks) we report the results of R1 and R2 separately.12
4.2 WSsim: Graded Ratings for WordNet Senses
In the WSsim task, annotators rated the applicability of each sense of the target word on
a five-point scale. We first do a qualitative analysis, then turn to a quantitative analysis
of annotation results.
4.2.1 Qualitative Analysis. Table 6 shows an example of WSsim annotation. The target
is the verb dismiss, which was annotated in R2. The first column gives the WordNet
sense number (sn).13 Note that in the task, the annotators were given the synonyms
and full description but in this figure we only supply part of the description for the
sake of space. As can be seen, three of the annotators chose a single-sense annotation
by giving a rating of 5 to one sense and ratings of 1 to all others. Two annotators gave
ratings of 1 and 2 to all but one sense. The other three annotators gave positive ratings
(ratings of at least 3 [similar], see Table 4) to at least two of the senses. All annotators
agree that the first sense fits the usage perfectly, and all annotators agree that senses
3 and 5 do not apply. The second sense, on the other hand, has an interestingly wide
distribution of judgments, ranging from 1 to 4. This is the judicial sense of the verb, as
in ?this case is dismissed.? Some annotators consider this sense to be completely distinct
from sense 1, whereas others see a connection. There is disagreement among annotators,
about sense 6. This is the sense ?dismiss, dissolve,? as in ?the president dissolved the
parliament.? Six of the annotators consider this sense completely unrelated to ?dismiss
our actions as irrelevant,? whereas two annotators view it as highly related (though
not completely identical). It is noteworthy that each of the two opinions, a rating of 1
12 It is known that when responses are collected on an ordinal scale, the possibility exists for different
individuals to use the scale differently. As such, it is common practice to standardize responses using a
z-score, which maps a response X to z = X??? . The calculation of z-scores makes reference to the mean
and the standard deviation of an annotator?s responses. Because responses were not normally distributed
in our task, a transformation that relies on measures of central tendency is not appropriate. So we do not
use z-scores in this paper. We repeated all analyses with z-score transform anyway, and found the results
to be basically the same as those we report here with the raw values. Overall, using z-scores slightly
strengthened most findings, but there were no differences in statistical significance anywhere.
13 We use WordNet 3.0 for our annotation.
522
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Table 6
WSsim example, R2: Annotator judgments for the different senses of dismiss.
If we see ourselves as separate from the world, it is easy to dismiss our actions as irrelevant
or unlikely to make any difference. (902)
sn Description Ratings By Annotator Mean
1 bar from attention or consideration 5 5 5 5 5 5 5 5 5
2 cease to consider 1 4 1 3 2 2 1 3 2.125
3 stop associating with 1 2 1 1 1 2 1 1 1.25
4 terminate the employment of 1 4 1 2 1 1 1 1 1.5
5 cause or permit a person to leave 1 2 1 1 1 1 1 2 1.25
6 declare void 1 1 1 4 1 1 1 4 1.75
and a rating of 4, was chosen by multiple annotators. Because multiple annotators give
each judgment, these data seem to reflect a genuine difference in perceived sense. We
discuss inter-annotator agreement, both overall and considering individual annotators,
subsequently.
Table 7 gives an example sentence from R1, where the annotated target is the noun
paper. All annotators agree that sense 5, ?scholarly article,? applies fully. Sense 2 (?essay?)
also gets ratings of ? 3 from all annotators. The first annotator seems also to have
perceived the ?physical object? connotation to apply strongly to this example, and has
expressed this quite consistently by giving high marks to sense 1 as well as 7.
Table 8 shows a sample annotated sentence with an adjective target, neat, annotated
in R2. In this case, only one annotator chose single-sense annotation by marking exclu-
sively sense 4. One annotator gave ratings ? 3 (similar) to all senses of the lemma. All
other annotators saw at least two senses as applying (with ratings ? 3) and at least one
sense as not applying at all (with a rating of 1). Sense 4 has received positive ratings (that
is, ratings ? 3) throughout. Senses 1, 2, and 6 have mixed ratings, and senses 3 and 5
have positive ratings only from the one annotator who marked everything as applying.
Interestingly, ratings for senses 1, 2, and 6 diverge sharply, with some annotators seeing
them as not applying at all, and some giving them ratings in the 3?5 range. Note that the
Table 7
WSsim example, R1: Annotator judgments for the different senses of paper.
This can be justified thermodynamically in this case, and this will be done in a separate
paper which is being prepared. (br-j03, sent. 4)
sn Description Ratings Mean
1 a material made of cellulose pulp 4 1 1 1.3
2 an essay (especially one written as an assignment) 3 3 5 3.7
3 a daily or weekly publication on folded sheets; contains
news and articles and advertisements
2 1 3 2
4 a medium for written communication 5 3 1 3
5 a scholarly article describing the results of observations
or stating hypotheses
5 5 5 5
6 a business firm that publishes newspapers 2 1 1 1.3
7 the physical object that is the product of a newspaper
publisher
4 1 1 1.7
523
Computational Linguistics Volume 39, Number 3
Table 8
WSsim example, R2: Annotator judgments for the different senses of neat.
Over the course of the 20th century scholars have learned that such images tried to make
messy reality neater than it really is (103)
sn Description Ratings By Annotator Mean
1 free from clumsiness; precisely or
deftly executed
1 5 1 4 5 5 5 5 3.375
2 refined and tasteful in appearance or
behavior or style
3 4 1 4 4 3 1 3 2.875
3 having desirable or positive qualities
especially those suitable for a
thing specified
1 3 1 1 1 1 1 1 1.25
4 marked by order and cleanliness in
appearance or habits
4 5 5 3 4 5 5 5 4.5
5 not diluted 1 4 1 1 1 1 1 1 1.375
6 showing care in execution 1 4 1 3 4 1 3 3 2.5
Table 9
Correlation matrix for pairwise correlation agreement for WSsim-1. The last row provides the
agreement of the annotator in that column against the average from the other annotators.
A B C
A 1.00 0.47 0.51
B 0.47 1.00 0.54
C 0.51 0.54 1.00
against avg 0.56 0.58 0.61
annotators who give ratings of 1 are not the same for these three ratings, pointing to dif-
ferent, but quite nuanced, judgments of the ?make reality neater? usage in this sentence.
4.2.2 Inter-annotator Agreement. We now turn to a quantitative analysis, starting with
inter-annotator agreement. For the graded WSsim annotation, it does not make sense
to compute the percentage of perfect agreement. As discussed earlier, we report inter-
annotator agreement in terms of correlation, using Spearman?s rho. We calculate pair-
wise agreements and report the average over all pairs. The pairwise correlations are
shown in the matrix in Table 9. We have used capital letters to represent the individ-
uals, preserving the same letter for the same person across tasks. In the last row we
show agreement of each annotator?s judgments against the average judgment from the
other annotators. The pairwise correlations range from 0.47 to 0.54 and all pairwise
correlations were highly significant (p  0.001), with an average of rho = 0.504. This
is a very reasonable result given that Mitchell and Lapata (2008) report a rho of 0.40
on a graded semantic similarity task.14 The lowest correlation against the average
14 Direct comparison across tasks is not appropriate, but we wish to point out that for graded semantic
judgments this level of correlation is perfectly reasonable. The Mitchell and Lapata (2008) data
set has been used in an evaluation exercise (GEMS-2011, https://sites.google.com/site/
geometricalmodels/shared-evaluation). Mitchell and Lapata point out that Spearman?s rho
tends to yield lower coefficients compared with parametric alternatives such as Pearson?s.
524
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Table 10
Correlation matrix for pairwise correlation agreement for WSsim-2. The last row provides the
agreement of the annotator in that column against the average from the other annotators.
A C D F G H I J
A 1.00 0.55 0.58 0.60 0.61 0.63 0.61 0.59
C 0.55 1.00 0.54 0.66 0.57 0.55 0.65 0.52
D 0.58 0.54 1.00 0.55 0.58 0.52 0.56 0.54
F 0.60 0.66 0.55 1.00 0.62 0.62 0.72 0.59
G 0.61 0.57 0.58 0.62 1.00 0.63 0.62 0.62
H 0.63 0.55 0.52 0.62 0.63 1.00 0.64 0.64
I 0.61 0.65 0.56 0.72 0.62 0.64 1.00 0.58
J 0.59 0.52 0.54 0.59 0.62 0.64 0.58 1.00
against avg 0.70 0.58 0.62 0.64 0.70 0.71 0.66 0.71
from the other annotators was 0.56. We discuss the annotations of individuals in Sec-
tion 4.6, including our decision to retain the judgments of all annotators for our gold
standard.
From the correlation matrix in Table 10 we see that for WSsim-2, pairwise corre-
lations ranged from 0.52 to 0.72. The average value of the pairwise correlations was
rho = 0.60, and again every pair was highly significant (p  0.001). The lowest correla-
tion against the average from all the other annotators was 0.58.
4.2.3 Choice of Single Sense Versus Multiple Senses. In traditional word sense annotation,
annotators can mark more than one sense as applicable, but annotation guidelines often
encourage them to view the choice of a single sense as the norm. In WSsim, annotators
gave ratings for all senses of the target. So we would expect that in WSsim, there would
be a higher proportion of senses selected as applicable. Indeed we find this to be the
case: Table 11 shows the proportion of sentences where some annotator has assigned
more than one sense with a judgment of 5, the highest value. Both WSsim-1 and WSsim-
2 have a much higher proportion of sentences with multiple senses chosen than the
traditional sense-annotated data sets SemCor and SE-3. Interestingly, we notice that
the percentage for WSsim-1 is considerably higher than for WSsim-2. In principle, this
could be due to differences in the lemmas that were annotated, or differences in the
sense perception of the annotators between R1 and R2. Another potential influencing
Table 11
WSsim annotation: Proportion of sentences where multiple senses received a rating of 5 (highest
judgment) from the same annotator.
Proportion
WSsim-1 46%
WSsim-2 30%
WSsim-2, WSsim first 36%
WSsim-2, WSbest first 23%
SemCor 0.3%
SE-3 8%
525
Computational Linguistics Volume 39, Number 3
factor is the order of annotation experiments: As described earlier, half of the R2 anno-
tators did WSbest annotation before doing WSsim-2, and half did the two experiments
in the opposite order. As Table 11 shows, those doing the graded task WSsim-2 before
the binary task WSbest had a greater proportion of multiple senses annotated with
the highest response. This demonstrates that annotators in a word meaning task can
be influenced by factors outside of the current annotation task, in this case another
annotation task that they have done previously. We take this as an argument in favor of
using as many annotators as possible in order to counteract factors that contribute noise.
In our case, we counter the influence of previous annotation tasks somewhat by using
multiple annotators and altering the order of the WSsim and WSbest tasks. Another
option would have been to use different annotators for different tasks; by using the
same set of annotators for all four tasks, however, we can better control for individual
variation.
4.2.4 Use of the Graded Scale. We next ask whether annotators in WSsim made use of the
whole five-point scale, or whether they mostly chose the extreme ratings of 1 and 5.
If the latter were the case, this could indicate that they viewed the task of word sense
assignment as binary. Figure 1a shows the relative frequency distribution of responses
from all annotators over the five scores for both R1 and R2. Figures 2a and 3a show the
same but for each individual annotator. In both rounds the annotators chose the rating
of 1 (?completely different,? see Table 4) most often. This is understandable because each
item is a sentence and sense combination and there will typically be several irrelevant
senses for a given sentence. The second most frequent choice was 5 (?identical?). Both
rounds had plenty of judgments somewhere between the two poles, so the annotators
do not seem to view the task of assigning word sense as completely binary. Although
the annotators vary, they all use the intermediate categories to some extent and certainly
the intermediate category judgments do not originate from a minority of annotators.
We notice that R2 annotators tended to give more judgments of 1 (?completely
different?) than the R1 annotators. One possible reason is again that half our annota-
tors did WSbest before WSsim-2. If this were the cause for the lower judgments, we
would expect more ratings of 1 for the annotators who did the traditional word sense
annotation (WSbest) first. In Table 12 we list the relative frequency of each rating for the
different groups of annotators. We certainly see an increase in the judgments of 1 where
Figure 1
WSsim and Usim R1 and R2 ratings.
526
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Figure 2
WSsim and Usim R1 individual ratings.
Figure 3
WSsim and Usim R2 individual ratings.
WSbest is performed before WSsim-2. Again, this may indicate that annotators were
leaning more towards finding a single exact match because they were influenced by
the WSbest task they had done before. Annotators in that group were also slightly less
inclined to take the middle ground, but this was true of both groups of R2 annotators
compared with the R1 annotators. We think that this difference between the two rounds
may well be due to the lemmas and data.
In Table 18, we show the average range15 and average variance of the judgments
per item for each of the graded annotation tasks. WSsim naturally has less variation
15 As an example, the first two senses (1 and 2) in Table 6 have ranges of 0 and 3, respectively.
527
Computational Linguistics Volume 39, Number 3
Table 12
The relative frequency of the annotations at each judgment from all annotators.
Judgment
Exp 1 2 3 4 5
WSsim-1 0.43 0.106 0.139 0.143 0.181
WSsim-2 0.696 0.081 0.067 0.048 0.109
WSsim-2, WSsim first 0.664 0.099 0.069 0.048 0.12
WSsim-2, WSbest first 0.727 0.063 0.065 0.048 0.097
Usim-1 0.360 0.202 0.165 0.150 0.123
Usim-2 0.316 0.150 0.126 0.112 0.296
compared with Usim because, for any sentence, there are inevitably many WordNet
senses which are irrelevant to the context at hand and which will obtain a judgment
of 1 from everyone. This is particularly the case for WSsim-2 where the annotators
gave more judgments of 1, as discussed previously. The majority of items have a range
of less than two for WSsim. We discuss the Usim figures further in the following
section.
4.3 Usim: Graded Ratings for Usage Similarity
In Usim annotation, annotators compared pairs of usages of a target word (SPAIRs) and
rated their similarity on the five-point scale given in Table 4. The annotators were also
permitted a response of ?don?t know.? Such responses were rare but were used when
the annotators really could not judge usage similarity, perhaps because the meaning
of one sentence was not clear. We removed any pairs where one of the annotators had
given a ?don?t know? verdict (9 in R1, 28 in R2). For R1 this meant that we were left with
a total of 1,512 SPAIRs and in R2 we had a resultant 1,142 SPAIRs.
4.3.1 Qualitative Analysis. We again start by inspecting examples of Usim annotation.
Table 13 shows the annotation for an SPAIR of the verb dismiss. The first of the two
sentences talks about ?dismissing actions as irrelevant,? the second is about dismissing
a person. Interestingly, the second usage could be argued to carry both a connotation
of ?ushering out? and a connotation of ?disregarding.? Annotator opinions on this SPAIR
vary from a 1 (completely different) to a 5 (identical), but most annotators seem to view
the two usages as related to an intermediate degree. This is adequately reflected in the
average rating of 3.125. Table 14 compares the sentence from Table 8 to another sentence
Table 13
Usim example: Annotator judgments for a pair of usages of dismiss.
Sentences Ratings
If we see ourselves as separate from the world, it is easy to dismiss
our actions as irrelevant or unlikely to make any difference.
1, 2, 3, 3,
3, 4, 4, 5
Simply thank your Gremlin for his or her opinion, dismiss him or
her, and ask your true inner voice to turn up its volume.
528
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Table 14
Usim example: Annotator judgments for a pair of usages of neat.
Sentences Ratings
Over the course of the 20th century scholars have learned that such
images tried to make messy reality neater than it really is.
3, 3, 4, 4,
4, 4, 5, 5
Strong field patterns created by hedgerows give the landscape a
neat, well structured appearance.
Table 15
Usim example: Annotator judgments for a pair of usages of account.
Sentences Ratings
Samba-3 permits use of multiple account data base backends. 1, 2, 3, 3,
3, 4, 4, 4
Within a week, Scotiabank said that it had frozen some accounts
linked to Washington?s hit list.
with the target neat. The first sentence is a metaphorical use (making reality neater),
the second is literal (landscape with neat appearance), but still the SPAIR gets high
ratings of 3?5 throughout for an average of 4.0. Note that the WordNet senses, shown
in Table 8, do not distinguish the literal and metaphorical uses of the adjective, either.
Table 15 shows two uses of the noun account. The first pertains to accounts on a software
system, the second to bank accounts. The spread of annotator ratings shows that these
two uses are not the same, but that some relation exists. The average rating for this
SPAIR is 3.0.
4.3.2 Inter-annotator Agreement. We again calculate inter-annotator agreement as the
average over pairwise Spearman?s correlations. The pairwise correlations are shown
in the matrix in Table 16. In the last row we show agreement of each annotator?s
judgments against the average judgment from the other annotators. For Usim-1 the
range of correlation coefficients is between 0.50 and 0.64 with an average correlation
of rho = 0.548. All the pairs are highly significantly correlated (p  0.001). The smallest
correlation for any individual against the average is 0.55. The correlation matrix for
Usim-2 is provided in Table 17; the range of correlation coefficients is between 0.42 and
Table 16
Correlation matrix for pairwise correlation agreement for Usim-1. The last row provides the
agreement of the annotator in that column against the average from the other annotators.
A D E
A 1.00 0.50 0.64
D 0.50 1.00 0.50
E 0.64 0.50 1.00
against avg 0.67 0.55 0.67
529
Computational Linguistics Volume 39, Number 3
Table 17
Correlation matrix for pairwise correlation agreement for Usim-2. The last row provides the
agreement of the annotator in that column against the average from the other annotators.
A C D F G H I J
A 1.00 0.70 0.52 0.70 0.69 0.72 0.73 0.67
C 0.70 1.00 0.48 0.72 0.60 0.66 0.71 0.69
D 0.52 0.48 1.00 0.48 0.49 0.51 0.50 0.42
F 0.70 0.72 0.48 1.00 0.66 0.71 0.74 0.68
G 0.69 0.60 0.49 0.66 1.00 0.71 0.65 0.62
H 0.72 0.66 0.51 0.71 0.71 1.00 0.70 0.65
I 0.73 0.71 0.50 0.74 0.65 0.70 1.00 0.72
J 0.67 0.69 0.42 0.68 0.62 0.65 0.72 1.00
against avg 0.82 0.78 0.58 0.80 0.76 0.80 0.81 0.76
0.73. All these correlations are highly significant (p 0.001) with an average correlation
of rho = 0.62. The lowest agreement between any individual and the average judgment
of the others is 0.58. Again, we note that these are all respectable values for tasks
involving semantic similarity ratings.
Use of the graded scale. Figure 1b shows how annotators made use of the graded scale
in Usim-1 and Usim-2. It graphs the relative frequency of each of the judgments on the
five-point scale. Figures 2b and 3b show the same but for each individual annotator. In
both annotation rounds, the rating 1 (completely different) was chosen most frequently.
There are also in both annotation rounds many ratings in the middle points of the
scale, indeed we see a larger proportion of mid-range scores for Usim than for WSsim
in general, as shown in Table 12. Figures 2b and 3b show that although individuals
differ, all use the mid points to some extent and it is certainly not the case that these
mid-range judgments come from a minority of annotators. In Usim, annotators com-
pared pairs of usages, whereas in WSsim, they compared usages with sense defini-
tions. The sense definitions suggest a categorization that may bias annotators towards
categorical choices. Comparing the two annotation rounds for Usim, we see that in
Usim-2 there seem to be many more judgments at 5 than in Usim-1. This is similar
to our findings for WSsim, where we also obtained more polar judgments for R2 than
for R1.
There is a larger range on average for Usim-2 compared with the other tasks as
shown earlier by Table 18. This is understandable given that there are eight annotators
Table 18
Average range and average variance of judgments for each of the graded experiments.
avg range avg variance
WSsim-1 1.78 1.44
WSsim-2 1.55 0.71
Usim-1 1.41 0.92
Usim-2 2.50 1.12
530
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
for R2 compared with R1,16 and so a greater chance of a larger range per item. There is
substantial variation by lemma. In Usim-2, fire.v, rough.a, and coach.n have an average
range of 1.33, 1.76, and 1.93, respectively, whereas suffer.v, neat.a, and function.n have
average ranges of 3.14, 3.16, and 3.58, respectively. The variation in range appears to
depend on the lemma rather than POS. This variation can be viewed as a gauge of how
difficult the lemma is. Although the range is larger in Usim-2, however, the average
variance per item (i.e., the variance considering the eight annotators) is 1.12 and lower
than that for WSsim-1.
Usim and the triangle inequality. In Euclidean space, the lengths of two sides of a triangle,
taken together, must always be greater than the length of the third side. This is the
triangle inequality:
length(longest) < length(second longest) + length(shortest)
We now ask whether the triangle inequality holds for Usim ratings. If Usim similarities
are metric, that is, if we can view the ratings as proximity in a Euclidean ?meaning
space,? then the triangle inequality would have to hold. This question is interesting for
what it says about the psychology of usage similarity judgments. Classic results due
to Tversky and colleagues (Tversky 1977; Tversky and Gati 1982) show that human
judgments of similarity are not always metric. Tversky (1977), varying an example
by William James, gives the following example, which involves words, but explicitly
ignores context:
Consider the similarity between countries: Jamaica is similar to Cuba (because of
geographical proximity); Cuba is similar to Russia (because of their political affinity);
but Jamaica and Russia are not similar at all. [. . . ] the perceived distance of Jamaica to
Russia exceeds the perceived distance of Jamaica to Cuba, plus that of Cuba to
Russia?contrary to the triangle inequality.
Note, however, that Tversky was considering similarity judgments for different words,
whereas we look at different usages of the same word. The question of whether the
triangle inequality holds for Usim ratings is also interesting for modeling reasons.
Several recent approaches model word meaning in context through points in vector
space (Erk and Pado 2008; Mitchell and Lapata 2008; Dinu and Lapata 2010; Reisinger
and Mooney 2010; Thater, Fu?rstenau, and Pinkal 2010; Washtell 2010; Van de Cruys,
Poibeau, and Korhonen 2011). They work on the tacit assumption that similarity of
word usages is metric?an assumption that we can directly test here. Third, the triangle
inequality question is also relevant for future annotation; we will discuss this in more
detail subsequently.
To test whether Usim ratings obey the triangle inequality, we first convert the
similarity ratings that the annotators gave to dissimilarity ratings: Let savg be the mean
similarity rating over all annotators, then we use the dissimilarity rating d = 6 ? savg
(as 5 was the highest possible similarity score).
We examine the proportion of sentence triples where the triangle inequality holds
(that is, we consider every triple of sentences that share the same target lemma). In those
16 A likely reason for the larger range in WSsim-1 compared with WSsim-2 is that in WSsim-2 half the
annotators had performed WSbest before WSsim-2 and produced more judgments of 1 compared with
WSsim-1.
531
Computational Linguistics Volume 39, Number 3
cases where the triangle inequality is violated, we also assess the degree to which it is
violated, calculated as the average distance that is missed: Let Tmiss be the set of triples
for which the triangle inequality does not hold, then we compute
m = 1|Tmiss|
?
t?Tmiss
length(longestt) ? (length(second longestt) + length(shortestt))
This is the average amount by which the longest side is ?too long.?
For the first round of annotation, Usim-1, we found that 99.2% of the sentence
triples obey the triangle inequality. For the triples that miss it, the average amount
by which the longest side is too long is m = 0.520. This is half a point on the five-
point rating scale, a low amount. In R2, all sentence triples obey the triangle inequality.
One potential reason for this is that we have eight annotators for R2, and a larger
sample of annotators reduces the variation from individuals. Another reason may
be that the annotators in R2 could view two more sentences of context than those
in R1.
Tables 19 and 20 show results of the triangle inequality analysis, but by individual
annotator. Every annotator has at least 93% of sentence triples obeying the principle. For
the triples that miss it, they tend to miss it by between one and two points. The results
for individuals accord with the triangle inequality principle, though to a lesser extent
compared with the analysis using the average, which reduces the impact of variation
from individuals.
As discussed previously, this result (that the triangle inequality holds for Usim
annotation triples) is interesting because it contrasts with Tversky?s findings (Tversky
1977; Tversky and Gati 1982) that similarity ratings between different words are not
metric. And although we consider similarity ratings for usages of the same word, not
different words, we would argue that our findings point to the importance of consider-
ing the context in which a word is used. It would be interesting to test whether similarity
ratings for different words, when used in context, obey the triangle inequality. To
reference the Tversky example, and borrowing some terminology from Cruse, evoking
the ISLAND facet of Jamaica and Cuba versus the COMMUNIST STATE facet of Cuba and
Russia would account for the non-metricality of the similarity judgments as Tversky
Table 19
Triangle inequality analysis by annotator, Usim-1.
average A D E
perc obey 93.8 97.2 97.3
missed by 1.267 1.221 1.167
Table 20
Triangle inequality analysis by annotator, Usim-2.
A C D F G H I J
perc obey 94.1 97.5 98.4 97.2 93.6 97.0 97.4 97.4
missed by 1.508 1.405 1.122 1.824 1.477 1.281 1.759 1.338
532
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Table 21
WSbest annotations.
sense selected Proportion with
no yes multiple choice
WSbest 19,599 2,401 0.13
WSbest, WSsim-2 first 9,779 1,221 0.15
WSbest, WSbest first 9,820 1,180 0.11
points out, and moreover highlight the lack of an apt comparison between Jamaica and
Russia at all. There is some motivation for this idea in the psychological literature on
structural alignment and alignable differences (Gentner and Markman 1997; Gentner
and Gunn 2001).
In addition, our finding that the triangle inequality holds for Usim annotation
will be useful for future Usim annotation. Usage similarity annotation is costly (and
somewhat tedious) as annotators give ratings for each pair of sentences for a given
target lemma. Given that we can expect usage similarity to be metric, we can eliminate
the need for some of the ratings. Once annotators have rated two usage pairs out of a
triple, their ratings set an upper limit on the similarity of the third pair. In the best case, if
usages s1 and s2 have a distance of 1 (i.e., a similarity of 5), and s1 and s3 have a distance
of 1, then the distance of s1 and s3 can be at most 2. For all usage triples where two
pairs have been judged highly similar, we can thus omit obtaining a rating for the third
pair. A second option for obtaining more Usim annotation is to use crowdsourcing. In
crowdsourcing annotation, quality control is always an issue, and again we can make
use of the triangle inequality to detect spurious annotation: Ratings that grossly violate
the triangle inequality can be safely discarded.
4.4 WSbest
The WSbest task reflects the traditional methodology in word sense annotation where
words are annotated with the best fitting sense. The guidelines17 allow for selecting
more than one sense provided all fit the example equally well. Table 21 shows that,
as one would expect given the number of senses in WordNet, there are more unse-
lected senses than selected. We again find an influence of task order: When annota-
tors did the graded annotation (WSsim-2) before WSbest, there were more multiple
assignments (see the last column) and therefore more senses selected. This difference
is statistically significant (?2 test, p = 0.02). Regardless of the order of tasks, we no-
tice that the proportion of multiple sense choice is far lower than the equivalent for
WSsim (see Table 11), as is expected due to the different annotation schemes and
guidelines.
We calculated inter-annotator agreement using pairwise agreement, as is standard
in WSD. There are several ways to calculate pairwise agreement in cases of multiple
selection, though these details are not typically given in WSD papers. We use the size
of the intersection of selections divided by the maximum number of selections from
17 See http://www.dianamccarthy.co.uk/downloads/WordMeaningAnno2012/wsbest.html.
533
Computational Linguistics Volume 39, Number 3
Table 22
Inter-annotator agreement without one individual for WSbest and SYNbest R2.
average A C D F G H I J
WSbest 0.574 0.579 0.564 0.605 0.560 0.582 0.566 0.566 0.568
SYNbest 0.261 0.261 0.259 0.285 0.254 0.256 0.245 0.260 0.267
either annotator. This is equivalent to 1 for agreement and 0 for disagreement in cases
where both annotators have selected only one sense. Formally, let i ? I be one annotated
sentence. Let A be the set of annotators and let PA = {{a, a?} | a, a? ? A} be the set of
annotator pairs. Let ai be the set of senses that annotator a ? A has chosen for sentence
i. Then pairwise agreement between annotators is calculated as:
ITA WSbest =
?
i?I
?
{a,a?}?PA
|ai?a?i |
max(|ai|,|a?i |)
|PA| ? |I|
(1)
The average ITA was calculated as 0.574.18 If we restrict the calculation to items
where each annotator only selected one sense (not multiple), the average is 0.626.
For SE-3, ITA was 0.628 on the English Lexical Sample task, not including the
multiword data (Mihalcea, Chklovski, and Kilgarriff 2004). This annotation exercise
used volunteers from the Web (Mihalcea and Chklovski 2003). Like our study, it had
taggers without lexicography background and gave a comparable ITA to our 0.626. We
calculated pairwise agreement for eight annotators. To carry out the experiment under
maximally similar conditions to previous studies, we also calculated ITA for items with
only one response and use only the four annotators who performed WSbest first. This
resulted in an average ITA of 0.638.
We also calculated the agreement for WSbest in R2 as in Equation 1 but with each
individual removed to see the change in agreement. The results are in the first row of
Table 22.
4.5 SYNbest
The SYNbest task is a repetition of the LEXSUB task (McCarthy and Navigli 2007, 2009)
except that annotators were asked to provide one synonym at most. As in LEXSUB,
agreement between a pair of annotators was counted as the proportion of all the
sentences for which the two annotators had given the same response.
As in WSbest, let A be the set of annotators. I is the set of test items, but as in
LEXSUB we only include those where at least two annotators have provided at least
one substitute: If only one annotator can think of a substitute then it is likely to be a
problematic item. As in WSbest, let ai be the set19 of responses (substitutes) for an item
18 Although there is a statistical difference in the number of multiple assignments depending upon whether
WSsim-2 is completed before or after WSbest, ITA on the WSbest task does not significantly differ
between the two sets.
19 Though, in fact, unlike LEXSUB and WSbest, we only collect one substitute per annotator for SYNbest.
534
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
i ? I for annotator a ? A. Let PA again be the set of pairs of annotators from A. Pairwise
agreement between annotators is calculated as in LEXSUB as:
PA =
?
i?I
?
{a,a?}?PA
|ai?a?i |
|ai?a?i |
|PA| ? |I|
(2)
Note that in contrast to pairwise agreement for traditional word sense annotation
(WSbest), the credit for each item (the intersection of annotations from the annotator
pair) is divided by the union of the responses. For traditional WSD evaluation, it is
divided by the number of responses from either annotator, which is usually one. For
lexical substitution this is important as the annotations are not collected over a predeter-
mined inventory. In LEXSUB, the PA figure was 0.278, whereas we obtain PA = 0.261 on
SYNbest. There were differences in the experimental set-up. We had eight annotators,
compared with five, and for SYNbest each annotator only provided one substitute.
Additionally, our experiment involved only a subset of the data used in LEXSUB. The
figures are not directly comparable, but are reasonably in line.
In our task, out of eight annotators we had at most three people who could not find
a substitute for any given item, so there were always at least five substitutes per item.
In LEXSUB there were 16 items excluded from testing in the full data set of 2010 because
there was only one token substitute provided by the set of annotators.
We also calculated the agreement for SYNbest as in Equation 2 but with each
individual removed to see the change in agreement. The results are in the second row
of Table 22.
4.6 Discussion of the Annotations of Individuals
We do not pose these annotation tasks as having ?correct responses.? We wish instead
to obtain the annotators? opinions, and accept the fact that the judgments will vary.
Nevertheless, we would not wish to conduct our analysis using annotators who were
not taking the task seriously. In the analyses that follow in subsequent sections, we
use the average judgment from our annotators to reduce variation from individuals.
Nevertheless, before doing so, in this subsection we briefly discuss the analysis of the
individual annotations provided earlier in this section in support of our decision to use
all annotators for the gold standard.
Although there was variation in the profile of annotations for individuals, all of the
annotators showed reasonable correlation on the graded task and at a level in excess
of that achieved on other graded semantic tasks (Mitchell and Lapata 2008). There will
inevitably be one annotator that has the lowest correlation with the others on any given
task, but we found that this was not the same annotator on every task. For example,
C on WSsim-2 has the lowest correlation with the average, yet concurs with others
much more on Usim-2 and leaving C out would reduce agreement on WSbest and on
SYNbest. D has lower correlation with others on several tasks, though higher than C on
WSsim-2. When we redo the triangle inequality analysis in Section 4.3 individually we
see from Tables 19 and 20 that annotator D is the highest performing annotator in terms
of meeting the triangle inequality principle in R2 and is a close runner-up in R1. These
results indicate that although annotators may use the graded scale in different ways,
their annotations tally to a reasonable extent. We therefore used all annotators for the
gold standard.
535
Computational Linguistics Volume 39, Number 3
4.7 Agreement Between Annotations in Different Frameworks
In this paper we are considering various different annotations of the same underly-
ing phenomenon: word meaning as it appears in context. In doing so, we contrast
traditional WSD methodology (SE-3, SemCor, and WSbest) with graded judgments of
sense applicability (WSsim), usage similarity (Usim), and lexical substitution as in
LEXSUB and SYNbest. In this section we compare the annotations from these different
paradigms where the annotations are performed on the same underlying data. For
WSsim and Usim, we use average ratings as the point of comparison.
4.7.1 Agreement Between WSsim and Traditional Sense Assignment. To compare WSsim
ratings on a five-point scale with traditional sense assignment on the same data, we
convert the traditional word sense assignments to ratings on a five-point scale: Any
sense that is assigned is given a score of 5, and any sense that is not assigned is
given a score of 1. If multiple senses are chosen in the gold standard, then they are
all given scores of 5. We then correlate the converted ratings of the traditional word
sense assignment with the average WSsim ratings using Spearman?s rho.
As described earlier, most of the sentences annotated in WSsim-1 were taken from
either SE-3 or SemCor. The correlation of WSsim-1 and SE-3 is rho = 0.416, and the cor-
relation of WSsim-1 with SemCor is rho = 0.425. Both are highly significant (p  0.001).
For R2 we can directly contrast WSsim with the traditional sense annotation in
WSbest on the same data. This allows a fuller comparison of traditional and graded
tagging because we have a data set annotated with both methodologies, under the same
conditions, and with the same set of annotators. We use the mode (most common) sense
tag from our eight annotators as the traditional gold standard label for WSbest and
assign a rating of 5 to that sense, and a rating of 1 elsewhere. We again used Spearman?s
rho to measure correlation between WSbest and WSsim and obtained rho = 0.483
(p  0.001).
4.7.2 Agreement Between WSsim and Usim. WSsim and Usim provide two graded an-
notations of word usage in context. To compare the two, we convert WSsim scores
to usage similarity ratings as in Usim. In WSsim, each sense has a rating (aver-
aged over annotators), so a sentence has a vector of ratings with a ?dimension? for
each sense. For example, the vector of average ratings for the sentence in Table 6 is
?5, 2.125, 1.25, 1.5, 1.25, 1.75?. All sentences with the same target will have vectors in the
same space, as they share the same sense list. Accordingly, we can compare a pair u,u?
of sentences that share a target using Euclidean distance:
d(u, u?) =
?
?
i
(ui ? u?i)2
where ui is the ith dimension of the vector u of ratings for sentence u. Note that this gives
us a dissimilarity rating for u,u?. We can now compare these sentence pair dissimilarities
to the similarity ratings of the Usim annotation.
In R1 we found a correlation of rho = ?0.596 between WSsim and Usim ratings.20
The basis of this comparison is small, at three lemmas with 10 sentences each, giving
20 The negative correlation is due to the comparison of dissimilarity ratings with similarity ratings.
536
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Table 23
Spearman?s correlation between lexical paraphrase overlap on the one hand, and Usim
similarity or WSsim dissimilarity on the other hand.
tasks rho
Usim-1 vs. LEXSUB 0.590
Usim-2 vs. SYNbest 0.764
WSsim-1 vs. LEXSUB ?0.495
WSsim-2 vs. SYNbest ?0.749
135 sentence pairs in total, because that is all the data available annotated in both
paradigms. For R2 we can perform the analysis on the whole Usim-2 and WSsim-2
data, which gives us 26 lemmas, with 1,142 sentence pairs.21 Correlation on R2 data is
rho = ?0.816. The degree of correlation is striking. We conclude that there is a very
strong relationship between the annotations for Usim and WSsim. This bodes well for
using Usim as a resource for evaluating sense inventories, an idea that we will pursue
further in Section 6: It reflects word meaning but is not tied to any given sense inventory.
4.7.3 Agreement of WSsim and Usim with Lexical Substitution. Lexical paraphrases (sub-
stitutes) have been used as a means of evaluating WSD systems in a task where the
inventory is not predefined (McCarthy and Navigli 2007, 2009). Because the R1 an-
notation was done in part on data that had previously been annotated with lexical
substitutions, and R2 included lexical substitution annotation, we can compare para-
phrase annotation with the results of WSsim and Usim. Again, we need to transform
annotations to make the comparison feasible. We convert all annotations to a Usim-
like format using sentence pair similarity or dissimilarity ratings. For WSsim, we use
the transformation described previously, using Euclidean distance between sense rating
vectors. We transform lexical substitution annotation using multiset intersection, as
the lexical substitution annotation of a sentence is a multiset of substitutes22 from all
annotators. If sentences s1, s2 have substitute multisets subs1 and subs2, respectively, and
freqi(w) is the frequency of substitute w in multiset subsi, then we calculate multiset
intersection as
INTER(s1, s2) =
1
max(|subs1|, |subs2|)
?
w?subs1?subs2
min( freq1(w), freq2(w))
Again, as before and in LEXSUB, we only keep sentences for which at least two
annotators could come up with a substitute. We also did not include any items that
were tagged with the wrong POS in LEXSUB.23
Table 23 shows correlation, in terms of Spearman?s rho, of Usim and WSsim
annotation with lexical substitution annotation. The values of Usim and WSsim are
based on mean scores averaged over all annotators. The INTER values computed for the
21 This is the number of pairs remaining after we exclude any pairs where one of the annotators provided a
?do not know? response.
22 The frequency of a substitute in a multiset depends on the number of annotators that picked the
substitute for the particular data point.
23 This was relevant only for the trial portion of LEXSUB, as the test portion was manually verified.
537
Computational Linguistics Volume 39, Number 3
lexical substitution annotation yield similarity ratings for sentence pairs; accordingly,
correlations of transformed lexical substitution with Usim are positive, and correlations
of transformed lexical substitution with the WSsim-based sentence dissimilarity ratings
are negative. All correlations are highly significant (p  0.001). We anticipated a higher
correlation of SYNbest with R2 annotation compared with that obtained using LEXSUB
and R1 annotation: In R2 the set of annotators is larger, the same set of annotators do
all experiments, and the SYNbest annotation focuses on obtaining one substitute per
annotator (whereas LEXSUB allowed annotators to supply up to three paraphrases). This
turned out to be in fact the case, as a comparison of rows 1 and 2 of Table 23 shows,
and likewise a comparison of rows 3 and 4. We notice that the correlation is slightly
stronger for Usim compared with WSsim, for both annotation rounds. One possible
reason for this is that the comparison of lexical substitution data with Usim involves
only one transformation of annotation data (the INTER calculation), whereas the com-
parison with WSsim involves two (INTER and also the Euclidean distance transforma-
tion). We can expect each transformation of annotation data to be ?lossy? in the sense
of introducing additional variance. Furthermore, WSsim relies on WordNet, which
may add a layer of structure that does not reflect the overlap in semantic similarity
between usages.
4.7.4 Summary. The Usim framework enables us to compare different annotation
schemes for word meaning, as it is relatively straightforward to map all annotations
to sentence pair (dis-)similarity ratings. We found strong relationships between WSsim
and Usim annotation, and between both graded annotation frameworks on the one
hand and traditional word sense annotation or lexical substitutions on the other hand.
This provides some validation for the novel annotation frameworks. Also, if all labeling
schemes provide comparable results, that opens up opportunities for choosing the best-
fitting labeling scheme for each situation. All these tasks pursue the same endeavor,
although the graded annotations and substitutions strive to capture the more subtle
nuances of meaning that are not adequately represented by the winner takes all ap-
proach of traditional methodology. WSsim is closest to the traditional methodology and
would suit systems needing to output WordNet sense labels, for example because they
want to exploit the semantic relations in WordNet. Usim is application-independent. It
allows for evaluation of systems that relate usages, whether into clusters or simply on
a continuum. It could, for example, be used as a resource-independent gold standard
for word sense induction. Lexical substitution tasks are particularly useful where the
application being considered would benefit from lexical paraphrasing, for example, text
simplification, summarization, or query expansion in information retrieval.
5. Examining Sense Groupings Emerging from WSsim Annotation
Recently there has been considerable work on grouping fine-grained senses, often from
WordNet, into more coarse-grained sense groups (Palmer, Dang, and Fellbaum 2007).
The use of coarse-grained sense groups has been shown to yield considerable improve-
ments in inter-annotator agreement in manual annotation, as well as in the accuracy of
WSD systems (Palmer, Dang, and Fellbaum 2007; Pradhan et al 2007). In our WSsim
annotation, we have used fine-grained WordNet senses, but we want to check that our
results are not an artifact of this fine-grained inventory. Furthermore, the annotation
results might be useful for identifying senses that could be grouped or for identifying
senses where grouping is not straightforward.
538
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
In WSsim, annotators gave ratings for each sense of a target word. If an annotator
perceives two senses of some target word as very similar, they will probably give them
similar ratings, and not just for a single sentence but across all the sentences featuring
the target word in question. So by looking for pairs of senses that tended to receive
similar ratings across all sentences, we can identify sense descriptions that according
to our annotators describe similar senses. Conversely, we expect that unrelated senses
would have dissimilar ratings. If there were many senses that the WSsim annotators
implicitly ?grouped? by giving them similar ratings throughout, we would have to
revise our finding that WSsim annotators often perceived more than one sense to be
applicable, as they would have perceived only what could be described as one implicit
sense group.
If a coarse-grained sense grouping is designed with the aim of reflecting sense
distinctions that would be intuitively plausible to an untrained speaker of the language,
then senses in a common group should also be similar according to WSsim annotation.
So when WSsim annotators give very different ratings to senses that are in the same
coarse-grained group, or very similar ratings to senses that are in different groups, this
can point to problems in a coarse-grained sense group.
In this section, first we describe two existing sense groupings (Hovy et al 2006;
Navigli, Litkowski, and Hargraves 2007). Then we test the extent that the annotations
accord with sense groupings by:
1. comparing judgments against the existing groupings, and re-examining
the question of how often WSsim annotators found multiple different
WordNet senses highly applicable.
2. using the WSsim data to examine the extent that the annotations could be
used to induce sense groupings.
5.1 Existing Sense Grouping Efforts
OntoNotes. The OntoNotes project (Hovy et al 2006; Chen and Palmer 2009) annotates
word sense, along with coreference and semantic roles. The senses that it uses for verbs
are WordNet 2.1 and 2.0, manually grouped based on both syntactic and semantic
criteria. Examples of these criteria include the causative/inchoative distinction, and
semantic features of particular argument positions, like animacy. Once the sense groups
for a lemma are constructed manually, they are used in trial annotation. If an inter-
annotator agreement of approximately 90% is reached, the lemma?s sense groups are
used for annotation; otherwise they are revised. Chen and Palmer report that the sense
groups used in OntoNotes have resulted in a rise in inter-annotator agreement as well
as annotator productivity. The third column of Table 24 shows OntoNotes groups for
the noun account.
5.1.1 The SemEval-2007 English All Words Task (EAW). For the English All Words task
at SemEval-2007, WordNet 2.1 senses were grouped by mapping them to the more
coarse-grained Oxford Dictionary of English senses. For the training data, this mapping
was done automatically; for the test data, the mapping was done by hand (Navigli,
Litkowski, and Hargraves 2007). For our analysis, we used only lemmas that were
included in the test data where the mapping had been produced manually.
539
Computational Linguistics Volume 39, Number 3
Table 24
WordNet 2.1 senses of the noun account, and their groups in OntoNotes (ON) and EAW.
WordNet sense WordNet ON EAW
sense no. group group
business relationship: ?he asked to see the executive
who handled his account?
3 1.1 5
report: ?by all accounts they were a happy couple? 8 1.2 2
explanation: ?I expected a brief account? 4 1.2 2
history, story: ?he gave an inaccurate account of
the plot [...]?
1 1.3 2
report, story: ?the account of his speech [...] made
the governor furious?
2 1.3 2
account statement: ?they send me an accounting
every month?
7 1.4 4
bill: ?send me an account of what I owe? 9 1.4 4
score: ?don?t do it on my account? 5 1.5 3
importance: ?a person of considerable account? 6 1.6 3
the quality of taking advantage: ?she turned her
writing skills to good account?
10 1.7 1
Column (4) of Table 24 shows EAW groups for the noun account.24 The two resources
largely agree in the groupings for account. But whereas EAW groups senses 1, 2, 4, and
8 together, OntoNotes splits those senses into two groups.
5.2 Does WSsim Annotation Conform to Existing Sense Groups?
In the WSsim annotation, we have used the fine-grained senses of WordNet 3.0. But
annotators were free to give high ratings for a sentence to more than one sense. So
it is possible that they implicitly used more coarse-grained sense distinctions. In this
and the following section, we will explore the question of whether, and to what extent,
WSsim annotators used implicit coarse-grained sense groups. In this section, we will
first ask whether their annotation matched the sense groups of either OntoNotes or
EAW. OntoNotes and EAW differ in the lemmas they cover. Also, as we saw earlier,
when they both cover a lemma, they do not always agree in the sense groups that they
propose. So we study the agreement of WSsim annotation with the two sense groupings
separately. We only study the lemmas which are in both the WSsim data and in either
OntoNotes or the EAW test data, listed in Table 25.
Tables 26 and 27 show the results. Table 26 looks at the number of sentences where
two senses both had high ratings, but are in different groupings in either OntoNotes or
EAW. The first row shows how many sentences there were where two senses received
a judgment of ? 3, but the two senses were not in a common OntoNotes/EAW group.
The second row shows the same for judgments ? 4, and the last row for judgments
of 5 only. In general, the percentages are higher for EAW than for OntoNotes. This is
not due to any difference in granularity between the two resources. The EAW sense
groups encompass on average 2.3 fine-grained senses for the R1 lemmas and 2.6 for the
24 The table shows the EAW groups of the WordNet senses, but the group numbering is our own for ease of
reference as no labels are given in EAW.
540
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Table 25
Lemmas in R1 and R2 WSsim that have coarse-grained mappings in OntoNotes and SemEval
2007 EAW.
R1 R2
lemma ON EAW ON EAW
account.n
? ?
add.v
?
ask.v
? ?
call.v
? ?
coach.n
?
different.a
?
dismiss.v
? ?
fire.v
?
fix.v
?
hold.v
? ?
lead.n
?
new.a
?
order.v
? ?
paper.n
?
rich.a
?
shed.v
?
suffer.v
? ?
win.v
? ?
Table 26
Sentences that have positive judgments for senses in different coarse groupings: percentage, and
absolute number in parentheses. J. = WSsim judgment, averaged over annotators.
OntoNotes EAW
J. Rd. 1 Rd. 2 Rd. 1 Rd. 2
? 3 28% (42) 52% (52) 78% (157) 62% (50)
? 4 13% (19) 16% (16) 41% (82) 22% (18)
5 3% (5) 3% (3) 8% (17) 6% (5)
Table 27
Sentences that have widely different judgments for pairs of senses in the same coarse grouping:
percentage, and absolute number in parentheses. J1 = WSsim judgment for the sense with the
lower rating, averaged over annotators; J2 = averaged WSsim judgment for the higher-rated of
the two senses.
OntoNotes EAW
J1 J2 Rd. 1 Rd. 2 Rd. 1 Rd. 2
? 2 ? 4 35% (52) 30% (30) 20% (39) 60% (48)
? 2 5 11% (16) 4% (4) 2% (4) 15% (12)
541
Computational Linguistics Volume 39, Number 3
R2 lemmas, and for OntoNotes the mean group sizes are 2.3 (R1) and 2.4 (R2). More
likely it is due to the individual lemmas. We observe that ratings of ?similar? or higher
are frequent. In all conditions except WSsim-1/OntoNotes, we find percentages over
50%. On the other hand, there are many fewer sentences where two senses received
judgments of ?very similar? or ?identical? but were not in the same OntoNotes or
EAW group, but these cases do exist. For example, there were five sentences with the
target dismiss.v which in WSsim received an average judgment of 4 or 5 for senses from
two different OntoNotes groups, 1.1 and 1.2. As dismiss is an R2 lemma, for which
only 10 sentences were annotated, this means that this phenomenon was found in
half the sentences annotated. The two sense groups are related: One is a literal, the
other a metaphorical, use of the verb. OntoNotes group 1.1 is defined as ?refuse to give
consideration to something or someone,? and group 1.2 is ?discharge, let go, persuade
to leave, send away.? One such sentence was the second sentence in Table 13.
Table 27 lists the number of sentences where two senses in the same OntoNotes
or EAW grouping received widely different ratings in the WSsim annotation. The first
row shows how many sentences there were where one sense received a rating of ? 2
and another sense from the same OntoNotes or EAW group had a rating of ? 4. The
second row shows the same for sense pairs in the same coarse-grained group where
one received a rating of ? 2 and the other the highest possible rating of 5. (Note that
the table considers judgments averaged over all annotators, so this row counts only
sentences where all annotators agreed on the highest rating.) An example of such a case
is Rich people manage their money well. In WSsim the first sense in WordNet (possessing
material wealth) received an average score of 5 (i.e. a unanimous verdict), whereas all
other senses received a score of less than 2. This included the third sense (of great worth
or quality; ?a rich collection of antiques?), which had an average of 1.625, and sense 8
(suggestive of or characterized by great expense; ?a rich display?) with an average of 1.125.
Both senses 3 and 8 are in the same group as sense 1 in EAW.
These are sentences where the WSsim annotation suggests a more fine-grained
analysis than the OntoNotes and EAW groups offer. The percentages are substantial:
For the more inclusive analysis in the first row, the numbers are between 20% and 60%
of all sentences, and between 2% and 15% of sentences even fall into the more restrictive
case in the second row. There is no clear trend in whether we see more of this type of
disagreement for OntoNotes or for EAW, or for the first or the second round of WSsim
annotation.
We see that there are a considerable number of sentences where either two senses
from the same OntoNotes or EAW group have received diverging WSsim ratings, or
two senses from different groups have received high ratings. In this way, the WSsim
annotation can be used to scrutinize sense groupings: If one aim of the sense groupings
is to form groups that would match intuitive sense judgments by untrained subjects,
then WSsim annotation would suggest that the senses of dismiss.v that correspond to
?dismiss a person? and ?dismiss an idea? may be too close together to be placed in
different groups.
5.3 Inducing Sense Relatedness from WSsim Annotation
In the WSsim annotation, annotators have annotated each occurrence of a target word
with a rating for each of the WordNet senses for the target, as illustrated in Tables 6?8.
This allows us, conversely, to chart the ratings that a WordNet sense received across
all sentences. Table 28 shows this chart for two senses of the noun account. In the
table, ratings are averaged across all annotators. In this case, the averaged ratings are
542
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Table 28
WSsim ratings for two senses of the noun account for 10 annotated sentences (averaged over
annotators).
WordNet Sentence
sense 1 2 3 4 5 6 7 8 9 10
1 1.00 2.25 1.13 4.25 1.13 1.0 1.13 1.13 1.13 4.25
4 1.50 3.00 1.25 2.88 1.50 1.50 1.63 1.00 1.38 3.88
Figure 4
Correlation between sense pairs: Distribution of rho values (Spearman?s rho).
similar for the two senses: They tend to be high for the same sentences, and low for the
same sentences. In general, senses that are closely related should tend to receive similar
ratings: high on the same sentences, and low on the same sentences, as illustrated for
the two senses in Table 28.
This then means that we can test the correlation on the ratings for two senses to see
if the WSsim annotators perceived them to be similar. We compute correlation for any
pair of senses for a common lemma, again using Spearman?s rho.25 Figure 4 shows the
distribution of rho values obtained for all the sense pairs, as histograms for R1 (left)
and R2 (right). When two senses are strongly positively correlated, this means that
the annotators likely viewed them as similar. When two senses are strongly negatively
correlated, this means they are probably so different that they tend never to be assigned
high ratings for the same sentences. We see that in both rounds, there were roughly as
many positive correlations as negative correlations. In R1, the rho values seem more or
less equally distributed over the range from ?1 to 1. In R2, there were more annotators
and the distribution is closer to a normal distribution with more rho values close to 0.
25 We exclude senses that received a uniform rating of 1 on all items. For R1 there were no such cases and
for R2 there were only 14 out of a total of 275 senses.
543
Computational Linguistics Volume 39, Number 3
We have shown the OntoNotes and EAW sense groups for the noun account. We can
now look at the WSsim-derived correlations for the same lemma, shown in Figure 5. The
first row in each box shows the WordNet sense number, and the second row shows the
OntoNotes and EAW sense groups. All three labels are those used in Table 24. Each edge
represents a correlation in the WSsim annotation. To avoid clutter, only correlations
with rho ? 0.5 are included, and a sense is only shown if it is correlated with any other
sense. Edge thickness corresponds to the value of the correlation coefficient rho between
each two senses; rho is also annotated on the edges. The first thing to note is that WSsim-
based correlation does not give us sense groups. Correlations are of different strengths,
and different cutoffs would result in different link graphs. Even for the chosen cutoff
of rho = 0.5, the correlations do not induce cliques (in the graph-theoretic sense). For
example, the sixth sense of account shows a correlation of rho ? 0.5 with the eighth
sense, but not with any of the other senses to which the eighth sense is linked. The
figure also shows that there are some senses that are strongly correlated in their annota-
tion but are not grouped in one or the other of the existing groupings. For example,
senses 3 (the executive who handles his account) and 7 (account statement) are strongly
correlated, but are in different groups in OntoNotes as well as in EAW. There are also
senses that share the same group in one of the coarse grained inventories, but have
a weak or even a negative correlation based on the WSsim annotation. For example,
Figure 5
Sense correlation in the WSsim annotation for the noun account. Showing all correlations with
rho ? 0.5. Upper row in each box: WordNet sense number. Lower row: OntoNotes and EAW
sense groups. Edge thickness expresses strength of correlation.
544
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Figure 6
Overall correlation versus annotation in single sentences: Number of sentences in which two
senses with an overall correlation ? ? have both been annotated with a judgment of ? j, for
j = 3, 4, 5. (Judgments averaged over annotators.)
for the lemma paper, senses 1 (a material made from cellulose pulp) and 4 (a medium for
written communication) are in the same EAW group, but have a correlation in WSsim of
rho = ?0.52.
In Section 5.2 we asked whether the many cases where WSsim annotators gave
high ratings to more than a single sense could possibly be explained by them im-
plicitly using more coarse-grained senses. We answered this question by comparing
the WSsim annotation to OntoNotes and EAW sense groups, finding a considerable
number of sentences where two senses received a high rating but were not from the
same sense group. Now we can repeat the question, but try to answer it using the
WSsim sense relations obtained from correlation: Is it possible that WSsim annotators
implicitly used more coarse-grained senses, but just not the OntoNotes or EAW sense
groups?
We tested how often annotators gave ratings of at least similar (i.e., ratings ? 3) to
senses that were related at a level ? rho, for rho ranging from ?1 to 1. The question
that we want to answer is: If annotators give high ratings to multiple senses on the
same sentence, is it always to senses that are strongly positively correlated, or do they
sometimes pick multiple senses that are not strongly correlated, or even senses that
are negatively correlated? The results are shown in Figure 6. First, we can see that
there is a sizeable number of sentences where two senses that are negatively correlated
have both received a positive judgment. For R1, the numbers for negatively correlated
senses are 135 ( j ? 3), 29 ( j ? 4), and 2 ( j = 5). For R2, the numbers of sentences are
lower absolutely and in proportion, with 29 ( j ? 3), 7 ( j ? 4), and 0 ( j = 5). It is also
interesting to look at a less stringent threshold than rho ? 0; we can use the significance
levels p ? 0.05 and p ? 0.01 for this. If we look at sense pairs that were not positively
correlated at p ? 0.05 (p ? 0.01), there were 185 (205) sentences in R1 and 54 (88)
sentences in R2 where two such senses both received judgments of 3 or higher. Note
that the significance levels of p ? 0.05, p ? 0.01 are here just arbitrary thresholds at
which to inspect the data; they are not thresholds that determine the significance of
545
Computational Linguistics Volume 39, Number 3
some hypothesis.26 This brings us back to the question asked above of whether the
WSsim annotators implicitly used more coarse-grained senses. If they had implicitly
used more coarse-grained senses, we would have expected to see very few cases where
unrelated senses got a high rating on the same sentence. What we found instead was
that such cases were relatively frequent, which implies that WSsim annotators in both
rounds ?mix and match? senses specifically for each sentence that they evaluate. For
example, the senses 1 (she dismissed his advances) and 5 (I was dismissed after I gave my
report) of dismiss are negatively correlated (rho = ?0.61) yet have average judgments of
3.25 and 4.125 on the second example in Table 13.
5.3.1 Summary. In this section we have analyzed the WSsim annotation in comparison
with more coarse-grained sense repositories. One aim was to find out whether anno-
tators really used the fine granularity that the WSsim task offered or whether they
implicitly used more coarse-grained senses. Both by comparing the WSsim annotation
to coarse-grained OntoNotes and EAW sense groups, and by comparing the WSsim
annotation to the sense relations implied by WSsim, we find that annotators did make
use of the ability to combine sense ratings in a way that was particular to each sentence
they annotated. We also conclude that WSsim annotation can be used to evaluate
OntoNotes and EAW groupings with respect to the level to which senses are intuitively
distinguishable to untrained subjects. Here, WSsim annotation can uncover senses in
different groups that WSsim annotators often conflate, or senses in a single coarse group
that WSsim annotators treat differently.
6. Usim and Sense Groupings
One of the major motivations for the Usim task is that it allows us to examine the
meanings of words without recourse to a predefined inventory. We have demonstrated
in this paper that the data from this task can be compared directly to paraphrase
data as well as to data annotated for word sense. In the previous section we have
focused on using our WSsim data to examine existing sense groupings. WSsim is useful
precisely because it has sense annotations from an existing inventory, WordNet, so we
can use the graded annotations to see how these senses relate, and also relationships
between coarser grained inventories with mappings to WordNet. Usim does not capture
this information, nevertheless it might be useful as a resource for examining sense
groupings. We can use it to examine the extent to which sense groupings keep usages
together that have a high usage similarity according to Usim, and keep sentences
with low usage similarity apart. In this analysis, we use the data from R2 because
this has Usim judgments for sentences alongside traditional word sense annotations
(WSbest). As WSbest annotation, we use the mode of the chosen senses27 (as in the
analysis in Section 4.7) for each sentence in R2, and map it to its coarse-grained sense in
26 We are performing multiple tests on the same senses, which increases the likelihood of falsely assuming
two senses to be significantly correlated at some significance level (Type I errors). The significance levels
are only arbitrary thresholds in our case, however. In addition, our analysis focuses on sense pairs that
are not significantly positively correlated. For that reason, Type I errors actually reduce our estimate
of the number of sentences in which two non-related senses both received high ratings. Conversely,
correcting for multiple testing makes our estimate less conservative: If we count sentences with positive
ratings for sense pairs that are not positively correlated at p ? 0.05 with Bonferroni correction, the number
of sentences rises from 185 to 207 for judgments of 3 or higher.
27 We perform this analysis only on sentences where there was one sense found as mode and where this had
a coarse-grained mapping in either the EAW or OntoNotes resources.
546
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
EAW and/or OntoNotes. We then compute the average Usim similarity for all pairs of
sentences with the same coarse-grained sense, and compare it with the average Usim
similarity for sentence pairs with different coarse-grained senses. The results are shown
in the first row of figures in Table 29. We see that the OntoNotes and EAW sense groups
do indeed partition the sentences such that pairs within the same group have high usage
similarity (4 or above) and those in different groups have low usage similarity (2 or
below).
The second part of Table 29 performs the same analysis on the basis of individual
lemmas. A dash (?) means that either there was no coarse mapping, or there were no
sentence pairs in this category. For example, there were no sentence pairs identified
as having different OntoNotes groups or EAW groups for the lemma suffer.v. For the
lemmas call.v and dismiss.v, the two sense inventories give rise to the same groupings of
sentence pairs.
In the table, we see many lemmas where the groupings concur with the Usim
judgments. One example is account.n, where the sentence pairs in the same coarse group
get high average Usim values, whereas sentence pairs with different coarse groups have
low average Usim values. There are, however, a few lemmas where the average Usim
values indicate that either the coarse groupings might benefit from another inspection,
or that the lemma has meanings with subtle relationships where grouping is not a
straightforward exercise. One example is new.a, which has the same high Usim values
for both same and different categories in EAW. Another is shed.v, where the sentences
annotated with the same OntoNotes groups actually have a lower average Usim value
than those with different groups.
We can also use Usim judgments to analyze individual sense groups. This could
be useful in determining specific groups that might warrant further revision, or that
represent meanings which are simply difficult to distinguish. To demonstrate this, we
analyzed all coarse-grained sense groups with at least one sentence pair in R2, that is, all
groups that had at least two R2 sentences whose WSbest mode mapped to that coarse
Table 29
Average Usim rating for R2 where WSbest annotations suggested the same or different coarse
grouping.
OntoNotes EAW
same different same different
4.0 1.9 4.1 2.0
by lemma
account.n 4.0 1.6 4.0 1.5
call.v 4.3 1.4 4.3 1.4
coach.n 4.6 2.3 ? ?
dismiss.v 3.8 2.6 3.8 2.6
fire.v 4.6 1.2 ? ?
fix.v 4.2 1.1 ? ?
hold.v 4.5 2.0 3.8 1.9
lead.v ? ? 2.9 1.5
new.a ? ? 4.6 4.6
order.v 4.3 1.7 ? ?
rich.a ? ? 4.6 2.0
shed.v 2.9 3.3 ? ?
suffer.v 4.2 ? 4.2 ?
547
Computational Linguistics Volume 39, Number 3
group. (Naturally, due to the skewed nature of sense distributions and the fact that
we only have ten sentences for each lemma, some groups do not meet this criterion.)
We find that the majority of groups that were analyzed have an average Usim rating
of over 4. This is the case for 75% of the analyzed EAW groups and 76% of OntoNotes
groups. There were, however, groups with very low values. One example was group 1.1
of shed.v in OntoNotes, with an average Usim rating of 2.9. This group includes both
literal senses (trees shed their leaves) and metaphorical senses (he shed his image as a
pushy boss) of the verb shed. Another example is group 7 of lead.n in EAW, also with
an average Usim of 2.9. This group includes taking the lead as well as lead actor, so quite
a diverse collection of usages. Two example sentences annotated with these two senses
are shown here. This pair had an average Usim value of 1.25.
My students perform a wide variety of music and they can be found singing leading
roles in their high school and college musical productions, singing lead in rock and
wedding bands, winning classical music competitions, singing at the summer
conservatory of The Papermill Playhouse, and learning to sing so they can sing with
local choirs.
And as a result of President Bush?s initiative, which he took as part of the G-8
Presidency, and also the other changes in which the US, UK has been in the lead, not
least in Afghanistan and Iraq, you can now feel the winds of change blowing through
the Arab world.
In the future we hope to obtain more Usim data. When we have more data, we will
investigate whether the groupings that Usim identifies as problematic tend to be the
same ones that require more iterations in inventory construction (Hovy et al 2006). We
also plan to test whether groupings with low Usim ratings tend to have lower inter-
tagger agreement on traditional WSD annotation.
7. Computational Modeling
The graded meaning annotation data from Usim and WSsim annotation can be used to
evaluate computational models of word meaning. In this section we summarize existing
work on modeling the R1 data, which has already been made publicly available.
The WSsim data can be used to evaluate graded word meaning models as well
as traditional WSD systems. Instead of evaluating only the highest-confidence sense of a
WSD model, we can take a more nuanced look at a model?s predictions, and give credit if
it proposes multiple appropriate senses. In Erk and McCarthy (2009) we take advantage
of this fact to evaluate and compare two supervised models on the WSsim data: a
traditional WSD model, and a distributional model that forms one prototype vector for
each sense of a given lemma. Both are trained on traditional single-sense annotation, but
the prototype model does not see any negative data during training in order to avoid
spurious negative data. For training, each word occurrence is represented as either a
first-order or a second-order bag-of-words vector of its sentence. In an evaluation using
weighted variants of precision and recall, we find that when the traditional WSD model
is credited for all the senses that it proposes, rather than only the single sense with
the highest confidence value, it does much better on both measures. This shows that
the model does propose multiple appropriate senses, such that its performance may be
underestimated in traditional evaluation. As was to be expected, the prototype models
that do not see negative data during training have much higher recall at lower precision,
for an overall better F-score (again using weighted variants of the evaluation measures).
548
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Thater, Fu?rstenau, and Pinkal (2010) address the WSsim data with an unsupervised
model. It represents a word sense as the sum of the vectors for all synonyms in its synset,
plus the vectors for all hypernyms scaled down by a factor of 10. They also use a more
complex, syntax-based model to derive occurrence representations. Unfortunately their
results are not directly comparable to Erk and McCarthy (2009) because they evaluate
on a subset of the data (verb lemmas only).
The Usim data, which directly describes the similarity of pairs of usages, can be
used to evaluate distributional models of word meaning in context. So far, only one type
of model has been evaluated on this data to the best of our knowledge: the clustering-
based approach of Reisinger and Mooney (2010). They use the Usim data to test to what
extent their clusters correspond to human intuitions on a word?s senses. Their result is
negative, as a low correlation of human judgments and predictions suggests to them
that the induced clusters are not a good match for human senses. The Usim data is
particularly interesting for a different way of evaluating distributional and vector space
approaches for word meaning in context. These have been evaluated on the tasks of
lexical substitution (Erk and Pado 2008; Dinu and Lapata 2010; Thater, Fu?rstenau, and
Pinkal 2010; Van de Cruys, Poibeau, and Korhonen 2011), information retrieval, and
word sense disambiguation (Schu?tze 1998), but Usim, in contrast, offers a different and
more direct evaluation perspective.
8. Conclusion
In this paper we have explored the question of whether word meaning can be described
in a graded fashion. Our aim has been to use annotation with graded ratings to capture
untrained speakers? intuitions on word meaning. Our motivation has been two-fold. On
the one hand we are drawing on current theories of cognition, which hold that mental
concepts have ?fuzzy boundaries.? On the other hand we wanted to give a basis to
current computational models for word meaning in context that predicts degrees of
similarity between word occurrences. We have addressed this question through two
novel types of graded annotations of word meaning in context that draws on methods
from psycholinguistic experimentation. WSsim obtains word sense annotations from a
given sense inventory but uses graded judgments for each sense. Usim judges similarity
of pairs of usages of the same lemma.
The analysis of annotation results lets us answer our main question in the affir-
mative. Annotators can describe word meaning through graded ratings with good
inter-annotator agreement, measured through pairwise correlation. Even though no in-
depth training on sense distinctions was provided, the pairwise correlations were good
in every single case, indicating that all annotators did the tasks in a similar fashion.
In both tasks, all annotators made use of the full graded scale, and did not treat the
task as binary. The Usim annotation provides us with a means of comparing different
word meaning annotation paradigms. We have used it to demonstrate that there is
strong correlation of these new annotations with both traditional WSD labels, and with
overlap of lexical paraphrases. This is as we anticipated, as all of these annotations are
describing the same phenomenon of word meaning in context through different means.
In additional analysis of the WSsim annotation, we found a high proportion of
sentences (between 23% and 46%) in which multiple senses received high positive
judgments from the same annotators. At the same time, annotators used the WSsim
ratings in a nuanced and fine-grained fashion, sometimes assigning high ratings on the
same sentence to two senses that overall patterned very differently. Analyzing Usim
annotation, we found that all annotators? ratings obey the triangle inequality in almost
549
Computational Linguistics Volume 39, Number 3
all cases. This can be taken as a measure of intra-annotator consistency on the task.
It also means that current distributional approaches to word meaning in context are
justified in viewing usage similarity as metric. Triangle inequality can be used to check
the validity of future Usim annotation.
We do not propose that either one of our annotations is a panacea for evaluation
of systems that represent word meaning in context, but we argue that they provide
data sets that better reflect the fluid nature of word meaning and allow us to evaluate
questions related to word meaning in a new fashion. In this paper, we have used
both WSsim and Usim data to analyze existing coarse-grained sense inventories. We
have demonstrated that it is often not straightforward to group sentences into disjoint
senses, depending on the lemma. We have also shown how both WSsim and Usim style
judgments can be used to identify problematic lemmas, as well as sense groupings that
may warrant another inspection to check whether they match naive speakers? intuitive
judgments. The graded annotation can also be used to identify lemmas whose usages
are difficult to group into clear distinct senses. This information can in the future be
used to handle such lemmas differently when making sense inventories, in annotation,
and in computational systems.
An important next question to consider is the use of WSsim and Usim data to eval-
uate computational models of word meaning. As we have shown (Erk and McCarthy
2009), WSsim data can be used to evaluate traditional WSD systems in a graded fashion.
We plan to do a more large-scale evaluation to assess to what extent the performance
of current WSD systems is underestimated. Also, fine-grained WSsim annotation can be
used for a comparison of fine-grained and coarse-grained traditional WSD systems. We
have also shown (Erk and McCarthy 2009) that WSsim can be used to evaluate graded
word sense assignment systems. Although we used a supervised setting, however, we
trained on traditional sense annotation. We plan to collect more WSsim annotation in
order to be able to train word sense assignment systems on graded data, for example,
using a regression model.
In the same vein, we will extend the available Usim data to cover many more
sentences by using crowdsourcing. The use of Usim for supervised training of word
meaning models is particularly interesting as all existing usage similarity models are
unsupervised; given previous results in WSD, we can expect that supervision will
improve the performance of models of usage meaning. One way of using Usim data
in training is to learn a similarity metric. Metric learning (see, e.g., Davis et al 2007)
induces a distance measure from given constraints stating similarity or dissimilarity of
items.
Our novel graded annotation frameworks, WSsim and Usim, are validated both
through good agreement between those data sets themselves, as well as good agreement
between those data sets and traditional word sense annotation and lexical substitutions.
Because all labeling schemes provide comparable results, this allows different ways of
evaluating systems providing different perspectives on system output. Furthermore,
the different paradigms may suit different types of systems. Lexical substitution tasks
(McCarthy 2002; McCarthy and Navigli 2009) are particularly useful where the ap-
plication being considered would benefit from lexical paraphrasing, for example, text
simplification, summarization, or query expansion in information retrieval. WSsim is
closest to the traditional methodology (WSbest) and would suit systems needing to
output WordNet sense labels, for example, because they want to exploit the semantic
relations in WordNet for tasks such as inferencing or producing lexical chains. Unlike
WSbest, it avoids a winner-takes-all approach and allows for more nuanced sense
tagging. Usim is application-independent. It allows for evaluation of systems that relate
550
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
usages, whether into clusters or simply on a continuum. It could, for example, be used
as a resource-independent gold standard for word sense induction by calculating the
within and across class similarities. Aside from its use as an enabling technology within
a natural language processing application, a system that performs well at the Usim task
may be useful in its own right. For example, it could be used to enable lexicographers
to work on groups of examples that reflect similar meanings, or find further examples
close to the one being scrutinized.
Acknowledgments
The annotation was funded by a UK Royal
Society Dorothy Hodgkin Fellowship to
Diana McCarthy. This work was supported
by National Science Foundation grant
IIS-0845925 for Katrin Erk. We are grateful
to Huw McCarthy for implementing the
interface for round 2 of the annotation. We
thank the anonymous reviewers for many
helpful comments and suggestions.
References
Agirre, Eneko and Philip Edmonds,
editors. 2007. Word Sense Disambiguation:
Algorithms and Applications. Springer,
Dordrecht.
Agirre, Eneko, Llu??s Ma`rquez, and Richard
Wicentowski, editors. 2007. Proceedings
of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007).
Prague.
Baroni, Marco and Roberto Zamparelli. 2010.
Nouns are vectors, adjectives are matrices:
Representing adjective-noun constructions
in semantic space. In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing, pages 1,183?1,193,
Cambridge, MA.
Brown, Susan. 2008. Choosing sense
distinctions for WSD: Psycholinguistic
evidence. In Proceedings of ACL-08: HLT,
Short Papers (Companion Volume),
pages 249?252, Columbus, OH.
Brown, Susan. 2010. Finding Meaning:
Sense Inventories for Improved Word Sense
Disambiguation. Ph.D. thesis, University
of Colorado at Boulder.
Burchardt, Aljoscha, Katrin Erk, Annette
Frank, Andrea Kowalski, Sebastian Pado,
and Manfred Pinkal. 2006. The SALSA
corpus: A German resource for lexical
semantics. In Proceedings of the Fifth
International Conference on Language
Resources and Evaluation (LREC 2006),
pages 969?974, Genoa.
Carpuat, Marine and Dekai Wu. 2007a. How
phrase sense disambiguation outperforms
word sense disambiguation for statistical
machine translation. In Proceedings
of the 11th Conference on Theoretical and
Methodological Issues in Machine Translation
(TMI 2007), pages 43?52, Skovde.
Carpuat, Marine and Dekai Wu. 2007b.
Improving statistical machine translation
using word sense disambiguation. In
Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning (EMNLP-CoNLL 2007),
pages 61?72, Prague.
Chen, Jinying and Martha Palmer. 2009.
Improving English verb sense
disambiguation performance with
linguistically motivated features and clear
sense distinction boundaries. Journal of
Language Resources and Evaluation (Special
Issue on SemEval-2007), 43:181?208.
Coecke, Bob, Mehrnoosh Sadrzadeh, and
Stephen Clark. 2010. Mathematical
foundations for a compositional
distributed model of meaning. Lambek
Festschrift, Linguistic Analysis, 36:345?384.
Coleman, Linda and Paul Kay. 1981.
Prototype semantics: The English word
?lie.? Language, 57:26?44.
Copestake, Ann and Ted Briscoe. 1995.
Semi-productive polysemy and sense
extension. Journal of Semantics, 12:15?67.
Cruse, D. A. 1995. Polysemy and related
phenomena from a cognitive linguistic
viewpoint. In Philip Saint-Dizier and
Evelyne Viegas, editors, Computational
Lexical Semantics. Cambridge University
Press, pages 33?49.
Davis, Jason, Brian Kulis, Prateek Jain,
Suvrit Sra, and Inderjit Dhillon. 2007.
Information-theoretic metric learning.
In Proceedings of the 24th International
Conference on Machine Learning,
pages 209?216, Corvallis, OR.
Deschacht, Koen and Marie-Francine Moens.
2009. Semi-supervised semantic role
labeling using the Latent Words Language
Model. In Proceedings of EMNLP,
pages 21?29, Singapore.
Dinu, Georgiana and Mirella Lapata. 2010.
Measuring distributional similarity in
context. In Proceedings of the 2010
551
Computational Linguistics Volume 39, Number 3
Conference on Empirical Methods in Natural
Language Processing, pages 1,162?1,172,
Cambridge, MA.
Edmonds, Philip and Scott Cotton, editors.
2001. Proceedings of the SensEval-2
Workshop. Toulouse. See http://www.sle.
sharp.co.uk/senseval.
Erk, Katrin and Diana McCarthy. 2009.
Graded word sense assignment. In
Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing, pages 440?449, Singapore.
Erk, Katrin, Diana McCarthy, and Nicholas
Gaylord. 2009. Investigations on word
senses and word usages. In Proceedings
of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International
Joint Conference on Natural Language
Processing of the AFNLP, pages 10?18,
Suntec.
Erk, Katrin and Sebastian Pado. 2008. A
structured vector space model for word
meaning in context. In Proceedings of
EMNLP-08, pages 897?906, Waikiki, HI.
Erk, Katrin and Sebastian Pado. 2010.
Exemplar-based models for word meaning
in context. In Proceedings of the ACL 2010
Conference Short Papers, pages 92?97,
Uppsala.
Erk, Katrin and Carlo Strapparava, editors.
2010. Proceedings of the 5th International
Workshop on Semantic Evaluation(SemEval).
Uppsala.
Frazier, Lyn and Keith Rayner. 1990. Taking
on semantic commitments: Processing
multiple meanings vs. multiple senses.
Journal of Memory and Language,
29:181?200.
Gentner, Dedre and Virginia Gunn. 2001.
Structural alignment facilitates the
noticing of differences. Memory and
Cognition, 21:565?577.
Gentner, Dedre and Arthur Markman. 1997.
Structural alignment in analogy and
similarity. American Psychologist, 52:45?56.
Grefenstette, Edward and Mehrnoosh
Sadrzadeh. 2011. Experimental
support for a categorical compositional
distributional model of meaning.
In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language
Processing, pages 1,394?1,404, Edinburgh.
Hampton, James A. 1979. Polymorphous
concepts in semantic memory. Journal
of Verbal Learning and Verbal Behavior,
18:441?461.
Hampton, James A. 2007. Typicality, graded
membership, and vagueness. Cognitive
Science, 31:355?384.
Hanks, Patrick. 2000. Do word meanings
exist? Computers and the Humanities,
34(1?2):205?215.
Hovy, Eduard H., Mitchell Marcus, Martha
Palmer, Lance Ramshaw, and Ralph
Weischedel. 2006. OntoNotes: The 90%
solution. In Proceedings of the Human
Language Technology Conference of the North
American Chapter of the ACL (NAACL-2006),
pages 57?60, New York.
Ide, Nancy and Yorick Wilks. 2006. Making
sense about sense. In Eneko Agirre and
Philip Edmonds, editors, Word Sense
Disambiguation, Algorithms and Applications.
Springer, Dordrecht, pages 47?73.
Kilgarriff, Adam. 1992. Polysemy. Ph.D.
thesis, University of Sussex.
Kilgarriff, Adam. 1997. I don?t believe in
word senses. Computers and the Humanities,
31(2):91?113.
Kilgarriff, Adam. 2006. Word senses.
In Eneko Agirre and Philip Edmonds,
editors, Word Sense Disambiguation:
Algorithms and Applications. Springer,
Dordrecht, pages 29?46.
Kilgarriff, Adam and Martha Palmer,
editors. 2000. Senseval: Special Issue of the
Journal Computers and the Humanities,
volume 34(1?2). Kluwer, Dordrecht.
Kilgarriff, Adam and Joseph Rosenzweig.
2000. Framework and results for English
Senseval. Computers and the Humanities,
34(1-2):15?48.
Kintsch, Walter. 2007. Meaning in context. In
T. K. Landauer, D. McNamara, S. Dennis,
and W. Kintsch, editors, Handbook of Latent
Semantic Analysis. Erlbaum, Mahwah, NJ,
pages 89?105.
Klein, Devorah and Gregory Murphy. 2001.
The representation of polysemous words.
Journal of Memory and Language,
45:259?282.
Klein, Devorah and Gregory Murphy. 2002.
Paper has been my ruin: Conceptual
relations of polysemous senses. Journal of
Memory and Language, 47:548?570.
Klepousniotou, Ekaterini. 2002. The
processing of lexical ambiguity:
Homonymy and polysemy in the mental
lexicon. Brain and Language, 81:205?223.
Klepousniotou, Ekaterini, Debra Titone, and
Caroline Romero. 2008. Making sense of
word senses: The comprehension of
polysemy depends on sense overlap.
Journal of Experimental Psychology: Learning,
Memory, and Cognition, 34(6):1,534?1,543.
Krishnamurthy, Ramesh and Diane
Nicholls. 2000. Peeling an onion: The
lexicographers? experience of manual
552
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
sense-tagging. Computers and the
Humanities, 34(1-2):85?97.
Landauer, Thomas and Susan Dumais. 1997.
A solution to Plato?s problem: The Latent
Semantic Analysis theory of acquisition,
induction, and representation of
knowledge. Psychological Review,
104:211?240.
Landes, Shari, Claudia Leacock, and
Randee Tengi. 1998. Building semantic
concordances. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical
Database. The MIT Press, Cambridge, MA.
Lefever, Els and Ve?ronique Hoste. 2010.
Semeval-2010 task 3: Cross-lingual word
sense disambiguation. In Proceedings of the
5th International Workshop on Semantic
Evaluation, pages 15?20, Uppsala.
McCarthy, Diana. 2002. Lexical substitution
as a task for wsd evaluation. In Proceedings
of the ACL Workshop on Word Sense
Disambiguation: Recent Successes and
Future Directions, pages 109?115,
Philadelphia, PA.
McCarthy, Diana and Roberto Navigli. 2007.
SemEval-2007 task 10: English lexical
substitution task. In Proceedings of the
4th International Workshop on Semantic
Evaluations (SemEval-2007), pages 48?53,
Prague.
McCarthy, Diana and Roberto Navigli. 2009.
The English lexical substitution task.
Language Resources and Evaluation Special
Issue on Computational Semantic Analysis
of Language: SemEval-2007 and Beyond,
43(2):139?159.
McNamara, Timothy P. 2005. Semantic
Priming: Perspectives from Memory and Word
Recognition. Psychology Press, New York.
Mihalcea, Rada and Timothy Chklovski.
2003. Open Mind Word Expert: Creating
large annotated data collections with web
users? help. In Proceedings of the EACL 2003
Workshop on Linguistically Annotated
Corpora (LINC 2003), pages 53?60,
Budapest.
Mihalcea, Rada, Timothy Chklovski, and
Adam Kilgarriff. 2004. The SENSEVAL-3
English lexical sample task. In Rada
Mihalcea and Phil Edmonds, editors,
Proceedings SENSEVAL-3 Second
International Workshop on Evaluating
Word Sense Disambiguation Systems,
pages 25?28, Barcelona.
Mihalcea, Rada and Phil Edmonds, editors.
2004. Proceedings SENSEVAL-3 Second
International Workshop on Evaluating
Word Sense Disambiguation Systems,
Barcelona.
Mihalcea, Rada, Ravi Sinha, and Diana
McCarthy. 2010. Semeval-2010 task 2:
Cross-lingual lexical substitution. In
Proceedings of the 5th International
Workshop on Semantic Evaluation,
pages 9?14, Uppsala.
Miller, George A., Claudia Leacock,
Randee Tengi, and Ross T Bunker.
1993. A semantic concordance. In
Proceedings of the ARPA Workshop
on Human Language Technology,
pages 303?308, Plainsboro, NJ.
Mitchell, Jeff and Mirella Lapata. 2008.
Vector-based models of semantic
composition. In Proceedings of ACL-08:
HLT, pages 236?244, Columbus, OH.
Mitchell, Jeff and Mirella Lapata. 2010.
Composition in distributional models
of semantics. Cognitive Science,
34(8):1388?1429.
Moon, Taesun and Katrin Erk. In press.
An inference-based model of word
meaning in context as a paraphrase
distribution. ACM Transactions on
Intelligent Systems and Technology
special issue on paraphrasing.
Murphy, Gregory L. 1991. Meaning and
concepts. In Paula Schwanenflugel, editor,
The Psychology of Word Meanings. Lawrence
Erlbaum Associates, Mahwah, NJ,
pages 11?35.
Murphy, Gregory L. 2002. The Big Book of
Concepts. MIT Press, Cambridge, MA.
Navigli, Roberto. 2009. Word sense
disambiguation: a survey. ACM
Computing Surveys, 41(2):1?69.
Navigli, Roberto, Kenneth C. Litkowski,
and Orin Hargraves. 2007. SemEval-2007
task 7: Coarse-grained English all-words
task. In Proceedings of the 4th International
Workshop on Semantic Evaluations
(SemEval-2007), pages 30?35, Prague.
Palmer, Martha, Hoa Trang Dang, and
Christiane Fellbaum. 2007. Making
fine-grained and coarse-grained sense
distinctions, both manually and
automatically. Natural Language
Engineering, 13:137?163.
Passonneau, Rebecca, Ansaf Salleb-Aouissi,
Vikas Bhardwaj, and Nancy Ide. 2010.
Word sense annotation of polysemous
words by multiple annotators. In
Proceedings of LREC-7, pages 3,244?3,249,
Valleta.
Pickering, Martin and Steven Frisson. 2001.
Processing ambiguous verbs: Evidence
from eye movements. Journal of
Experimental Psychology: Learning,
Memory, and Cognition, 27:556?573.
553
Computational Linguistics Volume 39, Number 3
Pradhan, Sameer, Edward Loper, Dmitriy
Dligach, and Martha Palmer. 2007.
Semeval-2007 task 17: English lexical
sample, SRL and all words. In 4th
International Workshop on Semantic
Evaluations (SemEval-4) at ACL-2007,
pages 87?92, Prague.
Preiss, Judita and David Yarowsky, editors.
2001. Proceedings of Senseval-2 Second
International Workshop on Evaluating Word
Sense Disambiguation Systems, Toulouse.
Pustejovsky, James. 1991. The generative
lexicon. Computational Linguistics,
17(4):409?441.
Reddy, Siva, Ioannis P. Klapaftis, Diana
McCarthy, and Suresh Manandhar. 2011.
Dynamic and static prototype vectors for
semantic composition. In Proceedings of The
5th International Joint Conference on Natural
Language Processing 2011 (IJCNLP 2011),
pages 210?218, Chiang Mai.
Reisinger, Joseph and Raymond J. Mooney.
2010. Multi-prototype vector-space models
of word meaning. In Proceedings of Human
Language Technologies: The 11th Annual
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 109?117, Los Angeles, CA.
Resnik, Philip and David Yarowsky. 2000.
Distinguishing systems and distinguishing
senses: New evaluation methods for word
sense disambiguation. Natural Language
Engineering, 5(3):113?133.
Rosch, Eleanor. 1975. Cognitive
representations of semantic categories.
Journal of Experimental Psychology: General,
104:192?233.
Rosch, Eleanor and Carolyn B. Mervis. 1975.
Family resemblance: Studies in the internal
structure of categories. Cognitive
Psychology, 7:573?605.
Schu?tze, Hinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97?123.
Senseval-2. 2001. Web page:
http://www.sle.sharp.co.uk/senseval2.
Snyder, Benjamin and Martha Palmer.
2004. The English all-words task. In
3rd International Workshop on Semantic
Evaluations (SensEval-3) at ACL-2004,
pages 41?43, Barcelona.
Socher, Richard, Eric H. Huang, Jeffrey
Pennin, Andrew Y. Ng, and Christopher D.
Manning. 2011. Dynamic pooling and
unfolding recursive autoencoders for
paraphrase detection. In Advances in
Neural Information Processing Systems 24,
pages 801?809, Grenada.
Stokoe, Christopher. 2005. Differentiating
homonymy and polysemy in information
retrieval. In Proceedings of HLT/EMNLP-05,
pages 403?410, Vancouver.
Taylor, John R. 2003. Linguistic Categorization.
Oxford University Press, New York.
Thater, Stefan, Hagen Fu?rstenau, and
Manfred Pinkal. 2010. Contextualizing
semantic representations using
syntactically enriched vector models.
In Proceedings of the 48th Annual Meeting
of the Association for Computational
Linguistics, pages 948?957, Uppsala.
Tuggy, David H. 1993. Ambiguity, polysemy
and vagueness. Cognitive Linguistics,
4(2):273?290.
Tversky, Amos. 1977. Features of similarity.
Psychological Review, 84(4):327?352.
Tversky, Amos and Itamar Gati. 1982.
Similarity, separability, and the triangle
inequality. Psychological Review,
89(2):123?154.
Van de Cruys, Tim, Thierry Poibeau, and
Anna Korhonen. 2011. Latent vector
weighting for word meaning in context.
In Proceedings of the 2011 Conference
on Empirical Methods in Natural
Language Processing, pages 1,012?1,022,
Edinburgh.
Washtell, Justin. 2010. Expectation vectors:
A semiotics inspired approach to
geometric lexical-semantic representation.
In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language
Semantics, pages 45?50, Uppsala.
Williams, John. 1992. Processing polysemous
words in context: Evidence for interrelated
meanings. Journal of Psycholinguistic
Research, 21:193?218.
Zhong, Zhi and Hwee Tou Ng. 2010. It
makes sense: A wide-coverage word
sense disambiguation system for free
text. In Proceedings of the ACL 2010
System Demonstrations, pages 78?83,
Uppsala.
Zhong, Zhi, Hwee Tou Ng, and Yee Seng
Chan. 2008. Word sense disambiguation
using OntoNotes: An empirical study.
In Proceedings of the 2008 Conference
on Empirical Methods in Natural
Language Processing, pages 1,002?1,010,
Honolulu, HI.
554
Proceedings of the ACL 2010 Conference Short Papers, pages 92?97,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Exemplar-Based Models for Word Meaning In Context
Katrin Erk
Department of Linguistics
University of Texas at Austin
katrin.erk@mail.utexas.edu
Sebastian Pado?
Institut fu?r maschinelle Sprachverarbeitung
Stuttgart University
pado@ims.uni-stuttgart.de
Abstract
This paper describes ongoing work on dis-
tributional models for word meaning in
context. We abandon the usual one-vector-
per-word paradigm in favor of an exemplar
model that activates only relevant occur-
rences. On a paraphrasing task, we find
that a simple exemplar model outperforms
more complex state-of-the-art models.
1 Introduction
Distributional models are a popular framework
for representing word meaning. They describe
a lemma through a high-dimensional vector that
records co-occurrence with context features over a
large corpus. Distributional models have been used
in many NLP analysis tasks (Salton et al, 1975;
McCarthy and Carroll, 2003; Salton et al, 1975), as
well as for cognitive modeling (Baroni and Lenci,
2009; Landauer and Dumais, 1997; McDonald and
Ramscar, 2001). Among their attractive properties
are their simplicity and versatility, as well as the
fact that they can be acquired from corpora in an
unsupervised manner.
Distributional models are also attractive as a
model of word meaning in context, since they do
not have to rely on fixed sets of dictionary sense
with their well-known problems (Kilgarriff, 1997;
McCarthy and Navigli, 2009). Also, they can
be used directly for testing paraphrase applicabil-
ity (Szpektor et al, 2008), a task that has recently
become prominent in the context of textual entail-
ment (Bar-Haim et al, 2007). However, polysemy
is a fundamental problem for distributional models.
Typically, distributional models compute a single
?type? vector for a target word, which contains co-
occurrence counts for all the occurrences of the
target in a large corpus. If the target is polyse-
mous, this vector mixes contextual features for all
the senses of the target. For example, among the
top 20 features for coach, we get match and team
(for the ?trainer? sense) as well as driver and car
(for the ?bus? sense). This problem has typically
been approached by modifying the type vector for
a target to better match a given context (Mitchell
and Lapata, 2008; Erk and Pado?, 2008; Thater et
al., 2009).
In the terms of research on human concept rep-
resentation, which often employs feature vector
representations, the use of type vectors can be un-
derstood as a prototype-based approach, which uses
a single vector per category. From this angle, com-
puting prototypes throws away much interesting
distributional information. A rival class of mod-
els is that of exemplar models, which memorize
each seen instance of a category and perform cat-
egorization by comparing a new stimulus to each
remembered exemplar vector.
We can address the polysemy issue through an
exemplar model by simply removing all exem-
plars that are ?not relevant? for the present con-
text, or conversely activating only the relevant
ones. For the coach example, in the context of
a text about motorways, presumably an instance
like ?The coach drove a steady 45 mph? would be
activated, while ?The team lost all games since the
new coach arrived? would not.
In this paper, we present an exemplar-based dis-
tributional model for modeling word meaning in
context, applying the model to the task of decid-
ing paraphrase applicability. With a very simple
vector representation and just using activation, we
outperform the state-of-the-art prototype models.
We perform an in-depth error analysis to identify
stable parameters for this class of models.
2 Related Work
Among distributional models of word, there are
some approaches that address polysemy, either
by inducing a fixed clustering of contexts into
senses (Schu?tze, 1998) or by dynamically modi-
92
fying a word?s type vector according to each given
sentence context (Landauer and Dumais, 1997;
Mitchell and Lapata, 2008; Erk and Pado?, 2008;
Thater et al, 2009). Polysemy-aware approaches
also differ in their notion of context. Some use a
bag-of-words representation of words in the cur-
rent sentence (Schu?tze, 1998; Landauer and Du-
mais, 1997), some make use of syntactic con-
text (Mitchell and Lapata, 2008; Erk and Pado?,
2008; Thater et al, 2009). The approach that we
present in the current paper computes a representa-
tion dynamically for each sentence context, using
a simple bag-of-words representation of context.
In cognitive science, prototype models predict
degree of category membership through similar-
ity to a single prototype, while exemplar theory
represents a concept as a collection of all previ-
ously seen exemplars (Murphy, 2002). Griffiths et
al. (2007) found that the benefit of exemplars over
prototypes grows with the number of available ex-
emplars. The problem of representing meaning in
context, which we consider in this paper, is closely
related to the problem of concept combination in
cognitive science, i.e., the derivation of representa-
tions for complex concepts (such as ?metal spoon?)
given the representations of base concepts (?metal?
and ?spoon?). While most approaches to concept
combination are based on prototype models, Voor-
spoels et al (2009) show superior results for an
exemplar model based on exemplar activation.
In NLP, exemplar-based (memory-based) mod-
els have been applied to many problems (Daele-
mans et al, 1999). In the current paper, we use an
exemplar model for computing distributional repre-
sentations for word meaning in context, using the
context to activate relevant exemplars. Comparing
representations of context, bag-of-words (BOW)
representations are more informative and noisier,
while syntax-based representations deliver sparser
and less noisy information. Following the hypothe-
sis that richer, topical information is more suitable
for exemplar activation, we use BOW representa-
tions of sentential context in the current paper.
3 Exemplar Activation Models
We now present an exemplar-based model for
meaning in context. It assumes that each target
lemma is represented by a set of exemplars, where
an exemplar is a sentence in which the target occurs,
represented as a vector. We use lowercase letters
for individual exemplars (vectors), and uppercase
Sentential context Paraphrase
After a fire extinguisher is used, it must
always be returned for recharging and
its use recorded.
bring back (3),
take back (2),
send back (1),
give back (1)
We return to the young woman who is
reading the Wrigley?s wrapping paper.
come back (3),
revert (1), revisit
(1), go (1)
Table 1: The Lexical Substitution (LexSub) dataset.
letters for sets of exemplars.
We model polysemy by activating relevant ex-
emplars of a lemma E in a given sentence context
s. (Note that we use E to refer to both a lemma
and its exemplar set, and that s can be viewed as
just another exemplar vector.) In general, we define
activation of a set E by exemplar s as
act(E, s) = {e ? E | sim(e, s) > ?(E, s)}
where E is an exemplar set, s is the ?point of com-
parison?, sim is some similarity measure such as
Cosine or Jaccard, and ?(E, s) is a threshold. Ex-
emplars belong to the activated set if their similarity
to s exceeds ?(E, s).1 We explore two variants of
activation. In kNN activation, the k most simi-
lar exemplars to s are activated by setting ? to the
similarity of the k-th most similar exemplar. In
q-percentage activation, we activate the top q%
of E by setting ? to the (100-q)-th percentile of the
sim(e, s) distribution. Note that, while in the kNN
activation scheme the number of activated exem-
plars is the same for every lemma, this is not the
case for percentage activation: There, a more fre-
quent lemma (i.e., a lemma with more exemplars)
will have more exemplars activated.
Exemplar activation for paraphrasing. A para-
phrases is typically only applicable to a particular
sense of a target word. Table 1 illustrates this on
two examples from the Lexical Substitution (Lex-
Sub) dataset (McCarthy and Navigli, 2009), both
featuring the target return. The right column lists
appropriate paraphrases of return in each context
(given by human annotators). 2 We apply the ex-
emplar activation model to the task of predicting
paraphrase felicity: Given a target lemma T in a
particular sentential context s, and given a list of
1In principle, activation could be treated not just as binary
inclusion/exclusion, but also as a graded weighting scheme.
However, weighting schemes introduce a large number of
parameters, which we wanted to avoid.
2Each annotator was allowed to give up to three para-
phrases per target in context. As a consequence, the number
of gold paraphrases per target sentence varies.
93
potential paraphrases of T , the task is to predict
which of the paraphrases are applicable in s.
Previous approaches (Mitchell and Lapata, 2008;
Erk and Pado?, 2008; Erk and Pado?, 2009; Thater
et al, 2009) have performed this task by modify-
ing the type vector for T to the context s and then
comparing the resulting vector T ? to the type vec-
tor of a paraphrase candidate P . In our exemplar
setting, we select a contextually adequate subset
of contexts in which T has been observed, using
T ? = act(T, s) as a generalized representation of
meaning of target T in the context of s.
Previous approaches used all of P as a repre-
sentation for a paraphrase candidate P . However,
P includes also irrelevant exemplars, while for a
paraphrase to be judged as good, it is sufficient that
one plausible reading exists. Therefore, we use
P ? = act(P, s) to represent the paraphrase.
4 Experimental Evaluation
Data. We evaluate our model on predicting para-
phrases from the Lexical Substitution (LexSub)
dataset (McCarthy and Navigli, 2009). This dataset
consists of 2000 instances of 200 target words in
sentential contexts, with paraphrases for each tar-
get word instance generated by up to 6 participants.
Paraphrases are ranked by the number of annota-
tors that chose them (cf. Table 1). Following Erk
and Pado? (2008), we take the list of paraphrase can-
didates for a target as given (computed by pooling
all paraphrases that LexSub annotators proposed
for the target) and use the models to rank them for
any given sentence context.
As exemplars, we create bag-of-words co-
occurrence vectors from the BNC. These vectors
represent instances of a target word by the other
words in the same sentence, lemmatized and POS-
tagged, minus stop words. E.g., if the lemma
gnurge occurs twice in the BNC, once in the sen-
tence ?The dog will gnurge the other dog?, and
once in ?The old windows gnurged?, the exemplar
set for gnurge contains the vectors [dog-n: 2, other-
a:1] and [old-a: 1, window-n: 1]. For exemplar
similarity, we use the standard Cosine similarity,
and for the similarity of two exemplar sets, the
Cosine of their centroids.
Evaluation. The model?s prediction for an item
is a list of paraphrases ranked by their predicted
goodness of fit. To evaluate them against a
weighted list of gold paraphrases, we follow Thater
et al (2009) in using Generalized Average Preci-
para- actT actP
meter kNN perc. kNN perc.
10 36.1 35.5 36.5 38.6
20 36.2 35.2 36.2 37.9
30 36.1 35.3 35.8 37.8
40 36.0 35.3 35.8 37.7
50 35.9 35.1 35.9 37.5
60 36.0 35.0 36.1 37.5
70 35.9 34.8 36.1 37.5
80 36.0 34.7 36.0 37.4
90 35.9 34.5 35.9 37.3
no act. 34.6 35.7
random BL 28.5
Table 2: Activation of T or P individually on the
full LexSub dataset (GAP evaluation)
sion (GAP), which interpolates the precision values
of top-n prediction lists for increasing n. Let G =
?q1, . . . , qm? be the list of gold paraphrases with
gold weights ?y1, . . . , ym?. Let P = ?p1, . . . , pn?
be the list of model predictions as ranked by the
model, and let ?x1, . . . , xn? be the gold weights
associated with them (assume xi = 0 if pi 6? G),
where G ? P . Let I(xi) = 1 if pi ? G, and zero
otherwise. We write xi = 1i
?i
k=1 xk for the av-
erage gold weight of the first i model predictions,
and analogously yi. Then
GAP (P,G) =
1
?m
j=1 I(yj)yj
n?
i=1
I(xi)xi
Since the model may rank multiple paraphrases the
same, we average over 10 random permutations of
equally ranked paraphrases. We report mean GAP
over all items in the dataset.
Results and Discussion. We first computed two
models that activate either the paraphrase or the
target, but not both. Model 1, actT , activates only
the target, using the complete P as paraphrase, and
ranking paraphrases by sim(P, act(T, s)). Model
2, actP, activates only the paraphrase, using s as
the target word, ranking by sim(act(P, s), s).
The results for these models are shown in Ta-
ble 2, with both kNN and percentage activation:
kNN activation with a parameter of 10 means that
the 10 closest neighbors were activated, while per-
centage with a parameter of 10 means that the clos-
est 10% of the exemplars were used. Note first
that we computed a random baseline (last row)
with a GAP of 28.5. The second-to-last row (?no
activation?) shows two more informed baselines.
94
The actT ?no act? result (34.6) corresponds to a
prototype-based model that ranks paraphrase can-
didates by the distance between their type vectors
and the target?s type vector. Virtually all exem-
plar models outperform this prototype model. Note
also that both actT and actP show the best results
for small values of the activation parameter. This
indicates paraphrases can be judged on the basis
of a rather small number of exemplars. Neverthe-
less, actT and actP differ with regard to the details
of their optimal activation. For actT , a small ab-
solute number of activated exemplars (here, 20)
works best , while actP yields the best results for
a small percentage of paraphrase exemplars. This
can be explained by the different functions played
by actT and actP (cf. Section 3): Activation of the
paraphrase must allow a guess about whether there
is reasonable interpretation of P in the context s.
This appears to require a reasonably-sized sample
from P . In contrast, target activation merely has to
counteract the sparsity of s, and activation of too
many exemplars from T leads to oversmoothing.
We obtained significances by computing 95%
and 99% confidence intervals with bootstrap re-
sampling. As a rule of thumb, we find that 0.4%
difference in GAP corresponds to a significant dif-
ference at the 95% level, and 0.7% difference in
GAP to significance at the 99% level. The four
activation methods (i.e., columns in Table 2) are
significantly different from each other, with the ex-
ception of the pair actT/kNN and actP/kNN (n.s.),
so that we get the following order:
actP/perc > actP/kNN ? actT/kNN > actT/perc
where > means ?significantly outperforms?. In par-
ticular, the best method (actT/kNN) outperforms
all other methods at p<0.01. Here, the best param-
eter setting (10% activation) is also significantly
better than the next-one one (20% activation). With
the exception of actT/perc, all activation methods
significantly outperform the best baseline (actP, no
activation).
Based on these observations, we computed a
third model, actTP, that activates both T (by kNN)
and P (by percentage), ranking paraphrases by
sim(act(P, s), act(T, s)). Table 3 shows the re-
sults. We find the overall best model at a similar
location in parameter space as for actT and actP
(cf. Table 2), namely by setting the activation pa-
rameters to small values. The sensitivity of the
parameters changes considerably, though. When
P activation (%) ? 10 20 30
T activation (kNN) ?
5 38.2 38.1 38.1
10 37.6 37.8 37.7
20 37.3 37.4 37.3
40 37.2 37.2 36.1
Table 3: Joint activation of P and T on the full
LexSub dataset (GAP evaluation)
we fix the actP activation level, we find compara-
tively large performance differences between the
T activation settings k=5 and k=10 (highly signif-
icant for 10% actP, and significant for 20% and
30% actP). On the other hand, when we fix the
actT activation level, changes in actP activation
generally have an insignificant impact.
Somewhat disappointingly, we are not able to
surpass the best result for actP alone. This indicates
that ? at least in the current vector space ? the
sparsity of s is less of a problem than the ?dilution?
of s that we face when we representing the target
word by exemplars of T close to s. Note, however,
that the numerically worse performance of the best
actTP model is still not significantly different from
the best actP model.
Influence of POS and frequency. An analysis
of the results by target part-of-speech showed that
the globally optimal parameters also yield the best
results for individual POS, even though there are
substantial differences among POS. For actT , the
best results emerge for all POS with kNN activation
with k between 10 and 30. For k=20, we obtain a
GAP of 35.3 (verbs), 38.2 (nouns), and 35.1 (adjec-
tives). For actP, the best parameter for all POS was
activation of 10%, with GAPs of 36.9 (verbs), 41.4
(nouns), and 37.5 (adjectives). Interestingly, the
results for actTP (verbs: 38.4, nouns: 40.6, adjec-
tives: 36.9) are better than actP for verbs, but worse
for nouns and adjectives, which indicates that the
sparsity problem might be more prominent than for
the other POS. In all three models, we found a clear
effect of target and paraphrase frequency, with de-
teriorating performance for the highest-frequency
targets as well as for the lemmas with the highest
average paraphrase frequency.
Comparison to other models. Many of the
other models are syntax-based and are therefore
only applicable to a subset of the LexSub data.
We have re-evaluated our exemplar models on the
subsets we used in Erk and Pado? (2008, EP08, 367
95
Models
EP08 EP09 TDP09
EP08 dataset 27.4 NA NA
EP09 dataset NA 32.2 36.5
actT actP actTP
EP08 dataset 36.5 38.0 39.9
EP09 dataset 39.1 39.9 39.6
Table 4: Comparison to other models on two sub-
sets of LexSub (GAP evaluation)
datapoints) and Erk and Pado? (2009, EP09, 100 dat-
apoints). The second set was also used by Thater et
al. (2009, TDP09). The results in Table 4 compare
these models against our best previous exemplar
models and show that our models outperform these
models across the board. 3 Due to the small sizes
of these datasets, statistical significance is more
difficult to attain. On EP09, the differences among
our models are not significant, but the difference
between them and the original EP09 model is.4 On
EP08, all differences are significant except for actP
vs. actTP.
We note that both the EP08 and the EP09
datasets appear to be simpler to model than the
complete Lexical Substitution dataset, at least by
our exemplar-based models. This underscores an
old insight: namely, that direct syntactic neighbors,
such as arguments and modifiers, provide strong
clues as to word sense.
5 Conclusions and Outlook
This paper reports on work in progress on an ex-
emplar activation model as an alternative to one-
vector-per-word approaches to word meaning in
context. Exemplar activation is very effective in
handling polysemy, even with a very simple (and
sparse) bag-of-words vector representation. On
both the EP08 and EP09 datasets, our models sur-
pass more complex prototype-based approaches
(Tab. 4). It is also noteworthy that the exemplar
activation models work best when few exemplars
are used, which bodes well for their efficiency.
We found that the best target representations re-
3Since our models had the advantage of being tuned on
the dataset, we also report the range of results across the
parameters we tested. On the EP08 dataset, we obtained 33.1?
36.5 for actT; 33.3?38.0 for actP; 37.7-39.9 for actTP. On the
EP09 dataset, the numbers were 35.8?39.1 for actT; 38.1?39.9
for actP; 37.2?39.8 for actTP.
4We did not have access to the TDP09 predictions to do
significance testing.
sult from activating a low absolute number of exem-
plars. Paraphrase representations are best activated
with a percentage-based threshold. Overall, we
found that paraphrase activation had a much larger
impact on performance than target activation, and
that drawing on target exemplars other than s to
represent the target meaning in context improved
over using s itself only for verbs (Tab. 3). This sug-
gests the possibility of considering T ?s activated
paraphrase candidates as the representation of T in
the context s, rather than some vector of T itself,
in the spirit of Kintsch (2001).
While it is encouraging that the best parameter
settings involved the activation of only few exem-
plars, computation with exemplar models still re-
quires the management of large numbers of vectors.
The computational overhead can be reduced by us-
ing data structures that cut down on the number
of vector comparisons, or by decreasing vector di-
mensionality (Gorman and Curran, 2006). We will
experiment with those methods to determine the
tradeoff of runtime and accuracy for this task.
Another area of future work is to move beyond
bag-of-words context: It is known from WSD
that syntactic and bag-of-words contexts provide
complementary information (Florian et al, 2002;
Szpektor et al, 2008), and we hope that they can be
integrated in a more sophisticated exemplar model.
Finally, we will to explore task-based evalua-
tions. Relation extraction and textual entailment
in particular are tasks where similar models have
been used before (Szpektor et al, 2008).
Acknowledgements. This work was supported
in part by National Science Foundation grant IIS-
0845925, and by a Morris Memorial Grant from
the New York Community Trust.
References
R. Bar-Haim, I. Dagan, I. Greental, and E. Shnarch.
2007. Semantic inference at the lexical-syntactic
level. In Proceedings of AAAI, pages 871?876, Van-
couver, BC.
M. Baroni and A. Lenci. 2009. One distributional
memory, many semantic spaces. In Proceedings of
the EACL Workshop on Geometrical Models of Nat-
ural Language Semantics, Athens, Greece.
W. Daelemans, A. van den Bosch, and J. Zavrel. 1999.
Forgetting exceptions is harmful in language learn-
ing. Machine Learning, 34(1/3):11?43. Special Is-
sue on Natural Language Learning.
K. Erk and S. Pado?. 2008. A structured vector space
96
model for word meaning in context. In Proceedings
of EMNLP, pages 897?906, Honolulu, HI.
K. Erk and S. Pado?. 2009. Paraphrase assessment in
structured vector space: Exploring parameters and
datasets. In Proceedings of the EACL Workshop on
Geometrical Models of Natural Language Seman-
tics, Athens, Greece.
R. Florian, S. Cucerzan, C. Schafer, and D. Yarowsky.
2002. Combining classifiers for word sense disam-
biguation. Journal of Natural Language Engineer-
ing, 8(4):327?341.
J. Gorman and J. R. Curran. 2006. Scaling distribu-
tional similarity to large corpora. In Proceedings of
ACL, pages 361?368, Sydney.
T. Griffiths, K. Canini, A. Sanborn, and D. J. Navarro.
2007. Unifying rational models of categorization
via the hierarchical Dirichlet process. In Proceed-
ings of CogSci, pages 323?328, Nashville, TN.
A. Kilgarriff. 1997. I don?t believe in word senses.
Computers and the Humanities, 31(2):91?113.
W. Kintsch. 2001. Predication. Cognitive Science,
25:173?202.
T. Landauer and S. Dumais. 1997. A solution to Platos
problem: the latent semantic analysis theory of ac-
quisition, induction, and representation of knowl-
edge. Psychological Review, 104(2):211?240.
D. McCarthy and J. Carroll. 2003. Disambiguating
nouns, verbs, and adjectives using automatically ac-
quired selectional preferences. Computational Lin-
guistics, 29(4):639?654.
D. McCarthy and R. Navigli. 2009. The English lexi-
cal substitution task. Language Resources and Eval-
uation, 43(2):139?159. Special Issue on Compu-
tational Semantic Analysis of Language: SemEval-
2007 and Beyond.
S. McDonald and M. Ramscar. 2001. Testing the dis-
tributional hypothesis: The influence of context on
judgements of semantic similarity. In Proceedings
of CogSci, pages 611?616.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL,
pages 236?244, Columbus, OH.
G. L. Murphy. 2002. The Big Book of Concepts. MIT
Press.
G Salton, A Wang, and C Yang. 1975. A vector-
space model for information retrieval. Journal of the
American Society for Information Science, 18:613?
620.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?124.
I. Szpektor, I. Dagan, R. Bar-Haim, and J. Goldberger.
2008. Contextual preferences. In Proceedings of
ACL, pages 683?691, Columbus, OH.
S. Thater, G. Dinu, and M. Pinkal. 2009. Ranking
paraphrases in context. In Proceedings of the ACL
Workshop on Applied Textual Inference, pages 44?
47, Singapore.
W. Voorspoels, W. Vanpaemel, and G. Storms. 2009.
The role of extensional information in conceptual
combination. In Proceedings of CogSci.
97
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1077?1086,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Simple Unsupervised Grammar Induction
from Raw Text with Cascaded Finite State Models
Elias Ponvert, Jason Baldridge and Katrin Erk
Department of Linguistics
The University of Texas at Austin
Austin, TX 78712
{ponvert,jbaldrid,katrin.erk}@mail.utexas.edu
Abstract
We consider a new subproblem of unsuper-
vised parsing from raw text, unsupervised par-
tial parsing?the unsupervised version of text
chunking. We show that addressing this task
directly, using probabilistic finite-state meth-
ods, produces better results than relying on
the local predictions of a current best unsu-
pervised parser, Seginer?s (2007) CCL. These
finite-state models are combined in a cascade
to produce more general (full-sentence) con-
stituent structures; doing so outperforms CCL
by a wide margin in unlabeled PARSEVAL
scores for English, German and Chinese. Fi-
nally, we address the use of phrasal punctua-
tion as a heuristic indicator of phrasal bound-
aries, both in our system and in CCL.
1 Introduction
Unsupervised grammar induction has been an ac-
tive area of research in computational linguistics for
over twenty years (Lari and Young, 1990; Pereira
and Schabes, 1992; Charniak, 1993). Recent work
(Headden III et al, 2009; Cohen and Smith, 2009;
Ha?nig, 2010; Spitkovsky et al, 2010) has largely
built on the dependency model with valence of Klein
and Manning (2004), and is characterized by its re-
liance on gold-standard part-of-speech (POS) anno-
tations: the models are trained on and evaluated us-
ing sequences of POS tags rather than raw tokens.
This is also true for models which are not successors
of Klein and Manning (Bod, 2006; Ha?nig, 2010).
An exception which learns from raw text and
makes no use of POS tags is the common cover links
parser (CCL, Seginer 2007). CCL established state-
of-the-art results for unsupervised constituency pars-
ing from raw text, and it is also incremental and ex-
tremely fast for both learning and parsing. Unfortu-
nately, CCL is a non-probabilistic algorithm based
on a complex set of inter-relating heuristics and a
non-standard (though interesting) representation of
constituent trees. This makes it hard to extend.
Note that although Reichart and Rappoport (2010)
improve on Seginer?s results, they do so by select-
ing training sets to best match the particular test
sentences?CCL itself is used without modification.
Ponvert et al (2010) explore an alternative strat-
egy of unsupervised partial parsing: directly pre-
dicting low-level constituents based solely on word
co-occurrence frequencies. Essentially, this means
segmenting raw text into multiword constituents. In
that paper, we show?somewhat surprisingly?that
CCL?s performance is mostly dependent on its ef-
fectiveness at identifying low-level constituents. In
fact, simply extracting non-hierarchical multiword
constituents from CCL?s output and putting a right-
branching structure over them actually works better
than CCL?s own higher level predictions. This result
suggests that improvements to low-level constituent
prediction will ultimately lead to further gains in
overall constituent parsing.
Here, we present such an improvement by using
probabilistic finite-state models for phrasal segmen-
tation from raw text. The task for these models is
chunking, so we evaluate performance on identifica-
tion of multiword chunks of all constituent types as
well as only noun phrases. Our unsupervised chun-
kers extend straightforwardly to a cascade that pre-
dicts higher levels of constituent structure, similar
to the supervised approach of Brants (1999). This
forms an overall unsupervised parsing system that
outperforms CCL by a wide margin.
1077
Mrs. Ward for one was relieved
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
1
(a) Chunks: (Mrs. Ward), (for one), and (was relieved)
All
came
from
Cray Research
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
(b) Only one chunk extracted: (Cray Research)
Fig. 1: Examples of constituent chunks extracted from
syntactic trees
2 Data
We use the standard data sets for unsupervised con-
stituency parsing research: for English, the Wall
Street Journal subset of the Penn Treebank-3 (WSJ,
Marcus et al 1999); for German, the Negra corpus
v2 (Krenn et al, 1998); for Chinese, the Penn Chi-
nese Treebank v5.0 (CTB, Palmer et al, 2006). We
lower-case text but otherwise do not alter the raw
text of the corpus. Sentence segmentation and tok-
enization from the treebank is used. As in previous
work, punctuation is not used for evaluation.
In much unsupervised parsing work the test sen-
tences are included in the training material. Like Co-
hen and Smith, Headden III et al, Spitkovsky et al,
we depart from this experimental setup and keep the
evaluation sets blind to the models during training.
For English (WSJ) we use sections 00-22 for train-
ing, section 23 for test and we develop using section
24; for German (Negra) we use the first 18602 sen-
tences for training, the last 1000 sentences for de-
velopment and the penultimate 1000 sentences for
testing; for Chinese (CTB) we adopt the data-split
of Duan et al (2007).
3 Tasks and Benchmark
Evaluation. By unsupervised partial parsing, or
simply unsupervised chunking, we mean the seg-
mentation of raw text into (non-overlapping) multi-
word constituents. The models are intended to cap-
ture local constituent structure ? the lower branches
of a constituent tree. For this reason we evaluate
WSJ
Chunks 203K
NPs 172K
Chnk ? NPs 161K
Negra
Chunks 59K
NPs 33K
Chnk ? NPs 23K
CTB
Chunks 92K
NPs 56K
Chnk ? NPs 43K
Table 1: Constituent chunks and base NPs in the datasets.
% constituents % words
WSJ
Chunks 32.9 57.7
NPs 27.9 53.1
Negra
Chunks 45.4 53.6
NPs 25.5 42.4
CTB
Chunks 32.5 55.4
NPs 19.8 42.9
Table 2: Percentage of gold standard constituents and
words under constituent chunks and base NPs.
using what we call constituent chunks, the subset
of gold standard constituents which are i) branch-
ing (multiword) but ii) non-hierarchical (do not con-
tain subconstituents). We also evaluate our models
based on their performance at identifying base noun
phrases, NPs that do not contain nested NPs.
Examples of constituent chunks extracted from
treebank constituent trees are in Fig. 1. In English
newspaper text, constituent chunks largely corre-
spond with base NPs, but this is less the case with
Chinese and German. Moreover, the relationship be-
tween NPs and constituent chunks is not a subset re-
lation: some base NPs do have internal constituent
structure. The numbers of constituent chunks and
NPs for the training datasets are in Table 1. The per-
centage of constituents in these datasets which fall
under these definitions, and the percentage of words
under these constituents, are in Table 2.
For parsing, the standard unsupervised parsing
metric is unlabeled PARSEVAL. It measures preci-
sion and recall on constituents produced by a parser
as compared to gold standard constituents.
CCL benchmark. We use Seginer?s CCL as a
benchmark for several reasons. First, there is a
free/open-source implementation facilitating exper-
1078
imental replication and comparison.1 More im-
portantly, until recently it was the only unsuper-
vised raw text constituent parser to produce re-
sults competitive with systems which use gold POS
tags (Klein and Manning, 2002; Klein and Man-
ning, 2004; Bod, 2006) ? and the recent improved
raw-text parsing results of Reichart and Rappoport
(2010) make direct use of CCL without modifica-
tion. There are other raw-text parsing systems of
note, EMILE (Adriaans et al, 2000), ABL (van Za-
anen, 2000) and ADIOS (Solan et al, 2005); how-
ever, there is little consistent treebank-based evalu-
ation of these models. One study by Cramer (2007)
found that none of the three performs particularly
well under treebank evaluation. Finally, CCL out-
performs most published POS-based models when
those models are trained on unsupervised word
classes rather than gold POS tags. The only excep-
tion we are aware of is Ha?nig?s (2010) unsuParse+,
which outperforms CCL on Negra, though this is
shown only for sentences with ten or fewer words.
Phrasal punctuation. Though punctuation is usu-
ally entirely ignored in unsupervised parsing re-
search, Seginer (2007) departs from this in one key
aspect: the use of phrasal punctuation ? punctuation
symbols that often mark phrasal boundaries within a
sentence. These are used in two ways: i) they im-
pose a hard constraint on constituent spans, in that
no constituent (other than sentence root) may extend
over a punctuation symbol, and ii) they contribute to
the model, specifically in terms of the statistics of
words seen adjacent to a phrasal boundary. We fol-
low this convention and use the following set:
. ? ! ; , -- ? ?
The last two are ideographic full-stop and comma.2
4 Unsupervised partial parsing
We learn partial parsers as constrained sequence
models over tags encoding local constituent struc-
ture (Ramshaw and Marcus, 1995). A simple tagset
is unlabeled BIO, which is familiar from supervised
chunking and named-entity recognition: the tag B
1http://www.seggu.net/ccl
2This set is essentially that of Seginer (2007). While it is
clear from our analysis of CCL that it does make use of phrasal
punctuation in Chinese, we are not certain whether ideographic
comma is included.
denotes the beginning of a chunk, I denotes mem-
bership in a chunk andO denotes exclusion from any
chunk. In addition we use the tag STOP for sentence
boundaries and phrasal punctuation.
HMMs and PRLGs. The models we use for un-
supervised partial parsing are hidden Markov mod-
els, and a generalization we refer to as probabilis-
tic right linear grammars (PRLGs). An HMM mod-
els a sequence of observed states (words) x =
{x1, x2, . . . , xN} and a corresponding set of hid-
den states y = {y1, y2, . . . , yN}. HMMs may be
thought of as a special case of probabilistic context-
free grammars, where the non-terminal symbols are
the hidden state space, terminals are the observed
states and rules are of the form NONTERM ?
TERM NONTERM (assuming y1 and yN are fixed
and given). So, the emission and transition emanat-
ing from yn would be characterized as a PCFG rule
yn ? xn yn+1. HMMs factor rule probabilities into
emission and transition probabilities:
P (yn ? xn yn+1) = P (xn, yn+1|yn)
? P (xn|yn) P (yn+1|yn).
However, without making this independence as-
sumption, we can model right linear rules directly:
P (xn, yn+1|yn) = P (xn|yn, yn+1) P (yn+1|yn).
So, when we condition emission probabilities on
both the current state yn and the next state yn+1, we
have an exact model. This direct modeling of the
right linear grammar rule yn ? xn yn+1 is what
we call a probabilistic right-linear grammar. To be
clear, a PRLG is just an HMM without the indepen-
dence of emissions and transitions. See Smith and
Johnson (2007) for a discussion, where they refer to
PRLGs as Mealy HMMs.
We use expectation maximization to estimate
model parameters. For the E step, the forward-
backward algorithm (Rabiner, 1989) works identi-
cally for the HMM and PRLG. For the M step, we
use maximum likelihood estimation with additive
smoothing on the emissions probabilities. So, for
the HMM and PRLG models respectively, for words
1079
STOP B
O I
1
Fig. 2: Possible tag transitions as a state diagram.
STOP B I O
STOP .33 .33 .33
B 1
I .25 .25 .25 .25
O .33 .33 .33
Fig. 3: Uniform initialization of transition probabilities
subject to the constraints in Fig. 2: rows correspond to
antecedent state, columns to following state.
w and tags s, t:
P? (w|t) =
C(t, w) + ?
C(t) + ?V
P? (w|s, t) =
C(t, w, s) + ?
C(t, s) + ?V
where C are the soft counts of emissions C(t, w),
rules C(t, w, s) = C(t ? w s), tags C(t) and tran-
sitions C(t, s) calculated during the E step; V is the
number of terms w, and ? is a smoothing parameter.
We fix ? = .1 for all experiments; more sophisti-
cated smoothing could avoid dependence on ?.
We do not smooth transition probabilities (so
P? (s|t) = C(t, s)/C(t)) for two reasons. First, with
four tags, there is no data-sparsity concern with re-
spect to transitions. Second, the nature of the task
imposes certain constraints on transition probabili-
ties: because we are only interested in multiword
chunks, we expressly do not want to generate a B
following a B ? in other words P (B|B) = 0.
These constraints boil down to the observation
that the B and I states will only be seen in BII? se-
quences. This may be expressed via the state transi-
tion diagram in Fig. 2. The constraints of also dic-
tate the initial model input to the EM process. We
use uniform probability distributions subject to the
constraints of Fig. 2. So, initial model transition
probabilities are given in Fig. 3. In EM, if a parame-
ter is equal to zero, subsequent iterations of the EM
process will not ?unset? this parameter; thus, this
form of initialization is a simple way of encoding
constraints on model parameters. We also experi-
mented with random initial models (subject to the
constraints in Fig. 2). Uniform initialization usu-
ally works slightly better; also, uniform initializa-
tion does not require multiple runs of each experi-
ment, as random initialization does.
Motivating the HMMand PRLG. This approach
? encoding a chunking problem as a tagging prob-
lem and learning to tag with HMMs ? goes back
to Ramshaw and Marcus (1995). For unsupervised
learning, the expectation is that the model will learn
to generalize on phrasal boundaries. That is, the
models will learn to associate terms like the and a,
which often occur at the beginnings of sentences and
rarely at the end, with the tag B, which cannot occur
at the end of a sentence. Likewise common nouns
like company or asset, which frequently occur at the
ends of sentences, but rarely at the beginning, will
come to be associated with the I tag, which cannot
occur at the beginning.
The basic motivation for the PRLG is the assump-
tion that information is lost due to the independence
assumption characteristic of the HMM. With so few
states, it is feasible to experiment with the more fine-
grained PRLG model.
Evaluation. Using the low-level predictions of
CCL as as benchmark, we evaluate the HMM and
PRLG chunkers on the tasks of constituent chunk
and base NP identification. Models were initialized
uniformly as illustrated in Fig. 3. Sequence models
learn via EM. We report accuracy only after conver-
gence, that is after the change in full dataset per-
plexity (log inverse probability) is less than %.01
between iterations. Precision, recall and F-score are
reported for full constituent identification ? brack-
ets which do not match the gold standard exactly are
false positives.
Model performance results on held-out test
datasets are reported in Table 3. ?CCL? refers to the
lowest-level constituents extracted from full CCL
output, as a benchmark chunker. The sequence mod-
els outperform the CCL benchmark at both tasks and
on all three datasets. In most cases, the PRLG se-
quence model performs better than the HMM; the
exception is CTB, where the PRLG model is behind
the HMM in evaluation, as well as behind CCL.
As the lowest-level constituents of CCL were not
specifically designed to describe chunks, we also
1080
English / WSJ German / Negra Chinese / CTB
Task Model Prec Rec F Prec Rec F Prec Rec F
Chunking
CCL 57.5 53.5 55.4 28.4 29.6 29.0 23.5 23.9 23.7
HMM 53.8 62.2 57.7 35.0 37.7 36.3 37.4 41.3 39.3
PRLG 76.2 63.9 69.5 39.6 47.8 43.3 23.0 18.3 20.3
NP
CCL 46.2 51.1 48.5 15.6 29.2 20.3 10.4 17.3 13.0
HMM 47.7 65.6 55.2 23.8 46.2 31.4 17.0 30.8 21.9
PRLG 76.8 76.7 76.7 24.6 53.4 33.6 21.9 28.5 24.8
Table 3: Unsupervised chunking results for local constituent structure identification and NP chunking on held-out test
sets. CCL refers to the lowest constituents extracted from CCL output.
WSJ Negra CTB
Chunking 57.8 36.0 25.5
NPs 57.8 38.8 23.2
Table 4: Recall of CCL on the chunking tasks.
checked the recall of all brackets generated by CCL
against gold-standard constituent chunks. The re-
sults are given in Table 4. Even compared to this,
the sequence models? recall is almost always higher.
The sequence models, as well as the CCL bench-
mark, show relatively low precision on the Negra
corpus. One possible reason for this lies in the
design decision of Negra to use relatively flat tree
structures. As a result, many structures that in
other treebanks would be prepositional phrases with
embedded noun phrases ? and thus non-local con-
stituents ? are flat prepositional phrases here. Exam-
ples include ?auf die Wiesbadener Staatsanwaelte?
(on Wiesbaden?s district attorneys) and ?in Han-
novers Nachbarstadt? (in Hannover?s neighbor city).
In fact, in Negra, the sequence model chunkers
often find NPs embedded in PPs, which are not an-
notated as such. For instance, in the PP ?hinter den
Kulissen? (behind the scenes), both the PRLG and
HMM chunkers identify the internal NP, though this
is not identified in Negra and thus considered a false
positive. The fact that the HMM and PRLG have
higher recall on NP identification on Negra than pre-
cision is further evidence towards this.
Comparing the HMM and PRLG. To outline
some of the factors differentiating the HMM and
PRLG, we focus on NP identification in WSJ.
The PRLG has higher precision than the HMM,
while the two models are closer in recall. Com-
paring the predictions directly, the two models of-
POS Sequence # of errors
TO VB 673
NNP NNP 450
MD VB 407
DT JJ 368
DT NN 280
Table 5: Top 5 POS sequences of the false positives pre-
dicted by the HMM.
ten have the same correct predictions and often miss
the same gold standard constituents. The improved
results of the PRLG are based mostly on the fewer
overall brackets predicted, and thus fewer false pos-
itives: for WSJ the PRLG incorrectly predicts 2241
NP constituents compared to 6949 for the HMM.
Table 5 illustrates the top 5 POS sequences of the
false positives predicted by the HMM.3 (Recall that
we use gold standard POS only for post-experiment
results analysis?the model itself does not have ac-
cess to them.) By contrast, the sequence represent-
ing the largest class of errors of the PRLG is DT NN,
with 165 errors ? this sequence represents the largest
class of predictions for both models.
Two of the top classes of errors, MD VB and
TO VB, represent verb phrase constituents, which
are often predicted by the HMM chunker, but not
by the PRLG. The class represented by NNP NNP
corresponds with the tendency of the HMM chun-
ker to split long proper names: for example, it sys-
tematically splits new york stock exchange into two
chunks, (new york) (stock exchange), whereas the
PRLG chunker predicts a single four-word chunk.
The most interesting class is DT JJ, which rep-
resents the difficulty the HMM chunker has at dis-
3For the Penn Treebank tagset, see Marcus et al (1993).
1081
1 Start with raw text:
there is no asbestos in our products now
2 Apply chunking model:
there (is no asbestos) in (our products) now
3 Create pseudowords:
there is in our now
4 Apply chunking model (and repeat 1?4 etc.):
(there is ) (in our ) now
5 Unwind and create a tree:
there
is no asbestos
in
our products
now
1Fig. 4: Cascaded chunking illustrated. Pseudowords are
indicated with boxes.
tinguishing determiner-adjective from determiner-
noun pairs. The PRLG chunker systematically gets
DT JJ NN trigrams as chunks. The greater con-
text provided by right branching rules allows the
model to explicitly estimate separate probabilities
forP (I ? recent I) versusP (I ? recent O). That
is, recent within a chunk versus ending a chunk. Bi-
grams like the acquisition allow the model to learn
rules P (B ? the I) and P (I ? acquisition O).
So, the PRLG is better able to correctly pick out the
trigram chunk (the recent acquisition).
5 Constituent parsing with a cascade of
chunkers
We use cascades of chunkers for full constituent
parsing, building hierarchical constituents bottom-
up. After chunking is performed, all multiword con-
stituents are collapsed and represented by a single
pseudoword. We use an extremely simple, but effec-
tive, way to create pseudoword for a chunk: pick the
term in the chunk with the highest corpus frequency,
and mark it as a pseudoword. The sentence is now a
string of symbols (normal words and pseudowords),
to which a subsequent unsupervised chunking model
is applied. This process is illustrated in Fig. 4.
Each chunker in the cascade chunks the raw text,
then regenerates the dataset replacing chunks with
pseudowords; this process is iterated until no new
chunks are found. The separate chunkers in the cas-
Text : Mr. Vinken is chairman of Elsevier N.V.
Level 1 :
Mr. Vinken
is chairman of
Elsevier N.V.
1Level 2 :
Mr. Vinken is chairman
of
Elsevier N.V.
1
Level 3 : Mr. Vinken is chairman of
Elsevier N.V.
1
Fig. 5: PRLG cascaded chunker output.
NPs PPs
Lev 1 Lev 2 Lev 1 Lev 2
WSJ
HMM 66.5 68.1 20.6 70.2
PRLG 77.5 78.3 9.1 77.6
Negra
HMM 54.7 62.3 24.8 48.1
PRLG 61.6 65.2 40.3 44.0
CTB
HMM 33.3 35.4 34.6 38.4
PRLG 30.9 33.6 31.6 47.1
Table 7: NP and PP recall at cascade levels 1 and 2. The
level 1 NP numbers differ from the NP chunking numbers
from Table 3 since they include root-level constituents
which are often NPs.
cade are referred to as levels. In our experiments the
cascade process took a minimum of 5 levels, and a
maximum of 7. All chunkers in the cascade have the
same settings in terms of smoothing, the tagset and
initialization.
Evaluation. Table 6 gives the unlabeled PARSE-
VAL scores for CCL and the two finite-state models.
PRLG achieves the highest F-score for all datasets,
and does so by a wide margin for German and Chi-
nese. CCL does achieve higher recall for English.
While the first level of constituent analysis has
high precision and recall on NPs, the second level
often does well finding prepositional phrases (PPs),
especially in WSJ; see Table 7. This is illustrated
in Fig. 5. This example also illustrates a PP attach-
ment error, which are a common problem for these
models.
We also evaluate using short ? 10-word or less ?
sentences. That said, we maintain the training/test
split from before. Also, making use of the open
1082
Parsing English / WSJ German / Negra Chinese / CTB
Model Prec Rec F Prec Rec F Prec Rec F
CCL 53.6 50.0 51.7 33.4 32.6 33.0 37.0 21.6 27.3
HMM 48.2 43.6 45.8 30.8 50.3 38.2 43.0 29.8 35.2
PRLG 60.0 49.4 54.2 38.8 47.4 42.7 50.4 32.8 39.8
Table 6: Unlabeled PARSEVAL scores for cascaded models.
source implementation by F. Luque,4 we compare
on WSJ and Negra to the constituent context model
(CCM) of Klein and Manning (2002). CCM learns
to predict a set of brackets over a string (in prac-
tice, a string of POS tags) by jointly estimating con-
stituent and distituent strings and contexts using an
iterative EM-like procedure (though, as noted by
Smith and Eisner (2004), CCM is deficient as a gen-
erative model). Note that this comparison is method-
ologically problematic in two respects. On the one
hand, CCM is evaluated using gold standard POS
sequences as input, so it receives a major source of
supervision not available to the other models. On the
other hand, the other models use punctuation as an
indicator of constituent boundaries, but all punctu-
ation is dropped from the input to CCM. Also, note
that CCM performs better when trained on short sen-
tences, so here CCM is trained only on the 10-word-
or-less subsets of the training datasets.5
The results from the cascaded PRLG chunker
are near or better than the best performance by
CCL or CCM in these experiments. These and the
full-length parsing results suggest that the cascaded
chunker strategy generalizes better to longer sen-
tences than does CCL. CCM does very poorly on
longer sentences, but does not have the benefit of us-
ing punctuation, as do the raw text models; unfortu-
nately, further exploration of this trade-off is beyond
the scope of this paper. Finally, note that CCM has
higher recall, and lower precision, generally, than
the raw text models. This is due, in part, to the chart
structure used by CCM in the calculation of con-
stituent and distituent probabilities: as in CKY pars-
ing, the chart structure entails the trees predicted will
be binary-branching. CCL and the cascaded models
can predict higher-branching constituent structures,
4http://www.cs.famaf.unc.edu.ar/
?francolq/en/proyectos/dmvccm/
5This setup is the same as Seginer?s (2007), except the
train/test split.
Prec Rec F
WSJ
CCM 62.4 81.4 70.7
CCL 71.2 73.1 72.1
HMM 64.4 64.7 64.6
PRLG 74.6 66.7 70.5
Negra
CCM 52.4 83.4 64.4
CCL 52.9 54.0 53.0
HMM 47.7 72.0 57.4
PRLG 56.3 72.1 63.2
CTB
CCL 54.4 44.3 48.8
HMM 55.8 53.1 54.4
PRLG 62.7 56.9 59.6
Table 8: Evaluation on 10-word-or-less sentences. CCM
scores are italicized as a reminder that CCM uses gold-
standard POS sequences as input, so its results are not
strictly comparable to the others.
so fewer constituents are predicted overall.
6 Phrasal punctuation revisited
Up to this point, the proposed models for chunking
and parsing use phrasal punctuation as a phrasal sep-
arator, like CCL. We now consider how well these
models perform in absence of this constraint.
Table 9a provides comparison of the sequence
models? performance on the constituent chunking
task without using phrasal punctuation in training
and evaluation. The table shows absolute improve-
ment (+) or decline (?) in precision and recall
when phrasal punctuation is removed from the data.
The punctuation constraint seems to help the chun-
kers some, but not very much; ignoring punctuation
seems to improve chunker results for the HMM on
Chinese. Overall, the effect of phrasal punctuation
on the chunker models? performance is not clear.
The results for cascaded parsing differ strongly
from those for chunking, as Table 9b indicates. Us-
ing phrasal punctuation to constrain bracket predic-
tion has a larger impact on cascaded parsing re-
1083
0 20 40 60
2
2.5
3
3.5
EM Iterations
Le
ng
th
a) Average Predicted Constituent Length
Actual average chunk length
1
0 20 40 60
20
30
40
50
EM Iterations
Pr
ec
isi
on
W/ Punctuation
No Punctuation
b) Chunking Precision
1
0 20 40 60
20
30
40
50
EM Iterations
Pr
ec
isi
on
c) Precision at All Brackets
1
Fig. 6: Behavior of the PRLG model on CTB over the course of EM.
WSJ Negra CTB
Prec Rec Prec Rec Prec Rec
HMM ?5.8 ?9.8 ?0.1 ?0.4 +0.7 +4.9
PRLG ?2.5 ?2.1 ?2.1 ?2.1 ?7.0 +1.2
a) Constituent Chunking
WSJ Negra CTB
Prec Rec Prec Rec Prec Rec
CCL ?14.1 ?13.5 ?10.7 ?4.6 ?11.6 ?6.0
HMM ?7.8 ?8.6 ?2.8 +1.7 ?13.4 ?1.2
PRLG ?10.1 ?7.2 ?4.0 ?4.5 ?22.0 ?11.8
b) (Cascade) Parsing
Table 9: Effects of dropping phrasal punctuation in un-
supervised chunking and parsing evaluations relative to
Tables 3 and 6.
sults almost across the board. This is not surpris-
ing: while performing unsupervised partial parsing
from raw text, the sequence models learn two gen-
eral patterns: i) they learn to chunk rare sequences,
such as named entities, and ii) they learn to chunk
high-frequency function words next to lower fre-
quency content words, which often correlate with
NPs headed by determiners, PPs headed by prepo-
sitions and VPs headed by auxiliaries. When these
patterns are themselves replaced with pseudowords
(see Fig. 4), the models have fewer natural cues to
identify constituents. However, within the degrees
of freedom allowed by punctuation constraints as
described, the chunking models continue to find rel-
atively good constituents.
While CCL makes use of phrasal punctuation in
previously reported results, the open source imple-
mentation allows it to be evaluated without this con-
straint. We did so, and report results in Table 9b.
CCL is, in fact, very sensitive to phrasal punctu-
ation. Comparing CCL to the cascaded chunkers
when none of them use punctuation constraints, the
cascaded chunkers (both HMMs and PRLGs) out-
perform CCL for each evaluation and dataset.
For the CTB dataset, best chunking performance
and cascaded parsing performance flips from the
HMM to the PRLG. More to the point, the PRLG
is actually with worst performing model at the con-
stituent chunking task, but the best performing cas-
cade parser; also, this model has the most serious
degrade in performance when phrasal punctuation is
dropped from input. To investigate, we track the
performance of the chunkers on the development
dataset over iterations of EM. This is illustrated in
Fig. 6 with the PRLG model. First of all, Fig. 6a re-
veals the average length of the constituents predicted
by the PRLG model increases over the course of
EM. However, the average constituent chunk length
is 2.22. So, the PRLG chunker is predicting con-
stituents that are longer than the ones targeted in
the constituent chunking task: regardless of whether
they are legitimate constituents or not, often they
will likely be counted as false positives in this evalu-
ation. This is confirmed by observing the constituent
chunking precision in Fig. 6b, which peaks when
the average predicted constituent length is about the
same the actual average length of those in the eval-
uation. The question, then, is whether the longer
chunks predicted correspond to actual constituents
or not. Fig. 6c shows that the PRLG, when con-
strained by phrasal punctuation, does continue to
improve its constituent prediction accuracy over the
course of EM. These correctly predicted constituents
are not counted as such in the constituent chunking
or base NP evaluations, but they factor directly into
1084
improved accuracy when this model is part of a cas-
cade.
7 Related work
Our task is the unsupervised analogue of chunking
(Abney, 1991), popularized by the 1999 and 2000
Conference on Natural Language Learning shared
tasks (Tjong et al, 2000). In fact, our models follow
Ramshaw and Marcus (1995), treating structure pre-
diction as sequence prediction using BIO tagging.
In addition to Seginer?s CCL model, the unsu-
pervised parsing model of Gao and Suzuki (2003)
and Gao et al (2004) also operates on raw text.
Like us, their model gives special treatment to lo-
cal constituents, using a language model to char-
acterize phrases which are linked via a dependency
model. Their output is not evaluated directly using
treebanks, but rather applied to several information
retrieval problems.
In the supervised realm, Hollingshead
et al (2005) compare context-free parsers with
finite-state partial parsing methods. They find that
full parsing maintains a number of benefits, in spite
of the greater training time required: they can train
on less data more effectively than chunkers, and are
more robust to shifts in textual domain.
Brants (1999) reports a supervised cascaded
chunking strategy for parsing which is strikingly
similar to the methods proposed here. In both,
Markov models are used in a cascade to predict hi-
erarchical constituent structure; and in both, the pa-
rameters for the model at each level are estimated
independently. There are major differences, though:
the models here are learned from raw text with-
out tree annotations, using EM to train parameters;
Brants? cascaded Markov models use supervised
maximum likelihood estimation. Secondly, between
the separate levels of the cascade, we collapse con-
stituents into symbols which are treated as tokens
in subsequent chunking levels; the Markov models
in the higher cascade levels in Brants? work actu-
ally emit constituent structure. A related approach
is that of Schuler et al (2010), who report a su-
pervised hierarchical hidden Markov model which
uses a right-corner transform. This allows the model
to predict more complicated trees with fewer levels
than in Brants? work or this paper.
8 Conclusion
In this paper we have introduced a new subprob-
lem of unsupervised parsing: unsupervised partial
parsing, or unsupervised chunking. We have pro-
posed a model for unsupervised chunking from raw
text that is based on standard probabilistic finite-
state methods. This model produces better local
constituent predictions than the current best unsu-
pervised parser, CCL, across datasets in English,
German, and Chinese. By extending these proba-
bilistic finite-state methods in a cascade, we obtain
a general unsupervised parsing model. This model
outperforms CCL in PARSEVAL evaluation on En-
glish, German, and Chinese.
Like CCL, our models operate from raw (albeit
segmented) text, and like it our models decode very
quickly; however, unlike CCL, our models are based
on standard and well-understood computational lin-
guistics technologies (hidden Markov models and
related formalisms), and may benefit from new re-
search into these core technologies. For instance,
our models may be improved by the application
of (unsupervised) discriminative learning techniques
with features (Berg-Kirkpatrick et al, 2010); or by
incorporating topic models and document informa-
tion (Griffiths et al, 2005; Moon et al, 2010).
UPPARSE, the software used for the experiments
in this paper, is available under an open-source li-
cense to facilitate replication and extensions.6
Acknowledgments. This material is based upon
work supported in part by the U. S. Army Research
Laboratory and the U. S. Army Research Office un-
der grant number W911NF-10-1-0533. Support for
the first author was also provided by Mike Hogg En-
dowment Fellowship, the Office of Graduate Studies
at The University of Texas at Austin.
This paper benefited from discussion in the Natu-
ral Language Learning reading group at UT Austin,
especially from Collin Bannard, David Beaver,
Matthew Lease, Taesun Moon and Ray Mooney. We
also thank the three anonymous reviewers for in-
sightful questions and helpful comments.
6 http://elias.ponvert.net/upparse.
1085
References
S. Abney. 1991. Parsing by chunks. In R. Berwick,
S. Abney, and C. Tenny, editors, Principle-based Pars-
ing. Kluwer.
P. W. Adriaans, M. Trautwein, and M. Vervoort. 2000.
Towards high speed grammar induction on large text
corpora. In SOFSEM.
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In HLT-NAACL.
R. Bod. 2006. Unsupervised parsing with U-DOP. In
CoNLL.
T. Brants. 1999. Cascaded markov models. In EACL.
E. Charniak. 1993. Statistical Language Learning. MIT.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsu-
pervised grammar induction. In HLT-NAACL.
B. Cramer. 2007. Limitations of current grammar induc-
tion algorithms. In ACL-SRW.
X. Duan, J. Zhao, and B. Xu. 2007. Probabilistic mod-
els for action-based Chinese dependency parsing. In
ECML/PKDD.
J. Gao and H. Suzuki. 2003. Unsupervised learning of
dependency structure for language modeling. In ACL.
J. Gao, J.Y. Nie, G. Wu, and G. Cao. 2004. Dependence
language model for information retrieval. In SIGIR.
T. L. Griffiths, M. Steyvers, D. M. Blei, and J. M. Tenen-
baum. 2005. Integrating topics and syntax. In NIPS.
C. Ha?nig. 2010. Improvements in unsupervised co-
occurence based parsing. In CoNLL.
W. P. Headden III, M. Johnson, and D. McClosky.
2009. Improving unsupervised dependency parsing
with richer contexts and smoothing. In HLT-NAACL.
K. Hollingshead, S. Fisher, and B. Roark. 2005. Com-
paring and combining finite-state and context-free
parsers. In HLT-EMNLP.
D. Klein and C. D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In ACL.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In ACL.
B. Krenn, T. Brants, W. Skut, and Hans Uszkoreit. 1998.
A linguistically interpreted corpus of German newspa-
per text. In Proceedings of the ESSLLI Workshop on
Recent Advances in Corpus Annotation.
K. Lari and S. J. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech & Language, 4:35 ? 56.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. Compuational Linguistics, pages
313?330.
M.P. Marcus, B. Santorini, M.A. Marcinkiewicz, and
A. Taylor, 1999. Treebank-3. LDC.
T. Moon, J. Baldridge, and K. Erk. 2010. Crouching
Dirichlet, hidden Markov model: Unsupervised POS
tagging with context local tag generation. In EMNLP.
M. Palmer, F. D. Chiou, N. Xue, and T. K. Lee, 2005.
Chinese Treebank 5.0. LDC.
F. Pereira and Y. Schabes. 1992. Inside-outside reesti-
mation from paritally bracketed corpora. In ACL.
E. Ponvert, J. Baldridge, and K. Erk. 2010. Simple unsu-
pervised prediction of low-level constituents. In ICSC.
L.R. Rabiner. 1989. A tutorial on hidden Markov models
and selected applications in speech recognition. Pro-
ceedings of the IEEE.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunking
using transformation-based learning. In Proc. of Third
Workshop on Very Large Corpora.
R. Reichart and A. Rappoport. 2010. Improved fully
unsupervised parsing with Zoomed Learning. In
EMNLP.
W. Schuler, S. AbdelRahman, T. Miller, and L. Schwartz.
2010. Broad-coverage parsing using human-like
memory constraints. Compuational Linguistics, 3(1).
Y. Seginer. 2007. Fast unsupervised incremental parsing.
In ACL.
N. A. Smith and J. Eisner. 2004. Annealing techniques
for unsupervised statistical language learning. In ACL.
N. A. Smith and M. Johnson. 2007. Weighted and prob-
abilistic CFGs. Computational Lingusitics.
Z. Solan, D. Horn, E. Ruppin, and S. Edelman. 2005.
Unsupervised learning of natural languages. PNAS,
102.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010.
From baby steps to leapfrog: How ?less is more? in
unsupervised dependency parsing. In NAACL-HLT.
E. F. Tjong, K. Sang, and S. Buchholz. 2000. Introduc-
tion to the CoNLL-2000 Shared Task: Chunking. In
CoNLL-LLL.
M. van Zaanen. 2000. ABL: Alignment-based learning.
In COLING.
1086
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1210?1219,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Probabilistic Soft Logic for Semantic Textual Similarity
Islam Beltagy
?
Katrin Erk
?
Raymond Mooney
?
?
Department of Computer Science
?
Department of Linguistics
The University of Texas at Austin
Austin, Texas 78712
?
{beltagy,mooney}@cs.utexas.edu
?
katrin.erk@mail.utexas.edu
Abstract
Probabilistic Soft Logic (PSL) is a re-
cently developed framework for proba-
bilistic logic. We use PSL to combine
logical and distributional representations
of natural-language meaning, where distri-
butional information is represented in the
form of weighted inference rules. We ap-
ply this framework to the task of Seman-
tic Textual Similarity (STS) (i.e. judg-
ing the semantic similarity of natural-
language sentences), and show that PSL
gives improved results compared to a pre-
vious approach based on Markov Logic
Networks (MLNs) and a purely distribu-
tional approach.
1 Introduction
When will people say that two sentences are sim-
ilar? This question is at the heart of the Semantic
Textual Similarity task (STS)(Agirre et al, 2012).
Certainly, if two sentences contain many of the
same words, or many similar words, that is a good
indication of sentence similarity. But that can be
misleading. A better characterization would be to
say that if two sentences use the same or similar
words in the same or similar relations, then those
two sentences will be judged similar.
1
Interest-
ingly, this characterization echoes the principle of
compositionality, which states that the meaning of
a phrase is uniquely determined by the meaning of
its parts and the rules that connect those parts.
Beltagy et al (2013) proposed a hybrid ap-
proach to sentence similarity: They use a very
1
Mitchell and Lapata (2008) give an amusing example of
two sentences that consist of all the same words, but are very
different in their meaning: (a) It was not the sales manager
who hit the bottle that day, but the office worker with the
serious drinking problem. (b) That day the office manager,
who was drinking, hit the problem sales worker with a bottle,
but it was not serious.
deep representation of sentence meaning, ex-
pressed in first-order logic, to capture sentence
structure, but combine it with distributional sim-
ilarity ratings at the word and phrase level. Sen-
tence similarity is then modelled as mutual entail-
ment in a probabilistic logic. This approach is in-
teresting in that it uses a very deep and precise
representation of meaning, which can then be re-
laxed in a controlled fashion using distributional
similarity. But the approach faces large hurdles
in practice, stemming from efficiency issues with
the Markov Logic Networks (MLN) (Richardson
and Domingos, 2006) that they use for performing
probabilistic logical inference.
In this paper, we use the same combined logic-
based and distributional framework as Beltagy et
al., (2013) but replace Markov Logic Networks
with Probabilistic Soft Logic (PSL) (Kimmig et
al., 2012; Bach et al, 2013). PSL is a proba-
bilistic logic framework designed to have efficient
inference. Inference in MLNs is theoretically in-
tractable in the general case, and existing approxi-
mate inference algorithms are computationally ex-
pensive and sometimes inaccurate. Consequently,
the MLN approach of Beltagy et al (2013) was
unable to scale to long sentences and was only
tested on the relatively short sentences in the Mi-
crosoft video description corpus used for STS
(Agirre et al, 2012). On the other hand, inference
in PSL reduces to a linear programming problem,
which is theoretically and practically much more
efficient. Empirical results on a range of prob-
lems have confirmed that inference in PSL is much
more efficient than in MLNs, and frequently more
accurate (Kimmig et al, 2012; Bach et al, 2013).
We show how to use PSL for STS, and describe
changes to the PSL framework that make it more
effective for STS. For evaluation, we test on three
STS datasets, and compare our PSL system with
the MLN approach of Beltagy et al, (2013) and
with distributional-only baselines. Experimental
1210
results demonstrate that, overall, PSL models hu-
man similarity judgements more accurately than
these alternative approaches, and is significantly
faster than MLNs.
The rest of the paper is organized as follows:
section 2 presents relevant background material,
section 3 explains how we adapted PSL for the
STS task, section 4 presents the evaluation, and
sections 5 and 6 discuss future work and conclu-
sions, respectively.
2 Background
2.1 Logical Semantics
Logic-based representations of meaning have a
long tradition (Montague, 1970; Kamp and Reyle,
1993). They handle many complex semantic phe-
nomena such as relational propositions, logical
operators, and quantifiers; however, their binary
nature prevents them from capturing the ?graded?
aspects of meaning in language. Also, it is difficult
to construct formal ontologies of properties and re-
lations that have broad coverage, and semantically
parsing sentences into logical expressions utilizing
such an ontology is very difficult. Consequently,
current semantic parsers are mostly restricted to
quite limited domains, such as querying a specific
database (Kwiatkowski et al, 2013; Berant et al,
2013). In contrast, our system is not limited to any
formal ontology and can use a wide-coverage tool
for semantic analysis, as discussed below.
2.2 Distributional Semantics
Distributional models (Turney and Pantel, 2010),
on the other hand, use statistics on contextual
data from large corpora to predict semantic sim-
ilarity of words and phrases (Landauer and Du-
mais, 1997; Mitchell and Lapata, 2010). They are
relatively easier to build than logical representa-
tions, automatically acquire knowledge from ?big
data,? and capture the ?graded? nature of linguis-
tic meaning, but do not adequately capture logical
structure (Grefenstette, 2013).
Distributional models are motivated by the ob-
servation that semantically similar words occur in
similar contexts, so words can be represented as
vectors in high dimensional spaces generated from
the contexts in which they occur (Landauer and
Dumais, 1997; Lund and Burgess, 1996). Such
models have also been extended to compute vec-
tor representations for larger phrases, e.g. by
adding the vectors for the individual words (Lan-
dauer and Dumais, 1997) or by a component-wise
product of word vectors (Mitchell and Lapata,
2008; Mitchell and Lapata, 2010), or more com-
plex methods that compute phrase vectors from
word vectors and tensors (Baroni and Zamparelli,
2010; Grefenstette and Sadrzadeh, 2011). We use
vector addition (Landauer and Dumais, 1997), and
component-wise product (Mitchell and Lapata,
2008) as baselines for STS. Vector addition was
previously found to be the best performing sim-
ple distributional method for STS (Beltagy et al,
2013).
2.3 Markov Logic Networks
Markov Logic Networks (MLN) (Richardson and
Domingos, 2006) are a framework for probabilis-
tic logic that employ weighted formulas in first-
order logic to compactly encode complex undi-
rected probabilistic graphical models (i.e., Markov
networks). Weighting the rules is a way of soft-
ening them compared to hard logical constraints
and thereby allowing situations in which not all
clauses are satisfied. MLNs define a probability
distribution over possible worlds, where a world?s
probability increases exponentially with the to-
tal weight of the logical clauses that it satisfies.
A variety of inference methods for MLNs have
been developed, however, developing a scalable,
general-purpose, accurate inference method for
complex MLNs is an open problem. Beltagy et
al. (2013) use MLNs to represent the meaning of
natural language sentences and judge textual en-
tailment and semantic similarity, but they were un-
able to scale the approach beyond short sentences
due to the complexity of MLN inference.
2.4 Probabilistic Soft Logic
Probabilistic Soft Logic (PSL) is a recently pro-
posed alternative framework for probabilistic logic
(Kimmig et al, 2012; Bach et al, 2013). It uses
logical representations to compactly define large
graphical models with continuous variables, and
includes methods for performing efficient proba-
bilistic inference for the resulting models. A key
distinguishing feature of PSL is that ground atoms
have soft, continuous truth values in the interval
[0, 1] rather than binary truth values as used in
MLNs and most other probabilistic logics. Given
a set of weighted logical formulas, PSL builds a
graphical model defining a probability distribution
over the continuous space of values of the random
variables in the model.
1211
A PSL model is defined using a set of weighted
if-then rules in first-order logic, as in the following
example:
?x, y, z. friend(x, y) ? votesFor(y, z)?
votesFor(x, z) | 0.3 (1)
?x, y, z. spouse(x, y) ? votesFor(y, z)?
votesFor(x, z) | 0.8 (2)
In our notation, we use lower case letters like
x, y, z to represent variables and upper case let-
ters for constants. The first rule states that a per-
son is likely to vote for the same person as his/her
friend. The second rule encodes the same regular-
ity for a person?s spouse. The weights encode the
knowledge that a spouse?s influence is greater than
a friend?s in this regard.
In addition, PSL includes similarity functions.
Similarity functions take two strings or two sets as
input and return a truth value in the interval [0, 1]
denoting the similarity of the inputs. For example,
in our application, we generate inference rules that
incorporate the similarity of two predicates. This
can be represented in PSL as:
?x. similarity(?predicate1?, ?predicate2?) ?
predicate1(x)? predicate2(x)
As mentioned above, each ground atom, a,
has a soft truth value in the interval [0, 1],
which is denoted by I(a). To compute soft truth
values for logical formulas, Lukasiewicz?s re-
laxation of conjunctions(?), disjunctions(?) and
negations(?) are used:
I(l
1
? l
1
) = max{0, I(l
1
) + I(l
2
)? 1}
I(l
1
? l
1
) = min{I(l
1
) + I(l
2
), 1}
I(?l
1
) = 1? I(l
1
)
Then, a given rule r ? r
body
? r
head
, is said to be
satisfied (i.e. I(r) = 1) iff I(r
body
) ? I(r
head
).
Otherwise, PSL defines a distance to satisfaction
d(r) which captures how far a rule r is from being
satisfied: d(r) = max{0, I(r
body
) ? I(r
head
)}.
For example, assume we have the set of evidence:
I(spouse(B,A)) = 1, I(votesFor(A,P )) =
0.9, I(votesFor(B,P )) = 0.3, and that r
is the resulting ground instance of rule (2).
Then I(spouse(B,A) ? votesFor(A,P )) =
max{0, 1 + 0.9 ? 1} = 0.9, and d(r) =
max{0, 0.9? 0.3} = 0.6.
Using distance to satisfaction, PSL defines a
probability distribution over all possible interpre-
tations I of all ground atoms. The pdf is defined
as follows:
p(I) =
1
Z
exp [?
?
r?R
?
r
(d(r))
p
]; (3)
Z =
?
I
exp [?
?
r?R
?
r
(d(r))
p
]
where Z is the normalization constant, ?
r
is the
weight of rule r, R is the set of all rules, and p ?
{1, 2} provides two different loss functions. For
our application, we always use p = 1
PSL is primarily designed to support MPE in-
ference (Most Probable Explanation). MPE infer-
ence is the task of finding the overall interpretation
with the maximum probability given a set of evi-
dence. Intuitively, the interpretation with the high-
est probability is the interpretation with the lowest
distance to satisfaction. In other words, it is the
interpretation that tries to satisfy all rules as much
as possible. Formally, from equation 3, the most
probable interpretation, is the one that minimizes
?
r?R
?
r
(d(r))
p
. In case of p = 1, and given
that all d(r) are linear equations, then minimizing
the sum requires solving a linear program, which,
compared to inference in other probabilistic logics
such as MLNs, can be done relatively efficiently
using well-established techniques. In case p = 2,
MPE inference can be shown to be a second-order
cone program (SOCP) (Kimmig et al, 2012).
2.5 Semantic Textual Similarity
Semantic Textual Similarity (STS) is the task of
judging the similarity of a pair of sentences on
a scale from 0 to 5, and was recently introduced
as a SemEval task (Agirre et al, 2012). Gold
standard scores are averaged over multiple hu-
man annotations and systems are evaluated using
the Pearson correlation between a system?s out-
put and gold standard scores. The best perform-
ing system in 2012?s competition was by B?ar et
al. (2012), a complex ensemble system that inte-
grates many techniques including string similarity,
n-gram overlap, WordNet similarity, vector space
similarity and MT evaluation metrics. Two of the
datasets we use for evaluation are from the 2012
competition. We did not utilize the new datasets
added in the 2013 competition since they did not
contain naturally-occurring, full sentences, which
is the focus of our work.
1212
2.6 Combining logical and distributional
methods using probabilistic logic
There are a few recent attempts to combine log-
ical and distributional representations in order to
obtain the advantages of both. Lewis and Steed-
man (2013) use distributional information to deter-
mine word senses, but still produce a strictly log-
ical semantic representation that does not address
the ?graded? nature of linguistic meaning that is
important to measuring semantic similarity.
Garrette et al (2011) introduced a framework
for combining logic and distributional models us-
ing probabilistic logic. Distributional similarity
between pairs of words is converted into weighted
inference rules that are added to the logical repre-
sentation, and Markov Logic Networks are used to
perform probabilistic logical inference.
Beltagy et al (2013) extended this framework
by generating distributional inference rules from
phrase similarity and tailoring the system to the
STS task. STS is treated as computing the prob-
ability of two textual entailments T |= H and
H |= T , where T and H are the two sentences
whose similarity is being judged. These two en-
tailment probabilities are averaged to produce a
measure of similarity. The MLN constructed to
determine the probability of a given entailment
includes the logical forms for both T and H as
well as soft inference rules that are constructed
from distributional information. Given a similar-
ity score for all pairs of sentences in the dataset,
a regressor is trained on the training set to map
the system?s output to the gold standard scores.
The trained regressor is applied to the scores in
the test set before calculating Pearson correlation.
The regression algorithm used is Additive Regres-
sion (Friedman, 2002).
To determine an entailment probability, first,
the two sentences are mapped to logical repre-
sentations using Boxer (Bos, 2008), a tool for
wide-coverage semantic analysis that maps a CCG
(Combinatory Categorial Grammar) parse into a
lexically-based logical form. Boxer uses C&C for
CCG parsing (Clark and Curran, 2004).
Distributional semantic knowledge is then en-
coded as weighted inference rules in the MLN.
A rule?s weight (w) is a function of the cosine
similarity (sim) between its antecedent and con-
sequent. Rules are generated on the fly for each
T and H . Let t and h be the lists of all words
and phrases in T and H respectively. For all
pairs (a, b), where a ? t, b ? h, it generates
an inference rule: a ? b | w, where w =
f(sim(
??
a ,
??
b )). Both a and b can be words or
phrases. Phrases are defined in terms of Boxer?s
output. A phrase is more than one unary atom
sharing the same variable like ?a little kid? which
in logic is little(K) ? kid(K). A phrase also can
be two unary atoms connected by a relation like
?a man is driving? which in logic is man(M) ?
agent(D,M) ? drive(D). The similarity func-
tion sim takes two vectors as input. Phrasal vec-
tors are constructed using Vector Addition (Lan-
dauer and Dumais, 1997). The set of generated
inference rules can be regarded as the knowledge
base KB.
Beltagy et al (2013) found that the logical con-
junction in H is very restrictive for the STS task,
so they relaxed the conjunction by using an aver-
age evidence combiner (Natarajan et al, 2010).
The average combiner results in computationally
complex inference and only works for short sen-
tences. In case inference breaks or times-out, they
back off to a simpler combiner that leads to much
faster inference but loses most of the structure of
the sentence and is therefore less accurate.
Given T , KB and H from the previous
steps, MLN inference is then used to compute
p(H|T,KB), which is then used as a measure of
the degree to which T entails H .
3 PSL for STS
For several reasons, we believe PSL is a more ap-
propriate probabilistic logic for STS than MLNs.
First, it is explicitly designed to support efficient
inference, therefore it scales better to longer sen-
tences with more complex logical forms. Sec-
ond, it was also specifically designed for com-
puting similarity between complex structured ob-
jects rather than determining probabilistic logical
entailment. In fact, the initial version of PSL
(Broecheler et al, 2010) was called Probabilis-
tic Similarity Logic, based on its use of similar-
ity functions. This initial version was shown to be
very effective for measuring the similarity of noisy
database records and performing record linkage
(i.e. identifying database entries referring to the
same entity, such as bibliographic citations refer-
ring to the same paper). Therefore, we have devel-
oped an approach that follows that of Beltagy et
al. (2013), but replaces Markov Logic with PSL.
This section explains how we formulate the STS
1213
task as a PSL program. PSL does not work very
well ?out of the box? for STS, mainly because
Lukasiewicz?s equation for the conjunction is very
restrictive. Therefore, we use a different interpre-
tation for conjunction that uses averaging, which
requires corresponding changes to the optimiza-
tion problem and the grounding technique.
3.1 Representation
Given the logical forms for a pair of sentences,
a text T and a hypothesis H , and given a set of
weighted rules derived from the distributional se-
mantics (as explained in section 2.6) composing
the knowledge base KB, we build a PSL model
that supports determining the truth value of H in
the most probable interpretation (i.e. MPE) given
T and KB.
Consider the pair of sentences is ?A man is driv-
ing?, and ?A guy is walking?. Parsing into logical
form gives:
T : ?x, y. man(x) ? agent(y, x) ? drive(y)
H : ?x, y. guy(x) ? agent(y, x) ? walk(y)
The PSL program is constructed as follows:
T : The text is represented in the evidence set. For
the example, after Skolemizing the existential
quantifiers, this contains the ground atoms:
{man(A), agent(B,A), drive(B)}
KB: The knowledge base is a set of lexical and
phrasal rules generated from distributional
semantics, along with a similarity score for
each rule (section 2.6). For the exam-
ple, we generate the rules: ?x. man(x) ?
vs sim(?man?, ?guy?)? guy(x) ,
?x.drive(x)?vs sim(?drive?, ?walk?)?
walk(x)
where vs sim is a similarity function that
calculates the distributional similarity score
between the two lexical predicates. All rules
are assigned the same weight because all
rules are equally important.
H: The hypothesis is represented as H ?
result(), and then PSL is queried for the
truth value of the atom result(). For
our example, the rule is: ?x, y. guy(x) ?
agent(y, x) ? walk(y)? result().
Priors: A low prior is given to all predicates. This
encourages the truth values of ground atoms
to be zero, unless there is evidence to the con-
trary.
For each STS pair of sentences S
1
, S
2
, we run
PSL twice, once where T = S
1
, H = S
2
and
another where T = S
2
, H = S
1
, and output the
two scores. To produce a final similarity score, we
train a regressor to learn the mapping between the
two PSL scores and the overall similarity score.
As in Beltagy et al, (2013) we use Additive Re-
gression (Friedman, 2002).
3.2 Changing Conjunction
As mentioned above, Lukasiewicz?s formula for
conjunction is very restrictive and does not work
well for STS. For example, for T: ?A man is driv-
ing? and H: ?A man is driving a car?, if we use the
standard PSL formula for conjunction, the output
value is zero because there is no evidence for a car
and max(0, X + 0 ? 1) = 0 for any truth value
0 ? X ? 1. However, humans find these sen-
tences to be quite similar.
Therefore, we introduce a new averaging inter-
pretation of conjunction that we use for the hy-
pothesis H . The truth value for a conjunction
is defined as I(p
1
? .... ? p
n
) =
1
n
?
n
i=1
I(p
i
).
This averaging function is linear, and the result is
a valid truth value in the interval [0, 1], therefore
this change is easily incorporated into PSL with-
out changing the complexity of inference which
remains a linear-programming problem.
It would perhaps be even better to use a
weighted average, where weights for different
components are learned from a supervised train-
ing set. This is an important direction for future
work.
3.3 Grounding Process
Grounding is the process of instantiating the vari-
ables in the quantified rules with concrete con-
stants in order to construct the nodes and links in
the final graphical model. In principle, ground-
ing requires instantiating each rule in all possible
ways, substituting every possible constant for each
variable in the rule. However, this is a combinato-
rial process that can easily result in an explosion in
the size of the final network. Therefore, PSL em-
ploys a ?lazy? approach to grounding that avoids
the construction of irrelevant groundings. If there
is no evidence for one of the antecedents in a par-
ticular grounding of a rule, then the normal PSL
formula for conjunction guarantees that the rule is
1214
Algorithm 1 Heuristic Grounding
Input: r
body
= a
1
? ....?a
n
: antecedent of a rule
with average interpretation of conjunction
Input: V : set of variables used in r
body
Input: Ant(v
i
): subset of antecedents a
j
con-
taining variable v
i
Input: Const(v
i
): list of possible constants of
variable v
i
Input: Gnd(a
i
): set of ground atoms of a
i
.
Input: GndConst(a, g, v): takes an atom a,
grounding g for a, and variable v, and returns
the constant that substitutes v in g
Input: gnd limit: limit on the number of
groundings
1: for all v
i
? V do
2: for all C ? Const(v
i
) do
3: score(C) =
?
a?Ant(v
i
)
(max I(g))
for g ? Gnd(a) ?GndConst(a, g, v
i
) = C
4: end for
5: sort Const(v
i
) on scores, descending
6: end for
7: return For all v
i
? V , take the Cartesian-
product of the sortedConst(v
i
) and return the
top gnd limit results
trivially satisfied (I(r) = 1) since the truth value
of the antecedent is zero. Therefore, its distance to
satisfaction is also zero, and it can be omitted from
the ground network without impacting the result of
MPE inference.
However, this technique does not work once
we switch to using averaging to interpret conjunc-
tions. For example, given the rule ?x. p(x) ?
q(x) ? t() and only one piece of evidence p(C)
there are no relevant groundings because there is
no evidence for q(C), and therefore, for normal
PSL, I(p(C) ? q(C)) = 0 which does not affect
I(t()). However, when using averaging with the
same evidence, we need to generate the grounding
p(C)?q(C) because I(p(C)?q(C)) = 0.5 which
does affect I(t()).
One way to solve this problem is to eliminate
lazy grounding and generate all possible ground-
ings. However, this produces an intractably large
network. Therefore, we developed a heuristic ap-
proximate grounding technique that generates a
subset of the most impactful groundings.
Pseudocode for this heuristic approach is shown
in algorithm 1. Its goal is to find constants that
participate in ground propositions with high truth
value and preferentially use them to construct a
limited number of groundings of each rule.
The algorithm takes the antecedents of a rule
employing averaging conjunction as input. It also
takes the grounding limit which is a threshold on
the number of groundings to be returned. The al-
gorithm uses several subroutines, they are:
? Ant(v
i
): given a variable v
i
, it returns the set
of rule antecedent atoms containing v
i
. E.g,
for the rule: a(x) ? b(y) ? c(x), Ant(x) re-
turns the set of atoms {a(x), c(x)}.
? Const(v
i
): given a variable v
i
, it returns the
list of possible constants that can be used to
instantiate the variable v
i
.
? Gnd(a
i
): given an atom a
i
, it returns the set
of all possible ground atoms generated for a
i
.
? GndConst(a, g, v): given an atom a and
grounding g for a, and a variable v, it finds
the constant that substitutes for v in g. E.g,
assume there is an atom a = a
i
(v
1
, v
2
), and
the ground atom g = a
i
(A,B) is one of its
groundings. GndConst(a, g, v
2
) would re-
turn the constant B since it is the substitution
for the variable v
2
in g.
Lines 1-6 loop over all variables in the rule. For
each variable, lines 2-5 construct a list of constants
for that variable and sort it based on a heuristic
score. In line 3, each constant is assigned a score
that indicates the importance of this constant in
terms of its impact on the truth value of the overall
grounding. A constant?s score is the sum, over all
antedents that contain the variable in question, of
the maximum truth value of any grounding of that
antecedent that contains that constant.
Pushing constants with high scores to the top
of each variable?s list will tend to make the over-
all truth value of the top groundings high. Line
7 computes a subset of the Cartesian product of
the sorted lists of constants, selecting constants in
ranked order and limiting the number of results to
the grounding limit.
One point that needs to be clarified about this
approach is how it relies on the truth values of
ground atoms when the goal of inference is to ac-
tually find these values. PSL?s inference is ac-
tually an iterative process where in each itera-
tion a grounding phase is followed by an opti-
mization phase (solving the linear program). This
loop repeats until convergence, i.e. until the truth
1215
values stop changing. The truth values used in
each grounding phase come from the previous op-
timization phase. The first grounding phase as-
sumes only the propositions in the evidence pro-
vided have non-zero truth values.
4 Evaluation
This section evaluates the performance of PSL on
the STS task.
4.1 Datasets
We evaluate our system on three STS datasets.
? msr-vid: Microsoft Video Paraphrase Cor-
pus from STS 2012. The dataset consists
of 1,500 pairs of short video descriptions
collected using crowdsourcing (Chen and
Dolan, 2011) and subsequently annotated for
the STS task (Agirre et al, 2012). Half of
the dataset is for training, and the second half
is for testing.
? msr-par: Microsoft Paraphrase Corpus from
STS 2012 task. The dataset is 5,801
pairs of sentences collected from news
sources (Dolan et al, 2004). Then, for STS
2012, 1,500 pairs were selected and anno-
tated with similarity scores. Half of the
dataset is for training, and the second half is
for testing.
? SICK: Sentences Involving Compositional
Knowledge is a dataset collected for SemEval
2014. Only the training set is available at this
point, which consists of 5,000 pairs of sen-
tences. Pairs are annotated for RTE and STS,
but we only use the STS data. Training and
testing was done using 10-fold cross valida-
tion.
4.2 Systems Compared
We compare our PSL system with several others.
In all cases, we use the distributional word vec-
tors employed by Beltagy et al (2013) based on
context windows from Gigaword.
? vec-add: Vector Addition (Landauer and
Dumais, 1997). We compute a vector rep-
resentation for each sentence by adding the
distributional vectors of all of its words and
measure similarity using cosine. This is a
simple yet powerful baseline that uses only
distributional information.
? vec-mul: Component-wise Vector Multipli-
cation (Mitchell and Lapata, 2008). The
same as vec-add except uses component-
wise multiplication to combine word vectors.
? MLN: The system of Beltagy et al (2013),
which uses Markov logic instead of PSL for
probabilistic inference. MLN inference is
very slow in some cases, so we use a 10
minute timeout. When MLN times out, it
backs off to a simpler sentence representation
as explained in section 2.6.
? PSL: Our proposed PSL system for combin-
ing logical and distributional information.
? PSL-no-DIR: Our PSL system without dis-
tributional inference rules(empty knowledge
base). This system uses PSL to compute sim-
ilarity of logical forms but does not use dis-
tributional information on lexical or phrasal
similarity. It tests the impact of the proba-
bilistic logic only
? PSL+vec-add: PSL ensembled with vec-
add. Ensembling the MLN approach with a
purely distributional approach was found to
improve results (Beltagy et al, 2013), so we
also tried this with PSL. The methods are en-
sembled by using both entailment scores of
both systems as input features to the regres-
sion step that learns to map entailment scores
to STS similarity ratings. This way, the train-
ing data is used to learn how to weight the
contribution of the different components.
? PSL+MLN: PSL ensembled with MLN in
the same manner.
4.3 Experiments
Systems are evaluated on two metrics, Pearson
correlation and average CPU time per pair of sen-
tences.
? Pearson correlation: The Pearson correlation
between the system?s similarity scores and
the human gold-standards.
? CPU time: This metric only applies to MLN
and PSL. The CPU time taken by the infer-
ence step is recorded and averaged over all
pairs in each of the test datasets. In many
cases, MLN inference is very slow, so we
timeout after 10 minutes and report the num-
ber of timed-out pairs on each dataset.
1216
msr-vid msr-par SICK
vec-add 0.78 0.24 0.65
vec-mul 0.76 0.12 0.62
MLN 0.63 0.16 0.47
PSL-no-DIR 0.74 0.46 0.68
PSL 0.79 0.53 0.70
PSL+vec-add 0.83 0.49 0.71
PSL+MLN 0.79 0.51 0.70
Best Score (B?ar
et al, 2012)
0.87 0.68 n/a
Table 1: STS Pearson Correlations
PSL MLN
time time timeouts/total
msr-vid 8s 1m 31s 132/1500
msr-par 30s 11m 49s 1457/1500
SICK 10s 4m 24s 1791/5000
Table 2: Average CPU time per STS pair, and
number of timed-out pairs in MLN with a 10
minute time limit. PSL?s grounding limit is set to
10,000 groundings.
We also evaluated the effect of changing the
grounding limit on both Pearson correlation and
CPU time for the msr-par dataset. Most of the
sentences in msr-par are long, which results is
large number of groundings, and limiting the num-
ber of groundings has a visible effect on the over-
all performance. In the other two datasets, the
sentences are fairly short, and the full number of
groundings is not large; therefore, changing the
grounding limit does not significantly affect the re-
sults.
4.4 Results and Discussion
Table 1 shows the results for Pearson correlation.
PSL out-performs the purely distributional base-
lines (vec-add and vec-mul) because PSL is able
to combine the information available to vec-add
and vec-mul in a better way that takes sentence
structure into account. PSL also outperforms
the unaided probabilistic-logic baseline that does
not use distributional information (PSL-no-DIR).
PSL-no-DIR works fairly well because there is
significant overlap in the exact words and struc-
ture of the paired sentences in the test data, and
PSL combines the evidence from these similari-
ties effectively. In addition, PSL always does sig-
nificantly better than MLN, because of the large
Figure 1: Effect of PSL?s grounding limit on the
correlation score for the msr-par dataset
number of timeouts, and because the conjunction-
averaging in PSL is combining evidence bet-
ter than MLN?s average-combiner, whose perfor-
mance is sensitive to various parameters. These
results further support the claim that using prob-
abilistic logic to integrate logical and distribu-
tional information is a promising approach to
natural-language semantics. More specifically,
they strongly indicate that PSL is a more effective
probabilistic logic for judging semantic similarity
than MLNs.
Like for MLNs (Beltagy et al, 2013), en-
sembling PSL with vector addition improved the
scores a bit, except for msr-par where vec-add?s
performance is particularly low. However, this en-
semble still does not beat the state of the art (B?ar et
al., 2012) which is a large ensemble of many dif-
ferent systems. It would be informative to add our
system to their ensemble to see if it could improve
it even further.
Table 2 shows the CPU time for PSL and MLN.
The results clearly demonstrate that PSL is an or-
der of magnitude faster than MLN.
Figures 1 and 2 show the effect of changing the
grounding limit on Pearson correlation and CPU
time. As expected, as the grounding limit is in-
creased, accuracy improves but CPU time also
increases. However, note that the difference in
scores between the smallest and largest grounding
limit tested is not large, suggesting that the heuris-
tic approach to limiting grounding is quite effec-
tive.
5 Future Work
As mentioned in Section 3.2, it would be good
to use a weighted average to compute the truth
1217
Figure 2: Effect of PSL?s grounding limit on CPU
time for the msr-par dataset
values for conjunctions, weighting some predi-
cates more than others rather than treating them
all equally. Appropriate weights for different com-
ponents could be learned from training data. For
example, such an approach could learn that the
type of an object determined by a noun should be
weighted more than a property specified by an ad-
jective. As a result, ?black dog? would be appro-
priately judged more similar to ?white dog? than
to ?black cat.?
One of the advantages of using a probabilis-
tic logic is that additional sources of knowledge
can easily be incorporated by adding additional
soft inference rules. To complement the soft in-
ference rules capturing distributional lexical and
phrasal similarities, PSL rules could be added that
encode explicit paraphrase rules, such as those
mined from monolingual text (Berant et al, 2011)
or multi-lingual parallel text (Ganitkevitch et al,
2013).
This paper has focused on STS; however, as
shown by Beltagy et al (2013), probabilistic logic
is also an effective approach to recognizing tex-
tual entailment (RTE). By using the appropriate
functions to combine truth values for various log-
ical connectives, PSL could also be adapted for
RTE. Although we have shown that PSL outper-
forms MLNs on STS, we hypothesize that MLNs
may still be a better approach for RTE. However, it
would be good to experimentally confirm this in-
tuition. In any case, the high computational com-
plexity of MLN inference could mean that PSL is
still a more practical choice for RTE.
6 Conclusion
This paper has presented an approach that uses
Probabilistic Soft Logic (PSL) to determine Se-
mantic Textual Similarity (STS). The approach
uses PSL to effectively combine logical seman-
tic representations of sentences with soft infer-
ence rules for lexical and phrasal similarities com-
puted from distributional information. The ap-
proach builds upon a previous method that uses
Markov Logic (MLNs) for STS, but replaces the
probabilistic logic with PSL in order to improve
the efficiency and accuracy of probabilistic infer-
ence. The PSL approach was experimentally eval-
uated on three STS datasets and was shown to out-
perform purely distributional baselines as well as
the MLN approach. The PSL approach was also
shown to be much more scalable and efficient than
using MLNs
Acknowledgments
This research was supported by the DARPA DEFT
program under AFRL grant FA8750-13-2-0026.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the author and do not necessarily reflect the view
of DARPA, DoD or the US government. Some ex-
periments were run on the Mastodon Cluster sup-
ported by NSF Grant EIA-0303609.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of Semantic Evaluation (SemEval-12).
Stephen H. Bach, Bert Huang, Ben London, and Lise
Getoor. 2013. Hinge-loss Markov random fields:
Convex inference for structured prediction. In Pro-
ceedings of Uncertainty in Artificial Intelligence
(UAI-13).
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. UKP: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In Proceedings of Semantic
Evaluation (SemEval-12).
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of Conference on Empirical Methods in
Natural Language Processing (EMNLP-10).
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
1218
Montague meets Markov: Deep semantics with
probabilistic logical form. In Proceedings of the
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM-13).
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of Association for Computational Lin-
guistics (ACL-11).
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP-13).
Johan Bos. 2008. Wide-coverage semantic analysis
with Boxer. In Proceedings of Semantics in Text
Processing (STEP-08).
Matthias Broecheler, Lilyana Mihalkova, and Lise
Getoor. 2010. Probabilistic Similarity Logic. In
Proceedings of Uncertainty in Artificial Intelligence
(UAI-20).
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of Association for Computational Lin-
guistics (ACL-11).
Stephen Clark and James R. Curran. 2004. Parsing
the WSJ using CCG and log-linear models. In Pro-
ceedings of Association for Computational Linguis-
tics (ACL-04).
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of the International Conference on
Computational Linguistics (COLING-04).
Jerome H Friedman. 2002. Stochastic gradient boost-
ing. Journal of Computational Statistics & Data
Analysis (CSDA-02).
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT-13).
Dan Garrette, Katrin Erk, and Raymond Mooney.
2011. Integrating logical representations with prob-
abilistic information using Markov logic. In Pro-
ceedings of International Conference on Computa-
tional Semantics (IWCS-11).
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of Conference on Empirical Methods in Natural
Language Processing (EMNLP-11).
Edward Grefenstette. 2013. Towards a formal distri-
butional semantics: Simulating logical calculi with
tensors. In Proceedings of Second Joint Conference
on Lexical and Computational Semantics (*SEM
2013).
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer.
Angelika Kimmig, Stephen H. Bach, Matthias
Broecheler, Bert Huang, and Lise Getoor. 2012.
A short introduction to Probabilistic Soft Logic.
In Proceedings of NIPS Workshop on Probabilistic
Programming: Foundations and Applications (NIPS
Workshop-12).
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-13).
T. K. Landauer and S. T. Dumais. 1997. A solution to
Plato?s problem: The Latent Semantic Analysis the-
ory of the acquisition, induction, and representation
of knowledge. Psychological Review.
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. Transactions
of the Association for Computational Linguistics
(TACL-13).
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of Association for Computational Linguistics (ACL-
08).
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Journal of
Cognitive Science.
Richard Montague. 1970. Universal grammar. Theo-
ria, 36:373?398.
Sriraam Natarajan, Tushar Khot, Daniel Lowd, Prasad
Tadepalli, Kristian Kersting, and Jude Shavlik.
2010. Exploiting causal independence in Markov
logic networks: Combining undirected and directed
models. In Proceedings of European Conference in
Machine Learning (ECML-10).
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning,
62:107?136.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research
(JAIR-10).
1219
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 11?21, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Montague Meets Markov: Deep Semantics with Probabilistic Logical Form
Islam Beltagy?, Cuong Chau?, Gemma Boleda?, Dan Garrette?, Katrin Erk?,
Raymond Mooney?
?Department of Computer Science
?Department of Linguistics
The University of Texas at Austin
Austin, Texas 78712
?{beltagy,ckcuong,dhg,mooney}@cs.utexas.edu
?gemma.boleda@utcompling.com,katrin.erk@mail.utexas.edu
Abstract
We combine logical and distributional rep-
resentations of natural language meaning by
transforming distributional similarity judg-
ments into weighted inference rules using
Markov Logic Networks (MLNs). We show
that this framework supports both judg-
ing sentence similarity and recognizing tex-
tual entailment by appropriately adapting the
MLN implementation of logical connectives.
We also show that distributional phrase simi-
larity, used as textual inference rules created
on the fly, improves its performance.
1 Introduction
Tasks in natural language semantics are very diverse
and pose different requirements on the underlying
formalism for representing meaning. Some tasks
require a detailed representation of the structure of
complex sentences. Some tasks require the ability to
recognize near-paraphrases or degrees of similarity
between sentences. Some tasks require logical infer-
ence, either exact or approximate. Often it is neces-
sary to handle ambiguity and vagueness in meaning.
Finally, we frequently want to be able to learn rele-
vant knowledge automatically from corpus data.
There is no single representation for natural lan-
guage meaning at this time that fulfills all require-
ments. But there are representations that meet some
of the criteria. Logic-based representations (Mon-
tague, 1970; Kamp and Reyle, 1993) provide an
expressive and flexible formalism to express even
complex propositions, and they come with standard-
ized inference mechanisms. Distributional mod-
hamster(gerbil(
sim( #                ?hamster, #         ?gerbil) = w
8x hamster(x) ! gerbil(x)  | f(w)
Figure 1: Turning distributional similarity into a
weighted inference rule
els (Turney and Pantel, 2010) use contextual sim-
ilarity to predict semantic similarity of words and
phrases (Landauer and Dumais, 1997; Mitchell and
Lapata, 2010), and to model polysemy (Schu?tze,
1998; Erk and Pado?, 2008; Thater et al, 2010).
This suggests that distributional models and logic-
based representations of natural language meaning
are complementary in their strengths (Grefenstette
and Sadrzadeh, 2011; Garrette et al, 2011), which
encourages developing new techniques to combine
them.
Garrette et al (2011; 2013) propose a framework
for combining logic and distributional models in
which logical form is the primary meaning repre-
sentation. Distributional similarity between pairs of
words is converted into weighted inference rules that
are added to the logical form, as illustrated in Fig-
ure 1. Finally, Markov Logic Networks (Richardson
and Domingos, 2006) (MLNs) are used to perform
weighted inference on the resulting knowledge base.
However, they only employed single-word distribu-
tional similarity rules, and only evaluated on a small
11
set of short, hand-crafted test sentences.
In this paper, we extend Garrette et al?s approach
and adapt it to handle two existing semantic tasks:
recognizing textual entailment (RTE) and seman-
tic textual similarity (STS). We show how this sin-
gle semantic framework using probabilistic logical
form in Markov logic can be adapted to support both
of these important tasks. This is possible because
MLNs constitute a flexible programming language
based on probabilistic logic (Domingos and Lowd,
2009) that can be easily adapted to support multiple
types of linguistically useful inference.
At the word and short phrase level, our approach
model entailment through ?distributional? similarity
(Figure 1). If X and Y occur in similar contexts, we
assume that they describe similar entities and thus
there is some degree of entailment between them. At
the sentence level, however, we hold that a stricter,
logic-based view of entailment is beneficial, and we
even model sentence similarity (in STS) as entail-
ment.
There are two main innovations in the formalism
that make it possible for us to work with naturally
occurring corpus data. First, we use more expres-
sive distributional inference rules based on the sim-
ilarity of phrases rather than just individual words.
In comparison to existing methods for creating tex-
tual inference rules (Lin and Pantel, 2001b; Szpek-
tor and Dagan, 2008), these rules are computed on
the fly as needed, rather than pre-compiled. Second,
we use more flexible probabilistic combinations of
evidence in order to compute degrees of sentence
similarity for STS and to help compensate for parser
errors. We replace deterministic conjunction by an
average combiner, which encodes causal indepen-
dence (Natarajan et al, 2010).
We show that our framework is able to han-
dle both sentence similarity (STS) and textual en-
tailment (RTE) by making some simple adapta-
tions to the MLN when switching between tasks.
The framework achieves reasonable results on both
tasks. On STS, we obtain a correlation of r = 0.66
with full logic, r = 0.73 in a system with weak-
ened variable binding, and r = 0.85 in an ensemble
model. On RTE-1 we obtain an accuracy of 0.57.
We show that the distributional inference rules ben-
efit both tasks and that more flexible probabilistic
combinations of evidence are crucial for STS. Al-
though other approaches could be adapted to handle
both RTE and STS, we do not know of any other
methods that have been explicitly tested on both
problems.
2 Related work
Distributional semantics Distributional models
define the semantic relatedness of words as the
similarity of vectors representing the contexts in
which they occur (Landauer and Dumais, 1997;
Lund and Burgess, 1996). Recently, such mod-
els have also been used to represent the meaning
of larger phrases. The simplest models compute
a phrase vector by adding the vectors for the indi-
vidual words (Landauer and Dumais, 1997) or by a
component-wise product of word vectors (Mitchell
and Lapata, 2008; Mitchell and Lapata, 2010).
Other approaches, in the emerging area of distribu-
tional compositional semantics, use more complex
methods that compute phrase vectors from word
vectors and tensors (Baroni and Zamparelli, 2010;
Grefenstette and Sadrzadeh, 2011).
Wide-coverage logic-based semantics Boxer
(Bos, 2008) is a software package for wide-coverage
semantic analysis that produces logical forms using
Discourse Representation Structures (Kamp and
Reyle, 1993). It builds on the C&C CCG parser
(Clark and Curran, 2004).
Markov Logic In order to combine logical and
probabilistic information, we draw on existing work
in Statistical Relational AI (Getoor and Taskar,
2007). Specifically, we utilize Markov Logic Net-
works (MLNs) (Domingos and Lowd, 2009), which
employ weighted formulas in first-order logic to
compactly encode complex undirected probabilistic
graphical models. MLNs are well suited for our ap-
proach since they provide an elegant framework for
assigning weights to first-order logical rules, com-
bining a diverse set of inference rules and perform-
ing sound probabilistic inference.
An MLN consists of a set of weighted first-order
clauses. It provides a way of softening first-order
logic by allowing situations in which not all clauses
are satisfied. More specifically, they provide a
well-founded probability distribution across possi-
ble worlds by specifying that the probability of a
12
world increases exponentially with the total weight
of the logical clauses that it satisfies. While methods
exist for learning MLN weights directly from train-
ing data, since the appropriate training data is lack-
ing, our approach uses weights computed using dis-
tributional semantics. We use the open-source soft-
ware package Alchemy (Kok et al, 2005) for MLN
inference, which allows computing the probability
of a query literal given a set of weighted clauses as
background knowledge and evidence.
Tasks: RTE and STS Recognizing Textual En-
tailment (RTE) is the task of determining whether
one natural language text, the premise, implies an-
other, the hypothesis. Consider (1) below.
(1) p: Oracle had fought to keep the forms from
being released
h: Oracle released a confidential document
Here, h is not entailed. RTE directly tests whether
a system can construct semantic representations that
allow it to draw correct inferences. Of existing RTE
approaches, the closest to ours is by Bos and Mark-
ert (2005), who employ a purely logical approach
that uses Boxer to convert both the premise and hy-
pothesis into first-order logic and then checks for
entailment using a theorem prover. By contrast, our
approach uses Markov logic with probabilistic infer-
ence.
Semantic Textual Similarity (STS) is the task of
judging the similarity of two sentences on a scale
from 0 to 5 (Agirre et al, 2012). Gold standard
scores are averaged over multiple human annota-
tions. The best performer in 2012?s competition was
by Ba?r et al (2012), an ensemble system that inte-
grates many techniques including string similarity,
n-gram overlap, WordNet similarity, vector space
similarity and MT evaluation metrics.
Weighted inference, and combined structural-
distributional representations One approach to
weighted inference in NLP is that of Hobbs et al
(1993), who proposed viewing natural language in-
terpretation as abductive inference. In this frame-
work, problems like reference resolution and syntac-
tic ambiguity resolution become inferences to best
explanations that are associated with costs. How-
ever, this leaves open the question of how costs are
assigned. Raina et al (2005) use this framework for
RTE, deriving inference costs from WordNet simi-
larity and properties of the syntactic parse.
Garrette et al (2011; 2013) proposed an approach
to RTE that uses MLNs to combine traditional log-
ical representations with distributional information
in order to support probabilistic textual inference.
This approach can be viewed as a bridge between
Bos and Markert (2005)?s purely logical approach,
which relies purely on hard logical rules and the-
orem proving, and distributional approaches, which
support graded similarity between concepts but have
no notion of logical operators or entailment.
There are also other methods that combine dis-
tributional and structured representations. Stern et
al. (2011) conceptualize textual entailment as tree
rewriting of syntactic graphs, where some rewrit-
ing rules are distributional inference rules. Socher
et al (2011) recognize paraphrases using a ?tree of
vectors,? a phrase structure tree in which each con-
stituent is associated with a vector, and overall sen-
tence similarity is computed by a classifier that inte-
grates all pairwise similarities. (This is in contrast to
approaches like Baroni and Zamparelli (2010) and
Grefenstette and Sadrzadeh (2011), who do not of-
fer a proposal for using vectors at multiple levels in
a syntactic tree simultaneously.)
3 MLN system
Our system extends that of Garrette et al (2011;
2013) to support larger-scale evaluation on standard
benchmarks for both RTE and STS. We conceptual-
ize both tasks as probabilistic entailment in Markov
logic, where STS is judged as the average degree of
mutual entailment, i.e. we compute the probability
of both S1 |= S2 and S2 |= S1 and average the re-
sults. Below are some sentence pairs that we use as
examples in the discussion below:
(2) S1: A man is slicing a cucumber.
S2: A man is slicing a zucchini.
(3) S1: A boy is riding a bicycle.
S2: A little boy is riding a bike.
(4) S1: A man is driving.
S2: A man is driving a car.
13
System overview. To compute the probability of
an entailment S1 |= S2, the system first constructs
logical forms for each sentence using Boxer and
then translates them into MLN clauses. In example
(2) above, the logical form for S1:
?x0, e1, x2
(
man(x0) ? slice(e1) ?Agent(e1, x0)?
cucumber(x2) ? Patient(e1, x2)
)
is used as evidence, and the logical form for S2 is
turned into the following formula (by default, vari-
ables are assumed to be universally quantified):
man(x) ? slice(e) ?Agent(e, x)?
zucchini(y) ? Patient(e, y)? result()
where result() is the query for which we have
Alchemy compute the probability.
However, S2 is not strictly entailed by S1 because
of the mismatch between ?cucumber? and ?zuc-
chini?, so with just the strict logical-form transla-
tions of S1 and S2, the probability of result() will
be zero. This is where we introduce distributional
similarity, in this case the similarity of ?cucumber?
and ?zucchini?, cos( #                  ?cucumber, #               ?zucchini). We cre-
ate inference rules from such similarities as a form
of background knowledge. We then treat similarity
as degree of entailment, a move that has a long tradi-
tion (e.g., (Lin and Pantel, 2001b; Raina et al, 2005;
Szpektor and Dagan, 2008)). In general, given two
words a and b, we transform their cosine similarity
into an inference-rule weight wt(a, b) using:
wt(a, b) = log( cos(
#?a , #?b )
1? cos( #?a , #?b )
)? prior (5)
Where prior is a negative weight used to initialize
all predicates, so that by default facts are assumed
to have very low probability. In our experiments,
we use prior = ?3. In the case of sentence pair
(2), we generate the inference rule:
cucumber(x)? zucchini(x) | wt(cuc., zuc.)
Such inference rules are generated for all pairs of
words (w1, w2) where w1 ? S1 and w2 ? S2.1
1We omit inference rules for words (a, b) where cos(a, b) <
? for a threshold ? set to maximize performance on the training
data. Low-similarity pairs usually indicate dissimilar words.
This removes a sizeable number of rules for STS, while for RTE
the tuned threshold was near zero.
The distributional model we use contains all lem-
mas occurring at least 50 times in the Gigaword cor-
pus (Graff et al, 2007) except a list of stop words.
The dimensions are the 2,000 most frequent of these
words, and cell values are weighted with point-wise
mutual information. 2
Phrase-based inference rules. Garrette et al only
considered distributional inference rules for pairs of
individual words. We extend their approach to dis-
tributional inference rules for pairs of phrases in or-
der to handle cases like (3). To properly estimate
the similarity between S1 and S2 in (3), we not only
need an inference rule linking ?bike? to ?bicycle?,
but also a rule estimating how similar ?boy? is to
?little boy?. To do so, we make use of existing ap-
proaches that compute distributional representations
for phrases. In particular, we compute the vector for
a phrase from the vectors of the words in that phrase,
using either vector addition (Landauer and Dumais,
1997) or component-wise multiplication (Mitchell
and Lapata, 2008; Mitchell and Lapata, 2010). The
inference-rule weight, wt(p1, p2), for two phrases
p1 and p2 is then determined using Eq. (5) in the
same way as for words.
Existing approaches that derive phrasal inference
rules from distributional similarity (Lin and Pantel,
2001a; Szpektor and Dagan, 2008; Berant et al,
2011) precompile large lists of inference rules. By
comparison, distributional phrase similarity can be
seen as a generator of inference rules ?on the fly?,
as it is possible to compute distributional phrase
vectors for arbitrary phrases on demand as they are
needed for particular examples.
Inference rules are generated for all pairs of con-
stituents (c1, c2) where c1 ? S1 and c2 ? S2, a
constituent is a single word or a phrase. The log-
ical form provides a handy way to extract phrases,
as they are generally mapped to one of two logical
constructs. Either we have multiple single-variable
predicates operating on the same variable. For ex-
ample the phrase ?a little boy? has the logical form
boy(x) ? little(x). Or we have two unary predi-
cates connected with a relation. For example, ?pizza
slice? and ?slice of pizza? are both mapped to the
2It is customary to transform raw counts in a way that cap-
tures association between target words and dimensions, for ex-
ample through point-wise mutual information (Lowe, 2001).
14
logical form, slice(x0) ? of(x0, x1) ? pizza(x1).
We consider all binary predicates as relations.
Average Combiner to determine similarity in the
presence of missing phrases. The logical forms
for the sentences in (4): are
S1: ?x0, e1
(
man(x0)?agent(x0, e1)?drive(e1)
)
S2: ?x0, e1, x2
(
man(x0) ? agent(x0, e1) ?
drive(e1) ? patient(e1, x2) ? car(x2)
)
If we try to prove S1 |= S2, the probability of
the result() will be zero: There is no evidence for
a car, and the hypothesis predicates are conjoined
using a deterministic AND. For RTE, this makes
sense: If one of the hypothesis predicates is False,
the probability of entailment should be zero. For the
STS task, this should in principle be the same, at
least if the omitted facts are vital, but it seems that
annotators rated the data points in this task more for
overall similarity than for degrees of entailment. So
in STS, we want the similarity to be a function of
the number of elements in the hypothesis that are
inferable. Therefore, we need to replace the deter-
ministic AND with a different way of combining
evidence. We chose to use the average evidence
combiner for MLNs introduced by Natarajan et al
(2010). To use the average combiner, the full logi-
cal form is divided into smaller clauses (which we
call mini-clauses), then the combiner averages their
probabilities. In case the formula is a list of con-
juncted predicates, a mini-clause is a conjunction
of a single-variable predicate with a relation predi-
cate(as in the example below). In case the logical
form contains a negated sub-formula, the negated
sub-formula is also a mini-clause. The hypothesis
above after dividing clauses for the average com-
biner looks like this:
man(x0) ? agent(x0, e1)? result(x0, e1, x2) | w
drive(e1) ? agent(x0, e1)? result(x0, e1, x2) | w
drive(e1) ? patient(e1, x2)? result(x0, e1, x2) | w
car(x2) ? patient(e1, x2)? result(x0, e1, x2) | w
where result is again the query predicate. Here,
result has all of the variables in the clause as argu-
ments in order to maintain the binding of variables
across all of the mini-clauses. The weights w are the
following function of n, the number of mini-clauses
(4 in the above example):
w = 1n ? (log(
p
1? p)? prior) (6)
where p is a value close to 1 that is set to maximize
performance on the training data, and prior is the
same negative weight as before. Setting w this way
produces a probability of p for the result() in cases
that satisfy the antecedents of all mini-clauses. For
the example above, the antecedents of the first two
mini-clauses are satisfied, while the antecedents of
the last two are not since the premise provides no
evidence for an object of the verb drive. The simi-
larity is then computed to be the maximum probabil-
ity of any grounding of the result predicate, which
in this case is around p2 .
3
An interesting variation of the average combiner
is to omit variable bindings between the mini-
clauses. In this case, the hypothesis clauses look
like this for our example:
man(x) ? agent(x, e)? result() | w
drive(e) ? agent(x, e)? result() | w
drive(e) ? patient(e, x)? result() | w
car(x) ? patient(e, x)? result() | w
This implementation loses a lot of information,
for example it does not differentiate between ?A
man is walking and a woman is driving? and ?A
man is driving and a woman is walking?. In fact,
logical form without variable binding degrades to a
representation similar to a set of independent syn-
tactic dependencies, 4 while the average combiner
with variable binding retains all of the information
in the original logical form. Still, omitting variable
binding turns out to be useful for the STS task.
It is also worth commenting on the efficiency of
the inference algorithm when run on the three dif-
ferent approaches to combining evidence. The aver-
age combiner without variable binding is the fastest
and has the least memory requirements because all
cliques in the ground network are of limited size
(just 3 or 4 nodes). Deterministic AND is much
slower than the average combiner without variable
binding, because the maximum clique size depends
on the sentence. The average combiner with vari-
able binding is the most memory intensive since the
3One could also give mini-clauses different weights depend-
ing on their importance, but we have not experimented with this
so far.
4However, it is not completely the same since we do not
divide up formulas under negation into mini-clauses.
15
number of arguments of the result() predicate can
become large (there is an argument for each individ-
ual and event in the sentence). Consequently, the
inference algorithm needs to consider a combinato-
rial number of possible groundings of the result()
predicate, making inference very slow.
Adaptation of the logical form. As discussed by
Garrette et al (2011), Boxer?s output is mapped to
logical form and augmented with additional infor-
mation to handle a variety of semantic phenomena.
However, we do not use their additional rules for
handling implicatives and factives, as we wanted to
test the system without background knowledge be-
yond that supplied by the vector space.
Unfortunately, current MLN inference algorithms
are not able to efficiently handle complex formu-
las with nested quantifiers. For that reason, we re-
placed universal quantifiers in Boxer?s output with
existentials since they caused serious problems for
Alchemy. Although this is a radical change to the
semantics of the logical form, due to the nature of
the STS and RTE data, it only effects about 5% of
the sentences, and we found that most of the uni-
versal quantifiers in these cases were actually due
to parsing errors. We are currently exploring more
effective ways of dealing with this issue.
4 Task 1: Recognizing Textual Entailment
4.1 Dataset
In order to compare directly to the logic-based sys-
tem of Bos and Markert (2005), we focus on the
RTE-1 dataset (Dagan et al, 2005), which includes
567 Text-Hypothesis (T-H) pairs in the development
set and 800 pairs in the test set. The data covers a
wide range of issues in entailment, including lexical,
syntactic, logical, world knowledge, and combina-
tions of these at different levels of difficulty. In both
development and test sets, 50% of sentence pairs are
true entailments and 50% are not.
4.2 Method
We run our system for different configurations of in-
ference rules and evidence combiners. For distri-
butional inference rules (DIR), three different lev-
els are tested: without inference rules (no DIR),
inference rules for individual words (word DIR),
and inference rules for words and phrases (phrase
DIR). Phrase vectors were built using vector addi-
tion, as point-wise multiplication performed slightly
worse. To combine evidence for the result() query,
three different options are available: without av-
erage combiner which is just using Deterministic
AND (Deterministic AND), average combiner with
variable binding (AvgComb) and average combiner
without variable binding (AvgComb w/o VarBind).
Different combinations of configurations are tested
according to its suitability for the task; RTE and
STS.
We also tested several ?distributional only? sys-
tems. The first such system builds a vector represen-
tation for each sentence by adding its word vectors,
then computes the cosine similarity between the sen-
tence vectors for S1 and S2 (VS-Add). The second
uses point-wise multiplication instead of vector ad-
dition (VS-Mul). The third scales pairwise words
similarities to the sentence level using weighted av-
erage where weights are inverse document frequen-
cies idf as suggested by Mihalcea et al (2006) (VS-
Pairwise).
For the RTE task, systems were evaluated using
both accuracy and confidence-weighted score (cws)
as used by Bos and Markert (2005) and the RTE-
1 challenge (Dagan et al, 2005). In order to map
a probability of entailment to a strict prediction of
True or False, we determined a threshold that op-
timizes performance on the development set. The
cws score rewards a system?s ability to assign higher
confidence scores to correct predictions than incor-
rect ones. For cws, a system?s predictions are sorted
in decreasing order of confidence and the score is
computed as:
cws = 1n
n?
i=1
#correct-up-to-rank-i
i
where n is the number of the items in the test set,
and i ranges over the sorted items. In our systems,
we defined the confidence value for a T-H pair as
the distance between the computed probability for
the result() predicate and the threshold.
4.3 Results
The results are shown in Table 1. They show
that the distributional only baselines perform very
poorly. In particular, they perform worse than strict
16
Method acc cws
Chance 0.50 0.50
Bos & Markert, strict 0.52 0.55
Best system in RTE-1 challenge
(Bayer et al, 2005)
0.59 0.62
VS-Add 0.49 0.53
VS-Mul 0.51 0.52
VS-Pairwise 0.50 0.50
AvgComb w/o VarBind + phrase
DIR
0.52 0.53
Deterministic AND + phrase DIR 0.57 0.57
Table 1: Results on the RTE-1 Test Set.
entailment from Bos and Markert (2005), a system
that uses only logic. This illustrates the important
role of logic-based representations for the entail-
ment task. Due to intractable memory demands of
Alchemy inference, our current system with deter-
ministic AND fails to execute on 118 of the 800 test
pairs, so, by default, the system classifies these cases
as False (non-entailing) with very low confidence.
Comparing the two configurations of our system,
using deterministic AND vs. the average combiner
without variable binding (last two lines in Table 1),
we see that for RTE, it is essential to retain the full
logical form.
Our system with deterministic AND obtains both
an accuracy and cws of 0.57. The best result in
the RTE-1 challenge by Bayer et al (2005) ob-
tained an accuracy of 0.59 and cws of 0.62. 5 In
terms of both accuracy and cws, our system outper-
forms both ?distributional only? systems and strict
logical entailment, showing again that integrating
both logical form and distributional inference rules
using MLNs is beneficial. Interestingly, the strict
entailment system of Bos and Markert incorporated
generic knowledge, lexical knowledge (from Word-
Net) and geographical knowledge that we do not
utilize. This demonstrates the advantage of us-
ing a model that operationalizes entailment between
words and phrases as distributional similarity.
5On other RTE datasets there are higher previous results.
Hickl (2008) achieves 0.89 accuracy and 0.88 cws on the com-
bined RTE-2 and RTE-3 dataset.
5 Task 2: Semantic Textual Similarity
5.1 Dataset
The dataset we use in our experiments is the MSR
Video Paraphrase Corpus (MSR-Vid) subset of the
STS 2012 task, consisting of 1,500 sentence pairs.
The corpus itself was built by asking annotators
from Amazon Mechanical Turk to describe very
short video fragments (Chen and Dolan, 2011). The
organizers of the STS 2012 task (Agirre et al, 2012)
sampled video descriptions and asked Turkers to as-
sign similarity scores (ranging from 0 to 5) to pairs
of sentences, without access to the video. The gold
standard score is the average of the Turkers? annota-
tions. In addition to the MSR Video Paraphrase Cor-
pus subset, the STS 2012 task involved data from
machine translation and sense descriptions. We do
not use these because they do not consist of full
grammatical sentences, which the parser does not
handle well. In addition, the STS 2012 data included
sentences from the MSR Paraphrase Corpus, which
we also do not currently use because some sentences
are long and create intractable MLN inference prob-
lems. This issue is discussed further in section 6.
Following STS standards, our evaluation compares
a system?s similarity judgments to the gold standard
scores using Pearson?s correlation coefficient r.
5.2 Method
Our system can be tested for different configuration
of inference rules and evidence combiners which
are explained in section 4.2. The tested systems on
the STS task are listed in table 2. Out experiments
showed that using average combiner (AvgComb) is
very memory intensive and MLN inference for 28 of
the 1,500 pairs either ran out of memory or did not
finish in reasonable time. In such cases, we back off
to AvgComb w/o VarBind.
We compare to several baselines; our MLN
system without distributional inference rules
(AvgComb + no DIR), and distributional-only
systems (VS-Add, VS-Mul, VS-Pairwise). These
are the natural baselines for our system, since they
use only one of the two types of information that
we combine (i.e. logical form and distributional
representations).
Finally, we built an ensemble that combines the
output of multiple systems using regression trained
17
Method r
AvgComb + no DIR 0.58
AvgComb + word DIR 0.60
AvgComb + phrase DIR 0.66
AvgComb w/o VarBind + no DIR 0.58
AvgComb w/o VarBind + word DIR 0.60
AvgComb w/o VarBind + phrase DIR 0.73
VS-Add 0.78
VS-Mul 0.58
VS-Pairwise 0.77
Ensemble (VS-Add + VS-Mul + VS-
Pairwise)
0.83
Ensemble ([AvgComb + phrase DIR] +
VS-Add + VS-Mul + VS-Pairwise)
0.85
Best MSR-Vid score in STS 2012 (Ba?r
et al, 2012)
0.87
Table 2: Results on the STS video dataset.
on the training data. We then compare the perfor-
mance of an ensemble with and without our sys-
tem. This is the same technique used by Ba?r et al
(2012) except we used additive regression (Fried-
man, 2002) instead of linear regression since it gave
better results.
5.3 Results
Table 2 summarizes the results of our experiments.
They show that adding distributional information
improves results, as expected, and also that adding
phrase rules gives further improvement: Using only
word distributional inference rules improves results
from 0.58 to 0.6, and adding phrase inference rules
further improves them to 0.66. As for variable bind-
ing, note that although it provides more precise in-
formation, the STS scores actually improve when it
is dropped, from 0.66 to 0.73. We offer two expla-
nations for this result: First, this information is very
sensitive to parsing errors, and the C&C parser, on
which Boxer is based, produces many errors on this
dataset, even for simple sentences. When the C&C
CCG parse is wrong, the resulting logical form is
wrong, and the resulting similarity score is greatly
affected. Dropping variable binding makes the sys-
tems more robust to parsing errors. Second, in con-
trast to RTE, the STS dataset does not really test the
important role of syntax and logical form in deter-
mining meaning. This also explains why the ?dis-
tributional only? baselines are actually doing better
than the MLN systems.
Although the MLN system on its own does not
perform better than the distributional compositional
models, it does provide complementary information,
as shown by the fact that ensembling it with the rest
of the models improves performance (0.85 with the
MLN system, compared to 0.83 without it). The per-
formance of this ensemble is close to the current best
result for this dataset (0.87).
6 Future Work
The approach presented in this paper constitutes a
step towards achieving the challenging goal of effec-
tively combining logical representations with dis-
tributional information automatically acquired from
text. In this section, we discuss some of limita-
tions of the current work and directions for future
research.
As noted before, parse errors are currently a sig-
nificant problem. We use Boxer to obtain a logi-
cal representation for a sentence, which in turn re-
lies on the C&C parser. Unfortunately, C&C mis-
parses many sentences, which leads to inaccurate
logical forms. To reduce the impact of misparsing,
we plan to use a version of C&C that can produce
the top-n parses together with parse re-ranking (Ng
and Curran, 2012). As an alternative to re-ranking,
one could obtain logical forms for each of the top-
n parses, and create an MLN that integrates all of
them (together with their certainty) as an underspec-
ified meaning representation that could then be used
to directly support inferences such as STS and RTE.
We also plan to exploit a greater variety of dis-
tributional inference rules. First, we intend to in-
corporate logical form translations of existing dis-
tributional inference rule collections (e.g., (Berant
et al, 2011; Chan et al, 2011)). Another issue
is obtaining improved rule weights based on dis-
tributional phrase vectors. We plan to experiment
with more sophisticated approaches to computing
phrase vectors such as those recently presented by
Baroni and Zamparelli (2010) and Grefenstette and
Sadrzadeh (2011). Furthermore, we are currently
deriving symmetric similarity ratings between word
pairs or phrase pairs, when really what we need is di-
18
rectional similarity. We plan to incorporate directed
similarity measures such as those of Kotlerman et al
(2010) and Clarke (2012).
A primary problem for our approach is the limita-
tions of existing MLN inference algorithms, which
do not effectively scale to large and complex MLNs.
We plan to explore ?coarser? logical representa-
tions such as Minimal Recursion Semantics (MRS)
(Copestake et al, 2005). Another potential approach
to this problem is to trade expressivity for efficiency.
Domingos and Webb (2012) introduced a tractable
subset of Markov Logic (TML) for which a future
software release is planned. Formulating the infer-
ence problem in TML could potentially allow us to
run our system on longer and more complex sen-
tences.
7 Conclusion
In this paper we have used an approach that com-
bines logic-based and distributional representations
for natural language meaning. It uses logic as
the primary representation, transforms distributional
similarity judgments to weighted inference rules,
and uses Markov Logic Networks to perform in-
ferences over the weighted clauses. This approach
views textual entailment and sentence similarity as
degrees of ?logical? entailment, while at the same
time using distributional similarity as an indicator
of entailment at the word and short phrase level. We
have evaluated the framework on two different tasks,
RTE and STS, finding that it is able to handle both
tasks given that we adapt the way evidence is com-
bined in the MLN. Even though other entailment
models could be applied to STS, given that similar-
ity can obviously be operationalized as a degree of
mutual entailment, this has not been done before to
our best knowledge. Our framework achieves rea-
sonable results on both tasks. On RTE-1 we obtain
an accuracy of 0.57. On STS, we obtain a correla-
tion of r = 0.66 with full logic, r = 0.73 in a system
with weakened variable binding, and r = 0.85 in an
ensemble model. We find that distributional word
and phrase similarity, used as textual inference rules
on the fly, leads to sizeable improvements on both
tasks. We also find that using more flexible proba-
bilistic combinations of evidence is crucial for STS.
Acknowledgements
This research was supported in part by the NSF CA-
REER grant IIS 0845925, by the DARPA DEFT
program under AFRL grant FA8750-13-2-0026, by
MURI ARO grant W911NF-08-1-0242 and by an
NDSEG grant. Any opinions, findings, and conclu-
sions or recommendations expressed in this material
are those of the author and do not necessarily reflect
the view of DARPA, AFRL, ARO, DoD or the US
government.
Some of our experiments were run on the
Mastodon Cluster supported by NSF Grant EIA-
0303609.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In Proceedings of
SemEval.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In SemEval-2012.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193, Cambridge, MA, October. Association for
Computational Linguistics.
Samuel Bayer, John Burger, Lisa Ferro, John Hender-
son, and Alexander Yeh. 2005. MITREs Submissions
to the EU Pascal RTE Challenge. In In Proceedings
of the PASCAL Challenges Workshop on Recognising
Textual Entailment, pages 41?44.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of ACL, Portland, OR.
Johan Bos and Katja Markert. 2005. Recognising tex-
tual entailment with logical inference. In Proceedings
of EMNLP 2005, pages 628?635, Vancouver, B.C.,
Canada.
Johan Bos. 2008. Wide-coverage semantic analysis with
boxer. In Johan Bos and Rodolfo Delmonte, editors,
Semantics in Text Processing. STEP 2008 Conference
Proceedings, Research in Computational Semantics,
pages 277?286. College Publications.
Tsz Ping Chan, Chris Callison-Burch, and Benjamin
Van Durme. 2011. Reranking bilingually extracted
19
paraphrases using monolingual distributional similar-
ity. In Proceedings of the GEMS 2011 Workshop on
GEometrical Models of Natural Language Semantics,
pages 33?42, Edinburgh, UK.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 190?200,
Portland, Oregon, USA, June.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proceed-
ings of ACL 2004, pages 104?111, Barcelona, Spain.
Daoud Clarke. 2012. A context-theoretic framework for
compositionality in distributional semantics. Compu-
tational Linguistics, 38(1).
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A
Sag. 2005. Minimal recursion semantics: An intro-
duction. Research on Language and Computation,
3(2-3):281?332.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL Recognising Textual Entailment
Challenge. In In Proceedings of the PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
pages 1?8.
Pedro Domingos and Daniel Lowd. 2009. Markov
Logic: An Interface Layer for Artificial Intelligence.
Synthesis Lectures on Artificial Intelligence and Ma-
chine Learning. Morgan & Claypool Publishers.
Pedro Domingos and W Austin Webb. 2012. A tractable
first-order probabilistic logic. In Proceedings of the
Twenty-Sixth National Conference on Artificial Intel-
ligence.
Katrin Erk and Sebastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of EMNLP 2008, pages 897?906, Honolulu,
HI.
Jerome H Friedman. 2002. Stochastic gradient boosting.
Computational Statistics & Data Analysis, 38(4):367?
378.
Dan Garrette, Katrin Erk, and Raymond Mooney. 2011.
Integrating logical representations with probabilistic
information using markov logic. In Proceedings of
IWCS, Oxford, UK.
Dan Garrette, Katrin Erk, and Raymond Mooney. 2013.
A formal approach to linking logical form and vector-
space lexical semantics. In Harry Bunt, Johan Bos,
and Stephen Pulman, editors, Computing Meaning,
Vol. 4.
Lise Getoor and Ben Taskar, editors. 2007. Introduction
to Statistical Relational Learning. MIT Press, Cam-
bridge, MA.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword Third Edi-
tion. http://www.ldc.upenn.edu/
Catalog/CatalogEntry.jsp?catalogId=
LDC2007T07.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
EMNLP, Edinburgh, Scotland, UK.
Andrew Hickl. 2008. Using Discourse Commitments
to Recognize Textual Entailment. In Proceedings of
COLING 2008, pages 337?344.
Jerry R. Hobbs, Mark Stickel, Douglas Appelt, and Paul
Martin. 1993. Interpretation as abduction. Artificial
Intelligence, 63(1?2):69?142.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic; An Introduction to Modeltheoretic Semantics of
Natural Language, Formal Logic and DRT. Kluwer,
Dordrecht.
Stanley Kok, Parag Singla, Matthew Richardson, and Pe-
dro Domingos. 2005. The Alchemy system for sta-
tistical relational AI. Technical report, Department
of Computer Science and Engineering, University
of Washington. http://www.cs.washington.
edu/ai/alchemy.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language En-
gineering, 16(04):359?389.
Thomas Landauer and Susan Dumais. 1997. A solution
to Platos problem: the latent semantic analysis theory
of acquisition, induction, and representation of knowl-
edge. Psychological Review, 104(2):211?240.
Dekang Lin and Patrick Pantel. 2001a. DIRT - discovery
of inference rules from text. In In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 323?328.
Dekang Lin and Patrick Pantel. 2001b. Discovery of
inference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Will Lowe. 2001. Towards a theory of semantic space.
In Proceedings of the Cognitive Science Society, pages
576?581.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments,
and Computers, 28:203?208.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the na-
tional conference on artificial intelligence, volume 21,
page 775. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244.
20
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Richard Montague. 1970. Universal grammar. Theoria,
36:373?398. Reprinted in Thomason (1974), pp 7-27.
Sriraam Natarajan, Tushar Khot, Daniel Lowd, Prasad
Tadepalli, Kristian Kersting, and Jude Shavlik. 2010.
Exploiting causal independence in markov logic net-
works: Combining undirected and directed models.
In Proceedings of European Conference in Machine
Learning (ECML), Barcelona, Spain.
Dominick Ng and James R Curran. 2012. Dependency
hashing for n-best ccg parsing. In In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proceedings of AAAI.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62:107?
136.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1).
Richard Socher, Eric Huang, Jeffrey Pennin, Andrew Ng,
and Christopher Manning. 2011. Dynamic pooling
and unfolding recursive autoencoders for paraphrase
detection. In J. Shawe-Taylor, R.S. Zemel, P. Bartlett,
F.C.N. Pereira, and K.Q. Weinberger, editors, Pro-
ceedings of NIPS.
Asher Stern, Amnon Lotan, Shachar Mirkin, Eyal
Shnarch, Lili Kotlerman, Jonathan Berant, and Ido Da-
gan. 2011. Knowledge and tree-edits in learnable en-
tailment proofs. In TAC, Gathersburg, MD.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings
of ACL 2010, pages 948?957, Uppsala, Sweden.
Richmond H. Thomason, editor. 1974. Formal Philoso-
phy. Selected Papers of Richard Montague. Yale Uni-
versity Press, New Haven.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
21
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 796?801,
Dublin, Ireland, August 23-24, 2014.
UTexas: Natural Language Semantics using Distributional Semantics and
Probabilistic Logic
Islam Beltagy
?
, Stephen Roller
?
, Gemma Boleda
?
, Katrin Erk
?
, Raymond J. Mooney
?
?
Department of Computer Science
?
Department of Linguistics
The University of Texas at Austin
{beltagy, roller, mooney}@cs.utexas.edu
gemma.boleda@upf.edu, katrin.erk@mail.utexas.edu
Abstract
We represent natural language semantics
by combining logical and distributional in-
formation in probabilistic logic. We use
Markov Logic Networks (MLN) for the
RTE task, and Probabilistic Soft Logic
(PSL) for the STS task. The system is
evaluated on the SICK dataset. Our best
system achieves 73% accuracy on the RTE
task, and a Pearson?s correlation of 0.71 on
the STS task.
1 Introduction
Textual Entailment systems based on logical infer-
ence excel in correct reasoning, but are often brit-
tle due to their inability to handle soft logical in-
ferences. Systems based on distributional seman-
tics excel in lexical and soft reasoning, but are un-
able to handle phenomena like negation and quan-
tifiers. We present a system which takes the best
of both approaches by combining distributional se-
mantics with probabilistic logical inference.
Our system builds on our prior work (Belt-
agy et al., 2013; Beltagy et al., 2014a; Beltagy
and Mooney, 2014; Beltagy et al., 2014b). We
use Boxer (Bos, 2008), a wide-coverage semantic
analysis tool to map natural sentences to logical
form. Then, distributional information is encoded
in the form of inference rules. We generate lexical
and phrasal rules, and experiment with symmetric
and asymmetric similarity measures. Finally, we
use probabilistic logic frameworks to perform in-
ference, Markov Logic Networks (MLN) for RTE,
and Probabilistic Soft Logic (PSL) for STS.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
2 Background
2.1 Logical Semantics
Logic-based representations of meaning have a
long tradition (Montague, 1970; Kamp and Reyle,
1993). They handle many complex semantic phe-
nomena such as relational propositions, logical
operators, and quantifiers; however, they can not
handle ?graded? aspects of meaning in language
because they are binary by nature.
2.2 Distributional Semantics
Distributional models use statistics of word co-
occurrences to predict semantic similarity of
words and phrases (Turney and Pantel, 2010;
Mitchell and Lapata, 2010), based on the obser-
vation that semantically similar words occur in
similar contexts. Words are represented as vec-
tors in high dimensional spaces generated from
their contexts. Also, it is possible to compute vec-
tor representations for larger phrases composition-
ally from their parts (Mitchell and Lapata, 2008;
Mitchell and Lapata, 2010; Baroni and Zampar-
elli, 2010). Distributional similarity is usually a
mixture of semantic relations, but particular asym-
metric similarity measures can, to a certain ex-
tent, predict hypernymy and lexical entailment
distributionally (Kotlerman et al., 2010; Lenci and
Benotto, 2012; Roller et al., 2014). Distribu-
tional models capture the graded nature of mean-
ing, but do not adequately capture logical struc-
ture (Grefenstette, 2013).
2.3 Markov Logic Network
Markov Logic Networks (MLN) (Richardson and
Domingos, 2006) are a framework for probabilis-
tic logic that employ weighted formulas in first-
order logic to compactly encode complex undi-
rected probabilistic graphical models (i.e., Markov
networks). Weighting the rules is a way of soft-
ening them compared to hard logical constraints.
796
MLNs define a probability distribution over pos-
sible worlds, where the probability of a world in-
creases exponentially with the total weight of the
logical clauses that it satisfies. A variety of in-
ference methods for MLNs have been developed,
however, computational overhead is still an issue.
2.4 Probabilistic Soft Logic
Probabilistic Soft Logic (PSL) is another recently
proposed framework for probabilistic logic (Kim-
mig et al., 2012). It uses logical representations to
compactly define large graphical models with con-
tinuous variables, and includes methods for per-
forming efficient probabilistic inference for the re-
sulting models. A key distinguishing feature of
PSL is that ground atoms (i.e., atoms without vari-
ables) have soft, continuous truth values on the
interval [0, 1] rather than binary truth values as
used in MLNs and most other probabilistic logics.
Given a set of weighted inference rules, and with
the help of Lukasiewicz?s relaxation of the logical
operators, PSL builds a graphical model defining a
probability distribution over the continuous space
of values of the random variables in the model
(Kimmig et al., 2012). Then, PSL?s MPE infer-
ence (Most Probable Explanation) finds the overall
interpretation with the maximum probability given
a set of evidence. This optimization problem is a
second-order cone program (SOCP) (Kimmig et
al., 2012) and can be solved in polynomial time.
2.5 Recognizing Textual Entailment
Recognizing Textual Entailment (RTE) is the task
of determining whether one natural language text,
the premise, Entails, Contradicts, or is not related
(Neutral) to another, the hypothesis.
2.6 Semantic Textual Similarity
Semantic Textual Similarity (STS) is the task of
judging the similarity of a pair of sentences on
a scale from 1 to 5 (Agirre et al., 2012). Gold
standard scores are averaged over multiple human
annotations and systems are evaluated using the
Pearson correlation between a system?s output and
gold standard scores.
3 Approach
3.1 Logical Representation
The first component in the system is Boxer (Bos,
2008), which maps the input sentences into logical
form, in which the predicates are words in the sen-
tence. For example, the sentence ?A man is driving
a car? in logical form is:
?x, y, z. man(x) ? agent(y, x) ? drive(y) ?
patient(y, z) ? car(z)
3.2 Distributional Representation
Next, distributional information is encoded in
the form of weighted inference rules connecting
words and phrases of the input sentences T and H .
For example, for sentences T : ?A man is driving
a car?, and H: ?A guy is driving a vehicle?, we
would like to generate rules like ?x. man(x) ?
guy(x) |w
1
, ?x.car(x)? vehicle(x) |w
2
, where
w
1
and w
2
are weights indicating the similarity of
the antecedent and consequent of each rule.
Inferences rules are generated as in Beltagy et
al. (2013). Given two input sentences T and H ,
for all pairs (a, b), where a and b are words or
phrases of T and H respectively, generate an infer-
ence rule: a ? b | w, where the rule weight w is
a function of sim(
??
a ,
??
b ), and sim is a similarity
measure of the distributional vectors
??
a ,
??
b . We
experimented with the symmetric similarity mea-
sure cosine, and asym, the supervised, asymmet-
ric similarity measure of Roller et al. (2014).
The asym measure uses the vector difference
(
??
a ?
??
b ) as features in a logistic regression clas-
sifier for distinguishing between four different
word relations: hypernymy, cohyponymy, meron-
omy, and no relation. The model is trained us-
ing the noun-noun subset of the BLESS data set
(Baroni and Lenci, 2011). The final similarity
weight is given by the model?s estimated probabil-
ity that the word relationship is either hypernymy
or meronomy: asym(
??
a ,
??
b ) = P (hyper(a, b))+
P (mero(a, b)).
Distributional representations for words are de-
rived by counting co-occurrences in the ukWaC,
WaCkypedia, BNC and Gigaword corpora. We
use the 2000 most frequent content words as ba-
sis dimensions, and count co-occurrences within
a two word context window. The vector space is
weighted using Positive Pointwise Mutual Infor-
mation.
Phrases are defined in terms of Boxer?s output
to be more than one unary atom sharing the same
variable like ?a little kid? (little(k) ? kid(k)),
or two unary atoms connected by a relation like
?a man is driving? (man(m) ? agent(d,m) ?
drive(d)). We compute vector representations of
797
phrases using vector addition across the compo-
nent predicates. We also tried computing phrase
vectors using component-wise vector multiplica-
tion (Mitchell and Lapata, 2010), but found it per-
formed marginally worse than addition.
3.3 Probabilistic Logical Inference
The last component is probabilistic logical infer-
ence. Given the logical form of the input sen-
tences, and the weighted inference rules, we use
them to build a probabilistic logic program whose
solution is the answer to the target task. A proba-
bilistic logic program consists of the evidence set
E, the set of weighted first order logical expres-
sions (rule base RB), and a query Q. Inference is
the process of calculating Pr(Q|E,RB).
3.4 Task 1: RTE using MLNs
MLNs are the probabilistic logic framework we
use for the RTE task (we do not use PSL here as
it shares the problems of fuzzy logic with proba-
bilistic reasoning). The RTE classification prob-
lem for the relation between T and H can be
split into two inference tasks. The first is test-
ing if T entails H , Pr(H|T,RB). The second
is testing if the negation of the text ?T entails H ,
Pr(H|?T,RB). In case Pr(H|T,RB) is high,
while Pr(H|?T,RB) is low, this indicates En-
tails. In case it is the other way around, this in-
dicates Contradicts. If both values are close, this
means T does not affect the probability of H and
indicative of Neutral. We train an SVM classifier
with LibSVM?s default parameters to map the two
probabilities to the final decision.
The MLN implementation we use is
Alchemy (Kok et al., 2005). Queries in Alchemy
can only be ground atoms. However, in our
case the query is a complex formula (H). We
extended Alchemy to calculate probabilities of
queries (Beltagy and Mooney, 2014). Probability
of a formula Q given an MLN K equals the ratio
between the partition function Z of the ground
network of K with and without Q added as a hard
rule (Gogate and Domingos, 2011)
P (Q | K) =
Z(K ? {(Q,?)})
Z(K)
(1)
We estimate Z of the ground networks using Sam-
pleSearch (Gogate and Dechter, 2011), an ad-
vanced importance sampling algorithm that is suit-
able for ground networks generated by MLNs.
A general problem with MLN inference is
its computational overhead, especially for the
complex logical formulae generated by our ap-
proach. To make inference faster, we reduce the
size of the ground network through an automatic
type-checking technique proposed in Beltagy and
Mooney (2014). For example, consider the ev-
idence ground atom man(M) denoting that the
constant M is of type man. Then, consider an-
other predicate like car(x). In case there are no in-
ference rule connecting man(x) and car(x), then
we know that M which we know is a man cannot
be a car, so we remove the ground atom car(M)
from the ground network. This technique reduces
the size of the ground network dramatically and
makes inference tractable.
Another problem with MLN inference is that
quantifiers sometimes behave in an undesir-
able way, due to the Domain Closure Assump-
tion (Richardson and Domingos, 2006) that MLNs
make. For example, consider the text-hypothesis
pair: ?There is a black bird? and ?All birds are
black?, which in logic are T : bird(B)?black(B)
and H : ?x. bird(x) ? black(x). Because of
the Domain Closure Assumption, MLNs conclude
that T entails H because H is true for all constants
in the domain (in this example, the single constant
B). We solve this problem by introducing extra
constants and evidence in the domain. In the ex-
ample above, we introduce evidence of a new bird
bird(D), which prevents the hypothesis from be-
ing true. The full details of the technique of deal-
ing with the domain closure is beyond the scope of
this paper.
3.5 Task 2: STS using PSL
PSL is the probabilistic logic we use for the STS
task since it has been shown to be an effective
approach for computing similarity between struc-
tured objects. We showed in Beltagy et al. (2014a)
how to perform the STS task using PSL. PSL
does not work ?out of the box? for STS, be-
cause Lukasiewicz?s equation for the conjunction
is very restrictive. We address this by replacing
Lukasiewicz?s equation for conjunction with an
averaging equation, then change the optimization
problem and grounding technique accordingly.
For each STS pair of sentences S
1
, S
2
, we run
PSL twice, once where E = S
1
, Q = S
2
and an-
other where E = S
2
, Q = S
1
, and output the two
scores. The final similarity score is produced from
798
an Additive Regression model with WEKA?s de-
fault parameters trained to map the two PSL scores
to the overall similarity score (Friedman, 1999;
Hall et al., 2009).
3.6 Task 3: RTE and STS using Vector
Spaces and Keyword Counts
As a baseline, we also attempt both the RTE and
STS tasks using only vector representations and
unigram counts. This baseline model uses a super-
vised regressor with features based on vector sim-
ilarity and keyword counts. The same input fea-
tures are used for performing RTE and STS, but a
SVM classifier and Additive Regression model is
trained separately for each task. This baseline is
meant to establish whether the task truly requires
the sophisticated logical inference of MLNs and
PSL, or if merely checking for logical keywords
and textual similarity is sufficient.
The first two features are simply the cosine and
asym similarities between the text and hypothesis,
using vector addition of the unigrams to compute
a single vector for the entire sentence.
We also compute vectors for both the text and
hypothesis using vector addition of the mutually
exclusive unigrams (MEUs). The MEUs are de-
fined as the unigrams of the premise and hypoth-
esis with common unigrams removed. For exam-
ple, if the premise is ?A dog chased a cat? and the
hypothesis is ?A dog watched a mouse?, the MEUs
are ?chased cat? and ?watched mouse.? We com-
pute vector addition of the MEUs, and compute
similarity using both the cosine and asym mea-
sures. These form two features for the regressor.
The last feature of the model is a keyword
count. We count how many times 13 different
keywords appear in either the text or the hypoth-
esis. These keywords include negation (no, not,
nobody, etc.) and quantifiers (a, the, some, etc.)
The counts of each keyword form the last 13 fea-
tures as input to the regressor. In total, there are
17 features used in this baseline system.
4 Evaluation
The dataset used for evaluation is SICK:
Sentences Involving Compositional Knowledge
dataset, a task for SemEval 2014 (Marelli et al.,
2014a; Marelli et al., 2014b). The dataset is
10,000 pairs of sentences, 5000 training and 5000
for testing. Sentences are annotated for both tasks.
SICK-RTE SICK-STS
Baseline 70.0 71.1
MLN/PSL + Cosine 72.8 68.6
MLN/PSL + Asym 73.2 68.9
Ensemble 73.2 71.5
Table 1: Test RTE accuracy and STS Correlation.
4.1 Systems Compared
We compare multiple configurations of our proba-
bilistic logic system.
? Baseline: Vector- and keyword-only baseline
described in Section 3.6;
? MLN/PSL + Cosine: MLN and PSL based
methods described in Sections 3.4 and 3.5,
using cosine as a similarity measure;
? MLN/PSL + Asym: MLN and PSL based
methods described in Sections 3.4 and 3.5,
using asym as a similarity measure;
? Ensemble: An ensemble method which uses
all of the features in the above methods as in-
puts for the RTE and STS classifiers.
4.2 Results and Discussion
Table 1 shows our results on the held-out test set
for SemEval 2014 Task 1.
On the RTE task, we see that both the MLN +
Cosine and MLN + Asym models outperformed
the Baseline, indicating that textual entailment re-
quires real inference to handle negation and quan-
tifiers. The MLN + Asym and Ensemble sys-
tems perform identically on RTE, further suggest-
ing that the logical inference subsumes keyword
detection.
The MLN + Asym system outperforms the
MLN + Cosine system, emphasizing the impor-
tance of asymmetric measures for predicting lex-
ical entailment. Intuitively, this makes perfect
sense: dog entails animal, but not vice versa.
In an error analysis performed on a development
set, we found our RTE system was extremely con-
servative: we rarely confused the Entails and Con-
tradicts classes, indicating we correctly predict the
direction of entailment, but frequently misclassify
examples as Neutral. An examination of these ex-
amples showed the errors were mostly due to miss-
ing or weakly-weighted distributional rules.
On STS, our vector space baseline outperforms
both PSL-based systems, but the ensemble outper-
forms any of its components. This is a testament to
799
the power of distributional models in their ability
to predict word and sentence similarity. Surpris-
ingly, we see that the PSL + Asym system slightly
outperforms the PSL + Cosine system. This may
indicate that even in STS, some notion of asymme-
try plays a role, or that annotators may have been
biased by simultaneously annotating both tasks.
As with RTE, the major bottleneck of our system
appears to be the knowledge base, which is built
solely using distributional inference rules.
Results also show that our system?s perfor-
mance is close to the baseline system. One of
the reasons behind that could be that sentences are
not exploiting the full power of logical represen-
tations. On RTE for example, most of the con-
tradicting pairs are two similar sentences with one
of them being negated. This way, the existence
of any negation cue in one of the two sentences is
a strong signal for contradiction, which what the
baseline system does without deeply representing
the semantics of the negation.
5 Conclusion & Future Work
We showed how to combine logical and distribu-
tional semantics using probabilistic logic, and how
to perform the RTE and STS tasks using it. The
system is tested on the SICK dataset.
The distributional side can be extended in many
directions. We would like to use longer phrases,
more sophisticated compositionality techniques,
and contextualized vectors of word meaning. We
also believe inference rules could be dramatically
improved by integrating from paraphrases collec-
tions like PPDB (Ganitkevitch et al., 2013).
Finally, MLN inference could be made more ef-
ficient by exploiting the similarities between the
two ground networks (the one with Q and the one
without). PLS inference could be enhanced by us-
ing a learned, weighted average of rules, rather
than the simple mean.
Acknowledgements
This research was supported by the DARPA DEFT
program under AFRL grant FA8750-13-2-0026.
Some experiments were run on the Mastodon
Cluster supported by NSF Grant EIA-0303609.
The authors acknowledge the Texas Advanced
Computing Center (TACC)
1
for providing grid re-
sources that have contributed to these results. We
thank the anonymous reviewers and the UTexas
1
http://www.tacc.utexas.edu
Natural Language and Learning group for their
helpful comments and suggestions.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of Semantic Evaluation (SemEval-12).
Marco Baroni and Alessandro Lenci. 2011. How
we BLESSed distributional semantic evaluation. In
Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
pages 1?10, Edinburgh, UK, July. Association for
Computational Linguistics.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of Conference on Empirical Methods in
Natural Language Processing (EMNLP-10).
Islam Beltagy and Raymond J. Mooney. 2014. Ef-
ficient Markov logic inference for natural language
semantics. In Proceedings of AAAI 2014 Workshop
on Statistical Relational AI (StarAI-14).
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
Montague meets Markov: Deep semantics with
probabilistic logical form. In Proceedings of the
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM-13).
Islam Beltagy, Katrin Erk, and Raymond Mooney.
2014a. Probabilistic soft logic for semantic textual
similarity. In Proceedings of Association for Com-
putational Linguistics (ACL-14).
Islam Beltagy, Katrin Erk, and Raymond Mooney.
2014b. Semantic parsing using distributional se-
mantics and probabilistic logic. In Proceedings
of ACL 2014 Workshop on Semantic Parsing (SP-
2014).
Johan Bos. 2008. Wide-coverage semantic analysis
with Boxer. In Proceedings of Semantics in Text
Processing (STEP-08).
J.H. Friedman. 1999. Stochastic gradient boosting.
Technical report, Stanford University.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT-13).
Vibhav Gogate and Rina Dechter. 2011. Sample-
search: Importance sampling in presence of deter-
minism. Artificial Intelligence, 175(2):694?729.
Vibhav Gogate and Pedro Domingos. 2011. Proba-
bilistic theorem proving. In 27th Conference on Un-
certainty in Artificial Intelligence (UAI-11).
800
Edward Grefenstette. 2013. Towards a formal distri-
butional semantics: Simulating logical calculi with
tensors. In Proceedings of Second Joint Conference
on Lexical and Computational Semantics (*SEM
2013).
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer.
Angelika Kimmig, Stephen H. Bach, Matthias
Broecheler, Bert Huang, and Lise Getoor. 2012.
A short introduction to Probabilistic Soft Logic.
In Proceedings of NIPS Workshop on Probabilistic
Programming: Foundations and Applications (NIPS
Workshop-12).
Stanley Kok, Parag Singla, Matthew Richardson, and
Pedro Domingos. 2005. The Alchemy system
for statistical relational AI. Technical report, De-
partment of Computer Science and Engineering,
University of Washington. http://www.cs.
washington.edu/ai/alchemy.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of the first Joint Conference on Lexical
and Computational Semantics (*SEM-12).
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014a. SemEval-2014 task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014b. A sick cure for the evaluation
of compositional distributional semantic models.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Thierry Declerck, Hrafn Loftsson, Bente
Maegaard, Joseph Mariani, Asuncion Moreno, Jan
Odijk, and Stelios Piperidis, editors, Proceedings of
the Ninth International Conference on Language Re-
sources and Evaluation (LREC?14), Reykjavik, Ice-
land, may. European Language Resources Associa-
tion (ELRA).
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of Association for Computational Linguistics (ACL-
08).
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(3):1388?1429.
Richard Montague. 1970. Universal grammar. Theo-
ria, 36:373?398.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning,
62:107?136.
Stephen Roller, Katrin Erk, and Gemma Boleda. 2014.
Inclusive yet selective: Supervised distributional hy-
pernymy detection. In Proceedings of the Twenty
Fifth International Conference on Computational
Linguistics (COLING-14), Dublin, Ireland.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141?188.
801
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 17?26,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
What Is Word Meaning, Really?
(And How Can Distributional Models Help Us Describe It?)
Katrin Erk
Department of Linguistics
University of Texas at Austin
katrin.erk@mail.utexas.edu
Abstract
In this paper, we argue in favor of re-
considering models for word meaning, us-
ing as a basis results from cognitive sci-
ence on human concept representation.
More specifically, we argue for a more
flexible representation of word meaning
than the assignment of a single best-fitting
dictionary sense to each occurrence: Ei-
ther use dictionary senses, but view them
as having fuzzy boundaries, and assume
that an occurrence can activate multiple
senses to different degrees. Or move away
from dictionary senses completely, and
only model similarities between individ-
ual word usages. We argue that distri-
butional models provide a flexible frame-
work for experimenting with alternative
models of word meanings, and discuss ex-
ample models.
1 Introduction
Word sense disambiguation (WSD) is one of
the oldest problems in computational linguis-
tics (Weaver, 1949) and still remains challeng-
ing today. State-of-the-art performance on WSD
for WordNet senses is at only around 70-80%
accuracy (Edmonds and Cotton, 2001; Mihalcea
et al, 2004). The use of coarse-grained sense
groups (Palmer et al, 2007) has led to consider-
able advances in WSD performance, with accura-
cies of around 90% (Pradhan et al, 2007). But
this figure averages over lemmas, and the problem
remains that while WSDworks well for some lem-
mas, others continue to be tough.
In WSD, polysemy is typically modeled
through a list of dictionary senses thought to be
mutually disjoint, such that each occurrence of
a word is characterized through one best-fitting
dictionary sense. Accordingly, WSD is typically
framed as a classification task. Interestingly, the
task of assigning a single best word sense is very
hard for human annotators, not just machines (Kil-
garriff and Rosenzweig, 2000).
In this paper we advocate the exploration of
alternative computational models of word mean-
ing. After all, one possible reason for the con-
tinuing difficulty of (manual as well as automatic)
word sense assignment is that the prevailing model
might be suboptimal. We explore three main hy-
potheses. The first builds on research on the hu-
man concept representation that has shown that
concepts in the human mind do not work like
sets with clear-cut boundaries; they show graded
membership, and there are typical members as
well as borderline cases (Rosch, 1975; Hamp-
ton, 2007). Accordingly, (A) we will suggest
that word meaning may be better modeled us-
ing a graded notion of sense membership than
through concepts with hard boundaries. Second,
even if senses have soft boundaries, the question
remains of whether they are disjoint. (B) We
will argue in favor of a framework where multi-
ple senses may apply to a single occurrence, to
different degrees. This can be viewed as a dy-
namical grouping of senses for each occurrence,
in contrast to static sense groups as in Palmer et
al. (2007). The first two hypotheses still rely on
an existing sense list. However, there is no univer-
sal agreement across dictionaries and across tasks
on the number of senses that words have (Hanks,
2000). Kilgarriff (1997) even argues that general,
task-independent word senses do not exist. (C) By
focusing on individual occurrences (usages) of
a lemma and their degree of similarity, we can
model word meaning without recourse to dic-
tionary senses.
In this paper, we are going to argue in favor of
the use of vector space as a basis for alternative
models of word meaning. Vector space models
have been used widely to model word sense (Lund
17
and Burgess, 1996; Deerwester et al, 1990; Lan-
dauer and Dumais, 1997; Sahlgren and Karlgren,
2005; Pado? and Lapata, 2007), their central prop-
erty being that proximity in space can be used to
predict semantic similarity. By viewing word oc-
currences as points in vector space, we can model
word meaning without recourse to senses. An ad-
ditional advantage of vector space models is that
they are also widely used in human concept rep-
resentation models, yielding many modeling ideas
that can be exploited for computational models.
In Section 2 we review the evidence that word
sense is a tough phenomenon to model, and we lay
out findings that support hypotheses (A)-(C). Sec-
tion 4 considers distributional models that repre-
sent word meaning without recourse to dictionary
senses, following (C). In Section 5 we discuss pos-
sibilities for embedding dictionary senses in vec-
tor space in a way that respects points (A) and (B).
2 Computational and cognitive models of
word meaning
In this section, we review the problems of (manual
and automatic) sense assignment, and we discuss
discusses cognitive models of concept representa-
tion and polysemy, following the three hypotheses
laid out in the introduction.
Word sense assignment. In computational lin-
guistics, the problem of polysemy is typically
phrased as one of choosing one best-fitting sense
for the given occurrence out of a dictionary-
defined sense list. However, this is a hard task
both for humans and for machines. With Word-
Net (Fellbaum, 1998), the electronic lexicon re-
source that is currently most widely used in com-
putational linguistics, inter-annotator agreement
(ITA) lies in the range of 67% to 78% (Landes
et al, 1998; Snyder and Palmer, 2004; Mihal-
cea et al, 2004), and state-of-the-art WSD sys-
tems achieve accuracy scores of 73% to 77% (Ed-
monds and Cotton, 2001; Mihalcea et al, 2004).
This problem is not specific to WordNet: Anal-
yses with the HECTOR dictionary led to simi-
lar numbers (Kilgarriff and Rosenzweig, 2000).
Sense granularity has been suggested as a reason
for the difficulty of the task (Palmer et al, 2007).
And in fact, the use of more coarse-grained senses
leads to greatly ITA as well as WSD accuracy,
with about a 10% improvement for either mea-
sure (Palmer et al, 2007; Pradhan et al, 2007). In
OntoNotes (Hovy et al, 2006), an ITA of 90% is
used as the criterion for the construction of coarse-
grained sense distinctions. However, intriguingly,
for some high-frequency lemmas such as leave
this ITA threshold is not reached even after mul-
tiple re-partitionings of the semantic space (Chen
and Palmer, 2009) ? indicating that the meaning of
these words may not be separable into senses dis-
tinct enough for consistent annotation. A recent
analysis of factors influencing ITA differences be-
tween lemmas (Passonneau et al, 2010) found
three main factors: sense concreteness, specificity
of the context in which a target word occurs, and
similarity between senses. It is interesting to note
that only one of those factors, the third, can be ad-
dressed through a change of dictionary.
More radical solutions than sense grouping that
have been proposed are to restrict the task to deter-
mining predominant sense in a given domain (Mc-
Carthy et al, 2004), or to work directly with para-
phrases (McCarthy and Navigli, 2009).
(A) Graded sense membership. Research on
the human concept representation (Murphy, 2002;
Hampton, 2007) shows that categories in the
human mind are not simply sets with clear-cut
boundaries. Some items are perceived as more
typical than others (Rosch, 1975; Rosch and
Mervis, 1975). Also, some items are clear mem-
bers, others are rated as borderline (Hampton,
1979). On borderline items, people are more likely
to change their mind about category member-
ship (McCloskey and Glucksberg, 1978). How-
ever, these results concern mental concepts, which
raises the question of the relation between mental
concepts and word senses. This relation is dis-
cussed in most depth by Murphy (1991; 2002),
who argues that while not every human concept
is associated with a word, word meanings show
many of the same phenomena as concepts in gen-
eral; word meaning is ?made up of pieces of con-
ceptual structure?. In cognitive linguistics there
has been much work on word meaning based on
models with graded membership and typically ef-
fects (Coleman and Kay, 1981; Lakoff, 1987;
Cruse, 1986; Taylor, 1989).
(B) Multiple senses per occurrence. While
most manual word sense annotation efforts al-
low annotators to assign more than one dictionary
sense to an occurrence, this is typically phrased
as an exception rather than the default. In the re-
cent WSsim annotation study (Erk et al, 2009),
18
Senses
Sentence 1 2 3 4 5 6 7 Annotator
This question provoked arguments in America about the
Norton Anthology of Literature by Women, some of the
contents of which were said to have had little value as
literature.
1 4 4 2 1 1 3 Ann. 1
4 5 4 2 1 1 4 Ann. 2
1 4 5 1 1 1 1 Ann. 3
Table 1: From (Erk et al, 2009): A sample annotation from the WSsim dataset. The senses are: 1:state-
ment, 2:controversy, 3:debate, 4:literary argument, 5:parameter, 6:variable, 7:line of reasoning
we asked three human annotators to judge the ap-
plicability of WordNet senses on a graded scale of
1 (completely different) to 5 (identical) and giv-
ing a rating for each sense rather than picking one.
Table 1 shows an example sentence with annota-
tor ratings for the senses of the target argument.
For this sentence, the annotators agree that senses
2 and 3 are highly applicable, but there also indi-
vidual differences in the perceived meaning: Only
annotator 2 views sense 1 as applying to a high
degree. In an annotation setting with graded judg-
ments, it does not make sense to measure exact
agreement on judgments. We instead evaluated
ITA using Spearman?s rho, a nonparametric corre-
lation test, finding highly significant correlations
(p  0.001) between each pair of annotators, as
well as highly significant correlations with the re-
sults of a previous, traditional word sense annota-
tion of the same dataset. The annotators made use
of the complete scale (1-5), often opting for inter-
mediate values of sense applicability. In addition,
we tested whether there were groups of senses
that always got the same ratings on any given sen-
tence (which would mean that the annotators im-
plicitly used more coarse-grained senses). What
we found instead is that the annotators seemed to
have mixed and matched senses for the individual
occurrences in a dynamic fashion.
(C) Describing word meaning without dictio-
nary senses. In lexicography, Kilgarriff (1997)
and Hanks (2000) cast doubt on the existence
of task-independent, distinct senses. In cogni-
tive science, Kintsch (2007) calls word meaning
?fluid and flexible?. And some researchers in lex-
ical semantics have suggested that word mean-
ings lie on a continuum between clear cut cases
of ambiguity on the one hand, and on the other
hand vagueness where clear cut boundaries do not
hold (Tuggy, 1993). There are some psycholog-
ical studies on whether different senses of a pol-
ysemous word are represented separately in the
mind or whether there is some joint representa-
tion. However, so far the evidence is inconclusive
1) We study the methods and concepts that each writer uses to
defend the cogency of legal, deliberative, or more generally
political prudence against explicit or implicit charges that
practical thinking is merely a knack or form of cleverness.
2) Eleven CIRA members have been convicted of criminal
charges and others are awaiting trial.
Figure 1: From (Erk et al, 2009): A sense pair
from the USim dataset, for the target charge.n.
Annotator judgments: 2,3,4
and varies strongly with the experimental setting.
Some studies found evidence for a separate rep-
resentation (Klein and Murphy, 2001; Pylkkanen
et al, 2006). Brown (2008) finds a linear change
in semantic similarity effects with sense distance,
which could possibly point to a continuous rep-
resentation of word meaning without clear sense
boundaries. But while there is no definitive answer
yet on the question of the mental representation
of polysemy, a computational model that does not
rely on distinct senses has the advantage of making
fewer assumptions. It also avoids the tough lexi-
cographic problem mentioned above, of deciding
on a best set of senses for a given domain.
In the recent USim annotation study (Erk et al,
2009), we tested whether human annotators could
reliably and consistently provide word meaning
judgments without the use of dictionary senses.
Three annotators rated the similarity of pairs of oc-
currences (usages) of a common target word, again
on a scale of 1-5. Figure 1 shows an example,
with the corresponding annotator judgments. The
results on this task were encouraging: Again us-
ing correlation to measure ITA, we found a highly
significant correlation (p  0.001) between the
judgments of each pair of annotators. Further-
more, there was a strong correlation on judgments
given with and without the use of dictionary senses
(USim versus WSsim) for the same data.
19
3 Vector space models of word meaning
in isolation
This section gives a brief overview of the use of
vector spaces to model concepts and word mean-
ing in cognition and computational linguistics.
In two of the current main theories of concept
representation, feature vectors play a prominent
role. Prototype theory (Hampton, 1979; Smith and
Medin, 1981) models degree of category member-
ship through similarity to a single prototype. Ex-
emplar models (Medin and Schaffer, 1978; Nosof-
sky, 1992; Nosofsky and Palmeri, 1997) represent
a concept as a collection of all previously seen ex-
emplars and compute degree of category member-
ship as similarity to stored exemplars. Both pro-
totypes and exemplars are typically represented as
feature vectors. Many models represent a concept
as a region rather than a point in space, often char-
acterized by a feature vector plus a separate di-
mension weight vector (Smith et al, 1988; Hamp-
ton, 1991; Ga?rdenfors, 2004). The features are
individually meaningful and interpretable and in-
clude sensory and motor features as well as func-
tion and taxonomic features. There are several
datasets with features elicited from human sub-
jects (McRae et al, 2005; Vigliocco et al, 2004).
In computational linguistics, distributional
models represent the meaning of a word as a vec-
tor in a high-dimensional space whose dimensions
characterize the contexts in which the word typi-
cally occurs (Lund and Burgess, 1996; Landauer
and Dumais, 1997; Sahlgren and Karlgren, 2005;
Pado? and Lapata, 2007). In the simplest case,
the dimensions are context words, and the values
are co-occurrence counts. In contrast to spaces
used in cognitive science, the dimensions in dis-
tributional models are typically not interpretable
(though see Almuhareb and Poesio (2005), Baroni
et al (2010)). A central property of distributional
models is that proximity in vector space is a pre-
dictor of semantic similarity. These models have
been used successfully in NLP (Deerwester et al,
1990; Manning et al, 2008), as well as in psy-
chology (Landauer and Dumais, 1997; Lowe and
McDonald, 2000; McDonald and Ramscar, 2001).
4 Vector space models of word meaning
in context
If we want to represent word meaning through
individual usages and their similarity only, with-
out the use of dictionary senses (along hypothesis
(C)), distributional models are an obvious choice,
if we can just represent each individual usage as
a point in space. However, vector space models
have mostly been used to represent the meaning of
a word in isolation: The vector for a word is com-
puted by summing over all its corpus occurrences,
thereby summing over all its meanings. There are
a few vector space models of meaning in context,
though they differ in what it is that they model.
One group of models computes a single vector for
a whole sentence, encoding both the words and the
syntactic structure (Smolensky, 1990; B. Coecke
and Clark, 2010). In this case, the dimensionality
of the vectors varies with the syntactic complexity
of the sentence in question. A second group also
computes a single vector for a whole expression,
but the vector for a larger expression is a combi-
nation of the word vectors for the words occurring
in the expression (Landauer and Dumais, 1997;
Mitchell and Lapata, 2008). Syntactic structure
is not encoded. The resulting vector, of the same
dimensionality as the word vectors, is then a com-
bination of the contexts in which the words of the
sentence occur. A third group of approaches de-
rives a separate vector for each word in a given
sentence (Erk and Pado?, 2008; Thater et al, 2009;
Erk and Pado?, 2010). While an approach of the
second type would derive a single, joint vector for,
say, the expression catch a ball, an approach from
the third group would derive two vectors, one for
the word catch in the context of ball, and one for
the word ball in the context of catch. In this third
group, the dimensionality of a vector for a word in
context is the same as for a word in isolation.
In this paper, we focus on the third type of ap-
proaches. Our aim is to study alternatives to dic-
tionary senses for characterizing word meaning.
So we need a meaning characterization for each
individual word in a given sentence context, rather
than a single vector for a larger expression.
We can also classify distributional approaches
to word meaning in context into prototype- and
exemplar-based approaches. Prototype-based ap-
proaches first compute a (prototype) vector for
each word in isolation, then modify this vec-
tor according to the context in a given occur-
rence (Landauer and Dumais, 1997; Mitchell
and Lapata, 2008; Erk and Pado?, 2008; Thater
et al, 2009). Typical methods for combining
prototype vectors are addition, component-wise
multiplication (introduced by Mitchell and Lap-
20
catch
he
fielder
dog
cold
baseball
drift
objsubj
accuse
say
claim
comp
-1
ball
whirl
fly
provide
throw
catch
organise
obj
-1
subj
-1
mod
red
golf
elegant
catch
...
cold
baseball
drift
obj
subj
...
comp
-1
ball
...
throw
catch
organise
obj
-1
subj
-1
mod
...
!
!
Figure 2: From (Erk and Pado?, 2008): Left: Vector representations for verb catch and noun ball. Lexical
information plus selectional preferences. Right: Computing context-specific meaning by combining
predicate and argument via selectional preference vectors
ata (2008)), and component-wise minimum. Then
there are multiple prototype approaches that stat-
ically cluster synonyms or occurrences to induce
word senses(Schu?tze, 1998; Pantel and Lin, 2002;
Reisinger and Mooney, 2010). Exemplar-based
approaches represent a word in isolation as a col-
lection of its occurrences or paraphrases, then se-
lect only the contextually appropriate exemplars
for a given occurrence context (Kintsch, 2001; Erk
and Pado?, 2010). In this paper we focus on the first
and third group of approaches, as they do not rely
on knowledge of how many word senses (clusters)
there should be.
A structured vector space model for word
meaning in context. In Erk and Pado? (2008), we
proposed the structured vector space model (SVS),
which relies solely on syntactic context for com-
puting a context-specific vector. It is a prototype-
based model, , and called structured because it ex-
plicitly represents argument structure, using multi-
ple vectors to represent each word. Figure 2 (left)
illustrates the representation. A word, for exam-
ple catch, has one vector describing the meaning
of the word itself, the lexical vector ~catch. It is
a vector for the word in isolation, as is usual for
prototype-based models. In addition, the represen-
tation for catch contains further vectors describing
the selectional preferences for each argument po-
sition. The obj preference vector of catch is com-
puted from the lexical vectors of all words that
have been observed as direct objects of catch in
some syntactically parsed corpus. In the example
in Figure 2, we have observed the direct objects
cold, baseball, and drift. In the simplest case,
the obj preference vector of catch is then com-
puted as the (weighted) sum of the three vectors
~cold, ~baseball and ~drift. Likewise, ball is repre-
sented by one vector for ball itself, one for ball ?s
preferences for its modifiers (mod), one vector for
the verbs of which it is a subject (subj?1), and one
for the verbs of which is an object (obj?1).
The vector for catch in a given context, say in
the context catch ball, is then computed as illus-
trated on the right side of Figure 2: The lexical
vector ~catch is combined with the obj?1 vector of
ball, modifying the vector ~catch in the direction of
verbs that typically take ball as an object. For the
vector combination, any of the usual operations
can be used: addition, component-wise multipli-
cation, or minimum. Likewise, the lexical vector
~ball is combined with the obj preference vector of
catch to compute the meaning of ball in the con-
text catch ball.
The standard evaluation for vector models of
meaning in context is to predict paraphrase appro-
priateness. Paraphrases always apply to a word
meaning, not a word. For example, contract is
an appropriate paraphrase for catch in the context
John caught the flu, but it is not an appropriate
paraphrase in the context John caught a butterfly.
A vector space model can predict paraphrase ap-
propriateness as the similarity (measured, for ex-
ample, using Cosine) of the context-specific vec-
tor of catch with the lexical vector of contract:
The more similar the vectors, the higher the pre-
dicted appropriateness of the paraphrase. We eval-
uated SVS on two datasets. The first is a tightly
controlled psycholinguistic dataset of subject/verb
pairs with paraphrases for the verbs only (Mitchell
and Lapata, 2008). The other is the Lexical Sub-
stitution dataset, which has annotator-generated
paraphrases for target words in a larger senten-
tial context and which is thus closer to typical
NLP application scenarios (McCarthy and Nav-
igli, 2009). SVS showed comparable performance
to the model by Mitchell and Lapata (2008) on the
21
former dataset, and outperformed the Mitchell and
Lapata model on the latter.
One obvious extension is to use all available
syntactic context, instead of focusing on a sin-
gle syntactic neighbor. We found no improve-
ment on SVS in a straightforward extension to
additional syntactic context items (Erk and Pado?,
2009). However, Thater et al (2009) did achieve
better performance with a different model that
used all syntactic context.
Taking larger context into account in an
exemplar-based model. But even if we take the
complete local syntactic context into account, we
are missing some evidence, in particular non-local
information. The word ball is interpreted differ-
ently in sentences (1a) and (1b) 1 even though its
predicate ran has more or less the samemeaning in
both sentences. What is different is the subject of
ran, player versus debutante, which is not a direct
syntactic neighbor of the ambiguous word ball.
(1)
(a) the player ran to the ball
(b) the debutante ran to the ball
Even though we are not using dictionary senses,
the types of evidence that should be useful for
computing occurrence-specific vectors should be
the same as for traditional WSD; and one of the
main type of features used there is bag-of-words
context. In (Erk and Pado?, 2010), we proposed an
exemplar-based model of word meaning in con-
text that relied on bag-of-words context informa-
tion from the whole sentence, but did not use syn-
tactic information. The model assumes that each
target lemma is represented by a set of exemplars,
where an exemplar is a sentence in which the tar-
get lemma occurs. Polysemy is then modeled by
activating (selecting) relevant exemplars of a tar-
get lemma in a given occurrence s.2 Both the ex-
emplars and the occurrence s are modeled as vec-
tors. We simply use first-order vectors that re-
flect the number of times each word occurs in a
given sentence. The activated exemplars are then
simply the ones whose vectors are most similar
to the vector of s. The results that we achieved
with the exemplar-based model on the Lexical
Substitution dataset were considerably better than
1These two examples are due to Ray Mooney.
2Instead of the binary selection of each exemplar that this
model uses, it would also be possible to assign each exemplar
a weight, making it partially selected.
those achieved with any of the syntax-based ap-
proaches (Erk and Pado?, 2008; Erk and Pado?,
2009; Thater et al, 2009).
While prototype models compute a vector by
first summing over all observed occurrences and
then having to suppress dimensions that are not
contextually appropriate, exemplar models only
take contextually appropriate exemplars into ac-
count in the first place, which is conceptually
simpler and thus more attractive. But there are
still many open questions, in particular the best
combination of bag-of-words context and syntac-
tic context as evidence for computing occurrence-
specific vector representations.
5 The role of dictionary senses
Word meaning models that rely only on individual
word usages and their similarities are more flex-
ible than dictionary-based models and make less
assumptions. On the other hand, dictionaries offer
not just sense lists but also a wealth of information
that can be used for inferences. WordNet (Fell-
baum, 1998) has relations between words and be-
tween synsets, most importantly synonymy and
hyponymy. VerbNet (Kipper et al, 2000) specifies
semantic properties of a predicate?s arguments, as
well as relations between the arguments.
In this section we discuss approaches for em-
bedding dictionary senses in a distributional model
in a way that supports hypotheses (A) and (B)
(graded sense membership, and description of an
occurrence through multiple senses) and that sup-
ports testing the applicability of dictionary-based
inference rules.
Mapping dictionary senses to points in vec-
tor space. Dictionary senses can be mapped to
points in vector space very straightforwardly if we
have sense-annotated corpus data. In that case,
we can compute a (prototype) vector for a sense
from all corpus occurrences annotated with that
sense. We used this simple model (Erk and Mc-
Carthy, 2009) to predict the graded sense appli-
cability judgments from the WSsim dataset. (See
Section 2 for more information on this dataset.)
The predictions of the vector space model sig-
nificantly correlate with annotator judgments. In
comparison with an approach that uses the con-
fidence levels of a standard WSD model as pre-
dictions, the vector space model shows higher re-
call but lower precision ? for definitions of preci-
sion and recall that are adapted to the graded case.
22
Another way of putting the findings is to say that
the WSD confidence levels tend to under-estimate
sense applicability, while the vector space model
tends to over-estimate it.
Attachment sites for inference rules. As dis-
cussed above, vector space models for word mean-
ing in context are typically evaluated on para-
phrase applicability tasks (Mitchell and Lapata,
2008; Erk and Pado?, 2008; Erk and Pado?, 2009;
Thater et al, 2009). They predict the applicabil-
ity of a paraphrase like (2) based on the similarity
between a context-specific vector for the lemma
(here, catch) and a context-independent vector for
the paraphrase. (in this case, contract).
X catch Y ? X contract Y (2)
Another way of looking at this is to consider the
inference rule (2) to be attached to a point in
space, namely the vector for contract, and to trig-
ger the inference rule for an occurrence of catch if
it is close enough to the attachment site. If we
know the WordNet sense of contract for which
rule (2) holds ? it happens to be sense 4 ?, we can
attach the rule to a vector for sense 4 of contract,
rather than a vector computed from all occurrences
of the lemma. Note that when we use dictionar-
ies as a source for inference rules, for example
by creating an inference rule like (2) for each two
words that share a synset and for each direct hy-
ponym/hypernym pair, we do know the WordNet
sense to which each inference rule attaches.
Mapping dictionary senses to regions in vector
space. In Erk (2009) we expand on the idea of
tying inference rules to attachment sites by repre-
senting a word sense not as a point but as a region
in vector space. The extent of the regions is esti-
mated through the use of both positive exemplars
(occurrences of the word sense in question), and
negative exemplars (occurrences of other words).
The computational models we use are inspired by
cognitive models of concept representation that
represent concepts as regions (Smith et al, 1988;
Hampton, 1991), in particular adopting Shepard?s
law (Shepard, 1987), which states that perceived
similarity to an exemplar decreases exponentially
with distance from its vector.
In the longer term, the goal for the association
of inference rules with attachment sites is to obtain
a principled framework for reasoning with par-
tially applicable inference rules in vector space.
6 Conclusion and outlook
In this paper, we have argued that it may be time
to consider alternative computational models of
word meaning, given that word sense disambigua-
tion, after all this time, is still a tough problem for
humans as well as machines. We have followed
three hypotheses. The first two involve dictionary
senses, suggesting that (A) senses may best be
viewed as applying to a certain degree, rather than
in a binary fashion, and (B) that it may make sense
to describe an occurrence through multiple senses
as a default rather than an exception. The third
hypothesis then departs from dictionary senses,
suggesting (C) focusing on individual word us-
ages and their similarities instead. We have argued
that distributional models are a good match for
word meaning models following hypotheses (A)-
(C): They can represent individual word usages as
points in vector space, and they can also represent
dictionary senses in a way that allows for graded
membership and overlapping senses, and we have
discussed some existing models, both prototype-
based and exemplar-based.
One big question is, of course, about the us-
ability of these alternative models of word mean-
ing in NLP applications. Will they do better than
dictionary-based models? The current evaluations,
testing paraphrase applicability in context, are a
step in the right direction, but more task-oriented
evaluation schemes have to follow.
We have argued that it makes sense to look to
cognitive models of mental concept representa-
tion. They are often based on feature vectors, and
there are many interesting ideas in these models
that have not yet been used (much) in computa-
tional models of word meaning. One of the most
exciting ones, perhaps, is that cognitive models of-
ten have interpretable dimensions. While dimen-
sions of distributional models are usually not in-
dividually interpretable, there are some first mod-
els (Almuhareb and Poesio, 2005; Baroni et al,
2010) that use patterns to extract meaningful di-
mensions from corpus data. This offers many new
perspectives: For which tasks can we improve per-
formance by selecting dimensions that are mean-
ingful specifically for that task (as in Mitchell et
al. (2008))? Can interpretable dimensions be used
for inferences? And, when we are computing vec-
tor space representations for word meaning in con-
text, is it possible to select meaningful dimensions
that are appropriate for a given context?
23
Acknowledgements. This work was supported
in part by National Science Foundation grant IIS-
0845925, and by a Morris Memorial Grant from
the New York Community Trust.
References
A. Almuhareb and M. Poesio. 2005. Finding concept
attributes in the web. In Proceedings of the Corpus
Linguistics Conference, Birmingham.
M. Sadrzadeh B. Coecke and S. Clark. 2010. Mathe-
matical foundations for a compositional distributed
model of meaning. Lambek Festschrift, Linguistic
Analysis, 36.
M. Baroni, B. Murphy, E. Barbu, and M. Poesio. 2010.
Strudel: A corpus-based semantic model based on
properties and types. Cognitive Science, 34(2):222?
254.
S. W. Brown. 2008. Choosing sense distinctions for
WSD: Psycholinguistic evidence. In Proceedings of
ACL/HLT, Columbus, OH.
J. Chen and M. Palmer. 2009. Improving English
verb sense disambiguation performance with lin-
guistically motivated features and clear sense dis-
tinction boundaries. Journal of Language Resources
and Evaluation, Special Issue on SemEval-2007,
43:181?208.
L. Coleman and P. Kay. 1981. The English word ?lie?.
Linguistics, 57.
D. A. Cruse. 1986. Lexical Semantics. Cambridge
University Press.
S. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnaas, and R. A. Harshman. 1990. Indexing by
latent semantic analysis. Journal of the Society for
Information Science, 41(6):391?407.
P. Edmonds and S. Cotton, editors. 2001. Proceed-
ings of the SensEval-2 Workshop, Toulouse, France.
ACL. See http://www.sle.sharp.co.uk/
senseval.
K. Erk and D. McCarthy. 2009. Graded word sense
assignment. In Proceedings of EMNLP, Singapore.
K. Erk and S. Pado?. 2008. A structured vector space
model for word meaning in context. In Proceedings
of EMNLP, Honolulu, HI.
K. Erk and S. Pado?. 2009. Paraphrase assessment in
structured vector space: Exploring parameters and
datasets. In Proceedings of the EACL Workshop on
Geometrical Methods for Natural Language Seman-
tics (GEMS).
K. Erk and S. Pado?. 2010. Exemplar-based models
for word meaning in context. In Proceedings of the
ACL, Uppsala.
K. Erk, D. McCarthy, and N. Gaylord. 2009. Inves-
tigations on word senses and word usages. In Pro-
ceedings of ACL, Singapore.
Katrin Erk. 2009. Representing words as regions in
vector space. In Proceedings of CoNLL.
C. Fellbaum, editor. 1998. WordNet: An electronic
lexical database. MIT Press, Cambridge, MA.
P. Ga?rdenfors. 2004. Conceptual spaces. MIT press,
Cambridge, MA.
J. A. Hampton. 1979. Polymorphous concepts in se-
mantic memory. Journal of Verbal Learning and
Verbal Behavior, 18:441?461.
J. A. Hampton. 1991. The combination of prototype
concepts. In P. Schwanenflugel, editor, The psy-
chology of word meanings. Lawrence Erlbaum As-
sociates.
J. A. Hampton. 2007. Typicality, graded membership,
and vagueness. Cognitive Science, 31:355?384.
P. Hanks. 2000. Do word meanings exist? Computers
and the Humanities, 34(1-2):205?215(11).
E. H. Hovy, M. Marcus, M. Palmer, S. Pradhan,
L. Ramshaw, and R. Weischedel. 2006. OntoNotes:
The 90% solution. In Proceedings of HLT-NAACL,
pages 57?60, New York.
A. Kilgarriff and J. Rosenzweig. 2000. Framework
and results for English Senseval. Computers and the
Humanities, 34(1-2):15?48.
A. Kilgarriff. 1997. I don?t believe in word senses.
Computers and the Humanities, 31(2):91?113.
W. Kintsch. 2001. Predication. Cognitive Science,
25:173?202.
W. Kintsch. 2007. Meaning in context. In T.K. Lan-
dauer, D. McNamara, S. Dennis, andW. Kintsch, ed-
itors, Handbook of Latent Semantic Analysis, pages
89?105. Erlbaum, Mahwah, NJ.
K. Kipper, H.T. Dang, and M. Palmer. 2000. Class-
based construction of a verb lexicon. In Proceedings
of AAAI/IAAI.
D.E. Klein and G.L. Murphy. 2001. The representa-
tion of polysemous words. Journal of Memory and
Language, 45:259?282.
G. Lakoff. 1987. Women, fire, and dangerous things.
The University of Chicago Press.
T. Landauer and S. Dumais. 1997. A solution to Platos
problem: the latent semantic analysis theory of ac-
quisition, induction, and representation of knowl-
edge. Psychological Review, 104(2):211?240.
S. Landes, C. Leacock, and R. Tengi. 1998. Build-
ing semantic concordances. In C. Fellbaum, editor,
WordNet: An Electronic Lexical Database. The MIT
Press, Cambridge, MA.
24
W. Lowe and S. McDonald. 2000. The direct route:
Mediated priming in semantic space. In Proceed-
ings of the Cognitive Science Society, pages 675?
680.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers, 28:203?208.
C. D. Manning, P. Raghavan, and H. Schu?tze. 2008.
Introduction to Information Retrieval. Cambridge
University Press.
D. McCarthy and R. Navigli. 2009. The English lexi-
cal substitution task. Language Resources and Eval-
uation, 43(2):139?159. Special Issue on Compu-
tational Semantic Analysis of Language: SemEval-
2007 and Beyond.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Finding predominant senses in untagged text.
In Proceedings of ACL, Barcelona.
M. McCloskey and S. Glucksberg. 1978. Natural cat-
egories: Well defined or fuzzy sets? Memory &
Cognition, 6:462?472.
S. McDonald and M. Ramscar. 2001. Testing the dis-
tributional hypothesis: The influence of context on
judgements of semantic similarity. In Proceedings
of the Cognitive Science Society, pages 611?616.
K. McRae, G. S. Cree, M. S. Seidenberg, and C. Mc-
Norgan. 2005. Semantic feature production norms
for a large set of living and nonliving things. Behav-
ior Research Methods, 37(4):547?559.
D. L. Medin and M. M. Schaffer. 1978. Context the-
ory of classification learning. Psychological Review,
85:207?238.
R. Mihalcea, T. Chklovski, and A. Kilgariff. 2004. The
Senseval-3 English lexical sample task. In Proceed-
ings of SensEval-3, Barcelona.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL,
Columbus, OH.
T. Mitchell, S. Shinkareva, A. Carlson, K. Chang,
V.Malave, R. Mason, and M. Just. 2008. Predicting
human brain activity associated with the meanings
of nouns. Science, 320(5880):1191?1195.
G. L. Murphy. 1991. Meaning and concepts. In
P. Schwanenflugel, editor, The psychology of word
meanings. Lawrence Erlbaum Associates.
G. L. Murphy. 2002. The Big Book of Concepts. MIT
Press.
R. M. Nosofsky and T. J. Palmeri. 1997. An exemplar-
based random walk model of speeded classification.
Psychological Review, 104(2):266?300.
R. M. Nosofsky. 1992. Exemplars, prototypes, and
similarity rules. In A. Healy, S. Kosslyn, and
R. Shiffrin, editors, From learning theory to connec-
tionist theory: essays in honor of W.K. Estes, vol-
ume 1, pages 149?168. Erlbaum, Hillsdale, NJ.
S. Pado? and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
M. Palmer, H. Trang Dang, and C. Fellbaum. 2007.
Making fine-grained and coarse-grained sense dis-
tinctions, both manually and automatically. Natural
Language Engineering, 13:137?163.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proceedings of KDD, Edmonton,
Canada.
R. Passonneau, A. Salleb-Aouissi, V. Bhardwaj, and
N. Ide. 2010. Word sense annotation of polyse-
mous words by multiple annotators. In Proceedings
of LREC-7, Valleta, Malta.
S. Pradhan, E. Loper, D. Dligach, and M. Palmer.
2007. Semeval-2007 task 17: English lexical sam-
ple, SRL and all words. In Proceedings of Se-
mEval?, Prague, Czech Republic.
L. Pylkkanen, R. Llinas, and G.L. Murphy. 2006. The
representation of polysemy: MEG evidence. Jour-
nal of Cognitive Neuroscience, 18:97?109.
J. Reisinger and R.J. Mooney. 2010. Multi-prototype
vector-space models of word meaning. In Proceed-
ing of NAACL.
E. Rosch and C. B. Mervis. 1975. Family resem-
blance: Studies in the internal structure of cate-
gories. Cognitive Psychology, 7:573?605.
E. Rosch. 1975. Cognitive representations of seman-
tic categories. Journal of Experimental Psychology:
General, 104:192?233.
M. Sahlgren and J. Karlgren. 2005. Automatic bilin-
gual lexicon acquisition using random indexing of
parallel corpora. Journal of Natural Language En-
gineering, Special Issue on Parallel Texts, 11(3).
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1).
R. Shepard. 1987. Towards a universal law of
generalization for psychological science. Science,
237(4820):1317?1323.
E. E. Smith and D. L. Medin. 1981. Categories and
Concepts. Harvard University Press, Cambridge,
MA.
E. E. Smith, D. Osherson, L. J. Rips, and M. Keane.
1988. Combining prototypes: A selective modifica-
tion model. Cognitive Science, 12(4):485?527.
25
P. Smolensky. 1990. Tensor product variable binding
and the representation of symbolic structures in con-
nectionist systems. Artificial Intelligence, 46:159?
216.
B. Snyder andM. Palmer. 2004. The English all-words
task. In 3rd International Workshop on Semantic
Evaluations (SensEval-3) at ACL-2004, Barcelona,
Spain.
J. Taylor. 1989. Linguistic Categorization: Prototypes
in Linguistic Theory. Oxford Textbooks in Linguis-
tics.
S. Thater, G. Dinu, and M. Pinkal. 2009. Ranking
paraphrases in context. In Proceedings of the ACL
Workshop on Applied Textual Inference, Singapore.
D. H. Tuggy. 1993. Ambiguity, polysemy and vague-
ness. Cognitive linguistics, 4(2):273?290.
G. Vigliocco, D. P. Vinson, W. Lewis, and M. F. Gar-
rett. 2004. Representing the meanings of object
and action words: The featural and unitary semantic
space hypothesis. Cognitive Psychology, 48:422?
488.
W. Weaver. 1949. Translation. In W.N. Locke and
A.D. Booth, editors, Machine Translation of Lan-
guages: Fourteen Essays. MIT Press, Cambridge,
MA.
26
Integrating Logical Representations with
Probabilistic Information using Markov Logic
Dan Garrette
University of Texas at Austin
dhg@cs.utexas.edu
Katrin Erk
University of Texas at Austin
katrin.erk@mail.utexas.edu
Raymond Mooney
University of Texas at Austin
mooney@cs.utexas.edu
Abstract
First-order logic provides a powerful and flexible mechanism for representing natural language
semantics. However, it is an open question of how best to integrate it with uncertain, probabilistic
knowledge, for example regarding word meaning. This paper describes the first steps of an approach
to recasting first-order semantics into the probabilistic models that are part of Statistical Relational
AI. Specifically, we show how Discourse Representation Structures can be combined with distribu-
tional models for word meaning inside a Markov Logic Network and used to successfully perform
inferences that take advantage of logical concepts such as factivity as well as probabilistic informa-
tion on word meaning in context.
1 Introduction
Logic-based representations of natural language meaning have a long history. Representing the meaning
of language in a first-order logical form is appealing because it provides a powerful and flexible way to
express even complex propositions. However, systems built solely using first-order logical forms tend
to be very brittle as they have no way of integrating uncertain knowledge. They, therefore, tend to have
high precision at the cost of low recall (Bos and Markert, 2005).
Recent advances in computational linguistics have yielded robust methods that use weighted or prob-
abilistic models. For example, distributional models of word meaning have been used successfully to
judge paraphrase appropriateness. This has been done by representing the word meaning in context as
a point in a high-dimensional semantics space (Erk and Pado?, 2008; Thater et al, 2010; Erk and Pado?,
2010). However, these models typically handle only individual phenomena instead of providing a mean-
ing representation for complete sentences. It is a long-standing open question how best to integrate the
weighted or probabilistic information coming from such modules with logic-based representations in a
way that allows for reasoning over both. See, for example, Hobbs et al (1993).
The goal of this work is to combine logic-based meaning representations with probabilities in a
single unified framework. This will allow us to obtain the best of both situations: we will have the
full expressivity of first-order logic and be able to reason with probabilities. We believe that this will
allow for a more complete and robust approach to natural language understanding. In order to perform
logical inference with probabilities, we draw from the large and active body of work related to Statistical
Relational AI (Getoor and Taskar, 2007). Specifically, we make use of Markov Logic Networks (MLNs)
(Richardson and Domingos, 2006) which employ weighted graphical models to represent first-order
logical formulas. MLNs are appropriate for our approach because they provide an elegant method of
assigning weights to first-order logical rules, combining a diverse set of inference rules, and performing
inference in a probabilistic way.
While this is a large and complex task, this paper proposes a series of first steps toward our goal.
In this paper, we focus on three natural language phenomena and their interaction: implicativity and
factivity, word meaning, and coreference. Our framework parses natural language into a logical form,
adds rule weights computed by external NLP modules, and performs inferences using an MLN. Our
end-to-end approach integrates multiple existing tools. We use Boxer (Bos et al, 2004) to parse natural
105
language into a logical form. We use Alchemy (Kok et al, 2005) for MLN inference. Finally, we use the
exemplar-based distributional model of Erk and Pado? (2010) to produce rule weights.
2 Background
Logic-based semantics. Boxer (Bos et al, 2004) is a software package for wide-coverage semantic anal-
ysis that provides semantic representations in the form of Discourse Representation Structures (Kamp
and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). Bos and Markert (2005)
describe a system for Recognizing Textual Entailment (RTE) that uses Boxer to convert both the premise
and hypothesis of an RTE pair into first-order logical semantic representations and then uses a theorem
prover to check for logical entailment.
Distributional models for lexical meaning. Distributional models describe the meaning of a word
through the context in which it appears (Landauer and Dumais, 1997; Lund and Burgess, 1996), where
contexts can be documents, other words, or snippets of syntactic structure. Distributional models are able
to predict semantic similarity between words based on distributional similarity and they can be learned
in an unsupervised fashion. Recently distributional models have been used to predict the applicability
of paraphrases in context (Mitchell and Lapata, 2008; Erk and Pado?, 2008; Thater et al, 2010; Erk and
Pado?, 2010). For example, in ?The wine left a stain?, ?result in? is a better paraphrase for ?leave? than is
?entrust?, while the opposite is true in ?He left the children with the nurse?. Usually, the distributional
representation for a word mixes all its usages (senses). For the paraphrase appropriateness task, these
representations are then reweighted, extended, or filtered to focus on contextually appropriate usages.
Markov Logic. An MLN consists of a set of weighted first-order clauses. It provides a way of softening
first-order logic by making situations in which not all clauses are satisfied less likely but not impossible
(Richardson and Domingos, 2006). More formally, letX be the set of all propositions describing a world
(i.e. the set of all ground atoms), F be the set of all clauses in the MLN, wi be the weight associated
with clause fi ? F , Gfi be the set of all possible groundings of clause fi, and Z be the normalization
constant. Then the probability of a particular truth assignment x to the variables in X is defined as:
P (X = x) =
1
Z exp
?
?
?
fi?F
wi
?
g?Gfi
g(x)
?
? =
1
Z exp
?
?
?
fi?F
wini(x)
?
? (1)
where g(x) is 1 if g is satisfied and 0 otherwise, and ni(x) =
?
g?Gfi
g(x) is the number of groundings
of fi that are satisfied given the current truth assignment to the variables in X . This means that the
probability of a truth assignment rises exponentially with the number of groundings that are satisfied.
Markov Logic has been used previously in other NLP application (e.g. Poon and Domingos (2009)).
However, this paper marks the first attempt at representing deep logical semantics in an MLN.
While it is possible learn rule weights in anMLN directly from training data, our approach at this time
focuses on incorporating weights computed by external knowledge sources. Weights for word meaning
rules are computed from the distributional model of lexical meaning and then injected into the MLN.
Rules governing implicativity and coreference are given infinite weight (hard constraints).
3 Evaluation and phenomena
Textual entailment offers a good framework for testing whether a system performs correct analyses and
thus draws the right inferences from a given text. For example, to test whether a system correctly handles
implicative verbs, one can use the premise p along with the hypothesis h in (1) below. If the system
analyses the two sentences correctly, it should infer that h holds. While the most prominent forum using
textual entailment is the Recognizing Textual Entailment (RTE) challenge (Dagan et al, 2005), the RTE
datasets do not test the phenomena in which we are interested. For example, in order to evaluate our
system?s ability to determine word meaning in context, the RTE pair would have to specifically test word
106
sense confusion by having a word?s context in the hypothesis be different from the context of the premise.
However, this simply does not occur in the RTE corpora. In order to properly test our phenomena, we
construct hand-tailored premises and hypotheses based on real-world texts.
In this paper, we focus on three natural language phenomena and their interaction: implicativity and
factivity, word meaning, and coreference. The first phenomenon, implicativity and factivity, is concerned
with analyzing the truth conditions of nested propositions. For example, in the premise of the entailment
pair shown in example (1), ?arrange that? falls under the scope of ?forget to? and ?fail? is under the scope
of ?arrange that?. Correctly recognizing nested propositions is necessary for preventing false inferences
such as the one in example (2).
(1) p: Ed did not forget to arrange that Dave fail1
h: Dave failed
(2) p: The mayor hoped to build a new stadium2
h*: The mayor built a new stadium
For the second phenomenon, word meaning, we address paraphrasing and hypernymy. For example,
in (3) ?covering? is a good paraphrase for ?sweeping? while ?brushing? is not.
(3) p: A stadium craze is sweeping the country
h1: A stadium craze is covering the country
h2*: A stadium craze is brushing the country
The third phenomenon is coreference, as illustrated in (4). For this example, to correctly judge the
hypothesis as entailed, it is necessary to recognize that ?he? corefers with ?Christopher? and ?the new
ballpark? corefers with ?a replacement for Candlestick Park?.
(4) p: George Christopher has been a critic of the plan to build a replacement for Candlestick Park.
As a result, he won?t endorse the new ballpark.
h: Christopher won?t endorse a replacement for Candlestick Park.
Some natural language phenomena are most naturally treated as categorial, while others are more
naturally treated using weights or probabilities. In this paper, we treat implicativity and coreference as
categorial phenomena, while using a probabilistic approach to word meaning.
4 Transforming natural language text to logical form
In transforming natural language text to logical form, we build on the software package Boxer (Bos et al,
2004). We chose to use Boxer for two main reasons. First, Boxer is a wide-coverage system that can deal
with arbitrary text. Second, the DRSs that Boxer produces are close to the standard first-order logical
forms that are required for use by the MLN software package Alchemy. Our system transforms Boxer
output into a format that Alchemy can read and augments it with additional information.
To demonstrate our transformation procedure, consider again the premise of example (1). When
given to Boxer, the sentence produces the output given in Figure 1a. We then transform this output to the
format given in Figure 1b.
Flat structure. In Boxer output, nested propositional statements are represented as nested sub-DRS
structures. For example, in the premise of (1), the verbs ?forget to? and ?arrange that? both introduce
nested propositions, as is shown in Figure 1a where DRS x3 (the ?arranging that?) is the theme of ?forget
to? and DRS x5 (the ?failing?) is the theme of ?arrange that?.
In order to write logical rules about the truth conditions of nested propositions, the structure has to
be flattened. However, it is clearly not sufficient to just conjoin all propositions at the top level. Such an
approach, applied to example (2), would yield (hope(x1) ? theme(x1, x2) ? build(x2) ? . . .), leading
to the wrong inference that the stadium was built. Instead, we add a new argument to each predicate that
1Examples (1) and (16) and Figure 2 are based on examples by MacCartney and Manning (2009)
2Examples (2), (3), (4), and (18) are modified versions of sentences from document wsj 0126 from the Penn Treebank
107
x0 x1
named(x0,ed,per)
named(x1,dave,per)
?
x2 x3
forget(x2)
event(x2)
agent(x2,x0)
theme(x2,x3)
x3:
x4 x5
arrange(x4)
event(x4)
agent(x4,x0)
theme(x4,x5)
x5:
x6
fail(x6)
event(x6)
agent(x6,x1)
(a) Output from Boxer
transforms to????????
named(l0, ne per ed d s0 w0, z0)
named(l0, ne per dave d s0 w7, z1)
not(l0, l1)
pred(l1, v forget d s0 w3, e2)
event(l1, e2)
rel(l1, agent, e2, z0)
rel(l1, theme, e2, l2)
prop(l1, l2)
pred(l2, v arrange d s0 w5, e4)
event(l2, e4)
rel(l2, agent, e4, z0)
rel(l2, theme, e4, l3)
prop(l2, l3)
pred(l3, v fail d s0 w8, e6)
event(l3, e6)
rel(l3, agent, e6, z1)
(b) Canonical form
Figure 1: Converting the premise of (1) from Boxer output to MLN input
names the DRS in which the predicate originally occurred. Assigning the label l1 to the DRS containing
the predicate forget, we add l1 as the first argument to the atom pred(l1, v forget d s0 w3, e2).3 Having
flattened the structure, we need to re-introduce the information about relations between DRSs. For this
we use predicates not, imp, and or whose arguments are DRS labels. For example, not(l0, l1) states that
l1 is inside l0 and negated. Additionally, an atom prop(l0, l1) indicates that DRS l0 has a subordinate
DRS labeled l1.
One important consequence of our flat structure is that the truth conditions of our representation no
longer coincide with the truth conditions of the underlying DRS being represented. For example, we do
not directly express the fact that the ?forgetting? is actually negated, since the negation is only expressed
as a relation between DRS labels. To access the information encoded in relations between DRS labels, we
add predicates that capture the truth conditions of the underlying DRS. We use the predicates true(label)
and false(label) that state whether the DRS referenced by label is true or false. We also add rules that
govern how the predicates for logical operators interact with these truth values. For example, the rules in
(5) state that if a DRS is true, then any negated subordinate must be false and vice versa.
? p n.[not(p, n) ? (true(p) ? false(n)) ? (false(p) ? true(n))] (5)
Injecting additional information into the logical form. We want to augment Boxer output with addi-
tional information, for example gold coreference annotation for sentences that we subsequently analyze
with Boxer. In order to do so, we need to be able to tie predicates in the Boxer output back to words in
the original sentence. Fortunately, the optional ?Prolog? output format from Boxer provides the sentence
and word indices from the original sentence. When parsing the Boxer output, we extract these indices
and concatenate them to the word lemma to specific the exact occurrence of the lemma that is under
discussion. For example, the atom pred(l1, v forget d s0 w3, e2) indicates that event e2 refers to the
lemma ?forget? that appears in the 0th sentence of discourse d at word index 3.
Atomic formulas. We represent the words from the sentence as arguments instead of predicates in order
to simplify the set of inference rules we need to specify. Because our flattened structure requires that
the inference mechanism be reimplemented as a set of logical rules, it is desirable for us to be able to
write general rules that govern the interaction of atoms. With the representation we have chosen, we
can quantify over all predicates or all relations. For example, the rule in (6) states that a predicate is
accessible if it is found in an out-scoping DRS.
3The extension to the word, such as d s0 w3 for ?forget?, is an index providing the location of the original word that
triggered this atom; this is addressed in more detail shortly.
108
signature example
managed to +/- he managed to escape  he escaped
he did not manage to escape  he did not escape
refused to -/o he refused to fight  he did not fight
he did not refuse to fight 2 {he fought, he did not fight}
Figure 2: Implication Signatures
? l1 l2.[outscopes(l1, l2) ? ? p x.[pred(l1, p, x) ? pred(l2, p, x)]] (6)
We use three different predicate symbols to distinguish three types of atomic concepts: predicates,
named entities, and relations. Predicates and named entities represent words that appear in the text.
For example, named(l0, ne per ed d s0 w0, z0) indicates that variable z0 is a person named ?Ed? while
pred(l1, v forget d s0 w3, e2) says that e2 is a ?forgetting to? event. Relations capture the relationships
between words. For example, rel(l1, agent, e2, z0) indicates that z0, ?Ed?, is the ?agent? of the ?forgetting
to? event e2.
5 Handling the phenomena
Implicatives and factives
Nairn et al (2006) presented an approach to the treatment of inferences involving implicatives and fac-
tives. Their approach identifies an ?implication signature? for every implicative or factive verb that
determines the truth conditions for the verb?s nested proposition, whether in a positive or negative en-
vironment. Implication signatures take the form ?x/y? where x represents the implicativity in the the
positive environment and y represents the implicativity in the negative environment. Both x and y have
three possible values: ?+? for positive entailment, meaning the nested proposition is entailed, ?-? for
negative entailment, meaning the negation of the proposition is entailed, and ?o? for ?null? entailment,
meaning that neither the proposition nor its negation is entailed. Figure 2 gives concrete examples.
We use these implication signatures to automatically generate rules that license specific entailments
in the MLN. Since ?forget to? has implication signature ?-/+?, we generate the two rules in (7).
(7) (a) ? l1 l2 e.[(pred(l1, ?forget?, e) ? true(l1) ? rel(l1, ?theme?, e, l2) ? prop(l1, l2)) ? false(l2)]]4
(b) ? l1 l2 e.[(pred(l1, ?forget?, e) ? false(l1) ? rel(l1, ?theme?, e, l2) ? prop(l1, l2)) ? true(l2)]
To understand these rules, consider (7a). The rule says that if the atom for the verb ?forget to? appears
in a DRS that has been determined to be true, then the DRS representing any ?theme? proposition of that
verb should be considered false. Likewise, (7b) says that if the occurrence of ?forget to? appears in a
DRS determined to be false, then the theme DRS should be considered true.
Note that when the implication signature indicates a ?null? entailment, no rule is generated for that
case. This prevents the MLN from licensing entailments related directly to the nested proposition, but
still allows for entailments that include the factive verb. So he wanted to fly entails neither he flew nor he
did not fly, but it does still license he wanted to fly.
Ambiguity in word meaning
In order for our system to be able to make correct natural language inference, it must be able to handle
paraphrasing and deal with hypernymy. For example, in order to license the entailment pair in (8), the
system must recognize that ?owns? is a valid paraphrase for ?has?, and that ?car? is a hypernym of
?convertible?.
(8) p: Ed has a convertible
h: Ed owns a car
4Occurrence-indexing on the predicate ?forget? has been left out for brevity.
109
In this section we discuss our probabilistic approach to paraphrasing. In the next section we discuss
how this approach is extended to cover hypernymy. A central problem to solve in the context of para-
phrases is that they are context-dependent. Consider again example (3) and its two hypotheses. The first
hypothesis replaces the word ?sweeping? with a paraphrase that is valid in the given context, while the
second uses an incorrect paraphrase.
To incorporate paraphrasing information into our system, we first generate rules stating all paraphrase
relationships that may possibly apply to a given predicate/hypothesis pair, using WordNet (Miller, 2009)
as a resource. Then we associate those rules with weights to signal contextual adequacy. For any two
occurrence-indexed words w1, w2 occurring anywhere in the premise or hypothesis, we check whether
they co-occur in a WordNet synset. If w1, w2 have a common synset, we generate rules of the form
? l x.[pred(l, w1, x) ? pred(l, w2, x)] to connect them. For named entities, we perform a similar
routine: for each pair of matching named entities found in the premise and hypothesis, we generate a
rule ? l x.[named(l, w1, x) ? named(l, w2, x)].
We then use the distributional model of Erk and Pado? (2010) to compute paraphrase appropriateness.
In the case of (3) this means measuring the cosine similarity between the vectors for ?sweep? and ?cover?
(and between ?sweep? and ?brush?) in the sentential context of the premise. MLN formula weights are
expected to be log-odds (i.e., log(P/(1?P )) for some probability P ), so we rank all possible paraphrases
of a given word w by their cosine similarity to w and then give them probabilities that decrease by
rank according to a Zipfian distribution. So, the kth closest paraphrase by cosine similarity will have
probability Pk given by (9):
Pk ? 1/k (9)
The generated rules are given in (10) with the actual weights calculated for example (3). Note that
the valid paraphrase ?cover? is given a higher weight than the incorrect paraphrase ?brush?, which allows
the MLN inference procedure to judge h1 as a more likely entailment than h2.5 This same result would
not be achieved if we did not take context into consideration because, without context, ?brush? is a more
likely paraphrase of ?sweep? than ?cover?.
(10) (a) -2.602 ? l x.[pred(l, ?v sweep p s0 w4?, x) ? pred(l, ?v cover h s0 w4?, x)]
(b) -3.842 ? l x.[pred(l, ?v sweep p s0 w4?, x) ? pred(l, ?v brush h s0 w4?, x)]
Since Alchemy outputs a probability of entailment and not a binary judgment, it is necessary to
specify a probability threshold indicating entailment. An appropriate threshold between ?entailment?
and ?non-entailment? will be one that separates the probability of an inference with the valid rule from
the probability of an inference with the invalid rule. While we plan to automatically induce a threshold
in the future, our current implementation uses a value set manually.
Hypernymy
Like paraphrasehood, hypernymy is context-dependent: In ?A bat flew out of the cave?, ?animal? is
an appropriate hypernym for ?bat?, but ?artifact? is not. So we again use distributional similarity to
determine contextual appropriateness. However, we do not directly compute cosine similarities between
a word and its potential hypernym. We can hardly assume ?baseball bat? and ?artifact? to occur in similar
distributional contexts. So instead of checking for similarity of ?bat? and ?artifact? in a given context, we
check ?bat? and ?club?. That is, we pick a synonym or close hypernym of the word in question (?bat?)
that is also a WordNet hyponym of the hypernym to check (?artifact?).
A second problem to take into account is the interaction of hypernymy and polarity. While (8) is a
valid pair, (11) is not, because ?have a convertible? is under negation. So, we create weighted rules of
the form hypernym(w, h), along with inference rules to guide their interaction with polarity. We create
5Because weights are calculated according to the equation log(P/(1 ? P )), any paraphrase that has a probability of less
than 0.5 will have a negative weight. Since most paraphrases will have probabilities less than 0.5, most will yield negative
rule weights. However, the inferences are still handled properly in the MLN because the inference is dependent on the relative
weights.
110
these rules for all pairs of words w, h in premise and hypothesis such that h is a hypernym of w, again
using WordNet to determine potential hypernymy.
(11) p: Ed does not have a convertible
h: Ed does not own a car
Our inference rules governing the interaction of hypernymy and polarity are given in (12). The rule
in (12a) states that in a positive environment, the hyponym entails the hypernym while the rule in (12b)
states that in a negative environment, the opposite is true: the hypernym entails the hyponym.
(12) (a) ? l p1 p2 x.[(hypernym(p1, p2) ? true(l) ? pred(l, p1, x)) ? pred(l, p2, x)]]
(b) ? l p1 p2 x.[(hypernym(p1, p2) ? false(l) ? pred(l, p2, x)) ? pred(l, p1, x)]]
Making use of coreference information
As a test case for incorporating additional resources into Boxer?s logical form, we used the coreference
data in OntoNotes (Hovy et al, 2006). However, the same mechanism would allow us to transfer in-
formation into Boxer output from arbitrary additional NLP tools such as automatic coreference analysis
tools or semantic role labelers. Our system uses coreference information into two distinct ways.
The first way we make use of coreference data is to copy atoms describing a particular variable
to those variables that corefer. Consider again example (4) which has a two-sentence premise. This
inference requires recognizing that the ?he? in the second sentence of the premise refers to ?George
Christopher? from the first sentence. Boxer alone is unable to make this connection, but if we receive
this information as input, either from gold-labeled data or a third-party coreference tool, we are able to
incorporate it. Since Boxer is able to identify the index of the word that generated a particular predicate,
we can tie each predicate to any related coreference chains. Then, for each atom on the chain, we can
inject copies of all of the coreferring atoms, replacing the variables to match. For example, the word
?he? generates an atom pred(l0, male, z5)6 and ?Christopher? generates atom named(l0, christopher, x0).
So, we can create a new atom by taking the atom for ?christopher? and replacing the label and variable
with that of the atom for ?he?, generating named(l0, christopher, x5).
As a more complex example, the coreference information will inform us that ?the new ballpark?
corefers with ?a replacement for Candlestick Park?. However, our system is currently unable to handle
this coreference correctly at this time because, unlike the previous example, the expression ?a replace-
ment for Candlestick Park? results in a complex three-atom conjunct with two separate variables: pred(l2,
replacement, x6), rel(l2, for, x6, x7), and named(l2, candlestick park, x7). Now, unifying with the atom
for ?a ballpark?, pred(l0, ballpark, x3), is not as simple as replacing the variable because there are two
variables to choose from. Note that it would not be correct to replace both variables since this would
result in a unification of ?ballpark? with ?candlestick park? which is wrong. Instead we must determine
that x6 should be the one to unify with x3 while x7 is replaced with a fresh variable. The way that we can
accomplish this is to look at the dependency parse of the sentence that is produced by the C&C parser is
a precursor to running Boxer. By looking up both ?replacement? and ?Candlestick Park? in the parse, we
can determine that ?replacement? is the head of the phrase, and thus is the atom whose variable should
be unified. So, we would create new atoms, pred(l0, replacement, x3), rel(l0, for, x3, z0), and named(l0,
candlestick park, z0), where z0 is a fresh variable.
The second way that we make use of coreference information is to extend the sentential contexts
used for calculating the appropriateness of paraphrases in the distributional model. In the simplest case,
the sentential context of a word would simply be the other words in the sentence. However, consider the
context of the word ?lost? in the second sentence of (13).
(13) p1: In [the final game of the season]1, [the team]2 held on to their lead until overtime
p2: But despite that, [they]2 eventually lost [it all]1
6Atoms simplified for brevity
111
Here we would like to disambiguate ?lost?, but its immediate context, words like ?despite? and
?eventually?, gives no indication as to its correct sense. Our procedure extends the context of the sentence
by incorporating all of the words from all of the phrases that corefer with a word in the immediate
context. Since coreference chains 1 and 2 have words in p2, the context of ?lost? ends up including
?final?, ?game?, ?season?, and ?team? which give a strong indication of the sense of ?lost?. Note that
using coreference data is stronger than simply expanding the window because coreferences can cover
arbitrarily long distances.
6 Evaluation
As a preliminary evaluation of our system, we constructed a set of demonstrative examples to test our
ability to handle the previously discussed phenomena and their interactions and ran each example with
both a theorem prover and Alchemy. Note that when running an example in the theorem prover, weights
are not possible, so any rule that would be weighted in an MLN is simply treated as a ?hard clause?
following Bos and Markert (2005).
Checking the logical form. We constructed a list of 72 simple examples that exhaustively cover cases
of implicativity (positive, negative, null entailments in both positive and negative environments), hyper-
nymy, quantification, and the interaction between implicativity and hypernymy. The purpose of these
simple tests is to ensure that our flattened logical form and truth condition rules correctly maintain the
semantics of the underlying DRSs. Examples are given in (14).
(14) (a) The mayor did not manage to build a stadium 2 The mayor built a stadium
(b) Fido is a dog and every dog walks  A dog walks
Examples in previous sections. Examples (1), (2), (3), (8), and (11) all come out as expected. Each
of these examples demonstrates one of the phenomena in isolation. However, example (4) returns ?not
entailed?, the incorrect answer. As discussed previously, this failure is a result of our system?s inabil-
ity to correctly incorporate the complex coreferring expression ?a replacement for Candlestick Park?.
However, the system is able to correctly incorporate the coreference of ?he? in the second sentence to
?Christopher? in the first.
Implicativity and word sense. For example (15), ?fail to? is a negatively entailing implicative in a
positive environment. So, p correctly entails hgood in both the theorem prover and Alchemy. However,
the theorem prover incorrectly licenses the entailment of hbad while Alchemy does not. The probabilistic
approach performs better in this situation because the categorial approach does not distinguish between
a good paraphrase and a bad one. This example also demonstrates the advantage of using a context-
sensitive distributional model to calculate the probabilities of paraphrases because ?reward? is an a priori
better paraphrase than ?observe? according to WordNet since it appears in a higher ranked synset.
(15) p: The U.S. is watching closely as South Korea fails to honor U.S. patents7
hgood: South Korea does not observe U.S. patents
hbad: South Korea does not reward U.S. patents
Implicativity and hypernymy. MacCartney and Manning (2009) extended the work by Nairn et al
(2006) in order to correctly treat inference involving monotonicity and exclusion. Our approaches to
implicatives and factivity and hyper/hyponymy combine naturally to address these issues because of the
structure of our logical representations and rules. For example, no additional work is required to license
the entailments in (16).
(16) (a) John refused to dance  John didn?t tango
(b) John did not forget to tango  John danced
7Example (15) is adapted from Penn Treebank document wsj 0020 while (17) is adapted from document wsj 2358
112
Example (17) demonstrates how our system combines categorial implicativity with a probabilistic
approach to hypernymy. The verb ?anticipate that? is positively entailing in the negative environment.
The verb ?moderate? can mean ?chair? as in ?chair a discussion? or ?curb? as in ?curb spending?. Since
?restrain? is a hypernym of ?curb?, it receives a weight based on the applicability of the word ?curb? in
the context. Similarly, ?talk? receives a weight based on its hyponym ?chair?. Since our model predicts
?curb? to be a more probable paraphrase of ?moderate? in this context than ?chair? (even though the
priors according to WordNet are reversed), the system is able to infer hgood while rejecting hbad.
(17) p: He did not anticipate that inflation would moderate this year
hgood: Inflation restrained this year
hbad: Inflation talked this year
Word sense, coreference, and hypernymy. Example (18) demonstrates the interaction between para-
phrase, hypernymy, and coreference incorporated into a single entailment. The relevant coreference
chains are marked explicitly in the example. The correct inference relies on recognizing that ?he? in the
hypothesis refers to ?Joe Robbie? and ?it? to ?coliseum?, which is a hyponym of ?stadium?. Further,
our model recognizes that ?sizable? is a better paraphrase for ?healthy? than ?intelligent? even though
WordNet has the reverse order.
(18) p: [Joe Robbie]53 couldn?t persuade the mayor , so [he]53 built [[his]53 own coliseum]54.
[He]53 has used [it]54 to turn a healthy profit.8
hgood: Joe Robbie used a stadium to turn a sizable profit
hbad?1: Joe Robbie used a stadium to turn an intelligent profit
hbad?2: The mayor used a stadium to turn a healthy profit
7 Future work
The next step is to execute a full-scale evaluation of our approach using more varied phenomena and
naturally occurring sentences. However, the memory requirements of Alchemy are a limitation that
prevents us from currently executing larger and more complex examples. The problem arises because
Alchemy considers every possible grounding of every atom, even when a more focused subset of atoms
and inference rules would suffice. There is on-going work to modify Alchemy so that only the required
groundings are incorporated into the network, reducing the size of the model and thus making it possible
to handle more complex inferences. We will be able to begin using this new version of Alchemy very
soon and our task will provide an excellent test case for the modification.
Since Alchemy outputs a probability of entailment, it is necessary to fix a threshold that separates
entailment from nonentailment. We plan to use machine learning techniques to compute an appropriate
threshold automatically from a calibration dataset such as a corpus of valid and invalid paraphrases.
8 Conclusion
In this paper, we have introduced a system that implements a first step towards integrating logical seman-
tic representations with probabilistic weights using methods from Statistical Relational AI, particularly
Markov Logic. We have focused on three phenomena and their interaction: implicatives, coreference,
and word meaning. Taking implicatives and coreference as categorial and word meaning as probabilis-
tic, we have used a distributional model to generate paraphrase appropriateness ratings, which we then
transformed into weights on first order formulas. The resulting MLN approach is able to correctly solve
a number of difficult textual entailment problems that require handling complex combinations of these
important semantic phenomena.
8Only relevent coreferences have been marked
113
References
Bos, J., S. Clark, M. Steedman, J. R. Curran, and J. Hockenmaier (2004). Wide-coverage semantic
representations from a CCG parser. In Proceedings of COLING 2004, Geneva, Switzerland, pp. 1240?
1246.
Bos, J. and K. Markert (2005). Recognising textual entailment with logical inference. In Proceedings of
EMNLP 2005, pp. 628?635.
Clark, S. and J. R. Curran (2004). Parsing the WSJ using CCG and log-linear models. In Proceedings of
ACL 2004, Barcelona, Spain, pp. 104?111.
Dagan, I., O. Glickman, and B. Magnini (2005). The pascal recognising textual entailment challenge. In
In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.
Erk, K. and S. Pado? (2008). A structured vector space model for word meaning in context. In Proceedings
of EMNLP 2008, Honolulu, HI, pp. 897?906.
Erk, K. and S. Pado? (2010). Exemplar-based models for word meaning in context. In Proceedings of
ACL 2010, Uppsala, Sweden, pp. 92?97.
Getoor, L. and B. Taskar (Eds.) (2007). Introduction to Statistical Relational Learning. Cambridge, MA:
MIT Press.
Hobbs, J. R., M. Stickel, D. Appelt, and P. Martin (1993). Interpretation as abduction. Artificial Intelli-
gence 63(1?2), 69?142.
Hovy, E., M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel (2006). Ontonotes: The 90% solution.
In Proceedings of HLT/NAACL 2006, pp. 57?60.
Kamp, H. and U. Reyle (1993). From Discourse to Logic; An Introduction to Modeltheoretic Semantics
of Natural Language, Formal Logic and DRT. Dordrecht: Kluwer.
Kok, S., P. Singla, M. Richardson, and P. Domingos (2005). The Alchemy system for statistical relational
AI. Technical report, Department of Computer Science and Engineering, University of Washington.
http://www.cs.washington.edu/ai/alchemy.
Landauer, T. and S. Dumais (1997). A solution to Platos problem: the latent semantic analysis theory of
acquisition, induction, and representation of knowledge. Psychological Review 104(2), 211?240.
Lund, K. and C. Burgess (1996). Producing high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments, and Computers 28, 203?208.
MacCartney, B. and C. D. Manning (2009). An extended model of natural logic. In Proceedings of the
Eighth International Conference on Computational Semantics (IWCS-8), pp. 140?156.
Miller, G. A. (2009). Wordnet - about us. http://wordnet.princeton.edu.
Mitchell, J. and M. Lapata (2008). Vector-based models of semantic composition. In Proceedings of
ACL, pp. 236?244.
Nairn, R., C. Condoravdi, and L. Karttunen (2006). Computing relative polarity for textual inference. In
Proceedings of Inference in Computational Semantics (ICoS-5), Buxton, UK.
Poon, H. and P. Domingos (2009). Unsupervised semantic parsing. In Proceedings of EMNLP 2009, pp.
1?10.
Richardson, M. and P. Domingos (2006). Markov logic networks. Machine Learning 62, 107?136.
Thater, S., H. Fu?rstenau, and M. Pinkal (2010). Contextualizing semantic representations using syntac-
tically enriched vector models. In Proceedings of ACL 2010, Uppsala, Sweden, pp. 948?957.
114
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 7?11,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
Semantic Parsing using Distributional Semantics and Probabilistic Logic
Islam Beltagy
?
Katrin Erk
?
Raymond Mooney
?
?
Department of Computer Science
?
Department of Linguistics
The University of Texas at Austin
Austin, Texas 78712
?
{beltagy,mooney}@cs.utexas.edu
?
katrin.erk@mail.utexas.edu
Abstract
We propose a new approach to semantic
parsing that is not constrained by a fixed
formal ontology and purely logical infer-
ence. Instead, we use distributional se-
mantics to generate only the relevant part
of an on-the-fly ontology. Sentences and
the on-the-fly ontology are represented in
probabilistic logic. For inference, we
use probabilistic logic frameworks like
Markov Logic Networks (MLN) and Prob-
abilistic Soft Logic (PSL). This seman-
tic parsing approach is evaluated on two
tasks, Textual Entitlement (RTE) and Tex-
tual Similarity (STS), both accomplished
using inference in probabilistic logic. Ex-
periments show the potential of the ap-
proach.
1 Introduction
Semantic Parsing is probably best defined as the
task of representing the meaning of a natural lan-
guage sentence in some formal knowledge repre-
sentation language that supports automated infer-
ence. A semantic parser is best defined as having
three parts, a formal language, an ontology, and an
inference mechanism. Both the formal language
(e.g. first-order logic) and the ontology define the
formal knowledge representation. The formal lan-
guage uses predicate symbols from the ontology,
and the ontology provides them with meanings by
defining the relations between them.
1
. A formal
expression by itself without an ontology is insuf-
ficient for semantic interpretation; we call it un-
interpreted logical form. An uninterpreted logical
form is not enough as a knowledge representation
1
For conciseness, here we use the term ?ontology? to refer
to a set of predicates as well as a knowledge base (KB) of
axioms that defines a complex set of relationships between
them
because the predicate symbols do not have mean-
ing in themselves, they get this meaning from the
ontology. Inference is what takes a problem repre-
sented in the formal knowledge representation and
the ontology and performs the target task (e.g. tex-
tual entailment, question answering, etc.).
Prior work in standard semantic parsing uses a
pre-defined set of predicates in a fixed ontology.
However, it is difficult to construct formal ontolo-
gies of properties and relations that have broad
coverage, and very difficult to do semantic parsing
based on such an ontology. Consequently, current
semantic parsers are mostly restricted to fairly lim-
ited domains, such as querying a specific database
(Kwiatkowski et al., 2013; Berant et al., 2013).
We propose a semantic parser that is not re-
stricted to a predefined ontology. Instead, we
use distributional semantics to generate the needed
part of an on-the-fly ontology. Distributional se-
mantics is a statistical technique that represents
the meaning of words and phrases as distributions
over context words (Turney and Pantel, 2010; Lan-
dauer and Dumais, 1997). Distributional infor-
mation can be used to predict semantic relations
like synonymy and hyponymy between words and
phrases of interest (Lenci and Benotto, 2012;
Kotlerman et al., 2010). The collection of pre-
dicted semantic relations is the ?on-the-fly ontol-
ogy? our semantic parser uses. A distributional
semantics is relatively easy to build from a large
corpus of raw text, and provides the wide cover-
age that formal ontologies lack.
The formal language we would like to use in the
semantic parser is first-order logic. However, dis-
tributional information is graded in nature, so the
on-the-fly ontology and its predicted semantic re-
lations are also graded. This means, that standard
first-order logic is insufficient because it is binary
by nature. Probabilistic logic solves this problem
because it accepts weighted first order logic for-
mulas. For example, in probabilistic logic, the
7
synonymy relation between ?man? and ?guy? is
represented by: ?x. man(x) ? guy(x) | w
1
and
the hyponymy relation between ?car? and ?vehi-
cle? is: ?x. car(x) ? vehicle(x) | w
2
where w
1
and w
1
are some certainty measure estimated from
the distributional semantics.
For inference, we use probabilistic logic
frameworks like Markov Logic Networks
(MLN) (Richardson and Domingos, 2006) and
Probabilistic Soft Logic (PSL) (Kimmig et al.,
2012). They are Statistical Relational Learning
(SRL) techniques (Getoor and Taskar, 2007) that
combine logical and statistical knowledge in one
uniform framework, and provide a mechanism for
coherent probabilistic inference. We implemented
this semantic parser (Beltagy et al., 2013; Beltagy
et al., 2014) and used it to perform two tasks
that require deep semantic analysis, Recognizing
Textual Entailment (RTE), and Semantic Textual
Similarity (STS).
The rest of the paper is organized as follows:
section 2 presents background material, section
3 explains the three components of the semantic
parser, section 4 shows how this semantic parser
can be used for RTE and STS tasks, section 5
presents the evaluation and 6 concludes.
2 Background
2.1 Logical Semantics
Logic-based representations of meaning have a
long tradition (Montague, 1970; Kamp and Reyle,
1993). They handle many complex semantic phe-
nomena such as relational propositions, logical
operators, and quantifiers; however, they can not
handle ?graded? aspects of meaning in language
because they are binary by nature. Also, the logi-
cal predicates and relations do not have semantics
by themselves without an accompanying ontology,
which we want to replace in our semantic parser
with distributional semantics.
To map a sentence to logical form, we use Boxer
(Bos, 2008), a tool for wide-coverage semantic
analysis that produces uninterpreted logical forms
using Discourse Representation Structures (Kamp
and Reyle, 1993). It builds on the C&C CCG
parser (Clark and Curran, 2004).
2.2 Distributional Semantics
Distributional models use statistics on contextual
data from large corpora to predict semantic sim-
ilarity of words and phrases (Turney and Pantel,
2010; Mitchell and Lapata, 2010), based on the
observation that semantically similar words occur
in similar contexts (Landauer and Dumais, 1997;
Lund and Burgess, 1996). So words can be rep-
resented as vectors in high dimensional spaces
generated from the contexts in which they occur.
Distributional models capture the graded nature
of meaning, but do not adequately capture log-
ical structure (Grefenstette, 2013). It is possi-
ble to compute vector representations for larger
phrases compositionally from their parts (Lan-
dauer and Dumais, 1997; Mitchell and Lapata,
2008; Mitchell and Lapata, 2010; Baroni and
Zamparelli, 2010; Grefenstette and Sadrzadeh,
2011). Distributional similarity is usually a mix-
ture of semantic relations, but particular asymmet-
ric similarity measures can, to a certain extent,
predict hypernymy and lexical entailment distri-
butionally (Lenci and Benotto, 2012; Kotlerman
et al., 2010).
2.3 Markov Logic Network
Markov Logic Network (MLN) (Richardson and
Domingos, 2006) is a framework for probabilis-
tic logic that employ weighted formulas in first-
order logic to compactly encode complex undi-
rected probabilistic graphical models (i.e., Markov
networks). Weighting the rules is a way of soft-
ening them compared to hard logical constraints.
MLNs define a probability distribution over possi-
ble worlds, where a world?s probability increases
exponentially with the total weight of the logical
clauses that it satisfies. A variety of inference
methods for MLNs have been developed, however,
their computational complexity is a fundamental
issue.
2.4 Probabilistic Soft Logic
Probabilistic Soft Logic (PSL) is another recently
proposed framework for probabilistic logic (Kim-
mig et al., 2012). It uses logical representations to
compactly define large graphical models with con-
tinuous variables, and includes methods for per-
forming efficient probabilistic inference for the re-
sulting models. A key distinguishing feature of
PSL is that ground atoms have soft, continuous
truth values in the interval [0, 1] rather than bi-
nary truth values as used in MLNs and most other
probabilistic logics. Given a set of weighted in-
ference rules, and with the help of Lukasiewicz?s
relaxation of the logical operators, PSL builds a
graphical model defining a probability distribution
8
over the continuous space of values of the random
variables in the model. Then, PSL?s MPE infer-
ence (Most Probable Explanation) finds the over-
all interpretation with the maximum probability
given a set of evidence. It turns out that this op-
timization problem is second-order cone program
(SOCP) (Kimmig et al., 2012) and can be solved
efficiently in polynomial time.
2.5 Recognizing Textual Entailment
Recognizing Textual Entailment (RTE) is the task
of determining whether one natural language text,
the premise, Entails, Contradicts, or not related
(Neutral) to another, the hypothesis.
2.6 Semantic Textual Similarity
Semantic Textual Similarity (STS) is the task of
judging the similarity of a pair of sentences on
a scale from 1 to 5 (Agirre et al., 2012). Gold
standard scores are averaged over multiple human
annotations and systems are evaluated using the
Pearson correlation between a system?s output and
gold standard scores.
3 Approach
A semantic parser is three components, a formal
language, an ontology, and an inference mecha-
nism. This section explains the details of these
components in our semantic parser. It also points
out the future work related to each part of the sys-
tem.
3.1 Formal Language: first-order logic
Natural sentences are mapped to logical form us-
ing Boxer (Bos, 2008), which maps the input
sentences into a lexically-based logical form, in
which the predicates are words in the sentence.
For example, the sentence ?A man is driving a car?
in logical form is:
?x, y, z. man(x) ? agent(y, x) ? drive(y) ?
patient(y, z) ? car(z)
We call Boxer?s output alone an uninterpreted
logical form because predicates do not have mean-
ing by themselves. They still need to be connected
with an ontology.
Future work: While Boxer has wide coverage,
additional linguistic phenomena like generalized
quantifiers need to be handled.
3.2 Ontology: on-the-fly ontology
Distributional information is used to generate the
needed part of an on-the-fly ontology for the given
input sentences. It is encoded in the form of
weighted inference rules describing the seman-
tic relations connecting words and phrases in the
input sentences. For example, for sentences ?A
man is driving a car?, and ?A guy is driving a
vehicle?, we would like to generate rules like
?x.man(x)? guy(x) |w
1
indicating that ?man?
and ?guy? are synonyms with some certainty w
1
,
and ?x. car(x)? vehicle(x) | w
2
indicating that
?car? is a hyponym of ?vehicle? with some cer-
tainty w
2
. Other semantic relations can also be
easily encoded as inference rules like antonyms
?x. tall(x)? ?short(x) |w, contextonymy rela-
tion ?x. hospital(x) ? ?y. doctor(y) | w. For
now, we generate inference rules only as syn-
onyms (Beltagy et al., 2013), but we are experi-
menting with more types of semantic relations.
In (Beltagy et al., 2013), we generate infer-
ence rules between all pairs of words and phrases.
Given two input sentences T and H , for all pairs
(a, b), where a and b are words or phrases of T
and H respectively, generate an inference rule:
a ? b | w, where the rule?s weight w =
sim(
??
a ,
??
b ), and sim is the cosine of the angle
between vectors
??
a and
??
b . Note that this simi-
larity measure cannot yet distinguish relations like
synonymy and hypernymy. Phrases are defined in
terms of Boxer?s output to be more than one unary
atom sharing the same variable like ?a little kid?
which in logic is little(k) ? kid(k), or two unary
atoms connected by a relation like ?a man is driv-
ing? which in logic is man(m) ? agent(d,m) ?
drive(d). We used vector addition (Mitchell and
Lapata, 2010) to calculate vectors for phrases.
Future Work: This can be extended in many
directions. We are currently experimenting with
asymmetric similarity functions to distinguish se-
mantic relations. We would also like to use longer
phrases and other compositionality techniques as
in (Baroni and Zamparelli, 2010; Grefenstette and
Sadrzadeh, 2011). Also more inference rules can
be added from paraphrases collections like PPDB
(Ganitkevitch et al., 2013).
3.3 Inference: probabilistic logical inference
The last component is probabilistic logical infer-
ence. Given the logical form of the input sen-
tences, and the weighted inference rules, we use
them to build a probabilistic logic program whose
solution is the answer to the target task. A proba-
bilistic logic program consists of the evidence set
9
E, the set of weighted first order logical expres-
sions (rule base RB), and a query Q. Inference is
the process of calculating Pr(Q|E,RB).
Probabilistic logic frameworks define a proba-
bility distribution over all possible worlds. The
number of constants in a world depends on the
number of the discourse entities in the Boxer out-
put, plus additional constants introduced to han-
dle quantification. Mostly, all constants are com-
bined with all literals, except for rudimentary type
checking.
4 Tasks
This section explains how we perform the RTE
and STS tasks using our semantic parser.
4.1 Task 1: RTE using MLNs
MLNs are the probabilistic logic framework we
use for the RTE task (we do not use PSL here as
it shares the problems of fuzzy logic with proba-
bilistic reasoning). The RTE?s classification prob-
lem for the relation between T and H , and given
the rule base RB generated as in 3.2, can be
split into two inference tasks. The first is find-
ing if T entails H , Pr(H|T,RB). The second
is finding if the negation of the text ?T entails H ,
Pr(H|?T,RB). In case Pr(H|T,RB) is high,
while Pr(H|?T,RB) is low, this indicates En-
tails. In case it is the other way around, this indi-
cates Contradicts. If both values are close to each
other, this means T does not affect probability of
H and that is an indication of Neutral. We train a
classifier to map the two values to the final classi-
fication decision.
Future Work: One general problem with
MLNs is its computational overhead especially
for the type of inference problems we have. The
other problem is that MLNs, as with most other
probabilistic logics, make the Domain Closure
Assumption (Richardson and Domingos, 2006)
which means that quantifiers sometimes behave in
an undesired way.
4.2 Task 2: STS using PSL
PSL is the probabilistic logic we use for the STS
task since it has been shown to be an effective
approach to compute similarity between struc-
tured objects. PSL does not work ?out of the
box? for STS, because Lukasiewicz?s equation for
the conjunction is very restrictive. We addressed
this problem (Beltagy et al., 2014) by replacing
SICK-RTE SICK-STS
dist 0.60 0.65
logic 0.71 0.68
logic+dist 0.73 0.70
Table 1: RTE accuracy and STS Correlation
Lukasiewicz?s equation for the conjunction with
an averaging equation, then change the optimiza-
tion problem and the grounding technique accord-
ingly.
For each STS pair of sentences S
1
, S
2
, we run
PSL twice, once where E = S
1
, Q = S
2
and
another where E = S
2
, Q = S
1
, and output the
two scores. The final similarity score is produced
from a regressor trained to map the two PSL scores
to the overall similarity score.
Future Work: Use a weighted average where
different weights are learned for different parts of
the sentence.
5 Evaluation
The dataset used for evaluation is SICK:
Sentences Involving Compositional Knowledge
dataset, a task for SemEval 2014. The initial data
release for the competition consists of 5,000 pairs
of sentences which are annotated for both RTE and
STS. For this evaluation, we performed 10-fold
cross validation on this initial data.
Table 1 shows results comparing our full
approach (logic+dist) to two baselines, a
distributional-only baseline (dist) that uses vector
addition, and a probabilistic logic-only baseline
(logic) which is our semantic parser without distri-
butional inference rules. The integrated approach
(logic+dist) out-performs both baselines.
6 Conclusion
We presented an approach to semantic parsing that
has a wide-coverage for words and relations, and
does not require a fixed formal ontology. An
on-the-fly ontology of semantic relations between
predicates is derived from distributional informa-
tion and encoded in the form of soft inference rules
in probabilistic logic. We evaluated this approach
on two task, RTE and STS, using two probabilistic
logics, MLNs and PSL respectively. The semantic
parser can be extended in different direction, es-
pecially in predicting more complex semantic re-
lations, and enhancing the inference mechanisms.
10
Acknowledgments
This research was supported by the DARPA DEFT
program under AFRL grant FA8750-13-2-0026.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the author and do not necessarily reflect the view
of DARPA, DoD or the US government. Some ex-
periments were run on the Mastodon Cluster sup-
ported by NSF Grant EIA-0303609.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of Semantic Evaluation (SemEval-12).
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of Conference on Empirical Methods in
Natural Language Processing (EMNLP-10).
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
Montague meets Markov: Deep semantics with
probabilistic logical form. In Proceedings of the
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM-13).
Islam Beltagy, Katrin Erk, and Raymond Mooney.
2014. Probabilistic soft logic for semantic textual
similarity. In Proceedings of Association for Com-
putational Linguistics (ACL-14).
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP-13).
Johan Bos. 2008. Wide-coverage semantic analysis
with Boxer. In Proceedings of Semantics in Text
Processing (STEP-08).
Stephen Clark and James R. Curran. 2004. Parsing
the WSJ using CCG and log-linear models. In Pro-
ceedings of Association for Computational Linguis-
tics (ACL-04).
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT-13).
L. Getoor and B. Taskar, editors. 2007. Introduction
to Statistical Relational Learning. MIT Press, Cam-
bridge, MA.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of Conference on Empirical Methods in Natural
Language Processing (EMNLP-11).
Edward Grefenstette. 2013. Towards a formal distri-
butional semantics: Simulating logical calculi with
tensors. In Proceedings of Second Joint Conference
on Lexical and Computational Semantics (*SEM
2013).
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer.
Angelika Kimmig, Stephen H. Bach, Matthias
Broecheler, Bert Huang, and Lise Getoor. 2012.
A short introduction to Probabilistic Soft Logic.
In Proceedings of NIPS Workshop on Probabilistic
Programming: Foundations and Applications (NIPS
Workshop-12).
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-13).
T. K. Landauer and S. T. Dumais. 1997. A solution to
Plato?s problem: The Latent Semantic Analysis the-
ory of the acquisition, induction, and representation
of knowledge. Psychological Review.
Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of the first Joint Conference on Lexical
and Computational Semantics (*SEM-12).
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of Association for Computational Linguistics (ACL-
08).
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Journal of
Cognitive Science.
Richard Montague. 1970. Universal grammar. Theo-
ria, 36:373?398.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning,
62:107?136.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research
(JAIR-10).
11
Proceedings of Frame Semantics in NLP: A Workshop in Honor of Chuck Fillmore (1929?2014), pages 34?37,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Who evoked that frame? Some thoughts on context effects and event types
Katrin Erk
Department of Linguistics
The University of Texas at Austin
Austin, Texas 78712
katrin.erk@mail.utexas.edu
In memoriam Charles Fillmore, 1929-2014
Abstract
Lexical substitution is an annotation task
in which annotators provide one-word
paraphrases (lexical substitutes) for indi-
vidual target words in a sentence context.
Lexical substitution yields a fine-grained
characterization of word meaning that can
be done by non-expert annotators. We dis-
cuss results of a recent lexical substitution
annotation effort, where we found strong
contextual modulation effects: Many sub-
stitutes were not synonyms, hyponyms or
hypernyms of the targets, but were highly
specific to the situation at hand. This data
provides some food for thought for frame-
semantic analysis.
1 Introduction
Fillmore (1985) introduces the term ?semantics of
understanding?, or U-semantics. In contrast to the
semantics of truth (T-semantics), the goal of U-
semantics is to ?uncover the nature of the relation-
ship between linguistic texts and the interpreter?s
full understanding of the texts in their contexts?. A
central concept of the semantics of understanding
is that of the interpretive frames that are necessary
for understanding a sentence. Frames are the ?co-
herent schematizations of experience? underlying
the words in a given sentence.
This idea of a semantics of understanding, or
a frame semantics, has been made concrete in
FrameNet (Fillmore et al., 2003), a large lexical
database that lists frames for English words and
constructions. At this point, it comprises more
than 1,100 frames covering more than 12,000 lex-
ical units (LUs), which are pairs of a term and its
frame. Researchers working on other languages
have adopted the FrameNet idea. Among oth-
ers, there are now FrameNet resources for Span-
ish (Subirats and Petruck, 2003), Japanese (Ohara
et al., 2004), Italian (Tonelli and Pianta, 2008;
Lenci et al., 2010), as well as frame-semantic an-
notation for German (Erk et al., 2003).
The definition of frames proceeds in a corpus-
based fashion, driven by the data (Ellsworth et al.,
2004). We stand in this tradition by reporting on a
recent annotation effort (Kremer et al., 2014) that
collected lexical substitutes for content words in
part of the MASC corpus (Ide et al., 2008). If we
view substitute sets as indications of the relevant
frame, then this data can give us interesting indi-
cators on perceived frames in a naturally occurring
text.
2 Lexical substitution
The Lexical Substitution task was first introduced
in the context of SemEval 2007 (McCarthy and
Navigli, 2009). For this dataset, annotators are
asked to provide substitutes for a selected word
(the target word) in its sentence context ? at least
one substitute, but possible more, and ideally a
single word, though all the datasets contain some
multi-word substitutes. Multiple annotators pro-
vide substitutes for each target word occurrence.
Table 1 shows some examples.
By now, several lexical substitution datasets ex-
ist. Some are ?lexical sample? datasets, that is,
only occurrences of some selected lemmas are an-
notated (McCarthy and Navigli, 2009; Biemann,
2013), and some are ?all-words?, providing sub-
stitutes for all content words in the given sen-
tences (Sinha and Mihalcea, 2014; Kremer et al.,
2014). In addition, there is a cross-lingual lex-
ical substitution dataset (McCarthy et al., 2013),
where annotators provided Spanish substitutes for
English target words in English sentence context.
Lexical substitution is a method for character-
izing word meaning in context that has several at-
tractive properties. Lexical substitution makes it
possible to describe word meaning without hav-
ing to rely on any particular dictionary. In addi-
34
relation verb noun
syn 12.5 7.7
direct-hyper 9.3 7.6
trans-hyper 2.8 4.7
direct-hypo 11.6 8.0
trans-hypo 3.7 3.8
wn-other 60.7 66.5
not-in-wn 0.9 2.2
Table 2: Analysis of lexical substitution data: Re-
lation of the substitute to the target, in percentages
by part of speech (from Kremer et al. (2014))
tion, providing substitutes is a task that seems to
be well doable by untrained annotators: Both Bie-
mann (2013) and our recent annotation (Kremer et
al., 2014) used crowdsourcing to collect the sub-
stitutes.
1
3 Analyzing lexical substitutes
In a recent lexical substitution annotation ef-
fort (Kremer et al., 2014), we collected lexical
substitution annotation for all nouns, verbs, and
adjectives in a mixed news and fiction corpus, us-
ing untrained annotators via crowdsourcing. The
data came from MASC, a freely available part of
the American National Corpus that has already
been annotated for a number of linguistic phenom-
ena (Ide et al., 2008). All in all, more than 15,000
target tokens were annotated.
After the annotation, we performed a number of
analyses in order to better understand the nature
of lexical substitutes, by linking substitutes to in-
formation on WordNet (Fellbaum, 1998). Among
other things, we analyzed the relation between tar-
gets and substitutes: Did substitutes tend to be
synonyms, hypernyms, or hyponyms or the tar-
gets? To classify substitutes, the shortest route
from any synset of the target to any synset of
the substitute was used. The results are shown
in Table 2, for substitutes that are synonyms
(syn), hypernyms (direct-hyper, trans-hyper) and
hyponyms (direct-hypo, trans-hypo) of the target.
The ?wn-other? line shows the percentage for sub-
stitutes that are in WordNet but not a synonym,
hypo- or hypernym of the target, and ?not-in-wn?
1
The third example in Table 1 shows that errors do hap-
pen: The substitute ?accusation? is not appropriate there.
Analyses indicate that such errors are rare, though.
are substitutes not covered by WordNet. For sub-
stitutes that are synonyms, hypernyms, and hy-
ponyms, we see percentages between 8% and 15%
for both verbs and nouns. We also see that there
are few substitutes that are not in WordNet, only
1-2%. Strikingly, 60-66% of all substitutes are in
WordNet, but are ?wn-other?: neither synonyms
nor (transitive) hyponyms or hypernyms of the tar-
get. Some of these items can be viewed as missing
links in the taxonomy. For example, in the second
sentence of Table 2, two of the ?wn-other? sub-
stitutes of keep are own and possess. But while
own and possess are not linked to keep in Word-
Net, the FrameNet frame RETAINING, which has
keep as a lexical unit, inherits from POSSESSION,
which has both own and possess as lexical units.
But this does not apply to all the ?wn-other? sub-
stitutes. Some are better explained as effects of
contextual modulation, fine-grained meaning dis-
tinctions that the sentence context brings about. In
the first example in Table 1, there is the possibility
that the speaker could be laughing at the other per-
son, and the shoulder-clapping clarifies that this
possibility does not correspond to the facts. In
the second example in the table, the words pos-
sess, enshrine and stage are more specific than the
substitutes that are in WordNet, and maybe more
appropriate too. In the third example, the word
charge has the meaning of dependent, but the situ-
ation that the sentence describes suggests that the
dependents in questions may be something like
underlings or prisoners.
When we look at this data from a frame-
semantic analysis point of view, the first question
that arises is: How specific should the frames be
that are listed in FrameNet? For the second ex-
ample, would we want a very specific ?person as
precious jewel? frame to be associated with the
lexical unit ?keep?? From a U-semantics point of
view, one could argue that we would in fact want
to have this frame, after all: It describes a rec-
ognizable abstract situation that is important for
the understanding of this sentence. But it does not
seem that all ?wn-other? cases need to correspond
to particular frames of the target word. For ex-
ample, in the first sentence on Table 1, it does not
seem that clarify should be an actual frame involv-
ing the word show.
From a computational linguistics point of view,
a fine-grained analysis would be necessary in or-
der to correctly predict lexical substitutes like
35
sentence substitutes
I clapped her shoulder to show I was not laughing at her. demonstrate, express, establish, indicate, prove,
convey, imply, display, disclose, clarify
My fear is that she would live, and I would learn that I had lost her long
before Emil Malaquez translated her into a thing that can be kept, ad-
mired, and loved.
preserve, retain, hold, fix, store, own, possess,
enshrine, stage
The distinctive whuffle of pleasure rippled through the betas on the
bridge, and Rakal let loose a small growl, as if to caution his charges
against false hope.
dependent, command, accusation, private, com-
panion, follower, subordinate, prisoner, team-
mate, ward, junior, underling, enemy, group,
crew, squad, troop, team, kid
Table 1: Example from lexical substitution data: Target words underlined, and WordNet-unrelated sub-
stitutes shown in italics.
this ? but on the other hand, experience with
word sense disambiguation has shown that fine-
grained senses are hard to assign with good accu-
racy (Palmer et al., 2007).
Another question that this data poses is: What
are the items that evoke a frame? That is, what
words or phrases in a sentence are responsible that
a particular frame becomes important for under-
standing the sentence? In FrameNet it is a sin-
gle lemma, multi-word expression or construction
that evokes a frame. But one way of looking at
the contextual modulation effects in the lexical
substitution data is to say that multiple terms or
constructions in the context ?conspire? to make
a frame relevant. In the second sentence of Ta-
ble 1, we can point to multiple factors that lead
to substitutes like possess and enshrine. There is
fact that the THEME of keep is thing, along with
the fact that the same thing is being admired and
loved, and maybe also the fact that some woman
had been translated to said thing. This thought is
reminiscent of McRae and colleagues, who study
general event knowledge and argue that it is not
just verbs that introduce the events, but also argu-
ments (McRae et al., 2005) and combinations of
verbs and their arguments (Bicknell et al., 2010).
References
K. Bicknell, J. Elman, M. Hare, K. McRae, and M. Ku-
tas. 2010. Effects of event knowledge in process-
ing verbal arguments. Journal of Memory and Lan-
guage, 63(4):489?505.
C. Biemann. 2013. Creating a system for lexical sub-
stitutions from scratch using crowdsourcing. Lan-
guage Resources and Evaluation, 47(1):97?122.
M. Ellsworth, K. Erk, P. Kingsbury, and S. Pad?o. 2004.
PropBank, SALSA and FrameNet: How design de-
termines product. In Proceedings of the LREC
Workshop on Building Lexical Resources From Se-
mantically Annotated Corpora, Lisbon, Portugal.
K. Erk, A. Kowalski, S. Pad?o, and M. Pinkal. 2003.
Towards a resource for lexical semantics: A large
German corpus with extensive semantic annotation.
In Proceedings of ACL, Sapporo, Japan.
C. Fellbaum, editor. 1998. WordNet: An electronic
lexical database. MIT Press, Cambridge, MA.
C. J. Fillmore, C. Johnson, and M. Petruck. 2003.
Background to framenet. International Journal of
Lexicography, 16(3):235?250.
C. J. Fillmore. 1985. Frames and the semantics of
understanding. Quaderni di Semantica, 6:222?254.
N. Ide, C.F. Baker, C. Fellbaum, C. Fillmore, and
R. Passonneau. 2008. MASC: The manually anno-
tated sub-corpus of American English. In Proceed-
ings of LREC, pages 2455?2461, Marrakech, Mo-
rocco.
G. Kremer, K. Erk, S. Pad?o, and S. Thater. 2014. What
substitutes tell us - analysis of an ?all-words? lexical
substitution corpus. In Proceedings of EACL.
A. Lenci, M. Johnson, and G. Lapesa. 2010. Building
an italian framenet through semi-automatic corpus
analysis. In Proceedings of LREC, La Valletta.
D. McCarthy and R. Navigli. 2009. The English lexi-
cal substitution task. Language Resources and Eval-
uation, 43(2):139?159.
D. McCarthy, R. Sinha, and R. Mihalcea. 2013. The
cross-lingual lexical substitution task. Language
Resources and Evaluation, 47(3):607?638.
K. McRae, M. Hare, J. L. Elman, and T. Ferretti. 2005.
A basis for generating expectancies for verbs from
nouns. Memory & Cognition, 33(7):1174?1184.
K. Ohara, S. Fujii, T. Ohori, R. Suzuki, H. Saito, and
S. Ishizaki. 2004. The Japanese FrameNet project:
An introduction. In Proceedings of the LREC Work-
shop on Building Lexical Resources from Semanti-
cally Annotated Corpora, Lisbon, Portugal.
M. Palmer, H. Trang Dang, and C. Fellbaum. 2007.
Making fine-grained and coarse-grained sense dis-
tinctions, both manually and automatically. Natural
Language Engineering, 13:137?163.
36
R. Sinha and R. Mihalcea. 2014. Explorations in lexi-
cal sample and all-words lexical substitution. Natu-
ral Language Engineering, 20(1):99?129.
C. Subirats and M.R.L. Petruck. 2003. Surprise! Span-
ish FrameNet! In Proceedings of the Workshop on
Frame Semantics, XVII. International Congress of
Linguists, Prague, Czech Republic.
S. Tonelli and E. Pianta. 2008. Frame information
transfer from english to italian. In Proceedings of
LREC, Marrakech, Morocco.
37
