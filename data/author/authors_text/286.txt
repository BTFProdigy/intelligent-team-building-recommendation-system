A Multiclassifier based Document Categorization System: profiting from
the Singular Value Decomposition Dimensionality Reduction Technique
Ana Zelaia
UPV-EHU
Basque Country
ccpzejaa@si.ehu.es
In?aki Alegria
UPV-EHU
Basque Country
acpalloi@si.ehu.es
Olatz Arregi
UPV-EHU
Basque Country
acparuro@si.ehu.es
Basilio Sierra
UPV-EHU
Basque Country
ccpsiarb@si.ehu.es
Abstract
In this paper we present a multiclassifier
approach for multilabel document classifi-
cation problems, where a set of k-NN clas-
sifiers is used to predict the category of
text documents based on different training
subsampling databases. These databases
are obtained from the original training
database by random subsampling. In or-
der to combine the predictions generated
by the multiclassifier, Bayesian voting is
applied. Through all the classification pro-
cess, a reduced dimension vector represen-
tation obtained by Singular Value Decom-
position (SVD) is used for training and
testing documents. The good results of our
experiments give an indication of the po-
tentiality of the proposed approach.
1 Introduction
Document Categorization, the assignment of nat-
ural language texts to one or more predefined
categories based on their content, is an impor-
tant component in many information organization
and management tasks. Researchers have con-
centrated their efforts in finding the appropriate
way to represent documents, index them and con-
struct classifiers to assign the correct categories to
each document. Both, document representation
and classification method are crucial steps in the
categorization process.
In this paper we concentrate on both issues. On
the one hand, we use Latent Semantic Indexing
(LSI) (Deerwester et al, 1990), which is a vari-
ant of the vector space model (VSM) (Salton and
McGill, 1983), in order to obtain the vector rep-
resentation of documents. This technique com-
presses vectors representing documents into vec-
tors of a lower-dimensional space. LSI, which
is based on Singular Value Decomposition (SVD)
of matrices, has showed to have the ability to ex-
tract the relations among words and documents by
means of their context of use, and has been suc-
cessfully applied to Information Retrieval tasks.
On the other hand, we construct a multiclassi-
fier (Ho et al, 1994) which uses different train-
ing databases. These databases are obtained from
the original training set by random subsampling.
We implement this approach by bagging, and use
the k-NN classification algorithm to make the cat-
egory predictions for testing documents. Finally,
we combine all predictions made for a given doc-
ument by Bayesian voting.
The experiment we present has been evaluated
for Reuters-21578 standard document collection.
Reuters-21578 is a multilabel document collec-
tion, which means that categories are not mutu-
ally exclusive because the same document may be
relevant to more than one category. Being aware
of the results published in the most recent litera-
ture, and having obtained good results in our ex-
periments, we consider the categorization method
presented in this paper an interesting contribution
for text categorization tasks.
The remainder of this paper is organized as fol-
lows: Section 2, discusses related work on docu-
ment categorization for Reuters-21578 collection.
In Section 3, we present our approach to deal with
the multilabel text categorization task. In Section
4 the experimental setup is introduced, and details
about the Reuters database, the preprocessing ap-
plied and some parameter setting are provided. In
Section 5, experimental results are presented and
discussed. Finally, Section 6 contains some con-
clusions and comments on future work.
25
2 Related Work
As previously mentioned in the introduction, text
categorization consists in assigning predefined
categories to text documents. In the past two
decades, document categorization has received
much attention and a considerable number of ma-
chine learning based approaches have been pro-
posed. A good tutorial on the state-of-the-art of
document categorization techniques can be found
in (Sebastiani, 2002).
In the document categorization task we can find
two cases; (1) the multilabel case, which means
that categories are not mutually exclusive, because
the same document may be relevant to more than
one category (1 to m category labels may be as-
signed to the same document, being m the to-
tal number of predefined categories), and (2) the
single-label case, where exactly one category is
assigned to each document. While most machine
learning systems are designated to handle multi-
class data1, much less common are systems that
can handle multilabel data.
For experimentation purposes, there are stan-
dard document collections available in the pub-
lic domain that can be used for document catego-
rization. The most widely used is Reuters-21578
collection, which is a multiclass (135 categories)
and multilabel (the mean number of categories as-
signed to a document is 1.2) dataset. Many ex-
periments have been carried out for the Reuters
collection. However, they have been performed in
different experimental conditions. This makes re-
sults difficult to compare among them. In fact, ef-
fectiveness results can only be compared between
studies that use the same training and testing sets.
In order to lead researchers to use the same train-
ing/testing divisions, the Reuters documents have
been specifically tagged, and researchers are en-
couraged to use one of those divisions. In our
experiment we use the ?ModApte? split (Lewis,
2004).
In this section, we analize the category sub-
sets, evaluation measures and results obtained in
the past and in the recent years for Reuters-21578
ModApte split.
2.1 Category subsets
Concerning the evaluation of the classification
system, we restrict our attention to the TOPICS
1Categorization problems where there are more than two
possible categories.
group of categories that labels Reuters dataset,
which contains 135 categories. However, many
categories appear in no document and conse-
quently, and because inductive based learning
classifiers learn from training examples, these cat-
egories are not usually considered at evaluation
time. The most widely used subsets are the fol-
lowing:
? Top-10: It is the set of the 10 categories
which have the highest number of documents
in the training set.
? R(90): It is the set of 90 categories which
have at least one document in the training set
and one in the testing set.
? R(115): It is the set of 115 categories which
have at least one document in the training set.
In order to analyze the relative hardness of the
three category subsets, a very recent paper has
been published by Debole and Sebastiani (Debole
and Sebastiani, 2005) where a systematic, compar-
ative experimental study has been carried out.
The results of the classification system we pro-
pose are evaluated according to these three cate-
gory subsets.
2.2 Evaluation measures
The evaluation of a text categorization system is
usually done experimentally, by measuring the ef-
fectiveness, i.e. average correctness of the catego-
rization. In binary text categorization, two known
statistics are widely used to measure this effective-
ness: precision and recall. Precision (Prec) is the
percentage of documents correctly classified into a
given category, and recall (Rec) is the percentage
of documents belonging to a given category that
are indeed classified into it.
In general, there is a trade-off between preci-
sion and recall. Thus, a classifier is usually evalu-
ated by means of a measure which combines pre-
cision and recall. Various such measures have
been proposed. The breakeven point, the value at
which precision equals recall, has been frequently
used during the past decade. However, it has
been recently criticized by its proposer ((Sebas-
tiani, 2002) footnote 19). Nowadays, the F1 score
is more frequently used. The F1 score combines
recall and precision with an equal weight in the
following way:
F1 =
2 ? Prec ? Rec
Prec + Rec
26
Since precision and recall are defined only for
binary classification tasks, for multiclass problems
results need to be averaged to get a single perfor-
mance value. This will be done using microav-
eraging and macroaveraging. In microaveraging,
which is calculated by globally summing over all
individual cases, categories count proportionally
to the number of their positive testing examples.
In macroaveraging, which is calculated by aver-
aging over the results of the different categories,
all categories count the same. See (Debole and
Sebastiani, 2005; Yang, 1999) for more detailed
explanation of the evaluation measures mentioned
above.
2.3 Comparative Results
Sebastiani (Sebastiani, 2002) presents a table
where lists results of experiments for various train-
ing/testing divisions of Reuters. Although we are
aware that the results listed are microaveraged
breakeven point measures, and consequently, are
not directly comparable to the ones we present in
this paper, F1, we want to remark some of them.
In Table 1 we summarize the best results reported
for the ModApte split listed by Sebastiani.
Results reported by R(90) Top-10
(Joachims, 1998) 86.4
(Dumais et al, 1998) 87.0 92.0
(Weiss et.al., 1999) 87.8
Table 1: Microaveraged breakeven point results
reported by Sebastiani for the Reuters-21578
ModApte split.
In Table 2 we include some more recent re-
sults, evaluated according to the microaveraged
F1 score. For R(115) there is also a good result,
F1 = 87.2, obtained by (Zhang and Oles, 2001)2.
3 Proposed Approach
In this paper we propose a multiclassifier based
document categorization system. Documents in
the training and testing sets are represented in a
reduced dimensional vector space. Different train-
ing databases are generated from the original train-
2Actually, this result is obtained for 118 categories which
correspond to the 115 mentioned before and three more cat-
egories which have testing documents but no training docu-
ment assigned.
Results reported by R(90) Top-10
(Gao et al, 2003) 88.42 93.07
(Kim et al, 2005) 87.11 92.21
(Gliozzo and Strapparava, 2005) 92.80
Table 2: F1 results reported for the Reuters-21578
ModApte split.
ing dataset in order to construct the multiclassifier.
We use the k-NN classification algorithm, which
according to each training database makes a pre-
diction for testing documents. Finally, a Bayesian
voting scheme is used in order to definitively as-
sign category labels to testing documents.
In the rest of this section we make a brief re-
view of the SVD dimensionality reduction tech-
nique, the k-NN algorithm and the combination of
classifiers used.
3.1 The SVD Dimensionality Reduction
Technique
The classical Vector SpaceModel (VSM) has been
successfully employed to represent documents in
text categorization tasks. The newer method of
Latent Semantic Indexing (LSI) 3 (Deerwester et
al., 1990) is a variant of the VSM in which doc-
uments are represented in a lower dimensional
space created from the input training dataset. It
is based on the assumption that there is some
underlying latent semantic structure in the term-
document matrix that is corrupted by the wide va-
riety of words used in documents. This is referred
to as the problem of polysemy and synonymy. The
basic idea is that if two document vectors represent
two very similar topics, many words will co-occur
on them, and they will have very close semantic
structures after dimension reduction.
The SVD technique used by LSI consists in fac-
toring term-document matrix M into the product
of three matrices, M = U?V T where ? is a di-
agonal matrix of singular values in non-increasing
order, and U and V are orthogonal matrices of sin-
gular vectors (term and document vectors, respec-
tively). Matrix M can be approximated by a lower
rank Mp which is calculated by using the p largest
singular values of M . This operation is called
dimensionality reduction, and the p-dimensional
3http://lsi.research.telcordia.com,
http://www.cs.utk.edu/?lsi
27
space to which document vectors are projected is
called the reduced space. Choosing the right di-
mension p is required for successful application
of the LSI/SVD technique. However, since there
is no theoretical optimum value for it, potentially
expensive experimentation may be required to de-
termine it (Berry and Browne, 1999).
For document categorization purposes (Dumais,
2004), the testing document q is also projected to
the p-dimensional space, qp = qTUp??1p , and the
cosine is usually calculated to measure the seman-
tic similarity between training and testing docu-
ment vectors.
In Figure 1 we can see an ilustration of the doc-
ument vector projection. Documents in the train-
ing collection are represented by using the term-
document matrix M , and each one of the docu-
ments is represented by a vector in the Rm vec-
tor space like in the traditional vector space model
(VSM) scheme. Afterwards, the dimension p is se-
lected, and by applying SVD vectors are projected
to the reduced space. Documents in the testing
collection will also be projected to the same re-
duced space.
d1 d2
d3
d4d5
d2
d3
d4
d5d6d7
d9603 d1
d9603
d6d7
...
...
Rm
Reuters-21578, ModApte, Training
VSM
SVD
M
R p
d1 d2 d9603
Mp = Up?pV Tp
Figure 1: Vectors in the VSM are projected to the
reduced space by using SVD.
3.2 The k nearest neighbor classification
algorithm (k-NN)
k-NN is a distance based classification approach.
According to this approach, given an arbitrary test-
ing document, the k-NN classifier ranks its near-
est neighbors among the training documents, and
uses the categories of the k top-ranking neighbors
to predict the categories of the testing document
(Dasarathy, 1991). In this paper, the training and
testing documents are represented as reduced di-
mensional vectors in the lower dimensional space,
and in order to find the nearest neighbors of a
given document, we calculate the cosine similar-
ity measure.
In Figure 2 an ilustration of this phase can be
seen, where some training documents and a test-
ing document q are projected in the R p reduced
space. The nearest to the qp testing document are
considered to be the vectors which have the small-
est angle with qp. According to the category labels
of the nearest documents, a category label predic-
tion, c, will be made for testing document q.
d34
d61
d23
d135
d509
k?NN
R p
c
qp
Figure 2: The k-NN classifier is applied to qp test-
ing document and c category label is predicted.
We have decided to use the k-NN classifier be-
cause it has been found that on the Reuters-21578
database it performs best among the conventional
methods (Joachims, 1998; Yang, 1999) and be-
cause we have obtained good results in our pre-
vious work on text categorization for documents
written in Basque, a highly inflected language (Ze-
laia et al, 2005). Besides, the k-NN classification
algorithm can be easily adapted to multilabel cat-
egorization problems such as Reuters.
3.3 Combination of classifiers
The combination of multiple classifiers has been
intensively studied with the aim of improving the
accuracy of individual components (Ho et al,
1994). Two widely used techniques to implement
this approach are bagging (Breiman, 1996), that
uses more than one model of the same paradigm;
and boosting (Freund and Schapire, 1999), in
which a different weight is given to different train-
ing examples looking for a better accuracy.
In our experiment we have decided to construct
a multiclassifier via bagging. In bagging, a set of
training databases TDi is generated by selecting n
training examples drawn randomly with replace-
ment from the original training database TD of n
examples. When a set of n1 training examples,
28
n1 < n, is chosen from the original training col-
lection, the bagging is said to be applied by ran-
dom subsampling. This is the approach used in our
work. The n1 parameter has been selected via tun-
ing. In Section 4.3 the selection will be explained
in a more extended way.
According to the random subsampling, given a
testing document q, the classifier will make a la-
bel prediction ci based on each one of the train-
ing databases TDi. One way to combine the pre-
dictions is by Bayesian voting (Dietterich, 1998),
where a confidence value cvicj is calculated for
each training database TDi and category cj to be
predicted. These confidence values have been cal-
culated based on the original training collection.
Confidence values are summed by category. The
category cj that gets the highest value is finally
proposed as a prediction for the testing document.
In Figure 3 an ilustration of the whole ex-
periment can be seen. First, vectors in the
VSM are projected to the reduced space by using
SVD. Next, random subsampling is applied to the
training database TD to obtain different training
databases TDi. Afterwards the k-NN classifier is
applied for each TDi to make category label pre-
dictions. Finally, Bayesian voting is used to com-
bine predictions, and cj , and in some cases ck as
well, will be the final category label prediction of
the categorization system for testing document q.
In Section 4.3 the cases when a second category
label prediction ck is given are explained.
d1 d2 d9603
d1 d2
d3
d4d5
d2
d3
d4
d5d6d7
d9603 d1
Reuters?21578, ModApte, Test
d9603
d6d7
...
q1 q2 q3299
q
q
d34
d61
d23
d135
d509
TD2TD1 TD30
Reuters?21578, ModApte, Train
...
k?NN k?NN
d50
d778
d848d638d256
d98
d2787
d33
d1989
d55
d4612
d9
VSM
VSM
SVD
k?NN
Random
Subsampling
Bayesian voting 
TD
Rm R
m
R p R p R p
R p
M
Mp=Up?pV Tp
qp=qT Up??1p
c1 c2 c30 cj ,(ck)
qp
qpqp
Figure 3: Proposed approach for multilabel docu-
ment categorization tasks.
4 Experimental Setup
The aim of this section is to describe the document
collection used in our experiment and to give an
account of the preprocessing techniques and pa-
rameter settings we have applied.
When machine learning and other approaches
are applied to text categorization problems, a com-
mon technique has been to decompose the mul-
ticlass problem into multiple, independent binary
classification problems. In this paper, we adopt a
different approach. We will be primarily interested
in a classifier which produces a ranking of possi-
ble labels for a given document, with the hope that
the appropriate labels will appear at the top of the
ranking.
4.1 Document Collection
As previously mentioned, the experiment reported
in this paper has been carried out for the Reuters-
21578 dataset4 compiled by David Lewis and orig-
inally collected by the Carnegie group from the
Reuters newswire in 1987. We use one of the
most widely used training/testing divisions, the
?ModApte? split, in which 75 % of the documents
(9,603 documents) are selected for training and the
remaining 25 % (3299 documents) to test the ac-
curacy of the classifier.
Document distribution over categories in both
the training and the testing sets is very unbalanced:
the 10 most frequent categories, top-10, account
75% of the training documents; the rest is dis-
tributed among the other 108 categories.
According to the number of labels assigned to
each document, many of them (19% in training
and 8.48% in testing) are not assigned to any cat-
egory, and some of them are assigned to 12. We
have decided to keep the unlabeled documents in
both the training and testing collections, as it is
suggested in (Lewis, 2004)5.
4.2 Preprocessing
The original format of the text documents is in
SGML. We perform some preprocessing to fil-
ter out the unused parts of a document. We pre-
served only the title and the body text, punctua-
tion and numbers have been removed and all let-
ters have been converted to lowercase. We have
4http://daviddlewis.com/resources/testcollections
5In the ?ModApte? Split section it is suggested as fol-
lows: ?If you are using a learning algorithm that requires
each training document to have at least TOPICS category,
you can screen out the training documents with no TOPICS
categories. Please do NOT screen out any of the 3,299 docu-
ments - that will make your results incomparable with other
studies.?
29
used the tools provided in the web6 in order to ex-
tract text and categories from each document. We
have stemmed the training and testing documents
by using the Porter stemmer (Porter, 1980)7. By
using it, case and flection information are removed
from words. Consequently, the same experiment
has been carried out for the two forms of the doc-
ument collection: word-forms and Porter stems.
According to the dimension reduction, we have
created the matrices for the two mentioned doc-
ument collection forms. The sizes of the train-
ing matrices created are 15591 ? 9603 for word-
forms and 11114 ? 9603 for Porter stems. Differ-
ent number of dimensions have been experimented
(p = 100, 300, 500, 700).
4.3 Parameter setting
We have designed our experiment in order to op-
timize the microaveraged F1 score. Based on pre-
vious experiments (Zelaia et al, 2005), we have
set parameter k for the k-NN algorithm to k = 3.
This way, the k-NN classifier will give a category
label prediction based on the categories of the 3
nearest ones.
On the other hand, we also needed to decide
the number of training databases TDi to create. It
has to be taken into account that a high number of
training databases implies an increasing computa-
tional cost for the final classification system. We
decided to create 30 training databases. However,
this is a parameter that has not been optimized.
There are two other parameters which have been
tuned: the size of each training database and the
threshold for multilabeling. We now briefly give
some cues about the tuning performed.
4.3.1 The size of the training databases
As we have previously mentioned, documents
have been randomly selected from the original
training database in order to construct the 30 train-
ing databases TDi used in our classification sys-
tem. There are n = 9, 603 documents in the orig-
inal Reuters training collection. We had to decide
the number of documents to select in order to con-
struct each TDi. The number of documents se-
lected from each category preserves the propor-
tion of documents in the original one. We have
experimented to select different numbers n1 < n
6http://www.lins.fju.edu.tw/?tseng/Collections/Reuters-
21578.html
7http://tartarus.org/martin/PorterStemmer/
of documents, according to the following formula:
n1 =
115
?
i=1
2 + tij , j = 10, 20, . . . , 70,
where ti is the total number of training documents
in category i. In Figure 4 it can be seen the vari-
ation of the n1 parameter depending on the value
of parameter j. We have experimented different j
values, and evaluated the results. Based on the re-
sults obtained we decided to select j = 60, which
means that each one of the 30 training databases
will have n1 = 298 documents. As we can see,
the final classification system will be using train-
ing databases which are quite smaller that the orig-
inal one. This gives a lower computational cost,
and makes the classification system faster.
Pa
ra
m
et
er
 n
1
Parameter j
 200
 300
 400
 500
 600
 700
 800
 900
 1000
 1100
 10  20  30  40  50  60  70
Figure 4: Random subsampling rate.
4.3.2 Threshold for multilabeling
The k-NN algorithm predicts a unique cate-
gory label for each testing document, based on the
ranked list of categories obtained for each training
database TDi8. As previously mentioned, we use
Bayesian voting to combine the predictions.
The Reuters-21578 is a multilabel database, and
therefore, we had to decide in which cases to as-
sign a second category label to a testing document.
Given that cj is the category with the highest value
in Bayesian voting and ck the next one, the second
ck category label will be assigned when the fol-
lowing relation is true:
cvck > cvcj ? r, r = 0.1, 0.2, . . . , 0.9, 1
In Figure 5 we can see the mean number of cate-
gories assigned to a document for different values
8It has to be noted that unlabeled documents have been
preserved, and thus, our classification system treats unlabeled
documents as documents of a new category
30
of r. Results obtained were evaluated and based
on them we decided to select r = 0.4, which cor-
responds to a ratio of 1.05 categories.
Parameter r
M
ul
til
ab
el
in
g 
Ra
tio
 0.98
 1
 1.02
 1.04
 1.06
 1.08
 1.1
 1.12
 1.14
 1.16
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Figure 5: Threshold for multilabeling.
5 Experimental Results
In Table 3 microaveraged F1 scores obtained
in our experiment are shown. As it could be
expected, a simple stemming process increases
slightly results, and it can be observed that the best
result for the three category subsets has been ob-
tained for the stemmed corpus, even though gain
is low (less than 0.6).
The evaluation for the Top-10 category subset
gives the best results, reaching up to 93.57%. In
fact, this is the expected behavior, as the number of
categories to be evaluated is small and the number
of documents in each category is high. For this
subset the best result has been obtained for 100
dimensions, although the variation is low among
results for 100, 300 and 500 dimensions. When
using higher dimensions results become poorer.
According to the R(90) and R(115) subsets, the
best results are 87.27% and 87.01% respectively.
Given that the difficulty of these subsets is quite
similar, their behavior is also analogous. As we
can see in the table, most of the best results for
these subsets have been obtained by reducing the
dimension of the space to 500.
6 Conclusions and Future Work
In this paper we present an approach for multilabel
document categorization problems which consists
in a multiclassifier system based on the k-NN al-
gorithm. The documents are represented in a re-
duced dimensional space calculated by SVD. We
want to emphasize that, due to the multilabel char-
acter of the database used, we have adapted the
Dimension reduction
Corpus 100 300 500 700
Words(10) 93.06 93.17 93.44 92.00
Porter(10) 93.57 93.20 93.50 92.57
Words(90) 84.90 86.71 87.09 86.18
Porter(90) 85.34 86.64 87.27 86.30
Words(115) 84.66 86.44 86.73 85.84
Porter(115) 85.13 86.47 87.01 86.00
Table 3: Microaveraged F1 scores for Reuters-
21578 ModApte split.
classification system in order for it to be multilabel
too. The learning of the system has been unique
(9603 training documents) and the category label
predictions made by the classifier have been eval-
uated on the testing set according to the three cat-
egory sets: top-10, R(90) and R(115). The mi-
croaveraged F1 scores we obtain are among the
best reported for the Reuters-21578.
As future work, we want to experiment with
generating more than 30 training databases, and
in a preliminary phase select the best among them.
The predictions made using the selected training
databases will be combined to obtain the final pre-
dictions.
When there is a low number of documents avail-
able for a given category, the power of LSI gets
limited to create a space that reflects interesting
properties of the data. As future work we want
to include background text in the training col-
lection and use an expanded term-document ma-
trix that includes, besides the 9603 training doc-
uments, some other relevant texts. This may in-
crease results, specially for the categories with less
documents (Zelikovitz and Hirsh, 2001).
In order to see the consistency of our classi-
fier, we also plan to repeat the experiment for the
RCV1 (Lewis et al, 2004), a new benchmark col-
lection for text categorization tasks which consists
of 800,000 manually categorized newswire stories
recently made available by Reuters.
7 Acknowledgements
This research was supported by the Univer-
sity of the Basque Country (UPV00141.226-T-
15948/2004) and Gipuzkoa Council in a European
31
Union Program.
References
Berry, M.W. and Browne, M.: Understanding Search
Engines: Mathematical Modeling and Text Re-
trieval. SIAM Society for Industrial and Applied
Mathematics, ISBN: 0-89871-437-0, Philadelphia,
(1999)
Breiman, L.: Bagging Predictors. Machine Learning,
24(2), 123?140, (1996)
Cristianini, N., Shawe-Taylor, J. and Lodhi, H.: Latent
Semantic Kernels. Proceedings of ICML?01, 18th
International Conference on Machine Learning, 66?
73, Morgan Kaufmann Publishers, (2001)
Dasarathy, B.V.: Nearest Neighbor (NN) Norms:
NN Pattern Recognition Classification Techniques.
IEEE Computer Society Press, (1991)
Debole, F. and Sebastiani, F.: An Analysis of the Rela-
tive Hardness of Reuters-21578 Subsets. Journal of
the American Society for Information Science and
Technology, 56(6),584?596, (2005)
Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer,
T.K. and Harshman, R.: Indexing by Latent Seman-
tic Analysis. Journal of the American Society for
Information Science, 41, 391?407, (1990)
Dietterich, T.G.: Machine-Learning Research: Four
Current Directions. The AI Magazine, 18(4), 97?
136, (1998)
Dumais, S.T., Platt, J., Heckerman, D. and Sahami,
M.: Inductive Learning Algorithms and Repre-
sentations for Text Categorization. Proceedings of
CIKM?98: 7th International Conference on Infor-
mation and Knowledge Management, ACM Press,
148?155 (1998)
Dumais, S.: Latent Semantic Analysis. ARIST, An-
nual Review of Information Science Technology, 38,
189?230, (2004)
Freund, Y. and Schapire, R.E.: A Short Introduction to
Boosting. Journal of Japanese Society for Artificial
Intelligence, 14(5), 771-780, (1999)
Gao, S., Wu, W., Lee, C.H. and Chua, T.S.: A Maxi-
mal Figure-of-Merit Learning Approach to Text Cat-
egorization. Proceedings of SIGIR?03: 26th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
174?181, ACM Press, (2003)
Gliozzo, A. and Strapparava, C.: Domain Kernels
for Text Categorization. Proceedings of CoNLL?05:
9th Conference on Computational Natural Language
Learning, 56?63, (2005)
Ho, T.K., Hull, J.J. and Srihari, S.N.: Decision Combi-
nation in Multiple Classifier Systems. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence, 16(1), 66?75, (1994)
Joachims, T. Text Categorization with Support Vector
Machines: Learning with Many Relevant Features.
Proceedings of ECML?98: 10th European Confer-
ence onMachine Learning, Springer 1398, 137?142,
(1998)
Kim, H., Howland, P. and Park, H.: Dimension Re-
duction in Text Classification with Support Vector
Machines. Journal of Machine Learning Research,
6, 37?53, MIT Press, (2005)
Lewis, D.D.: Reuters-21578 Text Catego-
rization Test Collection, Distribution 1.0.
http://daviddlewis.com/resources/testcollections
README file (v 1.3), (2004)
Lewis, D.D., Yang, Y., Rose, T.G. and Li, F.: RCV1: A
New Benchmark Collection for Text Categorization
Research. Journal of Machine Learning Research,
5, 361?397, (2004)
Porter, M.F.: An Algorithm for Suffix Stripping. Pro-
gram, 14(3), 130?137, (1980)
Salton, G. and McGill, M.: Introduction to Modern
Information Retrieval. McGraw-Hill, New York,
(1983)
Sebastiani, F.: Machine Learning in Automated Text
Categorization. ACM Computing Surveys, 34(1),
1?47, (2002)
Weiss, S.M., Apte, C., Damerau, F.J., Johnson, D.E.,
Oles, F.J., Goetz, T. and Hampp, T.: Maximizing
Text-Mining Performance. IEEE Intelligent Sys-
tems, 14(4),63?69, (1999)
Yang, Y. An Evaluation of Statistical Approaches to
Text Categorization. Journal of Information Re-
trieval. Kluwer Academic Publishers, 1,(1/2), 69?
90, (1999)
Zelaia, A., Alegria, I., Arregi, O. and Sierra, B.: An-
alyzing the Effect of Dimensionality Reduction in
Document Categorization for Basque. Proceedings
of L&TC?05: 2nd Language & Technology Confer-
ence, 72?75, (2005)
Zelikovitz, S. and Hirsh, H.: Using LSI for Text
Classification in the Presence of Background Text.
Proceedings of CIKM?01: 10th ACM International
Conference on Information and Knowledge Man-
agement, ACM Press, 113?118, (2001)
Zhang, T. and Oles, F.J.: Text Categorization Based
on Regularized Linear Classification Methods. In-
formation Retrieval, 4(1): 5?31, Kluwer Academic
Publishers, (2001)
32
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 358?361,
Prague, June 2007. c?2007 Association for Computational Linguistics
UBC-ZAS: A k-NN based Multiclassifier System to perform WSD in a
Reduced Dimensional Vector Space
Ana Zelaia, Olatz Arregi and Basilio Sierra
Computer Science Faculty
University of the Basque Country
ana.zelaia@ehu.es
Abstract
In this article a multiclassifier approach for
word sense disambiguation (WSD) prob-
lems is presented, where a set of k-NN
classifiers is used to predict the category
(sense) of each word. In order to combine
the predictions generated by the multiclas-
sifier, Bayesian voting is applied. Through
all the classification process, a reduced di-
mensional vector representation obtained by
Singular Value Decomposition (SVD) is
used. Each word is considered an indepen-
dent classification problem, and so differ-
ent parameter setting, selected after a tun-
ing phase, is applied to each word. The ap-
proach has been applied to the lexical sam-
ple WSD subtask of SemEval 2007 (task
17).
1 Introduction
Word Sense Disambiguation (WSD) is an impor-
tant component in many information organization
and management tasks. Both, word representation
and classification method are crucial steps in the
word sense disambiguation process. In this article
both issues are considered. On the one hand, Latent
Semantic Indexing (LSI) (Deerwester et al, 1990),
which is a variant of the vector space model (VSM)
(Salton and McGill, 1983), is used in order to ob-
tain the vector representation of the corresponding
word. This technique compresses vectors represent-
ing word related contexts into vectors of a lower-
dimensional space. LSI, which is based on Singu-
lar Value Decomposition (SVD) (Berry and Browne,
1999) of matrices, has shown to have the ability
to extract the relations among features representing
words by means of their context of use, and has been
successfully applied to Information Retrieval tasks.
On the other hand, a multiclassifier (Ho et al,
1994) which uses different training databases is con-
structed. These databases are obtained from the
original training set by random subsampling. The
implementation of this approach is made by a model
inspired in bagging (Breiman, 1996), and the k-NN
classification algorithm (Dasarathy, 1991) is used to
make the sense predictions for testing words.
Our group (UBC-ZAS) has participated in the lex-
ical sample subtask of SemEval-2007 for task 17,
which consists on 100 different words for which a
training and testing database have been provided.
The aim of this article is to give a brief descrip-
tion of our approach to deal with the WSD task and
to show the results obtained. In Section 2, our ap-
proach is presented. In Section 3, the experimen-
tal setup is introduced. The experimental results are
presented and discussed in Section 4, and finally,
Section 5 contains some conclusions and comments
on future work.
2 Proposed Approach
In this article a multiclassifier based WSD system
which classifies word senses represented in a re-
duced dimensional vector space is proposed.
In Figure 1 an illustration of the experiment per-
formed for each one of the 100 words can be seen.
First, vectors in the VSM are projected to the re-
duced space by using SVD. Next, random subsam-
pling is applied to the training database TD to obtain
358
different training databases TDi. Afterwards the k-
NN classifier is applied for each TDi to make sense
label predictions. Finally, Bayesian voting scheme
is used to combine predictions, and cj will be the
final sense label prediction for testing word q.
d2
d4
d1
d7
.
.
.
q1 q2
q
q
d34
d23
d135
d509
TD2TD1
.
.
.
k?NN k?NN
d50
d256
d98
d2787
d33
d1989
d55
d4612
d9
VSM
VSM
SVD
k?NN
Random
Subsampling
Bayesian voting 
TD
d2d1
.. .
d3
d4
d5d6
d7
d6 d5
d3
. . .
d1 d2
d61
d778
d638 d848
Train
dn
dn
dn
Test
TDi
qn?PSfrag replacements
R m R m
R p R p R p
R p
M
Mp = Up?pV Tp
qp = qT Up??1p
c1 c2 ci cj
qpqpqp
Figure 1: Proposed approach for WSD task
In the rest of this section, the preprocessing ap-
plied, the SVD dimensionality reduction technique,
the k-NN algorithm and the combination of classi-
fiers used are briefly reviewed.
2.1 Preprocessing
In order to obtain the vector representation for each
of the word contexts (documents, cases) given by the
organizers of the SemEval-2007 task, we used the
features extracted by the UBC-ALM participating
group (Agirre and Lopez de Lacalle, 2007). These
features are local collocations (bigrams and trigrams
formed with the words around the target), syn-
tactic dependencies (object, subject, noun-modifier,
preposition, and sibling) and Bag-of-words features
(basically lemmas of the content words in the whole
context, and in a ?4-word window).
2.2 The SVD Dimensionality Reduction
Technique
The classical Vector Space Model (VSM) has been
successfully employed to represent documents in
text categorization tasks. The newer method of
Latent Semantic Indexing (LSI) 1 (Deerwester et
1http://lsi.research.telcordia.com,
http://www.cs.utk.edu/?lsi
al., 1990) is a variant of the VSM in which docu-
ments are represented in a lower dimensional space
created from the input training dataset. The SVD
technique used by LSI consists in factoring term-
document matrix M into the product of three ma-
trices, M = U?V T where ? is a diagonal matrix of
singular values, and U and V are orthogonal matri-
ces of singular vectors (term and document vectors,
respectively).
For classification purposes (Dumais, 2004), the
training and testing documents are projected to the
reduced dimensional space, qp = qT Up??1p , by us-
ing p singular values and the cosine is usually calcu-
lated to measure the similarity between training and
testing document vectors.
2.3 The k-NN classification algorithm
k-NN is a distance based classification approach.
According to this approach, given an arbitrary test-
ing case, the k-NN classifier ranks its nearest neigh-
bors among the training word senses, and uses the
sense of the k top-ranking neighbors to predict the
corresponding to the word which is being analyzed
(Dasarathy, 1991).
2.4 Combination of classifiers
The combination of multiple classifiers has been in-
tensively studied with the aim of improving the ac-
curacy of individual components (Ho et al, 1994).
A widely used technique to implement this approach
is bagging (Breiman, 1996), where a set of training
databases TDi is generated by selecting n training
cases drawn randomly with replacement from the
original training database TD of n cases. When a
set of n1 < n training cases is chosen from the orig-
inal training collection, the bagging is said to be ap-
plied by random subsampling. In fact, this is the
approach used in our work and the n1 parameter has
been selected via tuning.
According to the random subsampling, given a
testing case q, the classifier will make a label predic-
tion ci based on each one of the training databases
TDi. One way to combine the predictions is by
Bayesian voting (Dietterich, 1998), where a con-
fidence value cvicj is calculated for each training
database TDi and sense cj to be predicted. These
confidence values have been calculated based on the
training collection. Confidence values are summed
359
by sense; the sense cj that gets the highest value is
finally proposed as a prediction for the testing exam-
ples.
3 Experimental Setup
In the approach proposed in this article there are
some decisions that need to be taken, because it is
not clear (1) how many examples should be selected
from the TD of each word in order to create each one
of the TDi; (2) which is the appropriate dimension
to be used in order to represent word related con-
texts (cases) for each word database; (3) which is
the appropriate number of TDi that should be cre-
ated (number of classifiers to be used) and (4) which
is the appropriate number of neighbors to be consid-
ered by the k-NN algorithm.
Therefore, a parameter tuning phase was carried
out in order to fix the parameters. We decided to
adjust them for each word independently.
In the following, the parameters are introduced
and the tuning process carried out is explained. For
two of the parameters (the number of classifiers and
the number of neigbors for k-NN), the tuning phase
was performed based on our previous experiments
on document categorization tasks.
3.1 The size of each TDi
As it was mentioned, the multiclassifier is imple-
mented by random subsampling, where a set of n1 <
n vectors is chosen from the original training collec-
tion of n examples for a given word (n is a differ-
ent value for each one of the 100 words). Conse-
quently, the size of each TDi will vary depending
on the value of n1. The selection of different num-
bers of cases was experimented for each word in two
different ways:
a) according to the following equation:
n1 =
s
?
i=1
(2 + b tij c), j = 1, . . . , 10
where ti is the total number of training cases
in the sense ci and s is the total number of
senses for the given word. By dividing param-
eter ti by j, the number of cases selected from
each sense preserves the proportion of cases per
sense in the original one. However, it has to be
taken into account that some of the senses have
a very low number of cases assigned to them.
By summing 2, at least 2 cases will be selected
from each sense. In order to decide the optimal
value for parameter j, the classification experi-
ment was carried out varying j from 1 to 10 for
each word.
b) selecting a fixed number of cases for each of
the senses which appeared for the word in the
training database. Again, in the tuning phase,
different numbers of cases (from 1 to 10) have
been used for each of the 100 words in order to
select a value for each of the words.
We optimized the size of each TDi for each word by
selecting the number of cases sometimes by proce-
dure a) and sometimes by b).
3.2 The dimension of the reduced Vector Space
Model
Taking into account the wide differences among the
training case numbers for different words, we de-
cided to project vectors representing them to differ-
ent reduced dimensional spaces. The selection of
those dimensions is based on the number of training
cases available for each word, and limited to 500; the
used dimensions vary from 19 (for the word grant)
to 481 (for the word part).
3.3 The number of classifiers (TDi)
Based on previous experiments carried out for docu-
ment categorization (Zelaia et al, 2006), we decided
to create 30 classifiers for some words and 50 for
others, i.e. 30 or 50 individual k-NN algorithms will
be used by the multiclassifier in order to combine
opinions by Bayesian voting.
3.4 Number of neigbors for k-NN
Based on our previous experiments, we decided to
use k = 1 and k = 5, and to select the best for each
of the words. The cosine similarity measure is used
in order to find the nearest or the 5 nearests.
4 Experimental Results
The experiment was conducted by considering the
optimal values for parameters tuned by using the
training case set.
360
Results published in this section were calculated
by the SemEval-2007 organizers. Table 1 shows ac-
curacy rates obtained by the 13 participants in the
SemEval-2007, 17 task, lexical sample WSD sub-
task.
System Accuracy System Accuracy
1. 0.887 8. 0.803
2. 0.869 9. 0.799
3. 0.864 10. 0.796
4. 0.857 11. 0.743
5. 0.851 12. 0.538
6. 0.851 13. 0.521
7. 0.838
Table 1: Accuracy rates obtained by the 13 partici-
pants. SemEval-2007, 17 task (Lexical Sample)
The result obtained by our system is 0.799 (the
9th among 13 participants), 1 point over the mean
accuracy (0.786).
5 Conclusions and Future Work
Results obtained show that the construction of a
multiclassifier, together with the use of Bayesian
voting to combine label predictions, plays an im-
portant role in the improvement of results. We also
want to remark that we used the SVD dimensional-
ity reduction technique in order to reduce the vector
representation of cases.
The approach presented in this paper was already
used in a document categorization task. However,
we never used it for WSD task. Therefore, in order
to adapt the method to the new task, we fixed some
parameters based on our previous experiments (30-
50 classifiers, k = 1, 5 for the k-NN algorithm) and
tuned some other parameters by experimenting quite
a high number of TDi sizes and using different di-
mensions for each word. However, we noticed that
the application of our approach to a different task is
not straightforward. Greater effort will have to be
made in order to tune the different parameters to this
specific task of WSD.
One of the main difficulties we found was the dif-
ference in the number of training cases, comparing
with the high number usually available in other tasks
like text categorization.
As future work, we can think of applying a new
preprocessing approach in order to extract better fea-
tures from the training database which could help
the SVD technique improving the accuracy after
a dimensionality reduction is applied. The use of
Wordnet may help.
6 Acknowledgements
This research was supported by the University of
the Basque Country by the project ?ANHITZ 2006:
Language Technologies for Multilingual Interaction
in Intelligent Environments?, IE 06-185
We wish to thank to the UBC-ALM group for
helping us extracting learning features.
References
E. Agirre and O. Lopez de Lacalle. 2007. Ubc-alm:
Combining k-nn with svd for wsd. submited for pub-
lication to SemEval-2007.
M.W. Berry and M. Browne. 1999. Understanding
Search Engines: Mathematical Modeling and Text Re-
trieval. SIAM Society for Industrial and Applied
Mathematics, ISBN: 0-89871-437-0, Philadelphia.
L. Breiman. 1996. Bagging predictors. Machine Learn-
ing, 24(2):123?140.
B.V. Dasarathy. 1991. Nearest Neighbor (NN) Norms:
NN Pattern Recognition Classification Techniques.
IEEE Computer Society Press.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41:391?407.
T.G. Dietterich. 1998. Machine learning research: Four
current directions. The AI Magazine, 18(4):97?136.
S. Dumais. 2004. Latent semantic analysis. In ARIST
(Annual Review of Information Science Technology),
volume 38, pages 189?230.
T.K. Ho, J.J. Hull, and S.N. Srihari. 1994. Decision com-
bination in multiple classifier systems. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
16(1):66?75.
G. Salton and M. McGill. 1983. Introduction to Modern
Information Retrieval. McGraw-Hill.
A. Zelaia, I. Alegria, O. Arregi, and B. Sierra. 2006.
A multiclassifier based document categorization sys-
tem: profiting from the singular value decomposition
dimensionality reduction technique. In Proceedings of
the Workshop on Learning Structured Information in
Natural Language Applications, pages 25?32.
361
Proceedings of the 8th International Conference on Computational Semantics, pages 248?259,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
A Multiclassifier based Approach for Word Sense
Disambiguation using Singular Value
Decomposition
Ana Zelaia, Olatz Arregi and Basilio Sierra
Computer Science Faculty
University of the Basque Country
ana.zelaia@ehu.es
Abstract
In this paper a multiclassifier based approach is presented for a
word sense disambiguation (WSD) problem. A vector representation
is used for training and testing cases and the Singular Value Decom-
position (SVD) technique is applied to reduce the dimension of the
representation. The approach we present consists in creating a set of
k-NN classifiers and combining the predictions generated in order to
give a final word sense prediction for each case to be classified. The
combination is done by applying a Bayesian voting scheme. The ap-
proach has been applied to a database of 100 words made available by
the lexical sample WSD subtask of SemEval-2007 (task 17) organizers.
Each of the words was considered an independent classification prob-
lem. A methodological parameter tuning phase was applied in order to
optimize parameter setting for each word. Results achieved are among
the best and make the approach encouraging to apply to other WSD
tasks.
1 Introduction
Word sense disambiguation (WSD) is the problem of determining which
sense of a word is used when a word appears in a particular context. In
fact, WSD is an important component in many information organization
tasks, and fundamentally consists in a classification problem: given some
word-contexts corresponding to some possible senses, the WSD system has
to classify an occurrence of the word into one of its possible senses.
248
In the approach presented in this paper, a vector representation is used
for training and testing word cases and the Singular Value Decomposition
of matrices is applied in order to reduce the dimension of the representa-
tion. In particular, Latent Semantic Indexing (LSI) [2] is used to make the
dimension reduction. This technique compresses vectors representing word
related contexts into vectors of a lower-dimensional space and has shown to
have the ability to extract the relations among features representing words
by means of their context of use.
We present a multiclassifier [8] based approach which uses different train-
ing databases. These databases are obtained from the original training
dataset by random subsampling. The implementation of this approach is
made by a model inspired in bagging [3], and the k-NN classification algo-
rithm [4] is used to make sense predictions for testing words.
For experimentation, a previous tuning phase was performed to training
data in order to automatically set some system parameters to their optimal
values. Four are the parameters to be optimized, and the combination of all
of them gives the possibility to perform the complete disambiguation process
by 1440 different ways for each of the 100 words to be disambiguated. The
tuning phase has been performed in a sound manner with the aim to improve
our previous work [10]. Although the computational payload is high, it is a
systematic way to fix the optimal values for parameters.
The aim of this article is to give a brief description of our approach to
deal with the WSD task and to show the results achieved. In Section 2,
our approach is presented. In Section 3, the experimental setup is intro-
duced. The experimental results are presented and discussed in Section 4,
and finally, Section 5 contains some conclusions and future work.
2 Proposed Approach
In this section, our approach is presented and the techniques used are briefly
reviewed. First the dataset used in our experiments is described and previ-
ous results are presented. Next, the data preparation is explained in more
detail. A short introduction to the SVD theory and to the k-NN classifica-
tion algorithm is given afterwards. Finally, the multiclassifier construction
is shown.
249
2.1 Dataset and previous results
The dataset we use in the experiments was obtained from the 4th Interna-
tional Workshop on Semantic Evaluations (SemEval-2007) web page
1
, task
17, subtask 1: Coarse-grained English Lexical Sample WSD. This task con-
sists of lexical sample style training and testing data for 100 lemmas (35
nouns and 65 verbs) of different degree of polysemy (ranging from 1 to 13)
and number of instances annotated (ranging from 19 instances in training
for the word grant to 2536 instances at share).
The average inter-annotator agreement for these lemmas is over 90%. In
[9] task organizers describe the results achieved by the participating systems.
They define a baseline for the task based on giving the most frequent sense
in training (F-score: 78.0%). The best system performance (89.1%) was
closely approaching the inter-annotator agreement but still below it.
2.2 Data Preparation
Once we downloaded the training and testing datasets, some features were
extracted and vector representations were constructed for each training and
testing case. The features were extracted by [1] and are local collocations
(bigrams and trigrams formed with lemmas, word-forms or PoS tags around
the target), syntactic dependencies (using relations like object, subject, noun
modifier, preposition and sibling) and Bag-of-words features. This way, the
original training and testing databases were converted to feature databases.
2.3 The SVD technique using LSI
The SVD technique consists in factoring term-document matrix M into the
product of three matrices, M = U?V
T
where ? is a diagonal matrix of
singular values, and U and V are orthogonal matrices of singular vectors
(term and document vectors, respectively). Being k the number of singular
values in matrix ? and selecting the p highest singular values p < k, a
vector representation for the training and testing cases can be calculated in
the reduced dimensional vector space R
p
.
In our experiments we construct one feature-case matrix for each of the
100 words using the corresponding feature training dataset. Each of the
columns in this matrix gives a vector representation to each of the training
cases. As the number of training cases varies among different words, the
number of columns present in the matrices is different; consequently, the
1
http://nlp.cs.swarthmore.edu/semeval/tasks/task17/data.shtml
250
number of singular values changes as well. Taking this in consideration,
we calculate the SVD of each matrix and obtain the reduced vector repre-
sentations for training and testing cases for different p values. In order to
calculate the SVD of the matrices, we use Latent Semantic Indexing (LSI)
2
[5], which has been successfully used for classification purposes [7],
2.4 The k-NN classification algorithm
k-NN is a distance based classification approach. According to this ap-
proach, given an arbitrary testing case, the k-NN classifier ranks its nearest
neighbors among the training cases [4].
In the approach presented in this article, the training and testing cases
for each word are represented by vectors in each reduced dimensional vector
space. The nearest to a testing case are considered to be the vectors which
have the smallest angle with respect to it, and thus the highest cosine. That
is why the cosine is usually calculated to measure the similarity between
vectors. The word senses associated with the k top-ranking neighbors are
used to make a prediction for the testing case. Parameter k was optimized
for each word during tuning phase.
2.5 The multiclassifier construction
The combination of multiple classifiers has been intensively studied with the
aim of improving the accuracy of individual components [8]. A widely used
technique to implement this approach is bagging [3], where a set of training
databases TD
i
is generated by selecting n training cases drawn randomly
with replacement from the original training database TD of n cases. When
a set of n
1
< n training cases is chosen from the original training collection,
the bagging is said to be applied by random subsampling.
In our work, we construct a multiclassifier by applying random sub-
sampling for each word. As the number n of training cases is different for
each word, we optimize via tuning the parameter n
1
for each multiclassifier
constructed. This way, we work with training databases TD
i
of different
sizes. Moreover, the number of training databases TD
i
to create for each
multiclassifier, is also optimized via tuning.
Once the multiclassifiers are constructed, and given a testing case q for a
word, the corresponding multiclassifier will make a word-sense label predic-
tion c
i
based on each one of the training databases TD
i
. In order to calculate
these confidence values, word-sense predictions are made for training cases
2
http://lsi.research.telcordia.com, http://www.cs.utk.edu/?lsi
251
and the accuracies obtained give the confidence values which indicate the
accuracy level that may be expected when a prediction is made for a testing
case based on each training database TD
i
and word-sense c
j
to be predicted.
The way we combine such predictions is by applying Bayesian voting [6],
where a confidence value cv
i
c
j
is calculated for each training database TD
i
and word-sense c
j
to be predicted. In testing phase, confidence values ob-
tained for the testing cases are summed by sense; the sense c
j
that gets the
highest value is finally proposed as a prediction for the testing case q. This
process is repeated for every testing case.
In Fig. 1 an illustration of the experiment performed for each one of the
100 words can be seen. First, vectors in the original Vector Space are pro-
jected to the reduced space using SVD; next, random subsampling is applied
to the training database TD to obtain different training databases TD
i
; af-
terwards, the k-NN classifier is applied for each TD
i
to make sense label
predictions; finally, Bayesian voting scheme is used to combine predictions,
and c will be the final sense label prediction for testing case q.
3 Experimental Setup. The tuning phase
The experiments were carried out in two phases. First, a parameter tuning
phase was performed in order to set the following parameters to their optimal
values:
? The dimension p of the reduced dimensional vector space R
p
to which
word-case vectors are projected for each word.
? The number of classifiers, training databases TD
i
, to create for each
word.
? The number k of nearest neighbors to be considered by the k-NN
classifier for each word.
? The number n
1
of cases to select from the TD of each word in order
to create each one of the TD
i
, that is, the size of each TD
i
.
All the four parameters were adjusted independently for each word, be-
cause of the different characteristics of words with respect to the number of
training and testing cases present in the dataset and the number of word-
senses associated to each of them.
Validation and testing data subsets used in the tuning phase were ex-
tracted form the original training database TD for each word. Both subsets
252
100 words for WSD
affect.v allow.v
work.v
Original  training and
testing databases
.   .   . 
  
.   .   . 
  
Features: 
local collocations, 
syntactic dependencies 
and Bag of Words
Original Vector Space: R
m
m: Number of features
Training
Testing
 .   .   . 
  
 .   .   . 
  
 .   .   .   
d
2
d
n
q
1
d
1
d
2
d
n
q
1
d
1
q
2
q
n'
q
n'
q
2
Reduced Space: R
p
p: Singular Values, p m
 Random Subsampling
k-NN
k-NN
k-NN
c
1, 
cv
c1
1
c
2, 
cv
c2
2
c
i, 
cv
ci
i
Singular Value Decomposition (SVD) 
by Latent Semantic Indexing (LSI) (*)
SVD
R
m
R
m
R
p
R
p
R
p
R
p
i training databases (TD
i
)
generated by selecting n
1
 
cases (n
1
<n) randomly 
TD
1
TD
2
TD
i
d
1
d
2
d
n
d
3
d
1
d
2
d
3
d
n
q
q
d
2
1
d
1
1
d
1
2
 
.
 
.
 
.
 
 
 
 
 
.
 
.
 
.
 
d
n1
1
d
n1
2
d
2
2
d
n1
i
d
1
i
d
2
i
q
q
Bayesian Voting
c: word sense proposed 
for testing case q
Cosine Similarity Measure
(*) http://lsi.research.telcordia.com
      http://www.cs.utk.edu/~lsi
 .   .   .   
m
 
f
e
a
t
u
r
e
s
m
 
f
e
a
t
u
r
e
s
m
 
f
e
a
t
u
r
e
s
m
 
f
e
a
t
u
r
e
s
m
 
f
e
a
t
u
r
e
s
m
 
f
e
a
t
u
r
e
s
c
i
: Sense given by 
classifier i to case q
cv
ci
i
: Confidence Value 
calculated by classifier 
i for sense c
i
c
 .
 .
 .
  
 
Testing
case q
TD
Projection of
testing case q
Figure 1: Proposed multiclassifier approach for WSD task
were constructed by random selection of cases, where 75% of the cases were
selected for the validation subset and the rest for the tuning purposed made
testing subset.
In the following the optimization of parameters is explained. Parameters
were optimized in the same order as presented in this subsection, that is,
the dimension reduction first, the number of classifiers second, the number k
of nearest neighbors third and the size of each TD
i
last. When the first pa-
rameter was being optimized, all possibilities for the other three parameters
253
were taken into account, and the optimization of the parameter was made
based on the average of the 10% best results. Once a parameter was fixed,
the same method was applied in order to optimize the rest of the parame-
ters. This optimization method implies that the experiment was performed
for all the combinations of the four parameters. This implies a high compu-
tational cost during the tuning phase. For testing phase, the experiments
are performed using the optimal values for parameters.
3.1 The dimension p of R
p
This is the first parameter we tuned. As it was previously mentioned in
Section 2.3, the dimension p of the reduced dimensional vector space R
p
to
which training and testing cases are projected varies for different words. The
reason for that is the difference in the number of cases present in the dataset
for each word. For words with a high number of cases, the dimension was
previously reduced to 500 (see [2]). Then, for every word we experimented
by keeping the number of dimensions in a proportion. This proportion is
given by parameter ?. We analyze four proportions by setting parameter
? to: ? = 0 keep all dimensions, ? = 1 keep 2/3 of the dimensions, ? = 2
keep half of the dimensions and ? = 3 keep a third of the dimensions.
We calculated four different values for p. Training and testing cases were
represented in the four R
p
spaces and word-sense label predictions calculated
for all of them. All the possibilities were tried for the rest of the parameters
(detailed in the following subsections). For each value of ?, we selected the
10% best results from the 1440 we have, calculated the average of them and
set parameter ? to its optimal value for each word. The optimization of ?
gives a final optimal value for parameter p for each word.
3.2 The number of classifiers, TD
i
The number of classifiers, or TD
i
to create for each word is also a parameter
that needs to be tuned. This is because the number of cases present for each
word is quite variable, and this fact may have some influence in the number
of TD
i
to construct. In our work, we experimented with 6 different val-
ues for parameter i = 3, 5, 10, 20, 30, 40. We performed the disambiguation
process for each of them by considering the results for the optimal value
of parameter ?, already optimized, and all the possible values for the rest
of the parameters for each word. We then selected the best 10% average
results achieved for each value of i, calculated the average, and based on
these average results set the optimal value for parameter i for each word.
254
3.3 The number k of nearest neighbors for k-NN
At this stage of the tuning phase, and having already optimized the dimen-
sionality reduction and the number of classifiers to create for each word,
we take both optimal values and experiment with all possible values for the
rest of the parameters. We calculate the average for six different values of
k, k = 3, 5, 7, 9, 11, 13. We set the optimal value of k for each word based
on the maximum average obtained.
3.4 The size of training databases TD
i
: parameter n
1
As it was mentioned in Section 2.5, the parameter n
1
will be optimized for
each word in order to create training databases TD
i
of different sizes. The
selection of different values for n
1
was experimented for each word according
to the following equation:
n
1
=
s
?
i=1
(2 + ?
t
i
j
?), j = 1, . . . , 10
where t
i
is the total number of training cases in the sense c
i
and s is the
total number of senses for the given word. By dividing t
i
by j, the number
of training-cases selected from each word-sense preserves the proportion of
cases per sense in the original one. However, it has to be taken into account
that some of the word-senses have a very low number of training-cases as-
signed to them. By summing 2, at least 2 training-cases will be selected
from each word-sense. In order to decide the optimal value for j, the clas-
sification experiment was carried out varying j from 1 to 10 for each word.
Given that parameters p, i and k are already set to their optimal values for
each word, we calculate results for the 10 possible values of j, and set it to
its optimal value.
4 Experimental Results
The experiment was conducted by considering the optimal values for param-
eters tuned. Original training and testing datasets were used for the final
experiment, and results achieved were compared to the ones made available
by task organizers [9].
Our system achieved an F-score of 85.65%, which compared to the base-
line defined (78.0%) is a very good result, although still below the best
published by task organizers (89.1%).
255
In [9] the performance of the top-8 systems on individual verbs and nouns
is shown; 73 of the 100 lemmas are included in a table in two separated
groups. Lemmas that have perfect or almost perfect accuracies have been
removed. In TABLE 1 the average results achieved by our system for the two
groups of lemmas are compared to the ones published in the cited paper. We
can observe that our system performs better than the average of the top-8
systems disambiguating nouns, but slightly worse for verbs. In the overall,
our system is very near to the average performance of the top-8 systems.
Top-8 Our system
Verbs 70.44 67.78
Nouns 79.86 82.96
Overall 74.32 74.02
Table 1: Average performance compared to the top-8 in [9]
We want to remark that our system uses only the official training and
testing data, without including background knowledge of any type. Some of
the top-8 systems used background knowledge in order to assist in resolving
ambiguities.
 0.7
 0.72
 0.74
 0.76
 0.78
 0.8
 0.82
 0.84
 0  0.5  1  1.5  2  2.5  3
?
A
c
c
u
r
a
c
y
Figure 2: Average accuracy related to parameter ? = 0, 1, 2, 3
An analysis of the parameter optimization performed in the tuning phase
lead us to observe that there is a relation between the dimensionality reduc-
tion level applied by SVD and the accuracy achieved for a word disambigua-
tion (see Fig. 2). Words with more than 500 cases in the training dataset
were not depicted in the figure because an additional dimension reduction
was applied to them (see section 3.1). The graphic in Fig. 2 suggests that
256
0 1 2 3
50
10
0
15
0
20
0
?
n
se
n
se
s
Figure 3: Complexity related to parameter ? = 0, 1, 2, 3
a dimensionality reduction of half of the features, ? = 2, is appropriate for
words where a high level of accuracy is reached.
In order to analyze the adequacy of the parameter tuning performed, we
created a new variable dividing the case number n of the training database
by the number of senses for each word. This calculus is meant to represent
the complexity of each word. In Fig. 3 the interquartile relationships found
among the parameter ? and the complexity of the words is presented. For
each value of ? the segments represent the minimum and the maximum value
of the complexity, while the bold line shows the median and the rectangular
area represents the density of the second and third quartiles. As it can be
seen, the evolution of the median value, as well as the minimum values, are
similar to the observed in the accuracies. This allows to say that the ? value
was properly selected by the automatic selection used, and also that higher
values of ? would not ensure better solutions for the most complex words.
5 Conclusions and Future Work
The good results achieved by our system show that the construction of
multiclassifiers, together with the use of Bayesian voting to combine word-
257
sense label predictions, plays an important role in disambiguation tasks.
The use of the SVD technique in order to reduce the vector representation
of cases has been proved to behave appropriately.
We also want to remark that, our disambiguation system has been adapted
to the task of disambiguating each one of the 100 words by applying a
methodological parameter tuning directed to find the optimal values for
each word. This makes possible to have a unique disambiguation system
applicable to words with very different characteristics.
Moreover, in our experiments we used only the training data supplied for
sense disambiguation in test set, with no inclusion of background knowledge
at all, while most of the top-8 systems participating in the task do use some
kind of background knowledge. As future work, we intend to make use of
such knowledge and hope that results will increase. We also intend to apply
this approach to other disambiguation tasks.
References
[1] E. Agirre and O. Lopez de Lacalle. Ubc-alm: Combining k-nn with svd
for wsd. In Proceedings of the 4th International Workshop on Semantic
Evaluations, SemEval-2007, pages 342?345, 2007.
[2] M. Berry and M. Browne. Understanding Search Engines: Mathemati-
cal Modeling and Text Retrieval. SIAM Society for Industrial and Ap-
plied Mathematics, ISBN: 0-89871-437-0, Philadelphia, 1999.
[3] L. Breiman. Bagging predictors. Machine Learning, 24(2):123?140,
1996.
[4] B. Dasarathy. Nearest Neighbor (NN) Norms: NN Pattern Recognition
Classification Techniques. IEEE Computer Society Press, 1991.
[5] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman.
Indexing by latent semantic analysis. Journal of the American Society
for Information Science, 41:391?407, 1990.
[6] T. Dietterich. Machine learning research: Four current directions. The
AI Magazine, 18(4):97?136, 1998.
[7] S. Dumais. Latent semantic analysis. In ARIST (Annual Review of
Information Science Technology), volume 38, pages 189?230, 2004.
258
[8] T. Ho, J. Hull, and S. Srihari. Decision combination in multiple clas-
sifier systems. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 16(1):66?75, 1994.
[9] S. Pradhan, E. Loper, D. Dligach, and M. Palmer. Semeval-2007 task
17: English lexical sample, srl and all words. In A. for Computa-
tional Linguistics, editor, Proceedings of the 4th International Work-
shop on Semantic Evaluations, SemEval-2007, pages 87?92, 2007.
[10] A. Zelaia, O. Arregi, and B. Sierra. Ubc-zas: A k-nn based multiclas-
sifier system to perform wsd in a reduced dimensional vector space.
In Proceedings of the 4th International Workshop on Semantic Evalua-
tions, SemEval-2007, pages 358?361, 2007.
259
