Proceedings of NAACL HLT 2009: Short Papers, pages 85?88,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Shallow Semantic Parsing for Spoken Language Understanding
Bonaventura Coppola and Alessandro Moschitti and Giuseppe Riccardi
Department of Information Engineering and Computer Science - University of Trento, Italy
{coppola,moschitti,riccardi}@disi.unitn.it
Abstract
Most Spoken Dialog Systems are based on
speech grammars and frame/slot semantics.
The semantic descriptions of input utterances
are usually defined ad-hoc with no ability to
generalize beyond the target application do-
main or to learn from annotated corpora. The
approach we propose in this paper exploits
machine learning of frame semantics, bor-
rowing its theoretical model from computa-
tional linguistics. While traditional automatic
Semantic Role Labeling approaches on writ-
ten texts may not perform as well on spo-
ken dialogs, we show successful experiments
on such porting. Hence, we design and eval-
uate automatic FrameNet-based parsers both
for English written texts and for Italian dia-
log utterances. The results show that disflu-
encies of dialog data do not severely hurt per-
formance. Also, a small set of FrameNet-like
manual annotations is enough for realizing ac-
curate Semantic Role Labeling on the target
domains of typical Dialog Systems.
1 Introduction
Commercial services based on spoken dialog sys-
tems have consistently increased both in number and
in application scenarios (Gorin et al, 1997). De-
spite its success, current Spoken Language Under-
standing (SLU) technology is mainly based on sim-
ple conceptual annotation, where just very simple
semantic composition is attempted. In contrast, the
availability of richer semantic models as FrameNet
(Baker et al, 1998) is very appealing for the de-
sign of better dialog managers. The first step to en-
able the exploitation of frame semantics is to show
that accurate automatic semantic labelers can be de-
signed for processing conversational speech.
In this paper, we face the problem of perform-
ing shallow semantic analysis of speech transcrip-
tions from real-world dialogs. In particular, we ap-
ply Support Vector Machines (SVMs) and Kernel
Methods to the design of a semantic role labeler
(SRL) based on FrameNet. Exploiting Tree Kernels
(Collins and Duffy, 2002; Moschitti et al, 2008), we
can quickly port our system to different languages
and domains. In the experiments, we compare
results achieved on the English FrameNet against
those achieved on a smaller Italian FrameNet-like
corpus of spoken dialog transcriptions. They show
that the system is robust enough to disfluencies and
noise, and that it can be easily ported to new do-
mains and languages.
In the remainder of the paper, Section 2 presents
our basic Semantic Role Labeling approach, Sec-
tion 3 describes the experiments on the English
FrameNet and on our Italian dialog corpus, and Sec-
tion 4 draws the conclusions.
2 FrameNet-based Semantic Role Labeling
Semantic frames represent prototypical events or
situations which individually define their own set
of actors, or frame participants. For example,
the COMMERCE SCENARIO frame includes partic-
ipants as SELLER, BUYER, GOODS, and MONEY.
The task of FrameNet-based shallow semantic pars-
ing can be implemented as a combination of multi-
ple specialized semantic labelers as those in (Car-
reras and Ma`rquez, 2005), one for each frame.
Therefore, the general semantic parsing work-flow
includes 4 main steps: (i) Target Word Detec-
tion, where the semantically relevant words bringing
predicative information (the frame targets) are de-
tected, e.g. the verb to purchase for the above exam-
ple; (ii) Frame Disambiguation, where the correct
frame for every target word (which may be ambigu-
ous) is determined, e.g. COMMERCE SCENARIO;
(iii) Boundary Detection (BD), where the sequences
of words realizing the frame elements (or predicate
85
arguments) are detected; and (iv) Role Classification
(RC) (or argument classification), which assigns se-
mantic labels to the frame elements detected in the
previous step, e.g. GOODS. Therefore, we imple-
ment the full task of FrameNet-based parsing by a
combination of multiple specialized SRL-like label-
ers, one for each frame (Coppola et al, 2008). For
the design of each single labeler, we use the state-of-
the-art strategy developed in (Pradhan et al, 2005;
Moschitti et al, 2008).
2.1 Standard versus Structural Features
In machine learning tasks, the manual engineering
of effective features is a complex and time con-
suming process. For this reason, our SVM-based
SRL approach exploits the combination of two dif-
ferent models. We first used Polynomial Kernels
over handcrafted, linguistically-motivated, ?stan-
dard? SRL features (Gildea and Jurafsky, 2002;
Pradhan et al, 2005; Xue and Palmer, 2004).
Nonetheless, since we aim at modeling an SRL sys-
tem for a new language (Italian) and a new domain
(dialog transcriptions), the above features may re-
sult ineffective. Thus, to achieve independence on
the application domain, we exploited Tree Kernels
(Collins and Duffy, 2002) over automatic structural
features proposed in (Moschitti et al, 2005; Mos-
chitti et al, 2008). These are complementary to stan-
dard features and are obtained by applying Tree Ker-
nels (Collins and Duffy, 2002; Moschitti et al, 2008)
to basic tree structures expressing the syntactic rela-
tion between arguments and predicates.
3 Experiments
Our purpose is to show that an accurate automatic
FrameNet parser can be designed with reasonable
effort for Italian conversational speech. For this pur-
pose, we designed and evaluated both a semantic
parser for the English FrameNet (Section 3.1) and
one for a corpus of Italian spoken dialogs (Section
3.2). The accuracy of the latter and its comparison
against the former can provide evidence to sustain
out thesis or not.
3.1 Evaluation on the English FrameNet
In this experiment we trained and tested boundary
detectors (BD) and role classifiers (RC) as described
in Section 2. More in detail, (a) we trained 5 BDs
according to the syntactic categories of the possi-
ble target predicates, namely nouns, verbs, adjec-
tives, adverbs and prepositions; (b) we trained 782
one-versus-all multi-role classifiers RC, one for each
available frame and predicate syntactic category, for
a total of 5,345 binary classifiers; and (c) we ap-
plied the above models for recognizing predicate ar-
guments and their associated semantic labels in sen-
tences, where the frame label and the target predi-
cate were considered as given.
3.1.1 Data Set
We exploited the FrameNet 1.3 data base. After
preprocessing and parsing the sentences with Char-
niak?s parser, we obtained 135,293 semantically-
annotated and syntactically-parsed sentences.
The above dataset was partitioned into three sub-
sets: 2% of data (2,782 sentences) for training the
BDs, 90% (121,798 sentences) for training RC, and
1% (1,345 sentences) as test set. The remaining data
were discarded. Accordingly, the number of pos-
itive and negative training examples for BD were:
2,764 positive and 37,497 negative examples for ver-
bal, 1,189 and 35,576 for nominal, 615 and 14,544
for adjectival, 0 and 40 for adverbial, and 7 and 177
for prepositional predicates (for a total of 4,575 and
87,834). For RC, the total numbers were 207,662
and 1,960,423, which divided by the number of role
types show the average number of 39 positive versus
367 negative examples per role label.
3.1.2 Results
We tested several kernels over standard fea-
tures (Gildea and Jurafsky, 2002; Pradhan et al,
2005) and structured features (Moschitti et al,
2008): the Polynomial Kernel (PK, with a degree of
3), the Tree Kernel (TK) and its combination with
the bag of word kernel on the tree leaves (TKL).
Also, the combinations PK+TK and PK+TKL were
tested.
The 4 rows of Table 1 report the performance of
different classification tasks. They show in turn: (1)
the ?pure? performance of the BD classifiers, i.e.
considering correct the classification decisions also
when a correctly classified tree node does not ex-
actly correspond to its argument?s word boundaries.
Such mismatch frequently happens when the parse
tree (which is automatically generated) contains in-
86
PK TK PK+TK TKL PK+TKL
Eval sett. P R F1 P R F1 P R F1 P R F1 P R F1
BD .887 .675 .767 .949 .652 .773 .915 .698 .792 .938 .659 .774 .908 .701 .791
BD pj .850 .647 .735 .919 .631 .748 .875 .668 .758 .906 .636 .747 .868 .670 .757
BD+RC .654 .498 .565 .697 .479 .568 .680 .519 .588 .689 .484 .569 .675 .521 .588
BD+RC pj .625 .476 .540 .672 .462 .548 .648 .495 .561 .663 .466 .547 .644 .497 .561
Table 1: Results on FrameNet dataset: Polynomial Kernel, two different Tree Kernels, and their combinations (see
Section 3.1.2) with 2% training for BD and 90% for RC.
correct node attachments; (2) the real performance
of the BD classification when actually ?projected?
(?pj?) on the tree leaves, i.e. when matching not
only the constituent node as in 1, but also exactly
matching the selected words (leaves) with those in
the FrameNet gold standard. This also implies the
exact automatic syntactic analysis for the subtree;
(3) the same as in (1), with the argument role classi-
fication (RC) also performed (frame element labels
must also match); (4) the same as in (2), with RC
also performed. For each classification task, the Pre-
cision, Recall and F1 measure achieved by means
of different kernel combinations are shown in the
columns of the table. Only for the best configuration
in Table 1 (PK+TK, results in bold) the amount of
training data for the BD model was increased from
2% to 90%, resulting in a popular splitting for this
task(Erk and Pado, 2006). Results are shown in Ta-
ble 2: the PK+TK kernel achieves 1.0 Precision,
0.732 Recall, and 0.847 F1. These figures can be
compared to 0.855 Precision, 0.669 Recall and 0.751
F1 of the system described in (Erk and Pado, 2006)
and trained over the same amount of data. In con-
clusion, our best learning scheme is currently capa-
ble of tagging FrameNet data with exact boundaries
and role labels at 63% F1. Our next steps will be (1)
further improving the RC models using FrameNet-
specific information (such as Frame and role inheri-
tance), and (2) introducing an effective Frame clas-
sifier to automatically choose Frame labels.
Enhanced PK+TK
Eval Setting P R F1
BD (nodes) 1.0 .732 .847
BD (words) .963 .702 .813
BD+RC (nodes) .784 .571 .661
BD+RC (words) .747 .545 .630
Table 2: Results on the FrameNet dataset. Best configu-
ration from Table 1, raised to 90% of training data for BD
and RC.
Eval Setting P R F1 P R F1
PK
BD - - - .900 .869 .884
BD+RC - - - .769 .742 .756
TK PK+TK
BD .887 .856 .871 .905 .873 .889
BD+RC .765 .738 .751 .774 .747 .760
Table 3: Experiment Results on the Italian dialog corpus
for different learning schemes and kernel combinations.
3.2 Evaluation on Italian Spoken Dialogs
In this section, we present the results of BD and RC
of our FrameNet parser on the smaller Italian spoken
dialog corpus. We assume here as well that the target
word (i.e. the predicate for which arguments have to
be extracted) along with the correct frame are given.
3.2.1 Data Set
The Italian dialog corpus includes 50 real human-
human dialogs recorded and manually transcribed at
the call center of the help-desk facility of an Ital-
ian Consortium for Information Systems. The di-
alogs are fluent and spontaneous conversations be-
tween a caller and an operator, concerning hard-
ware and software problems. The dialog turns con-
tain 1,677 annotated frame instances spanning 154
FrameNet frames and 20 new ad hoc frames spe-
cific for the domain. New frames mostly con-
cern data processing such as NAVIGATION, DIS-
PLAY DATA, LOSE DATA, CREATE DATA. Being
intended as a reference resource, this dataset in-
cludes partially human-validated syntactic analysis,
i.e. lower branches corrected to fit arguments. We
divided such dataset into 90% training (1,521 frame
instances) and 10% testing (156 frame instances).
Each frame instance brings its own set of frame par-
ticipant (or predicate argument) instances.
For BD, the very same approach as in Section 3.1
was followed. For RC, we also followed the same
approach but, in order to cope with data sparse-
87
ness, we also attempted a different RC strategy by
merging data related to different syntactic predicates
within the same frame. So, within each frame, we
merged data related to verbal predicates, nominal
predicates, and so on. Due to the short space avail-
able, we will just report results for this latter ap-
proach, which performed sensitively better.
3.2.2 Results
The results are reported in Table 3. Each ta-
ble block shows Precision, Recall and F1 for ei-
ther PK, TK, or PK+TK. The rows marked as BD
show the results for the task of marking the exact
constituent boundaries of every frame element (ar-
gument) found. The rows marked as BD+RC show
the results for the two-stage pipeline of bothmarking
the exact constituent boundaries and also assigning
the correct semantic label. A few observations hold.
First, the highest F1 has been achieved using the
PK+TK combination. On this concern, we under-
line that kernel combinations always gave the best
performance in any experiment we run.
Second, we emphasize that the F1 of PK is sur-
prisingly high, since it exploits the set of standard
SRL feature (Gildea and Jurafsky, 2002; Pradhan
et al, 2005), originally developed for English and
left unmodified for Italian. Nonetheless, their per-
formance is comparable to the Tree Kernels and,
as we said, their combination improves the result.
Concerning the structured features exploited by Tree
Kernels, we note that they work as well without any
tuning when ported to Italian dialogs.
Finally, the achieved F1 is extremely good. In
fact, our corresponding result on the FrameNet cor-
pus (Table 2) is P=0.784, R=0.571, F1=0.661,
where the corpus contains much more data, its sen-
tences come from a standard written text (no dis-
fluencies are present) and it is in English language,
which is morphologically simpler than Italian. On
the other hand, the Italian corpus includes optimal
syntactic annotation which exactly fits the frame se-
mantics, and the number of frames is lower than in
the FrameNet experiment.
4 Conclusions
The good performance achieved for Italian dialogs
shows that FrameNet-based parsing is viable for la-
beling conversational speech in any language us-
ing a few training data. Moreover, the approach
works well for very specific domains, like help-
desk/customer conversations. Nonetheless, addi-
tional tests based on fully automatic transcription
and syntactic parsing are needed. However, our cur-
rent results show that future research on complex
spoken dialog systems is enabled to exploit automat-
ically generated frame semantics, which is our very
direction.
Acknowledgments
The authors wish to thank Daniele Pighin for the SRL subsys-
tem and Sara Tonelli for the Italian corpus. This work has been
partially funded by the European Commission - LUNA project
(contract n.33549), and by the Marie Curie Excellence Grant
for the ADAMACH project (contract n.022593).
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of COLING-ACL ?98, pages 86?90.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. In Proceedings of CoNLL-2005.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over Dis-
crete structures, and the voted perceptron. In ACL02.
Bonaventura Coppola, Alessandro Moschitti, and
Daniele Pighin. 2008. Generalized framework for
syntax-based relation mining. In IEEE-ICDM 2008.
Katrin Erk and Sebastian Pado. 2006. Shalmaneser - a
flexible toolbox for semantic role assignment. In Pro-
ceedings of LREC 2006, Genoa, Italy.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic La-
beling of Semantic Roles. Computational Linguistics.
A. L. Gorin, G. Riccardi, and J. H. Wright. 1997. How
may i help you? Speech Communication.
Alessandro Moschitti, Bonaventura Coppola, Daniele
Pighin, and Roberto Basili. 2005. Engineering of syn-
tactic features for shallow semantic parsing. In ACL
WS on Feature Engineering for ML in NLP.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin, and Daniel Jurafsky.
2005. Support Vector Learning for Semantic Argu-
ment Classification. Machine Learning.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings of
EMNLP 2004.
88
Proceedings of NAACL HLT 2007, pages 564?571,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
ISP: Learning Inferential Selectional Preferences 
 
Patrick Pantel?, Rahul Bhagat?, Bonaventura Coppola?, 
Timothy Chklovski?, Eduard Hovy? 
?Information Sciences Institute 
University of Southern California 
Marina del Rey, CA 
{pantel,rahul,timc,hovy}@isi.edu
?ITC-Irst and University of Trento 
Via Sommarive, 18 ? Povo 38050  
Trento, Italy 
coppolab@itc.it 
  
Abstract 
Semantic inference is a key component 
for advanced natural language under-
standing. However, existing collections of 
automatically acquired inference rules 
have shown disappointing results when 
used in applications such as textual en-
tailment and question answering. This pa-
per presents ISP, a collection of methods 
for automatically learning admissible ar-
gument values to which an inference rule 
can be applied, which we call inferential 
selectional preferences, and methods for 
filtering out incorrect inferences. We 
evaluate ISP and present empirical evi-
dence of its effectiveness. 
1 Introduction 
Semantic inference is a key component for ad-
vanced natural language understanding. Several 
important applications are already relying heavily 
on inference, including question answering 
(Moldovan et al 2003; Harabagiu and Hickl 2006), 
information extraction (Romano et al 2006), and 
textual entailment (Szpektor et al 2004). 
In response, several researchers have created re-
sources for enabling semantic inference. Among 
manual resources used for this task are WordNet 
(Fellbaum 1998) and Cyc (Lenat 1995). Although 
important and useful, these resources primarily 
contain prescriptive inference rules such as ?X di-
vorces Y ? X married Y?. In practical NLP appli-
cations, however, plausible inference rules such as 
?X married Y? ? ?X dated Y? are very useful. This, 
along with the difficulty and labor-intensiveness of 
generating exhaustive lists of rules, has led re-
searchers to focus on automatic methods for build-
ing inference resources such as inference rule 
collections (Lin and Pantel 2001; Szpektor et al 
2004) and paraphrase collections (Barzilay and 
McKeown 2001). 
Using these resources in applications has been 
hindered by the large amount of incorrect infer-
ences they generate, either because of altogether 
incorrect rules or because of blind application of 
plausible rules without considering the context of 
the relations or the senses of the words. For exam-
ple, consider the following sentence: 
Terry Nichols was charged by federal prosecutors for murder 
and conspiracy in the Oklahoma City bombing. 
and an inference rule such as: 
 X is charged by Y ? Y announced the arrest of X (1) 
Using this rule, we can infer that ?federal prosecu-
tors announced the arrest of Terry Nichols?. How-
ever, given the sentence: 
Fraud was suspected when accounts were charged by CCM 
telemarketers without obtaining consumer authorization. 
the plausible inference rule (1) would incorrectly 
infer that ?CCM telemarketers announced the ar-
rest of accounts?. 
This example depicts a major obstacle to the ef-
fective use of automatically learned inference 
rules. What is missing is knowledge about the ad-
missible argument values for which an inference 
rule holds, which we call Inferential Selectional 
Preferences. For example, inference rule (1) 
should only be applied if X is a Person and Y is a 
Law Enforcement Agent or a Law Enforcement 
Agency. This knowledge does not guarantee that 
the inference rule will hold, but, as we show in this 
paper, goes a long way toward filtering out errone-
ous applications of rules. 
In this paper, we propose ISP, a collection of 
methods for learning inferential selectional prefer-
ences and filtering out incorrect inferences. The 
564
presented algorithms apply to any collection of 
inference rules between binary semantic relations, 
such as example (1). ISP derives inferential selec-
tional preferences by aggregating statistics of in-
ference rule instantiations over a large corpus of 
text. Within ISP, we explore different probabilistic 
models of selectional preference to accept or reject 
specific inferences. We present empirical evidence 
to support the following main contribution: 
Claim: Inferential selectional preferences can be 
automatically learned and used for effectively fil-
tering out incorrect inferences. 
2 Previous Work 
Selectional preference (SP) as a foundation for 
computational semantics is one of the earliest top-
ics in AI and NLP, and has its roots in (Katz and 
Fodor 1963).  Overviews of NLP research on this 
theme are (Wilks and Fass 1992), which includes 
the influential theory of Preference Semantics by 
Wilks, and more recently (Light and Greiff 2002). 
Rather than venture into learning inferential 
SPs, much previous work has focused on learning 
SPs for simpler structures. Resnik (1996), the 
seminal paper on this topic, introduced a statistical 
model for learning SPs for predicates using an un-
supervised method. 
Learning SPs often relies on an underlying set of 
semantic classes, as in both Resnik?s and our ap-
proach. Semantic classes can be specified manu-
ally or derived automatically. Manual collections 
of semantic classes include the hierarchies of 
WordNet (Fellbaum 1998), Levin verb classes 
(Levin 1993), and FrameNet (Baker et al 1998). 
Automatic derivation of semantic classes can take 
a variety of approaches, but often uses corpus 
methods and the Distributional Hypothesis (Harris 
1964) to automatically cluster similar entities into 
classes, e.g. CBC (Pantel and Lin 2002). In this 
paper, we experiment with two sets of semantic 
classes, one from WordNet and one from CBC. 
Another thread related to our work includes ex-
tracting from text corpora paraphrases (Barzilay 
and McKeown 2001) and inference rules, e.g. 
TEASE1 (Szpektor et al 2004) and DIRT (Lin and 
Pantel 2001). While these systems differ in their 
approaches, neither provides for the extracted in-
                                                     
1 Some systems refer to inferences they extract as entail-
ments; the two terms are sometimes used interchangeably. 
ference rules to hold or fail based on SPs. Zanzotto 
et al (2006) recently explored a different interplay 
between SPs and inferences. Rather than examine 
the role of SPs in inferences, they use SPs of a par-
ticular type to derive inferences.  For instance the 
preference of win for the subject player, a nomi-
nalization of play, is used to derive that ?win ? 
play?. Our work can be viewed as complementary 
to the work on extracting semantic inferences and 
paraphrases, since we seek to refine when a given 
inference applies, filtering out incorrect inferences. 
3 Selectional Preference Models 
The aim of this paper is to learn inferential selec-
tional preferences for filtering inference rules. 
Let pi ? pj be an inference rule where p is a bi-
nary semantic relation between two entities x and 
y. Let ?x, p, y? be an instance of relation p. 
Formal task definition: Given an inference rule 
 pi ? pj and the instance ?x, pi, y?, our task is to 
determine if ?x, pj, y? is valid. 
Consider the example in Section 1 where we 
have the inference rule ?X is charged by Y? ? ?Y 
announced the arrest of X?. Our task is to auto-
matically determine that ?federal prosecutors an-
nounced the arrest of Terry Nichols? (i.e., 
?Terry Nichols, pj, federal prosecutors?) is valid 
but that ?CCM telemarketers announced the arrest 
of accounts? is invalid. 
Because the semantic relations p are binary, the 
selectional preferences on their two arguments may 
be either considered jointly or independently. For 
example, the relation p = ?X is charged by Y? 
could have joint SPs: 
 ?Person, Law Enforcement Agent? 
 ?Person, Law Enforcement Agency?  (2) 
 ?Bank Account, Organization? 
or independent SPs: 
 ?Person, *? 
 ?*, Organization? (3) 
 ?*, Law Enforcement Agent? 
This distinction between joint and independent 
selectional preferences constitutes the difference 
between the two models we present in this section. 
The remainder of this section describes the ISP 
approach. In Section 3.1, we describe methods for 
automatically determining the semantic contexts of 
each single relation?s selectional preferences. Sec-
tion 3.2 uses these for developing our inferential 
565
selectional preference models. Finally, we propose 
inference filtering algorithms in Section 3.3. 
3.1 Relational Selectional Preferences 
Resnik (1996) defined the selectional preferences 
of a predicate as the semantic classes of the words 
that appear as its arguments. Similarly, we define 
the relational selectional preferences of a binary 
semantic relation pi as the semantic classes C(x) of 
the words that can be instantiated for x and as the 
semantic classes C(y) of the words that can be in-
stantiated for y. 
The semantic classes C(x) and C(y) can be ob-
tained from a conceptual taxonomy as proposed in 
(Resnik 1996), such as WordNet, or from the 
classes extracted from a word clustering algorithm 
such as CBC (Pantel and Lin 2002). For example, 
given the relation ?X is charged by Y?, its rela-
tional selection preferences from WordNet could 
be {social_group, organism, state?} for X and 
{authority, state, section?} for Y. 
Below we propose joint and independent mod-
els, based on a corpus analysis, for automatically 
determining relational selectional preferences. 
Model 1: Joint Relational Model (JRM) 
Our joint model uses a corpus analysis to learn SPs 
for binary semantic relations by considering their 
arguments jointly, as in example (2). 
Given a large corpus of English text, we first 
find the occurrences of each semantic relation p. 
For each instance ?x, p, y?, we retrieve the sets C(x) 
and C(y) of the semantic classes that x and y be-
long to and accumulate the frequencies of the tri-
ples ?c(x), p, c(y)?, where c(x) ? C(x) and  
c(y) ? C(y)2. 
Each triple ?c(x), p, c(y)? is a candidate selec-
tional preference for p. Candidates can be incorrect 
when: a) they were generated from the incorrect 
sense of a polysemous word; or b) p does not hold 
for the other words in the semantic class. 
Intuitively, we have more confidence in a par-
ticular candidate if its semantic classes are closely 
associated given the relation p. Pointwise mutual 
information (Cover and Thomas 1991) is a com-
monly used metric for measuring this association 
strength between two events e1 and e2: 
                                                     
2 In this paper, the semantic classes C(x) and C(y) are ex-
tracted from WordNet and CBC (described in Section 4.2).  
 ( )( ) ( )21
21
21
,
log);(
ePeP
eeP
eepmi =  (3.1) 
We define our ranking function as the strength 
of association between two semantic classes, cx and 
cy3, given the relation p: 
 ( ) ( )( ) ( )pcPpcP pccPpcpcpmi yx yxyx
,
log; =  (3.2) 
Let |cx, p, cy| denote the frequency of observing 
the instance ?c(x), p, c(y)?. We estimate the prob-
abilities of Equation 3.2 using maximum likeli-
hood estimates over our corpus: 
( ) ?? ?= ,, ,,ppcpcP xx
 ( ) ???= ,, ,, pcppcP yy  ( ) ??= ,, ,,, p cpcpccP yxyx   (3.3) 
Similarly to (Resnik 1996), we estimate the 
above frequencies using: 
( )??
?=?
xcw
x wC
pw
pc
,,
,,
 
( )??
?=?
ycw
y wC
wp
cp
,,
,,
 
( ) ( )??? ?= yx cwcwyx wCwC
wpw
cpc
21 , 21
21 ,,,,
 
where |x, p, y| denotes the frequency of observing 
the instance ?x, p, y? and |C(w)| denotes the number 
of classes to which word w belongs. |C(w)| distrib-
utes w?s mass equally to all of its senses cw. 
Model 2: Independent Relational Model (IRM) 
Because of sparse data, our joint model can miss 
some correct selectional preference pairs. For ex-
ample, given the relation  
 Y announced the arrest of X 
we may find occurrences from our corpus of the 
particular class ?Money Handler? for X and ?Law-
yer? for Y, however we may never see both of 
these classes co-occurring even though they would 
form a valid relational selectional preference. 
To alleviate this problem, we propose a second 
model that is less strict by considering the argu-
ments of the binary semantic relations independ-
ently, as in example (3). 
Similarly to JRM, we extract each instance  
?x, p, y? of each semantic relation p and retrieve the 
set of semantic classes C(x) and C(y) that x and y 
belong to, accumulating the frequencies of the tri-
ples ?c(x), p, *? and ?*, p, c(y)?, where  
c(x) ? C(x) and c(y) ? C(y). 
All tuples ?c(x), p, *? and ?*, p, c(y)? are candi-
date selectional preferences for p. We rank candi-
dates by the probability of the semantic class given 
the relation p, according to Equations 3.3. 
                                                     
3 cx and cy are shorthand for c(x) and c(y) in our equations. 
566
3.2 Inferential Selectional Preferences 
Whereas in Section 3.1 we learned selectional 
preferences for the arguments of a relation p, in 
this section we learn selectional preferences for the 
arguments of an inference rule pi ? pj. 
Model 1: Joint Inferential Model (JIM) 
Given an inference rule pi ? pj, our joint model 
defines the set of inferential SPs as the intersection 
of the relational SPs for pi and pj, as defined in the 
Joint Relational Model (JRM). For example, sup-
pose relation pi = ?X is charged by Y? gives the 
following SP scores under the JRM: 
 ?Person, pi, Law Enforcement Agent? = 1.45 
 ?Person, pi, Law Enforcement Agency? = 1.21  
 ?Bank Account, pi, Organization? = 0.97 
and that pj = ?Y announced the arrest of X? gives 
the following SP scores under the JRM: 
 ?Law Enforcement Agent, pj, Person? = 2.01 
 ?Reporter, pj, Person? = 1.98  
 ?Law Enforcement Agency, pj, Person? = 1.61 
The intersection of the two sets of SPs forms the 
candidate inferential SPs for the inference pi ? pj: 
 ?Law Enforcement Agent, Person? 
 ?Law Enforcement Agency, Person? 
We rank the candidate inferential SPs according 
to three ways to combine their relational SP scores, 
using the minimum, maximum, and average of the 
SPs. For example, for ?Law Enforcement Agent, 
Person?, the respective scores would be 1.45, 2.01, 
and 1.73. These different ranking strategies pro-
duced nearly identical results in our experiments, 
as discussed in Section 5. 
Model 2: Independent Inferential Model (IIM) 
Our independent model is the same as the joint 
model above except that it computes candidate in-
ferential SPs using the Independent Relational 
Model (IRM) instead of the JRM. Consider the 
same example relations pi and pj from the joint 
model and suppose that the IRM gives the follow-
ing relational SP scores for pi: 
 ?Law Enforcement Agent, pi, *? = 3.43 
 ?*, pi, Person? = 2.17  
 ?*, pi, Organization? = 1.24 
and the following relational SP scores for pj: 
 ?*, pj, Person? = 2.87 
 ?Law Enforcement Agent, pj, *? = 1.92  
 ?Reporter, pj, *? = 0.89 
The intersection of the two sets of SPs forms the 
candidate inferential SPs for the inference pi ? pj: 
 ?Law Enforcement Agent, *? 
 ?*, Person?  
We use the same minimum, maximum, and av-
erage ranking strategies as in JIM. 
3.3 Filtering Inferences 
Given an inference rule pi ? pj and the instance  
?x, pi, y?, the system?s task is to determine whether 
?x, pj, y? is valid. Let C(w) be the set of semantic 
classes c(w) to which word w belongs. Below we 
present three filtering algorithms which range from 
the least to the most permissive: 
? ISP.JIM, accepts the inference ?x, pj, y? if the 
inferential SP ?c(x), pj, c(y)? was admitted by the 
Joint Inferential Model for some c(x) ? C(x) and 
c(y) ? C(y). 
? ISP.IIM.?, accepts the inference ?x, pj, y? if the 
inferential SPs ?c(x), pj, *? AND ?*, pj, c(y)? were 
admitted by the Independent Inferential Model 
for some c(x) ? C(x) and c(y) ? C(y) . 
? ISP.IIM.?, accepts the inference ?x, pj, y? if the 
inferential SP ?c(x), pj, *? OR ?*, pj, c(y)? was 
admitted by the Independent Inferential Model 
for some c(x) ? C(x) and c(y) ? C(y) . 
Since both JIM and IIM use a ranking score in 
their inferential SPs, each filtering algorithm can 
be tuned to be more or less strict by setting an ac-
ceptance threshold on the ranking scores or by se-
lecting only the top ? percent highest ranking SPs. 
In our experiments, reported in Section 5, we 
tested each model using various values of ?. 
4 Experimental Methodology 
This section describes the methodology for testing 
our claim that inferential selectional preferences 
can be learned to filter incorrect inferences. 
Given a collection of inference rules of the form 
pi ? pj, our task is to determine whether a particu-
lar instance ?x, pj, y? holds given that ?x, pi, y? 
holds4. In the next sections, we describe our collec-
tion of inference rules, the semantic classes used 
for forming selectional preferences, and evaluation 
criteria for measuring the filtering quality. 
                                                     
4 Recall that the inference rules we consider in this paper are 
not necessary strict logical inference rules, but plausible in-
ference rules; see Section 3. 
567
4.1 Inference Rules 
Our models for learning inferential selectional 
preferences can be applied to any collection of in-
ference rules between binary semantic relations. In 
this paper, we focus on the inference rules con-
tained in the DIRT resource (Lin and Pantel 2001). 
DIRT consists of over 12 million rules which were 
extracted from a 1GB newspaper corpus (San Jose 
Mercury, Wall Street Journal and AP Newswire 
from the TREC-9 collection). For example, here 
are DIRT?s top 3 inference rules for ?X solves Y?: 
 ?Y is solved by X?, ?X resolves Y?, ?X finds a solution to Y? 
4.2 Semantic Classes 
The choice of semantic classes is of great impor-
tance for selectional preference. One important 
aspect is the granularity of the classes. Too general 
a class will provide no discriminatory power while 
too fine-grained a class will offer little generaliza-
tion and apply in only extremely few cases. 
The absence of an attested high-quality set of 
semantic classes for this task makes discovering 
preferences difficult. Since many of the criteria for 
developing such a set are not even known, we de-
cided to experiment with two very different sets of 
semantic classes, in the hope that in addition to 
learning semantic preferences, we might also un-
cover some clues for the eventual decisions about 
what makes good semantic classes in general. 
Our first set of semantic classes was directly ex-
tracted from the output of the CBC clustering algo-
rithm (Pantel and Lin 2002). We applied CBC to 
the TREC-9 and TREC-2002 (Aquaint) newswire 
collections consisting of over 600 million words. 
CBC generated 1628 noun concepts and these were 
used as our semantic classes for SPs. 
Secondly, we extracted semantic classes from 
WordNet 2.1 (Fellbaum 1998). In the absence of 
any externally motivated distinguishing features 
(for example, the Basic Level categories from Pro-
totype Theory, developed by Eleanor Rosch 
(1978)), we used the simple but effective method 
of manually truncating the noun synset hierarchy5 
and considering all synsets below each cut point as 
part of the semantic class at that node. To select 
the cut points, we inspected several different hier-
archy levels and found the synsets at a depth of 4 
                                                     
5 Only nouns are considered since DIRT semantic relations 
connect only nouns. 
to form the most natural semantic classes. Since 
the noun hierarchy in WordNet has an average 
depth of 12, our truncation created a set of con-
cepts considerably coarser-grained than WordNet 
itself. The cut produced 1287 semantic classes, a 
number similar to the classes in CBC. To properly 
test WordNet as a source of semantic classes for 
our selectional preferences, we would need to ex-
periment with different extraction algorithms. 
4.3 Evaluation Criteria 
The goal of the filtering task is to minimize false 
positives (incorrectly accepted inferences) and 
false negatives (incorrectly rejected inferences). A 
standard methodology for evaluating such tasks is 
to compare system filtering results with a gold 
standard using a confusion matrix. A confusion 
matrix captures the filtering performance on both 
correct and incorrect inferences: 
  
where A represents the number of correct instances 
correctly identified by the system, D represents the 
number of incorrect instances correctly identified 
by the system, B represents the number of false 
positives and C represents the number of false 
negatives. To compare systems, three key meas-
ures are used to summarize confusion matrices: 
? Sensitivity, defined as CA
A
+ , captures a filter?s 
probability of accepting correct inferences; 
? Specificity, defined as DB
D
+ , captures a filter?s 
probability of rejecting incorrect inferences; 
? Accuracy, defined as DCBA
DA
+++
+ , captures the 
probability of a filter being correct. 
5 Experimental Results 
In this section, we provide empirical evidence to 
support the main claim of this paper. 
Given a collection of DIRT inference rules of 
the form pi ? pj, our experiments, using the meth-
odology of Section 4, evaluate the capability of our 
ISP models for determining if ?x, pj, y? holds given 
that ?x, pi, y? holds. 
GOLD STANDARD   
1 0 
1 A B 
SY
ST
E
M
 
0 C D 
568
5.1 Experimental Setup 
Model Implementation 
For each filtering algorithm in Section 3.3, ISP.JIM, 
ISP.IIM.?, and ISP.IIM.?, we trained their probabil-
istic models using corpus statistics extracted from 
the 1999 AP newswire collection (part of the 
TREC-2002 Aquaint collection) consisting of ap-
proximately 31 million words. We used the Mini-
par parser (Lin 1993) to match DIRT patterns in 
the text. This permits exact matches since DIRT 
inference rules are built from Minipar parse trees. 
For each system, we experimented with the dif-
ferent ways of combining relational SP scores: 
minimum, maximum, and average (see Section 
3.2). Also, we experimented with various values 
for the ? parameter described in Section 3.3. 
Gold Standard Construction 
In order to compute the confusion matrices de-
scribed in Section 4.3, we must first construct a 
representative set of inferences and manually anno-
tate them as correct or incorrect. 
We randomly selected 100 inference rules of the 
form pi ? pj from DIRT. For each pattern pi, we 
then extracted its instances from the Aquaint 1999 
AP newswire collection (approximately 22 million 
words), and randomly selected 10 distinct in-
stances, resulting in a total of 1000 instances. For 
each instance of pi, applying DIRT?s inference rule 
would assert the instance ?x, pj, y?. Our evaluation 
tests how well our models can filter these so that 
only correct inferences are made. 
To form the gold standard, two human judges 
were asked to tag each instance ?x, pj, y? as correct 
or incorrect. For example, given a randomly se-
lected inference rule ?X is charged by Y ? Y an-
nounced the arrest of X? and the instance ?Terry 
Nichols was charged by federal prosecutors?, the 
judges must determine if the instance ?federal 
prosecutors, Y announced the arrest of X, Terry 
Nichols? is correct. The judges were asked to con-
sider the following two criteria for their decision: 
? ?x, pj, y? is a semantically meaningful instance; 
? The inference pi ? pj holds for this instance. 
Judges found that annotation decisions can range 
from trivial to difficult. The differences often were 
in the instances for which one of the judges fails to 
see the right context under which the inference 
could hold. To minimize disagreements, the judges 
went through an extensive round of training. 
To that end, the 1000 instances ?x, pj, y? were 
split into DEV and TEST sets, 500 in each. The 
two judges trained themselves by annotating DEV 
together. The TEST set was then annotated sepa-
rately to verify the inter-annotator agreement and 
to verify whether the task is well-defined. The 
kappa statistic (Siegel and Castellan Jr. 1988) was 
? = 0.72. For the 70 disagreements between the 
judges, a third judge acted as an adjudicator. 
Baselines 
We compare our ISP algorithms to the following 
baselines: 
? B0: Rejects all inferences; 
? B1: Accepts all inferences; 
? Rand: Randomly accepts or rejects inferences. 
One alternative to our approach is admit instances 
on the Web using literal search queries. We inves-
tigated this technique but discarded it due to subtle 
yet critical issues with pattern canonicalization that 
resulted in rejecting nearly all inferences. How-
ever, we are investigating other ways of using Web 
corpora for this task. 
Table 1. Filtering quality of best performing systems according to the evaluation criteria defined in Section 4.3 on 
the TEST set ? the reported systems were selected based on the Accuracy criterion on the DEV set. 
PARAMETERS SELECTED FROM DEV SET 
SYSTEM 
RANKING STRATEGY ? (%) 
SENSITIVITY 
(95% CONF) 
SPECIFICITY 
(95% CONF) 
ACCURACY 
(95% CONF) 
B0 - - 0.00?0.00 1.00?0.00 0.50?0.04 
B1 - - 1.00?0.00 0.00?0.00 0.49?0.04 
Random - - 0.50?0.06 0.47?0.07 0.50?0.04 
ISP.JIM maximum 100 0.17?0.04 0.88?0.04 0.53?0.04 
ISP.IIM.? maximum 100 0.24?0.05 0.84?0.04 0.54?0.04 CBC 
ISP.IIM.? maximum 90 0.73?0.05 0.45?0.06 0.59?0.04? 
ISP.JIM minimum 40 0.20?0.06 0.75?0.06 0.47?0.04 
ISP.IIM.? minimum 10 0.33?0.07 0.77?0.06 0.55?0.04 WordNet 
ISP.IIM.? minimum 20 0.87?0.04 0.17?0.05 0.51?0.05 
? Indicates statistically significant results (with 95% confidence) when compared with all baseline systems using pairwise t-test. 
569
5.2 Filtering Quality 
For each ISP algorithm and parameter combina-
tion, we constructed a confusion matrix on the de-
velopment set and computed the system sensitivity, 
specificity and accuracy as described in Section 
4.3. This resulted in 180 experiments on the devel-
opment set. For each ISP algorithm and semantic 
class source, we selected the best parameter com-
binations according to the following criteria: 
? Accuracy: This system has the best overall abil-
ity to correctly accept and reject inferences. 
? 90%-Specificity: Several formal semantics and 
textual entailment researchers have commented 
that inference rule collections like DIRT are dif-
ficult to use due to low precision. Many have 
asked for filtered versions that remove incorrect 
inferences even at the cost of removing correct 
inferences. In response, we show results for the 
system achieving the best sensitivity while main-
taining at least 90% specificity on the DEV set. 
We evaluated the selected systems on the TEST 
set. Table 1 summarizes the quality of the systems 
selected according to the Accuracy criterion. The 
best performing system, ISP.IIM.?, performed  sta-
tistically significantly better than all three base-
lines. The best system according to the 90%-
Specificity criteria was ISP.JIM, which coinciden-
tally has the highest accuracy for that model as 
shown in Table 16. This result is very promising 
for researchers that require highly accurate infer-
ence rules since they can use ISP.JIM and expect to 
recall 17% of the correct inferences by only ac-
cepting false positives 12% of the time. 
Performance and Error Analysis 
Figures 1a) and 1b) present the full confusion ma-
trices for the most accurate and highly specific sys-
tems, with both systems selected on the DEV set. 
The most accurate system was ISP.IIM.?, which is 
the most permissive of the algorithms. This sug-
                                                     
6 The reported sensitivity of ISP.Joint in Table 1 is below 
90%, however it achieved 90.7% on the DEV set. 
gests that a larger corpus for learning SPs may be 
needed to support stronger performance on the 
more restrictive methods. The system in Figure 
1b), selected for maximizing sensitivity while 
maintaining high specificity, was 70% correct in 
predicting correct inferences. 
Figure 2 illustrates the ROC curve for all our 
systems and parameter combinations on the TEST 
set. ROC curves plot the true positive rate against 
the false positive rate. The near-diagonal line plots 
the three baseline systems. 
Several trends can be observed from this figure. 
First, systems using the semantic classes from 
WordNet tend to perform less well than systems 
using CBC classes. As discussed in Section 4.2, we 
used a very simplistic extraction of semantic 
classes from WordNet. The results in Figure 2 
serve as a lower bound on what could be achieved 
with a better extraction from WordNet. Upon in-
spection of instances that WordNet got incorrect 
but CBC got correct, it seemed that CBC had a 
much higher lexical coverage than WordNet. For 
example, several of the instances contained proper 
names as either the X or Y argument (WordNet has 
poor proper name coverage). When an argument is 
not covered by any class, the inference is rejected. 
Figure 2 also illustrates how our three different 
ISP algorithms behave. The strictest filters, ISP.JIM 
and ISP.IIM.?, have the poorest overall perform-
ance but, as expected, have a generally very low 
rate of false positives. ISP.IIM.?, which is a much 
more permissive filter because it does not require 
ROC on the TEST Set
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
1-Specificity
S
en
si
tiv
ity
Baselines WordNet CBC ISP.JIM ISP.IIM.AND ISP.IIM.OR
Figure 2. ROC curves for our systems on TEST. 
GOLD STANDARD a)  
1 0 
1 184 139 
SY
ST
E
M
 
0 63 114 
GOLD STANDARD b)  
1 0 
1 42 28 
SY
ST
E
M
 
0 205 225 
Figure 1. Confusion matrices for a) ISP.IIM.? ? best 
Accuracy; and b) ISP.JIM ? best 90%-Specificity.
570
both arguments of a relation to match, has gener-
ally many more false positives but has an overall 
better performance. 
We did not include in Figure 2 an analysis of the 
minimum, maximum, and average ranking strate-
gies presented in Section 3.2 since they generally 
produced nearly identical results. 
For the most accurate system, ISP.IIM.?, we ex-
plored the impact of the cutoff threshold ? on the 
sensitivity, specificity, and accuracy, as shown in 
Figure 3. Rather than step the values by 10% as we 
did on the DEV set, here we stepped the threshold 
value by 2% on the TEST set. The more permis-
sive values of ? increase sensitivity at the expense 
of specificity. Interestingly, the overall accuracy 
remained fairly constant across the entire range of 
?, staying within 0.05 of the maximum of 0.62 
achieved at ?=30%. 
Finally, we manually inspected several incorrect 
inferences that were missed by our filters. A com-
mon source of errors was due to the many incorrect 
?antonymy? inference rules generated by DIRT, 
such as ?X is rejected in Y???X is accepted in Y?. 
This recognized problem in DIRT occurs because 
of the distributional hypothesis assumption used to 
form the inference rules. Our ISP algorithms suffer 
from a similar quandary since, typically, antony-
mous relations take the same sets of arguments for 
X (and Y). For these cases, ISP algorithms learn 
many selectional preferences that accept the same 
types of entities as those that made DIRT learn the 
inference rule in the first place, hence ISP will not 
filter out many incorrect inferences. 
6 Conclusion 
We presented algorithms for learning what we call 
inferential selectional preferences, and presented 
evidence that learning selectional preferences can 
be useful in filtering out incorrect inferences. Fu-
ture work in this direction includes further explora-
tion of the appropriate inventory of semantic 
classes used as SP?s. This work constitutes a step 
towards better understanding of the interaction of 
selectional preferences and inferences, bridging 
these two aspects of semantics. 
References 
Barzilay, R.; and McKeown, K.R. 2001.Extracting Paraphrases from a 
Parallel Corpus. In Proceedings of ACL 2001. pp. 50?57. Toulose, 
France. 
Baker, C.F.; Fillmore, C.J.; and Lowe, J.B. 1998. The Berkeley 
FrameNet Project. In Proceedings of COLING/ACL 1998.  pp. 86-
90. Montreal, Canada. 
Cover, T.M. and Thomas, J.A. 1991. Elements of Information Theory. 
John Wiley & Sons. 
Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. MIT 
Press. 
Harabagiu, S.; and Hickl, A. 2006. Methods for Using Textual 
Entailment in Open-Domain Question Answering. In Proceedings 
of ACL 2006.  pp. 905-912. Sydney, Australia. 
Katz, J.; and Fodor, J.A. 1963. The Structure of a Semantic Theory. 
Language, vol 39. pp.170?210.  
Lenat, D. 1995. CYC: A large-scale investment in knowledge 
infrastructure. Communications of the ACM, 38(11):33?38. 
Levin, B. 1993. English Verb Classes and Alternations: A Preliminary 
Investigation. University of Chicago Press, Chicago, IL. 
Light, M. and Greiff, W.R. 2002. Statistical Models for the Induction 
and Use of Selectional Preferences. Cognitive Science,26:269?281. 
Lin, D. 1993. Parsing Without OverGeneration. In Proceedings of  
ACL-93. pp. 112-120. Columbus, OH. 
Lin, D. and Pantel, P. 2001. Discovery of Inference Rules for 
Question Answering. Natural Language Engineering 7(4):343-360. 
Moldovan, D.I.; Clark, C.; Harabagiu, S.M.; Maiorano, S.J. 2003. 
COGEX: A Logic Prover for Question Answering. In Proceedings 
of HLT-NAACL-03. pp. 87-93. Edmonton, Canada. 
Pantel, P. and Lin, D. 2002. Discovering Word Senses from Text. In 
Proceedings of KDD-02. pp. 613-619. Edmonton, Canada. 
Resnik, P. 1996. Selectional Constraints: An Information-Theoretic 
Model and its Computational Realization. Cognition, 61:127?159. 
Romano, L.; Kouylekov, M.; Szpektor, I.; Dagan, I.; Lavelli, A. 2006. 
Investigating a Generic Paraphrase-Based Approach for Relation 
Extraction. In EACL-2006. pp. 409-416. Trento, Italy. 
Rosch, E. 1978. Human Categorization. In E. Rosch and B.B. Lloyd 
(eds.) Cognition and Categorization. Hillsdale, NJ: Erlbaum.  
Siegel, S. and Castellan Jr., N. J. 1988. Nonparametric Statistics for 
the Behavioral Sciences. McGraw-Hill. 
Szpektor, I.; Tanev, H.; Dagan, I.; and Coppola, B. 2004. Scaling 
web-based acquisition of entailment relations. In Proceedings of 
EMNLP 2004. pp. 41-48. Barcelona,Spain. 
Wilks, Y.; and Fass, D. 1992. Preference Semantics: a family history. 
Computing and Mathematics with Applications, 23(2). A shorter 
version in the second edition of the Encyclopedia of Artificial 
Intelligence, (ed.) S. Shapiro. 
Zanzotto, F.M.; Pennacchiotti, M.; Pazienza, M.T. 2006. Discovering 
Asymmetric Entailment Relations between Verbs using Selectional 
Preferences. In COLING/ACL-06. pp. 849-856. Sydney, Australia. 
Figure 3. ISP.IIM.? (Best System)?s performance 
variation over different values for the ? threshold. 
ISP.IIM.OR (Best System)'s Performance vs. Tau-Thresholds
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 10 20 30 40 50 60 70 80 90 100
Tau-Thresholds
Sensitivity Specificity Accuracy
571
Scaling Web-based Acquisition of Entailment Relations
Idan Szpektor
idan@szpektor.net
?ITC-Irst, Via Sommarive, 18 (Povo) - 38050 Trento, Italy
?DIT - University of Trento, Via Sommarive, 14 (Povo) - 38050 Trento, Italy
?Department of Computer Science, Bar Ilan University - Ramat Gan 52900, Israel
Department of Computer Science, Tel Aviv University - Tel Aviv 69978, Israel
Hristo Tanev?
tanev@itc.it
Ido Dagan?
dagan@cs.biu.ac.il
Bonaventura Coppola??
coppolab@itc.it
Abstract
Paraphrase recognition is a critical step for nat-
ural language interpretation. Accordingly, many
NLP applications would benefit from high coverage
knowledge bases of paraphrases. However, the scal-
ability of state-of-the-art paraphrase acquisition ap-
proaches is still limited. We present a fully unsuper-
vised learning algorithm for Web-based extraction
of entailment relations, an extended model of para-
phrases. We focus on increased scalability and gen-
erality with respect to prior work, eventually aiming
at a full scale knowledge base. Our current imple-
mentation of the algorithm takes as its input a verb
lexicon and for each verb searches the Web for re-
lated syntactic entailment templates. Experiments
show promising results with respect to the ultimate
goal, achieving much better scalability than prior
Web-based methods.
1 Introduction
Modeling semantic variability in language has
drawn a lot of attention in recent years. Many ap-
plications like QA, IR, IE and Machine Translation
(Moldovan and Rus, 2001; Hermjakob et al, 2003;
Jacquemin, 1999) have to recognize that the same
meaning can be expressed in the text in a huge vari-
ety of surface forms. Substantial research has been
dedicated to acquiring paraphrase patterns, which
represent various forms in which a certain meaning
can be expressed.
Following (Dagan and Glickman, 2004) we ob-
serve that a somewhat more general notion needed
for applications is that of entailment relations (e.g.
(Moldovan and Rus, 2001)). These are directional
relations between two expressions, where the mean-
ing of one can be entailed from the meaning of the
other. For example ?X acquired Y? entails ?X owns
Y?. These relations provide a broad framework for
representing and recognizing semantic variability,
as proposed in (Dagan and Glickman, 2004). For
example, if a QA system has to answer the question
?Who owns Overture?? and the corpus includes the
phrase ?Yahoo acquired Overture?, the system can
use the known entailment relation to conclude that
this phrase really indicates the desired answer. More
examples of entailment relations, acquired by our
method, can be found in Table 1 (section 4).
To perform such inferences at a broad scale, ap-
plications need to possess a large knowledge base
(KB) of entailment patterns. We estimate such a
KB should contain from between a handful to a few
dozens of relations per meaning, which may sum
to a few hundred thousands of relations for a broad
domain, given that a typical lexicon includes tens of
thousands of words.
Our research goal is to approach unsupervised ac-
quisition of such a full scale KB. We focus on de-
veloping methods that acquire entailment relations
from the Web, the largest available resource. To
this end substantial improvements are needed in or-
der to promote scalability relative to current Web-
based approaches. In particular, we address two
major goals: reducing dramatically the complexity
of required auxiliary inputs, thus enabling to apply
the methods at larger scales, and generalizing the
types of structures that can be acquired. The algo-
rithms described in this paper were applied for ac-
quiring entailment relations for verb-based expres-
sions. They successfully discovered several rela-
tions on average per each randomly selected expres-
sion.
2 Background and Motivations
This section provides a qualitative view of prior
work, emphasizing the perspective of aiming at a
full-scale paraphrase resource. As there are still
no standard benchmarks, current quantitative results
are not comparable in a consistent way.
The major idea in paraphrase acquisition is often
to find linguistic structures, here termed templates,
that share the same anchors. Anchors are lexical
elements describing the context of a sentence. Tem-
plates that are extracted from different sentences
and connect the same anchors in these sentences,
are assumed to paraphrase each other. For example,
the sentences ?Yahoo bought Overture? and ?Yahoo
acquired Overture? share the anchors {X=Yahoo,
Y =Overture}, suggesting that the templates ?X buy
Y? and ?X acquire Y? paraphrase each other. Algo-
rithms for paraphrase acquisition address two prob-
lems: (a) finding matching anchors and (b) identify-
ing template structure, as reviewed in the next two
subsections.
2.1 Finding Matching Anchors
The prominent approach for paraphrase learning
searches sentences that share common sets of mul-
tiple anchors, assuming they describe roughly the
same fact or event. To facilitate finding many
matching sentences, highly redundant comparable
corpora have been used. These include multiple
translations of the same text (Barzilay and McKe-
own, 2001) and corresponding articles from multi-
ple news sources (Shinyama et al, 2002; Pang et
al., 2003; Barzilay and Lee, 2003). While facilitat-
ing accuracy, we assume that comparable corpora
cannot be a sole resource due to their limited avail-
ability.
Avoiding a comparable corpus, (Glickman and
Dagan, 2003) developed statistical methods that
match verb paraphrases within a regular corpus.
Their limited scale results, obtaining several hun-
dred verb paraphrases from a 15 million word cor-
pus, suggest that much larger corpora are required.
Naturally, the largest available corpus is the Web.
Since exhaustive processing of the Web is not feasi-
ble, (Duclaye et al, 2002) and (Ravichandran and
Hovy, 2002) attempted bootstrapping approaches,
which resemble the mutual bootstrapping method
for Information Extraction of (Riloff and Jones,
1999). These methods start with a provided known
set of anchors for a target meaning. For example,
the known anchor set {Mozart, 1756} is given as in-
put in order to find paraphrases for the template ?X
born in Y?. Web searching is then used to find occur-
rences of the input anchor set, resulting in new tem-
plates that are supposed to specify the same relation
as the original one (?born in?). These new templates
are then exploited to get new anchor sets, which
are subsequently processed as the initial {Mozart,
1756}. Eventually, the overall procedure results in
an iterative process able to induce templates from
anchor sets and vice versa.
The limitation of this approach is the requirement
for one input anchor set per target meaning. Prepar-
ing such input for all possible meanings in broad
domains would be a huge task. As will be explained
below, our method avoids this limitation by find-
ing all anchor sets automatically in an unsupervised
manner.
Finally, (Lin and Pantel, 2001) present a notably
different approach that relies on matching sepa-
rately single anchors. They limit the allowed struc-
ture of templates only to paths in dependency parses
connecting two anchors. The algorithm constructs
for each possible template two feature vectors, rep-
resenting its co-occurrence statistics with the two
anchors. Two templates with similar vectors are
suggested as paraphrases (termed inference rule).
Matching of single anchors relies on the gen-
eral distributional similarity principle and unlike the
other methods does not require redundancy of sets
of multiple anchors. Consequently, a much larger
number of paraphrases can be found in a regular
corpus. Lin and Pantel report experiments for 9
templates, in which their system extracted 10 cor-
rect inference rules on average per input template,
from 1GB of news data. Yet, this method also suf-
fers from certain limitations: (a) it identifies only
templates with pre-specified structures; (b) accuracy
seems more limited, due to the weaker notion of
similarity; and (c) coverage is limited to the scope
of an available corpus.
To conclude, several approaches exhaustively
process different types of corpora, obtaining vary-
ing scales of output. On the other hand, the Web is
a huge promising resource, but current Web-based
methods suffer serious scalability constraints.
2.2 Identifying Template Structure
Paraphrasing approaches learn different kinds of
template structures. Interesting algorithms are pre-
sented in (Pang et al, 2003; Barzilay and Lee,
2003). They learn linear patterns within similar con-
texts represented as finite state automata. Three
classes of syntactic template learning approaches
are presented in the literature: learning of predicate
argument templates (Yangarber et al, 2000), learn-
ing of syntactic chains (Lin and Pantel, 2001) and
learning of sub-trees (Sudo et al, 2003). The last
approach is the most general with respect to the tem-
plate form. However, its processing time increases
exponentially with the size of the templates.
As a conclusion, state of the art approaches still
learn templates of limited form and size, thus re-
stricting generality of the learning process.
3 The TE/ASE Acquisition Method
Motivated by prior experience, we identify two ma-
jor goals for scaling Web-based acquisition of en-
tailment relations: (a) Covering the broadest pos-
sible range of meanings, while requiring minimal
input and (b) Keeping template structures as gen-
eral as possible. To address the first goal we re-
quire as input only a phrasal lexicon of the rel-
evant domain (including single words and multi-
word expressions). Broad coverage lexicons are
widely available or may be constructed using known
term acquisition techniques, making it a feasible
and scalable input requirement. We then aim to
acquire entailment relations that include any of the
lexicon?s entries. The second goal is addressed by a
novel algorithm for extracting the most general tem-
plates being justified by the data.
For each lexicon entry, denoted a pivot, our
extraction method performs two phases: (a) ex-
tract promising anchor sets for that pivot (ASE,
Section 3.1), and (b) from sentences contain-
ing the anchor sets, extract templates for which
an entailment relation holds with the pivot (TE,
Section 3.2). Examples for verb pivots are:
?acquire?, ?fall to?, ?prevent? . We will use the pivot
?prevent? for examples through this section.
Before presenting the acquisition method we first
define its output. A template is a dependency parse-
tree fragment, with variable slots at some tree nodes
(e.g. ?X subj? prevent obj? Y? ). An entailment rela-
tion between two templates T1 and T2 holds if
the meaning of T2 can be inferred from the mean-
ing of T1 (or vice versa) in some contexts, but
not necessarily all, under the same variable instan-
tiation. For example, ?X subj? prevent obj? Y? entails
?X
subj
? reduce
obj
? Y risk? because the sentence ?as-
pirin reduces heart attack risk? can be inferred from
?aspirin prevents a first heart attack?. Our output
consists of pairs of templates for which an entail-
ment relation holds.
3.1 Anchor Set Extraction (ASE)
The goal of this phase is to find a substantial num-
ber of promising anchor sets for each pivot. A good
anchor-set should satisfy a proper balance between
specificity and generality. On one hand, an anchor
set should correspond to a sufficiently specific set-
ting, so that entailment would hold between its dif-
ferent occurrences. On the other hand, it should be
sufficiently frequent to appear with different entail-
ing templates.
Finding good anchor sets based on just the input
pivot is a hard task. Most methods identify good re-
peated anchors ?in retrospect?, that is after process-
ing a full corpus, while previous Web-based meth-
ods require at least one good anchor set as input.
Given our minimal input, we needed refined crite-
ria that identify a priori the relatively few promising
anchor sets within a sample of pivot occurrences.
ASE ALGORITHM STEPS:
For each pivot (a lexicon entry)
1. Create a pivot template, Tp
2. Construct a parsed sample corpus S for Tp:
(a) Retrieve an initial sample from the Web
(b) Identify associated phrases for the pivot
(c) Extend S using the associated phrases
3. Extract candidate anchor sets from S:
(a) Extract slot anchors
(b) Extract context anchors
4. Filter the candidate anchor sets:
(a) by absolute frequency
(b) by conditional pivot probability
Figure 1: Outline of the ASE algorithm.
The ASE algorithm (presented in Figure 1) per-
forms 4 main steps.
STEP (1) creates a complete template, called the
pivot template and denoted Tp, for the input pivot,
denoted P . Variable slots are added for the ma-
jor types of syntactic relations that interact with P ,
based on its syntactic type. These slots enable us to
later match Tp with other templates. For verbs, we
add slots for a subject and for an object or a modifier
(e.g. ?X subj? prevent obj? Y? ).
STEP (2) constructs a sample corpus, denoted S,
for the pivot template. STEP (2.A) utilizes a Web
search engine to initialize S by retrieving sentences
containing P . The sentences are parsed by the
MINIPAR dependency parser (Lin, 1998), keeping
only sentences that contain the complete syntactic
template Tp (with all the variables instantiated).
STEP (2.B) identifies phrases that are statistically
associated with Tp in S. We test all noun-phrases
in S , discarding phrases that are too common on
the Web (absolute frequency higher than a thresh-
old MAXPHRASEF), such as ?desire?. Then we se-
lect the N phrases with highest tf ?idf score1. These
phrases have a strong collocation relationship with
the pivot P and are likely to indicate topical (rather
than anecdotal) occurrences of P . For example, the
phrases ?patient? and ?American Dental Associa-
tion?, which indicate contexts of preventing health
problems, were selected for the pivot ?prevent?. Fi-
1Here, tf ?idf = freqS(X) ? log
(
N
freqW (X)
)
where freqS(X) is the number of occurrences in S containing
X , N is the total number of Web documents, and freqW (X)
is the number of Web documents containing X .
nally, STEP (2.C) expands S by querying the Web
with the both P and each of the associated phrases,
adding the retrieved sentences to S as in step (2.a).
STEP (3) extracts candidate anchor sets for Tp.
From each sentence in S we try to generate one can-
didate set, containing noun phrases whose Web fre-
quency is lower than MAXPHRASEF. STEP (3.A)
extracts slot anchors ? phrases that instantiate the
slot variables of Tp. Each anchor is marked
with the corresponding slot. For example, the
anchors {antibioticssubj? , miscarriage obj?} were ex-
tracted from the sentence ?antibiotics in pregnancy
prevent miscarriage?.
STEP (3.B) tries to extend each candidate set with
one additional context anchor, in order to improve
its specificity. This anchor is chosen as the highest
tf ?idf scoring phrase in the sentence, if it exists. In
the previous example, ?pregnancy? is selected.
STEP (4) filters out bad candidate anchor sets by
two different criteria. STEP (4.A) maintains only
candidates with absolute Web frequency within a
threshold range [MINSETF, MAXSETF], to guaran-
tee an appropriate specificity-generality level. STEP
(4.B) guarantees sufficient (directional) association
between the candidate anchor set c and Tp, by esti-
mating
Prob(Tp|c) ?
freqW (P ? c)
freqW (c)
where freqW is Web frequency and P is the pivot.
We maintain only candidates for which this prob-
ability falls within a threshold range [SETMINP,
SETMAXP]. Higher probability often corresponds
to a strong linguistic collocation between the
candidate and Tp, without any semantic entail-
ment. Lower probability indicates coincidental co-
occurrence, without a consistent semantic relation.
The remaining candidates in S become the in-
put anchor-sets for the template extraction phase,
for example, {Aspirinsubj? , heart attackobj?} for ?pre-
vent?.
3.2 Template Extraction (TE)
The Template Extraction algorithm accepts as its in-
put a list of anchor sets extracted from ASE for each
pivot template. Then, TE generates a set of syntactic
templates which are supposed to maintain an entail-
ment relationship with the initial pivot template. TE
performs three main steps, described in the follow-
ing subsections:
1. Acquisition of a sample corpus from the Web.
2. Extraction of maximal most general templates
from that corpus.
3. Post-processing and final ranking of extracted
templates.
3.2.1 Acquisition of a sample corpus from the
Web
For each input anchor set, TE acquires from the
Web a sample corpus of sentences containing it.
For example, a sentence from the sample corpus
for {aspirin, heart attack} is: ?Aspirin stops heart
attack??. All of the sample sentences are then
parsed with MINIPAR (Lin, 1998), which gener-
ates from each sentence a syntactic directed acyclic
graph (DAG) representing the dependency structure
of the sentence. Each vertex in this graph is labeled
with a word and some morphological information;
each graph edge is labeled with the syntactic rela-
tion between the words it connects.
TE then substitutes each slot anchor (see section
3.1) in the parse graphs with its corresponding slot
variable. Therefore, ?Aspirin stops heart attack??
will be transformed into ?X stop Y?. This way all
the anchors for a certain slot are unified under the
same variable name in all sentences. The parsed
sentences related to all of the anchor sets are sub-
sequently merged into a single set of parse graphs
S = {P1, P2, . . . , Pn} (see P1 and P2 in Figure 2).
3.2.2 Extraction of maximal most general
templates
The core of TE is a General Structure Learning al-
gorithm (GSL ) that is applied to the set of parse
graphs S resulting from the previous step. GSL
extracts single-rooted syntactic DAGs, which are
named spanning templates since they must span at
least over Na slot variables, and should also ap-
pear in at least Nr sentences from S (In our exper-
iments we set Na=2 and Nr=2). GSL learns maxi-
mal most general templates: they are spanning tem-
plates which, at the same time, (a) cannot be gener-
alized by further reduction and (b) cannot be further
extended keeping the same generality level.
In order to properly define the notion of maximal
most general templates, we introduce some formal
definitions and notations.
DEFINITION: For a spanning template t we define
a sentence set, denoted with ?(t), as the set of all
parsed sentences in S containing t.
For each pair of templates t1 and t2, we use the no-
tation t1  t2 to denote that t1 is included as a sub-
graph or is equal to t2. We use the notation t1 ? t2
when such inclusion holds strictly. We define T (S)
as the set of all spanning templates in the sample S.
DEFINITION: A spanning template t ? T (S) is
maximal most general if and only if both of the fol-
lowing conditions hold:
CONDITION A: For ?t? ? T (S), t?  t, it holds that
?(t) = ?(t?).
CONDITION B: For ?t? ? T (S), t ? t?, it holds that
?(t) ? ?(t?).
Condition A ensures that the extracted templates do
not contain spanning sub-structures that are more
?general? (i.e. having a larger sentence set); con-
dition B ensures that the template cannot be further
enlarged without reducing its sentence set.
GSL performs template extraction in two main
steps: (1) build a compact graph representation of
all the parse graphs from S; (2) extract templates
from the compact representation.
A compact graph representation is an aggregate
graph which joins all the sentence graphs from S
ensuring that all identical spanning sub-structures
from different sentences are merged into a single
one. Therefore, each vertex v (respectively, edge
e) in the aggregate graph is either a copy of a cor-
responding vertex (edge) from a sentence graph Pi
or it represents the merging of several identically
labeled vertices (edges) from different sentences in
S. The set of such sentences is defined as the sen-
tence set of v (e), and is represented through the set
of index numbers of related sentences (e.g. ?(1,2)?
in the third tree of Figure 2). We will denote with
Gi the compact graph representation of the first i
sentences in S. The parse trees P1 and P2 of two
sentences and their related compact representation
G2 are shown in Figure 2.
Building the compact graph representation
The compact graph representation is built incremen-
tally. The algorithm starts with an empty aggregate
graph G0 and then merges the sentence graphs from
S one at a time into the aggregate structure.
Let?s denote the current aggregate graph with
Gi?1(Vg, Eg) and let Pi(Vp, Ep) be the parse graph
which will be merged next. Note that the sentence
set of Pi is a single element set {i}.
During each iteration a new graph is created as
the union of both input graphs: Gi = Gi?1 ? Pi.
Then, the following merging procedure is per-
formed on the elements of Gi
1. ADDING GENERALIZED VERTICES TO Gi.
For every two vertices vg ? Vg, vp ? Vp having
equal labels, a new generalized vertex vnewg is cre-
ated and added to Gi. The new vertex takes the same
label and holds a sentence set which is formed from
the sentence set of vg by adding i to it. Still with
reference to Figure 2, the generalized vertices in G2
are ?X?, ?Y? and ?stop?. The algorithm connects the
generalized vertex vnewg with all the vertices which
are connected with vg and vp.
2. MERGING EDGES. If two edges eg ? Eg and
ep ? Ep have equal labels and their corresponding
adjacent vertices have been merged, then ea and ep
are also merged into a new edge. In Figure 2 the
edges (?stop?, ?X? ) and (?stop?, ?Y? ) from P1 and
P2 are eventually merged into G2.
3. DELETING MERGED VERTICES. Every vertex
v from Vp or Vg for which at least one generalized
vertex vnewg exists is deleted from Gi.
As an optimization step, we merge only vertices
and edges that are included in equal spanning tem-
plates.
Extracting the templates
GSL extracts all maximal most general templates
from the final compact representation Gn using the
following sub-algorithm:
1. BUILDING MINIMAL SPANNING TREES. For
every Na different slot variables in Gn having a
common ancestor, a minimal spanning tree st is
built. Its sentence set is computed as the intersec-
tion of the sentence sets of its edges and vertices.
2. EXPANDING THE SPANNING TREES. Every
minimal spanning tree st is expanded to the maxi-
mal sub-graph maxst whose sentence set is equal to
?(st). All maximal single-rooted DAGs in maxst
are extracted as candidate templates. Maximality
ensures that the extracted templates cannot be ex-
panded further while keeping the same sentence set,
satisfying condition B.
3. FILTERING. Candidates which contain an-
other candidate with a larger sentence set are filtered
out. This step guarantees condition A.
In Figure 2 the maximal most general template in
G2 is ?X
subj
? stop
obj
? Y? .
3.2.3 Post-processing and ranking of extracted
templates
As a last step, names and numbers are filtered out
from the templates. Moreover, TE removes those
templates which are very long or which appear with
just one anchor set and in less than four sentences.
Finally, the templates are sorted first by the number
of anchor sets with which each template appeared,
and then by the number of sentences in which they
appeared.
4 Evaluation
We evaluated the results of the TE/ASE algorithm
on a random lexicon of verbal forms and then as-
sessed its performance on the extracted data through
human-based judgments.
P1 : stop
subj
z
z
z
||zz
z
z
obj
A
A
A
  A
AA
A
P2 : stop
subj
z
z
z
||zz
z
z
obj

by
J
J
J
J
%%J
J
J
J
G2 : stop(1, 2)
subj(1,2)
rr
rr
xxrr
rr
obj(1,2)

by(2)
OO
OO
''O
OO
O
X Y X Y absorbing X(1, 2) Y (1, 2) absorbing(2)
Figure 2: Two parse trees and their compact representation (sentence sets are shown in parentheses).
4.1 Experimental Setting
The test set for human evaluation was generated by
picking out 53 random verbs from the 1000 most
frequent ones found in a subset of the Reuters cor-
pus2. For each verb entry in the lexicon, we pro-
vided the judges with the corresponding pivot tem-
plate and the list of related candidate entailment
templates found by the system. The judges were
asked to evaluate entailment for a total of 752 tem-
plates, extracted for 53 pivot lexicon entries; Table
1 shows a sample of the evaluated templates; all of
them are clearly good and were judged as correct
ones.
Pivot Template Entailment Templates
X prevent Y X provides protection against Y
X reduces Y
X decreases the risk of Y
X be cure for Y
X a day keeps Y away
X to combat Y
X accuse Y X call Y indictable
X testifies against Y
Y defense before X
X acquire Y X snap up Y
Y shareholders approve X
buyout
Y shareholders receive shares
of X stock
X go back to Y Y allowed X to return
Table 1: Sample of templates found by TE/ASE and
included in the evaluation test set.
Concerning the ASE algorithm, threshold pa-
rameters3 were set as PHRASEMAXF=107, SET-
MINF=102, SETMAXF=105, SETMINP=0.066,
and SETMAXP=0.666. An upper limit of 30 was
imposed on the number of possible anchor sets used
for each pivot. Since this last value turned out to
be very conservative with respect to system cover-
2Known as Reuters Corpus, Volume 1, English Language,
1996-08-20 to 1997-08-19.
3All parameters were tuned on a disjoint development lexi-
con before the actual experiment.
age, we subsequently attempted to relax it to 50 (see
Discussion in Section 4.3).
Further post-processing was necessary over ex-
tracted data in order to remove syntactic variations
referring to the same candidate template (typically
passive/active variations).
Three possible judgment categories have been
considered: Correct if an entailment relationship
in at least one direction holds between the judged
template and the pivot template in some non-bizarre
context; Incorrect if there is no reasonable context
and variable instantiation in which entailment holds;
No Evaluation if the judge cannot come to a definite
conclusion.
4.2 Results
Each of the three assessors (referred to as J#1, J#2,
and J#3) issued judgments for the 752 different
templates. Correct templates resulted to be 283,
313, and 295 with respect to the three judges. No
evaluation?s were 2, 0, and 16, while the remaining
templates were judged Incorrect.
For each verb, we calculate Yield as the absolute
number of Correct templates found and Precision as
the percentage of good templates out of all extracted
templates. Obtained Precision is 44.15%, averaged
over the 53 verbs and the 3 judges. Considering Low
Majority on judges, the precision value is 42.39%.
Average Yield was 5.5 templates per verb.
These figures may be compared (informally, as
data is incomparable) with average yield of 10.1
and average precision of 50.3% for the 9 ?pivot?
templates of (Lin and Pantel, 2001). The compar-
ison suggests that it is possible to obtain from the
(very noisy) web a similar range of precision as was
obtained from a clean news corpus. It also indi-
cates that there is potential for acquiring additional
templates per pivot, which would require further re-
search on broadening efficiently the search for addi-
tional web data per pivot.
Agreement among judges is measured by the
Kappa value, which is 0.55 between J#1 and J#2,
0.57 between J#2 and J#3, and 0.63 between J#1
and J#3. Such Kappa values correspond to moder-
ate agreement for the first two pairs and substantial
agreement for the third one. In general, unanimous
agreement among all of the three judges has been
reported on 519 out of 752 templates, which corre-
sponds to 69%.
4.3 Discussion
Our algorithm obtained encouraging results, ex-
tracting a considerable amount of interesting tem-
plates and showing inherent capability of discover-
ing complex semantic relations.
Concerning overall coverage, we managed to find
correct templates for 86% of the verbs (46 out of
53). Nonetheless, presented results show a substan-
tial margin of possible improvement. In fact yield
values (5.5 Low Majority, up to 24 in best cases),
which are our first concern, are inherently depen-
dent on the breadth of Web search performed by
the ASE algorithm. Due to computational time, the
maximal number of anchor sets processed for each
verb was held back to 30, significantly reducing the
amount of retrieved data.
In order to further investigate ASE potential, we
subsequently performed some extended experiment
trials raising the number of anchor sets per pivot
to 50. This time we randomly chose a subset of
10 verbs out of the less frequent ones in the origi-
nal main experiment. Results for these verbs in the
main experiment were an average Yield of 3 and an
average Precision of 45.19%. In contrast, the ex-
tended experiments on these verbs achieved a 6.5
Yield and 59.95% Precision (average values). These
results are indeed promising, and the substantial
growth in Yield clearly indicates that the TE/ASE
algorithms can be further improved. We thus sug-
gest that the feasibility of our approach displays the
inherent scalability of the TE/ASE process, and its
potential to acquire a large entailment relation KB
using a full scale lexicon.
A further improvement direction relates to tem-
plate ranking and filtering. While in this paper
we considered anchor sets to have equal weights,
we are also carrying out experiments with weights
based on cross-correlation between anchor sets.
5 Conclusions
We have described a scalable Web-based approach
for entailment relation acquisition which requires
only a standard phrasal lexicon as input. This min-
imal level of input is much simpler than required
by earlier web-based approaches, while succeeding
to maintain good performance. This result shows
that it is possible to identify useful anchor sets in
a fully unsupervised manner. The acquired tem-
plates demonstrate a broad range of semantic rela-
tions varying from synonymy to more complicated
entailment. These templates go beyond trivial para-
phrases, demonstrating the generality and viability
of the presented approach.
From our current experiments we can expect to
learn about 5 relations per lexicon entry, at least for
the more frequent entries. Moreover, looking at the
extended test, we can extrapolate a notably larger
yield by broadening the search space. Together with
the fact that we expect to find entailment relations
for about 85% of a lexicon, it is a significant step
towards scalability, indicating that we will be able
to extract a large scale KB for a large scale lexicon.
In future work we aim to improve the yield by in-
creasing the size of the sample-corpus in a qualita-
tive way, as well as precision, using statistical meth-
ods such as supervised learning for better anchor set
identification and cross-correlation between differ-
ent pivots. We also plan to support noun phrases
as input, in addition to verb phrases. Finally, we
would like to extend the learning task to discover the
correct entailment direction between acquired tem-
plates, completing the knowledge required by prac-
tical applications.
Like (Lin and Pantel, 2001), learning the context
for which entailment relations are valid is beyond
the scope of this paper. As stated, we learn entail-
ment relations holding for some, but not necessarily
all, contexts. In future work we also plan to find the
valid contexts for entailment relations.
Acknowledgements
The authors would like to thank Oren Glickman
(Bar Ilan University) for helpful discussions and as-
sistance in the evaluation, Bernardo Magnini for his
scientific supervision at ITC-irst, Alessandro Vallin
and Danilo Giampiccolo (ITC-irst) for their help in
developing the human based evaluation, and Prof.
Yossi Matias (Tel-Aviv University) for supervising
the first author. This work was partially supported
by the MOREWEB project, financed by Provincia
Autonoma di Trento. It was also partly carried out
within the framework of the ITC-IRST (TRENTO,
ITALY) ? UNIVERSITY OF HAIFA (ISRAEL) col-
laboration project. For data visualization and analy-
sis the authors intensively used the CLARK system
(www.bultreebank.org) developed at the Bulgarian
Academy of Sciences .
References
Regina Barzilay and Lillian Lee. 2003. Learning
to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In Proceedings
of HLT-NAACL 2003, pages 16?23, Edmonton,
Canada.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting paraphrases from a parallel corpus. In
Proceedings of ACL 2001, pages 50?57, Toulose,
France.
Ido Dagan and Oren Glickman. 2004. Probabilis-
tic textual entailment: Generic applied modeling
of language variability. In PASCAL Workshop on
Learning Methods for Text Understanding and
Mining, Grenoble.
Florence Duclaye, Franc?ois Yvon, and Olivier
Collin. 2002. Using the Web as a linguistic re-
source for learning reformulations automatically.
In Proceedings of LREC 2002, pages 390?396,
Las Palmas, Spain.
Oren Glickman and Ido Dagan. 2003. Identifying
lexical paraphrases from a single corpus: a case
study for verbs. In Proceedings of RANLP 2003.
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2003. Natural language based reformula-
tion resource and Web Exploitation. In Ellen M.
Voorhees and Lori P. Buckland, editors, Proceed-
ings of the 11th Text Retrieval Conference (TREC
2002), Gaithersburg, MD. NIST.
Christian Jacquemin. 1999. Syntagmatic and
paradigmatic representations of term variation.
In Proceedings of ACL 1999, pages 341?348.
Dekang Lin and Patrick Pantel. 2001. Discovery of
inference rules for Question Answering. Natural
Language Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation
of MINIPAR. In Proceedings of the Workshop
on Evaluation of Parsing Systems at LREC 1998,
Granada, Spain.
Dan Moldovan and Vasile Rus. 2001. Logic form
transformation of WordNet and its applicability
to Question Answering. In Proceedings of ACL
2001, pages 394?401, Toulose, France.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations:
Extracting paraphrases and generating new sen-
tences. In Proceedings of HLT-NAACL 2003, Ed-
monton, Canada.
Deepak Ravichandran and Eduard Hovy. 2002.
Learning surface text patterns for a Question An-
swering system. In Proceedings of ACL 2002,
Philadelphia, PA.
Ellen Riloff and Rosie Jones. 1999. Learning dic-
tionaries for Information Extraction by multi-
level bootstrapping. In Proceedings of the Six-
teenth National Conference on Artificial Intelli-
gence (AAAI-99), pages 474?479.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo,
and Ralph Grishman. 2002. Automatic para-
phrase acquisition from news articles. In Pro-
ceedings of Human Language Technology Con-
ference (HLT 2002), San Diego, USA.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern represen-
tation model for automatic IE pattern acquisition.
In Proceedings of ACL 2003.
Roman Yangarber, Ralph Grishman, Pasi
Tapanainen, and Silja Huttunen. 2000. Un-
supervised discovery of scenario-level patterns
for Information Extraction. In Proceedings of
COLING 2000.
Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 48?56,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Engineering of Syntactic Features for Shallow Semantic Parsing
Alessandro Moschitti?
? DISP - University of Rome ?Tor Vergata?, Rome, Italy
{moschitti, pighin, basili}@info.uniroma2.it
? ITC-Irst, ? DIT - University of Trento, Povo-Trento, Italy
coppolab@itc.it
Bonaventura Coppola?? Daniele Pighin? Roberto Basili?
Abstract
Recent natural language learning research
has shown that structural kernels can be
effectively used to induce accurate models
of linguistic phenomena.
In this paper, we show that the above prop-
erties hold on a novel task related to predi-
cate argument classification. A tree kernel
for selecting the subtrees which encodes
argument structures is applied. Experi-
ments with Support Vector Machines on
large data sets (i.e. the PropBank collec-
tion) show that such kernel improves the
recognition of argument boundaries.
1 Introduction
The design of features for natural language process-
ing tasks is, in general, a critical problem. The inher-
ent complexity of linguistic phenomena, often char-
acterized by structured data, makes difficult to find
effective linear feature representations for the target
learning models.
In many cases, the traditional feature selection
techniques (Kohavi and Sommerfield, 1995) are not
so useful since the critical problem relates to feature
generation rather than selection. For example, the
design of features for a natural language syntactic
parse-tree re-ranking problem (Collins, 2000) can-
not be carried out without a deep knowledge about
automatic syntactic parsing. The modeling of syn-
tactic/semantic based features should take into ac-
count linguistic aspects to detect the interesting con-
text, e.g. the ancestor nodes or the semantic depen-
dencies (Toutanova et al, 2004).
A viable alternative has been proposed in (Collins
and Duffy, 2002), where convolution kernels were
used to implicitly define a tree substructure space.
The selection of the relevant structural features was
left to the voted perceptron learning algorithm. An-
other interesting model for parsing re-ranking based
on tree kernel is presented in (Taskar et al, 2004).
The good results show that tree kernels are very
promising for automatic feature engineering, espe-
cially when the available knowledge about the phe-
nomenon is limited.
Along the same line, automatic learning tasks that
rely on syntactic information may take advantage of
a tree kernel approach. One of such tasks is the au-
tomatic boundary detection of predicate arguments
of the kind defined in PropBank (Kingsbury and
Palmer, 2002). For this purpose, given a predicate p
in a sentence s, we can define the notion of predicate
argument spanning trees (PAST s) as those syntac-
tic subtrees of s which exactly cover all and only
the p?s arguments (see Section 4.1). The set of non-
spanning trees can be then associated with all the
remaining subtrees of s.
An automatic classifier which recognizes the
spanning trees can potentially be used to detect the
predicate argument boundaries. Unfortunately, the
application of such classifier to all possible sen-
tence subtrees would require an exponential execu-
tion time. As a consequence, we can use it only to
decide for a reduced set of subtrees associated with
a corresponding set of candidate boundaries. Notice
how these can be detected by previous approaches
48
(e.g. (Pradhan et al, 2004)) in which a traditional
boundary classifier (tbc) labels the parse-tree nodes
as potential arguments (PA). Such classifiers, gen-
erally, are not sensitive to the overall argument struc-
ture. On the contrary, a PAST classifier (pastc) can
consider the overall argument structure encoded in
the associated subtree. This is induced by the PA
subsets.
The feature design for the PAST representation
is not simple. Tree kernels are a viable alternative
that allows the learning algorithm to measure the
similarity between two PAST s in term of all pos-
sible tree substructures.
In this paper, we designed and experimented a
boundary classifier for predicate argument labeling
based on two phases: (1) a first annotation of po-
tential arguments by using a high recall tbc and
(2) a PAST classification step aiming to select the
correct substructures associated with potential argu-
ments. Both classifiers are based on Support Vector
Machines learning. The pastc uses the tree kernel
function defined in (Collins and Duffy, 2002). The
results show that the PAST classification can be
learned with high accuracy (the f-measure is about
89%) and the impact on the overall boundary detec-
tion accuracy is good.
In the remainder of this paper, Section 2 intro-
duces the Semantic Role Labeling problem along
with the boundary detection subtask. Section 3 de-
fines the SVMs using the linear kernel and the parse
tree kernel for boundary detection. Section 4 de-
scribes our boundary detection algorithm. Section 5
shows the preliminary comparative results between
the traditional and the two-step boundary detection.
Finally, Section 7 summarizes the conclusions.
2 Automated Semantic Role Labeling
One of the largest resources of manually annotated
predicate argument structures has been developed in
the PropBank (PB) project. The PB corpus contains
300,000 words annotated with predicative informa-
tion on top of the Penn Treebank 2 Wall Street Jour-
nal texts. For any given predicate, the expected ar-
guments are labeled sequentially from Arg0 to Arg9,
ArgA and ArgM. Figure 1 shows an example of
the PB predicate annotation of the sentence: John
rented a room in Boston.
Predicates in PB are only embodied by verbs
whereas most of the times Arg0 is the subject, Arg1
is the direct object and ArgM indicates locations, as
in our example.
 
 
 
 
 
 
 
 
 
Predicate 
Arg. 0 
Arg. M 
S 
N 
NP 
D N 
VP 
V John 
in 
 rented 
a 
  room 
PP 
IN N 
Boston 
Arg. 1 
Figure 1: A predicate argument structure in a parse-tree rep-
resentation.
Several machine learning approaches for auto-
matic predicate argument extraction have been de-
veloped, e.g. (Gildea and Jurasfky, 2002; Gildea and
Palmer, 2002; Gildea and Hockenmaier, 2003; Prad-
han et al, 2004). Their common characteristic is
the adoption of feature spaces that model predicate-
argument structures in a flat feature representation.
In the next section, we present the common parse
tree-based approach to this problem.
2.1 Predicate Argument Extraction
Given a sentence in natural language, all the predi-
cates associated with the verbs have to be identified
along with their arguments. This problem is usually
divided in two subtasks: (a) the detection of the tar-
get argument boundaries, i.e. the span of its words
in the sentence, and (b) the classification of the argu-
ment type, e.g. Arg0 or ArgM in PropBank or Agent
and Goal in FrameNet.
The standard approach to learn both the detection
and the classification of predicate arguments is sum-
marized by the following steps:
1. Given a sentence from the training-set, gener-
ate a full syntactic parse-tree;
2. let P and A be the set of predicates and the
set of parse-tree nodes (i.e. the potential argu-
ments), respectively;
3. for each pair < p, a >? P ?A:
? extract the feature representation set, Fp,a;
49
? if the subtree rooted in a covers exactly
the words of one argument of p, put Fp,a
in T+ (positive examples), otherwise put
it in T? (negative examples).
For instance, in Figure 1, for each combination of
the predicate rent with the nodes N, S, VP, V, NP,
PP, D or IN the instances Frent,a are generated. In
case the node a exactly covers ?John?, ?a room? or
?in Boston?, it will be a positive instance otherwise
it will be a negative one, e.g. Frent,IN .
The T+ and T? sets are used to train the bound-
ary classifier. To train the multi-class classifier T+
can be reorganized as positive T+argi and negative
T?argi examples for each argument i. In this way,
an individual ONE-vs-ALL classifier for each argu-
ment i can be trained. We adopted this solution, ac-
cording to (Pradhan et al, 2004), since it is simple
and effective. In the classification phase, given an
unseen sentence, all its Fp,a are generated and clas-
sified by each individual classifier Ci. The argument
associated with the maximum among the scores pro-
vided by the individual classifiers is eventually se-
lected.
2.2 Standard feature space
The discovery of relevant features is, as usual, a
complex task. However, there is a common con-
sensus on the set of basic features. These stan-
dard features, firstly proposed in (Gildea and Juras-
fky, 2002), refer to unstructured information de-
rived from parse trees, i.e. Phrase Type, Predicate
Word, Head Word, Governing Category, Position
and Voice. For example, the Phrase Type indicates
the syntactic type of the phrase labeled as a predicate
argument, e.g. NP for Arg1 in Figure 1. The Parse
Tree Path contains the path in the parse tree between
the predicate and the argument phrase, expressed as
a sequence of nonterminal labels linked by direction
(up or down) symbols, e.g. V ? VP ? NP for Arg1 in
Figure 1. The Predicate Word is the surface form of
the verbal predicate, e.g. rent for all arguments.
In the next section we describe the SVM approach
and the basic kernel theory for the predicate argu-
ment classification.
3 Learning predicate structures via
Support Vector Machines
Given a vector space in <n and a set of positive and
negative points, SVMs classify vectors according to
a separating hyperplane, H(~x) = ~w ? ~x + b = 0,
where ~w ? <n and b ? < are learned by applying
the Structural Risk Minimization principle (Vapnik,
1995).
To apply the SVM algorithm to Predicate Argu-
ment Classification, we need a function ? : F ? <n
to map our features space F = {f1, .., f|F|} and our
predicate/argument pair representation, Fp,a = Fz ,
into <n, such that:
Fz ? ?(Fz) = (?1(Fz), .., ?n(Fz))
From the kernel theory we have that:
H(~x) =
( ?
i=1..l
?i~xi
)
? ~x+ b =
?
i=1..l
?i~xi ? ~x+ b =
?
i=1..l
?i?(Fi) ? ?(Fz) + b.
where, Fi ?i ? {1, .., l} are the training instances
and the product K(Fi, Fz) =<?(Fi) ??(Fz)> is the
kernel function associated with the mapping ?.
The simplest mapping that we can apply is
?(Fz) = ~z = (z1, ..., zn) where zi = 1 if fi ? Fz
and zi = 0 otherwise, i.e. the characteristic vector
of the set Fz with respect to F . If we choose the
scalar product as a kernel function we obtain the lin-
ear kernel KL(Fx, Fz) = ~x ? ~z.
An interesting property is that we do not need to
evaluate the ? function to compute the above vector.
Only the K(~x, ~z) values are in fact required. This al-
lows us to derive efficient classifiers in a huge (pos-
sible infinite) feature space, provided that the ker-
nel is processed in an efficient way. This property
is also exploited to design convolution kernel like
those based on tree structures.
3.1 The tree kernel function
The main idea of the tree kernels is the modeling of
a KT (T1, T2) function which computes the number
of common substructures between two trees T1 and
T2.
Given the set of substructures (fragments)
{f1, f2, ..} = F extracted from all the trees of the
training set, we define the indicator function Ii(n)
50
 S 
NP VP 
VP VP CC 
VB NP 
took DT NN 
the book 
and VB NP 
read PRP$ NN 
its title 
PRP 
John 
S 
NP VP 
VP 
VB NP 
read 
Sentence Parse-Tree 
S 
NP VP 
VP 
VB NP 
  took 
took{ARG0, ARG1} 
PRP 
John 
PRP 
John 
DT NN 
the book 
PRP$ NN 
its title 
read{ARG0, ARG1} 
Figure 2: A sentence parse tree with two predicative tree structures (PAST s)
which is equal 1 if the target fi is rooted at node n
and 0 otherwise. It follows that:
KT (T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2) (1)
where NT1 and NT2 are the sets of the T1?s
and T2?s nodes, respectively and ?(n1, n2) =?|F|
i=1 Ii(n1)Ii(n2). This latter is equal to the num-
ber of common fragments rooted at the n1 and n2
nodes. We can compute ? as follows:
1. if the productions at n1 and n2 are different
then ?(n1, n2) = 0;
2. if the productions at n1 and n2 are the same,
and n1 and n2 have only leaf children (i.e. they
are pre-terminals symbols) then ?(n1, n2) =
1;
3. if the productions at n1 and n2 are the same,
and n1 and n2 are not pre-terminals then
?(n1, n2) =
nc(n1)?
j=1
(1 + ?(cjn1 , cjn2)) (2)
where nc(n1) is the number of the children of n1
and cjn is the j-th child of the node n. Note that, as
the productions are the same, nc(n1) = nc(n2).
The above kernel has the drawback of assigning
higher weights to larger structures1. In order to over-
come this problem we scale the relative importance
of the tree fragments imposing a parameter ? in con-
ditions 2 and 3 as follows: ?(nx, nz) = ? and
?(nx, nz) = ?
?nc(nx)
j=1 (1 + ?(cjn1 , cjn2)).
1In order to approach this problem and to map similarity
scores in the [0,1] range, a normalization in the kernel space,
i.e. K?T (T1, T2) = KT (T1,T2)?KT (T1,T1)?KT (T2,T2) . is always applied
4 Boundary detection via argument
spanning
Section 2 has shown that traditional argument
boundary classifiers rely only on features extracted
from the current potential argument node. In or-
der to take into account a complete argument struc-
ture information, the classifier should select a set of
parse-tree nodes and consider them as potential ar-
guments of the target predicate. The number of all
possible subsets is exponential in the number of the
parse-tree nodes of the sentence, thus, we need to
cut the search space. For such purpose, a traditional
boundary classifier can be applied to select the set
of potential arguments PA. The reduced number of
PA subsets can be associated with sentence subtrees
which in turn can be classified by using tree kernel
functions. These measure if a subtree is compatible
or not with the subtree of a correct predicate argu-
ment structure.
4.1 The Predicate Argument Spanning Trees
(PAST s)
We consider the predicate argument structures an-
notated in PropBank along with the corresponding
TreeBank data as our object space. Given the target
predicate p in a sentence parse tree T and a subset
s = {n1, .., nk} of the T?s nodes, NT , we define as
the spanning tree root r the lowest common ancestor
of n1, .., nk. The node spanning tree (NST ), ps is
the subtree rooted in r, from which the nodes that
are neither ancestors nor descendants of any ni are
removed.
Since predicate arguments are associated with
tree nodes, we can define the predicate argu-
51
 S 
NP VP 
VB NP 
read 
John 
DT NN 
the title 
NP PP 
DT NN 
the book 
NP IN 
of 
Arg. 1 
Arg. 0 
S 
NP VP 
VB NP 
read 
John 
DT NN 
the title 
NP PP 
DT NN 
the book 
NP IN 
of 
S 
NP VP 
VB NP 
read 
John 
DT NN 
the title 
NP PP 
DT NN 
the book 
NP IN 
of 
S 
NP-0 VP 
John 
PP 
DT NN 
the book 
NP IN 
of 
S 
NP-0 VP 
VB NP 
read 
John 
DT NN 
the title 
NP-1 PP-2 
DT NN 
the book 
IN 
of 
NP 
(a) (b) (c) 
Correct PAST 
Incorrect  PAST 
Correct PAST 
Incorrect  PAST 
DT NN 
the title 
NP 
NP-1 VB 
read 
 
 
 
Figure 3: Two-step boundary classifier.
ment spanning tree (PAST ) of a predicate ar-
gument set, {a1, .., an}, as the NST over such
nodes, i.e. p{a1,..,an}. A PAST corresponds
to the minimal subparse tree whose leaves are
all and only the word sequence compounding
the arguments. For example, Figure 2 shows
the parse tree of the sentence "John took the
book and read its title". took{ARG0,ARG1}
and read{ARG0,ARG1} are two PAST structures
associated with the two predicates took and read,
respectively. All the other NST s are not valid
PAST s.
Notice that, labeling ps, ?s ? NT with a PAST
classifier (pastc) corresponds to solve the boundary
problem. The critical points for the application of
this strategy are: (1) how to design suitable features
for the PAST characterization. This new problem
requires a careful linguistic investigation about the
significant properties of the argument spanning trees
and (2) how to deal with the exponential number of
NST s.
For the first problem, the use of tree kernels over
the PAST s can be an alternative to the manual fea-
tures design as the learning machine, (e.g. SVMs)
can select the most relevant features from a high di-
mensional feature space. In other words, we can use
Eq. 1 to estimate the similarity between two PAST s
avoiding to define explicit features. The same idea
has been successfully applied to the parse-tree re-
ranking task (Taskar et al, 2004; Collins and Duffy,
2002) and predicate argument classification (Mos-
chitti, 2004).
For the second problem, i.e. the high computa-
tional complexity, we can cut the search space by us-
ing a traditional boundary classifier (tbc), e.g. (Prad-
han et al, 2004), which provides a small set of po-
tential argument nodes. Let PA be the set of nodes
located by tbc as arguments. We may consider the
set P of the NST s associated with any subset of
PA, i.e. P = {ps : s ? PA}. However, also
the classification ofP may be computationally prob-
lematic since theoretically there are |P| = 2|PA|
members.
In order to have a very efficient procedure, we
applied pastc to only the PA sets associated with
incorrect PAST s. A way to detect such incor-
rect NST s is to look for a node pair <n1, n2>?
PA ? PA of overlapping nodes, i.e. n1 is ances-
tor of n2 or viceversa. After we have detected such
nodes, we create two node sets PA1 = PA? {n1}
and PA2 = PA ? {n2} and classify them with the
pastc to select the correct set of argument bound-
aries. This procedure can be generalized to a set of
overlapping nodes O greater than 2 as reported in
Appendix 1.
Note that the algorithm selects a maximal set of
non-overlapping nodes, i.e. the first that is gener-
ated. Additionally, the worst case is rather rare thus
the algorithm is very fast on average.
The Figure 3 shows a working example of the
multi-stage classifier. In Frame (a), tbc labels as
potential arguments (gray color) three overlapping
nodes (in Arg.1). The overlap resolution algorithm
proposes two solutions (Frame (b)) of which only
one is correct. In fact, according to the second so-
lution the propositional phrase ?of the book? would
incorrectly be attached to the verbal predicate, i.e.
in contrast with the parse tree. The pastc, applied
52
to the two NST s, should detect this inconsistency
and provide the correct output. Note that, during the
learning, we generate the non-overlapping structures
in the same way to derive the positive and negative
examples.
4.2 Engineering Tree Fragment Features
In the Frame (b) of Figure 3, we show one of the
possible cases which pastc should deal with. The
critical problem is that the two NST s are perfectly
identical, thus, it is not possible to discern between
them using only their parse-tree fragments.
The solution to engineer novel features is to sim-
ply add the boundary information provided by the
tbc to the NST s. We mark with a progressive num-
ber the phrase type corresponding to an argument
node, starting from the leftmost argument. For ex-
ample, in the first NST of Frame (c), we mark
as NP-0 and NP-1 the first and second argument
nodes whereas in the second NST we have an hy-
pothesis of three arguments on the NP, NP and PP
nodes. We trasform them in NP-0, NP-1 and
PP-2.
This simple modification enables the tree ker-
nel to generate features useful to distinguish be-
tween two identical parse trees associated with dif-
ferent argument structures. For example, for the first
NST the fragments [NP-1 [NP][PP]], [NP
[DT][NN]] and [PP [IN][NP]] are gener-
ated. They do not match anymore with the [NP-0
[NP][PP]], [NP-1 [DT][NN]] and [PP-2
[IN][NP]] fragments of the second NST .
In order to verify the relevance of our model, the
next section provides empirical evidence about the
effectiveness of our approach.
5 The Experiments
The experiments were carried out with
the SVM-light-TK software available at
http://ai-nlp.info.uniroma2.it/moschitti/
which encodes the tree kernels in the SVM-light
software (Joachims, 1999). For tbc, we used the
linear kernel with a regularization parameter (option
-c) equal to 1 and a cost-factor (option -j) of 10 to
have a higher Recall. For the pastc we used ? = 0.4
(see (Moschitti, 2004)).
As referring dataset, we used the PropBank cor-
pora available at www.cis.upenn.edu/?ace,
along with the Penn TreeBank 2
(www.cis.upenn.edu/?treebank) (Marcus et
al., 1993). This corpus contains about 53,700
sentences and a fixed split between training and
testing which has been used in other researches, e.g.
(Pradhan et al, 2004; Gildea and Palmer, 2002).
We did not include continuation and co-referring
arguments in our experiments.
We used sections from 02 to 07 (54,443 argu-
ment nodes and 1,343,046 non-argument nodes) to
train the traditional boundary classifier (tbc). Then,
we applied it to classify the sections from 08 to
21 (125,443 argument nodes vs. 3,010,673 non-
argument nodes). As results we obtained 2,988
NST s containing at least an overlapping node pair
out of the total 65,212 predicate structures (accord-
ing to the tbc decisions). From the 2,988 over-
lapping structures we extracted 3,624 positive and
4,461 negative NST s, that we used to train the
pastc.
The performance was evaluated with the F1 mea-
sure2 over the section 23. This contains 10,406 ar-
gument nodes out of 249,879 parse tree nodes. By
applying the tbc classifier we derived 235 overlap-
ping NSTs, from which we extracted 204 PAST s
and 385 incorrect predicate argument structures. On
such test data, the performance of pastc was very
high, i.e. 87.08% in Precision and 89.22% in Recall.
Using the pastc we removed from the tbc the PA
that cause overlaps. To measure the impact on the
boundary identification performance, we compared
it with three different boundary classification base-
lines:
? tbc: overlaps are ignored and no decision is
taken. This provides an upper bound for the
recall as no potential argument is rejected for
later labeling. Notice that, in presence of over-
lapping nodes, the sentence cannot be anno-
tated correctly.
? RND: one among the non-overlapping struc-
tures with maximal number of arguments is
randomly selected.
2F1 assigns equal importance to Precision P and Recall R,
i.e. F1 = 2P?RP+R .
53
tbc tbc+RND tbc+Heu tbc+pastc
P R F P R F P R F P R F
All Struct. 92.21 98.76 95.37 93.55 97.31 95.39 92.96 97.32 95.10 94.40 98.42 96.36
Overl. Struct. 98.29 65.8 78.83 74.00 72.27 73.13 68.12 75.23 71.50 89.61 92.68 91.11
Table 1: Two-steps boundary classification performance using the traditional boundary classifier tbc, the random selection of
non-overlapping structures (RND), the heuristic to select the most suitable non-overlapping node set (Heu) and the predicate
argument spanning tree classifier (pastc).
? Heu (heuristic): one of the NST s which con-
tain the nodes with the lowest overlapping
score is chosen. This score counts the number
of overlapping node pairs in the NST . For ex-
ample, in Figure 3.(a) we have a NP that over-
laps with two nodes NP and PP, thus it is as-
signed a score of 2.
The third row of Table 1 shows the results of tbc,
tbc + RND, tbc + Heu and tbc + pastc in the
columns 2,3,4 and 5, respectively. We note that:
? The tbc F1 is slightly higher than the result ob-
tained in (Pradhan et al, 2004), i.e. 95.37%
vs. 93.8% on same training/testing conditions,
i.e. (same PropBank version, same training and
testing split and same machine learning algo-
rithm). This is explained by the fact that we
did not include the continuations and the co-
referring arguments that are more difficult to
detect.
? Both RND and Heu do not improve the tbc re-
sult. This can be explained by observing that in
the 50% of the cases a correct node is removed.
? When, to select the correct node, the pastc is
used, the F1 increases of 1.49%, i.e. (96.86 vs.
95.37). This is a very good result considering
that to increase the very high baseline of tbc is
hard.
In order to give a fairer evaluation of our approach
we tested the above classifiers on the overlapping
structures only, i.e. we measured the pastc improve-
ment on all and only the structures that required its
application. Such reduced test set contains 642 ar-
gument nodes and 15,408 non-argument nodes. The
fourth row of Table 1 reports the classifier perfor-
mance on such task. We note that the pastc im-
proves the other heuristics of about 20%.
6 Related Work
Recently, many kernels for natural language applica-
tions have been designed. In what follows, we high-
light their difference and properties.
The tree kernel used in this article was proposed
in (Collins and Duffy, 2002) for syntactic parsing re-
ranking. It was experimented with the Voted Percep-
tron and was shown to improve the syntactic parsing.
A refinement of such technique was presented in
(Taskar et al, 2004). The substructures produced by
the proposed tree kernel were bound to local prop-
erties of the target parse tree and more lexical infor-
mation was added to the overall kernel function.
In (Zelenko et al, 2003), two kernels over syn-
tactic shallow parser structures were devised for
the extraction of linguistic relations, e.g. person-
affiliation. To measure the similarity between two
nodes, the contiguous string kernel and the sparse
string kernel (Lodhi et al, 2000) were used. The
former can be reduced to the contiguous substring
kernel whereas the latter can be transformed in the
non-contiguous string kernel. The high running time
complexity, caused by the general form of the frag-
ments, limited the experiments on data-set of just
200 news items.
In (Cumby and Roth, 2003), it is proposed a de-
scription language that models feature descriptors
to generate different feature type. The descriptors,
which are quantified logical prepositions, are instan-
tiated by means of a concept graph which encodes
the structural data. In the case of relation extraction
the concept graph is associated with a syntactic shal-
low parse and the extracted propositional features
express fragments of a such syntactic structure. The
experiments over the named entity class categoriza-
tion show that when the description language selects
an adequate set of tree fragments the Voted Percep-
tron algorithm increases its classification accuracy.
In (Culotta and Sorensen, 2004) a dependency
54
tree kernel is used to detect the Named Entity classes
in natural language texts. The major novelty was
the combination of the contiguous and sparse ker-
nels with the word kernel. The results show that
the contiguous outperforms the sparse kernel and the
bag-of-words.
7 Conclusions
The feature design for new natural language learn-
ing tasks is difficult. We can take advantage from
the kernel methods to model our intuitive knowledge
about the target linguistic phenomenon. In this pa-
per we have shown that we can exploit the properties
of tree kernels to engineer syntactic features for the
predicate argument boundary detection task.
Preliminary results on gold standard trees suggest
that (1) the information related to the whole predi-
cate argument structure is important and (2) tree ker-
nel can be used to generate syntactic features.
In the future, we would like to use an approach
similar to the PAST classifier on parses provided
by different parsing models to detect boundary and
to classify semantic role more accurately .
Acknowledgements
We wish to thank Ana-Maria Giuglea for her help in
the design and implementation of the basic Seman-
tic Role Labeling system that we used in the experi-
ments.
References
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In ACL02.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of ICML 2000.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
the 42nd Meeting of the Association for Computational
Linguistics (ACL?04), Main Volume, pages 423?429,
Barcelona, Spain, July.
Chad Cumby and Dan Roth. 2003. Kernel methods for
relational learning. In Proceedings of the Twentieth
International Conference (ICML 2003), Washington,
DC, USA.
Daniel Gildea and Julia Hockenmaier. 2003. Identifying
semantic roles using combinatory categorial grammar.
In Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing, Sapporo,
Japan.
Daniel Gildea and Daniel Jurasfky. 2002. Automatic la-
beling of semantic roles. Computational Linguistic,
28(3):496?530.
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. In
Proceedings of the 40th Annual Conference of the
Association for Computational Linguistics (ACL-02),
Philadelphia, PA, USA.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to PropBank. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2002), Las Palmas, Spain.
Ron Kohavi and Dan Sommerfield. 1995. Feature sub-
set selection using the wrapper model: Overfitting and
dynamic search space topology. In The First Interna-
tional Conference on Knowledge Discovery and Data
Mining, pages 192?197. AAAI Press, Menlo Park,
California, August. Journal version in AIJ.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Christopher Watkins. 2000. Text clas-
sification using string kernels. In NIPS, pages 563?
569.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: The Penn Treebank. Computational Linguistics,
19:313?330.
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow semantic parsing. In proceedings of
the 42th Conference on Association for Computational
Linguistic (ACL-2004), Barcelona, Spain.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2005.
Support vector learning for semantic argument classi-
fication. to appear in Machine Learning Journal.
Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and
Christopher Manning. 2004. Max-margin parsing. In
Dekang Lin and Dekai Wu, editors, Proceedings of
EMNLP 2004, pages 1?8, Barcelona, Spain, July. As-
sociation for Computational Linguistics.
Kristina Toutanova, Penka Markova, and Christopher D.
Manning. 2004. The leaf projection path view of
parse trees: Exploring string kernels for hpsg parse se-
lection. In Proceedings of EMNLP 2004.
55
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel
methods for relation extraction. Journal of Machine
Learning Research.
Appendix 1: Generalized Boundary
Selection Algorithm
Let O be the set of overlapping nodes of PA, and
NO the set of non overlapping nodes of PA.
Let subs(?1)(A) = {B|B ? 2A, |B| = |A| ? 1}.
Let O? = subs(?1)(O).
while(true)
begin
1. H = ?
2. ?o ? O?:
(a) If o does not include any overlapping node
pair
then H = H ? {o}
3. If H 6= ? then:
(a) Let s? =argmaxo?H pastc(pNO?o),
where pNO?o represents the node span-
ning tree compatible with o, and the
pastc(pNO?o) is the score provided by the
PAST SVM categorizer on it
(b) If pastc(s?) > 0 then RETURN( s?)
4. If O? = {?} then RETURN( NO )
5. Else:
(a) O? = O? ?H
(b) O? = ?o?O? subs(?1)(o)
end
56
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 201?204, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Hierarchical Semantic Role Labeling
Alessandro Moschitti?
moschitti@info.uniroma2.it
? DISP - University of Rome ?Tor Vergata?, Rome, Italy
? ITC-Irst, ? DIT - University of Trento, Povo, Trento, Italy
Ana-Maria Giuglea?
ana-maria.giuglea@topex.ro
Bonaventura Coppola??
coppolab@itc.it
Roberto Basili?
basili@info.uniroma2.it
Abstract
We present a four-step hierarchical SRL
strategy which generalizes the classical
two-level approach (boundary detection
and classification). To achieve this, we
have split the classification step by group-
ing together roles which share linguistic
properties (e.g. Core Roles versus Ad-
juncts). The results show that the non-
optimized hierarchical approach is com-
putationally more efficient than the tradi-
tional systems and it preserves their accu-
racy.
1 Introduction
For accomplishing the CoNLL 2005 Shared Task
on Semantic Role Labeling (Carreras and Ma`rquez,
2005), we capitalized on our experience on the se-
mantic shallow parsing by extending our system,
widely experimented on PropBank and FrameNet
(Giuglea and Moschitti, 2004) data, with a two-
step boundary detection and a hierarchical argument
classification strategy.
Currently, the system can work in both basic and
enhanced configuration. Given the parse tree of an
input sentence, the basic system applies (1) a bound-
ary classifier to select the nodes associated with cor-
rect arguments and (2) a multi-class labeler to assign
the role type. For such models, we used some of the
linear (e.g. (Gildea and Jurasfky, 2002; Pradhan et
al., 2005)) and structural (Moschitti, 2004) features
developed in previous studies.
In the enhanced configuration, the boundary an-
notation is subdivided in two steps: a first pass in
which we label argument boundary and a second
pass in which we apply a simple heuristic to elimi-
nate the argument overlaps. We have also tried some
strategies to learn such heuristics automatically. In
order to do this we used a tree kernel to classify the
subtrees associated with correct predicate argument
structures (see (Moschitti et al, 2005)). The ratio-
nale behind such an attempt was to exploit the cor-
relation among potential arguments.
Also, the role labeler is divided into two steps:
(1) we assign to the arguments one out of four possi-
ble class labels: Core Roles, Adjuncts, Continuation
Arguments and Co-referring Arguments, and (2) in
each of the above class we apply the set of its spe-
cific classifiers, e.g. A0,..,A5 within the Core Role
class. As such grouping is relatively new, the tradi-
tional features may not be sufficient to characterize
each class. Thus, to generate a large set of features
automatically, we again applied tree kernels.
Since our SRL system exploits the PropBank for-
malism for internal data representation, we devel-
oped ad-hoc procedures to convert back and forth
to the CoNLL Shared Task format. This conversion
step gave us useful information about the amount
and the nature of the parsing errors. Also, we could
measure the frequency of the mismatches between
syntax and role annotation.
In the remainder of this paper, Section 2 describes
the basic system configuration whereas Section 3 il-
lustrates its enhanced properties and the hierarchical
structure. Section 4 describes the experimental set-
ting and the results. Finally, Section 5 summarizes
201
our conclusions.
2 The Basic Semantic Role Labeler
In the last years, several machine learning ap-
proaches have been developed for automatic role la-
beling, e.g. (Gildea and Jurasfky, 2002; Pradhan
et al, 2005). Their common characteristic is the
adoption of flat feature representations for predicate-
argument structures. Our basic system is similar to
the one proposed in (Pradhan et al, 2005) and it is
described hereafter.
We divided the predicate argument labeling in two
subtasks: (a) the detection of the arguments related
to a target, i.e. all the compounding words of such
argument, and (b) the classification of the argument
type, e.g. A0 or AM. To learn both tasks we used the
following algorithm:
1. Given a sentence from the training-set, generate
a full syntactic parse-tree;
2. Let P and A be respectively the set of predicates
and the set of parse-tree nodes (i.e. the potential ar-
guments);
3. For each pair <p, a> ? P ?A:
- extract the feature representation set, Fp,a;
- if the subtree rooted in a covers exactly the
words of one argument of p, put Fp,a in T+
(positive examples), otherwise put it in T?
(negative examples).
We trained the SVM boundary classifier on T+ and
T? sets and the role labeler i on the T+i , i.e. its pos-
itive examples and T?i , i.e. its negative examples,
where T+ = T+i ? T?i , according to the ONE-vs.-
ALL scheme. To implement the multi-class clas-
sifiers we select the argument associated with the
maximum among the SVM scores.
To represent the Fp,a pairs we used the following
features:
- the Phrase Type, Predicate Word, Head Word,
Governing Category, Position and Voice defined in
(Gildea and Jurasfky, 2002);
- the Partial Path, Compressed Path, No Direction
Path, Constituent Tree Distance, Head Word POS,
First and Last Word/POS in Constituent, SubCate-
gorization and Head Word of Prepositional Phrases
proposed in (Pradhan et al, 2005); and
- the Syntactic Frame designed in (Xue and Palmer,
2004).
Figure 1: Architecture of the Hierarchical Semantic Role La-
beler.
3 Hierarchical Semantic Role Labeler
Having two phases for argument labeling provides
two main advantages: (1) the efficiency is increased
as the negative boundary examples, which are al-
most all parse-tree nodes, are used with one clas-
sifier only (i.e. the boundary classifier), and (2) as
arguments share common features that do not occur
in the non-arguments, a preliminary classification
between arguments and non-arguments advantages
the boundary detection of roles with fewer training
examples (e.g. A4). Moreover, it may be simpler
to classify the type of roles when the not-argument
nodes are absent.
Following this idea, we generalized the above two
level strategy to a four-step role labeling by group-
ing together the arguments sharing similar proper-
ties. Figure 1, shows the hierarchy employed for ar-
gument classification:
During the first phase, we select the parse tree
nodes which are likely predicate arguments. An
SVM with moderately high recall is applied for such
purpose.
In the second phase, a simple heuristic which se-
lects non-overlapping nodes from those derived in
the previous step is applied. Two nodes n1 and n2
do not overlap if n1 is not ancestor of n2 and vicev-
ersa. Our heuristic simply eliminates the nodes that
cause the highest number of overlaps. We have also
studied how to train an overlap resolver by means of
tree kernels; the promising approach and results can
be found in (Moschitti et al, 2005).
In the third phase, we classify the detected argu-
ments in the following four classes: AX, i.e. Core
202
Arguments, AM, i.e. Adjuncts, CX, i.e. Continua-
tion Arguments and RX, i.e. the Co-referring Argu-
ments. The above classification relies on linguistic
reasons. For example Core arguments class contains
the arguments specific to the verb frames while Ad-
junct Arguments class contains arguments that are
shared across all verb frames.
In the fourth phase, we classify the members
within the classes of the previous level, e.g. A0 vs.
A1, ..., A5.
4 The Experiments
We experimented our approach with the CoNLL
2005 Shared Task standard dataset, i.e. the Pen-
nTree Bank, where sections from 02 to 21 are used
as training set, Section 24 as development set (Dev)
and Section 23 as the test set (WSJ). Additionally,
the Brown corpus? sentences were also used as the
test set (Brown). As input for our feature extractor
we used only the Charniak?s parses with their POSs.
The evaluations were carried out with the SVM-
light-TK software (Moschitti, 2004) available at
http://ai-nlp.info.uniroma2.it/moschitti/
which encodes the tree kernels in the SVM-light
software (Joachims, 1999). We used the default
polynomial kernel (degree=3) for the linear feature
representations and the tree kernels for the structural
feature processing.
As our feature extraction module was designed
to work on the PropBank project annotation format
(i.e. the prop.txt index file), we needed to generate
it from the CoNLL data. Each PropBank annota-
tion refers to a parse tree node which exactly cov-
ers the target argument but when using automatic
parses such node may not exist. For example, on
the CoNLL Charniak?s parses, (sections 02-21 and
24), we discovered that this problem affects 10,293
out of the 241,121 arguments (4.3%) and 9,741 sen-
tences out of 87,257 (11.5%). We have found out
that most of the errors are due to wrong parsing at-
tachments. This observation suggests that the capa-
bility of discriminating between correct and incor-
rect parse trees is a key issue in the boundary de-
tection phase and it must be properly taken into ac-
count.
4.1 Basic System Evaluation
For the boundary classifier we used a SVM with
the polynomial kernel of degree 3. We set the reg-
ularization parameter, c, to 1 and the cost factor,
j to 7 (to have a slightly higher recall). To re-
duce the learning time, we applied a simple heuristic
which removes the nodes covering the target predi-
cate node. From the initial 4,683,777 nodes (of sec-
tions 02-21), the heuristic removed 1,503,100 nodes
with a loss of 2.6% of the total arguments. How-
ever, as we started the experiments in late, we used
only the 992,819 nodes from the sections 02-08. The
classifier took about two days and half to converge
on a 64 bits machine (2.4 GHz and 4Gb Ram).
The multiclassifier was built with 52 binary ar-
gument classifiers. Their training on all arguments
from sec 02-21, (i.e. 242,957), required about a half
day on a machine with 8 processors (32 bits, 1.7
GHz and overll 4Gb Ram).
We run the role multiclassifier on the output of the
boundary classifier. The results on the Dev, WSJ and
Brown test data are shown in Table 1. Note that, the
overlapping nodes cause the generation of overlap-
ping constituents in the sentence annotation. This
prevents us to use the CoNLL evaluator. Thus, we
used the overlap resolution algorithm also for the ba-
sic system.
4.2 Hierarchical Role Labeling Evaluation
As the first two phases of the hierarchical labeler are
identical to the basic system, we focused on the last
two phases. We carried out our studies over the Gold
Standard boundaries in the presence of arguments
that do not have a perfect-covering node in the Char-
niak trees.
To accomplish the third phase, we re-organized
the flat arguments into the AX, AM, CX and RX
classes and we built a single multi-classifier. For
the fourth phase, we built a multi-classifier for each
of the above classes: only the examples related to
the target class were used, e.g. the AX mutliclas-
sifier was designed with the A0,..,A5 ONE-vs-ALL
binary classifiers.
In rows 2 and 3, Table 2 shows the numbers of
training and development set instances. Row 4 con-
tains the F1 of the binary classifiers of the third
phase whereas Row 5 reports the F1 of the result-
ing multi-classifier. Row 6 presents the F1s of the
multi-classifiers of the fourth phase.
Row 7 illustrates the F1 measure of the fourth
phase classifier applied to the third phase output. Fi-
203
Precision Recall F?=1
Development 74.95% 73.10% 74.01
Test WSJ 76.55% 75.24% 75.89
Test Brown 65.92% 61.83% 63.81
Test WSJ+Brown 75.19% 73.45% 74.31
Test WSJ Precision Recall F?=1
Overall 76.55% 75.24% 75.89
A0 81.05% 84.37% 82.67
A1 77.21% 74.12% 75.63
A2 67.02% 68.11% 67.56
A3 69.63% 54.34% 61.04
A4 74.75% 72.55% 73.63
A5 100.00% 40.00% 57.14
AM-ADV 55.23% 55.34% 55.28
AM-CAU 66.07% 50.68% 57.36
AM-DIR 50.62% 48.24% 49.40
AM-DIS 77.71% 78.44% 78.07
AM-EXT 68.00% 53.12% 59.65
AM-LOC 59.02% 63.09% 60.99
AM-MNR 67.67% 52.33% 59.02
AM-MOD 98.65% 92.56% 95.51
AM-NEG 97.37% 96.52% 96.94
AM-PNC 42.28% 45.22% 43.70
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 81.90% 74.52% 78.03
R-A0 79.50% 84.82% 82.07
R-A1 62.23% 75.00% 68.02
R-A2 100.00% 31.25% 47.62
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 100.00% 50.00% 66.67
R-AM-EXT 100.00% 100.00% 100.00
R-AM-LOC 85.71% 85.71% 85.71
R-AM-MNR 22.22% 33.33% 26.67
R-AM-TMP 67.69% 84.62% 75.21
V 97.34% 97.30% 97.32
Table 1: Overall results (top) and detailed results on
the WSJ test (bottom).
nally, in Row 8, we report the F1 of the basic system
on the gold boundary nodes. We note that the basic
system shows a slightly higher F1 but is less compu-
tational efficient than the hierarchical approach.
5 Final Remarks
In this paper we analyzed the impact of a hierarchi-
cal categorization on the semantic role labeling task.
The results show that such approach produces an ac-
curacy similar to the flat systems with a higher ef-
ficiency. Moreover, some preliminary experiments
show that each node of the hierarchy requires differ-
ent features to optimize the associated multiclassi-
fier. For example, we found that the SCF tree kernel
(Moschitti, 2004) improves the AX multiclassifier
AX AM CX RX
# train. examples 172,457 59,473 2,954 7,928
# devel. examples 5,930 2,132 105 284
Phase III: binary class. 97.29 97.35 70.86 93.15
Phase III 95.99
Phase IV 92.50 85.88 91.43 91.55
Phase III & IV 88.15
Basic System 88.61
Table 2: Hierarchical Semantic Role Labeler Results
whereas the PAF tree kernel seems more suited for
the classification within the other classes, e.g. AM.
Future work on the optimization of each phase is
needed to study the potential accuracy limits of the
proposed hierarchical approach.
Acknowledgements
We wish to thank Daniele Pighin for his valuable
support in the development of the SRL system.
References
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling. In pro-
ceedings of CoNLL?05.
Daniel Gildea and Daniel Jurasfky. 2002. Automatic labeling
of semantic roles. Computational Linguistic.
Ana-Maria Giuglea and Alessandro Moschitti. 2004. Knowl-
edge Discovering using FrameNet, VerbNet and PropBank.
In proceedings of the Workshop on Ontology and Knowledge
Discovering at ECML?04, Pisa, Italy.
T. Joachims. 1999. Making large-scale SVM learning practical.
In B. Scho?lkopf, C. Burges, and A. Smola, editors, Advances
in Kernel Methods - Support Vector Learning.
Alessandro Moschitti, Bonaventura Coppola, Daniele Pighin,
and Roberto Basili. 2005. Engineering of syntactic features
for shallow semantic parsing. In proceedings of the Feature
Engineering Workshop at ACL?05, Ann Arbor, USA.
Alessandro Moschitti. 2004. A study on convolution kernel
for shallow semantic parsing. In proceedings of ACL-2004,
Barcelona, Spain.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward,
James H. Martin, and Daniel Jurafsky. 2005. Support vector
learning for semantic argument classification. to appear in
Machine Learning Journal.
Nianwen Xue and Martha Palmer. 2004. Calibrating features
for semantic role labeling. In Proceedings of EMNLP?04,
Barcelona, Spain.
204
Proceedings of the TextGraphs-8 Workshop, pages 6?10,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
JoBimText Visualizer:
A Graph-based Approach to Contextualizing Distributional Similarity
Alfio Gliozzo1 Chris Biemann2 Martin Riedl2
Bonaventura Coppola1 Michael R. Glass1 Matthew Hatem1
(1) IBM T.J. Watson Research, Yorktown Heights, NY 10598, USA
(2) FG Language Technology, CS Dept., TU Darmstadt, 64289 Darmstadt, Germany
{gliozzo,mrglass,mhatem}@us.ibm.com coppolab@gmail.com
{biem,riedl}@cs.tu-darmstadt.de
Abstract
We introduce an interactive visualization com-
ponent for the JoBimText project. JoBim-
Text is an open source platform for large-scale
distributional semantics based on graph rep-
resentations. First we describe the underly-
ing technology for computing a distributional
thesaurus on words using bipartite graphs of
words and context features, and contextualiz-
ing the list of semantically similar words to-
wards a given sentential context using graph-
based ranking. Then we demonstrate the ca-
pabilities of this contextualized text expan-
sion technology in an interactive visualization.
The visualization can be used as a semantic
parser providing contextualized expansions of
words in text as well as disambiguation to
word senses induced by graph clustering, and
is provided as an open source tool.
1 Introduction
The aim of the JoBimText1 project is to build a
graph-based unsupervised framework for computa-
tional semantics, addressing problems like lexical
ambiguity and variability, word sense disambigua-
tion and lexical substitutability, paraphrasing, frame
induction and parsing, and textual entailment. We
construct a semantic analyzer able to self-adapt to
new domains and languages by unsupervised learn-
ing of semantics from large corpora of raw text. At
the moment, this analyzer encompasses contextual-
ized similarity, sense clustering, and a mapping of
senses to existing knowledge bases. While its pri-
mary target application is functional domain adap-
tation of Question Answering (QA) systems (Fer-
1http://sf.net/projects/jobimtext/
rucci et al, 2013), output of the semantic analyzer
has been successfully utilized for word sense disam-
biguation (Miller et al, 2012) and lexical substitu-
tion (Szarvas et al, 2013). Rather than presenting
the different algorithms and technical solutions cur-
rently implemented by the JoBimText community in
detail, in this paper we will focus on available func-
tionalities and illustrate them using an interactive vi-
sualization.
2 Underlying Technologies
While distributional semantics (de Saussure, 1959;
Harris, 1951; Miller and Charles, 1991) and the
computation of distributional thesauri (Lin, 1998)
has been around for decades, its full potential has yet
to be utilized in Natural Language Processing (NLP)
tasks and applications. Structural semantics claims
that meaning can be fully defined by semantic oppo-
sitions and relations between words. In order to per-
form a reliable knowledge acquisition process in this
framework, we gather statistical information about
word co-occurrences with syntactic contexts from
very large corpora. To avoid the intrinsic quadratic
complexity of the similarity computation, we have
developed an optimized process based on MapRe-
duce (Dean and Ghemawat, 2004) that takes advan-
tage of the sparsity of contexts, which allows scal-
ing the process through parallelization. The result of
this computation is a graph connecting the most dis-
criminative contexts to terms and explicitly linking
the most similar terms. This graph represents local
models of semantic relations per term rather than a
model with fixed dimensions. This representation
departs from the vector space metaphor (Schu?tze,
1993; Erk and Pado?, 2008; Baroni and Zamparelli,
6
2010), commonly employed in other frameworks for
distributional semantics such as LSA (Deerwester et
al., 1990) or LDA (Blei et al, 2003).
The main contribution of this paper is to de-
scribe how we operationalize semantic similarity in
a graph-based framework and explore this seman-
tic graph using an interactive visualization. We de-
scribe a scalable and flexible computation of a dis-
tributional thesaurus (DT), and the contextualization
of distributional similarity for specific occurrences
of language elements (i.e. terms). For related works
on the computation of distributional similarity, see
e.g. (Lin, 1998; Lin and Dyer, 2010).
2.1 Holing System
To keep the framework flexible and abstract with re-
spect to the pre-processing that identifies structure
in language material, we introduce the holing op-
eration, cf. (Biemann and Riedl, 2013). It is ap-
plied to observations over the structure of text, and
splits these observations into a pair of two parts,
which we call the ?Jo? and the ?Bim?2. All JoBim
pairs are maintained in the bipartite First-Order Jo-
Bim graph TC(T,C,E) with T set of terms (Jos),
C set of contexts (Bims), and e(t, c, f) ? E edges
between t ? T , c ? C with frequency f . While
these parts can be thought of as language elements
referred to as terms, and their respective context fea-
tures, splits over arbitrary structures are possible (in-
cluding pairs of terms for Jos), which makes this
formulation more general than similar formulations
found e.g. in (Lin, 1998; Baroni and Lenci, 2010).
These splits form the basis for the computation of
global similarities and for their contextualization. A
Holing System based on dependency parses is illus-
trated in Figure 1: for each dependency relation, two
JoBim pairs are generated.
2.2 Distributed Distributional Thesaurus
Computation
We employ the Apache Hadoop MapReduce Fram-
work3, and Apache Pig4, for parallelizing and dis-
tributing the computation of the DT. We describe
this computation in terms of graph transformations.
2arbitrary names to emphasize the generality, should be
thought of as ?term? and ?context?
3http://hadoop.apache.org
4http://pig.apache.org/
Figure 1: Jos and Bims generated applying a dependency
parser (de Marneffe et al, 2006) to the sentence I suffered
from a cold and took aspirin. The @@ symbolizes the
hole.
Staring from the JoBim graph TC with counts as
weights, we first apply a statistical test5 to com-
pute the significance of each pair (t, c), then we only
keep the p most significant pairs per t. This consti-
tutes our first-order graph for Jos FOJO. In analogy,
when keeping the p most significant pairs per c, we
can produce the first-order graph for Bims FOBIM .
The second order similarity graph for Jos is defined
as SOJO(T,E) with Jos t1, t2 ? T and undirected
edges e(t1, t2, s) with similarity s = |{c|e(t1, c) ?
FOJO, e(t2, c) ? FOJO}|, which defines similar-
ity between Jos as the number of salient features
two Jos share. SOJO defines a distributional the-
saurus. In analogy, SOBIM is defined over the
shared Jos for pairs of Bims and defines similar-
ity of contexts. This method, which can be com-
puted very efficiently in a few MapReduce steps, has
been found superior to other measures for very large
datasets in semantic relatedness evaluations in (Bie-
mann and Riedl, 2013), but could be replaced by any
other measure without interfering with the remain-
der of the system.
2.3 Contextualization with CRF
While the distributional thesaurus provides the sim-
ilarity between pairs of terms, the fidelity of a par-
ticular expansion depends on the context. From the
term-context associations gathered in the construc-
tion of the distributional thesaurus we effectively
have a language model, factorized according to the
holing operation. As with any language model,
smoothing is critical to performance. There may be
5we use log-likelihood ratio (Dunning, 1993) or LMI (Evert,
2004)
7
many JoBim (term-context) pairs that are valid and
yet under represented in the corpus. Yet, there may
be some similar term-context pair that is attested in
the corpus. We can find similar contexts by expand-
ing the term arguments with similar terms. However,
again we are confronted with the fact that the simi-
larity of these terms depends on the context.
This suggests some technique of joint inference
to expand terms in context. We use marginal in-
ference in a conditional random field (CRF) (Laf-
ferty et al, 2001). A particular world, x is defined
as single, definite sequence of either original or ex-
panded words. The weight of the world, w(x) de-
pends on the degree to which the term-context as-
sociations present in this sentence are present in the
corpus and the general out-of-context similarity of
each expanded term to the corresponding term in the
original sentence. Therefore the probability associ-
ated with any expansion t for any position xi is given
by Equation 1. Where Z is the partition function, a
normalization constant.
P (xi = t) =
1
Z
?
{x | xi=t}
ew(x) (1)
The balance between the plausibility of an ex-
panded sentence according to the language model,
and its per-term similarity to the original sentence is
an application specific tuning parameter.
2.4 Word Sense Induction, Disambiguation
and Cluster Labeling
The contextualization described in the previous sub-
section performs implicit word sense disambigua-
tion (WSD) by ranking contextually better fitting
similar terms higher. To model this more explicitly,
and to give rise to linking senses to taxonomies and
domain ontologies, we apply a word sense induction
(WSI) technique and use information extracted by
IS-A-patterns (Hearst, 1992) to label the clusters.
Using the aggregated context features of the clus-
ters, the word cluster senses are assigned in con-
text. The DT entry for each term j as given in
SOJO(J,E) induces an open neighborhood graph
Nj(Vj , Ej) with Vj = {j?|e(j, j?, s) ? E) and Ej
the projection of E regarding Vj , consisting of sim-
ilar terms to j and their similarities, cf. (Widdows
and Dorow, 2002).
We cluster this graph using the Chinese Whispers
graph clustering algorithm (Biemann, 2010), which
finds the number of clusters automatically, to ob-
tain induced word senses. Running shallow, part-
of-speech-based IS-A patterns (Hearst, 1992) over
the text collection, we obtain a list of extracted IS-
A relationships between terms, and their frequency.
For each of the word clusters, consisting of similar
terms for the same target term sense, we aggregate
the IS-A information by summing the frequency of
hypernyms, and multiplying this sum by the number
of words in the cluster that elicited this hypernym.
This results in taxonomic information for labeling
the clusters, which provides an abstraction layer for
terms in context6. Table 1 shows an example of this
labeling from the model described below. The most
similar 200 terms for ?jaguar? have been clustered
into the car sense and the cat sense and the high-
est scoring 6 hypernyms provide a concise descrip-
tion of these senses. This information can be used
to automatically map these cluster senses to senses
in an taxonomy or ontology. Occurrences of am-
biguous words in context can be disambiguated to
these cluster senses comparing the actual context
with salient contexts per sense, obtained by aggre-
gating the Bims from the FOJO graph per cluster.
sense IS-A labels similar terms
jaguar
N.0
car, brand,
company,
automaker,
manufacturer,
vehicle
geely, lincoln-mercury,
tesla, peugeot, ..., mit-
subishi, cadillac, jag, benz,
mclaren, skoda, infiniti,
sable, thunderbird
jaguar
N.1
animal, species,
wildlife, team,
wild animal, cat
panther, cougar, alligator,
tiger, elephant, bull, hippo,
dragon, leopard, shark,
bear, otter, lynx, lion
Table 1: Word sense induction and cluster labeling exam-
ple for ?jaguar?. The shortened cluster for the car sense
has 186 members.
3 Interactive Visualization
3.1 Open Domain Model
The open domain model used in the current vi-
sualization has been trained from newspaper cor-
6Note that this mechanism also elicits hypernyms for unam-
biguous terms receiving a single cluster by the WSI technique.
8
Figure 2: Visualization GUI with prior expansions for
?cold?. Jobims are visualized on the left, expansions on
the right side.
pora using 120 million sentences (about 2 Giga-
words), compiled from LCC (Richter et al, 2006)
and the Gigaword (Parker et al, 2011) corpus. We
constructed a UIMA (Ferrucci and Lally, 2004)
pipeline, which tokenizes, lemmatizes and parses
the data using the Stanford dependency parser (de
Marneffe et al, 2006). The last annotator in the
pipeline annotates Jos and Bims using the collapsed
dependency relations, cf. Fig. 1. We define the lem-
matized forms of the terminals including the part-
of-speech as Jo and the lemmatized dependent word
and the dependency relation name as Bim.
3.2 Interactive Visualization Features
Evaluating the impact of this technology in applica-
tions is an ongoing effort. However, in the context
of this paper, we will show a visualization of the ca-
pabilities allowed by this flavor of distributional se-
mantics. The visualization is a GUI as depicted in
Figure 2, and exemplifies a set of capabilities that
can be accessed through an API. It is straightfor-
ward to include all shown data as features for seman-
tic preprocessing. The input is a sentence in natural
language, which is processed into JoBim pairs as de-
scribed above. All the Jos can be expanded, showing
their paradigmatic relations with other words.
We can perform this operation with and without
taking the context into account (cf. Sect. 2.3). The
latter performs an implicit disambiguation by rank-
ing similar words higher if they fit the context. In
the example, the ?common cold? sense clearly dom-
inates in the prior expansions. However, ?weather?
and ?chill? appear amongst the top-similar prior ex-
pansions.
We also have implemented a sense view, which
displays sense clusters for the selected word, see
Figure 3. Per sense, a list of expansions is pro-
vided together with a list of possible IS-A types. In
this example, the algorithm identified two senses of
?cold? as a temperature and a disease (not all clus-
ter members shown). Given the JoBim graph of the
context (as displayed left in Fig. 2), the particular
occurrence of ?cold? can be disambiguated to Clus-
ter 0 in Fig. 3, since its Bims ?amod(@@,nasty)?
and ?-dobj(catch, @@)? are found in FOJO for far
more members of cluster 0 than for members of clus-
ter 1. Applications of this type of information in-
clude knowledge-based word sense disambiguation
(Miller et al, 2012), type coercion (Kalyanpur et al,
2011) and answer justification in question answering
(Chu-Carroll et al, 2012).
4 Conclusion
In this paper we discussed applications of the Jo-
BimText platform and introduced a new interactive
visualization which showcases a graph-based unsu-
pervised technology for semantic processing. The
implementation is operationalized in a way that it
can be efficiently trained ?off line? using MapRe-
duce, generating domain and language specific mod-
els for distributional semantics. In its ?on line? use,
those models are used to enhance parsing with con-
textualized text expansions of terms. This expansion
step is very efficient and runs on a standard laptop,
so it can be used as a semantic text preprocessor. The
entire project, including pre-computed data models,
is available in open source under the ASL 2.0, and
allows computing contextualized lexical expansion
on arbitrary domains.
References
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Comp. Ling., 36(4):673?721.
M. Baroni and R. Zamparelli. 2010. Nouns are vectors,
adjectives are matrices: representing adjective-noun
constructions in semantic space. In Proc. EMNLP-
2010, pages 1183?1193, Cambridge, Massachusetts.
C. Biemann and M. Riedl. 2013. Text: Now in 2D! a
framework for lexical expansion with contextual simi-
larity. Journal of Language Modelling, 1(1):55?95.
C. Biemann. 2010. Co-occurrence cluster features for
lexical substitutions in context. In Proceedings of
TextGraphs-5, pages 55?59, Uppsala, Sweden.
9
Figure 3: Senses induced for the term ?cold?.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet alocation. J. Mach. Learn. Res., 3:993?1022,
March.
J. Chu-Carroll, J. Fan, B. K. Boguraev, D. Carmel,
D. Sheinwald, and C. Welty. 2012. Finding needles
in the haystack: search and candidate generation. IBM
J. Res. Dev., 56(3):300?311.
M.-C. de Marneffe, B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses from
phrase structure parses. In Proc. LREC-2006, Genova,
Italy.
Ferdinand de Saussure. 1916. Cours de linguistique
ge?ne?rale. Payot, Paris, France.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied Data Processing on Large Clusters. In Proc. OSDI
?04, San Francisco, CA.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent se-
mantic analysis. Journal of the American Society for
Information Science, 41(6):391?407.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
K. Erk and S. Pado?. 2008. A structured vector space
model for word meaning in context. In Proc. EMNLP-
2008, pages 897?906, Honolulu, Hawaii.
S. Evert. 2004. The statistics of word cooccurrences:
word pairs and collocations. Ph.D. thesis, IMS, Uni-
versita?t Stuttgart.
D. Ferrucci and A. Lally. 2004. UIMA: An Architectural
Approach to Unstructured Information Processing in
the Corporate Research Environment. In Nat. Lang.
Eng. 2004, pages 327?348.
D. Ferrucci, A. Levas, S. Bagchi, D. Gondek, and E. T.
Mueller. 2013. Watson: Beyond Jeopardy! Artificial
Intelligence, 199-200:93?105.
Z. S. Harris. 1951. Methods in Structural Linguistics.
University of Chicago Press, Chicago.
M. A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. COLING-
1992, pages 539?545, Nantes, France.
A. Kalyanpur, J.W. Murdock, J. Fan, and C. Welty. 2011.
Leveraging community-built knowledge for type co-
ercion in question answering. In Proc. ISWC 2011,
pages 144?156. Springer.
J. D. Lafferty, A. McCallum, and F. C. N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proc.
ICML 2001, pages 282?289, San Francisco, CA, USA.
J. Lin and C. Dyer. 2010. Data-Intensive Text Processing
with MapReduce. Morgan & Claypool Publishers, San
Rafael, CA.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. COLING-98, pages 768?774,
Montre?al, Quebec, Canada.
G. A. Miller and W. G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
T. Miller, C. Biemann, T. Zesch, and I. Gurevych. 2012.
Using distributional similarity for lexical expansion
in knowledge-based word sense disambiguation. In
Proc. COLING-2012, pages 1781?1796, Mumbai, In-
dia.
R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda.
2011. English Gigaword Fifth Edition. Linguistic
Data Consortium, Philadelphia.
M. Richter, U. Quasthoff, E. Hallsteinsdo?ttir, and C. Bie-
mann. 2006. Exploiting the leipzig corpora collection.
In Proc. IS-LTC 2006, Ljubljana, Slovenia.
H. Schu?tze. 1993. Word space. In Advances in Neu-
ral Information Processing Systems 5, pages 895?902.
Morgan Kaufmann.
G. Szarvas, C. Biemann, and I. Gurevych. 2013. Super-
vised all-words lexical substitution using delexicalized
features. In Proc. NAACL-2013, Atlanta, GA, USA.
D. Widdows and B. Dorow. 2002. A graph model for
unsupervised lexical acquisition. In Proc. COLING-
2002, pages 1?7, Taipei, Taiwan.
10
