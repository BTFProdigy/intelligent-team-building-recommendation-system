Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 12?21, Prague, June 2007. c?2007 Association for Computational Linguistics
Using Semantic Roles to Improve Question Answering
Dan Shen
Spoken Language Systems
Saarland University
Saarbruecken, Germany
dan@lsv.uni-saarland.de
Mirella Lapata
School of Informatics
University of Edinburgh
Edinburgh, UK
mlap@inf.ed.ac.uk
Abstract
Shallow semantic parsing, the automatic
identification and labeling of sentential con-
stituents, has recently received much atten-
tion. Our work examines whether seman-
tic role information is beneficial to question
answering. We introduce a general frame-
work for answer extraction which exploits
semantic role annotations in the FrameNet
paradigm. We view semantic role assign-
ment as an optimization problem in a bipar-
tite graph and answer extraction as an in-
stance of graph matching. Experimental re-
sults on the TREC datasets demonstrate im-
provements over state-of-the-art models.
1 Introduction
Recent years have witnessed significant progress in
developing methods for the automatic identification
and labeling of semantic roles conveyed by senten-
tial constituents.1 The success of these methods, of-
ten referred to collectively as shallow semantic pars-
ing (Gildea and Jurafsky, 2002), is largely due to the
availability of resources like FrameNet (Fillmore et
al., 2003) and PropBank (Palmer et al, 2005), which
document the surface realization of semantic roles in
real world corpora.
More concretely, in the FrameNet paradigm, the
meaning of predicates (usually verbs, nouns, or ad-
jectives) is conveyed by frames, schematic repre-
sentations of situations. Semantic roles (or frame
1The approaches are too numerous to list; we refer the inter-
ested reader to Carreras and Ma`rquez (2005) for an overview.
elements) are defined for each frame and corre-
spond to salient entities present in the evoked situ-
ation. Predicates with similar semantics instantiate
the same frame and are attested with the same roles.
The FrameNet database lists the surface syntactic
realizations of semantic roles, and provides anno-
tated example sentences from the British National
Corpus. For example, the frame Commerce Sell has
three core semantic roles, namely Buyer, Goods, and
Seller ? each expressed by an indirect object, a di-
rect object, and a subject (see sentences (1a)?(1c)).
It can also be attested with non-core (peripheral)
roles (e.g., Means, Manner, see (1d) and (1e)) that
are more generic and can be instantiated in sev-
eral frames, besides Commerce Sell. The verbs sell,
vend, and retail can evoke this frame, but also the
nouns sale and vendor.
(1) a. [Lee]Seller sold a textbook [to
Abby]Buyer.
b. [Kim]Seller sold [the sweater]Goods.
c. [My company]Seller has sold [more
than three million copies]Goods.
d. [Abby]Seller sold [the car]Goods [for
cash]Means.
e. [He]Seller [reluctanctly]Manner sold
[his rock]Goods.
By abstracting over surface syntactic configura-
tions, semantic roles offer an important first step to-
wards deeper text understanding and hold promise
for a range of applications requiring broad cover-
age semantic processing. Question answering (QA)
is often cited as an obvious beneficiary of semantic
12
role labeling (Gildea and Jurafsky, 2002; Palmer et
al., 2005; Narayanan and Harabagiu, 2004). Faced
with the question Q: What year did the U.S. buy
Alaska? and the retrieved sentence S: . . .before Rus-
sia sold Alaska to the United States in 1867, a hypo-
thetical QA system must identify that United States
is the Buyer despite the fact that it is attested in one
instance as a subject and in another as an object.
Once this information is known, isolating the correct
answer (i.e., 1867 ) can be relatively straightforward.
Although conventional wisdom has it that seman-
tic role labeling ought to improve answer extraction,
surprising little work has been done to this effect
(see Section 2 for details) and initial results have
been mostly inconclusive or negative (Sun et al,
2005; Kaisser, 2006). There are at least two good
reasons for these findings. First, shallow semantic
parsers trained on declarative sentences will typi-
cally have poor performance on questions and gen-
erally on out-of-domain data. Second, existing re-
sources do not have exhaustive coverage and recall
will be compromised, especially if the question an-
swering system is expected to retrieve answers from
unrestricted text. Since FrameNet is still under de-
velopment, its coverage tends to be more of a prob-
lem in comparison to other semantic role resources
such as PropBank.
In this paper we propose an answer extraction
model which effectively incorporates FrameNet-
style semantic role information. We present an auto-
matic method for semantic role assignment which is
conceptually simple and does not require extensive
feature engineering. A key feature of our approach
is the comparison of dependency relation paths at-
tested in the FrameNet annotations and raw text. We
formalize the search for an optimal role assignment
as an optimization problem in a bipartite graph. This
formalization allows us to find an exact, globally op-
timal solution. The graph-theoretic framework goes
some way towards addressing coverage problems re-
lated with FrameNet and allows us to formulate an-
swer extraction as a graph matching problem. As a
byproduct of our main investigation we also exam-
ine the issue of FrameNet coverage and show how
much it impacts performance in a TREC-style ques-
tion answering setting.
In the following section we provide an overview
of existing work on question answering systems that
exploit semantic role-based lexical resources. Then
we define our learning task and introduce our ap-
proach to semantic role assignment and answer ex-
traction in the context of QA. Next, we present our
experimental framework and data. We conclude the
paper by presenting and discussing our results.
2 Related Work
Question answering systems have traditionally de-
pended on a variety of lexical resources to bridge
surface differences between questions and potential
answers. WordNet (Fellbaum, 1998) is perhaps the
most popular resource and has been employed in
a variety of QA-related tasks ranging from query
expansion, to axiom-based reasoning (Moldovan et
al., 2003), passage scoring (Paranjpe et al, 2003),
and answer filtering (Leidner et al, 2004). Besides
WordNet, recent QA systems increasingly rely on
syntactic information as a means of abstracting over
word order differences and structural alternations
(e.g., passive vs. active voice). Most syntax-based
QA systems (Wu et al, 2005) incorporate some
means of comparison between the tree representing
the question with the subtree surrounding the answer
candidate. The assumption here is that appropriate
answers are more likely to have syntactic relations
in common with their corresponding question. Syn-
tactic structure matching has been applied to pas-
sage retrieval (Cui et al, 2005) and answer extrac-
tion (Shen and Klakow, 2006).
Narayanan and Harabagiu (2004) were the first
to stress the importance of semantic roles in an-
swering complex questions. Their system identifies
predicate argument structures by merging semantic
role information from PropBank and FrameNet. Ex-
pected answers are extracted by performing proba-
bilistic inference over the predicate argument struc-
tures in conjunction with a domain specific topic
model. Sun et al (2005) incorporate semantic analy-
sis in their TREC05 QA system. They use ASSERT
(Pradhan et al, 2004), a publicly available shallow
semantic parser trained on PropBank, to generate
predicate-argument structures which subsequently
form the basis of comparison between question and
answer sentences. They find that semantic analysis
does not boost performance due to the low recall
of the semantic parser. Kaisser (2006) proposes a
13
SemStruc ac1SemStruc ac2
SemStruc aci
SemStruc q
Sent. Model I
Q Model I
Model II Answer
Figure 1: Architecture of answer extraction
question paraphrasing method based on FrameNet.
Questions are assigned semantic roles by matching
their dependency relations with those attested in the
FrameNet annotations. The assignments are used to
create question reformulations which are submitted
to Google for answer extraction. The semantic role
assignment module is not probabilistic, it relies on
strict matching, and runs into severe coverage prob-
lems.
In line with previous work, our method exploits
syntactic information in the form of dependency re-
lation paths together with FrameNet-like semantic
roles to smooth lexical and syntactic divergences be-
tween question and answer sentences. Our approach
is less domain dependent and resource intensive than
Narayanan and Harabagiu (2004), it solely employs
a dependency parser and the FrameNet database. In
contrast to Kaisser (2006), we model the semantic
role assignment and answer extraction tasks numer-
ically, thereby alleviating the coverage problems en-
countered previously.
3 Problem Formulation
We briefly summarize the architecture of the QA
system we are working with before formalizing the
mechanics of our FrameNet-based answer extraction
module. In common with previous work, our over-
all approach consists of three stages: (a) determining
the expected answer type of the question, (b) retriev-
ing passages likely to contain answers to the ques-
tion, and (c) performing a match between the ques-
tion words and retrieved passages in order to extract
the answer. In this paper we focus on the last stage:
question and answer sentences are normalized to a
FrameNet-style representation and answers are re-
trieved by selecting the candidate whose semantic
structure is most similar to the question.
The architecture of our answer extraction mod-
ule is shown in Figure 1. Semantic structures for
questions and sentences are automatically derived
using the model described in Section 4 (Model I). A
semantic structure SemStruc = ?p,Set(SRA)? con-
sists of a predicate p and a set of semantic role as-
signments Set(SRA). p is a word or phrase evok-
ing a frame F of FrameNet. A semantic role assign-
ment SRA is a ternary structure ?w,SR,s?, consist-
ing of frame element w, its semantic role SR, and
score s indicating to what degree SR qualifies as a
label for w.
For a question q, we generate a semantic struc-
ture SemStrucq. Question words, such as what, who,
when, etc., are considered expected answer phrases
(EAPs). We require that EAPs are frame elements
of SemStrucq. Likely answer candidates are ex-
tracted from answer sentences following some pre-
processing steps detailed in Section 6. For each
candidate ac, we derive its semantic structure
SemStrucac and assume that ac is a frame ele-
ment of SemStrucac. Question and answer seman-
tic structures are compared using a model based on
graph matching detailed in Section 5 (Model II).
We calculate the similarity of all derived pairs
?SemStrucq,SemStrucac? and select the candidate
with the highest value as an answer for the question.
4 Semantic Structure Generation
Our method crucially exploits the annotated sen-
tences in the FrameNet database together with the
output of a dependency parser. Our guiding assump-
tion is that sentences that share dependency rela-
tions will also share semantic roles as long as they
evoke the same or related frames. This is motivated
by much research in lexical semantics (e.g., Levin
(1993)) hypothesizing that the behavior of words,
particularly with respect to the expression and in-
terpretation of their arguments, is to a large ex-
tent determined by their meaning. We first describe
how predicates are identified and then introduce our
model for semantic role labeling.
Predicate Identification Predicate candidates are
identified using a simple look-up procedure which
compares POS-tagged tokens against FrameNet en-
tries. For efficiency reasons, we make the simplify-
ing assumption that questions have only one predi-
cate which we select heuristically: (1) verbs are pre-
14
ferred to other parts of speech, (2) if there is more
than one verb in the question, preference is given to
the verb with the highest level of embedding in the
dependency tree, (3) if no verbs are present, a noun
is chosen. For example, in Q: Who beat Floyd Pat-
terson to take the title away?, beat, take away, and
title are identified as predicate candidates and beat
is selected the main predicate of the question. For
answer sentences, we require that the predicate is ei-
ther identical or semantically related to the question
predicate (see Section 5).
In the example given above, the predicate beat
evoques a single frame (i.e., Cause harm). However,
predicates often have multiple meanings thus evo-
quing more than one frame. Knowing which is the
appropriate frame for a given predicate impacts the
semantic role assignment task; selecting the wrong
frame will unavoidably result in erroneous semantic
roles. Rather than disambiguiting polysemous pred-
icates prior to semantic role assignment, we perform
the assignment for each frame evoqued by the pred-
icate.
Semantic Role Assignment Before describing
our approach to semantic role labeling we define
dependency relation paths. A relation path R is a
relation sequence ?r1,r2, ...,rL?, in which rl (l =
1,2, ...,L) is one of predefined dependency relations
with suffix of traverse direction. An example of a
relation path is R = ?sub jU ,ob jD?, where the sub-
scripts U and D indicate upward and downward
movement in trees, respectively. Given an unanno-
tated sentence whose roles we wish to label, we as-
sume that words or phrases w with a dependency
path connecting them to p are frame elements. Each
frame element is represented by an unlabeled depen-
dency path Rw which we extract by traversing the
dependency tree from w to p. Analogously, we ex-
tract from the FrameNet annotations all dependency
paths RSR that are labeled with semantic role infor-
mation and correspond to p. We next measure the
compatibility of labeled and unlabeled paths as fol-
lows:
s(w,SR) =
maxRSR?M [sim(Rw,RSR) ?P(RSR)]
(2)
where M is the set of dependency relation paths
for SR in FrameNet, sim(Rw,RSR) the similarity be-
tween paths Rw and RSR weighted by the relative
w SRw SR
(a) (b)
Figure 2: Sample original bipartite graph (a) and its
subgraph with edge covers (b). In each graph, the
left partition represents frame elements and the right
partition semantic roles.
frequency of RSR in FrameNet (P(RSR)). We con-
sider both core and non-core semantic roles instan-
tiated by frames with at least one annotation in
FrameNet. Core roles tend to have more annotations
in Framenet and consequently are considered more
probable.
We measure sim(Rw,RSR), by adapting a string
kernel to our task. Our hypothesis is that the more
common substrings two dependency paths have,
the more similar they are. The string kernel we
used is similar to Leslie (2002) and defined as
the sum of weighted common dependency rela-
tion subsequences between Rw and RSR. For effi-
ciency, we consider only unigram and bigram sub-
sequences. Subsequences are weighted by a metric
akin to t f ? id f which measures the degree of asso-
ciation between a candidate SR and the dependency
relation r present in the subsequence:
weightSR(r) = fr ? log
(
1+ Nnr
)
(3)
where fr is the frequency of r occurring in SR; N is
the total number of SRs evoked by a given frame;
and nr is the number of SRs containing r.
For each frame element we thus generate a set
of semantic role assignments Set(SRA). This initial
assignment can be usefully represented as a com-
plete bipartite graph in which each frame element
(word or phrase) is connected to the semantic roles
licensed by the predicate and vice versa. (see Fig-
ure 2a). Edges are weighted and represent how com-
patible the frame elements and semantic roles are
(see equation (2)). Now, for each frame element w
15
Q: Who discovered prions?S: 1997: Stanley B. Prusiner, United States, discovery of prions, ...
SemStruc q
p: discover
Original SR assignments: 
Optimized SR assignments: 
0.06 Cognizer
Phenomenon
Ground
State
Evidence
EAP
prions 
0000
0.01
0.1
0.05
0.05
0.02
0.06 Cognizer
Phenomenon
Ground
State
Evidence
EAP
prions 
0.1
0.05
0.05
0.02
SemStruc ac (ac: Stanley B. Prusiner)
p: discovery
Original SR assignments: 
Optimized SR assignments: 
0.25 Cognizer
Phenomenon
Topic
Evidence
ac
prions 
0.15
0.2
0.16
0.25 Cognizer
Phenomenon
Topic
Evidence
ac
prions 
0.15
0.2
0.16
0.120.07
0 0
Figure 3: Semantic structures induced by our model
for a question and answer sentence
we could simply select the semantic role with the
highest score. However, this decision procedure is
local, i.e., it yields a semantic role assignment for
each frame element independently of all other ele-
ments. We therefore may end up with the same role
being assigned to two frame elements or with frame
elements having no role at all. We remedy this short-
coming by treating the semantic role assignment as
a global optimization problem.
Specifically, we model the interaction between all
pairwise labeling decisions as a minimum weight
bipartite edge cover problem (Eiter and Mannila,
1997; Cormen et al, 1990). An edge cover is a sub-
graph of a bipartite graph so that each node is linked
to at least one node of the other partition. This yields
a semantic role assignment for all frame elements
(see Figure 2b where frame elements and roles are
adjacent to an edge). Edge covers have been success-
fully applied in several natural language processing
tasks, including machine translation (Taskar et al,
2005) and annotation projection (Pado? and Lapata,
2006).
Formally, optimal edge cover assignments are so-
lutions of following optimization problem:
max
E is edge cover ?(ndw,ndSR)?E s(nd
w,ndSR)(4)
where, s(ndw,ndSR) is the compatibility score be-
tween the frame element node ndw and semantic role
node ndSR. Edge covers can be computed efficiently
in cubic time using algorithms for the equivalent
linear assignment problem. Our experiments used
Jonker and Volgenant?s (1987) solver.2
Figure 3 shows the semantic role assignments
generated by our model for the question Q: Who
discovered prions? and the candidate answer sen-
tence S: 1997: Stanley B. Prusiner, United States,
discovery of prions. . . Here we identify two predi-
cates, namely discover and discovery. The expected
answer phrase (EAP) who and the answer candi-
date Stanley B. Prusiner are assigned the COGNIZER
role. Note that frame elements can bear multiple se-
mantic roles. By inducing a soft labeling we hope to
render the matching of questions and answers more
robust, thereby addressing to some extent the cover-
age problems associated with FrameNet.
5 Semantic Structure Matching
We measure the similarity between a question and
its candidate answer by matching their predicates
and semantic role assignments. Since SRs are frame-
specific, we prioritize frame matching to SR match-
ing. Two predicates match if they evoke the same
frame or one of its hypernyms (or hyponyms). The
latter are expressed by the Inherits From and Is In-
herited By relations in the frame definitions. If the
predicates match, we examine whether the assigned
semantic roles match. Since we represent SR assign-
ments as graphs with edge covers, we can also for-
malize SR matching as a graph matching problem.
The similarity between two graphs is measured
as the sum of similarities between their subgraphs.
We first decompose a graph into subgraphs consist-
ing of one frame element node w and a set of SR
nodes connected to it. The similarity between two
subgraphs SubG1, and SubG2 is then formalized as:
(5) Sim(SubG1,SubG2) =
?
ndSR1 ? SubG1
ndSR2 ? SubG2
ndSR1 = ndSR2
1
|s(ndw,ndSR1 )? s(ndw,ndSR2 )|+1
where, ndSR1 and ndSR2 are semantic role nodes con-
nected to a frame element node ndw in SubG1 and
2The software is available from http://www.magiclogic.
com/assignment.html .
16
1757[11, 20]
2117[21, 50]
439[51, 100] 40[101, INF)
33800
1175[1, 5]1287[6, 10]
Figure 4: Distribution of Numbers of Predicates and
annotated sentences; each sub-pie, lists the number
of predicates (above) with their corresponding range
of annotated sentences (below)
SubG2, respectively. s(ndw,ndsr1 ) and s(ndw,ndSR2 )
are edge weights between two nodes in correspond-
ing subgraphs (see (2)). Our intuition here is that
the more semantic roles two subgraphs share for a
given frame element, the more similar they are and
the closer their corresponding edge weights should
be. Edge weights are normalized by dividing by the
sum of all edges in a subgraph.
6 Experimental Setup
Data All our experiments were performed on the
TREC02?05 factoid questions. We excluded NIL
questions since TREC doesn?t supply an answer for
them. We used the FrameNet V1.3 lexical database.
It contains 10,195 predicates grouped into 795 se-
mantic frames and 141,238 annotated sentences.
Figure 4 shows the number of annotated sentences
available for different predicates. As can be seen,
there are 3,380 predicates with no annotated sen-
tences and 1,175 predicates with less than 5 anno-
tated sentences. All FrameNet sentences, questions,
and answer sentences were parsed using MiniPar
(Lin, 1994), a robust dependency parser.
As mentioned in Section 4 we extract depen-
dency relation paths by traversing the dependency
tree from the frame element node to the predicate
node. We used all dependency relations provided
by MiniPar (42 in total). In order to increase cov-
erage, we combine all relation paths for predicates
that evoke the same frame and are labeled with the
same POS tag. For example, found and establish
are both instances of the frame Intentionally create
but the database does not have any annotated sen-
tences for found.v. In default of not assigning any
role labels for found.v, our model employs the rela-
tion paths for the semantically related establish.v.
Preprocessing Here we summarize the steps of
our QA system preceding the assignment of seman-
tic structure and answer extraction. For each ques-
tion, we recognize its expected answer type (e.g., in
Q: Which record company is Fred Durst with? we
would expect the answer to be an ORGANIZA-
TION ). Answer types are determined using classi-
fication rules similar to Li and Roth (2002). We also
reformulate questions into declarative sentences fol-
lowing the strategy proposed in Brill et al (2002).
The reformulated sentences are submitted as
queries to an IR engine for retrieving sentences with
relevant answers. Specifically, we use the Lemur
Toolkit3, a state-of-the-art language model-driven
search engine. We work only with the 50 top-ranked
sentences as this setting performed best in previ-
ous experiments of our QA system. We also add to
Lemur?s output gold standard sentences, which con-
tain and support an answer for each question. Specif-
ically, documents relevant for each question are re-
trieved from the AQUAINT Corpus4 according to
TREC supplied judgments. Next, sentences which
match both the TREC provided answer pattern and
at least one question key word are extracted and their
suitability is manually judged by humans. The set of
relevant sentences thus includes at least one sentence
with an appropriate answer as well as sentences that
do not contain any answer specific information. This
setup is somewhat idealized, however it allows us to
evaluate in more detail our answer extraction mod-
ule (since when an answer is not found, we know it
is the fault of our system).
Relevant sentences are annotated with their
named entities using Lingpipe5, a MUC-based
named entity recognizer. When we successfully
classify a question with an expected answer type
3See http://www.lemurproject.org/ for details.
4This corpus consists of English newswire texts and is used
as the main document collection in official TREC evaluations.
5The software is available from www.alias-i.com/
lingpipe/
17
(e.g., ORGANIZATION in the example above), we
assume that all NPs attested in the set of relevant
sentences with the same answer type are candidate
answers; in cases where no answer type is found
(e.g., as in Q: What are prions made of? ), all NPs
in the relevant answers set are considered candidate
answers.
Baseline We compared our answer extraction
method to a QA system that exploits solely syntac-
tic information without making use of FrameNet or
any other type of role semantic annotations. For each
question, the baseline identifies key phrases deemed
important for answer identification. These are verbs,
noun phrases, and expected answer phrases (EAPs,
see Section 3). All dependency relation paths con-
necting a key phrase and an EAP are compared to
those connecting the same key phrases and an an-
swer candidate. The similarity of question and an-
swer paths is computed using a simplified version
of the similarity measure6 proposed in Shen and
Klakow (2006).
Our second baseline employs Shalmaneser (Erk
and Pado?, 2006), a publicly available shallow se-
mantic parser7, for the role labeling task instead of
the graph-based model presented in Section 4. The
software is trained on the FrameNet annotated sen-
tences using a standard feature set (see Carreras and
Ma`rquez (2005) for details). We use Shalmaneser
to parse questions and answer sentences. The parser
makes hard decisions about the presence or absence
of a semantic role. Unfortunately, this prevents us
from using our method for semantic structure match-
ing (see Section 5) which assumes a soft labeling.
We therefore came up with a simple matching strat-
egy suitable for the parser?s output. For question
and answer sentences matching in their frame as-
signment, phrases bearing the same semantic role as
the EAP are considered answer candidates. The lat-
ter are ranked according to word overlap (i.e., iden-
tical phrases are ranked higher than phrases with no
6Shen and Klakow (2006) use a dynamic time warping al-
gorithm to calculate the degree to which dependency relation
paths are correlated. Correlations for individual relations are es-
timated from training data whereas we assume a binary value (1
for identical relations and 0 otherwise). The modification was
necessary to render the baseline system comparable to our an-
swer extraction model which is unsupervised.
7The software is available from http://www.coli.
uni-saarland.de/projects/salsa/shal/ .
overlap at all).
7 Results
Our evaluation was motivated by the following ques-
tions: (1) How does the incompleteness of FrameNet
impact QA performance on the TREC data sets? In
particular, we wanted to examine whether there are
questions for which in principle no answer can be
found due to missing frame entries or missing an-
notated sentences. (2) Are all questions and their
corresponding answers amenable to a FrameNet-
style analysis? In other words, we wanted to assess
whether questions and answers often evoke the same
or related frames (with similar roles). This is a pre-
requisite for semantic structure matching and ulti-
mately answer extraction. (3) Do the graph-based
models introduced in this paper bring any perfor-
mance gains over state-of-the-art shallow semantic
parsers or more conventional syntax-based QA sys-
tems? Recall that our graph-based models were de-
signed especially for the QA answer extraction task.
Our results are summarized in Tables 1?3. Table 1
records the number of questions to be answered for
the TREC02?05 datasets (Total). We also give infor-
mation regarding the number of questions which are
in principle unanswerable with a FrameNet-style se-
mantic role analysis.
Column NoFrame shows the number of questions
which don?t have an appropriate frame or predicate
in the database. For example, there is currently no
predicate entry for sponsor or sink (e.g., Q: Who
is the sponsor of the International Criminal Court?
and Q: What date did the Lusitania sink? ). Column
NoAnnot refers to questions for which no semantic
role labeling is possible because annotated sentences
for the relevant predicates are missing. For instance,
there are no annotations for win (e.g., Q: What divi-
sion did Floyd Patterson win? ) or for hit (e.g., Q:
What was the Beatles? first number one hit? ). This
problem is not specific to our method which admit-
tedly relies on FrameNet annotations for performing
the semantic role assignment (see Section 4). Shal-
low semantic parsers trained on FrameNet would
also have trouble assigning roles to predicates for
which no data is available.
Finally, column NoMatch reports the number of
questions which cannot be answered due to frame
18
Data Total NoFrame NoAnnot NoMatch Rest
TREC02 444 87 (19.6) 29 (6.5) 176 (39.6) 152 (34.2)
TREC03 380 55 (14.5) 30 (7.9) 183 (48.2) 112 (29.5)
TREC04 203 47 (23.1) 14 (6.9) 67 (33.0) 75 (36.9)
TREC05 352 70 (19.9) 23 (6.5) 145 (41.2) 114 (32.4)
Table 1: Number of questions which cannot be answered using a FrameNet style semantic analysis; numbers
in parentheses are percentages of Total (NoFrame: frames or predicates are missing; NoAnnot: annotated
sentences are missing, NoMatch: questions and candidate answers evoke different frames.
mismatches. Consider Q: What does AARP stand
for? whose answer is found in S: The American
Association of Retired Persons (AARP) qualify for
discounts. . .. The answer and the question evoke dif-
ferent frames; in fact here a semantic role analysis is
not relevant for locating the right answer. As can be
seen NoMatch cases are by far the most frequent.
The number of questions remaining after excluding
NoFrame, NoAnnot, and NoMatch are shown under
the Rest heading in Table 1.
These results indicate that FrameNet-based se-
mantic role analysis applies to approximately 35%
of the TREC data. This means that an extraction
module relying solely on FrameNet will have poor
performance, since it will be unable to find answers
for more than half of the questions beeing asked. We
nevertheless examine whether our model brings any
performance improvements on this limited dataset
which is admittedly favorable towards a FrameNet
style analysis. Table 2 shows the results of our an-
swer extraction module (SemMatch) together with
two baseline systems. The first baseline uses only
dependency relation path information (SynMatch),
whereas the second baseline (SemParse) uses Shal-
maneser, a state-of-the-art shallow semantic parser
for the role labeling task. We consider an answer
correct if it is returned with rank 1. As can be seen,
SemMatch is significantly better than both Syn-
Match and SemParse, whereas the latter is signifi-
cantly worse than SynMatch.
Although promising, the results in Table 2 are not
very informative, since they show performance gains
on partial data. Instead of using our answer extrac-
tion model on its own, we next combined it with the
syntax-based system mentioned above (SynMatch,
see also Section 6 for details). If FrameNet is indeed
helpful for QA, we would expect an ensemble sys-
Model TREC02 TREC03 TREC04 TREC05
SemParse 13.16 8.92 17.33 13.16
SynMatch 35.53? 33.04? 40.00? 36.84?
SemMatch 53.29?? 49.11?? 54.67?? 59.65??
Table 2: System Performance on subset of TREC
datasets (see Rest column in Table 1); ?: signifi-
cantly better than SemParse; ?: significantly better
than SynMatch (p < 0.01, using a ?2 test).
Model TREC02 TREC03 TREC04 TREC05
SynMatch 32.88? 30.70? 35.95? 34.38?
+SemParse 25.23 23.68 28.57 26.70
+SemMatch 38.96?? 35.53?? 42.36?? 41.76??
Table 3: System Performance on TREC datasets (see
Total column in Table 1); ?: significantly better than
+SemParse; ?: significantly better than SynMatch
(p < 0.01, using a ?2 test).
tem to yield better performance over a purely syn-
tactic answer extraction module. The two systems
were combined as follows. Given a question, we first
pass it to our FrameNet model; if an answer is found,
our job is done; if no answer is returned, the ques-
tion is passed on to SynMatch. Our results are given
in Table 3. +SemMatch and +SemParse are ensem-
ble systems using SynMatch together with the QA
specific role labeling method proposed in this pa-
per and Shalmaneser, respectively. We also compare
these systems against SynMatch on its own.
We can now attempt to answer our third ques-
tion concerning our model?s performance on the
TREC data. Our experiments show that a FrameNet-
enhanced answer extraction module significantly
outperforms a similar module that uses only syn-
tactic information (compare SynMatch and +Sem-
Match in Table 3). Another interesting finding is that
19
the shallow semantic parser performs considerably
worse in comparison to our graph-based models and
the syntax-based system. Inspection of the parser?s
output highlights two explanations for this. First, the
shallow semantic parser has difficulty assigning ac-
curate semantic roles to questions (even when they
are reformulated as declarative sentences). And sec-
ondly, it tends to favor precision over recall, thus re-
ducing the number of questions for which answers
can be found. A similar finding is reported in Sun et
al. (2005) for a PropBank trained parser.
8 Conclusion
In this paper we assess the contribution of semantic
role labeling to open-domain factoid question an-
swering. We present a graph-based answer extrac-
tion model which effectively incorporates FrameNet
style role semantic information and show that it
achieves promising results. Our experiments show
that the proposed model can be effectively combined
with a syntax-based system to obtain performance
superior to the latter when used on its own. Fur-
thermore, we demonstrate performance gains over a
shallow semantic parser trained on the FrameNet an-
notated corpus. We argue that performance gains are
due to the adopted graph-theoretic framework which
is robust to coverage and recall problems.
We also provide a detailed analysis of the appro-
priateness of FrameNet for QA. We show that per-
formance can be compromised due to incomplete
coverage (i.e., missing frame or predicate entries
as well as annotated sentences) but also because of
mismatching question-answer representations. The
question and the answer may evoke different frames
or the answer simply falls outside the scope of a
given frame (i.e., in a non predicate-argument struc-
ture). Our study shows that mismatches are rela-
tively frequent and motivates the use of semantically
informed methods in conjunction with syntax-based
methods.
Important future directions lie in evaluating the
contribution of alternative semantic role frameworks
(e.g., PropBank) to the answer extraction task and
developing models that learn semantic roles di-
rectly from unannotated text without the support
of FrameNet annotations (Grenager and Manning,
2006). Beyond question answering, we also plan to
investigate the potential of our model for shallow
semantic parsing since our experience so far has
shown that it achieves good recall.
Acknowledgements We are grateful to Sebastian Pado?
for running Shalmaneser on our data. Thanks to Frank Keller
and Amit Dubey for insightful comments and suggestions. The
authors acknowledge the support of DFG (Shen; PhD stu-
dentship within the International Postgraduate College ?Lan-
guage Technology and Cognitive Systems?) and EPSRC (Lap-
ata; grant EP/C538447/1).
References
E. Brill, S. Dumais, M. Banko. 2002. An analysis of the
askMSR question-answering system. In Proceedings
of the EMNLP, 257?264, Philadelphia, PA.
X. Carreras, L. Ma`rquez, eds. 2005. Proceedings of the
CoNLL shared task: Semantic role labelling, 2005.
T. Cormen, C. Leiserson, R. Rivest. 1990. Introduction
to Algorithms. MIT Press.
H. Cui, R. X. Sun, K. Y. Li, M. Y. Kan, T. S. Chua.
2005. Question answering passage retrieval using de-
pendency relations. In Proceedings of the ACM SIGIR,
400?407. ACM Press.
T. Eiter, H. Mannila. 1997. Distance measures for
point sets and their computation. Acta Informatica,
34(2):109?133.
K. Erk, S. Pado?. 2006. Shalmaneser - a flexible toolbox
for semantic role assignment. In Proceedings of the
LREC, 527?532, Genoa, Italy.
C. Fellbaum, ed. 1998. WordNet. An Electronic Lexical
Database. MIT Press, Cambridge/Mass.
C. J. Fillmore, C. R. Johnson, M. R. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16:235?250.
D. Gildea, D. Jurafsky. 2002. Automatic labeling of se-
mantic roles. Computational Linguistics, 28(3):245?
288.
T. Grenager, C. D. Manning. 2006. Unsupervised dis-
covery of a statistical verb lexicon. In Proceedings of
the EMNLP, 1?8, Sydney, Australia.
R. Jonker, A. Volgenant. 1987. A shortest augmenting
path algorithm for dense and sparse linear assignment
problems. Computing, 38:325?340.
M. Kaisser. 2006. Web question answering by exploiting
wide-coverage lexical resources. In Proceedings of the
11th ESSLLI Student Session, 203?213.
J. Leidner, J. Bos, T. Dalmas, J. Curran, S. Clark, C. Ban-
nard, B. Webber, M. Steedman. 2004. The qed open-
domain answer retrieval system for TREC 2003. In
Proceedings of the TREC, 595?599.
C. Leslie, E. Eskin, W. S. Noble. 2002. The spectrum
kernel: a string kernel for SVM protein classification.
In Proceedings of the Pacific Biocomputing Sympo-
sium, 564?575.
B. Levin. 1993. English Verb Classes and Alternations:
A Preliminary Investigation. University of Chicago
Press, Chicago.
20
X. Li, D. Roth. 2002. Learning question classifiers. In
Proceedings of the 19th COLING, 556?562, Taipei,
Taiwan.
D. K. Lin. 1994. PRINCIPAR?an efficient, broad-
coverage, principle-based parser. In Proceedings of
the 15th COLING, 482?488.
D. Moldovan, C. Clark, S. Harabagiu, S. Maiorano.
2003. COGEX: A logic prover for question answer-
ing. In Proceedings of the HLT/NAACL, 87?93, Ed-
monton, Canada.
S. Narayanan, S. Harabagiu. 2004. Question answering
based on semantic structures. In Proceedings of the
19th COLING, 184?191.
S. Pado?, M. Lapata. 2006. Optimal constituent alignment
with edge covers for semantic projection. In Proceed-
ings of the COLING/ACL, 1161?1168.
M. Palmer, D. Gildea, P. Kingsbury. 2005. The Propo-
sition Bank: An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106.
D. Paranjpe, G. Ramakrishnan, S. Srinivasa. 2003. Pas-
sage scoring for question answering via bayesian infer-
ence on lexical relations. In Proceedings of the TREC,
305?210.
S. Pradhan, W. Ward, K. Hacioglu, J. Martin, D. Jurafsky.
2004. Shallow semantic parsing using support vector
machines. In Proceedings of the HLT/NAACL, 141?
144, Boston, MA.
D. Shen, D. Klakow. 2006. Exploring correlation of de-
pendency relation paths for answer extraction. In Pro-
ceedings of the COLING/ACL, 889?896.
R. X. Sun, J. J. Jiang, Y. F. Tan, H. Cui, T. S. Chua,
M. Y. Kan. 2005. Using syntactic and semantic re-
lation analysis in question answering. In Proceedings
of the TREC.
B. Taskar, S. Lacoste-Julien, D. Klein. 2005. A discrim-
inative matching approach to word alignment. In Pro-
ceedings of the HLT/EMNLP, 73?80, Vancouver, BC.
M. Wu, M. Y. Duan, S. Shaikh, S. Small, T. Strzalkowski.
2005. University at albany?s ilqua in trec 2005. In
Proceedings of the TREC, 77?83.
21
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 507 ? 518, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Exploring Syntactic Relation Patterns  
for Question Answering 
Dan Shen1,2, Geert-Jan M. Kruijff 1, and Dietrich Klakow2 
1
 Department of Computational Linguistics, Saarland University, 
Building 17, Postfach 15 11 50, 66041 Saarbruecken, Germany 
{dshen, gj}@coli.uni-sb.de 
2
 Lehrstuhl Sprach Signal Verarbeitung,Saarland University, 
Building 17, Postfach 15 11 50, 66041 Saarbruecken, Germany 
{dietrich.klakow}@lsv.uni-saarland.de 
Abstract. In this paper, we explore the syntactic relation patterns for open-
domain factoid question answering.  We propose a pattern extraction method to 
extract the various relations between the proper answers and different types of 
question words, including target words, head words, subject words and verbs, 
from syntactic trees.  We further propose a QA-specific tree kernel to partially 
match the syntactic relation patterns.  It makes the more tolerant matching be-
tween two patterns and helps to solve the data sparseness problem.  Lastly, we 
incorporate the patterns into a Maximum Entropy Model to rank the answer 
candidates.  The experiment on TREC questions shows that the syntactic rela-
tion patterns help to improve the performance by 6.91 MRR based on the com-
mon features. 
1   Introduction 
Question answering is to find answers for open-domain natural language questions in 
a large document collection.  A typical QA system usually consists of three basic 
modules: 1. Question Processing (QP) Module, which finds some useful information 
from questions, such as expected answer type and key words; 2. Information Retrieval 
(IR) Module, which searches a document collection to retrieve a set of relevant sen-
tences using the key words; 3. Answer Extraction (AE) Module, which analyzes the 
relevant sentences using the information provided by the QP module and identify the 
proper answer.  In this paper, we will focus on the AE module. 
In order to find the answers, some evidences, such as expected answer types and 
surface text patterns, are extracted from answer sentences and incorporated in the AE 
module using a pipelined structure, a scoring function or some statistical-based meth-
ods.  However, the evidences extracted from plain texts are not sufficient to identify a 
proper answer.  For examples, for ?Q1910: What are pennies made of??, the expected 
answer type is unknown; for ?Q21: Who was the first American in space??, the sur-
face patterns may not detect the long-distance relations between the question key 
phrase ?the first American in space? and the answer ?Alan Shepard? in ?? that car-
ried Alan Shepard on a 15 - minute suborbital flight in 1961 , making him the first 
508 D. Shen, G.-J.M. Kruijff, and D. Klakow 
American in space.?  To solve these problems, more evidences need to be extracted 
from the more complex data representations, such as parse trees. 
In this paper, we explore the syntactic relation patterns (SRP) for the AE module.  
An SRP is defined as a kind of relation between a question word and an answer can-
didate in the syntactic tree.  Different from the textual patterns, the SRPs capture the 
relations based on the sentence syntactic structure rather than the sentence surface.  
Therefore, they may get the deeper understanding of the relations and capture the long 
range dependency between words regardless of their ordering and distance in the 
surface text.  Based on the observation of the task, we find that the syntactic relations 
between different types of question words and answers vary a lot with each other.  We 
classify the question words into four classes, including target words, head words, 
subject phrases and verbs, and generate the SRPs for them respectively.  Firstly, we 
generate the SRPs from the training data and score them based on the support and 
confidence measures.  Next, we propose a QA-specific tree kernel to calculate the 
similarity between two SRPs in order to match the patterns from the unseen data into 
the pattern set.  The tree kernel makes the partial matching between two patterns and 
helps to solve the data sparseness problem.  Lastly, we incorporate the SRPs into a 
Maximum Entropy Model along with some common features to classify the answer 
candidates.  The experiment on TREC questions shows that the syntactic relation 
patterns improve the performance by 6.91 MRR based on the common features. 
Although several syntactic relations, such as subject-verb and verb-object, have 
been also considered in some other systems, they are basically extracted using a small 
number of hand-built rules.  As a result, they are limited and costly.  In our task, we 
automatically extract the various relations between different question words and an-
swers and more tolerantly match the relation patterns using the tree kernel. 
2   Related Work 
The relations between answers and question words have been explored by many suc-
cessful QA systems based on certain sentence representations, such as word sequence, 
logic form, parse tree, etc. 
In the simplest case, a sentence is represented as a sequence of words.  It is as-
sumed that, for certain type of questions, the proper answers always have certain 
surface relations with the question words.  For example, ?Q: When was X born??, the 
proper answers often have such relation ?<X> ( <Answer>--? with the question 
phrase X .  [14] first used a predefined pattern set in QA and achieved a good per-
formance at TREC10.  [13] further developed a bootstrapping method to learn the 
surface patterns automatically.  When testing, most of them make the partial matching 
using regular expression.  However, such surface patterns strongly depend on the 
word ordering and distance in the text and are too specific to the question type. 
LCC [9] explored the syntactic relations, such as subject, object, prepositional at-
tachment and adjectival/adverbial adjuncts, based on the logic form transformation.  
Furthermore they used a logic prover to justify the answer candidates.  The prover is 
accurate but costly. 
Most of the QA systems explored the syntactic relations on the parse tree.  Since 
such relations do not depend on the word ordering and distance in the sentence, they 
may cope with the various surface expressions of the sentence.  ISI [7] extracted the 
 Exploring Syntactic Relation Patterns for Question Answering 509 
relations, such as ?subject-verb? and ?verb-object?, in the answer sentence tree and 
compared with those in the question tree.  IBM?s Maximum Entropy-based model 
[10] integrated a rich feature set, including words co-occurrence scores, named entity, 
dependency relations, etc.  For the dependency relations, they considered some prede-
fined relations in trees by partial matching.  BBN [15] also considered the verb-
argument relations. 
However, most of the current QA systems only focus on certain relation types, 
such as verb-argument relations, and extract them from the syntactic tree using some 
heuristic rules.  Therefore, extracting such relations is limited in a very local context 
of the answer node, such as its parent or sibling nodes, and does not involve long 
range dependencies.  Furthermore, most of the current systems only concern the rela-
tions to certain type of question words, such as verb.  In fact, different types of ques-
tion words may have different indicative relations with the proper answers.  In this 
paper, we will automatically extract more comprehensive syntactic relation patterns 
for all types of question words, partially match them using a QA-specific tree kernel 
and evaluate their contributions by integrating them into a Maximum Entropy Model. 
3   Syntactic Relation Pattern Generating 
In this section, we will discuss how to extract the syntactic relation patterns.  Firstly, 
we briefly introduce the question processing module which provides some necessary 
information to the answer extraction module.  Secondly, we generate the dependency 
tree of the answer sentence and map the question words into the tree using a Modified 
Edit Distance (MED) algorithm.  Thirdly, we define and extract the syntactic relation 
patterns in the mapped dependency tree.  Lastly, we score and filter the patterns. 
3.1   Question Processing Module 
The key words are extracted from the questions.  Considering that different key words 
may have different syntactic relations with the answers, we divide the key words into 
the following four types: 
1. Target Words, which are extracted from what / which questions.  Such words indi-
cate the expected answer types, such as ?party? in ?Q1967: What party led ???. 
2. Head Words, which are extracted from how questions.  Such words indicate the 
expected answer heads, such as ?dog? in the ?Q210: How many dogs pull ??? 
3. Subject Phrases, which are extracted from all types of questions.  They are the base 
noun phrases of the questions except the target words and the head words. 
4. Verbs, which are the main verbs extracted from non-definition questions. 
The key words described above are identified and classified based on the question 
parse tree.  We employ the Collins Parser [2] to parse the questions and the answer 
sentences. 
3.2   Question Key Words Mapping 
From this section, we start to introduce the AE module.  Firstly, the answer sentences 
are tagged with named entities and parsed.  Secondly, the parse trees are transformed 
510 D. Shen, G.-J.M. Kruijff, and D. Klakow 
to the dependency trees based on a set of rules.  To simplify a dependency tree, some 
special rules are used to remove the non-useful nodes and dependency information.  
The rules include 
1. Since the question key words are always NPs and verbs, only the syntactic rela-
tions between NP and NP / NP and verb are considered. 
2. The original form of Base Noun Phrase (BNP) is kept and the dependency relations 
within the BNPs are not considered, such as adjective-noun.  A base noun phrase is 
defined as the smallest noun phrase in which there are no noun phrases embedded. 
An example of the dependency tree is shown in Figure 1.  We regard all BNP 
nodes and leaf nodes as answer candidates. 
 
Fig. 1. Dependency tree and Tagged dependency tree 
Next, we map the question key words into the simplified dependency trees.  We 
propose a weighted edit distance (WED) algorithm, which is to find the similarity 
between two phrases by computing the minimal cost of operations needed to transform 
one phrase into the other, where an operation is an insertion, deletion, or substitution. 
Different from the commonly-used edit distance algorithm [11], the WED defines 
the more flexible cost function which incorporates the morphological and semantic 
alternations of the words.  The morphological alternations indicate the inflections of 
noun/verb.  For example, for Q2149: How many Olympic gold medals did Carl Lewis 
win?   We map the verb win to the nominal winner in the answer sentence ?Carl 
Lewis, winner of nine Olympic gold medals, thinks that ??.  The morphological alter-
nations are found based on a stemming algorithm and the ?derivationally related 
forms? in WordNet [8].  The semantic alternations consider the synonyms of the 
words.  Some types of the semantic relations in WordNet enable the retrieval of syno-
nyms, such as hypernym, hyponym, etc.  For example, for Q212: Who invented the 
electric guitar?  We may map the verb invent to its direct hypernym create in answer 
sentences.  Based on the observation of the task, we set the substitution costs of the 
alternations as follows: Identical words have cost 0; Words with the same morpho-
logical root have cost 0.2; Words with the hypernym or hyponym relations have cost 
tagged dependency tree dependency tree 
live 
BNP 
NER_PER 
Ellington 
BNP 
NER_LOC 
BNP 
Washington his early NNP 20s
NER_DAT 
VER
VER: the verb of the question 
SUB: the subject words of the question 
TGT_HYP: the hypernym of the target word of the question  
live 
BNP 
NER_PER 
SUB 
Ellington 
BNP 
NER_LOC 
TGT_HYP 
BNP 
Washington his early NNP 20s 
NER_DAT 
Q1916: What city did Duke Ellington live in?  
A: Ellington lived in Washington until his early 20s. 
 Exploring Syntactic Relation Patterns for Question Answering 511 
0.4; Words in the same SynSet have cost 0.6; Words with subsequence relations have 
cost 0.8; otherwise, words have cost 1.  Figure 1 also shows an example of the tagged 
dependency tree. 
3.3   Syntactic Relation Pattern Extraction 
A syntactic relation pattern is defined as the smallest subtree which covers an answer 
candidate node and one question key word node in the dependency tree.  To capture 
different relations between answer candidates and different types of question words, 
we generate four pattern sets, called PSet_target, PSet_head, PSet_subject and 
PSet_verb, for the answer candidates.  The patterns are extracted from the training 
data.  Some pattern examples are shown in Table 1.  For a question Q, there are a set 
of relevant sentences SentSet.  The extraction process is as follows: 
1. for each question Q in the training data 
2. question processing model extract the key words of Q 
3. for each sentence s in SentSet 
a) parse s  
b) map the question key words into the parse tree 
c) tag all BNP nodes in the parse tree as answer candidates. 
d) for each answer candidate (ac) node 
 for each question word (qw) node 
      extract the syntactic relation pattern (srp) for ac and qw  
 add srp to PSet_target, PSet_head, PSet_subject or 
PSet_verb based on the types of qw. 
Table 1. Examples of the patterns in the four pattern sets 
PatternSet  Patterns Sup. Conf. 
(NPB~AC~TGT) 0.55 0.22 
(NPB~AC~null (NPB~null~TGT)) 0.08 0.06 PSet_target 
(NPB~null~null (NPB~AC~null) (NPB~null~TGT)) 0.02 0.09 
PSet_head (NPB~null~null (CD~AC~null) (NPB~null~HEAD)) 0.59 0.67 
(VP~null~null (NPB~null~SUB) (NPB~null~null 
(NPB~AC~null))) 
0.04 0.33 
PSet_subject 
(NPB~null~null (NPB~null~SUB) (NPB~AC~null)) 0.02 0.18 
PSet_verb (VP~null~VERB (NPB~AC~null)) 0.18 0.16 
3.4   Syntactic Relation Pattern Scoring 
The patterns extracted in section 3.3 are scored by support and confidence measures.  
Support and confidence measures are most commonly used to evaluate the association 
rules in the data mining area.  The support of a rule is the proportion of times the rule 
applies.  The confidence of a rule is the proportion of times the rule is correct.  In our 
task, we score a pattern by measuring the strength of the association rule from the 
pattern to the proper answer (the pattern is matched => the answer is correct). Let pi 
be any pattern in the pattern set PSet , 
512 D. Shen, G.-J.M. Kruijff, and D. Klakow 
the number of  in which  is correct
support( )
the size of 
p acipi PSet
=  
the number of  in which  is correct
confidence( )
the number of  
p acipi pi
=  
We score the patterns in the PSet_target, PSet_head, PSet_subject and PSet_verb 
respectively.  If the support value is less than the threshold supt or the confidence 
value is less than the threshold conft , the pattern is removed from the set.  In the ex-
periment, we set supt 0.01 and conft  0.5.  Table 1 lists the support and confidence of 
the patterns. 
4   Syntactic Relation Pattern Matching 
Since we build the pattern sets based on the training data in the current experiment, 
the pattern sets may not be large enough to cover all of the unseen cases.  If we make 
the exact match between two patterns, we will suffer from the data sparseness prob-
lem.  So a partial matching method is required.  In this section, we will propose a QA-
specific tree kernel to match the patterns. 
A kernel function 1 2( , ) : [0, ]K x x ? ?X X R , is a similarity measure between 
two objects 1x and 2x with some constraints.  It is the most important component of 
kernel methods [16].  Tree kernels are the structure-driven kernels used to calculate 
the similarity between two trees.  They have been successfully accepted in the natural 
language processing applications, such as parsing [4], part of speech tagging and 
named entity extraction [3], and information extraction [5, 17].  To our knowledge, 
tree kernels have not been explored in answer extraction. 
Suppose that a pattern is defined as a tree T with nodes 0 1{ , , ..., }nt t t  and each node 
it is attached with a set of attributes 0 1{ , , ..., }ma a a , which represent the local charac-
teristics of ti .  In our task, the set of the attributes include Type attributes, Ortho-
graphic attributes and Relation Role attributes, as shown in Table 2.  Figure 2 shows 
an example of the pattern tree T_ac#target. 
The core idea of the tree kernel ( , )1 2K T T  is that the similarity between two trees 
T1 and T2 is the sum of the similarity between their subtrees.  It can be calculated by 
dynamic programming and can capture the long-range relations between two nodes.  
The kernel we use is similar to [17] except that we define a task-specific matching 
function and similarity function, which are two primitive functions to calculate the 
similarity between two nodes in terms of their attributes.  
Matching function 
1 if . .  and . .   
( , )
0 otherwise                                           
i j i j
i j
t type t type t role t role
m t t
= =
=
???  
 Exploring Syntactic Relation Patterns for Question Answering 513 
Similarity function 
0{ ,..., }
( , ) ( . , . )i j i j
ma a a
s t t f t a t a
?
= ?  
where, ( . , . )i jf t a t a  is a compatibility function between two feature values 
. .
( . , . )
1   if 
0   otherwise
i j
i j
t a t a
f t a t a =
=???  
Table 2. Attributes of the nodes 
Attributes Examples 
POS tag CD, NNP, NN? Type 
syntactic tag NP, VP, ? 
Is Digit? DIG, DIGALL 
Is Capitalized? CAP, CAPALL 
Orthographic  
length of phrase LNG1, LNG2#3, LNGgt3 
Role1 Is answer candidate? true, false 
Role2 Is question key words? true, false 
 
Fig. 2. An example of the pattern tree T_ac#target 
5   ME-Based Answer Extraction 
In addition to the syntactic relation patterns, many other evidences, such as named 
entity tags, may help to detect the proper answers.  Therefore, we use maximum en-
tropy to integrate the syntactic relation patterns and the common features. 
5.1   Maximum Entropy Model 
[1] gave a good description of the core idea of maximum entropy model.  In our task, 
we use the maximum entropy model to rank the answer candidates for a question, 
T_ac#target 
Q1897: What is the name of the airport in Dallas Ft. Worth? 
S: Wednesday morning, the low temperature at the Dallas-Fort Worth Inter-
national Airport was 81 degrees. 
t4 t3 t2 
T: BNP 
O: null  
R1: true 
R2: false 
t1 
Dallas-Fort 
T: NNP 
O: CAPALL  
R1: false 
R2: false 
International 
T: JJ 
O: CAPALL  
R1: false 
R2: false 
Airport 
T: NNP 
O: CAPALL  
R1: false 
R2: true 
t0 
Worth 
T: NNP 
O: CAPALL 
R1: false 
R2: false 
 
514 D. Shen, G.-J.M. Kruijff, and D. Klakow 
which is similar to [12].  Given a question q and a set of possible answer candi-
dates 1 2{ , ... }nac ac ac , the model outputs the answer 1 2{ , ... }nac ac ac ac? with the 
maximal probability from the answer candidate set.  We define M feature func-
tions 1 2( ,{ , ... }, ),  m=1,...,Mm nf ac ac ac ac q .  The probability is modeled as  
1 2
1
1 2
1 2
' 1
exp[ ( ,{ , ... }, ))]
( | { , ... }, )
exp[ ( ',{ , ... }, )]
M
m m n
m
n M
m m n
ac m
f ac ac ac ac q
P ac ac ac ac q
f ac ac ac ac q
?
?
=
=
?
=
? ?
 
where, (m=1,...,M)
m
? are the model parameters, which are trained with General-
ized Iterative Scaling [6].  A Gaussian Prior is used to smooth the ME model. 
Table 3. Examples of the common features 
Features Examples Explanation 
NE#DAT_QT_DAT ac is NE (DATE) and qtarget is DATE NE  
NE#PER_QW_WHO ac is NE (PERSON) and qword is WHO 
SSEQ_Q ac is a subsequence of question 
CAP_QT_LOC ac is capitalized and qtarget is LOCATION 
Ortho-
graphic  
LNGlt3_QT_PER the length of ac ? 3 and qtarget is PERSON 
CD_QT_NUM syn. tag of ac is CD and qtarget is NUM Syntactic 
Tag  NNP_QT_PER syn. tag of ac is NNP and qtarget is PERSON 
Triggers TRG_HOW_DIST ac matches the trigger words for HOW questions which 
ask for distance  
5.2   Features 
For the baseline maximum entropy model, we use four types of common features: 
1. Named Entity Features: For certain question target, if the answer candidate is 
tagged as certain type of named entity, one feature fires. 
2. Orthographic Features: They capture the surface format of the answer candi-
dates, such as capitalizations, digits and lengths, etc.  
3. Syntactic Tag Features: For certain question target, if the word in the answer 
candidate belongs to a certain syntactic / POS type, one feature fires. 
4. Triggers: For some how questions, there are always some trigger words which are 
indicative for the answers.  For example, for ?Q2156: How fast does Randy John-
son throw??, the word ?mph? may help to identify the answer ?98-mph? in ?John-
son throws a 98-mph fastball.? 
Table 3 shows some examples of the common features.  All of the features are the 
binary features.  In addition, many other features, such as the answer candidate fre-
quency, can be extracted based on the IR output and are thought as the indicative 
evidences for the answer extraction [10].  However, in this paper, we are to evaluate 
the answer extraction module independently, so we do not incorporate such features 
in the current model. 
 Exploring Syntactic Relation Patterns for Question Answering 515 
In order to evaluate the effectiveness of the automatically generated syntactic rela-
tion patterns, we also manually build some heuristic rules to extract the relation fea-
tures from the trees and incorporate them into the baseline model.  The baseline 
model uses 20 rules.  Some examples of the hand-extracted relation features are 
listed as follows, 
z If the ac node is the same of the qtarget node, one feature fires. 
z If the ac node is the sibling of the qtarget node, one feature fires. 
z If the ac node is the child of the qsubject node, one feature fires. 
z ? 
Next, we will discuss the use of the syntactic relation features.  Firstly, for each 
answer candidate, we extract the syntactic relations between it and all mapped ques-
tion key words in the sentence tree.  Then for each extracted relation, we match it in 
the pattern set PSet_target, PSet_head, PSet_subject or PSet_verb.  A tree kernel 
discussed in Section 4 is used to calculate the similarity between two patterns.  Fi-
nally, if the maximal similarity is above a threshold ? , the pattern with the maximal 
similarity is chosen and the corresponding feature fires.  The experiments will evalu-
ate the performance and the coverage of the pattern sets based on different ?  values. 
6   Experiment 
We apply the AE module to the TREC QA task.  Since this paper focuses on the AE 
module alone, we only present those sentences containing the proper answers to the 
AE module based on the assumption that the IR module has got 100% precision.  The 
AE module is to identify the proper answers from the given sentence collection. 
We use the questions of TREC8, 9, 2001 and 2002 for training and the questions of 
TREC2003 for testing.  The following steps are used to generate the data: 
1. Retrieve the relevant documents for each question based on the TREC judgments. 
2. Extract the sentences, which match both the proper answer and at least one ques-
tion key word, from these documents.   
3. Tag the proper answer in the sentences based on the TREC answer patterns. 
In TREC 2003, there are 413 factoid questions in which 51 questions (NIL ques-
tions) are not returned with the proper answers by TREC.  According to our data 
generation process, we cannot provide data for those NIL questions because we can-
not get the sentence collections.  Therefore, the AE module will fail on all of the NIL 
questions and the number of the valid questions should be 362 (413 ? 51).  In the 
experiment, we still test the module on the whole question set (413 questions) to keep 
consistent with the other?s work.  The training set contains 1252 questions.  The per-
formance of our system is evaluated using the mean reciprocal rank (MRR).  Fur-
thermore, we also list the percentages of the correct answers respectively in terms of 
the top 5 answers and the top 1 answer returned.  No post-processes are used to adjust 
the answers in the experiments. 
In order to evaluate the effectiveness of the syntactic relation patterns in the answer 
extraction, we compare the modules based on different feature sets.  The first ME 
module ME1 uses the common features including NE features, Orthographic features, 
516 D. Shen, G.-J.M. Kruijff, and D. Klakow 
Syntactic Tag features and Triggers.  The second ME module ME2 uses the common 
features and some hand-extracted relation features, described in Section 5.2.  The 
third module ME3 uses the common features and the syntactic relation patterns which 
are automatically extracted and partial matched with the methods proposed in Section 
3 and 4.  Table 4 shows the overall performance of the modules.  Both ME2 and ME3 
outperform ME1 by 3.15 MRR and 6.91 MRR respectively.  This may indicate that 
the syntactic relations between the question words and the answers are useful for the 
answer extraction.  Furthermore, ME3 got the higher performance (+3.76 MRR) than 
ME2.  The probable reason may be that the relations extracted by some heuristic rules 
in ME2 are limited in the very local contexts of the nodes and they may not be suffi-
cient.  On the contrary, the pattern extraction methods we proposed can explore the 
larger relation space in the dependency trees. 
Table 4. Overall performance 
 ME1 ME2 ME3 
Top1  44.06 47.70 51.81 
Top5 53.27 55.45 58.85 
MRR 47.75 50.90 54.66 
Table 5. Performances for two pattern matching methods 
PartialMatch  ExactMatch 
( ? =1) ? =0.8 ? =0.6 ? =0.4 ? =0.2 ? =0 
Top1 50.12 51.33 51.81 51.57 50.12 50.12 
Top5 57.87 58.37 58.85 58.60 57.16 57.16 
MRR 53.18 54.18 54.66 54.41 52.97 52.97 
Furthermore, we evaluate the effectiveness of the pattern matching method in Sec-
tion 4.  We compare two pattern matching methods: the exact matching (ExactMatch) 
and the partial matching (PartialMatch) using the tree kernel.  Table 5 shows the 
performances for the two pattern matching methods.  For PartialMatch, we also 
evaluate the effect of the parameter ?  (described in Section 5.2) on the performance.  
In Table 5, the best PartialMatch ( ?  = 0.6) outperforms ExactMatch by 1.48 MRR.  
Since the pattern sets extracted from the training data is not large enough to cover the 
unseen cases, ExactMatch may have too low coverage and suffer with the data sparse-
ness problem when testing, especially for PSet_subject (24.32% coverage using Ex-
actMatch vs. 49.94% coverage using PartialMatch).  In addition, even the model with 
ExactMatch is better than ME2 (common features + hand-extracted relations) by 2.28 
MRR.  It indicates that the relation patterns explored with the method proposed in 
Section 3 are more effective than the relations extracted by the heuristic rules. 
Table 6 shows the size of the pattern sets PSet_target, PSet_head, PSet_subject 
and PSet_verb and their coverage for the test data based on different ?  values.  
PSet_verb gets the low coverage (<5% coverage).  The probable reason is that the 
verbs in the answer sentences are often different from those in the questions, therefore 
only a few question verbs can be matched in the answer sentences.  PSet_head also 
gets the relatively low coverage since the head words are only exacted from how 
questions and there are only 49/413 how questions with head words in the test data.  
 Exploring Syntactic Relation Patterns for Question Answering 517 
Table 6. Size and coverage of the pattern sets 
coverage (*%)  size 
? =1 ? =0.8 ? =0.6 ? =0.4 ? =0.2 ? =0 
PSet_target 45 49.85 53.73 57.01 58.14 58.46 58.46 
PSet_head 42 5.82 6.48 6.69 6.80 6.80 6.80 
PSet_subject 123 24.32 44.82 49.94 51.29 51.84 51.84 
PSet_verb 125 2.21 3.49 3.58 3.58 3.58 3.58 
We further evaluate the contributions of different types of patterns.  We respec-
tively combine the pattern features in different pattern set and the common features.  
Some findings can be concluded from Table 7: All of the patterns have the positive 
effects based on the common features, which indicates that all of the four types of the 
relations are helpful for answer extraction.  Furthermore, P_target (+4.21 MRR) and 
P_subject (+2.47 MRR) are more beneficial than P_head (+1.25 MRR) and P_verb 
(+0.19 MRR).  This may be explained that the target and subject patterns may have 
the effect on the more test data than the head and verb patterns since PSet_target and 
PSet_subject have the higher coverage for the test data than PSet_head and 
PSet_verb, as shown in Table 6.  
Table 7. Performance on feature combination 
Combination of features MRR 
common features 47.75 
common features + P_target 51.96 
common features + P_head 49.00 
common features + P_subject 50.22 
common features + P_verb 47.94 
7   Conclusion 
In this paper, we study the syntactic relation patterns for question answering.  We 
extract the various syntactic relations between the answers and different types of 
question words, including target words, head words, subject words and verbs and 
score the extracted relations based on support and confidence measures.  We further 
propose a QA-specific tree kernel to partially match the relation patterns from the 
unseen data to the pattern sets.  Lastly, we incorporate the patterns and some com-
mon features into a Maximum Entropy Model to rank the answer candidates.  The 
experiment shows that the syntactic relation patterns improve the performance by 
6.91 MRR based on the common features. Moreover, the contributions of the pat-
tern matching methods are evaluated.  The results show that the tree kernel-based 
partial matching outperforms the exact matching by 1.48 MRR.  In the future, we 
are to further explore the syntactic relations using the web data rather than the  
training data. 
518 D. Shen, G.-J.M. Kruijff, and D. Klakow 
References 
1. Berger, A., Della Pietra, S., Della Pietra, V.: A maximum entropy approach to natural lan-
guage processing. Computational Linguistics (1996), vol. 22, no. 1, pp. 39-71 
2. Collins, M.: A New Statistical Parser Based on Bigram Lexical Dependencies. In: Pro-
ceedings of ACL-96 (1996) 184-191 
3. Collins, M.: New Ranking Algorithms for Parsing and Tagging: Kernel over Discrete 
Structures, and the Voted Perceptron. In: Proceeings of ACL-2002 (2002). 
4. Collins, M., Duffy, N.: Convolution Kernels for Natural Language. Advances in Neural 
Information Processing Systems 14, Cambridge, MA.  MIT Press (2002) 
5. Culotta, A., Sorensen, J.: Dependency Tree Kernels for Relation Extraction. In: Proceed-
ings of ACL-2004 (2004) 
6. Darroch, J., Ratcliff, D.: Generalized iterative scaling for log-linear models. The annuals 
of Mathematical Statistics (1972), vol. 43, pp. 1470-1480 
7. Echihabi, A., Hermjakob, U., Hovy, E., Marcu, D., Melz, E., Ravichandran, D.: Multiple-
Engine Question Answering in TextMap. In: Proceedings of the TREC-2003 Conference, 
NIST (2003) 
8. Fellbaum, C.: WordNet - An Electronic Lexical Database. MIT Press, Cambridge, MA 
(1998) 
9. Harabagiu, S., Moldovan, D., Clark, C., Bowden, M., Williams, J., Bensley, J.: Answer 
Mining by Combining Extraction Techniques with Abductive Reasoning. In: Proceedings 
of the TREC-2003 Conference, NIST (2003) 
10. Ittycheriah, A., Roukos, S.: IBM's Statistical Question Answering System - TREC 11. In: 
Proceedings of the TREC-2002 Conference, NIST (2002) 
11. Levenshtein, V. I.: Binary Codes Capable of Correcting Deletions, Insertions and Rever-
sals. Doklady Akademii Nauk SSSR 163(4) (1965) 845-848 
12. Ravichandran, D., Hovy, E., Och, F. J.: Statistical QA - Classifier vs. Re-ranker: What's 
the difference? In: Proceedings of Workshop on Multilingual Summarization and Question 
Answering, ACL (2003) 
13. Ravichandran, D., Hovy, E.: Learning Surface Text Patterns for a Question Answering 
System. In: Proceedings of ACL-2002 (2002) 41-47 
14. Soubbotin, M. M., Soubbotin, S. M.: Patterns of Potential Answer Expressions as Clues to 
the Right Answer. In: Proceedings of the TREC-10 Conference, NIST (2001) 
15. Xu, J., Licuanan, A., May, J., Miller, S., Weischedel, R.: TREC 2002 QA at BBN: Answer 
Selection and Confidence Estimation. In: Proceedings of the TREC-2002 Conference, 
NIST (2002) 
16. Vapnik, V.: Statistical Learning Theory, John Wiley, NY, (1998) 732. 
17. Zelenko, D., Aone, C., Richardella, A.: Kernel Methods for Relation Extraction. Journal of 
Machine Learning Research (2003) 1083-1106. 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 889?896,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Exploring Correlation of Dependency Relation Paths
for Answer Extraction
Dan Shen
Department of Computational Linguistics
Saarland University
Saarbruecken, Germany
dshen@coli.uni-sb.de
Dietrich Klakow
Spoken Language Systems
Saarland University
Saarbruecken, Germany
klakow@lsv.uni-saarland.de
Abstract
In this paper, we explore correlation of
dependency relation paths to rank candi-
date answers in answer extraction. Using
the correlation measure, we compare de-
pendency relations of a candidate answer
and mapped question phrases in sentence
with the corresponding relations in ques-
tion. Different from previous studies, we
propose an approximate phrase mapping
algorithm and incorporate the mapping
score into the correlation measure. The
correlations are further incorporated into
a Maximum Entropy-based ranking model
which estimates path weights from train-
ing. Experimental results show that our
method significantly outperforms state-of-
the-art syntactic relation-based methods
by up to 20% in MRR.
1 Introduction
Answer Extraction is one of basic modules in open
domain Question Answering (QA). It is to further
process relevant sentences extracted with Passage /
Sentence Retrieval and pinpoint exact answers us-
ing more linguistic-motivated analysis. Since QA
turns to find exact answers rather than text snippets
in recent years, answer extraction becomes more
and more crucial.
Typically, answer extraction works in the fol-
lowing steps:
? Recognize expected answer type of a ques-
tion.
? Annotate relevant sentences with various
types of named entities.
? Regard the phrases annotated with the ex-
pected answer type as candidate answers.
? Rank candidate answers.
In the above work flow, answer extraction heav-
ily relies on named entity recognition (NER). On
one hand, NER reduces the number of candidate
answers and eases answer ranking. On the other
hand, the errors from NER directly degrade an-
swer extraction performance. To our knowledge,
most top ranked QA systems in TREC are sup-
ported by effective NER modules which may iden-
tify and classify more than 20 types of named en-
tities (NE), such as abbreviation, music, movie,
etc. However, developing such named entity rec-
ognizer is not trivial. Up to now, we haven?t found
any paper relevant to QA-specific NER develop-
ment. So, it is hard to follow their work. In this pa-
per, we just use a general MUC-based NER, which
makes our results reproducible.
A general MUC-based NER can?t annotate a
large number of NE classes. In this case, all
noun phrases in sentences are regarded as candi-
date answers, which makes candidate answer sets
much larger than those filtered by a well devel-
oped NER. The larger candidate answer sets result
in the more difficult answer extraction. Previous
methods working on surface word level, such as
density-based ranking and pattern matching, may
not perform well. Deeper linguistic analysis has
to be conducted. This paper proposes a statisti-
cal method which exploring correlation of depen-
dency relation paths to rank candidate answers. It
is motivated by the observation that relations be-
tween proper answers and question phrases in can-
didate sentences are always similar to the corre-
sponding relations in question. For example, the
question ?What did Alfred Nobel invent?? and the
889
candidate sentence ?... in the will of Swedish in-
dustrialist Alfred Nobel, who invented dynamite.?
For each question, firstly, dependency relation
paths are defined and extracted from the question
and each of its candidate sentences. Secondly,
the paths from the question and the candidate sen-
tence are paired according to question phrase map-
ping score. Thirdly, correlation between two paths
of each pair is calculated by employing Dynamic
Time Warping algorithm. The input of the cal-
culation is correlations between dependency re-
lations, which are estimated from a set of train-
ing path pairs. Lastly, a Maximum Entropy-based
ranking model is proposed to incorporate the path
correlations and rank candidate answers. Further-
more, sentence supportive measure are presented
according to correlations of relation paths among
question phrases. It is applied to re-rank the can-
didate answers extracted from the different candi-
date sentences. Considering phrases may provide
more accurate information than individual words,
we extract dependency relations on phrase level
instead of word level.
The experiment on TREC questions shows that
our method significantly outperforms a density-
based method by 50% in MRR and three state-
of-the-art syntactic-based methods by up to 20%
in MRR. Furthermore, we classify questions by
judging whether NER is used. We investigate
how these methods perform on the two question
sets. The results indicate that our method achieves
better performance than the other syntactic-based
methods on both question sets. Especially for
more difficult questions, for which NER may not
help, our method improves MRR by up to 31%.
The paper is organized as follows. Section 2
discusses related work and clarifies what is new in
this paper. Section 3 presents relation path corre-
lation in detail. Section 4 and 5 discuss how to in-
corporate the correlations for answer ranking and
re-ranking. Section 6 reports experiment and re-
sults.
2 Related Work
In recent years? TREC Evaluation, most top
ranked QA systems use syntactic information in
answer extraction. Next, we will briefly discuss
the main usages.
(Kaisser and Becker, 2004) match a question
into one of predefined patterns, such as ?When
did Jack Welch retire from GE?? to the pattern
?When+did+NP+Verb+NPorPP?. For each ques-
tion pattern, there is a set of syntactic structures for
potential answer. Candidate answers are ranked
by matching the syntactic structures. This method
worked well on TREC questions. However, it
is costing to manually construct question patterns
and syntactic structures of the patterns.
(Shen et al, 2005) classify question words into
four classes target word, head word, subject word
and verb. For each class, syntactic relation pat-
terns which contain one question word and one
proper answer are automatically extracted and
scored from training sentences. Then, candidate
answers are ranked by partial matching to the syn-
tactic relation patterns using tree kernel. However,
the criterion to classify the question words is not
clear in their paper. Proper answers may have ab-
solutely different relations with different subject
words in sentences. They don?t consider the cor-
responding relations in questions.
(Tanev et al, 2004; Wu et al, 2005) compare
syntactic relations in questions and those in an-
swer sentences. (Tanev et al, 2004) reconstruct
a basic syntactic template tree for a question, in
which one of the nodes denotes expected answer
position. Then, answer candidates for this ques-
tion are ranked by matching sentence syntactic
tree to the question template tree. Furthermore, the
matching is weighted by lexical variations. (Wu et
al., 2005) combine n-gram proximity search and
syntactic relation matching. For syntactic rela-
tion matching, question tree and sentence subtree
around a candidate answer are matched from node
to node.
Although the above systems apply the different
methods to compare relations in question and an-
swer sentences, they follow the same hypothesis
that proper answers are more likely to have same
relations in question and answer sentences. For
example, in question ?Who founded the Black Pan-
thers organization??, where, the question word
?who? has the dependency relations ?subj? with
?found? and ?subj obj nn? with ?Black Panthers
organization?, in sentence ?Hilliard introduced
Bobby Seale, who co-founded the Black Panther
Party here ...?, the proper answer ?Bobby Seale?
has the same relations with most question phrases.
These methods achieve high precision, but poor
recall due to relation variations. One meaning
is often represented as different relation combi-
nations. In the above example, appositive rela-
890
tion frequently appears in answer sentences, such
as ?Black Panther Party co-founder Bobby Seale
is ordered bound and gagged ...? and indicates
proper answer Bobby Seale although it is asked in
different way in the question.
(Cui et al, 2004) propose an approximate de-
pendency relation matching method for both pas-
sage retrieval and answer extraction. The simi-
larity between two relations is measured by their
co-occurrence rather than exact matching. They
state that their method effectively overcomes the
limitation of the previous exact matching meth-
ods. Lastly, they use the sum of similarities of
all path pairs to rank candidate answers, which is
based on the assumption that all paths have equal
weights. However, it might not be true. For ex-
ample, in question ?What book did Rachel Carson
write in 1962??, the phrase ?Rachel Carson? looks
like more important than ?1962? since the former
is question topic and the latter is a constraint for
expected answer. In addition, lexical variations
are not well considered and a weak relation path
alignment algorithm is used in their work.
Based on the previous works, this paper ex-
plores correlation of dependency relation paths be-
tween questions and candidate sentences. Dy-
namic time warping algorithm is adapted to cal-
culate path correlations and approximate phrase
mapping is proposed to cope with phrase varia-
tions. Finally, maximum entropy-based ranking
model is developed to incorporate the correlations
and rank candidate answers.
3 Dependency Relation Path Correlation
In this section, we discuss how the method per-
forms in detail.
3.1 Dependency Relation Path Extraction
We parse questions and candidate sentences with
MiniPar (Lin, 1994), a fast and robust parser for
grammatical dependency relations. Then, we ex-
tract relation paths from dependency trees.
Dependency relation path is defined as a struc-
ture P =< N1, R,N2 > where, N1, N2 are
two phrases and R is a relation sequence R =<
r1, ..., ri > in which ri is one of the predefined de-
pendency relations. Totally, there are 42 relations
defined in MiniPar. A relation sequence R be-
tween two phrases N1, N2 is extracted by travers-
ing from the N1 node to the N2 node in a depen-
dency tree.
Q: What book did Rachel Carson write in 1962?
Paths for Answer Ranking
N1 (EAP)           R                               N2
What               det                             book
What               det obj subj                 Rachel Carson
What               det obj                        write
What               det obj mod pcomp-n    1962
Paths for Answer Re-rankingbook                obj subj                       Rachel Carsonbook                obj                              writebook                obj mod pcomp-n          1962
...
S: Rachel Carson ?s 1962 book " Silent Spring " said 
    dieldrin causes mania.
Paths for Answer Ranking
N1 (CA)              R                              N2Silent Spring      title                           bookSilent Spring      title gen                     Rachel CarsonSilent Spring      title num                    1962
Paths for Answer Re-rankingbook                    gen                         Rachel Carsonbook                    num                        1962         
...
Figure 1: Relation Paths for sample question and
sentence. EAP indicates expected answer position;
CA indicates candidate answer
For each question, we extract relation paths
among noun phrases, main verb and question
word. The question word is further replaced with
?EAP?, which indicates the expected answer po-
sition. For each candidate sentence, we firstly
extract relation paths between answer candidates
and mapped question phrases. These paths will
be used for answer ranking (Section 4). Secondly,
we extract relation paths among mapped question
phrases. These paths will be used for answer re-
ranking (Section 5). Question phrase mapping will
be discussed in Section 3.4. Figure 1 shows some
relation paths extracted for an example question
and candidate sentence.
Next, the relation paths in a question and each
of its candidate sentences are paired according to
their phrase similarity. For any two relation path
Pi and Pj which are extracted from the ques-
tion and the candidate sentence respectively, if
Sim(Ni1, Nj1) > 0 and Sim(Ni2, Nj2) > 0,
Pi and Pj are paired as < Pi, Pj >. The ques-
tion phrase ?EAP? is mapped to candidate answer
phrase in the sentence. The similarity between two
891
Path Pairs for Answer RankingN1 (EAP / CA)     Rq                               Rs               N2Silent Spring      det                              title             bookSilent Spring      det obj subj                  title gen       Rachel CarsonSilent Spring      det obj mod pcomp-n     title num      1962
Path Pairs for Answer Re-rankingN1                     Rq                               Rs               N2book                  obj subj                        gen             Rachel Carsonbook                  obj mod pcomp-n           num            1962
...
Figure 2: Paired Relation Path
phrases will be discussed in Section 3.4. Figure 2
further shows the paired relation paths which are
presented in Figure 1.
3.2 Dependency Relation Path Correlation
Comparing a proper answer and other wrong can-
didate answers in each sentence, we assume that
relation paths between the proper answer and
question phrases in the sentence are more corre-
lated to the corresponding paths in question. So,
for each path pair < P1, P2 >, we measure the
correlation between its two paths P1 and P2.
We derive the correlations between paths by
adapting dynamic time warping (DTW) algorithm
(Rabiner et al, 1978). DTW is to find an optimal
alignment between two sequences which maxi-
mizes the accumulated correlation between two
sequences. A sketch of the adapted algorithm is
as follows.
Let R1 =< r11, ..., r1n >, (n = 1, ..., N)
and R2 =< r21, ..., r2m >, (m = 1, ...,M) de-
note two relation sequences. R1 and R2 consist
of N and M relations respectively. R1(n) =
r1n and R2(m) = r2m. Cor(r1, r2) denotes
the correlation between two individual relations
r1, r2, which is estimated by a statistical model
during training (Section 3.3). Given the corre-
lations Cor(r1n, r2m) for each pair of relations
(r1n, r2m) within R1 and R2, the goal of DTW is
to find a path, m = map(n), which map n onto the
corresponding m such that the accumulated corre-
lation Cor? along the path is maximized.
Cor? = max
map(n)
{ N?
n=1
Cor(R1(n), R2(map(n))
}
A dynamic programming method is used to de-
termine the optimum path map(n). The accumu-
lated correlation CorA to any grid point (n,m)
can be recursively calculated as
CorA(n,m) = Cor(r1n, r2m) + maxq?m CorA(n? 1, q)
Cor? = CorA(N,M)
The overall correlation measure has to be nor-
malized as longer sequences normally give higher
correlation value. So, the correlation between two
sequences R1 and R2 is calculated as
Cor(R1, R2) = Cor?/max(N,M)
Finally, we define the correlation between two
relation paths P1 and P2 as
Cor(P1, P2) = Cor(R1, R2)? Sim(N11, N21)
? Sim(N12, N22)
Where, Sim(N11, N21) and Sim(N12, N22)
are the phrase mapping score when pairing
two paths, which will be described in Section
3.4. If two phrases are absolutely different
Cor(N11, N21) = 0 or Cor(N12, N22) = 0, the
paths may not be paired since Cor(P1, P2) = 0.
3.3 Relation Correlation Estimation
In the above section, we have described how to
measure path correlations. The measure requires
relation correlations Cor(r1, r2) as inputs. We
apply a statistical method to estimate the relation
correlations from a set of training path pairs. The
training data collecting will be described in Sec-
tion 6.1.
For each question and its answer sentences in
training data, we extract relation paths between
?EAP? and other phrases in the question and
paths between proper answer and mapped ques-
tion phrases in the sentences. After pairing the
question paths and the corresponding sentence
paths, correlation of two relations is measured by
their bipartite co-occurrence in all training path
pairs. Mutual information-based measure (Cui et
al., 2004) is employed to calculate the relation cor-
relations.
Cor(rQi , rSj ) = log
??? ?(rQi , rSj )
fQ(rQi )? fS(rSj )
where, rQi and rSj are two relations in question
paths and sentence paths respectively. fQ(rQi ) and
fS(rSj ) are the numbers of occurrences of rQi in
question paths and rSj in sentence paths respec-
tively. ?(rQi , rSj ) is 1 when rQi and rSj co-occur
in a path pair, and 0 otherwise. ? is a factor to
discount the co-occurrence value for long paths. It
is set to the inverse proportion of the sum of path
lengths of the path pair.
892
3.4 Approximate Question Phrase Mapping
Basic noun phrases (BNP) and verbs in questions
are mapped to their candidate sentences. A BNP
is defined as the smallest noun phrase in which
there are no noun phrases embedded. To address
lexical and format variations between phrases, we
propose an approximate phrase mapping strategy.
A BNP is separated into a set of heads
H = {h1, ..., hi} and a set of modifiers M =
{m1, ...mj}. Some heuristic rules are applied to
judge heads and modifiers: 1. If BNP is a named
entity, all words are heads. 2. The last word of
BNP is head. 3. Rest words are modifiers.
The similarity between two BNPs
Sim(BNPq, BNPs) is defined as:
Sim(BNPq, BNPs) = ?Sim(Hq, Hs)
+ (1? ?)Sim(Mq,Ms)
Sim(Hq, Hs) =
?
hi?Hq
?
hj?Hs
Sim(hi,hj)
|Hq
?
Hs|
Sim(Mq,Ms) =
?
mi?Mq
?
mj?Ms
Sim(mi,mj)
|Mq
?
Ms|
Furthermore, the similarity between two heads
Sim(hi, hj) are defined as:
? Sim = 1, if hi = hj after stemming;
? Sim = 1, if hi = hj after format alternation;
? Sim = SemSim(hi, hj)
These items consider morphological, format
and semantic variations respectively. 1. The mor-
phological variations match words after stemming,
such as ?Rhodes scholars? and ?Rhodes scholar-
ships?. 2. The format alternations cope with
special characters, such as ?-? for ?Ice-T? and
?Ice T?, ?&? for ?Abercrombie and Fitch? and
?Abercrombie & Fitch?. 3. The semantic simi-
larity SemSim(hi, hj) is measured using Word-
Net and eXtended WordNet. We use the same
semantic path finding algorithm, relation weights
and semantic similarity measure as (Moldovan and
Novischi, 2002). For efficiency, only hypernym,
hyponym and entailment relations are considered
and search depth is set to 2 in our experiments.
Particularly, the semantic variations are not con-
sidered for NE heads and modifiers. Modifier sim-
ilarity Sim(mi,mj) only consider the morpho-
logical and format variations. Moreover, verb sim-
ilarity measure Sim(v1, v2) is the same as head
similarity measure Sim(hi, hj).
4 Candidate Answer Ranking
According to path correlations of candidate an-
swers, a Maximum Entropy (ME)-based model is
applied to rank candidate answers. Unlike (Cui et
al., 2004), who rank candidate answers with the
sum of the path correlations, ME model may es-
timate the optimal weights of the paths based on
a training data set. (Berger et al, 1996) gave a
good description of ME model. The model we
use is similar to (Shen et al, 2005; Ravichandran
et al, 2003), which regard answer extraction as a
ranking problem instead of a classification prob-
lem. We apply Generalized Iterative Scaling for
model parameter estimation and Gaussian Prior
for smoothing.
If expected answer type is unknown during
question processing or corresponding type of
named entities isn?t recognized in candidate sen-
tences, we regard all basic noun phrases as can-
didate answers. Since a MUC-based NER loses
many types of named entities, we have to handle
larger candidate answer sets. Orthographic fea-
tures, similar to (Shen et al, 2005), are extracted to
capture word format information of candidate an-
swers, such as capitalizations, digits and lengths,
etc. We expect they may help to judge what proper
answers look like since most NER systems work
on these features.
Next, we will discuss how to incorporate path
correlations. Two facts are considered to affect
path weights: question phrase type and path
length. For each question, we divide question
phrases into four types: target, topic, constraint
and verb. Target is a kind of word which indicates
the expected answer type of the question, such as
?party? in ?What party led Australia from 1983 to
1996??. Topic is the event/person that the ques-
tion talks about, such as ?Australia?. Intuitively, it
is the most important phrase of the question. Con-
straint are the other phrases of the question ex-
cept topic, such as ?1983? and ?1996?. Verb is
the main verb of the question, such as ?lead?. Fur-
thermore, since shorter path indicates closer rela-
tion between two phrases, we discount path corre-
lation in long question path by dividing the corre-
lation by the length of the question path. Lastly,
we sum the discounted path correlations for each
type of question phrases and fire it as a feature,
such as ?Target Cor=c, where c is the correla-
tion value for question target. ME-based rank-
ing model incorporate the orthographic and path
893
correlation features to rank candidate answers for
each of candidate sentences.
5 Candidate Answer Re-ranking
After ranking candidate answers, we select the
highest ranked one from each candidate sentence.
In this section, we are to re-rank them according
to sentence supportive degree. We assume that a
candidate sentence supports an answer if relations
between mapped question phrases in the candidate
sentence are similar to the corresponding ones in
question. Relation paths between any two ques-
tion phrases are extracted and paired. Then, corre-
lation of each pair is calculated. Re-rank formula
is defined as follows:
Score(answer) = ??
?
i
Cor(Pi1, Pi2)
where, ? is answer ranking score. It is the nor-
malized prediction value of the ME-based ranking
model described in Section 4. ?
i
Cor(Pi1, Pi2) is
the sum of correlations of all path pairs. Finally,
the answer with the highest score is returned.
6 Experiments
In this section, we set up experiments on TREC
factoid questions and report evaluation results.
6.1 Experiment Setup
The goal of answer extraction is to identify ex-
act answers from given candidate sentence col-
lections for questions. The candidate sentences
are regarded as the most relevant sentences to the
questions and retrieved by IR techniques. Quali-
ties of the candidate sentences have a strong im-
pact on answer extraction. It is meaningless to
evaluate the questions of which none candidate
sentences contain proper answer in answer extrac-
tion experiment. To our knowledge, most of cur-
rent QA systems lose about half of questions in
sentence retrieval stage. To make more questions
evaluated in our experiments, for each of ques-
tions, we automatically build a candidate sentence
set from TREC judgements rather than use sen-
tence retrieval output.
We use TREC99-03 questions for training and
TREC04 questions for testing. As to build training
data, we retrieve all of the sentences which con-
tain proper answers from relevant documents ac-
cording to TREC judgements and answer patterns.
Then, We manually check the sentences and re-
move those in which answers cannot be supported.
As to build candidate sentence sets for testing, we
retrieve all of the sentences from relevant docu-
ments in judgements and keep those which contain
at least one question key word. Therefore, each
question has at least one proper candidate sentence
which contains proper answer in its candidate sen-
tence set.
There are 230 factoid questions (27 NIL ques-
tions) in TREC04. NIL questions are excluded
from our test set because TREC doesn?t supply
relevant documents and answer patterns for them.
Therefore, we will evaluate 203 TREC04 ques-
tions. Five answer extraction methods are evalu-
ated for comparison:
? Density: Density-based method is used as
baseline, in which we choose candidate an-
swer with the shortest surface distance to
question phrases.
? SynPattern: Syntactic relation patterns
(Shen et al, 2005) are automatically ex-
tracted from training set and are partially
matched using tree kernel.
? StrictMatch: Strict relation matching fol-
lows the assumption in (Tanev et al, 2004;
Wu et al, 2005). We implement it by adapt-
ing relation correlation score. In stead of
learning relation correlations during training,
we predefine them as: Cor(r1, r2) = 1 if
r1 = r2; 0, otherwise.
? ApprMatch: Approximate relation match-
ing (Cui et al, 2004) aligns two relation paths
using fuzzy matching and ranks candidates
according to the sum of all path similarities.
? CorME: It is the method proposed in this pa-
per. Different from ApprMatch, ME-based
ranking model is implemented to incorpo-
rate path correlations which assigns different
weights for different paths respectively. Fur-
thermore, phrase mapping score is incorpo-
rated into the path correlation measure.
These methods are briefly described in Section
2. Performance is evaluated with Mean Reciprocal
Rank (MRR). Furthermore, we list percentages of
questions correctly answered in terms of top 5 an-
swers and top 1 answer returned respectively. No
answer validations are used to adjust answers.
894
Table 1: Overall performance
Density SynPattern StrictMatch ApprMatch CorME
MRR 0.45 0.56 0.57 0.60 0.67
Top1 0.36 0.53 0.49 0.53 0.62
Top5 0.56 0.60 0.67 0.70 0.74
6.2 Results
Table 1 shows the overall performance of the five
methods. The main observations from the table
are as follows:
1. The methods SynPattern, StrictMatch, Ap-
prMatch and CorME significantly improve
MRR by 25.0%, 26.8%, 34.5% and 50.1%
over the baseline method Density. The im-
provements may benefit from the various ex-
plorations of syntactic relations.
2. The performance of SynPattern (0.56MRR)
and StrictMatch (0.57MRR) are close. Syn-
Pattern matches relation sequences of can-
didate answers with the predefined relation
sequences extracted from a training data
set, while StrictMatch matches relation se-
quences of candidate answers with the cor-
responding relation sequences in questions.
But, both of them are based on the assump-
tion that the more number of same rela-
tions between two sequences, the more sim-
ilar the sequences are. Furthermore, since
most TREC04 questions only have one or two
phrases and many questions have similar ex-
pressions, SynPattern and StrictMatch don?t
make essential difference.
3. ApprMatch and CorME outperform SynPat-
tern and StrictMatch by about 6.1% and
18.4% improvement in MRR. Strict matching
often fails due to various relation representa-
tions in syntactic trees. However, such vari-
ations of syntactic relations may be captured
by ApprMatch and CorME using a MI-based
statistical method.
4. CorME achieves the better performance by
11.6% than ApprMatch. The improvement
may benefit from two aspects: 1) ApprMatch
assigns equal weights to the paths of a can-
didate answer and question phrases, while
CorME estimate the weights according to
phrase type and path length. After training a
ME model, the weights are assigned, such as
5.72 for topic path ; 3.44 for constraints path
and 1.76 for target path. 2) CorME incorpo-
rates approximate phrase mapping scores into
path correlation measure.
We further divide the questions into two classes
according to whether NER is used in answer ex-
traction. If the expected answer type of a ques-
tion is unknown, such as ?How did James Dean
die?? or the type cannot be annotated by NER,
such as ?What ethnic group/race are Crip mem-
bers??, we put the question in Qw/oNE set, oth-
erwise, we put it in QwNE. For the questions in
Qw/oNE, we extract all basic noun phrases and
verb phrases as candidate answers. Then, answer
extraction module has to work on the larger can-
didate sets. Using a MUC-based NER, the rec-
ognized types include person, location, organiza-
tion, date, time and money. In TREC04 questions,
123 questions are put in QwNE and 80 questions
in Qw/oNE.
Table 2: Performance on two question sets QwNE
and Qw/oNE
QwNE Qw/oNE
Density 0.66 0.11
SynPattern 0.71 0.36
StrictMatch 0.70 0.36
ApprMatch 0.72 0.42
CorME 0.79 0.47
We evaluate the performance on QwNE and
Qw/oNE respectively, as shown in Table 2.
The density-based method Density (0.11MRR)
loses many questions in Qw/oNE, which indi-
cates that using only surface word information
is not sufficient for large candidate answer sets.
On the contrary, SynPattern(0.36MRR), Strict-
Pattern(0.36MRR), ApprMatch(0.42MRR) and
CorME (0.47MRR) which capture syntactic infor-
mation, perform much better than Density. Our
method CorME outperforms the other syntactic-
based methods on both QwNE and Qw/oNE. Es-
895
pecially for more difficult questions Qw/oNE, the
improvements (up to 31% in MRR) are more ob-
vious. It indicates that our method can be used to
further enhance state-of-the-art QA systems even
if they have a good NER.
In addition, we evaluate component contribu-
tions of our method based on the main idea of
relation path correlation. Three components are
tested: 1. Appr. Mapping (Section 3.4). We re-
place approximate question phrase mapping with
exact phrase mapping and withdraw the phrase
mapping scores from path correlation measure. 2.
Answer Ranking (Section 4). Instead of using
ME model, we sum all of the path correlations to
rank candidate answers, which is similar to (Cui
et al, 2004). 3. Answer Re-ranking (Section
5). We disable this component and select top 5
answers according to answer ranking scores.
Table 3: Component Contributions
MRR
Overall 0.67
- Appr. Mapping 0.63
- Answer Ranking 0.62
- Answer Re-ranking 0.66
The contribution of each component is evalu-
ated with the overall performance degradation af-
ter it is removed or replaced. Some findings are
concluded from Table 3. Performances degrade
when replacing approximate phrase mapping or
ME-based answer ranking, which indicates that
both of them have positive effects on the systems.
This may be also used to explain why CorME out-
performs ApprMatch in Table 1. However, remov-
ing answer re-ranking doesn?t affect much. Since
short questions, such as ?What does AARP stand
for??, frequently occur in TREC04, exploring the
phrase relations for such questions isn?t helpful.
7 Conclusion
In this paper, we propose a relation path
correlation-based method to rank candidate an-
swers in answer extraction. We extract and pair
relation paths from questions and candidate sen-
tences. Next, we measure the relation path cor-
relation in each pair based on approximate phrase
mapping score and relation sequence alignment,
which is calculated by DTW algorithm. Lastly,
a ME-based ranking model is proposed to incor-
porate the path correlations and rank candidate
answers. The experiment on TREC questions
shows that our method significantly outperforms
a density-based method by 50% in MRR and three
state-of-the-art syntactic-based methods by up to
20% in MRR. Furthermore, the method is espe-
cially effective for difficult questions, for which
NER may not help. Therefore, it may be used to
further enhance state-of-the-art QA systems even
if they have a good NER. In the future, we are to
further evaluate the method based on the overall
performance of a QA system and adapt it to sen-
tence retrieval task.
References
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguisitics, 22:39?71.
Hang Cui, Keya Li, Renxu Sun, Tat-Seng Chua, and
Min-Yen Kan. 2004. National university of singa-
pore at the trec-13 question answering. In Proceed-
ings of TREC2004, NIST.
M. Kaisser and T. Becker. 2004. Question answering
by searching large corpora with linguistic methods.
In Proceedings of TREC2004, NIST.
Dekang Lin. 1994. Principar?an efficient, broad-
coverage, principle-based parser. In Proceedings of
COLING1994, pages 42?488.
Dan Moldovan and Adrian Novischi. 2002. Lexical
chains for question answering. In Proceedings of
COLING2002.
L. R. Rabiner, A. E. Rosenberg, and S. E. Levinson.
1978. Considerations in dynamic time warping al-
gorithms for discrete word recognition. In Proceed-
ings of IEEE Transactions on acoustics, speech and
signal processing.
Deepak Ravichandran, Eduard Hovy, and Franz Josef
Och. 2003. Statistical qa - classifier vs. re-ranker:
What?s the difference? In Proceedings of ACL2003
workshop on Multilingual Summarization and Ques-
tion Answering.
Dan Shen, Geert-Jan M. Kruijff, and Dietrich Klakow.
2005. Exploring syntactic relation patterns for ques-
tion answering. In Proceedings of IJCNLP2005.
H. Tanev, M. Kouylekov, and B. Magnini. 2004. Com-
bining linguisitic processing and web mining for
question answering: Itc-irst at trec-2004. In Pro-
ceedings of TREC2004, NIST.
M. Wu, M. Y. Duan, S. Shaikh, S. Small, and T. Strza-
lkowski. 2005. University at albany?s ilqua in trec
2005. In Proceedings of TREC2005, NIST.
896
Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 65?72,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Studying Feature Generation from Various Data Representations for 
Answer Extraction 
 
Dan Shen?? Geert-Jan M. Kruijff? Dietrich Klakow? 
? Department of Computational Linguistics 
Saarland University 
Building 17,Postfach 15 11 50 
66041 Saarbruecken, Germany 
? Lehrstuhl Sprach Signal Verarbeitung 
Saarland University 
Building 17, Postfach 15 11 50 
66041 Saarbruecken, Germany 
{dshen,gj}@coli.uni-sb.de 
{dietrich.klakow}@lsv.uni-saarland.de 
 
 
Abstract 
In this paper, we study how to generate 
features from various data representations, 
such as surface texts and parse trees, for 
answer extraction.  Besides the features 
generated from the surface texts, we 
mainly discuss the feature generation in 
the parse trees.  We propose and compare 
three methods, including feature vector, 
string kernel and tree kernel, to represent 
the syntactic features in Support Vector 
Machines.  The experiment on the TREC 
question answering task shows that the 
features generated from the more struc-
tured data representations significantly 
improve the performance based on the 
features generated from the surface texts.  
Furthermore, the contribution of the indi-
vidual feature will be discussed in detail. 
1 Introduction 
Open domain question answering (QA), as defined 
by the TREC competitions (Voorhees, 2003), 
represents an advanced application of natural lan-
guage processing (NLP).  It aims to find exact an-
swers to open-domain natural language questions 
in a large document collection.  For example: 
Q2131: Who is the mayor of San Francisco? 
Answer: Willie Brown 
A typical QA system usually consists of three 
basic modules: 1. Question Processing (QP) Mod-
ule, which finds some useful information from the 
questions, such as expected answer type and key 
words.  2. Information Retrieval (IR) Module, 
which searches a document collection to retrieve a 
set of relevant sentences using the question key 
words.  3. Answer Extraction (AE) Module, which 
analyzes the relevant sentences using the informa-
tion provided by the QP module and identify the 
answer phrase. 
In recent years, QA systems trend to be more 
and more complex, since many other NLP tech-
niques, such as named entity recognition, parsing, 
semantic analysis, reasoning, and external re-
sources, such as WordNet, web, databases, are in-
corporated.  The various techniques and resources 
may provide the indicative evidences to find the 
correct answers.  These evidences are further com-
bined by using a pipeline structure, a scoring func-
tion or a machine learning method. 
In the machine learning framework, it is critical 
but not trivial to generate the features from the 
various resources which may be represented as 
surface texts, syntactic structures and logic forms, 
etc.  The complexity of feature generation strongly 
depends on the complexity of data representation.  
Many previous QA systems (Echihabi et al, 2003; 
Ravichandran, et al, 2003; Ittycheriah and Roukos, 
2002; Ittycheriah, 2001; Xu et al, 2002) have well 
studied the features in the surface texts.  In this 
paper, we will use the answer extraction module of 
QA as a case study to further explore how to gen-
erate the features for the more complex sentence 
representations, such as parse tree.  Since parsing 
gives the deeper understanding of the sentence, the 
features generated from the parse tree are expected 
to improve the performance based on the features 
generated from the surface text.  The answer ex-
65
traction module is built using Support Vector Ma-
chines (SVM).  We propose three methods to rep-
resent the features in the parse tree: 1. features are 
designed by domain experts, extracted from the 
parse tree and represented as a feature vector; 2. 
the parse tree is transformed to a node sequence 
and a string kernel is employed; 3. the parse tree is 
retained as the original representation and a tree 
kernel is employed. 
Although many textual features have been used 
in the others? AE modules, it is not clear that how 
much contribution the individual feature makes.  In 
this paper, we will discuss the effectiveness of 
each individual textual feature in detail.  We fur-
ther evaluate the effectiveness of the syntactic fea-
tures we proposed.  Our experiments using TREC 
questions show that the syntactic features improve 
the performance by 7.57 MRR based on the textual 
features.  It indicates that the new features based 
on a deeper language understanding are necessary 
to push further the machine learning-based QA 
technology.  Furthermore, the three representations 
of the syntactic features are compared.  We find 
that keeping the original data representation by 
using the data-specific kernel function in SVM 
may capture the more comprehensive evidences 
than the predefined features.  Although the features 
we generated are specific to the answer extraction 
task, the comparison between the different feature 
representations may be helpful to explore the syn-
tactic features for the other NLP applications. 
2 Related Work 
In the machine learning framework, it is crucial to 
capture the useful evidences for the task and inte-
grate them effectively in the model.  Many re-
searchers have explored the rich textual features 
for the answer extraction. 
IBM (Ittycheriah and Roukos, 2002; Ittycheriah, 
2001) used a Maximum Entropy model to integrate 
the rich features, including query expansion fea-
tures, focus matching features, answer candidate 
co-occurrence features, certain word frequency 
features, named entity features, dependency rela-
tion features, linguistic motivated features and sur-
face patterns.  ISI?s (Echihabi et al 2003; Echihabi 
and Marcu, 2003) statistical-based AE module im-
plemented a noisy-channel model to explain how a 
given sentence tagged with an answer can be re-
written into a question through a sequence of sto-
chastic operations.  (Ravichandran et al, 2003) 
compared two maximum entropy-based QA sys-
tems, which view the AE as a classification prob-
lem and a re-ranking problem respectively, based 
on the word frequency features, expected answer 
class features, question word absent features and 
word match features.  BBN (Xu et al 2002) used a 
HMM-based IR system to score the answer candi-
dates based on the answer contexts.  They further 
re-ranked the scored answer candidates using the 
constraint features, such as whether a numerical 
answer quantifies the correct noun, whether the 
answer is of the correct location sub-type and 
whether the answer satisfies the verb arguments of 
the questions.  (Suzuki et al 2002) explored the 
answer extraction using SVM. 
However, in the previous statistical-based AE 
modules, most of the features were extracted from 
the surface texts which are mainly based on the 
key words/phrases matching and the key word fre-
quency statistics.  These features only capture the 
surface-based information for the proper answers 
and may not provide the deeper understanding of 
the sentences.  In addition, the contribution of the 
individual feature has not been evaluated by them.  
As for the features extracted from the structured 
texts, such as parse trees, only a few works ex-
plored some predefined syntactic relation features 
by partial matching.  In this paper, we will explore 
the syntactic features in the parse trees and com-
pare the different feature representations in SVM.  
Moreover, the contributions of the different fea-
tures will be discussed in detail. 
3 Answer Extraction 
Given a question Q and a set of relevant sentences 
SentSet which is returned by the IR module, we 
consider all of the base noun phrases and the words 
in the base noun phrases as answer candidates aci.  
For example, for the question ?Q1956: What coun-
try is the largest in square miles??, we extract the 
answer candidates { Russia, largest country, larg-
est, country, world, Canada, No.2.} in the sentence 
?I recently read that Russia is the largest country 
in the world, with Canada No. 2.?  The goal of the 
AE module is to choose the most probable answer 
from a set of answer candidates 1 2{ , ,... }mac ac ac  
for the question Q. 
We regard the answer extraction as a classifica-
tion problem, which classify each question and 
66
answer candidate pair <Q, aci> into the positive 
class (the correct answer) and the negative class 
(the incorrect answer), based on some features.  
The predication for each <Q, aci> is made inde-
pendently by the classifier, then, the ac with the 
most confident positive prediction is chosen as the 
answer for Q.  SVM have shown the excellent per-
formance for the binary classification, therefore, 
we employ it to classify the answer candidates.   
Answer extraction is not a trivial task, since it 
involves several components each of which is 
fairly sophisticated, including named entity recog-
nition, syntactic / semantic parsing, question analy-
sis, etc.  These components may provide some 
indicative evidences for the proper answers.  Be-
fore generating the features, we process the sen-
tences as follows: 
1. tag the answer sentences with named entities. 
2. parse the question and the answer sentences us-
ing the Collins? parser (Collin, 1996). 
3. extract the key words from the questions, such 
as the target words, query words and verbs. 
In the following sections, we will briefly intro-
duce the machine learning algorithm.  Then, we 
will discuss the features in detail, including the 
motivations and representations of the features. 
4 Support Vector Machines  
Support Vector Machines (SVM) (Vapnik, 1995) 
have strong theoretical motivation in statistical 
learning theory and achieve excellent generaliza-
tion performance in many language processing 
applications, such as text classification (Joachims, 
1998). 
SVM constructs a binary classifier that predict 
whether an instance x ( n?w R ) is positive 
( ( ) 1f =x ) or negative ( ( ) 1f = ?x ), where, an 
instance may be represented as a feature vector or 
a structure like sequence of characters or tree.  In 
the simplest case (linearly separable instances), the 
decision f( ) sgn( b )? +x = w x is made based 
on a separating hyperplane 0b? + =w x  ( n?w R , 
b?R ).  All instances lying on one side of the hy-
perplane are classified to a positive class, while 
others are classified to a negative class. 
Given a set of labeled training instances 
( ) ( ) ( ){ }1 1 2 2, , , ,..., ,m mD y y y= x x x , where ni ?x R  
and { }1, 1iy = ? , SVM is to find the optimal hy-
perplane that separates the positive and negative 
training instances with a maximal margin.  The 
margin is defined as the distance from the separat-
ing hyperplane to the closest positive (negative) 
training instances.  SVM is trained by solving a 
dual quadratic programming problem. 
Practically, the instances are non-linearly sepa-
rable.  For this case, we need project the instances 
in the original space Rn to a higher dimensional 
space RN based on the kernel function 
1 2 1 2( , ) ( ), ( )K =<? ? >x x x x ,where, ( ): n N? ?x R R  is 
a project function of the instance.  By this means, a 
linear separation will be made in the new space.  
Corresponding to the original space Rn, a non-
linear separating surface is found.  The kernel 
function has to be defined based on the Mercer?s 
condition.  Generally, the following kernel func-
tions are widely used. 
Polynomial kernel: ( , ) ( 1) pi j i jk = ? +x x x x  
Gaussian RBF kernel:  
2 22( , ) i j-i jk e
??= x xx x  
5 Textual Features 
Since the features extracted from the surface texts 
have been well explored by many QA systems 
(Echihabi et al, 2003; Ravichandran, et al, 2003; 
Ittycheriah and Roukos, 2002; Ittycheriah, 2001; 
Xu et al, 2002), we will not focus on the textual 
feature generation in this paper.  Only four types of 
the basic features are used: 
1. Syntactic Tag Features: the features capture 
the syntactic/POS information of the words in 
the answer candidates.  For the certain ques-
tion, such as ?Q1903: How many time zones 
are there in the world??, if the answer candi-
date consists of the words with the syntactic 
tags ?CD NN?, it is more likely to be the 
proper answer. 
2. Orthographic Features: the features capture 
the surface format of the answer candidates, 
such as capitalization, digits and lengths, etc.  
These features are motivated by the observa-
tions, such as, the length  of the answers are 
often less than 3 words for the factoid ques-
tions; the answers may not be the subse-
quences of the questions; the answers often 
contain digits for the certain questions. 
3. Named Entity Features: the features capture 
the named entity information of the answer 
67
candidates.  They are very effective for the 
who, when and where questions, such as, For 
?Q1950: Who created the literary character 
Phineas Fogg??, the answer ?Jules Verne? is 
tagged as a PERSON name in the sentences 
?Jules Verne 's Phileas Fogg made literary 
history when he traveled around the world in 
80 days in 1873.?.  For the certain question tar-
get, if the answer candidate is tagged as the 
certain type of named entity, one feature fires. 
4. Triggers: some trigger words are collected for 
the certain questions.  For examples, for 
?Q2156: How fast does Randy Johnson 
throw??, the trigger word ?mph? for the ques-
tion words ?how fast? may help to identify the 
answer ?98-mph? in ?Johnson throws a 98-
mph fastball?. 
6 Syntactic Features 
In this section, we will discuss the feature genera-
tion in the parse trees.  Since parsing outputs the 
highly structured data representation of the sen-
tence, the features generated from the parse trees 
may provide the more linguistic-motivated expla-
nation for the proper answers.  However, it is not 
trivial to find the informative evidences from a 
parse tree. 
The motivation of the syntactic features in our 
task is that the proper answers often have the cer-
tain syntactic relations with the question key words.  
Table 1 shows some examples of the typical syn-
tactic relations between the proper answers (a) and 
the question target words (qtarget).  Furthermore, 
the syntactic relations between the answers and the 
different types of question key words vary a lot.  
Therefore, we capture the relation features for the 
different types of question words respectively.  The 
question words are divided into four types: 
z Target word, which indicates the expected an-
swer type, such as ?city? in ?Q: What city is 
Disneyland in??. 
z Head word, which is extracted from how ques-
tions and indicates the expected answer head, 
such as ?dog? in ?Q210: How many dogs 
pull ??? 
z Subject words, which are the base noun phrases 
of the question except the target word and the 
head word. 
z Verb, which is the main verb of the question. 
To our knowledge, the syntactic relation fea-
tures between the answers and the question key 
words haven?t been explored in the previous ma-
chine learning-based QA systems.  Next, we will 
propose three methods to represent the syntactic 
relation features in SVM. 
6.1 Feature Vector 
It is the commonly used feature representation in 
most of the machine learning algorithms.  We pre-
define a set of syntactic relation features, which is 
an enumeration of some useful evidences of the 
answer candidates (ac) and the question key words 
in the parse trees.  20 syntactic features are manu-
ally designed in the task.  Some examples of the 
features are listed as follows, 
z if the ac node is the same of the qtarget node, 
one feature fires. 
z if the ac node is the sibling of the qtarget node, 
one feature fires. 
z if the ac node the child of the qsubject node, 
one feature fires. 
The limitation of the manually designed features is 
that they only capture the evidences in the local 
context of the answer candidates and the question 
key words.  However, some question words, such 
as subject words, often have the long range syntac-
1. a node is the same as the qtarget node and qtarget is the hypernym of a. 
Q: What city is Disneyland in? 
S: Not bad for a struggling actor who was working at Tokyo Disneyland a few years ago. 
2. a node is the parent of qtarget node. 
Q: What is the name of the airport in Dallas Ft. Worth? 
S: Wednesday morning, the low temperature at the Dallas-Fort Worth International Airport was 81 degrees. 
3. a node is the sibling of the qtarget node. 
Q: What book did Rachel Carson write in 1962? 
S: In her 1962 book Silent Spring, Rachel Carson, a marine biologist, chronicled DDT 's poisonous effects, ?. 
Table 1: Examples of the typical relations between answer and question target word.  In Q, the italic word is 
question target word.  In S, the italic word is the question target word which is mapped in the answer sentence; 
the underlined word is the proper answer for the question Q. 
68
Figure 1: An example of the path from the answer 
candidate node to the question subject word node 
tic relations with the answers.  To overcome the 
limitation, we will propose some special kernels 
which may keep the original data representation 
instead of explicitly enumerate the features, to ex-
plore a much larger feature space. 
6.2 String Kernel 
The second method represents the syntactic rela-
tion as a linked node sequence and incorporates a 
string kernel in SVM to handle the sequence. 
We extract a path from the node of the answer 
candidate to the node of the question key word in 
the parse tree.  The path is represented as a node 
sequence linked by symbols indicating upward or 
downward movement through the tree. For exam-
ple, in Figure 1, the path from the answer candi-
date node ?211,456 miles? to the question subject 
word node ?the moon? is 
? NPB ADVP VP S NPB? ? ? ? ?, where ? ? ? and 
? ? ? indicate upward movement and downward 
movement in the parse tree.  By this means, we 
represent the object from the original parse tree to 
the node sequence.  Each character of the sequence 
is a syntactic/POS tag of the node.  Next, a string 
kernel will be adapted to our task to calculate the 
similarity between two node sequences. 
 
 
 
 
 
(Haussler, 1999) first described a convolution 
kernel over the strings.  (Lodhi et al, 2000) applied 
the string kernel to the text classification.  (Leslie 
et al, 2002) further proposed a spectrum kernel, 
which is simpler and more efficient than the previ-
ous string kernels, for protein classification prob-
lem.  In their tasks, the string kernels achieved the 
better performance compared with the human-
defined features. 
The string kernel is to calculate the similarity 
between two strings.  It is based on the observation 
that the more common substrings the strings have, 
the more similar they are.  The string kernel we 
used is similar to (Leslie et al, 2002).  It is defined 
as the sum of the weighted common substrings.  
The substring is weighted by an exponentially de-
caying factor ? (set 0.5 in the experiment) of its 
length k.  For efficiency, we only consider the sub-
strings which length are less than 3.  Different 
from (Leslie et al, 2002), the characters (syntac-
tic/POS tag) of the string are linked with each 
other.  Therefore, the matching between two sub-
strings will consider the linking information.  Two 
identical substrings will not only have the same 
syntactic tag sequences but also have the same 
linking symbols.  For example, for the node se-
quences NP VP VP S NP? ? ? ?  and NP NP VP NP? ? ? , 
there is a matched substring (k = 2): NP VP? . 
6.3 Tree Kernel 
The third method keeps the original representation 
of the syntactic relation in the parse tree and incor-
porates a tree kernel in SVM. 
Tree kernels are the structure-driven kernels to 
calculate the similarity between two trees.  They 
have been successfully accepted in the NLP appli-
cations.  (Collins and Duffy, 2002) defined a ker-
nel on parse tree and used it to improve parsing.  
(Collins, 2002) extended the approach to POS tag-
ging and named entity recognition.  (Zelenko et al, 
2003; Culotta and Sorensen, 2004) further ex-
plored tree kernels for relation extraction. 
We define an object (a relation tree) as the 
smallest tree which covers one answer candidate 
node and one question key word node.  Suppose 
that a relation tree T has nodes 0 1{ , , ..., }nt t t  and 
each node it is attached with a set of attrib-
utes 0 1{ , , ..., }ma a a , which represents the local char-
acteristics of ti .  In our task, the set of the 
attributes includes Type attributes, Orthographic 
attributes and Relation Role attributes, as shown in 
Table 2.  The core idea of the tree kernel ( , )1 2K T T  
is that the similarity between two trees T1 and T2 is 
PUNC
. away 221,456 miles 
S 
PP NPB VP 
VBZ ADVP
NPB RB 
the moon 
is 
Q1980: How far is the moon from Earth in miles? 
S: At its perigee, the closest approach to Earth , the 
moon is 221,456 miles away. 
?? 
69
T1_ac#target 
T2_ac#target 
Q1897: What is the name of the airport in Dallas Ft. Worth? 
S: Wednesday morning, the low temperature at the Dallas-Fort 
Worth International Airport was 81 degrees. 
t4t3 t2
T: BNP 
O: null  
R1: true 
R2: false 
t1
Dallas-Fort
T: NNP 
O: CAPALL 
R1: false 
R2: false
International 
T: JJ 
O: CAPALL  
R1: false 
R2: false 
Airport 
T: NNP 
O: CAPALL 
R1: false 
R2: true
Q35: What is the name of the highest mountain in Africa? 
S: Mount Kilimanjaro, at 19,342 feet, is Africa's highest moun-
tain, and is 5,000 feet higher than ?. 
Mount 
T: NNP 
O: CAPALL 
R1: false 
R2: true
Kilimanjaro
T: NNP 
O: CAPALL 
R1: false 
R2: false 
T: BNP 
O: null  
R1: true 
R2: false 
t0 
w0 
w1 w2 
Worth 
T: NNP 
O: CAPALL 
R1: false 
R2: false
the sum of the similarity between their subtrees.  It 
is calculated by dynamic programming and cap-
tures the long-range syntactic relations between 
two nodes.  The kernel we use is similar to (Ze-
lenko et al, 2003) except that we define a task-
specific matching function and similarity function, 
which are two primitive functions to calculate the 
similarity between two nodes in terms of their at-
tributes. 
Matching function 
1 if . .  and . .   
( , )
0 otherwise                                           
i j i j
i j
t type t type t role t role
m t t
= == ???  
Similarity function 
0{ ,..., }
( , ) ( . , . )i j i j
ma a a
s t t f t a t a
?
= ?  
where, ( . , . )i jf t a t a  is a compatibility function be-
tween two feature values 
. .
( . , . )
1   if 
0   otherwise
i j
i j
t a t a
f t a t a =
=???  
Figure 2 shows two examples of the relation tree 
T1_ac#targetword and T2_ac#targetword.  The 
kernel we used matches the following pairs of the 
nodes <t0, w0>, <t1, w2>, <t2, w2> and <t4, w1>. 
 
Attributes Examples 
POS tag CD, NNP, NN?Type 
syntactic tag NP, VP, ? 
Is Digit? DIG, DIGALL 
Is Capitalized? CAP, CAPALL 
Ortho-
graphic  
length of phrase LNG1, LNG2#3, 
LNGgt3 
Role1 Is answer candidate? true, false 
Role2 Is question key words? true, false 
Table 2: Attributes of the nodes 
7 Experiments 
We apply the AE module to the TREC QA task.  
To evaluate the features in the AE module inde-
pendently, we suppose that the IR module has got 
100% precision and only passes those sentences 
containing the proper answers to the AE module.  
The AE module is to identify the proper answers 
from the given sentence collection. 
We use the questions of TREC8, 9, 2001 and 
2002 for training and the questions of TREC2003 
for testing.  The following steps are used to gener-
ate the data: 
1. Retrieve the relevant documents for each ques-
tion based on the TREC judgments. 
2. Extract the sentences, which match both the 
proper answer and at least one question key word, 
from these documents. 
3. Tag the proper answer in the sentences based on 
the TREC answer patterns 
 
Figure 2: Two objects representing the relations be-
tween answer candidates and target words. 
 
In TREC 2003, there are 413 factoid questions 
in which 51 questions (NIL questions) are not re-
turned with the proper answers by TREC.  Accord-
ing to our data generation process, we cannot 
provide data for those NIL questions because we 
cannot get the sentence collections.  Therefore, the 
AE module will fail on all of the NIL questions 
and the number of the valid questions should be 
362 (413 ? 51).  In the experiment, we still test the 
module on the whole question set (413 questions) 
to keep consistent with the other?s work.  The 
training set contains 1252 questions.  The perform-
ance of our system is evaluated using the mean 
reciprocal rank (MRR).  Furthermore, we also list 
the percentages of the correct answers respectively 
70
in terms of the top 5 answers and the top 1 answer 
returned.  We employ the SVMLight (Joachims, 
1999) to incorporate the features and classify the 
answer candidates.  No post-processes are used to 
adjust the answers in the experiments. 
Firstly, we evaluate the effectiveness of the tex-
tual features, described in Section 5.  We incorpo-
rate them into SVM using the three kernel 
functions: linear kernel, polynomial kernel and 
RBF kernel, which are introduced in Section 4.  
Table 3 shows the performance for the different 
kernels.  The RBF kernel (46.24 MRR) signifi-
cantly outperforms the linear kernel (33.72 MRR) 
and the polynomial kernel (40.45 MRR).  There-
fore, we will use the RBF kernel in the rest ex-
periments. 
 Top1 Top5 MRR 
linear 31.28 37.91 33.72 
polynomial 37.91 44.55 40.45 
RBF 42.67 51.58 46.24 
Table 3: Performance for kernels 
 
In order to evaluate the contribution of the indi-
vidual feature, we test out module using different 
feature combinations, as shown in Table 4.  Sev-
eral findings are concluded: 
1. With only the syntactic tag features Fsyn., the 
module achieves a basic level MRR of 31.38.  The 
questions ?Q1903: How many time zones are there 
in the world?? is correctly answered from the sen-
tence ?The world is divided into 24 time zones.?.  
2. The orthographic features Forth. show the posi-
tive effect with 7.12 MRR improvement based on 
Fsyn..  They help to find the proper answer ?Grover 
Cleveland? for the question ?Q2049: What presi-
dent served 2 nonconsecutive terms?? from the 
sentence ?Grover Cleveland is the forgotten two-
term American president.?, while Fsyn. wrongly 
identify ?president? as the answer. 
3. The named entity features Fne are also benefi-
cial as they make the 4.46 MRR increase based on 
Fsyn.+Forth.   For the question ?Q2076: What com-
pany owns the soft drink brand "Gatorade"??, Fne 
find the proper answer ?Quaker Oats? in the sen-
tence ?Marineau , 53 , had distinguished himself 
by turning the sports drink Gatorade into a mass 
consumer brand while an executive at Quaker Oats 
During his 18-month??, while Fsyn.+Forth. return 
the wrong answer ?Marineau?. 
4. The trigger features Ftrg lead to an improve-
ment of 3.28 MRR based on Fsyn.+Forth+Fne.  They 
correctly answer more questions.  For the question 
?Q1937: How fast can a nuclear submarine 
travel??, Ftrg return the proper answer ?25 knots? 
from the sentence ?The submarine , 360 feet 
( 109.8 meters ) long , has 129 crew members and 
travels at 25 knots.?, but the previous features fail 
on it. 
Fsyn Forth. Fne Ftrg Top1 Top5 MRR
?    26.50 38.92 31.38
? ?   34.69 43.61 38.50
? ? ?  39.85 47.82 42.96
? ? ? ? 42.67 51.58 46.24
Table 4: Performance for feature combinations 
 
Next, we will evaluate the effectiveness of the syn-
tactic features, described in Section 6.  Table 5 
compares the three feature representation methods, 
FeatureVector, StringKernel and TreeKernel.   
z FeatureVector (Section 6.1).  We predefine 
some features in the syntactic tree and present 
them as a feature vector.  The syntactic fea-
tures are added with the textual features and 
the RBF kernel is used to cope with them. 
z StringKernel (Section 6.2).  No features are 
predefined.  We transform the syntactic rela-
tions between answer candidates and question 
key words to node sequences and a string ker-
nel is proposed to cope with the sequences.  
Then we add the string kernel for the syntactic 
relations and the RBF kernel for the textual 
features. 
z TreeKernel (Section 6.3).  No features are 
predefined.  We keep the original representa-
tions of the syntactic relations and propose a 
tree kernel to cope with the relation trees.  
Then we add the tree kernel and the RBF ker-
nel. 
 Top1 Top2 MRR
Fsyn.+Forth.+Fne+Ftrg 42.67 51.58 46.24
FeatureVector 46.19 53.69 49.28
StringKernel 48.99 55.83 52.29
TreeKernel 50.41 57.46 53.81
Table 5: Performance for syntactic feature repre-
sentations 
 
Table 5 shows the performances of FeatureVec-
tor, StringKernel and TreeKernel.  All of them im-
prove the performance based on the textual 
features (Fsyn.+Forth.+Fne+Ftrg) by 3.04 MRR, 6.05 
MRR and 7.57 MRR respectively.  The probable 
reason may be that the features generated from the 
structured data representation may capture the 
71
more linguistic-motivated evidences for the proper 
answers.  For example, the syntactic features help 
to find the answer ?nitrogen? for the question 
?Q2139: What gas is 78 percent of the earth 's at-
mosphere?? in the sentence ?One thing they have-
n't found in the moon's atmosphere so far is 
nitrogen, the gas that makes up more than three-
quarters of the Earth's atmosphere.?, while the 
textual features fail on it.  Furthermore, the String-
Kernel (+3.01MRR) and TreeKernel (+4.53MRR) 
achieve the higher performance than FeatureVec-
tor, which may be explained that keeping the 
original data representations by incorporating the 
data-specific kernels in SVM may capture the 
more comprehensive evidences than the predefined 
features.  Moreover, TreeKernel slightly outper-
forms StringKernel by 1.52 MRR.  The reason may 
be that when we transform the representation of the 
syntactic relation from the tree to the node se-
quence, some information may be lost, such as the 
sibling node of the answer candidates.  Sometimes 
the information is useful to find the proper answers. 
8 Conclusion  
In this paper, we study the feature generation based 
on the various data representations, such as surface 
text and parse tree, for the answer extraction.  We 
generate the syntactic tag features, orthographic 
features, named entity features and trigger features 
from the surface texts.  We further explore the fea-
ture generation from the parse trees which provide 
the more linguistic-motivated evidences for the 
task.  We propose three methods, including feature 
vector, string kernel and tree kernel, to represent 
the syntactic features in Support Vector Machines.  
The experiment on the TREC question answering 
task shows that the syntactic features significantly 
improve the performance by 7.57MRR based on 
the textual features.  Furthermore, keeping the 
original data representation using a data-specific 
kernel achieves the better performance than the 
explicitly enumerated features in SVM. 
References  
M. Collins.  1996.  A New Statistical Parser Based on 
Bigram Lexical Dependencies.  In Proceedings of 
ACL-96, pages 184-191. 
M. Collins. 2002.  New Ranking Algorithms for Parsing 
and Tagging: Kernel over Discrete Structures, and 
the Voted Perceptron.  In Proceedings of ACL-2002. 
M. Collins and N. Duffy.  2002.  Convolution Kernels 
for Natural Language.  Advances in Neural Informa-
tion Processing Systems 14, Cambridge, MA.  MIT 
Press. 
A. Culotta and J. Sorensen.  2004.  Dependency Tree 
Kernels for Relation Extraction.  In Proceedings of 
ACL-2004. 
A. Echihabi, U. Hermjakob, E. Hovy, D. Marcu, E. 
Melz, D. Ravichandran.  2003.  Multiple-Engine 
Question Answering in TextMap.  In Proceedings of 
the TREC-2003 Conference, NIST. 
A. Echihabi, D. Marcu. 2003. A Noisy-Channel Ap-
proach to Question Answering. In Proceedings of the 
ACL-2003. 
D. Haussler. 1999.  Convolution Kernels on Discrete 
Structures.  Technical Report UCS-CRL-99-10, Uni-
versity of California, Santa Cruz. 
A. Ittycheriah and S. Roukos.  2002.  IBM?s Statistical 
Question Answering System ? TREC 11.  In Pro-
ceedings of the TREC-2002 Conference, NIST. 
A. Ittycheriah. 2001. Trainable Question Answering 
System.  Ph.D. Dissertation, Rutgers, The State Uni-
versity of New Jersey, New Brunswick, NJ. 
T. Joachims.  1999.  Making large-Scale SVM Learn-
ing Practical.  Advances in Kernel Methods - Sup-
port Vector Learning, MIT-Press, 1999. 
T. Joachims.  1998.  Text Categorization with Support 
Vector Machines: Learning with Many Relevant Fea-
tures.  In Proceedings of the European Conference on 
Machine Learning, Springer. 
C. Leslie, E. Eskin and W. S. Noble.  2002.  The spec-
trum kernel: A string kernel for SVM protein classi-
fication.  Proceedings of the Pacific Biocomputing 
Symposium. 
H. Lodhi, J. S. Taylor, N. Cristianini and C. J. C. H. 
Watkins.  2000.  Text Classification using String 
Kernels.  In NIPS, pages 563-569. 
D. Ravichandran, E. Hovy and F. J. Och.  2003.  Statis-
tical QA ? Classifier vs. Re-ranker: What?s the dif-
ference?  In Proceedings of Workshop on Mulingual 
Summarization and Question Answering, ACL 2003. 
J. Suzuki, Y. Sasaki, and E. Maeda. 2002. SVM Answer 
Selection for Open-domain Question Answering. In 
Proc. of COLING 2002, pages 974?980. 
V. N. Vapnik.  1998.  Statistical Learning Theory.  
Springer. 
E.M. Voorhees.  2003. Overview of the TREC 2003 
Question Answering Track.  In Proceedings of the 
TREC-2003 Conference, NIST. 
J. Xu, A. Licuanan, J. May, S. Miller and R. Weischedel.  
2002.  TREC 2002 QA at BBN: Answer Selection 
and Confidence Estimation.  In Proceedings of the 
TREC-2002 Conference, NIST. 
D. Zelenko, C. Aone and A. Richardella.  2003.  Kernel 
Methods for Relation Extraction.  Journal of Ma-
chine Learning Research, pages 1083-1106. 
72
Multi-Criteria-based Active Learning for Named Entity Recognition 
Dan Shen??1 Jie Zhang?? Jian Su? Guodong Zhou? Chew-Lim Tan? 
? Institute for Infocomm Technology 
21 Heng Mui Keng Terrace 
Singapore 119613 
? Department of Computer Science 
National University of Singapore 
3 Science Drive 2, Singapore 117543 
{shendan,zhangjie,sujian,zhougd}@i2r.a-star.edu.sg 
{shendan,zhangjie,tancl}@comp.nus.edu.sg 
                                                                 
1 Current address of the first author: Universit?t des Saarlandes, Computational Linguistics Dept., 66041 Saarbr?cken, Germany 
dshen@coli.uni-sb.de 
 
 
 
 
Abstract 
In this paper, we propose a multi-criteria -
based active learning approach and effec-
tively apply it to named entity recognition. 
Active learning targets to minimize the 
human annotation efforts by selecting ex-
amples for labeling.  To maximize the con-
tribution of the selected examples, we 
consider the multiple criteria: informative-
ness, representativeness and diversity  and 
propose measures to quantify them.  More 
comprehensively, we incorporate all the 
criteria using two selection strategies, both 
of which result in less labeling cost than 
single-criterion-based method.  The results 
of the named entity recognition in both 
MUC-6 and GENIA show that the labeling 
cost can be reduced by at least 80% with-
out degrading the performance. 
1 Introduction 
In the machine learning approaches of natural lan-
guage processing (NLP), models are generally 
trained on large annotated corpus.  However, anno-
tating such corpus is expensive and time-
consuming, which makes it difficult to adapt an 
existing model to a new domain.  In order to over-
come this difficulty, active learning (sample selec-
tion) has been studied in more and more NLP 
applications such as POS tagging (Engelson and 
Dagan 1999), information extraction (Thompson et 
al. 1999), text classif ication (Lewis and Catlett 
1994; McCallum and Nigam 1998; Schohn and 
Cohn 2000; Tong and Koller 2000; Brinker 2003), 
statistical parsing (Thompson et al 1999; Tang et 
al. 2002; Steedman et al 2003), noun phrase 
chunking (Ngai and Yarowsky 2000), etc. 
Active learning is based on the assumption that 
a small number of annotated examples and a large 
number of unannotated examples are available.  
This assumption is valid in most NLP tasks.  Dif-
ferent from supervised learning in which the entire 
corpus are labeled manually, active learning is to 
select the most useful example for labeling and add 
the labeled example  to training set to retrain model.  
This procedure is repeated until the model achieves 
a certain level of performance.  Practically, a batch 
of examples are selected at a time, called batched-
based sample selection (Lewis and Catlett 1994) 
since it is time consuming to retrain the model if 
only one new example is added to the training set.  
Many existing work in the area focus on two ap-
proaches: certainty-based methods (Thompson et 
al. 1999; Tang et al 2002; Schohn and Cohn 2000; 
Tong and Koller 2000; Brinker 2003) and commit-
tee-based methods (McCallum and Nigam 1998; 
Engelson and Dagan 1999; Ngai and Yarowsky 
2000) to select the most informative examples for 
which the current model are most uncertain. 
Being the first piece of work on active learning 
for name entity recognition (NER) task, we target 
to minimize the human annotation efforts yet still 
reaching the same level of performance as a super-
vised learning approach.  For this purpose, we 
make a more comprehensive consideration on the 
contribution of individual examples, and more im-
portantly maximizing the contribution of a batch 
based on three criteria : informativeness, represen-
tativeness and diversity. 
First, we propose three scoring functions to 
quantify the informativeness of an example , which 
can be used to select the most uncertain examples.  
Second, the representativeness measure is further 
proposed to choose the examples representing the 
majority.  Third, we propose two diversity consid-
erations (global and local) to avoid repetition 
among the examples of a batch.  Finally, two com-
bination strategies with the above three criteria are 
proposed to reach the maximum effectiveness on 
active learning for NER. 
We build our NER model using Support Vec-
tor Machines (SVM).  The experiment shows that 
our active learning methods achieve a promising 
result in this NER task.  The results in both MUC-
6 and GENIA show that the amount of the labeled 
training data can be reduced by at least 80% with-
out degrading the quality of the named entity rec-
ognizer.  The contributions not only come from the 
above measures, but also the two sample selection 
strategies which effectively incorporate informa-
tiveness, representativeness and diversity criteria.  
To our knowledge, it is the first work on consider-
ing the three criteria all together for active learning.  
Furthermore, such measures and strategies can be 
easily adapted to other active learning tasks as well.  
 
2 Multi-criteria for NER Active Learning 
Support Vector Machines (SVM) is a powerful 
machine learning method, which has been applied 
successfully in NER tasks, such as (Kazama et al 
2002; Lee et al 2003).  In this paper, we apply ac-
tive learning methods to a simple  and effective 
SVM model to recognize one class of names at a 
time, such as protein names, person names, etc.  In 
NER, SVM is to classify a word into positive class 
?1? indicating that the word is a part of an entity, 
or negative class ?-1? indicating that the word is 
not a part of an entity.  Each word in SVM is rep-
resented as a high-dimensional feature vector in-
cluding surface word information, orthographic 
features, POS feature and semantic trigger features 
(Shen et al 2003).  The semantic trigger features 
consist of some special head nouns for an entity 
class which is supplied by users.  Furthermore, a 
window (size = 7), which represents the local con-
text of the target word w, is also used to classify w.   
However, for active learning in NER, it is not 
reasonable to select a single word without context 
for human to label.  Even if we require human to 
label a single word, he has to make an addition 
effort to refer to the context of the word.  In our 
active learning process, we select a word sequence 
which consists of a machine-annotated named en-
tity and its context rather than a single word.  
Therefore, all of the measures we propose for ac-
tive learning should be applied to the machine-
annotated named entities and we have to further 
study how to extend the measures for words to 
named entities.  Thus, the active learning in SVM-
based NER will be more complex than that in sim-
ple classification tasks, such as text classif ication 
on which most SVM active learning works are 
conducted (Schohn and Cohn 2000; Tong and 
Koller 2000; Brinker 2003).  In the next part, we 
will introduce informativeness, representativeness 
and diversity measures for the SVM-based NER. 
2.1 Informativeness 
The basic idea of informativeness criterion is simi-
lar to certainty-based sample selection methods, 
which have been used in many previous works.  In 
our task, we use a distance-based measure to 
evaluate the informativeness of a word and extend 
it to the measure of an entity using three scoring 
functions.  We prefer the examples with high in-
formative degree for which the current model are 
most uncertain. 
2.1.1 Informativeness Measure for Word 
In the simplest linear form, training SVM is to find 
a hyperplane that can separate the posit ive and 
negative examples in training set with maximum 
margin.  The margin is defined by the distance of 
the hyperplane to the nearest of the positive and 
negative examples.  The training examples which 
are closest to the hyperplane are called support 
vectors.  In SVM, only the support vectors are use-
ful for the classification, which is different from 
statistical models.  SVM training is to get these 
support vectors and their weights from training set 
by solving quadratic programming problem.  The 
support vectors can later be used to classify the test 
data. 
Intuitively, we consider the informativeness of 
an example  as how it can make effect on the sup-
port vectors by adding it to training set.  An exam-
ple may be informative for the learner if the 
distance of its feature vector to the hyperplane is 
less than that of the support vectors to the hyper-
plane (equal to 1).  This intuition is also justified 
by (Schohn and Cohn 2000; Tong and Koller 2000) 
based on a version space analysis.  They state that 
labeling an example that lies on or close to the hy-
perplane is guaranteed to have an effect on the so-
lution.  In our task, we use the distance to measure 
the informativeness of an example. 
The distance of a word?s feature vector to the 
hyperplane is computed as follows: 
1
( ) ( , )
N
i i i
i
Dist y k ba
=
= +?w s w  
where w is the feature vector of the word, ai, yi, si 
corresponds to the weight, the class and the feature 
vector of the ith support vector respectively.  N is 
the number of the support vectors in current model. 
We select the example with minimal Dist, 
which indicates that it comes closest to the hyper-
plane in feature space.  This example is considered 
most informative for current model. 
2.1.2 Informativeness Measure for Named 
Entity 
Based on the above informativeness measure for a 
word, we compute the overall informativeness de-
gree of a named entity NE.  In this paper, we pro-
pose three scoring functions as follows. Let NE = 
w1?wN in which wi is the feature vector of the ith 
word of NE. 
? Info_Avg: The informativeness of NE is 
scored by the average distance of the words in 
NE to the hyperplane.  
 ( ) 1 ( )
i
i
N E
Info NE Dist
?
= - ?
w
w  
 where, wi is the feature vector of the ith word in 
NE. 
? Info_Min: The informativeness of NE is 
scored by the minimal distance of the words in 
NE. 
 ( ) 1 { ( )}
i
iNE
Info NE Min Dist
?
= -
w
w  
? Info_S/N: If the distance of a word to the hy-
perplane is less than a threshold a (= 1 in our 
task), the word is considered with short dis-
tance.  Then, we compute the proportion of the 
number of words with short distance to the to-
tal number of words in the named entity and 
use this proportion to quantify the informa-
tiveness of the named entity.  
 
( ( ) )
( ) i
i
N E
NUM Dist
Info NE
N
a
?
<
= w
w
 
In Section 4.3, we will evaluate the effective-
ness of these scoring functions. 
2.2 Representativeness 
In addition to the most informative example, we 
also prefer the most representative example.  The 
representativeness of an example can be evaluated 
based on how many examples there are similar or 
near to it.  So, the examples with high representa-
tive degree are less likely to be an outlier.  Adding 
them to the training set will have effect on a large 
number of unlabeled examples.  There are only a 
few works considering this selection criterion 
(McCallum and Nigam 1998; Tang et al 2002) and 
both of them are specific to their tasks, viz. text 
classification and statistical parsing.  In this section, 
we compute the simila rity between words using a 
general vector-based measure, extend this measure 
to named entity level using dynamic time warping 
algorithm and quantify the representativeness of a 
named entity by its density. 
2.2.1 Similarity Measure  between Words 
In general vector space model, the similarity be-
tween two vectors may be measured by computing 
the cosine value of the angle between them.  The 
smaller the angle is, the more similar between the 
vectors are.  This measure, called cosine-similarity 
measure, has been widely used in information re-
trieval tasks (Baeza-Yates and Ribeiro-Neto 1999).    
In our task, we also use it to quantify the similarity 
between two words.  Particularly, the calculation in 
SVM need be projected to a higher dimensional 
space by using a certain kernel function ( , )i jK w w .  
Therefore, we adapt the cosine-similarity measure 
to SVM as follows: 
( , )
( , )
( , ) ( , )
i j
i j
i i j j
k
Sim
k k
=
w w
w w
w w w w
 
where, wi and wj are the feature vectors of the 
words i and j.  This calculation is also supported by 
(Brinker 2003)?s work.  Furthermore, if we use the 
linear kernel ( , )i j i jk = ?w w w w , the measure is 
the same as the traditional cosine similarity meas-
ure cos i j
i j
q
?
=
?
w w
w w
 and may be regarded as a 
general vector-based similarity measure. 
2.2.2 Similarity Meas ure between Named En-
tities 
In this part, we compute the similarity between two 
machine-annotated named entities given the simi-
larities between words.  Regarding an entity as a 
word sequence, this work is analogous to the 
alignment of two sequences.  We employ the dy-
namic time warping (DTW) algorithm (Rabiner et 
al. 1978) to find an optimal alignment between the 
words in the sequences which maximize the accu-
mulated similarity degree between the sequences.  
Here, we adapt it to our task.  A sketch of the 
modified algorithm is as follows. 
Let NE1 = w11w12?w1n?w1N, (n = 1,?, N) and 
NE2 = w21w22?w2m?w2M, (m = 1,?, M) denote two 
word sequences to be matched.  NE1 and NE2 con-
sist of M and N words respectively.  NE1(n) = w1n 
and NE2(m) = w2m.  A similarity value Sim(w1n ,w2m) 
has been known for every pair of words (w1n,w2m) 
within NE1 and NE2.  The goal of DTW is to find a 
path, m = map(n), which map n onto the corre-
sponding m such that the accumulated similarity 
Sim* along the path is maximized. 
1 2
{ ( )} 1
* { ( ( ), ( ( ))}
N
m a p n n
Sim M a x Sim N E n N E m a p n
=
= ?  
A dynamic programming method is used to deter-
mine the optimum path map(n).  The accumulated 
similarity SimA to any grid point (n, m) can be re-
cursively calculated as 
1 2( , ) ( , ) ( 1, )A n m Aq mSim n m Sim w w M a x S i m n q?= + -
Finally, * ( , )ASim Sim N M=  
Certainly, the overall similarity measure Sim* 
has to be normalized as longer sequences normally 
give higher similarity value.  So, the similarity be-
tween two sequences NE1 and NE2 is calculated as 
1 2
*( , )
( , )
SimSim NE NE
Max N M
=  
2.2.3 Representativeness Measure for Named 
Entity 
Given a set of machine-annotated named entities 
NESet = {NE1, ? , NEN}, the representativeness of 
a named entity NEi in NESet is quantified by its 
density.  The density of NEi is defined as the aver-
age similarity between NEi and all the other enti-
ties NEj in NESet as follows. 
( , )
( )
1
i j
j i
i
Sim NE NE
Density N E
N
?=
-
?
 
If NEi has the largest density among all the entities 
in NESet, it can be regarded as the centroid of NE-
Set and also the most representative examples in 
NESet. 
2.3 Diversity 
Diversity criterion is to maximize the training util-
ity of a batch.  We prefer the batch in which the 
examples have high variance to each other.  For 
example, given the batch size 5, we try not to se-
lect five repetitious examples at a time.  To our 
knowledge, there is only one work (Brinker 2003) 
exploring this criterion.  In our task, we propose 
two methods: local and global, to make the exam-
ples diverse enough in a batch.   
2.3.1 Global Consideration 
For a global consideration, we cluster all named 
entities in NESet based on the similarity measure 
proposed in Section 2.2.2.  The named entities in 
the same cluster may be considered similar to each 
other, so we will select the named entities from 
different clusters at one time.  We employ a K-
means clustering algorithm (Jelinek 1997), which 
is shown in Figure 1. 
Given: 
NESet = {NE1, ? , NEN} 
Suppose: 
The number of clusters is K 
Initialization: 
Randomly equally partition {NE1, ? , NEN} into K 
initial clusters Cj (j = 1, ? , K). 
Loop until the number of changes for the centroids of 
all clusters is less than a threshold 
? Find the centroid of each cluster Cj (j = 1, ? , K). 
 arg ( ( , ))
j i j
j i
NE C NE C
NECent max Sim NE NE
? ?
= ?  
? Repartition {NE1, ? , NEN} into K clusters.  NEi 
will be assigned to Cluster Cj if 
 
( , ) ( , ),i j i wSim NE NECent Sim NE NECent w j? ?  
Figure 1: Global Consideration for Diversity: K-
Means Clustering algorithm 
In each round, we need to compute the pair-
wise similarities within each cluster to get the cen-
troid of the cluster.  And then, we need to compute 
the similarities between each example and all cen-
troids to repartition the examples.  So, the algo-
rithm is time-consuming.  Based on the assumption 
that N examples are uniformly distributed between 
the K clusters, the time complexity of the algo-
rithm is about O(N2/K+NK) (Tang et al 2002).  In 
one of our experiments, the size of the NESet (N) is 
around 17000 and K is equal to 50, so the time 
complexity is about O(106).  For efficiency, we 
may filter the entities in NESet before clustering 
them, which will be further discussed in Section 3.  
2.3.2 Local Consideration 
When selecting a machine-annotated named entity, 
we compare it with all previously selected named 
entities in the current batch.  If the similarity be-
tween them is above a threshold ?, this example 
cannot be allowed to add into the batch.  The order 
of selecting examples is based on some measure, 
such as informativeness measure, representative-
ness measure or their combination.  This local se-
lection method is shown in Figure 2.  In this way, 
we avoid selecting too similar examples (similarity 
value ?  ?) in a batch.  The threshold ? may be the 
average similarity between the examples in NESet. 
 
Given: 
NESet = {NE1, ? , NEN} 
BatchSet with the maximal size K. 
Initialization:  
BatchSet  = empty 
Loop until BatchSet is full 
? Select NEi based on some measure from NESet. 
? RepeatFlag = false; 
? Loop from j = 1 to CurrentSize(BatchSet)  
 If ( , )i jSim NE NE b? Then 
 RepeatFlag = true; 
 Stop the Loop; 
? If RepeatFlag == false Then 
add NEi into BatchSet 
? remove NEi from NESet 
Figure 2: Local Consideration for Diversity 
 
This consideration only requires O(NK+K2) 
computational time.  In one of our experiments (N 
 ?17000 and K = 50), the time complexity is about 
O(105).  It is more efficient than clustering algo-
rithm described in Section 2.3.1.  
 
3 Sample Selection strategies 
In this section, we will study how to combine and 
strike a proper balance between these criteria, viz. 
informativeness, representativeness and diversity, 
to reach the maximum effectiveness on NER active 
learning.  We build two strategies to combine the 
measures proposed above.  These strategies are 
based on the varying priorities of the criteria and 
the varying degrees to satisfy the criteria. 
? Strategy 1: We first consider the informative-
ness criterion.  We choose m examples with the 
most informativeness score from NESet to an in-
termediate set called INTERSet.  By this pre-
selecting, we make the selection process faster in 
the later steps since the size of INTERSet is much 
smaller than that of NESet.  Then we cluster the 
examples in INTERSet and choose the centroid of 
each cluster into a batch called BatchSet.  The cen-
troid of a cluster is the most representative exam-
ple in that cluster since it has the largest density.  
Furthermore, the examples in different clusters 
may be considered diverse to each other.  By this 
means, we consider representativeness and diver-
sity criteria at the same time.  This strategy is 
shown in Figure 3.  One limitation of this strategy 
is that clustering result may not reflect the distribu-
tion of whole sample space since we only cluster 
on INTERSet for efficiency.  The other is that since 
the representativeness of an example is only evalu-
ated on a cluster.  If the cluster size is too small, 
the most representative example in this cluster may 
not be representative in the whole sample space. 
 
Given: 
NESet = {NE1, ? , NEN} 
BatchSet with the maximal size K. 
INTERSet with the maximal size M 
Steps :  
? BatchSet  = ?  
? INTERSet = ?  
? Select M entities with most Info score from NESet 
to INTERSet. 
? Cluster the entities in INTERSet into K clusters 
? Add the centroid entity of each cluster to BatchSet 
Figure 3: Sample Selection Strategy 1 
 
? Strategy 2: (Figure 4) We combine the infor-
mativeness and representativeness criteria  using 
the functio ( ) (1 ) ( )i iInfo NE Density NEl l+ - , in 
which the Info and Density  value of NEi are nor-
malized first.  The individual importance of each 
criterion in this function is adjusted by the trade-
off parameter l ( 0 1l? ? ) (set to 0.6 in our 
experiment).  First, we select a candidate example 
NEi with the maximum value of this function from 
NESet.  Second, we consider diversity criterion 
using the local method in Section 3.3.2.  We add 
the candidate example NEi to a batch only if NEi is 
different enough from any previously selected ex-
ample in the batch.  The threshold ? is set to the 
average pair-wise similarity of the entities in NE-
Set. 
 
Given: 
NESet = {NE1, ? , NEN} 
BatchSet with the maximal size K. 
Initialization:  
BatchSet  = ?  
Loop until BatchSet is full 
? Select NEi which have the maximum value for the 
combination function between Info score and Den-
sity socre from NESet. 
arg ( ( ) (1 ) ( ))
i
i i i
N E NESet
N E Max Info NE Density NEl l
?
= + -
 
? RepeatFlag = false; 
? Loop from j = 1 to CurrentSize(BatchSet)  
 If ( , )i jSim NE NE b? Then 
 RepeatFlag = true; 
 Stop the Loop; 
? If RepeatFlag == false Then 
add NEi into BatchSet 
? remove NEi from NESet 
Figure 4: Sample Selection Strategy 2 
 
4 Experimental Results and Analysis 
4.1 Experiment Settings  
In order to evaluate the effectiveness of our selec-
tion strategies, we apply them to recognize protein 
(PRT) names in biomedical domain using GENIA 
corpus V1.1 (Ohta et al 2002) and person (PER), 
location (LOC), organization (ORG) names in 
newswire domain using MUC-6 corpus.  First, we 
randomly split the whole corpus into three parts: an 
initial training set to build an in itial model, a test 
set to evaluate the performance of the model and 
an unlabeled set to select examples.  The size of 
each data set is shown in Table 1.  Then, iteratively, 
we select a batch of examples following the selec-
tion strategies proposed, require human experts to 
label them and add them into the training set.  The 
batch size K = 50 in GENIA and 10 in MUC-6.  
Each example is defined as a machine-recognized 
named entity and its context words (previous 3 
words and next 3 words). 
Domain Class Corpus Initial Training Set Test Set Unlabeled Set 
Biomedical PRT GENIA1.1 10 sent. (277 words) 900 sent. (26K words) 8004 sent. (223K words) 
PER 5 sent. (131 words) 7809 sent. (157K words) 
LOC 5 sent. (130 words) 7809 sent. (157K words) 
 
Newswire 
ORG 
 
MUC-6 
 5 sent. (113 words) 
 
602 sent. (14K words) 
 7809 sent. (157K words) 
Table 1: Experiment settings for active learning using GENIA1.1(PRT) and MUC-6(PER,LOC,ORG) 
The goal of our work is to minimize the human 
annotation effort to learn a named entity recognizer 
with the same performance level as supervised 
learning.  The performance of our model is evalu-
ated using ?precision/recall/F-measure?. 
4.2 Overall Result in GENIA and MUC-6 
In this section, we evaluate our selection strategies 
by comparing them with a random selection 
method, in which a batch of examples is randomly 
selected iteratively, on GENIA and MUC-6 corpus.  
Table 2 shows the amount of training data needed 
to achieve the performance of supervised learning 
using various selection methods, viz. Random, 
Strategy1 and Strategy2.  In GENIA, we find: 
? The model achieves 63.3 F-measure using 223K  
words in the supervised learning. 
? The best performer is Strategy2 (31K words), 
requiring less than 40% of the training data that 
Random (83K words) does and 14% of the train-
ing data that the supervised learning does. 
? Strategy1 (40K words) performs slightly worse 
than Strategy2, requiring 9K more words.  It is 
probably because Strategy1 cannot avoid select-
ing outliers if a cluster is too small. 
? Random (83K words) requires about 37% of the 
training data that the supervised learning does.  It 
indicates that only the words in and around a 
named entity are useful for classification and the 
words far from the named entity may not be 
helpful. 
 
Class Supervised Random Strategy1 Strategy2 
PRT 223K (F=63.3) 83K 40K 31K 
PER 157K (F=90.4) 11.5K 4.2K 3.5K 
LOC 157K (F=73.5) 13.6K 3.5K 2.1K 
ORG 157K (F=86.0) 20.2K 9.5K 7.8K 
Table 2: Overall Result in GENIA and MUC-6 
Furthermore, when we apply our model to news-
wire domain (MUC-6) to recognize person, loca-
tion and organization names, Strategy1 and 
Strategy2 show a more promising result by com-
paring with the supervised learning and Random, 
as shown in Table 2.  On average, about 95% of 
the data can be reduced to achieve the same per-
formance with the supervised learning in MUC-6.  
It is probably because NER in the newswire do-
main is much simpler than that in the biomedical 
domain (Shen et al 2003) and named entities are 
less and distributed much sparser in the newswire 
texts than in the biomedical texts. 
 
4.3 Effectiveness of Informativeness-based 
Selection Method 
In this section, we investigate the effectiveness of 
informativeness criterion in NER task.  Figure 5 
shows a plot of training data size versus F-measure 
achieved by the informativeness-based measures in 
Section 3.1.2: Info_Avg, Info_Min  and Info_S/N as 
well as Random.  We make the comparisons in 
GENIA corpus.  In Figure 5, the horizontal line is 
the performance level (63.3 F-measure) achieved 
by supervised learning (223K words).  We find 
that the three informativeness-based measures per-
form similarly and each of them outperforms Ran-
dom.  Table 3 highlights the various data sizes to 
achieve the peak performance using these selection 
methods.  We find that Random (83K words) on 
average requires over 1.5 times as much as data to 
achieve the same performance as the informative-
ness-based selection methods (52K words). 
 
0.5
0.55
0.6
0.65
0 20 40 60 80K words
F
Supervised
Random
Info_Min
Info_S/N
Info_Avg
 
Figure 5: Active learning curves: effectiveness of the three in-
formativeness-criterion-based selections comparing with the 
Random selection. 
Supervised Random Info_Avg Info_Min Info_ S/N 
223K 83K 52.0K 51.9K 52.3K 
Table 3: Training data sizes for various selection methods to 
achieve the same performance level as the supervised learning 
 
4.4 Effectiveness of Two Sample Selection 
Strategies 
In addition to the informativeness criterion, we 
further incorporate representativeness and diversity 
criteria into active learning using two strategies 
described in Section 3.  Comparing the two strate-
gies with the best result of the single-criterion-
based selection methods Info_Min , we are to jus-
tify that representativeness and diversity are also 
important factors for active learning.  Figure 6 
shows the learning curves for the various methods: 
Strategy1, Strategy2 and Info_Min.  In the begin-
ning iterations (F-measure < 60), the three methods 
performed similarly.  But with the larger training 
set, the efficiencies of Stratety1 and Strategy2 be-
gin to be evident.  Table 4 highlights the final re-
sult of the three methods.  In order to reach the 
performance of supervised learning, Strategy1 
(40K words) and Strategyy2 (31K words) require 
about 80% and 60% of the data that Info_Min 
(51.9K) does.  So we believe the effective combi-
nations of informativeness, representativeness and 
diversity will help to learn the NER model more 
quickly and cost less in annotation. 
0.5
0.55
0.6
0.65
0 20 40 60 K words
F
Supervised
Info_Min
Strategy1
Strategy2
 
Figure 6: Active learning curves: effectiveness of the two 
multi-criteria-based selection strategies comparing with the 
informativeness-criterion-based selection (Info_Min). 
Info_Min Strategy1 Strategy2 
51.9K 40K 31K 
Table 4: Comparisons of training data sizes for the multi-
criteria-based selection strategies and the informativeness-
criterion-based selection (Info_Min) to achieve the same per-
formance level as the supervised learning. 
 
5 Related Work 
Since there is no study on active learning for NER 
task previously, we only introduce general active 
learning methods here.  Many existing active learn-
ing methods are to select the most uncertain exam-
ples using various measures (Thompson et al 1999; 
Schohn and Cohn 2000; Tong and Koller 2000; 
Engelson and Dagan 1999; Ngai and Yarowsky 
2000).  Our informativeness-based measure is 
similar to these works.  However these works just 
follow a single criterion.  (McCallum and Nigam 
1998; Tang et al 2002) are the only two works 
considering the representativeness criterion in ac-
tive learning.  (Tang et al 2002) use the density 
information to weight the selected examples while 
we use it to select examples.  Moreover, the repre-
sentativeness measure we use is relatively general 
and easy to adapt to other tasks, in which the ex-
ample selected is a sequence of words, such as text 
chunking, POS tagging, etc.  On the other hand, 
(Brinker 2003) first incorporate diversity in active 
learning for text classification.  Their work is simi-
lar to our local consideration in Section 2.3.2.  
However, he didn?t further explore how to avoid 
selecting outliers to a batch.  So far, we haven?t 
found any previous work integrating the informa-
tiveness, representativeness and diversity all to-
gether. 
 
6 Conclusion and Future Work 
In this paper, we study the active learning in a 
more complex NLP task, named entity recognition.  
We propose a multi-criteria -based approach to se-
lect examples based on their informativeness, rep-
resentativeness and diversity, which are 
incorporated all together by two strategies (local 
and global).  Experiments show that, in both MUC-
6 and GENIA, both of the two strategies combin-
ing the three criteria outperform the single criterion 
(informativeness).  The labeling cost can be sig-
nificantly reduced by at least 80% comparing with 
the supervised learning.  To our best knowledge, 
this is not only the first work to report the empiri-
cal results of active learning for NER, but also the 
first work to incorporate the three criteria all to-
gether for selecting examples. 
Although the current experiment results are 
very promising, some parameters in our experi-
ment, such as the batch size K and the l in the 
function of strategy 2, are decided by our experi-
ence in the domain.  In practical application, the 
optimal value of these parameters should be de-
cided automatically based on the training process.  
Furthermore, we will study how to overcome the 
limitation of the strategy 1 discussed in Section 3 
by using more effective clustering algorithm.  An-
other interesting work is to study when to stop ac-
tive learning.  
 
References 
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Mod-
ern Information Retrieval. ISBN 0-201-39829-X. 
K. Brinker. 2003. Incorporating Diversity in Ac-
tive Learning with Support Vector Machines. In 
Proceedings of ICML, 2003. 
S. A. Engelson and I. Dagan. 1999. Committee-
Based Sample Selection for Probabilistic Classi-
fiers. Journal of Artifical Intelligence Research. 
F. Jelinek. 1997. Statistical Methods for Speech 
Recognition. MIT Press. 
J. Kazama, T. Makino, Y. Ohta and J. Tsujii. 2002. 
Tuning Support Vector Machines for Biomedi-
cal Named Entity Recognition. In Proceedings 
of the ACL2002 Workshop on NLP in Biomedi-
cine. 
K. J. Lee, Y. S. Hwang and H. C. Rim. 2003. Two-
Phase Biomedical NE Recognition based on 
SVMs.  In Proceedings of the ACL2003 Work-
shop on NLP in Biomedicine. 
D. D. Lewis and J. Catlett. 1994. Heterogeneous 
Uncertainty Sampling for Supervised Learning. 
In Proceedings of ICML, 1994. 
A. McCallum and K. Nigam. 1998. Employing EM 
in Pool-Based Active Learning for Text Classi-
fication. In Proceedings of ICML, 1998. 
G. Ngai and D. Yarowsky. 2000. Rule Writing or 
Annotation: Cost-efficient Resource Usage for 
Base Noun Phrase Chunking. In Proceedings of 
ACL, 2000. 
T. Ohta, Y. Tateisi, J. Kim, H. Mima and J. Tsujii. 
2002. The GENIA corpus: An annotated re-
search abstract corpus in molecular biology do-
main. In Proceedings of HLT 2002. 
L. R. Rabiner, A. E. Rosenberg and S. E. Levinson. 
1978. Considerations in Dynamic Time Warping 
Algorithms for Discrete Word Recognition.  In 
Proceedings of IEEE Transactions on acoustics, 
speech and signal processing. Vol. ASSP-26, 
NO.6. 
D. Schohn and D. Cohn. 2000. Less is More: Ac-
tive Learning with Support Vector Machines. In 
Proceedings of the 17th International Confer-
ence on Machine Learning. 
D. Shen, J. Zhang, G. D. Zhou, J. Su and C. L. Tan. 
2003. Effective Adaptation of a Hidden Markov 
Model-based Named Entity Recognizer for Bio-
medical Domain. In Proceedings of the 
ACL2003 Workshop on NLP in Biomedicine. 
M. Steedman, R. Hwa, S. Clark, M. Osborne, A. 
Sarkar, J. Hockenmaier, P. Ruhlen, S. Baker and 
J. Crim. 2003. Example Selection for Bootstrap-
ping Statistical Parsers. In Proceedings of HLT-
NAACL, 2003. 
M. Tang, X. Luo and S. Roukos. 2002. Active 
Learning for Statistical Natural Language Pars-
ing. In Proceedings of the ACL 2002. 
C. A. Thompson, M. E. Califf and R. J. Mooney. 
1999. Active Learning for Natural Language 
Parsing and Information Extraction. In Proceed-
ings of ICML 1999. 
S. Tong and D. Koller. 2000. Support Vector Ma-
chine Active Learning with Applications to Text 
Classification. Journal of Machine Learning Re-
search. 
V. Vapnik. 1998. Statistical learning theory. 
N.Y.:John Wiley. 
 
Effective Adaptation of a Hidden Markov Model-based Named Entity 
Recognizer for Biomedical Domain 
Dan Shen?? Jie Zhang?? Guodong Zhou? Jian Su? Chew-Lim Tan?
? Institute for Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore 119613 
? Department of Computer Science 
National University of Singapore 
3 Science Drive 2, Singapore 117543 
{shendan,zhangjie,zhougd,sujian}@i2r.a-star.edu.sg 
{tancl}@comp.nus.edu.sg 
 
 
Abstract 
In this paper, we explore how to adapt a 
general Hidden Markov Model-based 
named entity recognizer effectively to 
biomedical domain.  We integrate various 
features, including simple deterministic 
features, morphological features, POS 
features and semantic trigger features, to 
capture various evidences especially for 
biomedical named entity and evaluate 
their contributions.  We also present a 
simple algorithm to solve the abbreviation 
problem and a rule-based method to deal 
with the cascaded phenomena in biomedi-
cal domain.  Our experiments on GENIA 
V3.0 and GENIA V1.1 achieve the 66.1 
and 62.5 F-measure respectively, which 
outperform the previous best published 
results by 8.1 F-measure when using the 
same training and testing data.  
1 Introduction 
As the research in biomedical domain has grown 
rapidly in recent years, a huge amount of nature 
language resources have been developed and be-
come a rich knowledge base.  The technique of 
named entity (NE) recognition (NER) is strongly 
demanded to be applied in biomedical domain.  
Since in previous work, many NER systems have 
been applied successfully in newswire domain 
(Zhou and Su 2002; Bikel et al 1999; Borthwich et 
al. 1999), more and more explorations have been 
done to port existing NER system into biomedical 
domain (Kazama et al 2002; Takeuchi et al 2002; 
Nobata et al 1999 and 2000; Collier et al 2000; 
Gaizauskas et al 2000; Fukuda et al 1998; Proux 
et al 1998).  However, compared with those in 
newswire domain, these systems haven?t got high 
performance.  It is probably because of the follow-
ing factors of biomedical NE (Zhang et al 2003): 
1. Some modifiers are often before basic NEs, 
e.g. activated B cell lines, and sometimes biomedi-
cal NEs are very long, e.g. 47 kDa sterol regula-
tory element binding factor.  This kind of factor 
highlights the difficulty for identifying the bound-
ary of NE. 
2. Two or more NEs share one head noun by 
using conjunction or disjunction construction, e.g. 
91 and 84 kDa proteins.  It is hard to identify these 
NEs respectively. 
3. An entity may be found with various spelling 
forms, e.g. N-acetylcysteine, N-acetyl-cysteine, 
NAcetylCysteine, etc.  Since the use of capitaliza-
tion is casual, the capitalization information may 
not be so evidential in this domain. 
4. NE may be cascaded.  One NE may be em-
bedded in another NE, e.g. <PROTEIN><DNA> 
kappa 3</DNA> binding factor </PROTEIN>.  
More effort must be made to identify this kind of 
NE. 
5. Abbreviations are frequently used in bio-
medical domain, e.g. TCEd, IFN, TPA, etc.  Since 
abbreviations don?t have many evidences for cer-
tain NE class, it is difficult to classify them cor-
rectly. 
These factors above make NER in biomedical 
domain difficult.  Therefore, it is necessary to ex-
plore more evidential features and more effective 
methods to cope with such difficulties. 
In this paper, we will study how to adapt a gen-
eral Hidden Markov Model (HMM)-based NE rec-
ognizer (Zhou and Su 2002) to biomedical domain.  
We specially explore various evidences for bio-
medical NE and propose methods to cope with ab-
breviations and cascaded phenomena.  As a result, 
features (simple deterministic features, morpho-
logical features, part-of-speech features and head 
noun trigger features) and methods (abbreviation 
recognition algorithm and rule-based cascaded 
phenomena resolution) are integrated in our system.  
The experiment shows that system outperforms the 
best published system by 8.1 F-measure. 
In Section 2, we will introduce the HMM-
based NE recognizer briefly.  In Section 3, we will 
focus on the features that we have used.  The 
methods and the adaptations of different features 
will be discussed in detail.  In Section 5 and 6, we 
will present the solutions of abbreviation and cas-
caded phenomena. Finally, our experiment results 
will be presented and the contributions of different 
features will be analyzed in Section 7. 
2 
3 
3.1 
HMM-based Named Entity Recognizer 
Our system is adapted from a HMM-based NE 
recognizer, which has been proved very effective 
in MUC (Zhou and Su 2002). 
The purpose of HMM is to find the most likely 
tag sequence T for a given sequence 
of tokens G  that maxi-
mizes . 
n
n ttt ???= 211
n gg ?= 211
)1
nG
ng??
|( 1
nTP
In token sequence G , the token g  is defined 
as , where w is the word and is 
the feature set related with the word . 
n
1 i
i
>=< iii wfg , i if
w
In tag sequenceT , each tag consists of three 
parts: 1. Boundary category, which denotes the 
position of the current word in NE.  2. Entity cate-
gory, which indicates the NE class.  3. Feature set, 
which will be discussed in Section 3. 
n
1 it
When we incorporate a plentiful feature set in 
HMM, we will encounter data sparseness problem.  
An alternative back-off modeling approach by 
means of constraint relaxation is applied in our 
model (Zhou and Su 2002).  It enables the decod-
ing process effectively find a near optimal fre-
quently occurred pattern entry in determining the 
NE tag probability distribution of current word. 
Finally, the Viterbi algorithm (Viterbi 1967) is 
implemented to find the most likely tag sequence 
in the state space of the possible tag distribution 
based on the state transition probabilities.  Fur-
thermore, some constraints on the boundary cate-
gory and entity category between two consecutive 
tags are applied to filter the invalid NE tags (Zhou 
and Su 2002). 
Feature Set 
Simple Deterministic Features (Fsd) 
The purpose of simple deterministic features is to 
capture the capitalization, digitalization and word 
formation information.  This kind of features have 
been widely used in both newswire NER system, 
such as (Zhou and Su 2002), and biomedical NER 
system, such as (Nobata et al 1999; Gaizauskas et 
al. 2000; Collier et al 2000; Takeuchi and Collier 
2002; Kazama et al 2002).  Based on the charac-
teristics of biomedical NEs, we designed simple 
deterministic features manually.  Table 1 shows the 
simple deterministic features with descending or-
der of priority. 
 
Fsd Name Example 
Comma , 
Dot . 
LRB ( 
RRB ) 
LSB [ 
RSB ] 
RomanDigit II 
GreekLetter Beta 
StopWord in, at 
ATCGsequence AACAAAG 
OneDigit 5 
AllDigits 60 
DigitCommaDigit 1,25 
DigitDotDigit 0.5 
OneCap T 
AllCaps CSF 
CapLowAlpha All 
CapMixAlpha IgM 
LowMixAlpha kDa 
AlphaDigitAlpha H2A 
AlphaDigit T4 
DigitAlphaDigit 6C2 
DigitAlpha 19D 
Table 1: Simple deterministic features 
From Table 1, we can find that: 
1. Features such as comma, dot, StopWord, etc. 
are designed intuitively to provide information to 
detect the boundary of NE. 
2. Features Parenthesis is often used to indicate 
the definition of abbreviation in biomedical docu-
ments. 
3. Features GreekLetter and RomanDigit are 
specially designed to capture the symbols 
frequently occurred in biomedical NE. 
4. Feature ATCG sequence identify the similar-
ity of words according to their word formations, 
e.g. AACAAAG, CTCAGGA, etc. 
5. Features dealing with mixed alphabets and 
digits such as AlphaDigitAlpha, CapMixAlpha, etc. 
are beneficial for biomedical abbreviations. 
Furthermore, we evaluate these features and 
compare with those used in MUC (Zhou and Su, 
2002).  The reported result of the simple determi-
nistic features used in MUC can achieve F-
measure of 74.1 (Zhou and Su 2002), but when 
they are used in biomedical domain, they only get 
F-measure of 24.3.  By contrast, using the simple 
deterministic features we designed for biomedical 
NER, the system achieves F-measure of 29.4.  Ac-
cording to the comparison, some findings may be 
concluded as follows: 
1) Simple deterministic features are domain de-
pendent, which suggests that it is necessary to de-
sign special features for biomedical NER. 
2) Simple deterministic features have weaker 
predictive power for NE classes in biomedical do-
main than in newswire domain. 
3.2 Morphological Feature (Fm) 
Morphological information, such as prefix/suffix, 
is considered as an important cue for terminology 
identification.  In our system, we get most frequent 
100 prefixes and suffixes from training data as 
candidates.  Then, each of these candidates is 
evaluated according to formula f1.  ( )
i
ii
i N
OUTIN
Wt
## ?=   (f1) 
in which, #INi is the number that prefix/suffix i 
occurs within NEs; #OUTi is the number that pre-
fix/suffix i occurs out of NEs; Ni is the total num-
ber of prefix/suffix i. 
The formula assumes that the particular pre-
fix/suffix, which is most likely inside NEs and 
least likely outside NEs, may be thought as a good 
evidence for distinguishing the NEs.  The candi-
dates with Wt above a certain threshold (0.7 in ex-
periment) are chosen.  Then, we calculated the 
frequency of each prefix/suffix in each NE class 
and group the prefixes/suffixes with the similar 
distribution among NE classes into one feature.  
This is because prefixes/suffixes with the similar 
distribution have the similar contribution, and it 
will avoid suffering from the data sparseness prob-
lem.  Some of morphological features were listed 
in Table 2. 
 
Fm Name Prefix/Suffix Example 
sOOC ~cin actinomycin 
 ~mide Cycloheximide 
 ~zole Sulphamethoxazole 
sLPD ~lipid Phospholipids 
 ~rogen Estrogen 
 ~vitamin dihydroxyvitamin 
sCTP ~blast erythroblast 
 ~cyte thymocyte 
 ~phil eosinophil 
sPEPT ~peptide neuropeptide 
sMA ~ma hybridoma 
sVIR ~virus cytomegalovirus 
Table 2: Examples of morphological features 
 
From Table 2, the suffixes ~cin, ~mide, ~zole 
have been grouped into one feature sOOC because 
they all have the high frequency in the NE class 
OtherOrganicCompound and relatively low fre-
quencies in the other NE classes.   In our system, 
totally 37 prefixes and suffixes were selected and 
grouped to 23 features. 
3.3 Part-of-Speech Features (Fpos) 
In the previous NER research in newswire domain, 
part-of-speech (POS) features were stated not use-
ful, as POS features may affect the use of some 
important capitalization information (Zhou and Su 
2002).  However, since more and more words with 
lower case are included in NEs, capitalization in-
formation in biomedical domain is not as eviden-
tial as it in newswire domain (Zhang et al 2003).  
Moreover, since many biomedical NEs are descrip-
tive and long, identifying NE boundary is not a 
trivial task.  POS tagging can provide the evidence 
of noun phrase region based on word syntactic in-
formation and the noun phrases are most likely to 
be NE.  Therefore, we reconsidered the POS tag-
ging.   
In previous research, (Kazama et al 2002) 
make use of POS information and conclude that it 
only slightly improves performance.  Moreover, 
(Collier et al 2000; Nobata et al 2000; Takeuchi 
and Collier. 2002) don?t incorporate POS informa-
tion in their systems.  The probable reason ex-
plained by them is that since POS tagger they used 
is trained on newswire articles, the assigned POS 
tags are often incorrect in biomedical documents.  
On the whole, it can be concluded that POS infor-
mation hasn?t been well used in previous work. 
In our experiment, a POS tagger was trained us-
ing 80% of GENIA V2.1 corpus (536 abstracts, 
123K words) and evaluated on the rest 20% (134 
abstracts, 29K words).  We use GENIA corpus to 
train the POS tagger in order to let it be adapted for 
biomedical domain.  As for comparison, we also 
trained the POS tagger on Wall Street Journal arti-
cles (2500 articles, 756K words) and tested on the 
20% of GENIA corpus.  The results are shown in 
Table 3. 
 
Training set Testing set Precision 
2500 WSJ articles 84.31 
536 GENIA abstracts 
134 GENIA 
abstracts 97.37 
Table 3: Comparison of POS tagger using dif-
ferent training data  
 
From Table 3, it can be found that POS tagger 
trained on the biomedical documents performs 
much better on the biomedical testing documents 
than that trained on WSJ articles.  This is consis-
tent with earlier explanation for why POS features 
are not so useful in biomedical NER (Nobata et al 
2000; Takeuchi and Collier 2002).   
3.4 Semantic Trigger Features 
Semantic trigger features are collected to capture 
the evidence of certain NE class based on the se-
mantic information of some key words.  Initially, 
we design two types of semantic triggers: head 
noun triggers and special verb triggers. 
3.4.1 Head Noun Triggers (Fhnt) 
Head noun means the main noun or noun phrase of 
some compound words and describes the function 
or the property, e.g. ?B cells? is the head noun for 
the NE ?activated human B cells?.  Compared with 
the other words in NE, head noun is a much more 
decisive factor for distinguishing NE classes.  For 
instance, 
<OtherName>IFN-gamma treatment</OtherName> 
<DNA>IFN-gamma activation sequence</DNA> 
In our work, we extract uni-gram and bi-grams 
of head nouns automatically from training data, 
and rank them by frequency.  According to the ex-
periment, we selected 60% top ranked head nouns 
as trigger features for each NE class.  Some exam-
ples are shown in Table 4. 
In the future application, we may also extract 
the head nouns from some public resources to en-
hance the triggers. 
 
1-gram 2-grams 
PROTEIN 
interleukin activator protein 
interferon binding protein 
kinase cell receptor 
ligand gene product 
CELL TYPE 
lymphocyte blast cell 
astrocyte blood lymphocyte 
eosinophil killer cell 
fibroblast peripheral monocyte 
DNA 
DNA X chromosome 
breakpoint alpha promoter 
cDNA binding motif 
chromosome promoter element 
Table 4: Examples of head noun triggers 
3.4.2 Special Verb Triggers (Fsvt) 
Besides collecting the triggers, such as head noun 
triggers, from the NEs themselves, we also extract 
the triggers from the local contexts of the NEs.  
Recently, some frequently occurred verbs in bio-
medical document have been proved useful for 
extracting the interaction between entities (Thomas 
et al 2000; Sekimizu et al 1998).  In biomedical 
NER, we have the intuition that particular verbs 
may also provide the evidence for boundary and 
NE class.  For instance, the verb bind is often used 
to indicate the interaction between proteins. 
In our system, we selected 20 most frequent 
verbs which occur adjacent to NE from training 
data automatically as the verb trigger features, 
which is shown in Table 5.   
 
 
Special Verb Triggers 
activate express 
bind induce 
inhibit interact 
regulate stimulate 
Table 5: Examples of special verb triggers 
4 Method for Abbreviation Recognition 
Abbreviations are widely used in biomedical do-
main.  Identifying the class of them constitutes an 
important and difficult problem (Zhang et al 2003). 
In our current system, we incorporate a method 
to classify abbreviation by mapping the abbrevia-
tion to its full form. This approach is based on the 
assumption that it is easier to classify the full form 
than abbreviation.  In most cases, this assumption 
is valid because the full form has more evidences 
than its abbreviation to capture the NE class.  
Moreover, if we can map the abbreviation to its 
full form in the current document, the recognized 
abbreviation is still helpful for classifying the same 
forthcoming abbreviations in the same document, 
as in (Zhou and Su 2002). 
In practice, abbreviation and its full form often 
occur simultaneously with parenthesis when first 
appear in biomedical documents.  There are two 
cases: 
1. full form (abbreviation) 
2. abbreviation (full form) 
Most patterns conform to the first case and if 
the content inside the parenthesis includes more 
than two words, the second case is assumed 
(Schwartz and Hearst 2003).   
In these two cases, the use of parenthesis is 
both evidential and confusing.  On one hand, it is 
evidential because it can provide the indication to 
map the abbreviation to its full form.  On the other 
hand, it is confusing because it makes the annota-
tion of NE more complicated.  Sometimes, the ab-
breviation and its full form are annotated 
separately, such as  
<CellType>human mononulear leuko-
cytes</CellType>(<CellType>hMNL</CellType>), 
and sometimes, they are all embedded in the whole 
entity, such as 
<OtherName>leukotriene B4 (LTB4) genera-
tion</OtherName>.   
Therefore, parenthesis needs to be treated specially.  
We develop an abbreviation recognition algorithm 
described in Figure 1. 
In preprocessing stage, we remove the abbre-
viations and parentheses from the sentence, when 
the abbreviation is first defined.  This measure will 
make the annotation simpler and the NE recognizer 
more effective.  The main work in this stage is to 
judge which case the current pattern belongs to and 
record the original positions of the abbreviation 
and parenthesis. 
After applying the HMM-based NE recognizer 
to the sentence, we restore the abbreviation and 
parenthesis to the original position in the sentence.  
Next, the abbreviation is classified.  There are two 
priorities of the class (from high to low): the class 
of its full form identified by the recognizer, and the 
class of the abbreviation itself identified by the 
recognizer.  At last, the same abbreviation occur-
ring in the rest sentences of the current document 
are assigned the same NE class.   
 
for each sentence Si in the document{ 
if exist parenthesis{ 
judge the case of { 
?full form (abbr.)?; 
?abbr. (full form)?; 
} 
store the abbr. A and position Pa  to a list; 
record the parenthesis position Pp; 
remove A and parenthesis from sentence; 
apply HMM-based NE recognizer to Si; 
restore A and parenthesis into Pa, Pp; 
if Pp within an identified NE E with the class CE 
parenthesis is included in E; 
else{ 
parenthesis is not included; 
   classify A to CE; 
   classify A in the rest part of document to CE; 
} 
} 
else apply HMM-based NE recognizer to Si; 
} 
Figure 1: Abbreviation recognition algorithm 
5 Solution of Cascaded Phenomena 
In (Zhang et al 2003), they state that 16.57% of 
NEs in GENIA V3.0 have cascaded annotations, 
such as  
<RNA><DNA>CIITA</DNA> mRNA</RNA>.   
Currently, we only consider the longest NE and 
ignore the embedded NEs.   
Based on the features described in section 3, 
our system counters some problems when dealing 
with cascaded NEs.  The probable reason is that 
the features we used are not so effective for this 
kind of NEs.   
For instance, POS is based on the assumption 
that NE is most likely to be a noun phrase.  For 
cascaded NE, this assumption may not always be 
valid because one NE may consist of two or more 
noun phrases connected by some special words, 
such as TSH receptor specific T cell lines. 
Moreover, in section 3.4.1, we have shown that 
head noun is the significant clue for distinguishing 
NE classes.  Even for cascaded NEs, head noun 
features are still effective to some extent, such as 
IL-2 mRNA.  However, cascaded NEs sometimes 
contain two or more head nouns, which belong to 
different NE classes.  For example, <DNA>IgG Fc 
receptor type IC gene</DNA>, in which receptor 
is the head noun of protein and gene is the head 
noun of DNA.  In general, the latter head noun will 
be more important.  Unfortunately, it seems that 
sometimes the shorter NE is more possible to be 
identified, such as <protein>IgG Fc recep-
tor</protein> type IC gene.   
On the whole, we have to explore an additional 
method to cope with the cascaded phenomena 
separately.  In our experiment, we attempt to solve 
this problem based on some rules. 
In GENIA corpus, we find that there are four 
basic types of cascaded NEs: 
1. < <NE> head noun >  
2. < modifier <NE> > 
3. < <NE1> <NE2> > 
4. < <NE1> word <NE2> > 
Moreover, these cascaded NEs may be generated 
iteratively.  For instance, 
5. < modifier <NE> head noun > 
6. < <NE1> <NE2> head noun > 
The rules are constructed automatically from 
the cascaded NEs in training data.  Corresponding 
to the four basic types of cascaded NEs mentioned 
before, we propose four patterns and apply them 
iteratively in each sentence: 
1. <entity1> head noun ? <entity2>  
e.g. <Protein> binding motif ? <DNA> 
2. <entity1> <entity2> ? <entity3> 
e.g. <Lipid> <Protein> ? <Protein> 
3. modifier <entity1> ? <entity2> 
e.g. anti <Protein> ? <Protein> 
4. <entity1> word <entity2> ? <entity3> 
e.g. <Virus> infected <Multicell> ? <Multicell> 
In our system, 102 rules are incorporated to 
classify the cascaded NEs. 
6 
6.1 
6.2 
Experiments 
GENIA Corpus 
GENIA corpus is the largest annotated corpus in 
molecular biology domain available to public 
(Ohta et al 2002).  In our experiment, three ver-
sions are used: 
? GENIA Version 1.1 (V1.1) -- It contains 670 
MEDLINE abstracts.  Since a lot of previous re-
lated work used this version, we use it to compare 
our result with others?. 
? GENIA Version 2.1 (V2.1) -- It contains the 
same 670 abstracts as V1.1 and POS tagging.  We 
use it to train and evaluate our POS tagger. 
? GENIA Version 3.0 (V3.0) -- It contains 2000 
abstracts, which is the superset of V1.1.  We use it 
to get the latest result and find out the effect of 
training data size. 
The annotation of NE is based on the GENIA 
ontology.  In our task, we use 23 distinct NE 
classes.  As for the conjunctive and disjunctive 
NEs, we ignore such cases and take the whole con-
struction as one entity.  In addition, for the cas-
caded annotations in V3.0, currently, we only 
consider the longest one level of the annotations. 
Experimental Results 
The system is evaluated using standard ?preci-
sion/recall/F-measure?, in which ?F-measure? is 
defined as F-measure = (2PR) / (P+R). 
We evaluate our NER system on both V3.0 and 
V1.1, each of which has been split into a training 
set and a testing set.  As for V1.1, we divide the 
corpus into 590 abstracts (136K words) as training 
set and the rest 80 abstracts (17K words) as testing 
set.  As for V3.0, we use the same testing set as 
V1.1 and the rest 1920 abstracts (447K words) as 
training set. 
 
Corpus P R F 
Our system on V3.0 66.5 65.7 66.1 
Our system on V1.1 63.8 61.3 62.5 
Kazama?s on V1.1 56.2 52.8 54.4 
Table 6: Comparison of overall performance 
 
Table 6 shows the overall performance of our 
system on V3.0 and V1.1, and the best reported 
system on V1.1 described in (Kazama et al 2002).  
On V1.1, we use the same training and testing data 
and capture the same NE classes as (Kazama et al 
2002).  Our system (62.5 F-measure) outperforms 
Kazama?s (54.4 F-measure) by 8.1 F-measure.  
This probably benefits from the various evidential 
features and the effective methods we proposed.  
Furthermore, as our expectation, the performance 
achieved on V3.0 (66.1 F-measure) is better than 
that on V1.1 (62.5 F-measure), which indicate that 
our system still has some room for improvement 
with the larger training data set. 
 
 
Figure 2: Performance of each NE class 
 
In addition, Figure 2 shows the detailed per-
formance chart of each NE class on V3.0.  In the 
figure, the numbers in the parenthesis are the num-
ber that NEs of that class occur in training/testing 
data.  It can be found that the performances vary a 
lot among the NE classes.  Some NE classes that 
have very few training data, such as Carbohydrate 
and Organism, get extremely low performance.  
In order to evaluate the contributions of differ-
ent features, we evaluate our system using different 
combinations of features (Table 7). 
From Table 7, several findings are concluded:  
1) With only Fsd, our system achieves a basic 
level F-measure of 29.4. 
2) Fm shows the positive effect with 2.4 F-
measure improvement based on the basic level.  
However, it only can slightly improve the perform-
ance (+1.2 F-measure) based on Fsd, Fpos and Fhnt.  
The probable reason is that the evidences included 
in Fm have already been captured by Fhnt.  More-
over, the evidences captured by Fhnt are more accu-
rate than that captured by Fm.  The contribution 
made by Fm may come from where there is no indi-
cation of Fhnt. 
 
Fsd Fm Fpos Fhnt Fsvt P R F 
?     42.4 22.5 29.4 
? ?    44.8 24.6 31.8 
? ? ?   58.3 50.9 54.3 
?  ? ?  62.0 61.6 61.8 
? ? ? ?  64.4 61.7 63.0 
? ? ? ? ? 60.6 59.3 60.0 
Table 7: Effects of different features on V3.0 
 
3) Fpos is proved very beneficial as it makes 
great increase on F-measure (+22.5) based on Fsd 
and Fm.   
4) Fhnt leads to an improvement of 8.7 F-
measure based on Fsd, Fm and Fpos. 
5) Out of our expectation, the use of Fsvt de-
creases both precision and recall, which may be 
explained as the present and past participles of 
some special verbs often play the adjective-like 
roles inside biomedical NEs, such as IL10-
inhibited lymphocytes.  
 
 P R F 
Fsd+Fm+Fpos+Fhnt 64.4 61.7 63.0 
+abbr. recog. algorithm 64.6 62.5 63.5 
+rule-based casc. method 66.2 65.8 66.0 
+both 66.5 65.7 66.1 
Table 8: Effects of solution for abbr. and casc. 
 
From Table 8, it can be found that the abbrevia-
tion recognition method slightly improves the per-
formance by 0.5 F-measure.  The probable reason 
is that the recognition of abbreviation relies too 
much on the recognition of its full form.  Once the 
full form is wrongly classified, the abbreviation 
and the forthcoming ones throughout the document 
are wrong altogether.  In the near future, the pre-
defined abbreviation dictionary may be incorpo-
rated to enhance the decision of NE class. 
Moreover, it can be found that the rule-based 
method effectively solves the problem of cascaded 
phenomena and shows prominent improvement 
(+3.0 F-measure) based on the performance of 
?Fsd+Fm+Fpos+Fhnt?. 
7 Conclusion 
In the paper, we describe our exploration on how 
to adapt a general HMM-based named entity rec-
ognizer to biomedical domain.  We integrate vari-
ous evidences for biomedical NER, including 
lexical, morphological, syntactic and semantic in-
formation.  Furthermore, we present a simple algo-
rithm to solve the abbreviation problem and a rule-
based method to deal with the cascaded phenom-
ena. Based on such evidences and methods, our 
system is successfully adapted to biomedical do-
main and achieves significantly better performance 
than the best published system.  In the near future, 
more effective abbreviation recognition algorithm 
and some pre-defined NE lists for some classes 
may be incorporated to enhance our system. 
Acknowledgements 
We would like to thank Mr. Tan Soon Heng for his 
support of biomedical knowledge.   
References 
M. Bikel Danie, R.Schwartz and M. Weischedel Ralph. 
1999.  An Algorithm that Learns What's in a Name. 
In Proc. of Machine Learning (Special Issue on NLP). 
A. Borthwick.  1999.  A Maximum Entropy Approach 
to Named Entity Recognition. Ph.D. Thesis. New 
York University. 
N. Collier, C. Nobata, and J. Tsujii.  2000.  Extracting 
the names of genes and gene products with a hidden 
Markov model.  In Proc. of COLING 2000, pages 
201-207. 
K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi.  
1998.  Toward information extraction: identifying 
protein names from biological papers.  In Proc. of the 
Pacific Symposium on Biocomputing?98 (PSB?98), 
pages 707-718, January. 
R. Gaizauskas, G. Demetriou and K. Humphreys.  Term 
Recognition and Classification in Biological Science 
Journal Articles.  2000.  In Proc. of the Computional 
Terminology for Medical and Biological Applications 
Workshop of the 2nd International Conference on 
NLP, pages 37-44. 
J. Kazama, T. Makino, Y.Ohta, and J. Tsujii.  2002.  
Tuning Support Vector Machines for Biomedical 
Named Entity Recognition.  In Proc. of the Work-
shop on Natural Language Processing in the Bio-
medical Domain (at ACL?2002), pages 1-8. 
C. Nobata, N. Collier, and J. Tsujii.  1999.  Automatic 
term identification and classification in biology texts.  
In Proc. of the 5th NLPRS, pages 369-374. 
C. Nobata, N. Collier, and J. Tsujii.  2000.  Comparison 
between tagged corpora for the named entity task.  In 
Proc. of the Workshop on Comparing Corpora (at 
ACL?2000), pages 20-27. 
T. Ohta, Y. Tateisi, J. Kim, H. Mima, and J. Tsujii.  
2002.  The GENIA corpus: An annotated research 
abstract corpus in molecular biology domain.  In 
Proc. of HLT 2002. 
D. Proux, F. Rechenmann, L. Julliard, V. Pillet and B. 
Jacq.  1998.  Detecting Gene Symbols and Names in 
Biological Texts: A First Step toward Pertinent In-
formation Extraction.  In Proc. of Genome Inform 
Ser Workshop Genome Inform, pages 72-80. 
A.S. Schwartz and M.A. Hearst.  2003.  A Simple Algo-
rithm for Identifying Abbreviation Definitions in 
Biomedical Text.  In Proc. of the Pacific Symposium 
on Biocomputing (PSB 2003) Kauai. 
T. Sekimizu, H. Park, and J. Tsujii.  1998.  Identifying 
the interaction between genes and gene products 
based on frequently seen verbs in medline abstracts.  
In Proc. of Genome Informatics, Universal Academy 
Press, Inc.  
K. Takeuchi and N. Collier.  2002.  Use of Support Vec-
tor Machines in Extended Named Entity Recognition.  
In Proc. of the Sixth Conference on Natural Lan-
guage Learning (CONLL 2002), pages 119-125. 
J. Thomas, D. Milward, C. Ouzounis, S. Pulman, and M. 
Carroll.  2000.  Automatic extraction of protein inter-
actions from scientific abstracts.  In Proc. of the Pa-
cific Symposium on Biocomputing?2000 (PSB?2000), 
pages 541-551, Hawaii, January. 
A. J. Viterbi.  1967.  Error bounds for convolutional 
codes and an asymptotically optimum decoding algo-
rithm.  In Proc. of IEEE Transactions on Information 
Theory, pages 260-269. 
J. Zhang, D. Shen, G. Zhou, J. Su and C. Tan. 2003.  
Exploring Various Evidences for Recognition of 
Named Entities in Biomedical Domain.  Submitted to 
EMNLP 2003. 
G. Zhou and J. Su.  2002.  Named Entity Recognition 
using an HMM-based Chunk Tagger.  In Proc. of the 
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 473-480. 
