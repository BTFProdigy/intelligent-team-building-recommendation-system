Chart-Based Transfer Rule Application in Machine Translation 
Adam Meyers 
New York University 
meyers@cs.nyu.edu 
Mich iko  Kosaka 
Monlnouth University 
kosaka@monmouth.edu 
Ralph GrishInan 
New York University 
gr ishman@cs.nyu.edu 
Abstract 
35"ansfer-based Machine Translation systems re- 
quire a procedure for choosing the set; of transfer 
rules for generating a target language transla- 
tion from a given source language sentence. In 
an MT system with many comI)eting transfer 
rules, choosing t;he best, set of transfer ules for 
translation may involve the evaluation of an ex- 
plosive number of competing wets. We propose a
sohltion t;o this problem l)ased on current best- 
first chart parsing algorithms. 
1 Introduct ion 
ri~'ansfer-based Machine 'Kanslation systenls re- 
quire a procedure for choosing the set of trans- 
tier rules for generating a target  language I;rans- 
lation from a given source language sentence. 
This procedure is trivial tbr a system if, given 
a (:ontext, one transtb.r ule. can l)e selected un- 
~mfl)iguously. O|;herwise, choosing the besl; set; 
of transfer ules may involve the. evaluation of 
mmmrous competing sets. In fact, the number 
of l)ossible transfer ule combinations increas- 
es exponentially with the length of the source, 
language sentence,. This situation mirrors the 
t)roblem of choosing productions in a nondeter- 
ministic parser, in this paI)er, we descril)e a 
system for choosing transfer ules, based on s- 
tatistical chart parsing (Bol)row, 1990; Chitrao 
and Grishman, 1990; Caraballo and Charniak, 
1997; Charniak et al, 1998). 
In our Machine %'anslation system, transfer 
rules are generated automatically from parsed 
parallel text along the lines of (Matsulnoto el; 
al,, 1993; Meyers et al, 1996; Meyers et al, 
1998b). Our system tends to acquire a large 
nmnber of transt~r rules, due lnainly to 3,1terna- 
tive ways of translating the same sequences of 
words, non-literal translations in parallel text 
and parsing e, rrors. It is therefore crucial that 
our system choose the best set of rules efficient- 
ly. While the technique discussed he.re obviously 
applies to similar such systems, it could also ap- 
ply to hand-coded systems in which each word 
or group of words is related to more than one 
transfer ule. D)r example, both Multra (Hein, 
1996) and the Eurotra system described in (Way 
el; al., 1997) require components for deciding 
which combination of transtbr ules to use. The 
proi).osed technique may 1)e used with syst;ems 
like these, t)rovided that all transfer ules are as- 
signed initial scores rating thcqr at)propriateness 
for translation. These al)t)rol)riateness ratings 
couhl be dependent or independent of context. 
2 Previous Work 
The MT literature deserib(;s everal techniques 
tbr deriving the appropriate translation. Statis- 
tical systems l;hal; do not incorporate linguistic 
analysis (Brown el: al., 1993) typically choose 
the most likely translation based on a statis- 
tical mode.l, i.e.., translation probability deter- 
mines the translation. (Hein, 1996) reports a set; 
of (hand-coded) fea|;llre structure based prefi~r- 
ence rules to choose among alternatives in Mu\]- 
tra. There is some discussion about adding 
some transtbr ules automatically acquired flom 
corpora to Multra? Assuming that they over- 
generate rules (as we did), a system like the one 
we propose should 1)e beneficial. In (Way et al, 
1997), many ditDrent criteria are used to dloose 
trmlsi~;r ules to execute including: pretbrmlces 
for specific rules over general ones, and comt)lex 
rule nol, ation that insures that tb.w rules can 21)- 
ply to the same set, of words. 
The Pangloss Mark III system (Nirenburg 
~This translatioll procedm'e would probably comple- 
menI~ not; replace exist, ing procedures in these systelns. 
2http : / / s tp .  l i ng .  uu. se /~corpora /p lug / repor t  s / 
ansk_last/ is a report on this 1)reject; for Multra. 
537 
and Frederking, 1995) uses a chart-walk algo- 
rithm to combine the results of three MT en- 
gines: an example-based ngine, a knowledge- 
based engine, and a lexical-transfer engine. 
Each engine contributes its best edges and tile 
chart-walk algorithm uses dynamic program- 
ruing to find the combination of edges with the 
best overall score that covers the input string. 
Scores of edges are normalized so that the scores 
fi'om the different engines are comparable and 
weighted to favor engines which tend to produce 
better results. Pangloss's algorithm combines 
whole MT systems. In contrast, our algorith- 
m combines output of individual transfer ules 
within a single MT system. Also, we use a best- 
first search that incorporates a probabilistic- 
based figure of merit, whereas Pangloss uses an 
empirically based weighting scheme and what 
appears to be a top-down search. 
Best-first probabilistic chart parsers (Bo- 
brow, 1990; Chitrao and Grishman, 1990; Cara- 
ballo and Charniak, 1997; Charniak et al, 1998) 
strive to find the best parse, without exhaus- 
tively trying a l l  possible productions. A proba- 
bilistic figure of merit (Caraballo and Charniak, 
1997; Charniak et al, 1998) is devised for rank- 
ing edges. The highest ranking edges are pur- 
sued first and the parser halts after it produces 
a complete parse. We propose an algorithm for 
choosing and applying transthr ules based on 
probability. Each final translation is derived 
from a specific set of transfer ules. If the pro- 
cedure immediately selected these transfer rules 
and applied them in tile correct order, we would 
arrive at tile final translation while creating the 
minimum number of edges. Our procedure uses 
about 4 tinms this minimum number of edges. 
With respect o chart parsing, (Charniak et al, 
1998) report that their parser can achieve good 
results while producing about three times tile 
mininmm number of edges required to produce 
the final parse. 
3 Test  Data  
We conducted two experiments. For experimen- 
t1, we parsed a sentence-aligned pair of Span- 
ish and English corpora, each containing 1155 
sentences of Microsoft Excel Help Text. These 
pairs of parsed sentences were divided into dis- 
tinct training and test sets, ninety percent for 
training and ten percent fbr test. The training 
Source Tree Target Tree 
D = vo lvcr  D' = reca lcu late  
s,,I,J  
A = Exce l  E = ca lcu la r  
Obj~en A' = Excel I C' = workbook 
B' = va lues  
/ C = l ibro 
k , 
B =va lores  \ae  
F = t raba jo  
Excel vuelve a calcular Excel recalculates 
valores en libro de trabajo values iu workbook 
Figure 1: Spanish and English I{egularized 
Parse 2?ees 
set was used to acquire transfer ules (Meyers 
et al, 1998b) which were then used to translate 
tile sentences in tile test set. This paper focus- 
es on our technique for applying these transfer 
rules in order to translate the test sentences. 
The test and training sets in experiment1 
were rotated, assigning a different enth of the 
sentences to the test set in each rotation. In this 
ww we tested tile program on the entire corpus. 
Only one test set (one tenth of the corpus) was 
used for tuning the system (luring development. 
~:ansfer rules, 11.09 on average, were acquired 
t'rom each training set and used for translation 
of the corresponding test set. For Experiment 
2, we parsed 2617 pairs of aligned sentences and 
used the same rotation procedure for dividing 
test and training corpora. The Experiment 2
corpus included the experinlentl corpus. An av- 
erage of 2191 transfer ules were acquired from 
a given set of Experinmnt 2 training sentences. 
Experimentl isorchestrated in a carefld man- 
ner that may not be practical for extremely 
large corpora, and Experiment 2 shows how the 
program performs if we scale up and elilniuate 
some of the fine-tuning. Apart from corpus size, 
there are two main difference between the two 
experiments: (1) the experimentl corpus was 
aligned completely by hand, whereas the Exper- 
iment 2 corpus was aligned automatically using 
the system described ill (Meyers et al, 1998a); 
and (2) the parsers were tuned to the experi- 
mentl sentences, but not the Experiment 2 sen- 
tences (that did not overlap with experinmntl). 
538 
1) A = Excel  
2) B =va lores  
C = l ibro 
v 
A' = Excel 
B'  = values 
.~) 
r 
C' = workbook  
F = trabajo 
D = volvcr S.IJ.i~ 
4) 
1 E = ealcular  
O b. \ ]~en l 
2 3 
1)' = recalculate 
1 2 3 
Figure 2: A S('t of %-ansfer Rules 
4 Parses  and  Trans fer  Ru les  
Figure 1 is a pair of "regularized" parses t br a 
corresi)onding pair of Spanish and Fmglish sen- 
tences fi'om Microsoft Excel hell) text. These 
at'(; F-structure-like dependency analyses of sen- 
tences that represent 1)redicate argument struc- 
ture. This representation serves to neutralize 
some ditfbrences between related sentence tyt)es, 
e.g., the regularized parse of related active and 
t)a,~sive senten(:es are identical, except tbr the 
{i'.ature value pair {Mood, Passive}. Nodes (wfl- 
ues) are labeled with head words and arcs (fea- 
tures) are labeled with gramma~;ical thnetions 
(subject, object), 1)repositions (in) and subor- 
dinate conjunctions (beNre). a For demonstra- 
tion purposes, the source tree in Figure 1 is the 
input to our translation system and the target 
tree is the outl)ut. 
The t;ransfer rules in Figure 2 can be 
used to convert the intmt; tree into the out- 
1)at tree. These transtbr rules are pairs of 
corresponding rooted substructures, where a 
substructure (Matsumoto et al, 1993) is a 
connected set of arcs and nodes. A rule 
aMorphologieal features and their values (Gram- 
Number: plural) are also represented as ares and nodes. 
consists of o, ither a pair of "open" substructures 
(rule 4) or a pair of "closed" substructures (rules 
1, 2 and 3). Closed substructures consist of s- 
ingle nodes (A,A',B,B',C') or subtrees (the left 
hand side of rule 3). Open substructures con- 
tain one or more open arcs, arcs without heads 
(both sul)structures in rule 4). 
5 Simplif ied Translat ion with 
Tree-based Transfer Rules 
The rules in Figure 2 could combine by filling 
in the open arcs in rule 4 with the roots of the 
substructures in rules 1, 2 and 3. The result 
would be a closed edge which maps the left; tree 
in l,'igure, 1 into the right tree. Just as edges of a 
chart parser are based on the context free rules 
used by the chart parser, edges of our trans- 
lation system are, based on these trans~L'r ules. 
Initial edges are identical to transtb, r rules. Oth- 
er edges result from combining one closed edge 
with one open edge. Figure 3 lists the sequence 
of edges which wouhl result from combining the 
initial edges based (m Rules 1-4 to replicate, the 
trees in Figure 1. The translation proceeds by 
incrementally matching the left hand sides of 
Rules 1-4 with the intmt tree (and insuring that 
the tree is completely covered by these rules). 
The right-hand sides of these comt)atil)le rules 
are also (:ombined t;o 1)reduce the translal;iolL 
This is an idealized view of our system in which 
each node in the input tree matches the left;- 
hand side of exactly one transfer rule: there is 
no ambiguity and no combinatorial explosion. 
The reality is that more than one transfer ules 
may be activated tbr each node, as suggested 
in Figure 4. 4 If each of the six nodes of the 
source tree corresponded to five transfer rules, 
there are 56 = 15625 possible combinations of 
rules to consider. To produce t lm output  in Fig- 
ure 3, a minimum of seven edges would be re- 
quired: four initial edges derived ti'om the o- 
riginal transfer ules plus three additional edges 
representing the combination of edges (steps 2, 
3 and 4 in Figure 3). The speed of our system is 
measured by the number of actual edges divided 
by this minimuln. 
4The third example listed would actually involve two 
trm~sfer rules, one translating "volver" to "ret)cat" and 
the second translating "calcular" to "calculal;e". 
539 
1) 
2) 
D = vo lver  
S u ~  
1 E = ca lcu la r  
Obj~n 
2 3 
D = vo lver  
A = Exce l  E = ca lcu la r  
Obj~n 
2 3 
v 
v 
D' = reca lcu la te  
I 2 3 
D' = reca lcu la te  
A' = Excel 2 3 
3) 
D = volver 
A = Exce l  E = ca leu la r  
B = valores 3 
D'  = reca lcu la te  
A' = Excel / 3 
g' = values 
4) 
D = volver 
A = Excel E = ca lcu la r  
Ob/~n 
B = va io res  C = l ib ro  
de 
F = t raba jo  
v 
D'  = reca lcu la te  
A' = Excel \ C' = workbook 
B' = va lues  
Figure 3: An Idealized Translation Procedure 
6 Best  F i r s t  T rans la t ion  Procedure  
The following is an outline of our best first 
search procedure for finding a single translation: 
1. For each node N, find TN, the set of com- 
patible transfer ules 
2. Create initial edges for all TN 
3. Repeat until a "finished" edge is tbund or 
an edge limit is reached: 
(a) Find the highest scoring edge E 
(b) If complete, combine E with compati- 
ble incoml)lete dges 
(c) If incomplete, combine E with com- 
patible complete dges 
(d) Incomplete dge + complete edge = 
new edge 
The procedure creates one initial edge 
for each matching transfer rule in the 
database 5 and puts these edges in a 
'~The left-hand side of a matching transfer rule is com- 
patible with a substructure in the input source tree. 
540 
D'  = recalculate 
D = velvet 1 2 3 / %  
Sub, i / ~ a !)' = calculate 
/ \ 
/ E = \ '4 .  '+" 
3 again 
D = repeat 
Sabj ~ b j  
1 E = calculation 
Figure 4: Multiple \[lYansfer Rules for Each Sub- 
structm:e 
queue prioritized by score. The pro- 
cedure iteratively combines the best 
s(:oring edge with some other comt)al;ilfle 
edge to t)roduce a new edge. and inserts the new 
edge in the queu('.. The score for each new edge 
is a function of the scores of the edges used to 
produce it:. The process contimms m~til either 
an edge limit is reache(l (the system looks like 
it; will take too long to terminate) or a complete 
edge is t)roduced whose left-hand side is the 
input tree: we (:all this edge a "finished edge". 
We use the tbllowing technique for calculating 
the score tbr initial edges. 6 The score tbr each 
initial edge E rooted at N, based on rule/~, is 
calculated as follows: 
1. SCO17.F=I(S) " " F,.c.,~(n) = ~'?.q'~D~(~a  ~t N~) 
Where the fl'equency (Freq) of a rule is the 
nmnber of times it matched an exmnple in 
the training corpus, during rule ~cquisition. 
The denominator is the combined fl'equen- 
cies of all rules that match N. 
aThis is somewhat det)cndent on the way these |;rans- 
fer rules are derived. Other systems would t)robably have 
to use some other scoring system. 
Ezperiment 1:1155 sentences 
Norm No Norm 
Total Translations 
Over Edge Limit 
Actual Edges 
Miniature Edges 
Edge Ratio 
Accuracy 
1153 
2 
93,719 
22,125 
3.3 
70.9 
1127 
28 
579,278 
20,125 
1.4.8 
70.9 
Ezpcriment 2:2617 sentences 
Norm No Norm 
Total Translations 
Over Edge Limit 
Actual Edges 
Minimum Edges 
Edge Ratio 
A(:curacy 
2610 
7 
262,172 
48,570 
4.0 
62.6 
2544 
73 
1,398,796 
42,770 
15.5 
61.5 
Figure 5: Result:s 
2, S s ) = s ,o,.(;.l ( S ) - No,., , , ,  
Where the Norm (normalization) t~ctor is 
equal to the highest SCORE1 for any rule 
matching N. 
Since the log.2 of probabilities are necessarily 
negative, this has the effect of setting the E of 
each of the most t)rol)able initial edges to zero. 
The scores tbr non-initial edges are calculated 
by ad(ling u I) the scores of the initial e(tges of 
which they are comt)osed. 7 
Without any normMization (Score(S) = 
SCORE1 (,9)), small trees are favored over large 
trees. This slows down the process of finding the 
final result. The normalization we use insures 
that the most probable set; of transihr ules are 
considered early on. 
7 Resu l ts  
Figure 5 gives our results for both experiments 
1 and 2, both with normalization (Norm) and 
without (No Norm). "Total Translations" refer 
to the number of sen|;ences which were translat- 
ed successfully 1)y the system and "Over Edge 
Limit" refers to the numl)er of sentences which 
caused the system to exceed the edge limit, i.e., 
once the system produces over 10,000 edges, 
trm~slation failure is assmned. The system cur- 
7Scoring for special cases is not; included in this paper. 
These cases include rules for conjunctions and rules ibr 
words that do not match any transfer ules in a given 
context (we currently leave the word untranslated.) 
541 
rently will only fail to produce some transla- 
tion for any input if the edge limit is exceed- 
ed. "Actual Edges" reibrs to the total number 
of edges used tbr attempting to translate very 
sentence in the corpus. "Minimum Edges" refer 
to the total minimum number of edges required 
for successful translations. The "Edge Ratio" 
is a ratio between: (1) "Total Edges" less the 
mnnber of edges used in failed translations; and 
(2) The "Minimum Edges". This ratio, in com- 
l)ination with, the number of "Over Edge Limit" 
measures the efficiency of a given system. "Ac- 
curacy" is an assessment of translation quality 
which we will discuss in the next section. 
Normalization caused significant speed-up for 
both experiments. If you compare the total 
number of edges used with and without nor- 
malization, speed-up is a factor of 6.2 for Ex- 
periment I and 5.3 for Experiment 2. If you 
compare actual edge ratios, speed-up is a factor 
of 4:.5 tbr Experiment 1 and 3.9 tbr Experiment 
2. In addition, the number of failed parses went 
down by a fhctor of 10 for both experiments. As 
should be expected, accuracy was virtually the 
same with and without normalization, although 
normalization <lid cause a slight improvemen- 
t. Normalization should produce the essentially 
the same result in less time. 
These results suggest that we can probably 
count on a speed-up of at least 4 and a signif 
icant decline in failed parses by using normM- 
ization. The ditferences in performance on the 
two corpora are most likely due to the degree of 
hand-tuning for Experiment 1. 
7.1 Our  Accuracy  Measure  
"Accuracy" in Figure 5 is the average of the 
tbllowing score for each translated sentence: 
ITNYu ~ TMSI 
1/2 x (ITNYuI + ITMsl) 
TNZU is the set of words in NYU's translation 
and TMS is the set of words in the original Mi- 
crosoft translation. If TNYU = "A B C D E" 
and TMS = "A B C F", then the intersection 
set "A B C" is length 3 (the numerator) and 
the average length of TNZU and TMS is 4 1/2 
(the denominator). The accuracy score equals 
3 + 4 1/2 = 2/3. This is a Dice coefficient com- 
parison of our translation with the original. It is 
an inexpensive nmthod of measuring the pertbr- 
mance of a new version of our system, hnprove- 
ments in the average accuracy score for our san> 
ple set; of sentences usually reflect an improve- 
ment in overall translation quality. While it is 
significant hat the accuracy scores in Figure 5 
did not go down when we normalized the scores, 
the slight improvement in accuracy should not 
be given nmch weight. Our accuracy score is 
flawed in that it cannot account for the follow- 
ing facts: (1) good paraphrases are perfectly ac- 
ceptable; (2) some diflbrences in word selection 
are more significant han others; and (3) errors 
in syntax are not directly accounted tbr. 
NYU's system translates the Spanish sen- 
tence "1. Selection la celda en la que desea 
introducir una rethrencia" as "1. select the cel- 
l that you want to enter a reference in". Mi- 
crosoft translates this sentence as "1. Select the 
cell in which you want; to enter the reference". 
Our system gives NYU's translation an accu- 
racy score of .75 due to the degree of overlap 
with Microsoft's translation. A truman reviewer 
wouhl probably rate NYU's translation as com- 
pletely acceptable. In contrast, NYU's system 
produced the following unacceptable translation 
which also received a score of .75: the Spanish 
sentence "Elija la funcidn que desea pegar en la 
f6rmula en el cuadro de di~logo Asistente para 
flmciones" is translated as " "Choose the flmc- 
tion that wants to paste Function Wizard in the 
formula in the dialog box", in contr,~st with Mi- 
crosoft's translation "Choose the flmction you 
want to paste into the tbrmula fl'om the Func- 
tion Wizard dialog box". In fact, some good 
translations will get worse scores than some 
bad ones, e.g., an acceptable one word trans- 
lation can even get a score of 0, e.g.,"SUPR" 
was translated as "DEL" by Microsoft and as 
"Delete" by NYU. Nevertheless, by averaging 
this accuracy score over many examples, it has 
proved a valuable measure for comparing differ- 
ent versions of a particular system: better sys- 
tems get better results. Similarly, after tweak- 
ing the system, a better translation of a partic- 
ular sentence will usually yield a better score. 
8 Future  Work  
Fnture work should address two limitations of 
our current system: (1) Bad parses yield bad 
transihr rules; and (2) sparse data limits the size 
of our transfer rule database and our options for 
542 
applying transfer ules selectively. To nttack the 
"bad parse" problem, we are eonsideriug using 
our MT system with less-detailed parsers, since 
these parsers typically produce less error-prone 
output. We will have to conduct exl)erimcnts 
to determine the minimum level of detM1 that 
is needed, a 
Previous to the work reported in this paper, 
we ran our MT system on bilinguM corpora in 
which the sentences were Migned manuMly. The 
cost of manuM aligmnent limited the size of the 
corpora we could use. A lot of our recent MT 
research as bo.en tbcused on solving this sparse 
data prol)lem through our develoi)ment of a sen- 
tence alignment progrmn (Meyers et al, 1998a). 
We now have 300,000 automaticMly aligned sen- 
tences in the Microsoft help text domain tbr fu- 
ture experiineni;s. In addition to provi(ting us 
with many more transfer ules, this shouhl Mlow 
us to colh'.ct transfer rule co-occurrence infor- 
mation which we c~m then use to apply tr;mstbr 
rules more effectively, perhaps improving trans- 
b~tion quality. In a preliminary experime, nt a- 
hmg these lines using the Experiment 1. tort)us, 
co-occurrence information had no noticeable f  
feet. However, we are hot)eflfl that flltm'e ex- 
t)eriments with 300,000 Migned sentences (300 
tinies as nnlch data) will 1)e more successful. 
Re ferences  
Robert J. Bobrow. 1990. S1;~Ltistical agenda 
parsing. In I)ARPA Speech and Lang'uagc 
Workshop, pages 222-224. 
Peter Brown~ Stephen A. Delb~ t)ietra, Vin- 
cent J. Della Pietra, and Robert L. Mer- 
cer. 1993. The Mathematics of Statistical 
M~zchine 'h'anslation: 1)arametcr Estimation. 
Computational Lin.quistics, 19:263-312. 
Sh;~ron A. Caraballo and Eugene Chm'niak. 
1997. New figures of merit tbr best-tirst prot)- 
M)ilistie chart parsing. Computational Lin- 
guistics, 24:275-298. 
Eugene Ctmrniak, Sharon Goldwater, and M~rk 
Johnson. 1998. Edge-Based Best-First Chart 
Parsing. In Proceedings of the Sixth Annual 
Workshop for Very Lawc Corpora, Montreal. 
SOne could set u 1) a contimmm from detailed parser- 
s like Proteus down to shallow verb-group/noun-grouI) 
recognizers, with the Penn treetmnk based parsers ly- 
ing somewhere in the middle. As one travels down t, he 
eonLinlmIn t;o t;he lower detail parsers, tim error rate nat- 
urally decreases. 
Mahesh V. Chitrao and RMph Gris}unan. 1990. 
St;~tisti('al pnrsing of messages. In \])AIIPA 
Speech and La'n,g'uagc Workshop, pages 263 
266. 
Annn Sggvall ltein. 1996. Pretbrence Mecha- 
nisms of the Multra Machine %'ansb~tion Sys- 
tem. In Barbara H. Partee and Petr Sgall, 
editors, Discourse and Meaning: Papers in 
11onor of Eva 11aji~ovd. John Benja.mins Pub- 
lishing Company, Amsterdam. 
Y. Matsumoto, H. Ishimoto, T. Utsuro, and 
M. Nagao. 1993. Structural Matching of 
Parallel Texts. In 31st Annual Meeting of 
the Association for Computational Linguis- 
tics: "Proceedings of the Uo~@rencc". 
Adam Meyers, Roman Ymlgm'ber, a.nd Ralph 
Grishman. 1996. Alignment of Shared 
Forests fi)r BilinguM Corpora. In Proceedings 
of Coliw.I 1996: The 16th International Con- 
fercncc on Computational Linguistics, l)ages 
460 465. 
Adam Meyers, Miehiko Kosak~, and Ralph Gr- 
ishman. 1998m A Multilingual Procedure 
for Dict;ionary-B;~sed Sentence Aligmnent. In 
Proceedings of AMTA '98: Machine Transla- 
tion and th, c ht:fo'rmation Soup, t)~ges 187. 
198. 
Adam Meyers, R,om~m Ym~g~rber, Ralph Gr- 
ishmml, Cntherine Macleod, mM Antonio 
Moreno-S~mdow~l. 1998|). l)eriving ~l~:a.ns- 
fin: Rules from Domimmce-Preserving Align- 
ments. In I)'rocccdim.ls o.f Coling-A CL98: Th.c 
171h International Conference on Computa- 
tional Ling,uistics and the 36th, Meeting of the 
Association for Computational Linguistics. 
Sergei Nirenlmrg mM Robert E. l~:ederking. 
1995. The Pangloss Mark III Machine 'l?nms- 
lt~tion System: Multi-Engine System Archi- 
tecture. Te(:hnical report, NMSU Oil,L, USC 
ISI, ;rod CMU CMT. 
Andrew Way, Ian Crookston, and Jane Shell;on. 
1997. A Typology of ~IYanslation Prol)lems 
for Eurotra Translation Machines. Machine 
\[l}'anslation, 12:323 374. 
543 
 1 
Discriminative Slot Detection Using Kernel Methods 
Shubin Zhao, Adam Meyers, Ralph Grishman 
Department of Computer Science 
New York University 
715 Broadway, New York, NY 10003 
shubinz, meyers, grishman@cs.nyu.edu 
 
Abstract 
Most traditional information extraction 
approaches are generative models that assume 
events exist in text in certain patterns and these 
patterns can be regenerated in various ways. 
These assumptions limited the syntactic clues 
being considered for finding an event and 
confined these approaches to a particular 
syntactic level. This paper presents a 
discriminative framework based on kernel SVMs 
that takes into account different levels of 
syntactic information and automatically 
identifies the appropriate clues. Kernels are used 
to represent certain levels of syntactic structure 
and can be combined in principled ways as input 
for an SVM. We will show that by combining a 
low level sequence kernel with a high level 
kernel on a GLARF dependency graph, the new 
approach outperformed a good rule-based 
system on slot filler detection for MUC-6. 
1 Introduction 
The goal of Information Extraction (IE) is to 
extract structured facts of interest from text and 
present them in databases or templates. Much of 
the IE research was promoted by the US 
Government-sponsored MUCs (Message 
Understanding Conferences). The techniques used 
by Information Extraction depend greatly on the 
sublanguage used in a domain, such as financial 
news or medical records. The training data for an 
IE system is often sparse since the target domain 
changes quickly. Traditional IE approaches try to 
generate patterns for events by various means 
using training data. For example, the FASTUS 
(Appelt et al, 1996) and Proteus (Grishman, 1996) 
systems, which performed well for MUC-6, used 
hand-written rules for event patterns. The symbolic 
learning systems, like AutoSlog (Riloff, 1993) and 
CRYSTAL (Fisher et al, 1996), generated patterns 
automatically from specific examples (text 
segments) using generalization and predefined 
pattern templates. There are also statistical 
approaches (Miller et al, 1998) (Collins et al, 
1998) trying to encode event patterns in statistical 
CFG grammars. All of these approaches assume 
events occur in text in certain patterns. However 
this assumption may not be completely correct and 
it limits the syntactic information considered by 
these approaches for finding events, such as 
information on global features from levels other 
than deep processing. This paper will show that a 
simple bag-of-words model can give us reliable 
information about event occurrence. When training 
data is limited, these other approaches may also be 
less effective in their ability to generate reliable 
patterns.    
  The idea for overcoming these problems is to 
avoid making any prior assumption about the 
syntactic structure an event may assume; instead, 
we should consider all syntactic features in the 
target text and use a discriminative classifier to 
decide that automatically. Discriminative 
classifiers make no attempt to resolve the structure 
of the target classes. They only care about the 
decision boundary to separate the classes. In our 
case, we only need criteria to predict event 
elements from text using the syntactic features 
provided. This seems a more suitable solution for 
IE where training data is often sparse. 
 This paper presents an approach that uses kernel 
functions to represent different levels of syntactic 
structure (information). With the properties of 
kernel functions, individual kernels can be 
combined freely into comprehensive kernels that 
cross syntactic levels. The classifier we chose to 
use is SVM (Support Vector Machine), mostly due 
to its ability to work in high dimensional feature 
spaces. The experimental results of this approach 
show that it can outperform a hand-crafted rule 
system for the MUC-6 management succession 
domain. 
2 Background 
2.1 Information Extraction 
The major task of IE is to find the elements of an 
event from text and combine them to form 
templates or populate databases.  Most of these 
elements are named entities (NEs) involved in the 
event. To determine which entities in text are 
involved, we need to find reliable clues around 
each entity. The extraction procedure starts with 
 2 
text preprocessing, ranging from tokenization and 
part-of-speech tagging to NE identification and 
parsing. Traditional approaches would use various 
methods of analyzing the results of deep 
preprocessing to find patterns. Here we propose to 
use support vector machines to identify clues 
automatically from the outputs of different levels 
of preprocessing. 
2.2 Support Vector Machine 
For a two-class classifier, with separable training 
data, when given a set of n labeled vector examples 
    }1,1{),,(),...,,(),,( 2211 ?+?inn yyXyXyX ,  
a support vector machine (Vapnik, 1998) produces 
the separating hyperplane with largest margin 
among all the hyperplanes that successfully 
classify the examples. Suppose that all the 
examples satisfy the following constraint:  
             1),( ?+><? bXWy ii  
It is easy to see that the margin between the two  
bounding hyperplanes 1, ?=+>< bXW i is 
2/||W||. So maximizing the margin is equivalent to 
minimizing ||W||2 subject to the separation 
constraint above. In machine learning theory, this 
margin relates to the upper bound of the VC-
dimension of a support vector machine. Increasing 
the margin reduces the VC-dimension of the 
learning system, thus increasing the generalization 
capability of the system.  So a support vector 
machine produces a classifier with optimal 
generalization capability. This property enables 
SVMs to work in high dimensional vector spaces. 
2.3 Kernel SVM 
The vectors in SVM are usually feature vectors 
extracted by a certain procedure from the original 
objects, such as images or sentences. Since the 
only operator used in SVM is the dot product 
between two vectors, we can replace this operator 
by a function ),( ji SS?  on the object domain. In 
our case, Si and Sj are sentences. Mathematically 
this is still valid as long as ),( ji SS?  satisfies 
Mercer?s condition 1 . Function ),( ji SS?  is often 
referred to as a kernel function or just a kernel.  
Kernel functions provide a way to compute the 
similarity between two objects without 
transforming them into features.  
   
The kernel set has the following properties: 
                                                     
1
 The matrix must be positive semi-definite 
1. If ),(1 yxK  and ),(2 yxK are kernels on YX ? , 
0, >?? , then ),(),( 21 yxKyxK ?? +  is a kernel 
on YX ? . 
2. If ),(1 yxK  and ),(2 yxK are kernels on YX ? , 
then ),(),( 21 yxKyxK ?  is a kernel on YX ? . 
3. If ),(1 yxK  is a kernel on YX ? and 
),(2 vuK  is a kernel on VU ? , then 
),(),()),(),,(( 21 vuKyxKvyuxK += is a kernel 
on )()( VYUX ??? . 
When we have kernels representing information 
from different sources, these properties enable us 
to incorporate them into one kernel. The general 
kernels such as RBF or polynomial kernels (M?ller 
et al, 2001), which extend features nonlinearly 
into higher dimensional space, can also be applied 
to either the combination kernel or to each 
component kernel individually. 
2.4 Related Work 
  There have been a number of SVM applications 
in NLP using particular levels of syntactic 
information. (Lodhi et al, 2002) compared a word-
based string kernel and n-gram kernels at the 
sequence level for a text categorization task. The 
experimental results showed that the n-gram 
kernels performed quite well for the task. Although 
string kernels can capture common word 
subsequences with gaps, its geometric penalty 
factor may not be suitable for weighting the long 
distance features. (Collins et al, 2001) suggested 
kernels on parse trees and other structures for 
general NLP tasks. These kernels count small 
subcomponents multiple times so that in practice 
one has to be careful to avoid overfitting. This can 
be achieved by limiting the matching depth or 
using a penalty factor to downweight large 
components.  
(Zelenko et al, 2003) devised a kernel on 
shallow parse trees to detect relations between 
named entities, such as the person-affiliation 
relation between a person name and an 
organization name. The so-called relation kernel 
matches from the roots of two trees and continues 
recursively to the leaf nodes if the types of two 
nodes match.  
All the kernels used in these works were applied 
to a particular syntactic level. This paper presents 
an approach for information extraction that uses 
kernels to combine information from different 
levels and automatically identify which 
information contributes to the task. This 
framework can also be applied to other NLP tasks. 
 
 3 
3 A Discriminative Framework 
  The discriminative framework proposed here is 
called ARES (Automated Recognition of Event 
Slots). It makes no assumption about the text 
structure of events. Instead, kernels are used to 
represent syntactic information from various 
syntactic sources. The structure of ARES is shown 
in Fig 1. The preprocessing modules include a 
part-of-speech tagger, name tagger, sentence parser 
and GLARF parser, but are not limited to these. 
Other general tools can also be included, which are 
not shown in the diagram. The triangles in the 
diagram are kernels that encode the corresponding 
syntactic processing result. In the training phase, 
the target slot fillers are labeled in the text so that 
SVM slot detectors can be trained through the 
kernels to find fillers for the key slots of events. In 
the testing phase, the SVM classifier will predict 
the slot fillers from unlabeled text and a merging 
procedure will merge slots into events if necessary. 
The main kernel we propose to use is on GLARF 
(Meyers et al, 2001) dependency graphs. 
 
 
Fig 1. Structure of the discriminative model 
 
  The idea is that an IE model should not commit 
itself to any syntactic level. The low level 
information, such as word collocations, may also 
give us important clues. Our experimentation will 
show that for the MUC-6 management succession 
domain, even bag-of-words or n-grams can give us 
helpful information about event occurrence. 
3.1 Syntactic Kernels 
  To make use of syntactic information from 
different levels, we can develop kernel functions or 
syntactic kernels to represent a certain level of 
syntactic structure. The possible syntactic kernels 
include 
? Sequence kernels: representing sequence 
level information, such as bag-of-words, n-
grams, string kernel, etc. 
? Phrase kernel: representing information at 
an intermediate level, such as kernels 
based on multiword expressions, chunks or 
shallow parse trees. 
? Parsing kernel: representing detailed 
syntactic structure of a sentence, such as 
kernels based on parse trees or dependency 
graphs. 
 
  These kernels can be used alone or combined 
with each other using the properties of kernels. 
They can also be combined with high-order kernels 
like polynomial or RBF kernels, either individually 
or on the resulting kernel. 
As the depth of analysis of the preprocessing 
increases, the accuracy of the result decreases. 
Combining the results of deeper processing with 
those of shallower processing (such as n-grams) 
can also give us a back-off ability to recover from 
errors in deep processing. 
In practice each kernel can be tested for the task 
as the sole input to an SVM to determine if this 
level of information is helpful or not. After 
figuring out all the useful kernels, we can try to 
combine them to make a comprehensive kernel as 
final input to the classifier. The way to combine 
them and the parameters in combination can be 
determined using validation data. 
4 Introduction to GLARF 
GLARF (Grammatical and Logical Argument 
Regularization Framework) [Meyers et al, 2001] is 
a  hand-coded system that produces comprehensive 
word dependency graphs from Penn TreeBank-II 
(PTB-II) parse trees to facilitate applications like 
information extraction. GLARF is designed to 
enhance PTB-II parsing to produce more detailed 
information not provided by parsing, such as 
information about object, indirect object and 
appositive relations. GLARF can capture more 
regularization in text by transforming non-
canonical (passive, filler-gap) constructions into 
their canonical forms (simple declarative clauses). 
This is very helpful for information extraction 
where training data is often sparse. It also 
represents all syntactic phenomena in uniform 
typed PRED-ARG structures, which is convenient 
for computational purposes. For a sentence, 
GLARF outputs depencency triples derived 
automatically from the GLARF typed feature 
structures [Meyers et al, 2001]. A directed 
dependency graph of the sentence can also be 
constructed from the depencency triples. The 
following is the output of GLARF for the sentence 
?Tom Donilon, who also could get a senior job 
??. 
<SBJ,   get,  Tom Donilon> 
<OBJ,  get,   job> 
<ADV,  get,  also> 
<AUX,  get,  could> 
<T-POS,  job, a> 
  
 
 
 
 
Texts 
Input 
 
 
Output 
Templates 
POS      
Tagger 
Sent      
Parser 
Glarf      
Parser 
Name      
Tagger 
SGML     
Parser Event      
Merger 
Slot
 D
etecto
r
 
Documents 
 4 
<A-POS,  job,  senior> 
  . . . 
GLARF can produce logical relations in addition 
to surface relations, which is helpful for IE tasks. It 
can also generate output containing the base form 
of words so that different tenses of verbs can be 
regularized. Because of all these features, our main 
kernels are based on the GLARF dependency 
triples or dependency graphs.  
5 Event and Slot Kernels 
Here we will introduce the kernels used by ARES 
for event occurrence detection (EOD) and slot 
filler detection (SFD).  
5.1 EOD Kernels 
  In Information Extraction, one interesting issue 
is event occurrence detection, which is determining 
whether a sentence contains an event occurrence or 
not. If this information is given, it would be much 
easier to find the relevant entities for an event from 
the current sentence or surrounding sentences. 
Traditional approaches do matching (for slot 
filling) on all sentences, even though most of them 
do not contain any event at all. Event occurrence 
detection is similar to sentence level information 
retrieval, so simple models like bag-of-words or n-
grams could work well. We tried two kernels to do 
this, one is a sequence level n-gram kernel and the 
other is a GLARF-based kernel that matches 
syntactic details between sentences. In the 
following formulae, we will use an identity 
function ),( yxI that gives 1 when yx ?  and 0 
otherwise, where x and y are strings or vectors of 
strings.  
 
1. N-gram kernel ),( 21 SSN?  that counts common 
n-grams between two sentences. Given two 
sentence: >=<
1
,..., 211 NwwwS , and >=< 2,..., 211 NwwwS ,  
a bigram kernel ),( 21 SSbi?  is 
??
?
=
++
?
=
><><
1
1
11
1
1
21
),,,(
N
j
jjii
N
i
wwwwI .   
Kernels can be inclusive, in other words, the 
trigram kernel includes bigrams and unigrams. For 
the unigram kernel a stop list is used that removes 
words other than nouns, verbs, adjectives and 
adverbs. 
2. Glarf kernel ),( 21 GGg? : this kernel is based 
on the GLARF dependency result. Given the triple 
outputs of two sentences produced by  
GLARF: },,{1 ><= iii aprG , 11 Ni ??  and 
},,{2 ><= jjj aprG , 21 Nj ?? , where ri, pi, ai 
correspond to the role label, predicate word and 
argument word respectively in GLARF output, it 
matches the two triples, their predicates and 
arguments respectively. So ),( 21 GGg?  equals 
)),(),(),,,,,((
21
11
??
==
++><><
N
j
jijijjjiii
N
i
aaIppIapraprI ??  
In our experiments, ? and ?  were set to 1. 
5.2 SFD Kernels 
 Slot filler detection (SFD) is the task of 
determining which named entities fill a slot in 
some event template.  Two kernels were proposed 
for SFD: the first one matches local contexts of 
two target NEs, while the second one combines the 
first one with an n-gram EOD kernel.  
  1. ),(1 jiSFD GG? : This kernel was also defined 
on a GLARF dependency graph (DG), a directed 
graph constructed from its typed PRED-ARG 
outputs. The arcs labeled with roles go from 
predicate words to argument words. This kernel 
matches local context surrounding a name in a 
GLARF dependency graph. In preprocessing, all 
the names of the same type are translated into one 
symbol (a special word). The matching starts from 
two anchor nodes (NE nodes of the same type) in 
the two DG?s and recursively goes from these 
nodes to their successors and predecessors, until 
the words associated with nodes do not match. In 
our experiment, the matching depth was set to 2. 
Each node n contains a predicate word w
 
and  
relation pairs },{ >< ii ar , pi ??1  representing 
its p arguments and the roles associated with them.  
A matching function ),( 21 nnC  is defined as 
??
==
+><><
21
11
)),(),,,((
p
j
jijjii
p
i
rrIararI . 
Then ),(1 jiSFD GG? : can be written as 
??
?
?
?
?
?
?
++
ji
jj
ii
ji
jj
ii
nn
Eedn
Eedn
ji
nn
ESuccn
ESuccn
jiji nnCnnCEEC
)(Pr
)(Pr
)(
)(
),(),(),(  
where Ei and Ej are the anchor nodes in the two 
DG?s; ji nn ? is true if the predicate words 
associated with them match. Functions Succ(n) and 
Pred(n) give the successor and predecessor node 
set of a node n. The reason for setting a depth limit 
is that it covers most of the local syntax of a node 
(before matching stops); another reason is that the 
cycles currently present in GLARF dependency 
graph prohibit unbounded recursive matching. 
  2. ),(2 jiSFD SS? : This kernel combines linearly 
the n-gram event kernel and the slot kernel above, 
 5 
in the hope that the general event occurrence 
information provided by EOD kernel can help the 
slot kernel to ignore NEs in sentences that do not 
contain any event occurrence.  
),(),(),( 12 jiSFDjiNjiSFD GGSSSS ????? += , 
where ?? , were set to be 1 in our experiments. 
The Glarf event kernel was not used, simply 
because it uses information from the same source 
as ),(1 jiSFD GG? . The n-gram kernel was chosen 
to be the trigram kernel, which gives us the best 
EOD performance among n-gram kernels. 
We also tried the dependency graph kernel 
proposed by (Collins et al, 2001), but it did not 
give us better result. 
6 Experiments 
6.1 Corpus 
  The experiments of ARES were done on the 
MUC-6 corporate management succession domain 
using the official training data and, for the final 
experiment, the official test data as well. The 
training data was split into a training set (80%) and 
validation set (20%). In ARES, the text was 
preprocessed by the Proteus NE tagger and 
Charniak sentence parser. Then the GLARF 
processor produced dependency graphs based on 
the parse trees and NE results. All the names were 
transformed into symbols representing their types, 
such as #PERSON# for all person names. The 
reason is that we think the name itself does not 
provide a significant clue; the only thing that 
matters is what type of name occurs at certain 
position. 
  Two tasks have been tried: one is EOD (event 
occurrence detection) on sentences; the other is 
SFD (slot filler detection) on named entities, 
including person names and job titles. EOD is to 
determine whether a sentence contains an event or 
not. This would give us general information about 
sentence-level event occurrences. SFD is to find 
name fillers for event slots. The slots we 
experimented with were the person name and job 
title slots in MUC-6. We used the SVM package 
SVMlight in our experiments, embedding our own 
kernels as custom kernels. 
6.2 EOD Experiments 
  In this experiment, ARES was trained on the 
official MUC-6 training data to do event 
occurrence detection. The data contains 1940 
sentences, of which 158 are labeled as positive 
instances (contain an event). Five-fold cross 
validation was used so that the training and test set 
contain 80% and 20% of the data respectively. 
Three kernels defined in the previous section were 
tried. Table 1 shows the performance of each 
kernel. Three n-gram kernels were tested: unigram, 
bigram and trigram. Subsequences longer than 
trigrams were also tried, but did not yield better 
results. 
  The results show that the trigram kernel 
performed the best among n-gram kernels. GLARF 
kernel did better than n-gram kernels, which is 
reasonable because it incorporates detailed syntax 
of a sentence. But generally speaking, the n-gram 
kernels alone performed fairly well for this task, 
which indicates that low level text processing can 
also provide useful information. The mix kernel 
that combines the trigram kernel with GLARF 
kernel gave the best performance, which might 
indicate that the low level information provides 
additional clues or helps to overcome errors in 
deep processing. 
 
Kernel Precision Recall F-score 
Unigram 66.0% 66.5% 66.3% 
Bigram 73.9% 60.3% 66.4% 
Trigram 77.5% 61.5% 68.6% 
GLARF 77.5% 63.9% 70.1% 
Mix 81.5% 66.4% 73.2% 
 
Table 1. EOD performance of ARES using 
different kernels. The Mix kernel is a linear 
combination of the trigram kernel and the Glarf 
kernel. 
6.3 SFD Experiments 
The slot filler detection (SFD) task is to find the 
named entities in text that can fill the 
corresponding slots of an event.2 We treat job title 
as a named entity throughout this paper, although it 
is not included in the traditional MUC named 
entity set. The slots we used for evaluation were 
PERSON_IN (the person who took a position),  
PERSON_OUT (the person who left a position) 
and POST (the position involved). We generated 
the two person slots from the official MUC-6 
templates and the corresponding filler strings in 
text were labeled. Three SVM predictors were 
trained to find name fillers of each slot. Two 
experiments have been tried on MUC-6 training 
data using five-fold cross validation. 
  The first experiment of ARES used slot kernel 
),(1 jiSFD GG?  alone, relying solely on local 
                                                     
2
 We used this task for evaluation, rather than the 
official MUC template-filling task, in order to assess the 
system?s ability to identify slot fillers separately from its 
ability to combine them into templates. 
 6 
context around a NE. From the performance table 
(Table 2), we can see that local context can give a 
fairly good clue for finding PERSON_IN and 
POST, but not for PERSON_OUT. The main 
reason is that local context might be not enough to 
determine a PERSON_OUT filler. It often requires 
inference or other semantic information. For 
example, the sentence ?Aaron Spelling, the 
company's vice president, was named president.?, 
indicates that ?Aaron Spelling? left the position of 
vice president, therefore it should be a 
PERSON_OUT. But the sentence ?Aaron Spelling, 
the company's vice president, said ??, which is 
very similar to first one in syntax, has no such 
indication at all. In complicated cases, a person can 
even hold two positions at the same time. 
 
Accuracy Precision Recall F-score 
PER_IN 63.6% 62.5% 63.1% 
PER_OUT 54.8% 54.2% 54.5% 
POST 64.4% 55.2% 59.4% 
 
Table 2. SFD performance of ARES using kernel 
),(1 jiSFD GG? . 
 
  In this experiment, the SVM predictor 
considered all the names identified by the NE 
tagger; however, most of the sentences do not 
contain an event occurrence at all, so NEs in these 
sentences should be ignored no matter what their 
local context is. To achieve this we need general 
information about event occurrence, and this is just 
what the EOD kernel can provide. In our second 
experiment, we tested the kernel ),(2 jiSFD SS? , 
which is a linear combination of the trigram EOD 
kernel and the SFD kernel ),(1 jiSFD GG? . Table 3 
shows the performance of the combination kernel, 
from which we can see that there is clear 
performance improvement for all three slots. We 
also tried to use the mix kernel which gave us the 
best EOD performance, but it did not yield a better 
result. The reason we think is that the GLARF 
EOD kernel and SFD kernel are from the same 
syntactic source, so the information was repeated. 
 
Accuracy Precision Recall F-score 
PER_IN 86.6% 60.5% 71.2% 
PER_OUT 69.2% 58.2% 63.2% 
POST 68.5% 68.9% 68.7% 
 
Table 3. SFD performance of ARES using kernel 
),(2 jiSFD SS? . It combines the Glarf SFD kernel 
with trigram EOD kernel. For PER_OUT,  
unigram EOD kernel was used. 
 
Since five-fold cross validation was used, ARES 
was trained on 80% of the MUC-6 training data in 
these two experiments.  
6.4 Comparison with MUC-6 System 
This experiment was done on the official MUC-
6 training and test data, which contain 50K words 
and 40K words respectively. ARES used the 
official corpora as training and test sets, except that 
in the training data, all the slot fillers were 
manually labeled. We compared the performance 
of ARES with the NYU Proteus system, a rule-
based system that performed well for MUC-6. To 
score the performance for these three slots, we 
generated the slot-filler pairs as keys for a 
document from the official MUC-6 templates and 
removed duplicate pairs. The scorer matches the 
filler string in the response file of ARES to the 
keys.  The response result for Proteus was 
extracted in the same way from its template output. 
Table 4. shows the result of ARES using the 
combination kernel in the previous experiment. 
 
Accuracy Precision Recall F-score 
PER_IN 77.3% 62.2% 68.9% 
PER_OUT 58.9% 69.7% 63.9% 
POST 77.1% 71.5% 73.6% 
 
Table 4. Slot performance ARES using kernel 
),(2 jiSFD SS?  on MUC-6 test data.  
 
Table 5 shows the test result of the Proteus 
system. Comparing the numbers we can see that 
for slot PERSON_IN and POST, ARES 
outperformed the Proteus system by a few points. 
The result is promising considering that this model 
is fully automatic and does not involve any post-
processing. As for the PERSON_OUT slot, the 
performance of ARES was not as good. As we 
have discussed before, relying purely on syntax 
might not help us much;  we may need an 
inference model to resolve this problem. 
 
Accuracy Precision Recall F-score 
PER_IN 85.7% 51.2% 64.1% 
PER_OUT 78.4% 58.6% 67.1% 
POST 83.3% 59.7% 69.5% 
 
Table 5. Slot performance of the rule-based 
Proteus system for MUC-6. 
7 Related Work 
(Chieu et al, 2003) reported a feature-based 
SVM system (ALICE) to extract MUC-4 events of 
 7 
terrorist attacks. The Alice-ME system 
demonstrated competitive performance with rule-
based systems. The features used by Alice are 
mainly from parsing. Comparing with ALICE, our 
system uses kernels on dependency graphs to 
replace explicit features, an approach which is 
fully automatic and requires no enumeration of 
features. The model we proposed can combine 
information from different syntactic levels in 
principled ways. In our experiments, we used both 
word sequence  information and parsing level 
syntax information. The training data for ALICE 
contains 1700 documents, while for our system it 
is just 100 documents. When data is sparse, it is 
more difficult for an automatic system to 
outperform a rule-based system that incorporates 
general knowledges. 
8 Discussion and Further Works 
    This paper describes a discriminative approach 
that can use syntactic clues automatically for slot 
filler detection. It outperformed a hand-crafted 
system on sparse data by considering different 
levels of syntactic clues. The result also shows that 
low level syntactic information can also come into 
play in finding events, thus it should not be ignored 
in the IE framework. 
    For slot filler detection, several classifiers were 
trained to find names for each slot and there is no 
correlation among these classifiers. However, 
entity slots in events are often strongly correlated, 
for example the PER_IN and POST slots for 
management succession events. Since these 
classifiers take the same input and produce 
different results, correlation models can be used to 
integrate these classifiers so that the identification 
of slot fillers might benefit each other.  
    It would also be interesting to experiment with 
the tasks that are more difficult for pattern 
matching, such as determining the on-the-job 
status property in MUC-6. Since events often span 
multiple sentences, another direction is to explore 
cross-sentence models, which is difficult for 
traditional approaches. For our approach it is 
possible to extend the kernel from one sentence to 
multiple sentences, taking into account the 
correlation between NE?s in adjacent sentences. 
9 Acknowledgements 
This research was supported in part by the 
Defense Advanced Research Projects Agency as 
part of the TIDES program, under Grant N66001-
001-1-8917 from the Space and Naval Warfare 
Systems Center, San Diego, and by the National 
Science Foundation under Grant ITS-0325657.  
This paper does not necessarily reflect the position 
of the U.S. Government. 
References  
D. Appelt, J. Hobbs, J. Bear, D. Israel, M. Kameyama,  
A. Kehler, D. Martin, K. Meyers, and M. Tyson 
1996. SRI International FASTUS system: MUC-6 test 
results and analysis. In Proceedings of the Sixth 
Message Understanding Conference.  
H. L. Chieu, H. T. Ng, & Y. K. Lee. 2003. Closing the 
Gap: Learning-Based Information Extraction 
Rivaling Knowledge-Engineering Methods. In 
Proceedings of the 41st Annual Meeting of the 
Association for Computational Linguistics.  
M. Collins and S. Miller. 1998. Semantic Tagging using 
a Probabilistic Context Free Grammar, In 
Proceedings of the Sixth Workshop on Very Large 
Corpora. 
M. Collins and N. Duffy. 2001. Convolution Kernels for 
Natural Language, Advances in Neural Information 
Processing Systems 14, MIT Press. 
D. Fisher, S. Soderland, J. McCarthy, F. Feng and W. 
Lehnert. 1996. Description of The UMass System As 
Used For MUC-6. In Proceedings of the Sixth 
Message Understanding Conference. 
R. Grishman. 1996. The NYU System for MUC-6 or 
Where's the Syntax?. In Proceedings of the Sixth 
Message Understanding Conference.  
H. Lodhi, C. Sander, J. Shawe-Taylor, N. Christianini 
and C. Watkins. 2002. Text Classification using 
String Kernels. Journal of Machine Learning 
Research. 
A. Meyers, R. Grishman, M. Kosaka and S. Zhao. 2001. 
Covering Treebanks with GLARF. In Proceedings of 
of the ACL Workshop on Sharing Tools and 
Resources. 
S. Miller, M. Crystal, H. Fox, L. Ramshaw, R. 
Schwartz, R. Stone, and R. Weischedel. 1998. BBN: 
Description of The SIFT System As Used For MUC-
7, In Proceedings of the Seventh Message 
Understanding Conference. 
K.-R. M?ller, S. Mika, G. Ratsch, K. Tsuda, B. 
Scholkopf. 2001. An introduction to kernel-based 
learning algorithms, IEEE Trans. Neural Networks, 
12, 2, pages 181-201. 
E. Riloff. 1993. Automatically constructing a dictionary 
for information extraction tasks. In Proceedings of 
the 11th National Conference on Artificial 
Intelligence, 811-816. 
V. N. Vapnik. 1998. Statistical Learning Theory. Wiley-
Interscience Publication. 
D. Zelenko, C. Aone and A. Richardella. 2003. Kernel 
methods for relation extraction. Journal of Machine 
Learning Research. 
 
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 146?154,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The Role of Implicit Argumentation in Nominal SRL
Matt Gerber
Dept. of Computer Science
Michigan State University
gerberm2@msu.edu
Joyce Y. Chai
Dept. of Computer Science
Michigan State University
jchai@cse.msu.edu
Adam Meyers
Dept. of Computer Science
New York University
meyers@cs.nyu.edu
Abstract
Nominals frequently surface without overtly
expressed arguments. In order to measure the
potential benefit of nominal SRL for down-
stream processes, such nominals must be ac-
counted for. In this paper, we show that a
state-of-the-art nominal SRL system with an
overall argument F1 of 0.76 suffers a perfor-
mance loss of more than 9% when nominals
with implicit arguments are included in the
evaluation. We then develop a system that
takes implicit argumentation into account, im-
proving overall performance by nearly 5%.
Our results indicate that the degree of implicit
argumentation varies widely across nominals,
making automated detection of implicit argu-
mentation an important step for nominal SRL.
1 Introduction
In the past few years, a number of studies have
focused on verbal semantic role labeling (SRL).
Driven by annotation resources such as FrameNet
(Baker et al, 1998) and PropBank (Palmer et al,
2005), many systems developed in these studies
have achieved argument F1 scores near 80% in
large-scale evaluations such as the one reported by
Carreras and Ma`rquez (2005).
More recently, the automatic identification of
nominal argument structure has received increased
attention due to the release of the NomBank cor-
pus (Meyers, 2007a). NomBank annotates predicat-
ing nouns in the same way that PropBank annotates
predicating verbs. Consider the following example
of the verbal predicate distribute from the PropBank
corpus:
(1) Freeport-McMoRan Energy Partners will be
liquidated and [Arg1 shares of the new
company] [Predicate distributed] [Arg2 to the
partnership?s unitholders].
The NomBank corpus contains a similar instance of
the deverbal nominalization distribution:
(2) Searle will give [Arg0 pharmacists] [Arg1
brochures] [Arg1 on the use of prescription
drugs] for [Predicate distribution] [Location in
their stores].
This instance demonstrates the annotation of split ar-
guments (Arg1) and modifying adjuncts (Location),
which are also annotated in PropBank. In cases
where a nominal has a verbal counterpart, the inter-
pretation of argument positions Arg0-Arg5 is con-
sistent between the two corpora.
In addition to deverbal (i.e., event-based) nomi-
nalizations, NomBank annotates a wide variety of
nouns that are not derived from verbs and do not de-
note events. An example is given below of the parti-
tive noun percent:
(3) Hallwood owns about 11 [Predicate %] [Arg1 of
Integra].
In this case, the noun phrase headed by the predicate
% (i.e., ?about 11% of Integra?) denotes a fractional
part of the argument in position Arg1.
Since NomBank?s release, a number of studies
have applied verbal SRL techniques to the task of
nominal SRL. For example, Liu and Ng (2007) re-
ported an argument F1 of 0.7283. Although this
result is encouraging, it does not take into account
nominals that surface without overt arguments. Con-
sider the following example:
(4) The [Predicate distribution] represents [NP
available cash flow] [PP from the partnership]
[PP between Aug. 1 and Oct. 31].
146
As in (2), distribution in (4) has a noun phrase and
multiple prepositional phrases in its environment,
but not one of these constituents is an argument to
distribution in (4); rather, any arguments are implic-
itly supplied by the surrounding discourse. As de-
scribed by Meyers (2007a), instances such as (2) are
called ?markable? because they contain overt argu-
ments, and instances such as (4) are called ?unmark-
able? because they do not. In the NomBank corpus,
only markable instances have been annotated.
Previous evaluations (e.g., those by Jiang and
Ng (2006) and Liu and Ng (2007)) have been based
on markable instances, which constitute 57% of all
instances of nominals from the NomBank lexicon.
In order to use nominal SRL systems for down-
stream processing, it is important to develop and
evaluate techniques that can handle markable as well
as unmarkable nominal instances. To address this
issue, we investigate the role of implicit argumenta-
tion for nominal SRL. This is, in part, inspired by the
recent CoNLL Shared Task (Surdeanu et al, 2008),
which was the first evaluation of syntactic and se-
mantic dependency parsing to include unmarkable
nominals. In this paper, we extend this task to con-
stituent parsing with techniques and evaluations that
focus specifically on implicit argumentation in nom-
inals.
We first present our NomBank SRL system,
which improves the best reported argument F1 score
in the markable-only evaluation from 0.7283 to
0.7630 using a single-stage classification approach.
We show that this system, when applied to all nomi-
nal instances, achieves an argument F1 score of only
0.6895, a loss of more than 9%. We then present
a model of implicit argumentation that reduces this
loss by 46%, resulting in an F1 score of 0.7235 on
the more complete evaluation task. In our analyses,
we find that SRL performance varies widely among
specific classes of nominals, suggesting interesting
directions for future work.
2 Related work
Nominal SRL is related to nominal relation interpre-
tation as evaluated in SemEval (Girju et al, 2007).
Both tasks identify semantic relations between a
head noun and other constituents; however, the tasks
focus on different relations. Nominal SRL focuses
primarily on relations that hold between nominaliza-
tions and their arguments, whereas the SemEval task
focuses on a range of semantic relations, many of
which are not applicable to nominal argument struc-
ture.
Early work in identifying the argument struc-
ture of deverbal nominalizations was primarily rule-
based, using rule sets to associate syntactic con-
stituents with semantic roles (Dahl et al, 1987;
Hull and Gomez, 1996; Meyers et al, 1998). La-
pata (2000) developed a statistical model to classify
modifiers of deverbal nouns as underlying subjects
or underlying objects, where subject and object de-
note the grammatical position of the modifier when
linked to a verb.
FrameNet and NomBank have facilitated machine
learning approaches to nominal argument struc-
ture. Gildea and Jurafsky (2002) presented an early
FrameNet-based SRL system that targeted both ver-
bal and nominal predicates. Jiang and Ng (2006)
and Liu and Ng (2007) have tested the hypothe-
sis that methodologies and representations used in
PropBank SRL (Pradhan et al, 2005) can be ported
to the task of NomBank SRL. These studies report
argument F1 scores of 0.6914 and 0.7283, respec-
tively. Both studies also investigated the use of fea-
tures specific to the task of NomBank SRL, but ob-
served only marginal performance gains.
NomBank argument structure has also been used
in the recent CoNLL Shared Task on Joint Parsing
of Syntactic and Semantic Dependencies (Surdeanu
et al, 2008). In this task, systems were required to
identify syntactic dependencies, verbal and nominal
predicates, and semantic dependencies (i.e., argu-
ments) for the predicates. For nominals, the best se-
mantic F1 score was 0.7664 (Surdeanu et al, 2008);
however this score is not directly comparable to the
NomBank SRL results of Liu and Ng (2007) or the
results in this paper due to a focus on different as-
pects of the problem (see the end of section 5.2 for
details).
3 NomBank SRL
Given a nominal predicate, an SRL system attempts
to assign surrounding spans of text to one of 23
classes representing core arguments, adjunct argu-
ments, and the null or non-argument. Similarly to
147
verbal SRL, this task is traditionally formulated as
a two-stage classification problem over nodes in the
syntactic parse tree of the sentence containing the
predicate.1 In the first stage, each parse tree node is
assigned a binary label indicating whether or not it
is an argument. In the second stage, argument nodes
are assigned one of the 22 non-null argument types.
Spans of text subsumed by labeled parse tree nodes
constitute arguments of the predication.
3.1 An improved NomBank SRL baseline
To investigate the effects of implicit argumenta-
tion, we first developed a system based on previ-
ous markable-only approaches. Our system follows
many of the traditions above, but differs in the fol-
lowing ways. First, we replace the standard two-
stage pipeline with a single-stage logistic regression
model2 that predicts arguments directly. Second,
we model incorporated arguments (i.e., predicates
that are also arguments) with a simple maximum
likelihood model that predicts the most likely argu-
ment label for a predicate based on counts from the
training data. Third, we use the following heuris-
tics to resolve argument conflicts: (1) If two argu-
ments overlap, the one with the higher probability is
kept. (2) If two non-overlapping arguments are of
the same type, the one with the higher probability
is kept unless the two nodes are siblings, in which
case both are kept. Heuristic (2) accounts for split
argument constructions.
Our NomBank SRL system uses features that are
selected with a greedy forward search strategy sim-
ilar to the one used by Jiang and Ng (2006). The
top half of Table 2 (next page) lists the selected ar-
gument features.3 We extracted training nodes from
sections 2-21 of NomBank, used section 24 for de-
velopment and section 23 for testing. All parse
trees were generated by Charniak?s re-ranking syn-
tactic parser (Charniak and Johnson, 2005). Follow-
ing the evaluation methodology used by Jiang and
Ng (2006) and Liu and Ng (2007), we obtained sig-
1The syntactic parse can be based on ground-truth annota-
tion or derived automatically, depending on the evaluation.
2We use LibLinear (Fan et al, 2008).
3For features requiring the identification of support verbs,
we use the annotations provided in NomBank. Preliminary ex-
periments show a small loss when using automatic support verb
identification.
Dev. F1 Testing F1
Jiang and Ng (2006) 0.6677 0.6914
Liu and Ng (2007) - 0.7283
This paper 0.7454 0.7630
Table 1: Markable-only NomBank SRL results for ar-
gument prediction using automatically generated parse
trees. The f-measure statistics were calculated by ag-
gregating predictions across all classes. ?-? indicates
that the result was not reported.
Markable-only All-token % loss
P 0.7955 0.6577 -17.32
R 0.7330 0.7247 -1.13
F1 0.7630 0.6895 -9.63
Table 3: Comparison of the markable-only and all-
token evaluations of the baseline argument model.
nificantly better results, as shown in Table 1 above.4
3.2 The effect of implicit nominal arguments
The presence of implicit nominal arguments
presents challenges that are not taken into account
by the evaluation described above. To assess the im-
pact of implicit arguments, we evaluated our Nom-
Bank SRL system over each token in the testing
section. The system attempts argument identifica-
tion for all singular and plural nouns that have at
least one annotated instance in the training portion
of the NomBank corpus (morphological variations
included).
Table 3 gives a comparison of the results from the
markable-only and all-token evaluations. As can be
seen, assuming that all known nouns take overt argu-
ments results in a significant performance loss. This
loss is due primarily to a drop in precision caused by
false positive argument predictions made for nomi-
nals with implicit arguments.
4 Accounting for implicit arguments in
nominal SRL
A natural solution to the problem described above
is to first distinguish nominals that bear overt
arguments from those that do not. We treat this
4As noted by Carreras and Ma`rquez (2005), the discrepancy
between the development and testing results is likely due to
poorer syntactic parsing performance on the development sec-
tion.
148
A
rg
u
m
en
tf
ea
tu
re
s
# Description N S
1 12 & parse tree path from n to pred
2 Position of n relative to pred & parse tree path from n to pred *
3 First word subsumed by n
4 12 & position of n relative to pred
5 12 & 14
6 Head word of n?s parent *
7 Last word subsumed n
8 n?s syntactic category & length of parse tree path from n to pred
9 First word of n?s right sibling * *
10 Production rule that expands the parent of pred
11 Head word of the right-most NP in n if n is a PP *
12 Stem of pred
13 Parse tree path from n to the lowest common ancestor of n and pred
14 Head word of n
15 12 & n?s syntactic category
16 Production rule that expands n?s parent * *
17 Parse tree path from n to the nearest support verb *
18 Last part of speech (POS) subsumed by n *
19 Production rule that expands n?s left sibling *
20 Head word of n, if the parent of n is a PP
21 The POS of the head word of the right-most NP under n if n is a PP
... Features 22-31 are available upon request 0 3
N
o
m
in
al
fe
at
u
re
s
1 n?s ancestor subcategorization frames (ASF) (see section 4) *
2 n?s word
3 Syntactic category of n?s right sibling
4 Parse tree paths from n to each support verb *
5 Last word of n?s left sibling * *
6 Parse tree path from n to previous nominal, with lexicalized source (see section 4) *
7 Last word of n?s right sibling *
8 Production rule that expands n?s left sibling * *
9 Syntactic category of n *
10 PropBank markability score (see section 4) *
11 Parse tree path from n to previous nominal, with lexicalized source and destination *
12 Whether or not n is followed by PP *
13 Parse tree path from n to previous nominal, with lexicalized destination *
14 Head word of n?s parent *
15 Whether or not n surfaces before a passive verb * *
16 First word of n?s left sibling *
17 Parse tree path from n to closest support verb, with lexicalized destination *
18 Whether or not n is a head *
19 Head word of n?s right sibling
20 Production rule that expands n?s parent * *
21 Parse tree paths from n to all support verbs, with lexicalized destinations *
22 First word of n?s right sibling * *
23 Head word of n?s left sibling *
24 If n is followed by a PP, the head of that PP?s object *
25 Parse tree path from n to previous nominal *
26 Token distance from n to previous nominal *
27 Production rule that expands n?s grandparent *
Table 2: Features, sorted by gain in selection algorithm. & denotes concatenation. The last two columns indicate
(N)ew features (not used in Liu and Ng (2007)) and features (S)hared by the argument and nominal models.
149
as a binary classification task over token nodes.
Once a nominal has been identified as bearing
overt arguments, it is processed with the argument
identification model developed in the previous
section. To classify nominals, we use the features
shown in the bottom half of Table 2, which were
selected with the same algorithm used for the
argument classification model. As shown by Table
2, the sets of features selected for argument and
nominal classification are quite different, and many
of the features used for nominal classification have
not been previously used. Below, we briefly explain
a few of these features.
Ancestor subcategorization frames (ASF)
As shown in Table 2, the most informative feature
is ASF. For a given token t, ASF is actually a set
of sub-features, one for each parse tree node above
t. Each sub-feature is indexed (i.e., named) by its
distance from t. The value of an ASF sub-feature
is the production rule that expands the correspond-
ing node in the tree. An ASF feature with two
sub-features is depicted below for the token ?sale?:
VP: ASF2 = V P ? V,NP
V (made) NP: ASF1 = NP ? Det,N
Det (a) N (sale)
Parse tree path lexicalization A lexicalized parse
tree path is one in which surface tokens from the
beginning or end of the path are included in the path.
This is a finer-grained version of the traditional
parse tree path that captures the joint behavior of
the path and the tokens it connects. For example,
in the tree above, the path from ?sale? to ?made?
with a lexicalized source and destination would be
sale : N ? NP ? V P ? V : made. Lexicalization
increases sparsity; however, it is often preferred
by the feature selection algorithm, as shown in the
bottom half of Table 2.
PropBank markability score This feature is
the probability that the context (? 5 words) of a de-
verbal nominal is generated by a unigram language
model trained over the PropBank argument words
for the corresponding verb. Entities are normalized
Precision Recall F1
Baseline 0.5555 0.9784 0.7086
MLE 0.6902 0.8903 0.7776
LibLinear 0.8989 0.8927 0.8958
Table 4: Evaluation results for identifying nominals
with explicit arguments.
to their entity type using BBN?s IdentiFinder, and
adverbs are normalized to their related adjective us-
ing the ADJADV dictionary provided by NomBank.
The normalization of adverbs is motivated by the
fact that adverbial modifiers of verbs typically have
a corresponding adjectival modifier for deverbal
nominals.
5 Evaluation results
Our evaluation methodology reflects a practical sce-
nario in which the nominal SRL system must pro-
cess each token in a sentence. The system can-
not safely assume that each token bears overt argu-
ments; rather, this decision must be made automat-
ically. In section 5.1, we present results for the au-
tomatic identification of nominals with overt argu-
ments. Then, in section 5.2, we present results for
the combined task in which nominal classification is
followed by argument identification.
5.1 Nominal classification
Following standard practice, we train the nomi-
nal classifier over NomBank sections 2-21 using
LibLinear and automatically generated syntactic
parse trees. The prediction threshold is set to the
value that maximizes the nominal F1 score on
development section (24), and the resulting model
is tested over section 23. For comparison, we
implemented the following simple classifiers.
Baseline nominal classifier Classifies a token
as overtly bearing arguments if it is a singular or
plural noun that is markable in the training data.
As shown in Table 4, this classifier achieves nearly
perfect recall.5
MLE nominal classifier Operates similarly to
5Recall is less than 100% due to (1) part-of-speech errors
from the syntactic parser and (2) nominals that were not anno-
tated in the training data but exist in the testing data.
150
00.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
0.0
5 0.1 0.1
5 0.2 0.2
5 0.3
(0.2
5) 0
.
35 0.4 0.4
5 0.5 0.5
5 0.6
(0.5
) 0.
65 0.7 0.7
5
(0.7
5) 0
.
8
0.8
5 0.9 0.9
5 1
Observed markable probability
%
 
o
f n
o
m
in
al
 
in
st
an
ce
s
(a) Distribution of nominals. Each interval on the x-axis denotes a set of nominals that are markable between (x?5)%
and x% of the time in the training data. The y-axis denotes the percentage of all nominal instances in TreeBank that
is occupied by nominals in the interval. Quartiles are marked below the intervals. For example, quartile 0.25 indicates
that one quarter of all nominal instances are markable 35% of the time or less.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.0
5 0.1 0.1
5 0.2 0.2
5 0.3 0.3
5 0.4 0.4
5 0.5 0.5
5 0.6 0.6
5 0.7 0.7
5 0.8 0.8
5 0.9 0.9
5 1
Observed markable probability
Pr
ed
ic
at
e 
n
o
m
in
al
 
F1
Baseline
LibLinear
(b) Nominal classification performance with respect to the
distribution in Figure 1a. The y-axis denotes the combined
F1 for nominals in the interval.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.0
5 0.1 0.1
5 0.2 0.2
5 0.3 0.3
5 0.4 0.4
5 0.5 0.5
5 0.6 0.6
5 0.7 0.7
5 0.8 0.8
5 0.9 0.9
5 1
Observed markable probability
A
rg
u
m
en
t F
1
Baseline
MLE
LibLinear
(c) All-token argument classification performance with re-
spect to the distribution in Figure 1a. The y-axis denotes the
combined F1 for nominals in the interval.
Figure 1: Evaluation results with respect to the distribution of nominals in TreeBank.
the baseline classifier, but also produces a score
for the classification. The value of the score is
equal to the probability that the nominal bears overt
arguments, as observed in the training data. A
prediction threshold is imposed on this score as
determined by the development data (t = 0.23).
As shown by Table 4, this exchanges recall for
precision and leads to a significant increase in the
overall F1 score.
The last row in Table 4 shows the results for
the LibLinear nominal classifier, which significantly
outperforms the others, achieving balanced preci-
sion and recall scores near 0.9. In addition, it is
able to recover from part-of-speech errors because
it does not filter out non-noun instances; rather, it
combines part-of-speech information with other lex-
ical and syntactic features to classify nominals.
Interesting observations can be made by grouping
nominals according to the probability with which
they are markable in the corpus. Figure 1a gives
the overall distribution of markable nominals in the
training data. As shown, 50% of nominal instances
are markable only 65% of the time or less, making
nominal classification an important first step. Using
this view of the data, Figure 1b presents the over-
all F1 scores for the baseline and LibLinear nominal
151
classifiers.6 As expected, gains in nominal classi-
fication diminish as nominals become more overtly
associated with arguments. Furthermore, nominals
that are rarely markable (i.e., those in interval 0.05)
remain problematic due to a lack of positive training
instances and the unbalanced nature of the classifi-
cation task.
5.2 Combined nominal-argument classification
We now turn to the task of combined nominal-
argument classification. In this task, systems must
first identify nominals that bear overt arguments. We
evaluated three configurations based on the nominal
classifiers from the previous section. Each config-
uration uses the argument classification model from
section 3.
As shown in Table 3, overall argument classifi-
cation F1 suffers a loss of more than 9% under the
assumption that all known nouns bear overt argu-
ments. This corresponds precisely to using the base-
line nominal classifier in the combined nominal-
argument task. The MLE nominal classifier is able
to reduce this loss by 25% to an F1 of 0.7080. The
LibLinear nominal classifier reduces this loss by
46%, resulting in an overall argument classification
F1 of 0.7235. This improvement is the direct result
of filtering out nominal instances that do not bear
overt arguments.
Similarly to the nominal evaluation, we can view
argument classification performance with respect to
the probability that a nominal bears overt arguments.
This is shown in Figure 1c for the three configura-
tions. The configuration using the MLE nominal
classifier obtains an argument F1 of zero for nom-
inals below its prediction threshold. Compared to
the baseline nominal classifier, the LibLinear clas-
sifier achieves argument classification gains as large
as 150.94% (interval 0.05), with an average gain of
52.87% for intervals 0.05 to 0.4. As with nomi-
nal classification, argument classification gains di-
minish for nominals that express arguments more
overtly - we observe an average gain of only 2.15%
for intervals 0.45 to 1.00. One possible explana-
tion for this is that the argument prediction model
has substantially more training data for the nomi-
nals in intervals 0.45 to 1.00. Thus, even if the nom-
6Baseline and MLE are identical above the MLE threshold.
Nominals
Deverbal Deverbal-like Other
Baseline 0.7975 0.6789 0.6757
MLE 0.8298 0.7332 0.7486
LibLinear 0.9261 0.8826 0.8905
Arguments
Baseline 0.7059 0.6738 0.7454
MLE 0.7206 0.6641 0.7675
LibLinear 0.7282 0.7178 0.7847
Table 5: Nominal and argument F1 scores for dever-
bal, deverbal-like, and other nominals in the all-token
evaluation.
inal classifier makes a false positive prediction in the
0.45 to 1.00 interval range, the argument model may
correctly avoid labeling any arguments.
As noted in section 2, these results are not di-
rectly comparable to the results of the recent CoNLL
Shared Task (Surdeanu et al, 2008). This is due to
the fact that the semantic labeled F1 in the Shared
Task combines predicate and argument predictions
into a single score. The same combined F1 score for
our best two-stage nominal SRL system (logistic re-
gression nominal and argument models) is 0.7806;
however, this result is not precisely comparable be-
cause we do not identify the predicate role set as re-
quired by the CoNLL Shared Task.
5.3 NomLex-based analysis of results
As demonstrated in section 1, NomBank annotates
many classes of deverbal and non-deverbal nomi-
nals, which have been categorized on syntactic and
semantic bases in NomLex-PLUS (Meyers, 2007b).
To help understand what types of nominals are par-
ticularly affected by implicit argumentation, we fur-
ther analyzed performance with respect to these
classes.
Figure 2a shows the distribution of nominals
across classes defined by the NomLex resource. As
shown in Figure 2b, many of the most frequent
classes exhibit significant gains. For example, the
classification of partitive nominals (13% of all nom-
inal instances) with the LibLinear classifier results
in gains of 55.45% and 33.72% over the baseline
and MLE classifiers, respectively. For the 5 most
common classes, which constitute 82% of all nomi-
nals instances, we observe average gains of 27.47%
and 19.30% over the baseline and MLE classifiers,
152
00.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
no
m
pa
rtit
ive
no
m
like
re
lat
ion
al
no
m
ing
att
rib
ute
en
vir
on
m
en
t
ab
ility
no
m
ad
j
wo
rk-
of-
ar
t
gro
up
no
m
ad
jlike job
sh
ar
e
ev
en
t
typ
e
ve
rs
ion
ha
llm
ar
k
ab
le-
no
m fie
ld
NomLex class
%
 
o
f n
o
m
in
al
 
in
st
an
ce
s
(a) Distribution of nominals across the NomLex classes. The
y-axis denotes the percentage of all nominal instances that is
occupied by nominals in the class.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
no
m
pa
rtit
ive
no
m
like
re
lat
ion
al
no
m
ing
att
rib
ute
en
vir
on
m
en
t
ab
ility
no
m
ad
j
wo
rk-
of-
ar
t
gro
up
no
m
ad
jlike job
sh
ar
e
ev
en
t
typ
e
ve
rs
ion
ha
llm
ar
k
ab
le-
no
m fie
ld
NomLex class
Pr
ed
ic
at
e 
n
o
m
in
al
 
F1
Baseline
MLE
LibLinear
(b) Nominal classification performance with respect to the
NomLex classes in Figure 2a. The y-axis denotes the com-
bined F1 for nominals in the class.
Figure 2: Evaluation results with respect to NomLex classes.
respectively.
Table 5 separates nominal and argument classifi-
cation results into sets of deverbal (NomLex class
nom), deverbal-like (NomLex class nom-like), and
all other nominalizations. A deverbal-like nominal
is closely related to some verb, although not mor-
phologically. For example, the noun accolade shares
argument interpretation with award, but the two are
not morphologically related. As shown by Table 5,
nominal classification tends to be easier - and ar-
gument classification harder - for deverbals when
compared to other types of nominals. The differ-
ence in argument F1 between deverbal/deverbal-like
nominals and the others is due primarily to relational
nominals, which are relatively easy to classify (Fig-
ure 2b); additionally, relational nominals exhibit a
high rate of argument incorporation, which is eas-
ily handled by the maximum-likelihood model de-
scribed in section 3.1.
6 Conclusions and future work
The application of nominal SRL to practical NLP
problems requires a system that is able to accurately
process each token it encounters. Previously, it was
unclear whether the models proposed by Jiang and
Ng (2006) and Liu and Ng (2007) would operate ef-
fectively in such an environment. The systems de-
scribed by Surdeanu et al (2008) are designed with
this environment in mind, but their evaluation did
not focus on the issue of implicit argumentation.
These two problems motivate the work presented in
this paper.
Our contribution is three-fold. First, we improve
upon previous nominal SRL results using a single-
stage classifier with additional new features. Sec-
ond, we show that this model suffers a substantial
performance degradation when evaluated over nom-
inals with implicit arguments. Finally, we identify a
set of features - many of them new - that can be used
to reliably detect nominals with explicit arguments,
thus significantly increasing the performance of the
nominal SRL system.
Our results also suggest interesting directions for
future work. As described in section 5.2, many nom-
inals do not have enough labeled training data to
produce accurate argument models. The general-
ization procedures developed by Gordon and Swan-
son (2007) for PropBank SRL and Pado? et al (2008)
for NomBank SRL might alleviate this problem.
Additionally, instead of ignoring nominals with im-
plicit arguments, we would prefer to identify the im-
plicit arguments using information contained in the
surrounding discourse. Such inferences would help
connect entities and events across sentences, provid-
ing a fuller interpretation of the text.
Acknowledgments
The authors would like to thank the anonymous re-
viewers for their helpful suggestions. The first two
authors were supported by NSF grants IIS-0535112
and IIS-0347548, and the third author was supported
by NSF grant IIS-0534700.
153
References
Collin Baker, Charles Fillmore, and John Lowe. 1998.
The Berkeley FrameNet project. In Christian Boitet
and Pete Whitelock, editors, Proceedings of the Thirty-
Sixth Annual Meeting of the Association for Computa-
tional Linguistics and Seventeenth International Con-
ference on Computational Linguistics, pages 86?90,
San Francisco, California. Morgan Kaufmann Publish-
ers.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the conll-2005 shared task: Semantic role labeling.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28:245?288.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic re-
lations between nominals. In Proceedings of the 4th
International Workshop on Semantic Evaluations.
A. Gordon and R. Swanson. 2007. Generalizing seman-
tic role annotations across syntactically similar verbs.
In Proceedings of ACL, pages 192?199.
Z. Jiang and H. Ng. 2006. Semantic role labeling of
nombank: A maximum entropy approach. In Proceed-
ings of the 2006 Conference on Empirical Methods in
Natural Language Processing.
Maria Lapata. 2000. The automatic interpretation
of nominalizations. In Proceedings of the Seven-
teenth National Conference on Artificial Intelligence
and Twelfth Conference on Innovative Applications of
Artificial Intelligence, pages 716?721. AAAI Press /
The MIT Press.
Chang Liu and Hwee Ng. 2007. Learning predictive
structures for semantic role labeling of nombank. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 208?215,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Adam Meyers. 2007a. Annotation guidelines for nom-
bank - noun argument structure for propbank. Techni-
cal report, New York University.
Adam Meyers. 2007b. Those other nombank dictionar-
ies. Technical report, New York University.
Sebastian Pado?, Marco Pennacchiotti, and Caroline
Sporleder. 2008. Semantic role assignment for event
nominalisations by leveraging verbal data. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 665?
672, Manchester, UK, August. Coling 2008 Organiz-
ing Committee.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Sameer Pradhan, Wayne Ward, and James H. Martin.
2005. Towards robust semantic role labeling. In Asso-
ciation for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings
of the Twelfth Conference on Computational Natu-
ral Language Learning, pages 159?177, Manchester,
England, August. Coling 2008 Organizing Committee.
154
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 423?431,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Who, What, When, Where, Why?  
Comparing Multiple Approaches to the Cross-Lingual 5W Task 
Kristen Parton*, Kathleen R. McKeown*, Bob Coyne*, Mona T. Diab*,  
Ralph Grishman?, Dilek Hakkani-T?r?, Mary Harper?, Heng Ji?, Wei Yun Ma*,  
Adam Meyers?, Sara Stolbach*, Ang Sun?, Gokhan Tur?, Wei Xu? and Sibel Yaman? 
 
*Columbia University 
New York, NY, USA 
{kristen, kathy, 
coyne, mdiab, ma, 
sara}@cs.columbia.edu 
 
?New York University 
New York, NY, USA 
{grishman, meyers, 
asun, xuwei} 
@cs.nyu.edu 
?International Computer 
Science Institute 
Berkeley, CA, USA 
{dilek, sibel} 
@icsi.berkeley.edu 
 
?Human Lang. Tech. Ctr. of 
Excellence, Johns Hopkins 
and U. of Maryland, 
College Park  
mharper@umd.edu 
?City University of  
New York 
New York, NY, USA 
hengji@cs.qc.cuny.edu 
 
 
?SRI International 
Palo Alto, CA, USA 
gokhan@speech.sri.com 
 
 
  
Abstract 
Cross-lingual tasks are especially difficult 
due to the compounding effect of errors in 
language processing and errors in machine 
translation (MT). In this paper, we present an 
error analysis of a new cross-lingual task: the 
5W task, a sentence-level understanding task 
which seeks to return the English 5W's (Who, 
What, When, Where and Why) corresponding 
to a Chinese sentence. We analyze systems 
that we developed, identifying specific prob-
lems in language processing and MT that 
cause errors. The best cross-lingual 5W sys-
tem was still 19% worse than the best mono-
lingual 5W system, which shows that MT 
significantly degrades sentence-level under-
standing. Neither source-language nor target-
language analysis was able to circumvent 
problems in MT, although each approach had 
advantages relative to the other. A detailed 
error analysis across multiple systems sug-
gests directions for future research on the 
problem. 
1 Introduction 
In our increasingly global world, it is ever more 
likely for a mono-lingual speaker to require in-
formation that is only available in a foreign lan-
guage document. Cross-lingual applications ad-
dress this need by presenting information in the 
speaker?s language even when it originally ap-
peared in some other language, using machine 
translation (MT) in the process. In this paper, we 
present an evaluation and error analysis of a 
cross-lingual application that we developed for a 
government-sponsored evaluation, the 5W task. 
The 5W task seeks to summarize the informa-
tion in a natural language sentence by distilling it 
into the answers to the 5W questions: Who, 
What, When, Where and Why. To solve this 
problem, a number of different problems in NLP 
must be addressed: predicate identification, ar-
gument extraction, attachment disambiguation, 
location and time expression recognition, and 
(partial) semantic role labeling. In this paper, we 
address the cross-lingual 5W task: given a 
source-language sentence, return the 5W?s trans-
lated (comprehensibly) into the target language. 
Success in this task requires a synergy of suc-
cessful MT and answer selection.  
The questions we address in this paper are: 
? How much does machine translation (MT) 
degrade the performance of cross-lingual 
5W systems, as compared to monolingual 
performance? 
? Is it better to do source-language analysis 
and then translate, or do target-language 
analysis on MT? 
? Which specific problems in language 
processing and/or MT cause errors in 5W 
answers?  
In this evaluation, we compare several differ-
ent approaches to the cross-lingual 5W task, two 
that work on the target language (English) and 
one that works in the source language (Chinese). 
423
A central question for many cross-lingual appli-
cations is whether to process in the source lan-
guage and then translate the result, or translate 
documents first and then process the translation. 
Depending on how errorful the translation is, 
results may be more accurate if models are de-
veloped for the source language. However, if 
there are more resources in the target language, 
then the translate-then-process approach may be 
more appropriate. We present a detailed analysis, 
both quantitative and qualitative, of how the ap-
proaches differ in performance.  
We also compare system performance on hu-
man translation (which we term reference trans-
lations) and MT of the same data in order to de-
termine how much MT degrades system per-
formance. Finally, we do an in-depth analysis of 
the errors in our 5W approaches, both on the 
NLP side and the MT side. Our results provide 
explanations for why different approaches suc-
ceed, along with indications of where future ef-
fort should be spent. 
2 Prior Work 
The cross-lingual 5W task is closely related to 
cross-lingual information retrieval and cross-
lingual question answering (Wang and Oard 
2006; Mitamura et al 2008). In these tasks, a 
system is presented a query or question in the 
target language and asked to return documents or 
answers from a corpus in the source language. 
Although MT may be used in solving this task, it 
is only used by the algorithms ? the final evalua-
tion is done in the source language. However, in 
many real-life situations, such as global business, 
international tourism, or intelligence work, users 
may not be able to read the source language. In 
these cases, users must rely on MT to understand 
the system response. (Parton et al 2008) exam-
ine the case of ?translingual? information re-
trieval, where evaluation is done on translated 
results in the target language. In cross-lingual 
information extraction (Sudo et al 2004) the 
evaluation is also done on MT, but the goal is to 
learn knowledge from a large corpus, rather than 
analyzing individual sentences.  
The 5W task is also closely related to Seman-
tic Role Labeling (SRL), which aims to effi-
ciently and effectively derive semantic informa-
tion from text. SRL identifies predicates and 
their arguments in a sentence, and assigns roles 
to each argument. For example, in the sentence 
?I baked a cake yesterday.?, the predicate 
?baked? has three arguments. ?I? is the subject of 
the predicate, ?a cake? is the object and ?yester-
day? is a temporal argument.  
Since the release of large data resources anno-
tated with relevant levels of semantic informa-
tion, such as the FrameNet (Baker et al, 1998) 
and PropBank corpora (Kingsbury and Palmer, 
2003), efficient approaches to SRL have been 
developed (Carreras and Marquez, 2005). Most 
approaches to the problem of SRL follow the 
Gildea and Jurafsky (2002) model. First, for a 
given predicate, the SRL system identifies its 
arguments' boundaries. Second, the Argument 
types are classified depending on an adopted 
lexical resource such as PropBank or FrameNet. 
Both steps are based on supervised learning over 
labeled gold standard data. A final step uses heu-
ristics to resolve inconsistencies when applying 
both steps simultaneously to the test data.  
Since many of the SRL resources are English, 
most of the SRL systems to date have been for 
English. There has been work in other languages 
such as German and Chinese (Erk 2006; Sun 
2004; Xue and Palmer 2005). The systems for 
the other languages follow the successful models 
devised for English, e.g. (Gildea and Palmer, 
2002; Chen and Rambow, 2003; Moschitti, 2004; 
Xue and Palmer, 2004; Haghighi et al, 2005). 
3 The Chinese-English 5W Task 
3.1 5W Task Description 
We participated in the 5W task as part of the 
DARPA GALE (Global Autonomous Language 
Exploitation) project. The goal is to identify the 
5W?s (Who, What, When, Where and Why) for a 
complete sentence. The motivation for the 5W 
task is that, as their origin in journalism suggests, 
the 5W?s cover the key information nuggets in a 
sentence. If a system can isolate these pieces of 
information successfully, then it can produce a 
pr?cis of the basic meaning of the sentence. Note 
that this task differs from QA tasks, where 
?Who? and ?What? usually refer to definition 
type questions. In this task, the 5W?s refer to se-
mantic roles within a sentence, as defined in Ta-
ble 1.  
In order to get al 5W?s for a sentence correct, 
a system must identify a top-level predicate, ex-
tract the correct arguments, and resolve attach-
ment ambiguity. In the case of multiple top-level 
predicates, any of the top-level predicates may be 
chosen. In the case of passive verbs, the Who is 
the agent (often expressed as a ?by clause?, or 
not stated), and the What should include the syn-
tactic subject.  
424
Answers are judged Correct1 if they identify a 
correct null argument or correctly extract an ar-
gument that is present in the sentence. Answers 
are not penalized for including extra text, such as 
prepositional phrases or subordinate clauses, 
unless the extra text includes text from another 
answer or text from another top-level predicate. 
In sentence 2a in Table 2, returning ?bought and 
cooked? for the What would be Incorrect. Simi-
larly, returning ?bought the fish at the market? 
for the What would also be Incorrect, since it 
contains the Where. Answers may also be judged 
Partial, meaning that only part of the answer was 
returned. For example, if the What contains the 
predicate but not the logical object, it is Partial.  
Since each sentence may have multiple correct 
sets of 5W?s, it is not straightforward to produce 
a gold-standard corpus for automatic evaluation. 
One would have to specify answers for each pos-
sible top-level predicate, as well as which parts 
of the sentence are optional and which are not 
allowed. This also makes creating training data 
for system development problematic. For exam-
ple, in Table 2, the sentence in 2a and 2b is the 
same, but there are two possible sets of correct 
answers. Since we could not rely on a gold-
standard corpus, we used manual annotation to 
judge our 5W system, described in section 5. 
3.2 The Cross-Lingual 5W Task 
In the cross-lingual 5W task, a system is given a 
sentence in the source language and asked to 
produce the 5W?s in the target language. In this 
task, both machine translation (MT) and 5W ex-
traction must succeed in order to produce correct 
answers. One motivation behind the cross-lingual 
5W task is MT evaluation. Unlike word- or 
phrase-overlap measures such as BLEU, the 5W 
evaluation takes into account ?concept? or ?nug-
get? translation. Of course, only the top-level 
predicate and arguments are evaluated, so it is 
not a complete evaluation. But it seeks to get at 
the understandability of the MT output, rather 
than just n-gram overlap. 
Translation exacerbates the problem of auto-
matically evaluating 5W systems. Since transla-
tion introduces paraphrase, rewording and sen-
tence restructuring, the 5W?s may change from 
one translation of a sentence to another transla-
tion of the same sentence. In some cases, roles 
may swap. For example, in Table 2, sentences 1a 
and 1b could be valid translations of the same 
                                                 
1
 The specific guidelines for determining correctness 
were formulated by BAE.  
Chinese sentence. They contain the same infor-
mation, but the 5W answers are different. Also, 
translations may produce answers that are textu-
ally similar to correct answers, but actually differ 
in meaning. These differences complicate proc-
essing in the source followed by translation. 
 
Example: On Tuesday, President Obama met with 
French President Sarkozy in Paris to discuss the 
economic crisis. 
W Definition Example  
answer 
WHO Logical subject of the 
top-level predicate in 
WHAT, or null. 
President 
Obama 
WHAT One of the top-level 
predicates in the sen-
tence, and the predi-
cate?s logical object. 
met with 
French Presi-
dent Sarkozy 
WHEN ARGM-TMP of the 
top-level predicate in 
WHAT, or null. 
On Tuesday 
WHERE ARGM-LOC of the 
top-level predicate in 
WHAT, or null. 
in Paris 
WHY ARGM-CAU of the 
top-level predicate in 
WHAT, or null. 
to discuss the 
economic crisis 
Table 1. Definition of the 5W task, and 5W answers 
from the example sentence above. 
4 5W System 
We developed a 5W combination system that 
was based on five other 5W systems. We se-
lected four of these different systems for evalua-
tion: the final combined system (which was our 
submission for the official evaluation), two sys-
tems that did analysis in the target-language 
(English), and one system that did analysis in the 
source language (Chinese). In this section, we 
describe the individual systems that we evalu-
ated, the combination strategy, the parsers that 
we tuned for the task, and the MT systems.  
 Sentence WHO WHAT 
1a Mary bought a cake 
from Peter. 
Mary bought a 
cake 
1b Peter sold Mary a 
cake. 
Peter sold Mary 
2a I bought the fish at 
the market yesterday 
and cooked it today. 
I bought the 
fish 
[WHEN: 
yesterday] 
2b I bought the fish at 
the market yesterday 
and cooked it today. 
I cooked it 
[WHEN: 
today] 
Table 2. Example 5W answers. 
425
4.1 Latent Annotation Parser 
For this work, we have re-implemented and en-
hanced the Berkeley parser (Petrov and Klein 
2007) in several ways: (1) developed a new 
method to handle rare words in English and Chi-
nese; (2) developed a new model of unknown 
Chinese words based on characters in the word; 
(3) increased robustness by adding adaptive 
modification of pruning thresholds and smooth-
ing of word emission probabilities. While the 
enhancements to the parser are important for ro-
bustness and accuracy, it is even more important 
to train grammars matched to the conditions of 
use. For example, parsing a Chinese sentence 
containing full-width punctuation with a parser 
trained on half-width punctuation reduces accu-
racy by over 9% absolute F. In English, parsing 
accuracy is seriously compromised by training a 
grammar with punctuation and case to process 
sentences without them.  
We developed grammars for English and Chi-
nese trained specifically for each genre by sub-
sampling from available treebanks (for English, 
WSJ, BN, Brown, Fisher, and Switchboard; for 
Chinese, CTB5) and transforming them for a 
particular genre (e.g., for informal speech, we 
replaced symbolic expressions with verbal forms 
and remove punctuation and case) and by utiliz-
ing a large amount of genre-matched self-labeled 
training parses. Given these genre-specific 
parses, we extracted chunks and POS tags by 
script. We also trained grammars with a subset of 
function tags annotated in the treebank that indi-
cate case role information (e.g., SBJ, OBJ, LOC, 
MNR) in order to produce function tags.   
4.2 Individual 5W Systems 
The English systems were developed for the 
monolingual 5W task and not modified to handle 
MT. They used hand-crafted rules on the output 
of the latent annotation parser to extract the 5Ws.  
English-function used the function tags from 
the parser to map parser constituents to the 5Ws. 
First the Who, When, Where and Why were ex-
tracted, and then the remaining pieces of the sen-
tence were returned as the What. The goal was to 
make sure to return a complete What answer and 
avoid missing the object. 
English-LF, on the other hand, used a system 
developed over a period of eight years (Meyers 
et al 2001) to map from the parser?s syntactic 
constituents into logical grammatical relations 
(GLARF), and then extracted the 5Ws from the 
logical form. As a back-up, it also extracted 
GLARF relations from another English-treebank 
trained parser, the Charniak parser (Charniak 
2001). After the parses were both converted to 
the 5Ws, they were then merged, favoring the 
system that: recognized the passive, filled more 
5W slots or produced shorter 5W slots (provid-
ing that the WHAT slot consisted of more than 
just the verb). A third back-up method extracted 
5Ws from part-of-speech tag patterns. Unlike 
English-function, English-LF explicitly tried to 
extract the shortest What possible, provided there 
was a verb and a possible object, in order to 
avoid multiple predicates or other 5W answers.  
Chinese-align uses the latent annotation 
parser (trained for Chinese) to parse the Chinese 
sentences. A dependency tree converter (Johans-
son and Nuges 2007) was applied to the constitu-
ent-based parse trees to obtain the dependency 
relations and determine top-level predicates. A 
set of hand-crafted dependency rules based on 
observation of Chinese OntoNotes were used to 
map from the Chinese function tags into Chinese 
5Ws.  Finally, Chinese-align used the alignments 
of three separate MT systems to translate the 
5Ws: a phrase-based system, a hierarchical 
phrase-based system, and a syntax augmented 
hierarchical phrase-based system. Chinese-align 
faced a number of problems in using the align-
ments, including the fact that the best MT did not 
always have the best alignment. Since the predi-
cate is essential, it tried to detect when verbs 
were deleted in MT, and back-off to a different 
MT system. It also used strategies for finding 
and correcting noisy alignments, and for filtering 
When/Where answers from Who and What.  
4.3 Hybrid System 
A merging algorithm was learned based on a de-
velopment test set. The algorithm selected all 
5W?s from a single system, rather than trying to 
merge W?s from different systems, since the 
predicates may vary across systems. For each 
document genre (described in section 5.4), we 
ranked the systems by performance on the devel-
opment data. We also experimented with a vari-
ety of features (for instance, does ?What? include 
a verb). The best-performing features were used 
in combination with the ranked list of priority 
systems to create a rule-based merger. 
4.4 MT Systems 
The MT Combination system used by both of the 
English 5W systems combined up to nine sepa-
rate MT systems. System weights for combina-
tion were optimized together with the language 
426
model score and word penalty for a combination 
of BLEU and TER (2*(1-BLEU) + TER). Res-
coring was applied after system combination us-
ing large language models and lexical trigger 
models. Of the nine systems, six were phrased-
based systems (one of these used chunk-level 
reordering of the Chinese, one used word sense 
disambiguation, and one used unsupervised Chi-
nese word segmentation), two were hierarchical 
phrase-based systems, one was a string-to-
dependency system, one was syntax-augmented, 
and one was a combination of two other systems. 
Bleu scores on the government supplied test set 
in December 2008 were 35.2 for formal text, 
29.2 for informal text, 33.2 for formal speech, 
and 27.6 for informal speech. More details may 
be found in (Matusov et al 2009). 
5 Methods 
5.1 5W Systems 
For the purposes of this evaluation2, we com-
pared the output of 4 systems: English-Function, 
English-LF, Chinese-align, and the combined 
system. Each English system was also run on 
reference translations of the Chinese sentence. 
So for each sentence in the evaluation corpus, 
there were 6 systems that each provided 5Ws. 
5.2 5W Answer Annotation 
For each 5W output, annotators were presented 
with the reference translation, the MT version, 
and the 5W answers. The 5W system names 
were hidden from the annotators. Annotators had 
to select ?Correct?, ?Partial? or ?Incorrect? for 
each W. For answers that were Partial or Incor-
rect, annotators had to further specify the source 
of the error based on several categories (de-
scribed in section 6). All three annotators were 
native English speakers who were not system 
developers for any of the 5W systems that were 
being evaluated (to avoid biased grading, or as-
signing more blame to the MT system). None of 
the annotators knew Chinese, so all of the judg-
ments were based on the reference translations. 
After one round of annotation, we measured 
inter-annotator agreement on the Correct, Partial, 
or Incorrect judgment only. The kappa value was 
0.42, which was lower than we expected. An-
other surprise was that the agreement was lower 
                                                 
2
 Note that an official evaluation was also performed by 
DARPA and BAE. This evaluation provides more fine-
grained detail on error types and gives results for the differ-
ent approaches. 
for When, Where and Why (?=0.31) than for 
Who or What (?=0.48). We found that, in cases 
where a system would get both Who and What 
wrong, it was often ambiguous how the remain-
ing W?s should be graded. Consider the sentence: 
?He went to the store yesterday and cooked lasa-
gna today.? A system might return erroneous 
Who and What answers, and return Where as ?to 
the store? and When as ?today.? Since Where 
and When apply to different predicates, they 
cannot both be correct. In order to be consistent, 
if a system returned erroneous Who and What 
answers, we decided to mark the When, Where 
and Why answers Incorrect by default. We added 
clarifications to the guidelines and discussed ar-
eas of confusion, and then the annotators re-
viewed and updated their judgments.  
After this round of annotating, ?=0.83 on the 
Correct, Partial, Incorrect judgments. The re-
maining disagreements were genuinely ambigu-
ous cases, where a sentence could be interpreted 
multiple ways, or the MT could be understood in 
various ways. There was higher agreement on 
5W?s answers from the reference text compared 
to MT text, since MT is inherently harder to 
judge and some annotators were more flexible 
than others in grading garbled MT. 
5.3 5W Error Annotation 
In addition to judging the system answers by the 
task guidelines, annotators were asked to provide 
reason(s) an answer was wrong by selecting from 
a list of predefined errors. Annotators were asked 
to use their best judgment to ?assign blame? to 
the 5W system, the MT, or both. There were six 
types of system errors and four types of MT er-
rors, and the annotator could select any number 
of errors. (Errors are described further in section 
6.) For instance, if the translation was correct, 
but the 5W system still failed, the blame would 
be assigned to the system. If the 5W system 
picked an incorrectly translated argument (e.g., 
?baked a moon? instead of ?baked a cake?), then 
the error would be assigned to the MT system. 
Annotators could also assign blame to both sys-
tems, to indicate that they both made mistakes.  
Since this annotation task was a 10-way selec-
tion, with multiple selections possible, there were 
some disagreements. However, if categorized 
broadly into 5W System errors only, MT errors 
only, and both 5W System and MT errors, then 
the annotators had a substantial level of agree-
ment (?=0.75 for error type, on sentences where 
both annotators indicated an error).  
427
5.4 5 W Corpus 
The full evaluation corpus is 350 documents, 
roughly evenly divided between four genres: 
formal text (newswire), informal text (blogs and 
newsgroups), formal speech (broadcast news) 
and informal speech (broadcast conversation). 
For this analysis, we randomly sampled docu-
ments to judge from each of the genres. There 
were 50 documents (249 sentences) that were 
judged by a single annotator. A subset of that set, 
with 22 documents and 103 sentences, was 
judged by two annotators. In comparing the re-
sults from one annotator to the results from both 
annotators, we found substantial agreement. 
Therefore, we present results from the single an-
notator so we can do a more in-depth analysis. 
Since each sentence had 5W?s, and there were 6 
systems that were compared, there were 7,500 
single-annotator judgments over 249 sentences. 
6 Results 
Figure 1 shows the cross-lingual performance 
(on MT) of all the systems for each 5W. The best 
monolingual performance (on human transla-
tions) is shown as a dashed line (% Correct 
only). If a system returned Incorrect answers for 
Who and What, then the other answers were 
marked Incorrect (as explained in section 5.2). 
For the last 3W?s, the majority of errors were due 
to this (details in Figure 1), so our error analysis 
focuses on the Who and What questions. 
6.1 Monolingual 5W Performance 
To establish a monolingual baseline, the Eng-
lish 5W system was run on reference (human) 
translations of the Chinese text. For each partial 
or incorrect answer, annotators could select one 
or more of these reasons: 
? Wrong predicate or multiple predicates. 
? Answer contained another 5W answer. 
? Passive handled wrong (WHO/WHAT). 
? Answer missed. 
? Argument attached to wrong predicate. 
Figure 1 shows the performance of the best 
monolingual system for each 5W as a dashed 
line. The What question was the hardest, since it 
requires two pieces of information (the predicate 
and object). The When, Where and Why ques-
tions were easier, since they were null most of 
the time. (In English OntoNotes 2.0, 38% of sen-
tences have a When, 15% of sentences have a 
Where, and only 2.6% of sentences have a Why.) 
The most common monolingual system error on 
these three questions was a missed answer, ac-
counting for all of the Where errors, all but one 
Why error and 71% of the When errors. The re-
maining When errors usually occurred when the 
system assumed the wrong sense for adverbs 
(such as ?then? or ?just?). 
 Missing Other 
5W 
Wrong/Multiple 
Predicates 
Wrong 
REF-func 37 29 22 7 
REF-LF 54 20 17 13 
MT-func 18 18 18 8 
MT-LF 26 19 10 11 
Chinese 23 17 14 8 
Hybrid 13 17 15 12 
Table 3. Percentages of Who/What errors attributed to 
each system error type. 
The top half of Table 3 shows the reasons at-
tributed to the Who/What errors for the reference 
corpus. Since English-LF preferred shorter an-
swers, it frequently missed answers or parts of 
Figure 1. System performance on each 5W. ?Partial? indicates that part of the answer was missing. Dashed lines 
show the performance of the best monolingual system (% Correct on human translations). For the last 3W?s, the 
percent of answers that were Incorrect ?by default? were: 30%, 24%, 27% and 22%, respectively, and 8% for the 
best monolingual system 
60 60 56 66
36 40 38 42
56 59 59 64 63
70 66 73 68 75 71 78
19201914
0
10
20
30
40
50
60
70
80
90
100
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
WHO WHAT WHEN WHERE WHY
Partia l
Correct
90
75 81
83 90
Best 
mono-
lingual
428
answers. English-LF also had more Partial an-
swers on the What question: 66% Correct and 
12% Partial, versus 75% Correct and 1% Partial 
for English-function. On the other hand, English-
function was more likely to return answers that 
contained incorrect extra information, such as 
another 5W or a second predicate. 
6.2 Effect of MT on 5W Performance 
The cross-lingual 5W task requires that systems 
return intelligible responses that are semantically 
equivalent to the source sentence (or, in the case 
of this evaluation, equivalent to the reference).  
As can be seen in Figure 1, MT degrades the 
performance of the 5W systems significantly, for 
all question types, and for all systems. Averaged 
over all questions, the best monolingual system 
does 19% better than the best cross-lingual sys-
tem. Surprisingly, even though English-function 
outperformed English-LF on the reference data, 
English-LF does consistently better on MT. This 
is likely due to its use of multiple back-off meth-
ods when the parser failed.  
6.3 Source-Language vs. Target-Language 
The Chinese system did slightly worse than ei-
ther English system overall, but in the formal 
text genre, it outperformed both English systems.  
Although the accuracies for the Chinese and 
English systems are similar, the answers vary a 
lot. Nearly half (48%) of the answers can be an-
swered correctly by both the English system and 
the Chinese system. But 22% of the time, the 
English system returned the correct answer when 
the Chinese system did not. Conversely, 10% of 
the answers were returned correctly by the Chi-
nese system and not the English systems. The 
hybrid system described in section 4.2 attempts 
to exploit these complementary advantages. 
After running the hybrid system, 61% of the 
answers were from English-LF, 25% from Eng-
lish-function, 7% from Chinese-align, and the 
remaining 7% were from the other Chinese 
methods (not evaluated here). The hybrid did 
better than its parent systems on all 5Ws, and the 
numbers above indicate that further improvement 
is possible with a better combination strategy.  
6.4 Cross-Lingual 5W Error Analysis 
For each Partial or Incorrect answer, annotators 
were asked to select system errors, translation 
errors, or both. (Further analysis is necessary to 
distinguish between ASR errors and MT errors.) 
The translation errors considered were: 
? Word/phrase deleted. 
? Word/phrase mistranslated. 
? Word order mixed up. 
? MT unreadable. 
Table 4 shows the translation reasons attrib-
uted to the Who/What errors. For all systems, the 
errors were almost evenly divided between sys-
tem-only, MT-only and both, although the Chi-
nese system had a higher percentage of system-
only errors. The hybrid system was able to over-
come many system errors (for example, in Table 
2, only 13% of the errors are due to missing an-
swers), but still suffered from MT errors. 
Table 4. Percentages of Who/What errors by each 
system attributed to each translation error type. 
Mistranslation was the biggest translation 
problem for all the systems. Consider the first 
example in Figure 3. Both English systems cor-
rectly extracted the Who and the When, but for 
Mistrans-
lation 
Deletion Word 
Order 
Unreadable 
MT-func 34 18 24 18 
MT-LF 29 22 21 14 
Chinese 32 17 9 13 
Hybrid 35 19 27 18 
MT: After several rounds of reminded, I was a little bit 
Ref: After several hints, it began to come back to me. 
 ??????,?????????? 
MT: The Guizhou province, within a certain bank robber, under the watchful eyes of a weak woman, and, with a 
knife stabbed the woman. 
Ref: I saw that in a bank in Guizhou Province, robbers seized a vulnerable young woman in front of a group of 
onlookers and stabbed the woman with a knife. 
 ?????????,?????????,???????,??,???????? 
MT: Woke up after it was discovered that the property is not more than eleven people do not even said that the 
memory of the receipt of the country into the country. 
Ref: Well, after waking up, he found everything was completely changed. Apart from having additional eleven 
grandchildren, even the motherland as he recalled has changed from a socialist country to a capitalist country. 
 ?????????????,?????????,????????????????????????? 
Figure 3 Example sentences that presented problems for the 5W systems. 
 
429
What they returned ?was a little bit.? This is the 
correct predicate for the sentence, but it does not 
match the meaning of the reference. The Chinese 
5W system was able to select a better translation, 
and instead returned ?remember a little bit.? 
Garbled word order was chosen for 21-24% of 
the target-language system Who/What errors, but 
only 9% of the source-language system 
Who/What errors. The source-language word 
order problems tended to be local, within-phrase 
errors (e.g., ?the dispute over frozen funds? was 
translated as ?the freezing of disputes?). The tar-
get-language system word order problems were 
often long-distance problems. For example, the 
second sentence in Figure 3 has many phrases in 
common with the reference translation, but the 
overall sentence makes no sense. The watchful 
eyes actually belong to a ?group of onlookers? 
(deleted). Ideally, the robber would have 
?stabbed the woman? ?with a knife,? rather than 
vice versa. Long-distance phrase movement is a 
common problem in Chinese-English MT, and 
many MT systems try to handle it (e.g., Wang et 
al. 2007). By doing analysis in the source lan-
guage, the Chinese 5W system is often able to 
avoid this problem ? for example, it successfully 
returned ?robbers? ?grabbed a weak woman? for 
the Who/What of this sentence. 
Although we expected that the Chinese system 
would have fewer problems with MT deletion, 
since it could choose from three different MT 
versions, MT deletion was a problem for all sys-
tems. In looking more closely at the deletions, 
we noticed that over half of deletions were verbs 
that were completely missing from the translated 
sentence. Since MT systems are tuned for word-
based overlap measures (such as BLEU), verb 
deletion is penalized equally as, for example, 
determiner deletion. Intuitively, a verb deletion 
destroys the central meaning of a sentence, while 
a determiner is rarely necessary for comprehen-
sion. Other kinds of deletions included noun 
phrases, pronouns, named entities, negations and 
longer connecting phrases.  
Deletion also affected When and Where. De-
leting particles such as ?in? and ?when? that in-
dicate a location or temporal argument caused 
the English systems to miss the argument. Word 
order problems in MT also caused attachment 
ambiguity in When and Where. 
The ?unreadable? category was an option of 
last resort for very difficult MT sentences. The 
third sentence in Figure 3 is an example where 
ASR and MT errors compounded to create an 
unparseable sentence.  
7 Conclusions 
In our evaluation of various 5W systems, we dis-
covered several characteristics of the task. The 
What answer was the hardest for all systems, 
since it is difficult to include enough information 
to cover the top-level predicate and object, with-
out getting penalized for including too much. 
The challenge in the When, Where and Why 
questions is due to sparsity ? these responses 
occur in much fewer sentences than Who and 
What, so systems most often missed these an-
swers. Since this was a new task, this first 
evaluation showed clear issues on the language 
analysis side that can be improved in the future. 
The best cross-lingual 5W system was still 
19% worse than the best monolingual 5W sys-
tem, which shows that MT significantly degrades 
sentence-level understanding. A serious problem 
in MT for systems was deletion. Chinese con-
stituents that were never translated caused seri-
ous problems, even when individual systems had 
strategies to recover. When the verb was deleted, 
no top level predicate could be found and then all 
5Ws were wrong.  
One of our main research questions was 
whether to extract or translate first. We hypothe-
sized that doing source-language analysis would 
be more accurate, given the noise in Chinese 
MT, but the systems performed about the same. 
This is probably because the English tools (logi-
cal form extraction and parser) were more ma-
ture and accurate than the Chinese tools.  
Although neither source-language nor target-
language analysis was able to circumvent prob-
lems in MT, each approach had advantages rela-
tive to the other, since they did well on different 
sets of sentences. For example, Chinese-align 
had fewer problems with word order, and most 
of those were due to local word-order problems.  
Since the source-language and target-language 
systems made different kinds of mistakes, we 
were able to build a hybrid system that used the 
relative advantages of each system to outperform 
all systems. The different types of mistakes made 
by each system suggest features that can be used 
to improve the combination system in the future. 
Acknowledgments 
This work was supported in part by the Defense 
Advanced Research Projects Agency (DARPA) 
under contract number HR0011-06-C-0023. Any 
opinions, findings and conclusions or recom-
mendations expressed in this material are the 
authors' and do not necessarily reflect those of 
the sponsors. 
430
References  
Collin F. Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet project. In 
COLING-ACL '98: Proceedings of the Conference, 
held at the University of Montr?al, pages 86?90. 
Xavier Carreras and Llu?s M?rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role 
labeling. In Proceedings of the Ninth Conference 
on Computational Natural Language Learning 
(CoNLL-2005), pages 152?164. 
Eugene Charniak. 2001. Immediate-head parsing for 
language models. In Proceedings of the 39th An-
nual Meeting on Association For Computational 
Linguistics (Toulouse, France, July 06 - 11, 2001).   
John Chen and Owen Rambow. 2003. Use of deep 
linguistic features for the recognition and labeling 
of semantic arguments. In Proceedings of the 2003 
Conference on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan. 
Katrin Erk and Sebastian Pado. 2006. Shalmaneser ? 
a toolchain for shallow semantic parsing. Proceed-
ings of LREC. 
Daniel Gildea and Daniel Jurafsky. 2002. Automatic 
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288. 
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. 
In Proceedings of the 40th Annual Conference of 
the Association for Computational Linguistics 
(ACL-02), Philadelphia, PA, USA. 
Mary Harper and Zhongqiang Huang. 2009. Chinese 
Statistical Parsing, chapter to appear. 
Aria Haghighi, Kristina Toutanova, and Christopher 
Manning. 2005. A joint model for semantic role la-
beling. In Proceedings of the Ninth Conference on 
Computational Natural Language Learning 
(CoNLL-2005), pages 173?176.  
Paul Kingsbury and Martha Palmer. 2003. Propbank: 
the next level of treebank. In Proceedings of Tree-
banks and Lexical Theories. 
Evgeny Matusov, Gregor Leusch, & Hermann Ney: 
Learning to combine machine translation systems.  
In: Cyril Goutte, Nicola Cancedda, Marc Dymet-
man, & George Foster (eds.) Learning machine 
translation. (Cambridge, Mass.: The MIT Press, 
2009); pp.257-276. 
Adam Meyers, Ralph Grishman, Michiko Kosaka and 
Shubin Zhao.  2001. Covering Treebanks with 
GLARF. In Proceedings of the ACL 2001 Work-
shop on Sharing Tools and Resources. Annual 
Meeting of the ACL. Association for Computa-
tional Linguistics, Morristown, NJ, 51-58. 
Teruko Mitamura, Eric Nyberg, Hideki Shima, 
Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, 
Ruihua Song, Chuan-Jie Lin, Tetsuya Sakai, 
Donghong Ji, and Noriko Kando. 2008. Overview 
of the NTCIR-7 ACLIA Tasks: Advanced Cross-
Lingual Information Access. In Proceedings of the 
Seventh NTCIR Workshop Meeting. 
Alessandro Moschitti, Silvia Quarteroni, Roberto 
Basili, and Suresh Manandhar. 2007. Exploiting 
syntactic and shallow semantic kernels for question 
answer classification. In Proceedings of the 45th 
Annual Meeting of the Association of Computa-
tional Linguistics, pages 776?783.  
Kristen Parton, Kathleen R. McKeown, James Allan, 
and Enrique Henestroza. Simultaneous multilingual 
search for translingual information retrieval. In 
Proceedings of ACM 17th Conference on Informa-
tion and Knowledge Management (CIKM), Napa 
Valley, CA, 2008. 
Slav Petrov and Dan Klein. 2007. Improved Inference 
for Unlexicalized Parsing. North American Chapter 
of the Association for Computational Linguistics 
(HLT-NAACL 2007). 
Sudo, K., Sekine, S., and Grishman, R. 2004. Cross-
lingual information extraction system evaluation. 
In Proceedings of the 20th international Confer-
ence on Computational Linguistics. 
Honglin Sun and Daniel Jurafsky. 2004. Shallow Se-
mantic Parsing of Chinese. In Proceedings of 
NAACL-HLT. 
Cynthia A. Thompson, Roger Levy, and Christopher 
Manning. 2003. A generative model for semantic 
role labeling. In 14th European Conference on Ma-
chine Learning. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. In Dekang Lin 
and Dekai Wu, editors, Proceedings of EMNLP 
2004, pages 88?94, Barcelona, Spain, July. Asso-
ciation for Computational Linguistics. 
Xue, Nianwen and Martha Palmer. 2005. Automatic 
semantic role labeling for Chinese verbs. InPro-
ceedings of the Nineteenth International Joint Con-
ference on Artificial Intelligence, pages 1160-1165.  
Chao Wang, Michael Collins, and Philipp Koehn. 
2007. Chinese Syntactic Reordering for Statistical 
Machine Translation. Proceedings of the 2007 Joint 
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), 737-745. 
Jianqiang Wang and Douglas W. Oard, 2006. "Com-
bining Bidirectional Translation and Synonymy for 
Cross-Language Information Retrieval," in 29th 
Annual International ACM SIGIR Conference on 
Research and Development in Information Re-
trieval, pp. 202-209. 
431
Covering Treebanks with GLARF
Adam Meyers
 
and Ralph Grishman   and Michiko Kosaka  and Shubin Zhao  
 
New York University, 719 Broadway, 7th Floor, NY, NY 10003 USA
 Monmouth University, West Long Branch, N.J. 07764, USA
meyers/grishman/shubinz@cs.nyu.edu, kosaka@monmouth.edu
Abstract
This paper introduces GLARF, a frame-
work for predicate argument structure.
We report on converting the Penn Tree-
bank II into GLARF by automatic
methods that achieved about 90% pre-
cision/recall on test sentences from the
Penn Treebank. Plans for a corpus
of hand-corrected output, extensions of
GLARF to Japanese and applications
for MT are also discussed.
1 Introduction
Applications using annotated corpora are often,
by design, limited by the information found in
those corpora. Since most English treebanks pro-
vide limited predicate-argument (PRED-ARG)
information, parsers based on these treebanks do
not produce more detailed predicate argument
structures (PRED-ARG structures). The Penn
Treebank II (Marcus et al, 1994) marks sub-
jects (SBJ), logical objects of passives (LGS),
some reduced relative clauses (RRC), as well as
other grammatical information, but does not mark
each constituent with a grammatical role. In our
view, a full PRED-ARG description of a sen-
tence would do just that: assign each constituent
a grammatical role that relates that constituent to
one or more other constituents in the sentence.
For example, the role HEAD relates a constituent
to its parent and the role OBJ relates a constituent
to the HEAD of its parent. We believe that the
absence of this detail limits the range of appli-
cations for treebank-based parsers. In particu-
lar, they limit the extent to which it is possible
to generalize, e.g., marking IND-OBJ and OBJ
roles allows one to generalize a single pattern to
cover two related examples (?John gave Mary a
book? = ?John gave a book to Mary?). Distin-
guishing complement PPs (COMP) from adjunct
PPs (ADV) is useful because the former is likely
to have an idiosyncratic interpretation, e.g., the
object of ?at? in ?John is angry at Mary? is not
a locative and should be distinguished from the
locative case by many applications.
In an attempt to fill this gap, we have begun
a project to add this information using both au-
tomatic procedures and hand-annotation. We are
implementing automatic procedures for mapping
the Penn Treebank II (PTB) into a PRED-ARG
representation and then we are correcting the out-
put of these procedures manually. In particular,
we are hoping to encode information that will en-
able a greater level of regularization across lin-
guistic structures than is possible with PTB.
This paper introduces GLARF, the Grammati-
cal and Logical Argument Representation Frame-
work. We designed GLARF with four objec-
tives in mind: (1) capturing regularizations ?
noncanonical constructions (e.g., passives, filler-
gap constructions, etc.) are represented in terms
of their canonical counterparts (simple declara-
tive clauses); (2) representing all phenomena us-
ing one simple data structure: the typed feature
structure (3) consistently labeling all arguments
and adjuncts for phrases with clear heads; and (4)
producing clear and consistent PRED-ARGs for
phrases that do not have heads, e.g., conjoined
structures, named entities, etc. ? rather than try-
ing to squeeze these phrases into an X-bar mold,
we customized our representations to reflect their
head-less properties. We believe that a framework
for PRED-ARG needs to satisfy these objectives
to adequately cover a corpus like PTB.
We believe that GLARF, because of its uni-
form treatment of PRED-ARG relations, will be
valuable for many applications, including ques-
tion answering, information extraction, and ma-
chine translation. In particular, for MT, we ex-
pect it will benefit procedures which learn trans-
lation rules from syntactically analyzed parallel
corpora, such as (Matsumoto et al, 1993; Mey-
ers et al, 1996). Much closer alignments will
be possible using GLARF, because of its multi-
ple levels of representation, than would be pos-
sible with surface structure alone (An example is
provided at the end of Section 2). For this reason,
we are currently investigating the extension of our
mapping procedure to treebanks of Japanese (the
Kyoto Corpus) and Spanish (the UAM Treebank
(Moreno et al, 2000)). Ultimately, we intend to
create a parallel trilingual treebank using a com-
bination of automatic methods and human correc-
tion. Such a treebank would be valuable resource
for corpus-trained MT systems.
The primary goal of this paper is to discuss the
considerations for adding PRED-ARG informa-
tion to PTB, and to report on the performance of
our mapping procedure. We intend to wait until
these procedures are mature before beginning an-
notation on a larger scale. We also describe our
initial research on covering the Kyoto Corpus of
Japanese with GLARF.
2 Previous Treebanks
There are several corpora annotated with PRED-
ARG information, but each encode some dis-
tinctions that are different. The Susanne Cor-
pus (Sampson, 1995) consists of about 1/6 of the
Brown Corpus annotated with detailed syntactic
information. Unlike GLARF, the Susanne frame-
work does not guarantee that each constituent be
assigned a grammatical role. Some grammatical
roles (e.g., subject, object) are marked explicitly,
others are implied by phrasetags (Fr corresponds
to the GLARF node label SBAR under a REL-
ATIVE arc label) and other constituents are not
assigned roles (e.g., constituents of NPs). Apart
from this concern, it is reasonable to ask why
we did not adapt this scheme for our use. Su-
sanne?s granularity surpasses PTB-based GLARF
in many areas with about 350 wordtags (part of
speech) and 100 phrasetags (phrase node labels).
However, GLARF would express many of the de-
tails in other ways, using fewer node and part of
speech (POS) labels and more attributes and role
labels. In the feature structure tradition, GLARF
can represent varying levels of detail by adding
or subtracting attributes or defining subsumption
hierarchies. Thus both Susanne?s NP1p word-
tag and Penn?s NNP wordtag would correspond
to GLARF?s NNP POS tag. A GLARF-style
Susanne analysis of ?Ontario, Canada? is (NP
(PROVINCE (NNP Ontario)) (PUNCTUATION
(, ,)) (COUNTRY (NNP Canada)) (PATTERN
NAME) (SEM-FEATURE LOC)). A GLARF-
style PTB analysis uses the roles NAME1 and
NAME2 instead of PROVINCE and COUNTRY,
where name roles (NAME1, NAME2) are more
general than PROVINCE and COUNTRY in a
subsumption hierarchy. In contrast, attempts to
convert PTB into Susanne would fail because de-
tail would be unavailable. Similarly, attempts to
convert Susanne into the PTB framework would
lose information. In summary, GLARF?s ability
to represent varying levels of detail allows dif-
ferent types of treebank formats to be converted
into GLARF, even if they cannot be converted into
each other. Perhaps, GLARF can become a lingua
franca among annotated treebanks.
The Negra Corpus (Brants et al, 1997) pro-
vides PRED-ARG information for German, simi-
lar in granularity to GLARF. The most significant
difference is that GLARF regularizes some phe-
nomena which a Negra version of English would
probably not, e.g., control phenomena. Another
novel feature of GLARF is the ability to represent
paraphrases (in the Harrisian sense) that are not
entirely syntactic, e.g., nominalizations as sen-
tences. Other schemes seem to only regularize
strictly syntactic phenomena.
3 The Structure of GLARF
In GLARF, each sentence is represented by a
typed feature structure. As is standard, we
model feature structures as single-rooted directed
acyclic graphs (DAGs). Each nonterminal is la-
beled with a phrase category, and each leaf is la-
beled with either: (a) a (PTB) POS label and a
word (eat, fish, etc.) or (b) an attribute value (e.g.,
singular, passive, etc.). Types are based on non-
terminal node labels, POSs and other attributes
(Carpenter, 1992). Each arc bears a feature label
which represents either a grammatical role (SBJ,
OBJ, etc.) or some attribute of a word or phrase
(morphological features, tense, semantic features,
etc.).1 For example, the subject of a sentence is
the head of a SBJ arc, an attribute like SINGU-
LAR is the head of a GRAM-NUMBER arc, etc.
A constituent involved in multiple surface or log-
ical relations may be at the head of multiple arcs.
For example, the surface subject (S-SBJ) of a pas-
sive verb is also the logical object (L-OBJ). These
two roles are represented as two arcs which share
the same head. This sort of structure sharing anal-
ysis originates with Relational Grammar and re-
lated frameworks (Perlmutter, 1984; Johnson and
Postal, 1980) and is common in Feature Structure
frameworks (LFG, HPSG, etc.). Following (John-
son et al, 1993)2, arcs are typed. There are five
different types of role labels:
 Attribute roles: Gram-Number (grammati-
cal number), Mood, Tense, Sem-Feature (se-
mantic features like temporal/locative), etc.
 Surface-only relations (prefixed with S-),
e.g., the surface subject (S-SBJ) of a passive.
 Logical-only Roles (prefixed with L-), e.g.,
the logical object (L-OBJ) of a passive.
 Intermediate roles (prefixed with I-) repre-
senting neither surface, nor logical positions.
In ?John seemed to be kidnapped by aliens?,
?John? is the surface subject of ?seem?, the
logical object of ?kidnapped?, and the in-
termediate subject of ?to be?. Intermedi-
ate arcs capture are helpful for modeling the
way sentences conform to constraints. The
intermediate subject arc obeys lexical con-
straints and connect the surface subjects of
?seem? (COMLEX Syntax class TO-INF-
RS (Macleod et al, 1998a)) to the subject
of the infinitive. However, the subject of the
infinitive in this case is not a logical sub-
ject due to the passive. In some cases, in-
termediate arcs are subject to number agree-
ment, e.g., in ?Which aliens did you say
were seen??, the I-SBJ of ?were seen? agrees
with ?were?.
 Combined surface/logical roles (unprefixed
arcs, which we refer to as SL- arcs). For ex-
1A few grammatical roles are nonfunctional, e.g., a con-
stituent can have multiple ADV constituents. We number
these roles (ADV1, ADV2,  ) to preserve functionality.
2That paper uses two arc types: category and relational.
ample, ?John? in ?John ate cheese? would be
the target of a SBJ subject arc.
Logical relations, encoded with SL- and L-
arcs, are defined more broadly in GLARF than
in most frameworks. Any regularization from a
non-canonical linguistic structure to a canonical
one results in logical relations. Following (Harris,
1968) and others, our model of canonical linguis-
tic structure is the tensed active indicative sen-
tence with no missing arguments. The following
argument types will be at the head of logical (L-)
arcs based on counterparts in canonical sentences
which are at the head of SL- arcs: logical argu-
ments of passives, understood subjects of infini-
tives, understood fillers of gaps, and interpreted
arguments of nominalizations (In ?Rome?s de-
struction of Carthage?, ?Rome? is the logical sub-
ject and ?Carthage? is the logical object). While
canonical sentence structure provides one level
of regularization, canonical verb argument struc-
tures provide another. In the case of argument al-
ternations (Levin, 1993), the same role marks an
alternating argument regardless of where it occurs
in a sentence. Thus ?the man? is the indirect ob-
ject (IND-OBJ) and ?a dollar? is the direct object
(OBJ) in both ?She gave the man a dollar? and
?She gave a dollar to the man? (the dative alter-
nation). Similarly, ?the people? is the logical ob-
ject (L-OBJ) of both ?The people evacuated from
the town? and ?The troops evacuated the people
from the town?, when we assume the appropriate
regularization. Encoding this information allows
applications to generalize. For example, a single
Information Extraction pattern that recognizes the
IND-OBJ/OBJ distinction would be able to han-
dle these two examples. Without this distinction,
2 patterns would be needed.
Due to the diverse types of logical roles, we
sub-type roles according to the type of regu-
larization that they reflect. Depending on the
application, one can apply different filters to a
detailed GLARF representation, only looking at
certain types of arcs. For example, one might
choose all logical (L- and SL-) roles for an
application that is trying to acquire selection
restrictions, or all surface (S- and SL-) roles
if one was interested in obtaining a surface
parse. For other applications, one might want to
choose between subtypes of logical arcs. Given
(S (NP-SBJ (PRP they))
(VP (VP (VBD spent)
(NP-2 ($ $)
(CD 325,000)
(-NONE- *U*))
(PP-TMP-3 (IN in)
(NP (CD 1989))))
(CC and)
(VP (NP=2 ($ $)
(CD 340,000)
(-NONE- *U*))
(PP-TMP=3 (IN in)
(NP (CD 1990))))))
Figure 1: Penn representation of gapping
a trilingual treebank, suppose that a Spanish
treebank sentence corresponds to a Japanese
nominalization phrase and an English nominal-
ization phrase, e.g.,
Disney ha comprado Apple Computers
Disney?s acquisition of Apple Computers
Furthermore, suppose that the English treebank
analyzes the nominalization phrase both as an
NP (Disney = possessive, Apple Computers =
object of preposition) and as a paraphrase of a
sentence (Disney = subject, Apple Computers
= object). For an MT system that aligns the
Spanish and English graph representation, it
may be useful to view the nominalization phrase
in terms of the clausal arguments. However,
in a Japanese/English system, we may only
want to look at the structure of the English
nominalization phrase as an NP.
4 GLARF and the Penn Treebank
This section focuses on some characteristics of
English GLARF and how we map PTB into
GLARF, as exemplified by mapping the PTB rep-
resentation in Figure 1 to the GLARF representa-
tion in Figure 2. In the process, we will discuss
how some of the more interesting linguistic phe-
nomena are represented in GLARF.
4.1 Mapping into GLARF
Our procedure for mapping PTB into GLARF
uses a sequence of transformations. The first
transformation applies to PTB, and the out-
put of each 	

 is the input of
 
	ffIntroduction to
Frontiers in Corpus Annotation
Adam Meyers
New York University
meyers@cs.nyu.edu
A new annotated corpus can have a pivotal role in the
future of computational linguistics. Corpus annotation
can define new NLP tasks and set new standards. This
may put many of the papers presented at this workshop
on the cutting edge of our field.
A standard, however, is a double edged sword. A stan-
dard corpus urges users to accept the theory of how to
represent things that underlie that corpus. For example,
a Penn Treebank theory of grammar is implicit in Penn-
Treebank-based parsers. This can be a problem if one
rejects some aspects of that theory. Also one may object
to a particular system of annotation because some theo-
ries generalize to cover new ground (e.g., new languages)
better than others. Nevertheless, advantages of accepting
a corpus as standard include the following:
  It is straight-forward to compare the performance of
the set of systems that produce the same form of out-
put, e.g., Penn Treebank-based parsers can be com-
pared in terms of how well they reproduce the Penn
Treebank.
  Alternative systems based on a standard are largely
interchangeable. Thus a system that uses one Penn-
Treebank-based parser as a component can easily
be adapted to use another better performing Penn-
Treebank-based parser.
  Standards can be built on. For example, if one ac-
cepts the framework of the Penn Treebank, it is easy
to move on to representations of ?deeper? structure
as suggested in three papers in this volume (Milt-
sakaki et al, 2004; Babko-Malaya et al, 2004; Mey-
ers et al, 2004).
It is my view that these advantages outweigh the dis-
advantages. I propose that the papers in this volume be
viewed with the following question in mind: How can the
work covered by this collection of papers be integrated to-
gether? Put differently, to what extent are these resources
mergeable?
The first six papers describe linguistic annotation in
four languages: Spanish (Alca?ntara and Moreno, 2004),
English (Miltsakaki et al, 2004; Babko-Malaya et al,
2004; Meyers et al, 2004), Czech (Sgall et al, 2004)
and German(Baumann et al, 2004). The sixth, seventh
and eighth papers (Baumann et al, 2004; C?mejrek et al,
2004; Helmreich et al, 2004) explore questions of mul-
tilingual annotation of syntax and semantics, beginning
to answer the question of how annotation systems can be
made compatible across languages. Indeed (Helmreich
et al, 2004) explores the question of integration across
languages, as well as levels of annotation. (Baumann
et al, 2004) also describes how a number of different
linguistic levels can be related in annotation (pragmatic
and prosodic) among two languages (English and Ger-
man). The ninth and tenth papers (Langone et al, 2004;
Z?abokrtsky? and Lopatkova?, 2004) are respectively about
a corpus related to a lexicon and the reverse: a lexicon
related to a corpus. This opens up the wider theme of the
intergration of a number of different linguistic resources.
As the natural language community produces more and
more linguistic resources, especially corpora, it seems
important to step back and look at the larger picture. If
these resources can be fit together as part of a larger puz-
zle, this could produce a sketch of the future of our field.
References
M. Alca?ntara and A. Moreno. 2004. Syntax to Seman-
tics Transformation: Application to Treebanking. In
HLT-NAACL 2004 Workshop: Frontiers in Corpus An-
notation, Boston, Massachusetts.
O. Babko-Malaya, M. Palmer, N. Xue, A. Joshi, and
S. Kulick. 2004. Proposition Bank II: Delving Deeper.
In HLT-NAACL 2004 Workshop: Frontiers in Corpus
Annotation, Boston, Massachusetts.
S. Baumann, C. Brinkmann, S. Hansen-Schirra, G. Krui-
jff, I. Kruijff-Korbayova?, S. Neumann, and E. Teich.
2004. Multi-dimensional annotation of linguistic cor-
pora for investigating information structure. In HLT-
NAACL 2004 Workshop: Frontiers in Corpus Annota-
tion, Boston, Massachusetts.
M. C?mejrek, J. Cur?i?n, and J. Havelka. 2004. Prague
Czech-English Dependency Treebank: Any Hopes for
a Common Annotation Scheme? In HLT-NAACL 2004
Workshop: Frontiers in Corpus Annotation, Boston,
Massachusetts.
S. Helmreich, D. Farwell, B. Dorr, N. Habash, L. Levin,
T. Mitamura, F. Reeder, K. Miller, E. Hovy, O. Ram-
bow, and A.Siddharthan. 2004. Interlingual annota-
tion of multilingual text corpora. In HLT-NAACL 2004
Workshop: Frontiers in Corpus Annotation, Boston,
Massachusetts.
H. Langone, B. R. Haskell, and G. A. Miller. 2004.
Annotating WordNet. In HLT-NAACL 2004 Work-
shop: Frontiers in Corpus Annotation, Boston, Mas-
sachusetts.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The NomBank
Project: An Interim Report. In HLT-NAACL 2004
Workshop: Frontiers in Corpus Annotation, Boston,
Massachusetts.
E. Miltsakaki, A. Joshi, R. Prasad, and B. Webber. 2004.
Annotating Discourse Connectives and Their Argu-
ments. In HLT-NAACL 2004 Workshop: Frontiers in
Corpus Annotation, Boston, Massachusetts.
P. Sgall, J. Panevova?, and E. Hajic?ova?. 2004. Deep Syn-
tactic Annotation: Tectogrammatical Representation
and Beyond. In HLT-NAACL 2004 Workshop: Fron-
tiers in Corpus Annotation, Boston, Massachusetts.
Z. Z?abokrtsky? and M. Lopatkova?. 2004. Valency Frames
of Czech Verbs in VALLEX 1.0. In HLT-NAACL 2004
Workshop: Frontiers in Corpus Annotation, Boston,
Massachusetts.
The NomBank Project: An Interim Report
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely,
Veronika Zielinska, Brian Young and Ralph Grishman
New York University
meyers/reevesr/macleod/szekely/zielinsk/byoung/grishman@cs.nyu.edu
Abstract
This paper describes NomBank, a project that
will provide argument structure for instances of
common nouns in the Penn Treebank II corpus.
NomBank is part of a larger effort to add ad-
ditional layers of annotation to the Penn Tree-
bank II corpus. The University of Pennsylva-
nia?s PropBank, NomBank and other annota-
tion projects taken together should lead to the
creation of better tools for the automatic analy-
sis of text. This paper describes the NomBank
project in detail including its specifications and
the process involved in creating the resource.
1 Introduction
This paper introduces the NomBank project. When com-
plete, NomBank will provide argument structure for in-
stances of about 5000 common nouns in the Penn Tree-
bank II corpus. NomBank is part of a larger effort to
add layers of annotation to the Penn Treebank II cor-
pus. PropBank (Kingsbury et al, 2002; Kingsbury and
Palmer, 2002; University of Pennsylvania, 2002), Nom-
Bank and other annotation projects taken together should
lead to the creation of better tools for the automatic anal-
ysis of text. These annotation projects may be viewed
as part of what we think of as an a la carte strategy for
corpus-based natural language processing. The fragile
and inaccurate multistage parsers of a few decades were
replaced by treebank-based parsers, which had better per-
formance, but typically provided more shallow analyses.1
As the same set of data is annotated with more and more
levels of annotation, a new type of multistage processing
becomes possible that could reintroduce this information,
1A treebank-based parser output is defined by the treebank
on which it is based. As these treebanks tend to be of a fairly
shallow syntactic nature, the resulting parsers tend to be so also.
but in a more robust fashion. Each stage of processing
is defined by a body of annotated data which provides a
symbolic framework for that level of representation. Re-
searchers are free to create and use programs that map
between any two levels of representation, or which map
from bare sentences to any level of representation.2 Fur-
thermore, users are free to shop around among the avail-
able programs to map from one stage to another. The
hope is that the standardization imposed by the anno-
tated data will insure that many researchers will be work-
ing within the same set of frameworks, so that one re-
searcher?s success will have a greater chance of benefit-
ing the whole community.
Whether or not one adapts an a la carte approach,
NomBank and PropBank projects provide users with data
to recognize regularizations of lexically and syntactically
related sentence structures. For example, suppose one has
an Information Extraction System tuned to a hiring/firing
scenario (MUC, 1995). One could use NomBank and
PropBank to generalize patterns so that one pattern would
do the work of several. Given a pattern stating that the ob-
ject (ARG1) of appoint is John and the subject (ARG0)
is IBM, a PropBank/NomBank enlightened system could
detect that IBM hired John from the following strings:
IBM appointed John, John was appointed by IBM, IBM?s
appointment of John, the appointment of John by IBM
and John is the current IBM appointee. Systems that do
not regularize across predicates would require separate
patterns for each of these environments.
The NomBank project went through several stages be-
fore annotation could begin. We had to create specifica-
tions and various lexical resources to delineate the task.
Once the task was set, we identified classes of words. We
used these classes to approximate lexical entries, make
time estimates and create automatic procedures to aid in
2Here, we use the term ?level of representation? quite
loosely to include individual components of what might con-
ventionally be considered a single level.
1. Her gift of a book to John [NOM]
REL = gift, ARG0 = her, ARG1 = a book, ARG2 =
to John
2. his promise to make the trains run on time [NOM]
REL = promise, ARG0 = his, ARG2-PRD = to make
the trains run on time
3. her husband [DEFREL RELATIONAL NOUN]
REL = husband, ARG0 = husband, ARG1 = her
4. a set of tasks [PARTITIVE NOUN]
REL = set, ARG1 = of tasks
5. The judge made demands on his staff [NOM
w/SUPPORT]
REL = demands, SUPPORT = made, ARG0 = The
judge, ARG2 = on his staff
6. A savings institution needs your help [NOM
w/SUPPORT]
REL = help, SUPPORT = needs, ARG0 = your,
ARG2 = A savings institution
7. 12% growth in dividends next year [NOM
W/ARGMs]
REL = growth, ARG1 = in dividends, ARG2-EXT
= 12%, ARGM-TMP = next year
8. a possible U.S. troop reduction in South Ko-
rea[NOM W/ARGMs]
REL = reduction, ARG1 = U.S. troop, ARGM-LOC
= in South Korea, ARGM-ADV = possible
Figure 1: Sample NomBank Propositions
annotation. For the first nine months of the project, the
NomBank staff consisted of one supervisor and one anno-
tator. Once the specifications were nailed down, we hired
additional annotators to complete the project. This pa-
per provides an overview of the project including an ab-
breviated version of the specifications (the full version is
obtainable upon request) and a chronicle of our progress.
2 The Specifications
Figure 1 lists some sample NomBank propositions along
with the class of the noun predicate (NOM stands for
nominalization, DEFREL is a type of relational noun).
For each ?markable? instance of a common noun in the
Penn Treebank, annotators create a ?proposition?, a sub-
set of the features   REL, SUPPORT, ARG0, ARG1,
ARG2, ARG3, ARG4, ARGM  paired with pointers to
phrases in Penn Treebank II trees. A noun instance is
markable if it is accompanied by one of its arguments
(ARG0, ARG1, ARG2, ARG3, ARG4) or if it is a nomi-
nalization (or similar word) and it is accompanied by one
of the allowable types of adjuncts (ARGM-TMP, ARGM-
LOC, ARGM-ADV, ARGM-EXT, etc.) ? the same set of
adjuncts used in PropBank.3
The basic idea is that each triple   REL, SENSE,
ARGNUM  uniquely defines an argument, given a par-
ticular sense of a particular REL (or predicate), where
ARGNUM is one of the numbered arguments (ARG0,
ARG1, ARG2, ARG3, ARG4) and SENSE is one of the
senses of that REL. The arguments are essentially the
same as the initial relations of Relational Grammar (Perl-
mutter and Postal, 1984; Rosen, 1984). For example,
agents tend to be classified as ARG0 (RG?s initial sub-
ject), patients and themes tend to be classified as ARG1
(RG?s initial object) and indirect objects of all kinds tend
to be classified as ARG2.
The lexical entry or frame for each noun provides
one inventory of argument labels for each sense of that
word.4 Each proposition (cf. figure 1) consists of an in-
stance of an argument-taking noun (REL) plus arguments
(ARG0, ARG1, ARG2,  ), SUPPORT items and/or ad-
juncts (ARGM). SUPPORT items are words that link ar-
guments that occur outside an NP to the nominal predi-
cate that heads that NP, e.g., ?made? SUPPORTS ?We?
as the ARG0 of decision in We made a decision. ARGMs
are adjuncts of the noun. However, we only mark the
sort of adjuncts that also occur in sentences: locations
(ARGM-LOC), temporal (ARGM-TMP), sentence ad-
verbial (ARGM-ADV) and various others.
3 Lexical Entries and Noun Classes
Before we could begin annotation, we needed to classify
all the common nouns in the corpus. We needed to know
which nouns were markable and make initial approxima-
tions of the inventories of senses and arguments for each
noun. Toward this end, we pooled a number of resources:
COMLEX Syntax (Macleod et al, 1998a), NOMLEX
(Macleod et al, 1998b) and the verb classes from (Levin,
1993). We also used string matching techniques and hand
classification in combination with programs that automat-
ically merge crucial features of these resources. The re-
sult was NOMLEX-PLUS, a NOMLEX-style dictionary,
which includes the original 1000 entries in NOMLEX
plus 6000 additional entries (Meyers et al, 2004). The re-
sulting noun classes include verbal nominalizations (e.g.,
destruction, knowledge, believer, recipient), adjectival
nominalizations (ability, bitterness), and 16 other classes
such as relational (father, president) and partitive nouns
(set, variety). NOMLEX-PLUS helped us break down
3To make our examples more readable, we have replaced
pointers to the corpus with the corresponding strings of words.
4For a particular noun instance, only a subset of these argu-
ments may appear, e.g., the ARG2 (indirect object) to Dorothy
can be left out of the phrase Glinda?s gift of the slippers.
the nouns into classes, which in turn helped us gain an
understanding of the difficulty of the task and the man-
power needed to complete the task.
We used a combination of NOMLEX-PLUS and Prop-
Bank?s lexical entries (or frames) to produce automatic
approximations of noun frames for NomBank. These en-
tries specify the inventory of argument roles for the an-
notators. For nominalizations of verbs that were covered
in PropBank, we used straightforward procedures to con-
vert existing PropBank lexical entries to nominal ones.
However, other entries needed to be created by automatic
means, by hand or by a combination of the two. Figure 2
compares the PropBank lexical entry for the verb claim
with the NomBank entry for the noun claim. The noun
claim and the verb claim share both the ASSERT sense
and the SEIZE sense, permitting the same set of argu-
ment roles for those senses. However, only the ASSERT
sense is actually attested in the sample PropBank corpus
that was available when we began working on NomBank.
Thus we added the SEIZE sense to both the noun and
verb entries. The noun claim also has a LAWSUIT sense
which bears an entry similar to the verb sue. Thus our
initial entry for the noun claim was a copy of the verb en-
try at that time. An annotator edited the frames to reflect
noun usage ? she added the second and third senses to
the noun frame and updated the verb frame to include the
second sense.
In NOMLEX-PLUS, we marked anniversary and ad-
vantage as ?cousins? of nominalizations indicating that
their lexical entries should be modeled respectively on
the verbs commemorate and exploit, although both en-
tries needed to be modified in some respect. We use the
term ?cousins? of nominalizations to refer to those nouns
which take argument structure similar to some verb (or
adjective), but which are not morphologically related to
that word. Examples are provided in Figure 3 and 4. For
adjective nominalizations, we began with simple proce-
dures which created frames based on NOMLEX-PLUS
entries (which include whether the subject is +/-sentient).
The entry for ?accuracy? (the nominalization of the ad-
jective accurate) plus a simple example is provided in fig-
ure 5 ? the ATTRIBUTE-LIKE frame is one of the most
common frames for adjective nominalizations. To cover
the remaining nouns in the corpus, we created classes
of lexical items and manually constructed one frame for
each class. Each member of a class was was given the
corresponding frame. Figure 6 provides a sample of these
classes, along with descriptions of their frames. As with
the nominalization cousins, annotators sometimes had to
adjust these frames for particular words.
4 A Merged Representation
Beginning with the PropBank and NomBank propo-
sitions in Figure 7, it is straight-forward to derive the
1. ASSERT Sense
Roles: ARG0 = AGENT, ARG1 = TOPIC
Noun Example: Her claim that Fred can y
REL = claim, ARG0 = her, ARG1 = that Fred
can fly
Verb Example: She claimed that Fred can y
REL = claimed, ARG0 = She, ARG1 = that
Fred can fly
2. SEIZE Sense
Roles: ARG0 = CLAIMER, ARG1 = PROPERTY,
ARG2 = BENEFICIARY
Noun Example: He laid claim to Mexico for Spain
REL = claim, SUPPORT = laid, ARG0 = He,
ARG1 = to Mexico, ARG2 = for Spain
Verb Example: He claimed Mexico for Spain
REL = claim, ARG0 = He, ARG1 = Mexico,
ARG2 = for Spain
3. SUE Sense
Roles: ARG0 = CLAIMANT, ARG1 = PURPOSE,
ARG2 = DEFENDANT, ARG3 = AWARD
Noun Example: His $1M abuse claim against Dan
ARG0 = His, ARG1 = abuse, ARG2 = against
Dan, ARG3 = $1M
Verb Example: NOT A VERB SENSE
Figure 2: Verb and Noun Senses of claim
1. HONOR (based on a sense of commemorate)
Roles: ARG0 = agent, ARG1 = thing remembered,
ARG2 = times celebrated
Noun Example: Investors celebrated the second
anniversary of Black Monday.
REL = anniversary, SUPPORT = celebrated,
ARG0 = Investors, ARG1 = of Black Monday,
ARG2 = second
Figure 3: One sense for anniversary
1. EXPLOIT
Roles: ARG0 = exploiter, ARG1 = entity exploited
Noun Example: Investors took advantage of Tues-
day ?s stock rally.
REL = advantage, SUPPORT = took, ARG0 =
Investors, ARG1 = of Tuesday?s stock rally
Figure 4: One sense for advantage
1. ATTRIBUTE-LIKE
Roles: ARG1 = theme
Noun Example: the accuracy of seasonal adjust-
ments built into the employment data
REL = accuracy, ARG1 = of seasonal adjust-
ments built into 
Figure 5: One Sense for accuracy
ACTREL Relational Nouns with beneficiaries
Roles: ARG0 = JOB HOLDER, ARG1 = THEME,
ARG2 = BENEFICIARY
Example: ACME will gain printing customers
REL = customers, SUPPORT = gain, ARG0 =
customers, ARG1 = printing, ARG2 = ACME
DEFREL Relational Nouns for personal relationships
Roles: ARG0 = RELATION HOLDER, ARG1 =
RELATION RECEPTOR
Example: public enemies REL = enemies, ARG0
= enemies, ARG1 = public
ATTRIBUTE Nouns representing attribute relations
Roles: ARG1 = THEME, ARG2 = VALUE
Example: a lower grade of gold
REL = grade, ARG1 = of gold, ARG2 = lower
ABILITY-WITH-AGENT Ability-like nouns
Roles: ARG0 = agent, ARG1 = action
Example: the electrical current-carrying capacity
of new superconductor crystals
REL = capacity, ARG0 = of new superconduc-
tor crystals, ARG1 = electrical current-carrying
ENVIRONMENT Roles: ARG1 = THEME
Example: the circumstances of his departure
REL = circumstances, ARG1 = of his departure
Figure 6: Frames for Classes of Nouns
PropBank: REL = gave, ARG0 = they, ARG1 = a
standing ovation, ARG2 = the chefs
NomBank: REL = ovation, ARG0 = they, ARG1 = the
chefs, SUPPORT = gave
Figure 7: They gave the chefs a standing ovation
gave
chefsthe
a ovationstanding
They
S
REL
NP
NP
SUPPORT
NP
ARG1
ARG1
ARG2
ARG0
ARG0
REL
Figure 8: They gave the chefs a standing ovation
combined PropBank/NomBank graphical representation
in Figure 8 in which each role corresponds to an arc la-
bel. For this example, think of the argument structure of
the noun ovation as analogous to the verb applaud. Ac-
cording to our analysis, they are both the givers and the
applauders and the chefs are both the recipients of some-
thing given and the ones who are applauded. Gave and
ovation have two distinct directional relations: a stand-
ing ovation is something that is given and gave serves as
a link between ovation and its two arguments. This dia-
gram demonstrates how NomBank is being designed for
easy integration with PropBank. We believe that this is
the sort of predicate argument representation that will be
needed to easily merge this work with other annotation
efforts.
5 Analysis of the Task
As of this writing we have created the various lexicons
associated with NomBank. This has allowed us to break
down the task as follows:
 There are approximately 240,000 instances of com-
mon nouns in the PTB (approximately one out of
every 5 words).
 At least 36,000 of these are nouns that cannot take
arguments and therefore need not be looked at by an
annotator.
 There are approximately 99,000 instances of verbal
nominalizations or related items (e.g., cousins)
 There are approximately 34,000 partitives (includ-
ing 6,000 instances of the percent sign), 18,000 sub-
ject nominalizations, 14,000 environmental nouns,
14,000 relational nouns and fewer instances of the
various other classes.
 Approximately 1/6 of the cases are instances of
nouns which occur in multiple classes.5
The difficulty of the annotation runs the gamut from
nominalization instances which include the most argu-
ments, the most adjuncts and the most instances of sup-
port to the partitives, which have the simplest and most
predictable structure.
6 Error Analysis and Error Detection
We have conducted some preliminary consistency tests
for about 500 instances of verbal nominalizations dur-
ing the training phases of NomBank. These tests yielded
inter-annotator agreement rates of about 85% for argu-
ment roles and lower for adjunct roles. We are currently
engaging in an effort to improve these results.6
We have identified certain main areas of disagreement
including: disagreements concerning SUPPORT verbs
and the shared arguments that go with them; disagree-
ments about role assignment to prenominals; and differ-
ences between annotators caused by errors (typos, slips
of the mouse, ill-formed output, etc.) In addition to im-
proving our specifications and annotator help texts, we
are beginning to employ some automatic means for error
detection.
6.1 Support
For inconsistencies with SUPPORT, our main line of at-
tack has been to outline problems and solutions in our
specifications. We do not have any automatic system in
effect yet, although we may in the near future.
SUPPORT verbs (Gross, 1981; Gross, 1982; Mel?c?uk,
1988; Mel?c?uk, 1996; Fontenelle, 1997) are verbs which
5When a noun fits into multiple categories, those categories
may predict multiple senses, but not necessarily. For example,
drive has a nominalization sense (He went for a drive) and an
attribute sense (She has a lot of drive). Thus the lexical entry
for drive includes both senses. In constrast, teacher in the math
teacher has the same analysis regardless of whether one thinks
of it as the nominalization of teach or as a relational (ACTREL)
noun.
6Consistency is the average precision and recall against a
gold standard. The preliminary tests were conducted during
training, and only on verbal nominalizations.
connect nouns to one (or more) of their arguments via ar-
gument sharing. For example, in John took a walk, the
verb took ?shares? its subject with the noun walk. SUP-
PORT verbs can be problematic for a number of reasons.
First of all the concept of argument sharing is not black
and white. To illustrate these shades of gray, compare
the relation of Mary to attack in: Mary?s attack against
the alligator, Mary launched an attack against the alliga-
tor, Mary participated in an attack against the alligator,
Mary planned an attack against the alligator and Mary
considered an attack against the alligator. In each subse-
quent example, Mary?s ?level of agency? decreases with
respect to the noun attack. However, in each case Mary
may still be viewed as some sort of potential attacker. It
turned out that the most consistent position for us to take
was to assume all degrees of argument-hood (in this case
subject-hood) were valid. So, we would mark Mary as the
ARG0 of attack in all these instances. This is consistent
with the way control and raising structures are marked
for verbs, e.g., John is the subject of leave and do in John
did not seem to leave and John helped do the project un-
der most accounts of verbal argument structure that take
argument sharing (control, raising, etc.) into account.
Of course a liberal view of SUPPORT has the danger
of overgeneration. Consider for example, Market con-
ditions led to the cancellation of the planned exchange.
The unwary annotator might assume that market condi-
tions is the ARG0 (or subject) of cancellation. In fact,
the combination lead to and cancellation do not have any
of the typical features of SUPPORT described in figure 9.
However, the final piece of evidence is that market con-
ditions violate the selection restrictions of cancellation.
Thus the following paraphrase is ill-formed *Market con-
ditions canceled the planned exchange. This suggests
that market conditions is the subject of lead and not the
subject of cancellation. Therefore, this is not an instance
of support in spite of the apparent similarity.
We require that the SUPPORT relation be lexical. In
other words, there must be something special about a
SUPPORT verb or the combination of the SUPPORT
verb and the noun to license the argument sharing rela-
tion. In addition to SUPPORT, we have cataloged several
argument sharing phenomena which are markable. For
example, consider the sentence, President Bush arrived
for a celebration. Clearly, President Bush is the ARG0
of celebration (one of the people celebrating). However,
arrive is not a SUPPORT verb. The phrase for a cele-
bration is a subject-oriented adverbial, similar to adverbs
like willingly, which takes the subject of the sentence as
an argument. Thus President Bush could also be the sub-
ject of celebration in President Bush waddled into town
for the celebration and many similar sentences that con-
tain this PP.
Finally, there are cases where argument sharing may
 Support verb/noun pairs can be idiosyncratically
connected to the point that some researchers would
call them idioms or phrasal verbs, e.g., take a walk,
keep tabs on.
 The verb can be essentially ?empty?, e.g., make an
attack, have a visit.
 The ?verb/noun? combination may take a different
set of arguments than either does alone, e.g., take
advantage of.
 Some support verbs share the subject of almost any
nominalization in a particular argument slot. For ex-
ample attempt shares its subject with most follow-
ing nominalizations, e.g., He attempted an attack.
These are the a lot like raising/control predicates.
 In some cases, the support verb and noun are from
similar semantic classes, making argument sharing
very likely, e.g., fight a battle.
Figure 9: Possible Features of Support
be implied by discourse processes, but which we do
not mark (as we are only handling sentence-level phe-
nomena). For example, the words proponent and rival
strongly imply that certain arguments appear in the dis-
course, but not necessarily in the same sentence. For ex-
ample in They didn?t want the company to fall into the
hands of a rival, there is an implication that the company
is an ARG1 of rival, i.e., a rival should be interpreted as
a rival of the company.7 The connection between a rival
and the company is called a ?bridging? relation (a pro-
cess akin to coreference, cf. (Poesio and Vieira, 1998))
In other words, fall into the hands of does not link ?ri-
val? with the company by means of SUPPORT. The fact
that a discourse relation is responsible for this connection
becomes evident when you see that the link between ri-
val and company can cross sentence boundaries, e.g., The
company was losing money. This was because a rival had
come up with a really clever marketing strategy.
6.2 Prenominal Adjectives and Error Detection
ARGM is the annotation tag used for nonarguments, also
known as adjuncts. For nouns, it was decided to only tag
such types of adjuncts as are also found with verbs, e.g.,
temporal, locative, manner, etc. The rationale for this in-
cluded: (1) only the argument-taking common nouns are
being annotated and other sorts of adjuncts occur with
common nouns in general; (2) narrowing the list of po-
tential labels helped keep the labeling consistent; and (3)
this was the minimum set of adjuncts that would keep the
7The noun rival is a subject nominalization of the verb rival.
noun annotation consistent with the verb annotation.
Unfortunately, it was not always clear whether a
prenominal modifier (particularly an adjective) fell into
one of our classes or not. If an annotator felt that a modi-
fier was somehow ?important?, there was a temptation to
push it into one of the modifier classes even if it was not
a perfect fit. Furthermore, some annotators had a broader
view than others as to the sorts of semantic relationships
that fell within particular classes of adjuncts, particularly
locative (LOC), manner (MNR) and extent (EXT). Un-
like the SUPPORT verbs, which are often idiosyncratic to
particular nominal predicates, adjunct prenominal modi-
fiers usually behave the same way regardless of the noun
with which they occur.
In order to identify these lexical properties of prenom-
inals, we created a list of all time nouns from COMLEX
Syntax (ntime1 and ntime2) and we created a specialized
dictionary of adjectives with adverbial properties which
we call ADJADV. The list of adjective/adverb pairs in
ADJADV came from two sources: (1) a list of adjec-
tives that are morphologically linked to -ly adverbs cre-
ated using some string matching techniques; and (2) ad-
jective/adverb pairs from CATVAR (Habash and Dorr,
2003). We pruned this list to only include adjectives
found in the Penn Treebank and then edited out inappro-
priate word pairs. We completed the dictionary by trans-
ferring portions of the COMLEX Syntax adverb entries
to the corresponding adjectives.
We now use ADJADV and our list of temporal nouns
to evaluate NOMBANK annotation of modifiers. Each
annotated left modifier is compared against our dictio-
naries. If a modifier is a temporal noun, it can bear the
ARGM-TMP role (temporal adjunct role), e.g., the tem-
poral noun morning can fill the ARGM-TMP slot in the
morning broadcast. Most other common nouns are com-
patible with argument role slots (ARG0, ARG1, etc.),
e.g., the noun news can fill the ARG1 slot in the news
broadcast. Finally, roles associated with adjectives de-
pend on their ADJADV entry, e.g., possible can be an
ARGM-ADV in possible broadcasts due to the epistemic
feature encoded in the lexical entry for possible (derived
from the corresponding adjverb possibly). Discrepancies
between these procedures and the annotator are resolved
on a case by case basis. If the dictionary is wrong, the
dictionary should be changed, e.g., root, as in root cause
was added to the dictionary as a potential MNR adjective
with a meaning like the adverb basically. However, if
the annotator is wrong, the annotation should be changed,
e.g., if an annotator marked ?slow? as a ARGM-TMP, the
program would let them know that it should be a ARGM-
MNR. This process both helps with annotation accuracy
and enriches our lexical database.
6.3 Other Automatically Detected Errors
We used other procedures to detect errors including:
Nom-type Argument nominalizations are nominaliza-
tions that play the role of one of the arguments in
the ROLESET. Thus the word acquirer should be
assigned the ARG0 role in the following example
because acquirer is a subject nominalization:
a possible acquirer of Manville
REL = acquirer, ARG0 = acquirer, ARG1 = of
Manville, ARGM-ADV = possible
A procedure can compare the NOMLEX-PLUS en-
try for each noun to each annotated instance of that
noun to check for incompatibilities.
Illformedness Impossible instances are ruled out.
Checks are made to make sure obligatory labels
(REL) are present and illegal labels are not. Simi-
larly, procedures make sure that infinitive arguments
are marked with the -PRD function tag (a PropBank
convention).
Probable Illformedness Certain configurations of role
labels are possible, but very unlikely. For example,
the same argument role should not appear more than
once (the stratal uniqueness condition in Relational
Grammar or the theta criterion in Principles and pa-
rameters, etc.). Furthermore, it is unlikely for the
first word of a sentence to be an argument unless
the main predicate is nearby (within three words) or
unless there is a nearby support verb. Finally, it is
unlikely that there is an empty category that is an
argument of a predicate noun unless the empty cate-
gory is linked to some real NP.8
WRONG-POS We use procedures that are part of our
systems for generating GLARF, a predicate argu-
ment framework discussed in (Meyers et al, 2001a;
Meyers et al, 2001b), to detect incorrect parts of
speech in the Penn Treebank. If an instance is pre-
dicted to be a part of speech other than a common
noun, but it is still tagged, that instance is flagged.
For example, if a word tagged as a singular common
noun is the first word in a VP, it is probably tagged
with the wrong part of speech.
6.4 The Results of Error Detection
The processes described in the previous subsections are
used to create a list of annotation instances to check along
with short standardized descriptions of what was wrong,
e.g., wrong-pos, non-functional (if there were two iden-
tical argument roles), etc. Annotators do a second pass
8Empty categories mark ?invisible? constituents in the Tree-
bank, e.g., the subject of want in John  wanted e  to leave.
PARTITIVE-QUANT
Roles: ARG1 = QUANTIFIED
Example: lots of internal debate
REL = lots, ARG1 = of internal debate
Figure 10: The entry for lot
on just these instances (currently about 5 to 10% of the
total). We will conduct a formal evaluation of this proce-
dure over the next month.
7 Future Research: Automatic Annotation
We are just starting a new phase in this project: the cre-
ation of an automatic annotator. Using techniques similar
to those described in (Meyers et al, 1998) in combina-
tion with our work on GLARF (Meyers et al, 2001a;
Meyers et al, 2001b), we expect to build a hand-coded
PROPBANKER a program designed to produce a Prop-
Bank/NomBank style analysis from Penn Treebank style
input. Although the PropBanker should work with in-
put in the form of either treebank annotation or treebank-
based parser output, this project only requires applica-
tion to the Penn Treebank itself. While previous pro-
grams with similar goals (Gildea and Jurafsky, 2002)
were statistics-based, this tool will be based completely
on hand-coded rules and lexical resources.
Depending on its accuracy, automatically produced an-
notation should be useful as either a preprocessor or as
an error detector. We expect high precision for very sim-
ple frames, e.g., nouns like lot as in figure 10. Annota-
tors will have the opportunity to judge whether particu-
lar automatic annotation is ?good enough? to serve as a
preprocessor. We hypothesize that a comparison of auto-
matic annotation that fails this level of accuracy against
the hand annotation will still be useful for detecting er-
rors. Comparisons between the hand annotated data and
the automatically annotated data will yield a set of in-
stances that warrant further checking along the same lines
as our previously described error checking mechanisms.
8 Summary
This paper outlines our current efforts to produce Nom-
Bank, annotation of the argument structure for most com-
mon nouns in the Penn Treebank II corpus. This is part of
a larger effort to produce more detailed annotation of the
Penn Treebank. Annotation for NomBank is progress-
ing quickly. We began with a single annotator while we
worked on setting the task and have ramped up to four an-
notators. We continue to work on various quality control
procedures which we outline above. In the near future,
we intend to create an automatic annotation program to
be used both as a preprocessor for manual annotation and
as a supplement to error detection.
The argument structure of NPs has been less studied
both in theoretical and computational linguistics, than
the argument structure of verbs. As with our work on
NOMLEX, we are hoping that NomBank will substan-
tially contribute to improving the NLP community?s abil-
ity to understand and process noun argument structure.
Acknowledgments
Nombank is supported under Grant N66001-001-1-8917
from the Space and Naval Warfare Systems Center San
Diego. This paper does not necessarily reflect the posi-
tion or the policy of the U.S. Government.
We would also like to acknowledge the people at the
University of Pennsylvania who helped make NomBank
possible, including, Martha Palmer, Scott Cotton, Paul
Kingsbury and Olga Babko-Malaya. In particular, the use
of PropBank?s annotation tool and frame files proved in-
valuable to our effort.
References
T. Fontenelle. 1997. Turning a bilingual dictionary into
a lexical-semantic database. Lexicographica Series
Maior 79. Max Niemeyer Verlag, Tu?bingen.
D. Gildea and D. Jurafsky. 2002. Automatic Labeling of
Semantic Roles. Computational Linguistics, 28:245?
288.
M. Gross. 1981. Les bases empiriques de la notion de
pre?dicat se?mantique. In A. Guillet and C. Lecl?ere,
editors, Formes Syntaxiques et Pr?edicat S?emantiques,
volume 63 of Langages, pages 7?52. Larousse, Paris.
M. Gross. 1982. Simple Sentences: Discussion of Fred
W. Householder?s Paper ?Analysis, Synthesis and Im-
provisation?. In Text Processing. Text Analysis and
Generation. Text Typology and Attribution. Proceed-
ings of Nobel Symposium 51.
N. Habash and B. Dorr. 2003. CatVar: A Database of
Categorial Variations for English. In Proceedings of
the MT Summit, pages 471?474, New Orleans.
P. Kingsbury and M. Palmer. 2002. From treebank to
propbank. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), Las Palmas, Spain.
P. Kingsbury, M. Palmer, and Mitch Marcus. 2002.
Adding semantic annotation to the penn treebank. In
Proceedings of the Human Language Technology Con-
ference, San Diego, California.
B. Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. The University of
Chicago Press, Chicago.
C. Macleod, R. Grishman, and A. Meyers. 1998a.
COMLEX Syntax. Computers and the Humanities,
31(6):459?481.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and
R. Reeves. 1998b. Nomlex: A lexicon of nominal-
izations. In Proceedings of Euralex98.
I. A. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University Press of New York, Albany.
I. A. Mel?c?uk. 1996. Lexical Functions: A Tool for
the Description of Lexical Relations in a Lexicon. In
Lexical Functions in Lexicography and Natural Lan-
guage Processing. John Benjamins Publishing Com-
pany, Amsterdam.
A. Meyers, C. Macleod, R. Yangarber, R. Grishman,
Leslie Barrett, and Ruth Reeves. 1998. Using NOM-
LEX to Produce Nominalization Patterns for Informa-
tion Extraction. In Coling-ACL98 workshop Proceed-
ings: the Computational Treatment of Nominals.
A. Meyers, R. Grishman, M. Kosaka, and S. Zhao.
2001a. Covering Treebanks with GLARF. In
ACL/EACL Workshop on Sharing Tools and Resources
for Research and Education.
A. Meyers, M. Kosaka, S. Sekine, R. Grishman, and
S. Zhao. 2001b. Parsing and GLARFing. In Proceed-
ings of RANLP-2001, Tzigov Chark, Bulgaria.
A. Meyers, R. Reeves, Catherine Macleod, Rachel Szeke-
ley, Veronkia Zielinska, Brian Young, and R. Grish-
man. 2004. The Cross-Breeding of Dictionaries. In
Proceedings of LREC-2004, Lisbon, Portugal. To ap-
pear.
MUC-6. 1995. Proceedings of the Sixth Message Under-
standing Conference. Morgan Kaufman. (MUC-6).
D. M. Perlmutter and P. M. Postal. 1984. The 1-
Advancement Exclusiveness Law. In D. M. Perlmutter
and C. G. Rosen, editors, Studies in Relational Gram-
mar 2. The University of Chicago Press, Chicago.
M. Poesio and R. Vieira. 1998. A Corpus-based Inves-
tigation of Definite Description Use. Computational
Linguistics, 24(2):183?216.
C. G. Rosen. 1984. The Interface between Semantic
Roles and Initial Grammatical Relations. In D.. M.
Perlmutter and C. G. Rosen, editors, Studies in Rela-
tional Grammar 2. The University of Chicago Press,
Chicago.
University of Pennsylvania. 2002. Annotation guidelines
for PropBank. http://www.cis.upenn.edu/
?ace/propbank-guidelines-feb02.pdf.
Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 1?4,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Introduction to
Frontiers in Corpus Annotation II
Pie in the Sky
Adam Meyers
New York University
meyers@cs.nyu.edu
1 Introduction
The Frontiers in Corpus Annotation workshops are op-
portunities to discuss the state of the art of corpus annota-
tion in computational linguistics. Corpus annotation has
pushed the enitre field in new directions by providing new
task definitions and new standards of analysis. At the first
Frontiers in Corpus Annotation workshop at HLT-NAACL
2004 we compared assumptions underlying different an-
notation projects in light of both multilingual applications
and the pursuit of merged representations that incorporate
the result of various annotation projects.
Beginning September, 2004, several researchers have
been collaborating to produce detailed semantic anno-
tation of two difficult sentences. The effort aimed to
produce a single unified representation that goes beyond
what may currently be feasible to annotate consistently
or to generate automatically. Rather this ?pie in the sky?
annotation effort was an attempt at defining a future goal
for semantic analysis. We decided to use the ?Pie in the
Sky? annotation effort (http://nlp.cs.nyu.edu/meyers/pie-
in-the-sky.html) as a theme for this year?s workshop.
Consequently this theme has been brought out in many
of the papers contained in this volume.
The first 4 papers (Pustejovsky et al, 2005; E. W.
Hinrichs and S. Ku?bler and K. Naumann, 2005; Bies
et al, 2005; Dinesh et al, 2005) all discuss some as-
pect of merging annotation. (Pustejovsky et al, 2005)
describes issues that arise for merging argument struc-
tures for verbs, nouns and discourse connectives, as well
as time and anaphora representations. (E. W. Hinrichs
and S. Ku?bler and K. Naumann, 2005) focuses on the
merging of syntactic, morphological, semantic and ref-
erential annotation. (E. W. Hinrichs and S. Ku?bler and
K. Naumann, 2005) also points out that the ?Pie in the
Sky?representation lacks syntactic features. This brings
to light an important point of discussion: should linguis-
tic analyses be divided out into separate ?levels? cor-
responding to syntax, morphology, discourse, etc. or
should/can a single representation represent all such ?lev-
els?? As currently conceived, ?Pie in the Sky? is in-
tended to be as ?language neutral? as possible ? this may
make adding a real syntactic level difficult. However, ar-
guably, surface relations can be added on as features to
Pie in the Sky, even if we delete or ignore those features
for some (e.g., language neutral) purposes. Still, other
papers present further difficulties for maintaining a sin-
gle representation that covers multiple modes of analysis.
(Bies et al, 2005) discusses possible conflicts between
named entity analyses and syntactic structure and (Di-
nesh et al, 2005) discusses a conflict between discourse
structure and syntactic structure. I think it is reasonable to
assume that some such conflicts will be resolvable, e.g.,
I believe that the named entity conflicts point to short-
comings of the original Penn Treebank analysis. How-
ever, the discourse structure/syntactic structure conflicts
may be harder to solve. In fact, some annotation projects,
e.g., the Prague Dependency Treebank (Hajic?ova? and Ce-
plova?, 2000), assume that multiple analyses or ?levels?
are necessary to describe the full range of phenomena.
The 5th through 7th papers (Inui and Okumura, 2005;
Calhoun et al, 2005; Wilson and Wiebe, 2005) investi-
gate some additional types of annotation that were not
part of the distributed version of Pie in the Sky, but which
could be added in principle. In fact, with help from
the authors of (Calhoun et al, 2005), I did incorporate
their analysis into the latest version (number 6) of the?Pie
in the Sky? annotation. Furthermore, it turns out that
some units of Information Structure cross the boundaries
of the syntactic/semantic constituents, thus raising the
sort of difficulties discussed in the previous paragraph.
Specifically, information structure divides sentences into
themes and rhemes. For the sample two sentences, the
rheme boundaries do correspond to syntactic units, but
the theme boundaries cross syntactic boundaries, forming
units made up of parts of multiple syntactic constituents.
(Palmer et al, 2005; Xue, 2005) (the eighth and
1
eleventh papers) make comparisons of annotated phe-
nomena across English and Chinese. It should be pointed
out that seven of the papers at this workshop are pre-
dominantly about the annotation of English, one is about
German annotation and one is about Japanese annotation.
These two are the only papers at the workshop that explic-
itly discuss attempts to apply the same annotation scheme
across two languages.
(McShane et al, 2005; Poesio and Artstein, 2005) (the
ninth and tenth papers) both pertain to issues about im-
proving the annotation process. (Poesio and Artstein,
2005) discusses some better ways of assessing inter-
annotator agreement, particularly when there is a gray
area between correct and incorrect annotation. (McShane
et al, 2005) discusses the issue of human-aided annota-
tion (human correction of a machine-generated analysis)
as it pertains to a single-integrated annotation scheme,
similar in many ways to ?Pie in the Sky?, although it has
been in existence for a lot longer.
2 Issues for Discussion
These papers raise a number of important issues for dis-
cussion, some of which I have already touched on.
Question 1: Should the community annotate lots of
individual phenomena independently of one another or
should we assume an underlying framework and per-
form all annotation tasks so they are compatible with that
framework?
Some of the work presented describes the annotation
of fairly narrow linguistic phenomena. Pie in the Sky can
be viewed as a framework for unifying these annotation
schemata into a single representation (a Unified Linguis-
tic Annotation framework in the sense of (Pustejovsky et
al., 2005)). Other work presented assumes that the in-
tegrated framework is the object of the annotation rather
than the result of merging annotations (E. W. Hinrichs
and S. Ku?bler and K. Naumann, 2005; McShane et al,
2005). There are pros and cons to both approaches.
When researchers decide to annotate one small piece
of linguistic analysis (verb argument structure, noun ar-
gument structure, coreference, discourse structure, etc.),
this has the following potential advantages: (1) explor-
ing one phenomenon in depth may provide a better char-
acterization of that phenomenon. If individual phenom-
ena are examined with this level of care, perhaps we will
end up with a better overall analysis; (2) a very focused
task definition for the annotator may improve interanno-
tator agreement; and (3) it is sometimes easier to ana-
lyze a phenomenon in isolation, especially if there is not
a large literature of previous work about it ? indeed, try-
ing to integrate this new phenomenon before adequately
understanding it may unduly bias one?s research. How-
ever, by ignoring a more complete theory, these anno-
tation projects run the risk of task-based biases, e.g.,
classifying predication as coreference or coreference as
argument-hood. While an underlying all-inclusive the-
ory could be a useful roadmap, unifying the results of
several annotation efforts (and resolving inconsistencies)
may yield the same result (as suggested in (Pustejovsky
et al, 2005)) while maintaining the advantages of inves-
tigating the phenomena separately. On the other hand, as
this merging process has not come to completion yet, the
jury is still out.
Let?s say that, for the sake of argument, the reader ac-
cepts the research program where individual annotation
efforts are slowly merged into one ?Pie in the Sky? type
system. There is still another obvious question that arises:
Question 2: Why make up a brand new system like
?Pie in the Sky? when there are so many existing frame-
works around? For example, Head-Driven Phrase Struc-
ture Grammar (Pollard and Sag, 1994) assumes a fairly
large feature structure that would seem to accommodate
every possible level of linguistic analysis (although in
practice most authors in that framework only work on the
syntactic and semantic portion of that feature structure).
Our initial motivation for starting fresh is that we
wanted the framework to use the minimal features nec-
essary to represent the input annotation systems and to
extend them as much as possible. In addition, part of the
experiment was an aim to keep features in a somewhat
language-neutral form and it is not clear that there are ex-
isting frameworks that both share this bias and are suffi-
ciently expressive for our purposes. However, ultimately
it might be beneficial to convert ?Pie in the Sky? to one
or more pre-existing frameworks.
So far, we have limited the scope of ?Pie in the Sky?
to semantic and (recently) some discourse information as
well. However, there are some cases where we found it
necessary to include syntactic information, e.g., although
heads are semantic arguments of adjective modifiers, the
surface relation between the head of the noun phrase and
its constituents is important for determining other parts
of meaning. For example, although explosive would bear
the same argument relation to powerful in both (a) The
explosive is powerful and (b) the powerful explosive, the
interpretation of (b) requires that powerful be part of the
same unit as explosive, e.g., for the proper interpretation
of He bought a powerful explosive. Thus it may seem
like a good idea to ultimately fill out ?Pie in the Sky? into
a larger framework. However, we would still want to be
able to pick out the language-neutral components of the
analysis from the language-specific ones.
Question 3: D. Farwell, a member of the workshop
committee, has pointed out that there are levels within se-
mantics. The question is how should these multiple levels
be handled? The annotated examples did not include phe-
nomena such as metaphor, metonymy or idiomaticity that
may have multiple interpretations: literal and intended.
2
For example, an adequate interpretation of I love listen-
ing to Mozart would require Mozart to be decomposed
into music by Mozart (although arguably the representa-
tion of some of the complex discourse references were of
this flavor).
3 What?s in the Latest Pie in the Sky
Analysis
As of this writing, the latest ?Pie in the Sky? analysis
includes: (1) argument structure of all parts of speech
(verbs, nouns, adjectives, determiners, conjunctions, etc.)
using the PropBank/NomBank/Discourse Treebank argu-
ment labels (ARG0, ARG1, ARG2,     ), reminiscent of
Relational Grammar of the 1970s and 1980s (Perlmut-
ter, 1984), (2) some more specifically labeled FrameNet
(Baker et al, 1998) roles for these same constituents;
(3) morphological and part of speech features; (4) point-
ers to gazetteers, both real and hypothetical (thanks to
B. Sundheim); (5) Veracity/According-To features based
on NYU?s proposed FactBank annotation scheme; (6)
various coreference features including some based on a
proposed extension to NomBank; (7) temporal features
based on Timex2 (Ferro et al, 2002) and TimeML (Puste-
jovsky et al, 2004); and (8) Information Structure fea-
tures based on (Calhoun et al, 2005). For more de-
tail, please see: http://nlp.cs.nyu.edu/meyers/pie-in-the-
sky.html
4 The Future of ?Pie in the Sky?
After this workshop, we plan to retire the current two
?Pie in the Sky? sentences and start again with some new
text. I observed the following obstacles during this ex-
periment: (1) annotation projects were somewhat hesi-
tant to volunteer their time (so we are extremely grateful
to all projects that did so.); (2) the target material was not
long enough for some annotation approaches to be able
to really make their mark, e.g., two sentences are not so
interesting for discourse purposes.; and (3) partially due
to its length, some interesting phenomena were not well-
represented (idioms, metonymy, etc.)
The lack of volunteers may, in part, be related to the
scale of the project. We built the project up slowly and
invited people to join in, rather than posting a request for
annotations to an international list. Initially, this was nec-
essary just to make the project possible to manage. Addi-
tionally, inadequacies of the data were probably barriers
for projects that focused on discourse phenomena or phe-
nomena that was not well-represented by our data. Nev-
ertheless, using more data may place too heavy a burden
on annotation projects and this could make projects hesi-
tant to participate.
With these issues in mind, I note that several sites an-
notated two longer documents for the recent U.S. Govern-
ment sponsored Semantic Annotation Planning Meeting
at the University of Maryland. This success was, in part,
due to the chance for annotation sites to attract govern-
ment interest in funding their projects. While we will not
attempt to duplicate this workshop, I believe that there
is an underlying issue that is very important. The field
really needs a single test corpus for all new annotation
projects.
This test corpus would meet a number of important
needs of the annotation community: (1) it would pro-
vide a testbed for new annotation schemata; (2) it would
provide a large corpus that is annotated in a fairly com-
plete framework ? this way focused annotation projects
may be able to more easily write specifications in light
of where their particular set of phenomena fit into some
larger framework; and (3) it would provide a steady flow
of input annotation in order to produce a single unified
annotation framework.
To make this idea a reality, we need to obtain a con-
sensus on what people would like to annotate. Addi-
tionally, we need volunteers to translate this same cor-
pus into other languages, as we would inevitably choose
an English corpus. Of course, if we could find a suitable
text that was already translated in multiple languages, this
would save time. The perfect text would be article length
(loosely defined); include difficult to handle phenomena
(idioms, metonymy, etc.); include a wide range of an-
notatable linguistic phenomena and not have copyright
restrictions which would hamper the project. It would,
of course, be helpful if the annotation community would
provide input on which text to choose ? this would avoid
a situation where one could not annotate the test text be-
cause the target phenomenon is not represented there.
In summary, I have used this introduction to both sum-
marize how the papers of this workshop fit together, to
propose some unifying themes for discussion, and to pro-
pose an agenda for how to proceed after the workshop is
over. We hope to see some of these ideas come to fruition
before ?Frontiers in Corpus Annotation III.?
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceedings
of Coling-ACL98: The 17th International Conference
on Computational Linguistics and the 36th Meeting of
the Association for Computational Linguistics, pages
86?90.
A. Bies, S. Kulick, and M. Mandel. 2005. Parallel En-
tity and Treebank Annotation. In ACL 2005 Workshop:
Frontiers in Corpus Annotation II: Pie in the Sky.
S. Calhoun, M. Nissim, M. Steedman, and J. Brenier.
2005. A Framework for Annotating Information Struc-
3
ture in Discourse. In ACL 2005 Workshop: Frontiers
in Corpus Annotation II: Pie in the Sky.
N. Dinesh, A. Lee, E. Miltsakaki, R. Prasad, A. Joshi,
and B. Webber. 2005. Attribution and the (Non-
)Alignment of Syntactic and Discourse Arguments of
Connectives. In ACL 2005 Workshop: Frontiers in
Corpus Annotation II: Pie in the Sky.
E. W. Hinrichs and S. Ku?bler and K. Naumann. 2005. A
Unified Reprewsentation for Morphological, Syntac-
tic, Semantic, and Referential Annotations. In ACL
2005 Workshop: Frontiers in Corpus Annotation II:
Pie in the Sky.
L. Ferro, L. Gerber, I. Mani, B. Sundheim, and G. Wil-
son. 2002. Instruction Manual for the Annotation of
Temporal Expressions. MITRE Washington C3 Cen-
ter, McLean, Virginia.
Eva Hajic?ova? and Mark?eta Ceplova?. 2000. Deletions
and Their Reconstruction in Tectogrammatical Syntac-
tic Tagging of Very Large Corpora. In Proceedings of
Coling 2000: The 18th International Conference on
Computational Linguistics, pages 278?284.
T. Inui and M. Okumura. 2005. Investigating the Char-
acteristics of Causal Relations in Japanese Text. In
ACL 2005 Workshop: Frontiers in Corpus Annotation
II: Pie in the Sky.
M. McShane, S. Nirenburg, S. Beale, and T. O?Hara.
2005. Semantically Rich Human-Aided Machine An-
notation. In ACL 2005 Workshop: Frontiers in Corpus
Annotation II: Pie in the Sky.
M. Palmer, N. Xue, O. Babko-Malaya, J. Chen, and
B. Snyder. 2005. A Parallel Proposition Bank II for
Chinese and English. In ACL 2005 Workshop: Fron-
tiers in Corpus Annotation II: Pie in the Sky.
David. M. Perlmutter. 1984. Studies in Relational Gram-
mar 1. The University of Chicago Press, Chicago.
M. Poesio and R. Artstein. 2005. The Reliability of
Anaphoric Annotation, Reconsidered: Taking Ambi-
guity into Account. In ACL 2005 Workshop: Frontiers
in Corpus Annotation II: Pie in the Sky.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press and
CSLI Publications, Chicago and Stanford.
J. Pustejovsky, B. Ingria, R. Sauri, J. Castano, J. Littman,
R. Gaizauskas, A. Setzer, G. Katz, and I. Mani. 2004.
The Specification Language TimeML. In I. Mani,
J. Pustejovsky, and R. Gaizauskas, editors, The Lan-
guage of Time: A Reader. Oxford University Press,
Oxford.
J. Pustejovsky, A. Meyers, M. Palmer, and M. Poe-
sio. 2005. Merging PropBank, NomBank, TimeBank,
Penn Discourse Treebank and Coreference. In ACL
2005 Workshop: Frontiers in Corpus Annotation II:
Pie in the Sky.
T. Wilson and J. Wiebe. 2005. Annotating Attributions
and Private States. In ACL 2005 Workshop: Frontiers
in Corpus Annotation II: Pie in the Sky.
N. Xue. 2005. Annotating Discourse Connectives in the
Chinese Treebank. In ACL 2005 Workshop: Frontiers
in Corpus Annotation II: Pie in the Sky.
4
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 38?53,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Annotation Compatibility Working Group Report* 
 
 
Contributors: A. Meyers, A. C. Fang, L. Ferro, S. K?bler, T. Jia-Lin, M. Palmer, M. Poesio, 
A. Dolbey, K. K. Schuler, E. Loper, H. Zinsmeister, G. Penn, N. Xue, E. Hinrichs, J. Wiebe, 
J. Pustejovsky, D. Farwell, E. Hajicova, B. Dorr, E. Hovy, B. A. Onyshkevych, L. Levin 
Editor: A Meyers meyers@cs.nyu.edu 
*As this report is a compilation, some sections may not reflect the views of individual contributors. 
Abstract 
This report explores the question of compatibility 
between annotation projects including translating 
annotation formalisms  to each other or to 
common forms. Compatibility issues are crucial 
for systems that use the results of multiple 
annotation projects. We hope that this report will 
begin a concerted effort in the field to track the 
compatibility of annotation schemes for part of 
speech tagging, time annotation, treebanking, 
role labeling and other phenomena. 
1. Introduction 
Different corpus annotation projects are driven 
by different goals, are applied to different types 
of data (different genres, different languages, 
etc.) and are created by people with different 
intellectual backgrounds. As a result of these and 
other factors, different annotation efforts make 
different underlying theoretical assumptions. 
Thus, no annotation project is really theory-
neutral, and in fact, none should be. It is the 
theoretical concerns which make it possible to 
write the specifications for an annotation project 
and which cause the resulting annotation to be 
consistent and thus usable for various natural 
language processing (NLP) applications. Of 
course the theories chosen for annotation projects 
tend to be theories that are useful for NLP. They 
place a high value on descriptive adequacy (they 
cover the data), they are formalized sufficiently 
for consistent annotation to be possible, and they 
tend to share major theoretical assumptions with 
other annotation efforts, e.g., the noun is the head 
of the noun phrase, the verb is the head of the 
sentence, etc. Thus the term theory-neutral is 
often used to mean something like NLP-friendly. 
Obviously, the annotation compatibility problem 
that we address here is much simpler than it 
would be if we had to consider theories which 
place a low emphasis on NLP-friendly properties 
(Minimalism. Optimality Theory, etc.).  
As annotation projects are usually research 
efforts, the inherent theoretical differences may 
be viewed as part of a search for the truth and the 
enforcement of adherence to a given (potentially 
wrong) theory could hamper this search. In 
addition, annotation of particular phenomena 
may be simplified by making theoretical 
assumptions conducive to describing those 
phenomena. For example, relative pronouns 
(e.g., that in the NP the book that she read) may 
be viewed as pronouns in an anaphora annotation 
project, but as intermediate links to arguments 
for a study of predicate argument structure.  
On the other hand, many applications would 
benefit by merging the results of different 
annotation projects. Thus, differences between 
annotation projects may be viewed as obstacles. 
For example, combining two or more corpora 
annotated with the same information may 
improve a system (i.e., "there's no data like more 
data.") To accomplish this, it may be necessary 
to convert corpora annotated according to one set 
of specifications into a different system or to 
convert two annotation systems into a third 
system. For example, to obtain lots of part of 
speech data for English, it is advantageous to 
convert POS tags from several tagsets (see 
Section 2) into a common form. For more 
temporal data than is available in Timex3 format, 
one might have to convert Timex2 and Timex3 
tags into a common form (See Section 5).  
Compromises that do not involve conversion can 
be flawed. For example, a machine learner may 
determine that feature A in framework 1 predicts 
feature A' in framework 2. However, the system 
may miss that features A and B in framework 1 
actually both correspond to feature A', i.e., they 
are subtypes. In our view, directly modeling the 
parameters of compatibility would be preferable.  
38
Some researchers have attempted to combine a 
number of different resource annotations into a 
single merged form. One motivation is that the 
merged representation may be more than the sum 
of its parts. It is likely that inconsistencies and 
errors (often induced by task-specific biases) can 
be identified and adjusted in the merging 
process; inferences may be drawn from how the 
component annotation systems interact; a 
complex annotation in a single framework may 
be easier for a system to process than several 
annotations in different frameworks; and a 
merged framework will help guide further 
annotation research (Pustojevsky, et. al. 2005). 
Another reason to merge is that a merged 
resource in language A may be similar to an 
existing resource in language B. Thus merging 
resources may present opportunities for 
constructing nearly parallel resources, which in 
turn could prove useful for a multilingual 
application. Merging PropBank (Kingsbury, and 
Palmer 2002) and NomBank (Meyers, et. al. 
2004) would yield a predicate argument structure 
for nouns and verbs, carrying more similar 
information to the Praque Dependency 
TreeBank's TectoGrammatical structure 
(Hajicova and Ceplova, 2000) than either 
component. 
This report and an expanded online version 
http://nlp.cs.nyu.edu/wiki/corpuswg/Annotation
Compatibility  both describe how to find 
correspondences between annotation 
frameworks. This information can be used to 
combine various annotation resources in 
different ways, according to one?s research goals, 
and, perhaps, could lead to some standards for 
combining annotation. This report will outline 
some of our initial findings in this effort with an 
eye towards maintaining and updating the online 
version in the future. We hope this is a step 
towards making it easier for systems to use 
multiple annotation resources.  
2. Part of Speech and Phrasal Categories 
On our website, we provide correspondences 
among a number of different part of speech 
tagsets in a version of the table from pp. 141--
142 of Manning and Sch?tze (1999),  modified 
to include the POS classes from CLAWS1 and 
ICE.  Table 1 is a sample taken from this table 
for expository purposes (the full table is not 
provided due to space limitations). Traditionally, 
part of speech represents a fairly coarse-grained 
division among types of words, usually 
distinguishing among: nouns, verbs, adjectives, 
adverbs, determiners and possibly a few other 
classes. While part of speech classifications may 
vary for particular words, especially closed class 
items, we have observed a larger problem. Most 
part of speech annotation projects incorporate 
other distinctions into part of speech 
classification. Furthermore, they incorporate 
different types of distinctions. As a result, 
conversion between one tagset and another is 
rarely one to one. It can, in fact, be many to 
many, e.g., BROWN does not distinguish the 
Table 1: Part of Speech Compatibility  
Extending Manning and Sch?tze 1999, pp. 141-142, 
 to cover Claws1 and ICE -- Longer Version Online  
Class Wrds  
Claws 
c5, 
Claws1  
Brow
n  
PTB  ICE  
Adj 
Hap-
py, 
bad  
AJ0  JJ  JJ  ADJ. ge  
Adj, 
comp  
hap-
pier, 
wors
e  
AJC  JJR  JJR  ADJ. 
comp  
Adj, 
super 
nic-
est-
worst  
AJS  JJT  JJS  ADJ. 
sup  
Adj,  
past 
part  
eaten JJ  ??  VBN
, JJ  
ADJ. 
edp  
Adj, 
pres 
part 
calm-
ing  JJ  ??  
VBG
, JJ  
ADJ. 
ingp  
Adv 
slow-
ly, 
sweet
-ly  
AV0  RB  RB  ADV. ge  
Adv 
comp 
 
faster  AV0  RBR  RBR  ADV. 
comp  
Adv 
super  
fast-
est  AV0  RBT  RBS  
ADV. 
sup  
Adv 
Part 
 
up, 
off, 
out  
AVP, 
RP, RI  RP  RP  
ADV. 
{phras, 
ge}  
Conj 
coord  
and, 
or  
CJC, 
CC  CC  CC  
CON-
JUNC. 
39
coord  
Det  
this, 
each, 
ano-
ther  
DT0, 
DT  DT  DT  
PRON.
dem.si
ng, 
PRON
(recip)  
Det. 
pron  
any, 
some  
DT0, 
DTI  DT1  DT  
PRON.
nonass, 
PRON.
ass  
Det 
pron 
Plur 
 
these 
those  
DT0, 
DTS  DTS  DT  
PRON.
dem. 
plu  
Det 
preq  quite  
DT0, 
aBL  ABL  PDT  
ADV 
.intens  
Det 
preq 
 
all, 
half  
DT0, 
ABN  ABN  PDT  
PRON.
univ, 
PRON.
quant  
Noun  
air-
craft, 
data  
NN0  NN  NN  N.com.
sing  
Noun
sing 
 
cat, 
pen  NN1  NN  NN  
N.com.
sing  
Noun
plur 
  
cats, 
pens  NN2  NNS  NNS  
N.com.
plu  
Noun
prop 
sing 
 
Paris, 
Mike 
 
NP0  NP  NNP  N.prop
.sing  
Verb. 
base 
pres 
 
take, 
live  VVB  VB  VBP  
V.X. 
{pres, 
imp}  
Verb, 
infin 
take, 
live  VVI  VB  VB  
V.X. 
infin  
Verb, 
past   
took, 
lived  VVD  VBD  VBD  
V.X. 
past  
Verb, 
pres 
part 
 
tak-
ing, 
liv-
ing  
VVG  VBG  VBG  V.X. ingp  
Verb, 
past-
part 
 
taken
, 
lived  
VVN  VBN  VBN  V.X. 
edp  
Verb, 
pres 
takes
, 
VVZ  VBZ  VBZ  V.X. pres  
infinitive form of a verb (VB in the Penn 
Treebank, V.X.infin in ICE) from the present-
tense form (VBP in the Penn Treebank, V.X.pres 
in ICE) that has the same spelling (e.g., see in 
They see no reason to leave). In contrast, ICE 
distinguishes among several different 
subcategories of verb (cop, intr, cxtr, dimontr, 
ditr, montr and TRANS) and the Penn Treebank 
does not.1 In a hypothetical system which merges 
all the different POS tagsets, it would be 
advantageous to factor out different types of 
features (similar to ICE), but include all the 
distinctions made by all the tag sets. For 
example, if a token give is tagged as VBP in the 
Penn Treebank, VBP would be converted into 
VERB.anysubc.pres. If another token give was 
tagged VB in Brown, VB would be converted to 
VERB.anysubc{infin,n3pres} (n3pres = not-3rd-
person and present tense). This allows systems to 
acquire the maximum information from corpora, 
tagged by different research groups.  
CKIP Chinese-Treebank (CCTB) and Penn 
Chinese Treebank (PCTB) are two important 
resources for Treebank-derived Chinese NLP 
tasks (CKIP, 1995; Xia et al, 2000; Xu et al, 
2002; Li et al, 2004). CCTB is developed in 
traditional Chinese (BIG5-encoded) at the 
Academia Sinica, Taiwan (Chen et al, 1999; 
Chen et al, 2003). CCTB uses the Information-
based Case Grammar (ICG) framework to 
express both syntactic and semantic descriptions. 
The present version CCTB3 (Version 3) provides 
61,087 Chinese sentences, 361,834 words and 6 
files that are bracketed and post-edited by 
humans based on a 5-million-word tagged Sinica 
Corpus (CKIP, 1995). CKIP POS tagging is a 
hierarchical system. The first POS layers include 
eight main syntactic categories, i.e. N (noun), V 
(verb), D (adverb), A (adjective), C 
(conjunction), I (interjection), T (particles) and P 
(preposition). In CCTB, there are 6 non-terminal 
phrasal categories: S (a complete tree headed by 
a predicate), VP (a phrase headed by a 
predicate), NP (a phrase beaded by an N), GP (a 
phrase headed by locational noun or adjunct), PP 
                                                 
1
 In the ICE column of Table 1 X represents a the 
disjunction of verb subcategorization types {cop, 
intr, cxtr, dimontr, ditr, montr, trans}.  
 
40
(a phrase headed by a preposition) and XP (a 
conjunctive phrase that is headed by a 
conjunction). 
Top Layer (TL) Bottom Layer (BL) Exam-
ples PCTB CCTB PCTB CCTB 
 

in 
other 
words 
ADVP Head AD Dk 

there-
fore 
ADVP result AD Cbca 
	

be-
cause 
P reason P Cbaa 

past 
NP-
TMP time:NP NT Ndda 

last 
year 
NP-
TMP NP NT Ndaba 

amon
g 
NP-
ADV NP NN Nep 


also 
DVP ADV AD:DEV Dk 



 
in the 
last 
few 
years 
LCP-
TMP GP 
NT:LCG
P 
Nddc:N
g 
 PCTB annotates simplified Chinese texts (GB-
encoded) from newswire sources (Xinhua 
newswire, Hong Kong news and Sinorama news 
magazine, Taiwan). It is developed at the 
University of Pennsylvania (UPenn). The PCTB 
annotates Chinese texts with syntactic 
bracketing, part of speech information, empty 
categories and function tags (Xia et al 2000, 
2002, 2005). The predicate-argument structure of 
Chinese verbs for the PCTB is encoded in the 
Penn Chinese Proposition Bank (Xue, et. Al. 
2005). The present version PCTB5.1 (PCTB 
Version 5.1), contains 18,782 sentences, 507,222 
words, 824,983 Hanzi and 890 data files.  
PCTB?s bracketing annotation is in the same 
framework as other Penn Treebanks, bearing a 
loose connection to the Government and Binding 
Theory paradigm. The PCTB annotation scheme 
involves 33 POS-tags, 17 phrasal tags, 6 verb 
compound tags, 7 empty category tags and 26 
functional tags.  
Table 2 includes Top-Layer/Bottom-Layer POS 
and phrasal categories correspondences between 
PCTB4 and CCTB3 for words/phrases expressed 
as the same Chinese characters in the same order.  
3. Differences Between Frameworks 
We assume that certain high level differences 
between annotation schemata should be ignored 
if at all possible, namely those that represent 
differences of analyses that are notationally 
equivalent. In this section, we will discuss those 
sorts of differences with an eye towards 
evaluating whether real differences do in fact 
exist, so that way users of annotation can be 
careful should these differences be of 
significance to their particular application.  
To clarify, we are talking about the sort of high 
level differences which reflect differences in the 
linguistic framework used for representing 
annotation, e.g., many frameworks represent 
long distance dependencies in equivalent, but 
different ways. In this sense, the linguistic 
framework of the Penn Treebank is a phrase 
structure based framework that includes a 
particular set of node labels (POS tags, phrasal 
categories, etc.), function tags, indices, etc. 2.  
3.1 Dependency vs. Constituency 
Figure 1 is a candidate rule for converting a 
phrase structure tree to a dependency tree or vice 
versa. Given a phrase consisting of constituents 
C(n-i) to C(n+j), the rule assumes that: there is 
one unique constituent C(n) that is the head of 
the phrase; and it is possible to identify this head 
in the phrase structure grammar, either using a 
reliable heuristic or due to annotation that marks 
the head of the phrase. When converting the 
                                                 
2
 Linguistic frameworks are independent of encoding 
systems, e.g., Penn Treebank?s inline LISP-ish notation, can 
be converted to inline XML, offset annotation, etc., Such 
encoding differences are outside the scope of this report 
41
 Fig. 1: Candidate Consituency/Dependency Mapping  
phrase structure tree to the dependency tree, the 
rule promotes the head to the root of the tree. 
When converting a dependency tree to a phrase 
structure tree, the rule demotes the root to a 
constituent, possibly marking it as the head, and 
names the phrase based on the head?s part of 
speech, e.g., nouns are heads of NPs.  This rule is 
insufficient because: (1) Identifying the head of a 
phrase by heuristics is not 100% reliable and 
most phrase structure annotation does not include 
a marking for the head; (2)  Some phrase 
structure distinctions do not translate well to 
some Dependency Grammars, e.g., the VP 
analysis and nestings of prenominal modifiers3; 
and (3) The rule only works for phrases that fit 
the head plus modifiers pattern and many phrases 
do not fit this pattern (uncontroversially). 
While most assume that verbs act like the head 
of the sentence, a Subject + VP analysis of a 
sentence complicates this slightly. Regarding S-
bars (relative clauses, that-S, subordinate-
conjunction + S, etc.), there is some variation 
                                                 
3
 The Prague Depedency Treebank orders dependency 
branches from the same head to represent the scope of the 
dependencies. Applicative Universal Grammar (Shauyman 
1977) incorporates phrases into dependency structure.  
 
among theories whether the verb or the pre-S 
element (that, subordinate conjunction, etc.) is 
assigned the head label.  Names, Dates, and 
other "patterned" phrases don't seem to have a 
unique head. Rather there are sets of constituents 
which together act like the head. For example, in 
Dr. Mary Smith, the string Mary Smith acts like 
the head. Idioms are big can of worms. Their 
headedness properties vary quite a bit. 
Sometimes they act like normal headed phrases 
and sometimes they don't. For example, the 
phrase pull strings for John obeys all the rules of 
English that would be expected of a verb phrase 
that consists of a verb, an NP and a complement 
PP. In contrast, the phrase let alne (Fillmore, et. 
al. 1988) has a syntax unique to that phrase. 
Semi-idiomatic constructions (including phrasal 
verbs, complex prepositions, etc.) raise some of 
the same questions as idioms. While making 
headedness assumptions similar to other phrases 
is relatively harmless, there is some variation. 
For example, in the phrase Mary called Fred up 
on the phone, there are two common views: (a) 
called is the head of the VP (or S) and up is a 
particle that depends on called; and (b) the VP 
has a complex head called up. For most 
purposes, the choice between these two analyses 
is arbitrary. Coordinate structures also require 
different treatment from head + modifier phrases 
-- there are multiple head-like constituents. 
A crucial factor is that the notion head is used to 
represent different things. (cf. Corbett, et. al. 
1993, Meyers 1995).  However, there are two 
dominant notions. The first we will call the 
functor (following Categorial Grammar). The 
functor is the glue that holds the phrase together 
-- the word that selects for the other words, 
determines word order, licenses the construction, 
etc. For example, coordinate conjunctions are 
functors because they link the constituents in 
their phrase together.  The second head like 
notion we will call the thematic head, the word 
or words that determine the external selectional 
properties of the phrase and usually the phrasal 
category as well. For example, in the noun 
phrase the book and the rock, the conjunction 
and is the functor, but the nouns and book and 
rock are thematic heads. The phrase is a concrete 
noun phrase due to book and rock. Thus the 
following sentence is well-formed: I held the 
book and the rock, but the following sentence is 
ill-formed *I held the noise and the redness. 
Furthermore, the phrase the book and the rock is 
a noun phrase, not a conjunction phrase.  
42
In summary, there are some differences between 
phrase structure and dependency analyses which 
may be lost in translation, e.g., dependency 
analyses include head-marking by default and 
phrase structure analyses do not. On the other 
hand, phrase structure analyses include relations 
between groupings of words which may not 
always be preserved when translating to 
dependencies. Moreover, both identifying heads 
and combining words into phrases have their 
own sets of problems which can come to the 
forefront when translation between the two 
modalities is attempted.  To be descriptively 
adequate, frameworks that mark heads do deal 
with these issues. The problem is that they are 
dealt with in different ways across dependency 
frameworks and across those phrase structure 
frameworks where heads are marked. For 
example, conjunction may be handled as being a 
distinct phenomenon (another dimension) that 
can be filtered through to the real heads. 
Alternatively, a head is selected on theoretical or 
heuristic grounds (the head of the first the 
conjunct, the conjunction, etc.) When working 
with multiple frameworks, a user must adjust for 
the assumptions of each framework. 
3.2 Gap Filling Mechanisms 
It is well-known that there are several equivalent 
ways to represent long distance and lexically 
based dependencies, e.g., (Sag and Fodor, 1994). 
Re-entrant graphs (graphs with shared structure), 
empty category/antecedent pairs, representations 
of discontinuous constituents, among other 
mechanisms can all be used to represent that 
there is some relation R between two (or more) 
elements in a linguistic structure that is, in some 
sense, noncanonical. The link by any of these 
mechanisms can be used to show that the relation 
R holds in spite of violations of a proximity 
constraint (long distance dependencies), a special 
construction such as control, or many other 
conditions. Some examples follow:  
1. Whati did you read ei? (WH extraction)  
2. The terroristi was captured ei (Passive)  
3. Ii wanted ei to accept  it. (Control)  
It seems to us that the same types of cases are 
difficult for all such approaches. In the 
unproblematic cases, there is a gap (or 
equivalent) with a unique filler found in the same 
sentence. In the "difficult" cases, this does not 
hold. In some examples, the filler is hypothetical 
and should be interpreted something like the 
pronoun anyone (4 below) or the person being 
addressed (5). In other examples, the identity 
between filler and gap is not so straight-forward. 
In examples like (6), filler and gap are type 
identical, not token identical (they represent 
different reading events). In examples like (7), a 
gap can take split antecedents. Conventional 
filler/gap mechanims of all types have to be 
modified to handle these types of examples.  
4. They explained how e to drive the car  
5. e don't talk to me!  
6. Sally [read a linguistics article]i, but 
John didn't ei.  
7. Sallyi spoke with Johnj about e,,i,j,, 
leaving together.  
3.3  Coreference and Anaphora 
There is little agreement concerning coreference 
annotation in the research community. Funding 
for the creation of the existing anaphorically 
annotated corpora (MUC6/7, ACE) has come 
primarily from initiatives focused on specific 
application tasks, resulting in task-oriented 
annotation schemes. On the other hand, a few 
(typically smaller) corpora have also been 
created to be consistent with existing, highly 
developed theoretical accounts of anaphora from 
a linguistic perspective. Accordingly, many 
schemes for annotating coreference or anaphora 
have been proposed, differing significantly with 
respect to: (1) the task definition, i.e., what type 
of semantic relations are annotated; (2) the 
flexibility that annotators have.  
By far the best known and most used scheme is 
that originally proposed for MUC 6 and later 
adapted for ACE. This scheme was developed to 
support information extraction and its primary 
aim is to identify all mentions of the same 
objects in the text (?coreference?) so as to collect 
all predications about them. A <coref> element 
is used to identify mentions of objects (the 
MARKABLES); each markable is given an 
index; subsequent mentions of already 
introduced objects are indicated by means of the 
REF attribute, which specifies the index of the 
previous mention of the same object. For 
example, in (1), markable 10 is a mention of the 
same object as markable 9. (This example is 
adapted from a presentation by Jutta Jaeger.) 
43
1.  <coref id="9">The New Orleans Oil and 
Gas [...] company</coref> added that 
 <coref id="10" type="ident" ref="9"> 
it</coref> doesn?t expect [...].  
The purpose of the annotation is to support 
information extraction. To increase coding 
reliability, the MUC scheme conflates different 
semantic relations into a single IDENT relation. 
For example, coders marked pairs of NPs as 
standing in IDENT relations, even when these 
NPs would more normally be assumed to be in a 
predication relations, e.g., appositions as in 2 and 
NPs across a copula as in 3. This conflation of 
semantic relations is a convenient simplification 
in many cases but it is untenable in general, as 
discussed by van Deemter & Kibble (2001).  
2. Michael H. Jordan, the former head of 
Pepsico?s international operations  
3. Michael H. Jordan is the former head of 
Pepsico?s international operations  
From the point of view of markup technology, 
the way used to represent coreference relations in 
MUC is very restrictive. Only one type of link 
can be annotated at a time: i.e., it is not possibly 
to identify a markable as being both a mention of 
a previously introduced referent and as a 
bridging reference on a second referent. In 
addition, the annotators do not have the option to 
mark anaphoric expressions as ambiguous.  
The MATE m`eta-scheme? (Poesio, 1999) was 
proposed as a very general repertoire of markup 
elements that could be used to implement a 
variety of existing coreference schemes, such as 
MUC or the MapTask scheme, but also more 
linguistically motivated schemes. From the point 
of view of markup technology, the two crucial 
differences from the MUC markup method are 
that the MATE meta-scheme is (i) based on 
standoff technology, and, most relevant for what 
follows, (ii) follows the recommendations of the 
Text Encoding Initiative (TEI) which suggest 
separating relations (?LINKs?) from markables. 
LINKs can be used to annotate any form of 
semantic relations (indeed, the same notion was 
used in the TimeML annotation of temporal 
relations).  A structured link, an innovation of 
MATE, can represent ambiguity (Poesio & 
Artstein, 2005). In (4), for example, the 
antecedent of the pronoun realized by markable 
ne03 in utterance 3.3 could be either engine E2 
or the boxcar at Elmira; with the MATE scheme, 
coders can mark their uncertainty.  
4. [in file markable.xml]  
3.3: hook <COREF:DE ID=?ne01?>engine 
E2</COREF:DE> to  <COREF:DE ID=?ne02?> 
the boxcar at ? Elmira </COREF:DE>  
5.1: and send <COREF:DE ID=?ne03?> 
it</COREF:DE> to <COREF:DE ID=?ne04?> 
Corning</COREF:DE>  
[in a separate file ? e.g., link.xml]  
<COREF:LINK HREF= 
"markable.xml#id(ne03)" type=?ident?> 
<COREF:ANCHOR HREF= 
?markable.xml#id(ne01)? />  
<COREF:ANCHOR HREF= 
?markable.xml#id(ne02)? /> 
 </COREF:LINK>  
The MATE meta-scheme also allowed a richer 
set of semantic relations in addition to IDENT, 
including PART-OF, PRED for predicates, etc., 
as well as methods for marking antecedents not 
explicitly introduced via an NP, such as plans 
and propositions. Of course, using this added 
power is only sensible when accompanied by 
experimentally tested coding schemes.  
The MATE meta-scheme was the starting point 
for the coding scheme used in the GNOME 
project (Poesio 2004). In this project, a scheme 
was developed to model anaphoric relations in 
text in the linguistic sense?e.g., the information 
about discourse entities and their semantic 
relations expressed by the text. A relation called 
IDENT was included, but it was only used to 
mark the relation between mentions of the same 
discourse entity; so, for example, neither of the 
relations in (2) would be marked in this way.  
From the point of view of coding schemes used 
for resource creation, the MATE meta-scheme 
gave rise to two developments: the standoff 
representation used in the MMAX annotation 
tool, and the Reference Annotation Framework 
(Salmon-Alt & Romary, 2004). MMAX was the 
first usable annotation tool for standoff 
annotation of coreference (there are now at least 
three alternatives: Penn?s WordFreak, MITRE?s 
CALISTO, and the NITE XML tools). The 
markup scheme was a simplification of the 
MATE scheme, in several respects. First of all, 
cross-level reference is not done using href links, 
44
but by specifying once and for all which files 
contain the base level and which files contain 
each level of representation; each level points to 
the same base level. Secondly, markables and 
coref links are contained in the same file.  
5. [ markable file]  
<?xml version="1.0"?> <markables> ?? 
<markable id="markable_36" span= 
"word_5,word_6, word_7?member="set_22" > 
</markable> ?. <markable id="markable_37" 
span="word_14, word_15, word_16" 
member="set_22" > </markable> ?.  
</markables>  
The original version of MMAX, 0.94, only 
allowed to specify one identity link and one 
bridging reference per markable, but beginning 
version 2.0,  multiple pointers are possible. An 
interesting aspect of the proposal is that identity 
links are represented by specifying membership 
to coreference chains instead of linking to 
previous mentions. Multiple pointers were used 
in the ARRAU project to represent ambiguous 
links, with some restrictions. The RAF 
framework was proposed not to directly support 
annotation, but as a rich enough markup 
framework to be used for annotation exchange.  
3.3.2 Conversion 
Several types of conversion between formats for 
representing coreference information are 
routinely performed. Perhaps the most common 
problem is to convert between inline formats 
used for different corpora: e.g., to convert the 
MUC6 corpus into GNOME. However, it is 
becoming more and more necessary to to convert 
standoff into inline formats for processing (e.g., 
MMAX into MAS-XML), and viceversa.  
The increasing adoption of XML as a standard 
has made the technical aspect of conversion 
relatively straightforward, provided that the same 
information can be encoded. For example, 
because the GNOME format is richer than both 
the MUC and MMAX format, it should be 
straightforward to convert a MUC link into a 
GNOME link. However, the correctness of the 
conversion can only be ensured if the same 
coding instructions were followed; the MUC 
IDENT links used in (2) and (3) would not be 
expressed in the GNOME format as IDENT 
links. There is no standard method we know of 
for identifying these problematic links, although 
syntactic information can sometimes help. The 
opposite of course is not true; there is no direct 
way of representing the information in (4) in the 
MUC format.  Conversion between the MAS-
XML and the MMAX format is also possible, 
provided that pointers are used to represent both 
bridging references and identity links.  
4 Predicate-Argument Relations 
Predicate argument relations are labeled relations 
between two words/phrases of a linguistic 
description such that one is a semantic predicate 
or functor and the other is an argument of this 
predicate. In the sentence The eminent linguist 
read the book, there is a SUBJECT (or AGENT, 
READER, ARG0, DEPENDENT etc.) relation 
between the functor read and the phrase The 
eminent linguist or possibly the word linguist if 
assuming a dependency framework. Typically, 
the functor imposes selectional restrictions on the 
argument. The functor may impose word order 
restrictions as well, although this would only 
effect "local" arguments (e.g., not arguments 
related by WH extraction). Another popular way 
of expressing this relation is to say that read 
assigns the SUBJECT role to The eminent 
linguist in that sentence. Unfortunately, this way 
of stating the relation sometimes gives the false 
impression that a particular phrase can only be a 
member of one such relation. However, this is 
clearly not the case, e.g., in The eminent linguist 
who John admires read the book, The eminent 
linguist is the argument of: (1) a SUBJECT 
relation with read and an OBJECT relation with 
admires. Predicate-argument roles label relations 
between items and are not simply tags on phrases 
(like Named Entity Tags, for example).  
There are several reasons why predicate 
argument relations are of interest for natural 
language processing, but perhaps the most basic 
reason is that they provide a way to factor out the 
common meanings from equivalent or nearly 
equivalent utterances. For example, most 
systems would represent the relation between 
Mary and eat in much the same way in the 
sentences: Mary ate the sandwich, The sandwich 
was eaten by Mary, and Mary wanted to eat the 
sandwich. Crucially, the shared aspect of 
meaning can be modeled as a relation with eat 
(or ate) as the functor and Mary as the argument 
(e.g., SUBJECT). Thus providing predicate 
45
argument relations can provide a way to 
generalize over data and, perhaps, allow systems 
to mitigate against the sparse data problem.  
Systems for representing predicate argument 
relations vary drastically in granularity,  In 
particular, there is a long history of disagreement 
about the appropriate level of granularity of role 
labeling, the tags used to distinguish between 
predicate argument relations. At one extreme, no 
distinction is made between predicate relations, 
one simply marks that the functor and argument 
are in a predicate-argument relation (e.g.,  
unlabeled dependency trees).  In another 
approach, one might distinguish among the 
arguments of each predicate with a small set of 
labels, sometimes numbered -- examples of this 
approach include Relational Grammar 
(Perlmutter 1984), PropBank and NomBank. 
These labels have different meanings for each 
functor, e.g., the subject of eat, write and devour 
are distinct. This assumes a very high level of 
granularity, i.e., there are several times the 
number of possible relations as there are distinct 
functors. So 1000 verbs may license as many as 
5000 distinct relations.  Under other approaches, 
a small set of relation types are generalized 
across functors. For example, under Relational 
Grammar's Universal Alignment Hypothesis 
(Perlmutter and Postal 1984, Rosen 1984), 
subject, object and indirect object relations are 
assumed to be of the same types regardless of 
verb. These terms thus are fairly coarse-grained 
distinctions between types of predicate/argument 
relations between verbs and their arguments.  
Some predicate-neutral relations are more fine 
grained, including Panini's Karaka of 2000 years 
ago, and many of the more recent systems which 
make distinctions such as agent, patient, theme, 
recipient, etc. (Gruber 1965, Fillmore 1968, 
Jackendoff 1972).  The (current) International 
Annotation of Multilingual Text Corpora project 
(http://aitc.aitc.net.org/nsf/iamtc/) takes this 
approach. Critics claim that it can be difficult to 
maintain consistency across predicates with these 
systems without constantly increasing the 
inventory of role labels to describe idiosyncratic 
relations, e.g., the relation between the verbs 
multiply, conjugate, and their objects. For 
example, only a very idiosyncratic classification 
could capture the fact that only a large round 
object (like the Earth) can be the object of 
circumnavigate.  It can also be unclear which of 
two role labels apply. For example, there can be 
a thin line between a recipient and a goal, e.g., 
the prepositional object of John sent a letter to 
the Hospital could take one role or the other 
depending on a fairly subtle ambiguity.  
To avoid these problems, some annotation 
research (and some linguistic theories) has 
abandoned predicate-neutral approaches, in favor 
of the approaches that define predicate relations 
on a predicate by predicate basis. Furthermore, 
various balances have been attempted to solve 
some of the problems of the predicate-neutral 
relations. FrameNet defines roles on a scenario 
by scenario basis, which limits the growth of the 
inventory of relation labels and insures 
consistency within semantic domains or frames. 
On the other hand, the predicate-by-predicate 
approach is arguably less informative then the 
predicate-neutral approach, allowing for no 
generalization of roles across predicates. Thus 
although PropBank/NomBank use a strictly 
predicate by predicate approach, there have been 
some attempts to regularize the numbering for 
semantically related predicates. Furthermore, the 
descriptors used by the annotators to define roles 
can sometimes be used to help make finer 
distinctions (descriptors often include familiar 
role labels like agent, patient, etc.)  
The diversity of predicate argument labeling 
systems and the large inventory of possible role 
labels make it difficult to provide a simple 
mapping (like Table 1 for part of speech 
conversion) between these types of systems. The 
SemLink project provides some insight into how 
this mapping problem can be solved.  
4.2 SemLink 
SemLink is a project to link the lexical resources 
of FrameNet, PropBank, and VerbNet. The goal 
is to develop computationally explicit 
connections between these resources combining 
individual advantages and overcoming their 
limitations.  
4.2.1 Background 
VerbNet consists of hierarchies of verb classes, 
extended from those of Levin 1993. Each class 
and subclass is characterized extensionally by its 
set of verbs, and intensionally by argument lists 
and syntactic/semantic features of verbs. The full 
argument list consists of 23 thematic roles, and 
46
possible selectional restrictions on the arguments 
are expressed using binary predicates.  VerbNet 
has been extended from the Levin classes, and 
now covers 4526 senses for 3175 lexemes. A 
primary emphasis for VerbNet is grouping verbs 
into classes with coherent syntactic and semantic 
characterizations in order to facilitate acquisition 
of new class members based on observable 
syntactic/semantic behavior. The hierarchical 
structure and small number of thematic roles is 
intended to support generalizations.  
FrameNet consists of collections of semantic 
frames, lexical units that evoke these frames, and 
annotation reports that demonstrate uses of 
lexical units. Each semantic frame specifies a set 
of frame elements. These are elements that 
describe the situational props, participants and 
components that conceptually make up part of 
the frame. Lexical units appear in a variety of 
parts of speech, though we focus on verbs here. 
A lexical unit is a lexeme in a particular sense 
defined in its containing semantic frame. They 
are described in reports that list the syntactic 
realizations of the frame elements, and valence 
patterns that describe possible syntactic linking 
patterns. 3486 verb lexical units have been 
described in FrameNet which places a primary 
emphasis on providing rich, idiosyncratic 
descriptions of semantic properties of lexical 
units in context, and making explicit subtle 
differences in meaning. As such it could provide 
an important foundation for reasoning about 
context dependent semantic representations. 
However, the large number of frame elements 
and the current sparseness of annotations for 
each one has hindered machine learning.  
PropBank is an annotation of 1M words of the 
Wall Street Journal portion of the Penn Treebank 
II with semantic role labels for each verb 
argument. Although the semantic roles labels are 
purposely chosen to be quite generic, i.e., ArgO, 
Arg1, etc., they are still intended to consistently 
annotate the same semantic role across syntactic 
variations, e.g., Arg1 in "John broke the 
window" is the same window (syntactic object) 
that is annotated as the Arg1 in "The window 
broke" (syntactic subject). The primary goal of 
PropBank is to provide consistent general 
labeling of semantic roles for a large quantity of 
text that can provide training data for supervised 
machine learning algorithms.  PropBank can also 
provide frequency counts for (statistical) analysis 
or generation.  PropBank includes a lexicon 
which lists, for each broad meaning of each 
annotated verb, its "frameset", the possible 
arguments, their labels and all possible syntactic 
realizations. This lexical resource is used as a set 
of verb-specific guidelines by the annotators, and 
can be seen as quite similar in nature to 
FrameNet, although much more coarse-grained 
and general purpose in the specifics.  
To summarize, PropBank and FrameNet both 
annotate the same verb arguments, but assign 
different labels. PropBank has a small number of 
vague, general purpose labels with sufficient 
amounts of training data geared specifically to 
support successful machine learning. FrameNet 
provides a much richer and more explicit 
semantics, but without sufficient amounts of 
training data for the hundreds of individual frame 
elements. An ideal environment would allow us 
to train generic semantic role labelers on 
PropBank, run them on new data, and then be 
able to map the resulting PropBank argument 
labels on rich FrameNet frame elements.  
The goal of SemLink is to create just such an 
environment. VerbNet provides a level of 
representation that is still tied to syntax, in the 
way that PropBank is, but provides a somewhat 
more fine-grained set of role labels and a set of 
fairly high level, general purpose semantic 
predicates, such as contact(x,y), change-of-
location(x, path), cause(A, X), etc. As such it can 
be seen as a mediator between PropBank and 
FrameNet. In fact, our approach has been to use 
the explicit syntactic frames of VerbNet to semi-
automatically map the PropBank instances onto 
specific VerbNet classes and role labels. The 
mapping can then be hand-corrected. In parallel, 
SemLink has been creating a mapping table from 
VerbNet class(es) to FrameNet frame(s), and 
from role label to frame element. This will allow 
the SemLink project to automatically generate 
FrameNet representations for every VerbNet 
version of a PropBank instance with an entry in 
the VerbNet-FrameNet mapping table.  
4.2.2 VerbNet <==> FrameNet linking 
One of the tasks for the SemLink project is to 
provide explicit mappings between VerbNet and 
FrameNet. The mappings between these two 
resources which have complementary 
information about verbs and disjoint coverage 
open several possibilities to increase their 
47
robustness. The fact that these two resources are 
now mapped gives researchers different levels of 
representation for events these verbs represent to 
be used in natural language applications. The 
mapping between VerbNet and FrameNet was 
done in two steps: (1) mapping VerbNet verb 
senses to FrameNet lexical units; (2) mapping 
VerbNet thematic roles to the equivalent (if pre-
sent) FrameNet frame elements for the corre-
sponding class/frame mappings uncovered dur-
ing step 1.  
In the first task, VerbNet verb senses were 
mapped to corresponding FrameNet senses, if 
available.  Each verb member of a VerbNet class 
was assigned to a (set of) lexical units of Frame-
Net frames according to semantic meaning and 
to the roles this verb instance takes. These 
mappings are not one-to-one since VerbNet and 
FrameNet were built with distinctly different 
design philosophies. VerbNet verb classes are 
constructed by grouping verbs based mostly on 
their participation in diathesis alternations. In 
contrast, FrameNet is designed to group lexical 
items based on frame semantics, and a single 
FrameNet frame may contain sets of verbs with 
related senses but different subcategorization 
properties and sets of verbs with similar syntactic 
behavior may appear in multiple frames.  
The second task consisted of mapping VerbNet 
thematic roles to FrameNet frame elements for 
the pairs of classes/frames found in the first task. 
As in the first task, the mapping is not always 
one-to-one as FrameNet tends to record much 
more fine-grained distinctions than VerbNet.  
So far, 1892 VerbNet senses representing 209 
classes were successfully mapped to FrameNet 
frames. This resulted in 582 VerbNet class ? 
FrameNet frame mappings, across 263 unique 
FrameNet frames, for a total of 2170 mappings 
of VerbNet verbs to FrameNet lexical units. 
4.2.3 PropBank <==> VerbNet linking 
SemLink is also creating a mapping between 
VerbNet and PropBank, which will allow the use 
of the machine learning techniques that have 
been developed for PropBank annotations to 
generate more semantically abstract VerbNet 
representations. The mapping between VerbNet 
and PropBank can be divided into two parts: a 
"lexical mapping" and an "instance classifier." 
The lexical mapping defines the set of possible 
mappings between the two lexicons, independent 
of context. In particular, for each item in the 
source lexicon, it specifies the possible 
corresponding items in the target lexicon; and for 
each of these mappings, specifies how the 
detailed fields of the source lexicon item (such as 
verb arguments) map to the detailed fields of the 
target lexicon item. The lexical mapping 
provides a set of possible mappings, but does not 
specify which of those mappings should be used 
for each instance; that is the job of the instance 
classifier, which looks at a source lexicon item in 
context, and chooses the most appropriate target 
lexicon items allowed by the lexical mapping.  
The lexical mapping was created semi-
automatically, based on an initial mapping which 
put VerbNet thematic roles in correspondence 
with individual PropBank framesets. This lexical 
mapping consists of a mapping between the 
PropBank framesets and VerbNet's verb classes; 
and a mapping between the roleset argument 
labels and the VerbNet thematic roles.  During 
this initial mapping, the process of assigning a 
verb class to a frameset was performed manually 
while creating new PropBank frames. The 
thematic role assignment, on the other hand, was 
a semi-automatic process which finds the best 
match for the argument labels, based on their 
descriptors, to the set of thematic role labels of 
VerbNet. This process required human 
intervention due to the variety of descriptors for 
PropBank labels, the fact that the argument label 
numbers are not consistent across verbs, and 
gaps in frameset to verb class mappings.  
To build the instance classifier, SemLink started 
with two heuristic classifiers. The first classifier 
works by running the SenseLearner WSD engine 
to find the WordNet class of each verb; and then 
using the existing WordNet/VerbNet mapping to 
choose the corresponding VerbNet class. This 
heuristic is limited by the performance of the 
WSD engine, and by the fact that the 
WordNet/VerbNet mapping is not available for 
all VerbNet verbs. The second heuristic classifier 
examines the syntactic context for each verb 
instance, and compares it to the syntactic frames 
of each VerbNet class. The VerbNet class with a 
syntactic frame that most closely matches the 
instance's context is assigned to the instance.  
The SemLink group ran these two heuristic 
methods on the Treebank corpus and are hand-
48
correcting the results in order to obtain a 
VerbNet-annotated version of the Treebank 
corpus. Since the Treebank corpus is also 
annotated with PropBank information, this will 
provide a parallel VerbNet/PropBank corpus, 
which can be used to train a supervised classifier 
to map from PropBank frames to VerbNet 
classes (and vice versa). The feature space for 
this machine learning classifier includes 
information about the lexical and syntactic 
context of the verb and its arguments, as well as 
the output of the two heuristic methods.  
5. Version Control 
Annotation compatibility is also an issue for 
related  formalisms. Two columns in Table 1 are 
devoted to different CLAWS POS tagsets, but there 
are several more CLAWS tagsets 
(www.comp.lancs.ac.uk/ucrel/annotation.html), 
differing both in degree of detail and choice of 
distinctions made. Thus a detailed conversion 
table among even just the CLAWS tagsets may 
prove handy. Similar issues arise with the year to 
year changes of the ACE annotation guidelines 
(projects.ldc.upenn.edu/ace/ ) which include 
named entity, semantic classes for nouns, 
anaphora, relation and event annotation. As 
annotation formalisms mature,  specifications 
can change to improve annotation consistency, 
speed or the usefulness for some specific task. In 
the interest of using old and new annotation 
together (more training data), it is helpful to have 
explicit mappings for related formalisms. Table 2 
is a (preliminary) conversion table for Timex2 
and Timex3, the latter of which can be viewed 
essentially as an elaboration of the former.  
Table 3: Temporal Markup Translation Table4  
Description  TIMEX2  TIMEX3  Comment  
Contains a normal-
ized form of the 
date/time  
VAL="1964-10-16"  val="1964-10-16"  Some TIMEX2 points are TIMEX3 durations  
Captures temporal 
modifiers  MOD="APPROX"  mod="approx"  ---  
Contains a normal-
ized form of an 
anchoring 
data/time  
ANCHOR_VAL 
="1964-W22"  ---  
See TIMEX3 beginPoint and 
endPoint  
Captures relative 
direction between 
VAL and AN-
CHOR_VAL  
ANCHOR_DIR=  
"BEFORE"  ---  
See TIMEX3 beginPoint and 
endPoint  
Identifies set ex-
pressions  SET="YES"  type="SET"  ---  
Provides unique ID 
number  ID="12"  tid="12"  
Used to relate time expres-
sions to other objects  
Identifies type of 
expression  ---  type="DATE"  
Hold over from TIMEX. De-
rivable from format of 
VAL/val  
Identifies indexical 
expressions  ---  temporalFunction="true"  
In TIMEX3, indexical expres-
sions are normalized via a 
temporal function, applied as 
post-process  
Identifies reference 
time used to com-
pute val  
---  anchorTimeID="t12"  Desired in TIMEX2  
Identifies dis- ---  functionInDocu- Used for date stamps on 
                                                 
4
 This preliminary table shows the attributes side by side with only one sample value, although other values are possible 
49
course function  ment="CREATION_TIME"  documents  
Captures anchors 
for durations  ---  
beginPoint="t11", end-
Point="t12"  
Captured by TIMEX2 AN-
CHOR attributes  
Captures quantifi-
cation of a set ex-
pression  
---  quant="EVERY"  Desired in TIMEX2  
Captures number 
of reoccurences in 
set expressions  
---  freq="2X"  Desired in TIMEX2  
6. The Effect of Language Differences 
Most researchers involved in linguistic 
annotation (particularly for NLP) take it for 
granted that coverage of a particular grammar for 
a particular language is of the utmost important. 
The (explanatory) adequacy of the particular 
linguistic theory assumed for multiple languages 
is considered a much less important. Given the 
diversity of annotation paradigms, we may go a 
step further and claim that it may be necessary to 
change theories when going from one language 
to another. In particular, language-specific 
phenomena can complicate theories in ways that 
prove unnecessary for languages lacking these 
phenomena. For example, English requires a 
much simpler morphological framework then 
languages like German, Russian, Turkish or 
Pashto. It has also been claimed on several 
occasions that a VP analysis is needed in some 
languages (English), but not others (Japanese). 
For the purposes of annotation, it would seem 
simplest  to choose the simplest language-
specific framework that is capable of capturing 
the distinctions that one is attempting to 
annotate. If the annotation is robust, it should be 
possible to convert it automatically into some 
language-neutral formalism should one arise that 
maximizes descriptive and explanatory 
adequacy. In the meanwhile, it would seem 
unnecessary to complicate grammars of specific 
languages to account for phenomena which do 
not occur in those languages.  
6.1 The German T?Ba-D/Z Treebank 
German has a freer word order than English. 
This concerns the distribution of the finite verb 
and the distribution of arguments and adjuncts. 
German is a general Verb-Second language 
which means that in the default structure in 
declarative main clauses as well as in wh-
questions the finite verb surfaces in second 
position preceded by only one constituent which 
is not necessarily the subject. In embedded 
clauses the finite verb normally occurs in a verb-
phrase-final position following its arguments and 
adjuncts, and other non-finite verbal elements. 
German is traditionally assumed to have a head-
final verb phrase. The ordering of arguments and 
adjuncts is relatively free. Firstly almost any 
constituent can be topicalised preceding the finite 
verb in Verb-Second position. Secondly the 
order of the remaining arguments and adjuncts is 
still relatively free. Ross (1967) coined the term 
Scrambling to describe the variety of linear 
orderings. Various factors are discussed to play a 
role here such as pronominal vs. phrasal 
constituency, information structure, definiteness 
and animacy (e.g. Uszkoreit 1986).  
The annotation scheme of the German T?Ba-D/Z 
treebank was developed with special regard to 
these properties of German clause structure. The 
main ordering principle is adopted from 
traditional descriptive analysis of German (e.g. 
Herling 1821, H?hle 1986). It partitions the 
clause into 'topological fields' which are defined 
by the distribution of the verbal elements. The 
top level of the syntactic tree is a flat structure of 
field categories including: Linke Klammer - left 
bracket (LK) and Rechte Klammer - verbal 
complex (VC) for verbal elements and Vorfeld - 
initial field (VF), C-Feld - complementiser field 
(C), Mittelfeld - middle field (MF), Nachfeld - 
final field (NF) for other elements.  
Below the level of field nodes the annotation 
scheme provides hierarchical phrase structures 
except for verb phrases. There are no verb 
phrases annotated in T?Ba-D/Z. It was one of the 
major design decisions to capture the distribution 
of verbal elements and their arguments and 
adjuncts in terms of topological fields instead of 
hierarchical verb phrase structures. The free 
word order would have required to make 
extensive use of traces or other mechanisms to 
relate dislocated constituents to their base 
50
positions,  which in itself was problematic since 
there is no consensus among German linguists on 
what the base ordering is. An alternative which 
avoids commitment to specific base positions is 
to use crossing branches to deal with 
discontinuous constituents. This approach is 
adopted for example by the German TIGER 
treebank (Brants et al 2004). A drawback of 
crossing branches is that the treebank cannot be 
modeled by a context free grammar. Since T?Ba-
D/Z was intended to be used for parser training, 
it was not a desirable option. Arguments and 
adjuncts are thus related to their predicates by 
means of functional labels. In contrast to the 
Penn Treebank, T?Ba-D/Z assigns grammatical 
functions to all arguments and adjuncts. Due to 
the freer word order functions cannot be derived 
from relative positions only.  
The choice of labels of grammatical functions is 
largely based on the insight that grammatical 
functions in German are directly related to the 
case assignment (Reis 1982). The labels 
therefore do not refer to grammatical functions 
such as subject, direct object or indirect object 
but make a distinction between complement and 
adjunct functions and classify the nominal 
complements according to their case marking: 
accusative object (OA), dative object (OD), 
genitive object (OG), and also nominative 
'object' (ON) versus verbal modifier (V-MOD) or 
underspecified modifier (MOD).  
Within phrases a head daughter is marked at each 
projection level. Exceptions are elliptical 
phrases, coordinate structures, strings of foreign 
language, proper names and appositions within 
noun phrases. Modifiers of arguments and 
adjuncts are assigned a default non-head 
function. In case of discontinuous constituents 
the function of the modifier is either explicitly 
marked by means of a complex label such as 
OA-MOD (the modifier of an accusative object) 
or by means of a secondary edge REFINT in 
case the modified phrase has a default head or 
non-head function itself (which holds in the case 
of e.g. NP complements of prepositions).  
Figures 2 to 4 illustrate the German T?Ba-D/Z 
treebank annotation scheme (Telljohann et al 
(2005). ? it combines a flat topological analysis 
with structural and functional information.  
Fig. 2: verb-second  
Dort w?rde er sicher angenommen werden. 
 there would he surely accepted be 
 'He would be surely accepted there.'  
Fig. 3: verb-final  
Zu hoffen ist, da? der R?ckzug vollst?ndig sein 
wird. to hope is that the fallback complete be will 
 'We hope that they will retreat completely.' 
Fig. 4: discont. constituent marked OA-MOD  
Wie w?rdet ihr das Land nennen, in dem ihr 
geboren wurdet? 
 how would you the country call in which you 
born were 
 'How would you call the country in which you 
were born?'  
 
 
51
7. Concluding Remarks 
This report has laid out several major annotation 
compatibility issues, focusing primarily on 
conversion among different annotation 
frameworks that represent the same type of 
information. We have provided procedures for 
conversion, along with their limitations. As more 
work needs to be done in this area, we intend to 
keep the online version available for cooperative 
elaboration and extension. Our hope is that the 
conversion tables will be extended and more 
annotation projects will incorporate details of 
their projects in order to facilitate compatibility.  
The compatibility between annotation 
frameworks becomes a concern when (for 
example) a user attempts to use annotation 
created under two or more distinct frameworks 
for a single application. This is true regardless of 
whether the annotation is of the same type (the 
user wants more data for a particular 
phenomenon); or of different types (the user 
wants to combine different types of information). 
Acknowledgement  
This research was supported, in part, by the Na-
tional Science Foundation under Grant CNI-
0551615. 
References 
Brants, S., S. Dipper, P. Eisenberg, S. Hansen, E. 
Knig, W. Lezius, C. Rohrer, G. Smith & H. 
Uszkoreit, 2004. TIGER: Linguistic 
Interpretation of a German Corpus. In E. 
Hinrichs and K. Simov, eds, Research on 
Language and Computation, Special Issue. 
Volume 2: 597-620.  
Chen, K.-J., Luo, C.-C., Gao, Z.-M., Chang, M.-
C., Chen, F.-Y., and Chen, C.-J., 1999. The 
CKIP Chinese Treebank. In Journ ees ATALA 
sur les Corpus annot es pour la syntaxe, Talana, 
Paris VII: pp.85-96.  
Chen, K.-J. et al Building and Using Parsed 
Corpora, 2003. (A. Abeill? eds) KLUWER, 
Dordrecht. .  
CKIP, 1995. Technical Report no. 95-02, the 
content and illustration of Sinica corpus of 
Academia Sinica. Inst. of Information Science.  
G. Corbett, N. M. Fraser, and S. McGlashan, 
1993. Heads in Grammatical Theory. Cambridge 
University Press, Cambridge.  
K. Van Deemter and R. Kibble, 2001. On 
Coreferring: Coreference in MUC and related 
Annotation schemes. Journal of Computational 
Linguistics 26, 4, S. 629-637  
C. Fillmore, 1968. The Case for Case. In E. Bach 
and R. T. Harms, eds, Universals in Linguistic 
Theory. Holt, Rinehart and Winston, NY  
C. Fillmore, P. Kay & M. O?Connor. 1988. 
Regularity and Idiomaticity in Grammatical 
Constructions: The Case of Let Alone., 
Language,  64:  501-538. 
J. S. Gruber, 1965. Studies in Lexical Relations. 
Ph.D. thesis, MIT 
E. Hajicov and M. Ceplov, 2000. Deletions and 
Their Reconstruction in Tectogrammatical 
Syntactic Tagging of Very Large Corpora. In 
Proceedings of Coling 2000:  pp. 278-284.  
S. H. A. Herling, 1821. ?ber die Topik der 
deutschen Sprache. In Abhandlungen des 
frankfurterischen Gelehrtenvereins f?r deutsche 
Sprache. Frankfurt/M. Drittes St?ck.  
T. N. H?hle, 1986. Der Begriff M`ittelfeld'. 
Anmerkungen ?ber die Theorie der 
topologischen Felder. In A. Sch?ne (Ed.), 
Kontroversen alte und neue. Akten des 7. 
Internationalen Germanistenkongresses 
G?ttingen. 329-340.  
R. Jackendoff, 1972. Semantic Interpretation in 
Generative Grammar. MIT Press, Cambridge.  
P. Kingsbury and M. Palmer 2002. From 
treebank to propbank. In Proc.  LREC-2002 
H. Lee, C.-N. Huang,  J. Gao and X. Fan, 2004. 
Chinese chunking with another type of spec. In 
SIGHAN-2004. Barcelona: pp. 41-48.  
B. Levin 1993. English Verb Classes and 
Alternations: A Preliminary Investigation. Univ. 
of Chicago Press. 
C. Manning and H. Sch?tze. 1999. Foundations 
of Statistical Natural Language Processing, MIT.  
52
A. Meyers, R. Reeves, C. Macleod, R. Szekely, 
V. Zielinska, B. Young, and R. Grishman, 2004. 
The NomBank Project: An Interim Report. In 
NAACL/HLT 2004 Workshop Frontiers in 
Corpus Annotation.  
A. Meyers, 1995. The NP Analysis of NP. In 
Papers from the 31st Regional Meeting of the 
Chicago Linguistic Society, pp. 329-342.  
D. M. Perlmutter and P. M. Postal, 1984. The 1-
Advancement Exclusiveness Law. In D. M. 
Perlmutter & C. G. Rosen, eds 1984. Studies in 
Relational Grammar 2. Univ. of Chicago Press. 
D.. M. Perlmutter, 1984. Studies in Relational 
Grammar 1. Univ. of Chicago Press.  
M. Poesio, 1999. Coreference, in MATE 
Deliverable 2.1, http://www.ims.uni-
stuttgart.de/projekte/mate/mdag/cr/cr_1.html  
M. Poesio, 2004. "The MATE/GNOME Scheme 
for Anaphoric Annotation, Revisited", Proc. of 
SIGDIAL. 
M. Poesio and R. Artstein, 2005. The Reliability 
of Anaphoric Annotation, Reconsidered: Taking 
Ambiguity into Account. Proc. of ACL 
Workshop on Frontiers in Corpus Annotation. 
J. Pustejovsky, A. Meyers, M. Palmer, and M. 
Poesio, 2005. Merging PropBank, NomBank, 
TimeBank, Penn Discourse Treebank and 
Coreference. In ACL 2005 Workshop: Frontiers 
in Corpus Annotation II: Pie in the Sky.  
M. Reis, 1982. "Zum Subjektbegriff im 
Deutschen". In: Abraham, W. (Hrsg.): 
Satzglieder im Deutschen. Vorschl?ge zur 
syntaktischen, semantischen und pragmatischen 
Fundierung. T?bingen: Narr. 171-212.  
C. G. Rosen, 1984. The Interface between 
Semantic Roles and Initial Grammatical 
Relations. In D.. M. Perlmutter and C. G. Rosen, 
eds, Studies in Relational Grammar 2. Univ. of 
Chicago Press.  
J. R. Ross,  1967. Constraints on Variables in 
Syntax. Doctoral dissertation, MIT.  
I. A. Sag and J. D. Fodor, 1994. Extraction 
without traces. In R. Aranovich, W. Byrne, S. 
Preuss, and M. Senturia, eds, Proc. of the 
Thirteenth West Coast Conference on Formal 
Linguistics, volume 13,  CSLI Publications/SLA.  
S. Salmon-Alt and L. Romary, RAF: towards a 
Reference Annotation Framework, LREC 2004  
S. Shaumyan, 1977. Applicative Grammar as a 
Semantic Theory of Natural Language. Chicago 
Univ. Press.  
H. Telljohann, E. Hinrichs, S. K?bler and H. 
Zinsmeister. 2005. Stylebook of the T?binger 
Treebank of Written German (T?Ba-D/Z). 
Technical report. University of T?bingen.  
C. Thielen and A. Schiller, 1996. Ein kleines und 
erweitertes Tagset f?rs Deutsche. In: Feldweg, 
H.; Hinrichs, E.W. (eds.): Wiederverwendbare 
Methoden und Ressourcen zur linguistischen 
Erschliessung des Deutschen. Vol. 73 of 
Lexicographica. T?bingen: Niemeyer. 193-203.  
J.-L.Tsai, 2005. A Study of Applying BTM 
Model on the Chinese Chunk Bracketing. In 
LINC-2005, IJCNLP-2005, pp.21-30.  
H. Uszkoreit, 1986. "Constraints on Order" in 
Linguistics 24.  
F. Xia, M. Palmer, N. Xue, N., M. E. Okurowski, 
J. Kovarik, F.-D. Chiou, S. Huang, T. Kroch, and 
Marcus, M., 2000. Developing Guidelines and 
Ensuring Consistency for Chinese Text 
Annotation. In: Proc. of LREC-2000. Greece.  
N. Xue, F. Chiou and M. Palmer. Building a 
Large-Scale Annotated Chinese Corpus, 2002. 
In: Proc. of COLING-2002. Taipei, Taiwan.  
N. Xue, F. Xia,  F.-D. Chiou and M. Palmer, 
2005. The Penn Chinese TreeBank: Phrase 
Structure Annotation of a Large Corpus. 
Natural Language Engineering, 11(2)-207.  
 
53
Proceedings of the Linguistic Annotation Workshop, pages 184?190,
Prague, June 2007. c?2007 Association for Computational Linguistics
The Shared Corpora Working Group Report
Adam Meyers
New York
University
New York, NY
meyers
at cs.nyu.edu
Nancy Ide
Vassar College
Poughkeepsie, NY
ide at cs.vassar.edu
Ludovic Denoyer
University of Paris
Paris, France
ludovic.denoyer
at lip6.fr
Yusuke Shinyama
New York
University
New York, NY
yusuke
at cs.nyu.edu
Abstract
We seek to identify a limited amount of rep-
resentative corpora, suitable for annotation
by the computational linguistics annotation
community. Our hope is that a wide vari-
ety of annotation will be undertaken on the
same corpora, which would facilitate: (1)
the comparison of annotation schemes; (2)
the merging of information represented by
various annotation schemes; (3) the emer-
gence of NLP systems that use informa-
tion in multiple annotation schemes; and (4)
the adoption of various types of best prac-
tice in corpus annotation. Such best prac-
tices would include: (a) clearer demarca-
tion of phenomena being annotated; (b) the
use of particular test corpora to determine
whether a particular annotation task can fea-
sibly achieve good agreement scores; (c)
The use of underlying models for represent-
ing annotation content that facilitate merg-
ing, comparison, and analysis; and (d) To
the extent possible, the use of common an-
notation categories or a mapping among cat-
egories for the same phenomenon used by
different annotation groups.
This study will focus on the problem of
identifying such corpora as well as the suit-
ability of two candidate corpora: the Open
portion of the American National Corpus
(Ide and Macleod, 2001; Ide and Suder-
man, 2004) and the ?Controversial? portions
of the WikipediaXML corpus (Denoyer and
Gallinari, 2006).
1 Introduction
This working group seeks to identify a limited
amount of representative corpora, suitable for an-
notation by the computational linguistics annotation
community. Our hope is that a wide variety of anno-
tation will be undertaken on the same corpora, which
would facilitate:
1. The comparison of annotation schemes
2. The merging of information represented by var-
ious annotation schemes
3. The emergence of NLP systems that use infor-
mation in multiple annotation schemes; and
4. The adoption of various types of best practice
in corpus annotation, including:
(a) Clearer demarcation of the phenomena be-
ing annotated. Thus if predicate argu-
ment structure annotation adequately han-
dles relative pronouns, a new project that
is annotating coreference is less likely to
include relative pronouns in their annota-
tion; and
(b) The use of particular test corpora to de-
termine whether a particular annotation
task can feasibly achieve good agreement
scores.
(c) The use of underlying models for repre-
senting annotation content that facilitate
merging, comparison, and analysis.
184
(d) To the extent possible, the use of common
annotation categories or a mapping among
categories for the same phenomenon used
by different annotation groups.
In selecting shared corpora, we believe that the
following issues must be taken into consideration:
1. The diversity of genres, lexical items and lin-
guistic phenomena ? this will ensure that the
corpora will be useful to many different types
of annotation efforts. Furthermore, systems us-
ing these corpora and annotation as data will
be capable of handling larger and more varied
corpora.
2. The availability of the same or similar corpora
in a wide variety of languages;
3. The availability of corpora in a standard format
that can be easily processed ? there should be
mechanisms in place to maintain the availabil-
ity of corpora in this format in the future;
4. The ease in which the corpora can be obtained
by anyone who wants to process or annotate
them ? corpora with free licenses or that are in
the public domain are preferred
5. The degree with which the corpora is represen-
tative of text to be processed ? this criterion can
be met if the corpora is diverse (1 above) and/or
if more corpora of the same kind is available for
processing.
We have selected the following corpora for con-
sideration:1
1. The OANC: the Open sections of the ANC cor-
pus. These are the sections of the American
National Corpus subject to the opened license,
allowing them to be freely distributed. The full
Open ANC (Version 2.0) contains about 14.5
megawords of American English and covers a
variety of genres as indicated by the full path-
names taken from the ANC distribution (where
a final 1 or 2 indicates which DVD the directory
originates from):
1These corpora can be downloaded from:
http://nlp.cs.nyu.edu/wiki/corpuswg/SharedCorpora
? spoken/telephone/switchboard
? written 1/fiction/eggan
? written 1/journal/slate
? written 1/letters/icic
? written 2/non-fiction/OUP
? written 2/technical/biomed
? written 2/travel guides/berlitz1
? written 2/travel guides/berlitz2
? written 1/journal/verbatim
? spoken/face-to-face/charlotte
? written 2/technical/911report
? written 2/technical/plos
? written 2/technical/government
2. The Controversial-Wikipedia-Corpus, a section
of the Wikipedia XML corpus. WikipediaXML
is a corpus derived from Wikipedia, convert-
ing Wikipedia into an XML corpus suitable
for NLP processing. This corpus was selected
from:
? Those articles cited as controversial
according to the November 28, 2006
version of the following Wikipedia page:
http://en.wikipedia.org/wiki/Wikipedia:
List of controversial issues
? The talk pages corresponding to these ar-
ticles where Wikipedia users and the com-
munity debate aspects of articles. These
debates may be about content or editorial
considerations.
? Articles in Japanese that are linked to
the English pages (and the associated talk
pages) are also part of our corpus.
2 American National Corpus
The American National Corpus (ANC) project (Ide
and Macleod, 2001; Ide and Suderman, 2004) has
released over 20 million words of spoken and writ-
ten American English, available from the Linguis-
tic Data Consortium. The ANC 2nd release con-
sists of fiction, non-fiction, newspapers, technical
reports, magazine and journal articles, a substan-
tial amount of spoken data, data from blogs and
other unedited web sources, travel guides, techni-
cal manuals, and other genres. All texts are an-
notated for sentence boundaries; token boundaries,
185
lemma, and part of speech produced by two differ-
ent taggers ; and noun and verb chunks. A sub-
corpus of 10 million words reflecting the genre dis-
tribution of the full ANC is currently being hand-
validated for word and sentence boundaries, POS,
and noun and verb chunks. For a complete descrip-
tion of the ANC 2nd release and its contents, see
http://AmericanNationalCorpus.org.
Approximately 65 percent of the ANC data is dis-
tributed under an open license, which allows use and
re-distribution of the data without restriction. The
remainder of the corpus is distributed under a re-
stricted license that disallows re-distribution or use
of the data for commercial purposes for five years
after its release date, unless the user is a member of
the ANC Consortium. After five years, the data in
the restricted portions of the corpus are covered by
the open license.
ANC annotations are distributed as stand-off doc-
uments representing a set of graphs over the primary
data, thus allowing for layering of annotations and
inclusion of multiple annotations of the same type.
Because most existing tools for corpus access and
manipulation do not handle stand-off annotations,
we have developed an easy-to-use tool and user in-
terface to merge the user?s choice of stand-off anno-
tations with the primary data to form a single docu-
ment in any of several XML and non-XML formats,
which is distributed with the corpus. The ANC ar-
chitecture and format is described fully in (Ide and
Suderman, 2006).
2.1 The ULA Subcorpus
The Unified Linguistic Annotation (ULA) project
has selected a 40,000 word subcorpus of the Open
ANC for annotation with several different annota-
tion schemes including: the Penn Treebank, Prop-
Bank, NomBank, the Penn Discourse Treebank,
TimeML and Opinion Annotation.2 This initial sub-
corpus can be broken down as follows:
? Spoken Language
? charlotte: 5K words
? switchboard: 5K words
? letters: 10K words
2Other corpora being annotated by the ULA project include
sections of the Brown corpus and LDC parallel corpora.
? Slate (Journal): 5K words
? Travel guides: 5K words
? 911report: 5K words
? OUP books (Kaufman): 5K words
As the ULA project progresses, the participants
intend to expand the corpora annotated to include a
larger subsection of the OANC. They believe that the
diversity of this corpus make it a reasonable testbed
for tuning annotation schemes for diverse modali-
ties. The Travel guides and some of the slate arti-
cles have already been annotated by the FrameNet
project. Thus the inclusion of these documents fur-
thered the goal of producing a multiply annotated
corpus by one additional project.
It is the recommendation of this working group
that: (1) other groups annotate these same subcor-
pora; and (2) other groups choose additional corpora
from the OANC to annotate and publicly announce
which subsections they choose. We would be happy
to put all such subsections on our website for down-
load. The basic idea is to build up a consensus of
what should be mutually annotated, in part, based
on what groups choose to annotate and to try to get
annotation projects to gravitate toward multiply an-
notated, freely available corpora.
3 The WikipediaXML Corpus
3.1 Why Wikipedia?
The Wikipedia corpus consists of articles in a wide
range of topics written in different genres and
mainly (a) main pages are encyclopedia style arti-
cles; and (b) talk pages are discussions about main
pages they are linked to. The topics of these discus-
sions range from editing contents to disagreements
about content. Although Wikipedia texts are mostly
limited to these two genres, we believe that it is well
suited as training data for natural language process-
ing because:
1. they are lexically diverse (e.g., providing a lot
of lexical information for statistical systems);
2. the textual information is well structured
3. Wikipedia is a large and growing corpus
186
4. the articles are multilingual (cf. section 3.4)
5. and the corpus has various other properties that
many researchers feel would be interesting to
exploit.
To date research in Computational Linguistics us-
ing Wikipedia includes: Automatic derivation of
taxonomy information (Strube and Ponzetto, 2006;
Suchanek et al, 2007; Zesch and Gurevych, 2007;
Ponzetto, 2007); automatic recognition of pairs of
similar sentences in two languages (Adafre and de
Rijke, 2006); corpus mining (Ru?diger Gleim and
Alexander Mehler and Matthias Dehmer, 2007),
Named Entity Recognition (Toral and noz, 2007;
Bunescu and Pasc?a, 2007) and relation extraction
(Nguyen et al, 2007). In addition several shared
tasks have been set up using Wikipedia as the tar-
get corpus including question answering (cf. (D.
Ahn and V. Jijkoun and G. Mishne and K. Mu?ller
and M. de Rijke and S. Schlobach, 2004) and
http://ilps.science.uva.nl/WiQA/); and information
retrieval (Fuhr et al, 2006). Some other interest-
ing properties of Wikipedia that have yet to be ex-
plored to our knowledge include: (1) Most main ar-
ticles have talk pages which discuss them ? perhaps
this relation can be exploited by systems which try
to detect discussions about topics, e.g., searches for
discussions about current events topics; (2) There
are various meta tags, many of which are not in-
cluded in the WikipediaXML (see below), but nev-
ertheless are retrievable from the original HTML
files. Some of these may be useful for various ap-
plications. For example, the levels of disputabil-
ity of the content of the main articles is annotated
(cf. http://en.wikipedia.org/wiki/Wikipedia: Tem-
plate messages/Disputes ).
3.2 Why WikipediaXML?
WikipediaXML (Denoyer and Gallinari, 2006) is an
XML version of Wikipedia data, originally designed
for Information Retrieval tasks such as INEX (Fuhr
et al, 2006) and the XML Document Mining Chal-
lenge (Denoyer and P. Gallinari, 2006). Wikipedi-
aXML has become a standard machine readable
form for Wikipedia, suitable for most Computa-
tional Linguistics purposes. It makes it easy to
identify and read in the text portions of the doc-
ument, removing or altering html and wiki code
that is difficult to process in a standard way. The
WikipediaXML standard has (so far) been used to
process Wikipedia documents written in English,
German, French, Dutch, Spanish, Chinese, Arabic
and Japanese.
3.3 The Controversial Wikipedia Corpus
The English Wikipedia corpus is quite large (about
800K articles and growing). Frozen versions of
the corpus are periodically available for download.
We selected a 5 million word subcorpus which
we believed would be good for a wide variety
of annotation schemes. In particular, we chose
articles listed as being controversial (in the En-
glish speaking world) according to the November
28, 2006 version of the following Wikipedia
page: http://en.wikipedia.org/wiki/Wikipedia:
List of controversial issues. We believed that
controversial articles would be more likely than
randomly selected articles to: (1) include interesting
discourse phenomena and emotive language; and
(2) have interesting ?talk? pages (indeed, some of
Wikipedia pages have no associated talk pages).
3.4 The Multi-linguality of Wikipedia
One of the main good points of Wikipedia is the fact
that it is a very large multilingual resource. This
provides several advantages over single-language
corpora, perhaps the clearest such advantage being
the availability of same-genre/same-format text for
many languages. Although, Wikipedia in languages
other than English do not approach 800K articles in
size, there are currently at least 14 languages with
over 100K entries.
It should be clear however, that it is definitely not
a parallel corpus. Although pages are sometimes
translated in their entirety, this is the exception, not
the rule. Pages can be partially translated or summa-
rized into the target language. Individually written
pages can be linked after they are created if it is be-
lieved that they are about the same topic. Also, ini-
tially parallel pages can be edited in both languages,
causing them to diverge. We therefore decided to
do a small small pilot study to attempt to charac-
terize the degree of similarity between English arti-
cles in Wikipedia and articles written in other lan-
guages that have been linked. There are 476 En-
glish Wikipedia articles in the Controversial corpus
187
Classification Frequency
Totally Different 2
Same General Topic 3
Overlapping Topics 11
Same Topics 33
Parallel 1
and 384 associated ?talk? pages. There are approxi-
mately 10,000 articles of various languages that are
linked to the English articles. We asked some En-
glish/Japanese bilingual speakers to evaluate the de-
gree of similarity of as many of the the 305 Japanese
articles that were linked to English controversial ar-
ticles. As of this date, 50 articles were evaluated
with the results summarized as table 3.4.3 These
preliminary results suggest the following:
? Languate-linked Wikipedia would usually be
classified as ?comparable? corpora as 34 (68%)
of the articles were classified as covering the
same topics or being parallel.
? It may be possible to extract a parallel corpus
for a given pair of languages from Wikipedia.
If the above sample is representative, approxi-
mately 2% of the articles are parallel. (While
the existance of one parallel article does not
provide statistically significant evidence that
2% of Wikipedia is parallel, the article?s ex-
istance is still significant.) Furthermore, addi-
tional parallel sentences may be extracted from
some of the other comparable articles using
techniques along the lines of (Adafre and de Ri-
jke, 2006).
Obviously, a more detailed study would be neces-
sary to gain a more complete understanding of how
language-linked articles are related in Wikipedia.4
Such a study would include characterizations of all
linked articles for several languages. This study
could lead to some practical applications, e.g., (1)
the creation of parallel subcorpora for a number of
languages; (2) the selection of an English monolin-
gual subcorpus consisting of articles, each of which
3According to www.wikipedia.org there are currently over
350K Japanese articles.
4Long Wikipedia articles may be split into multiple articles.
This can result in N to 1, or even N to N, matches between
language-linked articles if a topic is split in one language, but
not in another.
is parallel to some article in some other language;
etc.; (3) A compilation of parallel sentences ex-
tracted from comparable articles. While parallel
subcorpora are of maximal utility, finding parallel
sentences could still be extremely useful. (Adafre
and de Rijke, 2006) reports one attempt to automat-
ically select parallel Dutch/English sentences from
language-linked Wikipedia articles with an accuracy
of approximately 45%. Even if higher accuracy can-
not be achieved, this still suggests that it is possible
to create a parallel corpus (of isolated sentences) us-
ing a combination of automatic and manual means.
A human translator would have to go through pro-
posed parallel sentences and eliminate about one
half of them, but would not have to do any man-
ual translation. Selection of corpora for annotation
purposes depends on a number of factors including:
the type of annotation (e.g., a corpus of isolated sen-
tences would not be appropriate for discourse anno-
tation); and possibly an application the annotation
is tuned for (e.g., Machine Translation, Information
Extraction, etc.)
It should be noted that the corpus was chosen for
the controversialness of its articles in the English-
speaking community. It should, however, not be ex-
pected that the same articles will be controversial
in other languages. More generally, the language-
linked Wikipedia articles may have different cultural
contexts depending on the language they are written
in. This is an additional feature that we could test
in a wider study. Furthermore, English pages are
somewhat special because they?re considered as the
common platform and expected to be neutral to any
country. But other lanauages somewhat reflects the
view of each country where the language is spoken.
Indeed, some EN articles are labeled as USA-centric
(cf. http://en.wikipedia.org/wiki/Category:USA-
centric).
Finally, our choice of a corpus based on contro-
versy may have not been the most efficient choice
if our goal had been specifically to find parallel cor-
pora. Just as choosing corpora of articles that are
controversial (in the English-speaking world) may
have helped finding articles interesting to annotate
it is possible that some other choice, e.g., techni-
cal articles, may have helped select articles likely
188
to be translated in full5 Thus further study may be
required to choose the right Wikipedia balance for a
set of priorities agreed upon by the annotation com-
munity.
4 Legal Issues
The American National Corpus has taken great pains
to establish that the open subset of the corpus is
freely usable by the community. The open license6
makes it clear that these corpora can be used for any
reason and are freely distributable.
In contrast, some aspects of the licensing agree-
ment of corpora derived from Wikipedia are unclear.
Wikipedia is governed by the GNU Free Document
License which includes a provision that ?derived
works? are subject to this license as well. While
most academic researchers would be uneffected by
this provision, the effect of this provision is unclear
with respect to commercial products.
Under one view, a machine translation system that
uses a statistical model trained on Wikipedia corpora
is not derived from these corpora. However, on an-
other view it is derived. We contacted Wikipedia
staff by letter asking for clarification on this issue
and received the following response from Michelle
Kinney on behalf of Wikipedia information team:
Wikipedia does not offer legal advice,
and therefore cannot help you decide how
the GNU Free Documentation License
(GFDL) or any other free license applies
to your particular situation. Please con-
tact a local bar association, law society or
similar association of jurists in your legal
jurisdiction to obtain a referral to a com-
petent legal professional.
You may also wish to review the full text
of the GFDL yourself:
http://en.wikipedia.org/wiki/Wikipedia:
Text of the GNU Free Documentation License
5Informally, we observe that linked Japanese/English pairs
of articles about abstract topics (e.g., Adultery, Agnosticsism,
Antisemitism, Capitalism, Censorship, Catholicism) are less
likely to contain parallel sentences than articles about specific
events or people (e.g., Adolf Hitler, Barbara Streisand, The Los
Angeles Riots, etc.)
6http://projects.ldc.upenn.edu/ANC/ANC SecondRelease
EndUserLicense Open.htm
While some candidate corpora are completely in
the public domain, e.g., political speeches and very
old documents, many candidate corpora are under
the GFDL or similar ?copyleft? licenses. These in-
clude other licenses by the GNU organization and
several Creative Commons licenses. It is simply un-
clear how copyleft licenses should be applied to cor-
pora used as data in computational linguistics and
we believe that this is an important legal question
for the Computational Linguistics community. In
addition to Wikipedia, this issue effects a wide vari-
ety of corpora (e.g., other wiki corpora, some of the
corpora being developed by the American National
Corpus, etc.).
However, getting such legal opinions is expensive
and has to be done carefully. Hypothetically, sup-
pose NYU?s legal department wrote an opinion let-
ter stating that products that were not corpora them-
selves were not to be considered derived works for
purposes of some list of copyleft licensing agree-
ments. Furthermore, let?s suppose that several anno-
tation projects relied on this opinion and produced
millions of dollars worth of annotation for one such
corpus. Large corporations still might not use these
corpora unless their own legal departments agreed
with NYU?s opinion. For the annotation community,
this could mean that certain annotation would only
be used by academics and not by industry, and most
annotation researchers would not be happy with this
outcome. It therefore may be worth some effort
on the part of whole NLP community to seek some
clear determinations on this issue.
5 Concluding Remarks
The working group selected two freely distributable
corpora for purposes of annotation. Our goal was to
choose texts for annotation by multiple annotation
research groups and describe the process and the pit-
falls involved in selecting those texts. We, further-
more, aimed to establish a protocol for sharing texts,
so that the same texts are annotated with multiple
annotation schemes. This protocol cannot be setup
carte blanche by this group of researchers. Rather,
we believe that our report in combination with the
discussion at the upcoming meeting of the Lingus-
tic Annotation Workshop will provide the jumpstart
necessary for such a protocol to be put in place.
189
References
Sisay Fissaha Adafre and Maarten de Rijke. 2006. Find-ing Similar Sentences across Multiple Languages in
Wikipedia. In EACL 2006 Workshop: Wikis and blogsand other dynamic text source, Trento, Italy.
Razvan Bunescu and Marius Pasc?a. 2007. Using En-
cyclopedic Knowledge for Named Entity Disambigua-tion. In Proc. of NAACL/HLT 2007.
D. Ahn and V. Jijkoun and G. Mishne and K. Mu?ller and
M. de Rijke and S. Schlobach. 2004. Using Wikipediaat the TREC QA Track. In Proc. TREC 2004.
Ludovic Denoyer and Patrick Gallinari. 2006. The
Wikipedia XML Corpus. SIGIR Forum.
L. Denoyer and A. Vercoustre P. Gallinari. 2006. Reporton the XML Mining Track at INEX 2005 and INEX
2006 : Categorization and Clustering of XML Docu-ments. In Advances in XML Information Retrieval andEvaluation: Fifthth Workshop of the INitiative for theEvaluation of XML Retrieval (INEX?06).
N. Fuhr, M. Lalmas, and S. Malik. 2006. Advances inXML Information Retrieval and Evaluation. In 5th In-ternational Workshop of the Initiative for the Evalua-tion of XML Retrieval, INEX 2006.
N. Ide and C. Macleod. 2001. The american national
corpus: A standardized resource of american english.In Proceedings of Corpus Linguistics 2001, Lancaster,UK.
N. Ide and K. Suderman. 2004. The american nationalcorpus first release. In Proceedings of LREC 2004,pages 1681?1684, Lisbon, Portugal.
N. Ide and K. Suderman. 2006. Integrating linguistic re-sources: The american national corpus model. In Pro-ceedings of the 6th International Conference on Lan-guage Resources and Evaluation, Genoa, Italy.
D. P.T. Nguyen, Y. Matsuo, and M. Ishizuka. 2007. Sub-tree Mining for Relation Extraction from Wikipedia.
In Proc. of NAACL/HLT 2007.
Simone Paolo Ponzetto. 2007. Creating a KnowledgeBase From a Collaboratively Generated Encyclopedia.
In Proc. of NAACL/HLT 2007.
Ru?diger Gleim and Alexander Mehler and MatthiasDehmer. 2007. Web Corpus Mining by instance of
Wikipedia. In Proc. 2nd Web as Corpus Workshop atEACL 2006.
M. Strube and S. P. Ponzetto. 2006. WikiRelate! Com-
puting semantic relatedness using Wikipedia. In Proc.of AAAI-06, pages 1419?1424.
F. M. Suchanek, G. Kasneci, and G.Weikum. 2007.
YAGO: A core of semantic knowledge. In Proc. ofWWW-07.
Antonio Toral and Rafael Mu noz. 2007. A proposal toautomatically build and maintain gazetteers for Named
Entity Recognition by using Wikipedia. In Proc. ofNAACL/HLT 2007.
Torsten Zesch and Iryna Gurevych. 2007. Analysis ofthe Wikipedia Category Graph for NLP Applications.
In Proc of NAACL-HLT 2007 Workshop: TextGraphs-2.
190
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 1?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The CoNLL-2009 Shared Task:
Syntactic and Semantic Dependencies in Multiple Languages
Jan Hajic?? Massimiliano Ciaramita? Richard Johansson? Daisuke Kawahara?
Maria Anto`nia Mart???? Llu??s Ma`rquez?? Adam Meyers?? Joakim Nivre?? Sebastian Pado???
Jan ?Ste?pa?nek? Pavel Stran?a?k? Mihai Surdeanu?? Nianwen Xue?? Yi Zhang??
?: Charles University in Prague, {hajic,stepanek,stranak}@ufal.mff.cuni.cz
?: Google Inc., massi@google.com
?: University of Trento, johansson@disi.unitn.it
?: National Institute of Information and Communications Technology, dk@nict.go.jp
??: University of Barcelona, amarti@ub.edu
??: Technical University of Catalonia, Barcelona, lluism@lsi.upc.edu
??: New York University, meyers@cs.nyu.edu
??: Uppsala University and Va?xjo? University, joakim.nivre@lingfil.uu.se
??: Stuttgart University, pado@ims.uni-stuttgart.de
??: Stanford University, mihais@stanford.edu
??: Brandeis University, xuen@brandeis.edu
??: Saarland University, yzhang@coli.uni-sb.de
Abstract
For the 11th straight year, the Conference
on Computational Natural Language Learn-
ing has been accompanied by a shared task
whose purpose is to promote natural language
processing applications and evaluate them in
a standard setting. In 2009, the shared task
was dedicated to the joint parsing of syntac-
tic and semantic dependencies in multiple lan-
guages. This shared task combines the shared
tasks of the previous five years under a unique
dependency-based formalism similar to the
2008 task. In this paper, we define the shared
task, describe how the data sets were created
and show their quantitative properties, report
the results and summarize the approaches of
the participating systems.
1 Introduction
Every year since 1999, the Conference on Com-
putational Natural Language Learning (CoNLL)
launches a competitive, open ?Shared Task?. A
common (?shared?) task is defined and datasets are
provided for its participants. In 2004 and 2005, the
shared tasks were dedicated to semantic role label-
ing (SRL) in a monolingual setting (English). In
2006 and 2007 the shared tasks were devoted to
the parsing of syntactic dependencies, using corpora
from up to 13 languages. In 2008, the shared task
(Surdeanu et al, 2008) used a unified dependency-
based formalism, which modeled both syntactic de-
pendencies and semantic roles for English. The
CoNLL-2009 Shared Task has built on the 2008 re-
sults by providing data for six more languages (Cata-
lan, Chinese, Czech, German, Japanese and Span-
ish) in addition to the original English1. It has thus
naturally extended the path taken by the five most
recent CoNLL shared tasks.
As in 2008, the CoNLL-2009 shared task com-
bined dependency parsing and the task of identify-
ing and labeling semantic arguments of verbs (and
other parts of speech whenever available). Partici-
pants had to choose from two tasks:
? Joint task (syntactic dependency parsing and
semantic role labeling), or
? SRL-only task (syntactic dependency parses
have been provided by the organizers, using
state-of-the art parsers for the individual lan-
guages).
1There are some format changes and deviations from the
2008 task data specification; see Sect. 2.3
1
In contrast to the previous year, the evaluation data
indicated which words were to be dealt with (for the
SRL task). In other words, (predicate) disambigua-
tion was still part of the task, whereas the identi-
fication of argument-bearing words was not. This
decision was made to compensate for the significant
differences between languages and between the an-
notation schemes used.
The ?closed? and ?open? challenges have been
kept from last year as well; participants could have
chosen one or both. In the closed challenge, systems
had to be trained strictly with information contained
in the given training corpus; in the open challenge,
systems could have been developed making use of
any kind of external tools and resources.
This paper is organized as follows. Section 2 de-
fines the task, including the format of the data, the
evaluation metrics, and the two challenges. A sub-
stantial portion of the paper (Section 3) is devoted
to the description of the conversion and develop-
ment of the data sets in the additional languages.
Section 4 shows the main results of the submitted
systems in the Joint and SRL-only tasks. Section 5
summarizes the approaches implemented by partic-
ipants. Section 6 concludes the paper. In all sec-
tions, we will mention some of the differences be-
tween last year?s and this year?s tasks while keeping
the text self-contained whenever possible; for details
and observations on the English data, please refer to
the overview paper of the CoNLL-2008 Shared Task
(Surdeanu et al, 2008) and to the references men-
tioned in the sections describing the other languages.
2 Task Definition
In this section we provide the definition of the shared
task; after introducing the two challenges and the
two tasks the participants were to choose, we con-
tinue with the format of the shared task data, fol-
lowed by a description of the evaluation metrics
used.
For three of the languages (Czech, English and
German), out-of-domain data (OOD) have also been
prepared for the final evaluation, following the same
guidelines and formats.
2.1 Closed and Open Challenges
Similarly to the CoNLL-2005 and CoNLL-2008
shared tasks, this shared task evaluation is separated
into two challenges:
Closed Challenge The aim of this challenge was to
compare performance of the participating systems in
a fair environment. Systems had to be built strictly
with information contained in the given training cor-
pus, and tuned with the development section. In
addition, the lexical frame files (such as the Prop-
Bank and NomBank for English, the valency dictio-
nary PDT-Vallex for Czech etc.) were provided and
may have been used. These restrictions mean that
outside parsers (not trained by the participants? sys-
tems) could not be used. However, we did provide
the output of a single, state-of-the-art dependency
parser for each language so that participants could
build a SRL-only system (using the provided parses
as inputs) within the closed challenge (as opposed to
the 2008 shared task).
Open Challenge Systems could have been devel-
oped making use of any kind of external tools and
resources. The only condition was that such tools or
resources must not have been developed with the an-
notations of the test set, both for the input and output
annotations of the data. In this challenge, we were
interested in learning methods which make use of
any tools or resources that might improve the per-
formance. The comparison of different systems in
this setting may not be fair, and thus ranking of sys-
tems is not necessarily important.
2.2 Joint and SRL-only tasks
In 2008, systems participating in the open challenge
could have used state-of-the-art parsers for the syn-
tactic dependency part of the task. This year, we
have provided the output of these parsers for all the
languages in an uniform way, thus allowing an or-
thogonal combination of the two tasks and the two
challenges. For the SRL-only task, participants in
the closed challenge simply had to use the provided
parses only.
Despite the provisions for the SRL-only task, we
are more interested in the approaches and results of
the Joint task. Therefore, primary system ranking is
provided for the Joint task while additional measures
2
are computed for various combinations of parsers
and SRL methods across the tasks and challenges.
2.3 Data Format
The data format used in this shared task has been
based on the CoNLL-2008 shared task, with some
differences. The data follows these general rules:
? The files contain sentences separated by a blank
line.
? A sentence consists of one or more tokens and
the information for each token is represented on
a separate line.
? A token consists of at least 14 fields. The fields
are separated by one or more whitespace char-
acters (spaces or tabs). Whitespace characters
are not allowed within fields.
The data is thus a large table with whitespace-
separated fields (columns). The fields provided in
the data are described in Table 1. They are identical
for all languages, but they may differ in contents;
for example, some fields might not be filled for all
the languages provided (such as the FEAT or PFEAT
fields).
For the SRL-only task, participants have been
provided will all the data but the PRED and
APREDs, which they were supposed to fill in with
their correct values. However, they did not have
to determine which tokens are predicates (or more
precisely, which are the argument-bearing tokens),
since they were marked by ?Y? in the FILLPRED
field.
For the Joint task, participants could not (in ad-
dition to the PRED and APREDs) see the gold-
standard nor the predicted syntactic dependencies
(HEAD, PHEAD) and their labels (DEPREL, PDE-
PREL). These syntactic dependencies were also to
be filled by participants? systems.
In both tasks, participants have been free to
use any other data (columns) provided, except the
LEMMA, POS and FEAT columns (to get more ?re-
alistic? results using only their automatically pre-
dicted variants PLEMMA, PPOS and PFEAT).
Besides the corpus proper, predicate dictionaries
have been provided to participants in order to be able
to properly match the predicates to the tokens in the
corpus; their contents could have been used e.g. as
features for the PRED/APREDs predictions (or even
for the syntactic dependencies, i.e., for filling in the
PHEAD and PDEPREL fields).
The system of filling-in the APREDs follows
the 2008 pattern; for each argument-bearing token
(predicate), a new APREDn column is created in the
order in which the predicate token is encountered
within the sentence (i.e., based on its ID seen as a
numerical value). Then, for each token in the sen-
tence, the value in the intersection of the APREDn
column and the token row is either left unfilled
(if the token is not an argument), or a predicate-
argument label(s) is(are) filled in.
The differences between the English-only 2008
task and this year?s multilingual task can be briefly
summarized as follows:
? only ?split?2 lemmas and forms have been pro-
vided in the English datasets (for the other lan-
guages, original tokenization from the respec-
tive treebanks has been used);
? rich morphological features have been added
wherever available;
? syntactic dependencies by state-of-the-art
parsers have been provided (for the SRL-only
task);
? multiple semantic labels for a single token have
been allowed (and properly evaluated) in the
APREDs columns;
? predicates have been pre-identified and marked
in both the training and test data;
? some of the fields (e.g. the APREDx) and val-
ues (ARG0? A0 etc.) have been renamed.
2.4 Evaluation Measures
It was required that participants submit results in all
seven languages in the chosen task and in any of (or
both) the challenges. Submission of out-of-domain
data files has been optional.
The main evaluation measure, according to which
systems are primarily compared, is the Joint task,
2Splitting of forms and lemmas in English has been intro-
duced in the 2008 shared task to match the tokenization con-
vention for the arguments in NomBank.
3
Field # Name Description
1 ID Token counter, starting at 1 for each new sentence
2 FORM Form or punctuation symbol (the token; ?split? for English)
3 LEMMA Gold-standard lemma of FORM
4 PLEMMA Automatically predicted lemma of FORM
5 POS Gold-standard POS (major POS only)
6 PPOS Automatically predicted major POS by a language-specific tagger
7 FEAT Gold-standard morphological features (if applicable)
8 PFEAT Automatically predicted morphological features (if applicable)
9 HEAD Gold-standard syntactic head of the current token (ID or 0 if root)
10 PHEAD Automatically predicted syntactic head
11 DEPREL Gold-standard syntactic dependency relation (to HEAD)
12 PDEPREL Automatically predicted dependency relation to PHEAD
13 FILLPRED Contains ?Y? for argument-bearing tokens
14 PRED (sense) identifier of a semantic ?predicate? coming from a current token
15... APREDn Columns with argument labels for each semantic predicate (in the ID order)
Table 1: Description of the fields (columns) in the data provided. The values of columns 9, 11 and 14 and above are
not provided in the evaluation data; for the Joint task, columns 9?12 are also empty in the evaluation data.
closed challenge, Macro F1 score. However, scores
can also be computed for a number of other condi-
tions:
? Task: Joint or SRL-only
? Challenge: open or closed
? Domain: in-domain data (IDD, separated from
training corpus) or out-of-domain data (OOD)
Joint task participants are also evaluated separately
on the syntactic dependency task (labeled attach-
ment score, LAS). Finally, systems competing in
both tasks are compared on semantic role labeling
alone, to assess the impact of the the joint pars-
ing/SRL task compared to an SRL-only task on pre-
parsed data.
Finally, as an explanatory measure, precision and
recall of the semantic labeling task have been com-
puted and tabulated.
We have decided to omit several evaluation fig-
ures that were reported in previous years, such as the
percentage of completely correct sentences (?Exact
Match?), unlabeled scores, etc. With seven lan-
guages, two tasks (plus two challenges, and the
IDD/OOD distinction), there are enough results to
get lost even as it is.
2.4.1 Syntactic Dependency Measures
The LAS score is defined similarly as in the pre-
vious shared tasks, as the percentage of tokens for
which a system has predicted the correct HEAD and
DEPREL columns. The unlabeled attachment score
(UAS), i.e., the percentage of tokens with correct
HEAD regardless if the DEPREL is correct, has not
been officially computed this year. No precision and
recall measures are applicable, since all systems are
supposed to output a single dependency with a single
label (see also below the footnote to the description
of the combined score).
2.4.2 Semantic Labeling Measures
The semantic propositions are evaluated by con-
verting them to semantic dependencies, i.e., we cre-
ate n semantic dependencies from every predicate
to its n arguments. These dependencies are labeled
with the labels of the corresponding arguments. Ad-
ditionally, we create a semantic dependency from
each predicate to a virtual ROOT node. The latter
dependencies are labeled with the predicate senses.
This approach guarantees that the semantic depen-
dency structure conceptually forms a single-rooted,
connected (but not necessarily acyclic) graph. More
importantly, this scoring strategy implies that if a
system assigns the incorrect predicate sense, it still
receives some points for the arguments correctly as-
signed. For example, for the correct proposition:
verb.01: A0, A1, AM-TMP
the system that generates the following output for
the same argument tokens:
4
verb.02: A0, A1, AM-LOC
receives a labeled precision score of 2/4 because two
out of four semantic dependencies are incorrect: the
dependency to ROOT is labeled 02 instead of 01
and the dependency to the AM-TMP is incorrectly la-
beled AM-LOC. Using this strategy we compute pre-
cision, recall, and F1 scores for semantic dependen-
cies (labeled only).
For some languages (Czech, Japanese) there may
be more than one label in a given argument position;
for example, this happens in Czech in special cases
of reciprocity when the same token serves as two or
more arguments to the same predicate. The scorer
takes this into account and considers such cases to
be (as if) multiple predicate-argument relations for
the computation of the evaluation measures.
For example, for the correct proposition:
v1f1: ACT|EFF, ADDR
the system that generates the following output for
the same argument tokens:
v1f1: ACT, ADDR|PAT
receives a labeled precision score of 3/4 because
the PAT is incorrect and labeled recall 3/4 be-
cause the EFF is missing (should the ACT|EFF and
ADDR|PAT be taken as atomic values, the scores
would then be zero).
2.4.3 Combined Syntactic and Semantic Score
We combine the syntactic and semantic measures
into one global measure using macro averaging. We
compute macro precision and recall scores by aver-
aging the labeled precision and recall for semantic
dependencies with the LAS for syntactic dependen-
cies:3
LMP = Wsem ? LPsem + (1?Wsem) ? LAS (1)
LMR = Wsem ? LRsem + (1 ?Wsem) ? LAS (2)
where LMP is the labeled macro precision and
LPsem is the labeled precision for semantic depen-
dencies. Similarly, LMR is the labeled macro re-
call and LRsem is the labeled recall for semantic
dependencies. Wsem is the weight assigned to the
3We can do this because the LAS for syntactic dependen-
cies is a special case of precision and recall, where the predicted
number of dependencies is equal to the number of gold depen-
dencies.
semantic task.4 The macro labeled F1 score, which
was used for the ranking of the participating sys-
tems, is computed as the harmonic mean of LMP
and LMR.
3 Data
The unification of the data formats for the various
languages appeared to be a challenge in itself. We
will briefly describe the processes of the conversion
of the existing treebanks in the seven languages of
the CoNLL-2009 shared task. In many instances,
the original treebanks had to be not only converted
format-wise, but also merged with other resources in
order to generate useful training and testing data that
fit the task description.
3.1 The Input Corpora
The data used as the input for the transformations
aimed at arriving at the data contents and format de-
scribed in Sect. 2.3 are described in (Taule? et al,
2008), (Xue and Palmer, 2009), (Hajic? et al, 2006),
(Surdeanu et al, 2008), (Burchardt et al, 2006) and
(Kawahara et al, 2002).
In the subsequent sections, the procedures for the
data conversion for the individual languages are de-
scribed. The data has been collected by the main
organization site and checked for format errors, and
repackaged for distribution.
There were three packages of the data distributed
to the participants: Trial, Training plus Develop-
ment, and Evaluation. The Trial data were rather
small, just to give the feeling of the format and
languages involved. A visual representation of the
Trial data was also created to make understanding
of the data easier. Any data in the same format
can be transformed and displayed in the Tree Editor
TrEd5 (Pajas and ?Ste?pa?nek, 2008) with the CoNLL
2009 Shared Task extension that can be installed
from within the editor. A sample visualization of an
English sentence after its conversion to the shared
task format (Sect. 2.3) is in Fig. 1.
Due to licensing requirements, every package of
the data had to be split into two portions. One
portion (Catalan, German, Japanese, and Spanish
data) was published on the task?s webpage for down-
4We assign equal weight to the two tasks, i.e., Wsem = 0.5.
5http://ufal.mff.cuni.cz/?pajas/tred
5
$QG
'(3 &&
VRPHWLPHV
703 5%
D
102' '7
UHSXWDEOH
102' --
FKDULW\
6%- 11
ZLWK
102' ,1
D
102' '7
KRXVHKROG
102' 11
QDPH QDPH
302' 11
JHWV JHW
5227 9%=
XVHG XVH
9& 9%1
DQG
&225' &&
GRHV
&21- 9%=
Q
W
$'9 5%
HYHQ
$'9 5%
NQRZ NQRZ
9& 9%
LW
2%- 353

3  
$0703$0703$0703
 

$$$$

 
 

$
 

 

$



$01(*

$0$'9
 


$

Figure 1: Visualisation of the English sentence ?And sometimes a reputable charity with a houshold name gets used
and doesn?t even know it.? (Penn Treebank, wsj 0559) showing jointly the labeled syntactic and semantic depen-
dencies. The basic tree shape comes from the syntactic dependencies; syntactic labels and POS tags are on the 2nd
line at each node. Semantic dependencies which do not follow the syntactic ones use dotted lines. Predicate senses
in parentheses (use:01, ...) follow the word label. SRLs (A0, AM-TMP, ...) are on the last line. Please note that
multiple semantic dependencies (e.g., there are four for charity: A0? know, A1? gets, A1? used, A1? name)
and self-dependencies (name) appear in this sentence.
load, the other portion (Czech, English, and Chinese
data) was invoiced and distributed by the Linguistic
Data Consortium under a special agreement free of
charge.
Distribution of the Evaluation package was a bit
more complicated, because there were two types of
the packages - one for the Joint task and one for the
SRL-only task. Every participant had to subscribe
to one of the two tasks; subsequently, they obtained
the appropriate data (again, from the webpage and
LDC).
Prior to release, each data file was checked to
eliminate errors. The following test were carried
out:
? For every sentence, number of PREDs rows
matches the number of APREDs columns.
? The first line of each file is never empty, while
the last line always is.
? The first character on a non-empty line is al-
ways a digit, the last one is never a whitespace.
? The number of empty lines (i.e. the number
of sentences) equals the number of lines begin-
ning with ?1?.
? The data contain no spaces nor double tabs.
Some statistics on the data can be seen in Ta-
bles 2, 3 and 4. Whereas the training sizes of the
data have not been that different as they were e.g.
for the 2007 shared task on multilingual dependency
parsing (Nivre et al, 2007)6, substantial differences
existed in the distribution of the predicates and ar-
guments, the input features, the out-of-vocabulary
rates, and other statistical characteristics of the data.
Data sizes have been relatively uniform in all the
datasets, with Japanese having the smallest dataset
6http://nextens.uvt.nl/depparse-wiki/
DataOverview
6
containing data for SRL annotation training. To
compensate at least for the dependency parsing part,
an additional, large Japanese corpus with syntactic
dependency annotation has been provided.
The average sentence length, the vocabulary sizes
for FORM and LEMMA fields and the OOV rates
characterize quite naturally the properties of the re-
spective languages (in the domain of the training and
evaluation data). It is no surprise that the FORM
OOV rate is the highest for Czech, a highly inflec-
tional language, and that the LEMMA OOV rate is
the highest for German (as a consequence of keeping
compounds as a single lemma). The other statistics
also reflect (to a large extent) the annotation speci-
fication and conventions used for the original tree-
banks and/or the result of the conversion process to
the unified CoNLL-2009 Shared Task format.
Starting with the POS and FEAT fields, it can be
seen that Catalan, Czech and Spanish use only the
12 major part-of-speech categories as values of the
POS field (with richly populated FEAT field); En-
glish and Chinese are the opposite extreme, disre-
garding the use of the FEAT field completely and
coding everything as a POS value. While for Chi-
nese this is quite understandable, English follows the
PTB tradition in this respect. German and Japanese
use relatively rich set of values in both the POS and
FEAT fields.
For the dependency relations (DEPREL), all
the languages use a similarly-sized set except for
Japanese, which only encodes the distinction be-
tween a root and a dependent node (and some in-
frequent special ones).
Evaluation data are over 10% of the size of the
training data for Catalan, Chinese, Czech, Japanese
and Spanish and roughly 5% for English and Ger-
man.
Table 3 shows the distribution of the five most fre-
quent dependency relations (determined as part of
the subtask of syntactic parsing). With the exception
of Japanese, which essentially does not label depen-
dency relations at this level, all the other languages
show little difference in this distribution. For exam-
ple, the unconditioned probability of ?subjects? is
almost the same for all the six other languages (be-
tween 6 and 8 percent). The probability mass cov-
ered by the first five most frequent DEPRELs is also
almost the same (again, except for Japanese), sug-
gesting that the labeling task might have similar dif-
ficulty7. The most skewed one is for Czech (after
Japanese).
Table 4 shows similar statistics for the argument
labels (PRED/APREDs); it also adds the average
number of arguments per ?predicate? token, since
this is part of the SRL task8. It is apparent from the
comparison of the ?Total? rows in this table and Ta-
ble 3 that the first five argument labels cover more
that their syntactic counterparts. For example, the
arguments A0-A4 account for all but 3% of all ar-
guments labels, whereas Spanish and Catalan have
much more rich set of argument labels, with a high
entropy of the most-frequent-label distribution.
3.2 Catalan and Spanish
The Catalan and Spanish datasets (Taule? et al, 2008)
were generated from the AnCora corpora9 through
an automatic conversion process from a constituent-
based formalism to dependencies (Civit et al, 2006).
AnCora corpora contain about half million words
for Catalan and Spanish annotated with syntactic
and semantic information. Text sources for the Cata-
lan corpus are EFE news agency (?75Kw), ACN
Catalan news agency (?225Kw), and ?El Perio?dico?
newspaper (?200Kw). The Spanish corpus comes
from the Lexesp Spanish balanced corpus (?75Kw),
the EFE Spanish news agency (?225Kw), and the
Spanish version of ?El Perio?dico? (?200Kw). The
subset from ?El Perio?dico? corresponds to the same
news in Catalan and Spanish, spanning from January
to December 2000.
Linguistic annotation is the same in both lan-
guages and includes: PoS tags with morphologi-
cal features (gender, number, person, etc.), lemma-
tization, syntactic dependencies (syntactic func-
tions), semantic dependencies (arguments and the-
matic roles), named entities and predicate semantic
classes (Lexical Semantic Structure, LSS). Tag sets
are shared by the two languages.
If we take into account the complete PoS tags,
7Yes, this is overgeneralization since this distribution does
not condition on the features, dependencies etc. But as a rough
measure, it often correlates well with the results.
8A number below 1 means there are some argument-bearing
words (often nouns) which have no arguments in the particular
sentence in which they appear.
9http://clic.ub.edu/ancora
7
Characteristic Catalan Chinese Czech English German Japanese Spanish
Training data size (sentences) 13200 22277 38727 39279 36020 4393a 14329
Training data size (tokens) 390302 609060 652544 958167 648677 112555a 427442
Avg. sentence length (tokens) 29.6 27.3 16.8 24.4 18.0 25.6 29.8
Tokens with argumentsb (%) 9.6 16.9 63.5 18.7 2.7 22.8 10.3
DEPREL types 50 41 49 69 46 5 49
POS types 12 41 12 48 56 40 12
FEAT types 237 1 1811 1 267 302 264
FORM vocabulary size 33890 40878 86332 39782 72084 36043 40964
LEMMA vocabulary size 24143 40878 37580 28376 51993 30402 26926
Evaluation data size (sent.) 1862 2556 4213 2399 2000 500 1725
Evaluation data size (tokens) 53355 73153 70348 57676 31622 13615 50630
Evaluation FORM OOVc 5.40 3.92 7.98/8.62d 1.58/3.76d 7.93/7.57d 6.07 5.63
Evaluation LEMMA OOVc 4.14 3.92 3.03/4.29d 1.08/2.30d 5.83/7.36d 5.21 3.69
Table 2: Elementary data statistics for the CoNLL-2009 Shared Task languages. The data themselves, the original
treebanks they were derived from and the conversion process are described in more detail in sections 3.2-3.7. All
evaluation data statistics are derived from the in-domain evaluation data.
aThere were additional 33257 sentences (839947 tokens) available for syntactic dependency parsing of Japanese; the type and
vocabulary statistics are computed using this larger dataset.
bPercentage of tokens with FILLPRED=?Y?.
cPercentage of FORM/LEMMA tokens not found in the respective vocabularies derived solely from the training data.
dOOV percentage for in-domain/out-of-domain data.
DEPREL Catalan Chinese Czech English German Japanese Spanish
sn 0.16 COMP 0.21 Atr 0.26 NMOD 0.27 NK 0.31 D 0.93 sn 0.16
spec 0.15 NMOD 0.14 AuxP 0.10 P 0.11 PUNC 0.14 ROOT 0.04 spec 0.15
Labels f 0.11 ADV 0.10 Adv 0.10 PMOD 0.10 MO 0.12 P 0.03 f 0.12
sp 0.09 UNK 0.09 Obj 0.07 SBJ 0.07 SB 0.07 A 0.00 sp 0.08
suj 0.07 SBJ 0.08 Sb 0.06 OBJ 0.06 ROOT 0.06 I 0.00 suj 0.08
Total 0.58 0.62 0.59 0.61 0.70 1.00 0.59
Table 3: Unigram probability for the five most frequent DEPREL labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five dependency labels shown.
APRED Catalan Chinese Czech English German Japanese Spanish
arg1-pat 0.22 A1 0.30 RSTR 0.30 A1 0.37 A0 0.40 GA 0.33 arg1-pat 0.20
arg0-agt 0.18 A0 0.27 PAT 0.18 A0 0.25 A1 0.39 WO 0.15 arg0-agt 0.19
Labels arg1-tem 0.15 ADV 0.20 ACT 0.17 A2 0.12 A2 0.12 NO 0.15 arg1-tem 0.15
argM-tmp 0.08 TMP 0.07 APP 0.06 AM-TMP 0.06 A3 0.06 NI 0.09 arg2-atr 0.08
arg2-atr 0.08 DIS 0.04 LOC 0.04 AM-MNR 0.03 A4 0.01 DE 0.06 argM-tmp 0.08
Total 0.71 0.91 0.75 0.83 0.97 0.78 0.70
Avg. 2.25 2.26 0.88 2.20 1.97 1.71 2.26
Table 4: Unigram probability for the five most frequent APRED labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five argument labels shown. The ?Avg.? line
shows the average number of arguments per predicate or other argument-bearing token (i.e. for those marked by
FILLPRED=?Y?).
8
AnCora has 280 different labels. Considering only
the main syntactic categories, the tag set is reduced
to 47 tags. The syntactic tag set consists of 50 dif-
ferent syntactic functions. Regarding semantic ar-
guments, we distinguish Arg0, Arg1, Arg2, Arg3,
Arg4, ArgM, and ArgL. The first five tags are num-
bered from less to more obliqueness with respect
to the verb, ArgM corresponds to adjuncts. The
list of thematic roles consists of 20 different labels:
AGT (Agent), AGI (Induced Agent), CAU (Cause),
EXP (Experiencer), SCR (Source), PAT (Patient),
TEM (Theme), ATR (Attribute), BEN (Beneficiary),
EXT (Extension), INS (Instrument), LOC (Loca-
tive), TMP (Time), MNR (Manner), ORI (Origin),
DES (Goal), FIN (Purpose), EIN (Initial State), EFI
(Final State), and ADV (Adverbial). Each argument
position can map onto specific thematic roles. By
way of example, Arg1 can be PAT, TEM or EXT. For
Named Entities, we distinguish six types: Organiza-
tion, Person, Location, Date, Number, and Others.
An incremental process guided the annotation of
AnCora, since semantics depends on morphosyntax,
and syntax relies on morphology. This procedure
made it possible to check, correct, and complete
the previous annotations, thus guaranteeing the final
quality of the corpora and minimizing the error rate.
The annotation process was carried out sequentially
from lower to upper layers of linguistic description.
All resulting layers are independent of each other,
thus making easier the data management. The ini-
tial annotation was performed manually for syntax,
semiautomatically in the case of arguments and the-
matic roles, and fully automatically for PoS (Mart??
et al, 2007; Ma`rquez et al, 2007).
The Catalan and Spanish AnCora corpora were
straightforwardly translated into the CoNLL-2009
shared task formatting (information about named
entities was skipped in this process). The resulting
Catalan corpus (including training, development and
test partitions) contains 16,786 sentences with an av-
erage length of 29.59 lexical tokens per sentence.
Long sentences abound in this corpus. For instance,
10.73% of the sentences are longer than 50 tokens,
and 4.42% are longer than 60. The corpus con-
tains 47,537 annotated predicates (2.83 predicates
per sentence, on average) with 107,171 arguments
(2.25 arguments per predicate, on average). From
the latter, 73.89% correspond to core arguments and
26.11% to adjuncts. Numbers for the Spanish cor-
pus are comparable in all aspects: 17,709 sentences
with 29.84 lexical tokens on average (11.58% of the
sentences longer than 50 tokens, 4.07% longer than
60); 54,075 predicates (3.05 per sentence, on aver-
age) and 122,478 arguments (2.26 per predicate, on
average); 73.34% core arguments and 26.66% ad-
juncts.
The following are important features of the Cata-
lan and Spanish corpora in the CoNLL-2009 shared
task setting: (1) all dependency trees are projective;
(2) no word can be the argument of more than one
predicate in a sentence; (3) semantic dependencies
completely match syntactic dependency structures
(i.e., no new edges are introduced by the semantic
structure); (4) only verbal predicates are annotated
(with exceptional cases referring to words that can
be adjectives and past participles); (5) the corpus is
segmented so multi-words, named entities, temporal
expressions, compounds, etc. are grouped together;
and (6) segmentation also accounts for elliptical pro-
nouns (there are marked as empty lexical tokens ?_?
with a pronoun POS tag).
Finally, the predicted columns (PLEMMA,
PPOS, and PFEAT) have been generated with the
FreeLing Open source suite of Language Analyz-
ers10. Accuracy in PLEMMA and PPOS columns
is above 95% for the two languages. PHEAD
and PDEPREL columns have been generated using
MaltParser11. Parsing accuracy (LAS) is above 86%
for the the two languages.
3.3 Chinese
The Chinese Corpus for the 2009 CoNLL Shared
Task was generated by merging the Chinese Tree-
bank (Xue et al, 2005) and the Chinese Proposition
Bank (Xue and Palmer, 2009) and then converting
the constituent structure to a dependency formalism
as specified in the CoNLL Shared Task. The Chi-
nese data used in the shared task is based on Chinese
Treebank 6.0 and the Chinese Proposition Bank 2.0,
both of which are publicly available via the Linguis-
tic Data Consortium.
The Chinese Treebank Project originated at Penn
and was later moved to University of Colorado at
10http://www.lsi.upc.es/?nlp/freeling
11http://w3.msi.vxu.se/?jha/maltparser
9
Boulder. Now it is the process of being to moved
to Brandeis University. The data sources of the Chi-
nese Treebank range from Xinhua newswire (main-
land China), Hong Kong news, and Sinorama Maga-
zine (Taiwan). More recently under DARPA GALE
funding it has been expanded to include broadcast
news, broadcast conversation, news groups and web
log data. It currently has over one million words
and is fully segmented, POS-tagged and annotated
with phrase structure. The version of the Chinese
Treebank used in this shared task, CTB 6.0, includes
newswire, magazine articles, and transcribed broad-
cast news 12. The training set has 609,060 tokens,
the development set has 49,620 tokens, and the test
set has 73,153 tokens.
The Chinese Proposition Bank adds a layer of se-
mantic annotation to the syntactic parses in the Chi-
nese Treebank. This layer of semantic annotation
mainly deals with the predicate-argument structure
of Chinese verbs and their nominalizations. Each
major sense (called frameset) of a predicate takes a
number of core arguments annotated with numeri-
cal labels Arg0 through Arg5 which are defined in
a predicate-specific manner. The Chinese Proposi-
tion Bank also annotates adjunctive arguments such
as locative, temporal and manner modifiers of the
predicate. The version of the Chinese Propbank used
in this CoNLL Shared Task is CPB 2.0, but nominal
predicates are excluded because the annotation is in-
complete.
Since the Chinese Treebank is annotated with
constituent structures, the conversion and merging
procedure converts the constituent structures to de-
pendencies by identifying the head for each con-
stituent in a parse tree and making its sisters its de-
pendents. The Chinese Propbank pointers are then
shifted from the entire constituent to the head of that
constituent. The conversion procedure identifies the
head by first exploiting the structural information
in the syntactic parse and detecting six broad cate-
gories of syntactic relations that hold between the
head and its dependents (predication, modification,
complementation, coordination, auxiliary, and flat)
and then designating the head based on these rela-
tions. In particular, the first conjunct of a coordina-
12A small number of files were taken out of the CoNLL
shared task data due to conversion problems and time con-
straints to fix them.
tion structure is designated as the head and the heads
of the other conjuncts are the conjunctions preced-
ing them. The conjunctions all ?modify? the first
conjunct.
3.4 Czech
For the training, development and evaluation data,
Prague Dependency Treebank 2.0 was used (Hajic?
et al, 2006). For the out-of-domain evaluation data,
part of the Czech side of the Prague Czech-English
Dependency Treebank (version 2, under construc-
tion) was used13, see also ( ?Cmejrek et al, 2004). For
the OOD data, no manual annotation of LEMMA,
POS, and FEAT existed, so the predicted values
were used. The same conversion procedure has been
applied to both sources.
The FORM column was created from the form
element of the morphological layer, not from the
?token? from the word-form layer. Therefore, most
typos, errors in word segmentation and tokenization
are corrected and numerals are normalized.
The LEMMA column was created from the
lemma element of the morphological layer. Only
the initial string of the element was used, so there is
no distinction between homonyms. However, some
components of the detailed lemma explanation were
incorporated into the FEAT column (see below).
The POS column was created form the morpho-
logical tag element, its first character more pre-
cisely.
The FEAT column was created from the remain-
ing characters of the tag element. In addition, the
special feature ?Sem? corresponds to a semantic fea-
ture of the lemma.
For the HEAD and DEPREL columns, the PDT
analytical layer was used. The DEPREL was taken
from the analytic function (the afun node at-
tribtue). There are 27 possible values for afun el-
ement: Pred, Pnom, AuxV, Sb, Obj, Atr, Adv,
Atv, AtvV, Coord, Apos, ExD, and a number
of auxiliary and ?double-function? labels. The first
nine of these are the ?most interesting? from the
point of view of the shared task, since they relate to
semantics more closely than the rest (at least from
the linguistic point of view). The HEAD is a pointer
to its parent, which means the PDT?s ord attribute
13http://ufal.mff.cuni.cz/pedt
10
(within-sentence ID / word position number) of the
parent. If a node is a member of a coordination
or apposition (is_member element), its DEPREL
obtains the _M suffix. The parenthesis annotation
(is_parenthesis_root element) was ignored.
The PRED and APREDs columns were created
from the tectogrammatical layer of PDT 2.0 and the
valency lexicon PDT-Vallex according to the follow-
ing rules:
? Every line corresponding to an analytical node
referenced by a lexical reference (a/lex.rf)
from the tectogrammatical layer has a PRED
value filled. If the referring non-generated
tectogrammatical node (is_generated not
equal to 1) has a valency frame assigned
(val_frame.rf), the value of PRED is the
identifier of the frame. Otherwise, it is set to
the same value as the LEMMA column.
? For every tectogrammatical node, a corre-
sponding analytical node is searched for:
1. If the tectogrammatical node is not
generated and has a lexical reference
(a/lex.rf), the referenced node is
taken.
2. Otherwise, if the tectogrammatical node
has a coreference (coref_text.rf or
coref_gram.rf) or complement refer-
ence (compl.rf) to a node that has an
analytical node assigned (by 1. or 2.), the
assigned node is taken.
APRED columns are filled with respect to the
following correspondence: for a tectogrammatical
node P and its effective child C with functor F, the
column for P?s corresponding analytical node at the
row for C?s corresponding analytical node is filled
with F. Some nodes can thus have several functors
in one APRED column, separated by a vertical bar
(see Sect. 2.4.2).
PLEMMA, PPOS and PFEAT were gener-
ated by the (cross-trained) morphological tagger
MORCE (Spoustova? et al, 2009), which gives full
combined accuracy (PLEMMA+PPOS+PFEAT)
slightly under 96%.
PHEAD and PDEPREL were generated by
the (cross-trained) MST parser for Czech (Chu?
Liu/Edmonds algorithm, (McDonald et al, 2005)),
which has typical dependency accuracy around
85%.
The valency lexicon, converted from (Hajic? et al,
2003), has four columns:
1. lemma (can occur several times in the lexicon,
with different frames)
2. frame identifier (as found in the PRED column)
3. list of space-separated actants and obligatory
members of the frame
4. example(s)
The source of the out-of-domain data uses an
extended valency lexicon (because of out-of-
vocabulary entries). For simplicity, the extended
lexicon was not provided; instead, such words were
not marked as predicates in the OOD data (their
FILLPRED was set to ?_?) and thus not evaluated.
3.5 English
The English corpus is almost identical to the cor-
pus used in the closed challenge in the CoNLL-2008
shared task evaluation (Surdeanu et al, 2008). This
corpus was generated through a process that merges
several input corpora and converts them from the
constituent-based formalism to dependencies. The
following corpora were used as input to the merging
procedure:
? Penn Treebank 3 ? The Penn Treebank 3 cor-
pus (Marcus et al, 1994) consists of hand-
coded parses of the Wall Street Journal (test,
development and training) and a small subset
of the Brown corpus (W. N. Francis and H.
Kucera, 1964) (test only).
? BBN Pronoun Coreference and Entity Type
Corpus ? BBN?s NE annotation of the Wall
Street Journal corpus (Weischedel and Brun-
stein, 2005) takes the form of SGML inline
markup of text, tokenized to be completely
compatible with the Penn Treebank annotation.
For the CoNLL-2008 shared task evaluation,
this corpus was extended by the task organizers
to cover the subset of the Brown corpus used as
a secondary testing dataset. From this corpus
we only used NE boundaries to derive NAME
11
dependencies between NE tokens, e.g., we cre-
ate a NAME dependency from Mary to Smith
given the NE mention Mary Smith.
? Proposition Bank I (PropBank) ? The Prop-
Bank annotation (Palmer et al, 2005) classifies
the arguments of all the main verbs in the Penn
Treebank corpus, other than be. Arguments are
numbered (Arg0, Arg1, . . .) based on lexical
entries or frame files. Different sets of argu-
ments are assumed for different rolesets. De-
pendent constituents that fall into categories in-
dependent of the lexical entries are classified as
various types of adjuncts (ArgM-TMP, -ADV,
etc.).
? NomBank ? NomBank annotation (Meyers et
al., 2004) uses essentially the same framework
as PropBank to annotate arguments of nouns.
Differences between PropBank and NomBank
stem from differences between noun and verb
argument structure; differences in treatment of
nouns and verbs in the Penn Treebank; and dif-
ferences in the sophistication of previous re-
search about noun and verb argument structure.
Only the subset of nouns that take arguments
are annotated in NomBank and only a subset of
the non-argument siblings of nouns are marked
as ArgM.
The complete merging process and the conversion
from the constituent representation to dependencies
is detailed in (Surdeanu et al, 2008).
The main difference between the 2008 and 2009
version of the corpora is the generation of word lem-
mas. In the 2008 version the only lemmas pro-
vided were predicted using the built-in lemmatizer
in WordNet (Fellbaum, 1998) based on the most fre-
quent sense for the form and the predicted part-of-
speech tag. These lemmas are listed in the 2009
corpus under the PLEMMA column. The LEMMA
column in the 2009 version of the corpus contains
lemmas generated using the same algorithm but us-
ing the correct Treebank part-of-speech tags. Addi-
tionally, the PHEAD and PDEPREL columns were
generated using MaltParser14, similarly to the open
challenge corpus in the CoNLL 2008 shared task.
14http://w3.msi.vxu.se/?nivre/research/
MaltParser.html
3.6 German
The German in-domain dataset is based on the an-
notated verb instances of the SALSA corpus (Bur-
chardt et al, 2006), a total of around 40k sen-
tences15. SALSA provides manual semantic role
annotation on top of the syntactically annotated
TIGER newspaper corpus, one of the standard Ger-
man treebanks. The original SALSA corpus uses se-
mantic roles in the FrameNet paradigm. We con-
structed mappings between FrameNet frame ele-
ments and PropBank argument positions at the level
of frame-predicate pairs semi-automatically. For the
frame elements of each frame-predicate pair, we first
identified the semantically defined PropBank Arg-
0 and Arg-1 positions. To do so, we annotated a
small number of very abstract frame elements with
these labels (Agent, Actor, Communicator as Arg-
0, and Theme, Effect, Message as Arg-1) and per-
colated these labels through the FrameNet hierar-
chy, adding further manual labels where necessary.
Then, we used frequency and grammatical realiza-
tion information to map the remaining roles onto
higher-numbered Arg roles. We considerably sim-
plified the annotations provided by SALSA, which
use a rather complex annotation scheme. In partic-
ular, we removed annotation for multi-word expres-
sions (which may be non-contiguous), annotations
involving multiple frames for the same predicate
(metaphors, underspecification), and inter-sentence
roles.
The out-of-domain dataset was taken from a study
on the multi-lingual projection of FrameNet annota-
tion (Pado and Lapata, 2005). It is sampled from
the EUROPARL corpus and was chosen to maxi-
mize the lexical coverage, i.e., it contains of a large
number of infrequent predicates. Both syntactic and
semantic structure were annotated manually, in the
TIGER and SALSA format, respectively. Since it
uses a simplified annotation schemes, we did not
have to discard any annotation.
For both datasets, we converted the syntactic
TIGER (Brants et al, 2002) representations into de-
pendencies with a similar set of head-finding rules
used for the preparation of the CoNLL-X shared task
German dataset. Minor modifications (for the con-
15Note, however, that typically not all predicates in each sen-
tence are annotated (cf. Table 2).
12
version of person names and coordinations) were
made to achieve better consistency with datasets
of other languages. Since the TIGER annotation
allows non-contiguous constituents, the resulting
dependencies can be non-projective. Secondary
edges were discarded in the conversion. As for the
automatically constructed features, we used Tree-
Tagger (Schmid, 1994) to produce the PLEMMA
and PPOS columns, and the Morphisto morphol-
ogy (Zielinski and Simon, 2008) for PFEAT.
3.7 Japanese
For Japanese, we used the Kyoto University Text
Corpus (Kawahara et al, 2002), which consists of
approximately 40k sentences taken from Mainichi
Newspapers. Out of them, approximately 5k sen-
tences are annotated with syntactic and semantic de-
pendencies, and are used the training, development
and test data of this year?s shared task. The remain-
ing sentences, which are annotated with only syntac-
tic dependencies, are provided for the training cor-
pus of syntactic dependency parsers.
This corpus adopts a dependency structure repre-
sentation, and thus the conversion to the CoNLL-
2009 format was relatively straightforward. How-
ever, since the original dependencies are annotated
on the basis of phrases (Japanese bunsetsu), we
needed to automatically convert the original annota-
tions to word-based ones using several criteria. We
used the following basic criteria: the words except
the last word in a phrase depend on the next (right)
word, and the last word in a phrase basically depends
on the head word of the governing phrase.
Semantic dependencies are annotated for both
verbal predicates and nominal predicates. The se-
mantic roles (APRED columns) consist of 41 sur-
face cases, many of which are case-marking post-
positions such as ga (nominative), wo (accusative)
and ni (dative). Semantic frame discrimination is not
annotated, and so the PRED column is the same as
the LEMMA column. The original corpus contains
coreference annotations and inter-sentential seman-
tic dependencies, such as inter-sentential zero pro-
nouns and bridging references, but we did not use
these annotations, which are not the target of this
year?s shared task.
To produce the PLEMMA, PPOS and PFEAT
columns, we used the morphological analyzer JU-
MAN 16 and the dependency and case structure an-
alyzer KNP 17. To produce the PHEAD and PDE-
PREL columns, we used the MSTParser 18.
4 Submissions and Results
Participants uploaded the results through the shared
task website, and the official evaluation was per-
formed centrally. Feedback was provided if any for-
mal problems were encountered (for a list of checks,
see the previous section). One submission had to
be rejected because only English results were pro-
vided. After the evaluation period had passed, the
results were anonymized and published on the web.
A total of 20 systems participated in the closed
challenge; 13 of them in the Joint task and seven in
the SRL-only task. Two systems participated in the
open challenge (Joint task). Moreover, 17 systems
provided output in the out-of-domain part of the task
(11 in the OOD Joint task and six in the OOD SRL-
only task).
The main results for the core task - the Joint task
(dependency syntax and semantic relations) in the
context of the closed challenge - are summarized and
ranked in Table 5.
The largest number of systems can be compared
in the SRL results table (Table 6), where all the sys-
tems have been evaluated solely on the SRL perfor-
mance regardless whether they participated in the
Joint or SRL-only task. However, since the results
might have been influenced by the supplied parser,
separate ranking is provided for both types of the
systems.
Additional breakdown of the results (open chal-
lenge, precision and recall tables for the semantic
labeling task, etc.) are available from the CoNLL-
2009 Shared Task website19.
5 Approaches
Table 7 summarizes the properties of the systems
that participated in the closed the open challenges.
16http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
17http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
18http://sourceforge.net/projects/
mstparser
19http://ufal.mff.cuni.cz/conll2009-st
13
Rank System Average Catalan Chinese Czech English German Japanese Spanish
1 Che 82.64 81.84 76.38 83.27 87.00 82.44 85.65 81.90
2 Chen 82.52 83.01 76.23 80.87 87.69 81.22 85.28 83.31
3 Merlo 82.14 82.66 76.15 83.21 86.03 79.59 84.91 82.43
4 Bohnet 80.85 80.44 75.91 79.57 85.14 81.60 82.51 80.75
5 Asahara 78.43 75.91 73.43 81.43 86.40 69.84 84.86 77.12
6 Brown 77.27 77.40 72.12 75.66 83.98 77.86 76.65 77.21
7 Zhang 76.49 75.00 73.42 76.93 82.88 73.76 78.17 75.25
8 Dai 73.98 72.09 72.72 67.14 81.89 75.00 80.89 68.14
9 Lu Li 73.97 71.32 65.53 75.85 81.92 70.93 80.49 71.72
10 Llu??s 71.49 56.64 66.18 75.95 81.69 72.31 81.76 65.91
11 Vallejo 70.81 73.75 67.16 60.50 78.19 67.51 77.75 70.78
12 Ren 67.81 59.42 75.90 60.18 77.83 65.77 77.63 57.96
13 Zeman 51.07 49.61 43.50 57.95 50.27 49.57 57.69 48.90
Table 5: Official results of the Joint task, closed challenge. Teams are denoted by the last name (first name added
only where needed) of the author who registered for the evaluation data. Results are sorted in descending order of the
language-averaged macro F1 score on the closed challenge Joint task. Bold numbers denote the best result for a given
language.
Rank Rank in task System Average Catalan Chinese Czech English German Japanese Spanish
1 1 (SRLonly) Zhao 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46
2 2 (SRLonly) Nugues 80.31 80.01 78.60 85.41 85.63 79.71 76.30 76.52
3 1 (Joint) Chen 79.96 80.10 76.77 82.04 86.15 76.19 78.17 80.29
4 2 (Joint) Che 79.94 77.10 77.15 86.51 85.51 78.61 78.26 76.47
5 3 (Joint) Merlo 78.42 77.44 76.05 86.02 83.24 71.78 77.23 77.19
6 3 (SRLonly) Meza-Ruiz 77.46 78.00 77.73 75.75 83.34 73.52 76.00 77.91
7 4 (Joint) Bohnet 76.00 74.53 75.29 79.02 80.39 75.72 72.76 74.31
8 5 (Joint) Asahara 75.65 72.35 74.17 84.69 84.26 63.66 77.93 72.50
9 6 (Joint) Brown 72.85 72.18 72.43 78.02 80.43 73.40 61.57 71.95
10 7 (Joint) Dai 70.78 66.34 71.57 75.50 78.93 67.43 71.02 64.64
11 8 (Joint) Zhang 70.31 67.34 73.20 78.28 77.85 62.95 64.71 67.81
12 9 (Joint) Lu Li 69.72 66.95 67.06 79.08 77.17 61.98 69.58 66.23
13 4 (SRLonly) Baoli Li 69.26 74.06 70.37 57.46 69.63 67.76 72.03 73.54
14 10 (Joint) Vallejo 68.95 70.14 66.71 71.49 75.97 61.01 68.82 68.48
15 5 (SRLonly) Moreau 66.49 65.60 67.37 71.74 72.14 66.50 57.75 64.33
16 11 (Joint) Llu??s 63.06 46.79 59.72 76.90 75.86 62.66 71.60 47.88
17 6 (SRLonly) Ta?ckstro?m 61.27 57.11 63.41 71.05 67.64 53.42 54.74 61.51
18 7 (SRLonly) Lin 57.18 61.70 70.33 60.43 65.66 59.51 23.78 58.87
19 12 (Joint) Ren 56.69 41.00 72.58 62.82 67.56 54.31 58.73 39.80
20 13 (Joint) Zeman 32.14 24.19 34.71 58.13 36.05 16.44 30.13 25.36
Table 6: Official results of the semantic labeling, closed challenge, all systems. Teams are denoted by the last name
(first name added only where needed) of the author who registered for the evaluation data. Results are sorted in
descending order of the semantic labeled F1 score (closed challenge). Bold numbers denote the best result for a given
language. Separate ranking is provided for SRL-only systems.
The second column of the table highlights the over-
all architectures. We used + to indicate that the
components are sequentially connected. The lack of
a + sign indicates that the corresponding tasks are
performed jointly.
It is perhaps not surprising that most of the obser-
vations from the 2008 shared task still hold; namely,
the best systems overall do not use joint learning or
optimization (the best such system was placed third
in the Joint task, and there were only four systems
where the learning methodology can be considered
?joint?).
Therefore, most of the observations and conclu-
sions from 2008 shared task hold as well for the
current results. For details, we will leave it to the
reader to interpret the architectures and methods
14
O
v
er
a
ll
D
D
D
PA
PA
PA
Jo
in
t
M
L
Sy
st
em
a
A
rc
h.
b
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
c
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
Le
a
rn
in
g/
O
pt
.
M
et
ho
ds
Zh
ao
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
/g
lo
ba
l
se
ar
ch
(S
R
L-
o
n
ly
)
M
E
N
u
gu
es
(P
C+
A
I+
A
C)
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
be
am
se
ar
ch
+
re
ra
n
ki
n
g
(S
R
L-
o
n
ly
)
L2
-
re
gu
la
riz
ed
lin
.
re
gr
es
sio
n
Ch
en
P
+
PC
+
A
I+
A
C
gr
ap
h
pa
rt
ia
lly
M
ST
C
L
/E
cl
as
s
n
o
gr
ee
dy
(?)
n
o
M
E
Ch
e
D
+
PC
+
A
IC
gr
ap
h
n
o
M
ST
H
O
E
cl
as
s
n
o
IL
P
n
o
SV
M
,
M
E
M
er
lo
D
PA
IC
+
D
ge
n
er
at
iv
e,
tr
an
s
n
o
be
am
se
ar
ch
tr
an
s
n
o
be
am
se
ar
ch
sy
n
ch
ro
n
iz
ed
de
riv
at
io
n
IS
B
N
M
ez
a-
R
u
iz
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
M
ar
ko
v
LN
n
o
Cu
tti
n
g
Pl
an
e
(S
R
L-
o
n
ly
)
M
IR
A
B
o
hn
et
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
C
+
re
ar
ra
n
ge
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
IR
A
)
A
sa
ha
ra
D
+
PI
C
+
A
IC
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
n
-
be
st
re
la
x
.
n
o
pe
rc
ep
tr
o
n
D
ai
D
+
PC
+
A
C
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
pr
o
b
ite
ra
tiv
e
M
E
Zh
an
g
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
E
cl
as
s
n
o
cl
as
sifi
ca
tio
n
n
o
M
IR
A
,
M
E
Lu
Li
D
+
(P
C
||
A
IC
)
gr
ap
h
fo
r
ea
ch
la
n
g.
M
ST
C
L
/E
,
M
ST
E
cl
as
s
n
o
gr
ee
dy
n
o
M
E
B
ao
li
Li
PC
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
,
kN
N
,
M
E
Va
lle
jod
[D
+
P+
A
]C
+
D
I
cl
as
s
n
o
re
ra
n
ki
n
g
cl
as
s
n
o
re
ra
n
ki
n
g
u
n
ifi
ed
la
be
ls
M
B
L
M
o
re
au
D
+
PI
+
Cl
u
st
er
in
g
+
A
I+
A
C
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
CR
F
(S
R
L-
o
n
ly
)
CR
F
Ll
u
??s
D
+
D
A
IC
+
PC
gr
ap
h
n
o
M
ST
E
gr
ap
h
n
o
M
ST
E
ye
s,
M
ST
E
Av
g.
Pe
rc
ep
tr
o
n
Ta?
ck
st
ro?
m
D
+
PI
+
A
I
+
A
C
+
Co
n
st
ra
in
tS
at
isf
ac
tio
n
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
R
en
D
+
PC
+
A
IC
tr
an
s
n
o
gr
ee
dy
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
al
t),
M
E
Ze
m
an
D
I+
D
C+
PC
+
A
I+
A
C
tr
an
s
n
o
gr
ee
dy
w
ith
he
u
ris
tic
s
cl
as
s
n
o
gr
ee
dy
n
o
co
o
cc
u
rr
en
ce
Ta
bl
e
7:
Su
m
m
ar
y
o
fs
ys
te
m
ar
ch
ite
ct
u
re
s
fo
r
th
e
Co
N
LL
-
20
09
sh
ar
ed
ta
sk
;
al
ls
ys
te
m
s
ar
e
in
cl
u
de
d.
SR
L-
o
n
ly
sy
st
em
s
do
n
o
t
ha
v
e
th
e
D
co
lu
m
n
s
an
d
th
e
Jo
in
t
Le
ar
in
g/
O
pt
.
co
lu
m
n
s
fil
le
d
in
.
Th
e
sy
st
em
s
ar
e
so
rt
ed
by
th
e
se
m
an
tic
la
be
le
d
F 1
sc
o
re
av
er
ag
ed
o
v
er
al
lt
he
la
n
gu
ag
es
(sa
m
e
as
in
Ta
bl
e
6).
O
n
ly
th
e
sy
st
em
s
th
at
ha
v
e
a
co
rr
es
po
n
di
n
g
pa
pe
r
in
th
e
pr
o
ce
ed
in
gs
ar
e
in
cl
u
de
d.
A
cr
o
n
ym
s
u
se
d:
D
-
sy
n
ta
ct
ic
de
pe
n
de
n
ci
es
,
P
-
pr
ed
ic
at
e,
A
-
ar
gu
m
en
t,
I-
id
en
tifi
ca
tio
n
,
C
-
cl
as
sifi
ca
tio
n
.
O
v
er
a
ll
a
rc
h.
st
an
ds
fo
r
th
e
co
m
pl
et
e
sy
st
em
ar
ch
ite
ct
u
re
;D
A
rc
h.
st
an
ds
fo
r
th
e
ar
ch
ite
ct
u
re
o
ft
he
sy
n
ta
ct
ic
pa
rs
er
;D
C
o
m
b.
in
di
ca
te
s
if
th
e
fin
al
pa
rs
er
o
u
tp
u
tw
as
ge
n
er
at
ed
u
sin
g
pa
rs
er
co
m
bi
n
at
io
n
;D
In
fe
re
n
ce
st
an
ds
fo
r
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
sy
n
ta
ct
ic
pa
rs
in
g;
PA
A
rc
h.
st
an
ds
th
e
ty
pe
o
fa
rc
hi
te
ct
u
re
u
se
d
fo
r
PA
IC
;P
A
C
o
m
b.
in
di
ca
te
s
if
th
e
PA
o
u
tp
u
t
w
as
ge
n
er
at
ed
th
ro
u
gh
sy
st
em
co
m
bi
n
at
io
n
;P
A
In
fe
re
n
ce
st
an
ds
fo
r
th
e
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
PA
IC
;J
o
in
tL
ea
rn
in
g/
O
pt
.
in
di
ca
te
s
if
so
m
e
fo
rm
o
fjo
in
tl
ea
rn
in
g
o
r
o
pt
im
iz
at
io
n
w
as
im
pl
em
en
te
d
fo
r
th
e
sy
n
ta
ct
ic
+
se
m
an
tic
gl
o
ba
lt
as
k;
M
L
M
et
ho
ds
lis
ts
th
e
M
L
m
et
ho
ds
u
se
d
th
ro
u
gh
o
u
tt
he
co
m
pl
et
e
sy
st
em
.
a
A
u
th
o
rs
o
ft
w
o
sy
st
em
s:
?
B
ro
w
n
?
an
d
?
Li
n
?
di
dn
?
ts
u
bm
it
a
pa
pe
r,
so
th
ei
r
sy
st
em
s?
ar
ch
ite
ct
u
re
s
ar
e
u
n
kn
ow
n
.
b T
he
sy
m
bo
l+
in
di
ca
te
s
se
qu
en
tia
lp
ro
ce
ss
in
g
(ot
he
rw
ise
,
pa
ra
lle
l/jo
in
t).
Th
e
||
m
ea
n
s
th
at
se
v
er
al
di
ffe
re
n
ta
rc
hi
te
ct
u
re
s
sp
an
n
in
g
m
u
lti
pl
e
su
bt
as
ks
ra
n
in
pa
ra
lle
l.
c
M
ST
C
L
/E
as
u
se
d
by
M
cD
o
n
al
d
(20
05
),
M
ST
C
by
Ca
rr
er
as
(20
07
),M
ST
E
by
Ei
sn
er
(20
00
),
M
ST
H
O
E
=
M
ST
E
w
ith
hi
gh
er
-
o
rd
er
fe
at
u
re
s
(si
bl
in
gs
+
al
lg
ra
n
dc
hi
ld
re
n
).
d T
he
sy
st
em
u
n
ifi
es
th
e
sy
n
ta
ct
ic
an
d
se
m
an
tic
la
be
ls
in
to
o
n
e
la
be
l,
an
d
tr
ai
n
s
cl
as
sifi
er
s
o
v
er
th
em
.
It
is
th
u
s
di
ffi
cu
lt
to
sp
lit
th
e
sy
st
em
ch
ar
ac
te
ris
tic
in
to
a
?
D
?
/?
PA
?
pa
rt
.
15
when comparing Table 7 with the Tables 5 and 6).
6 Conclusion
This year?s task has been demanding in several re-
spects, but certainly the most difficulty came from
the fact that participants had to tackle all seven lan-
guages. It is encouraging that despite this added af-
fort the number of participating systems has been
almost the same as last year (20 vs. 22 in 2008).
There are several positive outcomes from this
year?s enterprise:
? we have prepared a unified format and data for
several very different lanaguages, as a basis
for possible extensions towards other languages
and unified treatment of syntactic depenndecies
and semantic role labeling across natural lan-
guages;
? 20 participants have produced SRL results for
all seven languages, using several different
methods, giving hope for a combined system
with even substantially better performance;
? initial results have been provided for three lan-
guages on out-of-domain data (being in fact
quite close to the in-domain results).
Only four systems tried to apply what can be de-
scribed as joint learning for the syntactic and seman-
tic parts of the task. (Morante et al, 2009) use a true
joint learning formulation that phrases syntactico-
semantic parsing as a series of classification where
the class labels are concatenations of syntactic and
semantic edge labels. They predict (a), the set of
syntactico-semantic edge labels for each pair of to-
kens; (b), the set of incoming syntactico-semantic
edge labels for each individual token; and (c), the
existence of an edge between each pair of tokens.
Subsequently, they combine the (possibly conflict-
ing) output of the three classifiers by a ranking ap-
proach to determine the most likely structure that
meets all well-formedness constraints. (Llu??s et al,
2009) present a joint approach based on an exten-
sion of Eisner?s parser to accommodate also seman-
tic dependency labels. This architecture is similar
to the one presented by the same authors in the past
edition, with the extension to a second-order syn-
tactic parsing and a particular setting for Catalan
and Spanish. (Gesmundo et al, 2009) use an in-
cremental parsing model with synchronous syntac-
tic and semantic derivations and a joint probability
model for syntactic and semantic dependency struc-
tures. The system uses a single input queue but two
separate stacks and synchronizes syntactic and se-
mantic derivations at every word. The synchronous
derivations are modeled with an Incremental Sig-
moid Belief Network that has latent variables for
both syntactic and semantic states and connections
from syntax to semantics and vice versa. (Dai et
al., 2009) designed an iterative system to exploit
the inter-connections between the different subtasks
of the CoNLL shared task. The idea is to decom-
pose the joint learning problem into four subtasks
? syntactic dependency identification, syntactic de-
pendency labeling, semantic dependency identifica-
tion and semantic dependency labeling. The initial
step is to use a pipeline approach to use the input of
one subtask as input to the next, in the order speci-
fied. The iterative steps then use additional features
that are not available in the initial step to improve the
accuracy of the overall system. For example, in the
iterative steps, semantic information becomes avail-
able as features to syntactic parsing, so on and so
forth.
Despite these results, it is still not clear whether
joint learning has a significant advantage over other
approaches (and if yes, then for what languages). It
is thus necessary to carefully plan the next shared
tasks; it might be advantageous to bring up a sim-
ilar task in the future once again, and/or couple it
with selected application(s). There, (we hope) the
benefits of the dependency representation combined
with semantic roles the way we have formulated it
in 2008 and 2009 will really show up.
Acknowledgments
We would like to thank the Linguistic Data Consor-
tium, mainly to Denise DiPersio, Tony Casteletto
and Christopher Cieri for their help and handling
of invoicing and distribution of the data for which
LDC has a license. For all of the trial, training and
evaluation data they had to act a very short notice.
All the data has been at the participants? disposal
(again) free of charge. We are grateful to all of them
for LDC?s continuing support of the CoNLL Shared
16
Tasks.
We would also like to thank organizers of the pre-
vious four shared tasks: Sabine Buchholz, Xavier
Carreras, Ryan McDonald, Amit Dubey, Johan Hall,
Yuval Krymolowski, Sandra Ku?bler, Erwin Marsi,
Jens Nilsson, Sebastian Riedel and Deniz Yuret.
This shared task would not have been possible with-
out their previous effort.
We also acknowledge the support of the M?SMT
of the Czech Republic, projects MSM0021620838
and LC536; the Grant Agency of the Academy of
sciences of the Czech Republic 1ET201120505 (for
Jan Hajic?, Jan ?Ste?pa?nek and Pavel Stran?a?k).
Llu??s Ma`rquez and M. Anto`nia Mart?? partici-
pation was supported by the Spanish Ministry of
Education and Science, through the OpenMT and
TextMess research projects (TIN2006-15307-C03-
02, TIN2006-15265-C06-06).
The following individuals directly contributed to
the Chinese Treebank (in alphabetic order): Meiyu
Chang, Fu-Dong Chiou, Shizhe Huang, Zixin Jiang,
Tony Kroch, Martha Palmer, Mitch Marcus, Fei
Xia, Nianwen Xue. The contributors to the Chi-
nese Proposition Bank include (in alphabetic order):
Meiyu Chang, Gang Chen, Helen Chen, Zixin Jiang,
Martha Palmer, Zhiyi Song, Nianwen Xue, Ping Yu,
Hua Zhong. The Chinese Treebank and the Chinese
Proposition Bank were funded by DOD, NSF and
DARPA.
Adam Meyers? work on the shared task has been
supported by the NSF Grant IIS-0534700 ?Structure
Alignment-based MT.?
We thank the Mainichi Newspapers for the per-
mission of distributing the sentences of the Kyoto
University Text Corpus for this shared task.
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
EMNLP-CoNLL 2007, pages 957?961, June. Prague,
Czech Republic.
Montserrat Civit, M. Anto`nia Mart??, and Nu?ria Buf??.
2006. Cat3LB and Cast3LB: from constituents to
dependencies. In Proceedings of the 5th Interna-
tional Conference on Natural Language Processing,
FinTAL, pages 141?153, Turku, Finland. Springer Ver-
lag, LNAI 4139.
Qifeng Dai, Enhong Chen, and Liu Shi. 2009. An it-
erative approach for joint dependency parsing and se-
mantic role labeling. In Proceedings of the 13th Con-
ference on Computational Natural Language Learning
(CoNLL-2009), June 4-5, Boulder, Colorado, USA.
June 4-5.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Tehcnologies, pages 29?62. Kluwer Academic
Publishers.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press, Cambridge.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the 13th Conference on
Computational Natural Language Learning (CoNLL-
2009), June 4-5, Boulder, Colorado, USA. June 4-5.
Jan Hajic?, Jarmila Panevova?, Zden?ka Ures?ova?, Alevtina
Be?mova?, Veronika Kola?r?ova?- ?Rezn??c?kova??, and Petr
Pajas. 2003. PDT-VALLEX: Creating a Large-
coverage Valency Lexicon for Treebank Annotation.
In J. Nivre and E. Hinrichs, editors, Proceedings of The
Second Workshop on Treebanks and Linguistic Theo-
ries, pages 57?68, Vaxjo, Sweden. Vaxjo University
Press.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan ?Ste?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k ?Zabokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Xavier Llu??s, Stefan Bott, and Llu??s Ma`rquez. 2009.
A second-order joint eisner model for syntactic and
semantic dependency parsing. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA. June 4-5.
17
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
Llu??s Ma`rquez, Luis Villarejo, M. Anto`nia Mart??, and
Mariona Taule?. 2007. SemEval-2007 Task 09: Mul-
tilevel semantic annotation of catalan and spanish.
In Proceedings of the 4th International Workshop on
Semantic Evaluations (SemEval-2007), pages 42?47,
Prague, Czech Republic.
M. Anto`nia Mart??, Mariona Taule?, Llu??s Ma`rquez, and
Manu Bertran. 2007. Anotacio?n semiautoma?tica
con papeles tema?ticos de los corpus CESS-ECE.
Procesamiento del Lenguaje Natural, SEPLN Journal,
38:67?76.
Ryan McDonald, Fernando Pereira, Jan Hajic?, and Kiril
Ribarov. 2005. Non-projective dependency parsing
using spanning tree algortihms. In Proceedings of
NAACL-HLT?05, Vancouver, Canada, pages 523?530.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank Project: An Interim Report. In NAACL/HLT
2004 Workshop Frontiers in Corpus Annotation,
Boston.
Roser Morante, Vincent Van Asch, and Antal van den
Bosch. 2009. A simple generative pipeline approach
to dependency parsing and semantic role labeling. In
Proceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL-2009), Boulder,
Colorado, USA. June 4-5.
Joakim Nivre, Johann Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The conll 2007 shared task on depen-
dency parsing. In Proceedings of the EMNLP-CoNLL
2007 Conference, pages 915?932, Prague, Czech Re-
public.
Sebastian Pado and Mirella Lapata. 2005. Cross-lingual
projection of role-semantic information. In Proceed-
ings of the Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP-2005), pages 859?
866, Vancouver, BC.
Petr Pajas and Jan ?Ste?pa?nek. 2008. Recent advances in
a feature-rich framework for treebank annotation. In
The 22nd International Conference on Computational
Linguistics - Proceedings of the Conference (COL-
ING?08), pages 673?680, Manchester.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the European ACL Cenference EACL?09,
Athens, Greece.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008), pages 159?177.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Martin ?Cmejrek, Jan Cur???n, Jan Hajic?, Jir??? Havelka,
and Vladislav Kubon?. 2004. Prague Czech-English
Dependency Treebank: Syntactically Anntoated Re-
sources for Machine Translation. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation (LREC-2004), pages 1597?
1600, Lisbon, Portugal.
W. N. Francis and H. Kucera. 1964. Brown Corpus Man-
ual of Information to accompany A Standard Corpus
of Present-Day Edited American English, for use with
Digital Computers. Revised 1971, Revised and Am-
plified 1979, available at www.clarinet/brown.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Technical report,
Lin- guistic Data Consortium.
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Andrea Zielinski and Christian Simon. 2008. Morphisto:
An open-source morphological analyzer for german.
In Proceedings of the Conference on Finite State Meth-
ods in Natural Language Processing.
18
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 146?154,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Automatic Recognition of Logical Relations for English, Chinese and
Japanese in the GLARF Framework
Adam Meyers?, Michiko Kosaka?, Nianwen Xue?, Heng Ji?, Ang Sun?, Shasha Liao? and Wei Xu?
? New York University, ?Monmouth University, ?Brandeis University, ? City University of New York
Abstract
We present GLARF, a framework for repre-
senting three linguistic levels and systems for
generating this representation. We focus on a
logical level, like LFG?s F-structure, but com-
patible with Penn Treebanks. While less fine-
grained than typical semantic role labeling ap-
proaches, our logical structure has several ad-
vantages: (1) it includes all words in all sen-
tences, regardless of part of speech or seman-
tic domain; and (2) it is easier to produce ac-
curately. Our systems achieve 90% for En-
glish/Japanese News and 74.5% for Chinese
News ? these F-scores are nearly the same as
those achieved for treebank-based parsing.
1 Introduction
For decades, computational linguists have paired a
surface syntactic analysis with an analysis represent-
ing something ?deeper?. The work of Harris (1968),
Chomsky (1957) and many others showed that one
could use these deeper analyses to regularize differ-
ences between ways of expressing the same idea.
For statistical methods, these regularizations, in ef-
fect, reduce the number of significant differences be-
tween observable patterns in data and raise the fre-
quency of each difference. Patterns are thus easier
to learn from training data and easier to recognize in
test data, thus somewhat compensating for the spare-
ness of data. In addition, deeper analyses are often
considered semantic in nature because conceptually,
two expressions that share the same regularized form
also share some aspects of meaning. The specific de-
tails of this ?deep? analysis have varied quite a bit,
perhaps more than surface syntax.
In the 1970s and 1980s, Lexical Function Gram-
mar?s (LFG) way of dividing C-structure (surface)
and F-structure (deep) led to parsers such as (Hobbs
and Grishman, 1976) which produced these two lev-
els, typically in two stages. However, enthusiasm
for these two-stage parsers was eclipsed by the ad-
vent of one stage parsers with much higher accu-
racy (about 90% vs about 60%), the now-popular
treebank-based parsers including (Charniak, 2001;
Collins, 1999) and many others. Currently, many
different ?deeper? levels are being manually anno-
tated and automatically transduced, typically using
surface parsing and other processors as input. One
of the most popular, semantic role labels (annota-
tion and transducers based on the annotation) char-
acterize relations anchored by select predicate types
like verbs (Palmer et al, 2005), nouns (Meyers et
al., 2004a), discourse connectives (Miltsakaki et al,
2004) or those predicates that are part of particular
semantic frames (Baker et al, 1998). The CONLL
tasks for 2008 and 2009 (Surdeanu et al, 2008;
Hajic? et al, 2009) has focused on unifying many of
these individual efforts to produce a logical structure
for multiple parts of speech and multiple languages.
Like the CONLL shared task, we link surface lev-
els to logical levels for multiple languages. How-
ever, there are several differences: (1) The logical
structures produced automatically by our system can
be expected to be more accurate than the compara-
ble CONLL systems because our task involves pre-
dicting semantic roles with less fine-grained distinc-
tions. Our English and Japanese results were higher
than the CONLL 2009 SRL systems. Our English F-
scores range from 76.3% (spoken) to 89.9% (News):
146
the best CONLL 2009 English scores were 73.31%
(Brown) and 85.63% (WSJ). Our Japanese system
scored 90.6%: the best CONLL 2009 Japanese score
was 78.35%. Our Chinese system 74.5%, 4 points
lower than the best CONLL 2009 system (78.6%),
probably due to our system?s failings, rather than the
complexity of the task; (2) Each of the languages
in our system uses the same linguistic framework,
using the same types of relations, same analyses of
comparable constructions, etc. In one case, this re-
quired a conversion from a different framework to
our own. In contrast, the 2009 CONLL task puts
several different frameworks into one compatible in-
put format. (3) The logical structures produced by
our system typically connect all the words in the sen-
tence. While this is true for some of the CONLL
2009 languages, e.g., Czech, it is not true about
all the languages. In particular, the CONLL 2009
English and Chinese logical structures only include
noun and verb predicates.
In this paper, we will describe the GLARF frame-
work (Grammatical and Logical Representation
Framework) and a system for producing GLARF
output (Meyers et al, 2001; Meyers, 2008). GLARF
provides a logical structure for English, Chinese and
Japanese with an F-score that is within a few per-
centage points of the best parsing results for that
language. Like LFG?s (LFG) F-structure, our log-
ical structure is less fine-grained than many of the
popular semantic role labeling schemes, but also has
two main advantages over these schemes: it is more
reliable and it is more comprehensive in the sense
that it covers all parts of speech and the resulting
logical structure is a connected graph. Our approach
has proved adequate for three genetically unrelated
natural languages: English, Chinese and Japanese.
It is thus a good candidate for additional languages
with accurate parsers.
2 The GLARF framework
Our system creates a multi-tiered representation in
the GLARF framework, combining the theory un-
derlying the Penn Treebank for English (Marcus et
al., 1994) and Chinese (Xue et al, 2005) (Chom-
skian linguistics of the 1970s and 1980s) with: (2)
Relational Grammar?s graph-based way of repre-
senting ?levels? as sequences of relations; (2) Fea-
ture structures in the style of Head-Driven Phrase
Structure Grammar; and (3) The Z. Harris style goal
of attempting to regularize multiple ways of saying
the same thing into a single representation. Our
approach differs from LFG F-structure in several
ways: we have more than two levels; we have a
different set of relational labels; and finally, our ap-
proach is designed to be compatible with the Penn
Treebank framework and therefore, Penn-Treebank-
based parsers. In addition, the expansion of our the-
ory is governed more by available resources than by
the underlying theory. As our main goal is to use
our system to regularize data, we freely incorporate
any analysis that fits this goal. Over time, we have
found ways of incorporating Named Entities, Prop-
Bank, NomBank and the Penn Discourse Treebank.
Our agenda also includes incorporating the results of
other research efforts (Pustejovsky et al, 2005).
For each sentence, we generate a feature structure
(FS) representing our most complete analysis. We
distill a subset of this information into a dependency
structure governed by theoretical assumptions, e.g.,
about identifying functors of phrases. Each GLARF
dependency is between a functor and an argument,
where the functor is the head of a phrase, conjunc-
tion, complementizer, or other function word. We
have built applications that use each of these two
representations, e.g., the dependency representation
is used in (Shinyama, 2007) and the FS represen-
tation is used in (K. Parton and K. R. McKeown
and R. Coyne and M. Diab and R. Grishman and
D. Hakkani-Tu?r and M. Harper and H. Ji and W. Y.
Ma and A. Meyers and S. Stolbach and A. Sun and
G. Tu?r and W. Xu and S. Yarman, 2009).
In the dependency representation, each sentence
is a set of 23 tuples, each 23-tuple characterizing up
to three relations between two words: (1) a SUR-
FACE relation, the relation between a functor and an
argument in the parse of a sentence; (2) a LOGIC1
relation which regularizes for lexical and syntac-
tic phenomena like passive, relative clauses, deleted
subjects; and (3) a LOGIC2 relation corresponding
to relations in PropBank, NomBank, and the Penn
Discourse Treebank (PDTB). While the full output
has all this information, we will limit this paper to
a discussion of the LOGIC1 relations. Figure 1 is
a 5 tuple subset of the 23 tuple GLARF analysis of
the sentence Who was eaten by Grendel? (The full
147
L1 Surf L2 Func Arg
NIL SENT NIL Who was
PRD PRD NIL was eaten
COMP COMP ARG0 eaten by
OBJ NIL ARG1 eaten Who
NIL OBJ NIL by Grendel
SBJ NIL NIL eaten Grendel
Figure 1: 5-tuples: Who was eaten by Grendel
Who
eaten
was
by
PRD
S?OBJ
L?OBJ
ARG1
COMP
ARG0
S?SENT
L?SBJ
Grendel
Figure 2: Graph of Who was eaten by Grendel
23 tuples include unique ids and fine-grained lin-
guistic features). The fields listed are: logic1 label
(L1), surface label (Surf), logic2 label (L2), func-
tor (Func) and argument (Arg). NIL indicates that
there is no relation of that type. Figure 2 repre-
sents this as a graph. For edges with two labels,
the ARG0 or ARG1 label indicates a LOGIC2 re-
lation. Edges with an L- prefix are LOGIC1 la-
bels (the edges are curved); edges with S-prefixes
are SURFACE relations (the edges are dashed); and
other (thick) edges bear unprefixed labels represent-
ing combined SURFACE/LOGIC1 relations. Delet-
ing the dashed edges yields a LOGIC1 representa-
tion; deleting the curved edges yields a SURFACE
representation; and a LOGIC2 consists of the edges
labeled ARGO and ARG1 relations, plus the sur-
face subtrees rooted where the LOGIC2 edges ter-
minate. Taken together, a sentence?s SURFACE re-
lations form a tree; the LOGIC1 relations form a
directed acyclic graph; and the LOGIC2 relations
form directed graphs with some cycles and, due to
PDTB relations, may connect sentences to previous
ones, e.g., adverbs like however, take the previous
sentence as one of their arguments.
LOGIC1 relations (based on Relational Gram-
mar) regularize across grammatical and lexical al-
ternations. For example, subcategorized verbal ar-
guments include: SBJect, OBJect and IND-OBJ (in-
direct Object), COMPlement, PRT (Particle), PRD
(predicative complement). Other verbal modifiers
include AUXilliary, PARENthetical, ADVerbial. In
contrast, FrameNet and PropBank make finer dis-
tinctions. Both PP arguments of consulted in John
consulted with Mary about the project bear COMP
relations with the verb in GLARF, but would have
distinct labels in both PropBank and FrameNet.
Thus Semantic Role Labeling (SRL) should be more
difficult than recognizing LOGIC1 relations.
Beginning with Penn Treebank II, Penn Treebank
annotation includes Function tags, hyphenated addi-
tions to phrasal categories which indicate their func-
tion. There are several types of function tags:
? Argument Tags such as SBJ, OBJ, IO (IND-
OBJ), CLR (COMP) and PRD?These are lim-
ited to verbal relations and not all are used in
all treebanks. For example, OBJ and IO are
used in the Chinese, but not the English tree-
bank. These labels can often be directly trans-
lated into GLARF LOGIC1 relations.
? Adjunct Tags such as ADV, TMP, DIR, LOC,
MNR, PRP?These tags often translate into a
single LOGIC1 tag (ADV). However, some of
these also correspond to LOGIC1 arguments.
In particular, some DIR and MNR tags are re-
alized as LOGIC1 COMP relations (based on
dictionary entries). The fine grained seman-
tic distinctions are maintained in other features
that are part of the GLARF description.
In addition, GLARF treats Penn?s PRN phrasal
category as a relation rather than a phrasal category.
For example, given a sentence like, Banana ketchup,
the agency claims, is very nutritious, the phrase
the agency claims is analyzed as an S(entence) in
GLARF bearing a (surface) PAREN relation to the
main clause. Furthermore, the whole sentence is a
COMP of the verb claims. Since PAREN is a SUR-
FACE relation, not a LOGIC1 relation, there is no
LOGIC1 cycle as shown by the set of 5-tuples in
Figure 3? a cycle only exists if you include both
SURFACE and LOGIC1 relations in a single graph.
Another important feature of the GLARF frame-
work is transparency, a term originating from N.
148
L1 Surf L2 Func Arg
NIL SBJ ARG1 is ketchup
PRD PRD ARG2 is nutritious
SBJ NIL NIL nutritious Ketchup
ADV ADV NIL nutritious very
N-POS N-POS NIL ketchup Banana
NIL PAREN NIL is claims
SBJ SBJ ARG0 claims agency
Q-POS Q-POS NIL agency the
COMP NIL ARG1 claims is
Figure 3: 5-tuples: Banana Ketchup, the agency claims,
is very nutritious
L1 Surf L2 Func Arg
SBJ SBJ ARG0 ate and
OBJ OBJ ARG1 ate box
CONJ CONJ NIL and John
CONJ CONJ NIL and Mary
COMP COMP NIL box of
Q-POS Q-POS NIL box the
OBJ OBJ NIL of cookies
Figure 4: 5-tuples: John and Mary ate the box of cookies
Sager?s unpublished work. A relation between two
words is transparent if: the functor fails to character-
ize the selectional properties of the phrase (or sub-
graph in a Dependency Analysis), but its argument
does. For example, relations between conjunctions
(e.g., and, or, but) and their conjuncts are transparent
CONJ relations. Thus although and links together
John and Mary, it is these dependents that deter-
mine that the resulting phrase is noun-like (an NP
in phrase structure terminology) and sentient (and
thus can occur as the subject of verbs like ate). An-
other common example of transparent relations are
the relations connecting certain nouns and the prepo-
sitional objects under them, e.g., the box of cookies
is edible, because cookies are edible even though
boxes are not. These features are marked in the
NOMLEX-PLUS dictionary (Meyers et al, 2004b).
In Figure 4, we represent transparent relations, by
prefixing the LOGIC1 label with asterisks.
The above description most accurately describes
English GLARF. However, Chinese GLARF has
most of the same properties, the main exception be-
ing that PDTB arguments are not currently marked.
For Japanese, we have only a preliminary represen-
tation of LOGIC2 relations and they are not derived
from PropBank/NomBank/PDTB.
2.1 Scoring the LOGIC1 Structure
For purposes of scoring, we chose to focus on
LOGIC1 relations, our proposed high-performance
level of semantics. We scored with respect to: the
LOGIC1 relational label, the identity of the functor
and the argument, and whether the relation is trans-
parent or not. If the system output differs in any of
these respects, the relation is marked wrong. The
following sections will briefly describe each system
and present an evaluation of its results.
The answer keys for each language were created
by native speakers editing system output, as repre-
sented similarly to the examples in this paper, al-
though part of speech is included for added clar-
ity. In addition, as we attempted to evaluate logi-
cal relation (or dependency) accuracy independent
of sentence splitting. We obtained sentence divi-
sions from data providers and treebank annotation
for all the Japanese and most of the English data, but
used automatic sentence divisions for the English
BLOG data. For the Chinese, we omitted several
sentences from our evaluation set due to incorrect
sentence splits. The English and Japanese answer
keys were annotated by single native speakers ex-
pert in GLARF. The Chinese data was annotated by
several native speakers and may have been subject
to some interannotator agreement difficulties, which
we intend to resolve in future work. Currently, cor-
recting system output is the best way to create an-
swer keys due to certain ambiguities in the frame-
work, some of which we hope to incorporate into fu-
ture scoring procedures. For example, consider the
interpretation of the phrase five acres of land in Eng-
land with respect to PP attachment. The difference
in meaning between attaching the PP in England
to acres or to land is too subtle for these authors?
we have difficulty imagining situations where one
statement would be accurate and the other would
not. This ambiguity is completely predictable be-
cause acres is a transparent noun and similar ambi-
guities hold for all such cases where a transparent
noun takes a complement and is followed by a PP
attachment. We believe that a more complex scor-
ing program could account for most of these cases.
149
Similar complexities arise for coordination and sev-
eral other phenomena.
3 English GLARF
We generate English GLARF output by applying a
procedure that combines:
1. The output of the 2005 version of the Charniak
parser described in (Charniak, 2001), which
label precision and recall scores in the 85%
range. The updated version of the parser seems
to perform closer to 90% on News data and per-
form lower on other genres. That performance
would reflect reports on other versions of the
Charniak parser for which statistics are avail-
able (Foster and van Genabith, 2008).
2. Named entity (NE) tags from the JET NE sys-
tem (Ji and Grishman, 2006), which achieves
F-scores ranging 86%-91% on newswire for
both English and Chinese (depending on
Epoch). The JET system identifies seven
classes of NEs: Person, GPE, Location, Orga-
nization, Facility, Weapon and Vehicle.
3. Machine Readable dictionaries: COMLEX
(Macleod et al, 1998), NOMBANK dictio-
naries (from http://nlp.cs.nyu.edu/
meyers/nombank/) and others.
4. A sequence of hand-written rules (citations
omitted) such that: (1) the first set of rules con-
vert the Penn Treebank into a Feature Structure
representation; and (2) each rule N after the
first rule is applied to an entire Feature Struc-
ture that is the output of rule N ? 1.
For this paper, we evaluated the English output for
several different genres, all of which approximately
track parsing results for that genre. For written
genres, we chose between 40 and 50 sentences.
For speech transcripts, we chose 100 sentences?we
chose this larger number because a lot of so-called
sentences contained text with empty logical de-
scriptions, e.g., single word utterances contain no
relations between pairs of words. Each text comes
from a different genre. For NEWS text, we used 50
sentences from the aligned Japanese-English data
created as part of the JENAAD corpus (Utiyama
Genre Prec Rec F
NEWS 731815 = 89.7%
715
812 = 90.0% 89.9%
BLOG 704844 = 83.4%
704
899 = 78.3% 80.8%
LETT 392434 = 90.3%
392
449 = 87.3% 88.8%
TELE 472604 = 78.1%
472
610 = 77.4% 77.8%
NARR 732959 = 76.3%
732
964 = 75.9% 76.1%
Table 1: English Aggregate Scores
Corpus Prec Rec F Sents
NEWS 90.5% 90.8% 90.6% 50
BLOG 84.1% 79.6% 81.7% 46
LETT 93.9% 89.2% 91.4% 46
TELE 81.4% 83.2% 84.9% 103
NARR 77.1% 78.1% 79.5% 100
Table 2: English Score per Sentence
and Isahara, 2003); the web text (BLOGs) was
taken from some corpora provided by the Linguistic
Data Consortium through the GALE (http:
//projects.ldc.upenn.edu/gale/) pro-
gram; the LETTer genre (a letter from Good Will)
was taken from the ICIC Corpus of Fundraising
Texts (Indiana Center for Intercultural Communi-
cation); Finally, we chose two spoken language
transcripts: a TELEphone conversation from
the Switchboard Corpus (http://www.ldc.
upenn.edu/Catalog/readme_files/
switchboard.readme.html) and one NAR-
Rative from the Charlotte Narrative and Conversa-
tion Collection (http://newsouthvoices.
uncc.edu/cncc.php). In both cases, we
assumed perfect sentence splitting (based on Penn
Treebank annotation). The ICIC, Switchboard
and Charlotte texts that we used are part of the
Open American National Corpus (OANC), in
particular, the SIGANN shared subcorpus of the
OANC (http://nlp.cs.nyu.edu/wiki/
corpuswg/ULA-OANC-1) (Meyers et al, 2007).
Comparable work for English includes: (1) (Gab-
bard et al, 2006), a system which reproduces the
function tags of the Penn Treebank with 89% accu-
racy and empty categories (and their antecedents)
with varying accuracies ranging from 82.2% to
96.3%, excluding null complementizers, as these are
theory-internal and have no value for filling gaps.
(2) Current systems that generate LFG F-structure
150
such as (Wagner et al, 2007) which achieve an F
score of 91.1 on the F-structure PRED relations,
which are similar to our LOGIC1 relations.
4 Chinese GLARF
The Chinese GLARF program takes a Chinese
Treebank-style syntactic parse and the output of a
Chinese PropBanker (Xue, 2008) as input, and at-
tempts to determine the relations between the head
and its dependents within each constituent. It does
this by first exploiting the structural information
and detecting six broad categories of syntactic rela-
tions that hold between the head and its dependents.
These are predication, modification, complementa-
tion, coordination, auxiliary, and flat. Predication
holds at the clause level between the subject and the
predicate, where the predicate is considered to be
the head and the subject is considered to the depen-
dent. Modification can also hold mainly within NPs
and VPs, where the dependents are modifiers of the
NP head or adjuncts to the head verb. Coordination
holds almost for all phrasal categories where each
non-punctuation child within this constituent is ei-
ther conjunction or a conjunct. The head in a co-
ordination structure is underspecified and can be ei-
ther a conjunct or a conjunction depending on the
grammatical framework. Complementation holds
between a head and its complement, with the com-
plement usually being a core argument of the head.
For example, inside a PP, the preposition is the head
and the phrase or clause it takes is the dependent. An
auxiliary structure is one where the auxiliary takes
a VP as its complement. This structure is identi-
fied so that the auxiliary and the verb it modifies can
form a verb group in the GLARF framework. Flat
structures are structures where a constituent has no
meaningful internal structure, which is possible in a
small number of cases. After these six broad cate-
gories of relations are identified, more fine-grained
relation can be detected with additional information.
Figure 5 is a sample 4-tuple for a Chinese translation
of the sentence in figure 3.
For the results reported in Table 3, we used the
Harper and Huang parser described in (Harper and
Huang, Forthcoming) which can achieve F-scores
as high as 85.2%, in combination with informa-
tion about named entities from the output of the
Figure 5: Agency claims, Banana Ketchup is very have
nutrition DE.
JET Named Entity tagger for Chinese (86%-91% F-
measure as per section 3). We used the NE tags to
adjust the parts of speech and the phrasal boundaries
of named entities (we do the same with English).
As shown in Table 3, we tried two versions of the
Harper and Huang parser, one which adds function
tags to the output and one that does not. The Chinese
GLARF system scores significantly (13.9% F-score)
higher given function tagged input, than parser out-
put without function tags. Our current score is about
10 points lower than the parser score. Our initial er-
ror analysis suggests that the most common forms
of errors involve: (1) the processing of long NPs;
(2) segmentation and POS errors; (3) conjunction
scope; and (4) modifier attachment.
5 Japanese GLARF
For Japanese, we process text with the KNP parser
(Kurohashi and Nagao, 1998) and convert the output
into the GLARF framework. The KNP/Kyoto Cor-
pus framework is a Japanese-specific Dependency
framework, very different from the Penn Treebank
framework used for the other systems. Process-
ing in Japanese proceeds as follows: (1) we pro-
cess the Japanese with the Juman segmenter (Kuro-
151
Type Prec Rec F
No Function Tags Version
Aggr 8431374 = 61.4%
843
1352 = 62.4% 61.8%
Aver 62.3% 63.5% 63.6%
Function Tags Version
Aggr 10311415 = 72.9%
1031
1352 = 76.3% 74.5%
Aver 73.0% 75.3% 74.9%
Table 3: 53 Chinese Newswire Sentences: Aggregate and
Average Sentence Scores
hashi et al, 1994) and KNP parser 2.0 (Kurohashi
and Nagao, 1998), which has reported accuracy of
91.32% F score for dependency accuracy, as re-
ported in (Noro et al, 2005). As is standard in
Japanese linguistics, the KNP/Kyoto Corpus (K)
framework uses a dependency analysis that has some
features of a phrase structure analysis. In partic-
ular, the dependency relations are between bun-
setsu, small constituents which include a head word
and some number of modifiers which are typically
function words (particles, auxiliaries, etc.), but can
also be prenominal noun modifiers. Bunsetsu can
also include multiple words in the case of names.
The K framework differentiates types of dependen-
cies into: the normal head-argument variety, coor-
dination (or parallel) and apposition. We convert
the head-argument variety of dependency straight-
forwardly into a phrase consisting of the head and
all the arguments. In a similar way, appositive re-
lations could be represented using an APPOSITIVE
relation (as is currently done with English). In the
case of bunsetsu, the task is to choose a head and
label the other constituents?This is very similar to
our task of labeling and subdividing the flat noun
phrases of the English Penn Treebank. Conjunction
is a little different because the K analysis assumes
that the final conjunct is the functor, rather than a
conjunction. We automatically changed this analy-
sis to be the same as it is for English and Chinese.
When there was no actual conjunction, we created a
theory-internal NULL conjunction. The final stages
include: (1) processing conjunction and apposition,
including recognizing cases that the parser does not
recognize; (2) correcting parts of speech; (3) label-
ing all relations between arguments and heads; (4)
recognizing and labeling special constituent types
Figure 6: It is the state?s duty to protect lives and assets.
Type Prec Rec F
Aggr 764843 = 91.0%
764
840 = 90.6% 90.8%
Aver 90.7% 90.6% 90.6%
Table 4: 40 Japanese Sentences from JENAA Corpus:
Aggregate and Average Sentence Scores
such as Named Entities, double quote constituents
and number phrases (twenty one); (5) handling com-
mon idioms; and (6) processing light verb and cop-
ula constructions.
Figure 6 is a sample 4-tuple for a Japanese
sentence meaning It is the state?s duty to protect
lives and assets. Conjunction is handled as dis-
cussed above, using an invisible NULL conjunction
and transparent (asterisked) logical CONJ relations.
Copulas in all three languages take surface subjects,
which are the LOGIC1 subjects of the PRD argu-
ment of the copula. We have left out glosses for the
particles, which act solely as case markers and help
us identify the grammatical relation.
We scored Japanese GLARF on forty sentences of
the Japanese side of the JENAA data (25 of which
are parallel with the English sentences scored). Like
the English, the F score is very close to the parsing
scores achieved by the parser.
152
6 Concluding Remarks and Future Work
In this paper, we have described three systems
for generating GLARF representations automati-
cally from text, each system combines the out-
put of a parser and possibly some other processor
(segmenter, Named Entity Recognizer, PropBanker,
etc.) and creates a logical representation of the sen-
tence. Dictionaries, word lists, and various other
resources are used, in conjunction with hand writ-
ten rules. In each case, the results are very close to
parsing accuracy. These logical structures are in the
same annotation framework, using the same labeling
scheme and the same analysis for key types of con-
structions. There are several advantages to our ap-
proach over other characterizations of logical struc-
ture: (1) our representation is among the most accu-
rate and reliable; (2) our representation connects all
the words in the sentence; and (3) having the same
representation for multiple languages facilitates run-
ning the same procedures in multiple languages and
creating multilingual applications.
The English system was developed for the News
genre, specifically the Penn Treebank Wall Street
Journal Corpus. We are therefore considering
adding rules to better handle constructions that ap-
pear in other genres, but not news. The experi-
ments describe here should go a long way towards
achieving this goal. We are also considering ex-
periments with parsers tailored to particular genres
and/or parsers that add function tags (Harper et al,
2005). In addition, our current GLARF system uses
internal Propbank/NomBank rules, which have good
precision, but low recall. We expect that we achieve
better results if we incorporate the output of state
of the art SRL systems, although we would have to
conduct experiments as to whether or not we can im-
prove such results with additional rules.
We developed the English system over the course
of eight years or so. In contrast, the Chinese and
Japanese systems are newer and considerably less
time was spent developing them. Thus they cur-
rently do not represent as many regularizations. One
obstacle is that we do not currently use subcate-
gorization dictionaries for either language, while
we have several for English. In particular, these
would be helpful in predicting and filling relative
clause and others gaps. We are considering auto-
matically acquiring simple dictionaries by recording
frequently occurring argument types of verbs over
a larger corpus, e.g., along the lines of (Kawahara
and Kurohashi, 2002). In addition, existing Japanese
dictionaries such as the IPAL (monolingual) dictio-
nary (technology Promotion Agency, 1987) or previ-
ously acquired case information reported in (Kawa-
hara and Kurohashi, 2002).
Finally, we are investigating several avenues for
using this system output for Machine Translation
(MT) including: (1) aiding word alignment for other
MT system (Wang et al, 2007); and (2) aiding the
creation various MT models involving analyzed text,
e.g., (Gildea, 2004; Shen et al, 2008).
Acknowledgments
This work was supported by NSF Grant IIS-
0534700 Structure Alignment-based MT.
References
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The
Berkeley FrameNet Project. In Coling-ACL98, pages
86?90.
E. Charniak. 2001. Immediate-head parsing for language
models. In ACL 2001, pages 116?123.
N. Chomsky. 1957. Syntactic Structures. Mouton, The
Hague.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. Foster and J. van Genabith. 2008. Parser Evaluation
and the BNC: 4 Parsers and 3 Evaluation Metrics. In
LREC 2008, Marrakech, Morocco.
R. Gabbard, M. Marcus, and S. Kulick. 2006. Fully pars-
ing the penn treebank. In NAACL/HLT, pages 184?
191.
D. Gildea. 2004. Dependencies vs. Constituents for
Tree-Based Alignment. In EMNLP, Barcelona.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,
J. ?Ste?pa?nek, P. Stran?a?k, M. Surdeanu, N. Xue, and
Y. Zhang. 2009. The CoNLL-2009 shared task:
Syntactic and semantic dependencies in multiple lan-
guages. In CoNLL-2009, Boulder, Colorado, USA.
M. Harper and Z. Huang. Forthcoming. Chinese Statis-
tical Parsing. In J. Olive, editor, Global Autonomous
Language Exploitation. Publisher to be Announced.
M. Harper, B. Dorr, J. Hale, B. Roark, I. Shafran,
M. Lease, Y. Liu, M. Snover, L. Yung, A. Krasnyan-
skaya, and R. Stewart. 2005. Parsing and Spoken
153
Structural Event. Technical Report, The John-Hopkins
University, 2005 Summer Research Workshop.
Z. Harris. 1968. Mathematical Structures of Language.
Wiley-Interscience, New York.
J. R. Hobbs and R. Grishman. 1976. The Automatic
Transformational Analysis of English Sentences: An
Implementation. International Journal of Computer
Mathematics, 5:267?283.
H. Ji and R. Grishman. 2006. Analysis and Repair of
Name Tagger Errors. In COLING/ACL 2006, Sydney,
Australia.
K. Parton and K. R. McKeown and R. Coyne and M. Diab
and R. Grishman and D. Hakkani-Tu?r and M. Harper
and H. Ji and W. Y. Ma and A. Meyers and S. Stol-
bach and A. Sun and G. Tu?r and W. Xu and S. Yarman.
2009. Who, What, When, Where, Why? Comparing
Multiple Approaches to the Cross-Lingual 5W Task.
In ACL 2009.
D. Kawahara and S. Kurohashi. 2002. Fertilization
of Case Frame Dictionary for Robust Japanese Case
Analysis. In Proc. of COLING 2002.
S. Kurohashi and M. Nagao. 1998. Building a Japanese
parsed corpus while improving the parsing system. In
Proceedings of The 1st International Conference on
Language Resources & Evaluation, pages 719?724.
S. Kurohashi, T. Nakamura, Y. Matsumoto, and M. Na-
gao. 1994. Improvements of Japanese Morpholog-
ical Analyzer JUMAN. In Proc. of International
Workshop on Sharable Natural Language Resources
(SNLR), pages 22?28.
C. Macleod, R. Grishman, and A. Meyers. 1998. COM-
LEX Syntax. Computers and the Humanities, 31:459?
481.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
A. Meyers, M. Kosaka, S. Sekine, R. Grishman, and
S. Zhao. 2001. Parsing and GLARFing. In Proceed-
ings of RANLP-2001, Tzigov Chark, Bulgaria.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004a. The Nom-
Bank Project: An Interim Report. In NAACL/HLT
2004 Workshop Frontiers in Corpus Annotation,
Boston.
A. Meyers, R. Reeves, Catherine Macleod, Rachel Szeke-
ley, Veronkia Zielinska, and Brian Young. 2004b. The
Cross-Breeding of Dictionaries. In Proceedings of
LREC-2004, Lisbon, Portugal.
A. Meyers, N. Ide, L. Denoyer, and Y. Shinyama. 2007.
The shared corpora working group report. In Pro-
ceedings of The Linguistic Annotation Workshop, ACL
2007, pages 184?190, Prague, Czech Republic.
A. Meyers. 2008. Using treebank, dictionaries and
glarf to improve nombank annotation. In Proceedings
of The Linguistic Annotation Workshop, LREC 2008,
Marrakesh, Morocco.
E. Miltsakaki, A. Joshi, R. Prasad, and B. Webber. 2004.
Annotating discourse connectives and their arguments.
In A. Meyers, editor, NAACL/HLT 2004 Workshop:
Frontiers in Corpus Annotation, pages 9?16, Boston,
Massachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
T. Noro, C. Koike, T. Hashimoto, T. Tokunaga, and
Hozumi Tanaka. 2005. Evaluation of a Japanese CFG
Derived from a Syntactically Annotated corpus with
Respect to Dependency Measures. In 2005 Workshop
on Treebanks and Linguistic theories, pages 115?126.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
J. Pustejovsky, A. Meyers, M. Palmer, and M. Poe-
sio. 2005. Merging PropBank, NomBank, TimeBank,
Penn Discourse Treebank and Coreference. In ACL
2005 Workshop: Frontiers in Corpus Annotation II:
Pie in the Sky.
L. Shen, J. Xu, and R. Weischedel. 2008. A New String-
to-Dependency Machine Translation Algorithm with a
Target Dependency Language Model. In ACL 2008.
Y. Shinyama. 2007. Being Lazy and Preemptive at
Learning toward Information Extraction. Ph.D. the-
sis, NYU.
M. Surdeanu, R. Johansson, A. Meyers, Ll. Ma?rquez,
and J. Nivre. 2008. The CoNLL-2008 Shared Task
on Joint Parsing of Syntactic and Semantic Dependen-
cies. In Proceedings of the CoNLL-2008 Shared Task,
Manchester, GB.
Information technology Promotion Agency. 1987. IPA
Lexicon of the Japanese Language for Computers
IPAL (Basic Verbs). (in Japanese).
M. Utiyama and H. Isahara. 2003. Reliable Measures
for Aligning Japanese-English News Articles and Sen-
tences. In ACL-2003, pages 72?79.
J. Wagner, D. Seddah, J. Foster, and J. van Genabith.
2007. C-Structures and F-Structures for the British
National Corpus. In Proceedings of the Twelfth In-
ternational Lexical Functional Grammar Conference,
Stanford. CSLI Publications.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
EMNLP-CoNLL 2007, pages 737?745.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese TreeBank: Phrase Structure Annotation
of a Large Corpus. Natural Language Engineering,
11:207?238.
N. Xue. 2008. Labeling Chinese Predicates with Seman-
tic roles. Computational Linguistics, 34:225?255.
154
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 116?120,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Transducing Logical Relations from Automatic and Manual GLARF
Adam Meyers?, Michiko Kosaka?, Heng Ji?, Nianwen Xue?,
Mary Harper?, Ang Sun?, Wei Xu? and Shasha Liao?
? New York Univ., ?Monmouth Univ., ?Brandeis Univ,, ?City Univ. of New York, ?Johns
Hopkins Human Lang. Tech. Ctr. of Excellence & U. of Maryland, College Park
Abstract
GLARF relations are generated from tree-
bank and parses for English, Chinese and
Japanese. Our evaluation of system out-
put for these input types requires consid-
eration of multiple correct answers.1
1 Introduction
Systems, such as treebank-based parsers (Char-
niak, 2001; Collins, 1999) and semantic role la-
belers (Gildea and Jurafsky, 2002; Xue, 2008), are
trained and tested on hand-annotated data. Evalu-
ation is based on differences between system out-
put and test data. Other systems use these pro-
grams to perform tasks unrelated to the original
annotation. For example, participating systems in
CONLL (Surdeanu et al, 2008; Hajic? et al, 2009),
ACE and GALE tasks merged the results of sev-
eral processors (parsers, named entity recognizers,
etc.) not initially designed for the task at hand.
This paper discusses differences between hand-
annotated data and automatically generated data
with respect to our GLARFers, systems for gen-
erating Grammatical and Logical Representation
Framework (GLARF) for English, Chinese and
Japanese sentences. The paper describes GLARF
(Meyers et al, 2001; Meyers et al, 2009) and
GLARFers and compares GLARF produced from
treebank and parses.
2 GLARF
Figure 1 includes simplified GLARF analyses for
English, Chinese and Japanese sentences. For
each sentence, a GLARFer constructs both a Fea-
ture Structure (FS) representing a constituency
analysis and a set of 31-tuples, each representing
1Support includes: NSF IIS-0534700 & IIS-0534325
Structure Alignment-based MT; DARPA HR0011-06-C-
0023 & HR0011-06-C-0023; CUNY REP & GRTI Program.
This work does not necessarily reflect views of sponsors.
up to three dependency relations between pairs of
words. Due to space limitations, we will focus on
the 6 fields of the 31-tuple represented in Figure 1.
These include: (1) a functor (func); (2) the de-
pending argument (Arg); (3) a surface (Surf) la-
bel based on the position in the parse tree with no
regularizations; (4) a logic1 label (L
?
1) for a re-
lation that reflects grammar-based regularizations
of the surface level. This marks relations for fill-
ing gaps in relative clauses or missing infinitival
subjects, represents passives as paraphrases as ac-
tives, etc. While the general framework supports
many regularizations, the relations actually repre-
sented depends on the implemented grammar, e.g.,
our current grammar of English regularizes across
passives and relative clauses, but our grammars
of Japanese and Chinese do not currently.; (5) a
logic2 label (L2) for Chinese and English, which
represents PropBank, NomBank and Penn Dis-
course Treebank relations; and (6) Asterisks (*)
indicate transparent relations, relations where the
functor inherits semantic properties of certain spe-
cial arguments (*CONJ, *OBJ, *PRD, *COMP).
Figure 1 contains several transparent relations.
The interpretation of the *CONJ relations in the
Japanese example, include not only that the nouns
[zaisan] (assets) and [seimei] (lives) are con-
joined, but also that these two nouns, together
form the object of the Japanese verb [mamoru]
(protect). Thus, for example, semantic selection
patterns should treat these nouns as possible ob-
jects for this verb. Transparent relations may serve
to neutralize some of the problematic cases of at-
tachment ambiguity. For example, in the English
sentence A number of phrases with modifiers are
not ambiguous, there is a transparent *COMP re-
lation between numbers and of and a transpar-
ent *OBJ relation between of and phrases. Thus,
high attachment of the PP with modifiers, would
have the same interpretation as low attachment
since phrases is the underlying head of number of
116
Figure 1: GLARF 5-tuples for 3 languages
phrases. In this same example, the adverb not can
be attached to either the copula are or the pred-
icative adjective, with no discernible difference in
meaning?this factor is indicated by the transparent
designation of the relations where the copula is a
functor. Transparent features also provide us with
a simple way of handling certain function words,
such as the Chinese word De which inherits the
function of its underlying head, connecting a vari-
ety of such modifiers to head nouns (an adjective
in the Chinese example.). For conjunction cases,
the number of underlying relations would multi-
ply, e.g., Mary and John bought and sold stock
would (underlyingly) have four subject relations
derived by pairing each of the underlying subject
nouns Mary and John with each of the underlying
main predicate verbs bought and sold.
3 Automatic vs. Manual Annotation
Apart from accuracy, there are several other ways
that automatic and manual annotation differs. For
Penn-treebank (PTB) parsing, for example, most
parsers (not all) leave out function tags and empty
categories. Consistency is an important goal for
manual annotation for many reasons including: (1)
in the absence of a clear correct answer, consis-
tency helps clarify measures of annotation quality
(inter-annotator agreement scores); and (2) consis-
tent annotation is better training data for machine
learning. Thus, annotation specifications use de-
faults to ensure the consistent handling of spurious
ambiguity. For example, given a sentence like I
bought three acres of land in California, the PP in
California can be attached to either acres or land
with no difference in meaning. While annotation
guidelines may direct a human annotator to prefer,
for example, high attachment, systems output may
have other preferences, e.g., the probability that
land is modified by a PP (headed by in) versus the
probability that acres can be so modified.
Even if the manual annotation for a particular
corpus is consistent when it comes to other factors
such as tokenization or part of speech, developers
of parsers sometimes change these guidelines to
suit their needs. For example, users of the Char-
niak parser (Charniak, 2001) should add the AUX
category to the PTB parts of speech and adjust
their systems to account for the conversion of the
word ain?t into the tokens IS and n?t. Similarly, to-
kenization decisions with respect to hyphens vary
among different versions of the Penn Treebank, as
well as different parsers based on these treebanks.
Thus if a system uses multiple parsers, such differ-
ences must be accounted for. Differences that are
not important for a particular application should
be ignored (e.g., by merging alternative analyses).
For example, in the case of spurious attachment
ambiguity, a system may need to either accept both
as right answers or derive a common representa-
tion for both. Of course, many of the particular
problems that result from spurious ambiguity can
be accounted for in hind sight. Nevertheless, it
is precisely this lack of a controlled environment
which adds elements of spurious ambiguity. Us-
ing new processors or training on new treebanks
can bring new instances of spurious ambiguity.
4 Experiments and Evaluation
We ran GLARFers on both manually created tree-
banks and automatically produced parses for En-
glish, Chinese and Japanese. For each corpus, we
created one or more answer keys by correcting
117
system output. For this paper, we evaluate solely
on the logic1 relations (the second column in fig-
ure 1.) Figure 2 lists our results for all three lan-
guages, based on treebank and parser input.
As in (Meyers et al, 2009), we generated 4-
tuples consisting of the following for each depen-
dency: (A) the logic1 label (SBJ, OBJ, etc.), (B)
its transparency (True or False), (C) The functor (a
single word or a named entity); and (D) the argu-
ment (a single word or a named entity). In the case
of conjunction where there was no lexical con-
junction word, we used either punctuation (com-
mas or semi-colons) or the placeholder *NULL*.
We then corrected these results by hand to produce
the answer key?an answer was correct if all four
members of the tuple were correct and incorrect
otherwise. Table 2 provides the Precision, Recall
and F-scores for our output. The F-T columns
indicates a modified F-score derived by ignoring
the +/-Transparent distinction (resulting changes
in precision, recall and F-score are the same).
For English and Japanese, an expert native
speaking linguist corrected the output. For Chi-
nese, several native speaking computational lin-
guists shared the task. By checking compatibil-
ity of the answer keys with outputs derived from
different sources (parser, treebank), we could de-
tect errors and inconsistencies. We processed the
following corpora. English: 86 sentence article
(wsj 2300) from the Wall Street Journal PTB test
corpus (WSJ); 46 sentence letter from Good Will
(LET), the first 100 sentences of a switchboard
telephone transcript (TEL) and the first 100 sen-
tences of a narrative from the Charlotte Narra-
tive and Conversation (NAR). These samples are
taken from the PTB WSJ Corpus and the SIGANN
shared subcorpus of the OANC. The filenames are:
110CYL067, NapierDianne and sw2014. Chi-
nese: a 20 sentence sample of text from the
Penn Chinese Treebank (CTB) (Xue et al, 2005).
Japanese: 20 sentences from the Kyoto Corpus
(KYO) (Kurohashi and Nagao, 1998)
5 Running the GLARFer Programs
We use Charniak, UMD and KNP parsers (Char-
niak, 2001; Huang and Harper, 2009; Kurohashi
and Nagao, 1998), JET Named Entity tagger (Gr-
ishman et al, 2005; Ji and Grishman, 2006)
and other resources in conjunction with language-
specific GLARFers that incorporate hand-written
rules to convert output of these processors into
a final representation, including logic1 struc-
ture, the focus of this paper. English GLAR-
Fer rules use Comlex (Macleod et al, 1998a)
and the various NomBank lexicons (http://
nlp.cs.nyu.edu/meyers/nombank/) for
lexical lookup. The GLARF rules implemented
vary by language as follows. English: cor-
recting/standardizing phrase boundaries and part
of speech (POS); recognizing multiword expres-
sions; marking subconstituents; labeling rela-
tions; incorporating NEs; regularizing infiniti-
val, passives, relatives, VP deletion, predica-
tive and numerous other constructions. Chi-
nese: correcting/standardizing phrase boundaries
and POS, marking subconstituents, labeling rela-
tions; regularizing copula constructions; incorpo-
rating NEs; recognizing dates and number expres-
sions. Japanese: converting to PTB format; cor-
recting/standardizing phrase boundaries and POS;
labeling relations; processing NEs, double quote
constructions, number phrases, common idioms,
light verbs and copula constructions.
6 Discussion
Naturally, the treebank-based system out-
performed parse-based system. The Charniak
parser for English was trained on the Wall Street
Journal corpus and can achieve about 90% accu-
racy on similar corpora, but lower accuracy on
other genres. Differences between treebank and
parser results for English were higher for LET and
NAR genres than for the TEL because the system
is not currently designed to handle TEL-specific
features like disfluencies. All processors were
trained on or initially designed for news corpora.
Thus corpora out of this domain usually produce
lower results. LET was easier as it consisted
mainly of short simple sentences. In (Meyers et
al., 2009), we evaluated our results on 40 Japanese
sentences from the JENAAD corpus (Utiyama
and Isahara, 2003) and achieved a higher F-score
(90.6%) relative to the Kyoto corpus, as JENAAD
tends to have fewer long complex sentences.
By using our answer key for multiple inputs, we
discovered errors and consequently improved the
quality of the answer keys. However, at times we
were also compelled to fork the answer keys?given
multiple correct answers, we needed to allow dif-
ferent answer keys corresponding to different in-
puts. For English, these items represent approxi-
mately 2% of the answer keys (there were a total
118
Treebank Parser
ID % Prec % Rec F F-T % Prec % Rec F F-T
WSJ 12381491 = 83.0
1238
1471 = 84.2 83.6 87.1
1164
1452 = 80.2
1164
1475 = 78.9 79.5 81.8
LET 419451 = 92.9
419
454 = 92.3 92.6 93.3
390
434 = 89.9
390
454 = 85.9 87.8 87.8
TEL 478627 = 76.2
478
589 = 81.2 78.6 82.2
439
587 = 74.8
439
589 = 74.5 74.7 77.4
NAR 8171013 = 80.7
817
973 =84.0 82.3 84.1
724
957 = 75.7
724
969 = 74.7 75.2 76.1
CTB 351400 = 87.8
351
394 = 89.1 88.4 88.7
352
403 = 87.3
352
438 = 80.4 83.7 83.7
KYO 525575 = 91.3
525
577 = 91.0 91.1 91.1
493
581 = 84.9
493
572 = 86.2 85.5 87.8
Figure 2: Logic1 Scores
Figure 3: Examples of Answer Key Divergences
of 74 4-tuples out of a total of 3487). Figure 3 lists
examples of answer key divergences that we have
found: (1) alternative tokenizations; (2) spurious
differences in attachment and conjunction scope;
and (3) ambiguities specific to our framework.
Examples 1 and 2 reflect different treatments of
hyphenation and contractions in treebank specifi-
cations over time. Parsers trained on different tree-
banks will either keep hyphenated words together
or separate more words at hyphens. The Treebank
treatment of can?t regularizes so that (can need
not be differentiated from ca), whereas the parser
treatment makes maintaining character offsets eas-
ier. In example 3, the Japanese parser recognizes
a single word whereas the treebank divides it into
a prefix plus stem. Example 4 is a case of differ-
ences in character encoding (zero).
Example 5 is a common case of spurious attach-
ment ambiguity for English, where a transparent
noun takes an of PP complement?nouns such as
form, variety and thousands bear the feature trans-
parent in the NOMLEX-PLUS dictionary (a Nom-
Bank dictionary based on NOMLEX (Macleod et
al., 1998b)). The relative clause attaches either
to the noun thousands or people and, therefore,
the subject gap of the relative is filled by either
thousands or people. This ambiguity is spurious
since there is no meaningful distinction between
these two attachments. Example 6 is a case of
attachment ambiguity due to a support construc-
tion (Meyers et al, 2004). The recipient of the
gift will be Goodwill regardless of whether the
PP is attached to give or gift. Thus there is not
much sense in marking one attachment more cor-
rect than the other. Example 7 is a case of conjunc-
tion ambiguity?the context does not make it clear
whether or not the pearls are part of a necklace or
just the beads are. The distinction is of little con-
sequence to the understanding of the narrative.
Example 8 is a case in which our grammar han-
dles a case ambiguously: the prenominal adjective
can be analyzed either as a simple noun plus ad-
jective phrase meaning various businesses or as a
noun plus relative clause meaning businesses that
are varied. Example 9 is a common case in Chi-
nese where the verb/noun distinction, while un-
clear, is not crucial to the meaning of the phrase ?
under either interpretation, 5 billion was exported.
7 Concluding Remarks
We have discussed challenges of automatic an-
notation when transducers of other annotation
schemata are used as input. Models underly-
ing different transducers approximate the origi-
nal annotation in different ways, as do transduc-
ers trained on different corpora. We have found
it necessary to allow for multiple correct answers,
due to such differences, as well as, genuine and
spurious ambiguities. In the future, we intend to
investigate automatic ways of identifying and han-
dling spurious ambiguities which are predictable,
including examples like 5,6 and 7 in figure 3 in-
volving transparent functors.
119
References
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In ACL 2001, pages 116?123.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
D. Gildea and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics,
28:245?288.
R. Grishman, D. Westbrook, and A. Meyers. 2005.
Nyu?s english ace 2005 system description. In ACE
2005 Evaluation Workshop.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre,
S. Pado?, J. ?Ste?pa?nek, P. Stran?a?k, M. Surdeanu,
N. Xue, and Y. Zhang. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In CoNLL-2009, Boulder, Col-
orado, USA.
Z. Huang and M. Harper. 2009. Self-training PCFG
Grammars with Latent Annotations across Lan-
guages. In EMNLP 2009.
H. Ji and R. Grishman. 2006. Analysis and Repair of
Name Tagger Errors. In COLING/ACL 2006, Syd-
ney, Australia.
S. Kurohashi and M. Nagao. 1998. Building a
Japanese parsed corpus while improving the pars-
ing system. In Proceedings of The 1st International
Conference on Language Resources & Evaluation,
pages 719?724.
C. Macleod, R. Grishman, and A. Meyers. 1998a.
COMLEX Syntax. Computers and the Humanities,
31:459?481.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and
R. Reeves. 1998b. Nomlex: A lexicon of nominal-
izations. In Proceedings of Euralex98.
A. Meyers, M. Kosaka, S. Sekine, R. Grishman, and
S. Zhao. 2001. Parsing and GLARFing. In Pro-
ceedings of RANLP-2001, Tzigov Chark, Bulgaria.
A. Meyers, R. Reeves, and Catherine Macleod. 2004.
NP-External Arguments: A Study of Argument
Sharing in English. In The ACL 2004 Workshop
on Multiword Expressions: Integrating Processing,
Barcelona, Spain.
A. Meyers, M. Kosaka, N. Xue, H. Ji, A. Sun, S. Liao,
and W. Xu. 2009. Automatic Recognition of Log-
ical Relations for English, Chinese and Japanese in
the GLARF Framework. In SEW-2009 at NAACL-
HLT-2009.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma?rquez,
and J. Nivre. 2008. The CoNLL-2008 Shared Task
on Joint Parsing of Syntactic and Semantic Depen-
dencies. In Proceedings of the CoNLL-2008 Shared
Task, Manchester, GB.
M. Utiyama and H. Isahara. 2003. Reliable Mea-
sures for Aligning Japanese-English News Articles
and Sentences. In ACL-2003, pages 72?79.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese Treebank: Phrase Structure Annota-
tion of a Large Corpus. Natural Language Engi-
neering.
N. Xue. 2008. Labeling Chinese Predicates with Se-
mantic roles. Computational Linguistics, 34:225?
255.
120
Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 5?12,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
  
Merging PropBank, NomBank, TimeBank, Penn Discourse Treebank and Coreference 
James Pustejovsky, Adam Meyers, Martha Palmer, Massimo Poesio 
 
Abstract 
Many recent annotation efforts for English 
have focused on pieces of the larger problem 
of semantic annotation, rather than initially 
producing a single unified representation. 
This paper discusses the issues involved in 
merging four of these efforts into a unified 
linguistic structure: PropBank, NomBank, the 
Discourse Treebank and Coreference 
Annotation undertaken at the University of 
Essex. We discuss resolving overlapping and 
conflicting annotation as well as how the 
various annotation schemes can reinforce 
each other to produce a representation that is 
greater than the sum of its parts. 
 
1. Introduction 
 
The creation of the Penn Treebank (Marcus et al 
1993) and the word sense-annotated SEMCOR 
(Fellbaum, 1997) have shown how even limited 
amounts of annotated data can result in major 
improvements in complex natural language 
understanding systems. These annotated corpora 
have led to high-level improvements for parsing 
and word sense disambiguation (WSD), on the 
same scale as previously occurred for Part of 
Speech tagging by the annotation of the Brown 
corpus and, more recently, the British National 
Corpus (BNC) (Burnard, 2000). However, the 
creation of semantically annotated corpora has 
lagged dramatically behind the creation of other 
linguistic resources: in part due to the perceived 
cost, in part due to an assumed lack of theoretical 
agreement on basic semantic judgments, in part, 
finally, due to the understandable unwillingness 
of  research groups to get involved in such an 
undertaking. As a result, the need for such 
resources has become urgent.   
 
Many recent annotation efforts for English have 
focused on pieces of the larger problem of 
semantic annotation, rather than producing a 
single unified representation like Head-driven 
Phrase Structure Grammar (Pollard and Sag 
1994) or the Prague Dependency Tecto-
gramatical Representation (Hajicova & Kucer-
ova, 2002). PropBank (Palmer et al 2005) 
annotates predicate argument structure anchored 
by verbs. NomBank (Meyers, et. al., 2004a) 
annotates predicate argument structure anchored 
by nouns.  TimeBank (Pustejovsky et al 2003) 
annotates the temporal features of propositions 
and the temporal relations between propositions. 
The Penn Discourse Treebank (Miltsakaki et al
2004a/b) treats discourse connectives as 
predicates and the sentences being joined as 
arguments. Researchers at Essex were 
responsible for the coreference markup scheme 
developed in MATE (Poesio et al 1999; Poesio, 
2004a) and have annotated corpora using this 
scheme including a subset of the Penn Treebank 
(Poesio and Vieira, 1998), and the GNOME 
corpus (Poesio, 2004a).  This paper discusses the 
issues involved in creating a Unified Linguistic 
Annotation (ULA) by merging annotation of 
examples using the schemata from these efforts. 
Crucially, all individual annotations can be kept 
separate in order to make it easy to produce 
alternative annotations of a specific type of 
semantic information without need to modify the 
annotation at the other levels. Embarking on 
separate annotation efforts has the advantage of 
allowing researchers to focus on the difficult 
issues in each area of semantic annotation and 
the disadvantage of inducing a certain amount of 
tunnel vision or task-centricity ? annotators 
working on a narrow task tend to see all 
phenomena in light of the task they are working 
on, ignoring other factors. However, merging 
these annotation efforts allows these biases to be 
dealt with. The result, we believe, could be a 
more detailed semantic account than possible if 
the ULA had been the initial annotation effort 
rather than the result of merging. 
 
There is a growing community consensus that 
general annotation, relying on linguistic cues, 
and in particular lexical cues, will produce an 
enduring resource that is useful, replicable and 
portable.  We provide the beginnings of one such 
level derived from several distinct annotation 
efforts. This level could provide the foundation 
for a major advance in our ability to 
automatically extract salient relationships from 
text. This will in turn facilitate breakthroughs in 
message understanding, machine translation, fact 
retrieval, and information retrieval. 
 
2. The Component Annotation Schemata 
 
We describe below existing independent 
annotation efforts, each one of which is focused 
on a specific aspect of the semantic 
representation task: semantic role labeling, 
5
  
coreference, discourse relations, temporal 
relations, etc.  They have reached a level of 
maturity that warrants a concerted attempt to 
merge them into a single, unified representation, 
ULA.  There are several technical and theoretical 
issues that will need to be resolved in order to 
bring these different layers together seamlessly.  
Most of these approaches have annotated the 
same type of data, Wall Street Journal text, so it 
is also important to demonstrate that the 
annotation can be extended to other genres such 
as spoken language.  The demonstration of 
success for the extensions would be the training 
of accurate statistical semantic taggers. 
 
PropBank: The Penn Proposition Bank focuses 
on the argument structure of verbs, and provides 
a corpus annotated with semantic roles, 
including participants traditionally viewed as 
arguments and adjuncts.  An important goal is to 
provide consistent semantic role labels across 
different syntactic realizations of the same verb, 
as in the window in [ARG0 John] broke [ARG1 
the window] and [ARG1 The window] broke. 
Arg0 and Arg1 are used rather than the more 
traditional Agent and Patient to keep the 
annotation as theory-neutral as possible, and to 
facilitate mapping to richer representations.  The 
1M word Penn Treebank II Wall Street Journal 
corpus has been successfully annotated with 
semantic argument structures for verbs and is 
now available via the Penn Linguistic Data 
Consortium as PropBank I (Palmer, et. al., 2005).   
Coarse-grained sense tags, based on groupings of 
WordNet senses, are being added, as well as 
links from the argument labels in the Frames 
Files to FrameNet frame elements.  There are 
close parallels to other semantic role labeling 
projects, such as FrameNet (Baker, et. al., 1998; 
Fillmore & Atkins, 1998; Fillmore & Baker, 
2001), Salsa (Ellsworth, et.al, 2004), Prague 
Tectogrammatics (Hajicova & Kucerova, 2002) 
and IAMTC, (Helmreich, et. al., 2004) 
 
NomBank: The NYU NomBank project can be 
considered part of the larger PropBank effort and 
is designed to provide argument structure for 
instances of about 5000 common nouns in the 
Penn Treebank II corpus (Meyers, et. al., 2004a).  
PropBank argument types and related verb 
Frames Files are used to provide a commonality 
of annotation.  This enables the development of 
systems that can recognize regularizations of 
lexically and syntactically related sentence 
structures, whether they occur as verb phrases or 
noun phrases. For example, given an IE system 
tuned to a hiring scenario (MUC-6, 1995), 
NomBank and PropBank annotation facilitate  
generalization over patterns. PropBank and 
NomBank would both support a single IE pattern 
stating that the object (ARG1) of appoint is John 
and the subject (ARG0) is IBM, allowing a 
system to detect that IBM hired John from each 
of the following strings: IBM appointed John, 
John was appointed by IBM, IBM's appointment 
of John, the appointment of John by IBM and 
John is the current IBM appointee.  
 
Coreference: Coreference involves the detection 
of subsequent mentions of invoked entities, as in 
George Bush,? he?.  Researchers at Essex (UK) 
were responsible for the coreference markup 
scheme developed in MATE (Poesio et al 1999; 
Poesio, 2004a), partially implemented in the 
annotation tool MMAX and now proposed as an 
ISO standard; and have been responsible for the 
creation of two small, but commonly used 
anaphorically annotated corpora ? the Vieira / 
Poesio subset of the Penn Treebank (Poesio and 
Vieira, 1998), and the GNOME corpus (Poesio, 
2004a).   Parallel coreference annotation efforts 
funded by ACE have resulted in similar 
guidelines, exemplified by BBN?s recent 
annotation of Named Entities, common nouns 
and pronouns.   These two approaches provide a 
suitable springboard for an attempt at achieving a 
community consensus on coreference. 
 
Discourse Treebank:  The Penn Discourse 
Treebank (PDTB) (Miltsakaki et al2004a/b) is 
based on the idea that discourse connectives are 
predicates with associated argument structure 
(for details see (Miltsakaki et al2004a, 
Miltsakaki et al2004b). The long-range goal is 
to develop a large scale and reliably annotated 
corpus that will encode coherence relations 
associated with discourse connectives, including 
their argument structure and anaphoric links, 
thus exposing a clearly defined level of discourse 
structure and supporting the extraction of a range 
of inferences associated with discourse 
connectives. This annotation references the Penn 
Treebank annotations as well as PropBank, and 
currently only considers Wall Street Journal text. 
 
TimeBank: The Brandeis TimeBank corpus, 
funded by ARDA, focuses on the annotation of 
all major aspects in natural language text 
associated with temporal and event information 
(Day, et al 2003, Pustejovsky, et al 2004). 
Specifically, this involves three areas of the 
annotation: temporal expressions, event-denoting 
6
  
expressions, and the links that express either an 
anchoring of an event to a time or an ordering of 
one event relative to another. Identifying events 
and their temporal anchorings is a critical aspect  
of reasoning, and without a robust ability to 
identify and extract events and their temporal 
anchoring from a text, the real aboutness of the 
article can be missed.  The core of TimeBank is a 
set of 200 news reports documents, consisting of 
WSJ, DUC, and ACE articles, each annotated to 
TimeML 1.2 specification. It is currently being 
extended to AQUAINT articles. The corpus is 
available from the timeml.org website. 
 
3. Unifying Linguistic Annotations 
  
Since September, 2004, researchers representing 
several different sites and annotation projects 
have begun collaborating to produce a detailed 
semantic annotation of two difficult sentences. 
These researchers aim to produce a single unified 
representation with some consensus from the 
NLP community. This effort has given rise to 
both a listserv email list and this workshop: 
http://nlp.cs.nyu.edu/meyers/pie-in-the-sky.html, 
http://nlp.cs.nyu.edu/meyers/frontiers/2005.html 
The merging operations discussed here would 
seem crucial to the furthering of this effort. 
 
3.1 The Initial Pie in the Sky Example 
 
The following two consecutive sentences have 
been annotated for Pie in the Sky.  
 
Two Sentences From ACE Corpus File 
NBC20001019.1830.0181 
 
? but Yemen's president says the FBI has told 
him the explosive material could only have 
come from the U.S., Israel or two Arab 
countries. 
? and to a former federal bomb investigator, 
that description suggests a powerful 
military-style plastic explosive c-4 that can 
be cut or molded into different shapes. 
 
Although the full Pie-in-the-Sky analysis 
includes information from many different 
annotation projects, the Dependency Structure in 
Figure 1 includes only those components that 
relate to PropBank, NomBank, Discourse 
annotation, coreference and TimeBank. Several 
parts of this representation require further 
explanation. Most of these are signified by the 
special arcs, arc labels, and nodes. Dashed lines 
represent transparent arcs, such as the transparent 
dependency between the argument (ARG1) of 
modal can and the or. Or is transparent in that it 
allows this dependency to pass through it to cut 
and mold. There are two small arc loops -- 
investigator is its own ARG0 and description is 
its own ARG1. Investigator is a relational noun 
in NomBank. There is assumed to be an 
underlying relation between the Investigator 
(ARG0), the beneficiary or employer (the ARG2) 
and the item investigated (ARG1). Similarly, 
description acts as its own ARG1 (the thing 
described). There are four special coreference arc 
labels: ARG0-CF, ARG-ANAPH, EVENT-
ANAPH and ARG1-SBJ-CF. At the target of 
these arcs are pointers referring to phrases from 
the previous sentence or previous discourse. The 
first three of these labels are on arcs with the 
noun description as their source. The ARG0-CF 
label indicates that the phrase Yemen's president 
(**1**) is the ARG0, the one who is doing the 
describing. The EVENT-ANAPH label points to 
a previous mention of the describing event, 
namely the clause: The FBI told him the 
explosive material? (**3**). However, as noted 
above, the NP headed by description represents 
the thing described in addition to the action. The 
ARG-ANAPH label points to the thing that the 
FBI told him the explosive material can only 
come from ? (**2**). The ARG1-SBJ-CF label 
links the NP from the discourse what the bomb 
was made from as the subject with the NP 
headed by explosive as its predicate, much the 
same as it would in a copular construction such 
as: What the bomb was made from is the 
explosive C-4. Similarly, the arc ARG1-APP 
marks C-4 as an apposite, also predicated to the 
NP headed by explosive. Finally, the thick arcs 
labeled SLINK-MOD represent TimeML SLINK 
relations between eventuality variables, i.e.,  the 
cut and molded events are modally subordinate 
to the suggests proposition. The merged 
representation aims to be compatible with the 
projects from which it derives, each of which 
analyzes a different aspect of linguistic analysis. 
Indeed most of the dependency labels are based 
on the annotation schemes of those projects. 
 
We have also provided the individual PropBank, 
NomBank and TimeBank annotations below in 
textual form, in order to highlight potential 
points of interaction. 
 
PropBank:  and [Arg2 to a former federal bomb 
investigator], [Arg0 that description]  
[Rel_suggest.01 suggests]  [Arg1 [Arg1 a powerful 
military-style plastic explosive c-4] that 
7
  
 [ArgM-MOD can] be [Rel_cut.01 cut] or  [Rel_mold.01 
molded] [ArgM-RESULT into different shapes]]. 
 
NomBank: and to a former [Arg2 federal] [Arg1 
bomb] [Rel investigator], that description 
suggests a powerful [Arg2 military] - [Rel style] 
plastic [Arg1 explosive] c-4 that can be cut 
or molded into different shapes. 
 
TimeML: and to a former federal bomb 
investigator, that description [Event = ei1 
suggests]  a powerful military-style plastic 
explosive c-4 that  can be [Event = ei2 modal=?can? cut] 
or  [Event = ei3 modal=?can? molded]  into different 
shapes. <SLINK eventInstanceID = ei1 
subordinatedEventID = ei2 relType = ?Modal?/> 
<SLINK eventInstanceID = ei1 
subordinatedEventID = ei3 relType = ?Modal?/> 
 
  
Figure 1. Dependency Analysis of Sentence 2  
 
Note that the subordinating Events indicated by 
the TimeML SLINKS refer to the predicate 
argument structures labeled by PropBank, and 
that the ArgM-MODal also labeled by PropBank 
contains modality information also crucial to the 
SLINKS. While the grammatical modal on cut 
and mold is captured as an attribute value on the 
event tag, the governing event predicate suggest 
introduces a modal subordination to its internal 
argument, along with its relative clause. While 
this markup is possible in TimeML, it is difficult 
to standardize (or automate, algorithmically) 
since arguments are not marked up unless they 
are event denoting.  
 
3.2 A More Complex Example 
 
To better illustrate the interaction between 
annotation levels, and the importance of merging 
information resident in one level but not 
necessarily in another, consider the sentence 
below which has more complex temporal 
properties than the Pie-in-the-Sky sentences and 
its dependency analysis (Figure 2). 
 
 According to reports, sea trials for a patrol boat 
developed by Kazakhstan are being conducted 
and the formal launch is planned for the 
beginning of April this year.  
 
 
Figure 2.  Dependency Analysis of a Sentence 
with Interesting Temporal Properties 
 
The graph above incorporates these distinct 
annotations into a merged representation, much 
like the previous analysis. This sentence has 
more TimeML annotation than the previous 
sentence.  Note the loops of arcs which show that 
According to plays two roles in the sentence: (1) 
it heads a constituent that is the ARGM-ADV of 
the verbs conducted and planned; (2) it indicates 
that the information in this entire sentence is 
attributed to the reports. This loop is problematic 
in some sense because the adverbial appears to 
modify a constituent that includes itself. In 
actuality, however, one would expect that the 
ARGM-ADV role modifies the sentence minus 
the adverbial, the constituent that you would get 
if you ignore the transparent arc from ARGM-
8
  
ADV to the rest of the sentence.  Alternatively, a 
merging decision may elect to delete the ARGM-
ADV arcs, once the more specific predicate 
argument structure of the sentence adverbial 
annotation is available. 
 
The PropBank annotation for this sentence 
would label arguments for develop, conduct and 
plan, as given below. 
 
 [ArgM-ADV According to reports], [Arg1sea trials for  
[Arg1 a patrol boat] [Rel_develop.02 developed] [Arg0 
by Kazakhstan]] are being  
[Rel_conduct.01 conducted]  and [Arg1 the formal 
launch] is [Rel_plan.01 planned]  
[ArgM-TMP for the beginning of April this year].  
 
NomBank would add arguments for report, trial, 
launch and beginning as follows: 
 
 According to [Rel_report.01 reports], [Arg1 [ArgM-LOC 
sea [Rel_trial.01 trials] [Arg1 for [Arg1-CF_launch.01 a 
patrol boat] developed by Kazakhstan] are being 
conducted and the [ArgM-MNR formal] [Rel_launch.01 
launch] is planned for the [[REL_beginning.01 
beginning] [ARG1 of April this year]].  
 
TimeML, however, focuses on the anchoring of 
events to explicit temporal expressions (or 
document creation dates) through TLINKs, as 
well as subordinating relations, such as those 
introduced by modals, intensional predicates, 
and other event-selecting predicates, through 
SLINKs. For discussion, only part of the 
complete annotation is shown below.  
  
According to [Event = ei1  reports], sea [Event = ei3  
trials] for a boat [Event = ei4  developed]  by 
Kazakhstan are being [Event = ei5  conducted] and 
the formal [Event = ei6  launch] 
 is  [Event = ei7  planned] for the [Timex3= t1  beginning 
of April] [Timex3= t2 this year]. 
<SLINK eventID=?ei1? subordinatedEvent=?ei5, 
ei7? relType=EVIDENTIAL/> 
<TLINK eventID=?ei4? relatedToEvent =?ei3? 
relType=BEFORE/> 
<TLINK eventID=?ei6? relatedToTime=?t1? 
relType=IS_INCLUDED /> 
<SLINK eventID=?ei7? 
subordinatedEvent=?ei6? relType=?MODAL?/> 
<TLINK eventID=?ei5? relatedToEvent=?ei3? 
relType=IDENTITY/> 
 
Predicates such as plan and nominals such as 
report are lexically encoded to introduce 
SLINKs with a specific semantic relation, in this 
case, a ?MODAL? relType,. This effectively 
introduces an intensional context over the 
subordinated events. 
 
These examples illustrate the type of semantic 
representation we are trying to achieve.  It is 
clear that our various layers already capture 
many of the intended relationships, but they do 
not do so in a unified, coherent fashion.  Our 
goal is to develop both a framework and a 
process for annotation that allows the individual 
pieces to be automatically assembled into a 
coherent whole.   
 
4.0 Merging Annotations  
 
4.1 First Order Merging of Annotation 
We begin by discussing issues that arise in 
defining a single format for a merged 
representation of PropBank, NomBank and 
Coreference, the core predicate argument 
structures and  referents for the arguments.   One 
possible representation format would be to 
convert each annotation into features and values 
to be added to a larger feature structure. 1 The 
resulting feature structure would combine stand 
alone and offset annotation ? it would include 
actual words and features from the text as well as 
special features that point to the actual text 
(character offsets) and, perhaps, syntactic trees 
(offsets along the lines of PropBank/NomBank). 
Alternative global annotation schemes include 
annotation graphs (Cieri & Bird, 2001), and 
MATE (Carletta, et. al., 1999).  There are many 
areas in which the boundaries between these 
annotations have not been clearly defined, such 
as the treatment of support constructions and 
light verbs, as discussed below.  Determining the 
most suitable format for the merged 
representation should be a top priority. 
 
4.2 Resolving Annotation Overlap 
There are many possible interactions between 
different types of annotation: aspectual verbs 
have argument labels in PropBank, but are also 
important roles for temporal relations.  Support 
                                                 
 
1 The Feature Structure has many advantages as a target 
representation including: (1) it is easy to add lots of detailed 
features; and (2) the mathematical properties of Feature 
Structures are well understood, i.e., there are well-defined 
rule-writing languages, subsumption and unification 
relations, etc. defined for Feature Structures (Carpenter, 
1992) The downside is that a very informative Feature 
Structure is difficult for a human to read.  
 
9
  
constructions also have argument labels, and the 
question arises as to whether these should be 
associated with the support verb or the 
predicative nominal.  Given the sentence They 
gave the chefs a standing ovation, a PropBank 
component will assign role labels to arguments 
of give; a NomBank component will assign 
argument structure to ovation that labels the 
same participants. If the representations are 
equivalent, the question arises as to which of 
them (or both) should be included in the merged 
representation. The following graph  (Figure 3) 
is a combined PropBank and NomBank analysis 
of this sentence. "They" is the ARG0 of both 
"give" and "ovation"; "the chefs" is the ARG2 of 
"give", but the "ARG1" of ovation; "ovation" is 
the ARG1 of "give" and "give" is a support verb 
for "ovation". For this case, a reasonable choice 
might be to preserve the argument structure from 
both NomBank and PropBank, and to do the 
same for other predicative nominals that have 
give (or receive, obtain, request?) as a support 
verb, e.g., (give a kiss/hug/squeeze, give a 
lecture/speech, give a promotion, etc.).   For 
other support constructions, such as take a walk, 
have a headache and make a mistake, the noun is 
really the main predicate and it is questionable 
whether the verbal argument structure carries  
gave
chefsthe
They
a ovationstanding
NP
NP
S
ARG0
REL
ARG2
ARG1
NP
ARG1 REL
ARG0SUPPORT
 
Figure 3. Merged PropBank/NomBank representation 
of They gave the chefs a standing ovation. 
much information, e.g., there are no selection 
restrictions between light verbs and their subject 
(ARG0) -- these are inherited from the noun. 
Thus make a mistake selects a different type of 
subject than make a gain, e.g., people and 
organizations make mistakes, but stock prices 
make gains. For these constructions, the merged 
representation might not need to include the 
(ARG0) relation between the subject of the 
sentence and make, and future propbanking 
efforts might do well to ignore the shared 
arguments of such instances and leave them for 
NomBank. However, the merged representation 
would inherit PropBank?s annotation of some 
other light verb features including: negation, e.g., 
They did not take a walk; modality, e.g., They 
might take a walk; and sentence adverbials, e.g., 
They probably will take a walk. 
 
4.3 Resolving Annotation Conflicts 
Interactions between linguistic phenomena can 
aid in quality control, and conflicts found during 
the deliberate merging of different annotations 
provides an opportunity to correct and fine-tune 
the original layers. For example, predicate 
argument structure (PropBank and NomBank) 
annotation sometimes assumes different 
constituent structure than the Penn Treebank. We 
have noticed some tendencies that help resolve 
these conflicts, e.g., prenominal noun 
constituents as in Indianapolis 500, which forms 
a single argument in NomBank, is correctly 
predicted to be a constituent, even though the 
Penn Treebank II assumes a flatter structure.  
 
Similarly, idioms and multiword expressions 
often cause problems for both PropBank and 
NomBank. PropBank annotators tend to view 
argument structure in terms of verbs and 
NomBank annotators tend to view argument 
structure in terms of nouns. Thus many examples 
that, perhaps, should be viewed as idioms are 
viewed as special senses of either verbs or nouns. 
Having idioms detected and marked before 
propbanking and nombanking could greatly 
improve efficiency.   
 
Annotation accuracy is often evaluated in terms 
of inter-annotation consistency. Task definitions 
may need to err on the side of being more 
inclusive in order to simplify the annotators task. 
For example, the NomBank project assumes the 
following definition of a support verb (Meyers, 
et.al., 2004b):  ?? a verb which takes at least 
two arguments NP1 and XP2 such that XP2 is an 
argument of the head of NP1. For example, in 
John took a walk, a support verb (took) shares 
one of its arguments (John) with the head of its 
other argument (walk).? The easiest way to 
apply this definition is without exception, so it 
will include idiomatic expressions such as keep 
tabs on, take place, pull strings. Indeed, the 
dividing line between support constructions and 
idioms is difficult to draw (Meyers 2004b).   
PropBank annotators are also quite comfortable 
with associating general meanings to the main 
verbs of idiomatic expressions and labeling their 
10
  
argument roles, as in cases like bring home the 
bacon and mince words with. Since idioms often 
have interpretations that are metaphorical 
extensions of their literal meaning, this is not 
necessarily incorrect.  It may be helpful to have 
the literal dependencies and the idiomatic 
reading both represented. The fact that both 
types of meaning are available is evidenced by 
jokes, irony, and puns.  
 
With respect to idioms and light verbs, TimeML 
can be viewed as a mediator between PropBank 
and NomBank. In TimeML, light verbs and the 
nominalizations accompanying them are marked 
with two separate EVENT tags. This guarantees 
an annotation independent of textual linearity 
and therefore ensures a parallel treatment for 
different textual configurations. In (a) the light 
verb construction "make an allusion" is 
constituted of a verb and an NP headed by an 
event-denoting noun, whereas in (b) the nominal 
precedes a VP, which in addition contains a 
second N:  
(a) Max [made an allusion] to the crime.  
(b) Several anti-war [demonstrations have taken 
place] around the globe. 
Both verbal and nominal heads are tagged 
because they both contribute relevant 
information to characterizing the nature of the 
event. The nominal element plays a role in the 
more semantically based task of event 
classification. On the other hand, the information 
in the verbal component is important at two 
different levels: it provides the grammatical 
features typically associated with verbal 
morphology, such as tense and aspect, and at the 
same time it may help in disambiguating cases 
like take/give a class, make/take a phone call. 
The two tagged events are marked as identical by 
a TLINK introduced for that purpose. The 
TimeML annotation for the example in (a) is 
provided below.  
Max [Event = ei1  made] an [Event = ei2  allusion] to 
the crime.  
<TLINK eventID="ei1"relatedToEvent="ei2" 
relType=IDENTITY> 
Some cases of support in NomBank could also 
be annotated as "bridging" anaphora. Consider 
the sentence: The pieces make up the whole. 
It is unclear whether make up is a support verb 
linking whole as the ARG1 of pieces or if pieces 
is linked to whole by bridging anaphora.  
There are also clearer cases. In Nastase, a rival 
player defeated Jimmy Connors in the third 
round, the word rival and Jimmy Connors are 
clearly linked by bridging. However, a wayward 
NomBank annotator might construct a support 
chain (player + defeated) to link rival with its 
ARG1 Jimmy Connors.  In such a case, a 
merging of annotation could reveal annotation 
errors. In contrast, a NomBank annotator would 
be correct in linking John as an argument of walk 
in John took a series of walks (the support chain 
took + series consists of a support verb and a 
transparent noun), but this may not be obvious to 
the non-NomBanker. Thus the merging of 
annotation may result in the more consistent 
specifications for all.  
 
In our view, this process of annotating all layers 
of information and then merging them in a 
supervised manner, taking note of the conflicts, 
is a necessary prerequisite to defining more 
clearly the boundaries between the different 
types of annotation and determining how they 
should fit together.  Other areas of annotation 
interaction include: (1) NomBank  and 
Coreference, e.g. deriving that John teaches 
Mary from John is Mary's teacher involves: (a) 
recognizing that teacher is an argument 
nominalization such that the teacher is the ARG0 
of teach (the one who teaches); and (b) marking 
John and teacher as being linked by predication 
(in this case, an instance of type coreference); 
and (2) Time and Modality -  when a fact used to 
be true, there are two time components: one in 
which the fact is true and one in which it is false. 
Clearly more areas of interaction will emerge as 
more annotation becomes available and as the 
merging of annotation proceeds.  
 
5. Summary 
 
We proposed a way of taking advantage of the 
current practice of separating aspects of semantic 
analysis of text into small manageable pieces. 
We propose merging these pieces, initially in a 
careful, supervised way, and hypothesize that the 
result could be a more detailed semantic analysis 
than was previously available. This paper 
discusses some of the reasons that the merging 
process should be supervised. We primarily gave 
examples involving the interaction of PropBank, 
NomBank and TimeML. However, as the 
merging process continues, we anticipate other 
conflicts that will require resolution. 
 
References 
 
C. F. Baker, F. Collin, C. J. Fillmore, and J. B.  
Lowe (1998), The Berkeley FrameNet 
project. In Proc. of COLING/ACL-98,  86--90 
11
  
O. Babko-Malaya, M. Palmer, X. Nianwen, S.  
Kulick, A. Joshi (2004), Propbank II, 
Delving Deeper, In Proc.  of HLT-NAACL 
Workshop: Frontiers in Corpus Annotation. 
R. Carpenter (1992), The Logic of Typed  
Feature Structures. Cambridge Univ. Press. 
J. Carletta and A. Isard (1999), The MATE  
Annotation Workbench: User Requirements. 
In Proc. of the ACL Workshop: Towards 
Standards and Tools for Discourse Tagging. 
Univ. of Maryland, 11-17 
C. Cieri and S. Bird (2001), Annotation Graphs  
and Servers and Multi-Modal Resources: 
Infrastructure for  Interdisciplinary Education, 
Research and Development Proc. of the ACL 
Workshop on Sharing Tools and Resources 
for Research  and Education, 23-30 
D. Day,  L. Ferro, R. Gaizauskas, P. Hanks, M.  
Lazo, J. Pustejovsky, R. Saur?, A. See, A. 
Setzer, and B. Sundheim (2003), The 
TIMEBANK Corpus. Corpus Linguistics. 
M. Ellsworth, K. Erk, P. Kingsbury and S. Pado  
(2004), PropBank, SALSA, and FrameNet: 
How Design Determines Product, in Proc. of  
LREC 2004 Workshop: Building Lexical 
Resources from Semantically Annotated 
Corpora.  
C. Fellbaum (1997), WordNet: An Electronic  
Lexical Database, MIT Press.. 
C. J. Fillmore and B. T. S. Atkins (1998), 
FrameNet and lexicographic relevance. In the 
Proc. of the First International Conference 
on Language Resources and Evaluation.  
C. J. Fillmore and C. F. Baker (2001), Frame  
semantics for text understanding. In Proc. of 
NAACL WordNet and Other Lexical 
Resources Workshop. 
E. Hajivcova and I. Kuvcerov'a (2002).  
Argument/Valency Structure in PropBank, 
LCS Database and Prague Dependency 
Treebank: A Comparative Pilot Study. In the 
Proc. of the Third International Conference 
on Language Resources and Evaluation 
(LREC 2002),  846--851. 
S. Helmreich, D. Farwell, B. Dorr, N. Habash, L. 
    Levin, T. Mitamura, F. Reeder, K. Miller, E. 
     Hovy, O. Rambow and A. Siddharthan,(2004), 
     Interlingual Annotation of Multilingual Text 
     Corpora, Proc. of the HLT-EACL Workshop 
     on Frontiers in Corpus Annotation. 
A, Meyers, R. Reeves, C. Macleod, R, Szekely,  
V. Zielinska, B. Young, and R. Grishman  
(2004a), The NomBank Project: An Interim 
Report, Proc. of HLT-EACL Workshop: 
Frontiers in Corpus Annotation. 
A. Meyers, R. Reeves, and C. Macleod (2004b),  
NP-External Arguments: A Study of 
Argument Sharing in English. In The ACL 
2004 Workshop on Multiword Expressions: 
Integrating Processing. 
E. Miltsakaki, R. Prasad, A. Joshi and B. Webber. 
 (2004a), The Penn Discourse Treebank. In 
Proc. 4th International Conference on 
Language Resources and Evaluation (LREC 
2004). 
E. Miltsakaki, R. Prasad, A. Joshi and B. Webber  
(2004b), Annotation of Discourse 
Connectives and their Arguments, in Proc. of 
HLT-NAACL Workshop: Frontiers in Corpus 
Annotation 
M.  Marcus, B. Santorini, and M. Marcinkiewicz  
(1993), Building a large annotated corpus of 
english: The penn treebank. Computational 
Linguistics, 19:313--330. 
M. Palmer, D. Gildea, P. Kingsbury (2005), The  
Proposition Bank: A Corpus Annotated with 
Semantic Roles, Computational Linguistics 
Journal, 31:1. 
M. Poesio (2004a), The MATE/GNOME  
Scheme for Anaphoric Annotation, Revisited, 
Proc. of SIGDIAL 
M. Poesio (2004b), Discourse Annotation and  
Semantic Annotation in the GNOME Corpus, 
Proc. of ACL Workshop on Discourse 
Annotation. 
M. Poesio and M. Alexandrov-Kabadjov (2004), 
A general-purpose, off-the-shelf system for 
anaphora resol.. Proc. of LREC. 
M. Poesio, F. Bruneseaux, and L. Romary  
(1999), The MATE meta-scheme for 
coreference in dialogues in multiple language, 
Proc. of the ACL Workshop on Standards for 
Discourse Tagging.  
M. Poesio and R. Vieira (1998), A corpus-based  
investigation of definite description use. 
Computational Linguistics, 24(2). 
C. Pollard and I. A. Sag (1994), Head-driven  
phrase structure grammar. Univ. of Chicago 
Press. 
J. Pustejovsky, R. Saur?, J. Casta?o, D. R. 
 Radev, R. Gaizauskas, A. Setzer, B. 
Sundheim and G. Katz (2004), Representing 
Temporal and Event Knowledge for QA 
Systems. In Mark T. Maybury (ed.), New 
Directions in Question Answering, MIT Press. 
J. Pustejovsky,  B. Ingria, R. Saur?, J. Casta?o, J.  
Littman, R. Gaizauskas, A. Setzer, G. Katz, 
and I. Mani (2003), The Specification 
Language TimeML. In I. Mani, J. 
Pustejovsky, and R. Gaizauskas, editors, The 
Language of Time: A Reader. Oxford Univ. 
Press. 
12
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 159?177
Manchester, August 2008
The CoNLL-2008 Shared Task on
Joint Parsing of Syntactic and Semantic Dependencies
Mihai Surdeanu
?,?
Richard Johansson
?
Adam Meyers
?
Llu??s M
`
arquez
??
Joakim Nivre
??,??
?: Barcelona Media Innovation Center, mihai.surdeanu@barcelonamedia.org
?: Yahoo! Research Barcelona, mihais@yahoo-inc.com
?: Lund University, richard@cs.lth.se
?: New York University, meyers@cs.nyu.edu
??: Technical University of Catalonia, lluism@lsi.upc.edu
??: V?axj?o University, joakim.nivre@vxu.se
??: Uppsala University, joakim.nivre@lingfil.uu.se
Abstract
The Conference on Computational Natu-
ral Language Learning is accompanied ev-
ery year by a shared task whose purpose
is to promote natural language processing
applications and evaluate them in a stan-
dard setting. In 2008 the shared task was
dedicated to the joint parsing of syntactic
and semantic dependencies. This shared
task not only unifies the shared tasks of
the previous four years under a unique
dependency-based formalism, but also ex-
tends them significantly: this year?s syn-
tactic dependencies include more informa-
tion such as named-entity boundaries; the
semantic dependencies model roles of both
verbal and nominal predicates. In this pa-
per, we define the shared task and describe
how the data sets were created. Further-
more, we report and analyze the results and
describe the approaches of the participat-
ing systems.
1 Introduction
In 2004 and 2005 the shared tasks of the Confer-
ence on Computational Natural Language Learn-
ing (CoNLL) were dedicated to semantic role la-
beling (SRL), in a monolingual setting (English).
In 2006 and 2007 the shared tasks were devoted to
the parsing of syntactic dependencies, using cor-
pora from up to 13 languages. The CoNLL-2008
shared task
1
proposes a unified dependency-based
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1
http://www.yr-bcn.es/conll2008
formalism, which models both syntactic depen-
dencies and semantic roles. Using this formalism,
this shared task merges both the task of syntactic
dependency parsing and the task of identifying se-
mantic arguments and labeling them with semantic
roles. Conceptually, the 2008 shared task can be
divided into three subtasks: (i) parsing of syntactic
dependencies, (ii) identification and disambigua-
tion of semantic predicates, and (iii) identification
of arguments and assignment of semantic roles for
each predicate. Several objectives were addressed
in this shared task:
? SRL is performed and evaluated using a
dependency-based representation for both
syntactic and semantic dependencies. While
SRL on top of a dependency treebank has
been addressed before (Hacioglu, 2004),
our approach has several novelties: (i) our
constituent-to-dependency conversion strat-
egy transforms all annotated semantic argu-
ments in PropBank and NomBank not just a
subset; (ii) we address propositions centered
around both verbal (PropBank) and nominal
(NomBank) predicates.
? Based on the observation that a richer set
of syntactic dependencies improves seman-
tic processing (Johansson and Nugues, 2007),
the syntactic dependencies modeled are more
complex than the ones used in the previous
CoNLL shared tasks. For example, we now
include apposition links, dependencies de-
rived from named entity (NE) structures, and
better modeling of long-distance grammatical
relations.
? A practical framework is provided for the
joint learning of syntactic and semantic de-
pendencies.
159
Given the complexity of this shared task, we
limited the evaluation to a monolingual, English-
only setting. The evaluation is separated into two
different challenges: a closed challenge, where
systems have to be trained strictly with informa-
tion contained in the given training corpus, and an
open challenge, where systems can be developed
making use of any kind of external tools and re-
sources. The participants could submit results in
either one or both challenges.
This paper is organized as follows. Section 2
defines the task, including the format of the data,
the evaluation metrics, and the two challenges.
Section 3 introduces the corpora used and our
constituent-to-dependency conversion procedure.
Section 4 summarizes the results of the submit-
ted systems. Section 5 discusses the approaches
implemented by participants. Section 6 analyzes
the results using additional non-official evaluation
measures. Section 7 concludes the paper.
2 Task Definition
In this section we provide the definition of the
shared task, starting with the format of the shared
task data, followed by a description of the eval-
uation metrics used and a discussion of the two
shared task challenges, i.e., closed and open.
2.1 Data Format
The data format used in this shared task was highly
influenced by the formats used in the 2004?2007
shared tasks. The data follows these general rules:
? The files contain sentences separated by a
blank line.
? A sentence consists of one or more tokens and
the information for each token is represented
on a separate line.
? A token consists of at least 11 fields. The
fields are separated by one or more whites-
pace characters (spaces or tabs). Whitespace
characters are not allowed within fields.
Table 1 describes the fields stored for each token
in the closed-track data sets. Columns 1?3 and
5?8 are available at both training and test time.
Column 4, which contains gold-standard part-of-
speech (POS) tags, is not given at test time. The
same holds for columns 9 and above, which con-
tain the syntactic and semantic dependency struc-
tures that the systems should predict.
The PPOS and PPOSS fields were automati-
cally predicted using the SVMTool POS tagger
(Gim?enez, 2004). To predict the tags in the train-
ing set, a 5-fold cross-validation procedure was
used. The LEMMA and SPLIT LEMMA fields
were predicted using the built-in lemmatizer in
WordNet (Fellbaum, 1998) based on the most fre-
quent sense for the form and part-of-speech tag.
Since NomBank uses a sub-word anal-
ysis in some hyphenated words (such as
[finger]
ARG
-[pointing]
PRED
), the data for-
mat represents the parts in hyphenated words as
separate tokens (columns 6?8). However, the
format also represents how the parts originally fit
together before splitting (columns 2?5). Padding
characters (? ?) are used in columns 2?5 to
ensure the same number of rows for all columns
corresponding to one sentence. All syntactic and
semantic dependencies are annotated relative to
the split word forms (columns 6?8).
Table 2 shows the columns available to the sys-
tems participating in the open challenge: named-
entity labels as in the CoNLL-2003 Shared Task
(Tjong Kim San and De Meulder, 2003) and
from the BBN Wall Street Journal Entity Corpus,
2
WordNet supersense tags, and the output of an off-
the-shelf dependency parser (Nivre et al, 2007b).
Columns 1?3 were predicted using the tagger of
Ciaramita and Altun (2006). Because the BBN
corpus shares lexical content with the Penn Tree-
bank, we generated the BBN tags using a 2-fold
cross-validation procedure.
2.2 Evaluation Measures
We separate the evaluation measures into two
groups: (i) official measures, which were used for
the ranking of participating systems, and (ii) addi-
tional unofficial measures, which provide further
insight into the performance of the participating
systems.
2.2.1 Official Evaluation Measures
The official evaluation measures consist of three
different scores: (i) syntactic dependencies are
scored using the labeled attachment score (LAS),
(ii) semantic dependencies are evaluated using a
labeled F
1
score, and (iii) the overall task is scored
with a macro average of the two previous scores.
We describe all these scoring measures next.
The LAS score is defined similarly as in the pre-
vious two shared tasks, as the percentage of to-
2
LDC catalog number LDC2005T33.
160
Number Name Description
1 ID Token counter, starting at 1 for each new sentence.
2 FORM Unsplit word form or punctuation symbol.
3 LEMMA Predicted lemma of FORM.
4 GPOS Gold part-of-speech tag from the Treebank (empty at test time).
5 PPOS Predicted POS tag.
6 SPLIT FORM Tokens split at hyphens and slashes.
7 SPLIT LEMMA Predicted lemma of SPLIT FORM.
8 PPOSS Predicted POS tags of the split forms.
9 HEAD Syntactic head of the current token, which is either a value of ID or zero (0).
10 DEPREL Syntactic dependency relation to the HEAD.
11 PRED Rolesets of the semantic predicates in this sentence.
12. . . ARG Columns with argument labels for each semantic predicate following textual order.
Table 1: Column format in the closed-track data. The columns in the lower part of the table are unseen
at test time and are to be predicted by systems.
Number Name Description
1 CONLL2003 Named entity labels using the tag set from the CoNLL-2003 shared task.
2 BBN NE labels using the tag set from the BBN Wall Street Journal Entity Corpus.
3 WNSS WordNet super senses.
4 MALT HEAD Head of the syntactic dependencies generated by MaltParser.
5 MALT DEPREL Label of syntactic dependencies generated by MaltParser.
Table 2: Column format in the open-track data.
kens for which a system has predicted the correct
HEAD and DEPREL columns (see Table 1). Same
as before, our scorer also computes the unlabeled
attachment score (UAS), i.e., the percentage of to-
kens with correct HEAD, and label accuracy, i.e.,
the percentage of tokens with correct DEPREL.
The semantic propositions are evaluated by con-
verting them to semantic dependencies, i.e., we
create a semantic dependency from every predicate
to all its individual arguments. These dependen-
cies are labeled with the labels of the correspond-
ing arguments. Additionally, we create a seman-
tic dependency from each predicate to a virtual
ROOT node. The latter dependencies are labeled
with the predicate senses. This approach guaran-
tees that the semantic dependency structure con-
ceptually forms a single-rooted, connected (but not
necessarily acyclic) graph. More importantly, this
scoring strategy implies that if a system assigns
the incorrect predicate sense, it still receives some
points for the arguments correctly assigned. For
example, for the correct proposition:
verb.01: ARG0, ARG1, ARGM-TMP
the system that generates the following output for
the same argument tokens:
verb.02: ARG0, ARG1, ARGM-LOC
receives a labeled precision score of 2/4 because
two out of four semantic dependencies are incor-
rect: the dependency to ROOT is labeled 02 in-
stead of 01 and the dependency to the ARGM-TMP
is incorrectly labeled ARGM-LOC. Using this strat-
egy we compute precision, recall, and F
1
scores
for both labeled and unlabeled semantic dependen-
cies.
Finally, we combine the syntactic and semantic
measures into one global measure using macro av-
eraging. We compute macro precision and recall
scores by averaging the labeled precision and re-
call for semantic dependencies with the LAS for
syntactic dependencies:
3
LMP = W
sem
? LP
sem
+ (1?W
sem
) ? LAS (1)
LMR = W
sem
? LR
sem
+ (1?W
sem
) ? LAS (2)
where LMP is the labeled macro precision and
LP
sem
is the labeled precision for semantic depen-
dencies. Similarly, LMR is the labeled macro re-
call and LR
sem
is the labeled recall for semantic
dependencies. W
sem
is the weight assigned to the
semantic task.
4
The macro labeled F
1
score, which
was used for the ranking of the participating sys-
tems, is computed as the harmonic mean of LMP
and LMR.
3
We can do this because the LAS for syntactic dependen-
cies is a special case of precision and recall, where the pre-
dicted number of dependencies is equal to the number of gold
dependencies.
4
We assign equal weight to the two tasks, i.e., W
sem
=
0.5.
161
2.2.2 Additional Evaluation Measures
We used several additional evaluation measures
to further analyze the performance of the partici-
pating systems.
The first additional measure used is Exact
Match, which reports the percentage of sentences
that are completely correct, i.e., all the generated
syntactic dependencies are correct and all the se-
mantic propositions are present and correct. While
this score is significantly lower than any of the of-
ficial scores, it will award systems that performed
joint learning or optimization for all subtasks.
In the same spirit but focusing on the seman-
tic subtasks, we report the Perfect Proposition F
1
score, where we score entire semantic frames or
propositions. This measure is similar to the PProps
accuracy score from the 2005 shared task (Carreras
and M`arquez, 2005), with the caveat that this year
this score is implemented as an F
1
measure, be-
cause predicates are not provided in the test data.
Hence, propositions may be over or under gener-
ated at prediction time.
Lastly, we analyze systems based on the ratio
between labeled F
1
score for semantic dependen-
cies and the LAS for syntactic dependencies. In
other words, this measure normalizes the seman-
tic scores relative to the performance of the pars-
ing component. This measure estimates the true
overall performance of the semantic subtasks, in-
dependent of the syntactic parser.
5
For example,
this score addresses the situations where the se-
mantic labeled F
1
score of one system is artificially
low because the corresponding syntactic compo-
nent does not perform well.
2.3 Closed and Open Challenges
Similarly to the CoNLL-2005 shared task, this
shared task evaluation is separated into two chal-
lenges:
Closed Challenge - systems have to be built
strictly with information contained in the given
training corpus, and tuned with the development
section. In addition, the PropBank and NomBank
lexical frames can be used. These restrictions
mean that constituent-based parsers or SRL sys-
tems can not be used in this challenge because the
constituent-based annotations are not provided in
our training set. The aim of this challenge is to
5
A correct evaluation of the stand-alone SRL systems
would require the usage of gold syntactic dependencies, but
these were not provided for the testing corpora.
compare the performance of the participating sys-
tems in a fair environment.
Open Challenge - systems can be developed mak-
ing use of any kind of external tools and resources.
The only condition is that such tools or resources
must not have been developed with the annota-
tions of the test set, both for the input and out-
put annotations of the data. In this challenge,
we are interested in learning methods which make
use of any tools or resources that might improve
the performance. For example, we encourage the
use of semantic information, as provided by NE
recognition or word-sense disambiguation (WSD)
systems (such state-of-the-art annotations are pro-
vided by the organizers, see Table 2). Also, in
this challenge participants are encouraged to use
constituent-based parsers and SRL systems, as
long as these systems were trained only with the
sections of Penn Treebank used in the shared task
training corpus. To encourage the participation of
the groups that are only interested in SRL, the or-
ganizers provide also the output of a state-of-the-
art dependency parser as input in this challenge.
The comparison of different systems in this setting
may not be fair, and thus ranking of systems is not
necessarily important.
3 Data
The corpora used in the shared task evaluation
were generated through a process that merges
several input corpora and converts them from
the constituent-based formalism to dependencies.
This section starts with an introduction of the in-
put corpora used, followed by a description of
the constituent-to-dependency conversion process.
The section concludes with an overview of the
shared task corpora.
3.1 Input Corpora
Input to our merging procedures includes the Penn
Treebank, BBN?s named entity corpus, PropBank
and NomBank. In this section, we will pro-
vide brief descriptions of these annotations in
terms of both form and content. All annotations
are currently being distributed by the Linguistic
Data Consortium, with the exception of NomBank,
which is freely downloadable.
6
6
http://nlp.cs.nyu.edu/meyers/NomBank.
html
162
3.1.1 Penn Treebank 3
The Penn Treebank 3 corpus (Marcus et al,
1993) consists of hand-coded parses of the Wall
Street Journal (test, development and training) and
a small subset of the Brown corpus (W. N. Fran-
cis and H. Ku?cera, 1964) (test only). These hand
parses are notated in-line and sometimes involve
changing the strings of the input data. For ex-
ample, in file wsj 0309, the token fearlast in the
text corresponds to the two tokens fear and last
in the annotated data. In a similar way, cannot
is regularly split to can and not. It is significant
that the other annotations assume the tokeniza-
tion of the Penn Treebank, as this makes it easier
for us to merge the annotation. The Penn Tree-
bank syntactic annotation includes phrases, parts
of speech, empty category representations of vari-
ous filler/gap constructions and other phenomena,
based on a theoretical perspective similar to that
of Government and Binding Theory (Chomsky,
1981).
3.1.2 BBN Pronoun Coreference and Entity
Type Corpus
BBN?s NE annotation of the Wall Street Journal
corpus (Weischedel and Brunstein, 2005) takes the
form of SGML inline markup of text, tokenized
to be completely compatible with the Penn Tree-
bank annotation, e.g., fearlast and cannot are split
in the same ways. Named entity categories in-
clude: Person, Organization, Location, GPE, Fa-
cility, Money, Percent, Time and Date, based on
the definitions of these categories in MUC (Chin-
chor and Robinson, 1998) and ACE
7
tasks. Sub-
categories are included as well. Note however that
from this corpus we only use NE boundaries to
derive NAME dependencies between NE tokens,
e.g., we create a NAME dependency from Mary to
Smith given the NE mention Mary Smith.
3.1.3 Proposition Bank I (PropBank)
The PropBank annotation (Palmer et al, 2005)
classifies the arguments of all the main verbs in the
Penn Treebank corpus, other than be. Arguments
are numbered (ARG0, ARG1, . . .) based on lexical
entries or frame files. Different sets of arguments
are assumed for different rolesets. Dependent con-
stituents that fall into categories independent of
the lexical entries are classified as various types
7
http://projects.ldc.upenn.edu/ace/
of ARGM (TMP, ADV, etc.).
8
Rather than us-
ing PropBank directly, we used the version created
for the CoNLL-2005 shared task (Carreras and
M`arquez, 2005). PropBank?s pointers to subtrees
are converted into the list of leaves of those sub-
trees, minus the empty categories. On occasion,
arguments of verbs end up being two non-adjacent
substrings. For example, the argument of claims in
the following sentence is indicated in bold: This
sentence, Mary claims, is self-referential. The
CoNLL-2005 format handles this by marking both
strings A1 (This sentence and is self-referential),
but adding a C- prefix to the argument tag on the
second argument. Another difference between the
PropBank annotation and the CoNLL-2005 ver-
sion of it is their treatments of filler gap construc-
tions involving empty categories. PropBank an-
notation includes the whole chain of empty cate-
gories, as well as the antecedent of the empty cate-
gory (the filler of the gap). In contrast, the CoNLL-
2005 version only includes the filler of the gap and
if there is no filler, the argument is omitted, e.g.,
no ARG0 (subject) for leave would be included in
I said to leave because the subject of leave is un-
specified.
3.1.4 NomBank
NomBank annotation (Meyers et al, 2004) uses
essentially the same framework as PropBank to an-
notate arguments of nouns. Differences between
PropBank and NomBank stem from differences
between noun and verb argument structure; differ-
ences in treatment of nouns and verbs in the Penn
Treebank; and differences in the sophistication of
previous research about noun and verb argument
structure. Only the subset of nouns that take ar-
guments are annotated in NomBank and only a
subset of the non-argument siblings of nouns are
marked as ARGM. These limitations were nec-
essary to make the NomBank task consistent and
tractable. In addition, long distance dependencies
of nouns, e.g., the relation between Mary and walk
in Mary took dozens of walks is handled as fol-
lows: Mary is marked as the ARG0 of walk and
took + dozens + of is marked as a support chain
in NomBank. In contrast, verbal long distance de-
pendencies can be handled by means of empty cat-
egories in the Penn Treebank, e.g., the relation be-
8
PropBank I is used here. Later versions of PropBank
mark instances of be in addition to other verbs. PropBank?s
use of the terms roleset and ARGM correspond approximately
to sense and adjunct in common usage.
163
tween John and walked in John seemed t to walk.
Support chains are needed because nominal long
distance dependencies are not captured under the
Penn Treebank?s system of empty categories.
3.2 Conversion to Dependencies
3.2.1 Syntactic Dependencies
There exists no large-scale dependency tree-
bank for English, and we thus had to construct a
dependency-annotated corpus automatically from
the Penn Treebank (Marcus et al, 1993). Since
dependency syntax represents grammatical struc-
ture by means of labeled binary head?dependent
relations rather than phrases, the task of the con-
version procedure is to identify and label the
head?dependent pairs. The idea underpinning
constituent-to-dependency conversion algorithms
(Magerman, 1994; Collins, 1999; Yamada and
Matsumoto, 2003) is that head?dependent pairs are
created from constituents by selecting one word in
each phrase as the head and setting all other as its
dependents. The dependency labels are then in-
ferred from the phrase?subphrase or phrase?word
relations.
Our conversion procedure (Johansson and
Nugues, 2007) differs from this basic approach by
exploiting the rich structure of the constituent for-
mat used in Penn Treebank 3:
? Grammatical function labels that often can be
directly used in the dependency framework.
? Long-distance grammatical relations repre-
sented by means of empty categories and sec-
ondary edges, which can be used to create (of-
ten nonprojective) dependency links.
Of the grammatical function tags available in the
Treebank, we removed the HLN, NOM, TPC, and
TTL tags since they represent structural properties
of single phrases rather than binary relations. For
compatibility between the WSJ and Brown cor-
pora, we removed the ETC, UNF, and IMP tags
from Brown and the CLR tag from WSJ.
Algorithms 1 and 2 show the constituent-to-
dependency conversion algorithm and function la-
beling, respectively. The first steps apply structural
transformations to the constituent trees. Next, a
head word is assigned to each constituent. After
this, grammatical functions are inferred, allowing
a dependency tree to be created.
To find head children (used in
assign-heads), a system of rules is used
Algorithm 1: Pseudocode for constituent-to-
dependency conversion.
procedure constituents-to-dependencies(T )
import-glarf(T )
reattach-traces(T )
split-small-clauses(T )
assign-heads(T.root)
assign-functions(T )
return create-dependency-tree(T )
procedure import-glarf(T )
Import a GLARF surface dependency graph G
for each multi-word name N in G
for each token d in N
Set the function tag of d to NAME
for each dependency link h ?
L
d in G
if L ? { APPOSITE, A-POS, N-POS, POST-HON, Q-POS,
RED-RELATIVE, SUFFIX, T-POS, TITLE }
or if h and d are inside a split word
Set the function tag of d to L in T
if h and d are part of a larger constituent
Add an NX constituent to T that brackets h and d
procedure reattach-traces(T )
for each empty category t in T
if t is linked to a constituent C via a secondary edge label L
and L ? {
*
ICH
*
,
*
T
*
,
*
RNR
*
}
disconnect C
disconnect the secondary edge
attach C to the parent of t
procedure split-small-clauses(T )
for each verb phrase C in T
if C has a child S and the phrase label of S is S
and S is not preceded by a ?? or , tag
and S has a subject child s
disconnect s
attach s to C
set the function tag of s to OBJ
set the function tag of S to OPRD
procedure assign-heads(N)
for each child C of N
assign-heads(C)
if is-coordinated(N)
e ? index of first CC or CONJP or , or :
else
e ? index of last child of N
find head child H between 1 and e according to head rules (Table 3)
N.head ? H.head
procedure is-coordinated(N)
if N has the label UCP return True
if N has a CC or CONJP child which is not leftmost return True
if N has a , or : child c, and c is not leftmost or rightmost or
crossed by an apposition link, return True
else return False
procedure create-dependency-tree(T )
D ? {}
for each token t in T
let C be the highest constituent that t is the head of
let P be the parent of C
let L be the function tag of C
D ? D ? P.head ?
L
t
return D
(Table 3). The first column in the table indicates
the phrase type, the second is the search direction,
and the third is a priority list of phrase types to
look for. For instance, to find the head of an S
phrase, we look from right to left for a VP. If
no VP is found, look for anything with a PRD
function tag, and so on.
Moreover, since the grammatical structure in-
164
ADJP ? NNS QP NN $ ADVP JJ VBN VBG ADJP JJR NP JJS DT
FW RBR RBS SBAR RB
ADVP ? RB RBR RBS FW ADVP TO CD JJR JJ IN NP JJS NN
CONJP ? CC RB IN
FRAG ? (NN
*
| NP) W
*
SBAR (PP | IN) (ADJP | JJ) ADVP
RB
INTJ ?
*
LST ? LS :
NAC, NP, NX, WHNP ? (NN
*
| NX) NP-? JJR CD JJ JJS RB QP NP
PP, WHPP ? IN TO VBG VBN RP FW
PRN ? S
*
N
*
W
*
PP|IN ADJP|JJ
*
ADVP|RB
*
PRT ? RP
QP ? $ IN NNS NN JJ RB DT CD NCD QP JJR JJS
RRC ? VP NP ADVP ADJP PP
S ? VP
*
-PRD S SBAR ADJP UCP NP
SBAR ? S SQ SINV SBAR FRAG IN DT
SBARQ ? SQ S SINV SBARQ FRAG
SINV ? VBZ VBD VBP VB MD VP
*
-PRD S SINV ADJP NP
SQ ? VBZ VBD VBP VB MD
*
-PRD VP SQ
UCP ?
*
VP ? VBD VBN MD VBZ VB VBG VBP VP
*
-PRD ADJP NN NNS
NP
WHADJP ? CC WRB JJ ADJP
WHADVP ? CC WRB
X ?
*
Table 3: Head rules.
Algorithm 2: Pseudocode for the function la-
beling procedure.
procedure assign-functions(T )
for each constituent C in T
if C has no function tag from Penn or GLARF
L ? infer-function(C)
Set the function tag of C to L
procedure infer-function(C)
let c be the head of C, P the parent of C, and p the head of P
if C is an object return OBJ
if C is PRN return PRN
if h is punctuation return P
if C is coordinated with P return COORD
if C is PP, ADVP, or SBAR and P is VP return ADV
if C is PRT and P is VP return PRT
if C is VP and P is VP, SQ, or SINV return VC
if C is TO and P is VP return IM
if P is SBAR and p is IN return SUB
if P is VP, S, SBAR, SBARQ, SINV, or SQ and C is RB return ADV
if P is NP, NX, NAC, or WHNP return NMOD
if P is ADJP, ADVP, WHADJP, or WHADVP return AMOD
if P is PP or WHPP return PMOD
else return DEP
side noun phrases (NP) is under-specified in the
Penn Treebank, we imported dependencies in-
side NPs and hyphenated words from a version
of the Penn Treebank mapped into GLARF, the
Grammatical and Logical Argument Representa-
tion Framework (Meyers et al, 2007).
The parts of GLARF?s NP analysis that are most
relevant to this task include: (i) identifying ap-
posites (APPO, e.g., that book depends on gift in
Mary?s gift, a book about cheese; (ii) the iden-
tification of name boundaries taken from BBN?s
NE annotation, e.g., identifying that Smith de-
pends on Mary which depends on appointment
in the Mary Smith appointment; (iii) identifying
TITLE and POSTHON dependencies, e.g., deter-
mining that Ms. and III depend on Mary in Ms.
Mary Smith III. These identifications were car-
ried out by hand-coded rules that have been fine
tuned as part of GLARF, over the past several
years. For example, identifying apposition con-
structions requires identifying that both the head
and the apposite can stand alone ? proper nouns
(John Smith), plural nouns (books), and singular
common nouns with determiners (the book) are
stand-alone cases, whereas singular nouns without
determiners (green book) do not qualify.
We split Treebank tokens at a hyphen (-) or a
forward slash (/) if the segments on either side of
these delimiters are: (a) a word in a dictionary
(COMLEX Syntax or any of the dictionaries avail-
able on the NOMLEX website); (b) part of a mark-
able Named Entity;
9
or (c) a prefix from the list:
co, pre, post, un, anti, ante, ex, extra, fore, non,
over, pro, re, super, sub, tri, bi, uni, ultra. For ex-
ample, York-based was split into 3 segments: (1)
York, (2) - and (3) based.
9
The CoNLL-2008 website contains a Named Entity To-
ken gazetteer to aid in this segmentation.
165
3.2.2 Semantic Dependencies
When encoding the semantic dependencies, it
was necessary to convert the underlying con-
stituent analysis of PropBank and NomBank into
a dependency analysis. Because semantic predi-
cates are already assigned to individual tokens in
both PropBank (the version used for the CoNLL-
2005 shared task) and NomBank, constituent-to-
dependency conversion is thus necessary only for
semantic arguments. Conceptually, this conver-
sion can be handled using similar heuristics as de-
scribed in Section 3.2.1. However, in order to
avoid replicating this effort and to ensure compat-
ibility between syntactic and semantic dependen-
cies, we decided to generate semantic dependen-
cies using only argument boundaries and the syn-
tactic dependencies generated in Section 3.2.1, i.e.,
ignoring syntactic constituents. Given this input,
we identify the head of a semantic argument using
the following heuristic:
The head of a semantic argument is as-
signed to the token inside the argument
boundaries whose head is a token out-
side the argument boundaries.
This heuristic works remarkably well: over 99%
of the PropBank arguments in the training corpus
have a single token whose head is located outside
of the argument boundaries. As a simple example,
consider the following annotated text: [sold]
PRED
[1214 cars]
ARG1
[in the U.S.]
ARGM-LOC
. Us-
ing the above heuristic, the head of the ARG1 ar-
gument is set to cars, because it has an OBJ de-
pendency to sold, and the head of the ARGM-
LOC argument is set to in, because it modifies sold
through a LOC dependency.
While this heuristic processes the vast majority
of arguments, there are several cases that require
special treatment. We discuss these situations in
the remainder of this section.
Arguments with several syntactic heads
For 0.7% of the semantic arguments, the above
heuristic detects several syntactic heads for the
given boundary. For example, in the text [it]
ARG0
[expects]
PRED
[its U.S. sales to remain steady
at about 1200 cars]
ARG1
, the above heuris-
tic assigns two syntactic heads to ARG1: sales,
which modifies expects through an OBJ depen-
dency, and to, which modifies expects through a
PRD dependency. These situations are caused
by the constituent-to-dependency conversion pro-
cess described in Section 3.2.1, which in some
cases interprets syntax differently than the orig-
inal Treebank annotation, e.g., the raising phe-
nomenon for the PRD dependency in the above
example. In such cases, we split the original argu-
ment into a sequence of discontinuous arguments,
e.g., the ARG1 in the above example becomes [its
U.S. sales]
ARG1
[to remain steady at about 1200
cars]
C-ARG1
.
Merging discontinuous arguments
While in the above case we split arguments, there
are situations where we can merge arguments that
were initially discontinuous in PropBank or Nom-
Bank. This typically happens when the Prop-
Bank/NomBank predicate is infixed inside one of
its arguments. For example, in the text [Million-
dollar conferences]
ARG1
were [held]
PRED
[to
chew on subjects such as... ]
C-ARG1
, PropBank
lists multiple constituents as aggregately filling the
ARG1 slot of held. These cases are detected au-
tomatically because the least common ancestor of
the argument pieces is actually one of the argument
segments. In the above example, to chew on sub-
jects such as... depends on Million-dollar confer-
ences because to modifies conferences through a
NMOD dependency. In these situations, we treat
the least common ancestor, e.g., conferences in the
above text, as the true argument. This heuristic al-
lowed us to merge 1665 (or 0.6% of total) argu-
ments that were initially discontinuous in the Prop-
Bank training corpus.
Empty categories
PropBank and NomBank both encode chains of
empty categories. As with the 2005 shared task
(Carreras and M`arquez, 2005), we used the head
of the antecedent of empty categories as arguments
rather than empty categories. Furthermore, empty
category arguments with no antecedents were ig-
nored.
10
For example, given The man wanted t to
make a speech, we assume that the A0 of make and
speech is man, rather than the chain consisting of
the empty category represented as t and man.
Annotation disagreements
NomBank and Penn Treebank annotators some-
times disagree about constituent structure. Nom-
10
Under our approach to filler gap constructions, the filler
is a shared argument (as in Relational Grammar, most Feature
Structure and Dependency Grammar frameworks), in con-
trast with the Penn Treebank?s empty category antecedent ap-
proach (more closely resembling the various Chomskian ap-
proaches).
166
Label Freq. Description
NMOD 324834 Modifier of nominal
P 135260 Punctuation
PMOD 115988 Modifier of preposition
SBJ 89371 Subject
OBJ 66677 Object
ROOT 49178 Root
ADV 47379 General adverbial
NAME 41138 Name-internal link
VC 35250 Verb chain
COORD 31140 Coordination
DEP 29456 Unclassified
TMP 26305 Temporal adverbial or nominal modifier
CONJ 24522 Second conjunct (dependent on conjunction)
LOC 18500 Locative adverbial or nominal modifier
AMOD 17868 Modifier of adjective or adverbial
PRD 16265 Predicative complement
APPO 16163 Apposition
IM 16071 Infinitive verb (dependent on infinitive marker to)
HYPH 14073 Token part of a hyphenated word (dependent on a preceding part of the hyphenated word)
HMOD 13885 Token inside a hyphenated word (dependent on the head of the hyphenated word)
SUB 12995 Subordinated clause (dependent on subordinating conjuction)
OPRD 11707 Predicative complement of raising/control verb
SUFFIX 10548 Possessive suffix (dependent on possessor)
DIR 6145 Adverbial of direction
TITLE 5917 Title (dependent on name)
MNR 4753 Adverbial of manner
POSTHON 4377 Posthonorific modifier of nominal
PRP 4013 Adverbial of purpose or reason
PRT 3235 Particle (dependent on verb)
LGS 3115 Logical subject of a passive verb
EXT 2374 Adverbial of extent
PRN 2176 Parenthetical
EXTR 658 Extraposed element in cleft
DTV 496 Dative complement (to) in dative shift
PUT 271 Complement of the verb put
BNF 44 Benefactor complement (for) in dative shift
VOC 24 Vocative
Table 4: Statistics for atomic syntactic labels.
Bank annotators are in effect assuming that the
constituents provided form a phrase. In this case,
the constituents are adjacent to each other. For ex-
ample, consider the NP the human rights discus-
sion. In this case, the Penn Treebank would treat
each of the four words the, human, rights, discus-
sion as daughters of a single NP node. However,
NomBank would treat human rights as a single
ARG1 of discussion. Since noun noun modifica-
tion constructions are head final, we can easily de-
termine (via GLARF) that rights is the markable
dependent of discussion.
Support chains
Finally, NomBank?s encoding of support chains is
handled as chains of dependencies in the data (al-
though these are not scored). For example, given
Mary took dozens of walks, where Mary is the
ARG0 of walks, the support chain took + dozens +
of is represented as a sequence of dependencies: of
depends on Mary, dozens depends on of and took
depends on dozens. Each of these dependencies is
labeled SU.
3.3 Overview of Corpora
The syntactic dependency types are divided into
atomic types that consist of a single label, and non-
atomic types consisting of more than one label.
There are 38 atomic and 70 non-atomic labels in
the corpus. There are three types of non-atomic
labels: those consisting of a PRD or OPRD con-
catenated with an adverbial label such as LOC or
TMP; gapping labels such as GAP-SBJ; and com-
bined adverbial tags such as LOC-TMP.
Table 4 shows statistics for the atomic syntac-
tic dependencies: label type, the frequency of the
label in the complete corpus, and a description of
the label. Table 5 shows the corresponding statis-
tics for non-atomic dependencies, excluding gap-
ping dependencies. The non-atomic labels are rare,
which made it difficult to learn these relations ef-
167
Label Frequency
LOC-PRD 798
PRD-TMP 51
PRD-PRP 45
LOC-OPRD 31
DIR-PRD 4
MNR-PRD 3
LOC-TMP 2
MNR-TMP 1
LOC-MNR 1
DIR-OPRD 1
Table 5: Statistics for non-atomic syntactic labels
excluding gapping labels.
Label Frequency
GAP-SBJ 116
GAP-OBJ 102
DEP-GAP 83
GAP-TMP 69
GAP-PRD 66
GAP-LGS 44
GAP-LOC 42
DIR-GAP 37
GAP-PMOD 22
GAP-VC 20
EXT-GAP 16
ADV-GAP 15
GAP-NMOD 13
GAP-LOC-PRD 6
DTV-GAP 6
AMOD-GAP 6
GAP-MNR 5
GAP-PRP 4
EXTR-GAP 3
GAP-SUB 1
GAP-PUT 1
GAP-OPRD 1
Table 6: Statistics for non-atomic labels containing
a gapping label.
fectively. Table 6 shows the table for non-atomic
labels containing a gapping label.
A dependency link w
i
? w
j
is said to be pro-
jective if all words occurring between w
i
and w
j
in
the surface word order are dominated by w
i
(where
dominance is the transitive closure of the direct
link relation). Nonprojective links are impossible
to handle for the search procedures in many types
of dependency parsers. It has been previously ob-
served that the majority of dependencies in all lan-
guages are projective, and this is particularly true
for English ? in the complete corpus, only 4118
links (0.4%) are nonprojective. 3312 sentences, or
7.6%, contain at least one nonprojective link.
Table 7 shows statistics for different types of
nonprojective links: nonprojectivity caused by
wh-movement, such as in Where are you going?
or What have you done?; split clauses such as
Type Frequency
wh-movement 1709
Split clause 734
Split noun phrase 590
Other 1085
Table 7: Statistics for nonprojective links.
POS Frequency
NN 68477
NNS 30048
VBD 24106
VB 23650
VBN 19339
VBG 14245
VBZ 10883
VBP 6330
Other 83
Table 8: Statistics for predicates, by POS tags.
Even to make love, he says, you need experience;
split noun phrases such as hold a hearing tomor-
row on the topic; and all other types of nonprojec-
tive links.
Lastly, Tables 8 and 9 summarizes statistics for
semantic predicates and roles. Table 8 shows the
number of non-support predicates with a given
POS tag in the whole corpus (we used GPOS or
PPOSS for predicates inside hyphenated words).
The last line shows the number of predicates with
a POS tag that does not start with NN or VB. This
last table entry is generated by POS tagger mis-
takes when producing the PPOSS tags, or by errors
in our NomBank/PropBank conversion software.
11
Nevertheless, the overall picture given by the table
indicates that predicates are almost perfectly dis-
tributed between nouns and verbs: there are 98525
nominal and 98553 verbal predicates.
Table 9 shows the number of arguments with a
given role label. For brevity we list only labels that
are instantiated at least 10 times in the whole cor-
pus. The total number of arguments labeled with a
role label with frequency lower than 10 is listed
in the last line in the table. The table indicates
that, while the top three most common role labels
are ?core? labels (A1, A0, A2), modifier arguments
(AM-
*
) account for approximately 20% of the total
number of arguments. On the other hand, discon-
tinuous arguments are not common: only 0.7% of
the total number of arguments have a continuation
label (C-
*
).
11
In very few situations, we select incorrect head tokens for
multi-word predicates.
168
Label Frequency
A1 161409
A0 109437
A2 51197
AM-TMP 25913
AM-MNR 13080
AM-LOC 11409
A3 10269
AM-MOD 9986
AM-ADV 9496
AM-DIS 5369
R-A0 4432
AM-NEG 4097
A4 3281
C-A1 3118
R-A1 2565
AM-PNC 2445
AM-EXT 1428
AM-CAU 1346
AM-DIR 1318
R-AM-TMP 797
R-A2 307
R-AM-LOC 246
R-AM-MNR 155
A5 91
AM-PRD 78
C-A0 70
C-A2 65
R-AM-CAU 50
C-A3 37
R-A3 29
C-AM-MNR 24
C-AM-ADV 20
AM-REC 16
AA 14
R-AM-PNC 12
C-AM-EXT 11
C-AM-TMP 11
C-A4 11
Frequency < 10 70
Table 9: Statistics for semantic roles.
4 Submissions and Results
Nineteen groups submitted test runs in the closed
challenge and five groups participated in the open
challenge. Three of the latter groups participated
only in the open challenge, and two of these sub-
mitted results only for the semantic subtask. These
results are summarized in Tables 10 and 11.
Table 10 summarizes the official results ? i.e.,
results at evaluation deadline ? for the closed chal-
lenge. Note that several teams corrected bugs
and/or improved their systems and they submit-
ted post-evaluation scores (accounted in the shared
task website). The table indicates that most of the
top results cluster together: three systems had a
labeled macro F
1
score on the WSJ+Brown cor-
pus around 82 points (che, ciaramita, and zhao);
five systems scored around 79 labeled macro F
1
points (yuret, samuelsson, zhang, henderson, and
watanabe). Remarkably, the top-scoring system
(johansson) is in a class of its own, with scores
2?3 points higher than the next system. This is
most likely caused by the fact that Johansson and
Nugues (2008) implemented a thorough system
that addressed all facets of the task with state-of-
the-art methods: second-order parsing model, ar-
gument identification/classification models sepa-
rately tuned for PropBank and NomBank, rerank-
ing inference for the SRL task, and, finally, joint
optimization of the complete task using meta-
learning (more details in Section 5).
Table 11 lists the official results in the open chal-
lenge. The results in this challenge are lower than
in the closed challenge, but this was somewhat
to be expected considering that there were fewer
participants in this challenge and none of the top
five groups in the closed challenge submitted re-
sults in the open challenge. Only one of the sys-
tems that participated in both challenges (zhang)
improved the results submitted in the closed chal-
lenge. Zhang et al (2008) achieved this by ex-
tracting features for their semantic subtask mod-
els both from the parser used in the closed chal-
lenge and a secondary parser that was trained on
a different corpus. The improvements measured
were relatively small for the in-domain WSJ cor-
pus (0.2 labeled macro F
1
points) but larger for the
out-of-domain Brown corpus (approximately 1 la-
beled macro F
1
point).
Tables 10 and 11 indicate that in both chal-
lenges the results on the out-of-domain corpus
(Brown) are much lower than the results measured
in-domain (WSJ). The difference is around 7?8
LAS points for the syntactic subtask and 12?14 la-
beled F
1
points for semantic dependencies. Over-
all, this yields a drop of approximately 10 labeled
macro F
1
points for most systems. This perfor-
mance decrease on out-of-domain corpora is con-
sistent with the results reported in CoNLL-2005
on SRL (using the same Brown corpus). These
results indicate that domain adaptation is a prob-
lem that is far from being solved for both syntactic
and semantic analysis of text. Furthermore, as the
scores on the syntactic and semantic subtasks in-
dicate, domain adaptation becomes even harder as
the task to be solved gets more complex.
We describe the participating systems in the next
section. Then, in Section 6, we revert to result
analysis using different evaluation measures and
different views of the data.
169
Labeled Macro F
1
Labeled Attachment Score Labeled F
1
(complete task) (syntactic dependencies) (semantic dependencies)
WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown
johansson 84.86 (1) 85.95 75.95 89.32 (1) 90.13 82.81 80.37 (1) 81.75 69.06
che 82.66 (2) 83.78 73.57 86.75 (5) 87.51 80.73 78.52 (2) 80.00 66.37
ciaramita 82.06 (3) 83.25 72.46 86.60 (11) 87.47 79.67 77.50 (3) 79.00 65.24
zhao 81.44 (4) 82.62 71.78 86.66 (8) 87.52 79.83 76.16 (4) 77.67 63.69
yuret 79.84 (5) 80.97 70.55 86.62 (10) 87.39 80.46 73.06 (5) 74.54 60.62
samuelsson 79.79 (6) 80.92 70.49 86.63 (9) 87.36 80.77 72.94 (6) 74.47 60.18
zhang 79.32 (7) 80.41 70.48 87.32 (2) 88.14 80.80 71.31 (7) 72.67 60.16
henderson 79.11 (8) 80.19 70.34 86.91 (4) 87.78 80.01 70.97 (8) 72.26 60.38
watanabe 79.10 (9) 80.30 69.29 87.18 (3) 88.06 80.17 70.84 (9) 72.37 58.21
morante 78.43 (10) 79.52 69.55 86.07 (12) 86.88 79.58 70.51 (10) 71.88 59.23
li 78.35 (11) 79.38 70.01 86.69 (6) 87.42 80.8 69.95 (11) 71.27 59.17
baldridge 77.49 (12) 78.57 68.53 86.67 (7) 87.42 80.64 67.92 (14) 69.35 55.95
chen 77.00 (13) 77.95 69.23 84.47 (16) 85.20 78.58 69.45 (12) 70.62 59.81
lee 76.90 (14) 77.96 68.34 84.82 (15) 85.69 77.83 68.71 (13) 69.95 58.63
sun 76.28 (15) 77.10 69.58 85.75 (13) 86.37 80.75 66.61 (15) 67.62 58.26
choi 71.23 (16) 72.22 63.44 77.56 (17) 78.58 69.46 64.78 (16) 65.72 57.4
trandabat 63.45 (17) 64.21 57.41 85.21 (14) 85.96 79.24 40.63 (17) 41.36 34.75
lluis 63.29 (18) 63.74 59.65 71.95 (18) 72.30 69.14 54.52 (18) 55.09 49.95
neumann 19.93 (19) 20.13 18.14 16.25 (19) 16.22 16.47 22.36 (19) 22.86 17.94
Table 10: Official results in the closed challenge (post-evaluation scores are available on the shared
task website). Teams are denoted by the last name of the first author of the corresponding paper in
the proceedings or the last name of the person who registered the team if no paper was submitted.
Italics indicate that there is no corresponding paper in the proceedings. Results are sorted in descending
order of the labeled macro F
1
score on the WSJ+Brown corpus. The number in parentheses next to the
WSJ+Brown scores indicates the system rank in the corresponding task.
Labeled Macro F
1
Labeled Attachment Score Labeled F
1
(complete task) (syntactic dependencies) (semantic dependencies)
WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown
vickrey ? ? ? ? ? ? 76.17 (1) 77.38 66.23
riedel ? ? ? ? ? ? 74.59 (2) 75.72 65.38
zhang 79.61 (1) 80.61 71.45 87.32 (1) 88.14 80.80 71.89 (3) 73.08 62.11
li 77.84 (2) 78.87 69.51 86.69 (2) 87.42 80.80 68.99 (4) 70.32 58.22
wang 76.19 (3) 78.39 59.89 84.56 (3) 85.50 77.06 67.12 (5) 70.41 42.67
Table 11: Official results in the open challenge (post-evaluation scores are available on the shared task
website). Teams are denoted by the last name of the first author of the corresponding paper in the
proceedings or the last name of the person who registered the team if no paper was submitted. Italics
indicate that there is no corresponding paper in the proceedings. Results are sorted in descending order of
the labeled F
1
score for semantic dependencies on the WSJ+Brown corpus. The number in parentheses
next to the WSJ+Brown scores indicates the system rank in the corresponding task.
5 Approaches
Table 5 summarizes the properties of the sys-
tems that participated in the closed the open chal-
lenges. The second column of the table high-
lights the overall architectures. We used + to in-
dicate that the components are sequentially con-
nected. The lack of a + sign indicates that the cor-
responding tasks are performed jointly. For exam-
ple, Riedel and Meza-Ruiz (2008) perform pred-
icate and argument identification and classifica-
tion jointly, whereas Ciaramita et al (2008) im-
plemented a pipeline architecture of three compo-
nents. We use the || to indicate that several differ-
ent architectures that span multiple subtasks were
deployed in parallel.
This summary of system architectures indicates
that it is common that systems combine sev-
eral components in the semantic or syntactic sub-
tasks ? e.g., nine systems jointly performed pred-
icate/argument identification and classification ?
but only four systems combined components be-
tween the syntactic and semantic subtasks: Hen-
derson et al (2008), who implemented a generative
history-based model (Incremental Sigmoid Belief
Networks with vectors of latent variables) where
syntactic and semantic structures are separately
170
generated but using a synchronized derivation (se-
quence of actions); Samuelsson et al (2008),
who, within an ensemble-based architecture, im-
plemented a joint syntactic-semantic model using
MaltParser with labels enriched with semantic in-
formation; Llu??s and M`arquez, who used a modi-
fied version of the Eisner algorithm to jointly pre-
dict syntactic and semantic dependencies; and fi-
nally, Sun et al (2008), who integrated depen-
dency label classification and argument identifi-
cation using a maximum-entropy Markov model.
Additionally, Johansson and Nugues (2008), who
had the highest ranked system in the closed chal-
lenge, integrate syntactic and semantic analysis in
a final reranking step, which maximizes the joint
syntactic-semantic score in the top k solutions. In
the same spirit, Chen et al (2008) search in the
top k solutions for the one that maximizes a global
measure, in this case the joint probability of the
complete problem. These joint learning strategies
are summarized in the Joint Learning/Opt. col-
umn in the table. The system of Riedel and Meza-
Ruiz (2008) deserves a special mention: even
though Riedel and Meza-Ruiz did not implement
a syntactic parser, they are the only group that per-
formed the complete SRL subtask ? i.e., predicate
identification and classification, argument identifi-
cation and classification ? jointly, simultaneously
for all the predicates in a sentence. They imple-
mented a joint SRL model using Markov Logic
Networks and they selected the overall best solu-
tion using inference based on the cutting-plane al-
gorithm.
Although some of the systems that implemented
joint approaches obtained good results, the top
five systems in the closed challenge are essen-
tially systems with pipeline architectures. Further-
more, Johansson and Nugues (2008) and Riedel
and Meza-Ruiz (2008) showed that joint learn-
ing/optimization improves the overall results, but
the improvement is not large. These initial ef-
forts indicate at least that the joint modeling of this
problem is not a trivial task.
The D Arch. and D Inference columns summa-
rize the parsing architectures and the correspond-
ing inference strategies. Similar to last year?s
shared task (Nivre et al, 2007), the vast majority of
parsing models fall in two classes: transition-based
(?trans? in the table) or graph-based (?graph?)
models. By and large, transition-based models use
a greedy inference strategy, whereas graph-based
models used different Maximum Spanning Tree
(MST) algorithms: Carreras (2007) ? MST
C
, Eis-
ner (2000) ? MST
E
, or Chu-Liu/Edmonds (Mc-
Donald et al, 2005; Chu and Liu, 1965; Edmonds,
1967) ? MST
CL/E
. More interestingly, most of
the best systems used some strategy to mitigate
parsing errors. In the top three systems in the
closed challenge, two (che and ciaramita) used
parser combination through voting and/or stacking
of different models (see the D Comb. column).
Samuelsson et al (2008) perform a MST infer-
ence with the bag of all dependencies output by
the individual MALT parser variants. Johansson
and Nugues (2008) use a single parsing model, but
this model is extended with second-order features.
The PA Arch. and PA Inference columns sum-
marize the architectures and inference strategies
used for the identification and classification of
predicates and arguments. The columns indicate
that most systems modeled the SRL problem as a
token-by-token classification problem (?class? in
the table) with a corresponding greedy inference
strategy. Some systems (e.g., yuret, samuelsson,
henderson, lluis) incorporate SRL within parsing,
in which case we report the corresponding parsing
architecture and inference approach. Vickrey and
Koller (2008) simplify the sentences to be labeled
using a set of hand-crafted rules before deploying
a classification model on top of a constituent-based
representation. Unlike in the case of parsing, few
systems (yuret, samuelssson, and morante) com-
bine several PA models and the combination is lim-
ited to simple voting strategies (see the PA Comb.
column).
Finally, the ML Methods column lists the Ma-
chine Learning (ML) methods used. The column
indicates that maximum entropy (ME) was the
most popular method (12 distinct systems relied
on it). Support Vector Machines (SVM) (eight sys-
tems) and the Perceptron algorithm (three systems)
were also popular ML methods.
6 Analysis
Section 4 summarized the results in the closed
and open challenges using the official evaluation
measures. In this section, we analyze the sub-
mitted runs using different evaluation measures,
e.g., Exact Match or Perfect Proposition F
1
scores,
and different views of the data, e.g., only non-
projective dependencies or NomBank versus Prop-
Bank frames.
171
O
v
e
r
a
l
l
D
D
D
P
A
P
A
P
A
J
o
i
n
t
M
L
c
l
o
s
e
d
A
r
c
h
.
A
r
c
h
.
C
o
m
b
.
I
n
f
e
r
e
n
c
e
A
r
c
h
.
C
o
m
b
.
I
n
f
e
r
e
n
c
e
L
e
a
r
n
i
n
g
/
O
p
t
.
M
e
t
h
o
d
s
j
o
h
a
n
s
s
o
n
D
+
P
I
+
P
C
+
A
I
+
A
C
g
r
a
p
h
n
o
M
S
T
C
c
l
a
s
s
n
o
r
e
r
a
n
k
r
e
r
a
n
k
P
e
r
c
e
p
t
r
o
n
,
M
E
c
h
e
D
+
P
I
+
P
C
+
A
I
C
g
r
a
p
h
s
t
a
c
k
i
n
g
M
S
T
C
L
/
E
c
l
a
s
s
n
o
I
L
P
n
o
M
E
c
i
a
r
a
m
i
t
a
D
+
P
I
C
+
A
I
C
t
r
a
n
s
v
o
t
i
n
g
,
g
r
e
e
d
y
c
l
a
s
s
n
o
r
e
r
a
n
k
n
o
S
V
M
,
M
E
,
s
t
a
c
k
i
n
g
P
e
r
c
e
p
t
r
o
n
z
h
a
o
D
+
A
I
C
+
P
I
+
P
C
t
r
a
n
s
n
o
g
r
e
e
d
y
c
l
a
s
s
n
o
g
r
e
e
d
y
n
o
M
E
y
u
r
e
t
D
+
(
P
I
C
+
A
I
+
A
C
|
|
g
r
a
p
h
n
o
M
S
T
E
c
l
a
s
s
,
v
o
t
i
n
g
g
r
e
e
d
y
n
o
M
L
E
,
P
I
C
+
A
I
C
)
g
e
n
e
r
a
t
i
v
e
M
B
L
s
a
m
u
e
l
s
s
o
n
D
+
P
I
+
t
r
a
n
s
M
S
T
C
L
/
E
g
r
e
e
d
y
c
l
a
s
s
,
v
o
t
i
n
g
g
r
e
e
d
y
u
n
i
fi
e
d
S
V
M
(
A
I
+
A
C
|
|
D
A
I
C
)
+
P
C
b
l
e
n
d
i
n
g
t
r
a
n
s
l
a
b
e
l
s
z
h
a
n
g
D
+
P
I
+
A
I
+
A
C
+
P
C
g
r
a
p
h
,
m
e
t
a
-
M
S
T
C
L
/
E
,
c
l
a
s
s
n
o
g
r
e
e
d
y
n
o
S
V
M
,
M
E
t
r
a
n
s
l
e
a
r
n
i
n
g
g
r
e
e
d
y
h
e
n
d
e
r
s
o
n
D
P
A
I
C
+
D
g
e
n
e
r
a
t
i
v
e
,
n
o
b
e
a
m
t
r
a
n
s
n
o
b
e
a
m
s
y
n
c
h
r
o
n
i
z
e
d
I
S
B
N
t
r
a
n
s
s
e
a
r
c
h
s
e
a
r
c
h
d
e
r
i
v
a
t
i
o
n
w
a
t
a
n
a
b
e
D
I
+
D
C
+
P
I
+
P
C
+
A
I
+
A
C
r
e
l
a
t
i
v
e
p
r
e
f
e
r
e
n
c
e
n
o
g
r
e
e
d
y
t
o
u
r
n
a
m
e
n
t
c
l
a
s
s
n
o
n
o
n
o
S
V
M
,
m
o
d
e
l
m
o
d
e
l
,
V
i
t
e
r
b
i
C
R
F
,
M
B
L
m
o
r
a
n
t
e
D
+
P
I
+
A
I
+
A
C
t
r
a
n
s
n
o
g
r
e
e
d
y
c
l
a
s
s
v
o
t
i
n
g
g
r
e
e
d
y
n
o
S
V
M
,
M
B
L
l
i
D
+
P
I
C
+
A
I
C
g
r
a
p
h
n
o
M
S
T
C
L
/
E
c
l
a
s
s
v
o
t
i
n
g
g
r
e
e
d
y
n
o
M
E
c
h
e
n
D
+
P
I
+
P
C
+
A
I
C
t
r
a
n
s
n
o
p
r
o
b
c
l
a
s
s
n
o
p
r
o
b
g
l
o
b
a
l
p
r
o
b
a
b
i
l
i
t
y
M
E
o
p
t
i
m
i
z
a
t
i
o
n
l
e
e
D
+
P
I
+
A
I
C
+
P
C
t
r
a
n
s
n
o
g
r
e
e
d
y
c
l
a
s
s
n
o
p
r
o
b
n
o
S
V
M
,
M
E
s
u
n
D
I
+
P
I
+
D
C
A
I
+
A
C
g
r
a
p
h
n
o
M
S
T
E
,
g
r
a
p
h
n
o
V
i
t
e
r
b
i
,
M
E
M
M
,
M
E
V
i
t
e
r
b
i
I
L
P
V
i
t
e
r
b
i
l
l
u
i
s
D
+
P
I
+
D
A
I
C
+
P
C
g
r
a
p
h
n
o
M
S
T
E
g
r
a
p
h
n
o
M
S
T
E
M
S
T
E
P
e
r
c
e
p
t
r
o
n
,
S
V
M
n
e
u
m
a
n
n
D
+
P
I
+
P
C
+
A
I
+
A
C
t
r
a
n
s
n
o
g
r
e
e
d
y
c
l
a
s
s
n
o
n
o
n
o
M
E
o
p
e
n
v
i
c
k
r
e
y
A
I
+
A
C
+
P
I
+
P
C
?
?
?
s
e
n
t
e
n
c
e
n
o
g
r
e
e
d
y
?
M
E
s
i
m
p
l
i
fi
c
a
t
i
o
n
,
c
l
a
s
s
r
i
e
d
e
l
P
A
I
C
?
?
?
M
a
r
k
o
v
n
o
C
u
t
t
i
n
g
?
M
I
R
A
L
o
g
i
c
P
l
a
n
e
N
e
t
w
o
r
k
w
a
n
g
P
I
+
A
I
C
t
r
a
n
s
,
n
o
g
r
e
e
d
y
,
c
l
a
s
s
n
o
p
r
o
b
n
o
S
V
M
,
M
E
g
r
a
p
h
M
S
T
C
L
/
E
M
I
R
A
T
a
b
l
e
1
2
:
S
u
m
m
a
r
y
o
f
s
y
s
t
e
m
a
r
c
h
i
t
e
c
t
u
r
e
s
t
h
a
t
p
a
r
t
i
c
i
p
a
t
e
d
i
n
t
h
e
c
l
o
s
e
d
a
n
d
o
p
e
n
c
h
a
l
l
e
n
g
e
s
.
T
h
e
c
l
o
s
e
d
-
c
h
a
l
l
e
n
g
e
s
y
s
t
e
m
s
a
r
e
s
o
r
t
e
d
b
y
m
a
c
r
o
l
a
b
e
l
e
d
F
1
s
c
o
r
e
o
n
t
h
e
W
S
J
+
B
r
o
w
n
c
o
r
p
u
s
.
B
e
c
a
u
s
e
s
o
m
e
o
p
e
n
-
c
h
a
l
l
e
n
g
e
s
y
s
t
e
m
s
d
i
d
n
o
t
i
m
p
l
e
m
e
n
t
s
y
n
t
a
c
t
i
c
p
a
r
s
i
n
g
,
t
h
e
s
e
s
y
s
t
e
m
s
a
r
e
s
o
r
t
e
d
b
y
l
a
b
e
l
e
d
F
1
s
c
o
r
e
o
f
t
h
e
s
e
m
a
n
t
i
c
d
e
p
e
n
d
e
n
c
i
e
s
o
n
t
h
e
W
S
J
+
B
r
o
w
n
c
o
r
p
u
s
.
O
n
l
y
t
h
e
s
y
s
t
e
m
s
t
h
a
t
h
a
v
e
a
c
o
r
r
e
s
p
o
n
d
i
n
g
p
a
p
e
r
i
n
t
h
e
p
r
o
c
e
e
d
i
n
g
s
a
r
e
i
n
c
l
u
d
e
d
.
S
y
s
t
e
m
s
t
h
a
t
p
a
r
t
i
c
i
p
a
t
e
d
i
n
b
o
t
h
c
h
a
l
l
e
n
g
e
s
a
r
e
l
i
s
t
e
d
o
n
l
y
i
n
t
h
e
c
l
o
s
e
d
c
h
a
l
l
e
n
g
e
.
A
c
r
o
n
y
m
s
u
s
e
d
:
D
-
s
y
n
t
a
c
t
i
c
d
e
p
e
n
d
e
n
c
i
e
s
,
P
-
p
r
e
d
i
c
a
t
e
,
A
-
a
r
g
u
m
e
n
t
,
I
-
i
d
e
n
t
i
fi
c
a
t
i
o
n
,
C
-
c
l
a
s
s
i
fi
c
a
t
i
o
n
.
O
v
e
r
a
l
l
a
r
c
h
.
s
t
a
n
d
s
f
o
r
t
h
e
c
o
m
p
l
e
t
e
s
y
s
t
e
m
a
r
c
h
i
t
e
c
t
u
r
e
;
D
A
r
c
h
.
s
t
a
n
d
s
f
o
r
t
h
e
a
r
c
h
i
t
e
c
t
u
r
e
o
f
t
h
e
s
y
n
t
a
c
t
i
c
p
a
r
s
e
r
;
D
C
o
m
b
.
i
n
d
i
c
a
t
e
s
i
f
t
h
e
fi
n
a
l
p
a
r
s
e
r
o
u
t
p
u
t
w
a
s
g
e
n
e
r
a
t
e
d
u
s
i
n
g
p
a
r
s
e
r
c
o
m
b
i
n
a
t
i
o
n
;
D
I
n
f
e
r
e
n
c
e
s
t
a
n
d
s
f
o
r
t
h
e
t
y
p
e
o
f
i
n
f
e
r
e
n
c
e
u
s
e
d
f
o
r
s
y
n
t
a
c
t
i
c
p
a
r
s
i
n
g
;
P
A
A
r
c
h
.
s
t
a
n
d
s
t
h
e
t
y
p
e
o
f
a
r
c
h
i
t
e
c
t
u
r
e
u
s
e
d
f
o
r
P
A
I
C
;
P
A
C
o
m
b
.
i
n
d
i
c
a
t
e
s
i
f
t
h
e
P
A
o
u
t
p
u
t
w
a
s
g
e
n
e
r
a
t
e
d
t
h
r
o
u
g
h
s
y
s
t
e
m
c
o
m
b
i
n
a
t
i
o
n
;
P
A
I
n
f
e
r
e
n
c
e
s
t
a
n
d
s
f
o
r
t
h
e
t
h
e
t
y
p
e
o
f
i
n
f
e
r
e
n
c
e
u
s
e
d
f
o
r
P
A
I
C
;
J
o
i
n
t
L
e
a
r
n
i
n
g
/
O
p
t
.
i
n
d
i
c
a
t
e
s
i
f
s
o
m
e
f
o
r
m
o
f
j
o
i
n
t
l
e
a
r
n
i
n
g
o
r
o
p
t
i
m
i
z
a
t
i
o
n
w
a
s
i
m
p
l
e
m
e
n
t
e
d
f
o
r
t
h
e
s
y
n
t
a
c
t
i
c
+
s
e
m
a
n
t
i
c
g
l
o
b
a
l
t
a
s
k
;
M
L
m
e
t
h
o
d
s
l
i
s
t
s
t
h
e
M
L
m
e
t
h
o
d
s
u
s
e
d
t
h
r
o
u
g
h
o
u
t
t
h
e
c
o
m
p
l
e
t
e
s
y
s
t
e
m
.
172
Exact Match Perfect Proposition F
1
(complete task) (semantic dependencies)
closed WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown
johansson 12.46 (1) 12.46 12.68 54.12 (1) 56.12 36.90
che 10.37 (2) 10.21 11.50 48.05 (2) 50.15 30.90
ciaramita 9.27 (3) 9.04 10.80 46.05 (3) 48.05 28.61
zhao 9.20 (4) 9.00 10.56 43.19 (4) 45.23 26.14
henderson 8.11 (5) 7.75 10.33 39.24 (5) 40.64 27.51
watanabe 7.79 (6) 7.54 9.39 36.44 (6) 38.09 22.72
yuret 7.65 (7) 7.33 9.62 34.61 (9) 36.13 21.78
zhang 7.40 (8) 7.46 7.28 34.96 (8) 36.25 24.22
li 7.12 (9) 6.71 9.62 32.08 (10) 33.45 20.62
samuelsson 6.94 (10) 6.62 8.92 35.20 (7) 36.96 20.22
chen 6.83 (11) 6.46 9.15 31.02 (12) 32.08 22.14
lee 6.69 (12) 6.29 9.15 31.40 (11) 32.52 22.18
morante 6.44 (13) 6.04 8.92 30.41 (14) 31.97 17.49
sun 5.38 (14) 4.96 7.98 30.43 (13) 31.51 21.40
baldridge 5.24 (15) 4.92 7.28 25.35 (15) 26.57 15.26
choi 3.33 (16) 3.50 2.58 24.77 (16) 25.71 17.37
trandabat 3.26 (17) 3.08 4.46 6.59 (18) 6.81 4.76
lluis 2.55 (18) 1.96 6.10 16.07 (17) 16.46 13.00
neumann 0.11 (19) 0.12 0.23 0.30 (19) 0.31 0.20
open
vickrey ? ? ? 44.94 (1) 46.68 30.28
riedel ? ? ? 42.77 (2) 44.18 31.15
zhang 8.14 (1) 8.04 8.92 35.46 (3) 36.74 24.84
li 6.90 (2) 6.46 9.62 29.91 (4) 31.30 18.41
wang 5.17 (3) 5.12 5.63 18.63 (5) 20.31 7.09
Table 13: Exact Match and Perfect Proposition F
1
scores for runs submitted in the closed and open
challenges. The closed-challenge systems are sorted in descending order of Exact Match scores on
the WSJ+Brown corpus. Open-challenge submissions are sorted in descending order of the Perfect
Proposition F
1
score. The number in parentheses next to the WSJ+Brown scores indicates the system
rank according to the corresponding scoring measure.
6.1 Exact Match and Perfect Propositions
Table 13 lists the Exact Match and Perfect Propo-
sition F
1
scores for test runs submitted in both
challenges. Both these scores measure the capac-
ity of a system to correctly parse structures with
granularity much larger than a simple dependency,
i.e., entire sentences for Exact Match and complete
propositions for Perfect Proposition F
1
(see Sec-
tion 2.2.2 for a formal definition of these evalua-
tion measures). The table indicates that these val-
ues are much smaller than the scores previously
reported, e.g., labeled macro F
1
. This is to be
expected: the probability of an incorrectly parsed
unit (sentence or proposition) is much larger given
its granularity. However, the main purpose of this
analysis is to investigate if systems that focused
on joint learning or optimization performed bet-
ter than others with respect to these global mea-
sures. This indeed seems to be the case for at
least two systems. The system of Johansson and
Nugues (2008), which jointly optimizes the la-
beled F
1
score (for semantic dependencies) and
then the labeled macro F
1
score (for the complete
task), increases its distance from the next ranked
system: its Perfect Proposition F
1
score is over
6 points higher than the score of the second sys-
tem in Table 13. The system of Henderson et
al. (2008), which was designed for joint learning
of the complete task, improves its rank from eighth
to fifth compared to the official results (Table 10).
6.2 Nonprojectivity
Table 14 shows the unlabeled F1 scores for pre-
diction of nonprojective syntactic dependencies.
Since nonprojectivity is quite rare, many teams
chose to ignore this issue. The table shows only
those systems that submitted well-formed depen-
dency trees, and whose output contained at least
one nonprojective link. The small number of non-
projective links in the training set makes it hard to
learn to predict such links, and this is also reflected
in the figures. In general, the figures for nonpro-
jective wh-movements and split clauses are higher,
and they are also the most common types. Also,
they are detectable by fairly simple patterns, such
as the presence of a wh-word or a pair of commas.
173
System All wh-mov. SpCl SpNP
choi 25.43 49.49 45.47 8.72
lee 46.26 50.30 64.84 20.69
nugues 46.15 58.96 59.26 11.32
samuelsson 24.47 38.15 0 9.83
titov 42.32 50.56 48.71 0
zhang 13.39 5.71 12.33 7.3
Table 14: Unlabeled F1-measures for nonprojec-
tive links. Results are given for all links, wh-
movements, split clauses, and split noun phrases.
6.3 Normalized SRL Performance
Table 6.3 lists the scores for the semantic sub-
task measured as the ratio of the labeled F
1
score
and LAS. As previously mentioned, this score es-
timates the performance of the SRL component
independent of the performance of the syntactic
parser. This analysis is not a substitute for the
actual experiment where the SRL components are
evaluated using correct syntactic information but,
nevertheless, it indicates several interesting facts.
First, the ranking of the top three systems in Ta-
ble 10 changes: the system of Che et al (2008)
is now ranked first, and the system of Johansson
and Nugues (2008) is second. This shows that Che
et al have a relatively stronger SRL component,
whereas Johansson and Nugues developed a bet-
ter parser. Second, several other systems improved
their ranking compared to Table 10: e.g., chen
from position thirteenth to ninth and choi from six-
teenth to eighth. This indicates that these systems
were penalized in the official ranking mainly due
to the relative poor performance of their parsers.
Note that this experiment is relevant only for
systems that implemented pipeline architectures,
where the semantic components are in fact sep-
arated from the syntactic ones; this excludes the
systems that blended syntax with SRL: henderson,
sun, and lluis. Furthermore, systems that had sig-
nificantly lower scores in syntax will receive an un-
reasonable boost in ranking according to this mea-
sure. Fortunately, there was only one such outlier
in this evaluation (neumann), shown in gray in the
table.
6.4 PropBank versus NomBank
Table 16 lists the labeled F
1
scores for semantic
dependencies for two different views of the test-
ing data sets: for propositions centered around ver-
bal predicates, i.e., from PropBank, and for propo-
sitions centered around nominal predicates, i.e.,
from NomBank.
Labeled F
1
/ LAS
closed WSJ+Brown WSJ Brown
neumann 137.60 (1) 140.94 108.93
che 90.51 (2) 91.42 82.21
johansson 89.98 (3) 90.70 83.40
ciaramita 89.49 (4) 90.32 81.89
zhao 87.88 (5) 88.75 79.78
yuret 84.35 (6) 85.30 75.34
samuelsson 84.20 (7) 85.24 74.51
choi 83.52 (8) 83.63 82.64
chen 82.22 (9) 82.89 76.11
morante 81.92 (10) 82.73 74.43
zhang 81.67 (11) 82.45 74.46
henderson 81.66 (12) 82.32 75.47
watanabe 81.26 (13) 82.18 72.61
lee 81.01 (14) 81.63 75.33
li 80.69 (15) 81.53 73.23
baldridge 78.37 (16) 79.33 69.38
sun 77.68 (17) 78.29 72.15
lluis 75.77 (18) 76.20 72.24
trandabat 47.68 (19) 48.12 43.85
open
zhang 82.33 82.91 76.87
li 79.58 80.44 72.05
wang 79.38 82.35 55.37
Table 15: Ratio of the labeled F
1
score for seman-
tic dependencies and LAS for syntactic dependen-
cies. Systems are sorted in descending order of this
ratio score on the WSJ+Brown corpus. We only
show systems that participated in both the syntac-
tic and semantic subtasks.
The table indicates that, generally, systems per-
formed much worse on nominal predicates than
on verbal predicates. This is to be expected con-
sidering that there is significant body of previ-
ous work that analyzes the SRL problem on Prop-
Bank, but minimal work for NomBank. On aver-
age, the difference between the labeled F
1
scores
for verbal predicates and nominal predicates on the
WSJ+Brown corpus is 7.84 points. Furthermore,
the average difference between labeled F
1
scores
on the Brown corpus alone is 12.36 points. This in-
dicates that the problem of SRL for nominal predi-
cates is more sensitive to domain changes than the
equivalent problem for verbal predicates. Our con-
jecture is that, because there is very little syntac-
tic structure between nominal predicates and their
arguments, SRL models for nominal predicates se-
lect mainly lexical features, which are more brittle
than syntactic or other non-lexicalized features.
Remarkably, there is one system (baldridge)
which performed better on the WSJ+Brown for
nominal predicates than verbal predicates. Un-
fortunately, this group did not submit a system-
description paper so it is not clear what was their
approach.
174
Labeled F
1
Labeled F
1
(verbal predicates) (nominal predicates)
closed WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown
johansson 84.45 (1) 86.37 71.87 74.32 (2) 75.42 60.13
che 80.46 (2) 82.17 69.33 75.18 (1) 76.64 56.87
ciaramita 80.15 (3) 82.09 67.62 73.17 (4) 74.42 57.69
zhao 77.67 (4) 79.40 66.38 73.28 (3) 74.69 54.81
samuelsson 76.17 (5) 78.03 64.00 68.13 (7) 69.58 49.24
yuret 75.91 (6) 77.88 63.02 68.81 (5) 69.98 53.58
zhang 74.82 (7) 76.62 63.15 65.61 (11) 66.82 50.18
li 74.36 (8) 76.14 62.92 62.61 (14) 63.76 47.09
henderson 73.80 (9) 75.40 63.36 66.26 (10) 67.44 50.73
watanabe 73.06 (10) 75.02 60.34 67.15 (8) 68.37 50.92
sun 72.97 (11) 74.45 63.50 58.68 (15) 59.73 45.75
morante 72.81 (12) 74.36 62.72 66.50 (9) 67.92 47.97
lee 72.34 (13) 74.15 60.49 62.83 (13) 63.66 52.18
chen 72.02 (14) 73.49 62.46 65.02 (12) 66.14 50.48
choi 70.00 (15) 71.28 61.71 56.16 (16) 57.19 44.05
baldridge 67.02 (16) 68.64 56.50 68.57 (6) 69.78 52.96
lluis 62.42 (17) 63.49 55.49 42.15 (17) 42.81 34.22
trandabat 42.88 (18) 43.79 37.06 37.14 (18) 37.89 27.50
neumann 22.87 (19) 23.53 18.24 21.7 (19) 22.04 17.14
open
vickrey 78.41 (1) 79.75 69.57 71.86 (1) 73.29 53.25
riedel 77.13 (2) 78.72 66.75 70.25 (2) 71.03 60.17
zhang 75.00 (3) 76.62 64.44 66.76 (3) 67.79 53.76
li 73.74 (4) 75.57 62.05 61.24 (5) 62.38 46.36
wang 67.50 (5) 70.34 49.72 66.53 (4) 69.83 28.96
Table 16: Labeled F
1
scores for frames centered around verbal and nominal predicates. The number in
parentheses next to the WSJ+Brown scores indicates the system rank in the corresponding data set.
Systems can mitigate the inherent differences
between verbal and nominal predicates with dif-
ferent models for the two sub-problems. This was
indeed the approach taken by two out of the top
three systems (johansson and che). Johansson and
Nugues (2008) developed different models for ver-
bal and nominal predicates and implemented sep-
arate feature selection processes for each model.
Che et al (2008) followed the same method but
they also implemented separate domain constraints
for inference for the two models.
7 Conclusion
The previous four CoNLL shared tasks popular-
ized and, without a doubt, boosted research in se-
mantic role labeling and dependency parsing. This
year?s shared task introduces a new task that es-
sentially unifies the problems addressed in the past
four years under a unique, dependency-based for-
malism. This novel task is attractive both from
a research perspective and an application-oriented
perspective:
? We believe that the proposed dependency-
based representation is a better fit for many
applications (e.g., Information Retrieval, In-
formation Extraction) where it is often suffi-
cient to identify the dependency between the
predicate and the head of the argument con-
stituent rather than extracting the complete ar-
gument constituent.
? It was shown that the extraction of syntac-
tic and semantic dependencies can be per-
formed with state-of-the-art performance in
linear time (Ciaramita et al, 2008). This can
give a significant boost to the adoption of this
technology in real-world applications.
? We hope that this shared task will motivate
several important research directions. For ex-
ample, is the dependency-based representa-
tion better for SRL than the constituent-based
formalism? Does joint learning improve syn-
tactic and semantic analysis?
? Surface (string related patterns, syntax, etc.)
linguistic features can often be detected with
greater reliability than deep (semantic) fea-
tures. In contrast, deep features can cover
more ground because they regularize across
differences in surface strings. Machine learn-
ing systems can be more effective by using
evidence from both deep and surface features
jointly (Zhao, 2005).
175
Even though this shared task was more complex
than the previous shared tasks, 22 different teams
submitted results in at least one of the challenges.
Building on this success, we hope to expand this
effort in the future with evaluations on multiple
languages and on larger out-of-domain corpora.
Acknowledgments
We want to thank the following people who helped
us with the generation of the data sets: Jes?us
Gim?enez, for generating the predicted POS tags
with his SVMTool POS tagger, and Massimiliano
Ciaramita, for generating columns 1, 2 and 3 in the
open-challenge corpus with his semantic tagger.
We also thank the following people who helped
us with the organization of the shared task: Paola
Merlo and James Henderson for the idea and the
implementation of the Exact Match measure, Se-
bastian Riedel for his dependency visualization
software,
12
Hai Zhao, for the the idea of the F
1
ratio score, and Carlos Castillo, for help with the
shared task website. Last but not least, we thank
the organizers of the previous four shared tasks:
Sabine Buchholz, Xavier Carreras, Ryan McDon-
ald, Amit Dubey, Johan Hall, Yuval Krymolowski,
Sandra K?ubler, Erwin Marsi, Jens Nilsson, Sebas-
tian Riedel, and Deniz Yuret. This shared task
would not have been possible without their previ-
ous effort.
Mihai Surdeanu is a research fellow in the
Ram?on y Cajal program of the Spanish Ministry of
Science and Technology. Richard Johansson was
funded by the Swedish National Graduate School
of Language Technology (GSLT). Adam Meyers?
participation was supported by the National Sci-
ence Foundation, award CNS-0551615 (Towards
a Comprehensive Linguistic Annotation of Lan-
guage) and IIS-0534700 (Collaborative Research:
Structure Alignment-based Machine Translation).
Llu??s M`arquez?s participation was supported by
the Spanish Ministry of Education and Science,
through research projects Trangram (TIN2004-
07925-C03-02) and OpenMT (TIN2006-15307-
C03-02).
References
X. Carreras. 2007. Experiments with a Higher-Order
Projective Dependency Parser. In Proc. of CoNLL-
2007 Shared Task.
12
http://code.google.com/p/whatswrong/
X. Carreras and L. M`arquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling.
In Proc. of CoNLL-2005.
X. Carreras and L. M`arquez. 2004. Introduction to the
CoNLL-2004 Shared Task: Semantic Role Labeling.
In Proc. of CoNLL-2004.
W. Che, Z. Li, Y. Hu, Y. Li, B. Qin, T. Liu and S.
Li. 2008. A Cascaded Syntactic and Semantic De-
pendency Parsing System. In Proc. of CoNLL-2008
Shared Task.
E. Chen, L. Shi and D. Hu. 2008. Probabilistic Model
for Syntactic and Semantic Dependency Parsing. In
Proc. of CoNLL-2008 Shared Task.
Chinchor, N. and P. Robinson. 1998. MUC-7
Named Entity Task Definition. In Proc. of Seventh
Message Understanding Conference (MUC-7).
http://www.itl.nist.gov/iaui/894.02/related projects/
/muc/proceedings/muc 7 toc.html.
Chomsky, Noam. 1981. Lectures on Government and
Binding. Foris Publications, Dordrecht.
Y.J. Chu and T.H. Liu. 1965. On the Shortest Ar-
borescence of a Directed Graph. In Science Sinica,
14:1396-1400.
M. Ciaramita, G. Attardi, F. Dell?Orletta, and M. Sur-
deanu. 2008. DeSRL: A Linear-Time Semantic
Role Labeling System. In Proc. of CoNLL-2008
Shared Task.
M. Ciaramita and Y. Altun. 2006. Broad Coverage
Sense Disambiguation and Information Extraction
with a Supersense Sequence Tagger. In Proc. of
EMNLP.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. Edmonds. 1967. Optimum Branchings. In Jour-
nal of Research of the National Bureau of Standards,
71B:233?240.
J. Eisner. 2000. Bilexical Grammars and Their Cubic-
Time Parsing Algorithms. New Developments in
Parsing Algorithms, Kluwer Academic Publishers.
W. N. Francis and H. Ku?cera. 1964. Brown Corpus.
Manual of Information to accompany A Standard
Corpus of Present-Day Edited American English, for
use with Digital Computers. Revised 1971, Revised
and Amplified 1979.
C. Fellbaum, editor. 1998. WordNet: An electronic
lexical database. MIT Press.
J. Gim?enez and L. M`arquez. 2004. SVMTool: A gen-
eral POS tagger generator based on Support Vector
Machines. In Proc. of LREC.
K. Hacioglu. 2004. Semantic Role Labeling Using De-
pendency Trees. In Proc. of COLING-2004.
176
J. Henderson, P. Merlo, G. Musillo and I. Titov. 2008.
A Latent Variable Model of Synchronous Parsing for
Syntactic and Semantic Dependencies. In Proc. of
CoNLL-2008 Shared Task.
R. Johansson and P. Nugues. 2008. Dependency-
based Syntactic?Semantic Analysis with PropBank
and NomBank. In Proc. of CoNLL-2008 Shared
Task.
R. Johansson and P. Nugues. 2007. Extended
Constituent-to-Dependency Conversion for English.
In Proc. of NODALIDA.
X. Llu??s and L. M`arquez. 2008. A Joint Model for
Parsing Syntactic and Semantic Dependencies. In
Proc. of CoNLL-2008 Shared Task.
D. Magerman. 1994. Natural Language Parsing as Sta-
tistical Pattern Recognition. Ph.D. thesis, Stanford
University.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a Large Annotated Corpus of En-
glish: the Penn Treebank. Computational Linguis-
tics, 19.
R. McDonald, F. Pereira, K. Ribarov and J. Hajic.
2005. Non-Projective Dependency Parsing using
Spanning Tree Algorithms In Proc. of HLT-EMNLP.
A. Meyers, R. Grishman, M. Kosaka, and S. Zhao.
2001. Covering Treebanks with GLARF. In Proc.
of the ACL/EACL 2001 Workshop on Sharing Tools
and Resources for Research and Education.
Meyers, A., R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank Project: An Interim Report. In
NAACL/HLT 2004 Workshop Frontiers in Corpus
Annotation, Boston.
J. Nivre, J. Hall, J. Nilsson and G. Eryigit. 2006. La-
beled Pseudo-Projective Dependency Parsing with
Support Vector Machines. In Proc. of CoNLL-X
Shared Task.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nilsson,
S. Riedel, D. Yuret. 2007. The CoNLL 2007 Shared
Task on Dependency Parsing. In Proc. of CoNLL-
2007.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryi?git, S.
K?ubler, S. Marinov, and E. Marsi. 2007b. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(2):95?135.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Seman-
tic Roles. Computational Linguistics, 31(1).
S. Riedel and I. Meza-Ruiz. 2008. Collective Seman-
tic Role Labelling with Markov Logic. In Proc. of
CoNLL-2008 Shared Task.
Y. Samuelsson, O. T?ackstr?om, S. Velupillai, J. Eklund,
M. Fishel and M. Saers. 2008. Mixing and Blending
Syntactic and Semantic Dependencies. In Proc. of
CoNLL-2008 Shared Task.
W. Sun, H. Li and Z. Sui. 2008. The Integration of De-
pendency Relation Classification and Semantic Role
Labeling Using Bilayer Maximum Entropy Markov
Models. In Proc. of CoNLL-2008 Shared Task.
E. F. Tjong Kim San and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 Shared Task: Language-
Independent Named Entity Recognition. In Proc. of
CoNLL-2003.
D. Vickrey and D. Koller. 2008. Applying Sentence
Simplification to the CoNLL-2008 Shared Task. In
Proc. of CoNLL-2008 Shared Task.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Technical report,
Linguistic Data Consortium.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines.
In Proc. of IWPT.
Y. Zhang, R. Wang and H. Uszkoreit. 2008. Hybrid
Learning of Dependency Structures from Heteroge-
neous Linguistic Resources. In Proc. of CoNLL-
2008 Shared Task.
Zhao, S. 2005. Information Extraction from Multiple
Syntactic Sources. Ph.D. thesis, NYU.
177
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 88?97,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Improving MT Word Alignment Using Aligned Multi-Stage Parses
Adam Meyers?, Michiko Kosaka?, Shasha Liao? and Nianwen Xue?
? New York University, ?Monmouth University, ?Brandeis University
Abstract
We use hand-coded rules and graph-aligned
logical dependencies to reorder English text
towards Chinese word order. We obtain a
1.5% higher F-score for Giza++ compared to
running with unprocessed text. We describe
this research and its implications for SMT.
1 Introduction
Some statistical machine translation (SMT) systems
use pattern-based rules acquired from linguistically
processed bitexts. They acquire these rules through
the alignment of a parsed structure in one language
with a raw string in the other language (Yamada and
Knight, 2001; Shen et al, 2008) or the alignment
of source/target language parse trees (Zhang et al,
2008; Cowan, 2008). This paper shows that ma-
chine translation (MT) can also benefit by aligning a
?deeper? level of analysis than parsed text, which in-
cludes semantic role labeling, regularization of pas-
sives and wh constructions, etc. We create GLARF
representations (Meyers et al, 2009) for English and
Chinese sentences, in the form of directed acyclic
graphs. We describe two graph-based techniques
for reordering English sentences to be closer to that
of corresponding Chinese sentences. One technique
is based on manually created rules and the other is
based on an automatic alignment of GLARF repre-
sentations of Chinese/English sentences. After re-
ordering, we align words of the reordered English
with the words of the Chinese, using the Giza++
word aligner(Och and Ney, 2003). For both tech-
niques, the resulting alignment has a higher F-score
than Giza++ on raw text (a 0.7% to 1.5% absolute
improvement). In principle, our reordered text can
be used to improve any Chinese/English SMT sys-
tem for which Giza++ (or other word aligners) are
part of the processing pipeline.
These experiments are a first step in using
GLARF-style analyses for MT, potentially improv-
ing systems that already perform well with aligned
text lacking large gaps in surface alignment. We hy-
pothesize that SMT systems are most likely to ben-
efit from deep analysis for structures where source
and target language word order differs the most. We
propose using deep analysis to reorder such struc-
tures in one language to more closely reflect the
word order of the other language. The text would be
reordered at two stages in an SMT system: (1) prior
to acquiring a translation model; and (2) either prior
to translation (if source text is reordered) or after
translation (if target text is reordered). Our system
moves large constituents (e.g., noun post-modifiers)
to bring English word order closer to that of parallel
Chinese sentences. This improves word alignment
and is likely to improve SMT.
For this work we use two English/Chinese bitext
corpora developed by the Linguistic Data Consor-
tium (LDC): the Tides FBIS corpus and the GALE
Y1 Q4 Chinese/English Word-Alignment corpus.
We used 2300 aligned sentences from FBIS for de-
velopment purposes. We divided the GALE corpus
into into a 3407 sentence development subcorpus
(DEV) and a 1505 sentence test subcorpus (TEST).
We used the LDC?s manual alignments of the FBIS
corpus to score these data.
88
2 Related Work in SMT
Four papers stand out as closely related to the
present study. (Collins et al, 2005; Wang et al,
2007) describe experiments which use manually cre-
ated parse-tree-based rules to reorder one side of
a bitext: German/English in (Collins et al, 2005)
and English/Chinese in (Wang et al, 2007). Both
achieve BLEU score improvements for SMT: 25.2%
to 26.8% for (Collins et al, 2005) and 28.52 to 30.86
for (Wang et al, 2007). (Wang et al, 2007) uses
rules very similar to our own as they use the same
language pair, although they reorder the Chinese,
whereas we reorder the English. The most signifi-
cant differences between our research and (Collins
et al, 2005; Wang et al, 2007) are: (1) our manual
rules benefit from a level of representation ?deeper?
than a surface parse; and (2) In addition to the hand-
coded rules, we also use automatic alignment-based
rules. (Wu and Fung, 2009) uses PropBank role la-
bels (Palmer et al, 2005) as the basis of a second
pass filter over an SMT system to improve the BLEU
score from 42.99 to 43.51. The main similarity to
the current study is the use of a level of represen-
tation that is ?deeper? than a surface parse. How-
ever, our application of linguistic structure is more
like that of (Wang et al, 2007) and our ?deep? level
connects all predicates and arguments in the sen-
tence, regardless of part of speech, rather than just
connecting verbs to their arguments. (Bryl and van
Genabith, 2010) describes an open source LFG F-
structure alignment tool with an algorithm similar to
our previous work. They evaluate their alignment
output on 20 manually-aligned German and English
F-structures. They leave the impact of their work on
MT to future research.
In addition to these papers, there has also been
some work on rule-based reordering preprocessors
to word alignment based on shallower linguistic in-
formation. For example (Crego and Marin?o, 2006)
reorders based on patterns of POS tags. We hypoth-
esize that this is similar to the above approaches in
that patterns of POS tags are likely to simulate pars-
ing or chunking.
3 Preparing the Data
The two stage parsers of previous decades (Hobbs
and Grishman, 1976) generated a syntactic repre-
sentation analogous to the (more accurate) output
of current treebank-based parsers (Charniak, 2001)
and an additional second stage output that regular-
ized constructions (passive, active, relative clauses)
to representations similar to active clauses with no
gaps, e.g., The book was read by Mary was given a
representation similar to that of Mary read the book.
Treating the active clause as canonical provides a
way to reduce variation in language and thus, mak-
ing it easier to acquire and apply statistical informa-
tion from corpora?there is more evidence for partic-
ular statistical patterns when applications learn pat-
terns and patterns more readily match data.
Two-stage parsers were influenced by linguistic
theories (Harris, 1968; Chomsky, 1957; Bresnan and
Kaplan, 1982) which distinguish a ?surface? and a
?deep? level. The deep level neutralizes differences
between ways to express the same meaning?a pas-
sive like The cheese was eaten by rats was analyzed
in terms of the active form Rats ate the cheese. Cur-
rently ?semantic parsing? refers to a similar repre-
sentation, e.g., (Wagner et al, 2007) or our own
GLARF (Meyers et al, 2009). However, the term is
also used for semantic role labelers (Gildea and Ju-
rafsky, 2002; Xue, 2008), systems which typically
label semantic relations between verbs and their ar-
guments and rarely cover arguments of other parts
of speech. Second stage semantic parsers like our
own, connect all the tokens in the sentence. Aligned
text processed in this way can (for example) repre-
sent differences in English/Chinese noun modifier
order, including relative clauses. In contrast, few
role labelers handle noun modifiers and none han-
dle relative clauses. Below, we describe the GLARF
framework and our system for generating GLARF
representations of English and Chinese sentences.
For each language, we combine several types of
information which may include: named entity (NE)
tagging, date/number regularization, recognition of
multi-word expressions (the preposition with respect
to, the noun hand me down and the verb ad lib),
role labels for predicates of all parts of speech, regu-
larizing passives and other constructions, error cor-
rection, among other processes into a single typed
feature structure (TFS) representation. This TFS
is converted into a set of 25-tuples representing
dependency-style relations between pairs of words
in the sentence. Three types of dependencies are
89
n1
know
SBJ OBJ
n3
of
n5 OBJ
SBJ OBJ
N?POS
COMP
n4
the
Q?POS
n2? n3?
n6?
n1?
I rules
tennis
n6
n2
 
 
  
Figure 1: Word-Aligned Logic1 Dependencies
represented: surface dependencies (close to the level
of the parser), logic1 dependencies (reflecting var-
ious regularizations) and logic2 dependencies (re-
flecting the output of a PropBanker, NomBanker
and Penn Discourse Treebank transducer).(Palmer
et al, 2005; Xue and Palmer, 2003; Meyers et al,
2004; Miltsakaki et al, 2004) The surface depen-
dency graph is a tree; The logic1 dependency graph
is an directed acyclic graph; and The logic2 depen-
dency graph is a directed graph with cycles, cover-
ing only a subset of the tokens in the sentence. For
these experiments, we focus on the logic1 relations,
but will sometimes use the surface relations as well.
Figure 1 is a simple dependency-based logic1 repre-
sentation of I know the rules of tennis and its Chi-
nese translation. The edge labels name the relations
between heads and dependents, e.g., I is the SBJ of
know and the dashed lines indicate word level corre-
spondences. Each node is labeled with both a word
and a unique node identifier (n1, n1?, etc.)
The English system achieves F-scores for logic1
dependencies on parsed news text in the 80?90%
range and the Chinese system achieves F-scores in
the 74?84% range, depending on the complexity of
the text. The English system has been created over
the course of about 9 years, and consequently is
more extensive than the Chinese system, which has
been created over the past 3 years. The systems are
described in more detail in (Meyers et al, 2009).
The GLARF representations are created in a se-
ries of steps involving several processors. The En-
glish pipeline includes: (1) dividing text into sen-
tences; (2) running the JET NE tagger (Ji and Gr-
ishman, 2006); (3) running scripts that clean up data
(to prevent parser crashes); (4) running a parser (cur-
rently Charniak?s 2005 parser based on (Charniak,
2001)); (5) running filters that: (a) correct com-
mon parsing errors; (b) merge NE information with
the parse, resolving conflicts in constituent bound-
aries by hand-coded rules; (c) regularize numbers,
dates, times and holidays; (d) identify heads and
label relations between constituents; (e) regularize
text grammatically (filling empty subjects, resolv-
ing relative clause and Wh gaps, etc.); (f) mark con-
junction scope; (g) identify transparent constituents
(e.g., recognizing, that A variety of different peo-
ple has the semantic features of people (human), not
those of variety, the syntactic head of the phrase.);
among other aspects. The Chinese pipeline is simi-
lar, except that it includes the LDC word segmenter
and a PropBanker (Xue, 2008). Also, the regulariza-
tion routines are not as completely developed, e.g.,
relative clause gaps and passives are not handled
yet. The Chinese system currently uses the Berke-
ley parser (Petrov and Klein, 2007). Each of these
pipelines derives typed feature structure representa-
tions, which are then converted into the 25 tuple rep-
resentation of 3 types of dependencies between pairs
of tokens: surface, logic1 and logic2.
To insure that the logic1 graphs are acyclic, we as-
sume that certain edges are surface only and that the
resulting directed acyclic graphs can have multiple
roots. It turns out that the multiple rooted cases are
mostly limited to a few constructions, the most com-
mon being parenthetical clauses and relative clauses.
A parenthetical clause takes the main clause as an
argument. For example, in The word ?potato?, he
claimed, is spelled with a final ?e?., the verb claimed,
takes the entire main clause as an argument, we as-
sume that he claimed is a dependent on the main
verb (is) spelled labeled PARENTHETICAL in our
surface dependency structure, but that the main verb
(is) spelled is a dependent of the verb claimed in
our logic1 structure, labeled COMPLEMENT. Thus
the logic1 surface dependency structure have dis-
tinct roots. In a relative clause, such as the book that
I read?, we assume that the clause that I read is a de-
pendent on the noun book in our surface dependency
structure with the label RELATIVE, but book is a de-
pendent on the verb read in our logic1 dependency
structure, with the label OBJ. This, means that our
logic1 dependency graphs for sentences containing
relative clauses are multi-rooted. One of the roots is
the same as the root of the surface tree and the other
root is the root of the relative clause graph (a rela-
90
tive pronoun or a main verb). Furthermore, there is
a surface path connecting the relative clause root to
the rest of the graph. Noncyclic graph traversal is
possible, provide that: (1) we use the surface path to
enter the graph representing the relative clause ? oth-
erwise, the traversal would skip the relative clause;
and (2) we halt the traversal if we reach this path a
second time ? this avoids traversing down an end-
less path. The parenthetical and relative clause are
representative of the handful of cases in which naive
representations would introduce loops. All cases of
which we are aware have the essential properties of
one of these two cases: (1) either introducing a dif-
ferent single root of the clause; or (2) introducing an
additional root that can be bridged by a surface path.
4 Manual Reordering Rules
We derived manual rules for making the English
Word Order more like the Chinese by manually in-
specting the data. We inspected the first 100-200
sentences of the DEV corpus by first transliterating
the Chinese into English ? replaced each Chinese
word with the aligned English counterpart. Several
patterns emerged which were easy to formalize into
rules in the GLARF framework. These patterns were
verified and sometimes generalized through discus-
sions with native Chinese speakers and linguists.
Our rules, similar to those of (Wang et al, 2007) are
as follows (results are discussed in section 6): (1)
Front a post-nominal PP headed by a preposition in
the list {of, in, with, about)}. (2) Front post-nominal
relative clause that begins with that or does not have
any relative pronoun, such that the main predicate is
not a copula plus adjective construction. (3) Front
post-nominal relative clause that begins with that or
has no relative pronoun if the main predicate is a
copula+adjective construction which is not negated
by a word from the set {no neither nor never not
n?t}. (4) Front post-nominal reduced relative in the
form of a passive or adjectival phrase. (5) Move ad-
verbials more than and less than after numbers that
they modify. (6) Move PPs that post-modify adjec-
tives to the position before the adjective. (7) Move
subordinate conjunctions before and after to the end
of the clause that they introduce. (8) Move an ini-
tial one-word-long title (Mr., Ms., Dr., President) to
the end of the name. (9) Move temporal adverbials
(adverb, PP, subordinate clause that is semantically
temporal) to pre-verb position.
5 Automatic Node Alignment and its
Application for Word Alignment
In this experiment, we automatically derive re-
orderings of the English sentences from an align-
ment between nodes in logic1 dependency graphs
for the English (source) and Chinese (target) sen-
tences. Source/Target designations are for conve-
nience, since the direction of MT is irrelevant.
We define an alignment as a partial function from
the nodes in the source graph and the nodes in the
target graph. We, furthermore, assume that this map-
ping is 1 to 1 for most node pairs, but can be n to 1
(or 1 to n). Furthermore, we allow some nodes, in
effect, to represent multiple tokens. These are iden-
tified as part of the GLARF analysis of a particular
sentence string and reflect language-specific rules.
Thus, for our purposes, a mapping between a source
and target node, each representing a multi-word ex-
pression is 1 to 1, rather than N to N.
We identify the following types of multi-word ex-
pressions for this purpose: (a) idiomatic expressions
from our monolingual lexicons, (b) dates, (c) times
(d) numbers and (e) ACE (Grishman, 2000) NEs.
Dates, holidays and times are regularized using ISO-
TimeML, e.g., January 3, 1977 becomes 1977-03-01
and numbers are converted to Arabic numbers.
5.1 ALIGN-ALG1
This work uses a modified version of ALIGN-
ALG1, a graph alignment algorithm we previously
used to align 1990s-style two-stage parser output for
MT experiments. ALIGN-ALG1 is an O(n2) algo-
rithm, n is the maximum number of nodes in the
source and target graphs (Meyers et al, 1996; Mey-
ers et al, 1998). Given Source Tree T and Target
Tree T ?, an alignment(T, T ?) is a partial function
from nodes N in T to nodes N ? in T ?. An exhaus-
tive search of possible alignments would consider all
non-intersecting combinations of the T ?T ? pairs of
source/target nodes ? There are at most T ! such pair-
ings where T >= T ?.1 However, ALIGN-ALG1 as-
sumes that some of these pairings are unlikely, and
1This ignores N to 1 matches, which we allow, although rel-
atively rarely.
91
favors pairings that assume the structure of the trees
correspond more closely. In particular, it is assumed
that ancestor nodes are more likely to match if most
of their descendant nodes match as well.
ALIGN-ALG1 finds the highest scoring align-
ment, where the score of an alignment is the sum
of the scores of the node pairs in the partial func-
tion. The score for each node pair (n, n?) partially
depends on the scores of a mapping from the chil-
dren of n to the children of n?. While the process
of calculating the scores is recursive, it can be made
efficient using dynamic programming.
ALIGN-ALG1 assumes that we align r and r?,
the roots of T and T ?. Calculating the scores for r
and r?, entails calculating the scores of pairs of their
children, and by extension all mappings from N to
N ? that obey the dominance preserving constraint:
Given nodes n1 and n2 in N and nodes n?1 and n?2
in N ?, where all 4 nodes are part of the alignment,
it cannot be the case that: n1 dominates n2, but
n?1 does not dominate n?2. Here, dominates means
is an ancestor in the dependency graph. ALIGN-
ALG1 scores each pair of nodes using the formula:
Score(n, n?) = Lex(n, n?) + ChildV al(n, n?),
where Lex(n, n?) is a score based on matching the
words labeling nodes n and n?, e.g., the score is 1 if
the pair is found in a bilingual dictionary and 0 oth-
erwise. Given n has children c0, . . . , ci and n? has
children c?0, . . . , c?j , to calculate ChildVal: (1) Cre-
ate Child-Matrix, a (i+ 1)? (j + 1) matrix (2) Fill
every position (1 <= x <= i, 1 <= x? <= j)
with Score(x, x?) (3) Fill every position (i+1, 1 <=
x? <= j) with Score(n, x?) minus a penalty (e.g.,
- .1) for collapsing an edge. This treats n? and x?
as a single unit, matched to n.2 (4) Fill every po-
sition (1 <= x <= i, j+1) with Score(x, n?) mi-
nus a penalty for collapsing an edge. Thus n + x is
paired with n?. (5) Set (i+1,j+1) to ??. Collapsing
both source and target edges is not permitted. (6) For
all sets of positions in the matrix such that no node
or column is repeated, select the set with the high-
est aggregate score. The aggregate score is the nu-
meric value of ChildV al(n, n?). If (n,n?) is part of
the alignment that is ultimately chosen, this choice
of node pairs is also part of the alignment. There
2The slight penalty represents that collapsing edges compli-
cate the analysis and is thus disfavored (Occam?s Razor).
are at most max(i + 1, j + 1)! possible pairings.
Rather than calculating them all, a greedy heuristic
can reduce the calculation time with minimal effect
on accuracy: the highest scoring cell in the matrix is
chosen first, conflicting cells are eliminated, the next
highest scoring cell is chosen, etc.
Consider the example in Figure 1, assum-
ing the dashed lines connect lexical matches
(the function LEX returns 1 for these node
pairs). Where n1 and n1? are the roots,
Score(n1, n1?) = 1 + ChildV al(n1, n1?). Cal-
culating ChildV al(n1, n1?) requires a recursive
descent down the pairs of nodes, until the bot-
tom most pair is scored. Score(n6, n6?) = 1.
Score(n5, n6?) = 0 + .9 (derived by collaps-
ing an edge and subtracting a penalty of .1).
Score(n3, n3?) = 1 + .9 = 1.9. Score(n2, n2?) =
1. ChildV al(n1, n1?) = 1 + 1.9 = 2.9. Thus
Score(n1, n1?) = 3.9. The alignment includes:
(n1, n1?), (n2, n2?), (n3, n3?), (n5, n6?), (n6, n6?).
The collapsing of edges helps recognize cases
where multiple predicates form substructures, e.g.,
take a walk, is angry, etc. in one tree can map to sin-
gle verbs in the other tree, allowing outgoing edges
from walk or angry to map to outgoing edges of the
corresponding verb, e.g., the agent and goal of John
walked to the store could map to the agent and goal
of John took a walk to the store.
In practice, ALIGN-ALG1 falls short because:
(1) Our translation dictionary does not have suffi-
cient coverage for the algorithm to perform well; (2)
The assumption that the roots of both graphs should
be aligned is often false. Parallel text often reflects
a dynamic, rather than a literal translation. In one
pair of aligned sentences in the FBIS corpus, the
English phrase the above mentioned requests cor-
responds to: meaning these re-
quests of Chen Shui-bian ? Chen Shui-bian has no
counterpart in the English. Parts of translations can
be omitted due to: (a) the discretion of the trans-
lators, (b) the expected world knowledge of partic-
ular language communities, (c) the cultural impor-
tance of particular information, etc.; (3) Violations
of the dominance-preserving constraint exist. The
most common type that we have observed consists
of sequences of transparent nouns and of (e.g., se-
ries of) in English corresponding to quantifiers in
92
Chinese ( ). Thus the head of the English con-
struction corresponds to the dependent of the Chi-
nese construction and vice versa.
5.2 Lexical Resources
Our primary bilingual Chinese/English dictionary
(LEX1) had insufficient coverage for ALIGN-ALG1
to be effective. LEX1 is a merger between:
The LDC 2002 Chinese-English Dictionary and
HowNet. In addition, we manually added additional
translations of units of measure from English. We
also used NEDICT, a name translation dictionary (Ji
et al, 2009) and AUTODICT, English/Chinese word
to word pairs with high similarity scores taken from
MT phase tables created as part of the (Zhang et al,
2007) system. The NEDICT was used both for pre-
cise matches and partial matches (since, NEs can
often be synonymous with substrings of NEs). In
addition, we used some WordNet (Fellbaum, 1998)
synonyms of English to expand the coverage of all
the dictionaries, allowing English words to match
Chinese word translations of their synonyms. We
allowed additional matches of function words that
served similar functions in the two languages includ-
ing: copulas, pronouns and determiners.
Finally, we use a mutual information (MI) based
approach to find further lexical information. We run
our alignment program over the corpus two times,
the first time, we acquire statistical information
useful for generating a MI-based score. This score
is used as a lexical score on the second pass for
items that do not match any of the dictionaries. On
the first pass, we tally the frequency of each pair
of source/target words s and t, such that neither
s, nor t are matched lexically to any other item
in the sentence. We, furthermore, keep track of
the number of times each word appears in the
corpus and the number of times each word appeared
unaligned in the corpus. We tally MI as follows:
pair?frequency2
1+(source?word?frequency?target?word?frequency)
One is added to the denominator as a variation on
add-one smoothing (Laplace, 1816), intended to
penalize low frequency scores. We calculate this
score in two ways: (a) using the global frequencies
of the source and target words; and (b) using the
frequency these words were unaligned. The larger
of the two scores is the one that is actually use.
Different lexicons are given different weights.
Matches between words in the hand-coded transla-
tion dictionary and NEDICT are given a score of
1.0. Matches in other dictionaries are allotted lower
scores to represent that these are based on automati-
cally acquired information, which we assume is less
reliable than manually coded information.3
5.3 ALIGN-ALG2
With ALIGN-ALG2, we partially address two lim-
itations of ALIGN-ALG1: (1) the assumption that
the roots of source and target graph are aligned;
and (2) the dominance-preserving constraint. Ba-
sically, we assume that structural similarity is fa-
vored, but not necessarily at the global level. Thus
it is likely that many subparts of corresponding trees
correspond closely, but not necessarily the highest
nodes in the trees.
We use ALIGN-ALG1 to align every possible pair
of S source nodes and T target nodes. Then we look
for P , the highest scoring node pair of all SXT
pairs. P and all the pairs of descendants that are
used to derive this score (the highest scoring pairs
of children, grand children, etc.) become the initial
output. Then we find all unmatched source and tar-
get children, and look up the highest scoring pair of
these nodes, and we repeat the process, adding the
resulting node pairs to the output. We continue to
repeat this process until either all the nodes are in-
cluded in the output or there is no remaining pair
with a score above a threshold score (we leave au-
tomatic methods of tuning this score to future work
and preliminarily have set this parameter to .3). This
means that: 1) some parts of the graphs are left un-
aligned (the alignment is a partial mapping); 2) the
alignment is more resilient to misalignment caused
by differences in graph structure, regardless of the
reason; and 3) the alignment may be between pair
of unconnected graphs, each containing subsets of
nodes and edges in the source and target graphs.
While more complex than ALIGN-ALG1, ALIGN-
ALG2 performs relatively quickly. After one itera-
tion using ALIGN-ALG1, scores are looked up, not
recalculated.
3Current informal weights of .2 to .6 may be replaced with
automatically tuned weights (hill-climbing, etc.) in future work.
93
5.4 Treating Multiple Tokens as One
In some cases, parsing and segmentation of text
can be corrected through minor modifications to our
alignment routine. Similarly, we use bilingual lex-
ical information to determine that certain other ad-
jacent tokens should be treated as single words for
purposes of alignment.
Given a language for which segmentation is a
common source of processing error (Chinese), if a
token is unaligned, we check to see whether subdi-
viding the token into two sub-tokens would allow
one or both of these sub-tokens to be alignable with
unaligned tokens in the other language. We iter-
ate through the string one token at a time, trying
all partitions. Given a source token ABC, consist-
ing of segments A, B and C, we test the two pairs of
subsequences {A, BC} and {AB, C}, to see which
of the two partitions (if any) could be aligned with
unaligned target tokens and we compare the scores
of both, selecting the highest score. Unless no par-
tition yields further source/target matches, we then
choose the highest scoring partition and add the re-
sulting node pairings to our alignment. In a similar
way, if there are a pair of aligned names consisting
of source tokens sj . . . sk and target tokens tj . . . tk,
we look for adjacent unaligned source nodes (a se-
quence of nodes ending in sj?1 or beginning with
sk+1) and/or adjacent target language nodes, such
that adding these nodes to the name sequence would
produce at least as high a lexical score. The lexi-
con can also be used to match two adjacent items to
the same word. We use a similar routine that checks
our lexicons for words that are adjacent to matching
words. This is particularly meaningful for the entries
automatically acquired by means of MI, as our cur-
rent method for acquiring MI would not distinguish
between 1 to 1 and N to 1 cases. Thus MI scores
for adjacent items typically does mean that an N to
1 match is appropriate. For example, the Chinese
word had high MI with every word
in the sequence (except and): ambassador extraor-
dinary and plenipotentiary (example is from FBIS).
This routine was able to cause our procedure to treat
this English sequence as a single token.
5.5 Using Node Alignment for Reordering
Given a node alignment, we can attempt to reorder
the source language so that words associated with
aligned nodes reflect the order of the words label-
ing the corresponding target nodes. Specifically,
we reorder our surface phrase structure-based repre-
sentation of the source language (English) and then
print out all the words yielded from the resulting
reordered tree. Reordering takes place in a bottom
up fashion as follows: for each phrase P with chil-
dren c0 . . . cn, reorder the structure beneath the child
nodes first. Then build the new-constituent right
to left, one child at a time from cn . . . c0. Start-
ing with an empty sequence, each item is put in
its proper place among the constituents in the se-
quence so far. At each step, place some ci after some
cj in ci+1 . . . cn, such that cj align precedes ci
and cj is after every ck in ci+1 . . . cn such that
ci align precedes ck. If cj does not exist, ci is
placed at the beginning of the sequence so far.
Definition of X align precedes Y , where X and
Y are nodes sharing the same parent: (1) Let pairsX
be the set of source/target pairs in the alignment such
that some (leaf node) descendant of X is the source
node in the pair; (2) Let pairsY be the set of pairs
in the alignment such that some descendant of Y is
the source node in the pair; (3) let Xtmax be the last
target member of a pair in pairsX , where the or-
der is determined by the word order of the target
words labeling the nodes; (4) let Ytmin be the first
target member of a pair in pairsY , where the order
is determined the same way; (5) let Xsmin be the
first source member of a pair in pairsx, according
to the source sentence word order; (6) let Ysmax be
the last source word in a pair in pairsY ordered the
same way. (7) X align precedes Y if: Xtmax pre-
cedes Ytmin and there is no source/target pair Q,R
in the alignment such that: (A) R precedes, Ytmin;
(B) Xtmax precedes R; (C) Q either precedes Xsmin
or follows Ysmax; (D) If Q precedes Ysmax, then R
does not precede Ytmin.
Essentially, the align precedes operator pro-
vides a conservative way to order the source sub-
trees S1 and S2 by their aligned target sub-tree coun-
terparts T1 and T2. The idea is that if T1 and T2
are ordered in an opposite manner to S1 and S2,
the source subtrees should trade places. However,
94
System DEV TEST
BASELINE 53.1% 49.9%
MANUAL 54.0% 50.6%
(p < .01) (not significant)
ALIGN 53.5% 51.1%
(p < .05) (p < .01)
ALIGN+MI 53.8% 51.4%
(p < .01) (p < .01)
Table 1: F Scores for Reordering Rules
a source/target pair Bs, Bt can block this reorder-
ing if doing so would upset the order of the moved
constituents relative to Bs and Bt e.g., if before the
move, Bs precedes S2 and Bt precedes T2, but af-
ter the move S2 would precede Bs. This reordering
proceeds from right to left, halting after placing c0.
6 Results
The results summarized in table 1, provide F-scores
(the harmonic mean of precision and recall) of the
word alignment resulting from running GIZA++
with and without our reordering rules, using the
LDC?s manually created word alignments for our
DEV and TEST corpora.4 Giza++ is run with En-
glish as source and Chinese as target. Our baseline
is the result of running Giza++ on the raw text. The
statistical significance of differences from the base-
line are provided in parentheses, next to each non-
baseline score(rounded to 2 significant digits). We
divided both corpora into 20 parts and ran all ver-
sions of the program on each section. We compared
the system output for each section against the base-
line and used the sign test to calculate statistical sig-
nificance. All system output except one5 achieved
at least p < .05 and most systems achieved signifi-
cance well below p < .01.
Informally, we observe that the rules reordering
common noun modifiers produce most of the total
4We used F-scores, which (Fraser and Marcu, 2007) show to
correlate well with improvements in BLEU. We weighted pre-
cision and recall evenly since we do not currently have BLEU
scores for MT that use these alignments and therefore cannot
tune the weights. Our results also showed improvements in
alignment error rate (AER) (Och and Ney, 2000), which incor-
porate the ?possible? and ?sure? portions of the manual align-
ment into F-score, but do not seem to correlate well with BLEU.
5When run on the test corpus, the manual system outper-
formed the baseline system on only 13 out of 20 sections.
improvement. However, space limitations prevent a
detailed exploration of these differences. The results
show that for both DEV and TEST corpora, both re-
ordering approaches improve F-scores of GIZA++
over the baseline. The manual rules (MANUAL)
seem to suffer somewhat from overtraining on the
DEV corpus, as they were designed based on DEV
corpus examples, whereas the alignment based ap-
proaches (ALIGN and subsequent entries in the ta-
ble) seem resilient to these effects. The use of Mu-
tual Information (ALIGN+MI) seems to further im-
prove the F-score.
The two approaches worked for many of the same
phenomena, e.g., they fronted many of the same
noun post-modifiers. The advantage of the hand-
coded rules seems to be that they cover reordering
of words which we cannot align. For example, a
rule that fronts post-nominal of phrases operates re-
gardless of dictionary coverage. Thus the rule-based
version fronted the of phrase in the NP the govern-
ment of the Guangxi Zhuangzu Autonomous Region
in our DEV corpus, due to the absolute application
of the rule. However, the alignment-based version
did not front the PP because the name was not found
in NEDICT. On the other hand, exceptions to this
rule were better handled by the alignment-based sys-
tem. For example, if series of aligns with the quan-
tifier , the PP would be incorrectly fronted
by the manual, but not the alignment-based system.
Also, the alignment-based method can handle cases
not covered by our rules with minimal labor. Thus,
the automatic system, but not the manual-rule sys-
tem fronted the locative PP in Guangxi to the po-
sition between been and quite in the sentence: for-
eign businessmen have been quite actively investing
in Guangxi. This is closer to the Chinese, but may
have been difficult to predict with an automatic rule
for several reasons, e.g., it is not clear if all post-
verbal locative phrases should front.
We further analyzed the DEV ALIGN+MI run to
determine both how often nodes were combined to-
gether by our algorithm to produce N to 1 align-
ments and the number of reorderings undertaken. It
turns out that out of the 59,032 pairs of nodes were
aligned for 3076 sentence pairs:6 55,391 alignments
6When sentences were misparsed in one language or the
other they were not reordered by the program.
95
were 1 to 1 (93.8% of the total) , 3443 alignments
were 2 to 1 (5.8% of the total) and 203 alignments
were N to 1, where N is greater than 2 (0.3% of the
total). The reordering program moved 1597 single
tokens; 2140 blocks 2 or 3 tokens long; 1203 blocks
of 4 or 5 tokens; 610 blocks of 6 or 7 tokens, 419
blocks of 8, 9 or 10 tokens, and 383 blocks of more
than 10 tokens.
7 Concluding Remarks
We have demonstrated that deep level linguistic
analysis can be used to improve word alignment re-
sults. It is natural to consider whether or not these
reorderings are likely to improve MT results. Both
the manual and alignment-based systems moved
post-nominal English modifiers to pre-nominal po-
sition, to reflect Chinese word order ? other move-
ments were much less frequent. In principle, these
selective reorderings may help SMT systems iden-
tify phrases of English that correspond to phrases of
Chinese, thus improving the quality of the phrase ta-
bles, especially when large chunks are moved. We
would also expect that the precision of our system to
be more important than the recall, since our system
would not yield an improvement if it produced too
much noise. Further experiments with current MT
systems are needed to assess whether this is actually
the case. We are considering such tests for future re-
search, using the Moses SMT system (Koehn et al,
2007).
Our representation had several possible advan-
tages over pure parse-based methods. We used se-
mantic features such as temporal, locative and trans-
parent (whether a low-content words inherits its se-
mantics) to help guide our alignment. The regu-
larized structure, also, helped identify long-distance
dependency relationships. We are also consider-
ing several improvements for our alignment-based
rules: (1) using additional dictionary resources such
as CATVAR (Habash and Dorr, 2003), so that cross-
part-of speech alignments can be more readily rec-
ognized; (2) finding more optimal orderings for
unaligned source language words. For example,
the alignment-based method reordered a bright star
arising from China?s policy to a bright arising from
China ?s policy star, separating bright from star,
even though bright star function as a unit; (3) incor-
porating and using multi-word bilingual dictionary
entries.; (4) automatic methods for tuning parame-
ters of our system that are currently hand-coded; (5)
training MI on a much larger corpus; (6) investigat-
ing possible ways to merge the manual-rules with
the alignment-based approach; and (7) performing
similar experiments with English/Japanese bitexts.
We would expect both parse-based approaches
and our system to handle mismatches that cover
large distances better than more shallow approaches
to reordering, e.g., (Crego and Marin?o, 2006) in the
same way that a full-parse handles constituent struc-
ture more completely than a chunker. In addition,
we would expect our approach to work best in lan-
guages where there are large differences in word or-
der, as these are exactly the cases that all predicate-
argument structure is designed to handle well (they
reduce apparent variation in structure). Towards this
end we are currently working on a Japanese/English
system. Obviously, the cost of developing GLARF
(or similar) systems are high, require linguistic ex-
pertise and may not be possible for resource-poor
languages. Nevertheless, we maintain that such sys-
tems are useful for many purposes and are there-
fore worth the cost. The GLARF system for En-
glish is available for download at http://nlp.
cs.nyu.edu/meyers/GLARF.html.
Acknowledgments
This work was supported by NSF Grant IIS-
0534700 Structure Alignment-based MT.
References
J. Bresnan and R. M. Kaplan. 1982. Syntactic Represen-
tation: Lexical-Functional Grammar: A Formal The-
ory for Grammatical Representation. In J. Bresnan,
editor, The Mental Representation of Grammatical Re-
lations. The MIT Press, Cambridge.
A. Bryl and J. van Genabith. 2010. f-align: An Open-
Source Alignment Tool for LFG f-Structures. In Pro-
ceedings of AMTA 2010.
E. Charniak. 2001. Immediate-head parsing for language
models. In ACL 2001, pages 116?123.
N. Chomsky. 1957. Syntactic Structures. Mouton, The
Hague.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause
Restructuring for Statistical Machine Translation. In
ACL 2005.
96
B. A. Cowan. 2008. A Tree-to-Tree Model for Statistical
Machine Translation. Ph.D. thesis, MIT.
J. M. Crego and J. B. Marin?o. 2006. Integration of POS-
tag-based source reordering into SMT decoding by an
extended search graph. In AMTA?06.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. The MIT Press, Cambridge.
A. Fraser and D. Marcu. 2007. Measuring Word
Alignment Quality for Statistical Machine Translation.
Computational Linguistics, 33:293?303.
D. Gildea and D. Jurafsky. 2002. Automatic Labeling of
Semantic Roles. Computational Linguistics, 28:245?
288.
R. Grishman. 2000. Entity Annotation Guidelines.
ftp://jaguar.ncsl.nist.gov/ace/phase1/edt phase1 v2.2.pdf.
N. Habash and B. Dorr. 2003. CatVar: A Database of
Categorial Variations for English. In Proceedings of
the MT Summit, pages 471?474, New Orleans.
Z. Harris. 1968. Mathematical Structures of Language.
Wiley-Interscience, New York.
J. R. Hobbs and R. Grishman. 1976. The Automatic
Transformational Analysis of English Sentences: An
Implementation. International Journal of Computer
Mathematics, 5:267?283.
H. Ji and R. Grishman. 2006. Analysis and Repair of
Name Tagger Errors. In COLING/ACL 2006, Sydney,
Australia.
H. Ji, R. Grishman, D. Freitag, M. Blume, J. Wang,
S. Khadivi, R. Zens, and H. Ney. 2009. Name Transla-
tion for Distillation. In Global Autonomous Language
Exploitation. Springer.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In ACL 2007 Demon-
stration Session, Prague.
P. Laplace. 1816. Essai philosophique sur les probabil-
its. Courcier Imprimeur, Paris.
Adam Meyers, Roman Yangarber, and Ralph Grishman.
1996. Alignment of Shared Forests for Bilingual Cor-
pora. In Proceedings of Coling 1996: The 16th In-
ternational Conference on Computational Linguistics,
pages 460?465.
Adam Meyers, Roman Yangarber, Ralph Grishman,
Catherine Macleod, and Antonio Moreno-Sandoval.
1998. Deriving Transfer Rules from Dominance-
Preserving Alignments. In Proceedings of Coling-
ACL98: The 17th International Conference on Com-
putational Linguistics and the 36th Meeting of the As-
sociation for Computational Linguistics.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. Annotating
Noun Argument Structure for NomBank. In Proceed-
ings of LREC-2004, Lisbon, Portugal.
A. Meyers, M. Kosaka, N. Xue, H. Ji, A. Sun, S. Liao,
and W. Xu. 2009. Automatic Recognition of Logi-
cal Relations for English, Chinese and Japanese in the
GLARF Framework. In SEW-2009 at NAACL-HLT-
2009.
E. Miltsakaki, A. Joshi, R. Prasad, and B. Webber. 2004.
Annotating discourse connectives and their arguments.
In A. Meyers, editor, NAACL/HLT 2004 Workshop:
Frontiers in Corpus Annotation, pages 9?16, Boston,
Massachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
F. J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In ACL 2000.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
S. Petrov and D. Klein. 2007. Improved Inference for
Unlexicalized Parsing. In HLT-NAACL 2007.
L. Shen, J. Xu, and R. Weischedel. 2008. A New String-
to-Dependency Machine Translation Algorithm with a
Target Dependency Language Model. In ACL 2008.
J. Wagner, D. Seddah, J. Foster, and J. van Genabith.
2007. C-Structures and F-Structures for the British
National Corpus. In Proceedings of the Twelfth In-
ternational Lexical Functional Grammar Conference,
Stanford. CSLI Publications.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
EMNLP-CoNLL 2007, pages 737?745.
D. Wu and P. Fung. 2009. Semantic roles for smt: A
hybrid two-pass model. In HLT-NAACL-2009, pages
13?16, Boulder, Colorado, June. Association for Com-
putational Linguistics.
N. Xue and M. Palmer. 2003. Annotating the Proposi-
tions in the Penn Chinese Treebank. In The Proceed-
ings of the 2nd SIGHAN Workshop on Chinese Lan-
guage Processing, Sapporo.
N. Xue. 2008. Labeling Chinese Predicates with Seman-
tic roles. Computational Linguistics, 34:225?255.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In ACL, pages 523?530.
Y. Zhang, R. Zens, and H. Ney. 2007. Chunk-Level
Reordering of Source Language Sentences with Auto-
matically Learned Rules for Statistical Machine Trans-
lation. In Proc. of NAACL/HLT 2007.
M. Zhang, H. Jiang, A. Aw, H. Li, C. L. Tan, and S. Li.
2008. A Tree Sequence Alignment-based Tree-to-Tree
Translation Model. In ACL 2008.
97
Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 20?29,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
A Preliminary Study of Tweet Summarization using Information Extraction
Wei Xu, Ralph Grishman, Adam Meyers
Computer Science Department
New York University
New York, NY 10003, USA
{xuwei,grishman,meyers}@cs.nyu.edu
Alan Ritter
Computer Science and Engineering
University of Washington
Seattle, WA 98125, USA
aritter@cs.washington.edu
Abstract
Although the ideal length of summaries dif-
fers greatly from topic to topic on Twitter, pre-
vious work has only generated summaries of
a pre-fixed length. In this paper, we propose
an event-graph based method using informa-
tion extraction techniques that is able to cre-
ate summaries of variable length for different
topics. In particular, we extend the Pagerank-
like ranking algorithm from previous work to
partition event graphs and thereby detect fine-
grained aspects of the event to be summarized.
Our preliminary results show that summaries
created by our method are more concise and
news-worthy than SumBasic according to hu-
man judges. We also provide a brief survey of
datasets and evaluation design used in previ-
ous work to highlight the need of developing a
standard evaluation for automatic tweet sum-
marization task.
1 Introduction
Tweets contain a wide variety of useful information
from many perspectives about important events tak-
ing place in the world. The huge number of mes-
sages, many containing irrelevant and redundant in-
formation, quickly leads to a situation of informa-
tion overload. This motivates the need for automatic
summarization systems which can select a few mes-
sages for presentation to a user which cover the most
important information relating to the event without
redundancy and filter out irrelevant and personal in-
formation that is not of interest beyond the user?s
immediate social network.
Although there is much recent work focusing on
the task of multi-tweet summarization (Becker et al,
2011; Inouye and Kalita, 2011; Zubiaga et al, 2012;
Liu et al, 2011a; Takamura et al, 2011; Harabagiu
and Hickl, 2011; Wei et al, 2012), most previous
work relies only on surface lexical clues, redun-
dancy and social network specific signals (e.g. user
relationship), and little work has considered taking
limited advantage of information extraction tech-
niques (Harabagiu and Hickl, 2011) in generative
models. Because of the noise and redundancy in
social media posts, the performance of off-the-shelf
news-trained natural language process systems is de-
graded while simple term frequency is proven pow-
erful for summarizing tweets (Inouye and Kalita,
2011). A natural and interesting research question
is whether it is beneficial to extract named entities
and events in the tweets as has been shown for clas-
sic multi-document summarization (Li et al, 2006).
Recent progress on building NLP tools for Twitter
(Ritter et al, 2011; Gimpel et al, 2011; Liu et al,
2011b; Ritter et al, 2012; Liu et al, 2012) makes
it possible to investigate an approach to summariz-
ing Twitter events which is based on Information Ex-
traction techniques.
We investigate a graph-based approach which
leverages named entities, event phrases and their
connections across tweets. A similar idea has been
studied by Li et al (2006) to rank the salience
of event concepts in summarizing news articles.
However, the extreme redundancy and simplicity of
tweets allows us to explicitly split the event graph
into subcomponents that cover various aspects of the
initial event to be summarized to create comprehen-
20
Work Dataset (size of each clus-
ter)
System Output Evaluation Metrics
Inouye and
Kalita (2011)
trending topics (approxi-
mately 1500 tweets)
4 tweets ROUGE and human (over-
all quality comparing to
human summary)
Sharifi et al
(2010)
same as above 1 tweet same as above
Rosa et al
(2011)
segmented hashtag top-
ics by LDA and k-means
clustering (average 410
tweets)
1, 5, 10 tweets Precision@k (relevance to
topic)
Harabagiu and
Hickl (2011)
real-word event topics (a
minimum of 2500 tweets)
top tweets until a limit of
250 words was reached
human (coverage and co-
herence)
Liu et al
(2011a)
general topics and hash-
tag topics (average 1.7k
tweets)
same lengths as of the
human summary, vary
for each topic (about 2 or
3 tweets)
ROUGE and human (con-
tent coverage, grammat-
icality, non-redundancy,
referential clarity, focus)
Wei et al
(2012)
segmented hashtag top-
ics according to burstiness
(average 10k tweets)
10 tweets ROUGE, Precison/Recall
(good readability and rich
content)
Takamura et al
(2011)
specific soccer games
(2.8k - 5.2k tweets)
same lengths as the hu-
man summary, vary for
each topic (26 - 41
tweets)
ROUGE (considering
only content words)
Chakrabarti and
Punera (2011)
specific football games
(1.8k tweets)
10 - 70 tweets Precision@k (relevance to
topic)
Table 1: Summary of datasets and evaluation metrics used in several previous work on tweet summarization
sive and non-redundant summaries. Our work is the
first to use a Pagerank-like algorithm for graph parti-
tioning and ranking in the context of summarization,
and the first to generate tweet summaries of variable
length which is particularly important for tweet sum-
marization. Unlike news articles, the amount of in-
formation in a set of topically clustered tweets varies
greatly, from very repetitive to very discrete. For ex-
ample, the tweets about one album release can be
more or less paraphrases, while those about another
album by a popular singer may involve rumors and
release events etc. In the human study conducted by
Inouye and Kalita (2011), annotators strongly prefer
different numbers of tweets in a summary for dif-
ferent topics. However, most of the previous work
produced summaries of a pre-fixed length and has
no evaluation on conciseness. Liu et al (2011a)
and Takamura et al (2011) also noticed the ideal
length of summaries can be very different from topic
to topic, and had to use the length of human refer-
ence summaries to decide the length of system out-
puts, information which is not available in practice.
In contrast, we developed a system that is capable
of detecting fine-grained sub-events and generating
summaries with the proper number of representative
tweets accordingly for different topics.
Our experimental results show that with informa-
tion extraction it is possible to create more mean-
ingful and concise summaries. Tweets that contain
real-world events are usually more informative and
readable. Event-based summarization is especially
beneficial in this situation due to the fact that tweets
are short and self-contained with simple discourse
structure. The boundary of 140 characters makes it
efficient to extract semi-structured events with shal-
low natural language processing techniques and re-
21
Tweets (Date Created) Named Entity Event Phrases Date Mentioned
Nooooo.. Season premiere of Doctor Who is on
Sept 1 world wide and we?ll be at World Con
(8/22/2012)
doctor who,
world con
season, is on,
premiere
sept 1
(9/1/2012)
guess what I DON?T get to do tomorrow!
WATCH DOCTOR WHO (8/31/2012)
doctor who watch tomorrow
(9/1/2012)
As I missed it on Saturday, I?m now catching up
on Doctor Who (9/4/2012)
doctor who missed,
catching up
saturday
(9/1/2012)
Rumour: Nokia could announce two WP8 de-
vices on September 5 http://t.co/yZUwDFLV (via
@mobigyaan)
nokia, wp8 announce september 5
(9/5/2012)
Verizon and Motorola won?t let Nokia have all
the fun ; scheduling September 5th in New York
http://t.co/qbBlYnSl (8/19/2012)
nokia, verizon,
motorola,
new york
scheduling september 5th
(9/5/2012)
Don?t know if it?s excitement or rooting for the
underdog, but I am genuinely excited for Nokia
come Sept 5: http://t.co/UhV5SUMP (8/7/2012)
nokia rooting,
excited
sept 5
(9/5/2012)
Table 2: Event-related information extracted from tweets
duces the complexity of the relationship (or no re-
lationship) between events according to their co-
occurrence, resulting in differences in constructing
event graphs from previous work in news domain
(Li et al, 2006).
2 Issues in Current Research on Tweet
Summarization
The most serious problem in tweet summarization
is that there is no standard dataset, and conse-
quently no standard evaluation methodology. Al-
though there are more than a dozen recent works on
social media summarization, astonishingly, almost
each research group used a different dataset and a
different experiment setup. This is largely attributed
to the difficulty of defining the right granularity of a
topic in Twitter. In Table 1, we summarize the exper-
iment designs of several selective works. Regardless
of the differences, researchers generally agreed on :
? clustering tweets topically and temporally
? generating either a very short summary for a
focused topic or a long summary for large-size
clusters
? difficulty and necessity to generate summaries
of variable length for different topics
Although the need of variable-length summaries
have been raised in previous work, none has pro-
vide a good solution (Liu et al, 2011a; Takamura
et al, 2011; Inouye and Kalita, 2011). In this pa-
per, our focus is study the feasibility of generating
concise summaries of variable length and improv-
ing meaningfulness by using information extraction
techniques. We hope this study can provide new in-
sights on the task and help in developing a standard
evaluation in the future.
3 Approach
We first extract event information including named
entities and event phrases from tweets and construct
event graphs that represent the relationship between
them. We then rank and partition the events using
PageRank-like algorithms, and create summaries of
variable length for different topics.
3.1 Event Extraction from Tweets
As a first step towards summarizing popular events
discussed on Twitter, we need a way to identify
events from Tweets. We utilize several natural lan-
guage processing tools that specially developed for
noisy text to extract text phrases that bear essential
event information, including named entities (Ritter
et al, 2011), event-referring phrases (Ritter et al,
22
2012) and temporal expressions (Mani and Wilson,
2000). Both the named entity and event taggers uti-
lize Conditional Random Fields models (Lafferty,
2001) trained on annotated data, while the temporal
expression resolver uses a mix of hand-crafted and
machine-learned rules. Example event information
extracted from Tweets are presented in Table 2.
The self-contained nature of tweets allows effi-
cient extraction of event information without deep
analysis (e.g. co-reference resolution). On the other
hand, individual tweets are also very terse, often
lacking sufficient context to access the importance
of events. It is crucial to exploit the highly redun-
dancy in Twitter. Closely following previous work
by Ritter et al (2012), we group together sets of
topically and temporally related tweets, which men-
tion the same named entity and a temporal refer-
ence resolved to the same unique calendar date. We
also employ a statistical significance test to measure
strength of association between each named entity
and date, and thereby identify important events dis-
cussed widely among users with a specific focus,
such as the release of a new iPhone as opposed to in-
dividual users discussing everyday events involving
their phones. By discarding frequent but insignifi-
cant events, we can produce more meaningful sum-
maries about popular real-world events.
3.2 Event Graphs
Since tweets have simple discourse and are self-
contained, it is a reasonable assumption that named
entities and event phrases that co-occurred together
in a single tweet are very likely related. Given a col-
lection of tweets, we represent such connections by
a weighted undirected graph :
? Nodes: named entities and event phrases are
represented by nodes and treated indifferently.
? Edges: two nodes are connected by an undi-
rected edge if they co-occurred in k tweets, and
the weight of edge is k.
We find it helpful to merge named entities and
event phrases that have lexical overlap if they are fre-
quent but not the topic of the tweet cluster. For ex-
ample, ?bbc?, ?radio 1?, ?bbc radio 1? are combined
together in a set of tweets about a band. Figure 1
shows a very small toy example of event graph. In
the experiments of this paper, we also exclude the
edges with k < 2 to reduce noise in the data and
calculation cost.
Figure 1: A toy event graph example built from the three
sentences of the event ?Nokia - 9/5/2012? in Table 2
3.3 Event Ranking and Partitioning
Graph-based ranking algorithms are widely used in
automatic summarization to decide salience of con-
cepts or sentences based on global information re-
cursively drawn from the entire graph. We adapt the
PageRank-like algorithm used in TextRank (Mihal-
cea and Tarau, 2004) that takes into account edge
weights when computing the score associated with a
vertex in the graph.
Formally, let G = (V,E) be a undirected graph
with the set of vertices V and set of edges E, whereE is a subset of V ? V . For a given vertex Vi, letAd(Vi) be the set of vertices that adjacent to it. The
weight of the edge between Vi and Vj is denoted aswij , and wij = wji. The score of a vertex Vi is
defined as follows:S(Vi) = (1  d) + d? X
Vj2Ad(Vi)
wij ? S(Vj)P
Vk2Ad(Vj) wjk
where d is a damping factor that is usually set to 0.85
(Brin and Page, 1998), and this is the value we are
also using in our implementation.
23
Starting from arbitrary values assigned to each
node in the graph, the computation iterates until con-
vergence. Note that the final salience score of each
node is not affected by the choice of the initial val-
ues assigned to each node in the graph, but rather the
weights of edges.
In previous work computed scores are then used
directly to select text fractions for summaries (Li et
al., 2006). However, the redundancy and simplic-
ity of tweets allow further exploration into sub-event
detection by graph partitioning. The intuition is that
the correlations between named entities and event
phrases within same sub-events are much stronger
than between sub-events. This phenomena is more
obvious and clear in tweet than in news articles,
where events are more diverse and complicated re-
lated to each other given lengthy context.
As theoretically studied in local partitioning prob-
lem (Andersen et al, 2006), a good partition of the
graph can be obtained by separating high ranked ver-
tices from low ranked vertices, if the nodes in the
graph have ranks that are distinguishable. Utilizing
a similar idea, we show that a simple greedy algo-
rithm is efficient to find important sub-events and
generate useful summaries in our tasks. As shown
in Figure 2 and 3, the high ranked nodes (whose
scores are greater than 1, the average score of all
nodes in the graph) in tweet event graphs show the
divisions within a topic. We search for strongly con-
nected sub-graphs, as gauged by parameter ?, from
the highest ranked node to lower ranked ones.The
proportion of tweets in a set that are related to a
sub-event is then estimated according to the ratio be-
tween the sum of node scores in the sub-graph ver-
sus the entire graph. We select one tweet for each
sub-event that best covers the related nodes with the
highest sum of node scores normalized by length as
summaries. By adding a cutoff (parameter  ) on
proportion of sub-event required to be included into
summaries, we can produce summaries with the ap-
propriate length according to the diversity of infor-
mation in a set of tweets.
In Figure 2, 3 and 4, the named entity which is
also the topic of tweet cluster is omitted since it is
connected with every node in the event graph. The
size of node represents the salience score, while the
shorter, straighter and more vertical the edge is, the
higher its weight. The nodes with rectangle shapes
Algorithm 1 Find important sub-events
Require: Ranked event graph G = (V,E), the
named entity V0 which is the topic of event
cluster, parameters ? and   that can be set
towards user preference over development data
1: Initialize the pool of high ranked nodesV?  {Vi|8Vi 2 V, S(Vi) > 1}   V0 and the
total weight W  PVi2V? S(Vi)
2: while V? 6= ; do
3: Pop the highest ranked node Vm from V?
4: Put Vm to a temporary sub-event e  {Vm}
5: for all Vn in V? do
6: if wmn/w0m > ? and w0n/w0m > ?
then
7: e  e [ {Vn}
8: end if
9: end for
10: We  PVi2e S(Vi)
11: if We/W >   then
12: Successfully find a sub-event e
13: Remove all nodes in e from V?
14: end if
15: end while
are named entities, while round shaped ones are
event phrases. Note that in most cases, sub-events
correspond to connected components in the event
graph of high ranked nodes as in Figure 2 and 3.
However, our simple greedy algorithm also allows
multiple sub-events for a single connected compo-
nent that can not be covered by one tweet in the
summary. For example, in Figure 4, two sub-eventse1 = {sell, delete, start, payment} and e2 =
{facebook, share user data, privacy policy, debut}
are chosen to accommodate the complex event.
4 Experiments
4.1 Data
We gathered tweets over a 4-month period spanning
November 2012 to February 2013 using the Twitter
Streaming API. As described in more details in pre-
vious work on Twitter event extraction by Ritter et
al. (2012), we grouped together all tweets which
mention the same named entity (recognized using
24
Figure 2: Event graph of ?Google - 1/16/2013?, an example of event cluster with multiple focuses
Figure 3: Event graph of ?Instagram - 1/16/2013?, an example of event cluster with a single but complex focus
25
Figure 4: Event graph of ?West Ham - 1/16/2013?, an
example of event cluster with a single focus
a Twitter specific name entity tagger1) and a refer-
ence to the same unique calendar date (resolved us-
ing a temporal expression processor (Mani and Wil-
son, 2000)). Tweets published during the whole pe-
riod are aggregated together to find top events that
happen on each calendar day. We applied the G2
test for statistical significance (Dunning, 1993) to
rank the event clusters, considering the corpus fre-
quency of the named entity, the number of times the
date has been mentioned, and the number of tweets
which mention both together. We randomly picked
the events of one day for human evaluation, that is
the day of January 16, 2013 with 38 events and an
average of 465 tweets per event cluster.
For each cluster, our systems produce two ver-
sions of summaries, one with a fixed number (set
to 3) of tweets and another one with a flexible num-
ber (vary from 1 to 4) of tweets. Both ? and   are
set to 0.1 in our implementation. All parameters are
set experimentally over a small development dataset
consisting of 10 events in Twitter data of September
2012.
1
https://github.com/aritter/twitter_nlp
4.2 Baseline
SumBasic (Vanderwende et al, 2007) is a simple
and effective summarization approach based on term
frequency, which we use as our baseline. It uses
word probabilities with an update function to avoid
redundancy to select sentences or posts in a social
media setting. It is shown to outperform three other
well-known multi-document summarization meth-
ods, namely LexRank (Erkan and Radev, 2004),
TextRank (Mihalcea and Tarau, 2004) and MEAD
(Radev et al, 2004) on tweets in (Inouye and Kalita,
2011), possibly because that the relationship be-
tween tweets is much simpler than between sen-
tences in news articles and can be well captured by
simple frequency methods. The improvement over
the LexRank model on tweets is gained by consid-
ering the number of retweets and influential users is
another side-proof (Wei et al, 2012) of the effective-
ness of frequency.
EventRank?Flexible EventRank?Fixed SumBasic
Annotator 1
0
1
2
3
4
5 compactnesscompletenessoverall
EventRank?Flexible EventRank?Fixed SumBasic
Annotator 2
0
1
2
3
4
5 compactnesscompletenessoverall
Figure 5: human judgments evaluating tweet summariza-
tion systems
26
Event System Summary
- Google ?s home page is a Zamboni game in celebration of Frank Zam-
boni ?s birthday January 16 #GameOn
EventRank
(Flexible)
- Today social , Tomorrow Google ! Facebook Has Publicly Redefined
Itself As A Search Company http://t.co/dAevB2V0 via @sai
Google
1/16/2013
- Orange says has it has forced Google to pay for traffic . The Head of
the Orange said on Wednesday it had ... http://t.co/dOqAHhWi
- Tomorrow?s Google doodle is going to be a Zamboni! I may have to
take a vacation day.
SumBasic - the game on google today reminds me of hockey #tooexcited #saturday
- The fact that I was soooo involved in that google doodle game says
something about this Wednesday #TGIW You should try it!
EventRank
(Flexible)
- So Instagram can sell your pictures to advertisers without u knowing
starting January 16th I?m bout to delete my instagram !
- Instagram debuts new privacy policy , set to share user data with Face-
book beginning January 16
Instagram
1/16/2013
- Instagram will have the rights to sell your photos to Advertisers as of
jan 16
SumBasic - Over for Instagram on January 16th
- Instagram says it now has the right to sell your photos unless you delete
your account by January 16th http://t.co/tsjic6yA
EventRank
(Flexible)
- RT @Bassa_Mufc : Wayne Rooney and Nani will feature in the FA Cup
replay with West Ham on Wednesday - Sir Alex Ferguson
West Ham
1/16/2013
- Wayne Rooney could be back to face West Ham in next Wednesday?s
FA Cup replay at Old Trafford. #BPL
SumBasic - Tomorrow night come on West Ham lol
- Nani?s fit abd WILL play tomorrow against West Ham! Sir Alex con-
firmed :)
Table 3: Event-related information extracted from tweets
4.3 Preliminary Results
We performed a human evaluation in which two an-
notators were asked to rate the system on a five-
point scale (1=very poor, 5=very good) for com-
pleteness and compactness. Completeness refers to
how well the summary cover the important content
in the tweets. Compactness refers to how much
meaningful and non-redundant information is in the
summary. Because the tweets were collected ac-
cording to information extraction results and ranked
by salience, the readability of summaries generated
by different systems are generally very good. The
top 38 events of January 16, 2013 are used as test
set. The aggregate results of the human evaluation
are displayed in Figure 5. Agreement between an-
notators measured using Pearson?s Correlation Co-
efficient is 0.59, 0.62, 0.62 respectively for compact-
ness, completeness and overall judgements.
Results suggest that the models described in this
paper produce more satisfactory results as the base-
line approaches. The improvement of EventRank-
Flexible over SumBasic is significant (two-tailedp < 0.05) for all three metrics according to stu-
dent?s t test. Example summaries of the events in
Figure 2, 3 and 4 are presented respectively in Table
3. The advantages of our method are the follow-
ing: 1) it finds important facts of real-world events
2) it prefers tweets with good readability 3) it in-
cludes the right amount of information with diversity
and without redundancy. For example, our system
picked only one tweet about ?West Ham -1/16/2013?
that convey the same message as the three tweets to-
27
gether of the baseline system. For another example,
among the tweets about Google around 1/16/2013,
users intensively talk about the Google doodle game
with a very wide range of words creatively, giving
word-based methods a hard time to pick up the di-
verse and essential event information that is less fre-
quent.
5 Conclusions and Future Work
We present an initial study of feasibility to gen-
erate compact summaries of variable lengths for
tweet summarization by extending a Pagerank-like
algorithm to partition event graphs. The evalua-
tion shows that information extraction techniques
are helpful to generate news-worthy summaries of
good readability from tweets.
In the future, we are interested in improving the
approach and evaluation, studying automatic met-
rics to evaluate summarization of variable length
and getting involved in developing a standard eval-
uation for tweet summarization tasks. We wonder
whether other graph partitioning algorithms may im-
prove the performance. We also consider extending
this graph-based approach to disambiguate named
entities or resolve event coreference in Twitter data.
Another direction of future work is to extend the
proposed approach to different data, for example,
temporal-aware clustered tweets etc.
Acknowledgments
This research was supported in part by NSF grant
IIS-0803481, ONR grant N00014-08-1-0431, and
DARPA contract FA8750- 09-C-0179, and carried
out at the University of Washington?s Turing Center.
We thank Mausam and Oren Etzioni of University
of Washington, Maria Pershina of New York Univer-
sity for their advice.
References
Reid Andersen, Fan Chung, and Kevin Lang. 2006.
Local graph partitioning using pagerank vectors. In
Foundations of Computer Science, 2006. FOCS?06.
47th Annual IEEE Symposium on, pages 475?486.
IEEE.
Hila Becker, Mor Naaman, and Luis Gravano. 2011. Se-
lecting quality twitter content for events. In Proceed-
ings of the Fifth International AAAI Conference onWe-
blogs and Social Media (ICWSM?11).
Sergey Brin and Lawrence Page. 1998. The anatomy of a
large-scale hypertextual web search engine. Computer
networks and ISDN systems, 30(1):107?117.
Deepayan Chakrabarti and Kunal Punera. 2011. Event
summarization using tweets. In Proceedings of the
Fifth International AAAI Conference on Weblogs and
Social Media, pages 66?73.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational linguis-
tics, 19(1):61?74.
G?nes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text sum-
marization. J. Artif. Intell. Res. (JAIR), 22:457?479.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for twit-
ter: Annotation, features, and experiments. In ACL.
Sanda Harabagiu and Andrew Hickl. 2011. Relevance
modeling for microblog summarization. In Fifth In-
ternational AAAI Conference on Weblogs and Social
Media.
David Inouye and Jugal K Kalita. 2011. Comparing twit-
ter summarization algorithms for multiple post sum-
maries. In Privacy, security, risk and trust (passat),
2011 ieee third international conference on and 2011
ieee third international conference on social comput-
ing (socialcom), pages 298?306. IEEE.
John Lafferty. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. pages 282?289. Morgan Kaufmann.
Wenjie Li, Wei Xu, Chunfa Yuan, Mingli Wu, and Qin
Lu. 2006. Extractive summarization using inter- and
intra- event relevance. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, ACL-44, pages 369?376,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Fei Liu, Yang Liu, and Fuliang Weng. 2011a. Why is
?sxsw? trending? exploring multiple text sources for
twitter topic summarization. ACL HLT 2011, page 66.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011b. Recognizing named entities in tweets.
In ACL.
Xiaohua Liu, Furu Wei, Ming Zhou, et al 2012. Quick-
view: Nlp-based tweet search. In Proceedings of the
ACL 2012 System Demonstrations, pages 13?18. As-
sociation for Computational Linguistics.
Inderjeet Mani and GeorgeWilson. 2000. Robust tempo-
ral processing of news. In Proceedings of the 38th An-
28
nual Meeting on Association for Computational Lin-
guistics, ACL ?00, pages 69?76, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Proceedings of EMNLP, vol-
ume 4, pages 404?411. Barcelona, Spain.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda Celebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam, Danyu
Liu, et al 2004. Mead-a platform for multidocument
multilingual text summarization. In Proceedings of
LREC, volume 2004.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An experi-
mental study.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter. In
KDD, pages 1104?1112. ACM.
Kevin Dela Rosa, Rushin Shah, Bo Lin, Anatole Gersh-
man, and Robert Frederking. 2011. Topical clustering
of tweets. Proceedings of the ACM SIGIR: SWSM.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal K
Kalita. 2010. Experiments in microblog summariza-
tion. In Proc. of IEEE Second International Confer-
ence on Social Computing.
Hiroya Takamura, Hikaru Yokono, and Manabu Oku-
mura. 2011. Summarizing a document stream. Ad-
vances in Information Retrieval, pages 177?188.
Lucy Vanderwende, Hisami Suzuki, Chris Brockett, and
Ani Nenkova. 2007. Beyond sumbasic: Task-focused
summarization with sentence simplification and lex-
ical expansion. Information Processing & Manage-
ment, 43(6):1606?1618.
Furu Wei, Ming Zhou, and Heung-Yeung Shum. 2012.
Twitter topic summarization by ranking tweets using
social influence and content quality. In COLING.
Arkaitz Zubiaga, Damiano Spina, Enrique Amig?, and
Julio Gonzalo. 2012. Towards real-time summariza-
tion of scheduled events from twitter streams. In Pro-
ceedings of the 23rd ACM conference on Hypertext
and social media, pages 319?320. ACM.
29
Proceedings of SADAATL 2014, pages 11?20,
Dublin, Ireland, August 24, 2014.
Jargon-Term Extraction by Chunking
Adam Meyers
?
, Zachary Glass
?
, Angus Grieve-Smith
?
, Yifan He
?
,
Shasha Liao
?
and Ralph Grishman
?
New York University
?
, Google
?
meyers/angus/yhe/grishman@cs.nyu.edu, zglass@alumni.princeton.edu
Abstract
NLP definitions of Terminology are usually application-dependent. IR terms are noun sequences
that characterize topics. Terms can also be arguments for relations like abbreviation, definition or
IS-A. In contrast, this paper explores techniques for extracting terms fitting a broader definition:
noun sequences specific to topics and not well-known to naive adults. We describe a chunking-
based approach, an evaluation, and applications to non-topic-specific relation extraction.
1 Introduction
Webster?s II New College Dictionary (Houghton Mifflin Company, 2001, p.1138) defines terminology
as: The vocabulary of technical terms and usages appropriate to a particular field, subject, science,
or art. Systems for automatically extracting instances of terminology (terms) usually assume narrow
operational definitions that are compatible with particular tasks. Terminology, in the context of Infor-
mation Retrieval (IR) (Jacquemin and Bourigault, 2003) refers to keyword search terms (microarray,
potato, genetic algorithm), single or multi-word (mostly nominal) expressions collectively representing
topics of documents that contain them. These same terms are also used for creating domain-specific
thesauri and ontologies (Velardi et al., 2001). We will refer to these types of terms as topic-terms and
this type of terminology topic-terminology. In other work, types of terminology (genes, chemical names,
biological processes, etc.) are defined relative to a specific field like Chemistry or Biology (Kim et al.,
2003; Corbett et al., 2007; Bada et al., 2010). These classes are used for narrow tasks, e.g., Information
Extraction (IE) slot filling tasks within a particular genre of interest (Giuliano et al., 2006; Bundschus
et al., 2008; BioCreAtIvE, 2006). Other projects are limited to Information Extraction tasks that may
not be terminology-specific, but have terms as arguments, e.g., (Schwartz and Hearst, 2003; Jin et al.,
2013) detect abbreviation and definition relations respectively and the arguments are terms. In contrast
to this previous work, we have built a system that extracts a larger set of terminology, which we call
jargon-terminology. Jargon-terms may include ultracentrifuge, which is unlikely to be a topic-term of a
current biology article, but will not include potato, a non-technical word that could be a valid topic-term.
We aim to find all the jargon-terms found in a text, not just the ones that fill slots for specific relations.
As we show, jargon-terminology closely matches the notional (e.g., Webster?s) definition of terminol-
ogy. Furthermore, the important nominals in technical documents tend to be jargon-terms, making them
likely arguments of a wide variety of possible IE relations (concepts or objects that are invented, two
nominals that are in contrast, one object that is ?better than? another, etc.). Specifically, the identification
of jargon-terms lays the ground for IE tasks that are not genre or task dependent. Our approach which
finds all instances of terms (tokens) in text is conducive to these tasks. In contrast, topic-term detection
techniques find smaller sets of terms (types), each term occurring multiple times and the set of terms
collectively represents a topic, in a similar way that a set of documents can represent a topic.
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organisers. License details: http://creativecommons.org/licenses/by/4.0/
11
This paper describes a system for extracting jargon-terms in technical documents (patents and journal
articles); the evaluation of this system using manually annotated documents; and a set of information
extraction (IE) relations which take jargon-terms as arguments. We incorporate previous work in termi-
nology extraction, assuming that terminology is restricted to noun groups (minus some left modifiers)
(Justeson and Katz, 1995);
1
and we use both topic-term extraction techniques (Navigli and Velardi,
2004) and relation-based extraction techniques (Jin et al., 2013) in components of our system. Rather
than looking at the distribution of noun groups as a whole for determining term-hood, we refine the
classes used by the noun group chunker itself, placing limitations on the candidate noun groups pro-
posed and then filtering the output by setting thresholds on the number and quality of the ?jargon-like?
components of the phrase. The resulting system admits not only topic-terms, but also other non-topic
instances of terminology. Using the more inclusive set of jargon-terms (rather than just topic-terms) as
arguments of the IE relations in section 6, we are able to detect a larger and more informative set of rela-
tion. Furthermore, these relations are salient for a wide variety of genres (unlike those in (BioCreAtIvE,
2006)) ? a genre-neutral definition of terminology makes this possible. For example, the CONTRAST
relation between the two bold face terms in necrotrophic effector system
A1
that is an exciting contrast
to the biotrophic effector models
A2
. would be applicable in most academic genres. Our jargon-terms
also contrast with the tactic of filling terminology slots in relations with any noun-group (Justeson and
Katz, 1995), as such a strategy overgenerates, lowering precision.
2 Topic-term Extraction
Topic-term extractors (Velardi et al., 2001; Tomokiyo and Hurst, 2003) collect candidate terms (N-grams,
noun groups, words) that are more representative of a foreground corpus (documents about a specific
topic) than they are of a background corpus (documents about a wide range of topics), using statistical
measures such as
Term Frequency
Inverse Document Frequency
(TFIDF), or a variation thereof. Due to the metrics used
and cutoffs assumed, the list of terms selected is usually no more than a few hundred distinct terms,
even for a large set of foreground documents and tend to be especially salient to that topic. The terms
can be phrases that lay people would not know (e.g., microarray, genetic algorithm) or common topics
for that document set (e.g., potato, computer). Such systems rank all candidate terms, using cutoffs
(minimum scores or percentages of the list) to separate out the highest-ranked terms as output. Thus
sets of topic-terms, derived this way, are dependent on the foreground and background assumed, and the
publication dates. So a precise definition would include such information, e.g., topic-terms(biomedical-
patents, random-patents, 1990?1999) would refer to those topic-terms that differentiate a foreground of
biomedical patents from the 1990s from a background of diverse patents from the same epoch. Narrower
topics are possible (e.g., comparing DNA-microarray patents to the same background); or broader ones
(e.g., if a diverse corpus including news articles, fiction and travel writing are the background set instead
of patents, then patent terms such as national stage application may be highly ranked in the output).
Thus topic-terms generated by these methods model a relationally based definition and are relative to the
chosen foregrounds, backgrounds and dates.
Topic-terms can include words/phrases like potato, wheat, rat, monkey, which may be common sub-
jects of some set of biomedical documents, but are not specific to a technical field. In contrast, jargon-
terms would include words (like ultracentrifuge, theorem, graduated cylinder) that are specific to tech-
nical language, but don?t tend to be topics of any current document of interest. Jargon-terms, like topic-
terms, can be defined relative to a particular foreground (which can also be represented as a set of
documents), but there is the implicit assumption that they all share the same background set: non-genre-
specific language (or simply a very diverse set of documents). It is also possible to refer to terminology in
general as the union of jargon-terms with respect to the set of specialized knowledge areas as foregrounds
and all sharing the same background of non-genre-specific language. Jargon-terms, like topic-terms, are
also time dependent, since some terms will eventually be absorbed into the common lexicon, e.g., com-
puter. However, we can make the simplifying assumption that we are talking about jargon in the present
1
We restrict our scope to nominal terminology, but acknowledge the importance of non-nominal terminology, e.g., event
verb terms (calcify, coactivate) which are crucial to IE.
12
time. Furthermore, jargon-term status is somewhat less time sensitive than topic-term status because ter-
minology is absorbed very sparingly (and very slowly) into the popular lexicon, whereas topics go in and
out of fashion quickly within a literature that is meant for an expert audience. Ignoring the potato type
cases, topic-terms are a proper subset of jargon-terms and, thus, the set of jargon-terms is larger than the
set of topic-terms. Finally, topic terms are ranked with respect to how well they can serve as keywords,
i.e., how specific they are to a particular document set, whereas +/-jargon-term is a binary distinction.
We built a topic term extractor that combines several metrics together in an ensemble including:
TFIDF, KL Divergence (Cover and Thomas, 1991; Hisamitsu et al., 1999) and a combination of Do-
main Relevance and Document Consensus (DRDC) based on (Navigli and Velardi, 2004). Furthermore,
we filtered the output by requiring that each term would be recognized as a term by the jargon-term chun-
ker described below in section 3. We manually scored the top 100 terms generated for two classes of
biology patents (US patent classes 435 and 436) and achieved accuracies of 85% and 76% respectively.
We also manually evaluated the top 100 terms taken from biology articles, yielding an accuracy of about
88%. As discussed, we use the output of this system for our jargon-term extraction system.
3 Jargon-term Extraction by Chunking
(Justeson and Katz, 1995) uses manual rules to detect noun groups (sequences of nouns and adjectives
ending in a noun) with the goal of detecting instances of topic-terms. They filter out those noun groups
that occur only once in the document on the theory that the multiply used noun groups are more likely to
be topics. They manually score their output from two computer science articles and one biotechnology
article, with 146, 350 and 834 instances of terms and achieve accuracies of 96%, 86% and 77%. (Frantzi
et al., 2000) uses linguistic rules similar to noun chunking to detect candidate terms; filters the results
using a stop list and other linguistic constraints; uses statistical filters to determine whether substrings
are likely to be terms as well; and uses statistical filters based on neighboring words (context). (Frantzi et
al., 2000) ranks their terms by scores and achieve about 75% accuracy for the top 40 terms ? their system
is tested on medical records (quite a different corpus form ours). Our system identifies all instances
of terminology (not just topic terms) and identifies many more instances per document (919, 1131 and
2166) than (Justeson and Katz, 1995) or (Frank, 2000). As we aim to find all instances of jargon-terms,
we evaluate for both precision and recall rather than just accuracy (section 5). Two of the documents
that we test on are patents, which have a very different word distribution than articles. In fact, due to
both the amount of repetition in patents and the presence of multiple types of terminology (legal terms
as well as topic-related terms), it is hard to imagine that eliminating terms occurring below a frequency
threshold (as with (Justeson and Katz, 1995)) would be an effective method of filtering. Furthermore,
(Frank, 2000) used a very different corpus than we did and they focused on a slightly different problem
(e.g., we did not attempt to find the highest-ranked terms and we did not attempt to find both long terms
and substrings which were terms). Thus while it is appropriate to compare our methodology, it is difficult
to compare our results.
We have implemented a hand-crafted term extractor, which we will call a jargon-term chunker because
it functions in much the same way as a noun group chunker. It uses a deterministic finite state machine,
based on parts of speech (POS) and a fine-tuned set of lexical categories. We observed that jargon-terms
are typically noun groups, minus some left modifiers, and normally include words that are not in standard
vocabulary or belong to certain other classes of words (e.g., nominalizations). While topic-term tech-
niques factor the distribution of whole term sequences into the choice of topic-terms, our method focuses
on the distribution of words within topic-term sequences. The primary function of POS classification is
to cluster words distributionally in a language. A POS tag reflects the syntactic distribution of the word
in the sense that words with the same POS should be able to replace each other in sentences. Morpholog-
ically, POSs are subject to the same morphological variation (prefixes, suffixes, tense, gender, number,
etc.). For example, the English word duck belongs to the POS noun because it tends to occur: after a
determiner, after an adjective, and ending a unit that can be the subject of a verb: nouns are substitutable
for each other. Furthermore, it has a plural form resulting from an -s or -es suffix, etc. Similarly, we
hold that the presence of particular classes of words within a noun group affects its potential to function
13
as a jargon-term. As will become evident, we can use topic-term-like metrics to identify some of these
word classes. Furthermore, given our previous assertion that topic-terms are a subset of jargon-terms,
we assume that the most saliently ranked topic-terms are also jargon-terms and words that are commonly
parts of topic-terms tend to be parts of jargon-terms. There are also ?morphological properties? that are
indicative of subsets of jargon-terms: allCap acronyms, chemical formulas, etc.
Our system classifies each word using POS tags, manually created dictionaries and the output of our
own topic-term system. These classifications are achieved in four stages. In the first stage we divide
the text into smaller segments using coordinate conjunctions (and, or, as well as, . . .) and punctuation
(periods, left/right parentheses and brackets, quotation marks, commas, colons, semi-colons). These
segments are typically smaller than the level of the sentence, but larger than most noun groups. These
segments are good units to process because they are larger than jargon-terms (substrings of noun groups)
and smaller than sentences (and thus provide a smaller search space). In the second stage, potential
jargon-term (PJs) are generated by processing tokens from left to right and classifying them using a
finite state machine (FSM). The third stage filters the PJs generated with a set of manually constructed
constraints, yielding a set of jargon-terms. A final filter (stage 4) identifies named entities and separates
them out from the true jargon-terms: it turns out that many named entities have similar phrase-internal
properties as jargon-terms.
The FSM (that generates PJs) in the second stage includes the following states (Ramshaw and Marcus,
1995): START (S) (marking the beginning of a segment), Begin Term (B-T), Inside Term (I-T), End
Term (E-T), and Other (O). A PJ is a sequence consisting of: (a) a single E-T; or (b) exactly one B-T,
followed by zero or more instances of I-T, followed by zero or one instances of E-T. Each transition to
a new state is conditioned on: (a) the (extended) POS tag of the current word; (b) the extended POS
tag of the previous word; and (c) the previous state. The extended POSs are derived from the output of
a Penn-Treebank-based POS tagger and refinements based on machine readable dictionaries, including
COMLEX Syntax (Macleod et al., 1997), NOMLEX (Macleod et al., 1998), and some manually encoded
dictionaries created for this project. Table 1 describes the transitions in the FSM (unspecified entries
mean no restriction). ELSE indicates that in all cases other than those listed, the FST goes to state O.
Extended POS tags are classified as follows.
Adjectives, words with POS tags JJ, JJR or JJS, are subdivided into:
STAT-ADJ: Words in this class are marked adjective in our POS dictionaries and found as the first word
in one of the top ranked topic-terms (for the topic associated with the input document).
TECH-ADJ: If an adjective ends in a suffix indicating (-ic, -cous, -xous, and several others) it is a
technical word, but it is not found in our list of exceptions, it is marked TECH-ADJ.
NAT-ADJ: An adjective, usually capitalized, that is the adjectival form of a country, state, city or conti-
nent, e.g., European, Indian, Peruvian, . . .
CAP-ADJ: An adjective such that the first letter is capitalized (but is not marked NAT-ADJ).
ADJ: Other adjectives
Nouns are marked NN or NNS by the POS tagger and are the default POS for out of vocabulary (OOV)
words. POS tags like NNP, NNPS and FW (proper nouns and foreign nouns) are not reliable for our POS
tagger (trained on news) when applied to patents and technical articles. So NOUN is also assumed for
these. Subclasses include:
O-NOUN: (Singular or plural) nouns not found in any of our dictionaries (COMLEX plus some person
names) or nouns found in lists of specialized vocabulary which currently include chemical names.
PER-NOUN: Nouns beginning with a capital that are in our dictionary of first and last names.
PLUR-NOUN: Nouns with POS NNS nouns that are not marked O-NOUN or PER-NOUN.
C-NOUN: Nouns with POS NN that are not marked O-NOUN or PER-NOUN.
Verbs Only ING-VERBs (VBG) and ED-VERBs (VBN and VBD) are needed for this task (other
verbs trigger state O). Finally, we use the following additional POS tags:
POSS: POS for ?s, split off from a possessive noun.
PREP: All prepositions (POS IN and TO)
ROM-NUM: Roman numerals (I, II, . . ., MMM)
14
Previous Current Previous New
POS POS State State
DET, PREP, POSS, VERB O
O-NOUN, C-NOUN, PLUR-NOUN ROM-NUM B-T, I-T E-T
PLUR-NOUN B-T,I-T I-T
ADJ, CAP-ADJ I-T I-T
C-NOUN, PER-NOUN, O-NOUN B-T, I-T I-T
O-NOUN CAP-ADJ, TECH-ADJ, B-T, I-T I-T
STAT-ADJ, NAT-ADJ
CAP-ADJ, TECH-ADJ, NAT-ADJ, E-T, O, S B-T
ING-VERB, ED-VERB, STAT-ADJ
C-NOUN, O-NOUN, PER-NOUN
TECH-ADJ, NAT-ADJ TECH-ADJ, NAT-ADJ B-T, I-T I-T
ADJ, CAP-ADJ ADJ, CAP-ADJ
ELSE O
Table 1: Transition Table
A potential jargon-term (PJ) is an actual jargon-term unless it is filtered out as follows. First, a jargon
term J must meet all of these conditions:
1. J must contain at least one noun.
2. J must be more than one character long, not counting a final period.
3. J must contain at least one word consisting completely of alphabetic characters.
4. J must not end in a common abbreviation from a list (e.g., cf., etc.)
5. J must not contain a word that violates a morphological filter, designed to rule out numeric identi-
fiers (patent numbers), mathematical formulas and other non-words. This rules out tokens beginning
with numbers that include letters; tokens including plus signs, ampersands, subscripts, superscripts;
tokens containing no alphanumeric characters at all, etc.
6. J must not contain a word that is a member of a list of common patent section headings.
Secondly, a jargon-term J must satisfy at least one of the following additional conditions:
1. J = highly ranked topic-term or a substring of J is a highly ranked topic-term.
2. J contains at least one O-NOUN.
3. J consists of at least 4 words, at least 3 of which are either nominalizations (C-NOUNs found in
NOMLEX-PLUS (Meyers et al., 2004; Meyers, 2007)) or TECH-ADJs.
4. J = nominalization at least 11 characters long.
5. J = multi-word ending in a common noun and containing a nominalization.
A final stage aims to distinguish named entities from jargon-terms. It turns out that named entities, like
jargon terms, include many out of vocabulary words. Thus we look for NEs among those PJs that remain
after stage 3 and contain capitalized words (a single capital letter followed by lowercase letters). These
NE filters are based on manually collected lists of named entities and nationality adjectives, as well as
common NE endings. Dictionary lookup is used to assign GPE (ACE?s Geopolitical Entity) to New York
or American; LOC(ation) to Aegean Sea and Ural Mountains; and FAC(ility) to Panama Canal and Suez
Canal. Plurals of nationality words, e.g., Americans are filtered out as non-terms. PJs are filtered by
endings typically associated with non-terms, e.g., et al signals PJs as citations to articles and honorifics
(Esq, PhD, Jr, Snr) signal PER(son) named entities. Finally, if at least one of the words in a multi-word
term is a first or last person name, we can further filter them by endings, where ORGanization endings
15
include Agency, Association, College and more than 65 others; GPE endings include Heights, Township,
Park; LOC(ation) endings include Street, Avenue and Boulevard. It turns out that 2 word capitalized
structures including at least one person name are usually either ORG or GPE in our patent corpus, and
we maintain this ambiguity, but mark them as non-terms.
We have described a first implementation of a jargon-term chunker based on a combination of prin-
ciples previously implemented in noun group chunking and topic-term extraction systems. The chunker
can use essentially the same algorithms as previous noun group chunkers, though in this case we used
a manual-rule based FSM. The extended POSs are defined according to conventional POS (represent-
ing substitutability, morphology, etc.), statistical topic-term extraction, OOV status (absence from our
dictionary) or presence in specialized dictionaries (NOMLEX, dictionary of chemicals, etc.). We use
topic-term extraction to identify both particular noun sequences (high-ranked topic-terms) and some of
their components (STAT-ADJ), and could extend this strategy to other components, e.g., common head
nouns. We approximated the concept of ?rare word? by noting which words were not found in our
standard dictionary (O-NOUN). As is well-known, ?noun? and ?adjective? are the first and second most
frequent POS for OOV words and both POSs are typically found as part of noun groups. Furthermore,
rare instances of O-NOUN (and OOV adjectives) are typically parts of jargon-terms. This approximation
is fine-tuned by the addition of word lists (e.g., chemicals). In future work, we can use more distribu-
tional information to fine-tune these categories, e.g., we can use topic-term techniques to identify single
topic words (nouns and adjectives) and experiment with these additional POS (instead of or in addition
to the current POS classes).
4 The Annotator Definition of Jargon-Term
For purposes of annotation, we defined jargon-term as a word or multi-word nominal expression that is
specific to some technical sublanguage. It need not be a proper noun, but it should be conventionalized
in one of the following two ways:
1. The term is defined early (possibly by being abbreviated) in the document and used repeatedly
(possibly only in its abbreviated form).
2. The term is special to a particular field or subfield (not necessarily the field of the document being
annotated). It is not enough if the document contains a useful description of an object of interest
? there must be some conventional, definable term that can be used and reused. Thus multi-word
expressions that are defined as jargon terms must be somewhat word-like ? mere descriptions that
are never reused verbatim are not jargon terms. (Justeson and Katz, 1995) goes further than we do:
they require that terms be reused within the document being annotated, whereas we only require
that they be reused (e.g., frequent hits in a web search).
Criterion 2 leaves open the question of how specific to a genre an expression must be to be considered a
jargon-term. At an intuitive level, we would like to exclude words like patient, which occur frequently
in medical texts, but are also commonly found in non-expert, everyday language. By contrast, we would
like to include words like tumor and chromosome, which are more intrinsic to technical language insofar
as they have specialized definitions and subtypes within medical language. To clarify, we posited that a
jargon-term must be sufficiently specialized so that a typical naive adult should not be expected to know
the meaning of the term. We developed 2 alternative models of a naive adult:
1. Homer Simpson, an animated TV character who caricatures the typical naive adult?the annotators
invoke the question: Would Homer Simpson know what this means?
2. The Juvenile Fiction sub-corpus of the COCA: The annotators go to http://corpus.byu.
edu/coca/ and search under FIC:Juvenile ? a single occurrence of an expression in this corpus
suggests that it is probably not a jargon-term.
In addition, several rules limited the span of terms to include the head and left modifiers that collocate
with the heads. Decisions about which modifiers to include in a term were difficult. However, as this
16
Strict Sloppy
Doc Terms Matches Pre Rec F Matches Pre Rec F
Annot 1
SRP 1131 798 70.8% 70.6% 70.7% 1041 92.5% 92.0% 92.2%
SUP 2166 1809 87.5% 83.5% 85.5% 1992 96.3% 92.0% 94.1%
VVA 919 713 90.9% 77.6% 83.7% 762 97.2% 82.9% 89.5%
Annot 2
SRP 1131 960 98.4% 84.9% 91.1% 968 99.2% 85.6% 91.9%
SUP 2166 1999 95.5% 92.3% 93.8% 2062 98.5% 95.2% 96.8%
VVA 919 838 97.4% 91.2% 94.2% 855 99.4% 93.0% 96.1%
Base 1
SRP 1131 602 24.3% 53.2% 33.4% 968 44.2% 96.8% 60.7%
SUP 2166 1367 36.5% 63.1% 46.2% 1897 50.6% 87.6% 64.2%
VVA 919 576 28.5% 62.7% 39.2% 887 44.0% 96.5% 60.4%
Base 2:
SRP 1131 66 24.9% 5.8% 9.5% 151 57.0% 13.4% 21.6%
SUP 2166 771 52.3% 35.6% 42.4% 1007 68.4% 46.5% 55.3%
VVA 919 270 45.8% 29.4% 35.8% 392 66.5% 42.6% 51.9%
System SRP 1131 932 39.0% 82.4% 53.0% 1121 46.9% 99.1% 63.7%
Without SUP 2166 1475 39.7% 68.1% 50.2% 1962 52.8% 90.6% 66.7%
Filter VVA 919 629 27.8% 68.4% 39.5% 900 39.8% 97.9% 56.6%
System
SRP 1131 669 69.0% 59.2% 63.7% 802 82.8% 70.9% 76.4%
SUP 2166 1193 64.7% 55.1% 59.5% 1526 82.8% 70.5% 76.1%
VVA 919 581 62.1% 63.2% 62.7% 722 77.2% 78.6% 77.9%
Table 2: Evaluation of Annotation, Baseline and Complete System Against Adjudicated Data
evaluation task came on the heels of the relation extraction task described in section 6, we based our
extent rules on the definitions and the set of problematic examples that were discussed and cataloged
during that project. This essentially formed the annotation equivalent of case-law for extents. We will
make our annotation specifications available on-line, along with discussions of these cases.
5 Evaluation
For evaluation purposes, we annotated all the instances of jargon-terms in a speech recognition patent
(SRP), a sunscreen patent (SUP) and an article about a virus vaccine (VVA). Each document was an-
notated by 2 people and then adjudicated by Annotator 2 after discussing controversial cases. Table 2
scores the system, annotator 1 and annotator 2, by comparing each against the answer key providing:
number of terms in the answer key, number of matches, precision, recall and F-measure. The ?strict?
scores are based on exact matches between system terms and answer key terms, whereas the ?sloppy?
scores count as correct instances where part of a system term matches part of an answer key term (span
errors). As the SRP document was annotated first, some of specification agreement process took place
after annotation and the scores for annotators are somewhat lower than for the other documents. How-
ever, Annotator 1?s scores for SUP and VVA are good approximations of how well a human being should
be expected to perform and the system?s scores should be compared to Annotator 1 (i.e., accounting for
the adjudicator?s bias).
There are 4 system results: two baseline systems and two stages of the system described in section 3.
Baseline 1 assumes terms derived by removing determiners from noun groups ? we used an MEMM
chunker using features from the GENIA corpus (Kim et al., 2003). That system has relatively high recall,
but overgenerates, yielding a lower precision and F-measure than our full system ? it is also inaccurate
at determining the extent of terms. Baseline 2 restricts the noun groups from this same chunker to those
with O-NOUN heads. This improves the precision at a high cost to recall. Similarly, we first ran our
system without filtering the potential jargon-terms, and then we ran the full system. Clearly our more
complex strategy performs better than these baselines and the linguistic filters increase precision more
than they reduce recall, resulting in higher F-measures (though low-precision high-recall output may be
better for some applications).
17
6 Relations with Jargon-Terms
(Meyers et al., 2014) describes the annotation of 200 PubMed articles from and 26 patents with several
relations, as well as a system for automatically extracting relations. It turned out that the automatic
system depended on the creation of a jargon-term extraction system and thus that work was the major
motivating factor for the research described here. Choosing topic-terms as potential arguments would
have resulted in low recall. In contrast, allowing any noun-group to be an argument would have lowered
precision, e.g., diagram, large number, accordance and first step are unlikely to be valid arguments of
relations. In the example: The resequencing pathogen microarray
A2
in the diagram is a promising new
technology., we can detect that the authors of the articles view pathogen microarray as significant, and
not the NG diagram. By selecting jargon-terms as potential arguments we are selecting the most probable
noun group arguments for our relations. For the current system (which does not use a parser), the system
performs best if non-jargon-terms are not considered as potential relation arguments at all. However, one
could imagine a wider coverage (and slower) system incorporating a preference for jargon-terms (like a
selection restriction) with dependency-based constraints.
We will only describe a few of these relations due to space considerations. Our relations include:
(1) ABBREVIATE, a relation between two terms that are equivalent. In the normal case, one term
is clearly a shorthand version of the other, e.g., ?The D. melanogaster gene Muscle LIM protein at
84B
A1
(abbreviated as Mlp84B
A2
)?. However, in the special case (ABBREVIATE:ALIAS) neither
term is a shorthand for the other. For example in ?Silver behenate
A1
, also known as CH3-(CH2)20-
COOAg
A2
?, the chemical name establishes that this substance is a salt, whereas the formula provides
the proportions of all its constituent elements; (2) ORIGINATE, the relation between an ARG1 (person,
organization or document) and an ARG2 (a term), such that the ARG1 is an inventor, discoverer, manu-
facturer, or distributor of the ARG2 and some of these roles are differentiated as subtypes of the relation.
Examples include the following: ?Eagle
A1
?s minimum essential media
A2
and DOPG
A2
was obtained
from Avanti Polar Lipids
A1
?. (3) EXEMPLIFY, an IS-A relation (Hearst, 1992) between terms so
that ARG1 is an instance of ARG2, e.g., ?Cytokines
A2
, for instance interferon
A1
?; and ?proteins
A2
such as insulin
A1
?; (4) CONTRAST relations, e.g., ?necrotrophic effector system
A1
that is an ex-
citing contrast to the biotrophic effector models
A2
?; (5) BETTER THAN relations, e.g., ?Bayesian
networks
A1
hold a considerable advantage over pairwise association tests
A2
?; and (6) SIGNIFICANT
relations, e.g., ?Anaerobic SBs
A2
are an emerging area of research and development? (ARG1, the author
of the article, is implicit). These relations are applicable to most technical genres.
7 Concluding Remarks
We have described a method for extracting instances of jargon-terms with an F-measure of between
62% and 77% (strict vs sloppy), about 73% to 84% of human performance. We expect this work to
facilitate the extraction of a wide reange of relations from technical documents. Previous work has
focused on generating topic-terminology or term types, extracted over sets of documents. In contrast, we
describe an effective method of extracting term tokens, which represent a larger percent of the instances
of terminology in documents and constitute arguments of many more potential relations. Our work on
relation extraction yielded very low recalls until we adopted this methodology. Consequently, we have
obtained recall of over 50% for many relations (with precision ranging from 70% for OPINION relations
like Significant to 96% for Originate.).
Acknowledgments
Supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Inte-
rior National Business Center contract number D11PC20154. The U.S. Government is authorized to
reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation
thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should
not be interpreted as necessarily representing the official policies or endorsements, either expressed or
implied, of IARPA, DoI/NBC, or the U.S. Government.
18
References
M. Bada, L. E. Hunter, M. Eckert, and M. Palmer. 2010. An overview of the craft concept annotation guidelines.
In The Linguistic Annotation Workshop, ACL 2010, pages 207?211.
BioCreAtIvE. 2006. Biocreative ii.
M. Bundschus, M. Dejori, M. Stetter, V Tresp, and H. Kriegel. 2008. Extraction of semantic biomedical relations
from text using conditional random fields. BMC Bioinformatics, 9.
P. Corbett, C. Batchelor, and S. Teufel. 2007. Annotation of chemical named entities. In BioNLP 2007, pages
57?64.
Thomas M. Cover and Joy A. Thomas. 1991. Elements of Information Theory. Wiley-Interscience, New York.
A. Frank. 2000. Automatic F-Structure Annotation of Treebank Trees. In Proceedings of The LFG00 Conference,
Berkeley.
K. Frantzi, S. Ananiadou, and H. Mima. 2000. Automatic recognition of multi-word terms:. the C-value/NC-value
method. International Journal on Digital Libraries, 3(2):115?130.
C. Giuliano, A. Lavelli, and L. Romano. 2006. Exploiting shallow linguistic information for relation extraction
from biomedical literature. In EACL 2006, pages 401?408, Trento.
M. Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. In ACL 1992, pages 539?545.
T. Hisamitsu, Y. Niwa, S. Nishioka, H. Sakurai, O. Imaichi, M. Iwayama, and A. Takano. 1999. Term extraction
using a new measure of term representativeness. In Proceedings of the First NTCIR Workshop on Research in
Japanese Text Retrieval and Term Recognition.
Houghton Mifflin Company. 2001. Webster?s II New College Dictionary. Houghton Mifflin Company.
C. Jacquemin and D. Bourigault. 2003. Term Extraction and Automatic Indexing. In R. Mitkov, editor, Handbook
of Computational Linguistics. Oxford University Press, Oxford.
Y. Jin, M. Kan, J. Ng, and X. He. 2013. Mining scientific terms and their definitions: A study of the acl anthology.
In EMNLP-2013.
J. S. Justeson and S. M. Katz. 1995. Technical terminology: some linguistic properties and an algorithm for
identification in text. Natural Language Engineering, 1(1):9?27.
J. D. Kim, T. Ohta, Y. Tateisi, and J. I. Tsujii. 2003. Genia corpus?a semantically annotated corpus for bio-
textmining. Bioinformatics, 19 (suppl 1):i180?i182.
C. Macleod, R. Grishman, and A. Meyers. 1997. COMLEX Syntax. Computers and the Humanities, 31:459?481.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and R. Reeves. 1998. Nomlex: A lexicon of nominalizations. In
Proceedings of Euralex98.
A. Meyers, R. Reeves, C. Macleod, R. Szekeley, V. Zielinska, and B. Young. 2004. The Cross-Breeding of
Dictionaries. In Proceedings of LREC-2004, Lisbon, Portugal.
A. Meyers, G. Lee, A. Grieve-Smith, Y. He, and H. Taber. 2014. Annotating Relations in Scientific Articles. In
LREC-2014.
A. Meyers. 2007. Those Other NomBank Dictionaries ? Manual for Dictionaries that Come with NomBank.
http:nlp.cs.nyu.edu/meyers/nombank/nomdicts.pdf.
R. Navigli and P. Velardi. 2004. Learning Domain Ontologies from Document Warehouses and Dedicated Web
Sites. Computational Linguistics, 30.
L. A. Ramshaw and M. P. Marcus. 1995. Text Chunking using Transformation-Based Learning. In ACL Third
Workshop on Very Large Corpora, pages 82?94.
A. Schwartz and M. Hearst. 2003. A simple algorithm for identifying abbreviation definitions in biomedical text.
In Pacific Composium on Biocomputing.
T. Tomokiyo and M. Hurst. 2003. A language model approach to keyphrase extraction. In ACL 2003 Workshop
on Multiword Expressions: Analysis, Acquisition and Treatment.
19
P. Velardi, M. Missikoff, and R. Basili. 2001. Identification of relevant terms to support the construction of domain
ontologies. In Workshop on Human Language Technology and Knowledge Management - Volume 2001, pages
5:1?5:8.
20
