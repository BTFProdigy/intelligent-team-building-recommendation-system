Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 377?386,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Inferring Selectional Preferences from Part-Of-Speech N-grams 
 
 
Hyeju Jang and Jack Mostow 
Project LISTEN (www.cs.cmu.edu/~listen), School of Computer Science 
Carnegie Mellon University 
Pittsburgh, PA 15213, USA 
hyejuj@cs.cmu.edu, mostow@cs.cmu.edu 
 
 
 
 
 
 
Abstract 
We present the PONG method to compute 
selectional preferences using part-of-speech 
(POS) N-grams.  From a corpus labeled with 
grammatical dependencies, PONG learns the 
distribution of word relations for each POS 
N-gram.  From the much larger but unlabeled 
Google N-grams corpus, PONG learns the 
distribution of POS N-grams for a given pair 
of words.  We derive the probability that one 
word has a given grammatical relation to the 
other. PONG estimates this probability by 
combining both distributions, whether or not 
either word occurs in the labeled corpus.  
PONG achieves higher average precision on 
16 relations than a state-of-the-art baseline in 
a pseudo-disambiguation task, but lower 
coverage and recall. 
1 Introduction 
Selectional preferences specify plausible fillers 
for the arguments of a predicate, e.g., celebrate.  
Can you celebrate a birthday?  Sure.  Can you 
celebrate a pencil?  Arguably yes:  Today the 
Acme Pencil Factory celebrated its one-billionth 
pencil.  However, such a contrived example is 
unnatural because unlike birthday, pencil lacks a 
strong association with celebrate.  How can we 
compute the degree to which birthday or pencil 
is a plausible and typical object of celebrate? 
Formally, we are interested in computing the 
probability Pr(r | t, R), where (as Table 1 
specifies), t is a target word such as celebrate, r 
is a word possibly related to it, such as birthday 
or pencil, and R is a possible relation between 
them, whether a semantic role such as the agent 
of an action, or a grammatical dependency such 
as the object of a verb.  We call t the ?target? 
because originally it referred to a vocabulary 
word targeted for instruction, and r its ?relative.? 
 
Notation Description 
R a relation between words 
t a target word 
r, r' possible relatives of t 
g a word N-gram 
gi and gj i
th and jth words of g 
p the POS N-gram of g 
 
Table 1:  Notation used throughout this paper 
 
Previous work on selectional preferences has 
used them primarily for natural language analytic 
tasks such as word sense disambiguation (Resnik, 
1997),  dependency parsing (Zhou et al 2011), 
and semantic role labeling (Gildea and Jurafsky, 
2002).  However, selectional preferences can 
also apply to natural language generation tasks 
such as sentence generation and question 
generation.  For generation tasks, choosing the 
right word to express a specified argument of a 
relation requires knowing its connotations ? that 
is, its selectional preferences.  Therefore, it is 
useful to know selectional preferences for many 
different relations.  Such knowledge could have 
many uses.  In education, they could help teach 
word connotations.  In machine learning they 
could help computers learn languages.  In 
machine translation, they could help generate 
more natural wording. 
This paper introduces a method named PONG 
(for Part-Of-Speech N-Grams) to compute 
selectional preferences for many different 
relations by combining part-of-speech 
information and Google N-grams.  PONG 
achieves higher precision on a pseudo-
377
disambiguation task than the best previous model 
(Erk et al 2010), but lower coverage. 
The paper is organized as follows.  Section 2 
describes the relations for which we compute 
selectional preferences.  Section 3 describes 
PONG.  Section 4 evaluates PONG.  Section 5 
relates PONG to prior work.  Section 6 concludes.   
2 Relations Used 
Selectional preferences characterize constraints 
on the arguments of predicates.  Selectional 
preferences for semantic roles (such as agent and 
patient) are generally more informative than for 
grammatical dependencies (such as subject and 
object).  For example, consider these 
semantically equivalent but grammatically 
distinct sentences: 
Pat opened the door. 
The door was opened by Pat.   
In both sentences the agent of opened, namely 
Pat, must be capable of opening something ? an 
informative constraint on Pat.  In contrast, 
knowing that the grammatical subject of opened 
is Pat in the first sentence and the door in the 
second sentence tells us only that they are nouns. 
Despite this limitation, selectional preferences 
for grammatical dependencies are still useful, for 
a number of reasons.  First, in practice they 
approximate semantic role labels.  For instance, 
typically the grammatical subject of opened is its 
agent.  Second, grammatical dependencies can be 
extracted by parsers, which tend to be more 
accurate than current semantic role labelers.  
Third, the number of different grammatical 
dependencies is large enough to capture diverse 
relations, but not so large as to have sparse data 
for individual relations.  Thus in this paper, we 
use grammatical dependencies as relations. 
A parse tree determines the basic grammatical 
dependencies between the words in a sentence.  
For instance, in the parse of Pat opened the door, 
the verb opened has Pat as its subject and door 
as its object, and door has the as its determiner.  
Besides these basic dependencies, we use two 
additional types of dependencies. 
Composing two basic dependencies yields a 
collapsed dependency (de Marneffe and Manning, 
2008).  For example, consider this sentence: 
The airplane flies in the sky. 
Here sky is the prepositional object of in, which 
is the head of a prepositional phrase attached to 
flies.  Composing these two dependencies yields 
the collapsed dependency prep_in between flies 
and sky, which captures an important semantic 
relation between these two content words:  sky is 
the location where flies occurs.  Other function 
words yield different collapsed dependencies.  
For example, consider these two sentences: 
The airplane flies over the ocean. 
The airplane flies and lands. 
Collapsed dependencies for the first sentence 
include prep_over between flies and ocean, 
which characterizes their relative vertical 
position, and conj_and between flies and lands, 
which links two actions that an airplane can 
perform.  As these examples illustrate, collapsing 
dependencies involving prepositions and 
conjunctions can yield informative dependencies 
between content words. 
Besides collapsed dependencies, PONG infers 
inverse dependencies.  Inverse selectional 
preferences are selectional preferences of 
arguments for their predicates, such as a 
preference of a subject or object for its verb.  
They capture semantic regularities such as the set 
of verbs that an agent can perform, which tend to 
outnumber the possible agents for a verb (Erk et 
al., 2010). 
3 Method 
To compute selectional preferences, PONG 
combines information from a limited corpus 
labeled with the grammatical dependencies 
described in Section 2, and a much larger 
unlabeled corpus.  The key idea is to abstract 
word sequences labeled with grammatical 
relations into POS N-grams, in order to learn a 
mapping from POS N-grams to those relations.  
For instance, PONG abstracts the parsed 
sentence Pat opened the door as NN VB DT NN, 
with the first and last NN as the subject and 
object of the VB.  To estimate the distribution of 
POS N-grams containing particular target and 
relative words, PONG POS-tags Google N-
grams (Franz and Brants, 2006). 
Section 3.1 derives PONG?s probabilistic 
model for combining information from labeled 
and unlabeled corpora.  Section 3.2 and Section 
3.3 describe how PONG estimates probabilities 
from each corpus.  Section 3.4 discusses a 
sparseness problem revealed during probability 
estimation, and how we address it in PONG. 
3.1 Probabilistic model 
We quantify the selectional preference for a 
relative r to instantiate a relation R of a target t as 
the probability Pr(r | t, R), estimated as follows.  
By the definition of conditional probability: 
378
Pr( , , )Pr( | , ) Pr( , )
r t Rr t R t R
 
We care only about the relative probability of 
different r for fixed t and R, so we rewrite it as:   
Pr( , , )r t R  
We use the chain rule: 
Pr( | , ) Pr( | ) Pr( )R r t r t t 
and notice that t is held constant: 
Pr( | , ) Pr( | )R r t r t 
We estimate the second factor as follows:
 
Pr( , ) freq( , )Pr( | ) Pr( ) freq( )
t r t rr t t t
 
We calculate the denominator freq(t) as the 
number of  N-grams in the Google N-gram 
corpus that contain t, and the numerator freq(t, r) 
as the number of N-grams containing both t and r. 
To estimate the factor Pr(R | r, t) directly from 
a corpus of text labeled with grammatical 
relations, it would be trivial to count how often a 
word r bears relation R to target word t.  
However, the results would be limited to the 
words in the corpus, and many relation 
frequencies would be estimated sparsely or 
missing altogether; t or r might not even occur. 
Instead, we abstract each word in the corpus as 
its part-of-speech (POS) label.  Thus we abstract 
The big boy ate meat as DT JJ NN VB NN.  We 
call this sequence of POS tags a POS N-gram.  
We use POS N-grams to predict word relations.  
For instance, we predict that in any word 
sequence with this POS N-gram, the JJ will 
modify (amod) the first NN, and the second NN 
will be the direct object (dobj) of the VB.   
This prediction is not 100% reliable.  For 
example, the initial 5-gram of The big boy ate 
meat pie has the same POS 5-gram as before.  
However, the dobj of its VB (ate) is not the 
second NN (meat), but the subsequent NN (pie).  
Thus POS N-grams predict word relations only 
in a probabilistic sense. 
To transform Pr(R | r, t) into a form we can 
estimate, we first apply the definition of 
conditional probability: 
 Pr( , , )Pr( | , ) Pr( , )
R t rR t r t r 
To estimate the numerator Pr(R, t, r), we first 
marginalize over the POS N-gram p: 
 Pr( , , , )  Pr( , )p
R t r p
t r
 
We expand the numerator using the chain rule: 
 Pr( | , , ) Pr( | , ) Pr( , )
Pr( , )p
R t r p p t r t r
t r
 
Cancelling the common factor yields: 
 
Pr( | , , ) Pr( | , )
p
R p t r p t r
 
We approximate the first term Pr(R | p, t, r) as 
Pr(R | p), based on the simplifying assumption 
that R is conditionally independent of t and r, 
given p.  In other words, we assume that given a 
POS N-gram, the target and relative words t and 
r give no additional information about the 
probability of a relation.  However, their 
respective positions i and j in the POS N-gram p 
matter, so we condition the probability on them: 
 Pr( | , , ) Pr( | , , )R p t r R p i j 
Summing over their possible positions, we get 
Pr( | , )
Pr( | , , ) Pr( | , )i j
p i j
R r t
R p i j p t g r g
 
As Figure 1 shows, we estimate Pr(R | p, i, j) by 
abstracting the labeled corpus into POS N-grams. 
We estimate Pr(p | t = gi, r = gj) based on the 
frequency of partially lexicalized POS N-grams 
like DT JJ:red NN:hat VB NN among Google N-
grams with t and r in the specified positions. 
Sections 3.2 and 3.3 describe how we estimate 
Pr(R | p, i, j) and Pr(p | t = gi, r = gj), respectively.  
Note that PONG estimates relative rather than 
absolute probabilities.  Therefore it cannot (and 
does not) compare them against a fixed threshold 
to make decisions about selectional preferences.  
3.2 Mapping POS N-grams to relations 
To estimate Pr(R | p, i, j), we use the Penn 
Treebank Wall Street Journal (WSJ) corpus, 
which is labeled with grammatical relations 
using the Stanford dependency parser (Klein and 
Manning, 2003).   
To estimate the probability Pr(R | p, i, j) of a 
relation R between a target at position i and a 
relative at position j in a POS N-gram p, we 
compute what fraction of the word N-grams g 
with POS N-gram p have relation R between 
some target t and relative r at positions i and j: 
Pr( | , , )
freq( . .POS( ) relation( , ) )
freq( . .POS( ) relation( , ))
i j
i j
R p i j
g s t g p g g R
g s t g p g g 
3.3 Estimating POS N-gram distributions 
Given a target and relative, we need to estimate 
their distribution of POS N-grams and positions. 
379
 
Figure 1:  Overview of PONG.   
From the labeled corpus, PONG extracts abstract mappings from POS N-grams to relations. 
From the unlabeled corpus, PONG estimates POS N-gram probability given a target and relative. 
 
A labeled corpus is too sparse for this purpose, 
so we use the much larger unlabeled Google N-
grams corpus (Franz and Brants, 2006). 
The probability that an N-gram with target t at 
position i and relative r at position j will have the 
POS N-gram p is: 
Pr( | , )
freq( . .POS( ) , , ))
freq( . . )
i j
i j
i j
p t g r g
g s t g p g t g r
g s t g t g r
  
To compute this ratio, we first use a well-
indexed table to efficiently retrieve all N-grams 
with words t and r at the specified positions.  We 
then obtain their POS N-grams from the Stanford 
POS tagger (Toutanova et al 2003), and count 
how many of them have the POS N-gram p. 
3.4 Reducing POS N-gram sparseness 
We abstract word N-grams into POS N-grams to 
address the sparseness of the labeled corpus, but 
even the POS N-grams can be sparse.  For n=5, 
the rarer ones occur too sparsely (if at all) in our 
labeled corpus to estimate their frequency. 
To address this issue, we use a coarser POS 
tag set than the Penn Treebank POS tag set.  As 
Table 2 shows, we merge tags for adjectives, 
nouns, adverbs, and verbs into four coarser tags.   
Coarse Original  
ADJ JJ, JJR, JJS 
ADVERB RB, RBR, RBS 
NOUN NN, NNS, NNP, NNPS 
VERB VB, VBD, VBG, VBN, VBP, VBZ 
Table 2:  Coarser POS tag set used in PONG 
To gauge the impact of the coarser POS tags, 
we calculated Pr(r | t, R) for 76 test instances 
used in an earlier unpublished study by Liu Liu, 
a former Project LISTEN graduate student.  Each 
instance consists of two randomly chosen words 
in the WSJ corpus labeled with a grammatical 
relation.  Coarse POS tags increased coverage of 
this pilot set ? that is, the fraction of instances for 
which PONG computes a probability ? from 69% 
to 92%. 
Using the universal tag set (Petrov et al 2011) 
as an even coarser tag set is an interesting future 
direction, especially for other languages.  Its 
smaller size (12 tags vs. our 23) should reduce 
data sparseness, but increase the risk of over-
generalization. 
4 Evaluation 
To evaluate PONG, we use a standard pseudo-
disambiguation task, detailed in Section 4.1.  
Section 4.2 describes our test set.  Section 4.3 
lists the metrics we evaluate on this test set.  
Section 4.4 describes the baselines we compare 
PONG against on these metrics, and Section 4.5 
describes the relations we compare them on.  
Section 4.6 reports our results.  Section 4.7 
analyzes sources of error. 
4.1 Evaluation task 
The pseudo-disambiguation task (Gale et al 
1992; Schutze, 1992) is as follows:  given a 
target word t, a relation R, a relative r, and a 
random distracter r', prefer either r or r', 
whichever is likelier to have relation R to word t. 
This evaluation does not use a threshold:  just 
prefer whichever word is likelier according to the 
model being evaluated.  If the model assigns only 
one of the words a probability, prefer it, based on 
the assumption that the unknown probability of 
the other word is lower.  If the model assigns the 
same probability to both words, or no probability 
to either word, do not prefer either word. 
380
4.2 Test set 
As a source of evaluation data, we used the 
British National Corpus (BNC).  As a common 
test corpus for all the methods we evaluated, we 
selected one half of BNC by sorting filenames 
alphabetically and using the odd-numbered files.  
We used the other half of BNC as a training 
corpus for the baseline methods we compared 
PONG to. 
A test set for the pseudo-disambiguation task 
task consists of tuples of the form (R, t, r, r').  To 
construct a test set, we adapted the process used 
by Rooth et al(1999) and Erk et al(2010). 
First, we chose 100 (R, t) pairs for each 
relation R at random from the test corpus. Rooth 
et al(1999) and Erk et al(2010) chose such 
pairs from a training corpus to ensure that it 
contained the target t.  In contrast, choosing pairs 
from an unseen test corpus includes target words 
whether or not they occur in the training corpus. 
To obtain a sample stratified by frequency, 
rather than skewed heavily toward high-
frequency pairs, Erk et al(2010) drew (R, t) 
pairs from each of five frequency bands in the 
entire British National Corpus (BNC):  50-100 
occurrences; 101-200; 201-500; 500-1000; and 
more than 1000.  However, we use only half of 
BNC as our test corpus, so to obtain a 
comparable test set, we drew 20 (R, t) pairs from 
each of the corresponding frequency bands in 
that half:  26-50 occurrences; 51-100; 101-250; 
251-500; and more than 500. 
For each chosen (R, t) pair, we drew a separate 
(R, t, r) triple from each of six frequency bands:  
1-25 occurrences; 26-50; 51-100; 101-250; 251-
500; and more than 500.  We necessarily omitted 
frequency bands that contained no such triples.  
We filtered out triples where r did not have the 
most frequent part of speech for the relation R.  
For example, this filter would exclude the triple 
(dobj, celebrate, the) because a direct object is 
most frequently a noun, but the is a determiner. 
Then, like Erk et al(2010), we paired the 
relative r in each (R, t, r) triple with a distracter r' 
with the same (most frequent) part of speech as 
the relative r, yielding the test tuple (R, t, r, r'). 
Rooth et al(1999) restricted distracter 
candidates to words with between 30 and 3,000 
occurrences in BNC; accordingly, we chose only 
distracters with between 15 and 1,500 
occurrences in our test corpus.  We selected r' 
from these candidates randomly, with probability 
proportional to their frequency in the test corpus.  
Like Rooth et al(1999), we excluded as 
distracters any actual relatives, i.e. candidates r' 
where the test corpus contained the triple (R, t, r').  
Table 3 shows the resulting number of (R, t, r, r') 
test tuples for each relation. 
 
Relation R # tuples for R # tuples for RT 
advmod 121 131 
amod 162 128 
conj_and 155 151 
dobj 145 167 
nn 173 158 
nsubj  97 124 
prep_of 144 153 
xcomp 139 140 
Table 3:  Test set size for each relation 
4.3 Metrics 
We report four evaluation metrics:  precision, 
coverage, recall, and F-score.  Precision (called 
?accuracy? in some papers on selectional 
preferences) is the percentage of all covered 
tuples where the original relative r is preferred.  
Coverage is the percentage of tuples for which 
the model prefers r to r' or vice versa.  Recall is 
the percentage of all tuples where the original 
relative is preferred, i.e., precision times 
coverage.  F-score is the harmonic mean of 
precision and recall. 
4.4 Baselines 
We compare PONG to two baseline methods.   
EPP is a state-of-the-art model for which Erk 
et al(2010) reported better performance than 
both Resnik?s (1996) WordNet model and 
Rooth?s (1999) EM clustering model.  EPP 
computes selectional preferences using 
distributional similarity, based on the assumption 
that relatives are likely to appear in the same 
contexts as relatives seen in the training corpus.  
EPP computes the similarity of a potential 
relative?s vector space representation to relatives 
in the training corpus. 
EPP has various options for its vector space 
representation, similarity measure, weighting  
scheme, generalization space, and whether to use 
PCA.  In re-implementing EPP, we chose the 
options that performed best according to Erk et al
(2010), with one exception.  To save work, we 
chose not to use PCA, which Erk et al(2010) 
described as performing only slightly better in 
the dependency-based space. 
381
Relation Target Relative Description 
advmod verb adverb Adverbial modifier 
amod noun adjective Adjective modifier 
conj_and noun noun Conjunction with ?and? 
dobj verb noun Direct object 
nn noun noun Noun compound modifier 
nsubj verb noun Nominal subject 
prep_of noun noun Prepositional modifier 
xcomp verb verb Open clausal complement 
 
Table 4: Relations tested in the pseudo-disambiguation experiment.   
Relation names and descriptions are from de Marneffe and Manning (2008) except for prep_of.   
Target and relative POS are the most frequent POS pairs for the relations in our labeled WSJ corpus. 
 
Relation 
Precision (%) Coverage (%) Recall (%) F-score (%) 
PONG EPP DEP PONG EPP DEP PONG EPP DEP PONG EPP DEP 
advmod 78.7 - 98.6 72.1 - 69.2 56.7 - 68.3 65.9 - 80.7 
advmodT 89.0 71.0 97.4 69.5 100 59.5 61.8 71.0 58.0 73.0 71.0 72.7 
amod 78.8 - 99.0 90.1 - 61.1 71.0 - 60.5 74.7 - 75.1 
amodT 84.1 74.0 97.3 83.6 99.2 57.0 70.3 73.4 55.5 76.6 73.7 70.6 
conj_and 77.2 74.2 100 73.6 100 52.3 56.8 74.2 52.3 65.4 74.2 68.6 
conj_andT 80.5 70.2 97.3 74.8 100 49.7 60.3 70.2 48.3 68.9 70.2 64.6 
dobj 87.2 80.0 97.7 80.7 100 60.0 70.3 80.0 58.6 77.9 80.0 73.3 
dobjT 89.6 80.2 98.1 92.2 100 64.1 82.6 80.2 62.9 86.0 80.2 76.6 
nn 86.7 73.8 97.2 95.3 99.4 63.0 82.7 73.4 61.3 84.6 73.6 75.2 
nnT 83.8 79.7 99.0 93.7 100 60.8 78.5 79.7 60.1 81.0 79.7 74.8 
nsubj 76.1 77.3 100 69.1 100 42.3 52.6 77.3 42.3 62.2 77.3 59.4 
nsubjT 78.5 66.9 95.0 86.3 100 48.4 67.7 66.9 46.0 72.7 66.9 62.0 
prep_of 88.4 77.8 98.4 84.0 100 44.4 74.3 77.8 43.8 80.3 77.8 60.6 
prep_ofT 79.2 76.5 97.4 81.7 100 50.3 64.7 76.5 49.0 71.2 76.5 65.2 
xcomp 84.0 61.9 95.3 85.6 100 61.2 71.9 61.9 58.3 77.5 61.9 72.3 
xcompT 86.4 78.6 98.9 89.3 100 63.6 77.1 78.6 62.9 81.5 78.6 76.9 
average 83.0 74.4 97.9 82.6 99.9 56.7 68.7 74.4 55.5 75.0 74.4 70.5 
 
Table 5:  Coverage, Precision, Recall, and F-score for various relations; RT is the inverse of relation R. 
PONG uses POS N-grams, EPP uses distributional similarity, and DEP uses dependency parses. 
 
To score a potential relative r0, EPP uses this 
formula:
,
, 0 0
arg ( , ) ,
( )( ) ( , )R tR t
r Seen s R t R t
wt rSelpref r sim r rZ 
Here sim(r0, r) is the nGCM similarity defined 
below between vector space representations of r0 
and a relative r seen in the training data: 
 
2
1
2
1
'
( , ') exp( ( ) )
'
i i
i
n
b b
nGCM
i
n
b
i
a a
sim a a
a a
where a a
 
The weight function wtr,t(a) is analogous to 
inverse document frequency in Information 
Retrieval. 
DEP, our second baseline method, runs the 
Stanford dependency parser to label the training 
corpus with grammatical relations, and uses their 
frequencies to predict selectional preferences.  
To do the pseudo-disambiguation task, DEP 
compares the frequencies of (R, t, r) and (R, t, r'). 
4.5 Relations tested 
To test PONG, EPP, and DEP, we chose the 
most frequent eight relations between content 
words in the WSJ corpus, which occur over 
10,000 times and are described in Table 4.  We 
also tested their inverse relations.  However, EPP 
does not compute selectional preferences for 
adjective and adverb as relatives.  For this reason, 
we did not test EPP on advmod and amod 
relations with adverbs and adjectives as relatives. 
382
4.6 Experimental results 
Table 5 displays results for all 16 relations.  To 
compute statistical significance conservatively in 
comparing methods, we used paired t-tests with 
N = 16 relations. 
PONG?s precision was significantly better 
than EPP (p<0.001) but worse than DEP 
(p<0.0001).  Still, PONG?s high precision 
validates its underlying assumption that POS N-
grams strongly predict grammatical 
dependencies. 
On coverage and recall, EPP beat PONG, 
which beat DEP (p<0.0001).  PONG?s F-score 
was higher, but not significantly, than EPP?s 
(p>0.5) or DEP?s (p>0.02). 
4.7 Error analysis 
In the pseudo-disambiguation task of choosing 
which of two words is related to a target, PONG 
makes errors of coverage (preferring neither 
word) and precision (preferring the wrong word). 
Coverage errors, which occurred 17.4% of the 
time on average, arose only when PONG failed 
to estimate a probability for either word.  PONG 
fails to score a potential relative r of a target t 
with a specified relation R if the labeled corpus 
has no POS N-grams that (a) map to R, (b) 
contain the POS of t and r, and (c) match Google 
word N-grams with t and r at those positions.  
Every relation has at least one POS N-gram that 
maps to it, so condition (a) never fails.  PONG 
uses the most frequent POS of t and r, and we 
believe that condition (b) never fails.  However, 
condition (c) can and does fail when t and r do 
not co-occur in any Google N-grams, at least that 
match a POS N-gram that can map to relation R.  
For example, oversee and diet do not co-occur in 
any Google N-grams, so PONG cannot score diet 
as a potential dobj of oversee. 
Precision errors, which occur 17% of the time 
on average, arose when (a) PONG scored the 
distracter but failed to score the true relative, or 
(b) scored them both but preferred the distracter.  
Case (a) accounted for 44.62% of the errors on 
the covered test tuples. 
One likely cause of errors in case (b) is over-
generalization when PONG abstracts a word N-
gram labeled with a relation by mapping its POS 
N-gram to that relation.  In particular, the coarse 
POS tag set may discard too much information.  
Another likely cause of errors is probabilities 
estimated poorly due to sparse data.   The 
probability of a relation for a POS N-gram rare in 
the training corpus is likely to be inaccurate.  So 
is the probability of a POS N-gram for rare co-
occurrences of a target and relative in Google 
word N-grams.  Using a smaller tag set may 
reduce the sparse data problem but increase the 
risk of over-generalization. 
5 Relation to Prior Work 
In predicting selectional preferences, a key 
issue is generalization.  Our DEP baseline simply 
counts co-occurrences of target and relative 
words in a corpus to predict selectional 
preferences, but only for words seen in the 
corpus.  Prior work, summarized in  
Table 6, has therefore tried to infer the similarity 
of unseen relatives to seen relatives. To illustrate, 
consider the problem of inducing that the direct 
objects of celebrate tend to be days or events. 
Resnik (1996) combined WordNet with a 
labeled corpus to model the probability that 
relatives of a predicate belong to a particular 
conceptual class.  This method could notice, for 
example, that the direct objects of celebrate tend 
to belong to the conceptual class event.  Thus it 
could prefer anniversary or occasion as the 
object of celebrate even if unseen in its training 
corpus.  However, this method depends strongly 
on the WordNet taxonomy. 
Rather than use linguistic resources such as 
WordNet, Rooth et al(1999) and Wald et al
(2008) induced semantically annotated 
subcategorization frames from unlabeled corpora. 
They modeled semantic classes as hidden 
variables, which they estimated using EM-based 
clustering.  Ritter (2010) computed selectional 
preferences by using unsupervised topic models 
such as LinkLDA, which infers semantic classes 
of words automatically instead of requiring a pre-
defined set of classes as input. 
The contexts in which a linguistic unit occurs 
provide information about its meaning.  Erk 
(2007) and Erk et al(2010) modeled the 
contexts of a word as the distribution of words  
that co-occur with it.  They calculated the 
semantic similarity of two words as the similarity 
of their context distributions according to various 
measures.  Erk et al(2010) reported the state-of-
the-art method we used as our EPP baseline. 
In contrast to prior work that explored various 
solutions to the generalization problem, we don?t 
so much solve this problem as circumvent it.  
Instead of generalizing from a training corpus 
directly to unseen words, PONG abstracts a word 
N-gram to a POS N-gram and maps it to the 
relations that the word N-gram is labeled with. 
383
 
Table 6:  Comparison with prior methods to compute selectional preferences 
 
To compute selectional preferences, whether the 
words are in the training corpus or not, PONG 
applies these abstract mappings to word N-grams 
in the much larger Google N-grams corpus. 
Some prior work on selectional preferences 
has used POS N-grams and a large unlabeled 
corpus.  The most closely related work we found 
was by Gormley et al(2011).  They used 
patterns in POS N-grams to generate test data for 
their selectional preferences model, but not to 
infer preferences.  Zhou et al(2011) identified 
selectional preferences of one word for another 
Reference Relation to 
target 
Lexical 
resource 
Primary  corpus 
(labeled) & 
information 
used 
Generalization  
corpus 
(unlabeled) & 
information used 
Method 
Resnik, 
1996 
Verb-object 
Verb-subject 
Adjective-noun 
Modifier-head 
Head-modifier 
Senses in 
WordNet 
noun 
taxonomy 
Target, relative, 
and relation in a 
parsed, partially 
sense-tagged 
corpus (Brown 
corpus) 
none Information 
theoretic 
model 
Rooth et 
al., 1999 
Verb-object 
Verb-subject 
none Target, relative, 
and relation in a 
parsed corpus 
(parsed BNC) 
none EM-based 
clustering 
Ritter, 
2010 
Verb-subject 
Verb-object 
Subject-verb-
object 
none Subject-verb-
object tuples 
from 500 million 
web-pages 
none LDA model 
Erk, 2007 Predicate and 
Semantic roles 
none Target, relative, 
and relation in a 
semantic role 
labeled corpus 
(FrameNet) 
Words and their 
relations in a 
parsed corpus 
(BNC) 
Similarity 
model based 
on word co-
occurrence  
Erk et al 
2010 
SYN option:  
Verb-subject 
Verb-object, and 
their inverse 
relations 
SEM option:  
verb and 
semantic roles 
that have nouns 
as their headword 
in a primary 
corpus, and their 
inverse relations 
none Target, relative, 
and relation in 
SYN   option:  a  
parsed corpus 
(parsed BNC) 
SEM   option:  a 
semantic role 
labeled corpus 
(FrameNet) 
Two options: 
 
WORDSPACE:  
an unlabeled 
corpus (BNC) 
 
DEPSPACE:  
Words and their 
subject and object 
relations in a 
parsed corpus 
(parsed BNC) 
Similarity 
model using 
vector space 
representation 
of words 
Zhou et 
al., 2011 
Any (relations 
not distinguished) 
none Counts of words 
in Web or 
Google N-gram 
none PMI 
(Pointwise 
Mutual 
Information) 
This paper All grammatical 
dependencies in a 
parsed corpus, 
and their inverse 
relations 
none POS N-gram 
distribution for 
relations in 
parsed WSJ 
corpus 
POS N-gram 
distribution for 
target and relative 
in Google N-gram 
Combine both 
POS N-gram 
distributions 
384
by using Pointwise Mutual Information (PMI) 
(Fano, 1961) to check whether they co-occur 
more frequently in a large corpus than predicted 
by their unigram frequencies.  However, their 
method did not distinguish among different 
relations. 
6 Conclusion 
This paper describes, derives, and evaluates 
PONG, a novel probabilistic model of selectional 
preferences.  PONG uses a labeled corpus to map 
POS N-grams to grammatical relations.  It 
combines this mapping with probabilities 
estimated from a much larger POS-tagged but 
unlabeled Google N-grams corpus. 
We tested PONG on the eight most common 
relations in the WSJ corpus, and their inverses ? 
more relations than evaluated in prior work.  
Compared to the state-of-the-art EPP baseline 
(Erk et al 2010), PONG averaged higher 
precision but lower coverage and recall.  
Compared to the DEP baseline, PONG averaged 
lower precision but higher coverage and recall.  
All these differences were substantial (p < 0.001). 
Compared to both baselines, PONG?s average F-
score was higher, though not significantly. 
Some directions for future work include:  First, 
improve PONG by incorporating models of 
lexical similarity explored in prior work.  Second, 
use the universal tag set to extend PONG to other 
languages, or to perform better in English.  Third, 
in place of grammatical relations, use rich, 
diverse semantic roles, while avoiding sparsity.  
Finally, use selectional preferences to teach word 
connotations by using various relations to 
generate example sentences or useful questions. 
Acknowledgments 
The research reported here was supported by the 
Institute of Education Sciences, U.S. Department 
of Education, through Grant R305A080157.  The 
opinions expressed are those of the authors and 
do not necessarily represent the views of the 
Institute or the U.S. Department of Education.  
We thank the helpful reviewers and Katrin Erk 
for her generous assistance. 
References 
de Marneffe, M.-C. and Manning, C.D. 2008. 
Stanford Typed Dependencies Manual. 
http://nlp.stanford.edu/software/dependencies_man
ual.pdf, Stanford University, Stanford, CA. 
Erk, K. 2007. A Simple, Similarity-Based Model for 
Selectional Preferences. In Proceedings of the 45th 
Annual Meeting of the Association of 
Computational Linguistics, Prague, Czech 
Republic, June, 2007, 216-223. 
Erk, K., Pad?, S. and Pad?, U. 2010. A Flexible, 
Corpus-Driven Model of Regular and Inverse 
Selectional Preferences. Computational Linguistics 
36(4), 723-763. 
Fano, R. 1961. Transmission  O F   Information:  A  
Statistical  Theory  of  Communications. MIT 
Press, Cambridge, MA. 
Franz, A. and Brants, T. 2006. All Our N-Gram Are 
Belong to You. 
Gale, W.A., Church, K.W. and Yarowsky, D. 1992. 
Work on Statistical Methods for Word Sense 
Disambiguation. In Proceedings of the AAAI Fall 
Symposium on Probabilistic Approaches to Natural 
Language, Cambridge, MA, October 23?25, 1992, 
54-60. 
Gildea, D. and Jurafsky, D. 2002. Automatic Labeling 
of Semantic Roles. Computational Linguistics 
28(3), 245-288. 
Gormley, M.R., Dredze, M., Durme, B.V. and Eisner, 
J. 2011. Shared Components Topic Models with 
Application to Selectional Preference, NIPS 
Workshop on Learning Semantics Sierra Nevada, 
Spain. 
im Walde, S.S., Hying, C., Scheible, C. and Schmid, 
H. 2008. Combining Em Training and the Mdl 
Principle for an Automatic Verb Classification 
Incorporating Selectional Preferences. In 
Proceedings of the 46th Annual Meeting of the 
Association for Computational Linguistics, 
Columbus, OH,  2008, 496-504. 
Klein, D. and Manning, C.D. 2003. Accurate 
Unlexicalized Parsing. In Proceedings of the 41st 
Annual Meeting of the Association for 
Computational Linguistics, Sapporo, Japan, July 7-
12, 2003, E.W. HINRICHS and D. ROTH, Eds. 
Petrov, S., Das, D. and McDonald, R.T. 2011. A 
Universal Part-of-Speech Tagset. ArXiv 
1104.2086. 
Resnik, P. 1996. Selectional Constraints: An 
Information-Theoretic Model and Its 
Computational Realization. Cognition 61, 127-159. 
Resnik, P. 1997. Selectional Preference and Sense 
Disambiguation. In ACL SIGLEX Workshop on 
385
Tagging Text with Lexical Semantics: Why, What, 
and How, Washington, DC, April 4-5, 1997, 52-57. 
Ritter, A., Mausam and Etzioni, O. 2010. A Latent 
Dirichlet Allocation Method for Selectional 
Preferences. In Proceedings of the 48th Annual 
Meeting of the Association for Computational 
Linguistics, Uppsala, Sweden,  2010, 424-434. 
Rooth, M., Riezler, S., Prescher, D., Carroll, G. and 
Beil, F. 1999. Inducing a Semantically Annotated 
Lexicon Via Em-Based Clustering. In Proceedings 
of the 37th Annual Meeting of the Association for 
Computational Linguistics on Computational 
Linguistics, College Park, MD,  1999, Association 
for Computational Linguistics, 104-111. 
Schutze, H. 1992. Context Space. In Proceedings of 
the AAAI Fall Symposium on Intelligent 
Probabilistic Approaches to Natural Language, 
Cambridge, MA,  1992, 113-120. 
Toutanova, K., Klein, D., Manning, C. and Singer, Y. 
2003. Feature-Rich Part-of-Speech Tagging with a 
Cyclic Dependency Network. In Proceedings of the 
Human Language Technology Conference and 
Annual Meeting of the North American Chapter of 
the Association for Computational Linguistics 
(HLT-NAACL), Edmonton, Canada,  2003, 252?
259. 
Zhou, G., Zhao, J., Liu, K. and Cai, L. 2011. 
Exploiting Web-Derived Selectional Preference to 
Improve Statistical Dependency Parsing. In 
Proceedings of the 49th Annual Meeting of the 
Association for Computational Linguistics, 
Portland, OR,  2011, 1556?1565. 
 
 
386
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 836?842,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Extracting Events with Informal Temporal References in Personal
Histories in Online Communities
Miaomiao Wen, Zeyu Zheng, Hyeju Jang, Guang Xiang, Carolyn Penstein Rose?
Language Technologies Institute, Carnegie Mellon University
{mwen,zeyuz,hyejuj,guangx,cprose}@cs.cmu.edu
Abstract
We present a system for extracting the
dates of illness events (year and month of
the event occurrence) from posting histo-
ries in the context of an online medical
support community. A temporal tagger re-
trieves and normalizes dates mentioned in-
formally in social media to actual month
and year referents. Building on this, an
event date extraction system learns to in-
tegrate the likelihood of candidate dates
extracted from time-rich sentences with
temporal constraints extracted from event-
related sentences. Our integrated model
achieves 89.7% of the maximum perfor-
mance given the performance of the tem-
poral expression retrieval step.
1 Introduction
In this paper we present a challenging new event
date extraction task. Our technical contribution
is a temporal tagger that outperforms previously
published baseline approaches in its ability to
identify informal temporal expressions (TE) and
that normalizes each of them to an actual month
and year (Chang and Manning, 2012; Strotgen
and Gertz, 2010). This temporal tagger then con-
tributes towards high performance at matching
event mentions with the month and year in which
they occurred based on the complete posting his-
tory of users. It does so with high accuracy on
informal event mentions in social media by learn-
ing to integrate the likelihood of multiple candi-
date dates extracted from event mentions in time-
rich sentences with temporal constraints extracted
from event-related sentences.
Despite considerable prior work in temporal in-
formation extraction, to date state-of-the-art re-
sources are designed for extracting temporally
scoped facts about public figures/organizations
from newswire or Wikipedia articles (Ji et al,
2011; McClosky and Manning, 2012; Garrido et
[11/15/2008] I have noticed some pulling recently and I 
won't start rads until March.
[11/20/2008] It is sloowwwly healing, so slowly, in fact, 
that she said she HOPES it will be healed by March, when 
I am supposed to start rads.
[1/13/2009] I still have one last chemo to go on the 19th 
and then start rads in 5 wks.
[1/31/2009] I go for my first meeting with the rad onc on 
 2/10 (my 50th birthday!).
[2/23/2009] I had my first rad today.
[3/31/2009] Tomorrow will be my last full rads
[4/2/2009] I started rads in Feb, just did #29 today.
[4/8/2009] The rad onc wants to see me again next week 
for a skin check as I have had cellulitis twice since August.
[6/21/2010] My friend Lisa had her port put in last week 
and will begin 2 weeks of radiation on Tuesday.
Figure 1: User posts containing keywords for the
start of Radiation. Event keywords are in bold and
temporal expressions are in italics.
al., 2012). When people are instead communi-
cating informally about their lives, they refer to
time more informally and frequently from their
personal frame of reference rather than from an
impersonal third person frame of reference. For
example, they may use their own birthday as a
time reference. The proportion of relative (e.g.,
?last week?, ?two days from now?), or personal
time references in our data is more than one and a
half times as high as in newswire and Wikipedia.
Therefore, it is not surprising that there would be
difficulty in applying a temporal tagger designed
for newswire to social media data (Strotgen and
Gertz, 2012; Kolomiyets et al, 2011). Recent be-
havioral studies (Choudhury et al, 2013; Park and
Choi, 2012; Wen et al, 2012) demonstrate that
user-focused event mentions extracted from social
media data can provide a useful timeline-like tool
for studying how behavior patterns change over
time in response to mentioned events. Our re-
search contributes towards automating this work.
2 Task
Our task is to extract personal illness events men-
tioned in the posting histories of online commu-
nity participants. The input to our system is
836
a candidate event and a posting history. The
output is the event date (month and year) for
the event if it occurred, or ?unknown? if it
did not occur. The process iterates through a
list of 10 cancer events (CEs). This list in-
cludes breast cancer Diagnosis, Metastasis, Re-
currence, Mastectomy, Lumpectomy, Reconstruc-
tion, Chemotherapy-Start, Chemotherapy-End,
Radiation-Start and Radiation-End. For each of
these target CEs, we manually designed an event
keyword set that includes the name of the event,
abbreviations, slang, aliases and related words.
For each of the 10 events, all sentences that
mention a related event keyword are extracted
from the user?s posting history. Figure 1 shows
sevaral sentences that were extracted for one user
for the start date of Radiation. The task is to de-
termine that the beginning of this user?s Radiation
therapy was 2/2009. Note that the user began to
post about Radiation before she started it. She first
reported planning to start Radiation in March, but
then rescheduled for February. Most of the TEs
are non-standard and need to be resolved to calen-
dar dates (year and month).
Once the full set of event mention sentences has
been extracted for a user, all the temporal expres-
sions (TEs) that appear in the same sentence with
an event mention are resolved to a set of candi-
date dates. Besides a standard event-time classi-
fier for within-sentence event-time anchoring, we
leverage a new source of temporal information to
train a constraint-based event-time classifier. Pre-
vious work only retrieves time-rich sentences that
include both the query and some TEs (Ji et al,
2011; McClosky and Manning, 2012; Garrido et
al., 2012). However, sentences that contain only
the event mention but no explicit TE can also be
informative. For example, the post time (usually
referred to as document creation time or DCT) of
the sentence ?metastasis was found in my bone?
might be labeled as being after the ?metastasis?
event date. These DCTs impose constraints on
the possible event dates, which can be integrated
with the event-time classifier, as a variant on re-
lated work(Chambers, 2012).
3 Related Work
Previous work on TE extraction has focused
mainly on newswire text (Strotgen and Gertz,
2010; Chang and Manning, 2012). This paper
presents a rule-based TE extractor that identifies
and resolves a higher percentage of nonstandard
TEs than earlier state-of-art temporal taggers.
Our task is closest to the temporal slot filling
track in the TAC-KBP 2011 shared task (Ji et al,
2011) and timelining task (McClosky and Man-
ning, 2012). Their goal was to extract the tempo-
ral bounds of event relations. Our task has two key
differences. First, they used newswire, Wikipedia
and blogs as data sources from which they extract
temporal bounds of facts found in Wikipedia in-
foboxes. Second, in the KBP task, the set of gold
event relations are provided as input, so that the
task is only to identify a date for an event that is
guaranteed to have been mentioned. In our task,
we provide a set of potential events. However,
most of the candidate events won?t have ever been
reported within a user?s posting history.
Temporal constraints have proven to be use-
ful for producing a globally consistent timeline.
In most temporal relation bound extraction sys-
tems, the constraints are included as input rather
than learned by the system (Talukdar et al, 2012;
Wang et al, 2011). A notable exception is Mc-
Closkyet al (2012) who developed an approach to
learning constraints such as that people cannot at-
tend school if they have not been born yet. A no-
table characteristic of our task is that constraints
are softer. Diseases may occur in very different
ways across patients. Recurring illnesses falsely
appear to have an unpredictable order. Thus, there
can be no universal logical constraints on the order
of cancer events.
Our approach to using temporal constraints is a
variant on previously published approaches. Gar-
rido et al (2012) made use of DCT (document cre-
ation time) as well, however, they have assumed
the DCT is within the time-range of the event
stated in the document, which is often not true
in our data. Chambers (2012) utilized the within-
sentence time-DCT relation to learn constrains for
predicting DCT. We learn the event-DCT relations
to produce constrains for the event date.
4 Corpus Annotation
We have scraped the posts, users, and profiles from
a large online cancer support community. From
this collection we extracted and then annotated
two separate corpora, one for evaluating our TE
retrieval and normalization, the other one for event
date extraction.
For creating the TE extraction corpus, we ran-
837
domly picked one post from each of 1,000 ran-
domly selected users. We used this sampling tech-
nique because each user tends to use a narrow
range of date expression forms. From these posts,
we manually extracted 601 TEs and resolved them
to a specific month and year or just year if the
month was not mentioned. Events not reported
to have occurred were annotated as ?unknown?.
Our corpus for event date extraction consists of
the complete posting history of 300 users that were
randomly drawn from our dataset. Three annota-
tors were provided with guidelines for how to in-
fer the date of the events (Wen et al, 2013). We
achieved .94 Kappa on identification of whether an
event has a reported event date in a user?s history
or not. In evaluation of agreement on extracted
dates, we achieved a .99 Cronbach?s alpha. From
this corpus, 509 events were annotated with occur-
rence dates (year and month). In our evaluation,
we use data from 250 users for training, and 50 for
testing.
5 Method
Now we explain on a more technical level how our
system works on our task. Given an event and a
user?s post history, the system searches for all of
the sentences that contain an event keyword (key-
word sentence) and all the sentences that contain
both a keyword and a TE (date sentence). The TEs
in the date sentences are resolved and then used as
candidate dates for the event. For selecting among
candidate dates, our model integrates two main
components. First, the Date Classifier is trained
from date sentences to predict how likely its can-
didate TE and the gold event date are to overlap.
Then, because constraints over event dates can be
informed by temporal relations between the event
date and the DCT, the Constraint-based Classifier
provides an indication of the plausibility of can-
didate dates. The integrated system combines the
predictions from both classifiers.
5.1 Temporal Tagger
We design a rule-based temporal tagger that is
built using regular expression patterns to recog-
nize informal TEs. Similar to SUTime (Chang and
Manning, 2012), we identify and resolve a wide
range of non-standard TE types such as ?Feb ?07
(2/2007)?. The additional types of TE we han-
dle include: 1)user-specific TEs: A user?s age,
cancer anniversary and survivorship can provide
temporal information about the user?s CEs. We
obtain the birth date of users from their personal
profile to resolve age date expressions such as ?at
the age of 57?. 2)non-whole numbers such as ?a
year and half? and ?1/2 weeks?. 3)abbreviations
of time units : e.g. ?wk? as the abbreviation of
?week?. 4)underspecified month mentions, we
resolve the year information according to the DCT
month, the mentioned month and the verb tense.
5.2 Date Classifier
We train a MaxEnt classifier to predict the tem-
poral relationship between the retrieved TE and
the event date as overlap or no-overlap, similar
to the within-sentence event-time anchoring task
in TempEval-2 (UzZaman and Allen, 2010). Fea-
tures for the classifier include many of those in
(McClosky and Manning, 2012; Yoshikawa et al,
2009): namely, event keyword and its dominant
verb, verb and preposition that dominate TE, de-
pendency path between TE and keyword and its
length, unigram and bigram word and POS fea-
tures. New features include the Event-Subject,
Negative and Modality features. In online sup-
port groups, users not only tell stories about them-
selves, they also share other patients? stories (as
shown in Figure 1). So we add subject fea-
tures to remove this kind of noise, which in-
cludes the governing subject of the event key-
word and its POS tag. Modality features include
the appearance of modals before the event key-
word (e.g., may, might). Negative features include
the presence/absence of negative words (e.g., no,
never). These two features indicate a hypothetical
or counter-factual expression of the event.
To calculate the likelihood of a candidate date
for an event, we need to aggregate the hard de-
cisions from the classifier. Let DSu be the set
of the user?s date sentences, let Du be the set of
dates resolved from each TE. We represent a Max-
Ent classifier by Prelation(R|t, ds) for a candidate
date t in date sentence ds and possible relation
R = {overlap, no-overlap}. We map the distri-
bution over relations to a distribution over dates
by defining PDateSentence(t|DSu):
PDateSentence(t|DSu) = (1)
1
Z(Du)
?
tj?Du
?tj (t)Prelation(overlap|tj , dsj)
?tj (t) =
{
1 if t = tj
0 otherwise
838
We refer to this model as the Date Classifier.
5.3 Constraint-based Classifier
Previous work only retrieves time-rich sentences
(i.e., date sentences) (Ling and Weld, 2010; Ji et
al., 2011; McClosky and Manning, 2012; Garrido
et al, 2012). However, keyword sentences can in-
form temporal constraints for events and therefore
should not be ignored. For example, ?Well, I?m
officially a Radiation grad!? indicates the user has
done radiation by the time of the post (DCT). ?Ra-
diation is not a choice for me.? indicates the user
probably never had radiation. The topic of the
sentence can also indicate the temporal relation.
For example, before chemotherapy, the users tend
to talk about choices of drug combinations. After
chemotherapy, they talk about side-effects.
This section departs from the above Date Clas-
sifier and instead predicts whether each keyword
sentence is posted before or overlap-or-after the
user?s event date. The goal is to automatically
learn time constraints for the event. This task is
similar to the sentence event-DCT ordering task
in TempEval-2 (UzZaman and Allen, 2010). We
create training examples by computing the tempo-
ral relation between the DCT and the user?s gold
event date. If the user has not reported an event
date, the label should be unknown.
We train a MaxEnt classifier on each event
mention paired with its corresponding DCT. All
the features used in the classifier component that
are not related to the TEs are included. Let
KSu be the set of the user?s keyword sentences,
let Du be the set of dates resolved from each
date sentence. We define a MaxEnt classifier by
Prelation(R|ks) for a keyword sentence ks and
possible relation R = {before, overlap-or-after,
unknown}. DCT is the post time of the keyword
sentence ks. The rel(DCT, t) function simply de-
termines if the DCT is before or overlap-or-after
the candidate date t. We map this distribution over
relations to a distribution over dates by defining
PKeywordSentence(t,KSu):
PKeywordSentence(t,KSu) = (2)
1
Z(Du)
?
ksj?KSu
Prelation(rel(dctj , t)|ksj)
rel(dct, t) =
{
before if dct < t
overlap-or-after if dct ? t
5.4 Integrated Model
Given the Date Classifier of Section 5.2 and the
Constraint-based Classifier of Section 5.3, we cre-
ate a Integrated Model combining the two with the
following linear interpolation as follows:
P (t|postsu) = ?PDateSentence(t|DSu)
+ (1? ?)PKeywordSentence(t|KSu)
where t is a candidate event date. The system will
output t that maximizes P (t|postsu) and unknown
if DSu is empty. ? was set to 0.7 by maximizing
accuracy using five-fold cross-validation over the
training set.
6 Evaluation Metric and Results
6.1 Temporal Expression Retrieval
We compare our temporal tagger?s performance
with SUTime (Chang and Manning, 2012) on the
601 manually extracted TEs. We exclude user-
specific TEs such as birthday references since SU-
Time cannot handle those. We first evaluate iden-
tification of the extent of a TE and then production
of the correctly resolved date for each recognized
expression. Table 1 shows that our tagger has sig-
nificantly higher precision and recall for both.
P R F1
Extents SUTime 97.5 75.4 85.0
Our tagger 97.9 91.8 94.8
Normalization SUTime 89.4 71.2 79.3
Our tagger 91.3 85.5 88.3
Table 1: Temporal expression retrieval results
6.2 Event-date Extraction
6.2.1 Evaluation metric
The extracted date is only considered correct if it
completely matches the gold date. For less than
4% of users, we have multiple dates for the same
event (e.g., a user had a mastectomy twice). Sim-
ilar to the evaluation metric in a previous study(Ji
et al, 2011), in these cases, we give the system the
benefit of the doubt and the extracted date is con-
sidered correct if it matches one of the gold dates.
In previous work (McClosky and Manning, 2012;
Ji et al, 2011), the evaluation metric score is de-
fined as 1/((1 + |d|)) where d is the difference
between the values in years. We choose a much
stricter evaluation metric because we need a pre-
cise event date to study user behavior changes.
6.2.2 Baselines and oracle
Based on our temporal tagger, we provide two
baselines to describe heuristic methods of ag-
gregating the hard decisions from the classifier
839
Baseline1 Baseline2 Date Integrated Oracle
CE count P R F1 P R F1 P R F1 P R F1 F1
Diagnosis 112 .64 .70 .67 .60 .66 .63 .68 .75 .71 .68 .75 .71 .80
Metastasis 7 .16 .58 .25 .12 .43 .19 .25 .86 .39 .25 .86 .39 .86
Recurrence 14 .14 .35 .20 .11 .29 .16 .13 .36 .19 .13 .36 .19 .47
Chemo-start 54 .49 .61 .54 .42 .52 .46 .52 .66 .58 .58 .74 .65 .76
Chemo-end 43 .44 .59 .50 .36 .49 .42 .47 .63 .54 .48 .66 .56 .84
Rad-start 38 .35 .47 .40 .30 .40 .34 .36 .47 .41 .40 .53 .46 .64
Rad-end 35 .48 .63 .54 .30 .39 .34 .50 .66 .57 .50 .66 .57 .84
Mastectomy 68 .58 .71 .64 .52 .62 .57 .62 .76 .68 .62 .76 .68 .77
Lumpectomy 33 .49 .71 .58 .43 .76 .46 .46 .79 .58 .46 .79 .62 .91
Reconstruction 43 .38 .57 .46 .29 .44 .35 .41 .63 .50 .43 .65 .52 .86
Table 2: Event-level five-fold cross-validation performance of models and baselines on training data.
learned in Section 5.3. The first baseline, Base-
line1, is to pick the date with the highest clas-
sifier?s prediction confidence. The second base-
line, Baseline2, is along the same lines as the
Combined Classifier used in (McClosky and Man-
ning, 2012). For example, if the candidate
date is ?6/2009? and we have retrieved two TEs
that are resolved to ?6/2009? and ?4/2008?, then
P (?6/2009?) = Prelation(overlap|?6/2009?) ?
Prelation(no-overlap|?4/2008?).
To set an upper bound on performance given our
TE retrieval system, we calculate the oracle score
by considering an extraction as correct if the gold
date is one of the retrieved candidate dates. The
oracle score can differ from a perfect score since
we can only use candidate temporal expressions
if (a)the relation is known and (b)mentions of the
event are retrievable, (c)the TE and event keyword
appear in the same sentence, and (d)our temporal
tagger is able to recognize and resolve it correctly.
6.2.3 Results
We present the performance of our models, base-
lines and the oracle in Table 2. Both the Date Clas-
sifier and Integrated model significantly outper-
form the baselines (p < 0.0001, McNemar?s test,
2-tailed). This shows the value of our approach to
leveraging redundancy of event date mentions. In-
corporating time constraints further improves the
F1 of the Date Classifier by 3%. The Integrated
model achieves 89.7% of the oracle result.
Model P R F1
Baseline1 46.1 63.7 53.5
Baseline2 39.3 54.4 45.6
Date Classifier 49.6 67.7 57.3
Integrated Model 51.0 69.3 58.8
Oracle 77.3 77.3 77.3
Table 3: Performance of systems on the test set.
Table 3 shows the performance of our systems
and baselines on individual event types. The Joint
Model derives most of its improvement from per-
formance related to the Chemotherapy/Radiation-
start date. This is mainly because Chemotherapy
and Radiation last for a period of time and there
are more event-related discussions containing the
event keyword. None of our systems improves on
cancer Metastasis and Recurrence. This is likely
due to the sparsity of these events.
7 Conclusion
We presented a novel event date extraction task
that requires extraction and resolution of non-
standard TEs, namely personal illness event dates,
from the posting histories of online community
participants. We constructed an evaluation corpus
and designed a temporal tagger for non-standard
TEs in social media. Using a much stricter stan-
dard correctness measure than in previous work,
our method achieves promising results that are sig-
nificantly better than two types of baseline. By
creating an analogous keyword set, our event date
extraction method could be easily adapted to other
datasets.
8 Acknowledgments
We want to thank Dong Nguyen and Yi-chia
Wang, who helped provide the data for this
project. The research reported here was sup-
ported by National Science Foundation grant IIS-
0968485.
840
References
Javier Artiles, Qi Li, Taylor Cassidy, Suzanne
Tamang, and Heng Ji. 2011. CUNY BLENDER
TACKBP2011 Temporal Slot Filling System De-
scription. In Proceedings of Text Analysis Confer-
ence (TAC).
Nathanael Chambers, Shan Wang, and Dan Juraf-
sky. 2007. Classifying temporal relations between
events. In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics.
Nathanael Chambers. 2012. Labeling documents with
timestamps: Learning from their time expressions.
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics.
Angel X. Chang and Christopher D. Manning. 2012.
SUTIME: A library for recognizing and normaliz-
ing time expressions. In 8th International Confer-
ence on Language Resources and Evaluation(LREC
2012).
De Choudhury, M., Counts, S., and Horvitz, E. 2013.
Major Life Changes and Behavioral Markers in So-
cial Media: Case of Childbirth. In Proc. CSCW
2013.
Guillermo Garrido, Anselmo Penas, Bernardo Ca-
baleiro, and Alvaro Rodrigo. 2012. Temporally An-
chored Relation Extraction. In Proceedings of the
50th annual meeting of the as-sociation for compu-
tational linguistics.
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the TAC 2011 Knowledge Base Popu-
lation track. In Proceedings of Text Analysis Con-
ference (TAC).
Hyuckchul Jung, James Allen, Nate Blaylock, Will de
Beaumont, Lucian Galescu, and Mary Swift. 2011.
Building timelines from narrative clinical records:
initial results based-on deep natural language under-
standing. In Proceedings of BioNLP 2011.
Oleksandr Kolomiyets, Steven Bethard and Marie-
Francine Moens. 2011. Model-Portability Experi-
ments for Textual Temporal Analysis. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics.
Oleksandr Kolomiyets, Steven Bethard, and Marie-
Francine Moens. 2012. Extracting narrative time-
lines as temporal dependency structures. In Pro-
ceedings of the 50th annual meeting of the Associ-
ation for Computational Linguistics.
Xiao Ling and Daniel S Weld. 2010 Temporal infor-
mation extraction. Proceedings of the Twenty Fifth
National Conference on Artificial Intelligence.
David McClosky and Christopher D. Manning. 2012.
Learning Constraints for Consistent Timeline Ex-
traction. Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP2012).
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
47th annual meeting of the Association for Compu-
tational Linguistics.
Heekyong Park and Jinwook Choi 2012. V-model: a
new innovative model to chronologically visualize
narrative clinical texts. In Proceedings of the 2012
ACM annual conference on Human Factors in Com-
puting Systems. ACM.
Catherine Plaisant, Brett Milash, Anne Rose, Seth Wid-
off, and Ben Shneiderman. 1996. LifeLines: vi-
sualizing personal histories. In Proceedings of the
SIGCHI conference on Human factors in computing
systems.
James Pustejovsky, Jos M. Castao, Robert Ingria, Roser
Sauri, Robert J. Gaizauskas, Andrea Setzer, Graham
Katz, and Dragomir R. Radev. 2003. TimeML: Ro-
bust specification of event and temporal expressions
in text. TimeML: Robust specification of event and
temporal expressions in text. In New Directions in
Question Answering?03.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev. 2003. The Timebank corpus. In
Corpus Linguistics.
Preethi Raghavan, Eric Fosler-Lussier, and Albert M.
Lai. 2012. Learning to Temporally Order Medical
Events in Clinical Text. In Proceedings of the 50th
annual meeting of the Association for computational
Linguistics.
Jannik Strotgen and Michael Gertz. 2010. Heidel-
Time:High Quality Rule-Based Extraction and Nor-
malizationof Temporal Expressions. In SemEval
?10.
Jannik Strotgen and Michael Gertz. 2012. Temporal
Tagging on Different Domains: Challenges, Strate-
gies, and Gold Standards. In LREC2012.
Partha Pratim Talukdar, Derry Wijaya, and Tom
Mitchell. 2012. Coupled temporal scoping of re-
lational facts. In Proceedings of the fifth ACM inter-
national conference on Web search and data mining.
ACM.
Naushad UzZaman and James F. Allen. 2010. TRIPS
and TRIOS system for TempEval-2: Extracting tem-
poral information from text. In Proceedings of the
5th International Workshop on Semantic Evaluation.
Yafang Wang, Bing Yang, Lizhen Qu, Marc Spaniol,
and GerhardWeikum. 2011. Harvesting facts from
textual web sources by constrained label propaga-
tion. In Proceedings of the 20th ACM International
Conference on Information and Knowledge Man-
agement.
841
Miaomiao Wen, Hyeju Jang, and Carolyn Rose?. 2013.
Coding Manual for Illness Event Date Extraction.
Carnegie Mellon University, School of Computer
Science, Language Technology Institute.
K.-Y. Wen, F. McTavish, G. Kreps, M. Wise, and D.
Gustafson. 2012. From diagnosis to death: A
case study of coping with breast cancer as seen
through online discussion group messages. Jour-
nal of Computer-Mediated Communication, 16:331-
361.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki
Asahara, and Yuji Matsumoto. 2009. Jointly identi-
fying temporal relations with markov logic. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP.
Li Zhou and George Hripcsak. 2007. Temporal rea-
soning with medical data?a review with emphasis
on medical natural language processing. Journal of
biomedical informatics 40.2 (2007): 183.
842
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 136?146,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Generating Diagnostic Multiple Choice Comprehension Cloze Questions 
 
 
Jack Mostow and Hyeju Jang 
Project LISTEN (www.cs.cmu.edu/~listen) 
Carnegie Mellon University 
RI-NSH 4103, 5000 Forbes Avenue 
Pittsburgh, PA 15213-3890, USA 
mostow@cs.cmu.edu, hyejuj@cs.cmu.edu 
 
 
 
 
 
 
Abstract 
This paper describes and evaluates DQGen, 
which automatically generates multiple choice 
cloze questions to test a child?s comprehen-
sion while reading a given text.  Unlike previ-
ous methods, it generates different types of 
distracters designed to diagnose different 
types of comprehension failure, and tests 
comprehension not only of an individual sen-
tence but of the context that precedes it.  We 
evaluate the quality of the overall questions 
and the individual distracters, according to 8 
human judges blind to the correct answers and 
intended distracter types.  The results, errors, 
and judges? comments reveal limitations and 
suggest how to address some of them. 
1 Introduction 
This paper presents an automated method to check 
a reader?s comprehension of a given text while 
reading it, and to diagnose comprehension failures.  
In contrast to testing reading comprehension skill, 
for which there are published tests with well-
established psychometric properties (e.g., 
Wiederholt & Bryant, 1992; Woodcock, 1998), 
testing comprehension during reading of a given 
text requires generating a test for that specific text. 
A widely used solution is to replace some of the 
words with blanks for the student to fill, typically 
by selecting from multiple candidates.  Such mul-
tiple choice fill-in-the-blank questions are called 
cloze questions.  They are trivial to score because 
the correct answer is simply the original text word. 
Cloze questions test the ability to decide which 
word is consistent with the surrounding context.  
Thus it taps the comprehension processes that 
judge various types of consistency, such as syntac-
tic, semantic, and inter-sentential. 
In a nutshell, these processes successively en-
code sentences, integrate them into an overall rep-
resentation of meaning, notice gaps and 
inconsistencies, and repair them (see, e.g., Kintsch, 
1993, 2005; van den Broek, Everson, Virtue, Sung, 
& Tzeng, 2002).  The reader?s resulting situation 
model represents ?the content or microworld that 
the text is about? (Graesser & Bertus, 1998). 
In this paper, we introduce DQGen (Diagnostic 
Question Generator), a system that uses natural 
language processing to generate diagnostic cloze 
questions that check the comprehension of some-
one reading a given text.  DQGen differs from pre-
vious methods for generating cloze questions in 
that it is designed to minimize disruption to the 
reading process, and to diagnose different types of 
comprehension failure. 
The intended application context that motivated 
the development of DQGen is an automated read-
ing tutor that listens to children read aloud and 
helps them build their oral reading fluency 
(Mostow, 2008).  Periodic comprehension checks 
should deter children from reading as fast as they 
can and ignoring what the text means.  When the 
child answers incorrectly, the wrong answers 
should provide clues to why they are wrong. 
136
The rest of this article is organized as follows.  
Section 2 describes the generated questions.  Sec-
tion 3 describes how DQGen generates distracters.  
Section 4 reports a pilot evaluation of it.  Section 5 
analyzes errors.  Section 6 relates DQGen to prior 
work.  Section 7 concludes. 
2 Form of Generated Cloze Questions 
Generating cloze questions requires deciding:   
1. Which sentences to make cloze questions?   
2. Which words to delete from them? 
3. How many distracters to provide for them? 
4. What types of distracters? 
To illustrate the results of DQGen?s decisions, 
Figure 1 shows one of the better questions it gen-
erated: 
 
 
Figure 1.  An example of a generated question 
 
The four decisions enumerated above involve 
tradeoffs among preserving the flow of reading, 
encouraging comprehension, and assessing it accu-
rately.  As this example illustrates, DQGen inserts 
cloze questions as comprehension checks at the 
end of paragraphs, where there are natural breaks, 
in order to minimize disruption to the flow of read-
ing.  If the last sentence is shorter than four words 
or DQGen fails to find an acceptable distracter of 
each type, it simply leaves the last sentence un-
changed rather than turn it into a bad cloze ques-
tion. 
DQGen deletes the last word of the sentence, in 
order to allow normal reading up till that point and 
thereby minimize disruption to the flow of reading.  
Deleting a word earlier in the sentence would force 
the reader to skip the deleted word and read ahead 
to answer the cloze question.  Indeed, a review of  
of comprehension assessments (Pearson & Hamm, 
2005) indicates that end-of-sentence multiple 
choice cloze questions are widely used:  ?Delete 
words at the end of sentences and provide a set of 
choices from which examinees are to pick the best 
answer (this tack is employed in several standard-
ized tests, including the Stanford Diagnostic Read-
ing Test and the Degrees of Reading Power).? 
The number of distracters involves a tradeoff.  
On the one hand, the more distracters, the less 
chance of lucky guesses, and the more types of 
distracters possible.  On the other hand, offering 
more distracters lengthens the disruption to the 
flow of reading and raises the cognitive load on the 
reader to remember the paragraph when reading 
the distracters.  As a compromise, DQGen adds 
three distracters, for a total of four choices to pre-
sent in randomized order ? typical for multiple 
choice questions on educational tests for children. 
DQGen uses three types of distracters.  Each 
type of distracter indicates a different type of com-
prehension failure when chosen incorrectly by the 
reader as the answer.  By aggregating children?s 
performance over questions with these same three 
types of distracters, we hope not only to test their 
comprehension, but to profile the difficulties en-
countered by a given child or posed by a given text. 
2.1 Ungrammatical distracters 
The first and presumably easiest type of distracter 
renders the completed sentence ungrammatical.  
Syntactic processing is part of comprehension but 
not necessarily well-developed in children. Analy-
sis of children's responses to 69,000 multiple cloze 
questions automatically generated, presented, and 
scored by the Reading Tutor (Mostow et al, 2004) 
found that children?s performance decreased as the 
number of distracters with the same part of speech 
as the correct answer increased.  However, this 
effect was weaker for lower-level readers, indicat-
ing less sensitivity to syntax (Hensler & Beck, 
2006).  Choosing an ungrammatical distracter indi-
cates failure to detect a syntactic inconsistency.  
The ungrammatical distracter, e.g., are in Figure 1, 
has a different part of speech (POS) than the cor-
rect answer germs.  
2.2 Nonsensical distracters 
The second type of distracter makes the completed 
sentence grammatical but nonsensical.  Choosing a 
nonsensical distracter indicates failure to detect a 
local semantic inconsistency with the rest of the 
sentence.  The nonsensical distracter has the same  
Some of those cells patrol your body.  They are 
hungry, and they eat germs! Some stop the 
trouble germs make.  Others make antibodies.  
They stick to germs.  That helps your body find 
and kill _____  . 
a) are  
b) intestines  
c) terrorists  
d) germs 
 
137
   Ungrammatical Nonsensical Plausible 
Source of candidates 
Other words in 
paragraph 
List of words at 
grade level up to 4 
Matching Google N-grams 
Same as correct answer? No No No (94.96%) 
Related to words earlier in paragraph? ? ? No (lowest score) 
Related to words earlier in sentence? ? ? Yes (55.77%) 
Contains a space? ?No No (100%) ?No 
Frequent enough for children to know? ?Yes ?Yes Yes (96.15%) 
Passes grammar checker? No (65.48%) Yes (52.62%) Yes (92.31%)* 
Same POS as answer? ?No Yes (26.67%) ? 
Matches a Google N-gram? No (95.83%) No (91.67%) ?Yes 
 
Table 1.  Sources and constraints for each distracter type, in order tested (with % satisfied in pilot data).   
Constraints guaranteed to be satisfied or violated without explicit testing are marked ?Yes or ?No. 
* We added this test after the pilot evaluation because Google N-grams aren?t always grammatical.
POS as the correct answer, but plugging it into the 
sentence forms a context not found in the Google 
N-grams corpus. For example, the nonsensical dis-
tracter in Figure 1 is intestines.    
2.3 Plausible distracters 
The third and hardest type of distracter makes the 
completed sentence meaningful in isolation but 
inconsistent with the preceding global context.  
This type of distracter is essential in testing inter-
sentential processing, i.e. ?understanding that 
reaches across sentences in a passage,? because  
otherwise ?an individual's ability to fill in cloze 
blanks does not depend on passage context? ? a 
frequent criticism of cloze questions (Pearson & 
Hamm, 2005).  A plausible distracter has the same 
POS as the correct answer, like a nonsensical dis-
tracter, but the sentence it forms when plugged into 
the blank sounds reasonable ? in isolation.  That is, 
it ends with an N-gram that occurs in the Google 
N-grams corpus.   However, it doesn?t make sense 
in the context of the preceding sentences, because 
the distracter is unrelated to the words in the pre-
ceding sentences.  For example, terrorists in Fig-
ure 1 is a plausible distracter. 
3 Generating and Filtering Distracters 
DQGen uses generate-and-test to construct each 
type of distracter:  it chooses randomly from a 
source of candidates and backtracks if the chosen 
candidate violates a constraint on that type of dis-
tracter.  If none of the candidates that satisfy a con-
straint survive subsequent tests, DQGen drops the 
constraint and considers candidates that violate it. 
The source and constraints vary by distracter type 
(ungrammatical, nonsensical, plausible).  Table 1 
summarizes the tests and the order they are applied.  
Sections 3.1-3.3 discuss them in further detail.  
3.1 Lexical constraints on distracters 
Three constraints apply at the word level. 
No spaces: We constrain all three types of dis-
tracters to be words rather than phrases.  This con-
straint is guaranteed for paragraph words and 
Google N-grams, DQGen?s respective sources of 
ungrammatical and plausible distracters.  However, 
our source of nonsensical distracters is a table 
(Biemiller, 2009) that specifies the grade level not 
only of words but also of some phrases, such as 
barbeque sauce, which DQGen therefore filters out.  
Table 2 shows an excerpt from the table used. 
 
Word Meaning Level ? 
barbecue sauce flavored sauce for meat 2  
intestines guts  4  
intimate close, friendly 10  
intimate a close friend 10  
Table 2. Excerpt from Biemiller's (2009) table 
 
Distinct: DQGen explicitly excludes the correct 
answer as a distracter.  Other constraints on differ-
ent types of distracters are mutually exclusive with 
138
each other.  Consequently, no answer choice can 
appear twice. 
Familiar: Distracters must be familiar to chil-
dren.  DQGen satisfies this constraint for ungram-
matical and nonsensical distracters by choosing 
them from the paragraph and a grade-leveled word 
list (Biemiller, 2009), respectively.  These sources 
suffice to provide candidates, but they are not 
comprehensive enough to test candidates from an-
other source, such as the Google N-grams used to 
generate plausible distracters.  To exclude words 
likely to be unfamiliar to children, DQGen filters 
out candidates whose unigram frequency falls be-
low 5,000,000.  We tuned this threshold by infor-
mal trial and error; higher thresholds proved too 
stringent to allow any distracters from the limited 
source of candidate plausible distracters. 
3.2 Constraints on completed sentences  
Three constraints pertain to making completed sen-
tences sensible or not. 
Grammatical: As Table 1 shows, all three types 
of distracters involve grammaticality constraints.  
Ungrammatical distracters must make the complet-
ed sentence ungrammatical, e.g., That helps your 
body find and kill are.  In contrast, nonsensical and 
plausible distracters must make the completed sen-
tence grammatical, e.g., That helps your body find 
and kill terrorists.   
To check grammaticality of a completed sen-
tence, we use the Link Grammar Parser (Sleator & 
Temperley, 1993), a syntactic dependency parser, 
as a grammar checker.  As a grammar checker, the 
Link Grammar Parser usually accepts grammatical 
sentences and rejects ungrammatical ones, perhaps 
because sentences in children?s text tend to be 
short.  However, it sometimes fails to accept a 
grammatical sentence, as the last row of Table 3 
illustrates. 
 
sentence grammatically parser 
The germs hide in food or 
people 
correct accepted 
The germs hide in food or 
world 
incorrect rejected 
So keep dirty hands away 
from cuts and your face. 
correct rejected 
Table 3. Examples of grammar checking by parser 
 
Part of speech: More than one POS may make 
a distracter grammatical.  DQGen uses the Stan-
ford POS Tagger (Toutanova, Klein, Manning, & 
Singer, 2003) to tag the correct answer and a can-
didate nonsensical distracter when used to com-
plete the sentence, and requires them to have the 
same POS.  This test is superfluous for ungram-
matical distracters and unnecessary for plausible 
distracters. 
Google N-gram: As a heuristic test of whether a 
completed sentence is plausible, we check whether 
its ending occurs in the Google N-grams corpus 
(Brants & Franz, 2006), which means that it ap-
pears at least 40 times on the Web.  For ungram-
matical and nonsensical distracters, the last 4 
words of the completed sentence must not occur in 
this corpus.  For plausible distracters, the last 4 
words followed by ?.? must occur.  To enforce this 
constraint, DQGen?s source of candidate plausible 
distracters consists of Google 5-grams of the form 
W X Y __ .   Here W, X, and Y are the words pre-
ceding the correct answer in the original sentence, 
e.g., find and kill.  If there are fewer than 5 such 5-
grams, DQGen allows 4-grams of the form X Y __ ., 
e.g. and kill __. 
3.3 Relevance to context 
Two constraints on distracters concern context. 
Irrelevant to words earlier in paragraph: A 
plausible distracter should not be too plausible, so 
DQGen tries to ensure that it is unrelated to the 
earlier sentences and hence unlikely to make sense 
in context.  We measure the relatedness of a dis-
tracter to words in the earlier sentences by how 
often it co-occurs with them when used as in the 
last sentence.  DQGen therefore first pairs the can-
didate distracter, e.g. terrorists, with the last con-
tent word preceding the blank, e.g., kill in That 
helps your body find and kill ____.   It then esti-
mates the probability of these two words (kill and 
terrorists) co-occurring with the words in the earli-
er sentences of the paragraph, using a Na?ve Bayes 
formula to score their relevance to that context: 
1
Pr( , | ) Pr( , ) Pr( | , )
n
i
i
c k w c k w c k
=
? ??  
The formula omits Pr( )w
?
because it?s the same for 
all candidate plausible distracters for a given ques-
tion.  Here c is a candidate distracter (e.g., terror-
ists), k is the last content word before the blank 
(e.g., kill), w
?
is a vector of the n content words 
earlier in the paragraph, and wi is the i
th such word.   
139
 
Figure 2.  Prompt for the pilot user test 
 
DQGen scores Pr( | , )iw c k  based on how often 
word wi co-occurs with words c and k in the same 
30-word window in the British National Corpus 
(BNC). 
The purpose of a plausible distracter is to detect 
failures of intersentential comprehension processes 
that monitor global consistency.  As a heuristic to 
violate global consistency, DQGen picks distract-
ers with the lowest relevance scores. 
Relevant to words earlier in sentence: A plau-
sible distracter should be relevant to the words ear-
lier in the sentence.  To score local relevance, 
DQGen uses a Na?ve Bayes formula similar to its 
formula for global relevance: 
1
Pr( | ) Pr( ) Pr( | )
n
i
i
c w c w c
=
? ??  
Here, c is a candidate distracter, w
?
is a vector of 
the n content words earlier in the sentence, and wi 
is the ith such word.  DQGen estimates Pr( | )iw c  in 
the same way as before, but omits k because n is so 
much smaller for the sentence than for the para-
graph context preceding it.  DQGen averages these 
local coherence scores over the candidates, and 
allows only candidates whose local coherence 
scores are above the mean.   
4 Pilot Study 
How good are the generated questions? To evalu-
ate DQGen, we asked human judges to score them.  
Section 4.1 explains how we evaluated questions, 
Section 4.2 reports inter-rater reliability, and Sec-
tion 4.3 presents results. 
4.1 Methodology 
For the evaluation, we used DQGen to insert sam-
ple questions in an informational text for children, 
The Germs, which explains the concept of germs 
and their danger.  Of the 18 paragraphs in this text, 
we rejected one because it was only two sentences 
long, and DQGen rejected another because the last 
sentence failed the grammar checker.  For each of 
the other 16 paragraphs, DQGen generated a cloze 
question with ungrammatical and nonsensical dis-
tracters, but it found plausible distracters for only 
13 of the questions, which we evaluated as follows. 
We recruited eight human judges, members of 
our research team but unfamiliar with DQGen.  We 
asked them to evaluate each question at two levels, 
using the form illustrated in Figure 2.  
At the high level, we evaluated the overall quali-
ty of each question by asking judges to rate it as 
140
Good, OK, or Bad.  We report the percentage of 
generated questions rated by human judges as ac-
ceptable, defined as Good or OK.  We used a 3-
point scale rather than a finer-grained scale both to 
get higher inter-rater reliability, and because we 
were interested more in how many of the questions 
were acceptable than in precise ratings of quality.   
At the low level, we evaluated how often 
DQGen generated the intended type of distracter.  
We asked the judges to categorize each of the mul-
tiple choices (correct answer plus 3 distracters) as 
Ungrammatical, Nonsensical but grammatical, 
Meaningful but incorrect given the preceding text, 
or Correct.  To avoid biasing their responses, we 
did not tell them that each question was supposed 
to have one choice in each category. 
To elicit additional feedback, the form invited 
judges to comment on the questions and distracters. 
4.2 Inter-rater reliability 
It is important to measure inter-rater reliability 
among human judges, especially on experimenter-
designed measures such as the form we used. 
The overall quality ratings involved ranked data 
from more than two judges, so to measure their 
inter-rater reliability we used Kendall?s Coefficient 
of Concordance (Kendall & Smith, 1939).  KCC 
for overall quality was .40 on a scale from 0 to 1.  
This low value reflects the considerable variation 
between the judges, whose average ratings of over-
all quality ranged from 1.3 to 2.6. 
Categorization of each answer choice involved 
unranked data from more than two judges, so we 
used Fleiss? Kappa (Shrout & Fleiss, 1979) to 
measure its inter-rater reliability.  Kappa was .58;   
a value of .4-.6 is considered moderate, .6-.8 sub-
stantial, and .8-1 outstanding (Landis & Koch, 
1977).  Figure 3 shows the Kappa values for each 
label by the judges. 
 
 
Figure 3.  Fleiss's Kappa for inter-rater reliability 
of each type of choice 
The low values of inter-rater reliability measures 
revealed the raters? lack of consensus, presumably 
due to differing interpretations of the instructions.  
For instance, one judge commented that instruction 
for rating the overall quality did not indicate 
whether a good question requires reading the pre-
ceding text.  Another issue was missing and multi-
ple categorical responses.   
Evidently we need to specify our rating criteria 
more clearly, both for overall quality and for indi-
vidual components, especially nonsensical and 
plausible distracters.  A worked-out example might 
help judges understand each type better, but must 
avoid phrasing biased toward how DQGen works. 
4.3 Results 
We computed average ratings of overall quality 
and agreement with the intended category of each 
answer choice.   
We averaged all the ratings of overall quality af-
ter converting Bad, OK, and Good ratings into 1, 2, 
and 3, respectively.  Overall quality ratings aver-
aged 2.04, which corresponds to OK.  For agree-
ment of judges with the intended category of each 
answer choice, Cohen?s Kappa was .60.  Note that 
in contrast to Section 4.2, where we used Kappa to 
measure inter-rater reliability, i.e., how well the 
judges agreed with each other on overall question 
quality, here we use Kappa to measure distracter 
quality, i.e., how well the judges agreed with 
DQGen on the intended type of answer choices. 
Individual judges ranged from 63% to 79% 
agreement with the intended answer (Cohen's 
Kappa .51 to .72).  As Figure 4 shows, agreement 
was stronger for correct answers and ungrammati-
cal distracters than for nonsensical and plausible 
distracters.  On average, judges rated 94% of the 
correct answers as correct and agreed with 
DQGen?s intended distracter type for 91% of the 
ungrammatical distracters, 63% of the nonsensical 
distracters, and only 32% of the plausible distract-
ers.  Apparently correct answers are obviously 
right and ungrammatical answers are obviously 
wrong, but nonsensical and plausible distracters 
are harder to classify. 
5 Analysis of errors 
We now discuss issues revealed by errors and 
judges? comments, and how to address them. 
141
 Figure 4.  Cohen?s Kappa for agreement with the 
intended type of each choice 
5.1 Dependence on preceding text 
The judges? most frequent comment about the 
quality of a question was that answering it did not 
require reading the preceding text.  The judges rat-
ed only 32% of the intended plausible distracters as 
plausible.  Evidently we need to identify further 
constraints on plausible distracters.  We may also 
need to identify constraints on sentences where 
plausible distracters exist for the correct answer. 
5.2 Idioms 
Answer choices, whether correct answers or dis-
tracters, are problematic when they form idioms 
such as twisted in knots or make do.  For instance, 
one pilot cloze question ended with twisted in ____, 
where the correct answer was knots.  Another 
question ended with get your body to make ____, 
with do as a supposedly ungrammatical distracter. 
Idioms pose multiple problems, although we 
found only two cases in our small pilot study.  First, 
we want to test comprehension of the paragraph, 
not just knowledge of specific idioms.  Second, the 
word that completes an idiom can be far likelier 
than any other choice, making it too easy to guess 
based solely on local context, whether correct or 
not.  Third, because idioms have non-componential 
semantics, the missing word is liable to be seman-
tically unrelated to other sentence words, causing 
DQGen to badly underestimate its local relevance. 
Detecting idioms automatically is a research 
problem in its own right (Li, Roth, & Sporleder, 
2010; Li & Sporleder, 2009).  We might be able to 
recognize idioms by using the fact that its N-gram 
frequency is much higher than expected based on 
the frequency of its individual words.  A simpler 
approach is to consult a dictionary of common 
phrases.  Either approach would require extension 
to handle parameterized idioms such as a chip on 
[someone?s] shoulder, or non-contiguous forms 
such as Actions do in fact speak louder than words. 
5.3 Lexical issues for distracters 
The pilot study exposed a number of issues affect-
ing the suitability of words as distracters. 
Same-root words 
DQGen ensures that answer choices are distinct.  
However, one question included two forms of the 
same word as choices, namely throats as the cor-
rect answer and throat as a plausible distracter.  
We need to ensure that answer choices are not only 
distinct but dissimilar, unless we want questions 
that focus on minor differences between them. 
Common verbs and modal verbs 
One judge commented that we might want to avoid 
common verbs as distracters, such as any form of 
be, do, have, and get, and modal verbs, such as can, 
cannot, and will, lest children notice that they are 
seldom the correct answer, and therefore eliminate 
them without considering them.  Accordingly, we 
plan to filter out common verbs and modal verbs. 
Word difficulty 
The same judge considered some words too diffi-
cult for children, such as gauge and roast.  Actual-
ly, Biemiller (2009) rates noun senses of these 
words at grade 2, but the verb sense of gauge as 
estimate at grade 10.  These examples illustrate a 
limitation of DQGen?s methods to pick familiar 
words as distracters.  It picks ungrammatical dis-
tracters from the words in the paragraph, nonsensi-
cal distracters from Biemiller?s word list, and 
plausible distracters from Google N-grams, filtered 
by unigram frequency to avoid rare words.  In all 
three cases, DQGen constrains words rather than 
word senses. 
A more sophisticated approach would determine 
a distracter?s word sense, or at least POS, when 
used to complete the sentence, and rate the famili-
arity of its specific sense or POS.  Tagging the dis-
tracter POS is easier than determining its word 
sense(s) when inserted in the sentence.  Rating the 
familiarity of different word senses would require 
either a grade-leveled list of them like Biemiller?s 
(2009), or a resource with information about the 
frequency of different word senses or POS. 
142
6 Relation to Prior Work 
How does this research relate to previous work? 
There has been considerable research on automatic 
generation of multiple choice cloze questions to 
test vocabulary, grammar, and comprehension.  
Although these types of questions differ in purpose, 
they have much in common when it comes to gen-
erating them automatically.  
6.1 Vocabulary and grammar cloze questions 
A multiple choice cloze question to test vocabulary 
and grammar is constructed from a sentence se-
lected from a corpus by deleting part of it (typical-
ly the target vocabulary word) and selecting 
distracters for it. 
Selecting distracters with the same POS and ap-
proximate frequency as the answer word is a com-
mon strategy (Brown, Frishkoff, & Eskenazi, 
2005; Coniam, 1997; Liu, Wang, & Gao, 2005). 
Besides matching the correct answer?s POS and 
frequency, Liu et al (2005) added a culture-
dependent strategy for generating distracters:  
choose English words with semantically similar 
translations in the learner?s native language to the 
translation of the answer word. 
Correia et al (2010) generated vocabulary ques-
tions for Portuguese with three types of distracters.  
One type of distracter had the same POS and word 
level as the target word, based on its unigram fre-
quency in Portuguese textbooks used in different 
grades.  A second type had the lowest Levenshtein 
distance to the target out of all words with its POS.  
A third type was misspellings of the target word 
using a table of common spelling mistakes.  Al-
dabe et al (2007) also included students? common 
mistakes as candidate distracters.  
Some work also used semantic similarity 
between a distracter and the answer word to choose 
distracters.  Pino et al (2008) selected distracters 
that made the completed sentence grammatical and 
tended to co-occur with the words in the sentence, 
but were semantically distant from the target word 
as measured by WordNet.  In constrast, Smith et al 
(2008) looked for distracters semantically similar 
to the answer word based on distributional simi-
larity. In addition, Sumita et al (2005) used a the-
saurus for the same purpose, and then consulted 
the web to filter out plausible distracters.  
Aldabe et al (2009) considered context in a 
question sentence when choosing distracters.  They 
used an n-gram language model to predict the 
probability of occurrence of a distracter with its 
preceding words. 
Gates et al (2011) generated phrase-type dis-
tracters, unlike other work.  They generated ques-
tions from a dictionary definition of the target 
vocabulary word.  Rather than delete the target 
word, they parsed the definition, deleted a phrase 
from it, and chose distracters with the same syntac-
tic phrase type from definitions of other words, 
filtered to exclude synonyms of the target word. 
6.2 Comprehension cloze questions 
In contrast to vocabulary and grammar questions 
constructed from isolated sentences, DQGen?s 
comprehension questions are for (and inserted into) 
connected text. 
The most closely related work was by Mostow 
et al (2004).  Their Reading Tutor dynamically 
generated multiple choice cloze questions to test 
children?s comprehension of randomly chosen sen-
tences while reading a story.  It randomly chose an 
approximate level of difficulty (?sight?, ?easy?, 
?hard?, and ?defined?) for which word to delete 
from the sentence, and which words to choose ran-
domly from the same story as distracters. 
Goto et al (2010) also generated questions from 
texts.  They used a training corpus of existing cloze 
questions to learn how to select sentences to turn 
into cloze questions, words to delete, and types of 
distracters distinguished by their relation to the 
answer word:  inflectional (e.g., ask ? asked); der-
ivational (e.g., work ? worker); orthographic (e.g., 
circulation ? circumcision); and semantic (e.g., 
synonyms and antonyms). 
Aldabe et al (2010) generated questions for 
learners? assessment in the science domain.  To 
generate distracters, they measured semantic simi-
larity by using Latent Semantic Analysis (LSA) 
and additional information such as semantic rela-
tionships between words.  Experts discarded dis-
tracters that could form a correct answer. 
DQGen differs from prior work on generating 
cloze questions for vocabulary and comprehension 
in two key respects.  First, each question it gener-
ates has multiple types of distracters designed to 
detect different types of comprehension failure.  
Second, to generate plausible distracters it consid-
ers their relation not only to the clozed sentence 
but to the entire paragraph that contains it. 
143
7 Conclusion 
We conclude by summarizing contributions, limi-
tations, and future work. 
7.1 Contributions 
This paper describes a method for generating mul-
tiple choice cloze questions to test students? com-
prehension while reading.  Unlike previous 
methods, some of which also generate multiple 
types of distracters, DQGen?s distracter types are 
diagnostic.  It generates ungrammatical, nonsensi-
cal, and plausible distracters in order to detect fail-
ures of syntactic, semantic, and intersentential 
processing, respectively.  Unlike prior methods, 
which test comprehension only of individual sen-
tences, DQGen?s plausible distracters take their 
preceding context into account. 
We observed that candidate plausible distracters 
with high relevance scores tend to be surprisingly 
sensible answers ? even though the formula 
doesn?t ?know? the correct answer or even the un-
grammatical and nonsensical distracters.  That is, 
grammaticality, N-grams, and a simple relevance 
measure often suffice to produce intelligent an-
swers to a cloze question despite their shallow rep-
resentation of the meaning of the paragraph ? that 
is, without really understanding it.  This finding is 
surprising insofar as one would expect good per-
formance on such questions to require a deep rep-
resentation such as the situation model constructed 
by human readers. 
7.2 Limitations 
Besides describing DQGen?s design and imple-
mentation, we report on an evaluation of 13 gener-
ated questions by eight human judges blind to 
correct answer and intended distracter type.  On 
average they rated overall question quality OK, but 
with a wide range from the least to most favorable 
judge.  They agreed well with DGQen in classify-
ing answers as ungrammatical or correct, but not as 
nonsensical or plausible.  They criticized many 
questions as answerable without reading the text. 
7.3 Future work 
Our analysis of errors and judges? comments re-
vealed several limitations and suggested ways to 
address some of them.  In addition to identifying 
further constraints on plausible distracters, we need 
to identify constraints on good sentences to turn 
into end-of-paragraph cloze questions, beyond just 
the ability to generate a distracter of each type.  
One criterion is reliability:  how well does perfor-
mance on a question correlate with performance on 
other questions about the same text?  Another cri-
terion is informativeness:  what do wrong answers 
reveal about comprehension? 
Besides improving DQGen, we need to test it on 
more stories (both narrative fiction and informa-
tional text) and readers (especially children, our 
target population) to expose additional problems 
and avoid overfitting their solutions. 
One possible use of DQGen is machine-assisted 
generation of comprehension questions, or more 
precisely, human-assisted machine generation, for 
example with the human vetting or selecting 
among candidate questions generated automatical-
ly, thereby reducing the amount of human effort 
currently required to compose comprehension 
questions, and producing them more systematically. 
Success in getting DQGen to produce cloze 
questions on a large scale would have useful appli-
cations.  Periodic comprehension checks should 
deter children from reading as fast as they can and 
ignoring what the text means.  Diagnostic feedback 
based on incorrect answers should shed light on the 
nature of their comprehension failures and may be 
valuable as feedback to teachers or as guidance to 
the reading tutor. 
Another use for large numbers of automatically 
generated cloze questions is to develop methods to 
monitor reading comprehension unobtrusively.  
Student responses to cloze questions could provide 
automated labels for data collected while they read 
the preceding text.   Such data could include oral 
reading (Zhang, Mostow, & Beck, 2007) or even 
EEG (Mostow, Chang, & Nelson, 2011).  Models 
trained and tested on the labeled data could esti-
mate reading comprehension based on unlabeled 
data ? that is, without interrupting to ask questions. 
Acknowledgements 
The research reported here was supported by the Insti-
tute of Education Sciences, U.S. Department of Educa-
tion, through Grant R305A080157.  The opinions 
expressed are those of the authors and do not necessari-
ly represent the views of the Institute or the U.S. De-
partment of Education.  We thank our colleagues who 
judged the generated questions, and the reviewers for 
their helpful comments. 
144
References  
Aldabe, I., & Maritxalar, M. (2010). Automatic 
Distractor Generation for Domain Specific Texts 
Advances in Natural Language Processing. Paper 
presented at the The 7th International Conference on 
NLP, Reykjavk, Iceland. 
Aldabe, I., Maritxalar, M., & Martinez, E. (2007). 
Evaluating and Improving Distractor-Generating 
Heuristics. Paper presented at the The Workshop on 
NLP for Educational Resources. In conjuction with 
RANLP07. 
Aldabe, I., Maritxalar, M., & Mitkov, R. (2009). A 
Study on the Automatic Selection of Candidate 
Sentences and Distractors. Paper presented at the 
Proceedings of the 14th International Conference on 
Artificial Intelligence in Education (AIED2009), 
Brighton, UK. 
Biemiller, A. (2009). Words Worth Teaching:  Closing 
the Vocabulary Gap. Columbus, OH: SRA/McGraw-
Hill. 
Brants, T., & Franz, A. (2006). Web IT 5-gram Version 
1. Philadelpha: Linguistic Data Consortium. 
Brown, J. C., Frishkoff, G. A., & Eskenazi, M. (2005). 
Automatic Question Generation for Vocabulary 
Assessment. Paper presented at the Proceedings of 
Human Language Technology Conference and 
Conference on Empirical Methods in Natural 
Language Processing (HLT/EMNLP), Vancouver.  
Coniam, D. (1997). A preliminary inquiry into using 
corpus word frequency data in the automatic 
generation of English language cloze tests. CALICO 
Journal, 14(2-4), 15-33.  
Correia, R., Baptista, J., Mamede, N., Trancoso, I., & 
Eskenazi, M. (2010, September 22-24). Automatic 
generation of cloze question distractors. Paper 
presented at the Proceedings of the Interspeech 2010 
Satellite Workshop on Second Language Studies: 
Acquisition, Learning, Education and Technology, 
Waseda University, Tokyo, Japan. 
Gates, D., Aist, G., Mostow, J., Mckeown, M., & Bey, J. 
(2011, November 4-6). How to Generate Cloze 
Questions from Definitions: a Syntactic Approach. 
Paper presented at the Proceedings of the AAAI 
Symposium on Question Generation, Arlington, VA. 
Goto, T., Kojiri, T., Watanabe, T., Iwata, T., & Yamada, 
T. (2010). Automatic Generation System of Multiple-
Choice Cloze Questions and its Evaluation. 
Knowledge Management & E-Learning: An 
International Journal (KM&EL), 2(3).  
Graesser, A. C., & Bertus, E. L. (1998). The 
Construction of Causal Inferences While Reading 
Expository Texts on Science and Technology. 
Scientific Studies of Reading, 2(3), 247-269.  
Hensler, B. S., & Beck, J. (2006, June 26-30). Better 
student assessing by finding difficulty factors in a 
fully automated comprehension measure [Best Paper 
nominee]. Paper presented at the Proceedings of the 
8th International Conference on Intelligent Tutoring 
Systems, Jhongli, Taiwan. 
Kendall, M. G., & Smith, B. B. (1939). The Problem of 
m Rankings. The Annals of Mathematical Statistics, 
10(3), 275-287.  
Kintsch, W. (1993). Information Accretion and 
Reduction in Text Processing: Inferences. Discourse 
Processes, 16(1-2), 193-202.  
Kintsch, W. (2005). An Overview of Top-Down and 
Bottom-Up Effects in Comprehension: The CI 
Perspective. Discourse Processes A Multidisciplinary 
Journal, 39(2&3), 125-128.  
Landis, J. R., & Koch, G. G. (1977). The measurement 
of observer agreement for categorical data. 
Biometrics, 33(1), 159-174.  
Li, L., Roth, B., & Sporleder, C. (2010). Topic models 
for word sense disambiguation and token-based 
idiom detection. Paper presented at the Proceedings 
of the 48th Annual Meeting of the Association for 
Computational Linguistics, Uppsala, Sweden.  
Li, L., & Sporleder, C. (2009). Classifier combination 
for contextual idiom detection without labelled data. 
Paper presented at the Proceedings of the 2009 
Conference on Empirical Methods in Natural 
Language Processing, Singapore.  
Liu, C.-L., Wang, C.-H., & Gao, Z.-M. (2005). Using 
Lexical Constraints to Enhance the Quality of 
Computer-Generated Multiple-Choice Cloze Items. 
Computational Linguistics and Chinese Language 
Processing, 10(3), 303-328.  
Liu, C.-L., Wang, C.-H., Gao, Z.-M., & Huang, S.-M. 
(2005). Applications of lexical information for 
algorithmically composing multiple-choice cloze 
items. Paper presented at the Proceedings of the 
second workshop on Building Educational 
Applications Using NLP, Ann Arbor, Michigan.  
Mostow, J. (2008). Experience from a Reading Tutor 
that listens:  Evaluation purposes, excuses, and 
methods. In C. K. Kinzer & L. Verhoeven (Eds.), 
Interactive literacy education:  facilitating literacy 
environments through technology (pp. 117-148). New 
York: Lawrence Erlbaum Associates, Taylor & 
Francis Group. 
Mostow, J., Beck, J. E., Bey, J., Cuneo, A., Sison, J., 
Tobin, B., & Valeri, J. (2004). Using automated 
questions to assess reading comprehension, 
vocabulary, and effects of tutorial interventions. 
Technology, Instruction, Cognition and Learning, 
2(1-2), 97-134.  
Mostow, J., Chang, K.-m., & Nelson, J. (2011, June 28 - 
July 2). Toward Exploiting EEG Input in a Reading 
Tutor [Best Paper Nominee]. Paper presented at the 
Proceedings of the 15th International Conference on 
Artificial Intelligence in Education, Auckland, NZ. 
145
Pearson, P. D., & Hamm, D. N. (2005). The history of 
reading comprehension assessment. S. G. Paris & S. 
A. Stahl (Eds.), Children's reading comprehension 
and assessment, 13-69.  
Pino, J., Heilman, M., & Eskenazi, M. (2008). A 
selection strategy to improve cloze question quality. 
Paper presented at the Proceedings of the Workshop 
on Intelligent Tutoring Systems for Ill-Defined 
Domains. 9th International Conference on Intelligent 
Tutoring Systems, Montreal, Canada. 
Shrout, P. E., & Fleiss, J. L. (1979). Intraclass 
correlations: Uses in assessing rater reliability. 
Psychological Bulletin, 86(2), 420-428.  
Sleator, D. D. K., & Temperley, D. (1993, August 10-
13). Parsing English with a link grammar. Paper 
presented at the Third International Workshop on 
Parsing Technologies, Tilburg, NL, and Durbuy, 
Belgium. 
Smith, S., Sommers, S., & Kilgarriff, A. (2008). 
Learning words right with the Sketch Engine and 
WebBootCat: Automatic cloze generation from 
corpora and the web. Paper presented at the 
Proceedings of the 25th International Conference of 
English Teaching and Learning & 2008 International 
Conference on English Instruction  and Assessment, 
Lisbon, Portugal.  
Sumita, E., Sugaya, F., & Yamamoto, S. (2005). 
Measuring non-native speakers' proficiency of 
English by using a test with automatically-generated 
fill-in-the-blank questions. Paper presented at the 
Proceedings of the second workshop on Building 
Educational Applications Using NLP, Ann Arbor, 
Michigan.  
Toutanova, K., Klein, D., Manning, C., & Singer, Y. 
(2003). Feature-rich part-ofspeech tagging with a 
cyclic dependency network. Paper presented at the 
HLT-NAACL, Edmonton, Canada.  
van den Broek, P., Everson, M., Virtue, S., Sung, Y., & 
Tzeng, Y. (2002). Comprehension and memory of 
science texts: Inferential processes and the 
construction of a mental representation. In J. L. J. 
Otero, & A. C. Graesser (Ed.), The psychology of 
science text comprehension. Mahwah, NJ: Erlbaum. 
Wiederholt, J. L., & Bryant, B. R. (1992). Gray Oral 
Reading Tests (3rd ed.). Austin, TX: Pro-Ed. 
Woodcock, R. W. (1998). Woodcock Reading Mastery 
Tests - Revised (WRMT-R/NU). Circle Pines, 
Minnesota: American Guidance Service. 
Zhang, X., Mostow, J., & Beck, J. E. (2007, July 9-13). 
Can a computer listen for fluctuations in reading 
comprehension? Paper presented at the Proceedings 
of the 13th International Conference on Artificial 
Intelligence in Education, Marina del Rey, CA. 
 
 
146
Proceedings of the Second Workshop on Metaphor in NLP, pages 1?10,
Baltimore, MD, USA, 26 June 2014.
c?2014 Association for Computational Linguistics
Conversational Metaphors in Use: Exploring the Contrast between
Technical and Everyday Notions of Metaphor
Hyeju Jang, Mario Piergallini, Miaomiao Wen, and Carolyn Penstein Ros
?
e
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{hyejuj, mpiergal, mwen, cprose}@cs.cmu.edu
Abstract
Much computational work has been done
on identifying and interpreting the mean-
ing of metaphors, but little work has been
done on understanding the motivation be-
hind the use of metaphor. To computation-
ally model discourse and social position-
ing in metaphor, we need a corpus anno-
tated with metaphors relevant to speaker
intentions. This paper reports a corpus
study as a first step towards computa-
tional work on social and discourse func-
tions of metaphor. We use Amazon Me-
chanical Turk (MTurk) to annotate data
from three web discussion forums cover-
ing distinct domains. We then compare
these to annotations from our own anno-
tation scheme which distinguish levels of
metaphor with the labels: nonliteral, con-
ventionalized, and literal. Our hope is that
this work raises questions about what new
work needs to be done in order to address
the question of how metaphors are used to
achieve social goals in interaction.
1 Introduction
Our goal is to understand and characterize
the ways that nonliteral language, especially
metaphors, play a role in a variety of conversa-
tional strategies. In contrast to the large body
of work on uncovering the intended propositional
meaning behind metaphorical expressions, we are
most interested in the illocutionary and perlocu-
tionary force of the same contributions.
People use metaphorical expressions in a vari-
ety of ways in order to position themselves so-
cially and express attitudes, as well as to make
their point more effective, attractive, and convinc-
ing. Metaphors can be used to describe unfa-
miliar situations and feelings when the speaker
feels that literal description is inadequate. They
can also be used to display the speaker?s creativ-
ity and wit. They can further be used as a tac-
tic for persuasion or manipulation by foreground-
ing aspects that would not ordinarily be relevant.
Cameron (2007) shows that we can understand
social interactions and their contexts better by
closely looking at these patterns of metaphor use.
Metaphors can vary in how conventionalized
they are, from those which have lost their orig-
inal concrete meanings to completely novel and
vivid metaphors. Intuitively, it also makes sense
that metaphors which are more conventional and
less obviously metaphorical will be used with
less conscious thought than more novel or vivid
metaphors. There are thus reasons to suspect
that distinguishing between levels of metaphoric-
ity could give insight into patterns of use.
In this paper, we are interested in where we
can draw a line between levels of metaphoricity.
As a first step towards our long-term goal, we
present a corpus study in three web discussion
forums including a breast cancer support group,
a Massive Open Online Course (MOOC), and
a forum for street gang members, which cover
distinctly different domains and have differing
community structure. First, we investigate how
laypeople intuitively recognize metaphor by con-
ducting Amazon Mechanical Turk (MTurk) ex-
periments. Second, we introduce a new annota-
tion scheme for metaphorical expressions. In our
annotation scheme, we try to map the metaphor
spectrum of nonliteralness to three types of lan-
guage: nonliteral, conventionalized, and literal.
Our hope is that this distinction provides some
benefit in examining the social and discourse
functions of metaphor. Next, we compare MTurk
1
results with our annotations. Different people will
place the dividing line between literal language
and metaphorical language in different places. In
this work we have the opportunity to gauge how
much everyday conceptions of metaphoricity di-
verge from theoretical perspectives and therefore
how much models of metaphoricity may need to
be adapted in order to adequately characterize
metaphors in strategic use.
The paper is organized as follows. Section 2
relates our work to prior work on annotation and
a corpus study. Section 3 describes the data used
for annotation. Section 4 illustrates the functions
metaphor serves in discourse through a qualitative
analysis of our data. Section 5 explains our anno-
tation scheme. Section 6 presents our annotation
and MTurk experiments. Section 7 discusses the
results. Section 8 concludes the paper.
2 Relation to Prior Work
In this section, we introduce the two main bodies
of relevant prior work on metaphor in language
technologies: computational metaphor processing
and metaphor annotation.
2.1 Computational Work on Metaphor
Much of of the computational work on metaphor
can be classified into two tasks: automatic identi-
fication and interpretation of metaphors.
Metaphor identification has been done using
different approaches: violation of selectional pref-
erences (Fass, 1991), linguistic cues (Goatly,
1997), source and target domain words (Stefanow-
itsch and Gries, 2006), clustering (Birke and
Sarkar, 2006; Shutova et al., 2010), and lexi-
cal relations in WordNet (Krishnakumaran and
Zhu, 2007). Gedigian et al. (2006) and Li and
Sporleder (2010) distinguished the literal and non-
literal use of a target expression in text. In addi-
tion, Mason (2004) performed source-target do-
main mappings.
Metaphor interpretation is another large part
of the computational work on metaphor. Start-
ing with Martin (1990), a number of re-
searchers including Narayanan (1999), Barn-
den and Lee (2002), Agerri et al. (2007),
and Shutova (2010) have worked on the task.
Metaphor identification and interpretation was
performed simultaneously in (Shutova, 2013;
Shutova et al., 2013b).
As we have seen so far, much of the com-
putation work has focused on detecting and un-
covering the intended meaning behind metaphor-
ical expressions. On the other hand, Klebanov
and Flor (2013) paid attention to motivations be-
hind metaphor use, specifically metaphors used
for argumentation in essays. They showed a
moderate-to-strong correlation between percent-
age of metaphorically used words in an essay and
the writing quality score. We will introduce their
annotation protocol in Section 2.2.
However, to the best of our knowledge, not
much computational work has been done on
understanding the motivation behind the use
of metaphor besides that of Klebanov and
Flor (2013). Our work hopefully lays additional
foundation for the needed computational work.
2.2 Metaphor Annotation
One of the main challenges in computational work
on metaphor is the lack of annotated datasets. An-
notating metaphorical language is nontrivial be-
cause of a lack of consensus regarding annotation
schemes and clear definitions. In this section, we
introduce some work dedicated to metaphor anno-
tation and a corpus study.
Wallington et al. (2003) conducted experiments
to investigate what identifies metaphors. Two dif-
ferent teams annotated the same text with differ-
ent instructions, one asked to label ?interesting
stretches? and the other ?metaphorical stretches?.
They also asked annotators to tag words or phrases
that indicated a metaphor nearby, in order to inves-
tigate signals of metaphoricity.
Pragglejaz Group (2007) presented a metaphor
annotation scheme, called the Metaphor Identifi-
cation Procedure (MIP), which introduced a sys-
tematic approach with clear decision rules. In this
scheme, a word is considered to be metaphorical if
it is not used according to its most basic concrete
meaning, and if its contextual meaning can be un-
derstood in comparison with the most basic con-
crete meaning. This method is relatively straight-
forward and can give high inter-reliability. De-
pending on how one decides upon the basic mean-
ing of words, this scheme can be used for different
applications. However, defining the basic mean-
ing of a word is nontrivial, and following the def-
2
inition of basic meaning introduced in the paper
tends to result in a large proportion of words be-
ing annotated as metaphor. Many of the annotated
words would not be considered to be metaphors
by a layperson due to their long and widespread
usage.
Later works by Steen (2010), Shutova and
Teufel (2010), and Shutova et al. (2013a) ex-
panded upon MIP. Steen (2010) discussed the
strengths and weaknesses of MIP, and intro-
duced the Metaphor Identification Procedure VU
University Amsterdam (MIPVU). Shutova and
Teufel (2010) and and Shutova et al. (2013a)
added a procedure for identifying underlying con-
ceptual mappings between source and target do-
mains.
So far, these presented schemes do not distin-
guish between degrees of metaphoricity, and were
not specifically designed for considering moti-
vations behind metaphor use. Unlike the anno-
tation schemes described above, Klebanov and
Flor (2013) built a metaphor annotation proto-
col for metaphors relevant to arguments in essays.
They were interested in identifying metaphors that
stand out and are used to support the writer?s ar-
gument. Instead of giving a formal definition of
a literal sense, the annotators were instructed to
mark words they thought were used metaphori-
cally, and to write down the point being made
by the metaphor, given a general definition of
metaphor and examples. Our work is similar to
this work in that both corpus studies pay attention
to motivations behind metaphor use. However,
our work focuses on more conversational discus-
sion data whereas they focused on essays, which
are more well-formed.
3 Data
We conducted experiments using data from three
different web forums including a Massive Open
Online Course (MOOC), a breast cancer support
group (Breastcancer), and a forum for street gang
members (Gang). We randomly sampled 21 posts
(100 sentences) from MOOC, 8 posts (103 sen-
tences) from Breastcancer and 44 posts (111 sen-
tences) from Gang.
We chose these three forums because they all
offer conversational data and they all differ in
terms of the social situation. The forums dif-
fer significantly in purpose, demographics and
the participation trajectory of members. There-
fore, we expect that people will use language dif-
ferently in the three sets, especially related to
metaphorical expressions.
MOOC: This forum is used primarily for task-
based reasons rather than socializing. People par-
ticipate in the forum for a course, and leave when
the course ends. As a result, the forum does
not have continuity over time; participants do not
spend long time with the same people.
Breastcancer: People join this forum for both
task-based and social reasons: to receive informa-
tional and emotional support. People participate
in the forum after they are diagnosed with cancer,
and may leave the forum when they recover. This
forum is also used episodically by many users, but
a small percentage of users stay for long periods
of time (2 or more years). Thus, continuity al-
lows shared norms to develop over years centered
around an intense shared experience.
Gang: In this forum, members belong to a dis-
tinct subculture prior to joining, whereas Breast-
cancer and MOOC members have less shared
identity before entering the forum. This forum
is purely social. There is no clear endpoint for
participation; members leave the forum whenever
they are not interested in it any more. Users may
stay for a week or two, or for years.
4 Qualitative Analysis
Metaphors can be used for a number of conver-
sational purposes such as increasing or decreas-
ing social distance or as a tactic of persuasion or
manipulation (Ritchie, 2013). In this section, we
perform a qualitative analysis on how metaphor
functions in our data. We illustrate some exam-
ples from each domain with an analysis of how
some functions of social positioning are observed.
The choice of metaphor may reflect something
about the attitude of the speaker. For example,
journey is a metaphor frequently used in the breast
cancer support discussion forum
1
as seen in exam-
ples (2) ? (5) from the Breastcancer forum. Peo-
ple compare chemotherapy to a journey by using
metaphors such as journey, road and moves along.
A journey has a beginning and a goal one trav-
els towards, but people may take different paths.
1
http:breastcancer.org
3
This conveys the experience of cancer treatment
as a process of progressing along a path, strug-
gling and learning, but allows for each person?s
experience to differ without judgment of personal
success or failure (Reisfield and Wilson, 2004).
By contrast, another common metaphor compares
cancer treatment to battles and war. This metaphor
instead conveys an activity rather than passivity, a
struggle against a defined foe, which can be won
if one fights hard enough. But it also creates neg-
ative connotations for some patients, as forgoing
treatment could then be seen as equivalent to sur-
render (ibid.).
(1) Hello Ladies! I was supposed to
start chemo in January, ... I cant
start tx until that is done. So I will
be joining you on your journey this
month. I AM SICK OF the ANXI-
ETY and WAITING.
(2) So Ladies, please add another
member to this club. Looks like we
well all be leaning on each other.
But I promise to pick you up if you
fall if you can catch me once in a
while!
(3) The road seems long now but it re-
ally moves along fast.
(4) I split this journey into 4 stages and
I only deal with one.
In addition, using metaphors can have an ef-
fect of increasing empathetic understanding be-
tween the participants (Ritchie, 2013). We can
see this in examples (1) ? (4), where participants
in the same thread use similar metaphors relat-
ing chemotherapy to a journey. Reusing each
other?s metaphors reduces emotional distance and
helps to build empathic understanding and bond-
ing through a shared perception of their situations.
Metaphor also serves to suggest associations
between things that one would not normally asso-
ciate. Example (5) from the MOOC forum frames
participation in discussions as stepping into an
arena, which refers to an area for sports or com-
petition. By making such an analogy, it conveys
an environment of direct competition in front of a
large audience. It suggests that a student may be
afraid of contributing to discussion because they
may make a wrong statement or weak argument
and another person could counter their contribu-
tions, and they will be embarrassed in front of
their classmates.
(5) Hi, Vicki, great point ? I do wish
that teachers in my growing up
years had been better facilitators
of discussion that allowed EVERY-
one to practice adn become skill-
ful at speaking...I think in the early
years some of us need some hand-
holding in stepping into the arena
and speaking
Metaphors can also be used simply as a form of
wordplay, to display one?s wit and creativity. This
can be seen in the exchange in examples (6) ? (8),
from the Gang forum. A common metaphor used
on that forum is to refer to someone as food to
mean that they are weak and unthreatening. The
writer in (6) expands on this metaphor to suggest
that the other person is especially weak by calling
him dessert, while the writer in (7) then challenges
him to fight by exploiting the meaning of hungry
as ?having a desire for food?. The first writer (8)
then dismisses him as not worth the effort to fight,
as he does not eat vegetables.
(6) So If She Is Food That Must Make
U Desert
(7) if u hungry nigga why wait?
(8) I Dont Eat Vegatables.
5 Our Annotation Scheme
When we performed qualitative analysis as in Sec-
tion 4, we found that more noticeable metaphors
such as ?journey?, ?pick you up?, and ?fall? in (1)
and (2) seem more indicative of speaker attitude
or positioning than metaphors such as ?point? in
(5). This might suggest the degree of metaphoric-
ity affects how metaphors function in discourse.
In this section, we describe our metaphor anno-
tation scheme, which tries to map this variation
among metaphors to a simpler three-point scale of
nonliteralness: nonliteral, conventionalized, and
literal.
5.1 Basic Conditions
Our annotation scheme targets language satisfying
the following three conditions:
4
1. the expression needs to have an original es-
tablished meaning.
2. the expression needs to be used in context to
mean something significantly different from
that original meaning.
3. the difference in meaning should not
be hyperbole, understatement, sarcasm or
metonymy
These conditions result in metaphorical ex-
pressions including simile and metaphorical id-
ioms. We consider simile to be a special case of
metaphor which makes an explicit comparison us-
ing words such as ?like?. We include metaphor-
ical idioms because they are obviously nonliteral
and metaphorical despite the fact that they have
lost their source domains.
Have an original meaning: The expression or
the words within the expression need to have orig-
inal established meanings. For example, in the
sentence ?I will be joining you on your journey
this month? of (1) in Section 4, the word ?journey?
refers to chemotherapy given the context, but has
a clear and commonly known original meaning of
a physical journey from one place to another.
Alter the original and established meanings
of the words: The usage needs to change the orig-
inal meaning of the expression in some way. The
intended meaning should be understood through
a comparison to the original meaning. For the
same example, in ?I will be joining you on your
journey this month?, the intended meaning can be
understood through a comparison to some char-
acteristics of a long voyage. For metaphorical id-
ioms such as ?he kicked the bucket,? the nonliteral
meaning of ?he died? is far from the literal mean-
ing of ?he struck the bucket with his foot.?
Should not merely be hyperbole, understate-
ment, sarcasm, or metonymy: To reduce the
scope of our work, the usage needs to alter the
original meaning of the expression but should not
simply be a change in the intensity or the polar-
ity of the meaning, nor should it be metonymy.
Language uses like hyperbole and understatement
may simply change the intensity of the meaning
without otherwise altering it. For sarcasm, the
intended meaning is simply the negation of the
words used. Metonymy is a reference by asso-
ciation rather than a comparison. For example, in
?The White House denied the rumor?, the White
House stands in for the president because it is as-
sociated with him, rather than because it is being
compared to him. Note that metaphorical expres-
sions used in conjunction with these techniques
will still be coded as metaphor.
5.2 Decision Steps
To apply the basic conditions to the actual annota-
tion procedure, we come up with a set of decision
questions (Table 1). The questions rely on a va-
riety of other syntactic and semantic distinctions
serving as filtering questions. An annotator fol-
lows the questions in order after picking a phrase
or word in a sentence he or she thinks might be
nonliteral language. We describe some of our de-
cisions below.
Unit: The text annotators think might be non-
literal is considered for annotation. We allow a
word, a phrase, a clause, or a sentence as the
unit for annotation as in (Wallington et al., 2003).
We request that annotators include as few words
as necessary to cover each metaphorical phrase
within a sentence.
Category: We request that annotators code a
candidate unit as nonliteral, conventionalized, or
literal. We intend the nonliteral category to in-
clude nonliteral language usage within our scope,
namely metaphors, similes, and metaphorical id-
ioms. The conventionalized category is intended
to cover the cases where the nonliteralness of the
expression is unclear because of its extensive us-
age. The literal category is assigned to words that
are literal without any doubt.
Syntactic forms: We do not include prepo-
sitions or light verbs. We do not consider
phrases that consist of only function words such
as modals, auxiliaries, prepositions/particles or
infinitive markers. We restrict the candidate
metaphorical expressions to those which contain
content words.
Semantic forms: We do not include single
compound words, conventional terms of address,
greeting or parting phrases, or discourse markers
such as ?well?. We also do not include termi-
nology or jargon specific to the domain being an-
notated such as ?twilight sedation? in healthcare,
since this may be simply borrowing others? words.
5
No. Question Decision
1 Is the expression using the primary or most concrete meanings of the words? Yes = L
2 Does the expression include a light verb that can be omitted without changing
the meaning, as in ?I take a shower? ? ?I shower?? If so, the light verb
expression as a whole is literal.
Yes = L
3 Is the metaphor composed of a single compound word, like ?painkiller?, used
in its usual meaning?
Yes = L
4 Is the expression a conventional term of address, greeting, parting phrase or a
discourse marker?
Yes = L
5 Is the expression using terminology or jargon very common in this domain or
medium?
Yes = L
6 Is the expression merely hyperbole/understatement, sarcasm or metonymy? Yes = L
7 Is the expression a fixed idiom like ?kick the bucket? that could have a very
different concrete meaning?
Yes = N
8 Is the expression a simile, using ?like? or ?as? to make a comparison between
unlike things?
Yes = N
9 Is the expression unconventional/creative and also using non-concrete mean-
ings?
Yes = N
10 Is there another common way to say it that would convey all the same nuances
(emotional, etc.)? Or, is this expression one of the only conventional ways of
conveying that meaning?
If yes to
the latter
= C
11 If you cannot otherwise make a decision between literal and nonliteral, just
mark it as C.
Table 1: Questions to annotate (N: Nonliteral, C: Conventionalized, L: Literal).
6 Experiment
In this section, we present our comparative study
of the MTurk annotations and the annotations
based on our annotation scheme. The purpose
of this experiment is to explore (1) how laypeo-
ple perceive metaphor, (2) how valid the anno-
tations from crowdsourcing can be, and (3) how
metaphors are different in the three different do-
mains.
6.1 Experiment Setup
We had two annotators who were graduate stu-
dents with some linguistic knowledge. Both were
native speakers of English. The annotators were
asked to annotate the data using our annotation
scheme. We will call the annotators trained an-
notators from now on.
In addition, we used Amazon?s Mechanical
Turk (MTurk) crowdsourcing marketplace to col-
lect laypeople?s recognition of metaphors. We
employed MTurk workers to annotate each sen-
tence with the metaphorical expressions. Each
sentence was given along with the full post it came
from. MTurkers were instructed to copy and paste
all the metaphors appearing in the sentence to
given text boxes. They were given a simple def-
inition of metaphor from Wikipedia along with a
few examples to guide them. Each sentence was
labeled by seven different MTurk workers, and we
paid $0.05 for annotating each sentence. To con-
trol annotation quality, we required that all work-
ers have a United States location and have 98%
or more of their previous submissions accepted.
We monitored the annotation job and manually
filtered out annotators who submitted uniform or
seemingly random annotations.
6.2 Results
To evaluate the reliability of the annotations, we
used weighted Kappa (Cohen, 1968) at the word
level, excluding stop words. The weighted Kappa
value for annotations following our annotation
scheme was 0.52, and the percent agreement was
95.68%. To measure inter-reliability between two
annotators per class, we used Cohen?s Kappa (Co-
6
hen, 1960). Table 2 shows the Kappa values for
each dataset and each class. Table 4 shows the
corpus statistics.
Dataset N C N+C Weighted
all 0.44 0.20 0.49 0.52
breastcancer 0.69 0.20 0.63 0.71
Gang 0.26 0.28 0.39 0.34
MOOC 0.41 0.13 0.47 0.53
Table 2: Inter-reliability between two trained an-
notators for our annotation scheme.
To evaluate the reliability of the annotations by
MTurkers, we calculated Fleiss?s kappa (Fleiss,
1971). Fleiss?s kappa is appropriate for assessing
inter-reliability when different items are rated by
different judges. We measured the agreement at
the word level, excluding stop words as in com-
puting the agreement between trained annotators.
The annotation was 1 if the MTurker coded a word
as a metaphorical use, otherwise the annotation
was 0. The Kappa values are listed in Table 3.
Dataset Fleiss?s Kappa
all 0.36
breastcancer 0.41
Gang 0.35
MOOC 0.30
Table 3: Inter-reliability among MTurkers.
We also measured the agreement between the
annotations based on our scheme and MTurk an-
notations to see how they agree with each other.
First, we made a gold standard after discussing
the annotations of trained annotators. Then, to
combine the seven MTurk annotations, we give
a score for an expression 1 if the majority of
MTurkers coded it as metaphorically used, other-
wise the score is 0. Then, we computed Kappa
value between trained annotators and MTurkers.
The agreement between trained annotators and
MTurkers was 0.51 for N and 0.40 for N + C. We
can see the agreement between trained annotators
and MTurkers is not that bad especially for N.
Figure 1 shows the percentage of words labeled
as N, C or L according to the number of MTurk-
ers who annotated the word as metaphorical. As
seen, the more MTurkers who annotated a word,
Dataset N N+ C
all 0.51 0.40
breastcancer 0.64 0.47
Gang 0.36 0.39
MOOC 0.65 0.36
Table 5: Inter-reliability between trained annota-
tors and MTurkers.
the more likely it was to be annotated as N or C
by our trained annotators. The distinction between
Nonliteral and Conventionalized, however, is a bit
muddier, although it displays a moderate trend to-
wards more disagreement between MTurkers for
the Conventionalized category. The vast majority
of words (>90%) were considered to be literal, so
the sample size for comparing the N and C cate-
gories is small.
Figure 1: Correspondence between MTurkers and
trained annotators. X-axis: the number of MTuck-
ers annotating a word as metaphor.
7 Discussion
In this section, we investigate the disagreements
between annotators. A problem inherent to the an-
notation of metaphor is that the boundary between
literal and nonliteral language is fuzzy. Different
annotators may draw the line in different places
even when it comes to phrases they are all famil-
iar with. It is also true that each person will have
a different life history, and so some phrases which
are uninteresting to one person will be strikingly
metaphorical to another. For example, someone
who is unfamiliar with the internet will likely find
the phrase ?surf the web? quite metaphorical.
Since we did not predefine the words or phrases
that annotators could consider, there were often
cases where one person would annotate just the
7
Dataset Posts Sent. Words Content Words N C N/Sent. C/Sent.
MOOC 21 100 2005 982 23 59 0.23 0.59
Breastcancer 8 103 1598 797 27 41 0.26 0.4
Gang 44 111 1403 519 30 51 0.27 0.46
Table 4: Data statistics.
noun and another might include the entire noun
phrase. If it was part of a conventional multi-word
expression, MTurkers seemed likely to include the
entire collocation, not merely the metaphorical
part. Boundaries were an issue to a lesser extent
with our trained annotators.
One of our datasets, the Gang forum, uses a lot
of slang and non-standard grammar and spellings.
One of our trained annotators is quite familiar with
this forum and the other is not. This was the set
they had the most disagreement on. For exam-
ple, the one annotator did not recognize names of
certain gangs and rap musicians, and thought they
were meant metaphorically. Similarly, the MTurk-
ers had trouble with many of the slang expressions
in this data.
Another issue for the MTurkers is the distinc-
tion between metaphor and other forms of nonlit-
eral language such as metonymy and hyperbole.
For example, in the Gang data, the term ?ass? is
used to refer to a whole person. This is a type
metonymy (synecdoche) using a part to refer to
the whole. MTurkers were likely to label such
expressions as metaphor. Hyperbolic expressions
like ?never in a million years? were also marked
by some MTurkers.
In a few cases, the sentence may have required
more context to decipher, such as previous posts
in the same thread. Another minor issue was that
some data had words misspelled as other words or
grammatical errors, which some MTurkers anno-
tated as metaphors.
Certain categories of conventionalized
metaphors that would be annotated in the
original presentation of MIP (Pragglejaz-Group,
2007) were never or almost never annotated by
MTurkers. These included light verbs such as
?make? or ?get? when used as causatives or
the passive ?get?, verbs of sensation used for
cognitive meanings, such as ?see? meaning ?un-
derstand?, and demonstratives and prepositions in
themselves. This may indicate something about
the relevance of these types of metaphors for
certain applications.
8 Conclusion
We annotated data from three distinct conver-
sational online forums using both MTurks and
our annotation scheme. The comparison between
these two annotations revealed a few things. One
is that MTurkers did not show high agreement
among themselves, but showed acceptable agree-
ment with trained annotators for the N category.
Another is that domain-specific knowledge is im-
portant for accurate identification of metaphors.
Even trained annotators will have difficulty if they
are not familiar with the domain because they may
not even understand the meaning of the language
used.
Our annotation scheme has room for improve-
ment. For example, we need to distinguish be-
tween the Conventionalized and Nonliteral cate-
gories more clearly. We will refine the coding
scheme further as we work with more annotators.
We also think there may be methods of pro-
cessing MTurk annotations to improve their cor-
respondence with annotations based on our cod-
ing scheme. This could address issues such as in-
consistent phrase boundaries or distinguishing be-
tween metonymy and metaphor. This could make
it possible to use crowdsourcing to annotate the
larger amounts of data required for computational
applications in a reasonable amount of time.
Our research is in the beginning phase work-
ing towards the goal of computational modeling
of social and discourse uses of metaphor. Our next
steps in that direction will be to work on develop-
ing our annotated dataset and then begin to investi-
gate the differing contexts that metaphors are used
in. Our eventual goal is to be able to apply compu-
tational methods to interpret metaphor at the level
of social positioning and discourse functions.
8
Acknowledgments
This work was supported by NSF grant IIS-
1302522, and Army research lab grant W911NF-
11-2-0042.
References
Rodrigo Agerri, John Barnden, Mark Lee, and Alan
Wallington. 2007. Metaphor, inference and domain
independent mappings. In Proceedings of RANLP,
pages 17?23. Citeseer.
John A Barnden and Mark G Lee. 2002. An artifi-
cial intelligence approach to metaphor understand-
ing. Theoria et Historia Scientiarum, 6(1):399?412.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for nearly unsupervised recognition of non-
literal language. In EACL.
Lynne J Cameron. 2007. Patterns of metaphor
use in reconciliation talk. Discourse & Society,
18(2):197?222.
J. Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Mea-
surement, 20:37?46.
Jacob Cohen. 1968. Weighted kappa: Nominal scale
agreement provision for scaled disagreement or par-
tial credit. Psychological bulletin, 70(4):213.
Dan Fass. 1991. met*: A method for discriminating
metonymy and metaphor by computer. Computa-
tional Linguistics, 17(1):49?90.
Joseph L Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological bul-
letin, 76(5):378?382.
Matt Gedigian, John Bryant, Srini Narayanan, and Bra-
nimir Ciric. 2006. Catching metaphors. In Pro-
ceedings of the Third Workshop on Scalable Natu-
ral Language Understanding, pages 41?48. Associ-
ation for Computational Linguistics.
Andrew Goatly. 1997. Language of Metaphors: Lit-
eral Metaphorical. Routledge.
Beata Beigman Klebanov and Michael Flor. 2013.
Argumentation-relevant metaphors in test-taker es-
says. Meta4NLP 2013, pages 11?20.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Proceedings of the Workshop on Computational
approaches to Figurative Language, pages 13?20.
Association for Computational Linguistics.
Linlin Li and Caroline Sporleder. 2010. Using gaus-
sian mixture models to detect figurative language in
context. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 297?300, Stroudsburg, PA,
USA. Association for Computational Linguistics.
James H Martin. 1990. A computational model of
metaphor interpretation. Academic Press Profes-
sional, Inc.
Zachary J Mason. 2004. Cormet: a computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23?44.
Srinivas Narayanan. 1999. Moving right along: A
computational model of metaphoric reasoning about
events. In AAAI/IAAI, pages 121?127.
Pragglejaz-Group. 2007. Mip: A method for iden-
tifying metaphorically used words in discourse.
Metaphor and symbol, 22(1):1?39.
Gary M Reisfield and George R Wilson. 2004. Use
of metaphor in the discourse on cancer. Journal of
Clinical Oncology, 22(19):4024?4027.
SL. David Ritchie. 2013. Metaphor (Key Topics in
Semantics and Pragmatics). Cambridge university
press.
Ekaterina Shutova and Simone Teufel. 2010.
Metaphor corpus annotated for source-target do-
main mappings. In LREC.
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics,
pages 1002?1010. Association for Computational
Linguistics.
Ekaterina Shutova, BarryJ. Devereux, and Anna Ko-
rhonen. 2013a. Conceptual metaphor theory meets
the data: a corpus-based human annotation study.
Language Resources and Evaluation, 47(4):1261?
1284.
Ekaterina Shutova, Simone Teufel, and Anna Korho-
nen. 2013b. Statistical metaphor processing. Com-
putational Linguistics, 39(2):301?353.
Ekaterina Shutova. 2010. Automatic metaphor inter-
pretation as a paraphrasing task. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 1029?1037.
Association for Computational Linguistics.
Ekaterina Shutova. 2013. Metaphor identification as
interpretation. Atlanta, Georgia, USA, page 276.
9
Gerard J Steen, Aletta G Dorst, J Berenike Herrmann,
Anna Kaal, Tina Krennmayr, and Trijntje Pasma.
2010. A method for linguistic metaphor identifica-
tion: From MIP to MIPVU, volume 14. John Ben-
jamins Publishing.
Anatol Stefanowitsch and Stefan Th Gries. 2006.
Corpus-based approaches to metaphor and
metonymy, volume 171. Walter de Gruyter.
AM Wallington, JA Barnden, P Buchlovsky, L Fel-
lows, and SR Glasbey. 2003. Metaphor annota-
tion: A systematic study. COGNITIVE SCIENCE
RESEARCH PAPERS-UNIVERSITY OF BIRMING-
HAM CSRP.
10
