Disambiguating Noun Compounds with Latent Semantic Indexing
Alan M. Buckeridge and Richard F. E. Sutcliffe
Department of Computer Science and Information Systems,
University of Limerick, Limerick, Ireland
{Alan.Buckeridge, Richard.Sutcliffe}@ul.ie
Abstract
Technical terms in text often appear as noun
compounds, a frequently occurring yet highly
ambiguous construction whose interpretation
relies on extra-syntactic information. Several
statistical methods for disambiguating com-
pounds have been reported in the literature, of-
ten with quite impressive results. However, a
striking feature of all these approaches is that
they rely on the existence of previously seen un-
ambiguous compounds, meaning they are prone
to the problem of sparse data. This difficulty
has been overcome somewhat through the use
of hand-crafted knowledge resources to collect
statistics on ?concepts? rather than noun to-
kens, but domain-independence has been sacri-
ficed by doing so. We report here on work inves-
tigating the application of Latent Semantic In-
dexing to provide a robust domain-independent
source of the extra-syntactic knowledge neces-
sary for noun compound disambiguation.
1 Introduction
Noun compounds are a frequently encountered
construction in natural language processing
(NLP), consisting of a sequence of two or more
nouns which together function syntactically as a
noun. In English, compounds consisting of two
nouns are predominantly right-headed. How-
ever, compound construction is recursive and
both the modifier and the head can themselves
be compounds, resulting in structural ambigui-
ties. Consider the following pair of noun com-
pounds:
1. (a) (cantilever (swing wing))
(b) ((information retrieval) experiment)
Both compounds consist of the same parts-of-
speech, yet the structures differ: (1a) is right-
branching, while (1b) is left-branching.
Phrase structure grammar rules for noun
compounds are often similar in form to
N ? N N (Lauer, 1995). This rule is applied
once to two-word noun compounds, and recur-
sively in the case of longer compounds; therefore
the syntax of compounds longer than two words
is underconstrained by grammar, resulting in a
syntactic ambiguity which grows exponentially
with the length of the compound.
Besides causing problems for syntactic
parsers, the ambiguities inherent in these struc-
tures pose difficulties for NLP systems which
attempt to analyse the underlying semantic
relationships present in compound technical
terms. Often, the first step in such analyses is
to decompose terms into nested modifier-head
pairs (Barker, 1998). However, such a decom-
position is non-trivial for the case of compounds
consisting of three or more nouns due to the
structural ambiguity of these constructions.
The identification of modifier-head pairs in
compounds also has applications within the field
of information retrieval (IR). Several stud-
ies have shown that extracting modifier-head
pairs from text and including these as com-
pound indexing terms can improve recall and
precision (Evans and Zhai, 1996; Pohlmann and
Kraaij, 1997; Strzalkowski and Vauthey, 1992).
Identification of these noun modifier relation-
ships is also important for terminology transla-
tion. However, obtaining correct modifier-head
pairs is once again hampered by ?the notori-
ous ambiguity of nominal compounds? (Strza-
lkowski and Vauthey, 1992, p.107).
To summarise, the syntactic disambigua-
tion of noun compounds is important for sev-
eral NLP applications; however, disambiguation
is difficult because attachments within com-
pounds are not syntactically governed. Clearly,
then, this lack of syntactic constraints forces us
to consider the use of extra-syntactic factors in
the process of disambiguation. The work re-
ported here describes an approach for automat-
ically deriving a syntactical analysis of noun
compounds by adapting Latent Semantic Index-
ing, a well-established IR technique, to supply
this extra-syntactic information.
2 Previous Work
The majority of corpus statistical approaches
to compound disambiguation use a variation of
what Lauer (1995) refers to as the adjacency
algorithm. This algorithm was originally pro-
posed by Marcus (1980), and essentially op-
erates by comparing the acceptability of im-
mediately adjacent noun pairs. Specifically,
given a sequence of three nouns n1 n2 n3 , if
(n2 n3 ) is a more acceptable constituent than
(n1 n2 ), then build (n1 (n2 n3 )); else build
((n1 n2 ) n3 ).
There remains the question of how ?accept-
ability? is to be determined computationally.
Several researchers (e.g., (Barker, 1998; Evans
and Zhai, 1996; Pohlmann and Kraaij, 1997;
Pustejovsky et al, 1993; Strzalkowski and Vau-
they, 1992)) collect statistics on the occur-
rence frequency of structurally unambiguous
two-noun compounds to inform the analysis
of the ambiguous compound. For example,
given the compound ?computer data bases?, the
structure (computer (data bases)) would be pre-
ferred if (data bases) occurred more frequently
than (computer data) in the corpus. However,
by assuming that sufficient examples of sub-
components exist in the training corpus, all the
above approaches risk falling foul of the sparse
data problem. Most noun-noun compounds are
rare, and statistics based on such infrequent
events may lead to an unreliable estimation
of the acceptability of particular modifier-head
pairs.
The work of Resnik (1993) goes some way
towards alleviating this problem. Rather than
collecting statistics on individual words, he in-
stead counts co-occurrences of concepts (as rep-
resented by WordNet synsets). He uses these
statistics to derive a measure, motivated by
information theory, called selectional associa-
tion (see Resnik (1993) for full details). ?Ac-
ceptability? in the adjacency algorithm is then
measured in terms of the selectional association
between a modifier and head. Selectional as-
sociations were calculated by training on ap-
proximately 15,000 noun-noun compounds from
the Wall Street Journal corpus in the Penn
Treebank. Of a sample of 156 three-noun
compounds drawn from the corpus, Resnik?s
method achieved 72.6% disambiguation accu-
racy.
Lauer (1995) similarly generalises from indi-
vidual nouns to semantic classes or concepts;
however, his classes are derived from semantic
categories in Roget?s Thesaurus. Similar to the
approaches discussed above, Lauer extracts a
training set of approximately 35,000 unambigu-
ous noun-noun modifier-head compounds to es-
timate the degree of association between Ro-
get categories. He calls this measure concep-
tual association, and uses this to calculate the
acceptability of noun pairs for the disambigua-
tion of three-noun compounds. However, his ap-
proach differs from most others in that he does
not use the adjacency algorithm, instead using
a dependency algorithm which operates as fol-
lows: Given a three-noun compound n1 n2 n3 ,
if (n1 n3 ) is more acceptable than (n1 n2 ), then
build (n1 (n2 n3 )); else build ((n1 n2 ) n3 ).
Lauer tested both the dependency and ad-
jacency algorithms on a set of 244 three-noun
compounds extracted from Grolier?s Encyclope-
dia and found that the dependency algorithm
consistently outperformed the adjacency algo-
rithm, achieving a maximum of 81% accuracy
on the task. Overall, he found that estimating
the parameters of his probabilistic model based
on the distribution of concepts rather than that
of individual nouns resulted in superior perfor-
mance, thus providing further evidence of the
effectiveness of conceptual association in noun
compound disambiguation.
All these approaches rely on a variation of
finding subconstituents elsewhere in the corpus
and using these to decide how the longer, am-
biguous compounds are structured. However,
there is always the possibility that these sys-
tems might encounter modifier-head pairs in
testing which never occurred in training, forcing
the system to ?back off? to some default strat-
egy. This problem is alleviated somewhat in
the work of Resnik and Lauer where statistics
are collected on pairs of concepts rather than
pairs of noun tokens. However, the methods of
Resnik and Lauer both depend on hand-crafted
knowledge sources; the applicability of their ap-
proaches is therefore limited by the coverage of
these resources. Thus their methods would al-
most certainly perform less well when applied
to more technical domains where much of the
vocabulary used would not be available in ei-
ther WordNet or Roget?s Thesaurus. Knowl-
edge sources such as these would have to be
manually augmented each time the system was
ported to a new domain. Therefore, it would
be preferable to have a method of measuring
conceptual associations which is less domain-
dependent and which does not rely on the pres-
ence of unambiguous subconstituents in train-
ing; we investigated whether Latent Semantic
Indexing might satisfy these requirements.
3 Latent Semantic Indexing
Latent Semantic Indexing (LSI) is a variant of
the vector-space approach to information re-
trieval. It takes as input a collection of docu-
ments, from which it constructs an m?n word-
document matrix A; cell aij of the matrix de-
notes the frequency with which term i occurs
in document j. At the core of LSI is singu-
lar value decomposition (SVD), a mathematical
technique closely related to eigenvector decom-
position and factor analysis. SVD factors the
matrix A into the product of three matrices:
A = U?V T . U and V contain the left and
right singular vectors of A, respectively, while
? is a diagonal matrix containing the singular
values of A in descending order. By retaining
only the k largest singular values1 and setting
the remaining smaller ones to zero, a new diag-
onal matrix ?k is obtained; then the product of
U?kV T is the m ? n matrix Ak which is only
approximately equal to A. This truncated SVD
re-represents the word-document relationships
in A using only the axes of greatest variation,
in effect compressing and smoothing the data in
A. It is this compression step which is said to
capture important regularities in the patterns
of word co-occurrences while ignoring smaller
variations that may be due to idiosyncrasies in
the word usage of individual documents. The
result of condensing the matrix in this way is
that words which occur in similar documents
1The optimal value of k may only be determined em-
pirically, and will depend on the particular application.
will be represented by similar vectors, even if
these words never actually co-occur in the same
document. Thus it is claimed that LSI cap-
tures deeper associative relationships than mere
word-word co-occurrences. See Berry et al
(1995) and Deerwester et al (1990) for more
thorough discussions of SVD and its application
to information retrieval.
Because word vectors are originally based on
their distribution of occurrence across docu-
ments, each vector can be interpreted as a sum-
mary of a word?s contextual usage; words are
thus similar to the extent that they occur in
similar contexts. Of interest for our purposes is
the fact that a measure of the similarity or as-
sociation between pairs of words can be calcu-
lated geometrically, typically by computing the
cosine of the angle between word vectors. Any
two words, which may or may not occur ad-
jacently in text, can be compared in this way;
this frees us from the restriction of relying on
unambiguous subconstituents in training to in-
form the analysis of ambiguous compounds in
testing.
There is a growing body of literature indicat-
ing that distributional information of the kind
captured by LSI plays an important role in var-
ious aspects of human cognition. For the work
reported here, the most interesting aspect of
distributional information is its purported abil-
ity to model conceptual categorisation. Sev-
eral studies (Burgess and Lund, 1999; Laham,
1997; Landauer et al, 1998; Levy and Bulli-
naria, 2001) have shown that similarity between
concepts can be measured quite successfully us-
ing simple vectors of contextual usage; results
show that the performance of such systems cor-
relates well with that of humans on the same
tasks. These results are all the more impres-
sive when we consider that such systems use no
hand-coded semantic knowledge; the conceptual
representations are derived automatically from
training corpora.
Noun compound disambiguation appears to
be an NLP application for which such measures
of conceptual association would be useful. Both
the adjacency and dependency algorithms de-
scribed above in Section 2 rely on some mea-
sure of the ?acceptability? of pairs of nouns
to disambiguate noun compounds. Techniques
such as LSI offer a simple, robust, and domain-
Noun Compound Branching
((Ami Pro) document) Left
(volunteer (rescue workers)) Right
(tourist (exchange rates)) Right
((cluster analysis) procedure) Left
((data base) subcommittee) Left
(Windows (Control Panel)) Right
Table 1: Some example noun compounds taken
from our test set. Each row shows an example
of a manually bracketed compound, along with
its branching direction.
independent way in which concepts and the as-
sociations between them can be represented. In
the next section, we describe an experiment ex-
ploring the efficacy of LSI?s conceptual repre-
sentations in disambiguating noun compounds.
4 LSI and Noun Compound
Disambiguation
4.1 Method
4.1.1 Materials
We used four corpora in our study: The Lotus
Ami Pro Word Processor for Windows User?s
Guide Release 3, a software manual (AmiPro);
document abstracts in library science (CISI);
document abstracts on aeronautics (CRAN);
and articles from Time magazine (Time). We
first ran the LSI software on the corpora to cre-
ate the word-by-document matrices. The soft-
ware also subsequently performed singular value
decomposition on the resulting matrices. Stop-
words were not excluded, as previous experience
had shown that doing so degraded performance
slightly.
We used Brill?s (1994) tagger to identify
three-noun sequences in each of the corpora.
Tagging was imperfect, and sequences which
were not true three-noun compounds were dis-
carded. The remaining noun compounds were
bracketed manually and constituted test sets for
each corpus; some examples are shown in Ta-
ble 1. Table 2 summarises the datasets used in
our study.
4.1.2 Procedure
Both the adjacency and dependency models
were investigated (see Section 2). Recall that
the adjacency algorithm operates by comparing
the acceptability of the subcomponents (n1 n2 )
and (n2 n3 ), whereas the dependency algo-
rithm compares the acceptability of (n1 n2 ) and
(n1 n3 ). ?Acceptability? in our approach was
measured by calculating the cosine of the angle
between each pair of word vectors. The cosine
ranges from ?1.0 to 1.0; a higher cosine indi-
cated a stronger association between each word
in a pair. In the case of a tie, a left branching
analysis was preferred, as the literature suggests
that this is the more common structure (Lauer,
1995; Resnik, 1993). Thus a default strategy of
always guessing a left branching analysis served
as the baseline in this study. Each of the cor-
pora contained terms not covered by WordNet
or Roget?s; thus it was not possible to use the
techniques of Resnik (1993) and Lauer (1995)
as baselines.
As we could not tell beforehand what the op-
timal value of k would be (see Section 3 above),
we used a range of factor values. The values
used ranged from 2 to the total number of doc-
uments in each collection. For each factor value,
we obtained the percentage accuracy of both the
adjacency and dependency models.
4.2 Results and Discussion
The results of the experiment are summarised in
Table 3 and Figure 1. In most cases the perfor-
mance rises quickly as the number of SVD fac-
tors used increases, and then tends to level off.
The best performance was 84% for the AmiPro
collection, obtained using the adjacency algo-
rithm and 280 SVD factors. As the task in-
volved choosing the best binary bracketing for
a noun compound, we would expect an accu-
racy of 50% by chance. These results com-
pare favourably with those of Resnik (1993)
and Lauer (1995) (73% and 81%, respectively),
but as their studies were conducted on differ-
ent corpora, it would be imprudent to make di-
rect comparisons at this stage. Results for the
other collections were less impressive?however,
above-baseline performances were obtained in
each case.
Substantial differences in the performances of
the adjacency and dependency algorithms were
only observed for the AmiPro collection, sug-
gesting that the superior performance of the de-
pendency algorithm in Lauer?s (1995) study was
largely corpus-dependent. This is reinforced by
the considerably superior performance of the
Collection Name AmiPro CISI CRAN Time
Number of Documents 704 1,460 1,400 425
Number of Tokens 138,091 187,696 217,035 252,808
Mean Tokens per Type 46.3 18.7 26.2 11.5
Number of test compounds 307 235 223 214
Table 2: Characteristics of the datasets.
Name AmiPro CISI CRAN Time
Baseline 58% 63% 74% 48%
Adjacency 84% (280) 73% (800) 75% (700) 62% (370)
Dependency 70% (200) 70% (1100) 75% (600) 62% (240)
Table 3: Percentage disambiguation accuracy on each collection. The Baseline row shows the
accuracy of always choosing a left-branching analysis. Highest accuracies for the Adjacency and
Dependency algorithms are shown, with the corresponding number of SVD factors in parentheses.
adjacency algorithm on the AmiPro data set.
Another interesting finding was that there
were more right-branching (52%) than left-
branching (48%) compounds in the Time collec-
tion. This contrasts with previous studies which
discuss the predominance of left-branching com-
pounds, and suggests that the choice for the de-
fault branching must be corpus-dependent (see
Barker (1998) for similar findings).
There also appears to be a positive relation-
ship between performance and the token-type
ratio. The number of tokens per type in the
AmiPro collection was 46.3; the worst perfor-
mance was found for the Time collection, which
had only 11.5 tokens per type. There are at
least two possible explanations for this relation-
ship between performance and token-type ratio:
First, there were more samples of each word
type in the AmiPro collection?this may have
helped LSI construct vectors which were more
representative of each word?s contextual usage,
thus leading to the superior performance on the
AmiPro compounds.
Second, LSI constructs a single vector for
each token?if a particular token is polysemous
in text then its vector will be a ?noisy? amalga-
mation of its senses, a factor often contribut-
ing to poor performance. However, due to
the controlled language and vocabulary used in
the software manual domain, few if any of the
words in the AmiPro collection are used to con-
vey more than one sense; once again, this may
have resulted in ?cleaner?, more accurate vec-
tors leading to the superior disambiguation per-
formance on the AmiPro compounds.
These points lead us to the tentative sugges-
tion that our approach appears most suitable
for technical writing such as software manuals.
As usual, however, this is a matter for future
investigation.
5 Conclusions and Future Research
In this study, we extended LSI beyond its usual
remit by adopting it as a measure of conceptual
association for noun compound disambiguation.
The results reported here are encouraging, the
highest accuracy of 84% on the AmiPro collec-
tion indicating the potential of our approach.
However, poorer performance was obtained for
the other collections indicating that there is
much room for improvement. We therefore in-
tend to pursue our investigation of the utility
of applying vector-based measures of seman-
tic similarity to the problem of syntactic dis-
ambiguation. An attractive feature of this ap-
proach for the processing of terminology is that
it requires no manually constructed knowledge
sources, meaning that it does not suffer the
same coverage limitations as the methods of
Lauer (1995) and Resnik (1993). In principle,
our approach can be applied to any domain.
Another attractive feature is that it does not
rely on counts of unambiguous subconstituents
in training. This means that it can be applied to
novel compounds for which no subcompounds
exist in training, something which would not be
55
60
65
70
75
80
85
0 100 200 300 400 500 600 700 800
Ac
cu
ra
cy
 (%
)
No. of Factors
AmiPro
Adjacency
Dependency
Baseline
(a) AmiPro
45
50
55
60
65
70
75
0 200 400 600 800 1000 1200 1400 1600
Ac
cu
ra
cy
 (%
)
No. of Factors
CISI
Adjacency
Dependency
Baseline
(b) CISI
60
62
64
66
68
70
72
74
76
0 200 400 600 800 1000 1200 1400
Ac
cu
ra
cy
 (%
)
No. of Factors
CRAN
Adjacency
Dependency
Baseline
(c) CRAN
46
48
50
52
54
56
58
60
62
64
0 50 100 150 200 250 300 350 400 450
Ac
cu
ra
cy
 (%
)
No. of Factors
Time
Adjacency
Dependency
Baseline
(d) Time
Figure 1: Results of an experiment investigating noun compound disambiguation using LSI. Each
figure shows percentage disambiguation accuracy of the adjacency and dependency models for a
range of SVD factors. The percentage of left-branching compounds in each test set, which served
as the baseline in our study, is also shown for comparison.
possible for the statistical techniques outlined
in Section 2. Our next step will thus be to in-
vestigate the efficacy of our approach on novel
compounds.
We are currently examining the use of other
techniques for deriving vector-based measures
of conceptual association; preliminary investiga-
tions using a ?sliding window? method (Burgess
and Lund, 1999; Levy and Bullinaria, 2001) to
disambiguate compounds from the AmiPro cor-
pus show results even better than those reported
here. Present work involves setting various pa-
rameters (e.g., window size, similarity metric,
weighting method) to study their effect on per-
formance. We are continuing to test both the
adjacency and dependency algorithms on this
corpus, and have consistently found better per-
formance using the former.
Future work will involve continuing to test
the technique in other domains; we also intend
training on larger and more diverse corpora.
Furthermore, we plan to investigate other ex-
amples of syntactic ambiguity, such as preposi-
tional phrase attachment. Such structures pose
many problems for traditional NLP systems,
but may prove amenable to the techniques dis-
cussed in this paper.
6 Acknowledgements
Our thanks go to Pat Hickey of ECE, University
of Limerick, for technical assistance; and to two
anonymous reviewers for helpful comments.
References
K. Barker. 1998. A trainable bracketer for
noun modifiers. In Proceedings of the Twelfth
Canadian Conference on Artificial Intelli-
gence, pages 196?210, Vancouver.
M. W. Berry, S. T. Dumais, and T. A. Letsche.
1995. Computational methods for intelligent
information access. In Proceedings of Super-
computing ?95, San Diego, CA.
E. Brill. 1994. Some advances in
transformation-based part of speech tagging.
In Proceedings of the 12th National Confer-
ence on Artificial Intelligence (AAAI-94).
C. Burgess and K. Lund. 1999. The dy-
namics of meaning in memory. In E. Di-
etrich and A. Markman, editors, Cognitive
Dynamics: Conceptual and Representational
Change in Humans and Machines, pages 17?
56. Lawrence Erlbaum Associates Inc., Hills-
dale, NJ.
S. Deerwester, S. T. Dumais, G. W. Furnas,
T. K. Landauer, and R. Harshman. 1990. In-
dexing by latent semantic analysis. Journal
of the American Society for Information Sci-
ence, 41:391?407.
D. A. Evans and C. Zhai. 1996. Noun-phrase
analysis in unrestricted text for information
retrieval. In Proceedings of the 34th Annual
Meeting of the Association for Computational
Linguistics, pages 17?24, Santa-Cruz, CA,
June.
D. Laham. 1997. Latent Semantic Analysis ap-
proaches to categorization. In Proceedings of
the 19th Annual Conference of the Cognitive
Science Society, page 979. Erlbaum.
T. K. Landauer, P. W. Foltz, and D. Laham.
1998. Introduction to Latent Semantic Anal-
ysis. Discourse Processes, 25:259?284.
M. Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds.
Ph.D. thesis, Macquarie University, Sydney,
Australia.
J. P. Levy and J. A. Bullinaria. 2001. Learn-
ing lexical properties from word usage pat-
terns: Which context words should be used?
In Proceedings of the Sixth Neural Computa-
tion and Psychology Workshop, pages 273?
282. London: Springer.
M. Marcus. 1980. A Theory of Syntactic Recog-
nition for Natural Language. MIT Press,
Cambridge, MA.
R. Pohlmann and W. Kraaij. 1997. The effect
of syntactic phrase indexing on retrieval per-
formance for Dutch texts. In Proceedings of
RIAO ?97, pages 176?187, Montre?al, June.
J. Pustejovsky, S. Bergler, and P. Anick. 1993.
Lexical semantic techniques for corpus anal-
ysis. Computational Linguistics, 19(2):331?
358.
P. S. Resnik. 1993. Selection and Information:
A Class-Based Approach to Lexical Relation-
ships. Ph.D. thesis, University of Pennsylva-
nia.
T. Strzalkowski and B. Vauthey. 1992. In-
formation retrieval using robust natural lan-
guage processing. In Proceedings of the
30th Annual Meeting of the Association for
Computational Linguistics, pages 104?111,
Newark, Delaware, USA.
A Qualitative Comparison of Scientific and Journalistic Texts from the 
Perspective of Extracting Definitions 
Igal Gabbay 
Documents and Linguistic  
Technology Group 
Department of Computer Science 
University of Limerick 
Limerick, Ireland 
Igal.Gabbay@ul.ie 
Richard F. E. Sutcliffe 
Documents and Linguistic  
Technology Group 
Department of Computer Science 
University of Limerick 
Limerick, Ireland 
Richard.Sutcliffe@ul.ie 
 
Abstract 
In this paper we highlight a selection of 
features of scientific text which distinguish it 
from news stories. We argue that features such 
as structure, selective use of past tense, voice 
and stylistic conventions can affect question 
answering in the scientific domain. We 
demonstrate this through qualitative 
observations made while working on 
retrieving definitions to terms related to 
salmon fish. 
1 Introduction 
An information retrieval system informs on the 
existence (or non-existence) and whereabouts of 
documents relating to the request of a user 
(Lancaster, 1968). On the other hand, a question 
answering (QA) system allows a user to ask a 
question in natural language and receive a concise 
answer, possibly with a validating context 
(Hirschman and Gaizauskas, 2001). 
Questions asking about definitions of terms (i.e., 
?What is X??) occur frequently in the query logs of 
search engines (Voorhees, 2003). However, due to 
their complexity, recent work in the field of 
question answering has largely neglected them and 
concentrated instead on answering factoid 
questions for which the answer is a single word or 
short phrase (Blair-Goldensohn et al, 2003). Much 
of this work has been motivated by the question 
answering track of the Text REtrieval Conference 
(TREC), which evaluates systems by providing 
them with a common challenge.  
In a recent project inspired by our experiences in 
TREC (Sutcliffe et al, 2003), a system was built 
for extracting definitions of technical terms from 
scientific texts. The topic was salmon fish biology, 
a very different one from that of news articles. 
What, then, is the effect of domain on the 
applicability of QA? In this paper we attempt to 
answer this question, focusing on definitions and 
drawing on our findings from previous projects. 
 
The rest of the paper is structured as follows: 
First, we review recent related work. Second, we 
summarise the objectives, methods and findings of 
the SOK-I QA project, named after the sockeye 
salmon. Third, we compare the characteristics of 
scientific text with those of newspaper articles 
illustrating our points with examples from our 
SOK-I collection as well from the New York 
Times, CLEF 1994 Los Angeles Times collection 
and AQUAINT corpus. Fourth, we discuss the 
implications that these have for definitional QA. 
Finally, we draw conclusions from the study. 
2 Recent Related Work 
Zweigenbaum (2003) describes biomedicine as a 
specialised domain and argues that it is not 
necessarily simpler than an open domain as is 
sometimes assumed. He identifies the following 
characteristics: 
 
? A  highly specialised language for both queries 
and articles; 
? A potential difference in technical level 
between user questions and target documents; 
? A problem concerning the variable (and 
possibly unknown) reliability of source 
documents and hence that of answers drawn 
from them; 
? A potential for using a taxonomy of general 
clinical questions to route queries to 
appropriate knowledge resources.  
The gap in technical level between non-expert 
users and target documents is addressed by 
Klavans and Muresan (2001). Their system, 
DEFINDER, mines consumer-oriented full text 
medical articles for terms and their definitions.  
The usefulness and readability of the definitions 
retrieved by DEFINDER were both rated by non-
experts as being significantly higher than those of 
online dictionaries. However, Klavans and 
Muresan do not focus specifically on the 
characteristics of the source documents in their 
domain. 
The view of Teufel and Moens (2002) that 
summarization of scientific articles requires a 
different approach from the one used in 
summarization of news articles may perhaps apply 
to QA. The innovation of their work is in defining 
principles for content selection specifically for 
scientific articles. As an example they observe that 
information fusion (the comparison of results from 
different sources to eliminate mis-information and 
minimize the loss of data caused by unexpected 
phrasing) will be inefficient when summarizing 
scientific articles, because new ideas are usually 
the main focus of scientific writing, whereas in the 
news domain events are frequently repeated over a 
short time.  
The lack of redundancy as a feature of technical 
domains is also mentioned by Moll? et al (2003). 
They argue that because of this and the limited 
amount of text, data-intensive approaches, which 
are often used in TREC, do not work well in 
technical domains. Instead, intensive NLP 
techniques are required.  They also mention formal 
writing and the use of technical terms not defined 
in standard lexicons as additional features.  
3 Answering Definition Questions Related to 
Salmon (the SOK-I Project) 
Many of the observations in this paper are based 
on a recent study concerned with answering 
definition related to salmon (Gabbay, 2004). While 
a full treatment of the work falls outside the scope 
of this paper, we summarise  the key points here. 
The objectives of the project were: 
 
? To test the effectiveness of lexical patterns 
without deep linguistic knowledge in capturing 
definitions in scientific papers; 
? To discover simple features which indicate 
sentences containing definitions; 
? To study the stylistic characteristics of 
definitions retrieved from scientific text. 
We chose the terminology-rich field of salmon 
fish biology as the research domain. A collection 
of 1,000 scientific articles (Science Direct, 2003) 
matching the keyword ?salmon? was used as the 
source of definitions. Most of the documents were 
in agricultural and biological sciences. Each 
sentence in the articles was indexed as a separate 
document. 
A system was then developed which could take 
as input a term (e.g. ?smolt?) and carry out the 
following steps: 
 
1. Retrieve all sentences in the collection 
containing the term; 
2. Extract any portions of these which 
matched a collection of syntactic patterns. 
 
The patterns used were similar to the ones used 
by Hearst (1992), Joho and Sanderson (2000) and 
Liu et al (2003) to retrieve hyponyms from an 
encyclopedia, descriptive phrases from news 
articles and definitions from Web pages, 
respectively.   
To evaluate the system four test collections of 
terms were used: 42 terms which were suggested 
by salmon researchers, and three collections 
containing 3,920, 2,000 and 1,120 terms 
respectively. The latter were extracted from a 
database on the Web called FishBase (2003). For 
each collection, the output corresponding to a term 
was inspected manually and each phrase matching 
a pattern was judged to be either Vital, Okay, 
Uncertain or Wrong.  
While a complete discussion of the results and 
methods used to obtain them can be found in 
Gabbay (2004), the main quantitative finding of 
the project was that techniques adopted could 
achieve a Recall of up to 60%. 
Drawing from our experiences in SOK-I and 
TREC, we turn in the next section to some specific 
observations regarding differences between salmon 
biology texts and newspaper articles. 
4 Scientific and Journalistic Texts Compared 
4.1 Outline 
From our QA studies in the salmon biology field 
as well as experiences with news articles in TREC 
and CLEF, many interesting differences between 
these areas have come to light which we 
summarise here. The comparison is divided into 
six features: structure, tense, voice, references, 
terminology and style. 
4.2 Structure 
Scientific articles normally follow the structure 
known as IMRAD (Introduction, Methods, Results, 
and Discussion). This is the most common 
organisation of scientific papers that report original 
research (Day, 1998).  For example, the guidelines 
to authors submitting papers to the journal 
Aquaculture (Elsevier Author Guide, 2003) specify 
the following required sections: Abstract, 
Keywords, Introduction, Methods and Materials, 
Results, Discussion, Conclusion, Acknowledg-
ments and References.  
The structure of a news story is often described 
as an inverted pyramid, with the most essential 
information at the top (Wikipedia, 2004).  The 
most important element is called the lead and is 
comparable to the abstract of scientific articles but 
limited to one or two sentences (leads are often 
absent in longer feature articles).  
The introduction of a scientific paper on the 
other hand often begins with general statements 
about the significance of the topic and its history in 
the field; the ?news? is generally given later (Teufel 
and Moens, 2002). 
4.3 Tense 
In scientific writing it is customary to use past 
tense when reporting original work and present 
tense when describing established knowledge 
(Day, 1998). For example, the following sentence 
reports an accepted fact:  
 
?The idea behind using short-term temperature 
manipulations to mark juvenile fish otoliths is 
to alter the appearance of D- and L-zones in one 
or more increments to produce an obvious 
pattern of events.?  (SD-1) 
Contrast this with the sentence  
 
?Otoliths (sagittal otoliths) were taken from 
each fish in the total sample or a subsample of 
the total catch.?  (SD-2) 
 
which describes a technique used specifically in 
the reported study.  Therefore, it is reasonable to 
expect that verbs in the past tense will be 
concentrated in the Methods and Results sections. 
The past tense seems to dominate journalistic 
writing. In news reporting the past tense is 
considered slower, whereas the present tense is 
used for dramatic effect (Evans, 1972). The 
following excerpt gives a sense of urgency due to 
the use of the present progressive: 
 
?Pacific salmon contaminated by industrial 
pollutants in the ocean are carrying the chemicals 
to Alaska?s lakes?? (NYT-1) 
 
4.4 Voice 
The passive voice is a major stylistic feature of 
scientific discourse where according to Ding 
(1998) it represents the world in terms of objects, 
things and materials. Therefore, grammatical 
subjects are more likely to refer to inanimate 
objects  than to humans.   
Journalistic prose generally uses the active voice 
which is thought to assist in reading 
comprehension but also reflects the focus of news 
reporting on people and organizations (and indeed 
80% of the definition questions in TREC were 
about a person or an organisation). For example, 
compare the first two sentences of a report 
appearing in the Brief Communication section of 
the journal Nature to the lead of the same report as 
it was printed in popularized form in the New York 
Times:  
 
?Pollutants are widely distributed by the 
atmosphere and the oceans. Contaminants can 
also be transported by salmon and amplified 
through the food chain.?  (NAT) 
?Pacific salmon contaminated by industrial  
pollutants in the ocean are carrying the 
chemicals to Alaska?s lakes, where they may 
affect people and wildlife?? (NYT-1)  
In the first excerpt the subject is the 
contaminants being transported by the salmon 
(passive), whereas in the second the subject is the 
salmon carrying them (active). 
4.5  Citations 
Previously published work is cited frequently in 
scientific text using a consistent format such as the 
Harvard author-year citation style which is being 
used in this paper. Most of the citations are silent 
(i.e., both the name(s) and the date are enclosed in 
brackets) and often appear at the end of sentences.  
In the news domain, sources are often quoted 
directly. If the source is another publication, it is 
mentioned but rarely referenced in a detailed 
format with volume, issue, page numbers etc. 
For example, the author of the study which was 
published in Nature is quoted directly:  
 
??They die in such huge numbers that it almost 
looks like you can walk across the lakes?, an 
author of the of the study Dr. Jules Blais, said?. 
(NYT-1) 
 
People can also be quoted indirectly by reported 
speech as in the following example, 
 
 ?The salmon act as biological pumps, Dr. Blais 
said?? (NYT-1) 
 
 
 
 
4.6  Terminology 
Specialised terms abound in scientific writing 
and constitute a jargon. Such terms do not usually 
appear in news stories. For example, in the entire 
TREC AQUAINT collection the term ?smolt? 
appears eight times but more than 1,300 times in 
the SOK-I collection we created for our project. 
The term ?smoltification? which appears almost 
600 times in SOK-I is missing entirely from 
AQUAINT. 
Journalistic prose relies much less on jargon. 
Journalists tend to favour short common words 
over long infrequent ones.  Compare the 
vocabulary of Nature:  
 
?Here we show that groups of migrating 
sockeye salmon (Oncorhynchus nerka) can act 
as bulk-transport vectors of persistent industrial 
pollutants known as polychlorinated biphenyls 
(PCBs), which they assimilate from the ocean 
and then convey over vast distances back to 
their natal spawning lakes. After spawning, the 
fish die in their thousands - delivering their 
toxic cargo to the lake sediment and increasing 
its PCB content by more than sevenfold when 
the density of returning salmon is high.? (NAT) 
 to the same story in the New York Times: 
?After spending most of their lives in the ocean, 
where they absorb widespread industrial 
chemicals like PCB?s, sockeye salmon flock to 
Alaska?s interior lakes in huge numbers to 
spawn and then die. Each salmon accumulates 
just a small quantity of PCB?s. But when the 
fish die together in the thousands, their 
decaying carcasses produce a sevenfold 
increase in the PCB concentrations of the 
spawning lakes, the study found.? (NYT-1) 
Note, for example, that the abbreviation ?PCB? is 
never expanded in the New York Times report. 
Presumably, the precise chemical name is of little 
interest to the average reader of the Times, 
whereas in scientific text there is a need to avoid 
any technical ambiguity. The Nature report also 
uses the more technical terms ?vectors? ?assimilate? 
and ?sediment?.  
4.7  Style 
Apart from a particular citation style, which is a 
dominant feature of scientific text, entities such as 
species or chemical compounds are usually written 
according to standard nomenclature and format. 
For example, the common name of an animal 
species is normally followed by the binomial 
scientific name in italics and often bracketed: 
 
?Here we show that groups of migrating 
sockeye salmon (Oncorhynchus nerka) can 
act??   (NAT) 
News stories usually only use the common name of 
a species (e.g. sockeye salmon). 
In the next section we will see how such features 
affect definitional QA. 
 
5 Implications for Definitional QA 
5.1  Structure 
Blair-Goldensohn, McKeown and Schlaikjer 
(2003) and Joho and Sanderson (2000) who 
worked in the news domain observed that 
definitions are likely to be found nearer the 
beginning of the document than its end. They 
relied on relative and absolute sentence position as 
a feature indicating the presence of definitions. 
However, our observations suggest that at least in 
the SOK-I collection, sentence position (either 
relative or absolute) is not a good indicator of text 
containing definitions. This might be the result of 
the structured organisation of scientific papers, 
where each section is more self-contained than 
paragraphs are in news reports. We expected to 
find most of the definitions in the Introduction but 
other sections yielded many definitions. Early in 
the project we considered discarding the 
References section during the document pre-
processing stage but later discovered it can contain 
definitions  such as: 
 
?Canthaxanthin: a pigmenter for salmonids? 
(SD-3)  
However, definitions from different sections of 
the paper may differ in nature and style. For 
instance, definitions extracted from the Methods 
are more technical: 
  
?Dry matter eaten was defined as dry matter 
waste feed collected divided by recovery 
percentage, subtracted from the dry matter fed.? 
(SD-4)  
It is worth exploring whether certain types of 
terms are more likely to be defined in particular 
sections. A similar approach was suggested by 
Shah at al. (2003) for extracting keywords from 
full-text papers in genetics. 
 
5.2 Tense 
Since the present tense is often used to state 
established knowledge, we expected that lexical 
patterns in the present tense would be more likely 
to match definitions to terms. We observed that 
many of the wrong answers in our output  matched 
the past tense version of the copular pattern 
(TERM was/were DEFINITION). Sometimes, 
however, actions performed on or by the term can 
elucidate it. This is especially common in the 
Methods section of papers. For example, the term 
?Secchi disc? is defined in FishBase as: 
 
 ?A 20 cm diameter disc marked in 2 black and 
2 white opposing quadrants, lowered into the 
water. The average of the depth at which it 
disappears from sight and the depth at which it 
reappears when lowered and raised in the water 
column is the Secchi disc reading, a measure of 
transparency.? 
We retrieved the following answer which was 
judged as Okay:  
?Secchi disc was used to measure water 
visibility (m of visibility) at 1400h?? (SD-5)  
5.3  Voice 
Certain lexical patterns for definitions are in  
passive voice.  For example the pattern 
DEFINITION is termed TERM matched the 
following sentence in the SOK-I collection: 
 
?The best-known physical damage caused by 
aggression is inflicted on the fins and is termed 
fin damage, fin erosion or fin rot.? (SD-6) 
On the other hand definitions to technical terms 
in news stories are more likely to be attached to 
their definers?experts such as ?biologists? in the 
following example: 
 
?human illness from the virus will probably 
remain rare since humans are likely to remain 
what biologists call ``dead-end hosts': they can 
be infected, but their immune systems almost 
always prevent the virus from multiplying 
enough to be passed back to mosquitoes and 
then to other hosts.? (NYT-2) 
 
5.4 Citations 
One of the most common definition patterns is a 
term followed by its definition in brackets: 
 
?Grilse (fish maturing after 1.5 years in sea 
water)? (SD-7) 
In our first experiment we observed that the 
pattern falsely matched citations, and references to 
figures and tables as in the following case: 
?redd (Fleming, 1998)? (SD-8) 
These were eliminated by creating a list of 
stopwords which are typical to bracketed 
references (e.g., ?et al?, ?fig.?, years).  
 
Sometimes we encountered names of cited authors 
which matched a term to be defined or part of it 
(e.g. Fry, Fish). In the future these names need to 
be disambiguated.  
5.5 Terminology 
Definitions in scientific text are generally more 
technical and precise than in the news domain. For 
example, in SOK-I we matched the following 
definition of smolt: 
 
?In Atlantic salmon culture, smolt is usually 
defined as a juvenile salmon that is able to 
survive and grow normally in sea water.? (SD-
9) 
In a newspaper we may find ?smolts? defined as in 
the following sentence: 
?Young, six-inch-long first-year salmon, called 
by the old Anglo-Saxon name of smolts, 
migrate to two main oceanic feeding areas from 
their home streams in New England?? (NYT-
3) 
In the last definition the focus was on the word  
?smolt? which may be foreign to many newspaper 
readers. On the other hand, the readers of scientific 
papers on salmon biology are probably familiar 
with the term but may need to know its exact 
usage.   
Scientific names of species are taxonomically 
informative to biologists but would normally mean 
little to a non-expert. For instance, in  scientific 
text ?steelhead trout? would be followed by its 
scientific name Oncorhynchus mykiss which tells 
the informed reader it is a species of the same 
genus to which other pacific salmons belong. In a 
news articles, we found the following sentences: 
 
?But in this case, the endangered animal is the 
steelhead trout, a relative of the salmon?? 
(LA-1) 
?Copper River king salmon, magnificent sea 
beasts as big and fleshy as Chinese temple 
dogs, had been running?? (LA-2) 
Often definitions of species and other terms will 
just burden the readers of a newspaper and 
therefore are unnecessary. For example, unlike 
biologists, they do not require an exact definition 
of ?salt water? which specifies the concentration of 
salt or of ?colour? in the context of salmon meat 
quality. 
Sometimes definitions retrieved from scientific 
text were found to contain terms which would have 
to be defined in a news article. For example 
?smolt? can be defined in terms of degree days?
the product of the daily water temperature, 
multiplied by the number of days it takes the 
salmon to reach the smolt stage. 
Even though the papers in the SOK-I collection 
seemed to target a homogenous audience, it was 
possible to find definitions which are suitable for 
different levels of expertise. For instance, the 
system retrieved the  chemical name   
 
?(3,3'-dihydroxy-,-carotene-4,4'-dione)? (SD-
10) 
 in response to the query ?astaxanthin?. Such an 
answer, although incomplete, could satisfy an 
expert in biochemistry. Another answer was:  
 
?Astaxanthin is an approved colour additive in 
the feed of salmonids? (SD-11) 
 
The first definition was found in a biochemistry 
paper on the digestability and accumulation of 
astaxanthin, whereas the second one was extracted 
from a fishery research paper which discusses 
potential issues for human health and safety from 
net-pen salmon farming. The readers of the second 
paper may be experts on fish biology but not 
necessarily on chemicals, food safety or even 
salmon farming, whereas the first paper is more 
limited to a single discipline.  
5.6  Style 
The standardised forms of species and chemical 
names in scientific text lend themselves to 
information extraction techniques which would not 
be effective in the news domain. Templates could 
be created for certain categories of biological 
terms. For example, for the category Species we 
can fill the slots for the scientific name, taxonomic 
family or order, distribution, life cycle, synonym, 
and threats to the species; In our experiments the 
pattern TERM (DEFINTION) was effective in 
recognising the scientific name when the query 
term was the common name of a species.  
 
6 Conclusions 
In this paper, we demonstrated how scientific and 
journalistic texts differ in structure, tense, voice, 
references, terminology and style. Our 
observations are based on a project in which we 
retrieved definitions to terms in the salmon fish 
domain. The above features could be exploited 
specifically in scientific QA. Features such as 
voice may play a more significant role in QA 
systems which employ deeper NLP techniques 
than the simple patterns we used. The uniform 
structure of scientific documents may allow us to 
typify definitions in each section before combining 
them to suit the need of users. Further analysis of 
the news domain may perhaps yield more 
observations which will also contribute to current 
mainstream open-domain QA research as seen in 
TREC and CLEF. 
  
7.  Sources of Cited Examples 
LA-1: CLEF 03 LA Times LA120894-0019 
LA-2: CLEF 03 LA Times LA070794-0021 
NAT: Nature, 425(6955), 255. 
NYT-1: 
http://www.nytimes.com/2003/09/23/science/ 
23SALM.html?ex=1079499600&en=1083ff4683d
95e8e&ei=5070 
NYT-2: AQUAINT NYT20000807.0291 
NYT-3: AQUAINT NYT19990913.0215 
SD-1: 
http://www.sciencedirect.com/science/article/B6T6
N-3XNJYSC-
P/2/704eaa76fae2ceb6b79ec11d844a44dd 
SD-2:  
http://www.sciencedirect.com/science/article/B6T6
N-409630G-
V/2/9bc703e5948960159743f99269998fb6 
SD-3:  
http://www.sciencedirect.com/science/article/B6T4
D-428FK2P-
C/2/d9e2c93377b34beb5ecc47165a4b1098 
SD-4: 
http://www.sciencedirect.com/science/article/B6T4
D-460WH4M-
1/2/fba8df4de8a057a606cc582a60046c09 
SD-5: 
http://www.sciencedirect.com/science/article/B6T4
C-43X1B91-
4/2/b7b7058db2c954eaf9d72b7bf2b5d141 
SD-6: 
http://www.sciencedirect.com/science/article/B6T4
D-3YXJYY2-
9/2/e3ddd1ccfbbece503a0ae26304a4b443 
 
 
SD-7: 
http://www.sciencedirect.com/science/article/B6T4
D-3WN6GV4-
/2/13a6aa37050ce68845d85c8eb111a82b 
SD-8:  
http://www.sciencedirect.com/science/article/B6T6
N-472BJBX-
4/2/054a7ff897821495aed30fe697c3b1c7 
SD-9: 
http://www.sciencedirect.com/science/article/B6T4
D-40FG8N8-
G/2/459f344b039746dc9dff2a3ca1f17679 
SD-10: 
http://www.sciencedirect.com/science/article/B6T2
R-41JM957-
K/2/5317d1c1daefee0ddadbc86a31288eb2 
SD-11: 
http://www.sciencedirect.com/science/article/B6T6
N-4846K7G-
2/2/1c8d6922218aabc83ad653b376d39ed9 
 
References  
Blair-Goldensohn, S., McKeown, K. R. and 
Schlaikjer, A. H. (2003). Retrieved November 
30, 2003. http://trec.nist.gov/act_part/ 
   t12_notebook/papers/columbiau.qa.pdf 
Day, R. A. (1998) How to write & publish a 
scientific paper, Cambridge University Press, 
Cambridge. 
Ding, D. (1998) In Essays in the study of scientific 
discourse : methods, practice, and pedagogy (Ed, 
Battalio, J. T.) Albex Publishing, Stamford, CT, 
pp. 117-138. 
Elsevier Author Guide (2003). Retrieved 
December 20, 2003. http://authors.elsevier.com/ 
   GuideForAuthors.html 
Evans, H. (1972) Editing and design : a five-
volume manual of English, typography and 
layout--Book 1 : Newsman's English, Heineman, 
London. 
FishBase (2003). http:\\www.fishbase.org 
Gabbay, I. 2004. Retrieving Definitions from 
Scientific Text in the Salmon Fish Domain by 
Lexical Pattern Matching. MA thesis in 
Technical Communication, University of 
Limerick, Limerick, Ireland. 
Hearst, M. A. (1992). "Automatic Acquisition of 
Hyponyms from Large Text Corpora". In 
Proceedings of the 14th International Conference 
on Computational Linguistics (COLING-'92). 
Nantes, France, ed. 539-545. 
Hirschman, L. and Gaizauskas, R. (2001) "Natural 
Language Question Answering: The View from 
Here". Journal of Natural Language Engineering, 
7(4), 325?342. 
Joho, H. and Sanderson, M. (2000). "Retrieving 
Descriptive Phrases from Large Amounts of Free 
Text". In Proceedings of the ninth international 
conference on Information and knowledge 
management (CIKM). McLean, VA, ed. 180-
186. 
Klavans, J. and Muresan, S. (2001). "Evaluation of 
DEFINDER: a system to mine definitions from 
consumer-oriented medical text". In ACM/IEEE 
Joint Conference on Digital Libraries, JCDL 
2001. Roanoke, Virginia. 201-202. 
Lancaster, F. W. (1968) Information Retrieval 
Systems: Characteristics, Testing and 
Evaluation, Wiley, New York. 
Liu, B., Wee, C. and Ng, H. T. (2003). "Mining 
topic-specific concepts and definitions on the 
web". In Proceedings of the twelfth international 
conference on World Wide Web. Budapest, 
Hungary. 251 - 260. 
Moll?, D., Schwitter, R., Rinaldi, F., Dowdall, J. 
and Hess, M. (2003). "NLP for Answer 
Extraction in Technical Domains". In Workshop 
on Natural Language Processing for Question 
Answering, EACL 2003. Budapest, de Rijke, M. 
and Webber, B., eds. 5-11. 
Shah, P., Perez-Iratxeta, C., Bork, P. and Andrade, 
M. (2003) "Information extraction from full text 
scientific articles: Where are the keywords?" 
BMC Bioinformatics, 4(1), 20. 
Sutcliffe, R. F. E., Gabbay, I., Mulcahy, M. and 
White, K. (2003). Retrieved November 2003 
http://trec.nist.gov/ 
    act_part/t12_notebook/papers/ulimerick.qa.pdf 
Teufel, S. and Moens, M. (2002) "Summarizing 
scientific articles: experiments with relevance 
and rhetorical status". Computational 
Linguistics, 28(4), 409-445. 
Voorhees, E. (2003). Retrieved November 30, 
2003. 
http://trec.nist.gov/act_part/t12_notebook/t12_no
tebook.html 
Wikipedia (2004). Retrieved March 10, 2004. 
http://en.wikipedia.org/wiki/News_style 
Zweigenbaum, P. (2003). "Question answering in 
biomedicine". In Workshop on Natural Language 
Processing for Question Answering, EACL 
2003. Budapest, de Rijke, M. and Webber, B., 
eds. 1-4. 
 
