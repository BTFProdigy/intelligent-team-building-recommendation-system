Learning Named Entity Hyponyms for Question Answering
Paul McNamee
JHU Applied Physics Laboratory
11100 Johns Hopkins Road
Laurel, MD 20723-6099, USA
paul.mcnamee@jhuapl.edu
Rion Snow
Stanford AI Laboratory
Stanford University
Stanford, CA 94305, USA
rion@cs.stanford.edu
Patrick Schone
Department of Defense
Fort George G. Meade, MD 20755-6000
pjschon@tycho.ncsc.mil
James Mayfield
JHU Applied Physics Laboratory
11100 Johns Hopkins Road
Laurel, MD 20723-6099, USA
james.mayfield@jhuapl.edu
Abstract
Lexical mismatch is a problem that con-
founds automatic question answering sys-
tems. While existing lexical ontologies such
as WordNet have been successfully used to
match verbal synonyms (e.g., beat and de-
feat) and common nouns (tennis is-a sport),
their coverage of proper nouns is less ex-
tensive. Question answering depends sub-
stantially on processing named entities, and
thus it would be of significant benefit if
lexical ontologies could be enhanced with
additional hypernymic (i.e., is-a) relations
that include proper nouns, such as Edward
Teach is-a pirate. We demonstrate how a re-
cently developed statistical approach to min-
ing such relations can be tailored to iden-
tify named entity hyponyms, and how as a
result, superior question answering perfor-
mance can be obtained. We ranked candi-
date hyponyms on 75 categories of named
entities and attained 53% mean average pre-
cision. On TREC QA data our method pro-
duces a 9% improvement in performance.
1 Introduction
To correctly extract answers, modern question an-
swering systems depend on matching words be-
tween questions and retrieved passages containing
answers. We are interested in learning hypernymic
(i.e., is-a) relations involving named entities because
we believe these can be exploited to improve a sig-
nificant class of questions.
For example, consider the following questions:
? What island produces Blue Mountain coffee?
? In which game show do participants compete
based on their knowledge of consumer prices?
? What villain is the nemesis of Dudley Do-
Right?
Knowledge that Jamaica is an island, that The Price
is Right is a game show, and that Snidely Whiplash
is a villain, is crucial to answering these questions.
Sometimes these relations are evident in the same
context as answers to questions, for example, in
?The island of Jamaica is the only producer of Blue
Mountain coffee?; however, ?Jamaica is the only
producer of Blue Mountain coffee? should be suf-
ficient, despite the fact that Jamaica is an island is
not observable from the sentence.
The dynamic nature of named entities (NEs)
makes it difficult to enumerate all of their evolv-
ing properties; thus manual creation and curation
of this information in a lexical resource such as
WordNet (Fellbaum, 1998) is problematic. Pasca
and Harabagiu discuss how insufficient coverage of
named entities impairs QA (2001). They write:
?Because WordNet was not designed
as an encyclopedia, the hyponyms of con-
cepts such as composer or poet are illus-
trations rather than an exhaustive list of
instances. For example, only twelve com-
poser names specialize the concept com-
poser ... Consequently, the enhancement
of WordNet with NE information could
help QA.?
799
The chief contribution of this study is demonstrat-
ing that an automatically mined knowledge base,
which naturally contains errors as well as correctly
distilled knowledge, can be used to improve QA per-
formance. In Section 2 we discuss prior work in
identifying hypernymic relations. We then explain
our methods for improved NE hyponym learning
and its evaluation (Section 3) and apply the relations
that are discovered to enhance question answering
(Section 4). Finally we discuss our results (Section
5) and present our conclusions (Section 6).
2 Hyponym Induction
We review several approaches to learning is-a rela-
tions.
2.1 Hearst Patterns
The seminal work in the field of hypernym learn-
ing was done by Hearst (1992). Her approach was
to identify discriminating lexico-syntactic patterns
that suggest hypernymic relations. For example, ?X,
such as Y?, as in ?elements, such as chlorine and
fluorine?.
2.2 KnowItAll
Etzioni et al developed a system, KnowItAll, that
does not require training examples and is broadly
applicable to a variety of classes (2005). Starting
with seed examples generated from high precision
generic patterns, the system identifies class-specific
lexical and part-of-speech patterns and builds a
Bayesian classifier for each category. KnowItAll
was used to learn hundreds of thousands of class
instances and clearly has potential for improving
QA; however, it would be difficult to reproduce the
approach because of information required for each
class (i.e., specifying synonyms such as town and
village for city) and because it relies on submitting a
large number of queries to a web search engine.
2.3 Query Logs
Pasca and Van Durme looked at learning entity class
membership for five high frequency classes (com-
pany, country, city, drug, and painter), using search
engine query logs (2007). They reported precision
at 50 instances between 0.50 and 0.82.
2.4 Dependency Patterns
Snow et al have described an approach with several
desirable properties: (1) it is weakly-supervised and
only requires examples of hypernym/hyponym rela-
tions and unannotated text; (2) the method is suit-
able for both common and rare categories; and, (3)
it achieves good performance without post filtering
using the Web (2005; 2006). Their method relies
on dependency parsing, a form of shallow parsing
where each word modifies a single parent word.
Hypernym/hyponym word pairs where the words1
belong to a single WordNet synset were identified
and served to generate training data in the follow-
ing way: making the assumption that when the two
words co-occur, evidence for the is-a relation is
present, sentences containing both terms were ex-
tracted from unlabeled text. The sentences were
parsed and paths between the nouns in the depen-
dency trees were calculated and used as features in a
supervised classifier for hypernymy.
3 Learning Named Entity Hyponyms
The present work follows the technique described
by Snow et al; however, we tailor the approach in
several ways. First, we replace the logistic regres-
sion model with a support vector machine (SVM-
Light). Second, we significantly increase the size
of training corpora to increase coverage. This ben-
eficially increases the density of training and test
vectors. Third, we include additional features not
based on dependency parses (e.g., morphology and
capitalization). Fourth, because we are specifically
interested in hypernymic relations involving named
entities, we use a bootstrapping phase where train-
ing data consisting primarily of common nouns are
used to make predictions and we then manually ex-
tract named entity hyponyms to augment the train-
ing data. A second learner is then trained using the
entity-enriched data.
3.1 Data
We rely on large amounts of text; in all our exper-
iments we worked with a corpus from the sources
given in Table 1. Sentences that presented difficul-
ties in parsing were removed and those remaining
1Throughout the paper, use of the term word is intended to
include named entities and other multiword expressions.
800
Table 1: Sources used for training and learning.
Size Sentences Genre
TREC Disks 4,5 81 MB 0.70 M Newswire
AQUAINT 1464 MB 12.17 M Newswire
Wikipedia (4/04) 357 MB 3.27 M Encyclopedia
Table 2: Characteristics of training sets.
Pos. Pairs Neg. Pairs Total Features
Baseline 7975 63093 162528
+NE 9331 63093 164298
+Feat 7975 63093 162804
were parsed with MINIPAR (Lin, 1998). We ex-
tracted 17.3 million noun pairs that co-occurred in
at least one sentence. All pairs were viewed as po-
tential hyper/hyponyms.
Our three experimental conditions are summa-
rized in Table 2. The baseline model used 71068
pairs as training data; it is comparable to the
weakly-supervised hypernym classifier of Snow et
al. (2005), which used only dependency parse fea-
tures, although here the corpus is larger. The entity-
enriched data extended the baseline training set by
adding positive examples. The +Feat model uses ad-
ditional features besides dependency paths.
3.2 Bootstrapping
Our synthetic data relies on hyper/hyponym pairs
drawn from WordNet, which is generally rich in
common nouns and lacking in proper nouns. But
certain lexical and syntactic features are more likely
to be predictive for NE hyponyms. For example, it
is uncommon to precede a named entity with an in-
definite article, and certain superlative adjectives are
more likely to be used to modify classes of entities
(e.g., ?the youngest coach?, ?the highest peak?). Ac-
cordingly we wanted to enrich our training data with
NE exemplars.
By manually reviewing highly ranked predictions
of the baseline system, we identified 1356 additional
pairs to augment the training data. This annotation
took about a person-day. We then rescanned the cor-
pus to build training vectors for these co-occurring
nouns to produce the +NE model vectors.
Table 3: Features considered for +Feat model.
Feature Comment
Hypernym con-
tained in hyponym
Sands Hotel is-a hotel
Length in chars /
words
Chars: 1-4, 5-8, 9-16, 17+
Words: 1, 2, 3, 4, 5, 6, 7+
Has preposition Treaty of Paris; Statue of Liberty
Common suffixes -ation, -ment, -ology, etc...
Figurative term Such as goal, basis, or problem
Abstract category Like person, location, amount
Contains digits Usually not a good hyponym
Day of week;
month of year
Indiscriminately co-occurs with
many nouns.
Presence and depth
in WordNet graph
Shallow hypernyms are unlikely to
have entity hyponyms. Presence in
WN suggests word is not an entity.
Lexname of 1st
synset in WordNet
Root classes like person, location,
quantity, and process.
Capitalization Helps identify entities.
Binned document
frequency
Partitioned by base 10 logs
3.3 Additional Features
The +Feat model incorporated an additional 276 bi-
nary features which are listed in Table 3. We consid-
ered other features such as the frequency of patterns
on the Web, but with over 17 million noun pairs this
was computationally infeasible.
3.4 Evaluation
To compare our different models we created a test
set of 75 categories. The classes are diverse and
include personal, corporate, geographic, political,
artistic, abstract, and consumer product entities.
From the top 100 responses of the different learn-
ers, a pool of candidate hyponyms was created, ran-
domly reordered, and judged by one of the authors.
To assess the quality of purported hyponyms we
used average precision, a measure in ranked infor-
mation retrieval evaluation, which combines preci-
sion and recall.
Table 4 gives average precision values for the
three models on 15 classes of mixed difficulty2. Per-
formance varies considerably based on the hyper-
nym category, and for a given category, by classifier.
N is the number of known correct instances found in
the pool that belong to a given category.
Aggregate performance, as mean average preci-
sion, was computed over all 75 categories and is
2These are not the highest performing classes
801
Table 4: Average precision on 15 categories.
N Baseline +NE +Feat
chemical element 78 0.9096 0.9781 0.8057
african country 48 0.8581 0.8521 0.4294
prep school 26 0.6990 0.7098 0.7924
oil company 132 0.6406 0.6342 0.7808
boxer 109 0.6249 0.6487 0.6773
sculptor 95 0.6108 0.6375 0.8634
cartoonist 58 0.5988 0.6109 0.7097
volcano 119 0.5687 0.5516 0.7722
horse race 23 0.4837 0.4962 0.7322
musical 80 0.4827 0.4270 0.3690
astronaut 114 0.4723 0.5912 0.5738
word processor 26 0.4437 0.4426 0.6207
chief justice 115 0.4029 0.4630 0.5955
perfume 43 0.2482 0.2400 0.5231
pirate 10 0.1885 0.3070 0.2282
Table 5: Mean average precision over 75 categories.
Baseline +NE +Feat
MAP 0.4801 0.5001 (+4.2%) 0.5320 (+10.8%)
given in Table 5. Both the +NE and +Feat models
yielded improvements that were statistically signif-
icant at a 99% confidence level. The +Feat model
gained 11% over the baseline condition. The maxi-
mum F-score for +Feat is 0.55 at 70% recall.
Mean average precision emphasizes precision at
low ranks, so to capture the error characteristics at
multiple operating points we present a precision-
recall graph in Figure 1. The +NE and +Feat models
both attain superior performance at all but the lowest
recall levels. For question answering this is impor-
tant because it is not known which entities will be
the focus of a question, so the ability to deeply mine
various entity classes is important.
Table 6 lists top responses for four categories.
3.5 Discussion
53% mean average precision seems good, but is it
good enough? For automated taxonomy construc-
tion precision of extracted hyponyms is critically
important; however, because we want to improve
question answering we prefer high recall and can
tolerate some mistakes. This is because only a small
set of passages that are likely to contain an answer
are examined in detail, and only from this subset
of passages do we need to reason about potential
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall Level
P
r
e
c
i
s
i
o
n
Feat
Ent
Baseline
Figure 1: Precision-recall graph for three classifiers.
hyponyms. In the next section we describe an ex-
periment which confirms that our learned entity hy-
ponyms are beneficial.
4 QA Experiments
4.1 QACTIS
To evaluate the usefulness of our learned NE hy-
ponyms for question answering, we used the QAC-
TIS system (Schone et al, 2005). QACTIS was
fielded at the 2004-2006 TREC QA evaluations and
placed fifth at the 2005 workshop. We worked with
a version of the software from July 2005.
QACTIS uses WordNet to improve matching of
question and document words, and a resource, the
Semantic Forest Dictionary (SFD), which contains
many hypernym/hyponym pairs. The SFD was pop-
ulated through both automatic and manual means
(Schone et al, 2005), and was updated based on
questions asked in TREC evaluations through 2004.
4.2 Experimental Setup
We used factoid questions from the TREC 2005-
2006 QA evaluations (Voorhees and Dang, 2005)
and measured performance with mean reciprocal
rank (MRR) and percent correct at rank 1.
All runs made use of WordNet 2.0, and we ex-
amined several other sources of hypernym knowl-
802
Table 6: Top responses for four categories using the +Feat model. Starred entries were judged incorrect.
Sculptor Horse Race Astronaut Perfume
1 Evelyn Beatrice Longman Tevis Cup Mark L Polansky * Avishag
2 Nancy Schon Kenilworth Park Gold Cup Richard O Covey Ptisenbon
3 Phidias Cox Plate George D Nelson Poeme
4 Stanley Brandon Kearl Grosser Bugatti Preis Guion Bluford Jr Parfums International
5 Andy Galsworthy Melbourne Cup Stephen S Oswald Topper Schroeder
6 Alexander Collin * Great Budda Hall Eileen Collins * Baccarin
7 Rachel Feinstein Travers Stakes Leopold Eyharts Pink Lady
8 Zurab K Tsereteli English Derby Daniel M Tani Blue Waltz
9 Bertel Thorvaldsen * Contrade Ronald Grabe WCW Nitro
10 Cildo Meireles Palio * Frank Poole Jicky
Table 7: Additional knowledge sources by size.
Classes Class Instances
Baseline 76 11,066
SFD 1,140 75,647
SWN 7,327 458,370
+Feat 44,703 1,868,393
edge. The baseline condition added a small subset
of the Semantic Forest Dictionary consisting of 76
classes seen in earlier TREC test sets (e.g., nation-
alities, occupations, presidents). We also tested: (1)
the full SFD; (2) a database from the Stanford Word-
net (SWN) project (Snow et al, 2006); and, (3) the
+Feat model discussed in Section 3. The number of
classes and entries of each is given in Table 7.
4.3 Results
We observed that each source of knowledge benefit-
ted questions that were incorrectly answered in the
baseline condition. Examples include learning a me-
teorite (Q84.1), a university (Q93.3), a chief oper-
ating officer (Q108.3), a political party (Q183.3), a
pyramid (Q186.4), and a movie (Q211.5).
In Table 8 we compare performance on questions
from the 2005 and 2006 test sets. We assessed
performance primarily on test questions that were
deemed likely to benefit from hyponym knowledge
? questions that had a readily discernible category
(e.g., ?What film ...?, ?In what country ...?) ? but we
also give results on the entire test set.
The WordNet-only run suffers a large decrease
compared to the baseline. This is expected because
WordNet lacks coverage of entities and the baseline
condition specifically populates common categories
of entities that have been observed in prior TREC
evaluations. Nonetheless, WordNet is useful to the
system because it addresses lexical mismatch that
does not involve entities.
The full SFD, the SWN, and the +Feat model
achieved 17%, 2%, and 9% improvements in answer
correctness, respectively. While no model had ex-
posure to the 2005-2006 TREC questions, the SFD
database was manually updated based on training
on the TREC-8 through TREC-2004 data sets. It
approximates an upper bound on gains attributable
to addition of hyponym knowledge: it has an un-
fair advantage over the other models because recent
question sets use similar categories to those in ear-
lier TRECs. Our +Feat model, which has no bias
towards TREC questions, realizes larger gains than
the SWN. This is probably at least in part because it
produced a more diverse set of classes and a signif-
icantly larger number of class instances. Compared
to the baseline condition the +Feat model sees a 7%
improvement in mean reciprocal rank and a 9% im-
provement in correct first answers; both results rep-
resent a doubling of performance compared to the
use of WordNet alne. We believe that these results
illustrate clear improvement attributable to automat-
ically learned hyponyms.
The rightmost columns in Table 8 reveal that the
magnitude of improvements, when measured over
all questions, is less. But the drop off is consistent
with the fact that only one third of questions have
clear need for entity knowledge.
5 Discussion
Although there is a significant body of work in auto-
mated ontology construction, few researchers have
examined the relationship between their methods
803
Table 8: QA Performance on TREC 2005 & 2006 Data
Hyponym-Relevant Subset (242) All Questions (734)
MRR % Correct MRR % Correct
WN-alone 0.189 (-45.6%) 12.8 (-51.6%) 0.243 (-29.0%) 18.26 (-30.9%)
Baseline 0.348 26.4 0.342 26.4
SFD 0.405 (+16.5%) 31.0 (+17.2%) 0.362 (+5.6%) 27.9 (+5.7%)
SWN 0.351 (+1.0%) 26.9 (+1.6%) 0.343 (+0.3%) 26.6 (+0.5%)
Feat 0.373 (+7.4%) 28.9 (+9.4%) 0.351 (+2.5%) 27.3 (+3.1%)
for knowledge discovery and improved question-
answering performance. One notable study was con-
ducted by Mann (2002). Our work differs in two
ways: (1) his method for identifying hyponyms was
based on a single syntactic pattern, and (2) he looked
at a comparatively simple task ? given a question
and one answer sentence containing the answer, ex-
tract the correct named entity answer.
Other attempts to deal with lexical mismatch in
automated QA include rescoring based on syntactic
variation (Cui et al, 2005) and identification of ver-
bal paraphrases (Lin and Pantel, 2001).
The main contribution of this paper is showing
that large-scale, weakly-supervised hyponym learn-
ing is capable of producing improvements in an end-
to-end QA system. In contrast, previous studies have
generally presented algorithmic advances and show-
cased sample results, but failed to demonstrate gains
in a realistic application. While the hypothesis that
discovering is-a relations for entities would improve
factoid QA is intuitive, we believe these experiments
are important because they show that automatically
distilled knowledge, even when containing errors
that would not be introduced by human ontologists,
is effective in question answering systems.
6 Conclusion
We have shown that highly accurate statistical learn-
ing of named entity hyponyms is feasible and that
bootstrapping and feature augmentation can signif-
icantly improve classifier accuracy. Mean aver-
age precision of 53% was attained on a set of 75
categories that included many fine-grained entity
classes. We also demonstrated that mining knowl-
edge about entities can be directly applied to ques-
tion answering, and we measured the benefit on
TREC QA data. On a subset of questions for
which NE hyponyms are likely to help we found that
learned hyponyms generated a 9% improvement in
performance compared to a strong baseline.
References
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-Seng
Chua. 2005. Question answering passage retrieval using
dependency relations. In SIGIR 2005, pages 400?407.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana M.
Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld,
and Alexander Yates. 2005. Unsupervised Named-Entity
Extraction from the Web: An Experimental Study. Artificial
Intelligence, 165(1):191?134.
Christine Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Marti A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In ACL 1992, pages 539?545.
Dekang Lin and Patrick Pantel. 2001. Discovery of inference
rules for question-answering. Natural Language Engineer-
ing, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of minipar.
In Workshop on the Evaluation of Parsing Systems.
Gideon S. Mann. 2002. Fine-grained proper noun ontolo-
gies for question answering. In COLING-02 on SEMANET,
pages 1?7.
Marius Pasca and Benjamin Van Durme. 2007. What you seek
is what you get: Extraction of class attributes from query
logs. In IJCAI-07, pages 2832?2837.
Marius Pasca and Sanda M. Harabagiu. 2001. The informa-
tive role of wordnet in open-domain question answering. In
Proceedings of the NAACL 2001 Workshop on WordNet and
Other Lexical Resources.
Patrick Schone, Gary Ciany, Paul McNamee, James Mayfield,
and Thomas Smith. 2005. QACTIS-based Question An-
swering at TREC 2005. In Proceedings of the 14th Text RE-
trieval Conference.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. Learn-
ing syntactic patterns for automatic hypernym discovery. In
NIPS 17.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006. Seman-
tic taxonomy induction from heterogenous evidence. In ACL
2006, pages 801?808.
804
Proceedings of NAACL HLT 2009: Short Papers, pages 25?28,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Translation Corpus Source and Size in Bilingual Retrieval
Paul McNamee and James Mayfield
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21218, USA
{paul.mcnamee,james.mayfield}@jhuapl.edu
Charles Nicholas
Dept. of Computer Science and Electrical Engineering
UMBC
Baltimore, MD 21250, USA
nicholas@umbc.edu
Abstract
This paper explores corpus-based bilingual re-
trieval where the translation corpora used vary
by source and size. We find that the quality of
translation alignments and the domain of the
bitext are important. In some settings these
factors are more critical than corpus size. We
also show that judicious choice of tokeniza-
tion can reduce the amount of bitext required
to obtain good bilingual retrieval performance.
1 Introduction
Large parallel corpora are an increasingly available
commodity. Such texts are the fuel of statistical
machine translation systems and are used in appli-
cations such as cross-language information retrieval
(CLIR). Several beliefs are commonly held regard-
ing the relationship between parallel text quality and
size for CLIR. It is thought that larger texts should
be better, because the problems of data sparseness
and untranslatable terms are reduced. Similarly, par-
allel text from a domain more closely related to a
document collection should lead to better bilingual
retrieval performance, again because better lexical
translations are available.
We compared four sources of parallel text us-
ing CLEF document collections in eight languages
(Braschler and Peters, 2004). English topic sets
from 2000 to 2007 were used. Corpus-based trans-
lation of query terms was performed and documents
were ranked using a statistical language model ap-
proach to retrieval (Ponte and Croft, 1998). Exper-
iments were conducted using unlemmatized words
and character 5-grams. No use was made of pre-
translation query expansion or automated relevance
feedback.
2 Translation Corpora
Information about the four parallel texts used in our
experiments is provided in Table 1. We restricted
our focus to Dutch (NL), English (EN), Finnish (FI),
French (FR), German (DE), Italian (IT), Portuguese
(PT), Spanish (ES), and Swedish (SV). These lan-
guages are covered by each parallel corpus.
2.1 Bible
The bible corpus is based on the 66 books in the Old
and New Testaments. Alignments at the verse level
were used; there are 31103 verses in the English text.
2.2 JRC-Acquis v3
This parallel text is based on EU laws comprising the
Acquis Communautaire and translations are avail-
able in 22 languages. The English portion of the
acquis data includes 1.2 million aligned passages
containing over 32 million words, which is approxi-
mately 40 times larger than the Biblical text. Align-
ments were provided with the corpus and were pro-
duced by the Vanilla algorithm.1 The alignments are
at roughly the sentence level, but only 85% corre-
spond to a single sentence in both languages.
2.3 Europarl v3
The Europarl corpus was assembled to support ex-
periments in statistical machine translation (Koehn,
2005). The documents consist of transcribed dia-
logue from the official proceedings of the European
Parliament. We used the precomputed alignments
that are provided with the corpus, and which are
based on the algorithm by Gale and Church (1991).
The alignments are believed to be of high quality.
1Available from http://nl.ijs.si/telri/vanilla/
25
Name Words Wrds/doc Alignments Genre Source
bible 785k 25.3 Near Perfect Religious http://unbound.biola.edu/
acquis 32M 26.3 Good EU law (1958 to 2006) http://wt.jrc.it/lt/acquis/
europarl 33M 25.5 Very Good Parliamentary oration
(1996 to 2006)
http://www.statmt.org/europarl/
ojeu 84M 34.5 Fair Governmental affairs
(1998 to 2004)
Derived from documents at
http://europea.eu.int/
Table 1: Parallel texts used in experiments.
2.4 Official Journal of the EU
The Official Journal of the European Union covers a
wide range of topics such as agriculture, trade, and
foreign relations. We constructed this parallel cor-
pus by downloading documents dating from January
1998 through April 2004 and converting the texts
from Adobe?s Portable Document Format (PDF) to
ISO-8859-1 encoded text using pdftotext. The doc-
uments were segmented into pages and into para-
graphs consisting of a small number of sentences
(typically 1 to 3); however this process was compli-
cated by the fact that many documents have outline
or tabular formatting. Alignments were produced
using Church?s char align software (1993).
Due to complexities of decoding the PDF, some of
the accented characters were not extracted properly,
but this is a problem mostly for the earlier material
in the collection. In total about 85 million words of
text per language was obtained, which is over twice
the size of either the acquis or europarl collections.
3 Translation
Using the pairwise-aligned corpora described above,
parallel indexes for each corpus were created using
words and 5-grams. Query translation was accom-
plished as follows. For each query term s, source
language documents from the aligned collection that
contain s are identified. If no document contains this
term, then it is left untranslated. Each target lan-
guage term t appearing in the corresponding docu-
ments is scored:
Score(t) = (Fl(t)? Fc(t))? IDF (t)1.25 (1)
where Fl and Fc are relative document frequencies
based on local subset of documents and the whole
corpus. IDF (t) is the inverse document frequency,
or log2( Ndf(t)). The candidate translation with thehighest score replaced the original query term and
the transformed query vector is used for retrieval
against the target language collection.
This is a straightforward approach to query trans-
lation. More sophisticated methods have been pro-
posed, including bidirectional translation (Wang and
Oard, 2006) and use of more than one translation
candidate per query term (Pirkola et al, 2003).
Subword translation, the direct translation of
character n-grams, offers several advantages over
translating words (McNamee and Mayfield, 2005).
N-grams provide morphological normalization,
translations of multiword expressions are suggested
by translation of word-spanning n-grams, and out-
of-vocabulary (OOV) words can be be partly trans-
lated with n-gram fragments. Additionally, there are
few OOV n-grams, at least for n = 4 and n = 5.
4 Experimental Results
We describe two experiments. The first examines
the efficacy of the different translation resources and
the second measures the relationship between cor-
pus size and retrieval effectiveness. English was the
sole source language.
4.1 Translation Resources
First the relationship between translation source and
bilingual retrieval effectiveness is studied. Table 2
reports mean average precision when word-based to-
kenization and translation was performed for each
of the target collections. For comparison the cor-
responding performance using topics in the target
language (mono) is also given. As expected, the
smallest bitext, bible, performs the worst. Averaged
across the eight languages only 39% relative effec-
tiveness is seen compared to monolingual perfor-
mance. Reports advocating the use of religious texts
for general purpose CLIR may have been overly op-
timistic (Chew et al, 2006). Both acquis and eu-
roparl are roughly 40 times larger in size than bible
26
Target mono bible acquis europarl ojeu
DE 0.3303 0.1338 0.1802 0.2427 0.1937
ES 0.4396 0.1454 0.2583 0.3509 0.2786
FI 0.3406 0.1288 0.1286 0.2135 0.1636
FR 0.3638 0.1651 0.2508 0.2942 0.2600
IT 0.3749 0.1080 0.2365 0.2913 0.2405
NL 0.3813 0.1502 0.2474 0.2974 0.2484
PT 0.3162 0.1432 0.2009 0.2365 0.2157
SV 0.3387 0.1509 0.2111 0.2447 0.1861
Average 0.3607 0.1407 0.2142 0.2714 0.2233
39.0% 59.4% 75.3% 61.9%
Table 2: Mean average precision for word-based transla-
tion of English topics using different corpora.
Target mono bible acquis europarl ojeu
DE 0.4201 0.1921 0.2952 0.3519 0.3169
ES 0.4609 0.2295 0.3661 0.4294 0.3837
FI 0.5078 0.1886 0.3552 0.3744 0.3743
FR 0.3930 0.2203 0.3013 0.3523 0.3334
IT 0.3997 0.2110 0.2920 0.3395 0.3160
NL 0.4243 0.2132 0.3060 0.3603 0.3276
PT 0.3524 0.1892 0.2544 0.2931 0.2769
SV 0.4271 0.1653 0.3016 0.3203 0.2998
Average 0.4232 0.2012 0.3090 0.3527 0.3286
47.5% 73.0% 83.3% 77.6%
Table 3: Mean average precision using 5-gram transla-
tions of English topics using different corpora.
and both do significantly better; however europarl is
clearly superior and achieves 75% of monolingual
effectiveness. Though nearly twice the size, ojeu
fails to outperform europarl and just barely beats
acquis. Likely reasons for this include difficulties
properly converting the ojeu data to text, problem-
atic alignments, and the substantially greater length
of the aligned passages.
The same observations can be seen from Table 3
where 5-grams were used for tokenization and trans-
lation instead of words. The level of performance
with 5-grams is higher and these improvements are
statistically significant with p < 0.01 (t-test).2 Av-
eraged across the eight languages gains from 30% to
47% were seen using 5-grams, depending on the re-
source. As a translation resource europarl still out-
performs the other sources in each of the eight lan-
guages and the relative ordering of {europarl, ojeu,
acquis, bible} is the same in both cases.
2Except in four cases: mono: In ES & IT p < 0.05; bible:
5-grams were not significantly different than words in FI & SV
4.2 Size of Parallel Text
To investigate how corpus size effects bilingual
retrieval we subsampled europarl and used these
smaller subcorpora for translation. The entire cor-
pus is 33 million words in size, and samples of 1%,
2%, 5%, 10%, 20%, 40%, 60%, and 80% were made
based on counting documents, which for europarl
is equivalent to counting sentences. Samples were
taken by processing the data in chronological order.
In Figure 1 (a-d) the effect of using larger parallel
corpora is plotted for four languages. Mean average
precision is on the vertical axes, and for visual effect
the chart for each language pair uses the same scale.
The general shape of the curves is to rise quickly as
increasing subsets from 1% to 10% are used and to
flatten as size increases further. Curves for the other
four languages (not shown) are quite similar. The
deceleration of improvement with increasing cor-
pus size can be explained by Heap?s Law. Similar
results have been obtained in the few studies that
have sought to quantify bilingual retrieval perfor-
mance as a function of translation resource size (Xu
and Weischedel, 2000; Demner-Fushman and Oard,
2003). In the higher complexity languages such as
German and Finnish, n-grams appear to be gaining
a slight improvement even when the entire corpus is
used; vocabulary size is greater in those languages.
The data for the 0% condition were based on
cognate matches for words and ?cognate n-grams?
that require no translation. The figure reveals that
even very small amounts of parallel text quickly im-
prove performance. The 2% condition is roughly the
size of bible, but is higher performing, likely due
to a better domain match.3 Using a subsample of
only 5% of available data from the highest perform-
ing translation resource, europarl, 5-grams outper-
formed plain words using any amount of bitext.
5 Conclusion
We examined issues in corpus-based bilingual re-
trieval, including the importance of parallel corpus
selection and size, and the relative effectiveness of
alternative tokenization methods. Size is not the
only important factor in corpus-based bilingual re-
3For example, the Biblical text does not contain the words
nuclear or energy and thus is greatly disadvantaged for a topic
about nuclear power.
27
(a) German (b) Spanish
(c) Finnish (d) French
???
???
???
???
???
???
???
???
???
?? ?? ?? ?? ?? ???
????? ?????? ?????
???
???
???
???
???
???
???
???
???
?? ?? ?? ?? ?? ???
????? ?????? ?????
???
???
???
???
???
???
???
???
???
?? ?? ?? ?? ?? ???
?????? ?????? ??????
???
???
???
???
???
???
???
???
???
?? ?? ?? ?? ?? ???
????? ?????? ?????
Figure 1: Performance improvement with corpus growth.
trieval, the quality of alignments, compatibility in
genre, and choice of tokenization are also important.
We found that character 5-gram tokenization out-
performs words when used both for translation and
document indexing. Large relative improvements
(over 30%) were observed with 5-grams, and when
only limited parallel data is available for translation,
n-grams are markedly more effective than words.
Future work could address some limitations of the
present study by using bidirectional translation mod-
els, considering other language families and source
languages other than English, and applying query
expansion techniques.
References
Martin Braschler and Carol Peters. 2004. Cross-
language evaluation forum: Objectives, results,
achievements. Inf. Retr., 7(1-2):7?31.
P. A. Chew, S. J. Verzi, T. L. Bauer, and J. T. Mc-
Clain. 2006. Evaluation of the Bible as a resource for
cross-language information retrieval. In Workshop on
Multilingual Language Resources and Interoperabil-
ity, pages 68?74.
Kenneth Ward Church. 1993. Char align: A program for
aligning parallel texts at the character level. In Pro-
ceedings ACL, pages 1?8.
Dina Demner-Fushman and Douglas W. Oard. 2003.
The effect of bilingual term list size on dictionary-
based cross-language information retrieval. In HICSS,
pages 108?117.
William A. Gale and Kenneth W. Church. 1991. A pro-
gram for aligning sentences in bilingual corpora. In
Proceedings ACL, pages 177?184.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Paul McNamee and James Mayfield. 2005. Translating
pieces of words. In ACM SIGIR, pages 643?644.
Ari Pirkola, Deniz Puolama?ki, and Kalervo Ja?rvelin.
2003. Applying query structuring in cross-language
retrieval. Inf. Process. Manage, 39(3):391?402.
Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In ACM
SIGIR, pages 275?281.
Jianqiang Wang and Douglas W. Oard. 2006. Combin-
ing bidirectional translation and synonymy for cross-
language information retrieval. In ACM SIGIR, pages
202?209.
Jinxi Xu and Ralph Weischedel. 2000. Cross-lingual in-
formation retrieval using hidden Markov models. In
EMNLP, pages 85?103.
28
 	
  Named Entity Recognition using Hundreds of Thousands of Features
James Mayfield and Paul McNamee and Christine Piatko
The Johns Hopkins University Applied Physics Laboratory
11100 Johns Hopkins Road, Laurel, Maryland 20723-6099 USA
{mayfield,mcnamee,piatko}@jhuapl.edu
Abstract
We present an approach to named entity recog-
nition that uses support vector machines to cap-
ture transition probabilities in a lattice. The
support vector machines are trained with hun-
dreds of thousands of features drawn from the
CoNLL-2003 Shared Task training data. Mar-
gin outputs are converted to estimated prob-
abilities using a simple static function. Per-
formance is evaluated using the CoNLL-2003
Shared Task test set; Test B results were F?=1
= 84.67 for English, and F?=1 = 69.96 for Ger-
man.
1 Introduction
Language independence is difficult to achieve in named
entity recognition (NER) because different languages ap-
pear to require different features. Most NER systems (or
taggers) are severely limited in the number of features
they may consider, because the computational expense of
handling large numbers of features is high, and because
the risk of overtraining increases with the number of fea-
tures. Thus, the feature set must be finely tuned to be
effective. Such constrained feature sets are naturally lan-
guage dependent.
Increasing the number of features that a tagger can han-
dle would ameliorate this problem, because the designer
could select many relatively simple features in lieu of a
few highly tuned features. Because support vector ma-
chines (SVMs) (Vapnik, 1995) can handle large numbers
of parameters efficiently while simultaneously limiting
overtraining, they are good candidates for application to
named entity recognition. This paper proposes a novel
way to use SVMs for named entity recognition called
SVM-Lattice, describes a large feature space that we used
on the CoNLL-2003 Shared Task (Tjong Kim Sang and
De Meulder, 2003), and presents results from that task.
2 Model
We are interested in a lattice-based approach to named
entity recognition. In this approach, each sentence is pro-
cessed individually. A lattice is built with one column
per word of the sentence (plus a start state). Each column
contains one vertex for each possible tag. Each vertex in
one column is connected by an edge to every vertex in the
next column that may legitimately follow it (some tran-
sitions, such as from I-LOC to B-PER are disallowed).
Given such a lattice, our task is first to assign probabili-
ties to each of the arcs, then to find the highest likelihood
path through the lattice based on those probabilities. This
path corresponds to the highest likelihood tagging of the
sentence.
Hidden Markov models break the probability calcula-
tions into two pieces: transition probabilities (the proba-
bility of moving from one vertex to another independent
of the word at the destination node), and emission proba-
bilities (the probability that a given word would be gener-
ated from a certain state independent of the path taken to
get to that state). These probability distributions are cal-
culated separately because the training data are typically
too sparse to support a reasonable maximum likelihood
estimate of the joint probability. However, there is no
reason that these two distributions could not be combined
given a suitable estimation technique.
A support vector machine is a binary classifier that uses
supervised training to predict whether a given vector is in
a target class. All SVM training and test data occupy
a single high-dimensional vector space. In its simplest
form, training an SVM amounts to finding the hyperplane
that separates the positive training samples from the neg-
ative samples by the largest possible margin. This hyper-
plane is then used to classify the test vectors; those that
lie on one side of the hyperplane are classified as mem-
bers of the positive class, while others are classified as
members of the negative class. In addition to the clas-
sification decision, the SVM also produces a margin for
each vector?its distance from the hyperplane.
SVMs have two useful properties for our purposes.
First, they can handle very high dimensional spaces, as
long as individual vectors are sparse (i.e., each vector has
extent along only a small subset of the dimensions). Sec-
ondly, SVMs are resistant to overtraining, because only
the training vectors that are closest to the hyperplane
(called support vectors) dictate the parameters for the hy-
perplane. So SVMs would seem to be ideal candidates
for estimating lattice probabilities.
Unfortunately, SVMs do not produce probabilities, but
rather margins. In fact, one of the reasons that SVMs
work so well is precisely because they do not attempt to
model the entire distribution of training points. To use
SVMs in a lattice approach, then, a mechanism is needed
to estimate probability of category membership given a
margin.
Platt (1999) suggests such a method. If the range of
possible margins is partitioned into bins, and positive and
negative training vectors are placed into these bins, each
bin will have a certain percentage of positive examples.
These percentages can be approximated by a sigmoid
function: P (y = 1 | f) = 1/(1 + exp(Ax + b)). Platt
gives a simple iterative method for estimating sigmoid
parameters A and B, given a set of training vectors and
their margins.
This approach can work well if a sufficient number of
positive training vectors are available. Unfortunately, in
the CoNLL-2003 shared task, many of the possible label
transitions have few exemplars. Two methods are avail-
able to handle insufficient training data: smoothing, and
guessing.
In the smoothing approach, linear interpolation is used
to combine the model for the source to target pair that
lacks sufficient data with the model made from a com-
bination of all transitions going to the target label. For
example, we could smooth the probabilities derived for
the I-ORG to I-LOC transition with the probability that
any tag would transition to the I-LOC state at the same
point in the sentence.
The second approach is to guess at an appropriate
model without examining the training data. While in the-
ory this could prove to be a terrible approach, in practice
for the Shared Task, selection of fixed sigmoid parame-
ters works better than using Platt?s method to train the
parameters. Thus, we fix A = ?2 and b = 0. We con-
tinue to believe that Platt?s method or something like it
will ultimately lead to superior performance, but our cur-
rent experiments use this untrained model.
Our overall approach then is to use SVMs to estimate
lattice transition probabilities. First, due to the low fre-
quency of B-XXX tags in the training data, we convert
each B-XXX tags to the corresponding I-XXX tag; thus,
our system never predicts B-XXX tags. Then, we featur-
ize the training data, forming sparse vectors suitable for
input to our SVM package, SVMLight 5.00 (Joachims,
1999). Our feature set is described in the following sec-
tion. Next, we train one SVM for each transition type
seen in the training data. We used a cubic kernel for all
of our experiments; this kernel gives a consistent boost
over a linear kernel, while still training in a reasonable
amount of time. If we were to use Platt?s approach, the re-
sulting classifiers would be applied to further (preferably
held-out) training data to produce a set of margins, which
would be used to estimate appropriate sigmoid parame-
ters for each classifier. Sigmoid estimates that suffered
from too few positive input vectors would be replaced
by static estimates, and the sigmoids would optionally be
smoothed.
To evaluate a test set, the test input is featurized using
the same features as were used with the training data, re-
sulting in a separate vector for each word of the input.
Each classifier built during the training phase is then ap-
plied to each test vector to produce a margin. The margin
is mapped to a probability estimate using the static sig-
moid described above. When all of the probabilities have
been estimated and applied to the lattice, a Viterbi-like
algorithm is used to find the most likely path through the
lattice. This path identifies the final tag for each word of
the input sentence.
3 Features
The advantage of the ability to handle large numbers of
features is that we do not need to consider how well a
feature is likely to work in a particular language before
proposing it. We use the following features:
1. the word itself, both unchanged and lower-cased;
2. the character 3-grams and 4-grams that compose the
word;
3. the word?s capitalization pattern and digit pattern;
4. the inverse of the word?s length;
5. whether the word contains a dash;
6. whether the word is inside double quote marks;
7. the inverse of the word?s position in the sentence,
and of the position of that sentence in the document;
8. the POS, CHUNK and LEMMA features from the
training data;
9. whether the word is part of any entity, according
to a previous application of the TnT-Subcat tagger
(Brants, 2000) (see below) trained on the tag set {O,
I-ENTITY} (Test A F?=1 performance was 94.70
English and 74.33 German on this tag set); and
Run Description Test LOC MISC ORG PER Overall
1. Tnt Test A 86.67 79.60 73.04 88.54 82.90
Test B 81.28 68.98 65.71 82.84 75.54
2. Tnt + subcat Test A 91.46 81.41 80.63 91.64 87.49
Test B 85.71 68.41 73.82 87.95 80.68
3. SVM-Lattice Test A 92.14 84.86 83.70 93.73 89.63
Test B 87.09 72.81 78.84 90.40 83.92
4. SVM-Lattice+ Test A 93.75 86.02 85.90 93.91 90.85
Test B 88.77 74.19 79.00 90.67 84.67
Table 1: English evaluation results. F?=1 measures for subcategories, and overall.
Run Description Test LOC MISC ORG PER Overall
1. Tnt Test A 59.51 49.58 48.71 53.77 53.29
Test B 66.16 46.45 50.00 64.51 59.01
2. Tnt + subcat Test A 67.62 54.97 56.18 65.04 61.46
Test B 66.13 46.01 55.35 74.07 62.90
3. SVM-Lattice Test A 67.04 54.18 65.77 64.01 63.48
Test B 68.47 51.88 60.67 73.07 65.47
4. SVM-Lattice+ Test A 72.58 58.13 65.76 74.92 68.72
Test B 73.60 50.98 63.69 80.20 69.96
Table 2: German evaluation results. F?=1 measures for subcategories, and overall.
10. the maximum likelihood estimate, based on the
training data, of the word?s prior probability of being
in each class.
In some runs, we also use:
11. the tag assigned by a previous application of the
SVM-Lattice tagger, or by another tagger.
Each of these features is applied not just to the word
being featurized, but also to a range of words on either
side of it. We typically use a range of three (or, phrased
differently, a centered window of seven). We also ap-
plied some of these features to the environment of the
first occurrence of the word in the document. For ex-
ample, if the first occurrence of ?Bush? in the document
were followed by ?League,? then the second occurrence
of ?Bush? would receive the feature ?first-occurrence-is-
followed-by-league.?
Some values of the above features will be encountered
during testing but not during training. For example, a
word that occurs in the test set but not the training set will
lack a known value for the first feature in the list above.
To handle these cases, we assign any feature that appears
only once in the training data to a special ?never-before-
seen? class. This gives us examples at training time of
unseen features, which we can then train on.
Using the Shared Task English training data, this ap-
proach to featurization leads to a feature space of well
over 600,000 features, while the German data results in
over a million features. Individual vectors typically have
extent along a few hundred of these features.
There is a significant practical consideration in apply-
ing the method. The vectors produced by the featur-
izer for input to the SVM package are voluminous, lead-
ing to significant I/O costs, and slowing tag assignment.
Two methods might ameliorate this problem. First, sim-
ple compression techniques would be quite effective in
reducing file sizes, if the SVM package would support
them. Secondly, most vectors represent negative exam-
ples; a portion of these could probably be eliminated en-
tirely without significantly affecting system performance.
We have done no tuning of our feature set, preferring
to spend our time adding new features and relying on the
SVMs to ignore useless features. This is advantageous
when applying the technique to a language that we do
not understand (such as any of the world?s various non-
English languages).
4 Results
We evaluated our approach using the CoNLL-2003 En-
glish and German training and test sets, and the conll-
eval scoring software. We ran two baseline tests using
Thorsten Brants? TnT tagger (2000), and two tests of
SVM-Lattice:
1. TnT: The TnT tagger applied as distributed.
2. TnT+subcat: The TnT tagger applied to a refined
tag set. Each tag type was subcategorized into about
forty subtag types; each instance of a tag in the text
was then replaced by the appropriate subtag. For ex-
ample, a number (e.g., 221) that was part of a loca-
tion received an I-LOC-alldigits tag; a location with
an initial capital letter (e.g., Baker) received an I-
LOC-initcap tag; and one of the 30 most common
words (e.g., of) that was part of a location received a
(word-specific) I-LOC-of tag. This run served both
to calibrate the SVM-Lattice performance scores,
and to provide input for the SVM-Lattice+ run be-
low.
3. SVM-Lattice: Features 1-10 (listed above in the
Features section)
4. SVM-Lattice+: Features 1-11, using the output of
runs SVM-Lattice and TnT+subcat as input fea-
tures.
Scores for each English test are shown in Table 1; Ger-
man tests are shown in Table 2. Table 3 shows the re-
sults of the SVM-Lattice+ run in more detail. The results
show that the technique performs well, at least compared
with the baseline technique provided with the CoNLL-
2003 data (whose English Test B F?=1 measure is 59.61
English and 30.30 German).
5 Conclusion
The SVM-Lattice approach appears to give good results
without language-specific tuning; it handily outperforms
the CoNLL-2003 Shared Task baseline, and beats a basic
HMM tagger as well. Use of SVMs allows the introduc-
tion of a large number of features. These features can
be introduced with little concern for dependency among
features, and without significant knowledge of the target
language. It is likely that our results reflect some degree
of overfitting, given the large number of parameters we
use; however, we suspect this effect is not large. Thus,
the SVM-Lattice technique is particularly well suited to
language-neutral entity recognition. We expect it will
also perform well on other tasks that can be cast as tag-
ging problems, such as part-of-speech tagging and syn-
tactic chunking.
Acknowledgments
Significant theoretical and implementation contributions
were made to this work by Claudia Pearce, for which we
are grateful.
We gratefully acknowledge the provision of the
Reuters Corpus Vol. 1: English language, 1996-08-20
to 1997-08-19 by Reuters Limited.
English devel. Precision Recall F?=1
LOC 94.42% 93.09% 93.75
MISC 88.80% 83.41% 86.02
ORG 85.24% 86.58% 85.90
PER 92.79% 95.06% 93.91
overall 90.97% 90.73% 90.85
English test Precision Recall F?=1
LOC 88.22% 89.33% 88.77
MISC 74.89% 73.50% 74.19
ORG 79.31% 78.69% 79.00
PER 89.71% 91.65% 90.67
overall 84.45% 84.90% 84.67
German devel. Precision Recall F?=1
LOC 72.77% 72.40% 72.58
MISC 71.00% 49.21% 58.13
ORG 72.57% 60.11% 65.76
PER 83.70% 67.81% 74.92
overall 75.48% 63.07% 68.72
German test Precision Recall F?=1
LOC 75.08% 72.17% 73.60
MISC 63.62% 42.54% 50.98
ORG 69.20% 58.99% 63.69
PER 86.53% 74.73% 80.20
overall 75.97% 64.82% 69.96
Table 3: Results for the development and test evaluations
for the English and German tasks.
References
Thorsten Brants. 2000. TnT-A statistical part-of-speech
tagger. In Proceedings of ANLP-2000. Seattle, Wash-
ington.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In C. Burges B. Scho?lkopf and
A. Smola, editors, Support Vector Learning. MIT
Press.
John C. Platt. 1999. Probabilistic Outputs for Sup-
port Vector Machines and Comparisons to Regular-
ized Likelihood Methods. In B. Scholkopf A. Smola,
P. Bartlett and D. Schuurmans, editors, Advances in
Large Margin Classifiers. MIT Press.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 Shared Task: Language
Independent Named Entity Recognition. In Proceed-
ings of CoNLL-2003. Edmonton, Canada.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag.
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 32?35,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
KELVIN: a tool for automated knowledge base construction
Paul McNamee, James Mayfield
Johns Hopkins University
Human Language Technology Center of Excellence
Tim Finin, Tim Oates
University of Maryland
Baltimore County
Dawn Lawrie
Loyola University Maryland
Tan Xu, Douglas W. Oard
University of Maryland
College Park
Abstract
We present KELVIN, an automated system for
processing a large text corpus and distilling a
knowledge base about persons, organizations,
and locations. We have tested the KELVIN
system on several corpora, including: (a) the
TAC KBP 2012 Cold Start corpus which con-
sists of public Web pages from the University
of Pennsylvania, and (b) a subset of 26k news
articles taken from English Gigaword 5th edi-
tion.
Our NAACL HLT 2013 demonstration per-
mits a user to interact with a set of search-
able HTML pages, which are automatically
generated from the knowledge base. Each
page contains information analogous to the
semi-structured details about an entity that are
present in Wikipedia Infoboxes, along with
hyperlink citations to supporting text.
1 Introduction
The Text Analysis Conference (TAC) Knowledge
Base Population (KBP) Cold Start task1 requires
systems to take set of documents and produce a
comprehensive set of <Subject, Predicate, Object>
triples that encode relationships between and at-
tributes of the named-entities that are mentioned in
the corpus. Systems are evaluated based on the fi-
delity of the constructed knowledge base. For the
2012 evaluation, a fixed schema of 42 relations (or
slots), and their logical inverses was provided, for
example:
? X:Organization employs Y:Person
1See details at http://www.nist.gov/tac/2012/
KBP/task_guidelines/index.html
? X:Person has-job-title title
? X:Organization headquartered-in Y:Location
Multiple layers of NLP software are required for
this undertaking, including at the least: detection of
named-entities, intra-document co-reference resolu-
tion, relation extraction, and entity disambiguation.
To help prevent a bias towards learning about
prominent entities at the expense of generality,
KELVIN refrains from mining facts from sources
such as documents obtained through Web search,
Wikipedia2, or DBpedia.3 Only facts that are as-
serted in and gleaned from the source documents are
posited.
Other systems that create large-scale knowledge
bases from general text include the Never-Ending
Language Learning (NELL) system at Carnegie
Mellon University (Carlson et al, 2010), and the
TextRunner system developed at the University of
Washington (Etzioni et al, 2008).
2 Washington Post KB
No gold-standard KBs were available to us to assist
during the development of KELVIN, so we relied on
qualitative assessment to gauge the effectiveness of
our extracted relations ? by manually examining ten
random samples for each relations, we ascertained
that most relations were between 30-80% accurate.
Although the TAC KBP 2012 Cold Start task was a
pilot evaluation of a new task using a novel evalua-
tion methodology, the KELVIN system did attain the
highest reported F1 scores.4
2http://en.wikipedia.org/
3http://www.dbpedia.org/
40.497 0-hop & 0.363 all-hops, as reported in the prelimi-
nary TAC 2012 Evaluation Results.
32
During our initial development we worked with
a 26,143 document collection of 2010 Washington
Post articles and the system discovered 194,059 re-
lations about 57,847 named entities. KELVIN learns
some interesting, but rather dubious relations from
the Washington Post articles5
? Sen. Harry Reid is an employee of the ?Repub-
lican Party.? Sen. Reid is also an employee of
the ?Democratic Party.?
? Big Foot is an employee of Starbucks.
? MacBook Air is a subsidiary of Apple Inc.
? Jill Biden is married to Jill Biden.
However, KELVIN also learns quite a number of
correct facts, including:
? Warren Buffett owns shares of Berkshire Hath-
away, Burlington Northern Santa Fe, the Wash-
ington Post Co., and four other stocks.
? Jared Fogle is an employee of Subway.
? Freeman Hrabowski works for UMBC,
founded the Meyerhoff Scholars Program, and
graduated from Hampton University and the
University of Illinois.
? Supreme Court Justice Elena Kagan attended
Oxford, Harvard, and Princeton.
? Southwest Airlines is headquartered in Texas.
? Ian Soboroff is a computer scientist6 employed
by NIST.7
3 Pipeline Components
3.1 SERIF
BBN?s SERIF tool8 (Boschee et al, 2005) provides
a considerable suite of document annotations that
are an excellent basis for building a knowledge base.
The functions SERIF can provide are based largely
5All 2010 Washington Post articles from English Gigaword
5th ed. (LDC2011T07).
6Ian is the sole computer scientist discovered in processing
a year of news. In contrast, KELVIN found 52 lobbyists.
7From Washington Post article (WPB ENG 20100506.0012
in LDC2011T07).
8Statistical Entity & Relation Information Finding.
Slotname Count
per:employee of 60,690
org:employees 44,663
gpe:employees 16,027
per:member of 14,613
org:membership 14,613
org:city of headquarters 12,598
gpe:headquarters in city 12,598
org:parents 6,526
org:country of headquarters 4,503
gpe:headquarters in country 4,503
Table 1: Most prevalent slots extracted by SERIF from
the Washington Post texts.
Slotname Count
per:title 44,896
per:employee of 39,101
per:member of 20,735
per:countries of residence 8,192
per:origin 4,187
per:statesorprovinces of residence 3,376
per:cities of residence 3,376
per:country of birth 1,577
per:age 1,233
per:spouse 1,057
Table 2: Most prevalent slots extracted by FACETS from
the Washington Post texts.
on the NIST ACE specification,9 and include: (a)
identifying named-entities and classifying them by
type and subtype; (b) performing intra-document
co-reference analysis, including named mentions,
as well as co-referential nominal and pronominal
mentions; (c) parsing sentences and extracting intra-
sentential relations between entities; and, (d) detect-
ing certain types of events.
In Table 1 we list the most common slots SERIF
extracts from the Washington Post articles.
3.2 FACETS
FACETS, another BBN tool, is an add-on pack-
age that takes SERIF output and produces role and
argument annotations about person noun phrases.
FACETS is implemented using a conditional-
9The principal types of ACE named-entities are per-
sons, organizations, and geo-political entities (GPEs).
GPEs are inhabited locations with a government. See
http://www.itl.nist.gov/iad/mig/tests/ace/
2008/doc/ace08-evalplan.v1.2d.pdf.
33
Figure 1: Simple rendering of KB page about former Florida congressman Joe Scarborough. Many facts are correct
? he lived in and was employed by the State of Florida; he has a brother George; he was a member of the Republican
House of Representatives; and, he is employed by MSNBC.
exponential learner trained on broadcast news. The
attributes FACETS can recognize include general at-
tributes like religion and age (which anyone might
have), as well as role-specific attributes, such as
medical specialty for physicians, or academic insti-
tution for someone associated with an university.
In Table 2 we report the most prevalent slots
FACETS extracts from the Washington Post.10
3.3 CUNY toolkit
To increase our coverage of relations we also in-
tegrated the KBP Slot Filling Toolkit (Chen et al,
2011) developed at the CUNY BLENDER Lab.
Given that the KBP toolkit was designed for the tra-
ditional slot filling task at TAC, this primarily in-
volved creating the queries that the tool expected as
input and parallelizing the toolkit to handle the vast
number of queries issued in the cold start scenarios.
To informally gauge the accuracy of slots
extracted from the CUNY tool, some coarse as-
sessment was done over a small collection of 807
New York Times articles that include the string
?University of Kansas.? From this collection, 4264
slots were identified. Nine different types of slots
were filled in order of frequency: per:title (37%),
per:employee of (23%), per:cities of residence
(17%), per:stateorprovinces of residence (6%),
10Note FACETS can independently extract some slots that
SERIF is capable of discovering (e.g., employment relations).
org:top members/employees (6%), org:member of
(6%), per:countries of residence (2%), per:spouse
(2%), and per:member of (1%). We randomly sam-
pled 10 slot-fills of each type, and found accuracy
to vary from 20-70%.
3.4 Coreference
We used two methods for entity coreference. Un-
der the theory that name ambiguity may not be a
huge problem, we adopted a baseline approach of
merging entities across different documents if their
canonical mentions were an exact string match af-
ter some basic normalizations, such as removing
punctuation and conversion to lower-case charac-
ters. However we also used the JHU HLTCOE
CALE system (Stoyanov et al, 2012), which maps
named-entity mentions to the TAC-KBP reference
KB, which was derived from a 2008 snapshot of En-
glish Wikipedia. For entities that are not found in the
KB, we reverted to exact string match. CALE entity
linking proved to be the more effective approach for
the Cold Start task.
3.5 Timex2 Normalization
SERIF recognizes, but does not normalize, temporal
expressions, so we used the Stanford SUTime pack-
age, to normalize date values.
34
Figure 2: Supporting text for some assertions about Mr. Scarborough. Source documents are also viewable by
following hyperlinks.
3.6 Lightweight Inference
We performed a small amount of light inference to
fill some slots. For example, if we identified that
a person P worked for organization O, and we also
extracted a job title T for P, and if T matched a set
of titles such as president or minister we asserted
that the tuple <O, org:top members employees, P>
relation also held.
4 Ongoing Work
There are a number of improvements that we are un-
dertaking, including: scaling to much larger corpora,
detecting contradictions, expanding the use of infer-
ence, exploiting the confidence of extracted infor-
mation, and applying KELVIN to various genres of
text.
5 Script Outline
The KB generated by KELVIN is best explored us-
ing a Wikipedia metaphor. Thus our demonstration
consists of a web browser that starts with a list of
moderately prominent named-entities that the user
can choose to examine (e.g., investor Warren Buf-
fett, Supreme Court Justice Elena Kagan, Southwest
Airlines Co., the state of Florida). Selecting any
entity takes one to a page displaying its known at-
tributes and relations, with links to documents that
serve as provenance for each assertion. On every
page, each entity is hyperlinked to its own canon-
ical page; therefore the user is able to browse the
KB much as one browses Wikipedia by simply fol-
lowing links. A sample generated page is shown in
Figure 1 and text that supports some of the learned
assertions in the figure is shown in Figure 2. We
also provide a search interface to support jump-
ing to a desired entity and can demonstrate access-
ing the data encoded in the semantic web language
RDF (World Wide Web Consortium, 2013), which
supports ontology browsing and executing complex
SPARQL queries (Prud?Hommeaux and Seaborne,
2008) such as ?List the employers of people living
in Nebraska or Kansas who are older than 40.?
References
E. Boschee, R. Weischedel, and A. Zamanian. 2005. Au-
tomatic information extraction. In Proceedings of the
2005 International Conference on Intelligence Analy-
sis, McLean, VA, pages 2?4.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010).
Z. Chen, S. Tamang, A. Lee, X. Li, and H. Ji. 2011.
Knowledge Base Population (KBP) Toolkit @ CUNY
BLENDER LAB Manual.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extraction
from the web. Commun. ACM, 51(12):68?74, Decem-
ber.
E Prud?Hommeaux and A. Seaborne. 2008. SPARQL
query language for RDF. Technical report, World
Wide Web Consortium, January.
Veselin Stoyanov, James Mayfield, Tan Xu, Douglas W.
Oard, Dawn Lawrie, Tim Oates, and Tim Finin. 2012.
A context-aware approach to entity linking. In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge Ex-
traction, AKBC-WEKEX ?12, pages 62?67, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
World Wide Web Consortium. 2013. Resource Descrip-
tion Framework Specification. ?http://http://
www.w3.org/RDF/. ?[Online; accessed 8 April,
2013]?.
35
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 44?52, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UMBC EBIQUITY-CORE: Semantic Textual Similarity Systems
Lushan Han, Abhay Kashyap, Tim Finin
Computer Science and
Electrical Engineering
University of Maryland, Baltimore County
Baltimore MD 21250
{lushan1,abhay1,finin}@umbc.edu
James Mayfield and Jonathan Weese
Human Language Technology
Center of Excellence
Johns Hopkins University
Baltimore MD 21211
mayfield@jhu.edu, jonny@cs.jhu.edu
Abstract
We describe three semantic text similarity
systems developed for the *SEM 2013 STS
shared task and the results of the correspond-
ing three runs. All of them shared a word sim-
ilarity feature that combined LSA word sim-
ilarity and WordNet knowledge. The first,
which achieved the best mean score of the 89
submitted runs, used a simple term alignment
algorithm augmented with penalty terms. The
other two runs, ranked second and fourth, used
support vector regression models to combine
larger sets of features.
1 Introduction
Measuring semantic text similarity has been a re-
search subject in natural language processing, infor-
mation retrieval and artificial intelligence for many
years. Previous efforts have focused on compar-
ing two long texts (e.g., for document classification)
or a short text with a long text (e.g., Web search),
but there are a growing number of tasks requiring
computing the semantic similarity between two sen-
tences or other short text sequences. They include
paraphrase recognition (Dolan et al, 2004), Twitter
tweets search (Sriram et al, 2010), image retrieval
by captions (Coelho et al, 2004), query reformula-
tion (Metzler et al, 2007), automatic machine trans-
lation evaluation (Kauchak and Barzilay, 2006) and
schema matching (Han et al, 2012).
There are three predominant approaches to com-
puting short text similarity. The first uses informa-
tion retrieval?s vector space model (Meadow, 1992)
in which each text is modeled as a ?bag of words?
and represented using a vector. The similarity be-
tween two texts is then computed as the cosine
similarity of the vectors. A variation on this ap-
proach leverages web search results (e.g., snip-
pets) to provide context for the short texts and en-
rich their vectors using the words in the snippets
(Sahami and Heilman, 2006). The second approach
is based on the assumption that if two sentences or
other short text sequences are semantically equiva-
lent, we should be able to align their words or ex-
pressions. The alignment quality can serve as a
similarity measure. This technique typically pairs
words from the two texts by maximizing the sum-
mation of the word similarity of the resulting pairs
(Mihalcea et al, 2006). The third approach com-
bines different measures and features using machine
learning models. Lexical, semantic and syntactic
features are computed for the texts using a variety
of resources and supplied to a classifier, which then
assigns weights to the features by fitting the model
to training data (Saric et al, 2012).
For evaluating different approaches, the 2013 Se-
mantic Textual Similarity (STS) task asked auto-
matic systems to compute sentence similarity ac-
cording to a scale definition ranging from 0 to 5,
with 0 meaning unrelated and 5 semantically equiv-
alent (Agirre et al, 2012; Agirre et al, 2013). The
example sentence pair ?The woman is playing the
violin? and ?The young lady enjoys listening to the
guitar? is scored as only 1 and the pair ?The bird is
bathing in the sink? and ?Birdie is washing itself in
the water basin? is given a score of 5.
The vector-space approach tends to be too shallow
for the task, since solving it well requires discrimi-
nating word-level semantic differences and goes be-
44
yond simply comparing sentence topics or contexts.
Our first run uses an align-and-penalize algorithm,
which extends the second approach by giving penal-
ties to the words that are poorly aligned. Our other
two runs use a support vector regression model to
combine a large number of general and domain spe-
cific features. An important and fundamental feature
used by all three runs is a powerful semantic word
similarity model based on a combination of Latent
Semantic Analysis (LSA) (Deerwester et al, 1990;
Landauer and Dumais, 1997) and knowledge from
WordNet (Miller, 1995).
The remainder of the paper proceeds as follows.
Section 2 presents the hybrid word similarity model.
Section 3 describes the align-and-penalize approach
used for the PairingWords run. In Section 4 we de-
scribe the SVM approach used for the Galactus and
Saiyan runs. Section 5 discusses the results and is
followed by a short conclusion.
2 Semantic Word Similarity Model
Our word similarity model was originally developed
for the Graph of Relations project (UMBC, 2013a)
which maps informal queries with English words
and phrases for an RDF linked data collection into
a SPARQL query. For this, we wanted a metric
in which only the semantics of a word is consid-
ered and not its lexical category. For example, the
verb ?marry? should be semantically similar to the
noun ?wife?. Another desiderata was that the met-
ric should give highest scores and lowest scores in
its range to similar and non-similar words, respec-
tively. In this section, we describe how we con-
structed the model by combining LSA word simi-
larity and WordNet knowledge.
2.1 LSA Word Similarity
LSA Word Similarity relies on the distributional hy-
pothesis that words occurring in the same contexts
tend to have similar meanings (Harris, 1968).
2.1.1 Corpus Selection and Processing
In order to produce a reliable word co-occurrence
statistics, a very large and balanced text corpus is
required. After experimenting with several cor-
pus choices including Wikipedia, Project Gutenberg
e-Books (Hart, 1997), ukWaC (Baroni et al, 2009),
Reuters News stories (Rose et al, 2002) and LDC
gigawords, we selected the Web corpus from the
Stanford WebBase project (Stanford, 2001). We
used the February 2007 crawl, which is one of the
largest collections and contains 100 million web
pages from more than 50,000 websites. The Web-
Base project did an excellent job in extracting tex-
tual content from HTML tags but still has abun-
dant text duplications, truncated text, non-English
text and strange characters. We processed the collec-
tion to remove undesired sections and produce high
quality English paragraphs. We detected paragraphs
using heuristic rules and only retrained those whose
length was at least two hundred characters. We elim-
inated non-English text by checking the first twenty
words of a paragraph to see if they were valid En-
glish words. We used the percentage of punctuation
characters in a paragraph as a simple check for typi-
cal text. We removed duplicated paragraphs using a
hash table. Finally, we obtained a three billion words
corpus of good quality English, which is available at
(Han and Finin, 2013).
2.1.2 Word Co-Occurrence Generation
We performed POS tagging and lemmatization on
the WebBase corpus using the Stanford POS tagger
(Toutanova et al, 2000). Word/term co-occurrences
are counted in a moving window of a fixed size
that scans the entire corpus1. We generated two co-
occurrence models using window sizes ?1 and ?4
because we observed different natures of the models.
?1 window produces a context similar to the depen-
dency context used in (Lin, 1998a). It provides a
more precise context but only works for comparing
words within the same POS. In contrast, a context
window of ?4 words allows us to compute semantic
similarity between words with different POS.
Our word co-occurrence models were based on
a predefined vocabulary of more than 22,000 com-
mon English words and noun phrases. We also
added to it more than 2,000 verb phrases extracted
from WordNet. The final dimensions of our word
co-occurrence matrices are 29,000 ? 29,000 when
words are POS tagged. Our vocabulary includes
only open-class words (i.e. nouns, verbs, adjectives
and adverbs). There are no proper nouns in the vo-
cabulary with the only exception of country names.
1We used a stop-word list consisting of only the three arti-
cles ?a?, ?an? and ?the?.
45
Word Pair ?4 model ?1 model
1. doctor NN, physician NN 0.775 0.726
2. car NN, vehicle NN 0.748 0.802
3. person NN, car NN 0.038 0.024
4. car NN, country NN 0.000 0.016
5. person NN, country NN 0.031 0.069
6. child NN, marry VB 0.098 0.000
7. wife NN, marry VB 0.548 0.274
8. author NN, write VB 0.364 0.128
9. doctor NN, hospital NN 0.473 0.347
10. car NN, driver NN 0.497 0.281
Table 1: Ten examples from the LSA similarity model
2.1.3 SVD Transformation
Singular Value Decomposition (SVD) has been
found to be effective in improving word similar-
ity measures (Landauer and Dumais, 1997). SVD
is typically applied to a word by document ma-
trix, yielding the familiar LSA technique. In
our case we apply it to our word by word ma-
trix. In literature, this variation of LSA is some-
times called HAL (Hyperspace Analog to Lan-
guage) (Burgess et al, 1998).
Before performing SVD, we transform the raw
word co-occurrence count fij to its log frequency
log(fij + 1). We select the 300 largest singular val-
ues and reduce the 29K word vectors to 300 dimen-
sions. The LSA similarity between two words is de-
fined as the cosine similarity of their corresponding
word vectors after the SVD transformation.
2.1.4 LSA Similarity Examples
Ten examples obtained using LSA similarity are
given in Table 1. Examples 1 to 6 illustrate that the
metric has a good property of differentiating simi-
lar words from non-similar words. Examples 7 and
8 show that the ?4 model can detect semantically
similar words even with different POS while the ?1
model yields much worse performance. Example 9
and 10 show that highly related but not substitutable
words can also have a strong similarity but the ?1
model has a better performance in discriminating
them. We call the ?1 model and the ?4 model
as concept similarity and relation similarity respec-
tively since the ?1 model has a good performance
on nouns and the ?4 model is good at computing
similarity between relations regardless of POS of
words, such as ?marry to? and ?is the wife of?.
2.2 Combining with WordNet Knowledge
Statistical word similarity measures have limita-
tions. Related words can have similarity scores as
high as what similar words get, as illustrated by
?doctor? and ?hospital? in Table 1. Word similar-
ity is typically low for synonyms having many word
senses since information about different senses are
mashed together (Han et al, 2013). By using Word-
Net, we can reduce the above issues.
2.2.1 Boosting LSA similarity using WordNet
We increase the similarity between two words if any
of the following relations hold.
? They are in the same WordNet synset.
? One word is the direct hypernym of the other.
? One word is the two-link indirect hypernym of
the other.
? One adjective has a direct similar to relation
with the other.
? One adjective has a two-link indirect similar to
relation with the other.
? One word is a derivationally related form of the
other.
? One word is the head of the gloss of the other
or its direct hypernym or one of its direct hy-
ponyms.
? One word appears frequently in the glosses of
the other and its direct hypernym and its direct
hyponyms.
We use the algorithm described in (Collins, 1999)
to find a word gloss header. We require a minimum
LSA similarity of 0.1 between the two words to filter
out noisy data when extracting WordNet relations.
We define a word?s ?significant senses? to deal
with the problem of WordNet trivial senses. The
word ?year?, for example, has a sense ?a body of
students who graduate together? which makes it a
synonym of the word ?class?. This causes problems
because ?year? and ?class? are not similar, in gen-
eral. A sense is significant, if any of the following
conditions are met: (i) it is the first sense; (ii) its
WordNet frequency count is not less than five; or
(iii) its word form appears first in its synset?s word
46
form list and it has a WordNet sense number less
than eight.
We assign path distance of zero to the category
1, path distance of one to the category 2, 4 and 6,
and path distance of two to the other categories. The
new similarity between word x and y by combining
LSA similarity and WordNet relations is shown in
the following equation
sim?(x, y) = simLSA(x, y) + 0.5e??D(x,y) (1)
where D(x, y) is the minimal path distance between
x and y. Using the e??D(x,y) to transform simple
shortest path length has been demonstrated to be
very effective according to (Li et al, 2003). The pa-
rameter ? is set to be 0.25, following their experi-
mental results. The ceiling of sim?(x, y) remains
1.0 and we simply cut the excess.
2.2.2 Dealing with words of many senses
For a word w with many WordNet senses (currently
ten or more), we use its synonyms with fewer senses
(at most one third of that of w) as its substitutions in
computing similarity with another word. Let Sx and
Sy be the sets of all such substitutions of the words
x and y respectively. The new similarity is obtained
using Equation 2.
sim(x, y) = max( max
sx?Sx?{x}
sim?(sx, y),
max
sy?Sy?{y}
sim?(x, sy)) (2)
An online demonstration of a similar model
developed for the GOR project is available
(UMBC, 2013b), but it lacks some of this version?s
features.
3 Align-and-Penalize Approach
First we hypothesize that STS similarity between
two sentences can be computed using
STS = T ? P ? ? P ?? (3)
where T is the term alignments score, P ? is the
penalty for bad term alignments and P ?? is the
penalty for syntactic contradictions led by the align-
ments. However P ?? had not been fully implemented
and was not used in our STS submissions. We show
it here just for completeness.
3.1 Aligning terms in two sentences
We start by applying the Stanford POS tagger to tag
and lemmatize the input sentences. We use our pre-
defined vocabulary, POS tagging data and simple
regular expressions to recognize multi-word terms
including noun and verb phrases, proper nouns,
numbers and time. We ignore adverbs with fre-
quency count larger than 500, 000 in our corpus and
stop words with general meaning.
Equation 4 shows our aligning function g which
finds the counterpart of term t ? S in sentence S?.
g(t) = argmax
t??S?
sim?(t, t?) (4)
sim?(t, t?) is a wrapper function over sim(x, y) in
Equation 2 that uses the relation similarity model.
It compares numerical and time terms by their val-
ues. If they are equal, 1 is returned; otherwise 0.
sim?(t, t?) provides limited comparison over pro-
nouns. It returns 1 between subject pronouns I, we,
they, he, she and their corresponding object pro-
nouns. sim?(t, t?) also outputs 1 if one term is the
acronym of the other term, or if one term is the head
of the other term, or if two consecutive terms in a
sentence match a single term in the other sentence
(e.g. ?long term? and ?long-term?). sim?(t, t?) fur-
ther adds support for matching words2 not presented
in our vocabulary using a simple string similarity al-
gorithm. It computes character bigram sets for each
of the two words without using padding characters.
Dice coefficient is then applied to get the degree of
overlap between the two sets. If it is larger than two
thirds, sim?(t, t?) returns a score of 1; otherwise 0.
g(t) is direction-dependent and does not achieve
one-to-one mapping. This property is useful in mea-
suring STS similarity because two sentences are of-
ten not exact paraphrases of one another. Moreover,
it is often necessary to align multiple terms in one
sentence to a single term in the other sentence, such
as when dealing with repetitions and anaphora or,
e.g., mapping ?people writing books? to ?writers?.
Let S1 and S2 be the sets of terms in two input
sentences. We define term alignments score T as the
following equation shows.
?
t?S1 sim
?(t, g(t))
2 ? |S1|
+
?
t?S2 sim
?(t, g(t))
2 ? |S2|
(5)
2We use the regular expression ?[A-Za-z][A-Za-z]*? to
identify them.
47
3.2 Penalizing bad term alignments
We currently treat two kinds of alignments as ?bad?,
as described in Equation 6. For the set Bi, we have
an additional restriction that neither of the sentences
has the form of a negation. In defining Bi, we used
a collection of antonyms extracted from WordNet
(Mohammad et al, 2008). Antonym pairs are a spe-
cial case of disjoint sets. The terms ?piano? and ?vi-
olin? are also disjoint but they are not antonyms. In
order to broaden the set Bi we will need to develop
a model that can determine when two terms belong
to disjoint sets.
Ai =
{
?t, g(t)? |t ? Si ? sim?(t, g(t)) < 0.05
}
Bi = {?t, g(t)? |t ? Si ? t is an antonymof g(t)}
i ? {1, 2} (6)
We show how we compute P ? in Equation 7.
PAi =
?
?t,g(t)??Ai (sim
?(t, g(t)) +wf (t) ? wp(t))
2 ? |Si|
PBi =
?
?t,g(t)??Bi (sim
?(t, g(t)) + 0.5)
2 ? |Si|
P ? = PA1 + PB1 + PA2 + PB2 (7)
The wf (t) and wp(t) terms are two weighting func-
tions on the term t. wf (t) inversely weights the log
frequency of term t and wp(t) weights t by its part of
speech tag, assigning 1.0 to verbs, nouns, pronouns
and numbers, and 0.5 to terms with other POS tags.
4 SVM approach
We used the scores from the align-and-penalize ap-
proach along with several other features to learn a
support vector regression model. We started by ap-
plying the following preprocessing steps.
? The sentences were tokenized and POS-tagged
using NLTK?s (Bird, 2006) default Penn Tree-
bank based tagger.
? Punctuation characters were removed from the
tokens except for the decimal point in numbers.
? All numbers written as words were converted
into numerals, e.g., ?2.2 million? was replaced
by ?2200000? and ?fifty six? by ?56?.
? All mentions of time were converted into mil-
itary time, e.g., ?5:40pm? was replaced by
?1740? and ?1h30am? by ?0130?.
? Abbreviations were expanded using a compiled
list of commonly used abbreviations.
? About 80 stopwords were removed.
4.1 Ngram Matching
The sentence similarities are derived as a function of
the similarity scores of their corresponding paired
word ngrams. These features closely resemble the
ones used in (Saric et al, 2012). For our system, we
used unigrams, bigrams, trigrams and skip-bigrams,
a special form of bigrams which allow for arbitrary
distance between two tokens.
An ngram from the first sentence is exclusively
paired with an ngram from the second which has the
highest similarity score. Several similarity metrics
are used to generate different features. For bigrams,
trigrams and skip-bigrams, the similarity score for
two ngrams is computed as the arithmetic mean of
the similarity scores of the individual words they
contain. For example, for the bigrams ?he ate? and
?she spoke?, the similarity score is the average of the
similarity scores between the words ?he? and ?she?
and the words ?ate? and ?spoke?.
The ngram overlap of two sentences is defined
as ?the harmonic mean of the degree to which
the second sentence covers the first and the de-
gree to which the first sentence covers the second?
(Saric et al, 2012). Given sets S1 and S2 containing
ngrams from sentences 1 and 2, and sets P1 and P2
containing their paired ngrams along with their sim-
ilarity scores, the ngram overlap score for a given
ngram type is computed using the following equa-
tion.
HM
(
?
n?P1 w(n).sim(n)
?
n?S1 w(n)
,
?
n?P2 w(n).sim(n)
?
n?S2 w(n)
)
(8)
In this formula, HM is the harmonic mean, w(n) is
the weight assigned for the given ngram and sim(n)
is the similarity score of the paired word.
By default, all the ngrams are assigned a uniform
weight of 1. But since different words carry differ-
ent amount of information, e.g. ?acclimatize? vs.
?take?, ?cardiologist? vs. ?person?, we also use in-
formation content as weights. The information con-
tent of a word is as defined in (Saric et al, 2012).
ic(w) = ln
(
?
w??C freq(w
?
)
freq(w)
)
(9)
48
Here C is the set of words in the corpus and freq(w)
is the frequency of a word in the corpus. The
weight of an ngram is the sum of its constituent word
weights. We use refined versions of Google ngram
frequencies (Michel et al, 2011) from (Mem, 2008)
and (Saric et al, 2012) to get the information con-
tent of the words. Words not in this list are assigned
the average weight.
We used several word similarity metrics for
ngram matching apart from the similarity metric de-
scribed in section 2. Our baseline similarity metric
was an exact string match which assigned a score
of 1 if two tokens contained the same sequence of
characters and 0 otherwise. We also used NLTK?s
library to compute WordNet based similarity mea-
sures such as Path Distance Similarity, Wu-Palmer
Similarity (Wu and Palmer, 1994) and Lin Similar-
ity (Lin, 1998b). For Lin Similarity, the Semcor cor-
pus was used for the information content of words.
4.2 Contrast Scores
We computed contrast scores between two sen-
tences using three different lists of antonym pairs
(Mohammad et al, 2008). We used a large list con-
taining 3.5 million antonym pairs, a list of about
22,000 antonym pairs from Wordnet and a list of
50,000 pairs of words with their degree of contrast.
Contrast scores between two sentences were derived
as a function of the number of antonym pairs be-
tween them similar to equation 8 but with negative
values to indicate contrast scores.
4.3 Features
We constructed 52 features from different combina-
tions of similarity metrics, their parameters, ngram
types (unigram, bigram, trigram and skip-bigram)
and ngram weights (equal weight vs. information
content) for all sentence pairs in the training data.
? We used scores from the align-and-penalize ap-
proach directly as a feature.
? Using exact string match over different ngram
types and ngram weights, we extracted eight
features (4 ? 4). We also developed four addi-
tional features (2 ? 2) by includin stopwords in
bigrams and trigrams, motivated by the nature
of MSRvid dataset.
? We used the LSA boosted similarity metric in
three modes: concept similarity, relation simi-
larity and mixed mode, which used the concept
model for nouns and relation model for verbs,
adverbs and adjectives. A total of 24 features
were extracted (4 ? 2 ? 3).
? For Wordnet-based similarity measures, we
used uniform weights for Path and Wu-Palmer
similarity and used the information content of
words (derived from the Semcor corpus) for
Lin similarity. Skip bigrams were ignored and
a total of nine features were produced (3 ? 3).
? Contrast scores used three different lists of
antonym pairs. A total of six features were ex-
tracted using different weight values (3 ? 2).
4.4 Support Vector Regression
The features described in 4.3 were used in dif-
ferent combinations to train several support vec-
tor regression (SVR) models. We used LIBSVM
(Chang and Lin, 2011) to learn the SVR models and
ran a grid search provided by (Saric et al, 2012) to
find the optimal values for the parameters C , g and
p. These models were then used to predict the scores
for the test sets.
The Galactus system was trained on all of STS
2012 data and used the full set of 52 features. The
FnWN dataset was handled slightly differently from
the others. We observed that terms like ?frame? and
?entity? were used frequently in the five sample sen-
tence pairs and treated them as stopwords. To ac-
commodate the vast difference in sentence lengths,
equation 8 was modified to compute the arithmetic
mean instead of the harmonic mean.
The Saiyan system employed data-specific train-
ing and features. The training sets were subsets of
the supplied STS 2012 dataset. More specifically,
the model for headlines was trained on 3000 sen-
tence pairs from MSRvid and MSRpar, SMT used
1500 sentence pairs from SMT europarl and SMT
news, while OnWN used only the 750 OnWN sen-
tence pairs from last year. The FnWN scores were
directly used from the Align-and-Penalize approach.
None of the models for Saiyan used contrast fea-
tures and the model for SMT also ignored similarity
scores from exact string match metric.
49
5 Results and discussion
Table 2 presents the official results of our three runs
in the 2013 STS task. Each entry gives a run?s Pear-
son correlation on a dataset as well as the rank of the
run among all 89 runs submitted by the 35 teams.
The last row shows the mean of the correlations and
the overall ranks of our three runs.
We tested performance of the align-and-penalize
approach on all of the 2012 STS datasets. It ob-
tained correlation values of 0.819 on MSRvid, 0.669
on MSRpar, 0.553 on SMTeuroparl, 0.567 on SMT-
news and 0.722 on OnWN for the test datasets, and
correlation values of 0.814 on MSRvid, 0.707 on
MSRpar and 0.646 on SMTeuroparl for the training
datasets. The performance of the approach without
using the antonym penalty is also tested, producing
correlation scores of 0.795 on MSRvid, 0.667 on
MSRpar, 0.554 on SMTeuroparl, 0.566 on SMTnew
and 0.727 on OnWN, for the test datasets, and 0.794
on MSRvid, 0.707 on MSRpar and 0.651 on SM-
Teuroparl for the training datasets. The average of
the correlation scores on all eight datasets with and
without the antonym penalty is 0.6871 and 0.6826,
respectively. Since the approach?s performance was
only slightly improved when the antonym penalty
was used, we decided to not include this penalty in
our PairingWords run in the hope that its simplicity
would make it more robust.
During development, our SVM approach
achieved correlations of 0.875 for MSRvid, 0.699
for MSRpar, 0.559 for SMTeuroparl, 0.625 for
SMTnews and 0.729 for OnWN on the 2012 STS
test data. Models were trained on their respective
training sets while SMTnews used SMTeuroparl and
OnWN used all the training sets. We experimented
with different features and training data to study
their influence on the performance of the models.
We found that the unigram overlap feature, based on
boosted LSA similarity and weighted by informa-
tion content, could independently achieve very high
correlations. Including more features improved the
accuracy slightly and in some cases added noise.
The difficulty in selecting data specific features and
training for novel datasets is indicated by Saiyan?s
contrasting performance on headlines and OnWN
datasets. The model used for Headlines was trained
on data from seemingly different domains (MSRvid,
Dataset Pairing Galactus Saiyan
Headlines (750 pairs) 0.7642 (3) 0.7428 (7) 0.7838 (1)
OnWN (561 pairs) 0.7529 (5) 0.7053 (12) 0.5593 (36)
FNWN (189 pairs) 0.5818 (1) 0.5444 (3) 0.5815 (2)
SMT (750 pairs) 0.3804 (8) 0.3705 (11) 0.3563 (16)
Weighted mean 0.6181 (1) 0.5927 (2) 0.5683 (4)
Table 2: Performance of our three systems on the four
test sets.
MSRpar) while OnWN was trained only on OnWN
from STS 2012. When the model for headlines
dataset was used to predict the scores for OnWN,
the correlation jumped from 0.55 to 0.71 indicating
that the earlier model suffered from overfitting.
Overfitting is not evident in the performance of
PairingWords and Galactus, which have more con-
sistent performance over all datasets. The relatively
simple PairingWords system has two advantages: it
is faster, since the current Galactus requires comput-
ing a large number of features; and its performance
is more predictable, since training is not needed thus
eliminating noise induced from diverse training sets.
6 Conclusion
We described three semantic text similarity systems
developed for the *SEM 2013 STS shared task and
the results of the corresponding three runs we sub-
mitted. All of the systems used a lexical similarity
feature that combined POS tagging, LSA word sim-
ilarity and WordNet knowledge.
The first run, which achieved the best mean score
out of all 89 submissions, used a simple term align-
ment algorithm augmented with two penalty met-
rics. The other two runs, ranked second and fourth
out of all submissions, used support vector regres-
sion models based on a set of more than 50 addi-
tional features. The runs differed in their feature
sets, training data and procedures, and parameter
settings.
Acknowledgments
This research was supported by AFOSR award
FA9550-08-1-0265 and a gift from Microsoft.
50
References
[Agirre et al2012] Eneko Agirre, Mona Diab, Daniel Cer,
and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task
6: a pilot on semantic textual similarity. In Proceed-
ings of the First Joint Conference on Lexical and Com-
putational Semantics, pages 385?393. Association for
Computational Linguistics.
[Agirre et al2013] Eneko Agirre, Daniel Cer, Mona Diab,
Aitor Gonzalez-Agirre, and Weiwei Guo. 2013. *sem
2013 shared task: Semantic textual similarity, includ-
ing a pilot on typed-similarity. In *SEM 2013: The
Second Joint Conference on Lexical and Computa-
tional Semantics. Association for Computational Lin-
guistics.
[Baroni et al2009] M. Baroni, S. Bernardini, A. Fer-
raresi, and E. Zanchetta. 2009. The wacky wide
web: A collection of very large linguistically pro-
cessed web-crawled corpora. Language Resources
and Evaluation, 43(3):209?226.
[Bird2006] Steven Bird. 2006. Nltk: the natural lan-
guage toolkit. In Proceedings of the COLING/ACL on
Interactive presentation sessions, COLING-ACL ?06,
pages 69?72, Stroudsburg, PA, USA. Association for
Computational Linguistics.
[Burgess et al1998] C. Burgess, K. Livesay, and K. Lund.
1998. Explorations in context space: Words, sen-
tences, discourse. Discourse Processes, 25:211?257.
[Chang and Lin2011] Chih-Chung Chang and Chih-Jen
Lin. 2011. LIBSVM: A library for support vector ma-
chines. ACM Transactions on Intelligent Systems and
Technology, 2:27:1?27:27.
[Coelho et al2004] T.A.S. Coelho, Pa?vel Pereira Calado,
Lamarque Vieira Souza, Berthier Ribeiro-Neto, and
Richard Muntz. 2004. Image retrieval using multiple
evidence ranking. IEEE Trans. on Knowl. and Data
Eng., 16(4):408?417.
[Collins1999] Michael John Collins. 1999. Head-driven
statistical models for natural language parsing. Ph.D.
thesis, University of Pennsylvania.
[Deerwester et al1990] Scott Deerwester, Susan T. Du-
mais, George W. Furnas, Thomas K. Landauer, and
Richard Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41(6):391?407.
[Dolan et al2004] Bill Dolan, Chris Quirk, and Chris
Brockett. 2004. Unsupervised construction of
large paraphrase corpora: exploiting massively paral-
lel news sources. In Proceedings of the 20th interna-
tional conference on Computational Linguistics, COL-
ING ?04. Association for Computational Linguistics.
[Ganitkevitch et al2013] Juri Ganitkevitch, Ben-
jamin Van Durme, and Chris Callison-Burch. 2013.
PPDB: The paraphrase database. In HLT-NAACL
2013.
[Han and Finin2013] Lushan Han and Tim Finin. 2013.
UMBC webbase corpus. http://ebiq.org/r/351.
[Han et al2012] Lushan Han, Tim Finin, and Anupam
Joshi. 2012. Schema-free structured querying of db-
pedia data. In Proceedings of the 21st ACM interna-
tional conference on Information and knowledge man-
agement, pages 2090?2093. ACM.
[Han et al2013] Lushan Han, Tim Finin, Paul McNamee,
Anupam Joshi, and Yelena Yesha. 2013. Improving
Word Similarity by Augmenting PMI with Estimates
of Word Polysemy. IEEE Transactions on Knowledge
and Data Engineering, 25(6):1307?1322.
[Harris1968] Zellig Harris. 1968. Mathematical Struc-
tures of Language. Wiley, New York, USA.
[Hart1997] M. Hart. 1997. Project gutenberg electronic
books. http://www.gutenberg.org/wiki/Main Page.
[Kauchak and Barzilay2006] David Kauchak and Regina
Barzilay. 2006. Paraphrasing for automatic evalua-
tion. In HLT-NAACL ?06, pages 455?462.
[Landauer and Dumais1997] T. Landauer and S. Dumais.
1997. A solution to plato?s problem: The latent se-
mantic analysis theory of the acquisition, induction,
and representation of knowledge. In Psychological
Review, 104, pages 211?240.
[Li et al2003] Y. Li, Z.A. Bandar, and D. McLean.
2003. An approach for measuring semantic similar-
ity between words using multiple information sources.
IEEE Transactions on Knowledge and Data Engineer-
ing, 15(4):871?882.
[Lin1998a] Dekang Lin. 1998a. Automatic retrieval and
clustering of similar words. In Proc. 17th Int. Conf. on
Computational Linguistics, pages 768?774, Montreal,
CN.
[Lin1998b] Dekang Lin. 1998b. An information-
theoretic definition of similarity. In Proceedings of the
Fifteenth International Conference on Machine Learn-
ing, ICML ?98, pages 296?304, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
[Meadow1992] Charles T. Meadow. 1992. Text Informa-
tion Retrieval Systems. Academic Press, Inc.
[Mem2008] 2008. Google word frequency counts.
http://bit.ly/10JdTRz.
[Metzler et al2007] Donald Metzler, Susan Dumais, and
Christopher Meek. 2007. Similarity measures for
short segments of text. In Proceedings of the 29th
European conference on IR research, pages 16?27.
Springer-Verlag.
[Michel et al2011] Jean-Baptiste Michel, Yuan K. Shen,
Aviva P. Aiden, Adrian Veres, Matthew K. Gray, The
Google Books Team, Joseph P. Pickett, Dale Hoiberg,
Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker,
51
Martin A. Nowak, and Erez L. Aiden. 2011. Quan-
titative analysis of culture using millions of digitized
books. Science, 331(6014):176?182, January 14.
[Mihalcea et al2006] Rada Mihalcea, Courtney Corley,
and Carlo Strapparava. 2006. Corpus-based and
knowledge-based measures of text semantic similarity.
In Proceedings of the 21st national conference on Ar-
tificial intelligence, pages 775?780. AAAI Press.
[Miller1995] G.A. Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):41.
[Mohammad et al2008] Saif Mohammad, Bonnie Dorr,
and Graeme Hirst. 2008. Computing word-pair
antonymy. In Proc. Conf. on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-2008), October.
[Rose et al2002] Tony Rose, Mark Stevenson, and Miles
Whitehead. 2002. The reuters corpus volume 1 - from
yesterdays news to tomorrows language resources. In
In Proceedings of the Third International Conference
on Language Resources and Evaluation, pages 29?31.
[Sahami and Heilman2006] Mehran Sahami and Timo-
thy D. Heilman. 2006. A web-based kernel function
for measuring the similarity of short text snippets. In
Proceedings of the 15th international conference on
World Wide Web, WWW ?06, pages 377?386. ACM.
[Saric et al2012] Frane Saric, Goran Glavas, Mladen
Karan, Jan Snajder, and Bojana Dalbelo Basic. 2012.
Takelab: systems for measuring semantic text simi-
larity. In Proceedings of the First Joint Conference
on Lexical and Computational Semantics, pages 441?
448. Association for Computational Linguistics.
[Sriram et al2010] Bharath Sriram, Dave Fuhry, Engin
Demir, Hakan Ferhatosmanoglu, and Murat Demirbas.
2010. Short text classification in twitter to improve
information filtering. In Proceedings of the 33rd in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 841?842.
ACM.
[Stanford2001] Stanford. 2001. Stanford WebBase
project. http://bit.ly/WebBase.
[Toutanova et al2000] Kristina Toutanova, Dan
Klein, Christopher Manning, William Mor-
gan, Anna Rafferty, and Michel Galley. 2000.
Stanford log-linear part-of-speech tagger.
http://nlp.stanford.edu/software/tagger.shtml.
[UMBC2013a] UMBC. 2013a. Graph of relations
project. http://ebiq.org/j/95.
[UMBC2013b] UMBC. 2013b. Semantic similarity
demonstration. http://swoogle.umbc.edu/SimService/.
[Wu and Palmer1994] Z. Wu and M. Palmer. 1994. Verb
semantic and lexical selection. In Proceedings of the
32nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL-1994), pages 133?138, Las
Cruces (Mexico).
52
