Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 688?695,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Unified Tagging Approach to Text Normalization 
Conghui Zhu 
Harbin Institute of Technology 
Harbin, China 
chzhu@mtlab.hit.edu.cn 
Jie Tang 
Department of Computer Science
Tsinghua University, China 
jietang@tsinghua.edu.cn
Hang Li 
Microsoft Research Asia  
Beijing, China 
hangli@microsoft.com
Hwee Tou Ng 
Department of Computer Science 
National University of Singapore, Singapore 
nght@comp.nus.edu.sg 
Tiejun Zhao 
Harbin Institute of Technology 
Harbin, China 
tjzhao@mtlab.hit.edu.cn 
 
 
Abstract 
This paper addresses the issue of text nor-
malization, an important yet often over-
looked problem in natural language proc-
essing. By text normalization, we mean 
converting ?informally inputted? text into 
the canonical form, by eliminating ?noises? 
in the text and detecting paragraph and sen-
tence boundaries in the text. Previously, 
text normalization issues were often under-
taken in an ad-hoc fashion or studied sepa-
rately. This paper first gives a formaliza-
tion of the entire problem. It then proposes 
a unified tagging approach to perform the 
task using Conditional Random Fields 
(CRF). The paper shows that with the in-
troduction of a small set of tags, most of 
the text normalization tasks can be per-
formed within the approach. The accuracy 
of the proposed method is high, because 
the subtasks of normalization are interde-
pendent and should be performed together. 
Experimental results on email data cleaning 
show that the proposed method signifi-
cantly outperforms the approach of using 
cascaded models and that of employing in-
dependent models. 
1 Introduction 
More and more ?informally inputted? text data be-
comes available to natural language processing, 
such as raw text data in emails, newsgroups, fo-
rums, and blogs. Consequently, how to effectively 
process the data and make it suitable for natural 
language processing becomes a challenging issue. 
This is because informally inputted text data is 
usually very noisy and is not properly segmented. 
For example, it may contain extra line breaks, extra 
spaces, and extra punctuation marks; and it may 
contain words badly cased. Moreover, the bounda-
ries between paragraphs and the boundaries be-
tween sentences are not clear. 
We have examined 5,000 randomly collected 
emails and found that 98.4% of the emails contain 
noises (based on the definition in Section 5.1). 
In order to perform high quality natural lan-
guage processing, it is necessary to perform ?nor-
malization? on informally inputted data first, spe-
cifically, to remove extra line breaks, segment the 
text into paragraphs, add missing spaces and miss-
ing punctuation marks, eliminate extra spaces and 
extra punctuation marks, delete unnecessary tokens, 
correct misused punctuation marks, restore badly 
cased words, correct misspelled words, and iden-
tify sentence boundaries. 
Traditionally, text normalization is viewed as an 
engineering issue and is conducted in a more or 
less ad-hoc manner. For example, it is done by us-
ing rules or machine learning models at different 
levels. In natural language processing, several is-
sues of text normalization were studied, but were 
only done separately. 
This paper aims to conduct a thorough investiga-
tion on the issue. First, it gives a formalization of 
688
the problem; specifically, it defines the subtasks of 
the problem. Next, it proposes a unified approach 
to the whole task on the basis of tagging. Specifi-
cally, it takes the problem as that of assigning tags 
to the input texts, with a tag representing deletion, 
preservation, or replacement of a token. As the 
tagging model, it employs Conditional Random 
Fields (CRF). The unified model can achieve better 
performances in text normalization, because the 
subtasks of text normalization are often interde-
pendent. Furthermore, there is no need to define 
specialized models and features to conduct differ-
ent types of cleaning; all the cleaning processes 
have been formalized and conducted as assign-
ments of the three types of tags. 
Experimental results indicate that our method 
significantly outperforms the methods using cas-
caded models or independent models on normali-
zation. Our experiments also indicate that with the 
use of the tags defined, we can conduct most of the 
text normalization in the unified framework. 
Our contributions in this paper include: (a) for-
malization of the text normalization problem, (b) 
proposal of a unified tagging approach, and (c) 
empirical verification of the effectiveness of the 
proposed approach. 
The rest of the paper is organized as follows. In 
Section 2, we introduce related work. In Section 3, 
we formalize the text normalization problem. In 
Section 4, we explain our approach to the problem 
and in Section 5 we give the experimental results. 
We conclude the paper in Section 6. 
2 Related Work 
Text normalization is usually viewed as an 
engineering issue and is addressed in an ad-hoc 
manner. Much of the previous work focuses on 
processing texts in clean form, not texts in 
informal form. Also, prior work mostly focuses on 
processing one type or a small number of types of 
errors, whereas this paper deals with many 
different types of errors. 
Clark (2003) has investigated the problem of 
preprocessing noisy texts for natural language 
processing. He proposes identifying token bounda-
ries and sentence boundaries, restoring cases of 
words, and correcting misspelled words by using a 
source channel model. 
Minkov et al (2005) have investigated the prob-
lem of named entity recognition in informally in-
putted texts. They propose improving the perform-
ance of personal name recognition in emails using 
two machine-learning based methods: Conditional 
Random Fields and Perceptron for learning HMMs. 
See also (Carvalho and Cohen, 2004). 
Tang et al (2005) propose a cascaded approach 
for email data cleaning by employing Support Vec-
tor Machines and rules. Their method can detect 
email headers, signatures, program codes, and ex-
tra line breaks in emails. See also (Wong et al, 
2007). 
Palmer and Hearst (1997) propose using a Neu-
ral Network model to determine whether a period 
in a sentence is the ending mark of the sentence, an 
abbreviation, or both. See also (Mikheev, 2000; 
Mikheev, 2002). 
Lita et al (2003) propose employing a language 
modeling approach to address the case restoration 
problem. They define four classes for word casing: 
all letters in lower case, first letter in uppercase, all 
letters in upper case, and mixed case, and formal-
ize the problem as assigning class labels to words 
in natural language texts. Mikheev (2002) proposes 
using not only local information but also global 
information in a document in case restoration. 
Spelling error correction can be formalized as a 
classification problem. Golding and Roth (1996) 
propose using the Winnow algorithm to address 
the issue. The problem can also be formalized as 
that of data conversion using the source channel 
model. The source model can be built as an n-gram 
language model and the channel model can be con-
structed with confusing words measured by edit 
distance. Brill and Moore, Church and Gale, and 
Mayes et al have developed different techniques 
for confusing words calculation (Brill and Moore, 
2000; Church and Gale, 1991; Mays et al, 1991). 
Sproat et al (1999) have investigated normaliza-
tion of non-standard words in texts, including 
numbers, abbreviations, dates, currency amounts, 
and acronyms. They propose a taxonomy of non-
standard words and apply n-gram language models, 
decision trees, and weighted finite-state transduc-
ers to the normalization. 
3 Text Normalization 
In this paper we define text normalization at three 
levels: paragraph, sentence, and word level. The 
subtasks at each level are listed in Table 1. For ex-
ample, at the paragraph level, there are two sub-
689
tasks: extra line-break deletion and paragraph 
boundary detection. Similarly, there are six (three) 
subtasks at the sentence (word) level, as shown in 
Table 1. Unnecessary token deletion refers to dele-
tion of tokens like ?-----? and ?====?, which are 
not needed in natural language processing. Note 
that most of the subtasks conduct ?cleaning? of 
noises, except paragraph boundary detection and 
sentence boundary detection. 
Level Task Percentages of Noises
Extra line break deletion 49.53 
Paragraph 
Paragraph boundary detection  
Extra space deletion 15.58 
Extra punctuation mark deletion 0.71 
Missing space insertion 1.55 
Missing punctuation mark insertion 3.85 
Misused punctuation mark correction 0.64 
Sentence 
Sentence boundary detection  
Case restoration 15.04 
Unnecessary token deletion 9.69 Word 
Misspelled word correction 3.41 
Table 1. Text Normalization Subtasks 
As a result of text normalization, a text is seg-
mented into paragraphs; each paragraph is seg-
mented into sentences with clear boundaries; and 
each word is converted into the canonical form. 
After normalization, most of the natural language 
processing tasks can be performed, for example, 
part-of-speech tagging and parsing. 
We have manually cleaned up some email data 
(cf., Section 5) and found that nearly all the noises 
can be eliminated by performing the subtasks de-
fined above. Table 1 gives the statistics. 
1.  i?m thinking about buying a pocket 
2.  pc    device for my wife this christmas,. 
3.  the worry that i have is that she won?t 
4.  be able to sync it to her outlook express  
5.  contacts? 
Figure 1. An example of informal text 
I?m thinking about buying a Pocket PC device for my 
wife this Christmas.// The worry that I have is that 
she won?t be able to sync it to her Outlook Express 
contacts.// 
Figure 2. Normalized text 
Figure 1 shows an example of informally input-
ted text data. It includes many typical noises. From 
line 1 to line 4, there are four extra line breaks at 
the end of each line. In line 2, there is an extra 
comma after the word ?Christmas?. The first word 
in each sentence and the proper nouns (e.g., 
?Pocket PC? and ?Outlook Express?) should be 
capitalized. The extra spaces between the words 
?PC? and ?device? should be removed. At the end 
of line 2, the line break should be removed and a 
space is needed after the period. The text should be 
segmented into two sentences. 
Figure 2 shows an ideal output of text normali-
zation on the input text in Figure 1. All the noises 
in Figure 1 have been cleaned and paragraph and 
sentence endings have been identified. 
We must note that dependencies (sometimes 
even strong dependencies) exist between different 
types of noises. For example, word case restoration 
needs help from sentence boundary detection, and 
vice versa. An ideal normalization method should 
consider processing all the tasks together. 
4 A Unified Tagging Approach 
4.1 Process 
In this paper, we formalize text normalization as a 
tagging problem and employ a unified approach to 
perform the task (no matter whether the processing 
is at paragraph level, sentence level, or word level). 
There are two steps in the method: preprocess-
ing and tagging. In preprocessing, (A) we separate 
the text into paragraphs (i.e., sequences of tokens), 
(B) we determine tokens in the paragraphs, and (C) 
we assign possible tags to each token. The tokens 
form the basic units and the paragraphs form the 
sequences of units in the tagging problem. In tag-
ging, given a sequence of units, we determine the 
most likely corresponding sequence of tags by us-
ing a trained tagging model. In this paper, as the 
tagging model, we make use of CRF. 
Next we describe the steps (A)-(C) in detail and 
explain why our method can accomplish many of 
the normalization subtasks in Table 1. 
(A). We separate the text into paragraphs by tak-
ing two or more consecutive line breaks as the end-
ings of paragraphs. 
(B). We identify tokens by using heuristics. 
There are five types of tokens: ?standard word?, 
?non-standard word?, punctuation mark, space, and 
line break. Standard words are words in natural 
language. Non-standard words include several 
general ?special words? (Sproat et al, 1999), email 
address, IP address, URL, date, number, money, 
percentage, unnecessary tokens (e.g., ?===? and 
690
?###?), etc. We identify non-standard words by 
using regular expressions. Punctuation marks in-
clude period, question mark, and exclamation mark. 
Words and punctuation marks are separated into 
different tokens if they are joined together. Natural 
spaces and line breaks are also regarded as tokens. 
(C). We assign tags to each token based on the 
type of the token. Table 2 summarizes the types of 
tags defined. 
Token Type Tag Description 
PRV Preserve line break 
RPA Replace line break by space Line break 
DEL Delete line break 
PRV Preserve space 
Space 
DEL Delete space 
PSB Preserve punctuation mark and view it as sentence ending 
PRV Preserve punctuation mark without viewing it as sentence ending 
Punctuation 
mark 
DEL Delete punctuation mark 
AUC Make all characters in uppercase 
ALC Make all characters in lowercase 
FUC Make the first character in uppercase
Word 
AMC Make characters in mixed case 
PRV Preserve the special token 
Special token 
DEL Delete the special token 
Table 2. Types of tags 
 
Figure 3. An example of tagging 
Figure 3 shows an example of the tagging proc-
ess. (The symbol ??? indicates a space). In the fig-
ure, a white circle denotes a token and a gray circle 
denotes a tag. Each token can be assigned several 
possible tags. 
Using the tags, we can perform most of the text 
normalization processing (conducting seven types 
of subtasks defined in Table 1 and cleaning 
90.55% of the noises). 
In this paper, we do not conduct three subtasks, 
although we could do them in principle. These in-
clude missing space insertion, missing punctuation 
mark insertion, and misspelled word correction. In 
our email data, it corresponds to 8.81% of the 
noises. Adding tags for insertions would increase 
the search space dramatically. We did not do that 
due to computation consideration. Misspelled word 
correction can be done in the same framework eas-
ily. We did not do that in this work, because the 
percentage of misspelling in the data is small. 
We do not conduct misused punctuation mark 
correction as well (e.g., correcting ?.? with ???). It 
consists of 0.64% of the noises in the email data. 
To handle it, one might need to parse the sentences. 
4.2 CRF Model 
We employ Conditional Random Fields (CRF) as 
the tagging model. CRF is a conditional probability 
distribution of a sequence of tags given a sequence 
of tokens, represented as P(Y|X) , where X denotes 
the token sequence and Y the tag sequence 
(Lafferty et al, 2001). 
In tagging, the CRF model is used to find the 
sequence of tags Y* having the highest likelihood 
Y* = maxYP(Y|X), with an efficient algorithm (the 
Viterbi algorithm). 
In training, the CRF model is built with labeled 
data and by means of an iterative algorithm based 
on Maximum Likelihood Estimation. 
Transition Features 
yi-1=y?, yi=y 
yi-1=y?, yi=y, wi=w 
yi-1=y?, yi=y, ti=t 
State Features 
wi=w, yi=y 
wi-1=w, yi=y 
wi-2=w, yi=y 
wi-3=w, yi=y 
wi-4=w, yi=y 
wi+1=w, yi=y 
wi+2=w, yi=y 
wi+3=w, yi=y 
wi+4=w, yi=y 
wi-1=w?, wi=w, yi=y
wi+1=w?, wi=w, yi=y 
ti=t, yi=y 
ti-1=t, yi=y 
ti-2=t, yi=y 
ti-3=t, yi=y 
ti-4=t, yi=y 
ti+1=t, yi=y 
ti+2=t, yi=y 
ti+3=t, yi=y 
ti+4=t, yi=y 
ti-2=t??, ti-1=t?, yi=y 
ti-1=t?, ti=t, yi=y 
ti=t, ti+1=t?, yi=y 
ti+1=t?, ti+2=t??, yi=y 
ti-2=t??, ti-1=t?, ti=t, yi=y 
ti-1=t??, ti=t, ti+1=t?, yi=y 
ti=t, ti+1=t?, ti+2=t??, yi=y 
Table 3. Features used in the unified CRF model 
691
4.3 Features 
Two sets of features are defined in the CRF model: 
transition features and state features. Table 3 
shows the features used in the model. 
Suppose that at position i in token sequence x, wi 
is the token, ti the type of token (see Table 2), and 
yi the possible tag. Binary features are defined as 
described in Table 3. For example, the transition 
feature yi-1=y?, yi=y implies that if the current tag is 
y and the previous tag is y?, then the feature value 
is true; otherwise false. The state feature wi=w, 
yi=y implies that if the current token is w and the 
current label is y, then the feature value is true; 
otherwise false. In our experiments, an actual fea-
ture might be the word at position 5 is ?PC? and the 
current tag is AUC. In total, 4,168,723 features 
were used in our experiments. 
4.4 Baseline Methods 
We can consider two baseline methods based on 
previous work, namely cascaded and independent 
approaches. The independent approach performs 
text normalization with several passes on the text. 
All of the processes take the raw text as input and 
output the normalized/cleaned result independently. 
The cascaded approach also performs normaliza-
tion in several passes on the text. Each process car-
ries out cleaning/normalization from the output of 
the previous process. 
4.5 Advantages 
Our method offers some advantages. 
(1) As indicated, the text normalization tasks are 
interdependent. The cascaded approach or the in-
dependent approach cannot simultaneously per-
form the tasks. In contrast, our method can effec-
tively overcome the drawback by employing a uni-
fied framework and achieve more accurate per-
formances. 
(2) There are many specific types of errors one 
must correct in text normalization. As shown in 
Figure 1, there exist four types of errors with each 
type having several correction results. If one de-
fines a specialized model or rule to handle each of 
the cases, the number of needed models will be 
extremely large and thus the text normalization 
processing will be impractical. In contrast, our 
method naturally formalizes all the tasks as as-
signments of different types of tags and trains a 
unified model to tackle all the problems at once. 
5 Experimental Results 
5.1 Experiment Setting 
Data Sets 
We used email data in our experiments. We ran-
domly chose in total 5,000 posts (i.e., emails) from 
12 newsgroups. DC, Ontology, NLP, and ML are 
from newsgroups at Google (http://groups-
beta.google.com/groups). Jena is a newsgroup at Ya-
hoo (http://groups.yahoo.com/group/jena-dev). Weka 
is a newsgroup at Waikato University (https://list. 
scms.waikato.ac.nz). Prot?g? and OWL are from a 
project at Stanford University 
(http://protege.stanford.edu/). Mobility, WinServer, 
Windows, and PSS are email collections from a 
company. 
Five human annotators conducted normalization 
on the emails. A spec was created to guide the an-
notation process. All the errors in the emails were 
labeled and corrected. For disagreements in the 
annotation, we conducted ?majority voting?.  For 
example, extra line breaks, extra spaces, and extra 
punctuation marks in the emails were labeled. Un-
necessary tokens were deleted. Missing spaces and 
missing punctuation marks were added and marked. 
Mistakenly cased words, misspelled words, and 
misused punctuation marks were corrected. Fur-
thermore, paragraph boundaries and sentence 
boundaries were also marked. The noises fell into 
the categories defined in Table 1. 
Table 4 shows the statistics in the data sets. 
From the table, we can see that a large number of 
noises (41,407) exist in the emails. We can also see 
that the major noise types are extra line breaks, 
extra spaces, casing errors, and unnecessary tokens. 
In the experiments, we conducted evaluations in 
terms of precision, recall, F1-measure, and accu-
racy (for definitions of the measures, see for ex-
ample (van Rijsbergen, 1979; Lita et al, 2003)). 
Implementation of Baseline Methods 
We used the cascaded approach and the independ-
ent approach as baselines. 
For the baseline methods, we defined several 
basic prediction subtasks: extra line break detec-
tion, extra space detection, extra punctuation mark 
detection, sentence boundary detection, unneces-
sary token detection, and case restoration. We 
compared the performances of our method with 
those of the baseline methods on the subtasks. 
692
Data Set 
Number 
of 
Email 
Number 
of 
Noises 
Extra 
Line 
Break 
Extra 
Space 
Extra
 Punc.
Missing
Space
Missing
Punc.
Casing
Error
Spelling
Error
Misused 
Punc.
Unnece-
ssary 
Token 
Number of 
Paragraph 
Boundary 
Number of 
Sentence 
Boundary
DC 100 702 476 31 8 3 24 53 14 2 91 457 291 
Ontology 100 2,731 2,132 24 3 10 68 205 79 15 195 677 1,132 
NLP 60 861 623 12 1 3 23 135 13 2 49 244 296 
ML 40 980 868 17 0 2 13 12 7 0 61 240 589 
Jena 700 5,833 3,066 117 42 38 234 888 288 59 1,101 2,999 1,836 
Weka 200 1,721 886 44 0 30 37 295 77 13 339 699 602 
Prot?g? 700 3,306 1,770 127 48 151 136 552 116 9 397 1,645 1,035 
OWL 300 1,232 680 43 24 47 41 152 44 3 198 578 424 
Mobility 400 2,296 1,292 64 22 35 87 495 92 8 201 891 892 
WinServer 400 3,487 2,029 59 26 57 142 822 121 21 210 1,232 1,151 
Windows 1,000 9,293 3,416 3,056 60 116 348 1,309 291 67 630 3,581 2,742 
PSS 1,000 8,965 3,348 2,880 59 153 296 1,331 276 66 556 3,411 2,590 
Total 5,000 41,407 20,586 6,474 293 645 1,449 6,249 1,418 265 4,028 16,654 13,580 
Table 4. Statistics on data sets 
For the case restoration subtask (processing on 
token sequence), we employed the TrueCasing 
method (Lita et al, 2003). The method estimates a 
tri-gram language model using a large data corpus 
with correctly cased words and then makes use of 
the model in case restoration. We also employed 
Conditional Random Fields to perform case 
restoration, for comparison purposes. The CRF 
based casing method estimates a conditional 
probabilistic model using the same data and the 
same tags defined in TrueCasing. 
For unnecessary token deletion, we used rules as 
follows. If a token consists of non-ASCII charac-
ters or consecutive duplicate characters, such as 
?===?, then we identify it as an unnecessary token. 
For each of the other subtasks, we exploited the 
classification approach. For example, in extra line 
break detection, we made use of a classification 
model to identify whether or not a line break is a 
paragraph ending. We employed Support Vector 
Machines (SVM) as the classification model (Vap-
nik, 1998). In the classification model we utilized 
the same features as those in our unified model 
(see Table 3 for details). 
In the cascaded approach, the prediction tasks 
are performed in sequence, where the output of 
each task becomes the input of each immediately 
following task. The order of the prediction tasks is: 
(1) Extra line break detection: Is a line break a 
paragraph ending? It then separates the text into 
paragraphs using the remaining line breaks. (2) 
Extra space detection: Is a space an extra space? (3) 
Extra punctuation mark detection: Is a punctuation 
mark a noise? (4) Sentence boundary detection: Is 
a punctuation mark a sentence boundary? (5) Un-
necessary token deletion: Is a token an unnecessary 
token? (6) Case restoration. Each of steps (1) to (4) 
uses a classification model (SVM), step (5) uses 
rules, whereas step (6) uses either a language 
model (TrueCasing) or a CRF model (CRF). 
In the independent approach, we perform the 
prediction tasks independently. When there is a 
conflict between the outcomes of two classifiers, 
we adopt the result of the latter classifier, as de-
termined by the order of classifiers in the cascaded 
approach. 
To test how dependencies between different 
types of noises affect the performance of normali-
zation, we also conducted experiments using the 
unified model by removing the transition features. 
Implementation of Our Method 
In the implementation of our method, we used the 
tool CRF++, available at http://chasen.org/~taku 
/software/CRF++/. We made use of all the default 
settings of the tool in the experiments. 
5.2 Text Normalization Experiments 
Results 
We evaluated the performances of our method 
(Unified) and the baseline methods (Cascaded and 
Independent) on the 12 data sets. Table 5 shows 
the five-fold cross-validation results. Our method 
outperforms the two baseline methods. 
Table 6 shows the overall performances of text 
normalization by our method and the two baseline 
methods. We see that our method outperforms the 
two baseline methods. It can also be seen that the 
performance of the unified method decreases when 
removing the transition features (Unified w/o 
Transition Features). 
693
We conducted sign tests for each subtask on the 
results, which indicate that all the improvements of 
Unified over Cascaded and Independent are statis-
tically significant (p << 0.01). 
Detection Task Prec. Rec. F1 Acc.
Independent 95.16 91.52 93.30 93.81
Cascaded 95.16 91.52 93.30 93.81Extra Line Break  
Unified 93.87 93.63 93.75 94.53
Independent 91.85 94.64 93.22 99.87
Cascaded 94.54 94.56 94.55 99.89Extra Space 
Unified 95.17 93.98 94.57 99.90
Independent 88.63 82.69 85.56 99.66
Cascaded 87.17 85.37 86.26 99.66
Extra 
 Punctuation 
Mark Unified 90.94 84.84 87.78 99.71
Independent 98.46 99.62 99.04 98.36
Cascaded 98.55 99.20 98.87 98.08Sentence Boundary  
Unified 98.76 99.61 99.18 98.61
Independent 72.51 100.0 84.06 84.27
Cascaded 72.51 100.0 84.06 84.27Unnecessary Token 
Unified 98.06 95.47 96.75 96.18
Independent 27.32 87.44 41.63 96.22Case  
Restoration 
(TrueCasing) Cascaded 28.04 88.21 42.55 96.35
Independent 84.96 62.79 72.21 99.01
Cascaded 85.85 63.99 73.33 99.07
Case  
Restoration 
(CRF) Unified 86.65 67.09 75.63 99.21
Table 5. Performances of text normalization (%) 
Text Normalization Prec. Rec. F1 Acc.
Independent (TrueCasing) 69.54 91.33 78.96 97.90
Independent (CRF) 85.05 92.52 88.63 98.91
Cascaded (TrueCasing) 70.29 92.07 79.72 97.88
Cascaded (CRF) 85.06 92.70 88.72 98.92
Unified w/o Transition 
Features 86.03 93.45 89.59 99.01
Unified 86.46 93.92 90.04 99.05
Table 6. Performances of text normalization (%) 
Discussions 
Our method outperforms the independent method 
and the cascaded method in all the subtasks, espe-
cially in the subtasks that have strong dependen-
cies with each other, for example, sentence bound-
ary detection, extra punctuation mark detection, 
and case restoration. 
The cascaded method suffered from ignorance 
of the dependencies between the subtasks. For ex-
ample, there were 3,314 cases in which sentence 
boundary detection needs to use the results of extra 
line break detection, extra punctuation mark detec-
tion, and case restoration. However, in the cas-
caded method, sentence boundary detection is con-
ducted after extra punctuation mark detection and 
before case restoration, and thus it cannot leverage 
the results of case restoration. Furthermore, errors 
of extra punctuation mark detection can lead to 
errors in sentence boundary detection. 
The independent method also cannot make use 
of dependencies across different subtasks, because 
it conducts all the subtasks from the raw input data. 
This is why for detection of extra space, extra 
punctuation mark, and casing error, the independ-
ent method cannot perform as well as our method. 
Our method benefits from the ability of model-
ing dependencies between subtasks. We see from 
Table 6 that by leveraging the dependencies, our 
method can outperform the method without using 
dependencies (Unified w/o Transition Features) by 
0.62% in terms of F1-measure. 
Here we use the example in Figure 1 to show the 
advantage of our method compared with the inde-
pendent and the cascaded methods. With normali-
zation by the independent method, we obtain: 
I?m thinking about buying a pocket PC   device for my wife 
this Christmas, The worry that I have is that she won?t be able 
to sync it to her outlook express contacts.// 
With normalization by the cascaded method, we 
obtain: 
I?m thinking about buying a pocket PC device for my wife 
this Christmas, the worry that I have is that she won?t be able 
to sync it to her outlook express contacts.// 
With normalization by our method, we obtain: 
I?m thinking about buying a Pocket PC device for my wife 
this Christmas.// The worry that I have is that she won?t be 
able to sync it to her Outlook Express contacts.// 
The independent method can correctly deal with 
some of the errors. For instance, it can capitalize 
the first word in the first and the third line, remove 
extra periods in the fifth line, and remove the four 
extra line breaks. However, it mistakenly removes 
the period in the second line and it cannot restore 
the cases of some words, for example ?pocket? and 
?outlook express?. 
In the cascaded method, each process carries out 
cleaning/normalization from the output of the pre-
vious process and thus can make use of the 
cleaned/normalized results from the previous proc-
ess. However, errors in the previous processes will 
also propagate to the later processes. For example, 
the cascaded method mistakenly removes the pe-
riod in the second line. The error allows case resto-
ration to make the error of keeping the word ?the? 
in lower case. 
694
TrueCasing-based methods for case restoration 
suffer from low precision (27.32% by Independent 
and 28.04% by Cascaded), although their recalls 
are high (87.44% and 88.21% respectively). There 
are two reasons: 1) About 10% of the errors in 
Cascaded are due to errors of sentence boundary 
detection and extra line break detection in previous 
steps; 2) The two baselines tend to restore cases of 
words to the forms having higher probabilities in 
the data set and cannot take advantage of the de-
pendencies with the other normalization subtasks. 
For example, ?outlook? was restored to first letter 
capitalized in both ?Outlook Express? and ?a pleas-
ant outlook?. Our method can take advantage of the 
dependencies with other subtasks and thus correct 
85.01% of the errors that the two baseline methods 
cannot handle. Cascaded and Independent methods 
employing CRF for case restoration improve the 
accuracies somewhat. However, they are still infe-
rior to our method. 
Although we have conducted error analysis on 
the results given by our method, we omit the de-
tails here due to space limitation and will report 
them in a future expanded version of this paper. 
We also compared the speed of our method with 
those of the independent and cascaded methods. 
We tested the three methods on a computer with 
two 2.8G Dual-Core CPUs and three Gigabyte 
memory. On average, it needs about 5 hours for 
training the normalization models using our 
method and 25 seconds for tagging in the cross-
validation experiments. The independent and the 
cascaded methods (with TrueCasing) require less 
time for training (about 2 minutes and 3 minutes 
respectively) and for tagging (several seconds). 
This indicates that the efficiency of our method 
still needs improvement. 
6 Conclusion 
In this paper, we have investigated the problem of 
text normalization, an important issue for natural 
language processing. We have first defined the 
problem as a task consisting of noise elimination 
and boundary detection subtasks. We have then 
proposed a unified tagging approach to perform the 
task, specifically to treat text normalization as as-
signing tags representing deletion, preservation, or 
replacement of the tokens in the text. Experiments 
show that our approach significantly outperforms 
the two baseline methods for text normalization. 
References 
E. Brill and R. C. Moore. 2000. An Improved Error 
Model for Noisy Channel Spelling Correction, Proc. 
of ACL 2000. 
V. R. Carvalho and W. W. Cohen. 2004. Learning to 
Extract Signature and Reply Lines from Email, Proc. 
of CEAS 2004. 
K. Church and W. Gale. 1991. Probability Scoring for 
Spelling Correction, Statistics and Computing, Vol. 1. 
A. Clark. 2003. Pre-processing Very Noisy Text, Proc. 
of Workshop on Shallow Processing of Large Cor-
pora. 
A. R. Golding and D. Roth. 1996. Applying Winnow to 
Context-Sensitive Spelling Correction, Proc. of 
ICML?1996. 
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data, Proc. of ICML 
2001. 
L. V. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla. 
2003. tRuEcasIng, Proc. of ACL 2003. 
E. Mays, F. J. Damerau, and R. L. Mercer. 1991. Con-
text Based Spelling Correction, Information Process-
ing and Management, Vol. 27, 1991. 
A. Mikheev. 2000. Document Centered Approach to 
Text Normalization, Proc. SIGIR 2000. 
A. Mikheev. 2002. Periods, Capitalized Words, etc. 
Computational Linguistics, Vol. 28, 2002. 
E. Minkov, R. C. Wang, and W. W. Cohen. 2005. Ex-
tracting Personal Names from Email: Applying 
Named Entity Recognition to Informal Text, Proc. of 
EMNLP/HLT-2005. 
D. D. Palmer and M. A. Hearst. 1997. Adaptive Multi-
lingual Sentence Boundary Disambiguation, Compu-
tational Linguistics, Vol. 23. 
C.J. van Rijsbergen. 1979. Information Retrieval. But-
terworths, London. 
R. Sproat, A. Black, S. Chen, S. Kumar, M. Ostendorf, 
and C. Richards. 1999. Normalization of non-
standard words, WS?99 Final Report. 
http://www.clsp.jhu.edu/ws99/projects/normal/. 
J. Tang, H. Li, Y. Cao, and Z. Tang. 2005. Email data 
cleaning, Proc. of SIGKDD?2005. 
V. Vapnik. 1998. Statistical Learning Theory, Springer. 
W. Wong, W. Liu, and M. Bennamoun. 2007. Enhanced 
Integrated Scoring for Cleaning Dirty Texts, Proc. of 
IJCAI-2007 Workshop on Analytics for Noisy Un-
structured Text Data. 
695
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 641?650,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Transfer Learning Based Cross-lingual
Knowledge Extraction for Wikipedia
Zhigang Wang?, Zhixing Li?, Juanzi Li?, Jie Tang?, and Jeff Z. Pan?
? Tsinghua National Laboratory for Information Science and Technology
DCST, Tsinghua University, Beijing, China
{wzhigang,zhxli,ljz,tangjie}@keg.cs.tsinghua.edu.cn
? Department of Computing Science, University of Aberdeen, Aberdeen, UK
jeff.z.pan@abdn.ac.uk
Abstract
Wikipedia infoboxes are a valuable source
of structured knowledge for global knowl-
edge sharing. However, infobox infor-
mation is very incomplete and imbal-
anced among the Wikipedias in differen-
t languages. It is a promising but chal-
lenging problem to utilize the rich struc-
tured knowledge from a source language
Wikipedia to help complete the missing in-
foboxes for a target language.
In this paper, we formulate the prob-
lem of cross-lingual knowledge extraction
from multilingual Wikipedia sources, and
present a novel framework, called Wiki-
CiKE, to solve this problem. An instance-
based transfer learning method is utilized
to overcome the problems of topic drift
and translation errors. Our experimen-
tal results demonstrate that WikiCiKE out-
performs the monolingual knowledge ex-
traction method and the translation-based
method.
1 Introduction
In recent years, the automatic knowledge extrac-
tion using Wikipedia has attracted significant re-
search interest in research fields, such as the se-
mantic web. As a valuable source of structured
knowledge, Wikipedia infoboxes have been uti-
lized to build linked open data (Suchanek et al,
2007; Bollacker et al, 2008; Bizer et al, 2008;
Bizer et al, 2009), support next-generation in-
formation retrieval (Hotho et al, 2006), improve
question answering (Bouma et al, 2008; Fer-
ra?ndez et al, 2009), and other aspects of data ex-
ploitation (McIlraith et al, 2001; Volkel et al,
2006; Hogan et al, 2011) using semantic web s-
tandards, such as RDF (Pan and Horrocks, 2007;
Heino and Pan, 2012) and OWL (Pan and Hor-
rocks, 2006; Pan and Thomas, 2007; Fokoue et
al., 2012), and their reasoning services.
However, most infoboxes in different Wikipedi-
a language versions are missing. Figure 1 shows
the statistics of article numbers and infobox infor-
mation for six major Wikipedias. Only 32.82%
of the articles have infoboxes on average, and the
numbers of infoboxes for these Wikipedias vary
significantly. For instance, the English Wikipedi-
a has 13 times more infoboxes than the Chinese
Wikipedia and 3.5 times more infoboxes than the
second largest Wikipedia of German language.
English German French Dutch Spanish Chinese
0
0.5
1
1.5
2
2.5
3
3.5
4
x 106
Languages
N
um
be
r o
f I
ns
ta
nc
es
 
 
Article
Infobox
Figure 1: Statistics for Six Major Wikipedias.
To solve this problem, KYLIN has been pro-
posed to extract the missing infoboxes from un-
structured article texts for the English Wikipedi-
a (Wu and Weld, 2007). KYLIN performs
well when sufficient training data are available,
and such techniques as shrinkage and retraining
have been used to increase recall from English
Wikipedia?s long tail of sparse infobox classes
(Weld et al, 2008; Wu et al, 2008). The extraction
performance of KYLIN is limited by the number
of available training samples.
Due to the great imbalance between different
Wikipedia language versions, it is difficult to gath-
er sufficient training data from a single Wikipedia.
Some translation-based cross-lingual knowledge
641
extraction methods have been proposed (Adar et
al., 2009; Bouma et al, 2009; Adafre and de Rijke,
2006). These methods concentrate on translating
existing infoboxes from a richer source language
version of Wikipedia into the target language. The
recall of new target infoboxes is highly limited
by the number of equivalent cross-lingual arti-
cles and the number of existing source infoboxes.
Take Chinese-English1 Wikipedias as an example:
current translation-based methods only work for
87,603 Chinese Wikipedia articles, 20.43% of the
total 428,777 articles. Hence, the challenge re-
mains: how could we supplement the missing in-
foboxes for the rest 79.57% articles?
On the other hand, the numbers of existing in-
fobox attributes in different languages are high-
ly imbalanced. Table 1 shows the comparison
of the numbers of the articles for the attributes
in template PERSON between English and Chi-
nese Wikipedia. Extracting the missing value for
these attributes, such as awards, weight, influences
and style, inside the single Chinese Wikipedia is
intractable due to the rarity of existing Chinese
attribute-value pairs.
Attribute en zh Attribute en zh
name 82,099 1,486 awards 2,310 38
birth date 77,850 1,481 weight 480 12
occupation 66,768 1,279 influences 450 6
nationality 20,048 730 style 127 1
Table 1: The Numbers of Articles in TEMPLATE
PERSON between English(en) and Chinese(zh).
In this paper, we have the following hypothesis:
one can use the rich English (auxiliary) informa-
tion to assist the Chinese (target) infobox extrac-
tion. In general, we address the problem of cross-
lingual knowledge extraction by using the imbal-
ance between Wikipedias of different languages.
For each attribute, we aim to learn an extractor to
find the missing value from the unstructured arti-
cle texts in the target Wikipedia by using the rich
information in the source language. Specifically,
we treat this cross-lingual information extraction
task as a transfer learning-based binary classifica-
tion problem.
The contributions of this paper are as follows:
1. We propose a transfer learning-based cross-
lingual knowledge extraction framework
1Chinese-English denotes the task of Chinese Wikipedia
infobox completion using English Wikipedia
called WikiCiKE. The extraction perfor-
mance for the target Wikipedia is improved
by using rich infoboxes and textual informa-
tion in the source language.
2. We propose the TrAdaBoost-based extractor
training method to avoid the problems of top-
ic drift and translation errors of the source
Wikipedia. Meanwhile, some language-
independent features are introduced to make
WikiCiKE as general as possible.
3. Chinese-English experiments for four typ-
ical attributes demonstrate that WikiCiKE
outperforms both the monolingual extrac-
tion method and current translation-based
method. The increases of 12.65% for pre-
cision and 12.47% for recall in the template
named person are achieved when only 30 tar-
get training articles are available.
The rest of this paper is organized as follows.
Section 2 presents some basic concepts, the prob-
lem formalization and the overview of WikiCiKE.
In Section 3, we propose our detailed approaches.
We present our experiments in Section 4. Some re-
lated work is described in Section 5. We conclude
our work and the future work in Section 6.
2 Preliminaries
In this section, we introduce some basic con-
cepts regarding Wikipedia, formally defining the
key problem of cross-lingual knowledge extrac-
tion and providing an overview of the WikiCiKE
framework.
2.1 Wiki Knowledge Base and Wiki Article
We consider each language version of Wikipedia
as a wiki knowledge base, which can be represent-
ed as K = {ai}pi=1, where ai is a disambiguated
article in K and p is the size of K .
Formally we define a wiki article a ? K as a
5-tuple a = (title, text, ib, tp, C), where
? title denotes the title of the article a,
? text denotes the unstructured text description
of the article a,
? ib is the infobox associated with a; specif-
ically, ib = {(attri, valuei)}qi=1 represents
the list of attribute-value pairs for the article
a,
642
Figure 2: Simplified Article of ?Bill Gates?.
? tp = {attri}ri=1 is the infobox template as-
sociated with ib, where r is the number of
attributes for one specific template, and
? C denotes the set of categories to which the
article a belongs.
Figure 2 gives an example of these five impor-
tant elements concerning the article named ?Bill
Gates?.
In what follows, we will use named subscripts,
such as aBill Gates, or index subscripts, such as ai,
to refer to one particular instance interchangeably.
We will use ?name in TEMPLATE PERSON?
to refer to the attribute attrname in the template
tpPERSON . In this cross-lingual task, we use the
source (S) and target (T) languages to denote the
languages of auxiliary and target Wikipedias, re-
spectively. For example, KS indicates the source
wiki knowledge base, and KT denotes the target
wiki knowledge base.
2.2 Problem Formulation
Mining new infobox information from unstruc-
tured article texts is actually a multi-template,
multi-slot information extraction problem. In our
task, each template represents an infobox template
and each slot denotes an attribute. In the Wiki-
CiKE framework, for each attribute attrT in an
infobox template tpT , we treat the task of missing
value extraction as a binary classification prob-
lem. It predicts whether a particular word (token)
from the article text is the extraction target (Finn
and Kushmerick, 2004; Lafferty et al, 2001).
Given an attribute attrT and an instance
(word/token) xi, XS = {xi}ni=1 and XT =
{xi}n+mi=n+1 are the sets of instances (words/tokens)
in the source and the target language respectively.
xi can be represented as a feature vector according
to its context. Usually, we have n ? m in our set-
ting, with much more attributes in the source that
those in the target. The function g : X 7? Y maps
the instance from X = XS ? XT to the true la-
bel of Y = {0, 1}, where 1 represents the extrac-
tion target (positive) and 0 denotes the background
information (negative). Because the number of
target instances m is inadequate to train a good
classifier, we combine the source and target in-
stances to construct the training data set as TD =
TDS ? TDT , where TDS = {xi, g(xi)}ni=1 and
TDT = {xi, g(xi)}n+mi=n+1 represent the source
and target training data, respectively.
Given the combined training data set TD, our
objective is to estimate a hypothesis f : X 7? Y
that minimizes the prediction error on testing data
in the target language. Our idea is to determine the
useful part of TDS to improve the classification
performance in TDT . We view this as a transfer
learning problem.
2.3 WikiCiKE Framework
WikiCiKE learns an extractor for a given attribute
attrT in the target Wikipedia. As shown in Fig-
ure 3, WikiCiKE contains four key components:
(1) Automatic Training Data Generation: given
the target attribute attrT and two wiki knowledge
bases KS and KT , WikiCiKE first generates the
training data set TD = TDS ? TDT automati-
cally. (2) WikiCiKE Training: WikiCiKE uses
a transfer learning-based classification method to
train the classifier (extractor) f : X 7? Y by using
TDS ? TDT . (3) Template Classification: Wi-
kiCiKE then determines proper candidate articles
which are suitable to generate the missing value of
attrT . (4) WikiCiKE Extraction: given a candi-
date article a, WikiCiKE uses the learned extractor
f to label each word in the text of a, and generate
the extraction result in the end.
3 Our Approach
In this section, we will present the detailed ap-
proaches used in WikiCiKE.
643
Figure 3: WikiCiKE Framework.
3.1 Automatic Training Data Generation
To generate the training data for the target at-
tribute attrT , we first determine the equivalen-
t cross-lingual attribute attrS . Fortunately, some
templates in non-English Wikipedia (e.g. Chinese
Wikipedia) explicitly match their attributes with
their counterparts in English Wikipedia. There-
fore, it is convenient to align the cross-lingual at-
tributes using English Wikipedia as bridge. For
attributes that can not be aligned in this way, cur-
rently we manually align them. The manual align-
ment is worthwhile because thousands of articles
belong to the same template may benefit from it
and at the same time it is not very costly. In Chi-
nese Wikipedia, the top 100 templates have cov-
ered nearly 80% of the articles which have been
assigned a template.
Once the aligned attribute mapping attrT ?
attrS is obtained, we collect the articles from both
KS and KT containing the corresponding attr.
The collected articles from KS are translated into
the target language. Then, we use a uniform au-
tomatic method, which primarily consists of word
labeling and feature vector generation, to generate
the training data set TD = {(x, g(x))} from these
collected articles.
For each collected article a =
{title, text, ib, tp, C} and its value of attr,
we can automatically label each word x in text
according to whether x and its neighbors are
contained by the value. The text and value are
processed as bags of words {x}text and {x}value.
Then for each xi ? {x}text we have:
g(xi) =
?
???
???
1 xi ? {x}value, |{x}value| = 1
1 xi?1, xi ? {x}value or xi, xi+1 ? {x}value,
|{x}value| > 1
0 otherwise
(1)
After the word labeling, each instance
(word/token) is represented as a feature vec-
tor. In this paper, we propose a general feature
space that is suitable for most target languages.
As shown in Table 2, we classify the features
used in WikiCiKE into three categories: format
features, POS tag features and token features.
Category Feature Example
Format First token of sentence `}L
feature Hello World!
In first half of sentence `}L
Hello World!
Starts with two digits 1231?
31th Dec.
Starts with four digits 1999t)
1999?s summer
Contains a cash sign 10?or 10$
Contains a percentage 10%
symbol
Stop words ?,0,?&
of, the, a, an
Pure number 365
Part of an anchor text 5q?
Movie Director
Begin of an anchor text 8??
Game Designer
POS tag POS tag of current token
features POS tags of
previous 5 tokens
POS tags of
next 5 tokens
Token Current token
features Previous 5 tokens
Next 5 tokens
Is current token
contained by title
Is one of previous 5
tokens contained by title
Table 2: Feature Definition.
The target training data TDT is directly gener-
ated from articles in the target language Wikipedi-
a. Articles from the source language Wikipedia
are translated into the target language in advance
and then transformed into training data TDS . In
next section, we will discuss how to train an ex-
tractor from TD = TDS ? TDT .
3.2 WikiCiKE Training
Given the attribute attrT , we want to train a clas-
sifier f : X 7? Y that can minimize the prediction
644
error for the testing data in the target language.
Traditional machine learning approaches attempt
to determine f by minimizing some loss function
L on the prediction f(x) for the training instance
x and its real label g(x), which is
f? = argmin
f??
?
L(f(x), g(x)) where (x, g(x)) ? TDT
(2)
In this paper, we use TrAdaBoost (Dai et al,
2007), which is an instance-based transfer learn-
ing algorithm that was first proposed by Dai to find
f? . TrAdaBoost requires that the source training
instances XS and target training instances XT be
drawn from the same feature space. In WikiCiKE,
the source articles are translated into the target
language in advance to satisfy this requirement.
Due to the topic drift problem and translation er-
rors, the joint probability distribution PS(x, g(x))
is not identical to PT (x, g(x)). We must adjust the
source training data TDS so that they fit the dis-
tribution on TDT . TrAdaBoost iteratively updates
the weights of all training instances to optimize the
prediction error. Specifically, the weight-updating
strategy for the source instances is decided by the
loss on the target instances.
For each t = 1 ? T iteration, given a weight
vector pt normalized from wt(wt is the weight
vector before normalization), we call a basic clas-
sifier F that can address weighted instances and
then find a hypothesis f that satisfies
f?t = argmin
f??F
?
L(pt, f(x), g(x))
(x, g(x)) ? TDS ? TDT
(3)
Let ?t be the prediction error of f?t at the tth iter-
ation on the target training instances TDT , which
is
?t = 1?n+m
k=n+1 wtk
?
n+m?
k=n+1
(wtk ? |f?t(xk)? yk|) (4)
With ?t, the weight vector wt is updated by the
function:
wt+1 = h(wt, ?t) (5)
The weight-updating strategy h is illustrated in
Table 3.
Finally, a final classifier f? can be obtained by
combining f?T/2 ? f?T .
TrAdaBoost has a convergence rate of
O(
?
ln(n/N)), where n and N are the number
of source samples and number of maximum
iterations respectively.
TrAdaBoost AdaBoost
Target + wt wt
samples ? wt ? ??1t wt ? ??1t
Source + wt ? ??1 No source training
samples ? wt ? ? sample available
+: correctly labelled ?: miss-labelled
wt: weight of an instance at the tth iteration
?t = ?t ? (1? ?t)
? = 1/(1 +
?
2 lnnT )
Table 3: Weight-updating Strategy of TrAd-
aBoost.
3.3 Template Classification
Before using the learned classifier f to extrac-
t missing infobox value for the target attribute
attrT , we must select the correct articles to be pro-
cessed. For example, the article aNew Y ork is not
a proper article for extracting the missing value of
the attribute attrbirth day .
If a already has an incomplete infobox, it is
clear that the correct tp is the template of its own
infobox ib. For those articles that have no infobox-
es, we use the classical 5-nearest neighbor algo-
rithm to determine their templates (Roussopoulos
et al, 1995) using their category labels, outlinks,
inlinks as features (Wang et al, 2012). Our classi-
fier achieves an average precision of 76.96% with
an average recall of 63.29%, and can be improved
further. In this paper, we concentrate on the Wiki-
CiKE training and extraction components.
3.4 WikiCiKE Extraction
Given an article a determined by template classi-
fication, we generate the missing value of attr
from the corresponding text. First, we turn the
text into a word sequence and compute the fea-
ture vector for each word based on the feature
definition in Section 3.1. Next we use f to label
each word, and we get a labeled sequence textl as
textl = {xf(x1)1 ...x
f(xi?1)
i?1 x
f(xi)
i x
f(xi+1)
i+1 ...x
f(xn)
n }
where the superscript f(xi) ? {0, 1} represents
the positive or negative label by f . After that, we
extract the adjacent positive tokens in text as the
predict value. In particular, the longest positive to-
ken sequence and the one that contains other pos-
itive token sequences are preferred in extraction.
E.g., a positive sequence ?comedy movie director?
is preferred to a shorter sequence ?movie direc-
tor?.
645
4 Experiments
In this section, we present our experiments to e-
valuate the effectiveness of WikiCiKE, where we
focus on the Chinese-English case; in other words,
the target language is Chinese and the source lan-
guage is English. It is part of our future work to
try other language pairs which two Wikipedias of
these languages are imbalanced in infobox infor-
mation such as English-Dutch.
4.1 Experimental Setup
4.1.1 Data Sets
Our data sets are from Wikipedia dumps2 generat-
ed on April 3, 2012. For each attribute, we collect
both labeled articles (articles that contain the cor-
responding attribute attr) and unlabeled articles
in Chinese. We split the labeled articles into two
subsets AT and Atest(AT ? Atest = ?), in which
AT is used as target training articles and Atest is
used as the first testing set. For the unlabeled arti-
cles, represented as A?test, we manually label their
infoboxes with their texts and use them as the sec-
ond testing set. For each attribute, we also collect a
set of labeled articles AS in English as the source
training data. Our experiments are performed on
four attributes, which are occupation, nationality,
alma mater in TEMPLATE PERSON, and coun-
try in TEMPLATE FILM. In particular, we extract
values from the first two paragraphs of the texts
because they usually contain most of the valuable
information. The details of data sets on these at-
tributes are given in Table 4.
Attribute |AS| |AT| |Atest| |A?test|
occupation 1,000 500 779 208
alma mater 1,000 200 215 208
nationality 1,000 300 430 208
country 1,000 500 1,000 ?
|A|: the number of articles in A
Table 4: Data Sets.
4.1.2 Comparison Methods
We compare our WikiCiKE method with two dif-
ferent kinds of methods, the monolingual knowl-
edge extraction method and the translation-based
method. They are implemented as follows:
1. KE-Mon is the monolingual knowledge ex-
tractor. The difference between WikiCiKE
and KE-Mon is that KE-Mon only uses the
Chinese training data.
2http://dumps.wikimedia.org/
2. KE-Tr is the translation-based extractor. It
obtains the values by two steps: finding their
counterparts (if available) in English using
Wikipedia cross-lingual links and attribute
alignments, and translating them into Chi-
nese.
We conduct two series of evaluation to compare
WikiCiKE with KE-Mon and KE-Tr, respectively.
1. We compare WikiCiKE with KE-Mon on the
first testing data set Atest, where most val-
ues can be found in the articles? texts in those
labeled articles, in order to demonstrate the
performance improvement by using cross-
lingual knowledge transfer.
2. We compare WikiCiKE with KE-Tr on the
second testing data set A?test, where the
existences of values are not guaranteed in
those randomly selected articles, in order to
demonstrate the better recall of WikiCiKE.
For implementation details, the weighted-SVM
is used as the basic learner f both in WikiCiKE
and KE-Mon (Zhang et al, 2009), and Baidu
Translation API3 is used as the translator both in
WikiCiKE and KE-Tr. The Chinese texts are pre-
processed using ICTCLAS4 for word segmenta-
tion.
4.1.3 Evaluation Metrics
Following Lavelli?s research on evaluation of in-
formation extraction (Lavelli et al, 2008), we per-
form evaluation as follows.
1. We evaluate each attr separately.
2. For each attr, there is exactly one value ex-
tracted.
3. No alternative occurrence of real value is
available.
4. The overlap ratio is used in this paper rather
than ?exactly matching? and ?containing?.
Given an extracted value v? = {w?} and its
corresponding real value v = {w}, two measure-
ments for evaluating the overlap ratio are defined:
recall: the rate of matched tokens w.r.t. the real
value. It can be calculated using
R(v?, v) = |v ? v
?|
|v|
3http://openapi.baidu.com/service
4http://www.ictclas.org/
646
precision: the rate of matched tokens w.r.t. the
extracted value. It can be calculated using
P (v?, v) = |v ? v
?|
|v?|
We use the average of these two measures to
evaluate the performance of our extractor as fol-
lows:
R = avg(Ri(v?, v)) ai ? Atest
P = avg(Pi(v?, v)) ai ? Atest and vi? 6= ?
The recall and precision range from 0 to 1 and
are first calculated on a single instance and then
averaged over the testing instances.
4.2 Comparison with KE-Mon
In these experiments, WikiCiKE trains extractors
on AS ? AT , and KE-Mon trains extractors just
on AT . We incrementally increase the number of
target training articles from 10 to 500 (if available)
to compare WikiCiKE with KE-Mon in different
situations. We use the first testing data set Atest to
evaluate the results.
Figure 4 and Table 5 show the experimental re-
sults on TEMPLATE PERSON and FILM. We can
see that WikiCiKE outperforms KE-Mon on all
three attributions especially when the number of
target training samples is small. Although the re-
call for alma mater and the precision for nation-
ality of WikiCiKE are lower than KE-Mon when
only 10 target training articles are available, Wi-
kiCiKE performs better than KE-Mon if we take
into consideration both precision and recall.
10 30 50 100 200 300 500
0
0.2
0.4
0.6
0.8
number of target training articles
 
 
P(KE?Mon)
P(WikiCiKE)
R(KE?Mon)
R(WikiCiKE)
(a) occupation
10 30 50 100 200
0.4
0.5
0.6
0.7
0.8
0.9
1
number of target training articles
 
 
P(KE?Mon)
P(WikiCiKE)
R(KE?Mon)
R(WikiCiKE)
(b) alma mater
10 30 50 100 200 300
0.5
0.6
0.7
0.8
0.9
1
number of target training articles
 
 
P(KE?Mon)
P(WikiCiKE)
R(KE?Mon)
R(WikiCiKE)
(c) nationality
10 30 50 100 200 300 500
0
5
10
15
20
pe
rc
en
t(%
)
number of target training articles
performance gain
 
 
P
R
(d) average improvements
Figure 4: Results for TEMPLATE PERSON.
Figure 4(d) shows the average improvements
yielded by WikiCiKE w.r.t KE-Mon on TEM-
PLATE PERSON. We can see that WikiCiKE
yields significant improvements when only a few
articles are available in target language and the im-
provements tend to decrease as the number of tar-
get articles is increased. In this case, the articles
in the target language are sufficient to train the ex-
tractors alone.
# KE-Mon WikiCiKEP R P R
10 81.1% 63.8% 90.7% 66.3%
30 78.8% 64.5% 87.5% 69.4%
50 80.7% 66.6% 87.7% 72.3%
100 82.8% 68.2% 87.8% 72.1%
200 83.6% 70.5% 87.1% 73.2%
300 85.2% 72.0% 89.1% 76.2%
500 86.2% 73.4% 88.7% 75.6%
# Number of the target training articles.
Table 5: Results for country in TEMPLATE
FILM.
4.3 Comparison with KE-Tr
We compare WikiCiKE with KE-Tr on the second
testing data set A?test.
From Table 6 it can be clearly observed that Wi-
kiCiKE significantly outperforms KE-Tr both in
precision and recall. The reasons why the recal-
l of KE-Tr is extremely low are two-fold. First,
because of the limit of cross-lingual links and in-
foboxes in English Wikipedia, only a very smal-
l set of values is found by KE-Tr. Furthermore,
many values obtained using the translator are in-
correct because of translation errors. WikiCiKE
uses translators too, but it has better tolerance to
translation errors because the extracted value is
from the target article texts instead of the output
of translators.
Attribute KE-Tr WikiCiKEP R P R
occupation 27.4% 3.40% 64.8% 26.4%
nationality 66.3% 4.60% 70.0% 55.0%
alma mater 66.7% 0.70% 76.3% 8.20%
Table 6: Results of WikiCiKE vs. KE-Tr.
4.4 Significance Test
We conducted a significance test to demonstrate
that the difference between WikiCiKE and KE-
Mon is significant rather than caused by statistical
errors. As for the comparison between WikiCiKE
and KE-Tr, significant improvements brought by
647
WikiCiKE can be clearly observed from Table 6
so there is no need for further significance test.
In this paper, we use McNemar?s significance test
(Dietterich and Thomas, 1998).
Table 7 shows the results of significance test
calculated for the average on all tested attributes.
When the number of target training articles is less
than 100, the ? is much less than 10.83 that cor-
responds to a significance level 0.001. It suggests
that the chance that WikiCiKE is not better than
KE-Mon is less than 0.001.
# 10 30 50 100 200 300 500
? 179.5 107.3 51.8 32.8 4.1 4.3 0.3
# Number of the target training articles.
Table 7: Results of Significance Test.
4.5 Overall Analysis
As shown in above experiments, we can see that
WikiCiKE outperforms both KE-Mon and KE-Tr.
When only 30 target training samples are avail-
able, WikiCiKE reaches comparable performance
of KE-Mon using 300-500 target training samples.
Among all of the 72 attributes in TEMPLATE
PERSON of Chinese Wikipedia, 39 (54.17%) and
55 (76.39%) attributes have less than 30 and 200
labeled articles respectively. We can see that Wi-
kiCiKE can save considerable human labor when
no sufficient target training samples are available.
We also examined the errors by WikiCiKE and
they can be categorized into three classes. For at-
tribute occupation when 30 target training sam-
ples are used, there are 71 errors. The first cat-
egory is caused by incorrect word segmentation
(40.85%). In Chinese, there is no space between
words so we need to segment them before extrac-
tion. The result of word segmentation directly
decide the performance of extraction so it caus-
es most of the errors. The second category is be-
cause of the incomplete infoboxes (36.62%). In
evaluation of KE-Mon, we directly use the val-
ues in infoboxex as golden values, some of them
are incomplete so the correct predicted values will
be automatically judged as the incorrect in these
cases. The last category is mismatched words
(22.54%). The predicted value does not match the
golden value or a part of it. In the future, we can
improve the performance of WikiCiKE by polish-
ing the word segmentation result.
5 Related Work
Some approaches of knowledge extraction from
the open Web have been proposed (Wu et al,
2012; Yates et al, 2007). Here we focus on the
extraction inside Wikipedia.
5.1 Monolingual Infobox Extraction
KYLIN is the first system to autonomously ex-
tract the missing infoboxes from the correspond-
ing article texts by using a self-supervised learn-
ing method (Wu and Weld, 2007). KYLIN per-
forms well when enough training data are avail-
able. Such techniques as shrinkage and retraining
are proposed to increase the recall from English
Wikipedia?s long tail of sparse classes (Wu et al,
2008; Wu and Weld, 2010). Different from Wu?s
research, WikiCiKE is a cross-lingual knowledge
extraction framework, which leverags rich knowl-
edge in the other language to improve extraction
performance in the target Wikipedia.
5.2 Cross-lingual Infobox Completion
Current translation based methods usually con-
tain two steps: cross-lingual attribute alignmen-
t and value translation. The attribute alignmen-
t strategies can be grouped into two categories:
cross-lingual link based methods (Bouma et al,
2009) and classification based methods (Adar et
al., 2009; Nguyen et al, 2011; Aumueller et al,
2005; Adafre and de Rijke, 2006; Li et al, 2009).
After the first step, the value in the source lan-
guage is translated into the target language. E.
Adar?s approach gives the overall precision of
54% and recall of 40% (Adar et al, 2009). How-
ever, recall of these methods is limited by the
number of equivalent cross-lingual articles and the
number of infoboxes in the source language. It is
also limited by the quality of the translators. Wi-
kiCiKE attempts to mine the missing infoboxes
directly from the article texts and thus achieves
a higher recall compared with these methods as
shown in Section 4.3.
5.3 Transfer Learning
Transfer learning can be grouped into four cate-
gories: instance-transfer, feature-representation-
transfer, parameter-transfer and relational-
knowledge-transfer (Pan and Yang, 2010).
TrAdaBoost, the instance-transfer approach, is
an extension of the AdaBoost algorithm, and
demonstrates better transfer ability than tradition-
648
al learning techniques (Dai et al, 2007). Transfer
learning have been widely studied for classifica-
tion, regression, and cluster problems. However,
few efforts have been spent in the information
extraction tasks with knowledge transfer.
6 Conclusion and Future Work
In this paper we proposed a general cross-lingual
knowledge extraction framework called Wiki-
CiKE, in which extraction performance in the tar-
get Wikipedia is improved by using rich infobox-
es in the source language. The problems of topic
drift and translation error were handled by using
the TrAdaBoost model. Chinese-English exper-
imental results on four typical attributes showed
that WikiCiKE significantly outperforms both the
current translation based methods and the mono-
lingual extraction methods. In theory, WikiCiKE
can be applied to any two wiki knowledge based
of different languages.
We have been considering some future work.
Firstly, more attributes in more infobox templates
should be explored to make our results much
stronger. Secondly, knowledge in a minor lan-
guage may also help improve extraction perfor-
mance for a major language due to the cultural and
religion differences. A bidirectional cross-lingual
extraction approach will also be studied. Last but
not least, we will try to extract multiple attr-value
pairs at the same time for each article.
Furthermore, our work is part of a more ambi-
tious agenda on exploitation of linked data. On the
one hand, being able to extract data and knowl-
edge from multilingual sources such as Wikipedi-
a could help improve the coverage of linked data
for applications. On the other hand, we are also
investigating how to possibly integrate informa-
tion, including subjective information (Sensoy et
al., 2013), from multiple sources, so as to better
support data exploitation in context dependent ap-
plications.
Acknowledgement
The work is supported by NSFC (No. 61035004),
NSFC-ANR (No. 61261130588), 863 High Tech-
nology Program (2011AA01A207), FP7-288342,
FP7 K-Drive project (286348), the EPSRC WhatIf
project (EP/J014354/1) and THU-NUS NExT Co-
Lab. Besides, we gratefully acknowledge the as-
sistance of Haixun Wang (MSRA) for improving
the paper work.
References
S. Fissaha Adafre and M. de Rijke. 2006. Find-
ing Similar Sentences across Multiple Languages
in Wikipedia. EACL 2006 Workshop on New Text:
Wikis and Blogs and Other Dynamic Text Sources.
Sisay Fissaha Adafre and Maarten de Rijke. 2005.
Discovering Missing Links in Wikipedia. Proceed-
ings of the 3rd International Workshop on Link Dis-
covery.
Eytan Adar, Michael Skinner and Daniel S. Weld.
2009. Information Arbitrage across Multi-lingual
Wikipedia. WSDM?09.
David Aumueller, Hong Hai Do, Sabine Massmann and
Erhard Rahm?. 2005. Schema and ontology match-
ing with COMA++. SIGMOD Conference?05.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
So?ren Auer, Christian Becker, Richard Cyganiak
and Sebastian Hellmann. 2009. DBpedia - A crys-
tallization Point for the Web of Data. J. Web Sem..
Christian Bizer, Tom Heath, Kingsley Idehen and Tim
Berners-Lee. 2008. Linked data on the web (L-
DOW2008). WWW?08.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim S-
turge and Jamie Taylor. 2008. Freebase: a Collabo-
ratively Created Graph Database for Structuring Hu-
man Knowledge. SIGMOD?08.
Gosse Bouma, Geert Kloosterman, Jori Mur, Gertjan
Van Noord, Lonneke Van Der Plas and Jorg Tiede-
mann. 2008. Question Answering with Joost at
CLEF 2007. Working Notes for the CLEF 2008
Workshop.
Gosse Bouma, Sergio Duarte and Zahurul Islam.
2009. Cross-lingual Alignment and Completion of
Wikipedia Templates. CLIAWS3 ?09.
Wenyuan Dai, Qiang Yang, Gui-Rong Xue and Yong
Yu. 2007. Boosting for Transfer Learning. ICM-
L?07.
Dietterich and Thomas G. 1998. Approximate Statis-
tical Tests for Comparing Supervised Classification
Learning Algorithms. Neural Comput..
Sergio Ferra?ndez, Antonio Toral, ??scar Ferra?ndez, An-
tonio Ferra?ndez and Rafael Mun?oz. 2009. Exploit-
ing Wikipedia and EuroWordNet to Solve Cross-
Lingual Question Answering. Inf. Sci..
Aidan Finn and Nicholas Kushmerick. 2004. Multi-
level Boundary Classification for Information Ex-
traction. ECML.
Achille Fokoue, Felipe Meneguzzi, Murat Sensoy and
Jeff Z. Pan. 2012. Querying Linked Ontological
Data through Distributed Summarization. Proc. of
the 26th AAAI Conference on Artificial Intelligence
(AAAI2012).
649
Yoav Freund and Robert E. Schapire. 1997.
A Decision-Theoretic Generalization of On-Line
Learning and an Application to Boosting. J. Com-
put. Syst. Sci..
Norman Heino and Jeff Z. Pan. 2012. RDFS Rea-
soning on Massively Parallel Hardware. Proc. of
the 11th International Semantic Web Conference
(ISWC2012).
Aidan Hogan, Jeff Z. Pan, Axel Polleres and Yuan Ren.
2011. Scalable OWL 2 Reasoning for Linked Data.
Reasoning Web. Semantic Technologies for the Web
of Data.
Andreas Hotho, Robert Ja?schke, Christoph Schmitz
and Gerd Stumme. 2006. Information Retrieval in
Folksonomies: Search and Ranking. ESWC?06.
John D. Lafferty, Andrew McCallum and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. ICML?01.
Alberto Lavelli, MaryElaine Califf, Fabio Ciravegna,
Dayne Freitag, Claudio Giuliano, Nicholas Kush-
merick, Lorenza Romano and Neil Ireson. 2008.
Evaluation of Machine Learning-based Information
Extraction Algorithms: Criticisms and Recommen-
dations. Language Resources and Evaluation.
Juanzi Li, Jie Tang, Yi Li and Qiong Luo. 2009. Ri-
MOM: A Dynamic Multistrategy Ontology Align-
ment Framework. IEEE Trans. Knowl. Data Eng..
Xiao Ling, Gui-Rong Xue, Wenyuan Dai, Yun Jiang,
Qiang Yang and Yong Yu. 2008. Can Chinese We-
b Pages be Classified with English Data Source?.
WWW?08.
Sheila A. McIlraith, Tran Cao Son and Honglei Zeng.
2001. Semantic Web Services. IEEE Intelligent
Systems.
Thanh Hoang Nguyen, Viviane Moreira, Huong N-
guyen, Hoa Nguyen and Juliana Freire. 2011. Mul-
tilingual Schema Matching for Wikipedia Infoboxes.
CoRR.
Jeff Z. Pan and Edward Thomas. 2007. Approximat-
ing OWL-DL Ontologies. 22nd AAAI Conference
on Artificial Intelligence (AAAI-07).
Jeff Z. Pan and Ian Horrocks. 2007. RDFS(FA): Con-
necting RDF(S) and OWL DL. IEEE Transaction
on Knowledge and Data Engineering. 19(2): 192 -
206.
Jeff Z. Pan and Ian Horrocks. 2006. OWL-Eu: Adding
Customised Datatypes into OWL. Journal of Web
Semantics.
Sinno Jialin Pan and Qiang Yang. 2010. A Survey on
Transfer Learning. IEEE Trans. Knowl. Data Eng..
Nick Roussopoulos, Stephen Kelley and Fre?de?ric Vin-
cent. 1995. Nearest Neighbor Queries. SIGMOD
Conference?95.
Murat Sensoy, Achille Fokoue, Jeff Z. Pan, Timothy
Norman, Yuqing Tang, Nir Oren and Katia Sycara.
2013. Reasoning about Uncertain Information and
Conflict Resolution through Trust Revision. Proc.
of the 12th International Conference on Autonomous
Agents and Multiagent Systems (AAMAS2013).
Fabian M. Suchanek, Gjergji Kasneci and Gerhard
Weikum. 2007. Yago: a Core of Semantic Knowl-
edge. WWW?07.
Max Volkel, Markus Krotzsch, Denny Vrandecic,
Heiko Haller and Rudi Studer. 2006. Semantic
Wikipedia. WWW?06.
Zhichun Wang, Juanzi Li, Zhigang Wang and Jie Tang.
2012. Cross-lingual Knowledge Linking across Wi-
ki Knowledge Bases. 21st International World Wide
Web Conference.
Daniel S. Weld, Fei Wu, Eytan Adar, Saleema Amer-
shi, James Fogarty, Raphael Hoffmann, Kayur Pa-
tel and Michael Skinner. 2008. Intelligence in
Wikipedia. AAAI?08.
Fei Wu and Daniel S. Weld. 2007. Autonomously Se-
mantifying Wikipedia. CIKM?07.
Fei Wu and Daniel S. Weld. 2010. Open Information
Extraction Using Wikipedia. ACL?10.
Fei Wu, Raphael Hoffmann and Daniel S. Weld. 2008.
Information Extraction from Wikipedia: Moving
down the Long Tail. KDD?08.
Wentao Wu, Hongsong Li, Haixun Wang and Kenny
Qili Zhu. 2012. Probase: a Probabilistic Taxonomy
for Text Understanding. SIGMOD Conference?12.
Alexander Yates, Michael Cafarella, Michele Banko,
Oren Etzioni, Matthew Broadhead and Stephen
Soderland. 2007. TextRunner: Open Information
Extraction on the Web. NAACL-Demonstrations?07.
Xinfeng Zhang, Xiaozhao Xu, Yiheng Cai and Yaowei
Liu. 2009. A Weighted Hyper-Sphere SVM. IC-
NC(3)?09.
650
