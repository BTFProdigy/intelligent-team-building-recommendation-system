Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 104?107,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
English?Hindi Transliteration Using Context-Informed PB-SMT:    
the DCU System for NEWS 2009 
Rejwanul Haque, Sandipan Dandapat, Ankit Kumar Srivastava,  
Sudip Kumar Naskar and Andy Way 
CNGL, School of Computing 
Dublin City University, Dublin 9, Ireland 
{rhaque,sdandapat,snaskar,asrivastava,away}@computing.dcu.ie 
 
Abstract 
This paper presents English?Hindi translit-
eration in the NEWS 2009 Machine Translit-
eration Shared Task adding source context 
modeling into state-of-the-art log-linear 
phrase-based statistical machine translation 
(PB-SMT). Source context features enable us 
to exploit source similarity in addition to tar-
get similarity, as modelled by the language 
model. We use a memory-based classification 
framework that enables efficient estimation of 
these features while avoiding data sparseness 
problems.We carried out experiments both at 
character and transliteration unit (TU) level. 
Position-dependent source context features 
produce significant improvements in terms of 
all evaluation metrics. 
1 Introduction 
Machine Transliteration is of key importance in 
many cross-lingual natural language processing 
applications, such as information retrieval, ques-
tion answering and machine translation (MT). 
There are numerous ways of performing auto-
matic transliteration, such as noisy channel mod-
els (Knight and Graehl, 1998), joint source chan-
nel models (Li et al, 2004), decision-tree models 
(Kang and Choi, 2000) and statistical MT models 
(Matthews, 2007). 
For the shared task, we built our machine 
transliteration system based on phrase-based sta-
tistical MT (PB-SMT) (Koehn et al, 2003) using 
Moses (Koehn et al, 2007).  We adapt PB-SMT 
models for transliteration by translating charac-
ters rather than words as in character-level trans-
lation systems (Lepage & Denoual, 2006). How-
ever, we go a step further from the basic PB-
SMT model by using source-language context 
features (Stroppa et al, 2007). We also create 
translation models by constraining the character-
level segmentations, i.e. treating a consonant-
vowel cluster as one transliteration unit.  
The remainder of the paper is organized as fol-
lows. In section 2 we give a brief overview of 
PB-SMT. Section 3 describes how context-
informed features are incorporated into state-of-
art log-linear PB-SMT. Section 4 includes the 
results obtained, together with some analysis. 
Section 5 concludes the paper. 
2 Log-Linear PB-SMT  
Translation is modelled in PB-SMT as a decision 
process, in which the translation Ie1 = e1 . . .  eI of 
a source sentence Jf1 = f1 . . . fJ is chosen to 
maximize (1): 
)1()().|(maxarg)|(maxarg 111
,
11
, 11
IIJ
eI
JI
eI
ePefPfeP
II
?  
where )|( 11
IJ efP  and )( 1
IeP  denote respec-
tively the translation model and the target lan-
guage model (Brown et al, 1993). In log-linear 
phrase-based SMT, the posterior probability 
)|( 11
JI feP  is directly modelled as a (log-linear) 
combination of features (Och and Ney, 2002), 
that usually comprise M translational features, 
and the language model, as in (2): 
?
?
?
m
m
KIJ
mm
JI sefhfeP
1
11111 ),,()|(log ?   
                             )(log 1
I
LM eP??                  (2) 
where k
K sss ...11 ?  denotes a segmentation of the 
source and target sentences respectively into the 
sequences of phrases )?,...,?( 1 kee  and )
?,...,?( 1 kff  
such that (we set i0 = 0) (3): 
,1 Kk ???  sk = (ik ; bk, jk), 
          
kk iik
eee ...? 11 ??? , 
                      
kk jbk
fff ...? ?                              (3) 
The translational features involved depend 
only on a pair of source/target phrases and do not 
take into account any context of these phrases. 
This means that each feature mh   in (2) can be 
rewritten as in (4): 
104
?
?
?
K
k
kkkm
KIJ
m sefhsefh
1
111 ),?,?(?),,(           (4) 
where mh? is a feature that applies to a single 
phrase-pair. Thus (2) can be rewritten as: 
? ??
? ??
?
K
k
K
k
kkkkkkm
m
m
m sefhsefh
1 11
),?,?(?),?,?(??        (5) 
where, m
m
m
mhh ??
1
?
?
? ? . In this context, the transla-
tion process amounts to: (i) choosing a segmen-
tation of the source sentence, (ii) translating each 
source phrase. 
3 Source Context Features in Log-
Linear PB-SMT 
The context of a source phrase kf?  is defined as 
the sequence before and after a focus phrase kf?  
=
kk ji
ff ... . Source context features (Stroppa et 
al., 2007) include the direct left and right context 
words (in our case, character/TU instead of word) 
of length l (resp. lii kk ff ?? ...1  and ljj kk ff ?? ...1 ) of 
a given focus phrase kf? = kk ji ff ... . A window of 
size 2l+1 features including the focus phrase is 
formed. Thus lexical contextual information (CI) 
can be described as in (6): 
CI = }...,...{ 11 ljjili kkkk ffff ????                    (6) 
As in (Haque et al, 2009), we considered a 
context window of ?1 and ?2 (i.e. l=1, 2) for our 
experiments. 
One natural way of expressing a context-
informed feature is as the conditional probability 
of the target phrase given the source phrase and 
its context information, as in (7): 
mh? ( kf? ,CI( kf? ), ke? , sk) = log P( ke? | kf? , CI( kf? ))  (7) 
3.1 Memory-Based Classification 
As (Stroppa et al, 2007) point out, directly esti-
mating P( ke? | kf? , CI( kf? )) using relative fre-
quencies is problematic. Indeed, Zens and Ney 
(2004) showed that the estimation of P( ke? | kf? ) 
using relative frequencies results in the overesti-
mation of the probabilities of long phrases, so 
smoothing factors in the form of lexical-based 
features are often used to counteract this bias 
(Foster et al, 2006). In the case of context-
informed features, since the context is also taken 
into account, this estimation problem can only 
become worse. To avoid such problems, in this 
work we use three memory-based classifiers: 
IGTree, IB1 and TRIBL 1  (Daelemans et al, 
2005). When predicting a target phrase given a 
source phrase and its context, the source phrase 
is intuitively the feature with the highest predic-
tion power; in all our experiments, it is the fea-
ture with the highest gain ratio (GR).  
In order to build the set of examples required 
to train the classifier, we modify the standard 
phrase-extraction method of (Koehn et al, 2003) 
to extract the context of the source phrases at the 
same time as the phrases themselves. Importantly, 
therefore, the context extraction comes at no ex-
tra cost.  
We refer interested readers to (Stroppa et al, 
2007) and (Haque et al, 2009) as well as the ref-
erences therein for more details of how Memory-
Based Learning (MBL) is used for classification 
of source examples for use in the log-linear MT 
framework. 
3.2 Implementation Issues 
We split named entities (NE) into characters. We 
break NEs into transliteration units (TU), which 
bear close resemblance to syllables. We split 
English NEs into TUs having C*V* pattern and 
Hindi NEs are divided into TUs having Ch+M 
pattern (M: Hindi Matra / vowel modifier, Ch: 
Characters other than Matras). We carry out ex-
periments on both character-level (C-L) and TU-
level (TU-L) data. We use a 5-gram language 
model for all our experiments. The Moses PB-
SMT system serves as our baseline system. 
The distribution of target phrases given a 
source phrase and its contextual information is 
normalised to estimate P( ke? | kf? ,CI( kf? )). There-
fore our expected feature is derived as in (8): 
mblh? = log P( ke? | kf? ,CI( kf? ))                         (8) 
As for the standard phrase-based approach, 
their weights are optimized using Minimum Er-
ror Rate Training (MERT) of (Och, 2003) for 
each of the experiments. 
As (Stroppa et al, 2007) point out, PB-SMT 
decoders such as Pharaoh (Koehn, 2004) or 
Moses (Koehn, 2007) rely on a static phrase-
table represented as a list of aligned phrases ac-
companied with several features. Since these fea-
                                               
1 An implementation of IGTree, IB1 and TRIBL is available 
in the TiMBL software package (http://ilk.uvt.nl/timbl). 
 
105
tures do not express the context in which those 
phrases occur, no context information is kept in 
the phrase-table, and there is no way to recover 
this information from the phrase-table. 
In order to take into account the context-
informed features for use with such decoders, the 
devset and testset that need to be translated are 
pre-processed. Each token appearing in the test-
set and devset is assigned a unique id. First we 
prepare the phrase table using the training data. 
Then we generate all possible phrases from the 
devset and testset. These devset and testset 
phrases are then searched for in the phrase table, 
and if found, then the phrase along with its con-
textual information is given to MBL for classifi-
cation. MBL produces class distributions accord-
ing to the maximum-match of the features con-
tained in the source phrase. We derive new 
scores from this class distribution and merge 
them with the initial information contained in the 
phrase table to take into account our feature 
functions ( mblh? ) in the log-linear model (2). 
In this way we create a dynamic phrase table 
containing both the standard and the context-
informed features. The new phrase table contains 
the source phrase (represented by the sequence 
of ids of the words composing the phrase), target 
phrase and the new score. 
Similarly, replacing all the words by their ids 
in the development set, we perform MERT using 
our new phrase table to optimize the feature 
weights. We translate the test set (words repre-
sented by ids) using our new phrase table. 
4 Results and Analysis 
We used 10,000 NEs from the NEWS 2009 Eng-
lish?Hindi training data (Kumaran and Kellner, 
2007) for the standard submission, and the addi-
tional English?Hindi parallel person names data 
(105,905 distinct name pairs) of the Election 
Commission of India2 for the non-standard sub-
missions. In addition to the baseline Moses sys-
tem, we carried out three different set of experi-
ments on IGTree, IB1 and TRIBL. Each of these 
experiments was carried out on both the standard 
data and the combined larger data, both at char-
acter level and the TU level, and considering 
?1/?2 tokens as context. For each experiment, 
we produce the 10-best distinct hypotheses. The 
results are shown in Table 1. 
We observed that many of the (unseen) TUs in 
the testset remain untranslated in TU-L systems 
                                               
2 http://www.eci.gov.in/DevForum/Fullname.asp 
due to the problems of data sparseness. When-
ever a TU-L system fails to translate a TU, we 
fallback on the corresponding C-L system to 
translate the TU as a post-processing step. 
The accuracy of the TU-L baseline system 
(0.391) is much higher compared to the C-L 
baseline system (0.290) on standard dataset. Fur-
thermore, contextual modelling of the source 
language gives an accuracy of 0.416 and 0.399 
for TU-L system and C-L system respectively. 
Similar trends are observed in case of larger 
dataset. However, the highest accuracy (0.445) 
has been achieved with the TU-L system using 
the larger dataset. 
5 Conclusion 
In this work, we employed source context model-
ing into the state-of-the-art log-linear PB-SMT 
for the English?Hindi transliteration task. We 
have shown that taking source context into ac-
count substantially improve the system perform-
ance (an improvement of 43.44% and 26.42% 
respectively for standard and larger datasets). 
IGTree performs best for TU-L systems while 
TRIBL seems to perform better for C-L systems 
on both standard and non-standard datasets. 
Acknowledgements 
We would like to thank Antal van den Bosch for 
his input on the use of memory based classifiers. 
We are grateful to SFI (http://www.sfi.ie) for 
generously sponsoring this research under grant 
07/CE/I1142. 
References  
Adimugan Kumaran and Tobias Kellner. A generic 
framework for machine transliteration. Proc. of the 
30th SIGIR, 2007. 
Byung-Ju Kang and Key-Sun Choi. Automatic trans-
literation and back-transliteration by decision tree 
learning. 2000. Proc. of LREC-2000, Athens, 
Greece, pp. 1135-1141. 
David Matthews. 2007. Machine Transliteration of 
Proper Names. Master's Thesis, University of Ed-
inburgh, Edinburgh, United Kingdom. 
Franz Och and Hermann Ney. 2002. Discriminative 
training and maximum entropy models for statisti-
cal machine translation. Proc. of ACL 2002, Phila-
delphia, PA, pp. 295?302. 
George Foster, Roland Kuhn, and Howard Johnson. 
2006. Phrasetable smoothing for statistical machine 
translation.  Proc. of EMNLP-2006, Sydney, Aus-
tralia, pp. 53-61. 
106
Table1: Experimental Results (S/B ? Standard / Big data, S*? TM on Standard data, but LM on Big data, 
C/TU ? Character / TU level, SD? Standard submission, NSD? Non-standard submission). Better results with 
bold faces have not been submitted in the NEWS 2009 Machine Transliteration Shared Task. 
Haizhou Li, Zhang Min and Su Jian. 2004. A joint 
source-channel model for machine translitera-
tion. Proc. of ACL 2004, Barcelona, Spain, 
pp.159-166. 
Kevin Knight and Jonathan Graehl. 1998. Machine 
Transliteration. Computational Linguistics, 
24(4):559-612. 
Nicolas Stroppa, Antal van den Bosch and Andy 
Way. 2007. Exploiting Source Similarity for 
SMT using Context-Informed Features. Proc. of  
TMI-2007, Sk?vde, Sweden, pp. 231-240. 
Peter F. Brown, S. A. D. Pietra, V. J. D. Pietra and 
R. L. Mercer. 1993. The mathematics of statisti-
cal machine translation: parameter estimation. 
Computational Linguistics 19 (2), pp. 263-311. 
Philipp Koehn, F. J. Och, and D. Marcu. 2003. Sta-
tistical phrase-based translation. Proc. of HLT-
NAACL 2003, Edmonton, Canada, pp. 48-54. 
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine trans-
lation models. Machine translation: from real 
users to research: Proc. of AMTA 2004, Berlin: 
Springer Verlag, 2004, pp. 115-124. 
Philipp Koehn, H. Hoang, A. Birch, C. Callison-
Burch, M. Federico, N. Bertoldi, B. Cowan, W. 
Shen, C. Moran, R. Zens, C. Dyer,  O. Bojar, A. 
Constantin and E. Herbst. 2007. Moses: open 
source toolkit for statistical machine translation. 
Proc. of ACL, Prague, Czech Republic, pp. 177-
180. 
Rejwanul Haque, Sudip Kumar Naskar, Yanjun Ma 
and Andy Way. 2009. Using Supertags as Source 
Language Context in SMT. Proc. of EAMT-09, 
Barcelona, Spain, pp. 234-241. 
Richard Zens and Hermann Ney. 2004. Improve-
ments in phrase-based statistical machine trans-
lation. Proc. of HLT/NAACL 2004, Boston, MA, 
pp. 257?264. 
Walter Daelemans & Antal van den Bosch. 2005. 
Memory-based language processing. Cambridge, 
UK, Cambridge University Press. 
Yves Lepage and Etienne Denoual. 2006. Objective 
evaluation of the analogy-based machine transla-
tion system ALEPH. Proc. of the 12th Annual 
Meeting of the Association of NLP, pp. 873-876. 
 S/B C/TU Context ACC M-F-Sc MRR MAP_ref MAP_10 MAP_sys 
C 0 .290 .814 .393 .286 .131 .131  
S TU 0 .391 .850 .483 .384 .160 .160 
C 0 .352 .830 .463 .346 .156 .156 
 
Baseline 
Moses  
B TU 0 .407 .853 .500 .402 .165 .165 
?1 .391 .858 .501 .384 .166 .166  
C ?2 .386 .860 .479 .379 .155 .155 
?1 .406 .858 .466 .398 .178 .178 
 
S 
 
TU ?2 .359 .838 .402 .349 .165 .165 
?1 .431 .865 .534 .423 .177 .177  
C ?2 (NSD1) .420 .867 .519 .413 .170 .170 
?1 .437 .863 .507 .429 .191 .191 
 
 
 
 
IB1  
B 
 
TU ?2 .427 .862 .487 .418 .194 .194 
?1 .372 .849 .482 .366 .160 .160  
C ?2 .371 .847 .476 .364 .156 .156 
?1 .412 .859 .486 .404 .164 .164 
 
S 
 
TU ?2 .416 .860 .493 .409 .166 .166 
?1 .413 .855 .518 .406 .173 .173  
C ?2 (NSD2) .407 .856 .507 .399 .168 .168 
?1 .445 .864 .527 .440 .176 .176 
 
 
 
 
IGTree  
B 
 
TU ?2 .427 .861 .516 .422 .173 .173 
?1 .382 .854 .493 .375 .164 .164  
C ?2 (SD) .399 .863 .488 .392 .157 .157 
?1 .408 .858 .474 .400 .181 .181 
 
S 
 
TU ?2 .395 .857 .453 .385 .182 .182 
?1 .439 .866 .543 .430 .179 .179  
C ?2 (NSD3) .421 .864 .519 .415 .171 .171 
?1 .444 .863 .512 .436 .193 .193 
 
 
 
 
TRIBL  
B 
 
TU ?2 .439 .865 .497 .430 .197 .197 
 S* C ?2 (NSD4) .419 .868 .464 .419 .338 .338 
107
Bengali, Hindi and Telugu to English Ad-hoc Bilingual task  
 
Sivaji Bandyopadhyay, Tapabrata Mondal, Sudip Kumar Naskar, 
Asif Ekbal, Rejwanul Haque, Srinivasa Rao Godavarthy 
 
Abstract 
 
This paper presents the experiments carried out at Jadavpur University as 
part of participation in the CLEF 2007 ad-hoc bilingual task. This is our first 
participation in the CLEF evaluation task and we have considered Bengali, 
Hindi and Telugu as query languages for the retrieval from English 
document collection. We have discussed our Bengali, Hindi and Telugu to 
English CLIR system as part of the ad-hoc bilingual task, English IR system 
for the ad-hoc monolingual task and the associated experiments at CLEF. 
Query construction was manual for Telugu-English ad-hoc bilingual task, 
while it was automatic for all other tasks. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 191?198,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Modified Joint Source-Channel Model for Transliteration 
 
 
Asif Ekbal 
Comp.  Sc. & Engg. Deptt. 
 Jadavpur University 
India 
ekbal_asif12@ 
yahoo.co.in 
Sudip Kumar Naskar 
Comp.  Sc. & Engg. Deptt. 
Jadavpur University 
India 
sudip_naskar@ 
hotmail.com 
Sivaji Bandyopadhyay 
Comp.  Sc. & Engg. Deptt.  
Jadavpur University 
India 
sivaji_cse_ju@ 
yahoo.com 
 
 
Abstract 
Most machine transliteration systems 
transliterate out of vocabulary (OOV) 
words through intermediate phonemic 
mapping. A framework has been 
presented that allows direct 
orthographical mapping between two 
languages that are of different origins 
employing different alphabet sets. A 
modified joint source?channel model 
along with a number of alternatives have 
been proposed. Aligned transliteration 
units along with their context are 
automatically derived from a bilingual 
training corpus to generate the 
collocational statistics. The transliteration 
units in Bengali words take the pattern 
C+M where C represents a vowel or a 
consonant or a conjunct and M represents 
the vowel modifier or matra. The English 
transliteration units are of the form C*V* 
where C represents a consonant and V 
represents a vowel. A Bengali-English 
machine transliteration system has been 
developed based on the proposed models. 
The system has been trained to 
transliterate person names from Bengali 
to English. It uses the linguistic 
knowledge of possible conjuncts and 
diphthongs in Bengali and their 
equivalents in English. The system has 
been evaluated and it has been observed 
that the modified joint source-channel 
model performs best with a Word 
Agreement Ratio of 69.3% and a 
Transliteration Unit Agreement Ratio of 
89.8%.    
1 Introduction 
In Natural Language Processing (NLP) 
application areas such as information retrieval, 
question answering systems and machine 
translation, there is an increasing need to 
translate OOV words from one language to 
another. They are translated through 
transliteration, the method of translating into 
another language by expressing the original 
foreign words using characters of the target 
language preserving the pronunciation in their 
original languages. Thus, the central problem in 
transliteration is predicting the pronunciation of 
the original word. Transliteration between two 
languages, that use the same set of alphabets, is 
trivial: the word is left as it is. However, for 
languages that use different alphabet sets, the 
names must be transliterated or rendered in the 
target language alphabets.  
Technical terms and named entities make up 
the bulk of these OOV words. Named entities 
hold a very important place in NLP applications. 
Proper identification, classification and 
translation of named entities are very crucial in 
many NLP applications and pose a very big 
challenge to NLP researchers. Named entities are 
usually not found in bilingual dictionaries and 
they are very productive in nature. Translation of 
named entities is a tricky task: it involves both 
translation and transliteration. Transliteration is 
commonly used for named entities, even when 
the words could be translated. Different types of 
named entities are translated differently. 
Numerical and temporal expressions typically 
use a limited set of vocabulary words (e.g., 
names of months, days of the week etc.) and can 
be translated fairly easily using simple 
translation patterns. The named entity machine 
transliteration algorithms presented in this work 
191
focus on person names, locations and 
organizations. A machine transliteration system 
that is trained on person names is very important 
in a multilingual country like India where large 
name collections like census data, electoral roll 
and railway reservation information must be 
available to multilingual citizens of the country 
in their vernacular. In the present work, the 
various proposed models have been evaluated on 
a training corpus of person names. 
A hybrid neural network and knowledge-based 
system to generate multiple English spellings for 
Arabic personal names is described in (Arbabi et 
al., 1994). (Knight and Graehl, 1998) developed 
a phoneme-based statistical model using finite 
state transducer that implements transformation 
rules to do back-transliteration. (Stalls and 
Knight, 1998) adapted this approach for back 
transliteration from Arabic to English for English 
names. A spelling-based model is described in 
(Al-Onaizan and Knight, 2002a; Al-Onaizan and 
Knight, 2002c) that directly maps English letter 
sequences into Arabic letter sequences with 
associated probability that are trained on a small 
English/Arabic name list without the need for 
English pronunciations. The phonetics-based and 
spelling-based models have been linearly 
combined into a single transliteration model in 
(Al-Onaizan and Knight, 2002b) for 
transliteration of Arabic named entities into 
English.  
Several phoneme-based techniques have been 
proposed in the recent past for machine 
transliteration using transformation-based 
learning algorithm (Meng et al, 2001; Jung et 
al., 2000; Vigra and Khudanpur, 2003). 
(Abduljaleel and Larkey, 2003) have presented a 
simple statistical technique to train an English-
Arabic transliteration model from pairs of names. 
The two-stage training procedure first learns 
which n-gram segments should be added to 
unigram inventory for the source language, and 
then a second stage learns the translation model 
over this inventory. This technique requires no 
heuristic or linguistic knowledge of either 
language. 
 (Goto et al, 2003) described an English-
Japanese transliteration method in which an 
English word is divided into conversion units 
that are partial English character strings in an 
English word and each English conversion unit is 
converted into a partial Japanese Katakana 
character string. It calculates the likelihood of a 
particular choice of letters of chunking into 
English conversion units for an English word by 
linking them to Katakana characters using 
syllables. Thus the English conversion units 
consider phonetic aspects. It considers the 
English and Japanese contextual information 
simultaneously to calculate the plausibility of 
conversion from each English conversion unit to 
various Japanese conversion units using a single 
probability model based on the maximum 
entropy method. 
 (Haizhou et al, 2004) presented a framework 
that allows direct orthographical mapping 
between English and Chinese through a joint 
source-channel model, called n-gram 
transliteration model. The orthographic 
alignment process is automated using the 
maximum likelihood approach, through the 
Expectation Maximization algorithm to derive 
aligned transliteration units from a bilingual 
dictionary. The joint source-channel model tries 
to capture how source and target names can be 
generated simultaneously, i.e., the context 
information in both the source and the target 
sides are taken into account. 
A tuple n-gram transliteration model (Marino 
et al, 2005; Crego et al, 2005) has been log-
linearly combined with feature functions to 
develop a statistical machine translation system 
for Spanish-to-English and English-to-Spanish 
translation tasks. The model approximates the 
joint probability between source and target 
languages by using trigrams. 
The present work differs from (Goto et al, 
2003; Haizhou et al, 2004) in the sense that 
identification of the transliteration units in the 
source language is done using regular 
expressions and no probabilistic model is used. 
The proposed modified joint source-channel 
model is similar to the model proposed by (Goto 
et. al., 2003) but it differs in the way the 
transliteration units and the contextual 
information are defined in the present work. No 
linguistic knowledge is used in (Goto et al, 
2003; Haizhou et al, 2004) whereas the present 
work uses linguistic knowledge in the form of 
possible conjuncts and diphthongs in Bengali. 
The paper is organized as follows. The 
machine transliteration problem has been 
formulated under both noisy-channel model and 
joint source-channel model in Section 2. A 
number of transliteration models based on 
collocation statistics including the modified joint 
source-channel model and their evaluation 
scheme have been proposed in Section 3. The 
Bengali-English machine transliteration scenario 
has been presented in Section 4. The proposed 
192
models have been evaluated and the result of 
evaluation is reported in Section 5. The 
conclusion is drawn in Section 6. 
2 Machine Transliteration and Joint 
Source-Channel Model 
A transliteration system takes as input a character 
string in the source language and generates a 
character string in the target language as output. 
The process can be conceptualized as two levels 
of decoding: segmentation of the source string 
into transliteration units; and relating the source 
language transliteration units with units in the 
target language, by resolving different 
combinations of alignments and unit mappings. 
The problem of machine transliteration has been 
studied extensively in the paradigm of the noisy 
channel model.  
For a given Bengali name B as the observed 
channel output, we have to find out the most 
likely English transliteration E that maximizes 
P(E?B). Applying Bayes? rule, it means to find 
E to maximize 
  P(B,E) = P(B?E) * P(E)                             (1) 
with equivalent effect. This is equivalent to 
modelling two probability distributions: P(B|E), 
the probability of transliterating E to B through a 
noisy channel, which is also called 
transformation rules, and P(E), the probability 
distribution of source, which reflects what is 
considered good English transliteration in 
general. Likewiswe, in English to Bengali (E2B) 
transliteration, we could find B that maximizes 
P(B,E) = P(E?B) * P(B)                               (2) 
for a given English name. In equations (1) and 
(2), P(B) and P(E) are usually estimated using n-
gram language models. Inspired by research 
results of grapheme-to-phoneme research in 
speech synthesis literature, many have suggested 
phoneme-based approaches to resolving P(B?E) 
and P(E?B), which approximates the probability 
distribution by introducing a phonemic 
representation. In this way, names in the source 
language, say B, are converted into an 
intermediate phonemic representation P, and then 
the phonemic representation is further converted 
into the target language, say English E. In 
Bengali to English (B2E) transliteration, the 
phoneme-based approach can be formulated as 
P(E?B) = P(E?P) * P(P?B) and conversely we 
have P(B?E) = P(B?P) * P(P?E) for E2B back-
transliteration. 
However, phoneme-based approaches are 
limited by a major constraint that could 
compromise transliteration precision. The 
phoneme-based approach requires derivation of 
proper phonemic representation for names of 
different origins. One may need to prepare 
multiple language-dependent grapheme-to-
phoneme(G2P) and phoneme-to-grapheme(P2G) 
conversion systems accordingly, and that is not 
easy to achieve. 
In view of close coupling of the source and 
target transliteration units, a joint source-channel 
model, or n-gram transliteration model (TM) has 
been proposed in (Haizhou et al, 2004). For K 
alligned transliteration units, we have 
P(B,E) = P(
  
b1, b2.....bk, e1, e2......ek ) 
           = P (<b,e>1, <b,e>2, .....<b,e>k) 
              K   
           = ? P ( <b,e>k? <b,e>1k-1)               (3) 
              k=1 
which provides an alternative to the phoneme-
based approach for resolving equations (1) and 
(2) by eliminating the intermediate phonemic 
representation. 
Unlike the noisy-channel model, the joint 
source-channel model does not try to capture 
how source names can be mapped to target 
names, but rather how source  and target names 
can be generated simultaneously. In other words, 
a joint probability model is estimated  that can be 
easily marginalized in order to yield conditional 
probability models for both transliteration  and 
back-transliteration. 
Suppose that we have a Bengali name ? = 
x1x2............xm  and an English transliteration ? = 
y1y2........yn where xi, i = 1: m are Bengali 
transliteration units and yj, j = 1: n are English 
transliteration units. An English transliteration 
unit may correspond to zero, one or more than 
one transliteration unit in Bengali. Often the 
values of m and n are different. 
 
x1 x2x3..... xi-1xixi+1....xm 
      
 
         y1      y2 ..yi .... yn 
 
where there exists an alignment ? with <b,e>1 
= <x1,y1>; <b,e>2 = <x2x3, y2>; ?. and <b,e>k = 
<xm,yn>. A transliteration unit correspondence 
<b, e> is called a transliteration pair. Thus B2E 
transliteration can be formulated as    
 
         ?  = argmax P (?, ?, ? )          (4) 
                   ?, ?  
 
and similarly the E2B back-transliteration as  
193
  ?   = argmax P (?, ?, ? )         (5) 
                   ?, ?  
An n-gram transliteration model is defined as 
the conditional probability or transliteration 
probability of a transliteration pair <b, e>k 
depending on its immediate n predecessor pairs: 
 
  P (B, E) = P (?, ?, ?) 
                         
               K   
           = ? P ( <b, e>k? <b, e>k-n+1k-1)     (6) 
             k=1   
3 Proposed Models and Evaluation 
Scheme 
  Machine transliteration has been viewed as a 
sense disambiguation problem. A number of 
transliteration models have been proposed that 
can generate the English transliteration from a 
Bengali word that is not registered in any 
bilingual or pronunciation dictionary. The 
Bengali word is divided into Transliteration 
Units (TU) that have the pattern C+M, where C 
represents a vowel or a consonant or conjunct 
and M represents the vowel modifier or matra. 
An English word is divided into TUs that have 
the pattern C*V*, where C represents a 
consonant and V represents a vowel. The TUs 
are considered as the lexical units for machine 
transliteration. The system considers the Bengali 
and English contextual information in the form 
of collocated TUs simultaneously to calculate the 
plausibility of transliteration from each Bengali 
TU to various English candidate TUs and 
chooses the one with maximum probability. This 
is equivalent to choosing the most appropriate 
sense of a word in the source language to identify 
its representation in the target language. The 
system learns the mappings automatically from 
the bilingual training corpus guided by linguistic 
features. The output of this mapping process is a 
decision-list classifier with collocated TUs in the 
source language and their equivalent TUs in 
collocation in the target language along with the 
probability of each decision obtained from a 
training corpus. The machine transliteration of 
the input Bengali word is obtained using direct 
orthographic mapping by identifying the 
equivalent English TU for each Bengali TU in 
the input and then placing the English TUs in 
order. The various proposed models differ in the 
nature of collocational stastistics used during 
machine transliteration process: monogram 
model with no context, bigram model with 
previous (with respect to the current TU to be 
transliterated) source TU as the context, bigram 
model with next source TU as the context, 
bigram model with previous source and target 
TUs as the context (this is the joint source 
channel model), trigram model with previous and 
next source TUs as the context and the modified 
joint source-channel model with previous and 
next source TUs and the previous target TU as 
the context.  
 
? Model A 
 
In this model, no context is considered in 
either the source or the target side. This is 
essentially the monogram model. 
                K 
P(B,E) = ? P(<b,e>k) 
                k=1 
 
? Model B 
 
This is essentially a bigram model with 
previous source TU, i.e., the source TU occurring 
to the left of the current TU to be transliterated, 
as the context. 
                K 
P(B,E) = ? P(<b,e>k | bk-1) 
              k=1  
 
?Model C 
 
 This is  essentially a bigram model with next 
source TU, i.e., the source TU occurring to the 
right of the current TU to be transliterated, as the 
context. 
                K 
P(B,E) =  ?  P(<b,e>k? bk+1 )           
               k=1   
 
? Model D 
 
This is essentially the joint source-channel 
model where the previous TUs in both the source 
and the target sides are considered as the context. 
The previous TU on the target side refers to the 
transliterated TU to the immediate left of the 
current target TU to be transliterated. 
                 K 
P(B,E) =  ? P( <b,e>k ?? | <b,e>k-1) 
                k=1 
 
 
 
194
? Model E 
 
This is basically the trigram model where the 
previous and the next source TUs are considered 
as the context  
                K 
P(B,E) =  ? P(<b,e>k | bk-1, bk+1) 
                k=1 
  
? Model F 
 
In this model, the previous and the next TUs in 
the source and the previous target TU are 
considered as the context. This is the modified 
joint source-channel model . 
                K 
P(B,E) = ? P (<b,e>k | <b,e>k-1, bk+1) 
              k=1  
 
The performance of the system is evaluated in 
terms of Transliteration Unit Agreement Ratio 
(TUAR) and Word Agreement Ratio (WAR) 
following the evaluation scheme in (Goto et al, 
2003). The evaluation parameter Character 
Agreement Ratio in (Goto et al, 2003) has been 
modified to Transliteration Unit Agreement 
Ratio as vowel modifier matra symbols in 
Bengali words are not independent and must 
always follow a consonant or a conjunct in a 
Transliteration Unit. Let, B be the input Bengali 
word, E be the English transliteration given by 
the user in open test and E/ be the system 
generates the transliteration..TUAR is defined as, 
TUAR = (L-Err)/ L, where L is the number of 
TUs in E, and Err is the number of wrongly 
transliterated TUs in E/ generated by the system. 
WAR is defined as, WAR= (S-Err/) / S, where S 
is the test sample size and Err/ is is the number of 
erroneous names generated by the system (when 
E/ does not match with E). Each of these models 
has been evaluated with linguistic knowledge of 
the set of possible conjuncts and diphthongs in 
Bengali and their equivalents in English. It has 
been observed that the Modified Joint Source 
Channel Model with linguistic knowledge 
performs best in terms of Word Agreement Ratio 
and Transliteration Unit Agreement Ratio. 
4 Bengali-English Machine 
Transliteration 
Translation of named entities is a tricky task: it 
involves both translation and transliteration. 
Transliteration is commonly used for named 
entities, even when the words could be translated 
[LXT?? V_ (janata dal) is translated to Janata Dal 
(literal translation) although LXT?? (Janata) and 
V_ (Dal) are vocabulary words]. On the other 
hand ^?V[?Y??[? ?[? ?`?[?V??_?^  (jadavpur 
viswavidyalaya) is translated to Jadavpur 
University in which ^?V[?Y??[? (Jadavpur) is 
transliterated to Jadavpur and ?[? ?`?[?V??_?^  
(viswavidyalaya) is translated to University.  
A bilingual training corpus has been kept that 
contains entries mapping Bengali names to their 
respective English transliterations. To 
automatically analyze the bilingual training 
corpus to acquire knowledge in order to map new 
Bengali names to English, TUs are extracted 
from the Bengali names and the corresponding 
English names, and Bengali TUs are associated 
with their English counterparts. 
Some examples are given below: 
%?\?X?VX (abhinandan) ? [% | ?\? | X | ?V | X] 
abhinandan  ? [a | bhi | na | nda | n ]  
E??b?]??T?? (krishnamoorti) ?  [E?? | b? | ]? | ?T??]  
krishnamurthy ? [ kri | shna | mu | rthy ]  
?`?E????? (srikant) ? [ ?`? | E?? | ??? ] 
srikant ? [ sri | ka | nt ]  
 
After retrieving the transliteration units from a 
Bengali-English name pair, it associates the     
Bengali TUs to the English TUs along with the 
TUs in context. 
For example, it derives the following 
transliteration pairs or rules from the name-pair: 
?[??[???V?X?U (rabindranath)  ?   rabindranath 
  
Source Language                 Target Language 
                      
previous TU  TU  next TU       previous TU    TU        
          -            ?[?      [??   ?       -                ra 
     ?[          [??     ?V?  ?           ra               bi  
     [??      ?V?     X?   ?        bi             ndra  
          ?V?      X?     U    ?       ndra            na 
        X?      U       -    ?        na              th 
                                              
195
But, in some cases, the number of 
transliteration units retrieved from the Bengali 
and English words may differ. The [ [??L?]?c?X 
(brijmohan) ? brijmohan ] name pair yields  5 
TUs  in Bengali side and  4 TUs in English side   
[ [?? | L | ?]? | c? | X ?  bri | jmo | ha | n]. In such 
cases, the system cannot align the TUs 
automatically and linguistic   knowledge is used 
to resolve the confusion. A knowledge base that 
contains a list of Bengali conjuncts and 
diphthongs and their possible English 
representations has been kept. The hypothesis 
followed in the present work is that the problem 
TU in the English side has always the maximum 
length.  If more than one English TU has the 
same length, then system starts its analysis from 
the first one.  In the above example, the TUs bri 
and jmo have the same length. The system 
interacts with the knowledge base and ascertains 
that bri is valid and jmo cannot be a valid TU in 
English since there is no corresponding conjunct 
representation in Bengali. So jmo is split up into 
2 TUs j and mo, and the system aligns the 5 TUs 
as [[?? | L | ?]? | c? | X ?  bri | j | mo | ha | n]. 
Similarly, [?_?E?X?U (loknath) ? loknath] is 
initially split as [ ?_? | E? | X? | U ]   ?   lo | kna | 
th], and then as [ lo | k | na | th ] since kna has the 
maximum length and it does not have any valid 
conjunct representation in Bengali. 
In some cases, the knowledge of Bengali 
diphthong resolves the problem. In the following           
example, [ ?[?? | + | ]? (raima) ? rai | ma], the 
number of TUs on both sides do not                  
match. The English TU rai is chosen for analysis 
as its length is greater than the other TU ma. The 
vowel sequence ai corresponds to a diphthong in 
Bengali that has two valid representations < %?+, 
B >. The first representation signifies that a 
matra is associated to the previous character 
followed by the character +. This matches the 
present Bengali input. Thus, the English vowel 
sequence ai is separated from the TU rai (rai ? r 
| ai) and the intermediate form of the name pair 
appears to be [?[?? | + | ]? (raima) ? r | ai | ma].  
Here, a matra is associated with the Bengali TU 
that corresponds to English TU r and so there 
must be a vowel attached with the TU r. TU ai is 
further splitted as a and i (ai ? a | i) and the first 
one (i.e. a) is assimilated with the previous TU 
(i.e. r) and finally the name pair appears as: [ ?[?? | 
+ | ]? (raima) ? ra | i | ma]. 
In the following two examples, the number of 
TUs on both sides does not match. 
[ ?V | [? | ?[?? | L (devraj)    ?   de | vra | j ]   
[ ?a? | ] | X? | U (somnath) ? so | mna | th] 
 
It is observed that both vr and mn represent 
valid conjuncts in Bengali but these examples 
contain the constituent Bengali consonants in 
order and not the conjunct representation. During 
the training phase, if, for some conjuncts, 
examples with conjunct representation are 
outnumbered by examples with constituent 
consonants representation, the conjunct is 
removed from the linguistic knowledge base and 
training examples with such conjunct 
representation are moved to a Direct example 
base which contains the English words and their 
Bengali transliteration. The above two name 
pairs can then be realigned as  
[ ?V | [? | ?[?? | L (devraj)    ?   de | v | ra | j ]   
[ ?a? | ] | X? | U (somnath) ? so | m | na | th] 
 
Otherwise, if such conjuncts are included in 
the linguistic knowledge base, training examples 
with constituent consonants representation are to 
be moved to the Direct example base. 
The Bengali names and their English 
transliterations are split into TUs in such a way 
that, it   results in a one-to-one correspondence 
after using the linguistic information. But in 
some       cases there exits zero-to-one or many-
to-one relationship. An example of Zero-to-One 
relationship [? ? h] is the name-pair [%? | {? 
(alla) ?  a | lla | h] while the name-pair [%? | + | 
?\? (aivy)   ? i | vy] is an example of Many-to-
One relationship [%?, + ? i]. These bilingual 
examples should also be included in the Direct 
example base. 
In some cases, the linguistic knowledge 
apparently solves the mapping problem, but not        
always. From the name-pair [[??[?F? (barkha) ? 
barkha], the system initially generates the       
mapping [[? | ?[? | F? ? ba | rkha] which is not 
one-to-one. Then it consults the linguistic          
knowledge base and breaks up the transliteration 
unit as (rkha ? rk | ha ) and generates the final 
196
aligned transliteration pair [[? | ?[? | F? ? ba | rk | 
ha ] (since it finds out that rk has a valid conjunct 
representation in Bengali but not rkh), which is 
an incorrect transliteration pair to train   the 
system. It should have been [[? | ?[? | F? ?  ba | r | 
kha]. Such type of errors can be detected by 
following the alignment process from the target 
side during the training phase. Such training 
examples may be either manually aligned or 
maintained in the Direct Example base. 
5 Results of the Proposed Models 
Approximately 6000 Indian person names have 
been collected and their English transliterations 
have been stored manually. This set acts as the 
training corpus on which the system is trained to 
generate the collocational statistics. These 
statistics serve as the decision list classifier to 
identify the target language TU given the source 
language TU and its context. The system also 
includes the linguistic knowledge in the form of 
valid conjuncts and diphthongs in Bengali and 
their English representation.  
All the models have been tested with an open 
test corpus of about 1200 Bengali names that 
contains their English transliterations. The total 
number of transliteration units (TU) in these 
1200 (Sample Size, i.e., S) Bengali names is 
4755 (this is the value of L), i.e., on an average a 
Bengali name contains 4 TUs. The test set was 
collected from users and it was checked that it 
does not contain names that are present in the 
training set. The total number of transliteration 
unit errors (Err) in the system-generated 
transliterations and the total number of words 
erroneously generated (Err/) by the system have 
been shown in Table 1 for each individual model. 
The models are evaluated on the basis of the two 
evaluation metrics, Word Agreement Ratio 
(WAR) and Transliteration Unit Agreement 
Ratio (TUAR). The results of the tests in terms 
of the evaluation metrics are shown in Table 2. 
The modified joint source-channel model (Model 
F) that incorporates linguistic knowledge 
performs best among all the models with a Word 
Agreement Ratio (WAR) of 69.3% and a 
Transliteration Unit Agreement Ratio (TUAR) of 
89.8%. The joint source-channel model with 
linguistic knowledge (Model D) has not 
performed well in the Bengali-English machine 
transliteration whereas the trigram model (Model 
E) needs further attention as its result are 
comparable to the modified joint source-channel 
model (Model F). All the models were also tested 
for back-transliteration, i.e., English to Bengali 
transliteration, with an open test corpus of 1000 
English names that contain their Bengali 
transliterations. The results of these tests in terms 
of the evaluation metrics WAR and TUAR are 
shown in Table 3. It is observed that the 
modified joint source-channel model performs 
best in back-transliteration with a WAR of 
67.9% and a TUAR of 89%.  
 
Model Error in TUs 
(Err) 
Error words 
(Err/) 
A 990 615 
B 795 512 
C 880 532 
D 814 471 
E 604 413 
F 486 369 
 
Table 1: Value of Err and Err/ for each model 
(B2E  transliteration) 
 
Model WAR 
(in %) 
TUAR 
(in %) 
A 48.8 79.2 
B 57.4 83.3 
C 55.7 81.5 
D 60.8 82.9 
E 65.6 87.3 
F 69.3 89.8 
 
Table 2: Results with Evaluation Metrics 
(B2E  transliteration) 
 
Model WAR 
(in %) 
TUAR 
(in %) 
A 49.6 79.8 
B 56.2 83.8 
C 53.9 82.2 
D 58.2 83.2 
E 64.7 87.5 
F 67.9 89.0 
 
Table 3: Results with Evaluation Metrics 
(E2B transliteration) 
6.    Conclusion 
It has been observed that the modified joint 
source-channel model with linguistic knowledge 
performs best in terms of Word Agreement Ratio 
(WAR) and Transliteration Unit Agreement 
Ratio (TUAR). Detailed examination of the 
197
evaluation results reveals that Bengali has 
separate short and long vowels and the 
corresponding matra representation while these 
may be represented in English by the same 
vowel. It has been observed that most of the 
errors are at the matra level i.e., a short matra 
might have been replaced by a long matra or vice 
versa. More linguistic knowledge is necessary to 
disambiguate the short and the long vowels and 
the matra representation in Bengali. The system 
includes conjuncts and diphthongs as part of the 
linguistic knowledge base. Triphthongs or 
tetraphthongs usually do not appear in Indian 
names. But, inclusion of them will enable the 
system to transliterate those few names that may 
include them. The models are to be trained 
further on sets of additional person names from 
other geographic areas. Besides person names, 
location and organization names are also to be 
used for training the proposed models. 
Acknowledgement 
Our thanks go to Council of Scientific and 
Industrial Research, Human Resource 
Development Group, New Delhi, India for 
supporting Sudip Kumar Naskar under Senior 
Research Fellowship Award (9/96(402) 2003-
EMR-I). 
References 
Abdul Jaleel Nasreen and Leah S. Larkey. 2003. 
Statistical Transliteration for English-Arabic Cross 
Language Information Retrieval. Proceedings of 
the Twelfth International Conference on 
Information and Knowledge Management (CIKM 
2003), New Orleans, USA, 139-146. 
Al-Onaizan Y. and Knight K. 2002a. Named Entity 
Translation: Extended Abstract. Proceedings of the 
Human Language Technology Conference (HLT 
2002), 122-124. 
Al-Onaizan Y. and Knight K.2002b. Translating 
Named Entities Using Monolingual and Bilingual 
Resources.  Proceedings of the 40th Annual 
Meeting of the ACL (ACL 2002), 400-408. 
Al-Onaizan Y. and Knight K. 2002c. Machine 
Transliteration of Names in Arabic Text. 
Proceedings of the ACL Workshop on 
Computational Approaches to Semitic Languages. 
Arbabi Mansur, Scott M. Fischthal, Vincent C. 
Cheng, and Elizabeth Bar. 1994. Algorithms for 
Arabic name transliteration. IBM Journal of 
Research and Development, 38(2): 183-193. 
Crego J.M., Marino J.B. and A. de Gispert. 2005. 
Reordered Search and Tuple Unfolding for Ngram-
based SMT. Proceedings of the MT-Summit X, 
Phuket, Thailand, 283-289. 
Marino J. B., Banchs R., Crego J. M., A. de Gispert, 
P.  Lambert, J. A. Fonollosa and M. Ruiz, Bilingual 
N-gram Statistical Machine Translation.  
Proceedings of the MT-Summit X, Phuket, 
Thailand, 275-282. 
Goto I., N. Kato, N. Uratani, and T. Ehara. 2003. 
Transliteration considering Context Information 
based on the Maximum Entropy Method. 
Proceeding of the MT-Summit IX, New Orleans, 
USA, 125?132. 
Haizhou Li, Zhang Min, Su Jian. 2004. A Joint 
Source-Channel Model for Machine 
Transliteration. Proceedings of the 42nd Annual 
Meeting of the ACL (ACL 2004), Barcelona, 
Spain, 159-166. 
Jung Sung Young, Sung Lim Hong, and Eunok Paek. 
2000. An English to Korean Transliteration Model 
of Extended Markov Window. Proceedings of 
COLING 2000, 1, 383-389. 
Knight K. and J. Graehl. 1998. Machine 
Transliteration, Computational Linguistics, 24(4): 
599-612. 
Meng Helen M., Wai-Kit Lo, Berlin Chen and Karen 
Tang. 2001. Generating Phonetic Cognates to 
handle Name Entities in English-Chinese Cross-
language Spoken Document Retrieval. Proceedings 
of the Automatic Speech Recognition and 
Understanding (ASRU) Workshop, Trento, Italy. 
Stalls, Bonnie Glover and Knight K. 1998. 
Translating names and technical terms in Arabic 
text. Proceedings of the COLING/ACL Workshop 
on Computational Approaches to Semitic 
Languages, Montral, Canada, 34-41. 
Virga Paola and Sanjeev Khudanpur. 2003. 
Transliteration of Proper Names in Crosslingual 
Information Retrieval. Proceedings of the ACL 
2003 Workshop on Multilingual and Mixed-
language Named Entity Recognition, Sapporo, 
Japan, 57-60.  
 
198
Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 89?94,
Trento, Italy, April 2006. c?2006 Association for Computational Linguistics
Handling of Prepositions in English to Bengali  
Machine Translation 
 
Sudip Kumar Naskar 
Dept. of Comp. Sc. & Engg., 
Jadavpur University, 
Kolkata, India 
sudip_naskar@hotmail.com 
Sivaji Bandyopadhyay 
Dept. of Comp. Sc. & Engg.,  
Jadavpur University, 
Kolkata, India 
sivaji_cse_ju@yahoo.com 
 
  
 
Abstract 
The present study focuses on the lexical 
meanings of prepositions rather than on 
the thematic meanings because it is in-
tended for use in an English-Bengali ma-
chine translation (MT) system, where the 
meaning of a lexical unit must be pre-
served in the target language, even 
though it may take a different syntactic 
form in the source and target languages. 
Bengali is the fifth language in the world 
in terms of the number of native speakers 
and is an important language in India. 
There is no concept of preposition in 
Bengali. English prepositions are trans-
lated to Bengali by attaching appropriate 
inflections to the head noun of the prepo-
sitional phrase (PP), i.e., the object of the 
preposition. The choice of the inflection 
depends on the spelling pattern of the 
translated Bengali head noun. Further 
postpositional words may also appear in 
the Bengali translation for some preposi-
tions. The choice of the appropriate post-
positional word depends on the WordNet 
synset information of the head noun. 
Idiomatic or metaphoric PPs are trans-
lated into Bengali by looking into a bi-
lingual example base. The analysis pre-
sented here is general and applicable for 
translation from English to many other 
Indo-Aryan languages that handle prepo-
sitions using inflections and postposi-
tions. 
1 Introduction 
Prepositions have been studied from a variety of 
perspectives. Both linguistic and computational 
(monolingual and cross-lingual) aspects of 
prepositions have been contemplated by several 
researchers. Jackendoff (1977), Emonds (1985), 
Rauh (1993) and Pullum and Huddleston (2002) 
have investigated the syntactic characteristics of 
preposition. Cognitive theorists have examined 
the polysemous nature of prepositions and ex-
plored the conceptual relationships of the 
polysemy, proposing the graphical mental im-
ages (Lakoff and Johnson, 1980; Brugman, 1981, 
1988; Herskovits, 1986; Langacker, 1987; Tyler 
and Evans, 2003). Fauconnier (1994) and Visetti 
and Cadiot (2002) have canvassed the pragmatic 
aspects of prepositions. A practical study of the 
usage of prepositions was carried out for the pur-
pose of teaching English as a second language 
(Wahlen, 1995; Lindstromberg, 1997; Yates, 
1999). The deictic properties of spatial preposi-
tions have been studied by Hill (1982), while the 
geographical information provided by them was 
an interest of computational research (Xu and 
Badler, 2000; Tezuka et al, 2001). 
In the fields of natural language processing, 
the problem of PP attachment has been a topic 
for research for quite a long time, and in recent 
years, the problem was explored with a neural 
network-based approach (Sopena et al, 1998) 
and with a syntax-based trainable approach (Yeh 
and Vilain, 1998). Although past research has 
revealed various aspects of prepositions, there is 
not much semantic research of prepositions 
available for computational use, which requires a 
vigorous formalization of representing the se-
mantics. A recent semantic study of prepositions 
for computational use is found in (Voss, 2002), 
with a focus on spatial prepositions. Spatial 
prepositions are divided into three categories ac-
cording to which one of the two thematic mean-
ings between place and path they acquire when 
they are in argument, adjunct and non-
subcategorized positions of particular types of 
89
verbs. The semantics of spatial prepositions dealt 
with in (Voss, 2002) is not lexical but thematic. 
There are some prepositions (e.g., over, with), 
which have many senses as preposition. By mak-
ing use of the semantic features of the Comple-
ments (reference object) and Heads (verb, verb 
phrase, noun or noun phrase governing a preposi-
tion or a PP), the meaning of the polysemous 
prepositions can be computationally disambigu-
ated. The different meanings of over call for dif-
ferent semantic features in its heads and com-
plements [Alam, 04]. 
Prepositional systems across languages vary to 
a considerably degree, and this cross-linguistic 
diversity increases as we move from core, physi-
cal senses of prepositions into the metaphoric 
extensions of prepositional meaning  (metaphor 
or rather, idiomaticity is one of the main realms 
of usage with prepositions) (Brala, 2000). 
The present study focuses on the lexical mean-
ings of prepositions rather than on the thematic 
meanings because it is intended for use in an 
English-Bengali machine translation (MT) sys-
tem, where the meaning of a sentence, a phrase 
or a lexical entry of the source language must be 
preserved in the target language, even though it 
may take a different syntactic form in the source 
and target languages. Bengali is the fifth lan-
guage in the world in terms of the number of na-
tive speakers and is an important language in 
India. It is the official language of neighboring 
Bangladesh. There is no concept of preposition 
in Bengali. English prepositions are translated to 
Bengali by attaching appropriate inflections to 
the head noun of the PP, i.e., the object of the 
preposition. The choice of the inflection depends 
on the spelling pattern of the translated Bengali 
head noun. Further postpositional words may 
also appear in the Bengali translation for some 
prepositions. The choice of the appropriate post-
positional word depends on the WordNet (Fell-
baum, 1998) synset information of the head 
noun. Idiomatic or metaphoric PPs are translated 
into Bengali by looking into a bilingual example 
base. 
A brief overview of the English-Bengali MT 
System is presented in Section 2. Different types 
of English prepositions and their identification in 
the MT system are described in Section 3. Inflec-
tions and postpositions in Bengali are outlined in 
Section 4. Translation of English prepositions to 
inflections and postpositions in Bengali are de-
tailed in Section 5. The conclusion is drawn in 
Section 6.  
2 A Brief Overview of the English-
Bengali MT System 
The handling of English prepositions during 
translation to Bengali has been studied with re-
spect to an English-Bengali MT system (Naskar 
and Bandyopadhyay, 2005) being developed. In 
order to translate from English to Bengali, the 
first thing we do is lexical analysis of the English 
sentence using the WordNet, to gather the lexical 
features of the morphemes. During morphologi-
cal analysis, the root words / terms (including 
idioms and named entities), along with associ-
ated grammatical information and semantic cate-
gories are extracted. A shallow parser identifies 
the constituent phrases of the source language 
sentence and tags them to encode all relevant 
information that might be needed to translate 
these phrases and perhaps resolve ambiguities in 
other phrases. Then these phrases are translated 
individually to the target language (Bengali) us-
ing Bengali synthesis rules. The noun phrases 
and PPs are translated using Example bases of 
syntactic transfer rules. Verb phrase translation 
scheme is rule based and uses Morphologi-
cal Paradigm Suffix Tables. Finally, those target 
language phrases are arranged using some 
heuristics, based on the word ordering rules of 
Bengali, to form the target language representa-
tion of the source language sentence.  
3 Prepositions in English  
A preposition is a word placed before a ?noun? 
to show in what relation the noun stands with 
regard to the other noun and verb words in the 
same sentence. The noun that follows a preposi-
tion, i.e., the reference object is in the accusative 
case and is governed by the preposition. Preposi-
tions can also be defined as words that begin 
prepositional phrases (PP). A PP is a group of 
words containing a preposition, an object of the 
preposition, and any modifiers of the object. 
Syntactically, prepositions can be arranged 
into three classes ? simple prepositions (e.g., at, 
by, for, from etc.), compound prepositions and 
phrase prepositions. A compound preposition is 
made up of a set of words which starts with and 
acts like a preposition (e.g., in spite of, in favor 
of, on behalf of etc.). A phrase preposition is a 
simple preposition preceded by a word from an-
other category, such as an adverb, adjective, or 
conjunction (e.g., instead of, prior to, because of, 
according to etc.). 
Frequently prepositions follow the verbs to-
gether forming phrasal verbs and remain sepa-
90
rate. A word that looks like a preposition but is 
actually part of a phrasal verb is often called a 
particle. E.g. ?Four men held up the bank.? Here 
held up is a verb [?to rob?]. Therefore, up is not 
a preposition, and bank is not the object of a 
preposition. Instead, bank is a direct object of the 
verb held up. A particle may not always appear 
immediately after the verb with which it makes 
up a phrasal verb (e.g., Four men held the bank 
up.). 
An idiomatic (metaphoric) PP starts with a 
preposition, but its meaning cannot be ascer-
tained from the meaning of its components. Ex-
amples of idiomatic PPs are: at times, by hook or 
crook etc. 
All these syntactical characteristics are used to 
identify prepositions in the English-Bengali MT 
system. Moreover, the inventory of prepositions 
in English is a close set. So, identification of 
prepositions is not much of a problem in English. 
A simple list serves the purpose. The preposi-
tions, compound prepositions, phrase preposi-
tions and idiomatic PPs are identified during 
morphological analysis. Some of the phrasal 
verbs (when the phrasal verb appears as a whole) 
are identified during the morphological analysis 
phase and some during parsing (when the parti-
cle does not accompany the verb). 
However, there are some words that act as 
prepositions and fall into other POS categories as 
well. For example, the word before can be used 
as an adverb (e.g., I could not come before), 
preposition (e.g., He came before me) or a con-
junction (e.g., He came before I came). Simi-
larly, the word round can be used as an adjective 
(e.g., Rugby is not played with a round ball), 
noun (e.g., Rafter was knocked out of the tour-
nament in the third round), adverb (e.g., They 
have moved all the furniture round), preposition 
(e.g., The earth revolves round the sun) and verb 
(e.g., His eyes rounded with anger). But depend-
ing on the POS of the neighboring words/terms, 
the parser easily identifies the correct POS of the 
word in the particular context. 
A preposition is usually placed in front of (is 
?pre-positioned? before) its object, but some-
times however may follow it (e.g., What are you 
looking at?). The preposition is often placed at 
the end when the reference object is an interroga-
tive pronoun (e.g., Where are you coming 
from?) or a relative pronoun (e.g., My grandfa-
ther was a collector of coins, which we used to 
fight over). In such cases, the system finds out 
that the preposition is not a particle and is not 
followed by a noun either, so it must be a 
stranded preposition. It searches the pronoun 
(relative or interrogative) that appears at its left 
and relates the stranded preposition to the pro-
noun. Thus during translation, the following 
conversion takes place.  
(1) Where are you coming 
from? ?? From where are you 
coming? 
(2) My grandfather was a 
collector of coins, which we 
used to fight over. ?? My 
grandfather was a collector 
of coins, over which we used 
to fight. 
But if the pronoun is missing, then the system 
has to find out the elliptical pronoun first.  
(3) I am grateful to the man 
I have spoken to. ? I am 
grateful to the man [whom] I 
have spoken to. ? I am 
grateful to the man to 
[whom] I have spoken. 
Prepositions represent several relations with 
the nouns governed by them. Spatial and tempo-
ral prepositions (which indicate a place or time 
relation) have received a relatively in-depth 
study for a number of languages. The semantics 
of other types of prepositions describing manner, 
instrument, amount or accompaniment largely 
remain unexplored. In case of an MT system, 
when a preposition has different representations 
in the target language for different relations indi-
cated by it, identification of the relation is neces-
sary. The WordNet synset information of the 
head noun of the PP, i.e., the object of the prepo-
sition serves to identify the relation. 
4 Inflections and Postpositions in Ben-
gali 
In Bengali, there is no concept of preposition. 
English prepositions are handled in Bengali us-
ing inflections (vibhaktis) to the reference objects 
and/or post-positional words after them. Inflec-
tions get attached to the reference objects. An 
inflection has no existence of its own in the lan-
guage, and it does not have any meaning as well. 
There are only a few inflections in Bengali: ? 
(null), -?#(-e), -?^ (-y), -??^ (-ye), -?T? (-te), -
?#?T? (-ete), -?E? (-ke), -??[? (-re), -?#??[? (-ere), 
-?[? (-r) and -?#?[? (-er) (an inflection is repre-
91
sented as a word with a leading ?-? in this paper). 
The placeholder indicated by a dashed circle 
represents a consonant or a conjunct. For exam-
ple, if -?# inflection is attached to the word 
[??L??[? (bazar [market]) the inflected word is 
[??L???[? (bazar-e [market-to]). On the other hand, 
post-positional words are independent words. 
They have meanings of their own and are used 
independently like other words. A post-positional 
word is positioned after an inflected noun (the 
reference object). Some examples of the post-
positional words in (colloquial) Bengali are: ?V??^ 
(diye [by]), ?U?E? (theke [from]), LX? (jonno 
[for]), E????K? (kachhe [near]), a?]?X  (samne [in 
front of]) etc. 
5 Translating English prepositions to 
Bengali 
When an English PP is translated into Bengali, 
the following transformation takes place: (prepo-
sition) (reference object) ?? (reference object) 
[(inflection)] [(postpositional-word)]. The corre-
spondence between English prepositions and 
Bengali postpositions (inflections and post-
positional words) is not direct. As far as the se-
lection of the appropriate target language repre-
sentation of a preposition is concerned the refer-
ence object plays a major role in determining the 
correct preposition sense. Deciding whether the 
preposition is used in a spatial sense, as opposed 
to a temporal or other senses, is determined by 
the semantics of the head noun of the reference 
object. A noun phrase (NP) denoting a place 
gives rise to a spatial PP. Similarly, an object 
referring to a time entity produces a temporal 
expression. These relationships can be estab-
lished by looking at the WordNet synset informa-
tion of the head noun of the PP. 
5.1 Translating English prepositions using 
Inflections in Bengali 
The translation of the three English prepositions 
'in', 'on', and 'at' involves identifying the possible 
inflection to be attached to the head noun of the 
PP. No postpositional words are placed after the 
head noun for these prepositions. The three 
prepositions 'in', 'on', and 'at' (which are both 
spatial and temporal in nature) can be translated 
into the Bengali inflections '-?#' (-e), '-?T?? (-te), 
-?#?T? (-ete) and '?^ '  (-y). Any of these 4 Bengali 
inflections can be placed after the reference ob-
ject for any of these 3 English spatial and tempo-
ral prepositions. The choice depends on the spell-
ing of the translated reference object. The rule is: 
if the last letter of the Bengali representation of 
the reference object is a consonant, ??#? (-e) or -
?#?T? (-ete) is added to it (e.g., at/in market? 
[??L???[? [bazar-e / bazar-ete]), else if the last let-
ter of the Bengali word is a matra (vowel modi-
fier) and if the matra is ?#??  (-a), any of ??T??    
(-te), or  '?^ '  (-y) can be added to the Bengali ref-
erence word (e.g., in evening? a?????T? / a?????^ 
[sandhya-te / sandhya-y]), otherwise '?T??  (-te) is 
added to it (e.g., at home? [???Q???T? [badi-te]). 
When translating the temporal expressions, if 
?on? is followed by a day (like Sunday, Monday 
etc.) or by a date in English, null inflection is 
added. 
To translate this type of PPs, we take the help 
of an example base, which contains bilingual 
translation examples. Here are some translation 
examples from the example base (TLR ? target 
language representation of the reference object). 
(1) at / in (place) ?? 
(TLR) - ?(?# / ??^ / ?T? ) [ - ( e / 
ye / te )] 
(2) of (NP) ?? (TLR) - ?( ?[? / 
?#?[? / ??^?[? ) [ - ( r / er / yer 
)] 
5.2 Translating English prepositions using 
Inflections and Postpositions in Bengali 
Most of the English prepositions are translated to 
Bengali as inflections and postpositions to the 
noun word representing the reference object. To 
translate this type of PPs, we take the help of an 
example base, which contains bilingual transla-
tion examples. Here are some translation exam-
ples from the example base (TLR ? target lan-
guage representation of the reference object).  
(1) before (artifact) ?? 
(TLR) - ?( ?[? / ?#?[? / ??^?[? ) a?]?X [ - 
( r / er / yer ) samne ] 
(2) before (!artifact) ?? 
(TLR) - ?( ?[? / ?#?[? / ??^?[? ) %??G [ - 
( r / er / yer ) age ] 
92
(3) round (place / physical 
object) ?? (TLR) - ?( ?[? / ?#?[? / 
??^?[? ) ?J???[??V?E? [ - ( r / er / yer 
) chardike ] 
(4) after (time) ?? (TLR) - 
?( ?[? / ?#?[? / ??^?[? ) Y??[?? [ - ( r / er 
/ yer ) pare ] 
(5) since (place / physical 
object / time) ?? (TLR) ?U?E? 
[theke] 
The choice of inflection depends on the spell-
ing of the translated reference object as said be-
fore. If the translated reference object ends with 
a vowel, ??^?[? is added to it; else if ends with a 
consonant, ?#?[? (er)is added to it; otherwise (it 
ends with a matra) ?[? (r)is appended with it. The 
postpositional word is placed after the inflected 
reference object in Bengali. The choice of the 
postpositional word depends on the semantic 
information about the reference objects as col-
lected from the WordNet. In cases with one 
postpositional word, there is no need to know the 
semantic features of the reference objects. For 
example, ?since?, as a preposition, is always 
translated as ?U?E? (theke) in Bengali, irrespec-
tive of the reference object. Again in some cases, 
this semantic information about the reference 
object does not suffice to translate the preposi-
tion properly.  
Consider the following examples that include 
the preposition before in two different senses. 
(1) He stood before the 
door. ?? ?a V?[?L??[? a?]?X V??Q???_ 
(se [he] darja-r samne [the 
door before] dandalo 
[stood]) 
(2) He reached before eve-
ning.  ?? ?a a?????[? %??G 
?Yg??K??_ (se [he] sondhya-r age 
[evening before] pouchhalo 
[reached]) 
(3) He reached before John. 
?? ?a L?X?[? %??G ?Yg??K??_ (se [he] 
jan-er age [John before] 
pouchhalo [reached]) 
From the WordNet, the system acquires the 
semantic information that ?door? is a hyponym of 
?artifact?, whereas ?evening? and ?me? (which 
represents a person) are not. Thus ?with? is trans-
lated to Bengali as - ?[? a?]?X in sentence (1), and 
takes the meaning - ?( ?[? / ?#?[? / ??^?[? ) %??G in 
sentence (2) and (3). 
As there is no ambiguity in the meaning of 
compound prepositions and phrase prepositions, 
a simple listing of them (along with their Bengali 
representations) suffices to translate them. We 
have prepared a list that contains the phrase 
prepositions and compound prepositions in Eng-
lish along with their Bengali translations. 
 
English Bengali 
in spite of a????C [satteo] 
away from - ?( ?[? / ?#?[? / ??^?[? ) ?U?E? V???[? 
[ - ( r / er / yer ) theke dure ] 
owing to - ?( ?[? / ?#?[? / ??^?[? ) E???[??S 
[ - ( r / er / yer ) karane ] 
apart from K??Q???C [ chhadao ] 
Instead of - ( ?[? / ?#?[? / ??^?[? ) Y??[?[??T?? 
[ - ( r / er / yer ) paribarte ] 
along with - ( ?[? / ?#?[? / ??^?[? ) a??U 
[ - ( r / er / yer ) sathhe ] 
5.3 Translation of English Idiomatic PPs 
The meaning of an idiomatic PP cannot be de-
rived from the meanings of its components. The 
simplest way to tackle them is to maintain a list-
ing of them. A list or a direct Example Base is 
used which contains idioms, which start with 
prepositions, along with their Bengali transla-
tions.  Such an idiom is treated like any other PP 
during the word-reordering phase. Here are some 
examples of them: 
(1) at times ?? a]??^ a]??^ 
(samaye samaye) 
(2) by hook or crook ?? 
?^\???[?+ ?c??E? (jebhabei hok) 
(3) to a fault ?? ]?y??T???[?N? 
(matratirikto) 
6 Conclusion 
In the present study, the handling of English 
prepositions in Bengali has been studied with 
reference to a machine translation system from 
English to Bengali. English prepositions are han-
93
dled in Bengali using inflections and / or using 
post-positional words. In machine translation, 
sense disambiguation of preposition is necessary 
when the target language has different represen-
tations for the same preposition. In Bengali, the 
choice of the appropriate inflection depends on 
the spelling of the reference object. The choice 
of the postpositional word depends on the se-
mantic information about the reference object 
obtained from the WordNet.   
Acknowledgements  
Our thanks go to Council of Scientific and In-
dustrial Research, Human Resource Develop-
ment Group, New Delhi, India for supporting 
Sudip Kumar Naskar under Senior Research Fel-
lowship Award (9/96(402) 2003-EMR-I). 
References  
Alam, Yukiko Sasaki. 2004. Decision Trees for Sense 
Disambiguation of Prepositions: Case of Over. In 
HLT/NAACL-04. 
Brala, Marija M. 2000. Understanding and trans-
lating (spatial) prepositions: An exercise in 
cognitive semantics for lexicographic pur-
poses. 
Brugman, Claudia. 1988. The story of over: 
Polysemy, semantics and the structure of the 
lexicon. New York: Garland Press. [1981. The 
story of over. Berkely, CA: UC-Berkely MA the-
sis.] 
Emonds, Joseph. 1985. A unified theory of syntac-
tic categories. Dordrecht: Foris. 
Fellbaum, Christiane D. ed. 1998. WordNet ? An 
Electronic Lexical Database, MIT Press, Cam-
bridge, MA.  
Fauconnier, Gilles. 1994. Mental spaces. Cam-
bridge: Cambridge University Press. 
Herskovits, Annette. 1986. Language and spatial 
cognition An interdisciplinary study of the 
prepositions in English. Cambridge: Cambridge 
University Press.  
Hill, Cliffort 1982. Up/down, front/back, left/right. 
A contrastive study of Hausa and English. In 
Weissenborn and Klein, 13-42. 
Jackendoff, Ray. 1977. The architecture of the lan-
guage. Cambridge, MA: MIT Press. 
Lakoff, George and Mark Johnson. 1980. Metaphors 
we live by. Chicago: University of Chicago Press. 
Langacker, Ronald. 1987. Foundations of cognitive 
grammar, vol. 1. Stanford, CA: Stanford Univer-
sity Press. 
Lindstromberg, Seth. 1997. English prepositions 
explained. Amsterdam: John Benjamins. 
Naskar, Sudip Kr. and Bandyopadhyay. Sivaji. 2005. 
A Phrasal EBMT System for Translating English 
to Bangla. In MT Summit X. 
Pullum, Geoffrey and Rodney Huddleston. 2002. 
Prepositions and prepositional phrases. In 
Huddleston and Pullum (eds.), 597-661. 
Rauh, Gisa. 1993. On the grammar of lexical and 
nonlexical prepositions in English. In Ze-
linskiy-Wibbelt (eds.), 99-150. 
Sopena, Joseph M., Agusti LLoberas and Joan L. 
Moliner. 1998. A connectionist approach to prepo-
sitional phrase attachment for real world texts. In 
COLING-ACL ?98, 1233-1237. 
Tezuka, Taro, Ryong Lee, Yahiko Kambayashi and 
Hiroki Takakura. 2001. Web-based inference rules 
for processing conceptual geographical relation-
ships. Proceedings of Web Information Sys-
tems Engineering, 14-21. 
Tyler A. and Evans V. 2003. Reconsidering prepo-
sitional polysemy networks: the case of over*. 
In B. Nerlich, Z. Todd, V. Herman, & D. D. Clarke 
(Eds.), Polysemy: Flexible patterns of meanings in 
mind and language, pp. 95-160. Berlin: Mouton de 
Gruyter. 
Visetti, Yves-Marie and Pierre Cadiot. 2002. Insta-
bility and the theory of semantic forms Start-
ing from the case of prepositions. In Fei-
genbaum, Susanne and Dennis Kurzon (eds.), 9-39. 
Voss, Clare. 2002. Interlingua-based machine 
translation of spatial expressions. University of 
Maryland: Ph.D. Dissertation. 
Wahlen, Gloria. 1995. Prepositions illustrated. 
Michigan: The University of Michigan Press. 
Xu, Yilun Dianna and Norman Badler. 2000. Algo-
rithms for generating motion trajectories described 
by prepositions. Proceedings of Computer Ani-
mation 2000, 30-35. 
Yates, Jean. 1999. The ins and outs of prepositions 
A guidebook for ESL students. New York: Bar-
ron?s. 
Yeh, Alexander S. and Marc B. Vilain. 1998. Some 
properties of preposition and subordinate conjunc-
tion attachments. In COLING-ACL ?98, 1436-
1442. 
94
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 203?206,
Prague, June 2007. c?2007 Association for Computational Linguistics
JU-SKNSB: Extended WordNet Based WSD on the English All-Words 
Task at SemEval-1 
Sudip Kumar Naskar 
Computer Sc. & Engg. Dept., 
Jadavpur University, 
Kolkata, India 
sudip.naskar@gmail.com 
Sivaji Bandyopadhyay 
Computer Sc. & Engg. Dept., 
Jadavpur University, 
Kolkata, India 
sivaji_cse_ju@yahoo.com 
 
 
Abstract 
This paper presents an Extended WordNet 
based word sense disambiguation system 
using a major modification to the Lesk al-
gorithm. The algorithm tries to disambigu-
ate nouns, verbs and adjectives. The algo-
rithm relies on the POS-sense tagged syn-
set glosses provided by the Extended 
WordNet. The basic unit of disambiguation 
of our algorithm is the entire sentence un-
der consideration. It takes a global ap-
proach where all the words in the target 
sentence are simultaneously disambigu-
ated. The context includes previous and 
next sentence. The system assigns the de-
fault WordNet first sense to a word when 
the algorithm fails to predict the sense of 
the word. The system produces a precision 
and recall of .402 on the SemEval-2007 
English All-Words test data. 
1 Introduction 
In Senseval 1, most of the systems disambiguating 
English words, were outperformed by a Lesk vari-
ant serving as baseline(Kilgariff & Rosenzweig, 
2000). On the other hand, during Senseval 2 and 
Senseval 3, Lesk baselines were outperformed by 
most of the systems in the lexical sample track 
(Edmonds, 2002). 
In this paper, we explore variants of the Lesk al-
gorithm on the English All Words SemEval 2007 
test data (465 instances), as well as on the first 10 
Semcor 2.0 files (9642 instances). The proposed 
WSD algorithm is POS-sense-tagged gloss (from 
Extended WordNet) based and is a major modifi-
cation of the original Lesk algorithm. 
2 Extended WordNet 
The eXtended WordNet (Harabagiu et al, 1999) 
project aims to transform the WordNet glosses into 
a format that allows the derivation of additional 
semantic and logic relations. It intends to syntacti-
cally parse the glosses, transform glosses into logi-
cal forms and tag semantically the nouns, verbs, 
adjectives and adverbs of the glosses automati-
cally. The last release of the Extended WordNet is 
based on WordNet 2.0 and has three stages: POS 
tagging and parsing, logic form transformation, 
and semantic disambiguation. 
3 Related Works 
Banerjee and Pedersen (2002) reports an adapta-
tion of Lesk?s dictionary-based WSD algorithm 
which makes use of WordNet glosses and tests on 
English lexical sample from SENSEVAL-2. They de-
fine overlap as the longest sequence of one or more 
consecutive content words that occurs in both 
glosses. Each overlap contributes a score equal to 
the square of the number of words in the overlap. 
A version of Lesk algorithm in combination 
with WordNet has been reported for achieving 
good results in (Ramakrishnan et al, 2004). 
Vasilescu et al (2004) carried on a series of ex-
periments on the Lesk algorithm, adapted to 
WordNet, and on some variants. They studied the 
effect of varying the number of words in the con-
texts, centered around the target word. 
But till now no work has been reported which 
makes use of Extended WordNet for Lesk-like 
gloss-oriented approach. 
203
4 Proposed Sense Disambiguation Algo-
rithm 
The proposed sense disambiguation algorithm is a 
major modification of the Lesk algorithm (Lesk, 
1986). WordNet and Extended WordNet are the 
main resources. 
4.1 Modifications to the Lesk Algorithm 
We modify the Lesk algorithm (Lesk, 1986) in 
several ways to create our baseline algorithm. The 
Lesk algorithm relies on glosses found in tradi-
tional dictionaries which often do not have enough 
words for the algorithm to work well. We choose 
the lexical database WordNet, to take advantage of 
the highly inter?connected set of relations among 
different words that WordNet offers, and Extended 
WordNet to capitalize on its (POS and sense) 
tagged glosses. 
The Lesk algorithm takes a local approach for 
sense disambiguation. The disambiguation of the 
various words in a sentence is a series of inde-
pendent problems and has no effect on each other. 
We propose a global approach where all the words 
(we mean by word, an open-class lemma) in the 
context window are simultaneously disambiguated 
in a bid to get the best combination of senses for 
all the words in the window instead of only the 
target word. The process can be thought of as sense 
disambiguation of the whole context, instead of a 
word.  
The Lesk algorithm disambiguates words in short 
phrases. But, the basic unit of disambiguation of 
our algorithm is the entire sentence under consid-
eration. We later modify the context to include the 
previous and next sentence. 
Another major change is that the dictionary 
definition or gloss of each of its senses is com-
pared to the glosses of every other word in the con-
text by the Lesk algorithm. But in the present 
work, the words themselves are compared with the 
glosses of every other word in the context. 
4.2 Choice of Which Glosses to Use 
While Lesk?s algorithm restricts its comparisons to 
the dictionary meanings of the words being disam-
biguated, our choice of dictionary allows us to also 
compare the meanings (i.e., glosses) of the words, 
as well as the words that are related to them 
through various relationships defined in WordNet. 
For each POS we choose a relation if links of its 
kind form at least 5% of the total number of links 
for that part of speech, with two exceptions. We 
use the attribute relation although there are not 
many links of its kind. But this relation links adjec-
tives, which are not well developed in WordNet, to 
nouns which have a lot of data about them. This 
potential to tap into the rich noun data prompted us 
to use this relation. Another exception is the an-
tonymy relationship. Although there are sufficient 
antonymy links for adjectives and adverbs, we 
have not utilized these relations. 
 
Noun Verb Adjective 
Hypernym 
Hyponym 
Holonym 
Meronym 
Attribute 
Hyponym 
Troponym 
Also see 
Attribute 
Also see 
Similar to 
Pertainym of 
Table 1. WordNet relations chosen for the disam-
biguation algorithm 
4.3 The Algorithm 
The gloss bag is constructed for every sense of 
every word in the sentence. The gloss-bag is con-
structed from the POS and sense tagged glosses of 
synsets, obtained from the Extended WordNet. For 
any synset, the words forming the synset and the 
gloss definition contribute to the gloss-bag. The 
non-content words are left out. Example sentences 
do not contribute to the gloss bag since they are not 
(POS and sense) tagged.  Each word along with its 
POS and sense-tag are stored in the gloss bag. For 
words with different POS, different relations are 
taken into account (according to Table 1) for build-
ing the corresponding gloss-bag. 
This gloss-bag creation process can be per-
formed offline or online. It can be performed dy-
namically on a as-when-needed basis. Or, gloss-
bags can be created for all WordNet entries only 
once and stored in a data file in prior. The issue is 
time versus space. 
Once, this gloss-bag creation process is over, the 
comparison process starts. Each word (say Wi) in 
the context is compared with each word in the 
gloss-bag for every sense (say Sk) of every other 
word (say Wj) in the context. If a match is found, 
they are checked further for part-of-speech match. 
If the words match in part-of-speech as well, a 
score is assigned to both the words: the word being 
matched (Wi) and the word whose gloss-bag con-
tains the match (Wj). This matching event indicates 
204
mutual confidence towards each other, so both 
words are rewarded for this event. Two two-
dimensional (one for word index and the other for 
sense index) vectors are maintained: sense_vote for 
the word in context, and sense_score for the word 
in gloss-bag. Say, for example, the context word 
(Wi # noun) matches with gloss word (Wn # noun # 
m) (i.e., Wi = Wn) in the gloss bag for kth sense of 
Wj. Then, a score of 1/(gloss bag size of (Wjk)) is 
assigned to both sense_vote[i][m] and 
sense_score[j][k]. Scores are normalized before 
assigning because of huge discrepancy in gloss-bag 
sizes. This process continues until each context 
word is matched against all gloss-bag words for 
each sense of every other context words. 
Once all the comparisons have been made, we add 
sense_vote value with the sense_score linearly 
value for each sense of every word to arrive at the 
combination score for this word-sense pair.  
The algorithm assigns a word the nth sense for 
which the corresponding sense_vote and 
sense_score produces the maximum sum, and it 
does not assign a word any sense when the corre-
sponding sense_vote and sense_score values are 0, 
even if the word has only one sense. In the event of 
a tie, we choose the one that is more frequent, as 
specified by WordNet.  
Assuming that there are N words in the window 
of context (i.e. the sentence), and that, on an aver-
age there are S senses per word, and G number of 
gloss words in each gloss bag per sense, N * S 
gloss bags need to be constructed, giving rise to a 
total of N * S * G gloss words. Now these many 
gloss words are compared against each of the N 
context words. Thus, N2 * S * G pairs of word 
comparisons need to be performed. Both, S and G 
vary heavily. 
5 Variants of the Algorithm 
The algorithm discussed thus far is our baseline 
algorithm. We made some changes, as described in 
the following two subsections, to investigate 
whether the performance of the algorithm can be 
improved. 
5.1 Increasing the Context Size 
The poor performance of the algorithm perhaps 
suggests that sentential context is not enough for 
this algorithm to work. So we went for a larger 
context: a context window containing the current 
sentence under consideration (target sentence), its 
preceding sentence and the succeeding sentence. 
This increment in context size indeed performed 
better than the baseline algorithm. 
5.2 Assigning Different Scores 
When constructing the gloss-bags for a word-sense 
pair, some words may appear in more than one 
gloss (by gloss we mean to say synonyms as well 
as gloss). So, we added another parameter with 
every (word#pos#sense) in a gloss bag: noc - the 
number of occurrence of this (word#pos#sense) 
combination in this gloss-bag. 
And, in case of a match of context word (say Wi) 
with a gloss-bag word (of say kth sense of word 
Wj), we scored the words in four ways to see if this 
phenomenon has any effect on the sense disam-
biguation process. Say, for example, the context 
word (Wi # noun) matches with gloss word (Wn # 
noun # m # noc) in the gloss bag for kth sense of Wj 
(i.e., the particular word appears noc times in the 
said gloss-bag) and the gloss bag size is gbs. Then, 
we reward Wi and Wj for this event in four ways 
given below. 
 
1. Assign 1/gbs to 
sense_vote[i][m] and 1/gbs 
to sense_score[j][k]. 
2. Assign 1/gbs to 
sense_vote[i][m] and noc/gbs 
to sense_score[j][k]. 
3. Assign noc/gbs  to 
sense_vote[i][m] and 1/gbs 
to sense_score[j][k]. 
4. Assign noc/gbs to 
sense_vote[i][m] and noc/gbs 
to sense_score[j][k]. 
 
The results of this four-way scoring proved that 
this indeed has influence on the disambiguation 
process. 
The WSD system is based on Extended Word-
Net version 2.0-1.1 (the latest release), which is in 
turn based on WordNet version 2.0. So, the system 
returns WordNet 2.0 sense indexes. These Word-
Net sense indexes are then mapped to WordNet 2.1 
sense indexes using sensemap 2.0 to 2.1. 
6 Evaluations 
The system has been evaluated on the SemEval-
2007 English All-Words Tasks (465 test in-
205
stances), as well as on the first 10 Semcor 2.0 
files, which are manually disambiguated text 
corpora using WordNet senses. 
We compute F-Score as 2*P*R / (P+R). Ta-
ble 2 shows the performance of the four variants of 
the system (with a context size of 3 sentences) 
on the first 10 Semcor 2.0 files. From table 2, it 
is clearly evident that model C produces the best 
result (precision - .621, recall - .533) among the 4 
scoring schemes. POS-wise evaluation results for 
model C on Semcor 2.0 data is given in table 3. 
 
Model  
A B C D 
Precision .618 .602 .621 .604 
Recall .531 .517 .533 .519 
F-Score .571 .556 .574 .558 
Table 2. Evaluation of the four models on Sem-
cor Data 
 
 Noun Verb Adj Overall
Precision .6977 .4272 .6694 .6211 
Recall .6179 .3947 .4602 .5335 
F-Score .6554 .4103 .5454 .574 
Table 3. POS-wise Evaluation for model C on 
Semcor Data 
 
Model C produced a precision of .393 and a re-
call of .359 on the SemEval-2007 English All-
Words test data (465 test instances). Table 4 
shows POS-wise evaluation results for this test 
data. 
 
 Noun Verb Overall 
Precision .507 .331 .393 
Recall .472 .299 .359 
F-Score .489 .314 .375 
Table 3. POS-wise Evaluation on SemEval-2007 
English All-Words test data 
 
When default WordNet first senses were as-
signed to the (40) words for which the algorithm 
failed to predict senses, both the precision and re-
call values went up to .402 (this result has been 
submitted in SemEval-2007). The WSD system 
stood 10th in the SemEval-2007 English All-
Words task. 
7 Discussions 
We believe that this somewhat poor showing can 
be partially attributed to the brevity of definitions 
in WordNet in particular and dictionaries in gen-
eral. The Lesk algorithm is crucially dependent on 
the lengths of glosses. However lexicographers 
aim to create short and precise definitions which, 
though a desirable quality in dictionaries, is disad-
vantageous to this algorithm. Nouns have the long-
est average glosses in WordNet, and indeed the 
highest recall obtained is on nouns. The character-
istics of the gloss bags need to be further investi-
gated. Again many of the sense tagged gloss words 
in Extended WordNet, which are determinant fac-
tors in this algorithm, are of  ?silver? or ?normal? 
quality. And finally, since the system returns 
WordNet 2.0 sense indexes which are mapped to 
WordNet 2.1 indexes with certain amount of con-
fidence using sensemap 2.0 to 2.1, there may be 
some loss of information during this mapping 
process. 
References 
A. Kilgarriff, and J. Rosenzweig. 2000. Framework and 
Results for English SENSEVAL. Computers and the 
Humanities, 34, 15-48. 
Florentina Vasilescu, Philippe Langlais, and Guy La-
palme. 2004. Evaluating Variants of the Lesk Ap-
proach for Disambiguating Words. LREC, Portugal. 
G. Ramakrishnan, B. Prithviraj, and P. Bhattacharyya. 
2004. A Gloss Centered Algorithm for Word Sense 
Disambiguation. Proceedings of the ACL SEN-
SEVAL 2004, Barcelona, Spain, 217-221. 
M. Lesk. 1986. Automatic sense disambiguation using 
machine readable dictionaries: How to tell a pine 
cone from a ice cream cone. Proceedings of SIGDOC 
?86. 
P. Edmonds. 2002. SENSEVAL : The Evaluation of 
Word Sense Disambiguation Systems, ELRA News-
letter, Vol. 7, No. 3. 
S. Banerjee. 2002. Adapting the Lesk Algorithm for 
Word Sense Disambiguation to WordNet. MS Thesis, 
University of Minnesota. 
S. Banerjee, and T. Pedersen. 2002. An Adapted Lesk 
Algorithm for Word Sense Disambiguation Using 
WordNet. CICLing, Mexico. 
S. Harabagiu, G. Miller, and D. Moldovan. 1999. 
WordNet2 - a morphologically and semantically en-
hanced resource. Proceedings of SIGLEX-99, Univ of 
Mariland. 1-8. 
206
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 20?23,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
A Web Application for the Diagnostic Evaluation of Machine Translation
over Specific Linguistic Phenomena
Antonio Toral Sudip Kumar Naskar Joris Vreeke Federico Gaspari Declan Groves
School of Computing
Dublin City University
Ireland
{atoral, snaskar, fgaspari, dgroves}@computing.dcu.ie joris.vreeke@dcu.ie
Abstract
This paper presents a web application and a
web service for the diagnostic evaluation of
Machine Translation (MT). These web-based
tools are built on top of DELiC4MT, an open-
source software package that assesses the per-
formance of MT systems over user-defined
linguistic phenomena (lexical, morphological,
syntactic and semantic). The advantage of the
web-based scenario is clear; compared to the
standalone tool, the user does not need to carry
out any installation, configuration or mainte-
nance of the tool.
1 Automatic Evaluation of Machine
Translation beyond Overall Scores
Machine translation (MT) output can be evaluated
using different approaches, which can essentially be
divided into human and automatic, both of which,
however, present a number of shortcomings. Hu-
man evaluation tends to be more reliable in a num-
ber of ways and can be tailored to a variety of situ-
ations, but is rather expensive (both in terms of re-
sources and time) and is difficult to replicate. On
the other hand, standard automatic MT evaluation
metrics such as BLEU (Papineni et al, 2002) and
METEOR (Banerjee and Lavie, 2005) are consid-
erably cheaper and provide faster results, but return
rather crude scores that are difficult to interpret for
MT users and developers alike. Crucially, current
standard automatic MT evaluation metrics also lack
any diagnostic value, i.e. they cannot identify spe-
cific weaknesses in the MT output. Diagnostic in-
formation can be extremely valuable for MT devel-
opers and users, e.g. to improve the performance of
the system or to decide which output is more suited
for particular scenarios.
An interesting alternative to the traditional MT
evaluation metrics is to evaluate the performance
of MT systems over specific linguistic phenomena.
While retaining the main advantage of automatic
metrics (low cost), this approach provides more fine-
grained linguistically-motivated evaluation. The lin-
guistic phenomena, also referred to as linguistic
checkpoints, can be defined in terms of linguistic in-
formation at different levels (lexical, morphological,
syntactic, semantic, etc.) that appear in the source
language. Examples of such linguistic checkpoints,
what translation information they can represent, and
their relevance for MT are provided in Table 1.
Checkpoint Relevance for MT
Lexical Words that can have multiple translations in
the target. For example, the preposition ?de?
in Spanish can be translated into English as
?of? or ?from? depending on the context.
Syntactic Syntactic constructs that are difficult to trans-
late. E.g., a checkpoint containing the se-
quence a noun (noun1) followed by the
preposition ?de?, followed by another noun
(noun2) when translating from Spanish to
English. The equivalent English construct
would be noun2?s noun1, the translation thus
involving some reordering.
Semantic Words with multiple meanings, which possi-
bly correspond to different translations in the
target language. Polysemous words can be
collected from electronic dictionaries such as
WordNet (Miller, 1995).
Table 1: Linguistic Checkpoints
Checkpoints can also be built by combining el-
20
ements from different categories. For example, by
combining lexical and syntantic elements, we could
define a checkpoint for prepositional phrases (syn-
tactic element) which start with the preposition ?de?
(lexical element).
Woodpecker (Zhou et al, 2008) is a tool that per-
forms diagnostic evaluation of MT systems over lin-
guistic checkpoints for English?Chinese. Probably
due to its limitation to one language pair, its pro-
prietary nature as well as rather restrictive licensing
conditions, Woodpecker does not seem to have been
widely used in the community, in spite of its ability
to support diagnostic evaluation.
DELiC4MT1 is an open-source software that fol-
lows the same approach as Woodpecker. However,
DELiC4MT is easily portable to any language pair2
and provides additional functionality such as filter-
ing of noisy checkpoint instances and support for
statistical significance tests. This paper focuses on
the usage of this tool through a web application and
a web service from the user?s perspective. Details
regarding its implementation, evaluation, etc. can
be found in (Toral et al, 2012; Naskar et al, 2011).
2 Web Services for Language Technology
Tools
There exist many freely available language pro-
cessing tools, some of which are distributed under
open-source licenses. In order to use these tools,
they need to be downloaded, installed, configured
and maintained, which results in high cost both in
terms of manual effort and computing resources.
The requirement for in-depth technical knowledge
severely limits the usability of these tools amongst
non-technical users, particularly in our case amongst
translators and post-editors.
Web services introduce a new paradigm in the
way we use software tools where only providers
of the tools are required to have knowledge re-
garding their installation, configuration and mainte-
nance. This enables wider adoption of the tools and
reduces the learning curve for users as the only infor-
mation needed is basic knowledge of the functional-
1http://www.computing.dcu.ie/?atoral/
delic4mt/
2It has already been tested on language pairs involving
the following languages: Arabic, Bulgarian, Dutch, English,
French, German, Hindi, Italian, Turkish and Welsh.
ity and input/output parameters (which can be easily
included, e.g. as part of an online tutorial). While
this paradigm is rather new in the field of compu-
tational linguistics, it is quite mature and successful
in other fields such as bioinformatics (Oinn et al,
2004; Labarga et al, 2007).
Related work includes two web applications in the
area of MT evaluation. iBLEU (Madnani, 2011) or-
ganises BLEU scoring information in a visual man-
ner. Berka et al (2012) perform automatic error de-
tection and classification of MT output.
Figure 1: Web interface for the web service.
3 Demo
The demo presented in this paper consists of a
web service and a web application built on top of
DELiC4MT that allow to assess the performance of
MT systems on different linguistic phenomena de-
21
Figure 2: Screenshot of the web application (visualisation of results).
fined by the user. The following subsections detail
both parts of the demo.
3.1 Web Service
A SOAP-compliant web service3 has been built on
top of DELiC4MT. It receives the following input
parameters (see Figure 1):
1. Word alignment between the source and target
sides of the testset, in the GIZA++ (Och and
Ney, 2003) output format.
2. Linguistic checkpoint defined as a Ky-
bot4 (Vossen et al, 2010) profile.
3. Output of the MT system to be evaluated, in
plain text, tokenised and one sentence per line.
4. Source and target sides of the testset (or
gold standard), in KAF format (Bosma et al,
2009).5
The tool then evaluates the performance of the
MT system (input parameter 3) on the linguistic phe-
nomenon (parameter 2) by following this procedure:
3http://registry.elda.org/services/301
4Kybot profiles can be understood as regular expressions
over KAF documents, http://kyoto.let.vu.nl/svn/
kyoto/trunk/modules/mining_module/
5An XML format for text analysis based on representation
standards from ISO TC37/SC4.
? Occurrences of the linguistic phenomenon (pa-
rameter 2) are identified in the source side of
the testset (parameter 4).
? The equivalent tokens of these occurrences in
the target side (parameter 5) are found by using
word alignment information (parameter 1).
? For each checkpoint instance, the tool checks
how many of the n-grams present in the refer-
ence of the checkpoint instance are contained
in the output produced by the MT system (pa-
rameter 3).
3.2 Web Application
The web application builds a graphical interface on
top of the web service. It allows the user to visualise
the results in a fine-grained manner, the user can see
the performance of the MT system for each single
occurrence of the linguistic phenomenon.
Sample MT output for the ?noun? checkpoint for
the English to French language direction is shown
in Figure 2. Two occurrences of the checkpoint are
shown. The first one regards the source noun ?mr.?
and its translation in the reference ?monsieur?, iden-
tified through word alignments. The alignment (4-
4) indicates that both the source and target tokens
appear at the fifth position (0-based index) in the
sentence. The reference token (?monsieur?) is not
found in the MT output and thus a score of 0/1
22
(0 n-gram matches out of a total of 1 possible n-
gram) is assigned to the MT system for this noun in-
stance. Conversely, the score for the second occur-
rence (?speaker?) is 1/1 since the MT output con-
tains the 1-gram of the reference translation (?ora-
teur?).
The recall-based overall score is shown at the bot-
tom of the figure (0.5025). This is calculated by
summing up the scores (matching n-grams) for all
the occurrences (803) and dividing the result by the
total number of possible n-grams (1598).
4 Conclusions
In this paper we have presented a web applica-
tion and a web service for the diagnostic evalua-
tion of MT output over linguistic phenomena using
DELiC4MT. The tool allows users and developers
of MT systems to easily receive fine-grained feed-
back on the performance of their MT systems over
linguistic checkpoints of their interest. The applica-
tion is open-source, freely available and adaptable to
any language pair.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Union Sev-
enth Framework Programme FP7/2007-2013 under
grant agreements FP7-ICT-4-248531 and PIAP-GA-
2012-324414 and through Science Foundation Ire-
land as part of the CNGL (grant 07/CE/I1142)
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Intrin-
sic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, Proceedings of the
ACL-05 Workshop, pages 65?72, University of Michi-
gan, Ann Arbor, Michigan, USA.
Jan Berka, Ondej Bojar, Mark Fishel, Maja Popovi, and
Daniel Zeman. 2012. Automatic MT Error Anal-
ysis: Hjerson Helping Addicter. In Proceedings of
the Eight International Conference on Language Re-
sources and Evaluation (LREC?12), Istanbul, Turkey.
European Language Resources Association (ELRA).
W. E. Bosma, Piek Vossen, Aitor Soroa, German Rigau,
Maurizio Tesconi, Andrea Marchetti, Monica Mona-
chini, and Carlo Aliprandi. 2009. KAF: a generic
semantic annotation format. In Proceedings of the
GL2009 Workshop on Semantic Annotation, Septem-
ber.
Alberto Labarga, Franck Valentin, Mikael Andersson,
and Rodrigo Lopez. 2007. Web services at the euro-
pean bioinformatics institute. Nucleic Acids Research,
35(Web-Server-Issue):6?11.
Nitin Madnani. 2011. iBLEU: Interactively Debugging
and Scoring Statistical Machine Translation Systems.
In Proceedings of the 2011 IEEE Fifth International
Conference on Semantic Computing, ICSC ?11, pages
213?214, Washington, DC, USA. IEEE Computer So-
ciety.
George A. Miller. 1995. WordNet: a lexical database for
English. Commun. ACM, 38(11):39?41, November.
Sudip Kumar Naskar, Antonio Toral, Federico Gaspari,
and Andy Way. 2011. A Framework for Diagnostic
Evaluation of MT based on Linguistic Checkpoints. In
Proceedings of the 13th Machine Translation Summit,
pages 529?536, Xiamen, China, September.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29:19?51, March.
Tom Oinn, Matthew Addis, Justin Ferris, Darren Marvin,
Martin Senger, Mark Greenwood, Tim Carver, Kevin
Glover, Matthew R. Pocock, Anil Wipat, and Peter Li.
2004. Taverna: a tool for the composition and en-
actment of bioinformatics workflows. Bioinformatics,
20(17):3045?3054, November.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Antonio Toral, Sudip Kumar Naskar, Federico Gaspari,
and Declan Groves. 2012. DELiC4MT: A Tool for
Diagnostic MT Evaluation over User-defined Linguis-
tic Phenomena. The Prague Bulletin of Mathematical
Linguistics, pages 121?132.
Piek Vossen, German Rigau, Eneko Agirre, Aitor Soroa,
Monica Monachini, and Roberto Bartolini. 2010. KY-
OTO: an open platform for mining facts. In Proceed-
ings of the 6th Workshop on Ontologies and Lexical
Resources, pages 1?10, Beijing, China.
Ming Zhou, Bo Wang, Shujie Liu, Mu Li, Dongdong
Zhang, and Tiejun Zhao. 2008. Diagnostic evalu-
ation of machine translation systems using automati-
cally constructed linguistic check-points. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics - Volume 1, COLING ?08, pages
1121?1128, Stroudsburg, PA, USA. Association for
Computational Linguistics.
23
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 143?148,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
MATREX: The DCU MT System for WMT 2010
Sergio Penkale, Rejwanul Haque, Sandipan Dandapat, Pratyush Banerjee, Ankit K. Srivastava,
Jinhua Du, Pavel Pecina, Sudip Kumar Naskar, Mikel L. Forcada, Andy Way
CNGL, School of Computing
Dublin City University, Dublin 9, Ireland
{ spenkale, rhaque, sdandapat, pbanerjee, asrivastava, jdu, ppecina, snaskar, mforcada, away }@computing.dcu.ie
Abstract
This paper describes the DCU machine
translation system in the evaluation cam-
paign of the Joint Fifth Workshop on Sta-
tistical Machine Translation and Metrics
in ACL-2010. We describe the modular
design of our multi-engine machine trans-
lation (MT) system with particular focus
on the components used in this partici-
pation. We participated in the English?
Spanish and English?Czech translation
tasks, in which we employed our multi-
engine architecture to translate. We also
participated in the system combination
task which was carried out by the MBR
decoder and confusion network decoder.
1 Introduction
In this paper, we present the DCU multi-engine
MT system MATREX (Machine Translation using
Examples). This system exploits example-based
MT, statistical MT (SMT), and system combina-
tion techniques.
We participated in the English?Spanish (en?
es) and English?Czech (en?cs) translation
tasks. For these two tasks, we employ several
individual MT systems: 1) Baseline: phrase-
based SMT (Koehn et al, 2007); 2) EBMT:
Monolingually chunking both source and target
sides of the dataset using a marker-based chunker
(Gough and Way, 2004); 3) Factored translation
model (Koehn and Hoang, 2007); 4) Source-side
context-informed (SSCI) systems (Stroppa et al,
2007); 5) the moses-chart (a Moses imple-
mentation of the hierarchical phrase-based (HPB)
approach of Chiang (2007)) and 6) Apertium (For-
cada et al, 2009) rule-based machine translation
(RBMT). Finally, we use a word-level combina-
tion framework (Rosti et al, 2007) to combine the
multiple translation hypotheses and employ a new
rescoring model to generate the final translation.
For the system combination task, we first use
the minimum Bayes-risk (MBR) (Kumar and
Byrne, 2004) decoder to select the best hypoth-
esis as the alignment reference for the confusion
network (CN) (Mangu et al, 2000). We then build
the CN using the TER metric (Snover et al, 2006),
and finally search for the best translation.
The remainder of this paper is organised as fol-
lows: Section 2 details the various components of
our system, in particular the multi-engine strate-
gies used for the shared task. In Section 3, we
outline the complete system setup for the shared
task and provide evaluation results on the test set.
Section 4 concludes the paper.
2 The MATREX System
2.1 System Architecture
The MATREX system is a combination-based
multi-engine architecture, which exploits as-
pects of both the EBMT and SMT paradigms.
The architecture includes various individual sys-
tems: phrase-based, example-based, hierarchical
phrase-based and tree-based MT.
The combination structure uses the MBR and
CN decoders, and is based on a word-level com-
bination strategy (Du et al, 2009). In the final
stage, we use a new rescoring module to process
the N -best list generated by the combination mod-
ule. Figure 1 illustrates the architecture.
2.2 Example-Based Machine Translation
The EBMT system uses a language-specific, re-
duced set of closed-class marker morphemes or
lexemes (Gough and Way, 2004) to define a way
to segment sentences into chunks, which are then
aligned using an edit-distance-style algorithm, in
which edit costs depend on word-to-word transla-143
Figure 1: System Framework.
tion probabilities and the amount of word-to-word
cognates (Stroppa and Way, 2006).
Once these phrase pairs were obtained they
were merged with the phrase pairs extracted by
the baseline system adding word alignment infor-
mation.
2.3 Apertium RBMT
Apertium1 is a free/open-source platform for
RBMT. The current version of the en?es system
in Apertium was used for the system combination
task (section 2.7), and its morphological analysers
and part-of-speech taggers were used to build a
factored Moses model.
2.4 Factored Translation Model
We also used a factored model for the en?es
translation task. Factored models (Koehn and
Hoang, 2007) facilitate the translation by break-
ing it down into several factors which are further
combined using a log-linear model (Och and Ney,
2002).
We used three factors in our factored translation
model, which are used in two different decoding
paths: a surface form (SF) to SF translation factor,
a lemma to lemma translation factor, and a part-of-
speech (PoS) to PoS translation factor.
Finally, we used two decoding paths based on
1http://www.apertium.org
the above three translation factors: an SF to SF
decoding path and a path which maps lemma to
lemma, PoS to PoS, and an SF generated using
the TL lemma and PoS. The lemmas and PoS for
en and es were obtained using Apertium (sec-
tion 2.3).
2.5 Source-Side Context-informed PB-SMT
One natural way to express a context-informed
feature (h?MBL) is to view it as the conditional
probability of the target phrases (e?k) given the
source phrase (f?k) and its source-side context in-
formation (CI):
h?MBL = logP (e?k|f?k,CI(f?k)) (1)
We use a memory-based machine learning
(MBL) classifier (TRIBL:2 Daelemans and
van den Bosch (2005)) that is able to estimate
P (e?k|f?k,CI(f?k)) by similarity-based reasoning
over memorized nearest-neighbour examples of
source?target phrase translations. In equation (1),
SSCI may include any feature (lexical, syntactic,
etc.), which can provide useful information to
disambiguate a given source phrase. In addition
to using local words and PoS-tags as features,
as in (Stroppa et al, 2007), we incorporate
grammatical dependency relations (Haque et al,
2009a) and supertags (Haque et al, 2009b) as
syntactic source context features in the log-linear
PB-SMT model.
In addition to the above feature, we derived a
simple binary feature h?best, defined in (2):
h?best =
{
1 if e?k maximizes P (e?k|f?k,CI(f?k))
0 otherwise
(2)
We performed experiments by integrating these
two features, h?MBL and h?best, directly into the
log-linear framework of Moses.
2.6 Hierarchical PB-SMT model
For the en?cs translation task, we built
a weighted synchronous context-free grammar
model (Chiang, 2007) of translation that uses
the bilingual phrase pairs of PB-SMT as a start-
ing point to learn hierarchical rules. We used
the open-source Tree-Based translation system
moses-chart3 to perform this experiment.
2An implementation of TRIBL is freely available as part
of the TiMBL software package, which can be downloaded
from http://ilk.uvt.nl/timbl
3http://www.statmt.org/moses/?n=Moses.SyntaxTutorial144
2.7 System Combination
For multiple system combination, we used an
MBR-CN framework (Du et al, 2009, 2010) as
shown in Figure 1. Due to the varying word or-
der in the MT hypotheses, it is essential to define
the backbone which determines the general word
order of the CN. Instead of using a single system
output as the skeleton, we employ an MBR de-
coder to select the best single system output Er
from the merged N -best list by minimizing the
BLEU (Papineni et al, 2002) loss, as in (3):
r = argmin
i
Ns?
j=1
(1? BLEU(Ej , Ei)) (3)
where Ns indicates the number of translations in
the merged N -best list, and {Ei}Nsi=1 are the trans-
lations themselves. In our task, we only merge the
1-best output of each individual system.
The CN is built by aligning other hypotheses
against the backbone, based on the TER metric.
Null words are allowed in the alignment. Ei-
ther votes or different confidence measures are as-
signed to each word in the network. Each arc in
the CN represents an alternative word at that po-
sition in the sentence and the number of votes for
each word is counted when constructing the net-
work. The features we used are as follows:
? word posterior probability (Fiscus, 1997);
? 3, 4-gram target language model;
? word length penalty;
? Null word length penalty;
We use MERT (Och, 2003) to tune the weights
of the CN.
2.8 Rescoring
Rescoring is a very important part in post-
processing which can select a better hypothesis
from the N -best list. We augmented our previ-
ous rescoring model (Du et al, 2009) with more
large-scale data. The features we used include:
? Direct and inverse IBM model;
? 3, 4-gram target language model;
? 3, 4, 5-gram PoS language model (Schmid,
1994; Ratnaparkhi, 1996);
? Sentence length posterior probability (Zens
and Ney, 2006);
? N -gram posterior probabilities within the N -
Best list (Zens and Ney, 2006);
? Minimum Bayes Risk probability;
? Length ratio between source and target sen-
tence;
The weights are optimized via MERT.
3 Experimental Setup
This section describes our experimental setup for
the en?cs and en?es translation tasks.
3.1 Data
Bilingual data: In the experiments we used data
sets provided by the workshop organizers. For the
en?cs translation table extraction we employed
both parallel corpora (News-Commentary10 and
CzEng 0.9), and for the en?es experiments, we
used the Europarl(Koehn, 2005), News Commen-
tary and United Nations parallel data. We used a
maximum sentence length of 80 for en?es and
40 for en?cs. Detailed statistics are shown in Ta-
ble 1.
Corpus Langs. Sent. Source
tokens
Target
tokens
Europarl en?es 1.6M 43M 45M
News-comm en?es 97k 2.4M 2.7M
UN en?es 5.9M 160M 190M
News-Comm en?cs 85k 1.8M 1.6M
CzEng en?cs 7.8M 80M 69M
Table 1: Statistics of en?cs and en?es parallel data.
Monolingual data: For language modeling pur-
poses, in addition to the target parts of the bilin-
gual data, we used the monolingual News corpus
for cs; and the Gigaword corpus for es. For both
languages, we used the SRILM toolkit (Stolcke,
2002) to train a 5-gram language model using all
monolingual data provided. However, for en?es
we used the IRSTLM toolkit (Federico and Cet-
tolo, 2007) to train a 5-gram language model using
the es Gigaword corpus. Both language models
use modified Kneser-Ney smoothing (Chen and
Goodman, 1996). Statistics for the monolingual
corpora are given in Table 2.
Corpus Language Sentences Tokens
E/N/NC/UN es 9,6M 290M
Gigaword es 40M 1,2G
News cs 13M 210M
Table 2: Statistics of Monolingual Data. E/N/NC/UN
refers to Europarl/News/News Commentary/United Nations
corpora.
For all the systems except Apertium, we first
lowercase and tokenize all the monolingual and
bilingual data using the tools provided by the
WMT10 organizers. After translation, system
combination output is detokenised and true-cased.145
3.2 English?Czech (en?cs) Experiments
The CzEng corpus (Bojar and Z?abokrtsky?, 2009)
is a collection of parallel texts from sources of dif-
ferent quality and as such it contains some noise.
As the first step, we discarded those sentence pairs
having more than 10% of non-Latin characters.
The CzEng corpus is quite large (8M sen-
tence pairs). Although we were able to build
a vanilla SMT system on all parallel data avail-
able (News-Commentary + CzEng), we also at-
tempted to build additional systems using News-
Commentary data (which we considered in-
domain) and various in-domain subsets of CzEng
hoping to achieve better results on domain-
specific data.
For our first system, we selected 128,218 sen-
tence pairs from CzEng labeled as news. For the
other two systems, we selected subsets of 2M and
4M sentence pairs identified as most similar to
the development sets (as a sample of in-domain
data) based on cosine similarity of their represen-
tation in a TF-IDF weighted vector space model
(cf. Byrne et al (2003)). We also applied the
pseudo-relevavance-feedback technique for query
expansion (Manning et al, 2008) to select another
subset with 2M sentence pairs.
We used the output of 15 systems for sys-
tem combination for the en?cs translation task.
Among these, 5 systems were built using Moses
and varying the size of the training data (DCU-
All, DCU-Ex2M, DCU-4M, DCU-2M and DCU-
News); 9 context-informed PB-SMT systems
(DCU-SSCI-*) using (combinations of) various
context features (word, PoS, supertags and depen-
dency relations) trained only on the News Com-
mentary data (marked with ? in Table 4); and one
system using the moses-chart decoder, also
trained on the news commentary data.
3.3 English?Spanish (en?es) Experiments
Three baseline systems using Moses were built,
where we varied the amount of training data used:
? epn: This system uses all of the Europarl and
News-Commentary parallel data.
? UN-half: This system uses the data suplied
to ?epn?, plus an additional 2.1M sentences
pairs randomly selected from the United Na-
tions corpus.
? all: This system uses all of the available par-
allel data.
For en?es we also obtained output from the
factored model (trained only on the news com-
mentary corpus) and the Apertium RBMT sys-
tem. We also derived phrase alignments using the
MaTrEx EBMT system (Stroppa and Way, 2006),
and added those phrase translations in the Moses
phrase table. The systems marked with ? use a
language model built using the Spanish Gigaword
corpus, in addition to the one built using the pro-
vided monolingual data. These 6 sets of system
outputs are then used for system combination.
3.4 Experimental Results
The evaluation results for en?es and en?cs ex-
periments are shown in Table 3 and Table 4 re-
spectively. The output of the systems marked ?
were submitted in the shared tasks.
System BLEU NIST METEOR TER
DCU-half ?? 29.77% 7.68 59.86% 59.55%
DCU-all ?? 29.63% 7.66 59.82% 59.74%
DCU-epn ?? 29.45% 7.66 59.71% 59.64%
DCU-ebmt ?? 29.38% 7.62 59.59% 60.11%
DCU-factor 22.58% 6.56 54.94% 67.65%
DCU-apertium 19.22% 6.37 49.68% 67.68%
DCU-system-
combination ? 30.42% 7.78 60.56% 58.71%
Table 3: en?es experimental results.
System BLEU NIST METEOR TER
DCU-All 10.91% 4.60 39.18% 81.76%
DCU-Ex2M 10.63% 4.56 39.12% 81.96%
DCU-4M 10.61% 4.56 39.26% 82.04%
DCU-2M 10.48% 4.58 39.35% 81.56%
DCU-Chart 9.34% 4.25 37.04% 83.87%
DCU-News 8.64% 4.16 36.27% 84.96%
DCU-SSCI-ccg? 8.26% 4.02 34.76% 85.58%
DCU-SSCI-
supertag-pair? 8.11% 3.95 34.93% 86.63%
DCU-SSCI-
ccg-ltag? 8.09% 3.96 34.90% 86.62%
DCU-SSCI-PR? 8.06% 4.00 34.89% 85.99%
DCU-SSCI-base? 8.05% 3.97 34.61% 86.02%
DCU-SSCI-PRIR? 8.03% 3.99 34.81% 85.98%
DCU-SSCI-ltag? 8.00% 3.95 34.57% 86.41%
DCU-SSCI-PoS? 7.91% 3.94 34.57% 86.51%
DCU-SSCI-word? 7.57% 3.88 34.16% 87.14%
DCU-system-
combination ? 13.22% 4.98 40.39% 78.59%
Table 4: en?cs experimental results.
4 Conclusion
This paper presents the Dublin City University
MT system in WMT2010 shared task campaign.
This was DCU?s first attempt to translate from en
to es and cs in any shared task. We developed a
multi-engine framework which combined the out-
puts of several individual MT systems and gener-
ated a new N -best list after CN decoding. Then by146
using some global features, the rescoring model
generated the final translation output. The experi-
mental results demonstrated that the combination
module and rescoring module are effective in our
framework for both language pairs, and produce
statistically significant improvements as measured
by bootstrap resampling methods (Koehn, 2004)
on BLEU over the single best system.
Acknowledgements: This work is supported
by Science Foundation Ireland (Grant No.
07/CE/I1142) and by PANACEA, a 7th Frame-
work Research Programme of the European
Union, contract number 7FP-ITC-248064. M.L.
Forcada?s sabbatical stay at Dublin City Univer-
sity is supported by Science Foundation Ireland
through ETS Walton Award 07/W.1/I1802 and by
the Universitat d?Alacant (Spain).
References
Bojar, O. and Z?abokrtsky?, Z. (2009). CzEng0.9:
Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics,
92:63?83.
Byrne, W., Khudanpur, S., Kim, W., Kumar, S.,
Pecina, P., Virga, P., Xu, P., and Yarowsky, D.
(2003). The Johns Hopkins University 2003
Chinese?English machine translation system.
In Proceedings of MT Summit IX, pages 447?
450, New Orleans, LA.
Chen, S. F. and Goodman, J. (1996). An Empir-
ical Study of Smoothing Techniques for Lan-
guage Modeling. In Proc. 34th Ann. Meeting of
the Association for Computational Linguistics,
pages 310?318, San Francisco, CA.
Chiang, D. (2007). Hierarchical phrase-
based translation. Computational Linguistics,
33(2):201?228.
Daelemans, W. and van den Bosch, A. (2005).
Memory-Based Language Processing (Studies
in Natural Language Processing). Cambridge
University Press, New York, NY.
Du, J., He, Y., Penkale, S., and Way, A. (2009).
MaTrEx: The DCU MT System for WMT2009.
In Proc. 3rd Workshop on Statistical Machine
Translation, EACL 2009, pages 95?99, Athens,
Greece.
Du, J., Pecina, P., and Way, A. (2010). An
Augmented Three-Pass System Combination
Framework: DCU Combination System for
WMT 2010. In Proc. ACL 2010 Joint Workshop
in Statistical Machine Translation and Metrics
Matr, Uppsala, Greece.
Federico, M. and Cettolo, M. (2007). Efficient
Handling of N-gram Language Models for Sta-
tistical Machine Translation. In Proceedings
of the Second Workshop on Statistical Machine
Translation, pages 88?95, Prague, Czech Re-
public.
Fiscus, J. G. (1997). A post-processing sys-
tem to yield reduced word error rates: Recog-
nizer output voting error reduction (ROVER).
In Proceedings 1997 IEEE Workshop on Auto-
matic Speech Recognition and Understanding
(ASRU), pages 347?352, Santa Barbara, CA.
Forcada, M. L., Tyers, F. M., and Ram??rez-
Sa?nchez, G. (2009). The free/open-source ma-
chine translation platform Apertium: Five years
on. In Proceedings of the First International
Workshop on Free/Open-Source Rule-Based
Machine Translation FreeRBMT?09, pages 3?
10.
Gough, N. and Way, A. (2004). Robust Large-
Scale EBMT with Marker-Based Segmenta-
tion. In Proceedings of the 10th International
Conference on Theoretical and Methodological
Issues in Machine Translation (TMI-04), pages
95?104, Baltimore, MD.
Haque, R., Naskar, S. K., Bosch, A. v. d., and
Way, A. (2009a). Dependency relations as
source context in phrase-based smt. In Proc.
23rd Pacific Asia Conference on Language, In-
formation and Computation, pages 170?179,
Hong Kong, China.
Haque, R., Naskar, S. K., Ma, Y., and Way, A.
(2009b). Using supertags as source language
context in SMT. In EAMT-2009: Proceed-
ings of the 13th Annual Conference of the Eu-
ropean Association for Machine Translation,
pages 234?241, Barcelona, Spain.
Koehn, P. (2004). Statistical significance tests for
machine translation evaluation. In Proceedings
of EMNLP, volume 4, pages 388?395.
Koehn, P. (2005). Europarl: A Parallel Corpus
for Statistical Machine Translation. In Machine
Translation Summit X, pages 79?86, Phuket,
Thailand.
Koehn, P. and Hoang, H. (2007). Factored Trans-
lation Models. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural147
Language Learning (EMNLP-CoNLL), pages
868?876, Prague, Czech Republic.
Koehn, P., Hoang, H., Birch, A., Callison-Burch,
C., Federico, M., Bertoldi, N., Cowan, B.,
Shen, W., Moran, C., Zens, R., Dyer, C., Bo-
jar, O., Constantin, A., and Herbst, E. (2007).
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In Annual Meeting of the As-
sociation for Computational Linguistics (ACL),
demonstration session, pages 177?180, Prague,
Czech Republic.
Kumar, S. and Byrne, W. (2004). Minimum
Bayes-Risk Decoding for Statistical Machine
Translation. In Proceedings of the Joint Meet-
ing of the Human Language Technology Con-
ference and the North American Chapter of
the Association for Computational Linguistics
(HLT-NAACL 2004), pages 169?176, Boston,
MA.
Mangu, L., Brill, E., and Stolcke, A. (2000). Find-
ing consensus in speech recognition: Word er-
ror minimization and other applications of con-
fusion networks. Computer Speech and Lan-
guage, 14(4):373?400.
Manning, C. D., Raghavan, P., and Schu?tze, H.
(2008). Introduction to Information Retrieval.
Cambridge University Press.
Och, F. (2003). Minimum error rate training
in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL),
pages 160?167, Sapporo, Japan.
Och, F. and Ney, H. (2002). Discriminative train-
ing and maximum entropy models for statistical
machine translation. In Proceedings of ACL,
volume 2, pages 295?302.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL-02), pages
311?318, Philadelphia, PA.
Ratnaparkhi, A. (1996). A Maximum Entropy
Model for Part-Of-Speech Tagging. In Pro-
ceedings of the Empirical Methods in Natural
Language Processing Conference (EMNLP),
pages 133?142, Philadelphia, PA.
Rosti, A.-V. I., Xiang, B., Matsoukas, S.,
Schwartz, R., Ayan, N. F., and Dorr, B. J.
(2007). Combining outputs from multiple ma-
chine translation systems. In Proceedings of the
Joint Meeting of the Human Language Technol-
ogy Conference and the North American Chap-
ter of the Association for Computational Lin-
guistics (HLT-NAACL 2007), pages 228?235,
Rochester, NY.
Schmid, H. (1994). Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings
of International Conference on New Methods
in Language Processing, pages 44?49, Manch-
ester, UK.
Snover, M., Dorr, B., Schwartz, R., Micciula, L.,
and Makhoul, J. (2006). A study of transla-
tion edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Amer-
icas (AMTA 2006), pages 223?231, Cambridge,
MA.
Stolcke, A. (2002). SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of
the International Conference Spoken Language
Processing, pages 901?904, Denver, CO.
Stroppa, N., van den Bosch, A., and Way, A.
(2007). Exploiting Source Similarity for SMT
using Context-Informed Features. In Proceed-
ings of the 11th International Conference on
Theoretical and Methodological Issues in Ma-
chine Translation (TMI-07), pages 231?240,
Sko?vde, Sweden.
Stroppa, N. and Way, A. (2006). MaTrEx: the
DCU machine translation system for IWSLT
2006. In Proceedings of the International Work-
shop on Spoken Language Translation, pages
31?36, Kyoto, Japan.
Zens, R. and Ney, H. (2006). N-gram Poste-
rior Probabilities for Statistical Machine Trans-
lation. In Proceedings of the Joint Meeting of
the Human Language Technology Conference
and the North American Chapter of the As-
sociation for Computational Linguistics (HLT-
NAACL 2006), pages 72?77, New York, NY.
148
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 46?54,
Beijing, August 2010
Handling Named Entities and Compound Verbs in             
Phrase-Based Statistical Machine Translation 
Santanu Pal*, Sudip Kumar Naskar?, Pavel Pecina?,  
Sivaji Bandyopadhyay* and Andy Way? 
*Dept. of Comp. Sc. & Engg. 
Jadavpur University 
santanupersonal1@gmail.com, sivaji_cse_ju@yahoo.com 
?CNGL, School of Computing 
Dublin City University 
{snaskar, ppecina, away}@computing.dcu.ie 
 
Abstract 
Data preprocessing plays a crucial role in 
phrase-based statistical machine transla-
tion (PB-SMT). In this paper, we show 
how single-tokenization of two types of 
multi-word expressions (MWE), namely 
named entities (NE) and compound 
verbs, as well as their prior alignment 
can boost the performance of PB-SMT. 
Single-tokenization of compound verbs 
and named entities (NE) provides sig-
nificant gains over the baseline PB-SMT 
system. Automatic alignment of NEs 
substantially improves the overall MT 
performance, and thereby the word 
alignment quality indirectly. For estab-
lishing NE alignments, we transliterate 
source NEs into the target language and 
then compare them with the target NEs. 
Target language NEs are first converted 
into a canonical form before the com-
parison takes place. Our best system 
achieves statistically significant im-
provements (4.59 BLEU points absolute, 
52.5% relative improvement) on an Eng-
lish?Bangla translation task. 
1 Introduction 
Statistical machine translation (SMT) heavily 
relies on good quality word alignment and 
phrase alignment tables comprising translation 
knowledge acquired from a bilingual corpus. 
Multi-word expressions (MWE) are defined 
as ?idiosyncratic interpretations that cross word 
boundaries (or spaces)? (Sag et al, 2002). Tradi-
tional approaches to word alignment following 
IBM Models (Brown et al, 1993) do not work 
well with multi-word expressions, especially 
with NEs, due to their inability to handle many-
to-many alignments. Firstly, they only carry out 
alignment between words and do not consider 
the case of complex expressions, such as multi-
word NEs. Secondly, the IBM Models only al-
low at most one word in the source language to 
correspond to a word in the target language 
(Marcu, 2001, Koehn et al, 2003). 
In another well-known word alignment ap-
proach, Hidden Markov Model (HMM: Vogel et 
al., 1996), the alignment probabilities depend on 
the alignment position of the previous word. It 
does not explicitly consider many-to-many 
alignment either. 
We address this many-to-many alignment 
problem indirectly. Our objective is to see how 
to best handle the MWEs in SMT. In this work, 
two types of MWEs, namely NEs and compound 
verbs, are automatically identified on both sides 
of the parallel corpus. Then, source and target 
language NEs are aligned using a statistical 
transliteration method. We rely on these auto-
matically aligned NEs and treat them as transla-
tion examples. Adding bilingual dictionaries, 
which in effect are instances of atomic transla-
tion pairs, to the parallel corpus is a well-known 
practice in domain adaptation in SMT (Eck et 
al., 2004; Wu et al, 2008). We modify the paral-
lel corpus by converting the MWEs into single 
tokens and adding the aligned NEs in the parallel 
corpus in a bid to improve the word alignment, 
and hence the phrase alignment quality. This 
46
preprocessing results in improved MT quality in 
terms of automatic MT evaluation metrics. 
The remainder of the paper is organized as 
follows. In section 2 we discuss related work. 
The System is described in Section 3.  Section 4 
includes the results obtained, together with some 
analysis. Section 5 concludes, and provides ave-
nues for further work. 
2 Related Work 
Moore (2003) presented an approach for si-
multaneous NE identification and translation. He 
uses capitalization cues for identifying NEs on 
the English side, and then he applies statistical 
techniques to decide which portion of the target 
language corresponds to the specified English 
NE. Feng et al (2004) proposed a Maximum 
Entropy model based approach for English?
Chinese NE alignment which significantly out-
performs IBM Model4 and HMM. They consid-
ered 4 features: translation score, transliteration 
score, source NE and target NE's co-occurrence 
score, and the distortion score for distinguishing 
identical NEs in the same sentence. Huang et al 
(2003) proposed a method for automatically ex-
tracting NE translingual equivalences between 
Chinese and English based on multi-feature cost 
minimization. The costs considered are translit-
eration cost, word-based translation cost, and NE 
tagging cost. 
Venkatapathy and Joshi (2006) reported a dis-
criminative approach of using the compositional-
ity information about verb-based multi-word 
expressions to improve word alignment quality. 
(Ren et al, 2009) presented log likelihood ratio-
based hierarchical reducing algorithm to auto-
matically extract bilingual MWEs, and investi-
gated the usefulness of these bilingual MWEs in 
SMT by integrating bilingual MWEs into Moses 
(Koehn et al, 2007) in three ways. They ob-
served the highest improvement when they used 
an additional feature to represent whether or not 
a bilingual phrase contains bilingual MWEs. 
This approach was generalized in Carpuat and 
Diab (2010). In their work, the binary feature 
was replaced by a count feature representing the 
number of MWEs in the source language phrase. 
Intuitively, MWEs should be both aligned in 
the parallel corpus and translated as a whole. 
However, in the state-of-the-art PB-SMT, it 
could well be the case that constituents of an 
MWE are marked and aligned as parts of con-
secutive phrases, since PB-SMT (or any other 
approaches to SMT) does not generally treat 
MWEs as special tokens. Another problem SMT 
suffers from is that verb phrases are often 
wrongly translated, or even sometimes deleted in 
the output in order to produce a target sentence 
considered good by the language model. More-
over, the words inside verb phrases seldom show 
the tendency of being aligned one-to-one; the 
alignments of the words inside source and target 
verb phrases are mostly many-to-many, particu-
larly so for the English?Bangla language pair. 
These are the motivations behind considering 
NEs and compound verbs for special treatment 
in this work. 
By converting the MWEs into single tokens, 
we make sure that PB-SMT also treats them as a 
whole. The objective of the present work is two-
fold; firstly to see how treatment of NEs and 
compound verbs as a single unit affects the 
overall MT quality, and secondly whether prior 
automatic alignment of these single-tokenized 
MWEs can bring about any further improvement 
on top of that. 
We carried out our experiments on an Eng-
lish?Bangla translation task, a relatively hard 
task with Bangla being a morphologically richer 
language. 
3 System Description 
3.1 PB-SMT 
Translation is modeled in SMT as a decision 
process, in which the translation Ie1 = e1 . . . ei . . 
. eI of a source sentence
Jf1 = f1 . . . fj . . . fJ is 
chosen to maximize (1): 
)().|(maxarg)|(maxarg 111
,
11
, 11
IIJ
eI
JI
eI
ePefPfeP
II
=      (1)  
where )|( 11
IJ efP  and )( 1
IeP  denote respec-
tively the translation model and the target lan-
guage model (Brown et al, 1993). In log-linear 
phrase-based SMT, the posterior probability 
)|( 11
JI feP  is directly modeled as a log-linear 
combination of features (Och and Ney, 2002), 
that usually comprise M translational features, 
and the language model, as in (2): 
 
47
?
=
=
M
m
KIJ
mm
JI sefhfeP
1
11111 ),,()|(log ?  
)(log 1
I
LM eP?+        (2)     
where k
k sss ...11 =  denotes a segmentation of the 
source and target sentences respectively into the 
sequences of phrases )?,...,?( 1 kee  and )?,...,?( 1 kff  
such that (we set i0 = 0) (3): 
,1 Kk ???  sk = (ik, bk, jk), 
          
kk iik
eee ...? 11 +?= , 
         
kk jbk
fff ...? = .          (3) 
and each feature mh?  in (2) can be rewritten as in 
(4): 
?
=
=
K
k
kkkm
KIJ
m sefhsefh
1
111 ),?,?(?),,(                  (4) 
where mh? is a feature that applies to a single 
phrase-pair. It thus follows (5): 
? ??
= ==
=
K
k
K
k
kkkkkkm
M
m
m sefhsefh
1 11
),?,?(?),?,?(??      (5) 
where m
M
m
mhh ??
1
?
=
= ? .            
3.2 Preprocessing of the Parallel Corpus 
The initial English?Bangla parallel corpus is 
cleaned and filtered using a semi-automatic 
process. We employed two kinds of multi-word 
information: compound verbs and NEs. Com-
pound verbs are first identified on both sides of 
the parallel corpus. Chakrabarty et al (2008) 
analyzed and identified a category of V+V com-
plex predicates called lexical compound verbs 
for Hindi. We adapted their strategy for identifi-
cation of compound verbs in Bangla. In addition 
to V+V construction, we also consider N+V and 
ADJ+V structures. 
NEs are also identified on both sides of trans-
lation pairs. NEs in Bangla are much harder to 
identify than in English (Ekbal and Bandyop-
adhyay, 2009). This can be attributed to the fact 
that (i) there is no concept of capitalization in 
Bangla; and (ii) Bangla common nouns are often 
used as proper names. In Bangla, the problem is 
compounded by the fact that suffixes (case 
markers, plural markers, emphasizers, specifiers) 
are also added to proper names, just like to any 
other common nouns. As a consequence, the ac-
curacy of Bangla NE recognizers (NER) is much 
poorer compared to that for English. Once the 
compound verbs and the NEs are identified on 
both sides of the parallel corpus, they are con-
verted into and replaced by single tokens. When 
converting these MWEs into single tokens, we 
replace the spaces with underscores (?_?). Since 
there are already some hyphenated words in the 
corpus, we do not use hyphenation for this pur-
pose; besides, the use of a special word separator 
(underscore in our case) facilitates the job of 
deciding which single-token (target language) 
MWEs to detokenize into words comprising 
them, before evaluation. 
3.3 Transliteration  Using Modified Joint 
Source-Channel Model 
Li et al (2004) proposed a generative framework 
allowing direct orthographical mapping of trans-
literation units through a joint source-channel 
model, which is also called n-gram translitera-
tion model. They modeled the segmentation of 
names into transliteration units (TU) and their 
alignment preferences using maximum likeli-
hood via EM algorithm (Dempster et al, 1977). 
Unlike the noisy-channel model, the joint 
source-channel model tries to capture how 
source and target names can be generated simul-
taneously by means of contextual n-grams of the 
transliteration units. For K aligned TUs, they 
define the bigram model as in (6): 
 )...,,...,(),( 2121 KK bbbeeePBEP =  
  ),...,,,( 21 KbebebeP ><><><=  
   ? ><><= K
=k
k bebeP
1
1-k
1 ),|,(         (6) 
where E refers to the English name and B the 
transliteration in Bengali, while ei and bi refer to 
the ith English and Bangla segment (TU) respec-
tively. 
Ekbal et al (2006) presented a modification to 
the joint source-channel model to incorporate 
different contextual information into the model 
for Indian languages. They used regular expres-
sions and language-specific heuristics based on 
consonant and vowel patterns to segment names 
into TUs. Their modified joint source-channel 
model, for which they obtained improvement 
48
over the original joint source-channel model, 
essentially considers a trigram model for the 
source language and a bigram model for the tar-
get, as in (7). 
 ? +><><= K
=k
kk ebebePBEP
1
11-k ),,|,(),(   (7) 
Ekbal et al (2006) reported a word agreement 
ratio of 67.9% on an English?Bangla translit-
eration task. In the present work, we use the 
modified joint source-channel model of (Ekbal 
et al, 2006) to translate names for establishing 
NE alignments in the parallel corpus. 
3.4 Automatic Alignment of NEs through 
Transliteration 
We first create an NE parallel corpus by extract-
ing the source and target (single token) NEs 
from the NE-tagged parallel translations in 
which both sides contain at least one NE. For 
example, we extract the NE translation pairs 
given in (9) from the sentence pair shown in (8), 
where the NEs are shown as italicized. 
(8a) Kirti_Mandir , where Mahatma_Gandhi 
was born , today houses a photo exhibition on 
the life and times of the Mahatma , a library, a 
prayer hall and other memorabilia . 
(8b) ??????_??n? , ?????? ???t?_??n? ??n????? , 
???????? ?????? ???t?? ???? o ??i ????? 
?????????? e??? ??tp????????? , e??? ??i?b?? o 
e??? p?????? ?? e?? a????? s ????????? ??????t 
??? ? 
(9a) Kirti_Mandir Mahatma_Gandhi Mahatma 
(9b) ??????_??n? ???t?_??n? ???t?? 
Then we try to align the source and target NEs 
extracted from a parallel sentence, as illustrated 
in (9). If both sides contain only one NE then the 
alignment is trivial, and we add such NE pairs to 
seed another parallel NE corpus that contains 
examples having only one token in both side. 
Otherwise, we establish alignments between the 
source and target NEs using transliteration. We 
use the joint source-channel model of translitera-
tion (Ekbal et al, 2006) for this purpose.  
If both the source and target side contains n 
number of NEs, and the alignments of n-1 NEs 
can be established through transliteration or by 
means of already existing alignments, then the 
nth alignment is trivial. However, due to the rela-
tive performance difference of the NERs for the 
source and target language, the number of NEs 
identified on the source and target sides is al-
most always unequal (see Section 4). Accord-
ingly, we always use transliteration to establish 
alignments even when it is assumed to be trivial. 
Similarly, for multi-word NEs, intra-NE word 
alignments are established through translitera-
tion or by means of already existing alignments. 
For a multi-word source NE, if we can align all 
the words inside the NE with words inside a tar-
get NE, then we assume they are translations of 
each other. Due to the relatively poor perform-
ance of the Bangla NER, we also store the im-
mediate left and right neighbouring words for 
every NE in Bangla, just in case the left or the 
right word is a valid part of the NE but is not 
properly tagged by the NER. 
As mentioned earlier, since the source side 
NER is much more reliable than the target side 
NER, we transliterate the English NEs, and try 
to align them with the Bangla NEs. For aligning 
(capitalized) English words to Bangla words, we 
take the 5 best transliterations produced by the 
transliteration system for an English word, and 
compare them against the Bangla words. Bangla 
NEs often differ in their choice of matras (vowel 
modifiers). Thus we first normalize the Bangla 
words, both in the target NEs and the transliter-
ated ones, to a canonical form by dropping the 
matras, and then compare the results. In effect, 
therefore, we just compare the consonant se-
quences of every transliteration candidate with 
that of a target side Bangla word; if they match, 
then we align the English word with the Bangla 
word. 
???? (? + ??+ ? + ?) -- ????? (? + ?? + ? + ?? + ?) 
      (10) 
The example in (10) illustrates the procedure. 
Assume, we are trying to align ?Niraj? with 
???????. The transliteration system produces 
?????? from the English word ?Niraj? and we 
compare ?????? with ???????. Since the conso-
nant sequences match in both words, ?????? is 
considered a spelling variation of ???????, and 
the English word ?Niraj? is aligned to the 
Bangla word ???????. 
In this way, we achieve word-level align-
ments, as well as NE-level alignments. (11) 
shows the alignments established from (8). The 
word-level alignments help to establish new 
49
word / NE alignments. Word and NE alignments 
obtained in this way are added to the parallel 
corpus as additional training data. 
(11a) Kirti-Mandir  ? ??????-??n?  
(11b) Kirti ? ?????? 
(11c) Mandir  ? ??n? 
(11d) Mahatma-Gandhi ? ???t?-??n?  
(11e) Mahatma ? ???t? 
(11f) Gandhi ? ??n? 
(11g) Mahatma ? ???t?? 
3.5 Tools and Resources Used 
A sentence-aligned English?Bangla parallel 
corpus containing 14,187 parallel sentences from 
a travel and tourism domain was used in the pre-
sent work. The corpus was obtained from the 
consortium-mode project ?Development of Eng-
lish to Indian Languages Machine Translation 
(EILMT) System? 1. 
The Stanford Parser2 and the CRF chunker3 
were used for identifying compound verbs in the 
source side of the parallel corpus. The Stanford 
NER4 was used to identify NEs on the source 
side (English) of the parallel corpus. 
The sentences on the target side (Bangla) 
were POS-tagged by using the tools obtained 
from the consortium mode project ?Develop-
ment of Indian Languages to Indian Languages 
Machine Translation (ILILMT) System?. NEs in 
Bangla are identified using the NER system of 
Ekbal and Bandyopadhyay (2008). We use the 
Stanford Parser, Stanford NER and the NER for 
Bangla along with the default model files pro-
vided, i.e., with no additional training. 
The effectiveness of the MWE-aligned paral-
lel corpus developed in the work is demonstrated 
by using the standard log-linear PB-SMT model 
as our baseline system: GIZA++ implementation 
of IBM word alignment model 4, phrase-
extraction heuristics described in (Koehn et al, 
2003), minimum-error-rate training (Och, 2003) 
on a held-out development set, target language 
model with Kneser-Ney smoothing (Kneser and 
                                                 
1 The EILMT and ILILMT projects are funded by the De-
partment of Information Technology (DIT), Ministry of 
Communications and Information Technology (MCIT), 
Government of India. 
2 http://nlp.stanford.edu/software/lex-parser.shtml 
3 http://crfchunker.sourceforge.net/ 
4 http://nlp.stanford.edu/software/CRF-NER.shtml 
Ney, 1995) trained with SRILM (Stolcke, 2002), 
and Moses decoder (Koehn et al, 2007). 
4 Experiments and Results 
We randomly extracted 500 sentences each for 
the development set and testset from the initial 
parallel corpus, and treated the rest as the train-
ing corpus. After filtering on maximum allow-
able sentence length of 100 and sentence length 
ratio of 1:2 (either way), the training corpus con-
tained 13,176 sentences. In addition to the target 
side of the parallel corpus, a monolingual Bangla 
corpus containing 293,207 words from the tour-
ism domain was used for the target language 
model. We experimented with different n-gram 
settings for the language model and the maxi-
mum phrase length, and found that a 4-gram 
language model and a maximum phrase length 
of 4 produced the optimum baseline result. We 
therefore carried out the rest of the experiments 
using these settings. 
English Bangla In training set 
T U T U 
Compound verbs 4,874 2,289 14,174 7,154
Single-word NEs 4,720 1,101 5,068 1,175
2-word NEs 4,330 2,961 4,147 3,417
>2 word NEs 1,555 1,271 1,390 1,278
Total NEs 10,605 5,333 10,605 5,870
Total NE words 22,931 8,273 17,107 9,106
Table 1.  MWE statistics (T - Total occur-
rence, U ? Unique). 
Of the 13,676 sentences in the training and 
development set, 13,675 sentences had at least 
one NE on both sides, only 22 sentences had 
equal number of NEs on both sides, and 13,654 
sentences had an unequal number of NEs. Simi-
larly, for the testset, all the sentences had at least 
one NE on both sides, and none had an equal 
number of NEs on both sides. It gives an indica-
tion of the relative performance differences of 
the NERs. 6.6% and 6.58% of the source tokens 
belong to NEs in the training and testset respec-
tively. These statistics reveal the high degree of 
NEs in the tourism domain data that demands 
special treatment. Of the 225 unique NEs ap-
pearing on the source side of the testset, only 65 
NEs are found in the training set.  
50
Experiments Exp BLEU METEOR NIST WER PER TER 
Baseline 1 8.74 20.39 3.98 77.89 62.95 74.60
NEs of any length as Single 
Token (New-MWNEaST) 
2 9.15 18.19 3.88 77.81 63.85 74.61
NEs of length >2 as  
Single Tokens (MWNE-
aST) 
3 8.76 18.78 3.86 78.31 63.78 75.15
 
 
NEs as Single  
Tokens  
(NEaST) 
2-Word NEs as Single To-
kens (2WNEaST) 
4 9.13 17.28 3.92 78.12 63.15 74.85
Compound Verbs as  Single Tokens 
(CVaST) ? 
5 9.56 15.35 3.96 77.60 63.06 74.46
Alignment of NEs of any 
length (New-MWNEA) ? 
6 13.33 24.06 4.44 74.79 60.10 71.25
Alignment of NEs of length 
upto 2 (New-2WNEA) ? 
7 10.35 20.93 4.11 76.49 62.20 73.05
Alignment of NEs of length 
>2 (MWNEA) ? 
8 12.39 23.13 4.36 75.51 60.58 72.06
 
 
 
 
NE Alignment 
(NEA) 
Alignment of NEs of length 
2 (2WNEA) ? 
9 11.2 23.14 4.26 76.13 60.72 72.57
New-MWNEaST 10 8.62 16.64 3.73 78.41 65.21 75.47
MWNEaST 11 8.74 14.68 3.84 78.40 64.05 75.40
 
CVaST 
+NEaST 2WNEaST 12 8.85 16.60 3.86 78.17 63.90 75.33
New-MWNEA? 13 11.22 21.02 4.16 75.99 61.96 73.06
New-2WNEA? 14 10.07 17.67 3.98 77.08 63.35 74.18
MWNEA? 15 10.34 16.34 4.07 77.12 62.38 73.88
 
CVaST +NEA 
2WNEA? 16 10.51 18.92 4.08 76.77 62.28 73.56
Table 2.  Evaluation results for different experimental setups (The ??? marked systems produce 
statistically significant improvements on BLEU over the baseline system).
Table 1 shows the MWE statistics of the 
parallel corpus as identified by the NERs. The 
average NE length in the training corpus is 
2.16 for English and 1.61 for Bangla. As can 
be seen from Table 1, 44.5% and 47.8% of the 
NEs are single-word NEs in English and 
Bangla respectively, which suggests that prior 
alignment of the single-word NEs, in addition 
to multi-word NE alignment, should also be 
beneficial to word and phrase alignment. 
Of all the NEs in the training and develop-
ment sets, the transliteration-based alignment 
process was able to establish alignments of 
4,711 single-word NEs, 4,669 two-word NEs 
and 1,745 NEs having length more than two. 
It is to be noted that, some of the single-word 
NE alignments, as well as two-word NE 
alignments, result from multi-word NE align-
ment. 
We analyzed the output of the NE align-
ment module and observed that longer NEs 
were aligned better than the shorter ones, 
which is quite intuitive, as longer NEs have 
more tokens to be considered for intra-NE 
alignment. Since the NE alignment process is 
based on transliteration, the alignment method 
does not work where NEs involve translation 
or acronyms. We also observed that English 
multi-word NEs are sometimes fused together 
into single-word NEs. 
We performed three sets of experiments: 
treating compound verbs as single tokens, 
treating NEs as single tokens, and the combi-
nation thereof. Again for NEs, we carried out 
three types of preprocessing: single-
tokenization of (i) two-word NEs, (ii) more 
than two-word NEs, and (iii) NEs of any 
length. We make distinctions among these 
three to see their relative effects. The devel-
opment and test sets, as well as the target lan-
guage monolingual corpus (for language mod-
eling), are also subjected to the same preproc-
essing of single-tokenizing the MWEs. For 
NE alignment, we performed experiments us-
ing 4 different settings: alignment of (i) NEs 
of length up to two, (ii) NEs of length two, 
51
(iii) NEs of length greater than two, and (iv) 
NEs of any length. Before evaluation, the sin-
gle-token (target language) underscored 
MWEs are expanded back to words compris-
ing the MWEs. 
Since we did not have the gold-standard 
word alignment, we could not perform intrin-
sic evaluation of the word alignment. Instead 
we carry out extrinsic evaluation on the MT 
quality using the well known automatic MT 
evaluation metrics: BLEU (Papineni et al, 
2002), METEOR (Banerjee and Lavie, 2005), 
NIST (Doddington, 2002), WER, PER and 
TER (Snover et al, 2006). As can be seen 
from the evaluation results reported in Table 
2, baseline Moses without any preprocessing 
of the dataset produces a BLEU score of 8.74. 
The low score can be attributed to the fact that 
Bangla, a morphologically rich language, is 
hard to translate into. Moreover, Bangla being 
a relatively free phrase order language (Ekbal 
and Bandyopadhyay, 2009) ideally requires 
multiple set of references for proper evalua-
tion. Hence using a single reference set does 
not justify evaluating translations in Bangla. 
Also the training set was not sufficiently large 
enough for SMT. Treating only longer than 2-
word NEs as single tokens does not help im-
prove the overall performance much, while 
single tokenization  of two-word NEs as single 
tokens produces some improvements (.39 
BLEU points absolute, 4.5% relative). Con-
sidering compound verbs as single tokens 
(CVaST) produces a .82 BLEU point im-
provement (9.4% relative) over the baseline. 
Strangely, when both compound verbs and 
NEs together are counted as single tokens, 
there is hardly any improvement. By contrast, 
automatic NE alignment  (NEA) gives a huge 
impetus to system performance, the best of 
them (4.59 BLEU points absolute, 52.5% rela-
tive improvement) being the alignment of NEs 
of any length that produces the best scores 
across all metrics. When NEA is combined 
with CVaST, the improvements are substan-
tial, but it can not beat the individual im-
provement on NEA. The (?) marked systems 
produce statistically significant improvements 
as measured by bootstrap resampling method 
(Koehn, 2004) on BLEU over the baseline 
system. Metric-wise individual best scores are 
shown in bold in Table 2. 
5 Conclusions and Future Work 
In this paper, we have successfully shown 
how the simple yet effective preprocessing of 
treating two types of MWEs, namely NEs and 
compound verbs, as single-tokens, in conjunc-
tion with prior NE alignment can boost the 
performance of PB-SMT system on an Eng-
lish?Bangla translation task. Treating com-
pound verbs as single-tokens provides signifi-
cant gains over the baseline PB-SMT system. 
Amongst the MWEs, NEs perhaps play the 
most important role in MT, as we have clearly 
demonstrated through experiments that auto-
matic alignment of NEs by means of translit-
eration improves the overall MT performance 
substantially across all automatic MT evalua-
tion metrics. Our best system yields 4.59 
BLEU points improvement over the baseline, 
a 52.5% relative increase. We compared a 
subset of the output of our best system with 
that of the baseline system, and the output of 
our best system almost always looks better in 
terms of either lexical choice or word order-
ing. The fact that only 28.5% of the testset 
NEs appear in the training set, yet prior auto-
matic alignment of the NEs brings about so 
much improvement in terms of MT quality, 
suggests that it not only improves the NE 
alignment quality in the phrase table, but word 
alignment and phrase alignment quality must 
have also been improved significantly. At the 
same time, single-tokenization of MWEs 
makes the dataset sparser, but yet improves 
the quality of MT output to some extent. Data-
driven approaches to MT, specifically for 
scarce-resource language pairs for which very 
little parallel texts are available, should benefit 
from these preprocessing methods. Data 
sparseness is perhaps the reason why single-
tokenization of NEs and compound verbs, 
both individually and in collaboration, did not 
add significantly to the scores. However, a 
significantly large parallel corpus can take 
care of the data sparseness problem introduced 
by the single-tokenization of MWEs. 
The present work offers several avenues for 
further work. In future, we will investigate 
how these automatically aligned NEs can be 
52
used as anchor words to directly influence the 
word alignment process. We will look into 
whether similar kinds of improvements can be 
achieved for larger datasets, corpora from dif-
ferent domains and for other language pairs. 
We will also investigate how NE alignment 
quality can be improved, especially where 
NEs involve translation and acronyms. We 
will also try to perform morphological analy-
sis or stemming on the Bangla side before NE 
alignment. We will also explore whether dis-
criminative approaches to word alignment can 
be employed to improve the precision of the 
NE alignment. 
Acknowledgements 
This research is partially supported by the Sci-
ence Foundation Ireland (Grant 07/CE/I1142) 
as part of the Centre for Next Generation Lo-
calisation (www.cngl.ie) at Dublin City Uni-
versity, and EU projects PANACEA (Grant 
7FP-ITC-248064) and META-NET (Grant 
FP7-ICT-249119). 
References 
Banerjee, Satanjeev, and Alon Lavie. 2005. An 
Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In 
proceedings of the ACL-2005 Workshop on In-
trinsic and Extrinsic Evaluation Measures for 
MT and/or Summarization, pp. 65-72. Ann Ar-
bor, Michigan., pp. 65-72. 
Brown, Peter F., Stephen A. Della Pietra, Vincent 
J. Della Pietra, and Robert L. Mercer. 1993. The 
mathematics of statistical machine translation: 
parameter estimation. Computational Linguis-
tics, 19(2):263-311. 
Carpuat, Marine, and Mona Diab. 2010. Task-
based Evaluation of Multiword Expressions: a 
Pilot Study in Statistical Machine Translation. 
In Proceedings of Human Language Technology 
conference and the North American Chapter of 
the Association for Computational Linguistics 
conference (HLT-NAACL 2010), Los Angeles, 
CA, pp. 242-245. 
Chakrabarti, Debasri, Hemang Mandalia, Ritwik 
Priya, Vaijayanthi Sarma, and Pushpak Bhat-
tacharyya. 2008. Hindi compound verbs and 
their automatic extraction. In Proceedings 
of  the 22nd International Conference on Com-
putational Linguistics (Coling 2008), Posters 
and demonstrations, Manchester, UK, pp. 27-
30. 
Dempster, A.P., N.M. Laird, and D.B. Rubin. 
1977). Maximum Likelihood from Incomplete 
Data via the EM Algorithm. Journal of the 
Royal Statistical Society, Series B (Methodo-
logical) 39 (1): 1?38. 
Doddington, George. 2002. Automatic evaluation 
of machine translation quality using n-gram 
cooccurrence statistics. In Proceedings of the 
Second International Conference on Human 
Language Technology Research (HLT-2002), 
San Diego, CA, pp. 128-132. 
Eck, Matthias, Stephan Vogel, and Alex Waibel. 
2004. Improving statistical machine translation 
in the medical domain using the Unified Medi-
cal Language System. In Proceedings of  the 
20th International Conference on Computational 
Linguistics (COLING 2004), Ge-
neva, Switzerland, pp. 792-798. 
Ekbal, Asif, and Sivaji Bandyopadhyay. 2009. 
Voted NER system using appropriate unlabeled 
data. In proceedings of the ACL-IJCNLP-2009 
Named Entities Workshop (NEWS 2009), 
Suntec, Singapore, pp. 202-210. 
Ekbal, Asif, and Sivaji Bandyopadhyay. 2008. 
Maximum Entropy Approach for Named Entity 
Recognition in Indian Languages. International 
Journal for Computer Processing of Lan-
guages (IJCPOL), Vol. 21(3):205-237. 
Feng, Donghui, Yajuan Lv, and Ming Zhou. 2004. 
A new approach for English-Chinese named en-
tity alignment. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2004), Barcelona, 
Spain, pp. 372-379. 
Huang, Fei, Stephan Vogel, and Alex Waibel. 
2003. Automatic extraction of named entity 
translingual equivalence based on multi-feature 
cost minimization. In Proceedings of the ACL-
2003 Workshop on Multilingual and Mixed-
language Named Entity Recognition, 2003, 
Sapporo, Japan, pp. 9-16. 
Kneser, Reinhard, and Hermann Ney. 1995. Im-
proved backing-off for m-gram language model-
ing. In Proceedings of the IEEE Internation 
Conference on Acoustics, Speech, and Signal 
Processing (ICASSP), vol. 1, pp. 181-184. De-
troit, MI. 
Koehn, Philipp, Franz Josef Och, and Daniel 
Marcu. 2003. Statistical phrase-based transla-
tion. In Proceedings of HLT-NAACL 2003: 
53
conference combining Human Language Tech-
nology conference series and the North Ameri-
can Chapter of the Association for Computa-
tional Linguistics conference series,  Edmonton, 
Canada, pp. 48-54. 
Koehn, Philipp, Hieu Hoang, Alexandra Birch, 
Chris Callison-Burch, Marcello Federico, Ni-
cola Bertoldi, Brooke Cowan, Wade Shen, 
Christine Moran, Richard Zens, Chris Dyer, 
Ond?ej Bojar, Alexandra Constantin, and Evan 
Herbst. 2007. Moses: open source toolkit for 
statistical machine translation. In Proceedings of 
the 45th Annual meeting of the Association for 
Computational Linguistics (ACL 2007): Pro-
ceedings of demo and poster sessions, Prague, 
Czech Republic, pp. 177-180. 
Koehn, Philipp. 2004. Statistical significance tests 
for machine translation evaluation. In  EMNLP-
2004: Proceedings of the 2004 Conference on 
Empirical Methods in Natural Language Proc-
essing, 25-26 July 2004, Barcelona, Spain, pp. 
388-395. 
Marcu, Daniel. 2001. Towards a Unified Approach 
to Memory- and Statistical-Based Machine 
Translation. In Proceedings of the 39th Annual 
Meeting of the Association for Computational 
Linguistics (ACL 2001), Toulouse, France, pp. 
386-393. 
Moore, Robert C. 2003. Learning translations of 
named-entity phrases from parallel corpora. In 
Proceedings of 10th Conference of the Euro-
pean Chapter of the Association for Computa-
tional Linguistics (EACL 2003), Budapest, 
Hungary; pp. 259-266. 
Och, Franz J. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of 
the 41st Annual Meeting of the Association for 
Computational Linguistics (ACL-2003), Sap-
poro, Japan, pp. 160-167. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for 
automatic evaluation of machine translation. In 
Proceedings of the 40th Annual Meeting of the 
Association for Computational Linguistics 
(ACL-2002), Philadelphia, PA, pp. 311-318. 
Ren, Zhixiang, Yajuan L?, Jie Cao, Qun Liu, and 
Yun Huang. 2009. Improving statistical ma-
chine translation using domain bilingual multi-
word expressions. In Proceedings of the 2009 
Workshop on Multiword Expressions, ACL-
IJCNLP 2009, Suntec, Singapore, pp. 47-54. 
Sag, Ivan A., Timothy Baldwin, Francis Bond, 
Ann Copestake and Dan Flickinger. 2002. Mul-
tiword expressions: A pain in the neck for NLP. 
In Proceedings of the 3rd International Confer-
ence on Intelligent Text Processing and Compu-
tational Linguistics (CICLing-2002), Mexico 
City, Mexico, pp. 1-15. 
Snover, Matthew, Bonnie Dorr, Richard Schwartz, 
Linnea Micciulla, and John Makhoul. 2006. A 
study of translation edit rate with targeted hu-
man annotation. In Proceedings of the 7th Con-
ference of the Association for Machine Transla-
tion in the Americas (AMTA 2006), Cambridge, 
MA, pp. 223-231. 
Vogel, Stephan, Hermann Ney, and Christoph 
Tillmann. 1996. HMM-based word alignment in 
statistical translation. In Proceedings of the 16th 
International Conference on Computational 
Linguistics (COLING 1996), Copenhagen, pp. 
836-841. 
Venkatapathy, Sriram, and Aravind K. Joshi. 2006. 
Using information about multi-word expres-
sions for the word-alignment task. In Proceed-
ings of Coling-ACL 2006: Workshop on Multi-
word Expressions: Identifying and Exploiting 
Underlying Properties, Sydney, pp. 20-27. 
Wu, Hua Haifeng Wang, and Chengqing Zong. 
2008. Domain adaptation for statistical machine 
translation with domain dictionary and mono-
lingual corpora. In Proceedings of the 22nd In-
ternational Conference on Computational Lin-
guistics (COLING 2008),  Manchester, UK, pp. 
993-1000. 
54
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 94?101,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
A Hybrid Word Alignment Model for Phrase-Based Statistical Ma-
chine Translation 
 
 
Santanu Pal*, Sudip Kumar Naskar? and Sivaji Bandyopadhyay* 
*Department of Computer Science & Engineering 
Jadavpur University, Kolkata, India 
santanu.pal.ju@gmail.com, sivaji_cse_ju@yahoo.com 
?
 Department of Computer & System Sciences 
Visva-Bharati University, Santiniketan, India 
sudip.naskar@gmail.com 
 
  
 
Abstract 
This paper proposes a hybrid word alignment 
model for Phrase-Based Statistical Machine 
translation (PB-SMT). The proposed hybrid 
alignment model provides most informative 
alignment links which are offered by both un-
supervised and semi-supervised word align-
ment models. Two unsupervised word align-
ment models (GIZA++ and Berkeley aligner) 
and a rule based aligner are combined togeth-
er. The rule based aligner only aligns named 
entities (NEs) and chunks. The NEs are 
aligned through transliteration using a joint 
source-channel model. Chunks are aligned 
employing a bootstrapping approach by trans-
lating the source chunks into the target lan-
guage using a baseline PB-SMT system and 
subsequently validating the target chunks us-
ing a fuzzy matching technique against the 
target corpus. All the experiments are carried 
out after single-tokenizing the multi-word 
NEs.  Our best system provided significant 
improvements over the baseline as measured 
by BLEU.  
1 Introduction 
Word alignment is the backbone of PB-SMT sys-
tem or any data driven approaches to Machine 
Translation (MT) and it has received a lot of at-
tention in the area of statistical machine transla-
tion (SMT) (Brown et al, 1993; Och and Ney, 
2003; Koehn et al, 2003). Word alignment is not 
an end task in itself and is usually used as an in-
termediate step in SMT. Word alignment is de-
fined as the detection of corresponding alignment 
of words from parallel sentences that are transla-
tion of each other. Statistical machine translation 
usually suffers from many-to-many word links 
which existing statistical word alignment algo-
rithms can not handle well.  
The unsupervised word alignment models are 
based on IBM models 1?5 (Brown et al, 1993) 
and the HMM model (Ney and Vogel, 1996; Och 
and Ney, 2003). Models 3, 4 and 5 are based on 
fertility based models which are asymmetric. To 
improve alignment quality, the Berkeley Aligner 
is based on the symmetric property by intersect-
ing alignments induced in each translation direc-
tion. 
In the present work, we propose improvement 
of word alignment quality by combining three 
word alignment tables (i) GIZA++ alignment (ii) 
Berkeley Alignment and (iii) rule based align-
ment. Our objective is to perceive the effective-
ness of the Hybrid model in word alignment by 
improving the quality of translation in the SMT 
system. In the present work, we have implement-
ed a rule based alignment model by considering 
several types of chunks which are automatically 
extracted on the source side. Each individual 
source chunk is translated using a baseline PB-
SMT system and validated with the target chunks 
on the target side. The validated source-target 
chunks are added in the rule based alignment 
table. Work has been carried out into three direc-
tions: (i) three alignment tables are combined 
together by taking their union; (ii) extra align-
ment pairs are added into the alignment table. 
This is a well-known practice in domain adapta-
tion in SMT (Eck et al, 2004; Wu et al, 2008); 
(iii) the alignment table is updated through semi-
supervised alignment technique. 
94
The remainder of the paper is organized as fol-
lows. Section 2 discusses related work. The pro-
posed hybrid word alignment model is described 
in Section 3. Section 4 presents the tools and re-
sources used for the various experiments. Section 
5 includes the results obtained, together with 
some analysis. Section 6 concludes and provides 
avenues for further work. 
2 Related Works  
Zhou et al (2004) proposed a multi lingual filter-
ing algorithm that generates bilingual chunk 
alignment from Chinese-English parallel corpus. 
The algorithm has three steps, first, from the par-
allel corpus; the most frequent bilingual chunks 
are extracted. Secondly, the participating chunks 
for alignments are combined into a cluster and 
finally one English chunk is generated corre-
sponding to a Chinese chunk by analyzing the 
highest co-occurrences of English chunks. Bilin-
gual knowledge can be extracted using chunk 
alignment (Zhou et. al., 2004). Pal et, al. (2012) 
proposed a bootstrapping method for chunk 
alignment; they used an SMT based model for 
chunk translation and then aligned the source-
target chunk pairs after validating the translated 
chunk. Ma et. al. (2007) simplified the task of 
automatic word alignment as several consecutive 
words together correspond to a single word in the 
opposite language by using the word aligner it-
self, i.e., by bootstrapping on its output. A Max-
imum Entropy model based approach for Eng-
lish?Chinese NE alignment which significantly 
outperforms IBM Model4 and HMM has been 
proposed by Feng et al (2004). They considered 
4 features: translation score, transliteration score, 
source NE and target NE's co-occurrence score 
and the distortion score for distinguishing identi-
cal NEs in the same sentence. Moore (2003) pre-
sented an approach where capitalization cues 
have been used for identifying NEs on the Eng-
lish side. Statistical techniques are applied to de-
cide which portion of the target language corre-
sponds to the specified English NE, for simulta-
neous NE identification and translation. 
To improve the learning process of unlabeled 
data using labeled data (Chapelle et al, 2006), 
the semi-supervised learning method is the most 
useful learning technique. Semi-supervised 
learning is a broader area of Machine Learning. 
Researchers have begun to explore semi-
supervised word alignment models that use both 
labeled and unlabeled data. Fraser and Marcu 
(2006) proposed a semi-supervised training algo-
rithm. The weighting parameters are learned 
from discriminative error training on labeled da-
ta, and the parameters are estimated by maxi-
mum-likelihood EM training on unlabeled data. 
They have also used a log-linear model which is 
trained on the available labeled data to improve 
performance. Interpolating human alignments 
with automatic alignments has been proposed by 
Callison-Burch et al (2004), where the align-
ments of higher quality have gained much higher 
weight than the lower-quality alignments. Wu et 
al. (2006) have developed two separate models 
of standard EM algorithm which learn separately 
from both labeled and unlabeled data. Two mod-
els are then interpolated as a learner in the semi-
supervised Ada-Boost algorithm to improve 
word alignment. Ambati et al (2010) proposed 
active learning query strategies to identify highly 
uncertain or most informative alignment links 
under an unsupervised word alignment model. 
Intuitively, multiword NEs on the source and 
the target sides should be both aligned in the par-
allel corpus and translated as a whole. However, 
in the state-of-the-art PB-SMT systems, the con-
stituents of multiword NE are marked and 
aligned as parts of consecutive phrases, since 
PB-SMT (or any other approaches to SMT) does 
not generally treat multiword NEs as special to-
kens. This is the motivations behind considering 
NEs for special treatment in this work by con-
verting into single tokens that makes sure that 
PB-SMT also treats them as a whole 
Another problem with SMT systems is the er-
roneous word alignment. Sometimes some words 
are not translated in the SMT output sentence 
because of the mapping to NULL token or erro-
neous mapping during word alignment. Verb 
phrase translation also creates major problems. 
The words inside verb phrases are generally not 
aligned one-to-one; the alignments of the words 
inside source and target verb phrases are mostly 
many-to-many particularly so for the English?
Bengali language pair.  
The first objective of the present work is to see 
how single tokenization and alignment of NEs on 
both the sides affects the overall MT quality. The 
second objective is to see whether Hybrid word 
alignment model of both unsupervised and semi-
supervised techniques enhance the quality of 
translation in the SMT system rather than the 
single tokenized NE level parallel corpus applied 
to the hybrid model.  
We carried out the experiments on English?
Bengali translation task. Bengali shows high 
morphological richness at lexical level. Lan-
95
guage resources in Bengali are not widely avail-
able. 
3 Hybrid Word Alignment Model 
The hybrid word alignment model is described as 
the combination of three word alignment models 
as follows: 
3.1 Word Alignment Using GIZA++ 
GIZA++ (Och and Ney, 2003) is a statistical 
word alignment tool which incorporates all the 
IBM 1-5 models. GIZA++ facilitates fast devel-
opment of statistical machine translation (SMT) 
systems. In case of low-resource language pairs 
the quality of word alignments is typically quite 
low and it also deviates from the independence 
assumptions made by the generative models. 
Although huge amount of parallel data enables 
the model parameters to acquire better estimation, 
a large number of language pairs still lacks from 
the unavailability of sizeable amount of parallel 
data. GIZA++ has some draw-backs. It allows at 
most one source word to be aligned with each 
foreign word. To resolve this issue, some tech-
niques have already been applied such as: the 
parallel corpus is aligned bidirectionally; then the 
two alignment tables are reconciled using differ-
ent heuristics e.g., intersection, union, and most 
recently grow-diagonal-final and grow-diagonal-
final-and heuristics have been applied. In spite of 
these heuristics, the word alignment quality for 
low-resource language pairs is still low and calls 
for further improvement. We describe our ap-
proach of improving word alignment quality in 
the following three subsections. 
3.2 Word Alignment Using Berkley Aligner 
The recent advancements in word alignment is 
implemented in Berkeley Aligner (Liang et al, 
2006) which allows both unsupervised and su-
pervised approach to align word from parallel 
corpus. We initially train the parallel corpus us-
ing unsupervised technique. We make a few 
manual corrections to the alignment table pro-
duced by the unsupervised aligner. Then we ap-
ply this corrected alignment table as gold stand-
ard training data for the supervised aligner. The 
Berkeley aligner is an extension of the Cross Ex-
pectation Maximization word aligner. Berkeley 
aligner is a very useful word aligner because it 
allows for supervised training, enabling us to 
derive knowledge from already aligned parallel 
corpus or we can use the same corpus by updat-
ing the alignments using some rule based meth-
ods. Our approach deals with the latter case. The 
supervised technique of Berkeley aligner helps 
us to align those words which could not be 
aligned by rule based word aligner.  
3.3 Rule Based Word Alignment 
The proposed Rule based aligner aligns Named 
Entities (NEs) and chunks. For NE alignment, 
we first identify NEs from the source side (i.e. 
English) using Stanford NER.  The NEs on the 
target side (i.e. Bengali) are identified using a 
method described in (Ekbal and Bandyopadhyay, 
2009). The accuracy of the Bengali Named Enti-
ty recognizers (NER) is much poorer compared 
to that of English NER due to several reasons: (i) 
there is no capitalization cue for NEs in Bengali; 
(ii) most of the common nouns in Bengali are 
frequently used as proper nouns; (iii) suffixes 
(case markers, plural markers, emphasizers, 
specifiers) get attached to proper names as well 
in Bengali. Bengali shallow parser 1  has been 
used to improve the performance of NE identifi-
cation by considering proper names as NE.  
Therefore, NER and shallow parser are jointly 
employed to detect NEs from the Bengali sen-
tences. The source NEs are then transliterated 
using a modified joint source-channel model 
(Ekbal et al, 2006) and aligned to their target 
side equivalents following the approach of Pal et 
al. (2010). The target side equivalents NEs are 
transformed into canonical form after omitting 
their ?matras?. Similarly Bengali NEs are also 
transformed into canonical forms as Bengali NEs 
may differ in their choice of matras (vowel mod-
ifiers). The transliterated NEs are then matched 
with the corresponding parallel target NEs and 
finally we align the NEs if match is found.   
After identification of multiword NEs on both 
sides, we pre-processed the corpus by replacing 
space with the underscore character (?_?). We 
have used underscore (?_?) instead of hyphen (?-
?) since there already exists some hyphenated 
words in the corpus.  The use of the underscore 
(?_?) character also facilitates to de-tokenize the 
single-tokenized NEs after decoding. 
For chunk alignment, the source sentences of 
the parallel corpus are parsed using Stanford 
POS tagger. The chunks of the sentences are ex-
tracted using CRF chunker2. The chunker detects 
the boundaries of noun, verb, adjective, adverb 
                                                 
1 
http://ltrc.iiit.ac.in/showfile.php?filename=downloads/shallo
w_parser.php 
2 http://crfchunker.sourceforge.net/ 
96
and prepositional chunks from the sentences. In 
case of prepositional phrase chunks, we have 
taken a special attention: we have expanded the 
prepositional phrase chunk by examining a single 
noun chunk followed by a preposition or a series 
of noun chunks separated by conjunctions such 
as 'comma', 'and' etc.  For each individual chunk, 
the head word is identified. Similarly target side 
sentences are parsed using a shallow parser. The 
individual target side Bengali chunks are extract-
ed from the parsed sentences.  The head words 
for all individual chunks on the target side are 
also marked. If the translated head word of a 
source chunk matches with the headword of a 
target chunk then we hypothesize that these two 
chunks are translations of each other.  
The extracted source chunks are translated us-
ing a baseline SMT model trained on the same 
corpus. The translated chunks are validated 
against the target chunks found in the corre-
sponding target sentence. During the validation 
process, if any match is found between the trans-
lated chunk and a target chunk then the source 
chunk is directly aligned with the original target 
chunk. Otherwise, the source chunk is ignored in 
the current iteration for any possible alignment 
and is considered in the next iterations. 
 
 
 
 
 
 
Figure 1.a: Rule based alignments 
 
 
 
 
 
 
Figure 1.b: Gold standard alignments 
 
Figure 1: Establishing alignments through Rule 
based methods. 
 
The extracted chunks on the source side may 
not have a one to one correspondence with the 
target side chunks. The alignment validation pro-
cess is focused on the proper identification of the 
head words and not between the translated 
source chunk and target chunk. The matching 
process has been carried out using a fuzzy 
matching technique. If both sides contain only 
one chunk after aligning the remaining chunks 
then the alignment is trivial. After aligning the 
individual chunks, we also establish word align-
ments between the matching words in those 
aligned chunks. Thus we get a sentence level 
source-target word alignment table.  
Figure 1 shows how word alignments are es-
tablished between a source-target sentence pair 
using the rule based method. Figure 1.a shows 
the alignments obtained through rule based 
method. The solid links are established through 
transliteration (for NEs) and translation. The dot-
ted arrows are also probable candidates for intra-
chunk word alignments; however they are not 
considered in the present work. Figure 1.b shows 
the gold standard alignments for this sentence 
pair.  
3.4  Hybrid Word alignment Model  
The hybrid word alignment method combines 
three different kinds of word alignments ? Gi-
za++ word alignment with grow-diag-final-and 
(GDFA) heuristic, Berkeley aligner and rule 
based aligner. We have followed two different 
strategies to combine the three different word 
alignment tables.  
 
Union 
In the union method all the alignment tables are 
united together and duplicate entries are removed. 
 
ADD additional Alignments  
In this method we consider either of the align-
ments generated by GIZA++ GDFA (A1) or 
Berkeley aligner (A2) as the standard alignment 
as the rule based aligner fails to align all words 
in the parallel sentences. From the three set of 
alignments A1, A2 and A3, we propose an 
alignment combination method as described in 
algorithm 1. 
 
ALGORITHM: 1 
 
Step 1: Choose either A1 or A2 as the standard 
alignment (SA). 
Step 2: Correct the alignments in SA using the 
alignment table of A3. 
Step 3: if A2 is considered as SA then find addi-
tional alignment from A1 and A3 using intersec-
tion method (A1?A3) otherwise find additional 
alignment from A2 and A3 (using A2?A3).   
Step 4: Add additional entries with SA. 
[Jaipur] [golapi sohor name] [porichito] [.] 
[Jaipur] [is known] [as [Pink City]] [.] 
 
[Jaipur] [golapi sohor name] [porichito] [.] 
[Jaipur] [is known] [as [Pink City]] [.] 
 
97
3.5 Berkeley Semi-supervised Alignment 
The correctness of the alignments is verified by 
manually checking the performance of the vari-
ous alignment system. We start with the com-
bined alignment table which is produced by Al-
gorithm 1. Iinitially, we take a subset of the 
alignments by manually inspecting from the 
combined alignment table. Then we train the 
Barkley supervised aligner with this labeled data. 
A subset of the unlabeled data from the com-
bined alignment table is tested with the super-
vised model. The output is then added as addi-
tional labeled training data for the supervised 
training method for the next iteration. Using this 
bootstrapping approach, the amount of labeled 
training data for the supervised aligner is gradu-
ally increased. The process is continued until 
there are no more unlabelled training data. In this 
way we tune the whole alignment table for the 
entire parallel corpus. The process is carried out 
in a semi-supervised manner. 
4 Tools and resources Used  
A sentence-aligned English-Bengali parallel cor-
pus containing 23,492 parallel sentences from 
the travel and tourism domain has been used in 
the present work. The corpus has been collected 
from the consortium-mode project ?Development 
of English to Indian Languages Machine Trans-
lation (EILMT) System - Phase II? 3. The Stan-
ford Parser4 and CRF chunker5 have been used 
for identifying chunks and Stanford NER has 
been used to identify named entities in the source 
side of the parallel corpus.  
The target side (Bengali) sentences are parsed 
by using the tools obtained from the consortium 
mode project ?Development of Indian Language 
to Indian Language Machine Translation (IL-
ILMT) System - Phase II6?. 
The effectiveness of the present work has been 
tested by using the standard log-linear PB-SMT 
model as our baseline system: phrase-extraction 
heuristics described in (Koehn et al, 2003), , 
MERT (minimum-error-rate training) (Och, 
2003) on a held-out development set, target 
                                                 
3  The EILMT project is funded by the Department of Elec-
tronics and Information Technology (DEITY), Ministry of 
Communications and Information Technology (MCIT), 
Government of India. 
4 http://nlp.stanford.edu/software/lex-parser.shtml 
5 http://crfchunker.sourceforge.net/ 
6   The IL-ILMT project is funded by the Department of 
Electronics and Information Technology (DEITY), Ministry 
of Communications and Information Technology (MCIT), 
Government of India. 
language model trained using SRILM toolkit 
(Stolcke, 2002) with Kneser-Ney smoothing 
(Kneser and Ney, 1995) and the Moses decoder 
(Koehn et al, 2007) have been used in the 
present study. 
5 Experiments and Results 
We have randomly selected 500 sentences each 
for the development set and the test set from the 
initial parallel corpus. The rest are considered as 
the training corpus. The training corpus was fil-
tered with the maximum allowable sentence 
length of 100 words and sentence length ratio of 
1:2 (either way). Finally the training corpus con-
tained 22,492 sentences. In addition to the target 
side of the parallel corpus, a monolingual Benga-
li corpus containing 488,026 words from the 
tourism domain was used for building the target 
language model. We experimented with different 
n-gram settings for the language model and the 
maximum phrase length and found that a 4-gram 
language model and a maximum phrase length of 
7 produced the optimum baseline result. We car-
ried out the rest of the experiments using these 
settings. 
We experimented with the system  over 
various combinations of word alignment models. 
Our hypothesis focuses mainly on the theme that 
proper alignment of words will result in 
improvement of the system performance in terms 
of translation quality.  
141,821 chunks were identified from the 
source corpus, of which 96,438 (68%) chunks 
were aligned by the system. 39,931 and 28,107 
NEs were identified from the source and target 
sides of the parallel corpus respectively, of which 
22,273 NEs are unique in English and 22,010 
NEs in Bengali. A total of 14,023 NEs have been 
aligned through transliteration.  
The experiments have been carried out with 
various experimental settings: (i) single 
tokenization of NEs on both sides of the parallel 
corpus, (ii) using Berkeley Aligner with 
unsupervised training, (iii) union of the three 
alignment models: rule based, GIZA++ with 
GDFA and Berkeley Alignment, (iv) 
hybridization of the three alignment models and 
(v) supervised Berkeley Aligner. Eextrinsic 
evaluation was carried out on the MT quality 
using BLEU (Papineni et al, 2002) and NIST 
(Doddington, 2002). 
98
 
 
Experiment Exp 
no. 
BLEU NIST 
Baseline system using GIZA++ with GDFA 1 10.92 4.13 
PB-SMT system using Berkeley Aligner 2 11.42 4.16 
Union of all Alignments 3 11.12 4.14 
PB-SMT System with Hybrid Alignment by considering (a) 
GIZA++ as the standard alignment) (b) Berkeley alignment 
as the standard alignment) 
4a? 15.38 4.30 
4b? 15.92 4.36 
Single tokenized NE + Exp 1 5 11.68 4.17 
Single tokenized NE + Exp 2 6 11.82 4.19 
Single tokenized NE + (a) Exp 4a (b) Exp 4b 7a? 16.58 4.45 
7b? 17.12 4.49 
PB-SMT System with semi-supervised Berkeley Aligner + 
Single tokenized NE 
8? 20.87 4.71 
 
Table: 1 Evaluation results for different experimental setups. (The ??? marked systems produce statis-
tically significant improvements on BLEU over the baseline system) 
 
 
The baseline system (Exp 1) is the state-of-art 
PB-SMT system where GIZA++ with grow-diag-
final-and has been used as the word alignment 
model. Experiment 2 provides better results than 
experiment 1 which signifies that Berkeley 
Aligner performs better than GIZA++ for the 
English-Bengali translation task. The union of all 
thee alignments (Exp 3) provides better scores 
than the baseline; however it cannot beat the re-
sults obtained with the Berkeley Aligner alone. 
Hybrid alignment model with GIZA++ as the  
standard alignment (Exp 4a) produces statistical-
ly significant improvements over the baseline. 
Similarly the use of Berkeley Aligner as the 
standard alignment for hybrid alignment model 
(Exp 4b) also results in statistically significant 
improvements over Exp 2. These two experi-
ments (Exp 4a and 4b) demonstrate the effec-
tiveness of the hybrid alignment model. It is to 
be noticed that hybrid alignment model works 
better with the Berkeley Aligner than with 
GIZA++. 
Single-tokenization of the NEs (Exp 5, 6, 7a 
and 7b) improves the system performance to 
some extent over the corresponding experiments 
without single-tokenization (Exp 1, 2, 4a and 
4b); however, these improvements are not statis-
tically significant. The Berkeley semi-supervised 
alignment method using a bootstrapping ap-
proach together with single-tokenization of NEs 
provided the overall best performance in terms of 
both BLEU and NIST and the corresponding im-
provement is statistically significant on BLEU 
over rest of the experiments. 
6 Conclusion and Future Work 
The paper proposes a hybrid word alignment 
model for PB-SMT. The paper also shows how 
effective pre-processing of NEs in the parallel 
corpus and direct incorporation of their align-
ment in the word alignment model can improve 
SMT system performance. In data driven ap-
proaches to MT, specifically for scarce resource 
data, this approach can help to upgrade the state-
of-art machine translation quality as well as the 
word alignment quality. . The hybrid model with 
the use of the semi-supervised technique of the 
Berkeley word aligner in a bootstrapping manner, 
together with single tokenization of NEs, pro-
vides substantial improvements (9.95 BLEU 
points absolute, 91.1% relative) over the base-
line. On manual inspection of the output we 
found that our best system provides more accu-
99
rate lexical choice as well as better word order-
ing than the baseline system.  
As future work we would like to explore how 
to get the best out of multiple word alignments. 
Furthermore, integrating the knowledge about 
multi-word expressions into the word alignment 
models is another future direction for this work. 
 
Acknowledgement 
 
The work has been carried out with support from 
the project ?Development of English to Indian 
Languages Machine Translation (EILMT) Sys-
tem - Phase II? funded by Department of Infor-
mation Technology, Government of India. 
References  
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment. 
In ACL-44: Proceedings of the 21st International 
Conference on Computational Linguistics and the 
44th annual meeting of the Association for Compu-
tational Linguistics (ACL-2006), Morristown, NJ, 
USA. pages 769?776. 
Brown, Peter F., Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
mathematics of statistical machine translation: pa-
rameter estimation. Computational Linguistics, 
19(2):263-311. 
Chris Callison-Burch, David Talbot, and Miles Os-
borne. 2004. Statistical machine translation with 
word- and sentence-aligned parallel corpora. In 
ACL 2004, page 175, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics. 
Dempster, A.P., N.M. Laird, and D.B. Rubin. 1977). 
Maximum Likelihood from Incomplete Data via 
the EM Algorithm. Journal of the Royal Statistical 
Society, Series B (Methodological) 39 (1): 1?38. 
Doddington, George. 2002. Automatic evaluation of 
machine translation quality using n-gram cooccur-
rence statistics. In Proceedings of the Second In-
ternational Conference on Human Language Tech-
nology Research (HLT-2002), San Diego, CA, pp. 
128-132. 
Eck, Matthias, Stephan Vogel, and Alex Waibel. 
2004. Improving statistical machine translation in 
the medical domain using the Unified Medical 
Language System. In Proc. of the 20th Internation-
al Conference on Computational Linguistics (COL-
ING 2004), Geneva, Switzerland, pp. 792-798. 
Ekbal, Asif, and Sivaji Bandyopadhyay. 2008. Maxi-
mum Entropy Approach for Named Entity Recog-
nition in Indian Languages. International Journal 
for Computer Processing of Languages (IJCPOL), 
Vol. 21 (3), 205-237. 
Ekbal, Asif, and Sivaji Bandyopadhyay. 2009. Voted 
NER system using appropriate unlabeled data. In 
proceedings of the ACL-IJCNLP-2009 Named En-
tities Workshop (NEWS 2009), Suntec, Singapore, 
pp.202-210. 
Feng, Donghui, Yajuan Lv, and Ming Zhou. 2004. A 
new approach for English-Chinese named entity 
alignment. In Proceedings of the 2004 Conference 
on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2004), Barcelona, Spain, pp. 
372-379. 
Feng, Donghui, Yajuan Lv, and Ming Zhou. 2004. A 
new approach for English-Chinese named entity 
alignment. In Proceedings of the 2004 Conference 
on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2004), Barcelona, Spain, pp. 
372-379. 
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment 
models. Computational Linguistics, pages 19?51. 
Huang, Fei, Stephan Vogel, and Alex Waibel. 2003. 
Automatic extraction of named entity translingual 
equivalence based on multi-feature cost minimiza-
tion. In Proceedings of the ACL-2003 Workshop 
on Multilingual and Mixed-language Named Entity 
Recognition, 2003, Sapporo, Japan, pp. 9-16. 
HuaWu, HaifengWang, and Zhanyi Liu. 2006. Boost-
ing statistical word alignment using labeled and un-
labeled data. In Proceedings of the COLING/ACL 
on Main conference poster sessions, pages 913?
920, Morristown, NJ, USA. Association for Com-
putational Linguistics.  
Kneser, Reinhard, and Hermann Ney. 1995. Improved 
backing-off for m-gram language modeling. In 
Proceedings of the IEEE Internation Conference on 
Acoustics, Speech, and Signal Processing 
(ICASSP), vol. 1, pp. 181?184. Detroit, MI. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003: conference com-
bining Human Language Technology conference 
series and the North American Chapter of the As-
sociation for Computational Linguistics conference 
series,  Edmonton, Canada, pp. 48-54. 
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Ber-
toldi, Brooke Cowan, Wade Shen, Christine Mo-
ran, Richard Zens, Chris Dyer, Ond?ej Bojar, Alex-
andra Constantin, and Evan Herbst. 2007. Moses: 
open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual meeting of 
the Association for Computational Linguistics 
(ACL 2007): Proceedings of demo and poster ses-
sions, Prague, Czech Republic, pp. 177-180. 
Koehn, Philipp. 2004. Statistical significance tests for 
machine translation evaluation. In  EMNLP-2004: 
100
Proceedings of the 2004 Conference on Empirical 
Methods in Natural Language Processing, 25-26 
July 2004, Barcelona, Spain, pp 388-395. 
O. Chapelle, B. Sch?olkopf, and A. Zien, editors. 
2006. Semi-Supervised Learning. MIT Press, 
Cambridge, MA. 
Och, Franz J. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of 
the 41st Annual Meeting of the Association for 
Computational Linguistics (ACL-2003), Sapporo, 
Japan, pp. 160-167. 
Pal, Santanu, Sivaji Bandyopadhyay. 2012, ?Boot-
strapping Chunk Alignment in Phrase-Based Sta-
tistical Machine Translation?, Joint Workshop on 
Exploiting Synergies between Information Retriev-
al and Machine Translation (ESIRMT) and Hybrid 
Approaches to Machine Translation (HyTra), 
EACL-2012, Avignon, France, pp. 93-100 . 
Pal, Santanu., Sudip Kumar Naskar, Pavel Pecina, 
Sivaji Bandyopadhyay and Andy Way. 2010, Han-
dling Named Entities and Compound Verbs in 
Phrase-Based Statistical Machine Translation, In 
proc. of the workshop on Multiword expression: 
from theory to application (MWE-2010), The 23rd 
International conference of computational linguis-
tics (Coling 2010),Beijing, Chaina, pp. 46-54. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. In Proceed-
ings of the 40th Annual Meeting of the Association 
for Computational Linguistics (ACL-2002), Phila-
delphia, PA, pp. 311-318. 
Percy Liang, Ben Taskar, Dan Klein. 2006.  6th Pro-
ceedings of the main conference on Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association of Computational 
Linguistics, HLT-NAACL-2006, Pages 104-111 
Stolcke, A. SRILM?An Extensible Language Mod-
eling Toolkit. Proc. Intl. Conf. on Spoken Lan-
guage Processing, vol. 2, pp. 901?904, Denver 
(2002). 
Vamshi Ambati, Stephan Vogel, Jaime Carbonell. 
2010, 10th Proceedings of the NAACL HLT 2010 
Workshop on Active Learning for Natural Lan-
guage Processing (ALNLP-2010), Pages 10-17. 
Vogel, Stephan, Hermann Ney, and Christoph Till-
mann. 1996. HMM-based word alignment in statis-
tical translation. In Proc. of the 16th International 
Conference on Computational Linguistics (COL-
ING 1996), Copenhagen, pp. 836-841. 
Wu, Hua Haifeng Wang, and Chengqing Zong. 2008. 
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proc. of the 22nd International Conference 
on Computational Linguistics (COLING 2008),  
Manchester, UK, pp. 993-1000. 
X. Zhu. 2005. Semi-Supervised Learning Literature 
Survey. Technical Report 1530, Computer Scienc-
es, University of Wisconsin-Madison. 
http://www.cs.wisc.edu/_jerryzhu/pub/ssl_survey.p
df. 
 
101
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 48?57,
Gothenburg, Sweden, April 27, 2014. c?2014 Association for Computational Linguistics
Automatic Building and Using Parallel Resources for SMT from 
Comparable Corpora 
Santanu Pal1, Partha Pakray2, Sudip Kumar Naskar3 
1Universit?t Des Saarlandes, Saarbr?cken, Germany 
2Computer & Information Science,  
Norwegian University of Science and Technology, Trondheim, Norway 
3Department of Computer Science & Engineering,  
Jadavpur University, Kolkata, India 
1santanu.pal@uni-saarland.de, 
2partha.pakray@idi.ntnu.no, 
3sudip.naskar@cse.jdvu.ac.in 
 
 
Abstract 
Building parallel resources for corpus 
based machine translation, especially 
Statistical Machine Translation (SMT), 
from comparable corpora has recently 
received wide attention in the field 
Machine Translation research. In this 
paper, we propose an automatic approach 
for extraction of parallel fragments from 
comparable corpora. The comparable 
corpora are collected from Wikipedia 
documents and this approach exploits the 
multilingualism of Wikipedia. The 
automatic alignment process of parallel 
text fragments uses a textual entailment 
technique and Phrase Based SMT (PB-
SMT) system.  The parallel text 
fragments extracted thus are used as 
additional parallel translation examples 
to complement the training data for a PB-
SMT system. The additional training data 
extracted from comparable corpora 
provided significant improvements in 
terms of translation quality over the 
baseline as measured by BLEU. 
1 Introduction 
Comparable corpora have recently attracted huge 
interest in natural language processing research. 
Comparable corpora are now considered as a rich 
resource for acquiring parallel resources such as 
parallel corpus or parallel text fragments,. 
Parallel text extracted from comparable corpora 
can take an important role in improving the 
quality of machine translation (MT) (Smith et al. 
2010).  Parallel text extracted from comparable 
corpora are typically added with the training 
corpus as additional training material which is 
expected to facilitate better performance of SMT 
systems specifically for low density language 
pairs. 
In the present work, we try to extract 
English?Bengali parallel fragments of text from 
comparable corpora. We have collected 
document aligned corpus of English?Bengali 
document pairs from Wikipedia which provides a 
huge collection of documents in many different 
languages. For automatic alignment of parallel 
fragments we have used two-way textual 
entailment (TE) system and a baseline SMT 
system.  
Textual entailment (TE), introduced by 
(Dagan and Glickman, 2004), is defined as a 
directional relationship between pairs of text 
expressions, denoted by the entailing text (T) and 
the entailed hypothesis (H). T entails H if the 
meaning of H can be inferred from the meaning 
of T. Textual Entailment has many applications 
in NLP tasks, such as summarization, 
information extraction, question answering, 
48
information retrieval, machine translation, etc. In 
machine translation, textual entailment can be 
applied to MT evaluation (Pado et al., 2009). A 
number of research works have been carried out 
on cross-lingual Textual entailment using MT 
(Mehdad et al.,2010; Negri et al., 2010; Neogi et 
al., 2012). However, to the best of our 
knowledge, the work presented here is the first 
attempt towards employing textual entailment for 
the purpose of extracting parallel text fragments 
from comparable corpora which in turn are used 
to improve MT system.  
Munteanu and Marcu (2006) suggested that 
comparable corpora tend to have parallel data at 
sub-sentential level. Hence, instead of finding 
sentence level parallel resource from comparable 
corpora, in the present work we mainly focus on 
finding parallel fragments of text. 
We carried out the task of automatic alignment 
of parallel fragments using three steps: (i) mining 
comparable corpora form Wikipedia, (ii) 
sentence level alignment using two-way TE and 
a baseline Bengali?English SMT system, and 
finally (iii) clustering the parallel sentence 
aligned comparable corpora using textual 
entailment and then aligning parallel fragments 
of text by textual entailment and a baseline 
Bengali?English SMT system.  
Although, we have collected document 
aligned comparable corpora, the documents in 
the corpus do not belong to any particular 
domain. Even with such a corpus we have been 
able to improve the performance of an existing 
machine translation system which was built on 
tourism domain data. This also signifies the 
contribution of this work towards domain 
adaptation of MT systems. 
The rest of the paper is organized as follows. 
Section 2 describes the related work. Section 3 
describes the mining process of the comparable 
corpora. The two-way TE system architecture is 
described in section 4. Section 5 describes the 
automatic alignment technique of parallel 
fragment of texts. Section 6 describes the tools 
and resources used for this work. The 
experiments and evaluation results are presented 
in section 7. Section 8 concludes and presents 
avenues for future work. 
2 Related Work  
Comparable corpora have been used in many 
research areas in NLP, especially in machine 
translation. Several earlier works have studied 
the use of comparable corpora in machine 
translation. However, most of these approaches 
(Fung and McKeown, 1997; Fung and Yee, 1998; 
Rapp, 1999; Chiao and Zweigenbaum, 2002; 
Dejean et al., 2002; Kaji, 2005; Otero, 2007; 
Saralegui et al., 2008; Gupta et al., 2013) are 
specifically focused on extracting word 
translations from comparable corpora. Most of 
the strategies follow a standard method based on 
the context vector similarity measure such as 
finding the target words that have the most 
similar distributions with a given source word. In 
most of the cases, a starting list contains the 
?seed expressions? and this list is required to 
build the context vectors of the words in both the 
languages. A bilingual dictionary can be used as 
a starting list. The bilingual list can also be 
prepared form parallel corpus using bilingual 
correlation method (Otero, 2007). Instead of a 
bilingual list, multilingual thesaurus could also 
be used for this purpose (Dejean, 2002).  
Wikipedia is a multilingual encyclopedia 
available in different languages and it can be 
used as a source of comparable corpora. Otero et 
al. (2010) stored the entire Wikipedia for any 
two languages and transformed it into a new 
collection: CorpusPedia. Our work shows that 
only a small ad-hoc corpus containing Wikipedia 
articles could prove to be beneficial for existing 
MT systems. 
In the NIST shared task on Recognizing 
Textual Entailment Challenge (RTE), several 
methods have been proposed to tackle the textual 
entailment problem. Most of these systems use 
some form of lexical matching, e.g., n-gram, 
word similarity, etc. and even simple word 
overlap. A number of systems represent the texts 
as parse trees (e.g., syntactic or dependency trees) 
49
before the actual task. Some of the systems use 
semantic features (e.g., logical inference, 
Semantic Role Labelling) for solving the text and 
hypothesis entailment problem. MacCartney et al. 
(2006) proposed a new architecture for textual 
inference in which finding a good alignment is 
separated from evaluating entailment. Agichtein 
et al. (2008) presented a supervised machine 
learning approach to train a classifier over a 
variety of lexical, syntactic, and semantic metrics. 
Malakasiotis (2009) used string similarity 
measures applied to shallow abstractions of the 
input sentences and a Maximum Entropy 
classifier to learn how to combine the resulting 
features.  
In the present work, we used the textual 
entailment system of Pakray et al. (2011) which 
performed well on various RTE tasks and 
datasets, as well as other NLP tasks like question 
answering, summarization, etc. We integrated a 
new module to by using reVerb 1  tool and 
optimized all the features produced by different 
modules. 
The main objective of the present work is to 
investigate whether textual entailment can be 
used to establish alignments between text 
fragments in comparable corpora and whether 
the parallel text fragments extracted thus can 
improve MT system performance. 
3 Mining Comparable Corpora 
We collected comparable corpora from 
Wikipedia - online collaborative encyclopedia 
available in a wide variety of languages. English 
Wikipedia contains largest volume of data such 
as millions of articles; there are many language 
editions with at least 100,000 articles. Wikipedia 
links articles on the same topic in different 
languages using ?interwiki? linking facility. 
Wikipedia is an enormously useful re-source for 
extracting parallel resources as the documents in 
different languages are already aligned. We first 
collect an English document from Wikipedia and 
then find the same document in Bengali if there 
                                                        
1 http://reverb.cs.washington.edu/ 
exists any inter-language link. Extracted 
English?Bengali document pairs from Wikipedia 
are already comparable since they are written 
about the same entity. Although each 
English?Bengali document pairs are comparable 
and they discuss about the same topic, most of 
the times they are not exact translation of each 
other; as a result parallel fragments of text are 
rarely found in these document pairs. The bigger 
the size of the fragment may result less probable 
parallel version will be found in the target side. 
Nevertheless, there is always chance of getting 
parallel phrase, tokens or even sentences in 
comparable documents.   
We designed a crawler to collect comparable 
corpora for English?Bengali document pairs. 
Based on an initial seed keyword list, the crawler 
first visits each English page of Wikipedia, saves 
the raw text (in HTML format), and then follows 
the cross-lingual link for each English page and 
collects the corresponding Bengali document. In 
this way, we collect English?Bengali comparable 
documents in the tourism domain. We retain only 
the textual information and all the other details 
are discarded. We extract English and Bengali 
sentences from each document. The extracted 
sentences from each English document are not 
parallel with the corresponding Bengali 
document. Moreover, Bengali documents are 
contained limited information compare to the 
English document. We align sentences of 
English?Bengali from these comparable corpora 
through a baseline PB-SMT system. A Bengali-
English baseline PB-SMT system has been 
developed which was trained on 
English?Bengali tourism domain corpus. We 
translated Bengali sentences into English. The 
translated sentence is then examined for 
entailment in the English comparable document 
by using two-way TE system proposed in section 
4. If it is more than 50% entailed with the target 
document then the target sentence is directly 
fetched form the comparable English document 
and the source-target sentence pair are saved in a 
list. In this way, we extract parallel sentences 
from comparable corpora. These parallel 
sentences except those are 100% entailed may 
50
not be completely parallel but they are 
comparable. So, we created a parallel fragment 
list which is proposed in section 5.  
4 Two-way Textual Entailment System 
A two-way automatic textual entailment (TE) 
recognition system that uses lexical, syntactic 
and semantic features has been described in this 
section. The system architecture has been shown 
in Figure 1. The TE system has used the Support 
Vector Machine (SVM) technique that uses 
thirty-one features for training purpose. In lexical 
module there are eighteen features and eleven 
features from syntactic module, one feature by 
using reVerb and one feature from semantic 
module. 
 
Fig.1 Two way TE architecture 
4.1 Lexical Module 
In this module six lexical comparisons and 
seventeen lexical distance comparisons between 
text and hypothesis has used.  
Six lexical comparisons are WordNet 
(Fellbaum, 1998) based unigram match, bigram 
match, longest common sub-sequence, skip-gram, 
stemming and named entity matching.  We have 
calculated weight from each of these six 
comparisons in equation (1). 
weight =
number - of - common - tokens - between - text - and - hypothesis?
number - of - tokens - in - hypothesis?  
(1) 
The API for WordNet Searching (JAWS) 2 
provides Java applications with the ability to 
retrieve data from the WordNet 2.1 database. 
For Named entity detection we have used Text 
Tokenization Toolkit (LT-TTT2)3 (Grover et. al., 
1999). The LT-TTT2 named entity component 
has been used.  
For lexical distance measure, we have used 
features of Vector Space Measures (Euclidean 
distance, Block distance, Minkowsky distance, 
Cosine similarity, Matching Coefficient), Set-
based Similarities (Dice, Jaccard, Overlap, 
Harmonic), Edit Distance Measures (Levenshtein 
distance, Smith-Waterman distance, Jaro 
Distance). Lexical distance measurement has 
used the libraries SimMetrics 4 , SimPack 5  and 
SecondString6. SimMetrics is a Similarity Metric 
Library, e.g., from edit distance (Levenshtein, 
Gotoh, Jaro etc) to other metrics, (e.g Soundex, 
Chapman). 
4.2 Syntactic Module  
The syntactic module compares the dependency 
relations in both hypothesis and text. The system 
extracts syntactic structures from the text-
hypothesis pairs using Combinatory Categorial 
Grammar (C&C CCG) Parser 7  and Stanford 
Parser 8  and compares the corresponding 
structures to determine if the entailment relation 
is established. Two different systems have been 
implemented one system used Stanford Parser 
output and another system used C&C CCG 
Parser. The system accepts pairs of text snippets 
(text and hypothesis) at the input and gives score 
for each comparison. Some of the important 
comparisons on the dependency structures of the 
text and the hypothesis are Subject-subject 
comparison, WordNet Based Subject-Verb 
                                                        
2 http://lyle.smu.edu/~tspell/jaws/index.html 
3 http://www.ltg.ed.ac.uk/software/lt-ttt2 
4 http://sourceforge.net/projects/simmetrics/ 
5https://files.ifi.uzh.ch/ddis/oldweb/ddis/research/simpack/in
dex.html 
6 http://sourceforge.net/projects/secondstring/ 
7 http://svn.ask.it.usyd.edu.au/trac/candc/wiki 
8 http://nlp.stanford.edu/software/lex-parser.shtml 
51
Comparison, Subject-Subject Comparison, 
Object-Verb Comparison, WordNet Based 
Object-Verb Comparison, Cross Subject-Object 
Comparison Number Comparison, Noun 
Comparison, Prepositional Phrase Comparison, 
Determiner Comparison and other relation 
Comparison.  
4.3 reVerb Module  
ReVerb 9  is a tool, which extracts binary 
relationships from English sentences.  The 
extraction format is in Table 1. 
Extraction Format arg1 rel arg2 
Example A person is playing a guitar 
reVerb Extracts arg1= {A person}  rel = {is 
playing} arg2 = {a guitar} 
 
Table 1: Example by reVerb Tool 
The system parsed the text and the hypothesis 
by reverb tool. Each of the relations compares 
between text and hypothesis and calculates a 
score for each pair. 
4.4 Semantic Module 
The semantic module based on the Universal 
Networking Language (UNL) (Uchida and Zhu, 
2001). The UNL can express information or 
knowledge in semantic network form with hyper-
nodes. The UNL is like a natural language for 
computers to represent and process human 
knowledge. There are two modules in UNL 
system - En-converter and De-converter module. 
The process of representing natural language 
sentences in UNL graphs is called En-converting 
and the process of generating natural language 
sentences out of UNL graphs is called De-
converting. An En-Converter is a language 
independent parser, which provides a framework 
for morphological, syntactic, and semantic 
analysis synchronously. The En-Converter is 
based on a word dictionary and a set of 
enconversion grammar rules. It analyses 
sentences according to the en-conversion rules. 
A De-Converter is a language independent 
                                                        
9 http://reverb.cs.washington.edu/ 
generator, which provides a framework for 
syntactic and morphological generation 
synchronously. 
An example UNL relation for a sentence 
?Pfizer is accused of murdering 11 children? is 
shown in Table 2. 
[S:00] 
{org:en} Pfizer is accused of murdering 11 children 
{/org} 
{unl} 
obj(accuse(icl>do,equ>charge,cob>abstract_thing,agt>per
son,obj>person).@entry 
.@present,pfizer.@topic) 
qua:01(child(icl>juvenile>thing).@pl,11) 
obj:01(murder(icl>kill>do,agt>thing,obj>living_thing).@
entry,child(icl>juvenile 
>thing).@pl) 
cob(accuse(icl>do,equ>charge,cob>abstract_thing,agt>per
son,obj>person).@entr 
y.@present,:01) 
{/unl}  
[/S] 
 
Table 2: Example of UNL  
The system converts the text and the 
hypothesis into UNL relations by En-Converter. 
Then it compares the UNL relations in both the 
text and the hypothesis and gives a score for each 
comparison. 
4.5 Feature Extraction Module 
The features are listed in Table 3: 
Name of Features No of features 
Lexical Module 18 
Syntactic Module 11 
reVerb Module 1 
Semantic Module 1 
 
Table 3: Features for SVM 
4.6 Support Vector Machines (SVM) 
Support Vector Machines (SVMs) 10  are 
supervised learning models used for 
classification and regression analysis. The basic 
SVM takes a set of input data and predicts, for 
                                                        
10 http://en.wikipedia.org/wiki/Support_vector_machine 
52
each given input, which of two possible classes 
form the output, making it a non-probabilistic 
binary linear classifier.   
The SVM based our Textual Entailment 
system has used the following data sets: RTE-1 
development and RTE-1 annotated test set, RTE-
2 development set and RTE-2 annotated test set, 
RTE-3 development set and RTE-3 annotated 
test set to deal with the two-way classification 
task. The system has used the LIBSVM -- A 
Library for Support Vector Machines 11  for the 
classifier to learn from this data set. 
5 Alignment of Parallel fragments using 
proposed TE system 
We have extracted parallel fragment from the 
parallel sentence aligned comparable resource 
list as well as the training data. Initially, we 
make cluster on the English side of this list with 
the help of two-way TE method. More than 50% 
entailed sentences have been considered to take a 
part of the same cluster. The TE system divides 
the complete set of comparable resources list into 
some smaller sets of cluster. Each cluster 
contains at least two English sentences. Each 
English cluster is corresponding to the set 
comparable Bengali sentences. So in this way we 
have developed a number of English Bengali 
parallel clusters. We intersect between the both 
English and Bengali sentences which are 
belonging to the same clusters.     
We try to align the English and Bengali 
fragments extracted from a parallel sentence 
aligned comparable resource list. If both sides 
contain only one fragment then the alignment is 
trivial, and we add such fragment pairs to seed 
another parallel fragment corpus that contains 
examples having only one token in both side. 
Otherwise, we establish alignments between the 
English and Bengali fragments using translation. 
If both the English and Bengali side contains n 
number of fragments, and the alignments of n-1 
fragments can be established through translation 
                                                        
11 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
or by means of already existing alignments, then 
the nth alignment is trivial.  
These parallel fragments of text, extracted 
from the comparable corpora are added with the 
tourism domain training corpus to enhance the 
performance of the baseline PB-SMT system. 
6 Tools and Resources 
A sentence-aligned English?Bengali parallel 
corpus contains 23,492 parallel sentences from 
the travel and tourism domain has been used in 
the present work. The corpus has been collected 
from the consortium-mode project ?Development 
of English to Indian Languages Machine 
Translation (EILMT) System 12 ?. The Stanford 
Parser 13  and CRF chunker 14  (Xuan-Hieu Phan, 
2006) have been used for parsing and chunking 
in the source side of the parallel corpus, 
respectively.  
The experiments were carried out using the 
standard log-linear PB-SMT model as our 
baseline system: GIZA++ implementation of 
IBM word alignment model 4, phrase-extraction 
heuristics described in (Koehn et al., 2003), 
minimum-error-rate training (Och, 2003) on a 
held-out development set, target language model 
trained using SRILM toolkit (Stolcke, 2002) with 
Kneser-Ney smoothing (Kneser and Ney, 1995) 
and the Moses decoder (Koehn et al., 2007) have 
been used in the present study. 
7 Experiments and Results 
We randomly identified 500 sentences each for 
the development set and the test set from the 
initial parallel corpus. The rest is considered as 
the training corpus. The training corpus was 
filtered with the maximum allowable sentence 
length of 100 words and sentence length ratio of 
1:2 (either way). Finally the training corpus 
                                                        
12 The EILMT project is funded by the Department of 
Electronics and Information Technology (DEITY), Ministry 
of Communications and Information Technology (MCIT), 
Government of India. 
13 http://nlp.stanford.edu/software/lex-parser.shtml 
14 http://crfchunker.sourceforge.net/ 
53
contained 22,492 sentences. In addition to the 
target side of the parallel corpus, we used a 
monolingual Bengali corpus containing 488,026 
words from the tourism domain for building the 
target language model. Experiments were carried 
out with different n-gram settings for the 
language model and the maximum phrase length 
and it was found that a 4-gram language model 
and a maximum phrase length of 7 produce the 
optimum baseline result on both the development 
and the test set. We carried out the rest of the 
experiments using these settings. 
The collected comparable corpus consisted of 
5582 English?Bengali document pairs. It is 
evident from Table 4 that English documents are 
more informative than the Bengali documents as 
the number of sentences in English documents is 
much higher than those in the Bengali documents. 
When the Bengali fragments of texts were passed 
to the Bengali?English translation module some 
of them could not be translated into English and 
also, some of them could be translated only 
partially. Therefore, some of the tokens were 
translated while some were not. Some of those 
partially translated text fragments were aligned 
through textual entailment; however, most of 
them were discarded. As can be seen from Table 
4, 9,117 sentences were entailed in the English 
side, of which the system was able to establish 
cross-lingual entailment for 2,361 
English?Bengali sentence pairs.  
 No. of 
English 
sentence 
No. of 
Bengali 
sentence 
Extraction from 
Comparable corpora 
579037 169978 
more than 50% Entailed 
English Sentences 
9117 - 
more than 50% Entailed 
(sentence aligned 
comparable) 
2361 2361 
parallel fragment of texts 
from sentence aligned 
comparable list 
3937 3937 
 
Table 4: Statistics of the sentence aligned comparable 
list and the aligned parallel text fragments.  
Finally, the textual entailment based alignment 
procedure was able to align 3937 parallel 
fragments as reported in Table 4. Manual 
inspection of the parallel list revealed that most 
of the aligned texts were of good quality. 
We carried out evaluation of the MT quality 
using four automatic MT evaluation metrics: 
BLEU (Papineni et al., 2002), METEOR 
(Banerjee and Lavie, 2005), NIST (Doddington, 
2002) and TER (Snover et al., 2006). Table 5 
shows the performance of the PB-SMT systems 
built on the initial training corpus and the larger 
training corpus containing parallel text fragments 
extracted from the comparable corpora. Treating 
the parallel text fragments extracted from the 
comparable corpora as additional training 
material results in significant improvement in 
terms of BLEU (1.73 points, 15.84% relative) 
over the baseline system. Similar improvements 
are also obtained for the other metrics.  The low 
evaluation scores could be attributed to the fact 
that Bengali is a morphologically rich language 
and has a relatively free phrase order; besides 
there were only one set of reference translations 
for the testset. 
Experiments BLEU NIST METEOR TER 
Baseline 10.92 4.16 0.3073 75.34 
Baseline  + 
parallel 
fragments of 
texts as 
additional 
training 
material 
12.65 4.32 0.3144 73.00 
 
Table 5: Evaluation results 
8 Conclusion and Future Work 
In this paper, we have successfully extracted 
English?Bengali parallel fragments of text from 
comparable corpora using textual entailment 
techniques. The parallel text fragments extracted 
thus were able to bring significant improvements 
in the performance of an existing machine 
translation system. For low density language 
pairs, this approach can help to improve the 
state-of-art machine translation quality. A 
manual inspection on a subset of the output 
revealed that the additional training material 
54
extracted from comparable corpora effectively 
resulted in better lexical choice and less OOV 
words than the baseline output.  As the collected 
parallel text does not belong to any particular 
domain, this work also signifies that out of 
domain data is also useful to enhance the 
performance of a domain specific MT system. 
This aspect of the work would be useful for 
domain adaptation in MT. As future work, we 
would like to carry out experiments on larger 
datasets.  
Acknowledgments 
The research leading to these results has received 
funding from the EU project EXPERT ?the 
People Programme (Marie Curie Actions) of the 
European Union's Seventh Framework 
Programme FP7/2007-2013<tel:2007-2013>/ 
under REA grant agreement no. [317471]. We 
acknowledge the support from Department of 
Computer and Information Science, Norwegian 
University of Science and Technology and also 
support from ABCDE fellowship programme 
2012-1013. 
References  
Banerjee, Satanjeev and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with 
Improved Correlation with Human Judgments. 
Proceedings of the ACL Workshop on Intrinsic and 
Extrinsic Evaluation Measures for Machine 
Translation and/or Summarization, Ann Arbor, 
Michigan, pages 65?72. 
Chiao, Yun-Chuang and Pierre Zweigenbaum. 2002. 
Looking for candidate translational equivalents in 
specialized, comparable corpora. In Proceedings of 
the 19th international conference on Computational 
linguistics, Volume 2, Association for 
Computational Linguistics, pages 1-5. 
Dagan, Ido and Oren Glickman. 2004. Probabilistic 
textual entailment: generic applied modeling of 
language variability, In PASCAL Workshop on 
Learning Methods for Text Understanding and 
Mining, Grenoble, France. 
De Marneffe, Marie-Catherine, Bill MacCartney, 
Trond Grenager, Daniel Cer, Anna Rafferty, and 
Christopher D. Manning. 2006. Learning to 
distinguish valid textual entailments. In B. Magnini 
and I. Dagan (eds.), Proceedings of the Second 
PASCAL Recognizing Textual Entailment 
Challenge. Venice: Springer, pages 74?79. 
D?jean, Herv?, ?ric Gaussier, and Fatia Sadat. 2002. 
Bilingual terminology extraction: an approach 
based on a multilingual thesaurus applicable to 
comparable corpora. In Proceedings of the 19th 
International Conference on Computational 
Linguistics COLING, Pages 218-224. 
 Doddington, George. 2002. Automatic evaluation of 
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second 
international conference on Human Language 
Technology Research . Morgan Kaufmann 
Publishers Inc, pages. 138-145. 
Fung, Pascale and Kathleen McKeown. 1997. Finding 
terminology translations from non-parallel corpora. 
In Proceedings of the 5th Annual Workshop on 
Very Large Corpora, pages 192-202. 
Fung, Pascale and Lo Yuen Yee. 1998. An IR 
approach for translating new words from 
nonparallel, comparable texts. In Proceedings of 
the 17th international conference on Computational 
linguistics-Volume 1, Association for 
Computational Linguistics, pages 414-420. 
Gupta, Rajdeep, Santanu Pal, and Sivaji 
Bandyopadhyay. 2013. Improving MT System 
Using Extracted Parallel Fragments of Text from 
Comparable Corpora. In proceedings of 6th 
workshop of Building and Using Comparable 
Corpora (BUCC), ACL, Sofia, Bulgaria, Pages 69-
76. 
Kneser, Reinhard and Hermann Ney. 1995. Improved 
backing-off for n-gram language modeling. In 
Proceedings of the IEEE International Conference 
on Acoustics, Speech and Signal Processing, 
volume I. pages 181-184. 
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico,Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ond rej Bojar, 
Alexandra Constantin, and Evan Herbst. Moses: 
open source toolkit for statistical machine 
translation. In Proceedings of the 45th Annual 
Meeting of the ACL on Interactive Poster and 
Demonstration Sessions. Association for 
Computational Linguistics, pages 177-180. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
2003. Statistical phrase-based translation. In 
55
Proceedings of the 2003 Conference of the North 
American Chapter of the Association for 
Computational Linguistics on Human Language 
Technology-Volume 1, Association for 
Computational Linguistics, pages 48-54. 
Mehdad, Yashar, Matteo Negri, and Marcello 
Federico. 2010. Towards Cross-Lingual Textual 
entailment. In Proceedings of the 11th Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics,  
NAACL-HLT 2010. LA, USA.  
Munteanu,  Dragos Stefan and Daniel Marcu. 2006. 
Extracting parallel sub-sentential fragments from 
non-parallel corpora. In Proceedings of the 21st 
International Conference on Computational 
Linguistics and the 44th annual meeting of the 
Association for Computational Linguistics, 
Association for Computational Linguistics, pages 
81-88. 
Negri, Matteo, and Yashar Mehdad. 2010. Creating a 
Bilingual Entailment Corpus through Translations 
with Mechanical Turk: $100 for a 10-day Rush. In 
Proceedings of the NAACL-HLT 2010, Creating 
Speech and Text Language Data With Amazon's 
Mechanical Turk Workshop. LA, USA.  
Neogi, Snehasis, Partha Pakray, Sivaji 
Bandyopadhyay, and Alexander Gelbukh. 2012. 
JU_CSE_NLP: Language Independent Cross-
lingual Textual Entailment System. (*SEM) First 
Joint Conference on Lexical and Computational 
Semantics, Collocated with NAACL-HLT 2012, 
Montreal, Canada.  
Och, F. Josef. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of 
the 41st Annual Meeting on Association for 
Computational Linguistics-Volume 1, Association 
for Computational Linguistics, pages 160-167. 
Och, F. Josef and Herman Ney. 2000. Giza++: 
Training of statistical translation models. 
Otero, P. Gamallo. 2007. Learning bilingual lexicons 
from comparable english and spanish corpora. 
Proceedings of MT Summit xI, pages 191-198. 
Otero, P. Gamallo and Isaac Gonz?lez L?pez. 2010. 
Wikipedia as multilingual source of comparable 
corpora. In Proceedings of the 3rd Workshop on 
Building and Using Comparable Corpora, LREC, 
pages 21-25. 
 Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for 
automatic evaluation of machine translation. In 
Proceedings of the 40th annual meeting on 
association for computational linguistics, 
Association for Computational Linguistics, pages 
311-318. 
Prodromos Malakasiotis. 2009. "AUEB at TAC 2009", 
In TAC 2009 Workshop, National Institute of 
Standards and Technology Gaithersburg, Maryland 
USA. 
Rapp, Reinhard. 1999. Automatic identification of 
word translations from unrelated English and 
German corpora. In Proceedings of the 37th annual 
meeting of the Association for Computational 
Linguistics on Computational Linguistics, 
Association for Computational Linguistics, pages 
519-526. 
Saralegui, X., San Vicente, I., and Gurrutxaga, A. 
2008. Automatic generation of bilingual lexicons 
from comparable corpora in a popular science 
domain. In LREC 2008 workshop on building and 
using comparable corpora. 
Pado, Sebastian, Michel Galley, Dan Jurafsky, and 
Christopher D. Manning. 2009. Textual entailment 
features for machine translation evaluation. In 
Proceedings of the EACL Workshop on Statistical 
Machine Translation, Athens, Greece, pages 37?41. 
 Smith, R. Jason, Chris Quirk, and Kristina Toutanova. 
2010. Extracting parallel sentences from 
comparable corpora using document level 
alignment. In Human Language Technologies: The 
2010 Annual Conference of the North American 
Chapter of the Association for Computational 
Linguistics, Association for Computational 
Linguistics, pages 403-411. 
Snover, Matthew, Bonnie Dorr, Richard Schwartz, 
Linnea Micciulla, and John Makhoul. 2006. A 
study of translation edit rate with targeted human 
annotation. Proceedings of Association for 
Machine Translation in the Americas, Cambridge, 
Massachusetts, USA, pages 223?231. 
Pakray, Partha, Snehasis Neogi, Pinaki Bhaskar, 
Soujanya Poria, Sivaji Bandyopadhyay, and 
Alexander Gelbukh. 2011. A Textual Entailment 
System using Anaphora Resolution. System Report, 
Text Analysis Conference Recognizing Textual 
Entailment Track (TAC RTE) Notebook, 
November 14-15, 2011, National Institute of 
56
Standards and Technology, Gaithersburg, 
Maryland USA 
Stolcke, Andreas. 2002. SRILM-an extensible 
language modeling toolkit. In Proceedings of the 
international conference on spoken language 
processing, Volume 2, pages 901-904. 
Wang, Rui and G?nter Neumann. 2007. Recognizing 
Textual Entailment Using Sentence Similarity 
based on Dependency Tree Skeletons. In 
Proceedings of the third PASCAL Recognising 
Textual Entailment Challenge. 
Xuan-Hieu Phan. 2006. CRFChunker: CRF English 
Phrase Chunker , http://crfchunker.sourceforge.net/. 
57
