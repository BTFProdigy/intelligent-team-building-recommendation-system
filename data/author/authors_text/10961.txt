Jurilinguistic Engineering in Cantonese Chinese: 
An N-gram-based Speech to Text Transcription System 
B K T'sou, K K Sin, S W K Chan, T B Y Lai, C Lun, K T Ko, G K K Chan, L Y L Cheung 
Language hfformation Sciences Research Centre 
City University of Itong Kong 
Tat Chee Avenue, Kowloon 
Hong Kong SAR, China 
Email: rlbtsou @nxmail.cityu.edu.hk 
Abstract  
A Cantonese Chinese transcription system to 
automatically convert stenograph code to 
Chinese characters ix reported. The major 
challenge in developing such a system is the 
critical homocode problem because of 
homonymy. The statistical N-gram model is 
used to compute the best combination of 
characters. Supplemented with a 0.85 million 
character corpus of donmin-specific training 
data and enhancement measures, the bigram 
and trigrmn implementations achieve 95% 
and 96% accuracy respectively, as compared 
with 78% accuracy in the baseline model. The 
system perforlnance is comparable with other 
adwmced Chinese Speech-to-Text input 
applications under development. The system 
meets an urgent need o1' the .ludiciary ot: post- 
1997 Hong Kong. 
Keyword: Speech to Text, Statistical 
Modelling, Cantonese, Chinese, Language 
Engineering 
1. Introduct ion 
British rule in Hong Kong lnade English the only 
official language in the legal domain for over a 
Century. After the reversion of Hong Kong 
sovereignty to China in 1997, legal bilingualism 
has brought on an urgent need to create a 
Computer-Aided Transcription (CAT) system for 
Cantonese Chinese to produce and maintain the 
massive legally tenable records of court 
proceedings conducted in the local majority 
language (T'sou, 1993, Sin and T'sou, 1994, Lun 
et al, 1995). With the support fl'om the Hong 
Kong Judiciary, we have developed a 
transcription system for converting stenograph 
code to Chinese characters. 
CAT has been widely used for English for 
many years and awlilable R~r Mandarin Chinese, 
but none has existed for Cantonese. Althongh 
Cantonese is a Chinese dialect, Cantonese and 
Mandarin differ considerably in terms of 
phonological struclure, phouotactics, word 
morphology, vocabulary and orthogral)hy. Mutual 
intelligibility between the two dialects is generally 
very low. For example, while Cantonese has lnole 
than 700 distinct syllables, Mandarin has only 
about 400. Cantonese has 6 tone contours and 
Mandarin only 4. As for vocabulary, 16.5% of the 
words in a 1 million character corpus of court 
proceedings in Canlonese cannot be found in a 
corlms consisting of 30 million character 
newspaper texts in Modern Written Chinese 
(T'sou el al, 1997). For orthography, Mainhmd 
China uses the Simplified Chinese character set, 
and Hong Kong uses the Traditional set l~lus 
4,702 special local Cantonese Chinese characters 
(Hong Kong Government, 1999). Such 
differences between Cantonese and Mandarin 
necessitate the Jtnilinguistic Engineering 
undertaking to develop an independent Cantonese 
CAT system for the local language nvironment. 
The major challenge in developing a 
Cantonese CAT system lies in the conversion of 
phonologically-based stenograph code into 
Chinese text. Chinese is a logographic language. 
Each character or logograph represents a syllable. 
While the total inventory of Cantonese syllable 
types is about 720, them am at least 14,000 
Chinese character types. The limited syllabary 
creates many homophones in the language (T'sou, 
1976). In a one million character corlms of court 
proceedings, 565 distinct syllable types were 
found, representing 2,922 distinct character types. 
Of the 565 syllable types, 470 have 2 or morn 
homophonous characters. In the extreme case, zi 
represents 35 homophonous character types. 
1121 
Coverage ofl lomophonous and Non-homophous Characters 
100.00% ?'o o~ 4:~ ~ ~os 4:e 
80.00% 
6 
60.00% 
# 40.00% 
c~ 20.00% 
0.00% 
? No. of High Freq. Characters %~ t~ p 
\[EJ HoInophonous Charactcrs \[\] Non-honaophonous chaliactcrs\] 
Figure I. Covcragc of Homonymous Characters 
These 470 syllables represent 2,810 homophonous 
character types which account for 94.7% of the 
text, as shown in Figure \]. The homocode 
problem nmst be properly resolved to ensure 
successful conversion. 
2. Computer-Aided Transcription (CAT) 
~dJ l  
t 
? I 
co 2F2L . 
Stcnoma~h code I i 
Slzg?2 E 
I Transcription \[ | 
/ ! EIt?illC ; | 
h Trigranl ,, Big,an, i ~ '9 
=, E ) 
! 
Proofreading 
\[ i 
' I 
' Bigram / Trip, ram 
i Statistical Dala 
Chinese Text i l l :~!  
Figure 2. Automatic Transcription Process 
Figure 2 outlines the transcription process in the 
Cantonese CAT system. Following typical 
courtroom CAT systems, our process is divided 
into three major stages. In Stage 1, simultaneous 
to a litigant speaking, a stenographer inputs 
speech, i.e. a sequence of transcribed syllables or 
stenograph codes, via a stenograph code generator. 
Each stenograph code basically stands for a 
syllable. In Stage 2, the transcription software 
converts the sequence of stenograph codes \[Sl . . . . .  
s,,} into the original character text {q . . . . .  c,,}. 
This procedure requires the conversion 
component to be tightly bound to the phonology 
and orthography of a specific language. To 
specifically address homonymy in Cantonese, the 
conversion procedure in our system is supported 
by bigram and trigram statistical data derived 
from domain-specific training. In Stage 3, manual 
editing of the transcribed texts corrects errors 
from typing mistakes or his-transcription. 
3. System Architecture 
3.1 Statistical Formulation 
To resolve massive ambiguity in speech to text 
conversion, the N-gram model is used to 
determine the most probable character sequence 
{q . . . . .  ck} given the input stenograph code 
sequence {s~ . . . . .  Sk}. The conditional probability 
(1) is to be maximized. 
(1) P(q . . . . .  c~l sl . . . . .  sk) 
where {q . . . . .  c~} stands for a sequence of N 
characters, and {sl . . . . .  sk} for a sequence of k 
input stenograph codes. 
The co-occurrence frequencies necessary for 
computation are acquired through training. 
However, a huge amount of data is needed to 
generate reliable statistical estimates for (1) if 
N > 3. Consequently, N-gram probability is 
approximated by bigram or trigram estimates. 
First, rewrite (1) as (2) using Bayes' rule. 
P(c, ..... c )xP(s, ..... s lc, ..... c , )  
(2) 
P(s, ..... s k ) 
As the value of P(s I . . . . .  st) remains unchanged 
for any choice of {q . . . . .  ct}, one needs only to 
maximize the numerator in (2), i.e. (3). 
(3) P(cl ..... Ck) X P(sl ..... s,\[cl ..... ck) 
(3) can then be approximated by (4) or (5) using 
bigram and trigram models respectively. 
(4) FL=,...., (P(c,lq_,) x P(s,.Iq)) 
(5) ?P(silci)) 
The transcription program is to compute the best 
sequence of {q . . . . .  c,} so as to maximize (4) or 
(5). The advantage of the approximations in (4) 
and (5) is that P(s,lc,), P(c,lc,.,) and P(c,lc,_2c,_,) 
can be readily estimated using a training corpus of 
manageable size. 
3.2 Viterbi Algorithm 
The Viterbi algorithm (Viterbi, 1967) is 
implemented toefficiently compute the maxinmm 
value of (4) and (5) for different choices of 
1122 
character sequences. Instead o1' exhaustively 
computing tile values for all possible character 
sequences, the algorithm only keeps track of the 
probability of the best character sequence 
terminating in each possible character candidate 
for a stenograph code. 
In the trigram implelnentatiou, size limitation 
in the training cortms makes it impossible to 
estimate all possible P(c i lc i_2ci . i )  because some 
{ci_2, ci_l, q} may never occur there. Following 
Jelinek (1990), P(cil ci.2ci_ i ) is approximated by 
the summation of weighted lrigram, bigram and 
unigram estimates in (6). 
(6) p(c i I ci_ 2 ci_ 1) 
f(ci_ 2 ci- I ci ) f(ci_ 1 c i ) f(c i ) 
= w 3 X + w 2 X + w 1 X -  
f(ci_ 2 ci_ 1 ) f(ci_ I ) Z f(cj ) 
where (i) w,, w2, w-s _> 0 are weights, (ii) 
wl-l-w2-{-H; 3 = 1, and (iii) Z f(q) is the stun of 
frequencies of all characters. Typically lhe best 
results can be obtained if w:~, the weight for 
trigram, is significantly greater than the olher two 
weights so that the trigram probability has 
dominant effect in the probability expression. In 
our tests, we sot wl=0.01, w2=0.09, aud u;3=0.9. 
The Viterbi algorithm substantially reduces the 
computational complexity flom O(m") to O(m.-~n) 
and O(nr~n) using bigram and trigram estimation 
rc:spectively where n is the number of stenograph 
code tokens in a sentence, and m is tile upper 
bound of the number of homophonous characters 
for a stenograph code. 
To maximize the transcription accuracy, we 
also refine the training corpus to ensure that the 
bigram and trigram statistical models reflect the 
comtroom lauguage closely. This is done by 
enlarging tile size of tile training corpus and by 
compiling domain-specific text corpora. 
3.,3 Special Encoding 
After some initial trial tests, error analysis was 
conducted to investigate the causes of the mis- 
transcribed characters. It showed that a noticeable 
amount of errors were due to high failure rate in 
the mtriewtl of seine characters in the 
transcription. The main reason is that high 
fiequency characters are more likely to interfere 
with the correct retrieval of other relatively lower 
frequency homophouous characters. For example, 
Cantonese, hal ('to be') and hal ('at') are 
homophouous in terms of seglnental makeup. 
Their absolute fiequcucies in our training corpus 
are 8,695 and 1,614 respectively. Because of the 
large fi'equency discrepancy, the latter was mis- 
transcribed as tile former 44% of the times in a 
trial test. 32 such high fi'equency characters were 
found to contribute to about 25% of all 
transcription errors. To minimize the interference, 
special encoding, which resulted flom shallow 
linguistic processing, is applied to the 32 
characters o that each of them is assigned a 
unique stenograph code. This was readily 
accepted by the court stenographers. 
4. hnplementation and Results 
4.1 Compilation of Corpora 
In our expreriments, authentic Chinese court 
proceedings from the Hong Kong Judiciary were 
used fox tile compilation of the training and 
testing corpora for the CAT prototypes. To ensure 
that tile training data is comparable with tile data 
to be transcribed, the training corpus should be 
large enough to obtain reliable estimates for 
P(silc,.), P(cilci j) and P(cilci_2ci_l).  in our trials, 
we quickly approached the point of diminishing 
return when the size of the training corpus reaches 
about 0.85 million characters. (See Section 4.2.2.) 
To further enhance training, the system also 
exploited stylistic and lexical variations across 
different legal domains, e.g. tra\[.'fic, assauh,  and 
f raud  offences. Since different case types show 
distinct domain-specific legal vocabulary or usage, 
simply integrating all texts in a single training 
corpus may obscure the characteristics o1' specific 
language domains, thus degrading the modelling. 
Hence domain-specific training corpora were also 
compiled to enhance performance. 
Two sets of data were created for testing and 
comparison: Gener ic  Coqms (GC) and Domain -  
,specific Cmpus  (DC). Whereas GC consists of 
texts representing various legal case types, DC is 
restricted to traffic offence cases. Each set 
consists of a training corpus of 0.85 million 
characters and a testing corpus of 0.2 million 
characters. The training corpus consists of 
Chinese characters along with the corresponding 
stenograph codes, and tile testing corpus consists 
solely of stenograph codes of the Chinese texts. 
4.2 Experimental Results 
For ewfluation, several prototypes were set up to 
1123 
test how different factors affected transcription 
accuracy. They included (i) use of bigram vs. 
trigram models, (ii) the size of the training 
corpora, (iii) domain-specific training, and (iv) 
special encoding. To measure conversion 
accuracy, the output text was compared with the 
original Chinese text in each test on a character by 
character basis, and the percentage of correctly 
transcribed characters was computed. Five sets of 
experiments are reported below. 
4.2.1 Bigram vs. Trigram 
Three prototypes were developed: the Bigram 
Prototype, CA Tva2, the Trigram Prototype, CA Tva.~, 
and the Baseline Prototype, CATo. CATva2 and 
CATvA.~ implelnent he conversion engines using 
the bigram and trigram Viterbi algorithm 
respectively. CA7o, was set up to serve as an 
experimental control. Instead of implementing the 
N-gram model, conversion is accomplished by 
selecting the highest fiequency item out of the 
homophonous character set for each stenograph 
code. GC was used throughout the three 
experiments. The training and testing data sets are 
0.85 and 0.20 million characters respectively. The 
results are summarized in Table 1. 
Corpus GC GC GC 
Accuracy 78.0% 92.4% 93.6% 
Table 1. Different N-gram Models 
The application of the bigram and trigram models 
offers about 14% and 15% improvement in 
accuracy over Control Prototype, CATo. 
4.2.2 Size of Training Corpora 
In this set of tests, the size of the training corpora 
was varied to determine the impact of the training 
corpus size on accuracy. The sizes tested are 0.20, 
0.35, 0.50, 0.63, 0.73 and 0.85 million characters. 
Each corpus is a proper subset of the immediately 
larger corpus so as to ensure the comparability of 
he trainin texts. CATvA 2 was used in the tests. 
Training Corpus GC GC GC 
Accurac~ 89.5% 91.2% 91.8% 
Training Corpus GC GC GC 
Accuracy 92.1% 92.3 % 92.4 % 
Table 2. Variable Training Data Size 
The results in Table 2 show that increasing the 
size of the training corpus enhances the accuracy 
incrementally. However, the point of diminishing 
return is reached when the size reaches 0.85 
million characters. We also tried doubling the 
corpus size to 1.50 million characters. It only 
yields 0.8% gain over the 0.85 million character 
corpus. 
4.2.3 Use of Domain-specific Training 
This set of tests evaluates the effectiveness of 
domain-specific training. Data fi'oln the two 
corpora, GC and DC, are utilized in the training of 
the bigram and trigram prototypes. The size of 
each training set is 0.85 million characters. The 
same set of 0.2 million character testing data from 
DC is used in all four conversion tests. Without 
increasing the size of the training data, setups with 
domain-specific training consistently ield about 
2% improvement. A more comprehensive set of 
corpora including Tra.lfic, Assault, and Robbeo~ is
bein )iled and will be re )ortcd in future. 
Prototypes 
Training Data 
CATvA; CATvA3 CATvaz CATvA3 
GC GC DC DC 
Testing Data DC DC DC DC 
Accuracy 92.6% 92.8% 94.7% 94.8% 
Table 3. Application of Domain-Specificity 
4.2.4 Special Encoding 
Following shallow linguistic processing, special 
encoding assigns unique codes to 32 characters to 
reduce confusion with other characters. Another 
round of tests was repeated, identical to the 
CATvA2 and CATvA 3 tests in Section 4.2.1, except 
for the use of special encoding. The use of 
training and testing corpora have 0.85 and 0.20 
million characters respective 
S ~ i ~  I;:~ :::NOfA~I~Ii~: 
Prototypes CATw~ CATvA~ CATw2 CATvA3 
Corpus GC GC GC GC 
Accuracy 92.4% 93.6% 94.7% 95.6% 
Table 4. Application of Special Encoding 
Table 4 shows that the addition of special 
encoding consistently offers about 2% increase in 
accuracy. Special encoding and hence shallow 
linguistic processing provide the most significant 
improvement in accuracy. 
4.2.5 Incorporation of Domain-Specificity and 
Special Eneoding 
As discussed above, both domain-specific training 
and special encoding raise the accuracy of 
transcription. The last set of tests deals with the 
integration of the two features. Special encoding 
1124 
is utilized in the training and testing data of DC 
which have 0.85 and 0.20 million characters 
respectively. 
~raining/Testing, Data DC DC 
I S. Encoding Applied Applied / zcuracy 95.4 % 96.2 % 
Table 5. Integration of D. Specificity and S. Encoding 
Recall that Domain-Specificity and Special 
Encoding each offers 2% improvelnent. Table 5 
shows that combining BOTH features offer about 
3% improvement over tests without them. (See 
non-domain-specific tests in Section 4.2.3) 
The 96.2% accuracy achieved by CATvA 3 
represents the best performance of our system. 
The result is conaparable with other relevant 
advanced systems for speech to text conversion. 
For example, Lee (1999) reported 94% accuracy 
in a Chinese speech to text transcription system 
under developlnent with very large training 
corpus. 
5. Conclusion 
We have created a Cantonese Chinese CAT 
system which uses the phonologically-based 
stenograph machine. The system delivers 
encouragingly accurate transcription in a language 
which has many hon\]ol~honous characters. To 
resolve problematic ambiguity in the conversion 
fi'on-i a I)honologically-based code to the 
logograt)hic Chinese characters, we made use of 
lhe N-gram statistical model. The Viterbi 
algorithm has enabled us to identify the most 
probable sequence of characters from the sels of 
possible homophonous characters. With the 
additional use of special encoding and domain- 
specific training, the Cantonese CAT system has 
attained 96% transcription accuracy. The success 
of the Jurilinguistic Engineering project can 
further enhance the efforts by the Hong Kong 
Judiciary to conduct trials in the language of the 
majority population. Further improvement to the 
system will include (i) more domain-specific 
training and testing across different case types, (2) 
firm-tuning for the optimal weights in the trigram 
formula, and (3) optilnizing the balance between 
training corpus size and shallow linguistic 
processing. 
Acknowledgement  
Support for the research reported here is provided 
mainly through the Research Grants Council of 
Hong Kong under Competitive Earmarked 
Research Grant (CERG) No. 9040326. 
References 
Hong Kong Govennnent. 1999. Hong Kong 
Szq~plemenmry Character Set. hfformation 
Technology Services Department & Official 
Languages Agency. 
Jelinek, F. 1990. "Self-organized Language Modeling 
lbr Speech Recognition." In A. Waibel and K.F. 
Lee, (eds.). Readings in Speech Recognition. San 
Mateo: CA: Morgan Kaufmann Publishers. 
Lee, K. F. 1999. "Towards a Multimedia, Multimodal, 
Multilingual Computer." Paper presented on behall' 
of Microsoft Research Institute, China in the 5th 
Natural Language Processing Pacil'ic Rim 
Symposium held in Belling, China, November 5-7, 
1999. 
Lun, S., K. K. Sin, B. K. T'sou and T. A. Cheng. 1997. 
"l)iannao Fuzhu Yueyu Suii Fangan." (The 
Cantonese Shorthand System for Computer-Aided 
Transcription) (in Chinese) Proceedings o.f the 5th 
lntermttional Confere,ce on Cantonese and Other 
Yue Dialects. B. H. Zhan (ed). Guangzhou: Jinan 
University Press. pp. 217--227. 
Sin, K. K. and B. K. T'sou. 1994. "Hong Kong 
Courtroon\] Language: Some Issues on Linguistics 
and Language Technology." Paper presented at lhe 
Third International Conference on Chinese 
Linguistics. Hong Kong. 
T'sou, B. K. 1976. "Homophony and Internal Change 
in Chinese." Computational Analyses of Asian and 
African Lauguages 3, 67--86. 
T'sou, t3. K. 1993. "Some Issues on Law and Language 
in the ltong Kong Special Administrative Region 
(HKSAR) of China." Language, Law attd Equality: 
Proceedings of the 3rd International Conference of 
the International Acaden G of Language Law (IALL). 
K. Prinsloo et al Pretoria (cds.): University of South 
Africa. pp. 314-331. 
T'sou, B. K., H. L. Lin, G. Liu, T. Chan, J. Hu, C. H. 
Chew, and J. K. P. Tse. 1997. "A Synchronous 
Chinese Language Corpus fi'om Different Speech 
Communities: Construction and Applications." 
Computational Linguistics and Chinese Language 
Ptwcessing 2:91-- 104. 
Viterbi, A. J. 1967. "Error Bounds for Convolution 
Codes and an Asymptotically Optimal l)ecoding 
Algorithm." IEEE 7'ransactions on htformation 
TheoG 13: 260--269. 
1125 
Harvesting the Bitexts of the Laws of Hong Kong From the Web
Chunyu Kit Xiaoyue Liu KingKui Sin Jonathan J. Webster
Department of Chinese, Translation and Linguistics
City University of Hong Kong, Tat Chee Ave., Kowloon, Hong Kong
{ctckit, xyliu0, ctsinkk, ctjjw}@cityu.edu.hk
Abstract
In this paper we present our recent work
on harvesting English-Chinese bitexts
of the laws of Hong Kong from the
Web and aligning them to the subpara-
graph level via utilizing the number-
ing system in the legal text hierarchy.
Basic methodology and practical tech-
niques are reported in detail. The re-
sultant bilingual corpus, 10.4M English
words and 18.3M Chinese characters,
is an authoritative and comprehensive
text collection covering the specific and
special domain of HK laws. It is par-
ticularly valuable to empirical MT re-
search. This piece of work has also laid
a foundation for exploring and harvest-
ing English-Chinese bitexts in a larger
volume from the Web.
1 Introduction
Bitexts, also referred to as parallel texts or bilin-
gual corpora, collections of bilingual text pairs
aligned at various levels of granularity, have been
playing a critical role in the current development
of machine translation technology. It is such
large data sets that give rise to the plausibility
of empirical approaches to machine translation,
most of which involve the application of a variety
of machine learning techniques to infer various
types of translation knowledge from bitext data
to facilitate automatic translation and enhance
translation quality. Large volumes of training
data of this kind are indispensable for construct-
ing statistical translation models (Brown et al,
1993; Melamed, 2000), acquiring bilingual lex-
icon (Gale and Church, 1991; Melamed, 1997),
and building example-based machine translation
(EBMT) systems (Nagao, 1984; Carl and Way,
2003; Way and Gough, 2003). They also provide
a basis for inferring lexical connection between
vocabularies in cross-languages information re-
trieval (Davis and Dunning, 1995).
Existing parallel corpora have illustrated their
particular value in empirical NLP research, e.g.,
Canadian Hansard Corpus (Gale and Church,
1991b), HK Hansard (Wu, 1994), INTERSECT
(Salkie, 1995), ENPC (Ebeling, 1998), the Bible
parallel corpus (Resnik et al, 1999) and many
others. The Web is being explored not only as a
super corpus for NLP and linguistic research (Kil-
garriff and Grefenstette, 2003) but also, more im-
portantly to MT research, as a treasure for mining
bitexts of various language pairs (Resnik, 1999;
Chen and Nie, 2000; Nie and Cai, 2001; Nie
and Chen, 2002; Resnik and Smith, 2003; Way
and Gough, 2003). The Web has been the play-
ground for many NLPers. More and more Web
sites are found to have cloned their Web pages in
several languages, aiming at conveying informa-
tion to audience in different languages. This gives
rise to a huge volume of wonderful bilingual or
multi-lingual resources freely available from the
Web for research. What we need to do is to har-
vest the right resources for the right applications.
In this paper we present our recent work on
harvesting English-Chinese parallel texts of the
laws of Hong Kong from the Web and construct-
71
ing a subparagraph-aligned bilingual corpus of
about 20 million words. The bilingual texts of the
laws is introduced in Section 2, with an emphasis
on HK?s legislation text hierarchy and its num-
bering system that can be utilized for text align-
ment to subparagraph level. Section 3 presents
basic methodology and technical details for har-
vesting and aligning bilingual Web page pairs, ex-
tracting content texts from the pages, and align-
ing text structures in terms of the text hierarchy
via utilizing consistent intrinsic features in the
Web pages and content texts. Section 4 presents
XML schema for encoding the alignment results
and illustrates the display mode for browsing the
aligned bilingual corpus. Section 5 concludes
the paper, highlighting the value of the corpus in
term of its volume, translation quality, specificity
and comprehensiveness, and alignment granular-
ity. Our future work to explore the Web for har-
vesting more quantities of parallel bitexts is also
briefly outlined.
2 Bilingual Texts of the Laws of HK
The laws of Hong Kong (HK) before 1987 were
exclusively enacted in English. They were trans-
lated into Chinese in the run-up to the handover
in 1997. Since then all HK laws have been en-
acted in both English and Chinese, both versions
being equally authentic. This gives rise to a valu-
able set of bitexts in large quantity and high qual-
ity that can be utilized to facilitate empirical MT
research.
2.1 BLIS Corpus
The bilingual texts of the laws of Hong Kong
have been made available to the public in re-
cent years by the Justice Department of the HK-
SAR through the bilingual laws information sys-
tem (BLIS). All these texts are freely accessible
from http://www.justice.gov.hk/.
BLIS provides the most comprehensive docu-
mentation of HK legislation. It contains all statute
laws of Hong Kong currently in operation, includ-
ing all ordinances and subsidiary legislation of
HK (and some of their past versions dating back
to 60 June 1997), the Basic Law and the Sino-
British Joint Declaration, the constitution of PRC
and national laws that apply in HK, and other rel-
evant instruments. The entire bilingual corpus of
Figure 1: Illustration of BLIS hierarchy
BLIS legal texts contains approximately 10 mil-
lion English words and 18 million Chinese char-
acters. Lexical resources of this kind are particu-
larly useful in bilingual legal terminology studies
and text alignment work.
2.2 Text Hierarchy
BLIS organizes the legal texts in terms of the
hierarchy of the Loose-Leaf Edition of the Laws
of Hong Kong. At the top level, the ordinances
are arranged by chapters, each of which is identi-
fied by an assigned number and a short title, e.g.,
Chapter 5 OFFICIAL LANGUAGES ORDINANCE /
?5? ??????. The assigned number for a
subsidiary legislation chapter consists of a chap-
ter number and a following uppercase letter, e.g.,
CAP 5C HIGH COURT CIVIL PROCEDURE (USE
OF LANGUAGE) RULES / ?5C? ???????
?(????)??.
The content of an ordinance, exclusive of its
long title, is divided and identified according to a
very rigid numbering system which encodes the
hierarchy of the texts of the laws. Both the Chi-
nese and English versions of an ordinance fol-
low exactly the same hierarchical structures such
as chapters (?), parts (?), sections (?), sub-
sections (?), paragraphs (?) and subparagraphs
(?). This allows us to align the bitexts along
72
Figure 2: BLIS texts in pair
this hierarchical structure, once they are down-
loaded from the BLIS official site. To our knowl-
edge, a well-aligned bilingual corpus of this size
covering a special domain so comprehensively is
seldom readily available for the Chinese-English
language pair.
Excerpts from the BLIS corpus are illustrated
in Figure 1 and 2, one illustrating its hierarchy and
the other a pair of BLIS bitexts. From the excerpts
we can see that not everything has an exact match
between a pair of BLIS Web pages. For example,
the Chinese side has a gazette number ?25 of 1998
s. 2? and a piece of ?remarks? at the beginning of
content text, whereas its English counterpart has
none of them.
3 Harvesting Bitexts from the Web
Basically two phases are involved in construct-
ing the bilingual corpus of the laws of HK. The
first phase is to harvest the monolingual texts of
HK laws from the BLIS site and align them into
pairs. It involves the following steps: (1) down-
loading Web pages one by one with the aid of a
Web crawler, (2) extracting the texts from them
by filtering out the HTML markup, and (3) align-
ing the extracted monolingual texts into bilingual
Figure 3: BLIS web pages connected as two dou-
ble linked lists
pairs. The second phase is to align finer-grained
text structures within each text pair.
3.1 Downloading BLIS Web Pages
A BLIS Web page does not necessarily corre-
spond to any particular text structure such as a
chapter, a part, a section, a subsection, or a para-
graph in the BLIS hierarchy. A chapter, espe-
cially a short one, may be organized into a few
sections in a Web page or in several contiguous
pages. Some sections, e.g., the long ones, are di-
vided into several pages. In general, BLIS does
not maintain any reliable match between its Web
pages and any particular text hierarchical struc-
tures.
Fortunately, in most cases a BLIS page always
has a counterpart in the other language. There is
a ?switch language? button on each page to link
to the counterpart page. Such linkage allows us
to download the Web pages in pairs and, conse-
quently, harvest a list of page-to-page aligned bi-
texts.
In addition to the pair link, each BLIS page also
carries links for the ?next? and the ?previous sec-
tion of enactment?. These two kinds of linkage
turn the pages into two double linked lists, each
in a language, as illustrated in Figure 3, with each
page as a node. Nodes in pairs are also double
linked between the two lists.
However, the pairwise linkage is not reliable
in the BLIS site, because there are missing Web
pages in one of the two languages in question
(see Table 3 below for more details). In order to
download all bitexts of legislation from the site,
we need to go through one linked list and down-
load each page and its counterpart, if there is one,
in the other language. Such scanning gives a list
of text pairs, where some pages may have a null
73
Total time Downloaded files
English 17 hours 50,638 (429MB)
Chinese 18 hours 50,510 (460MB)
Table 1: File downloading
File name
BLIS HTML page title Chinese English
Cap 5A ... 5A c.txt 5A e.txt
Cap 5A s 1 ... 5A-1 c.txt 5A-1 e.txt
Cap 5A s 2 ... 5A-2 c.txt 5A-2 e.txt
Cap 5A s 3 ... 5A-3 c.txt 5A-3 e.txt
Table 2: Naming downloaded files in terms of
BLIS numbering
counterpart. An alternative strategy is to down-
load each list separately, and then match the pages
into pairs sequentially with the aid of numbering
information in the header of each page ? see 3.2
below. These two strategies verify one another,
making sure that all pages are downloaded and
put in the right pairs.
The downloading is carried out by a Web
crawler implemented in Java. In order to accom-
plish the above strategies, it also has to handle a
number of technical issues.
? It sleeps for a while (e.g., 10 seconds) when
it finishes downloading a certain number of
pages (e.g., 50 pages), because the BLIS site
refuses continuous access from one site for a
too long time.
? When an error occurs, it remembers the cur-
rent URL. Then it re-starts from where it
stops.
The data about the file downloading from BLIS
site is given in Table 1. One can conceive that
if the time intervals for sleep and downloading
could be automatically tuned by the crawler to
maximize the downloading efficiency, it would
get the job done significantly more quickly. Our
option for 10 seconds sleep between every 50 files
is based on error records of a number of test runs.
3.2 Aligning Web Pages
Every BLIS Web page is identified by a subti-
tle that carries numbering information about the
page, as illustrated in Figure 1. Such a subtitle
is exactly retained in the page as its HTML title.
Files English Chinese
Aligned 50,506 (62.3MB)a 50,506 (38.5MB)
Missing 132 4
Total 50,638 50,510
Sizeb 10.4M words 18.3M char.s
aThe size of extracted texts.
bExclusive of punctuation marks.
Table 3: The number of aligned and missing files
This feature is utilized to align BLIS pages: all
downloaded files are named in terms of the num-
bering information extracted from their HTML ti-
tles, as illustrated in Table 2. Consequently, all
files are naturally aligned in pairs by their names.
Any file names not in a pair indicate the missing
counterparts in the other language. The statistics
of file alignment are given in Table 3.
3.3 Text Extraction
Basically, this task involves two aspects, namely,
filtering HTML markup and extracting content
text. A straightforward strategy is that we first
clean up HTML tags in each page and then the
non-legal content. The tags are in brackets, and
non-legal content in a consistent pattern through-
out all BLIS pages. However, a more convenient
way to do it is to make use of a reliable feature
in the BLIS pages: legal content is placed in be-
tween two ? the only two ? horizontal bars in each
page. Accordingly, we implement a strategy to
first extract every thing in between the two bars
and then clean up remaining HTML tags. The
output from this procedure includes
? a header as a fixed set of items, including
chapter number, title, heading, etc., and
? a piece of content text as a list of numbered
items each in a line. (See the header and con-
tent text in Figure 2.)
The text in a BLIS page is displayed as a sequence
of hierarchically numbered items, such as subsec-
tions, paragraphs and subparagraphs.
3.4 Text Alignment within Text Pairs
After page (or file) alignment, each page finds its
counterpart in the other language. After text ex-
traction, a page gives a content text consisting of
a list of numbered items, each in a line. A such
74
Remarks:
Adaptation amendments retroactively made - see
26 of 1999 s.3//a
(1) All Ordinances shall be enacted and published
in both official languages.//
(2) Nothing in subsection (1) shall require an
Ordinance to be enacted and published in
both official languages where that Ordinance
amends another Ordinance and-//
(a) that other Ordinance was enacted in the
English language only; and//
(b) no authentic text of that Ordinance has been
published in the Chinese language under
section 4B(1).//
(3) Nothing in subsection (1) shall require an
Ordinance to be enacted and published in both
official languages where the Chief Executive
in Council- (Amended 26 of 1999 s.3)//
aIndicating a text line break.
Table 4: Anchors in a sample text
item can be divided into a numbering item and the
remaining content text in the line, as illustrated in
Table 4. The Chinese counterpart of this text car-
ries similar lines, if no missing line in any page of
the pair.
Unfortunately, missing lines are found in some
BLIS pages, as exemplified in Figure 2. There is
no guarantee that matching text lines one by one
in sequence would carry out the expected align-
ment within a page pair. However, the numbering
items at the beginning of each line can be utilized
as anchors to facilitate the alignment. The strat-
egy along this line is given as follows.
1. Anchor identification: numbering items at
the beginning of each line are recognized
as anchors, with the beginning and the end
of the whole content text as two special an-
chors, resulting in a list of anchors for each
page;
2. Anchor alignment: match the two lists of an-
chors sequentially. If a pair of anchors does
not match, give up the smaller one (in terms
of the BLIS numbering hierarchy) and move
on to the next possible pair, working in ex-
actly the same procedure as matching iden-
tical anchor pairs between two sorted lists of
anchors.
3. Text line alignment: a pair of matched an-
chors give a pair of matched lines; an un-
matched anchor indicates a missing line in
the other language.
4 XML Markup for the Aligned Corpus
XML is applied to encode the text alignment
outcomes output from the above alignment pro-
cedure. It has been a standard for data repre-
sentation and exchange on the Web, and also
accepted by the NLP community as a standard
for linguistic data annotation and representation
(Ide et al, 2000; Mengel and Lezius, 2000;
Kim et al, 2001). There are a series of yearly
NLPXML workshops for it since 2001. It pro-
vides a platform-independent flexible and sophis-
ticated plain text format for data encoding and
manipulation. It is particularly suitable for hier-
archical linguistic data such as the hierarchically-
aligned bilingual corpus that we have produced.
What?s more, converting data to XML format not
only significantly reduces the complexity of data
exchange among different computer systems but
also enhances data transmission reliability and
eases Web browsing.
There have been many corpora that are anno-
tated with XML, e.g., HCRC Map Task Corpus
(Anderson et al, 1991), American National Cor-
pus (Ide and Macleod, 2001), the La Republica
corpus (Baroni et al, 2004). Below we present
the XML schema for our subparagraph-aligned
BLIS bitexts, with sample annotation, and nec-
essary Web browsing.
4.1 XML Schema
The current version of the XML schema for the
bilingual BLIS corpus, as given in Figure 4, fo-
cuses on encoding all text structures in the BLIS
hierarchy, including all elements in each BLIS
Web page. It is to be extended to cover finer-
grained structures such as clauses, phrases and
words, as we proceed to align the BLIS bitexts
at these linguistic levels. For simplicity, we al-
low para to subsume all types of text line, be
they a section, subsection, paragraph or subpara-
graph. The annotation of a sample bitext with this
schema is illustrated in Figure 5. Annotation of
this kind is carried out by a Java program auto-
matically for the entire bitext corpus.
4.2 Corpus Browsing
A number of display modes are designed for
browsing the subparagraph-aligned bitexts, in-
cluding bilingual modes and monolingual modes.
75
Figure 4: XML schema for aligned BLIS bitexts
In a bilingual mode, text line pairs are displayed
in sequence. Switch of language order or from
one mode to another is allowed any time during
browsing. The bilingual display mode is illus-
trated in Figure 6.
5 Conclusion
We have presented in the above sections our re-
cent work on harvesting and aligning the bitexts
of the laws of Hong Kong, including basic tech-
niques for downloading English-Chinese bilin-
gual legal texts from BLIS official site, sound
strategies for aligning the bitexts by utilizing the
numbering system in the legal texts, and neces-
sary XML annotation for the alignment results.
The value of the outcomes, i.e., the subparagraph-
aligned bilingual corpus, can be evaluated in
terms of the following aspects.
Corpus size The entire corpus is of 10.4M En-
glish words and 18.3M Chinese characters,
several times larger than the well-known
Penn Treebank Corpus in size.
Figure 5: Sample bitext in XML encoding
Translation quality All texts of the corpus are
prepared by the Law Drafting Division of
the Department of Justice, Hong Kong Gov-
ernment. Legal texts are known to be more
precise and less ambiguous than most other
types of text.
Specificity and comprehensiveness The corpus
covers specifically the domain of Hong Kong
legislation. It is the most authoritative and
complete text collection of the laws of Hong
Kong.
Alignment granularity The entire corpus is
aligned precisely to the subparagraph level.
Most subparagraphs in the legal texts are
phrases, fragments of a clause, or clauses; as
shown in Table 4.
76
Figure 6: Illustration of browsing modes
A bilingual corpus of this size and quality cov-
ering a specific domain so comprehensively is
particularly useful not only in empirical MT re-
search but also in computational studies of bilin-
gual terminology and legislation. Our future work
will focus on word alignment for inferring bilin-
gual lexical resources and on automatic recogni-
tion of legal terminology.
Also, our experience in constructing this bilin-
gual corpus has laid a foundation for us to con-
tinue to harvest more bilingual text materials from
the Web, e.g., from Hong Kong government?s
Web sites. We find that almost all Hong Kong
government web sites, which are in large num-
bers, maintain their Web pages consistently par-
allel in English and Chinese. We are not sure if
such bitexts in such pages are larger than that in
the BLIS site in volume. We do know they cover
a large number of distinct domains. This is partic-
ularly useful for MT. If we can harvest and align
the bitexts from such Web pages efficiently via
utilizing their intrinsic characteristics of URL cor-
respondence and text structure, it would not be a
dream any more to put an end to the time of hav-
ing too few existing translation materials for em-
pirical MT studies, at least, for the language pair
of Chinese and English.
Acknowledgements
The work described in this paper was supported
by the Research Grants Council of HKSAR,
China, through the CERG grants 9040861 and
9040482. We wish to thank our team members
for their help.
References
Anne H. Anderson, Miles Bader, Ellen G. Bard, Eliz-
abeth Boyle, Gwyneth Doherty, Simon Garrod,
Stephen Isard, Jacqueline Kowtko, Jan McAllis-
ter, Jim Miller, Catherine Sotillo, Henry Thompson,
and Regina Weinert. 1991. The HCRC map task
corpus. Language and Speech, 34(4):351?366.
Marco Baroni, Silvia Bernardini, Federica Comastri,
Lorenzo Piccioni, Alessandra Volpi, Guy Aston,
and Marco Mazzoleni. 2004. Introducing the La
Repubblica corpus: A large, annotated, TEI(XML)-
compliant corpus of newspaper Italian. In LREC
2004, pp. 1771-1774.
Simon P. Botley, Anthony M. McEnery, and Andrew
Wilson (eds.). 2000. Multilingual Corpora in
Teaching and Research. Amsterdam: Rodopi.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Michael Carl and Andy Way (eds.). 2003. Recent
Advances in Example-based Machine Translation.
Dordrecht: Kluwer.
Jiang Chen and Jian Y. Nie. 2000. Parallel Web text
mining for cross-language information retrieval. In
RIAO?2000, pp. 62?77. Paris.
Mark Davis and Ted Dunning. 1995. A TREC evalu-
ation of query translation methods for multi-lingual
text retrieval. In TREC-4, pp. 483?498. NIST.
Jarle Ebeling. 1998. Contrastive linguistics, transla-
tion, and parallel corpora. In Meta, 43(4):602?615.
William A. Gale and Kenneth W. Church. 1991. Iden-
tifying word correspondences in parallel texts. In
Fourth DARPA Workshop on Speech and Natural
Language, pp. 152?157. Asilomar, California.
William A. Gale and Kenneth W. Church. 1991b. A
Program for Aligning Sentences in Bilingual Cor-
pora. In ACL?91, pp. 177?184. Berkeley.
Nancy Ide, Patrice Bonhomme, and Laurent Romary.
2000. XCES: an XML-based encoding standard
for linguistic corpora. In LREC2000, pp. 825?830.
Athens, Greece.
77
Nancy Ide and Catherine Macleod. 2001. The Amer-
ican National Corpus: A Standardized Resource of
American English. Proceedings of Corpus Linguis-
tics 2001, Lancaster UK.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the Special Issue on the Web as Cor-
pus. Computational Linguistics, 29(3):333?347.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, Hideki
Mima and Jun?ichi Tsujii. 2001. XML-based lin-
guistic annotation of corpus. In NLPXML-1, pp. 47?
54. Tokyo.
I. Dan Melamed. 1997. Automatic discovery of
non-compositional compounds in parallel data. In
EMNLP?97, pp. 97?108. Brown University, Au-
gust.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Andreas Mengel and Wolfgang Lezius. 2000. An
XML-based representation format for syntactically
annotated corpora. In LREC2000, Volume 1,
pp. 121?126. Athens, Greece.
Makoto Nagao. 1984. A framework of a mechanical
translation between Japanese and English by anal-
ogy principle. Artificial and Human Intelligence,
pp. 173?180. Amsterdam: North-Holland.
Jian Y. Nie and Jian Cai. 2001. Filtering noisy paral-
lel corpora of Web pages. In IEEE Symposium on
Natural Language Processing and Knowledge En-
gineering, pp. 453?458. Tucson, AZ.
Jian Y. Nie and Jiang Chen. 2002. Exploiting the
Web as Parallel Corpora for Cross-Language Infor-
mation Retrieval. Web Intelligence, pp. 218?239.
Philip Resnik, Mari B. Olse, and Mona Diab. 1999.
The Bible as a parallel corpus: Annotating the
?Book of 2000 Tongues?. Computers and the Hu-
manities, 33(1-2):129?153.
Philip Resnik. 1999b. Mining the Web for Bilingual
Text. In ACL?99, pp. 527?534. Maryland.
Philip Resnik and Noah A. Smith. 2003. The Web
as a Parallel Corpus. Computational Linguistics,
29(3):349?380.
Raphael Salkie. 1995. INTERSECT: a parallel cor-
pus project at Brighton University. Computers and
Texts 9 (May 1995), pp. 4?5.
Jean Veronis. 2000. Parallel Text Processing. Dor-
drecht: Kluwer.
Andy Way and Nano Gough. 2003. wEBMT:
Developing and validating an example-based ma-
chine translation system using the World Wide Web.
Computational Linguistics, 29(3):421?457.
Dekai Wu. 1994. Aligning a parallel English-Chinese
corpus statistically with lexical criteria. In ACL?94,
pp. 80?87. Las Cruces, New Mexico, U.S.A.
78
 Some Considerations on Guidelines for  
Bilingual Alignment and Terminology Extraction 
 
Lawrence Cheung, Tom Lai, Robert Luk?, Oi Yee Kwong, King Kui Sin, Benjamin K. Tsou 
 
Language Information Sciences Research Centre 
City University of Hong Kong 
Tat Chee Avenue, Kowloon, Hong Kong  
{rlylc, cttomlai, rlolivia, ctsinkk, rlbtsou}@cityu.edu.hk  
?Department of Computing 
Hong Kong Polytechnic University  
Hung Hom, Kowloon, Hong Kong 
csrluk@comp.polyu.edu.hk 
 
Abstract  
Despite progress in the development of 
computational means, human input is still 
critical in the production of consistent and 
useable aligned corpora and term banks. This 
is especially true for specialized corpora and 
term banks whose end-users are often 
professionals with very stringent 
requirements for accuracy, consistency and 
coverage. In the compilation of a high quality 
Chinese-English legal glossary for ELDoS 
project, we have identified a number of issues 
that make the role human input critical for 
term alignment and extraction. They include 
the identification of low frequency terms, 
paraphrastic expressions, discontinuous units, 
and maintaining consistent term granularity, 
etc. Although manual intervention can more 
satisfactorily address these issues, steps must 
also be taken to address intra- and 
inter-annotator inconsistency.  
 
Keyword: legal terminology, bilingual 
terminology, bilingual alignment, 
corpus-based linguistics 
1. Introduction 
Multilingual terminology is an important 
language resource for a range of natural language 
processing tasks such as machine translation and 
cross-lingual information retrieval. The 
compilation of multilingual terminology is often 
time-consuming and involves much manual 
labour to be of practical use. Aligning texts of 
typologically different languages such as Chinese 
and English is even more challenging because of 
the significant differences in lexicon, syntax, 
semantics and styles. The discussion in the paper 
is based on issues arising from the extraction of 
bilingual legal terms from aligned 
Chinese-English legal corpus in the 
implementation of a bilingual a text retrieval 
system for the Judiciary of the Hong Kong Special 
Administrative Region (HKSAR) Government.  
 Much attention in computational 
terminology has been directed to the development 
of algorithms for extraction from parallel texts. 
For example, Chinese-English (Wu and Xia 1995), 
Swedish-English-Polish (Borin 2000), and 
Chinese-Korean (Huang and Choi 2000). Despite 
considerable progress, bilingual terminology so 
generated is often not ready for immediate and 
practical use. Machine extraction is often the first 
step of terminology extraction and must be used in 
conjunction with rigorous and well-managed 
manual efforts which are critical for the 
production of consistent and useable multilingual 
terminology. However, there has been relatively 
little discussion on the significance of human 
intervention. The process is far from being 
straightforward because of the different purposes 
of alignment, the requirements of target users and 
the corpus type. Indeed, there remain many 
problematical issues that will not be easy to be 
resolved satisfactorily by computational means in 
the near future, especially when typologically 
different languages are involved, and must require 
considerable manual intervention. Unfortunately, 
such critical manual input has often been treated as 
an obscure process. As with other human cognitive 
process (T?sou et al 1998), manual terminology 
markup is not a straightforward task and many 
issues deserve closer investigation. 
 In this paper, we will present some 
significant issues for Chinese-English alignment 
 and term extraction for the construction of a 
bilingual legal glossary. Section 2 describes the 
background of the associated bilingual alignment 
project. Section 3 discusses the necessity of 
manual input in bilingual alignment, and some 
principles adopted in the project to address these 
issues. Section 4 provides an outline for further 
works to improve terminology management, 
followed by a conclusion in Section 5. 
2. High Quality Terminology Alignment 
and Extraction 
2.1 Bilingual Legal Terminology in Hong 
Kong 
The implementation of a bilingual legal system in 
Hong Kong as a result of the return of 
sovereignty to China in 1997 has given rise to a 
need for the creation and standardization of 
Chinese legal terminology of the Common Law 
on par with the English one. The standardization 
of legal terminology will not only facilitate the 
mandated wider use of Chinese among legal 
professionals in various legal practices such as 
trials and production of legal documentation 
involving bilingual laws and judgments, but also 
promote greater consistency of semantic 
reference of terminology to minimize ambiguity 
and to avoid confusion of interpretation in legal 
argumentation.  
 In the early 90?s, Hong Kong law drafters 
and legal translation experts undertook the 
unprecedented task of translating Hong Kong 
Laws, which are based on the Common Law 
system, from English into Chinese. In the 
process, many new Chinese legal terms for the 
Common Law were introduced. On this basis, an 
English-Chinese Glossary of legal terms and a 
Chinese-English Glossary were published in 1995 
and 1999 respectively. The legal terminology was 
vetted by the high level Bilingual Laws Advisory 
Committee (BLAC) of Hong Kong. The 
glossaries which contain about 30,000 basic 
entries have become an important reference for 
Chinese legal terms in Hong Kong. The Bilingual 
Legal Information System (BLIS) developed by 
the Department of Justice, HKSAR provides 
simple keyword search for the glossaries and 
laws that are available in both Chinese and 
English. Nevertheless, the glossaries are far from 
being adequate for many different types of legal 
documentation, e.g. contracts, court judgments, 
etc. One major limitation of the BLIS glossary is 
its restricted coverage of legal terminology in the 
Laws of Hong Kong, within a basically 
prescriptive context as when the laws were studied 
at the time of its promulgation. There are other 
important bilingual references (Li and Poon 1998, 
Yiu and Au-Yeung 1992, Yiu and Cheung 1996) 
which focus more on the translation of Common 
Law concepts. These are almost exclusively 
nominal expressions. 
 In 2000, the City University of Hong 
Kong, in cooperation with the Judiciary, HKSAR, 
initiated a research project to develop a bilingual 
text retrieval system, Electronic Legal 
Documentation/Corpus System (ELDoS), which is 
supported by a bilingually aligned corpus of 
judgments. The purpose of the on-going project is 
twofold. First, the aligned legal corpus enables the 
retrieval of legal terms used in authentic contexts 
where the essence and spirit of the laws are tested 
(and contested) in reality, explicated and 
elaborated on, as an integral part of the evolving 
and defining body of important precedent cases 
unique to the Common Law tradition. Second, the 
corpus covers judgment texts involving 
interpretation of different language styles and 
vocabulary from Hong Kong laws. The alignment 
markup also serves as the basis for the compilation 
of a high-quality bilingual legal term bank. To 
complete the task within the tight timeframe, a 
team of annotators highly trained in law and 
language are involved in alignment markup and 
related editing. 
2.2 Need for Human Input 
The legal professionals which are the target users 
of ELDoS have very stringent demands on 
terminology in terms of accuracy, coverage and 
consistency. Aligned texts and extracted terms 
must therefore be carefully and thoroughly 
verified manually to minimize errors. 
Furthermore, many studies on terminology 
alignment and extraction deal predominantly with 
nominal expressions. Since the project aims to 
provide comprehensive information on the 
manifestations of legal vocabulary in Chinese and 
English texts, the retrieval system should not 
restrict users to nominal expressions but should 
also provide reference to many other phenomena 
such as alternation of part-of-speech (POS) (e.g. 
noun-verb alternation) inherent in bilingual texts, 
as will be seen in Section 3.  
 The availability of bilingual corpora has 
made it possible to construct representative term 
 banks. Nonetheless, current alignment and term 
extraction technology are still considered 
insufficient to meet the requirements for high 
quality terminology extraction. In ELDoS project, 
many issues are difficult to be handled 
satisfactorily by the computer in the foreseeable 
future. Although human input is essential for high 
quality term bank construction, the practice of 
manual intervention is not straightforward. 
Indeed, the manual efforts to correct the errors 
can be substantial, and the associated cost should 
not be underestimated. The annotator must first 
go through the entire texts to spot the errors and 
terms left out by the machines. In this process, 
both the source and target materials have to be 
consulted. The annotator must also ensure the 
consistency of the output. As a result, guidelines 
should be set up to streamline the process. 
3. Aspects of Terminology Alignment 
The approach adopted for the manual annotation 
of alignment markup and the maintenance of term 
bank in the ELDoS project will be described. 
Additional caution has been taken in the 
coordination of a team of annotators.  
3.1 Term Frequency 
An important reason for manual intervention in 
bilingual term alignment is the relatively poor 
recall rate for low frequency terms. Many 
extraction algorithms make use of statistical 
techniques to identify multi-word strings that 
frequently co-occur (Wu and Xia 1995; Kwong 
and Tsou 2001). These methods are less effective 
for locating low frequency terms. Of the 16,000 
terms extracted from ELDoS bilingual corpora, 
about 62% occur only once in about 80 
judgments. For high quality alignment and 
extraction, failure to include these low frequency 
terms would be totally unacceptable.  
3.2 Correspondence of Aligned Units 
Because of the different grammatical requirement 
and language style, a term in the source language 
often differs in different ways from the 
corresponding manifestations in the target 
language. These differences could be alternation 
of POS and the use of paraphrastic expressions. 
Although many term banks avoid such variations 
and focus primarily on equivalent nominals or 
verbs, the correspondence of terms between two 
typologically different languages is often more 
complicated. For example, the English nominal 
(?fulfilment?) is more naturally translated into 
Chinese as a verb (?l??, ?????, ????). 
More examples can be found in Table 1. 
 
Alternation of POS  
English  Chinese POS alternation 
The accused 
o+ 
det + adj ~ noun 
hold 
*? 
verb ~ noun 
fulfillment  
?
 
noun ~ verb 
administration  
?D 
noun ~ verb 
repudiation  
l? 
noun ~ neg + verb 
Table 1. Alternation of POS  
 
In some cases, there are simply no equivalent 
words in the target language. Paraphrasing or 
circumlocution may be necessary. Such 
correspondence is far less consistent and obvious 
to be identified by the computer.  
 
Paraphrasing/Circumlocution 
English Chinese 
The judge entered judgment in 
favour of the respondents in 
respect of their claim for arrears 
of wages, and severance payment. 
t?o31
?2??KDI
??9? 
In our view,? z??a?
 
?evidenced by the Defendant's 
letter ? 
?7:+3?}
???1$Ym
??S??? 
Table 2.  Examples of paraphrasing 
 
Because of language differences, legal terms can 
be contextually realized as anaphors in the target 
language. Examples of such correspondence 
would be useful for legal drafting and translation. 
Again, such anaphoric relations are more 
accurately handled by humans. 
 
Anaphoric Relation 
English Chinese 
He was subsequently 
charged?  
9?3??s? 
Liu JA dealt with that 
application on 14 March 
1996 and dismissed it. 
B9?t?W?
?1996?3?14?A?
 ?-??9?? 
Enforcement of a 
Convention award may 
also be refused if the 
award is in respect of a 
matter which is not capable 
of settlement by arbitration.
???*????1
??"lh?X*
?????M???
N+