Creating a Universal Networking Language Module 
within an Advanced NLP System 
Igor BOGUSLAVSKY, Nadezhda FRII), l,eonid IOM1)IN, Leonid KREII)LIN, hina SAGALOVA, 
Victor SIZOV 
Coml)utational l~inguistics Laboratory 
Institute for Information Transnlissioll Problems of lho P, ussian Academy of Sciences 
19 Bol'shcti Karetnyj, 101447 Moscow, Russia 
J )~nadya  ion ld in , leny{~lova~)~) . ru  
Abstract 
A multifunctional NIA 9 environment, 
\[!'I'AI~-3, is presented. The environment has 
several NI,I ~ applications, inchtding a machine 
translation system, a natural language interface 
to SQI, type databases, synonymous 
l~araphrasing of sentences, syntactic error 
correction module, and a computer-assisted 
language learning tool. Ihnphasis is laid on a 
new naodtile of tile processor responsible for tlio 
intorl\]lcc with the Universal Networking 
l A.lllgtlagC, il roCOlll plodtlcl by the UN 
Universily inlended for the facilitation of 
nnlltihlnguage, multiethnic access 1o 
communication etworks such as WWW. The 
UNL module of ETAP-3 naturally combines the 
two major al)proaches accepted in machine 
translation: the lransfer-based approach and the 
interlingua pl)roach. 
1. In t roductory  Renmrks  
ETAP-3 is a multilmrposc NIA ~ environmmlt 
that was conceived in the 1980s and has been 
worked out in the Institute for lnl~mnation 
Transmission Problems, Russian Academy of 
Sciences (Apresian et al 1992, l?,oguslavsky 
1995). The theoretical foundation of ETAP-3 is 
tile Meaning ?=> Text linguistic theory by Igor' 
Mel'6uk and the Integral Theory of Language by 
Jurij Apresian. 
Eq'AP-3 is a non-comlnercial environment 
primarily oriented at linguistic research rather 
than creating a marketable software product. The 
main focus of the research carried ()tit with 
I';TAP-3 is COlnputational modelling of natural 
languages. This attitude explains our effort to 
develop the models in a way as linguistically 
sound as possible. We strive to incorporate into the 
system much linguistic knowledge irrespective of 
whether this knowledge is essential for better text 
processing (e.g. machine translation) or not. In 
particular, we want our parser to produce what wc 
consider a correct syntactic representation of tim 
sentence - first of all because we believe that this 
interpretation is a true fact about tile natural 
language. We have had inany occasions to set that 
in the long run the iheorctical soundness and 
completeness of linguistic knowledge incorporated 
in an NIA ) application will pay. 
All NLP applications in F, TAP-3 are largely 
based on an original system of three-wdue logic 
and use an original formal language of linguistic 
descriptions, I~'Oi>&;T. 
2. ETAP-3 :  Modules,  Features,  Des ign ,  
hnplementat io l l  
2.11 ETAP-3  Modules  
The m~tjor NI,P modules of ETAP-3 are as 
lollows: 
? High Quality Machine Translation System 
? Natural Language Interface to SQL Type 
Databases 
? System of Synonymous Paraphrasing of 
Sentences 
? Syntactic Error Correction Tool 
? Computer-Aided Language LeamingTool 
? Tree \]?,ank Workbencll 
Another module, a new UNL converter 
responsible lot the interface with the Universal 
Networking Language, a recent product designed 
' Tile research reported here was in part supl)oricd by a grant (No 99-06-80277) fronl tile Russian Foundation 12)1" 
Fundamental Research, whose assistance isgratefully acknowledged. 
83 
by the UN University, is discussed in detail in 
Section 3. 
2.1.1. ETAP-3 MT System 
The most important module of ETAP-3 is 
the MT system that serves five language pairs: 
(1) English-Russian, (2) Russian - English, (3) 
Russian - Korean. (4) Russian - French, and (5) 
Russian - German. 
By far the most advanced are the first two 
of these pairs. The system disposes of 50,000- 
strong so-called combinatorial dictionaries of 
Russian and English that contain syntactic, 
derivational, semantic, subcategorization, and 
collocational information. The system relies on 
comprehensive grammars of the two languages. 
For the other language pairs smaller scale 
prototypes are available. 
ETAP-3 is able to present multiple 
translations when it encounters an ambiguity it 
cannot resolve. By default, the system produces 
one parse and one translation that it considers 
the most probable. If the user opts for multiple 
translation, the system remembers the 
unresolved ambiguities and provides all 
mutuany compatible parses and lexical choices. 
To give one example from the real output: the 
sentence They made a general remark that . . . .  
when submitted to the multiple translation 
option, yielded two Russian translations that 
correspond to radically different syntactic 
structures and lexical interpretations: (a) Oni 
sdelali obshchee zameehanie, chto... (= They 
made some comn-lon renlark that ...) and (b) Oni 
vynudili generala otmetit; chto... (= They forced 
some general to remark that ...). 
2.1.2. Natural Language Interface to SQL Type 
Databases 
This ETAP-3 module translates freely 
worded human queries to a database from 
Russian or English into SQL expressions. It can 
also produce the reverse generation of a NL 
query from an SQL expression. 
2.1.3. System of Synonymous Paraphrasing 
The module is designed for linguistic 
experiments in obtaining nmltiple meaning- 
retaining paraphrases of Russian and English 
sentences. The paraphrasing is based on the 
concept of lexical functions, one of the 
important innovations of the Meaning ?=> Text 
theory. The following example shows the kind 
of paraphrases that can be produced by the 
module: 
(1) The director ordered John to write a report - 
The director gave John an order to write a report 
- John was ordered by the director to write a 
report - John received an orcler fonn the director 
to write a report. 
It is a very promising direction of linguistic 
research and developlnent that can be applied in a 
wide range of activities, including language 
learning and acquisition, authoring, and text 
planning. Besides that, lexical functions are used 
for ensuring adequate lexical choice in machine 
translation and in the UNL module. 
2.1.4. Syntactic Error Correction Tool 
The module operates with Russian texts in 
which it finds a wide range of errors in 
grammatical agreement as well as case 
subcategorization a d offers the user the correct 
version. 
2.1.5. Computer-Aided Language Learning Tool 
The module is a standalone software 
application constructed as a dialogue type 
computer galne intended for advanced students of 
Russian, English, and German as foreign 
languages who wish to enrich their vocabulary, 
especially to master the collocations of these 
natural languages and their periphrastic abilities. 
The tool relies on the apparatus of lexical 
limctions. It can also be used native speakers of the 
three languages interested in increasing their 
command of the vocabulary (such as journalists, 
school teachers, or politicians). 
2.1.6. Tree Bank Workbench 
This is the module that utilizes the ETAP-3 
dictionaries, its morphological analyzer and the 
parser to produce a first-ever syntactically tagged 
corpus of Russian texts. It is a mixed type 
application that combines automatic parsing with 
human post-editing of tree structure. 
2.2. Major Features 
The following ate the most important features of 
the whole ETAP-3 environment and its modules: 
* Rule-Based Approach 
? Stratificational Approach 
? Transfer Approach 
? Syntactic Dependencies 
? Lexicalistic Approach 
? Multiple Translation 
? Maximum Reusabilty of Linguistic Resources 
84 
Ill tile current version of ETAP-3, its 
modules that process NL senteuces are strictly 
rule-based. However, ill a series of recent 
experiments, tile MT module was supplenlenled 
by all example-based component of a translation 
menlory type and a statistical component hat 
provides semiautonmtic extraction of translatiou 
equivalents t?om bilingual text corpora (see 
lomdin & Streiter 1999). 
ETAP-3 shares its stratificational feature 
with many other NLP systems. It is at tile level 
of tile normalized, or deep syntactic, structure 
that tile transfer flom tile source to tile target 
language takes place in MT. 
ETAP-3 makes use of syntactic dependency 
trees for sentence structure representation 
instead of constituent, or phrase, structure. 
Tile ETAP-3 system takes a lexicalistic 
stand ill tile sense that lexical data are 
considmed as important as gl'ammar 
infornlation. A dictionary entry contains, in 
addition to tile lemma name, information on 
syntactic and semantic features of tile word, its 
subcategorization flame, a default translation, 
and rules of various types, and wdues of lexical 
functions for which tile lemma is tile keyword. 
The word's syntactic t'eatures characterize its 
ability/nou-ability to participate ill specific 
syntactic onstructions. A word can have several 
syntactic features elected from a total of more 
than 200 items. Semantic features arc needed to 
check tile semantic agreement between the 
words ill a sentence. Tile subeategorization 
frame shows the surface marking of tile word's 
arguments (in terms of case, prepositions, 
conjtmctions, etc.). Rules are an essential part of 
the dictionary entry. All the rules operating in 
ETAP-3 are distributed betwecn tile granmmr 
and tile dictionary. Grammar rules me more 
general and apply lo large classes of words, 
whereas tile rules listed or simply referred to in 
the dictionary are restricted ill their scope and 
only apply to small classes of words or even 
individual words. This organization of tile rules 
ensures the self-tuning of tile system to tile 
processing of each particular senteuce. In 
processing a sentence, only those dictionary 
rules are actiwlted that are explicitly referred to 
ill the dictionary entries of tile words making up 
tile sentence. A sample dictionary enlry 
fl'agment for tile English noun chance illustrates 
what was said above: 
\[1\] CHANCE1 
\[21 POR:S 
\[3\] SYNT:COUNT,PREDTO,PREDTHAT 
\[4\] DES:'FACT','ABSTRACT' 
\[5\] D1.1 :OF,'PERSON' 
\[6\] D2.1 :OF,'FACT' 
\[7\] D2.2:TO2 
\[8\] D2.3:THAT1 
\[9\] SYN 1 :OPPORTUNITY 
\[10\] MAG N: GOOD 1~FAIR 1~EXCELLENT 
\[11\] ANTIMAGN: SLIGHT/SLIM/POOR/LITTLE1/ 
SMALL 
\[12\] OPER1 :HA VE/STAND1 
\[13\] REAL1-M: TAKE 
\[14\] ANTIREAL1 -M:MISSl 
\[15\] INCEPOPER1 :GET 
\[16\] FINOPER1 :LOSE 
\[17\] CAUSFUNC1 :GIVE<TOI>/GIVE 
\[18\] ZONE:R 
\[19\] TRANS:SHANS/SLUCHAJ 
\[20\] REG:TRADUCT2.00 
\[21\] TAKE:X 
\[22\] LOC:R 
\[23\] R:COMPOS/MODIF/POSSES 
\[24\] CHECK 
\[25\] 1.1 DEP-LEXA(X,Z,PREPOS,BY1) 
\[26\] N:01 
\[27\] CHECK 
\[2811.1 DOM(X,*,R) 
\[29\] DO 
\[30\] 1 ZAMRUZ:Z(PO1) 
\[31\] 2 ZAMRUZ:X(SLUCFtAJNOST') 
\[32\] N:02 
\[33\] CHECK 
\[34\] 2.1 DOM(X,*,*) 
\[351 DO 
\[36\] I ZAMRUZ:Z(SLUCHAJNO) 
\[37\] 2 STERUZ:X 
\[38\] TRAF:RA-EXPANS.16 
\[39\] LA:THAT1 
\[40\] TRAF:RA-EXPANS.22 
lane \[12\] - part of speech: a noun. 
Line 113\] - tile list of syntactic features. 
Lille \[4\] - tile list of senmntic features. 
Lines \[5\] - \[8\] - tile subcategorization fiame. 
Lines \[9\] - \[17\] - the list of lexical functions used 
to describe restricted lexical co-occurrence. 
Lille \[18\] - marks the end of the application- 
independent infomlation and beginning of the 
information used in tile English-Russian 
translation. 
Lille \[ 19\] - default ranslation into Russian. 
lanes \[20\] - \[37\] - a rule for translating tile phrase 
t) 3 , chance in different contexts. 
Lines \[38\] - \[39\] - a reference to the rule which 
introduces a semantically empty conjunction (that: 
a chance that we obtain a grant) .  
Line \[40\] - a reference to the rule which 
introduces particle to (a chance to win). 
85 
2.3. General  Architecture of the ETAP-3 
environment. 
To give a general idea of how the ETAP-3 NLP 
operates, we show here the layout of the MT 
module (Fig. I). In a way, all the other modules 
can be viewed as this module's deriwttives. 
OBJECTS STAGES 
Source son\[ollCO 
MorphS source 
SyntS source 
NormS source 
NorlnS target 
SyntS target 
MorphS target 
Target sentence 1 
Morphological nalysis 
Pal'sing 
Norlnalization 
Transfer 
Expansioll 
Syntactic synthesis 
Morphological synthesis 
Fig.1 
DICTIONARIES 
SOUI'CO 
morplaological 
dicl:ionarv 
SOllI'CO 
Combinatorial 
Dictionary 
.~ t~ Target 
Colnbinatorial 
~ I DictionarYTarget 
-~ lnorphological 
dictionary 
2.4. hnplementation 
The ETAP-3 environment has been implemented 
on a PC under Windows NT 4.0 environment. 
The environment has a number of auxiliary 
tools, including a sophisticated lexicographer's 
toolkit that allows the developers and the users 
to effectively maintain and update the ETAP-3 
dictionaries. 
3. The UNL Interface 
3.1 Aims and scenario 
The UNL project has a very ambitious goal: 
to break down or at least to drastically lower the 
language barrier for the Internet users. With time 
and space limitations already overcome, the 
Internet community is still separated by 
language boundaries. Theoretically, this seems 
to be the only major obstacle standing in the way 
of international and interpersonal 
communication i  the information society. This 
is why the problem of the language barrier on 
the Interact is perceived as one of the global 
problems of mankind, and a project aiming to 
solve this problem has been initiated under the 
UN auspices - by the Institute of Advanced 
Studies of the United Nations University. 
Started in 1996, the project curremly 
embraces 15 universities and research 
institutions fiom Brazil, China, Egypt, France, 
Germany, India, Indonesia, Italy, Japan, Jordan, 
Latvia, Mongolia, Russia, Spain, and Thailand. 
In the following years more groups are expected 
to join, so that in the long run all languages of 
the UN member states will be covered. 
The idea of the project is as follows. An 
interlingua has been developed which has 
sufficient expressive power to represent 
relevant information conveyed by natural 
languages. This interlingua entitled Universal 
Networking Language (UNL) has been 
proposed by H. Uchida (UNU/IAS). For each 
natural language, two systems should be 
developed: a "deconverter" capable of 
translating texts from UNL to this NL, and an 
"enconverter" which has to convert NL texts 
86 
into UNL. it sholtld be emphasized that the 
procedure of producing a UNL text ix not 
supposed to be fully autolnatic. It will be an 
interactive process with the labor divided 
between the COlnputer and a human expert 
("writer") in UNI+. 
This paradigm makes UNL radically 
different from conventional machine lranslation. 
Duo to the interactive oncoilversion, the UNL 
expression, which serves as inpul for generation, 
can be nlado as good as Clio wishes. The UNL 
writer will edit the rough result proposed by the 
OllConvorlor, corfect its errors, eliminate the 
renlaining ambiguities. He/she can run a 
deconvorior of his own language to lest the 
wtlidity of the UNL expression obtained alld 
then refine it again tin one is fully satisfied with 
the l'inal result. 
Anolhor ilnl)oriant distinction l'roill MT 
systonis is thai lhe inlorlirigua roprosenhilion of 
texts will be created and stored irrespectively of 
ils goiloration into particular languages. UNL 
Call be soon as all independent i-ileal-iS of iYloanillg 
ropreselllation. UNL doctlmonts Call 13e 
processed by indexing, retrieval and knowledge 
extraction tools without being converted to 
llattll'al lallguages. Gellcration \viii only be 
needed when the document has roached the 
htllll;_lll HSOl +. 
A doconvoftor and an enconvoi'tor for each 
lliligtlagC form ii IAlnguago Server residing in the 
hilernot. All language scrvolS will be cotlnoclod 
in the IJNL network. They \viii allow ally 
IlliOiTiOt user to doconvorl a UNI, docunleili 
found on the web into his/her native language, as 
well as to produce UNI, represelltatiOllS of the 
texts he/she wishes to nlako available to 
inultiethnic public. 
3.2 UNL language 
We cannot describe the UNL language here 
in all details: this topic deserves a special paper 
which will hopefully be written by the author of 
the language design - l)r. Hiroshi Uchida. We 
will only characterize it to the extent necessary 
for the description of our deconversion module. 
Full specification of UNL can be found at 
/lllp://WWW. tml. tax. ttnu. edu/. 
UNL is a comlmter language intended to 
represent infolmation in a way that allows to 
generate a text expressing this information in a 
very large number of nahtral anguages. A UNL 
expression is an oriented hyper-graph that 
corresponds to a NL sentence in the amount of 
information conveyed. The arcs of t11o graph are 
interpreted as senmntic relations of the type 
agent, ob.ject, lime, place, inslrttment, manlier, 
etc. The nodes of the graph are special units, the 
so-called Universal Words (UW) interpreted as 
concepts, or groups of UWs. The nodes can be 
supplied with attributes which provide 
additional information on their use in lhc given 
sentence, e.g. @imperative, @generic, @future, 
@obligation. 
Each UW is represented as an t~,nglish 
word that can be optionally supplied with 
semantic specifications to restrict its meaning. 
In most cases, these specifications locate the 
concept in the knowledge base. It is done in the 
following way: UW A(icl>B) ix interpreted as 
'A is subsumed under the category B'. For 
example, the UW coach used without any 
restrictions denotes anything the English coach 
can denote, ll' eric wants to be more precise, one 
can use restrictions: coach(icl>transl)ort ) 
denotes a bus, coaclz (icl>lmman) denotes a 
trainer and coach (icl>do) denotes the action of 
training, in a sense, the apparatus of restrictions 
allows to represent UWs as disambiguated 
l';nglish words. On ltle other hand, restrictions 
allow to denote concepts which are absent in 
I~;nglish. For cxmnple, in Russian there is a large 
group of motion words, whose meaning 
incorporates the idea of the mode of locomotion 
or tral/sportation: priletel' 'come by flying', 
prO@,/' 'come by ship', l)ril)olzti 'come by 
crawling', l)ril)eJlal ' 'come running', elc. 
l!nglish has no neutral words to denote these 
concepts. Still, on the basis of English one can 
constrttct lJWs that approximate required 
concepts, e.g. conw(met>shil) ) is interpreted as 
'come and the method o1' coming ix a ship'. 
IIere is an example of a UNL expression 
for the sentence 
(2) Howevel, hmgua,q,e dll/ferences are a barrier 
to the smoot/L/low of in.fomnation in our society. 
l';ach line is an expression of the kind 
rehttion(UWl, UW2). For simplicity, UWs are 
not supplied with restrictions. 
aoj (barrier. @entry. @present. @indef. @however, 
difference. @pl) 
rood(barrier. @entry. @present. @indef. @ however, 
Ilow. @dcl) 
mod(differencc.@pl, language) 
aoj(smoofli, flow. @del) 
meal(flow. @def, in fol'nmtion) 
scn(fk+w. @dcl, society) 
pos(society, we) 
P, ehttions used: ao i a relation that holds 
between a thing and its state, nmd - a relation 
87 
between a thing and its modifier, scn-  a relation 
between an event or a state and its abstract 
location, pos - a relation between a thing and its 
possessor. Attributes: @entry - denotes the top 
node of the structure, @present - present ense, 
@def - definite NP, @pl - plural, @however - a 
modal meaning corresponding to English 
however. 
3.3. UNL  - Russ ian  deeonvers ion  by  
means  o f  ETAP-3  
As was shown in Section 1, ETAP-3 is a 
transfer-based system where the transfer is 
carried out at the level of the Normalized 
Syntactic Structure (NormSS). This level is best 
suited for establishing correspondence with 
UNL, as UNL expressions and NormSS show 
striking similarities. The most important of theln 
are as follows: 
1. Both UNL expressions and NormSSs 
occupy an intermediate position between the 
surface and the semantic levels of 
representation. They roughly correspond to 
the so-called deep-syntactic level. At this 
level the meaning of the lexical items is not 
decomposed into the primitives, and the 
relations between the lexical items are 
language independent; 
2. The nodes of both UNL expressions and 
NormSSs are terminal elements (lexical 
items) and not syntactic ategories; 
3. The nodes carry additional characteristics 
(attributes); 
4. The arcs of both structures are non- 
symmetrical dependencies. 
At the same time, UNL expressions and 
NormSSs differ in several important respects: 
1. All the nodes of NormSSs are lexical items, 
while a node of a UNL expression can be a sub- 
graph; 
2. Nodes of a NormSS always correspond to one 
word sense, while UWs may either be broader or 
narrower than the corresponding English words: 
2.1. they can cover a meaning area that 
corresponds to several different word senses at a 
time (see above); 
2.2. they can correspond to a fi'ee word 
combination (e.g. computer-based or high- 
quality); 
2.3. they can correspond to a word form 
(e.g. best which a form of good or well); 
2.4. they can denote a concept that has no 
direct correspondence in English (see above). 
3. A NormSS is the simplest of all connected 
graphs - a tree, while a UNL expression is a 
hyper-graph. Its arcs may form a loop and 
connect sub-graphs; 
4. The relations between the nodes in a NormSS 
are purely syntactic and are not supposed to 
convey a meaning of their own, while the UNL 
relations denote semantic roles; 
5. Attributes of a NormSS mostly correspond to 
grammatical elements, while UNL attributes 
often convey a meaning that is expressed both 
in English and in Russian by means of lexical 
items (e.g. modals); 
6. A NormSS contains information on the word 
order, while a UNL expression does not say 
anything to this effect. 
The NormSS of tile sentence (2) looks as 
follows: 
be ,  present 
I~' prcdic "",ik 
however difference, pl barrier, i,l~t~l 
compos ~ l-compt 
f low,  def 
lanvuave / I \attrib ll\]~-Colnp\]~ 
smooth information in 
prepos~ 
society 
Fig. 2 modif~ 
our  
As UNL makes use of English lexical labels, it 
is expedient to bridge the gap between UNL and 
Russian via English NormSS which actually 
serves as an Intermediate Representation (IR). 
in this case tile UNL - Russian interface will be 
the simplest. After the English NormSS has 
been reached, conventional ETAP English-to- 
Russian machine translation mode of operation 
can be used. 
The UNL-to-Russian module carries out the 
following three steps: 
1. Transfer from UNL to the intermediate 
representation (IR). 
88 
2. Transfer fronl tile IR to tile Russian 
ilOllnalized syntactic structure (NorlriSS-1)@ 
3. (\]eneration of a P, ussian sentence from the 
NornlSS-R.  
Tile archilecture of tile UNL-Russian 
deconverter is shown in Fig. 3. 
It follows fi'om tile previous discussion that the 
UNL - NormSS interface should solve the 
following five tasks: 
1. An appropriate English lexeme for every 
UW should be selected where it is possible; 
a Russian lexeme will be provided by tile 
ETAP English - Russian transfer dictionary. 
If no appropriate English word can be found 
for a UW, other means of expression should 
be found. 
2. UNL syntactic relations should be 
tl-anslated, either by means of I~q'AP 
relations or widl tile help of lexical items. 
3. UNL attributes hould be translated, either 
by lneaus of granunatical features or with 
the help of lexical items (e.g. @however -
however). 
4. UNL graph should be converted in a tree. 
5. Word order should be established. 
The first aild (parlly) the second tasks are 
soh, ed by uleaus Of the infornlatiou stored in the 
UW English and English conlbinalorial 
dictionaries. All lhe rest (tasks 2 io 5) is done by 
the rules written in the logical-based I~'OP, tZT 
formalism. 
Let us give one example lo ilhlstrate the 
transformation f UNL relations into NL words. 
UNL has a tim relation that holds between an 
event and its linle. As is known, lhe choice of 
approl)riaie words to express lhis relation is to a 
largo oxleni doterilliried by lexical properties of 
tile word denoting tilne; cf. oz._It Moll(lay, at 
midnight, idAl summe#; rhtri, e~ the it,at; etc. In 
ETAP-3 all these cases are treated as tile lexical 
function LOC denoting (tenlporal) locality (on 
lexical functions see 2.1.3). Tile values of all 
lexical fimctions are given in the lexicon in the 
entries of their arguments ( ee an example in 2.2 
above). While processing tile UNL expression, 
the tim relation is linked to the lexical ftluclioll 
LOC which allows to l'iud a correct preposition, 
both in English and in Russian. 
3.4. Current state and prospects for the 
future 
Tile module of Russian deconversiou is 
operational and can be tested at 
ccc ctrcctcre 
II 
Intermediate Representation 
cccnccisc cormaciced 
ccntactic ctrcctcrec 
cnccisc cc :rcace ccntactic i 
I ctrcctcre i 
Fig.3 
I Rcssian cormaciced \[ 
ccntactic ctrcctcre 
ll 
i i  
II 
Rcssian ccrcace 2 ccntactic 
~' ~ct rcc tc re  
F-----G cnccisccorpcocoeicac I Rcssian corpcococicac ctrcctcre I ctrcctcre - - - - - -T  
centence centence 
hitp://proling.iitp.ru/Ooco. We plan to put it to 
geUela\] rise by aulunlu 2000. Tile interactive 
enconvorsion n\]odulo will be our next concorll. 
As sllo,vn ill Fig. 3, the interface botweou 
UNL and Russian is established at tile level of 
the English NorlllS. At this point ETAP 
English-to-Russian nlachine Iranslation facility 
can be switched which carries through tile 
phases of transfer and Russian generation. This 
architecture allows to obtain English generalion 
for relatively cheap, as ETAP has a Russiau-to- 
English mode of operation as well. First 
experiments in this direction have been carried 
Otll which proved quite promising. 
References 
Aprcsian Ju.D., I.M.B%uslavsky, L.L.Iomdin et al 
(1992). ETAP-2: The Linguistics of a Machine 
Translation System.//META, Vol. 37, No 1, pp. 
97-112. 
Boguslavsky 1.(1995). A bi-dircclional Russian-lo- 
English nlachino Iranslaiion system (ETAP-3). H 
Proceedings el' die Machine Trallslalion Sumnlii 
V. guxonlbourg. 
lomdin L.& O. Slroiter. (1999). Learning 1"1"o111 
Pa,'allel Corpora: Experiments in Machine 
Translation. // l)ialogue'99: Compulational 
IJnguislics and ils Applications International 
Workshop. Tarusa, Russia, June 1999. Vol.2, pp. 
79-88. 
89 
Dependency Treebank for Russian: 
Concept, Tools, Types of Infornmtion 
Igor BOGUSLAVSKY, Svetlana GRIGORIEVA, 
Nikolai GRIGORIEV, I,conid KREIDLIN, Nadezhda FRID 
I.aboratory for Computational IJnguistics 
Institute for Iufornmtion rl'rausuaission Problems 
Russian Academy of Sciences 
Bolshoi Karetnyi per. 19, 101447 Moscow-  RUSSIA 
{bogus, sveta, grig, lenya, nadya}Oiitp.ru 
Abstract 
'File paper describes a tagging scheme designed 
for the Russian Treebank, and presents tools used 
for corpus creation. 
1. ln t rodudory  Remarks 
The present paper describes a project aimed at 
developing the first annotated corpus of P, ussian 
texts. I.arge text coq~ora trove been used in the 
computational linguistics community long 
enough: at present, over 20 large corpora for the 
main European languages arc available, the 
largest of them containing hundreds of millions of 
words (I.anguage Resources (19971); Marcus, 
Santorini, and Marcinkiewicz (1993); Kurohashi, 
Nagao (1998)). So far, however, no annotated 
corpora for Russian have been developed. To the 
best of our knowledge, the present project is the 
first attempt to fill the gap. 
l)ifferent tasks require different annotation levels 
that entail different amount of additional 
information about text structure. The corpus that 
is; being created in the fiamework of the pre.sent 
project consists of several subcorpora that differ 
by the level of annotation. The following three 
levels are envisaged: 
? lemmalized leA'Is, for every word, its normal 
form (lemma) and part of speech are 
indicated; 
? mowhologically tagged leXlS: for every word, 
a full set of nlorl)hological attributes it 
specified along with the lenmm and the part of 
speech; 
? symactically tagged ldxlx: apart from tile full 
morphological markup at the word level, 
every sentence has a syntax structure. 
We annotate Russian texts with depmlde,wy 
structttres - a formalism that is more suitable for 
Slavonic languages with their relatively fiee word 
order. The structure not only contains inl'omlation 
on which words of the sentence are syntactically 
linked, but also relegates each link to one of the 
several dozen syntactic types (at present, we use 
78 syntactic relations). This formalism ensures a 
more complete and informative representation 
than ally other syntactically annotated corpus. 
This is a major innowttion, since the majority of 
syntactically annotated corpora, both those 
already awfilable and under construction, 
represent he syntactic structure by means of 
constituents. 
The closest analogue to our work is the Czech 
annotated corpus collected at Charles University 
in Prague - see I tajicova, Panevova, Sgall (19981). 
In this corpus, the syntactic data are also 
expressed in a dependency formalism, although 
the set of syntactic functional relations is much 
smaller as it only has 23 relations 
In what follows, we describe the types of texts 
used to create the coqms (Section 2), markup 
format (Section 3), annotation tools and 
procedures (Sectional), and types of linguistic 
data included in the markup (Section 5). 
2. Source text selection 
The well-known Uppsala University Corpus of 
contemporary Russian prose, totalling ca. 
1,000,000 words, has been chosen as the prilnary 
source for our work. The Uppsaht Corpus is well 
balanced between fiction and journalistic genre, 
with a smaller percentage of scientific and popular 
science texts. The Corpus includes samples of 
contemporary Russian prose, as well as excerpts 
flom newspapers and magazines of recent 
decades, and gives a representative coverage of 
987 
written Russian in modern use. Conversational 
examples are scarce and appear as dialogues 
inside fiction texts. 
3. Markup flDrnmt 
The design principles were fommlated as follows: 
? "layered" markup-  several annotation levels 
coexist and can be extracted or processed 
independently; 
? incrementality - it should be easy to add 
higher annotation levels; 
? convenient parsing of the annotated text by 
means of standard software packages. 
The most natural solution to meet this criteria is 
an XML-based markup language. We have tried 
to make our format compatible with TEI (Text 
Encoding for Interchange, see TEI Guidelines 
(1994)), inuoducing new elements or attributes 
only in situations where TEI markup does not 
provide adequate means to describe the text 
structure in the dependency grammar framework. 
Listed below are types of iuformation about text 
structure tlmt must be encoded in the markup, and 
relative tags/attributes u ed to bear them. 
a) Splitting of text into sentences. A special 
container element <S> (available in TEI) is used 
to delimit sentence boundaries. The element may 
have an (optional) ID attribute that supplies a 
unique identifier for the sentence within the text; 
this identifier may be used to store infommtion 
about extra-sentential relations in the text. It may 
also have a COMMENT attribute, used by linguists 
to store observations about particular syntactic 
phenomena encountered in the sentence; 
b) Splitting of sentences into lexical items 
~ .  The words are delimited by a container 
element <W>. Like sentences, words may have a 
unique "rD attribute that is used to reference the 
word within the sentence; 
c) Ascribing morphological features to words. 
Morphological information is ascribed to the word 
by means of two attributes born by the <W> tag: 
LlgNNg_- a normalized word form; 
FEAT - morphological features. 
d) Storing information about the syntax structure. 
To annotate the information about syntactic 
dependencies, we use two other attributes in the 
<W> element: 
DON- the ID of the master word; 
LINK - syntactic function label. 
There are also special provisions in the lbrmalism 
to store auxiliary information, e.g. multiple 
morphological nalyses and syntax trees. They are 
expected to disappear from the final version of the 
corpus. 
4. Annotat ion tools and procedures 
The procedure of corpus data acquisition is senti- 
automatic. An initial version of markup is 
generated by a computer using a general ~urpose 
morphological analyzer and syntax parser engine; 
after that, the results of the automatic processing 
are submitted to human post-editing. The analysis 
engine (morphology and parsing) is based upon 
the ETAP-3 machine translation engine - see 
Apresjan et al (1992, 1993). 
To support he creation of mmotated ata, a set of 
tools was designed and implemented. All tools are 
Win32 applications written in C++. The tools 
available are: 
? a program for sentence boundaries markup, 
called Chopper; 
" a post-editor for building, editing and mana- 
ging syntactically annotated texts - Slruclure 
Edilor (or SirEd). 
The amount of manual work required to build 
annotations depends on the complexity of the 
input data. SirEd offers different options for 
building structures. Most sentences can be reliably 
processed without any human intervention; in this 
case, a linguist should look through the processing 
result and confirm it. If the structure contains 
errors, the linguist can edit it using a user-friendly 
graphical interface (see screenshots below). If the 
errors are too many or no structure could be 
produced, the linguist may use a special split-and- 
rtm mode. This mode includes manual pre- 
chunking of the input phrase into pieces with a 
more transparent structure and applying the 
analyzer/parser to every chunk. Then the linguist 
must manually link the subtrees produced for 
every chunk into a single structure. 
If the linguist has encountered a very peculiar 
syntactic onstruction so that he/she is uncertain 
988 
glbOtil the ton'cot strticture, he/she lllay mark its 
"doubtful"  the whole sentence or sirlgh.', words 
whoso func:tions are not complele ly clear. The 
hiforniation wil l  be stored hi the niarkt/p, and 
Sir lgd will visualize the rOSl~eCtiVe SClltellce ;is 
one in need for further editing. 
\]qg. i presents the nlain dialog w indow for 
editinb,  soilteiico l)roportios. Al l  operator can edit 
il:to i luirkup di icct ly,  or edit single properlics u!;ing 
a gral,hk:al interfac:e. The sotirt:o loxl  il l lder 
analysis is wi-illcn in all edit WilldOW ill lhc top: 
,Volj<~ pis'mo ne hylo podpisamJ, ja m,r:novenslo 
do<r;adal.sja, klo e,qo #mpisal \[A/lhou<~J~ /lle lelier 
was su.,l sighted, 1 i~,slanlly guessed who had 
written itl. 'l'ho information about sin~,le words is 
wriltcn inlo a li:~t: e.g. the first word xotja 
\]althottgh\] has an identifier :I;D:-:"~ "; llle 
Icmnlatized forni is XO'IJA; its feature list 
coi~sisls of a sinp~le roattlre -- ;t l)art-of-spoech 
characlor\]slk: (it iS a conjtil lCtion); the word 
depends oil ;I word with IO="8"  by till adverbial 
.vottt'ce .venle~Tce I / raw mr~#'/,tq# 
rc:tation (link type is "adverb" ) .  By double- 
c l icking all itoi\]i hi the word list or prossh\]g the 
button, a l inguist can invoke dialog whidows f{}r 
edit ing 1}roportios {}f single words, l towovor ,  the 
i\]lost coI lvenient way of  editing the structure 
consists in invoking a Tree l~\]dilor whldow} 
shown in Fig. 2 with the Sall\]O soiltollco ~lS, hi the 
previous picture. 
The Tree Editor interface Js .shlipio alld nattlrai. 
Words of  the SOUlCO SOlltCllCt: ;11%; written on the 
left, their lelllllias aic pill hlto glay roclallgles, alld 
their inorl)hological foattnes arc written on the 
right. The syntactic relations are shown as arrows 
directed from the master to the slave; Ihe l ink 
typc.s are indicated in rotmdod rcclanglos oll lhe 
arcs. All text l\]elds except for tile sotlrco SOIl\[ClICK 
are edilable in-place. Moreover,  one can drag Ihe 
rOlllldod rectangles: dropping it on a word illeans 
that this word is; declared ;i new maStOl- It)l die 
word \['rOlil which the rectangle was dragged. A 
sh;glo r ightd)ulton click on the loll l l l la reel;ingle 
1 
S~r, terico l\['}~: I'1 ~6-5n~ statusI'tlll Strudure 
CVt~-D-/.-}tv\]--';~7'77tS,--'(:I-%;'C-(:\]I'4.\]"ilT~.';1 '  t.\[-t....~ll,}lTq-"--;--i<:J~r~q '- (INK-",,i-,~./,/ 
<'vV ()Olvl~<"'l" If !~",.1-~":3 HM EJL CP\[_I?, I II \[O.r{" ID- "2" I t_: MMA#'J~)9 
<W DOM-"4" t-E.RI-="IV'd>,I TM I\[ ~-<"3"ll_El,..lt'..,l./',=="l If!" I_INK=%i b~/4/4!4 
<W DOM::"I" I--EA\] = "V I 1POIIJ Elh.~)1.'1~ I l..4:-',1~.~ 1t3CPIz !-t HECC~/3" I 
<W DOM,~"4" ~ E/\I-"Vlll ~O\[\[I Eq ~'IPb'ILI KP c{:p\[~!q..CO\[7 CIffA~.' 
<W IX:?.'iJ,<.I" I ~@q="S HM E!I MW)K O.fl" 11 )-%" I_EMMA#',~' L
< W D O M =" a" \[" V.ACr =~"ADV" ID ="/' L E M Mi\:~" M I ~ IO L1 \[ HI) Of~ I_ll' 
<W DOM="root' Iz\[:I'<.F="V I IF~OI U t-r\[ j} HL11437~>"GI i'.,.f~l.>/., gOB" 
eFO ltOrlHerdf1. 
").k.~l.\] T 9, { ~\IV) 
I( ;bMO" LINK~"rmesxL4K" 
1'-I")1 IO ( /vV)  t 
" i0="'\]," LIZMi,.4A="F.:bn-17/ 
\ ." I\[,),= ~J I.EMM/',=' rio,c4) 
INK= \[ Ij-3 e LI. H K II ) .q ( f'i/"/>/// 
. \[,4K=" obc-l-I'>Hr-HoBe.t~,ljO<p, 
D="~' LEMMA= rA? /A\[{ 
Word ID . i Lemme . 
I Xo-ru \[1\] XO I;-1 
\[-1'~ rll4Cbl-4t. \[2\] I 1HC.'.L>I 4C 
t 4l Fie \[3\] IlL 
l,llq \[~t,lno \[4\] bbll I-> 
t41 N Ofl.F1.4C\[tH O \[ \[._\] \] I\]o?trlHCblDAFt-> 
it, I-4r ttt_iDOI411U \[7\] tvlrt I(J\[JE-_l tt4o 
dll.. j'lOl-i~).ftt~jl(-;\[} \[07 .\[\[O1-i\\[1 blLW'q/bCYl 
_,_1_ 
14 CblDA \[ 
blLi.,ATbOY- ~ j  
cpNJ I/\[ul oSoT | 
7HML,:LCPr-!~nEO~t // \[4\] ~,r~,-,~ l 
/-V,.RT // \[41 eq~,,,.-, q 
lp..:/, i ipOLl.I t--~ rlHq 1,13~..~1\] riosvt-oo~q 
V'Hr>OLtl ~Ft rlpb.iq~/-.... \[,ll nao,>~<H~,,__J 
S 141vl ELI MV.)t,, 0~I I  \[{iJ npe~li.iK 
/"?Jv i \[\[ii e~c-,- 
\ i  F1POlll EJ-i/ll,ll/I H31z,. rm, et v I 
t Setmp\[c,, iq.us.qi#q-i cer4unc;~ 
certain etbout it! F 
Cancel J 
Edit-lree.. _l 
Words: 
nsert \] 
I- I~ \] 
Comments... 
Fit;ure I. Sentence I roporiics dialog in Strli,,d. 
989 
dHao . . . . .  ~ " ~  )~~\ ]  v nPOLU Ell nlau 14sbnl~ cpE~ HECOB 
~o~n,ea ,o .  . . . . . . .  "~"~?( nacc-a.an "'~1 nOILIFII4CblBATb I V FIPOI?1 Eft FIPVlq KP CPEICt COB c-rPAfl 
s .M En Mw0n 
Mr.oBe .Ho  .- ,,,(a'o6~cr";'=). I MFHOBEHHO I ADV 
.~ora~az~c...('-~.-~ I1OI-AD.IMBATbCFI I V F1POLU En fl kiLl Vi3bFIB MW>K COB 
. . . . . . . . . . . . .  
nan.can, l (O~?~-> I rll4C~,Tbl I vnpou\ ]  Ell rlviq 143bFIB MY>K COl? 
Figure 2. Tree Editor dialog in StrEd. 
brings out the word properties dialog? All colors, 
sizes and fonts are customizable. 
5. Types of linguistic information by level 
M o rpK0Jg_g y information 
The morphological analyzer ascribes features to 
every word. The feature set for Russian includes: 
part of speech, animateness, gender, number, 
case, degree of comparison, short form (of 
adjectives and participles), representation (of 
verbs), aspect, tense, person, voice. 
Syntax information 
As we have already mentioned, the result of the 
parsing is a tree composed of links. Links are 
binary and oriented; they link single words rather 
than syntactic groups. For every syntactic group, 
one word (head) is chosen to represent it as a 
slave in larger syntactic units; all other members 
of the group become slaves of the head. 
In a typical case, the number of nodes in the 
syntactic tree corresponds to the number of word 
tokens. However, several exceptional situations 
occur in which the number of nodes may be less 
or even greater than the number of word tokens. 
The latter case is especially interesting. We 
postulate such a description in the following 
cases: 
a) Copulative sentences in the present tense 
where the auxiliary verb can be omitted. This 
is treated as a special "zero-form" of the 
copula, e.g. On - uchitel' \[He is a teacher, lit. 
He - teacher\]? The copula should be 
introduced in the syntactic representation. 
b) Elliptical constructs (omitted members of 
contrasted coordinative xpressions), like in 
Ja kupil rubashku, a on galstuk \[I bought a 
shirt, and he bought a necktie, lit. I bought a 
shirt, and he a necktie\]. 
The latter type of sentences should be discussed in 
more detail. Elliptical constructions are known to 
be one of the toughest problems in the 
formalization of natural language syntax. In our 
corpus, we decided to reconstruct he omitted 
elements in the syntactic trees, tamking them with 
a special '?phantom" feature. In the above 
example, a phantom node is inserted into the 
sentence between the words on 'he' and galstuk 
'necktie'. This new node will have a lemma 
POKUPAT" \[BUY\] and will beat" exactly the same 
morphological features as the wordform kupil 
\[bought\] physically present in the sentence, plus a 
special "phantom" marker. In certain cases, the 
feature set for the phantom may differ from that of 
the prototype, e.g. in a slightly modified phrase Ja 
kupil rubashku, aona  galstuk \[I bought a shirt, 
and she (bought) a necktie\] the phantom node will 
have the feminine gender, as required by the 
agreement with the subject of the second clause. 
Most real-life elliptical constructs can be 
represented in this way. 
The inventory of syntactic relationship types 
generated by the ETAP--3 system is wLst enough: 
at present, we count 78 different syntactic 
function types. All relationships are divided into 6 
990 
major groups: aclant, altribulive, quantitative, 
adverbial, coordinative, auxiliary. 
For readers' COlwenience, we will give equivalent 
English examples: 
Aelant relalionships link the predicate word to 
its arguments. Some examples (\[IX\] - master, 
\[Y\] - slave): 
predicative - Pete \[Y\] reads \[X\]; 
completive (1,2, 3 ) -  translate \[X\] 
the book \[Y, l-compl\] 
from \[Y1, 2-compl\] English 
into \[Y2, 3-compl\] Russian 
Ah-ibutive relationships often link a noun to a 
modifier expressed by an adjectve, another noun, 
a participle clause, etc: 
relative- The house \[X\] we live\[YI in. 
Quanlitalive relationships link a noun to a word 
with quantity semantics, or two such words one to 
another: 
quantitative - f ive \[Y\] pages \[IX\]; 
auxiliary-quantitative - gtirly \[Y\] five IX\]; 
Adverbial relationshil)s link the predicate word to 
various adverbial modifiers: 
adverbial- come \[Xl i ,  the evening \[Y\]; 
parenthetic - In my opinion IYI, lhal's \[IX\] righI. 
Coordinalive relationships serve for clauses 
coordinated by conjunctions: 
coordinative - buy apples \[XI and peaJwlYl ; 
coordinative-conj unctive - I)tty apples 
and \[X\] l)emw \[Y\]. 
Auxiliary relationships typically link two 
elements that form a single syntactic unit: 
analytical- will \[IX\] buy \[Y\]; 
The list of syntactic relations is not closed. Tile 
process of data acquisition brings up a variety of 
rare syntactic constructions, hardly covered by 
traditional grammars. In some cases, this has led 
to the introduction of new syntactic link types in 
order to reflect the semantic relation between 
single words and make tile syntactic structure 
unambiguous. 
Conclus ion 
Corpus crcation is not yet complctcd: at prcscnt, 
the flfll syntactic markup has been generated for 
4,000 sentences (55,000 words), which constitutes 
30% of the total amount planned. Our approach 
permits to include all information expressed by 
morphological and syntactic means in 
contemporary Russian. We expect that the new 
corpus will stimulate a broad range of further 
investigations, both theoretical and applied. 
We plan to make the corpus awtilable via EI,RA 
fiamework after completion. Samples of tagged 
text, documentation and structure editing tools 
will be available for download from our site: 
Ifltp://prolin~.iitp.ru/Corpus/preview.zip. 
Acknowledgements 
This work is supported by Russian Foundation of 
Fundamental Research, grant No. 98-0790072. 
References 
Apresjan Ju.D., Boguslavskij I.M., Iomdin L.I~., 
Lazurskij A.V., Sannikov V.Z. and Tsinman L.I.. 
(1992). The linguistics oJ'a Machine 7)'anslation 
System. Meta, 37 (1), pp. 97-112. 
Aprcsian Ju.D., Boguslavskij I.M., Iomdin 1..I.., 
I.azurskij A.V., Sannikov V.Z. and Tsinlnan L.I.. 
(1993). @~stbme d  tmduction atttomatique ETAP. 
In: ?a 7)'aductique. P.Bouillon and A.Clas (eds). 
l.es Presses de I'Universitd e Montrdal, Monlrdal. 
lIaiicova E., Panevova J., Sgall P. (1998). Lal~,guage 
Resources Need Amlolations To Make Them 
Really Reusable: 7"he Ibz~gtte Del;enden~o; 
"l)'eebank. in: Proceedings of lhe First Interna- 
tional Conference on I:anguage Resources & 
Evahmtion, pp. 713-718. 
Kur<>hashi S., Nagao M. (1998). BuiMing a Japanese 
Parsed Corpus while lmprovbzg the Parsin,~ 
System. In: Proceedings of the First Inlernational 
Conference on Language Resources & Evaluation, 
pp. 719-724 
I.anguagc Resources (1997). hu Survey of the State of 
the Art in IIuman Language Technology. Eds. 
G. B. Varile, A. Zampolli, Linguistica Computa- 
zionale, w)l. XII-XIII, pp. 381-408. 
Marcus M. P., Santorini B., and Marcinkiewicz M.-A. 
(1993). Building a large Am~otated Corpus o/" 
English: The Penn 7)vebank. Computational 
lfinguistics, Vol. 19, No. 2. 
TEI Guidelines (1994). TEl Guidelbws for Electronic 
7k.xt Encoding and h~tetwhange (P3). URI.: 
hlq)://elext.lil).virginia.edu/TEI.html 
991 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 641?648
Manchester, August 2008
Parsing the SYNTAGRUS Treebank of Russian
Joakim Nivre
V?axj?o University and
Uppsala University
joakim.nivre@vxu.se
Igor M. Boguslavsky
Universidad Polit?ecnica
de Madrid
Departamento de
Inteligencia Artificial
igor@opera.dia.fi.upm.es
Leonid L. Iomdin
Russian Academy
of Sciences
Institute for Information
Transmission Problems
iomdin@iitp.ru
Abstract
We present the first results on parsing the
SYNTAGRUS treebank of Russian with a
data-driven dependency parser, achieving
a labeled attachment score of over 82%
and an unlabeled attachment score of 89%.
A feature analysis shows that high parsing
accuracy is crucially dependent on the use
of both lexical and morphological features.
We conjecture that the latter result can be
generalized to richly inflected languages in
general, provided that sufficient amounts
of training data are available.
1 Introduction
Dependency-based syntactic parsing has become
increasingly popular in computational linguistics
in recent years. One of the reasons for the growing
interest is apparently the belief that dependency-
based representations should be more suitable for
languages that exhibit free or flexible word order
and where most of the clues to syntactic structure
are found in lexical and morphological features,
rather than in syntactic categories and word order
configurations. Some support for this view can be
found in the results from the CoNLL shared tasks
on dependency parsing in 2006 and 2007, where
a variety of data-driven methods for dependency
parsing have been applied with encouraging results
to languages of great typological diversity (Buch-
holz and Marsi, 2006; Nivre et al, 2007a).
However, there are still important differences in
parsing accuracy for different language types. For
? Joakim Nivre, Igor M. Boguslavsky, and Leonid
L. Iomdin, 2008. Licensed under the Creative Com-
mons Attribution-Noncommercial-Share Alike 3.0 Unported
license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
example, Nivre et al (2007a) observe that the lan-
guages included in the 2007 CoNLL shared task
can be divided into three distinct groups with re-
spect to top accuracy scores, with relatively low
accuracy for richly inflected languages like Arabic
and Basque, medium accuracy for agglutinating
languages like Hungarian and Turkish, and high
accuracy for more configurational languages like
English and Chinese. A complicating factor in this
kind of comparison is the fact that the syntactic an-
notation in treebanks varies across languages, in
such a way that it is very difficult to tease apart the
impact on parsing accuracy of linguistic structure,
on the one hand, and linguistic annotation, on the
other. It is also worth noting that the majority of
the data sets used in the CoNLL shared tasks are
not derived from treebanks with genuine depen-
dency annotation, but have been obtained through
conversion from other kinds of annotation. And
the data sets that do come with original depen-
dency annotation are generally fairly small, with
less than 100,000 words available for training, the
notable exception of course being the Prague De-
pendency Treebank of Czech (Haji?c et al, 2001),
which is one of the largest and most widely used
treebanks in the field.
This paper contributes to the growing litera-
ture on dependency parsing for typologically di-
verse languages by presenting the first results on
parsing the Russian treebank SYNTAGRUS (Bo-
guslavsky et al, 2000; Boguslavsky et al, 2002).
There are several factors that make this treebank
an interesting resource in this context. First of
all, it contains a genuine dependency annotation,
theoretically grounded in the long tradition of de-
pendency grammar for Slavic languages, repre-
sented by the work of Tesni`ere (1959) andMel??cuk
(1988), among others. Secondly, with close to
641
500,000 tokens, the treebank is larger than most
other available dependency treebanks and provides
a good basis for experimental investigations us-
ing data-driven methods. Thirdly, the Russian lan-
guage, which has not been included in previous ex-
perimental evaluations such as the CoNLL shared
tasks, is a richly inflected language with free word
order and thus representative of the class of lan-
guages that tend to pose problems for the currently
available parsing models. Taken together, these
factors imply that experiments using the SYNTA-
GRUS treebank may be able to shed further light
on the complex interplay between language type,
annotation scheme, and training set size, as deter-
minants of parsing accuracy for data-driven depen-
dency parsers.
The experimental parsing results presented in
this paper have been obtained using MaltParser,
a freely available system for data-driven depen-
dency parsing with state-of-the-art accuracy for
most languages in previous evaluations (Buchholz
and Marsi, 2006; Nivre et al, 2007a; Nivre et al,
2007b). Besides establishing a first benchmark for
the SYNTAGRUS treebank, we analyze the influ-
ence of different kinds of features on parsing ac-
curacy, showing conclusively that both lexical and
morphological features are crucial for obtaining
good parsing accuracy. All results are based on in-
put with gold standard annotations, which means
that the results can be seen to establish an upper
bound on what can be achieved when parsing raw
text. However, this also means that results are
comparable to those from the CoNLL shared tasks,
which have been obtained under the same condi-
tions.
The rest of the paper is structured as follows.
Section 2 introduces the SYNTAGRUS treebank,
section 3 describes the MaltParser system used in
the experiments, and section 4 presents experimen-
tal results and analysis. Section 5 contains conclu-
sions and future work.
2 The SYNTAGRUS Treebank
The Russian dependency treebank, SYNTAGRUS,
is being developed by the Computational Linguis-
tics Laboratory, Institute of Information Trans-
mission Problems, Russian Academy of Sciences.
Currently the treebank contains over 32,000 sen-
tences (roughly 460,000 words) belonging to texts
from a variety of genres (contemporary fiction,
popular science, newspaper and journal articles
dated between 1960 and 2008, texts of online
news, etc.) and it is growing steadily. It is an inte-
gral but fully autonomous part of the Russian Na-
tional Corpus developed in a nationwide research
project and can be freely consulted on the Web
(http://www.ruscorpora.ru/).
Since Russian is a language with relatively free
word order, SYNTAGRUS adopted a dependency-
based annotation scheme, in a way parallel to the
Prague Dependency Treebank (Haji?c et al, 2001).
The treebank is so far the only corpus of Russian
supplied with comprehensive morphological anno-
tation and syntactic annotation in the form of a
complete dependency tree provided for every sen-
tence.
Figure 1 shows the dependency tree for the
sentence Naibol~xee vozmuwenie uqastnikov
mitinga vyzval prodolawi$is rost cen
na benzin, ustanavlivaemyh neftnymi kom-
panimi (It was the continuing growth of petrol
prices set by oil companies that caused the greatest
indignation of the participants of the meeting). In
the dependency tree, nodes represent words (lem-
mas), annotated with parts of speech and morpho-
logical features, while arcs are labeled with syntac-
tic dependency types. There are over 65 distinct
dependency labels in the treebank, half of which
are taken from Mel??cuk?s Meaning?Text Theory
(Mel??cuk, 1988). Dependency types that are used
in figure 1 include:
1. predik (predicative), which, prototypically,
represents the relation between the verbal
predicate as head and its subject as depen-
dent;
2. 1-kompl (first complement), which denotes
the relation between a predicate word as head
and its direct complement as dependent;
3. agent (agentive), which introduces the rela-
tion between a predicate word (verbal noun
or verb in the passive voice) as head and its
agent in the instrumental case as dependent;
4. kvaziagent (quasi-agentive), which relates
any predicate noun as head with its first syn-
tactic actant as dependent, if the latter is
not eligible for being qualified as the noun?s
agent;
5. opred (modifier), which connects a noun
head with an adjective/participle dependent if
the latter serves as an adjectival modifier to
the noun;
642
Figure 1: A syntactically annotated sentence from the SYNTAGRUS treebank.
6. predl (prepositional), which accounts for the
relation between a preposition as head and a
noun as dependent.
Dependency trees in SYNTAGRUS may contain
non-projective dependencies. Normally, one token
corresponds to one node in the dependency tree.
There are however a noticeable number of excep-
tions, the most important of which are the follow-
ing:
1. compound words like ptidestitany$i
(fifty-storied), where one token corresponds
to two or more nodes;
2. so-called phantom nodes for the representa-
tion of hard cases of ellipsis, which do not
correspond to any particular token in the sen-
tence; for example,  kupil rubaxku, a on
galstuk (I bought a shirt and he a tie), which
is expanded into  kupil rubaxku, a on
kupil
PHANTOM
galstuk (I bought a shirt and
he bought
PHANTOM
a tie);
3. multiword expressions like po kra$ine$i mere
(at least), where several tokens correspond to
one node.
Syntactic annotation is performed semi-
automatically: sentences are first processed
by the rule-based Russian parser of an advanced
NLP system, ETAP-3 (Apresian et al, 2003) and
then edited manually by linguists who handle
errors of the parser as well as cases of ambiguity
that cannot be reliably resolved without extra-
linguistic knowledge. The parser processes raw
sentences without prior part-of-speech tagging.
Morphological annotation in SYNTAGRUS is
based on a comprehensive morphological dictio-
nary of Russian that counts about 130,000 entries
(over 4 million word forms). The ETAP-3 mor-
phological analyzer uses the dictionary to produce
morphological annotation of words belonging to
the corpus, including lemma, part-of-speech tag
and additional morphological features dependent
on the part of speech: animacy, gender, number,
case, degree of comparison, short form (of adjec-
tives and participles), representation (of verbs), as-
pect, tense, mood, person, voice, composite form,
and attenuation.
Statistics for the version of SYNTAGRUS used
for the experiments described in this paper are as
follows:
? 32,242 sentences, belonging to the fiction
genre (9.8%), texts of online news (12.4%),
newspaper and journal articles (77.8%);
? 461,297 tokens, including expressions with
non-alphabetical symbols (e.g., 10, 1.200,
$333, +70C, #) but excluding punctuation;
? 31,683 distinct word types, of which 635 with
a frequency greater than 100, 5041 greater
than 10, and 18231 greater than 1;
? 3,414 sentences (10.3%) with non-projective
643
POS DEP MOR LEM LEX
TOP + + + + +
TOP?1 +
HEAD(TOP) + +
LDEP(TOP) +
RDEP(TOP) +
NEXT + + + +
NEXT+1 + + + +
NEXT+2 +
NEXT+3 +
LDEP(NEXT) +
Table 1: History-based features (TOP = token on
top of stack; NEXT = next token in input buffer;
HEAD(w) = head of w; LDEP(w) = leftmost depen-
dent of w; RDEP(w) = leftmost dependent of w).
dependencies and 3,934 non-projective de-
pendency arcs in total;
? 478 sentences (1.5%) containing phantom
nodes and 631 phantom nodes in total.
3 MaltParser
MaltParser (Nivre et al, 2007b) is a language-
independent system for data-driven dependency
parsing, based on a transition-based parsing model
(McDonald and Nivre, 2007). More precisely, the
approach is based on four essential components:
? A transition-based deterministic algorithm
for building labeled projective dependency
graphs in linear time (Nivre, 2003).
? History-based feature models for predicting
the next parser action (Black et al, 1992;
Magerman, 1995; Ratnaparkhi, 1997).
? Discriminative classifiers for mapping histo-
ries to parser actions (Kudo and Matsumoto,
2002; Yamada and Matsumoto, 2003).
? Pseudo-projective parsing for recovering non-
projective structures (Nivre and Nilsson,
2005).
In the following subsections, we briefly describe
each of these four components in turn.
3.1 Parsing Algorithm
The parser uses the deterministic algorithm for la-
beled dependency parsing first proposed by Nivre
(2003). The algorithm builds a labeled dependency
graph in one left-to-right pass over the input, us-
ing a stack to store partially processed tokens and
adding arcs using four elementary actions (where
TOP is the token on top of the stack and NEXT is
the next token):
? Shift: Push NEXT onto the stack.
? Reduce: Pop the stack.
? Right-Arc(r): Add an arc labeled r from TOP
to NEXT; push NEXT onto the stack.
? Left-Arc(r): Add an arc labeled r from NEXT
to TOP; pop the stack.
Parser actions are predicted using a history-based
feature model (section 3.2) and SVM classifiers
(section 3.3). Although the parser only derives
projective graphs, the fact that these graphs are
labeled allows non-projective dependencies to be
captured using the pseudo-projective approach of
Nivre and Nilsson (2005) (section 3.4).
3.2 History-Based Feature Models
History-based parsing models rely on features of
the derivation history to predict the next parser ac-
tion (Black et al, 1992). The features used are
all symbolic and defined in terms of five different
node attributes:
? POS = part of speech (atomic)
? DEP = dependency type
? MOR = morphological features (set)
? LEM = lemma
? LEX = word form
Features of the type DEP have a special status in
that they are extracted during parsing from the par-
tially built dependency graph and are updated dy-
namically during parsing. The other four feature
types (LEX, LEM, POS, and MOR) are given as part
of the input to the parser and remain static during
the processing of a sentence. Of these four fea-
ture types, all except LEX presupposes that the in-
put has been preprocessed by a lemmatizer, tagger
and morphological analyzer, respectively, but for
the experiments reported below we use gold stan-
dard annotation from the treebank.
In order to study the influence of different fea-
tures, we have experimented with different combi-
nations of the five feature types, where the base-
line model contains only POS and DEP features,
while more complex models add MOR, LEM, and
LEX features in different combinations. The exact
644
features included for each feature type are shown
in table 1, where rows denote tokens in a parser
configuration (defined relative to the stack, the re-
maining input, and the partially built dependency
graph), and where columns correspond to feature
types. The selection of features in each group was
tuned on a development set as described in sec-
tion 4.
3.3 Discriminative Classifiers
We use support vector machines (Vapnik, 1995) to
predict the next parser action from a feature vector
representing the history. More specifically, we use
LIBSVM (Chang and Lin, 2001) with a quadratic
kernel K(x
i
, x
j
) = (?x
T
i
x
j
+ r)
2
and the built-
in one-versus-all strategy for multi-class classifica-
tion. Symbolic features are converted to numerical
features using the standard technique of binariza-
tion, and we split the set values of MOR features
into their atomic components. In order to speed
up training, we also divide the training data into
smaller bins according to the feature POS of NEXT,
and train separate classifiers on each bin.
3.4 Pseudo-Projective Parsing
Pseudo-projective parsing was proposed by Nivre
and Nilsson (2005) as a way of dealing with non-
projective structures in a projective data-driven
parser. We projectivize training data by a minimal
transformation, lifting non-projective arcs one step
at a time, and extending the arc label of lifted arcs
using the encoding scheme called HEAD by Nivre
and Nilsson (2005), which means that a lifted arc
is assigned the label r?h, where r is the original
label and h is the label of the original head in the
non-projective dependency graph.
Non-projective dependencies can be recovered
by an inverse transformation applied to the depen-
dency graph output by the parser, using a left-to-
right, top-down, breadth-first search, guided by the
extended arc labels r?h assigned by the parser.
4 Experiments
In this section we describe the first experiments on
parsing the SYNTAGRUS treebank using a data-
driven parser. The experimental setup is described
in section 4.1, while the experimental results are
presented and discussed in section 4.2.
4.1 Experimental Setup
All experiments have been performed on the ver-
sion of SYNTAGRUS described in section 2, con-
Model Count LAS UAS
Base = POS + DEP 46506 60.2 76.0
B1 = Base + MOR 46506 73.0 84.5
B2 = Base + LEM 46506 75.5 84.6
B3 = Base + LEX 46506 74.5 84.6
BM1 = B1 + LEM 46506 82.3 89.0
BM2 = B1 + LEX 46506 81.0 88.8
All = B1 + LEM + LEX 46506 82.3 89.1
Table 2: Parsing accuracy for different feature
models on the final test set (Count = Number of
tokens in the test set, LAS = Labeled attachment
score, UAS = Unlabeled attachment score).
verted to the CoNLL data format (Buchholz and
Marsi, 2006).
1
The available data were divided
into 80% for training, 10% for development, and
10% for final testing, using a pseudo-randomized
split. The development set was used for tuning
parameters of the parsing algorithm and pseudo-
projective parsing technique, and for feature selec-
tion within the feature groups not included in the
baseline model (i.e., MOR, LEM, and LEX). The
test set was used for evaluating the finally selected
models once.
The evaluation metrics used are labeled attach-
ment score (LAS) ? the percentage of tokens that
are assigned the correct head and dependency type
? and unlabeled attachment score (UAS) ? the per-
centage of tokens that are assigned the correct head
(regardless of dependency type). In addition, we
present precision and recall for non-projective de-
pendencies. Punctuation tokens are excluded in all
scores, but phantom tokens are included. We use
McNemar?s test for statistical significance.
4.2 Results and Discussion
Table 2 gives the parsing accuracy for different fea-
ture models on the held-out test set, measured as
labeled attachment score (LAS) and unlabeled at-
tachment score (UAS). With respect to LAS, there
are statistically significant differences between all
models except BM1 and All (p < 0.01). With re-
spect to UAS, there are statistically significant dif-
ferences between four groups, such that {Base} <
{B1, B2, B3} < {BM2} < {BM1, All}, but there
1
Since SYNTAGRUS only distinguishes ten different parts
of speech (not counting morphological features), the fields
CPOSTAG and POSTAG in the CoNLL format ? for coarse-
grained and fine-grained parts of speech ? were given the
same content.
645
are no differences within these groups.
2
Looking at the results for different models, we
see that while the baseline model (Base) achieves
a modest 60.2% LAS and 76.0% UAS, the addi-
tion of only one additional feature group (B1?B3)
boosts unlabeled accuracy by close to ten percent-
age points and labeled accuracy by up to fifteen
percentage points. Somewhat surprisingly, the dif-
ferences between models B1?B3 are very small,
and only differences with respect to LAS are statis-
tically significant, which may be taken to suggest
that morphological and lexical features capture the
same type of information. However, this hypothe-
sis is clearly refuted by the results for models BM1
and BM2, where the addition of lexical features on
top of morphological features gives a further gain
in LAS of eight to ten percentage points (and over
four percentage points for UAS).
Comparing the use of raw word forms (LEX) and
lemmas (LEM) as lexical features, we see a slight
advantage for the latter, at least for labeled accu-
racy. However, it must be remembered that the ex-
periments are based on gold standard input anno-
tation, which probably leads to an overestimation
of the value of LEM features. Finally, it is worth
noting that including both LEX and LEM features
does not result in a significant improvement over
the model with only LEM features, which may be a
sign of saturation, although this may again change
in the presence of noisy LEM features.
The experimental results show conclusively that
both morphological and lexical features are crucial
for achieving high parsing accuracy. It may seem
that they are most important for labeled accuracy,
where the gain in absolute percentage points is the
greatest with respect to the baseline, but it must
be remembered that the unlabeled scores start at a
higher level, thus leaving less room for improve-
ment. In fact, the total error reduction from Base
to All is over 50% for both LAS and UAS.
Table 3 gives a more detailed picture of parsing
performance for the best model (All), by breaking
down both LAS and UAS by the part-of-speech tag
of the dependent. We note that accuracy is higher
than average for nouns (S), adjectives (A), parti-
cles (PART), and reasonably good for verbs (V).
For prepositions (PR), conjunctions (CONJ), and
adverbs (ADV), accuracy is considerably lower,
which may be attributed to attachment ambigui-
2
For the difference BM2 < BM1, 0.01 < p < 0.05; for
all other differences, p < 0.01.
Part of Speech Count LAS UAS
S (noun) 7303 86.7 93.3
A (adjective) 7024 92.8 94.2
V (verb) 6946 81.9 85.8
PR (preposition) 5302 60.0 79.0
CONJ (conjunction) 2998 76.1 80.7
ADV (adverb) 2855 72.3 83.3
PART (particle) 1833 88.1 89.6
NUM (numeral) 807 88.7 93.6
NID (foreign word) 142 76.5 91.5
COM (compound) 32 93.8 96.9
P (proposition word) 7 57.1 85.7
INTJ (interjection) 5 0.0 20.0
Table 3: Accuracy by part of speech on the final
test set for All features (Count = Number of tokens
in the test set, LAS = Labeled attachment score,
UAS = Unlabeled attachment score).
ties. It is also worth noting that both prepositions
and adverbs have considerably higher UAS than
LAS (almost twenty percentage points for prepo-
sitions), which shows that even when they are at-
tached correctly they are are often mislabeled. The
remaining parts of speech are too infrequent to
warrant any conclusions.
Looking specifically at non-projective depen-
dencies, we find that the best model has a la-
beled precision of 68.8 and a labeled recall of 31.4.
The corresponding unlabeled figures are 73.3 and
33.4.
3
This confirms the results of previous studies
showing that the pseudo-projective parsing tech-
nique used by MaltParser tends to give high pre-
cision ? given that non-projective dependencies
are among the most difficult to parse correctly ?
but rather low recall (McDonald and Nivre, 2007).
It is also worth mentioning that phantom tokens,
i.e., empty tokens inserted for the analysis of cer-
tain elliptical constructions (see section 2), have
a labeled precision of 82.4 and a labeled recall
of 82.8 (89.2 and 89.6 unlabeled), which is very
close to the average accuracy, despite being very
infrequent. However, it must be remembered that
these tokens were given as part of the input in
these experiments. In order to correctly analyse
these tokens and their dependencies when pars-
ing raw text, they would have to be recovered in
a pre-processing phase along the lines of Dienes
3
The precision is the percentage of non-projective depen-
dencies predicted by the parser that were correct, while the
recall is the percentage of true non-projective dependencies
that were correctly predicted by the parser.
646
and Dubey (2003).
Summing up, the main result of the experimen-
tal evaluation is that both morphological and lex-
ical features are crucial for attaining high accu-
racy when training and evaluating on the repre-
sentations found in the SYNTAGRUS treebank of
Russian. With regard to morphological features
this is in line with a number of recent studies
showing the importance of morphology for pars-
ing languages with less rigid word order, includ-
ing work on Spanish (Cowan and Collins, 2005),
Hebrew (Tsarfaty, 2006; Tsarfaty and Sima?an,
2007), Turkish (Eryigit et al, 2006), and Swedish
(?vrelid and Nivre, 2007).
With regard to lexical features, the situation is
more complex in that there are a number of stud-
ies questioning the usefulness of lexical features
in statistical parsing and arguing that equivalent
or better results can be achieved with unlexical-
ized models provided that linguistic categories can
be split flexibly into more fine-grained categories,
either using hand-crafted splits, as in the seminal
work of Klein and Manning (2003), or using hid-
den variables and unsupervised learning, as in the
more recent work by Petrov et al (2006), among
others. There are even studies showing that lexi-
calization can be harmful when parsing richly in-
flected languages like German (Dubey and Keller,
2003) and Turkish (Eryi?git and Oflazer, 2006).
However, it is worth noting that most of these
results have been obtained either for models of
constituency-based parsing or for models of de-
pendency parsing suffering from sparse data.
4
In
the experiments presented here, we have used
a transition-based model for dependency parsing
that has much fewer parameters than state-of-the-
art probabilistic models for constituency parsing.
Moreover, we have been able to use a relatively
large training set, thereby minimizing the effect of
sparseness for lexical features. We therefore con-
jecture that the beneficial effect of lexical features
on parsing accuracy will generalize to other richly
inflected languages when similar conditions hold.
As far as we know, these are the first results for
a large-scale data-driven parser for Russian. There
do exist several rule-based parsers for Russian,
such as the ETAP-3 parser (Apresian et al, 2003)
and a Link Grammar parser,
5
as well as a prototype
of a hybrid system based on the ETAP-3 parser en-
4
The latter case applies to the probabilistic model of de-
pendency parsing explored by Eryi?git and Oflazer (2006).
5
http://sz.ru/parser/
riched with statistics extracted from SYNTAGRUS
(Boguslavsky et al, 2003; Chardin, 2004), but dif-
ferences in both input format and output repre-
sentations make it difficult to compare the perfor-
mance directly.
5 Conclusion
We have presented the first results on parsing the
SYNTAGRUS treebank of Russian using a data-
driven dependency parser. Besides establishing
a first benchmark for the SYNTAGRUS treebank,
we have analyzed the influence of different kinds
of features on parsing accuracy, showing conclu-
sively that both lexical and morphological features
are crucial for obtaining good parsing accuracy.
We hypothesize that this result can be generalized
to other richly inflected languages, provided that
sufficient amounts of data are available.
Future work includes a deeper analysis of the in-
fluence of individual features, both morphological
and lexical, as well as an evaluation of the parser
under more realistic conditions without gold stan-
dard annotation in the input. This will require not
only automatic morphological analysis and disam-
biguation but also a mechanism for inserting so-
called phantom tokens in elliptical constructions.
Acknowledgments
We want to thank Ivan Chardin for initiating this
collaboration and Jens Nilsson for converting the
SYNTAGRUS data to the CoNLL format. We are
grateful to the Russian Foundation of Basic Re-
search for partial support of this research (grant no.
07-06-00339).
References
Apresian, Ju., I. Boguslavsky, L. Iomdin, A. Lazursky,
V. Sannikov, V. Sizov, and L. Tsinman. 2003.
ETAP-3 linguistic processor: A full-fledged NLP
implementation of the MTT. In Proceedings of
the First International Conference on Meaning-Text
Theory, 279?288.
Black, E., F. Jelinek, J. D. Lafferty, D. M. Mager-
man, R. L. Mercer, and S. Roukos. 1992. To-
wards history-based grammars: Using richer models
for probabilistic parsing. In Proceedings of the 5th
DARPA Speech and Natural Language Workshop,
31?37.
Boguslavsky, I., S. Grigorieva, N. Grigoriev, L. Krei-
dlin, and N. Frid. 2000. Dependency treebank for
Russian: Concept, tools, types of information. In
Proceedings of COLING, 987?991.
647
Boguslavsky, I., I. Chardin, S. Grigorieva, N. Grigoriev,
L. Iomdin, L. Kreidlin, and N. Frid. 2002. Devel-
opment of a dependency treebank for Russian and
its possible applications in NLP. In Proceedings of
LREC, page 852856.
Boguslavsky, I. M., L. L. Iomdin, V. S. Sizov, and
I. Chardin. 2003. Parsing with a treebank. In Pro-
ceedings of the Conference on Cognitive Modeling
in Linguistics [In Russian].
Buchholz, S. and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Pro-
ceedings of CoNLL, 149?164.
Chang, C.-C. and C.-J. Lin, 2001. LIBSVM: A Library
for Support Vector Machines. Software available at
http://www.csie.ntu.edu.tw/?cjlin/libsvm.
Chardin, Ivan. 2004. Dependency Treebanks and Their
Use in Parsing. Ph.D. thesis, Russian Academy of
Science [In Russian].
Cowan, B. and M. Collins. 2005. Morphology and
reranking for the statistical parsing of spanish. In
Proceedings of HLT/EMNLP, 795?802.
Dienes, P. and A. Dubey. 2003. Deep syntactic pro-
cessing by combining shallow methods. In Proceed-
ings of ACL, 431?438.
Dubey, A. and F. Keller. 2003. Probabilistic parsing
for German using sister-head dependencies. In Pro-
ceedings of ACL, 96?103.
Eryi?git, G. and K. Oflazer. 2006. Statistical depen-
dency parsing of Turkish. In Proceedings of EACL,
89?96.
Eryigit, G., J. Nivre, and K. Oflazer. 2006. The in-
cremental use of morphological information and lex-
icalization in data-driven dependency parsing. In
Proceedings of the 21st International Conference
on the Computer Processing of Oriental Languages,
498?507.
Haji?c, J., B. Vidova Hladka, J. Panevov?a, E. Haji?cov?a,
P. Sgall, and P. Pajas. 2001. Prague Dependency
Treebank 1.0. LDC, 2001T10.
Klein, D. and C. D. Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of ACL, 423?430.
Kudo, T. and Y. Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In Pro-
ceedings of CoNLL, 63?69.
Magerman, D. M. 1995. Statistical decision-tree mod-
els for parsing. In Proceedings of ACL, 276?283.
McDonald, R. and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proceedings of EMNLP-CoNLL, 122?131.
Mel??cuk, I. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Nivre, J. and J. Nilsson. 2005. Pseudo-projective de-
pendency parsing. In Proceedings of ACL, 99?106.
Nivre, J., J. Hall, S. K?ubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007a. The CoNLL 2007
shared task on dependency parsing. In Proceedings
of the CoNLL Shared Task of EMNLP-CoNLL 2007,
915?932.
Nivre, J., J. Hall, J. Nilsson, A. Chanev, G. Eryi?git, S.
K?ubler, S. Marinov, and E. Marsi. 2007b. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13:95?135.
Nivre, J. 2003. An efficient algorithm for projective
dependency parsing. In Proceedings of IWPT, 149?
160.
?vrelid, L. and J. Nivre. 2007. When word order and
part-of-speech tags are not enough ? swedish depen-
dency parsing with rich linguistic features. In Pro-
ceedings of RANLP, 447?451.
Petrov, S., L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of COLING/ACL, 433?
440.
Ratnaparkhi, A. 1997. A linear observed time statis-
tical parser based on maximum entropy models. In
Proceedings of EMNLP, 1?10.
Tesni`ere, L. 1959.
?
El?ements de syntaxe structurale.
Editions Klincksieck.
Tsarfaty, R. and K. Sima?an. 2007. Three-dimensional
parametrization for parsing morphologically rich
languages. In Proceedings of IWPT, 156?167.
Tsarfaty, R. 2006. Integrated morphological and
syntactic disambiguation for modern hebrew. In
Proceedings of the COLING/ACL 2006 Student Re-
search Workshop, 49?54.
Vapnik, V. N. 1995. The Nature of Statistical Learning
Theory. Springer.
Yamada, H. and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proceedings of IWPT, 195?206.
648
Multilinguality in ETAP-3: Reuse of Lexical Resources 
Igor BOGUSLAVSKY 
Universidad Politecnica de Madrid 
28660 Boadilla del Monte, Madrid, Spain 
igor@opera.dia.fi.upm.es  
 
Leonid IOMDIN 
Institute for Information Transmission 
Problems, Russian Academy of Sciences 
19, B. Karetnyj 
Moscow, GSP-4, Russia  
iomdin@cl.iitp.ru 
 
Victor SIZOV 
Institute for Information Transmission Problems, Russian Academy of Sciences 
19, B. Karetnyj 
Moscow, GSP-4, Russia  
sizov@cl.iitp.ru 
 
Abstract 
The paper presents the work done at the Institute 
for Information Transmission Problems (Russian 
Academy of Sciences, Moscow) on the 
multifunctional linguistic processor ETAP-3. Its 
two multilingual options are discussed ? machine 
translation in a variety of language pairs and 
translation to and from UNL, a meaning 
representation language.  
For each working language, ETAP has one 
integral dictionary, which is used in all 
applications both for the analysis and synthesis 
(generation) of the given language. In difficult 
cases, interactive dialogue with the user is used for 
disambiguation. Emphasis is laid on multiple use 
of lexical resources in the multilingual 
environment.  
1 General Information on ETAP  
The multifunctional ETAP-3 linguistic 
processor, developed by the Computational 
Linguistics Laboratory (CLL) in Moscow (see e.g. 
Apresjan et al 1992a,b, 1993, 2003), is the product 
of more than two decades of laboratory research 
and development in the field of language 
modeling. The most important features of the 
processor are as follows. 
(1) ETAP-3 is based on the general linguistic 
framework of the Meaning ? Text theory, 
proposed by Igor Mel?cuk (e.g. Mel?cuk, 1974) 
and complemented by the theory of systematic 
lexicography and integrated description of 
language proposed by Jurij Apresjan [Apresjan 
1995, 2000].  
(2) ETAP-3 has a declarative organization of 
linguistic knowledge.  
(3) One of the major components of ETAP-3 is 
the innovative combinatorial dictionary. Apart 
from syntactic and semantic features and 
subcategorization frames, the dictionary entry may 
have rules of 8 types. Many dictionary entries 
contain lexical functions (LF).  
(3) ETAP-3 makes use of a formalism based on 
three-value predicate logic, in which all linguistic 
data are presented.  
(4) The ETAP-3 processor has a modular 
architecture. All stages of processing and all types 
of linguistic data are organized into modules, 
which warrants their reusability in many NLP 
applications both within and beyond ETAP-3 
environment.  
At the moment, the ETAP-3 environment 
comprises the following main options: 1) a rule -
based machine translation system; 2) a Universal 
Networking Language (UNL) translation engine; 
3) a system of synonymous paraphrasing of 
sentences; 4) a workbench for syntactic annotation 
of text corpora; and 5) a grammar checker. All the 
applications make use of the same dictionaries, but 
only the first and the second are multilingual. In 
Section 2 we will discuss multilingual lexical 
resources used in machine translation, and in 
Section 3 ? in the UNL module.  
2 Multilinguality in ETAP 
2.1 Structure of the Dictionary Entry 
To support multilinguality, the dictionary entry 
of the ETAP dictionary has several sub-zones. 
There is one general zone and several zones 
oriented towards various languages. The general 
zone stores all types of monolingual information: 
part of speech, syntactic features, semantic 
features, subcategorization frames, lexical 
functions, syntactic and pre-syntactic rules, 
generation rules, and some other data. Each bi-
lingual sub-zone serves for establishing 
correspondence between the given language and 
another one (see Fig. 1).  
For example, the Russian zone of an English 
dictionary entry contains all the information 
needed to translate English words into Russian, the 
Arabic zone provides translation into Arabic, etc. 
Conversely, the information needed to translate 
Russian words into English is stored in the English 
zone of the Russian dictionary entries. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 1 
 
2.2 Default and Specific Translation 
The information stored in a bi-lingual zone 
consists of two parts: a default translation and 
lexical translation rules. Default translation is a 
single word that translates the given word in non-
specific contexts (it is introduced by a special 
label: TRANS). Any other type of translation is 
carried out by means of rules. If the word is 
translated by a phrase consisting of several words, 
the rule shows how the words in the phrase are 
connected to each other and how this phrase is 
incorporated into the sentence. For example, in the 
entry bachelorship we find a reference to one of 
the standard translation rules (TRADUCT2.42). 
The slots of the rule are filled with specific lexical 
items, grammatical features or syntactic relations.  
TRAF:TRADUCT2.42 
LR1:STEPEN?,LR2:BAKALAVR,T2:SG, 
T3:QUASIAGENT 
The rule says that bachelorship  should be 
translated into Russian with a phrase consisting of 
two words ? stepen? (?degree?) and bakalavr 
(?bachelor?). These words should be connected by 
the quasiagent(ive) syntactic relation, and the 
number feature of bakalavr should be singular.  
If the word is translated in a specific way in a 
specific context or in specific phrases, the rule 
describes this context and the resulting structure. 
When a word is translated, normally first the 
translation rules in its dictionary entry are tried. If 
no rule applies in the given sentence, then the 
default translation is used.  
2.3 Multiple Translation 
The default option of ETAP produces a single 
translation of the sentence ? the one that 
corresponds to the first lexico-syntactic structure 
obtained by the parser. The option of multiple 
translation produces much more. First, it generates 
all lexico-syntactic structures that are compatible 
with the grammar and the dictionary. Since these 
structures are disambiguated both syntactically, 
and lexically, this set of structures contains all 
lexical variants for the source sentence. Then, for 
each structure all possible translation variants are 
tried. As is known, even disambiguated words can 
be translated into another language in different 
ways and it is not always possible to formulate a 
rule that could select an appropriate variant. For 
example, English adjuration can be translated into 
Russian as mol?ba and as zaklinanie, adventurer ? 
as avantjurist and as iskatel? prikljuchenij 
(literally, ?adventure seeker?), alarm ? as trevoga 
and as avarijnyj signal (?alarm signal?). In all these 
cases, we are most probably dealing with a single 
meaning of the English word and yet translation 
variants are not fully synonymous. Since we 
cannot choose among these variants by means of 
rules and at the same time do not want to lose any 
of them, we have to treat them as alternative 
translations to be activated in the ?Multiple 
translation? option. As mentioned in the previous 
section, there are two types of translation devices 
in the bilingual zones of the dictionary: a default 
translation (a single word) and rules. In both cases, 
it is possible to provide alternative translations. For 
example, in the entry for adjuration alternative 
translations are listed in the default part since both 
of them are single words: 
 
ADJURATION 
? 
TRANS: MOL?BA / ZAKLINANIE 
 
If the user selects the ?Single translation? option, 
only the first of these variants will be used. If 
ENGLISH WORD 
General information: 
- part of speech 
- syntactic features 
- semantic features 
- subcategorization frame 
- ?  
Russian zone 
Arabic zone 
? 
UNL zone  
he/she wishes to get al possible translations and 
activates the ?Multiple translation? option, both 
alternatives will be produced.  
In the adventurer entry, the alternative translation 
iskatel? prikljuchenij should be introduced by a 
rule, since it is not a single word but a phrase. Such 
rules are supplied by a special marker, OPT(ional), 
which shows that the translation is alternative.  
 
ADVENTURER 
? 
TRANS: AVANTJURIST 
TRAF:TRADUCT2.42 
OPT:1 
LR1:ISKATEL?2,LR2:PRIKLJUCHENIE,T2:PL, 
T3: ATTRIB 
 
This is another instance of the same rule that we 
saw above in the bachelorship example: the only 
difference is that it introduces different words, 
connects them with a different syntactic relation 
(attributive) and generates a different number 
feature. The marker OPT:1 shows that the 
translation introduced by this rule is less common 
than the default translation avantjurist and should 
be presented to the user after it. Should it be 
otherwise, the rule would have the marker OPT:0 
and have a priority over the default translation.  
2.4 Interactive selection of the translation 
equivalent  
It is well known that ambiguity of linguistic 
units is one of the most difficult problems in NLP. 
In ETAP there is no single stage of processing that 
expressly deals with disambiguation. The sentence 
is gradually disambiguated at different stages of 
processing on the basis of restrictions imposed by 
the linguistic knowledge of the system. However, 
in many cases this knowledge is not sufficient for 
complete disambiguation, since the understanding 
of a text by humans is not based on their linguistic 
knowledge alone. To cope with this problem, we 
are developing an interactive option that at certain 
pivotal points of text processing is expected to ask 
for human intervention and use human assistance 
to resolve those ambiguities that are beyond the 
scope of linguistic knowledge of the system 
(Boguslavsky et al2003). It should be stressed that 
the interactive tool is only resorted to if an 
ambiguity cannot be resolved automatically and 
therefore requires human intervention. This work 
is in line with the approach proposed in a series of 
publications by the GETA group (Blanchon, 1995, 
1996, 1997, Boitet & Blanchon, 1995).  
As mentioned above, the dialogue with the user 
is activated at different stages of the processing 
depending on the tasks solved at each stage. 
During the parsing, which results in the 
construction of the lexico-syntactic structure of the 
sentence, all lexical and syntactic ambiguity should 
be resolved. However, this is done entirely within 
the processing of the source language text and 
represents monolingua l ambiguity. It is not directly 
relevant for our topic of multilinguality. Of 
relevance here are cases of the so-called 
translational (or transfer) ambiguity (Hutchins, 
Somers, 1992: 87). The source language words can 
be unambiguous for the native speakers of this 
language but can be translated by a number of 
different target language expressions. In this sense, 
they are ambiguous from the viewpoint of the 
target language and have to be dealt with at the 
translation stage. An example is the English verb 
wash with respect to Russian. It translates 
differently depending on the type of object that is 
being washed: if it is something made of cloth, for 
example clothes, a special verb has to be chosen. If 
the dictionary provides semantic information on 
what objects are made of, the correct choice of the 
verb can in principle be made automatically. Cf., 
however, cases like We must wash it where such 
information is definitely missing.  
This must be viewed as a relatively inoffensive 
case, though, because most sentences will be 
translated correctly with the help of a simple rule 
(and if not, the mistake is not too important). There 
are many words for which it is much more difficult 
to write a disambiguation rule. A notorious 
example is English blue that corresponds to two 
Russian adjectives, one meaning ?light blue? and 
the other ? roughly ? ?dark blue?. The only way to 
translate this word correctly in most of the contexts 
is to get assistance from the user. The dialog with 
the user is based on the information stored in the 
dictionary and activated at the appropriate 
moment.  
This is how the interactive disambiguation 
currently works. The sentence to be translated is 
entered in the upper window of the ETAP 
environment (Fig. 2) 
 
 
 
Fig. 2 
When it comes to translating the word blue, the 
system finds that there are two options and no way 
to choose among them and activates the dialogue 
(Fig. 3). 
 
 
Fig. 3 
 
In the dialogue box each option is provided with 
a short comment and/or example that helps the user 
choose among them. The user has to click the 
appropriate option (in Fig. 3 ?light blue? is 
selected) and the system moves on. The result of 
the translation of this sentence is shown in Fig. 4.  
 
 
Fig. 4 
 
Should we have selected the other option in the 
dialogue in Fig. 3, the result would have been 
different (Fig. 5).  
 
 
Fig. 5 
 
It is important to note that the interactive 
disambiguation mode fully corresponds to the 
multiple translation possibilities discussed in the 
previous section. In particular, the dialogue takes 
into account all types of alternative translations 
irrespective of the way they are presented in the 
dictionary. It can be lexical or syntactic ambiguity 
that manifests itself in different lexico-syntactic 
structures of the source sentence, one-word 
translation variants within the same lexical 
meaning (of the adjuration type discussed above) 
or more complex phrases that translate a source 
word (of the adventurer type above).  
 
3 UNL module in ETAP 
One of ETAP-3 options is translation between 
Russian and the Universal Networking Language 
(UNL), put forward by H. Uchida of the United 
Nations University. Full specification of UNL and 
references to publications can be found at 
http://www.undl.org. 
UNL is a formal language intended to represent 
information in a way that allows the generation of 
a text expressing this information in a large 
number of natural languages. A UNL expression is 
an oriented hyper-graph that corresponds to a NL 
sentence in the amount of information conveyed. 
The arcs are interpreted as semantic relations like 
agent, object, time, place, manner, etc. The nodes 
are special units, the so-called Universal Words 
(UW), interpreted as concepts, or groups of UWs. 
The concepts are built on the basis of English. 
When needed, English concepts can be modified 
by means of semantic restrictions in order to match 
better with the concepts of other languages. The 
nodes can be supplied with attributes which 
provide additional information on their use in the 
given sentence, e.g. @imperative, @generic, 
@future, @obligation. 
3.1 Architecture  
Since ETAP-3 is an NLP system based on rich 
linguistic knowledge, it is natural to maximally re-
use its knowledge base and the whole architecture 
of the system in all applications. Our approach to 
UNL (described in Boguslavsky et al 2000) is to 
build a bridge between UNL and one of the 
internal representations of ETAP, namely 
Normalized Syntactic Structure (NormSS), and in 
this way link UNL with all other levels of text 
representation, including the conventional 
orthographic form of the text.  
The level of NormSS is best suited for 
establishing correspondence with UNL, as UNL 
expressions and NormSS show strong similarities. 
The most important of them are as follows: 
a) Both UNL expressions and NormSSs occupy 
an intermediate position between the surface and 
the semantic levels of representation. They roughly 
correspond to the so-called deep-syntactic level. At 
this level the meaning of lexical items is not 
decomposed into semantic primitives, and the 
relations between lexical items are language 
independent. 
b) The nodes of both UNL expressions and 
NormSSs are terminal elements (UWs in UNL vs. 
lexical items in NormSS) and not syntactic 
categories. 
c) The nodes carry additional characteristics 
used in particular to convey grammatical 
information (attributes). 
d) The arcs of both structures are non-
symmetrical dependencies. 
At the same time, UNL expressions and 
NormSSs differ in several important respects:  
a) All nodes of NormSSs are lexical items, while 
a node of a UNL expression can be a sub-graph. 
b) Nodes of a NormSS always correspond to one 
word sense, while UWs may either be broader or 
narrower than the corresponding English words. 
c) A NormSS is a tree, while a UNL expression 
is a hyper-graph, which is a much more 
complicated object. Its arcs may form loops and 
connect sub-graphs.  
d) The relations between the nodes in a NormSS 
are purely syntactic and are not supposed to 
convey a meaning of their own, while UNL 
relations denote semantic roles.  
e) Attributes of a NormSS mostly correspond to 
grammatical elements, while UNL attributes often 
convey a meaning that is expressed in English or 
other natural languages by means of lexical items 
(e.g. modals).  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 6 
 
UNL Structure  
English Normalized 
Syntactic Structure 
Russian Normalized 
Syntactic Structure 
Russian Surface 
Syntactic Structure  
Russian Morphological 
Structure 
Russian Sentence 
English Surface 
Syntactic Structure  
English Morphological 
Structure 
 
English Sentence  
 
f) A NormSS contains information on the word 
order, while a UNL expression does not say 
anything to this effect. 
These differences and similarities make the task 
of establishing a bridge between UNL and 
NormSS far from trivial but feasible. Between the 
two types of NormSS readily available in ETAP ? 
the Russian and the English one ? we have chosen 
the latter, since it is the English concepts that 
serve for UNL as building blocks.  
The architecture of the UNL module of ETAP-3 
is given in Fig. 6. 
3.2 UNL vs. English vs. Russian 
As shown in Fig. 6, the interface between UNL 
and Russian is established at the level of the 
English NormSS. It ensures the maximum reuse 
of ETAP?s English-to-Russian machine 
translation facility.  
In the simple case, this scenario suggests that 
the UNL ? Natural Language link can be localized 
within the English dictionary. This dictionary will 
only provide an English correspondence to UNL, 
which in most cases is not very difficult, and all 
the rest will be taken care of by the translation 
engine of ETAP. In this case, direct link between 
Russian and UNL is not needed at all, as long as 
ETAP covers the English-to-Russian 
correspondence.  
However, the situation is not that simple. If we 
try to look at one language (Russian) through the 
perspective of another one (English), we 
encounter well-known problems. Let us illustrate 
the issue with an example. In Russian, there is no 
neutral equivalent of the English non-causative 
verb to marry as represented in sentences like 
John married Ann in June. The expression that 
exactly corresponds to this English verb ? vstupat? 
v brak (?to contract a marriage?) ? is an official 
term and is not used in everyday life. Instead, 
Russian speakers make use of two different 
expressions: zhenit?sja, if the agent of the action is 
a male, and vyxodit? zamuzh, if it is a female. 
Since the English and the Russian words differ in 
their meaning, they correspond to different UWs. 
The UW for English to marry looks like (1), while 
Russian expressions have UNL equivalents with a 
more narrow meaning ? (2) and (3), respectively 
(for simplicity?s sake, only the relevant fragments 
of the UWs are given):  
(1) marry(agt>human) 
(2) marry(agt>male) 
(3) marry(agt>female)  
(Here agt stands for ?agent?).  
Suppose the UNL expression that we receive at 
the input of our generator contains UW (2). Since 
we have to pass through English, we must first 
translate this concept into English and then 
translate the English word into Russian. But 
English has no direct equivalent of (2). It only has 
a word with a more general meaning ? to marry. 
If our objective were to get the English text, this 
word would be perfectly in place. But since our 
target language is Russian, we cannot stop here 
and have to make a difficult choice between two 
different Russian equivalents.  
This is exactly the problem that faces any 
translator from English into Russian, human or 
machine. Sometimes such a problem can be easily 
solved with the help of the context, sometimes it 
is less easy to solve or even unsolvable. For 
example, in the case of blue vs. goluboj ? sinij 
discussed in 2.4 the context would hardly help to 
choose an appropriate Russian translation. 
However, in our example (2) the UNL source 
expression provides unambiguous information 
that allows avoiding this problem altogether, since 
the UW has only one correlate in Russian. If we 
pass from UNL to English and lose sight of the 
UNL source, we will lose the control of the 
semantic information and the quality of the output 
will deteriorate. This should not be permitted. Our 
solution to this problem is presented in 3.3. 
In view of the above, it may seem that a better 
idea would be to sacrifice the benefit of reuse and 
establish a direct link between UNL and Russian.  
However, the architecture shown in Fig. 6 has 
two more advantages that seem crucial.  
First, this architecture allows us to make the 
UNL module of ETAP multilingual, that is to link 
UNL not only with Russian but also with English. 
In view of this perspective, it is reasonable to 
produce a full-fledged English NormSS that is 
much closer to UNL than the Russian one.  
Second, the stock of the UNL concepts is 
continuously growing through the contributions 
coming from diverse languages. The UNL 
dictionaries of different languages grow at 
different rates and in different directions. Very 
often, the generator of language L1 receives the 
UNL input produced by the UNL group of 
language L2 that contains UWs that are absent 
from the UNL-to-L1 dictionary. This happens 
particularly often with the so called multi-word 
UWs of the type  
(4) International Research and Training 
Institute for the Advancement of Women 
(pof>General Assembly {(pof>United 
Nations)}).  
If our only source of lexical knowledge were 
the UNL ? Russian dictionary, we would not be 
able to interpret such UWs, had they not been 
introduced in this dictionary in advance.  
Our UNL-to-English architecture provides a 
universal solution to all difficulties of this kind. If 
the UW is not listed in the UNL dictionariy of 
ETAP, it is analyzed by means of the ETAP 
English dictionary and, if it is a multi-word 
expression, the English parser, which results in a 
reasonably good representation of the UW.  
Moreover, it is often possible to correctly 
translate a UW that is absent from ETAP?s UNL 
dictionary even if its headword is ambiguous. For 
example, if we receive UW  
(5) open(mod<thing)  
and do not find it in our UNL dictionary, we can 
replace it with the English word that stands in the 
position of the headword, that is open. However, 
this headword is ambiguous. In ETAP?s English 
dictionary there are three entries for open - the 
adjective, the verb and the noun. A simple rule 
allows selecting the correct entry on the basis of 
the UW restriction: (mod<thing) means that the 
headword serves as a modifier of things. Hence, 
its English correlate is an adjective and not a verb 
or a noun.  
3.3 UNL dictionary vs. English dictionary vs. 
Russian dictionary  
The UNL-related information is distributed 
among the three ETAP dictionaries: UNL, English 
and Russian. The general idea is to combine (a) 
the idea of having the English NormSS as an 
intermediate level between UNL and the Russian 
NormSS and as a source of Russian and English 
generation and (b) the requirement of adequately 
treating cases of non-isomorphism between the 
English and the Russian concepts.  
As shown in section 2.1, the ETAP dictionary 
entry contains several bilingual sub-zones, 
according to the number of working languages. In 
particular, the Russian dictionary has sub-zones 
for English and UNL, the English dictionary ? for 
Russian and UNL and the UNL dictionary ? for 
English and Russian.  
Let us consider two cases: (1) the Russian and 
the English words are synonymous (as, for 
example, to divorce and razvodit?sja) and (2) they 
are not synonymous (as, for example, to marry 
and zhenit?sja).  
The relevant fragments of the dictionary entries 
(with some simplifications) are as follows. 
 
UNL dictionary: 
NAME: divorce(agt>human) 
ZONE:EN 
TRANS: divorce 
ZONE:RU 
<none> 
NAME: marry(agt>human) 
ZONE:EN 
TRANS: marry 
ZONE:RU 
<none> 
NAME: marry(agt>male) 
ZONE:EN 
<none> 
ZONE:RU 
TRANS: zhenit?sja  
 
English dictionary 
NAME: divorce 
ZONE: RU 
 TRANS: razvodit?sja  
ZONE:UNL 
 TRANS: divorce(agt>human) 
NAME: marry 
ZONE: RU 
 TRANS: zhenit?sja / vyxodit? zamuzh 
ZONE:UNL 
 TRANS: marry(agt>human) 
 
Russian dictionary 
NAME: razvodit?sja  
ZONE: EN 
 TRANS: divorce 
ZONE:UNL 
 TRANS: divorce(agt>human) 
NAME: zhenit?sja  
ZONE: EN 
 TRANS: marry 
ZONE:UNL 
 TRANS: marry(agt>human) 
 
Suppose we have to process a UNL expression 
that contains UW ?divorce(agt>human)?. Since 
this concept corresponds to both English and 
Russian words, we can do safely without any 
information on the Russian word in the UNL 
dictionary and obtain the NormSS with English to 
divorce taken from the English zone of the UNL 
entry. This NormSS allows generating both 
English and Russian texts by means of the 
standard ETAP transfer and generation facilities.  
Let us consider the source UNL expression that 
contains UW ?marry(agt>human)?. It may have 
come from the language that, like English, 
German or Spanish, but unlike Russian or Polish, 
does not distinguish between the male -marriage 
and the female-marriage. The UNL dictionary 
entry for this UW will have the English translation 
but no Russian one, since Russian has no direct 
correlate for this concept. The problem of finding 
an appropriate Russian term is shifted to the level 
of the NormSS. At this level, we will have to find 
an equivalent of English to marry, just as if we 
translated from English and not from UNL. In this 
case, the UNL source does not help us make a 
choice between two types of marriage. What does 
help is the mechanism of the interactive resolution 
of translational ambiguity described above, in 2.4.  
Finally, let us examine the most interesting case 
- a UNL expression with UW ?marry(agt>male)?. 
The dictionary entry of this UW is symmetric to 
the entry of ?marry(agt>human)?: it contains a 
Russian correlate but no English one. In this 
situation, both English and Russian generations 
are not quite straightforward. As there is no direct 
English equivalent of this UW, the translation 
should be found by means of the UNL Knowledge 
Base (Uchida, 2003). In the absence of the 
operational version of KB, the general solution for 
processing an unknown UW is to extract the 
headword of the UW (marry) and treat it as an 
English word (cf. above, 3.2). This solves the 
problem of the generation of the English text. As 
for Russian, zhenit?sja indicated in the Russian 
zone of the UW entry is attached as a feature to 
the English node marry. At the stage of transfer 
from NormSS-English to NormSS-Russian, this 
feature will be lexicalized and replace the word 
marry.  
4 Conclusion 
The organization of lexical resources of the 
ETAP system allows reusing the dictionaries in 
diverse applications, such as machine translation 
in various language pairs and translation to and 
from UNL. In all the applications, there are three 
modes of operation supported by the dictionaries: 
automatic production of a single (most probable) 
translation, automatic production of all possible 
translations and the interactive translation with the 
dialogue-based disambiguation.  
References  
Apresjan Ju.D., Boguslavskij I.M., Iomdin L.L., 
Lazurskij A.V., Mitjushin L.G., Sannikov, V.Z., 
Cinman, L.L. (1992) Lingvisticheskij processor 
dlja slozhnyx informacionnyx sistem. [A 
linguistic processor for advanced information 
systems.] Moskva, Nauka. 256 p. 
Apresjan Ju.D., Boguslavskij I.M., Iomdin L.L., 
Lazurskij A.V., Sannikov V.Z. and Tsinman 
L.L. 1992b. The Linguistics of a Machine 
Translation System. Meta , 37 (1): 97-112. 
Apresjan Ju.D., Boguslavskij I.M., Iomdin L.L., 
Lazurskij A.V., Sannikov V.Z. and Tsinman 
L.L. 1993. Systeme de traduction automatique 
{ETAP}. In: La Traductique. P.Bouillon and 
A.Clas (eds). Montreal, Les Presses de 
l'Universite de Montreal. 
Apresjan, Ju.D. 1995. Integral?noe opisanie 
jazyka i sistemnaja leksikografija [An 
Integrated Description of Language and 
Systematic lexicography.] Moscow, Jazyki 
russkoj kul?tury.  
Apresjan, Ju. D. 2000. Systematic Lexicography. 
Oxford University Press, London, 304 p.  
Apresian Ju., I. Boguslavsky, L. Iomdin, A. 
Lazursky, V. Sannikov, V. Sizov, L. Tsinman. 
2003. ETAP-3 Linguistic Processor: a Full-
Fledged NLP Implementation of the MTT. In: 
MTT 2003, First International Conference on 
Meaning ? Text Theory. Paris, Ecole Normale 
Superieure, Paris, 279-288. 
Blanchon, H. Interagir pour traduire: la TAO 
personnelle pour redacteur monolingue. La 
Tribune des Industries de la Langues. Vol. 17-
18-19, 1995, pp. 28-34. 
Blanchon, H. A Customizable Interactive 
Disambiguation Methodology and Two 
Implementations to Disambiguate French and 
English Input. Proc. MIDDIM'96. Le col de 
porte, Isere, France. 12-14 Aout 1996. Vol. 1/1, 
1996, pp. 190-200. 
Blanchon, H. Interactive Disambguation of 
Natural Language Input: a Methodology and 
Two Implementations for French and English. 
Proc. IJCAI-97. Nagoya, Japan. August 23-29, 
1997. Vol. 2/2, 1997, pp. 1042-1047 
Boguslavsky I., N. Frid, L. Iomdin, L. Kreidlin, I. 
Sagalova, V. Sizov. 2000. Creating a Universal 
Networking Language Module within an 
Advanced NLP System. Proceedings of the 18th 
International Conference on Computational 
Linguistics (COLING 2000), 2000, 83-89.  
Boguslavsky I., L. Iomdin, V. Sizov. 2003. 
Interactive enconversion by means of the 
ETAP-3 system. In ?Proceedings of the 
International Conference on the Convergence of 
Knowledge, Culture, Language and Information 
Technologies?, Alexandria, 2003. 
Boitet, C. & Blanchon, H. Multilingual Dialogue-
Based MT for monolingual authors: the LIDIA 
project and a first mockup. Machine 
Translation. Vol. 9(2), 1995, pp 99-132. 
Hutchins W. J., H. L. Somers. 1992. An 
Introduction to Machine Translation. Academic 
Press, London. 
Mel?cuk I. 1974. Opyt teorii lingvisticheskix 
modelej ?Smysl ? Tekst?. Moscow, ?Nauka? 
Publishers.  
Uchida H. 2003. The UW Manual. 
http://www.undl.org. 
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1071?1080, Dublin, Ireland, August 23-29 2014.
Argument structure of adverbial derivatives in Russian? 
 
Igor Boguslavsky 
Institute for Information Transmission Problems, Russian Academy of Sciences /                 
19, B.Karetnyj, GSP-4, Moscow, Russia 
Universidad Polit?cnica de Madrid / Facultad de Inform?tica, Campus de Montegancedo, 
28660 Boadilla del Monte, Madrid, Spain 
 
Abstract 
Adverbial derivatives (AdvD) of nouns of the type v jarosti ?in a rage?, s nasla?deniem ?with pleasure?, 
pod predlogom ?under the pretext of? etc. often inherit the arguments (actants) of the noun they are 
derived from. However, as a rule, in case of AdvDs these arguments are realized in a way very different 
from the nouns. The main linguistic findings of the paper consist in the set of positions the arguments 
may take with respect to AdvD. In a general case, a actant slot of an AdvD can be either (a) blocked, or 
(b) filled by a dependent of the AdvD itself (e.g. pod predlogom bolezni ?under the pretext of illness?, v 
dokazatel?stvo svoej nevinovnosti ?as a proof of his innocence?), or (c) filled by the dominating verb (po 
privy?ke prosnulsja rano ?woke up early out of habit?, slushal pesnju s nasla?deniem ?listened to the song 
with relish?), or (d) filled somewhere within the clause organized by the dominating verb; in this case the  
AdvD argument may be identified based on (d1) its syntactic position (po privy?ke ?by habit?), or (d2) its 
semantic role with respect to its mother element (v podarok ?as a present?), or (d3) its communicative 
function (v bol??instve ?mostly?). A notation is proposed that permits to present the argument structure of 
AdvDs in a compact way. 
1    Introduction  
This paper is not about computation, it is about linguistics. It does not describe any electronic 
resource. It is not inspired by weaknesses of NLP applications that need to be fixed. We investigate 
certain heavily understudied and even largely unnoticed linguistic phenomena that deserve scientific 
study independently of whether their neglect causes serious errors in today?s NLP applications or not. 
However, on the other hand, taking these phenomena into account is definitely useful for applications, 
such as semantic parsing, question answering, recognizing textual entailments, information extraction 
(e.g. Meyers et al. 1998), machine reading, machine translation, etc. Indeed, semantic parsers should 
represent the content of the text by means of elementary propositions independently of the syntactic 
status of the main predicate in these propositions, be it a verb, a noun, or an adverbial. They should be 
able to understand that such expressions as I believe (that) he is wrong ? My opinion is (that) he is 
wrong) ? In my opinion <to my mind>, he is wrong are different NL realizations of the same 
proposition. Question answering systems should be able to obtain an answer to the question What 
habits does John have? from the sentence John woke up early out of habit, although the argument 
frame of the noun habit does not cover this type of construction (it is the argument frame of the 
adverbial derivative out of habit that does). Similarly, textual entailment recognition systems should 
understand that John woke up early out of habit entails John has a habit of waking up early, which 
again requires correlating argument frames of three different expressions: the noun habit, the ?support 
verb + noun? combination to have a habit and the adverbial out of habit .  
Syntactic derivation is one of the most direct manifestations of the systemic character of the 
lexicon. As is well-known, language is capable of representing the same meaning (or several very 
close meanings) by means of words belonging to different grammatical classes. It is often possible to 
replace words of a certain grammatical category with those of another grammatical category without 
significant modification of their lexical meaning. For example, the concept ?believe? can be realised 
by means of a verb (to believe) or a noun (opinion) or an adverbial phrase (in my opinion, to my mind). 
This is one of the important ideas of ?l?ments de syntaxe structurale de Tesni?re (1959). According to 
Tesni?re, the ability to transfer one category to another at will in fluid speech is the primary tool that 
????????????????????????????????????????????????????????
??This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings 
footer are added by the organisers. Licence details:  
http://creativecommons.org/licenses/by/4.0/ 
1071
makes truly productive speech possible. This mechanism is an integral part of the linguistic capacity 
of humans and deserves in-depth study.  
Lexical resources available to date are not sufficient for that. First, resources such as WordNet do 
not establish synonymy relations across category boundaries, and will not recognize these expressions 
as synonymous. Second, the task does not boil down to relating such expressions to the same concept. 
To reconstruct the proposition, one also needs to find the arguments of all the predicates and identify 
their roles. The latter task, also known as Semantic Role Labeling (SRL), is fairly well studied for the 
arguments of the verbs (cf. CoNLL-2004 and CoNLL-2005 shared tasks on semantic role labeling, 
Computational Linguistics Special Issue on Semantic Role Labeling, 2008). Much less is done in SRL 
of nouns and adjectives (Gerber 2011, Macleod 1997, 1998). Sometimes, adjectives and prepositions 
are included in (verbal and nominal) frames in FrameNet.  However, we are not aware of any attempt 
to investigate arguments of adverbials. This category of words is largely understudied. It is not even 
represented in WordNet. In the introductory paper to the Special Issue on Semantic Role Labeling, the 
SRL task for adverbials is not even mentioned (Marquez et al. 2008). 
Yet, adverbial derivatives are no less entitled to have arguments than the predicates they are 
derived from. If we want to find and identify the arguments of the verb to cause in (1), we would want 
to do the same in (2), where this concept is represented by means of the adverbial due to: 
(1) The minister's interview caused a dramatic fall of the market.  
(2) The market fell dramatically due to the minister's interview. 
However, the problem is that, in a general case, it is more difficult to find these arguments in the 
sentence than it is for prototypical verbal or nominal predicates. The positions of these arguments in 
may differ greatly from the positions of ?classical? arguments.  
The goal of this research is to investigate these non-classical arguments with a view to their 
adequate representation in the dictionary and their automatic detection in the text. We intend to show 
(a) that the arguments of adverbials need to be found and identified, however non-trivial this task may 
be, (b) what their different types are and (c) how the argument structure of adverbial derivatives can 
be represented in the dictionary.  
In this study, we will restrict ourselves to adverbial syntactic derivatives (AdvD) of Russian nouns 
and verbs. We will call a syntactic derivative of word L such a word, or phrase, L? that has the same 
(or very close) meaning as L, but belongs to a different syntactic category and hence displays a 
different behavior. We will denote the word L as the basic word, or keyword, of the derivation. L? may 
be a nominal derivative, or nominalization (to construct - construction, to believe ? opinion), a verbal 
derivative (revolution - revolutionize), an adjectival derivative (government ? governmental), or an 
adverbial derivative (speed ? at the speed of, cause ? due to/because of).    
The plan of our presentation will be as follows. In Section 2 we will characterize briefly some 
properties of AdvD in Russian, then in Section 3 we will review related work on adverbial derivatives 
within the framework of the Meaning ? Text theory. Section 4 will present different types of argument 
structures of adverbial derivatives. How these structures can be represented in the dictionary will be 
shown in Section 5. We will conclude in Section 6. 
2    Adverbial derivatives in Russian  
We will discuss two properties of AdvDs ? their grammatical status and their semantic load. From the 
point of view of the grammatical status, there are two types of AdvD in Russian ? grammatical and 
lexical ones. They will be explained in Sections 2.1 and 2.2. The semantic load of AdvD will be 
commented upon in Section 2.3. 
2.1 Grammatical AdvDs (verbal adverbs) 
Russian has a regular morphological way of constructing AdvDs of verbs ? verbal adverbs 
(deeprichastija) that can be derived of virtually any verb. They serve to express a secondary 
predication attached to the main one.  
(3) On sprosil eto, gljadja ej v glaza. 
?He asked that, looking into her eyes? 
Russian verbal adverbs are similar to participial constructions in adverbial usage (or gerunds) 
existing in a variety of languages. However, they also exhibit significant differences. An important 
peculiarity of the argument behaviour of Russian verbal adverbs is that their subject cannot be 
1072
expressed in the surface structure and should be co-referential with the subject of the main verb. Their 
other arguments do not have any characteristic properties and are attached to them just as they are 
attached to the finite form of the verb: 
(4a) Petr pokupaet ode?du v modnyx magazinax za ogromnye den?gi ?Peter buys clothes in fashionable 
shops for a lot of money?. 
(4b) Pokupaja ode?du v modnyx magazinax za ogromnye den?gi, Petr ve?no po u?i v dolgax ?buying 
clothes in fashionable shops for a lot of money, Peter is always in debt up to his neck?. 
Many languages have absolute constructions, absent in Russian, which allow the subject to be 
attached to the participle and to be non-coreferential with the subject of the main verb: 
(5) His wife buying clothes in fashionable shops, Peter is always in debt up to his neck. 
(6) Spanish: Habiendo pasado m?s de una hora, las piernas comenzaron a flaquear ?more than an 
hour having passed, his legs began to fail?.  
Verbal adverbs may be active, as in (3), or passive, as in (7): 
(7) Budu?i sorvannym, muxomor prodol?aet rasti ?being plucked, the amanita continues to grow?. 
It is to be noted that the implicit subject of the passive verbal adverb budu?i sorvannym ?being 
plucked? is the second argument of the active form sorvat? ?pluck? and, according to the general rule, 
is co-referential with the subject of the main verb muxomor ?amanita?.     
2.2 Lexical AdvDs  
Besides verbal adverbs derived by means of inflection, there is a large number of AdvDs that are 
expressed by adverbs (good ? well, systematic ? systematically, can - possibly), prepositions (cause ? 
due to/because of) or prepositional phrases (love ? with love). The latter case is the most important, 
since a large number of AdvDs is formed in this way. It is to be noted that different AdvDs are formed 
with different prepositions. Semantically, lexical AdvDs are in many cases equivalent to verbal 
adverbs. Some examples: ot?ajanie ?dispair? ? v ot?ajanii ?(being) in dispair?, interes ?interest? ? s 
interesom ?with interest, feeling interest?, ode?da ?clothes? ? v ode?de ?being dressed?, bodrstvovat? 
?be awake? ? najavu ?(being) awake?, obed ?dinner? ? za obedom ?at dinner?, za??ita ?protection? ? pod 
za??itoj ?under protection, being protected?, pomo?? ?help? ? s pomo??ju or pri pomo??i ?with the help 
of, being helped by?. 
2.3 Pure AdvDs vs. semantically loaded AdvDs    
One has to distinguish between two types of AdvD: ?pure? derivatives, which do not contain any 
additional meaning components absent in the meaning of the basic predicate, and semantically 
enriched derivatives, for which the reverse is true. As an example of the latter, let us consider the 
phrase pod imenem X ?under the name of X? as represented in  
(8) Napoleon exal pod imenem gerzoga Vi?entskogo, to est? Kolenkura ?Napoleon was travelling 
under the name of duke of Vicenza, that is of Colencour?. 
The meaning of this sentence contains a component of replacing the true name with another one. 
Pod imenem X ?under the name of X? does not mean that the person in question has the name of X, but 
rather that this person or somebody else wants other people to refer to him/her as X while the speaker 
knows, believes or admits that this is not the true name. Obviously, the noun imja ?name? has no such 
component (as opposed to pseudonym or nickname). It cannot even be ascribed to the preposition pod 
?under?, either, since in (8) this preposition has obviously the same meaning as in phrases pod 
nazvaniem <zagolovkom, rubrikoj> ?under the title <heading>? to which the component of 
concealment is completely alien. Phrases like pod imenem (or its English counterpart under the name) 
that are to a certain extent idiomatic are lexical units in their own right and have their own entries in 
the dictionary.  
As for ?pure?, non-idiomatic PP AdvDs, they hardly qualify for separate lexical units. However, 
irrespective of whether an AdvD is idiomatic or not, it should be supplied with the information about 
its arguments: if a sentence contains the phrase at request, e.g. I called Mary at the request of my 
father, we should be able to answer the question ?Who asked whom to do what?? In this example, we 
are entitled to infer that speaker?s father asked him/her to call Mary. 
Note: strictly speaking, the content of father?s request need not necessarily be ?Make a call to 
Mary?. He could have asked his son/daughter to invite Mary for dinner. However, the phrase I called 
Mary at the request of my father is still appropriate provided the act of inviting Mary contains calling 
1073
her as its essential part.  
In order to be able to make such an inference, one needs to represent the argument structure of 
AdvDs fully and unambiguously and relate it to the argument structure of the basic word.    
As an example of how the correlation between argument structures of different words can be 
established, we can recall the description of converse terms in the theory of Lexical Functions within 
the Meaning ? Text approach (Mel?uk et al. 1984a, 1984b, 1988, 1992). Conversives (the input and 
the output of the Lexical Function Conv) are a pair of words that denote the same situation but differ 
in the way their arguments are ordered, e.g. buy ? sell. Like any verb, buy and sell are supplied with 
subcategorization frames (aka government patterns in the Meaning ? Text approach) that list all their 
arguments and their means of expression. On the other hand, their being conversives implies that their 
lexical functional description should indicate the correlation between their argument structures. 
Namely, the first argument of sell (?who sells??) corresponds to the third argument of buy (?from 
whom buys??), the third argument of sell (?to whom??) ? to the first argument of buy (?who buys??), 
while the second and the fourth arguments (?what?? and ?for how much??) occupy the same positions 
within government patterns of both verbs. This correlation is rendered by the numerical index attached 
to the Conv symbol: Conv3214(sell) = buy: the j-th position in the index is occupied by i if the j-th 
argument of the output corresponds to the i-th actant of the input.  
For AdvDs the problem of the correlation of their argument structure with that of the basic 
predicate is particularly acute. While sell and buy are rightful lexical units entitled to have their own 
government patterns, adverbial collocations of the type v jarosti ?in anger?, po privy?ke ?by habit?, s 
akcentom ?with an accent? or pod predlogom ?under the pretext of? are not usually treated as separate 
lexical units. It is assumed that all necessary information about their meaning and use should be 
formulated in the lexical entry of the noun. To what extent does an AdvD inherit the argument 
structure of the noun? If not in full, how should its argument structure be described in the dictionary?  
Before answering this question, we will recall how syntactic derivatives, and AdvD in particular, 
are treated in the theory of lexical functions. 
3    Syntactic derivatives in the theory of lexical functions  
Two types of syntactic derivation are distinguished: ?zero? and ?actant? derivation. Zero syntactic 
derivatives (S0, V0, A0 ? Adv0) have the same meaning as the keyword but belong to a different part of 
speech: S0(investigate)=investigation, V0(investigation)=investigate, A0 (government)=governmental, 
Adv0 (good)=well. Actant derivatives (Si, Ai and Advi) are oriented towards one of the actants of the 
keyword in the following sense.  
Si is a standard name of the i-th actant of the keyword (S1(teach) = teacher, S2(teach) = (subject) 
matter [in high school], S3(teach) = pupil).  
Ai also has a bearing to the i-th actant, but in the adjectival syntactic status. This means that its 
typical syntactic function is to modify a noun that fills the i-th valence slot of the keyword. A 
grammatical way of expressing Ai is participles. A1 is equivalent to an active participle, and A2, to a 
passive participle. For example, adjectival derivatives of the verb to control are either an active 
participle controlling (A1) or a prepositional phrase under control (A2); cf. controlling organizations 
(?organizations that control something?) ? operations under control (?operations that are being 
controlled by someone?).  
Things are more complicated with adverbial actant derivatives (Advi). This function is defined as 
follows: 
?Advi ? determining property of the action by the i-th ? actant of L according to its role in the 
situation denoted by L. Adv1  is roughly equivalent to an active verbal adverb (?while L-ing?) and 
Adv2, to a passive verbal adverb (?while being L-ed?).   
Adv2(bombard) = under bombardment 
Adv1(speed) = at [a speed of...]? (Mel??uk 1996: 55). 
As this definition shows, the only link between the actantial structures of the keyword and the 
adverbial derivative of the Advi type is the i-th actant of the keyword. Although it is not stated 
explicitly, one can presume that the i-th actant?s position in the sentence is the position of the subject 
of the verb to which Advi is attached. In (9) the first actant of anger is obviously Mary, the first actant 
of the verb reject, and not John or anybody else. 
(9) Mary rejected John?s proposal with anger.  
1074
This is understandable, since lexical function Advi is intended to model the behaviour of verbal 
adverbs and, as mentioned above, they normally correlate strongly with the subject (first actant) of the 
main predicate (except for the absolute constructions). However, lexical function Advi  provides no 
information as to the position of other actants of the keyword. The next section will show that this 
information is essential for text understanding and that different AdvDs significantly differ in this 
respect.  
4 Argument structure of adverbial derivatives 
If we compare adverbial derivation with other types of syntactic derivation, we will encounter an 
important difference. Argument structure of such derivatives as Convij or Si can be easily 
characterized in terms of the argument structure of the keyword. When we pass from the keyword to 
such a derivative, we may find that an actant either stays in its initial position (teach mathematics ? 
teacher of mathematics), or changes its number (the verb dominates the preposition ? the preposition 
depends on the verb), or gets blocked altogether (drive home - *driver home). However, the syntactic 
position of the actant can only change in a very limited way. If a valence slot of the keyword is 
expressible in the sentence with its Convij or Si at all, the actant should either be attached to the 
derivative directly, or through a copula or other lexical functional verb (Peter teaches mathematics ? 
Peter is a teacher of mathematics) or by means of the apposition (Peter, a teacher of mathematics).  
The matters stand differently with AdvDs. Their actant properties are much more diverse than 
those of Convij or Si, or even of verbal adverbs. In some cases, the position of their actants in the 
sentence cannot be characterized in purely syntactic terms. In a general case, a valence slot of an 
AdvD can be either blocked, or: 
? filled by a dependent of the AdvD itself; 
? filled by the dominating verb; 
? filled somewhere within the clause organized by the dominating verb; in this case the  AdvD 
actant may be identified based on:   
o its syntactic position; 
o its semantic role; 
o its communicative function.    
We will illustrate all these possibilities below. 
4.1 Valence slots filled by a dependent of AdvD 
In the canonical case, AdvD inherits most of the governing properties of the keyword.  
(10a) skorost? 800 km/?as ?a speed of 800 km per hour?, 
(10b) Samolet letel so skorostju 800 km/?as ?the aircraft was flying at a speed of 800 km per hour?,  
(11a) sovet Ivana ?Ivan?s advice?, 
(11b) po sovetu Ivana  ?at Ivan?s advice?,  
(12a) Eto podarok ot Viktora lit. ?this is a present from Victor?, 
(12b) Ja polu?il eto v podarok ot brata  lit. ?I got it as a present from my brother?. 
In some cases, governing properties of AdvD are different from those of the keyword. Let us 
consider the pair predlog ?pretext? ? pod predlogom ?on/under the pretext of? that manifests an 
interesting correlation of actant properties. The noun predlog ?pretext? has three valence slots: P is a 
pretext for X for doing Q = ?wishing to do Q, which violates norms of ethics, or politeness, X uses 
situation P to do Q; he thinks that P justifies Q? (Boguslavskaya 2003). When predlog is used without 
the preposition pod, it can attach actant Q (= the action carried out) but not P (= false motive). The 
latter can only be expressed outside the phrase containing predlog: 
(13a)  Golovnaja bol?  [P]  ? xoro?ij predlog, ?toby ostat?sja doma [Q] ?headache [P] is a good pretext 
for staying at home [Q]?. 
(13b) *predlog golovnoj boli [P] ?the pretext of the headache [P]? 
AdvD pod predlogom has opposite governing properties. Actant P (= false motive) can now be a 
dependent of AdvD while actant Q (= the action) loses this property and moves to the position of the 
dominating word: 
(13c) Ona ostalas? doma  [Q] pod predlogom golovnoj boli [P] ?she stayed at home [Q] on the pretext 
of the headache [P]?.  
1075
4.2 Valence slots filled by the dominating verb 
Adverbial derivatives of many predicates which have a propositional valence slot fill it by means of 
the dominating verb. One example is (13c) above. In the following examples, the actant at issue is 
underlined in both the sentence with the basic predicate, and in the sentence with the AdvD. 
(14a) Ljusja dokazala polnuju sda?u svoix pozicij tem, ?to pocelovala Marata v nos ?Ljusja proved 
complete surrender by kissing Marat on the nose?. 
(14b) V konce koncov sama Ljusja priznala grubost? svoego zame?anija i v dokazatel?stvo polnoj 
sda?i svoix pozicij pocelovala Marata v nos ?after all, Ljusja herself acknowledged that her remark 
had been rude, and as a proof of complete surrender kissed Marat on the nose? (AdvD v dokazatel?stvo 
?as a proof?).  
(15a) On otvetil mnogozna?itel?nym my?aniem ?he responded with a significant mumble?.   
(15b) V otvet on ?to-to mnogozna?itel?no promy?al ?in response he mumbled something in a 
significant manner? (AdvD v otvet ?in response?). 
(16a) Ja s?itaju, ?to ?dat? bol??e ne?ego ?I think there is nothing more to wait for?. 
(16b) Po-moemu, ?dat? bol??e ne?ego ?in my opinion, there is nothing more to wait for? (AdvD po-
moemy ?in my opinion?). 
4.3 Valency slots filled by dependents of the dominating verb 
If a valency slot of an AdvD is filled by a dependent of the dominating verb, the question arises as to 
how to specify its position among other dependents of the verb. We will show that this position can be 
identified based on the syntactic function (4.3.1), semantic role (4.3.2) or communicative function 
(4.3.3). 
4.3.1 Syntactic function 
As mentioned in section 2, in the prototypical case of adverbial derivation, that of verbal adverbs, one 
of the actants of the keyword (the first or the second) is necessarily co-referential with the first actant 
(subject) of the dominating verb. Since this actant is not expressible as a dependent of the AdvD, the 
subject of the dominating verb is its only manifestation in the sentence. In this sense, we can say that 
the valence slot is filled by the subject of the dominating verb. If it is the first actant of the keyword 
that is co-referential with the subject, we are dealing with the active verbal adverb (Adv1, in Mel??uk?s 
terminology). If it is the second actant, the verbal adverb (Adv2) is passive. If the co-reference 
requirement is not met, sentences with verbal adverbs are usually ungrammatical in Russian. Cf. a 
textbook example of a wrong use of a verbal adverb *Podjez?aja k stancii, u menja sletela ?ljapa 
?when approaching the station, my hat fell down?.  
As for non-verbal AdvDs, this requirement holds for some of them and not for others. Let us 
discuss one example: the verb privyknut? ?have a habit of?.  It has two valencies ? ?who has the habit?? 
and ?what does the habit consist in??. Its AdvD is po privy?ke ?by habit?. Although it does not take any 
syntactic dependents, sentences with this AdvD provide unambiguous information on who has a habit 
and what it consists in/ hence, both valencies are filled:   
(17) Ivan po privy?ke ostavil dver? otkrytoj ?by habit, Ivan left the door open? 
The identity of the first actant of po privy?ke and the subject of the main verb can be easily 
demonstrated. Let?s take the verbs zanimat? ?to borrow? and odal?ivat? ?to lend?. Being conversives, 
they denote the same situation and sentences (18a) and (18b) are synonymous: 
(18a) Ivan zanjal u soseda 1000 rublej ?Ivan borrowed 1000 roubles from the neighbour? 
(18b) Sosed odol?il Ivanu 1000 rublej ?the neighbour lent Ivan 1000 roubles?.  
If AdvD po privy?ke ?by habit? is introduced in (18a) and (18b) in the same position, the sentences 
will no longer be synonymous. (19a) refers to the habit of Ivan while (19b) ? to the habit of the 
neighbour. 
(19a) Po privy?ke Ivan zanjal u soseda 1000 rublej ?by habit Ivan borrowed 1000 roubles from the 
neighbour? 
(19b) Po privy?ke sosed odol?il Ivanu 1000 rublej ?by habit the neighbour lent Ivan 1000 roubles?.   
4.3.2 Semantic role 
Another type of constraint is manifested by AdvDs v podarok ?as a present?, v dar ?as a gift?, v 
nagradu ?in reward?. Nouns of the present / gift / reward type have three valence slots: the agent of 
1076
presenting something (X), the theme (Y) and the recipient (Z). The AdvDs co-occur with a large set of 
verbs concentrated around the meaning of ?transfer?: polu?at? ?receive?, prinimat? ?accept?, trebovat? 
?demand?, prosit? ?request?; prinosit? ?bring (on foot)?, privozit? ?bring (by transport)?, dostavljat? 
?deliver?, posylat? ?send?, otpravljat? ?dispatch?, prednazna?at? ?intend for?, ?alovat? ?grant?, podnosit? 
?offer?, predlagat? ?offer?, peredavat? ?pass (to)?, vru?at? ?hand over?, davat? ?give?, otdavat? ?give 
back?, etc. It is impossible to associate the subject of the main verb with any single actant of AdvD, 
since each of the three actants can perform the role of the subject: 
(20a) Otec (X) privez do?eri v podarok o?erelje ?Father (X) brought a necklace as a present to his 
daughter?. 
(20b) Maria (Z) prinjala o?erelje v podarok ?Mary (Z) accepted the necklace as a present?. 
(20c) O?erelje (Y) dostalos? ej v podarok ot babu?ki  ?the necklace (Y) came to her as a present of her 
grandmother?.  
It is not syntactic constraints that regulate the position of the actants of these AdvD with respect to 
the main verb but semantic ones. The correlation between the valence slots of AdvD and the main 
verb can be formulated IN TERMS OF SEMANTIC ROLES as follows: if a valence slot of AdvD which 
corresponds to semantic role R (Agent, Theme, Recipient) is instantiated, it is either filled by an AdvD 
dependent (as in v podarok do?eri ?as a present to one?s daughter?, v podarok ot otca ?as a present 
from one?s father?), or by a dependent of the main verb which performs the role R with respect to the 
predicate of transfer within the meaning of the main verb. For example, in (20a-c) the subjects otec 
?father?, Maria ?Maria? and o?erelje ?necklace? all play different semantic roles with respect to the 
main verb: the father is the Agent of bringing, Maria is the Recipient of giving (?accept? ? ?agree to be 
given?), and the necklace is the Theme of coming. Accordingly, these words are the Agent, Recipient 
and Theme of the present, respectively.    
It should be stressed that the semantic role of a noun phrase with respect to the dominating verb 
may be different from its semantic role with respect to an inner predicate of this verb. For example, in   
(21a) Ivan potreboval poltsarstva ?Ivan demanded half of the kingdom? [= ?demanded that he were 
given half of the kingdom?] 
Ivan is the Agent of demanding and at the same time the Recipient of giving. What is important for 
AdvD of the v podarok type is the role of the actant with respect to giving. Therefore, in (21b) Ivan is 
the Recipient and not the Agent of reward: 
(21b) Ivan potreboval sebe v nagradu poltsarstva ?Ivan demanded half of the kingdom as a reward?. 
4.3.3 Communicative function 
Boguslavsky (2005) discussed the argument frames of noun bol??instvo ?majority, most of? and 
men??instvo ?minority?. It was shown that these words have three arguments: the whole, a part of the 
whole and the property of the part that is shared by most of the elements of the whole. Here we will 
only be interested in one of these arguments ? that of the whole, expressed prototypically by 
preposition iz ?of? as represented in phrases the majority of cases, most of the students. In sentences 
with AdvD v bol??instve ?mostly, for the most part? this valence slot is filled, as a rule, by the subject 
of the dominating verb:  
(22) Oni byli arestovany i podverglis? v bol??instve svoem ssylke v Gvianu i na Sej?el?skie ostrova  
?they were arrested and mostly exiled to Guiana or Seychelles? [= ?most of them were exiled??] 
However, this is not the only possible syntactic role for this actant. In (23) it is the direct object 
inostrannye knigi ?foreign books? that fills the valence slot of the whole: 
(23) Russkie knigi byli sobrany pokojnym mu?em knjagini?, inostrannye ?e ? v bol??instve vyvezla 
sama Anna Arkadjevna iz Pari?a  lit. ?Russian books were collected by the late husband of the 
princess?, while foreign books (dir. object) mostly Anna Arkadjevna brought from Paris herself? [= 
?most of the foreign books?]. 
And even this is not all. What is essential here is not the syntactic role of the actant but the 
communicative organization of the clause. The valence slot of the whole should be filled by the Topic. 
Since the position of the Topic is most often held by the subject, it is clear why it is the subject that for 
the most part fills this valence slot. The claim that the valency of the whole of v bol??instve ?mostly? is 
Topic-oriented can be confirmed by a  minimal pair of sentences below. 
Due to the relatively free word order in Russian, the Topic-Focus distinction is rarely marked 
syntactically or lexically. The same syntactic structure may correspond to different Topic-Focus 
1077
articulations. In most cases it is the clause-initial phrase that is the Topic of the sentence1. In (24a) and 
(24b) the syntactic structures are the same but the word order and the Topic-Focus articulation s are 
different. Therefore, the valency slot of the whole is filled differently: 
(24a) ?en??iny (Topic) v bol??instve svoem sideli v zale   
lit. ?the women (Topic) in majority were sitting in the hall? 
?most of the women were in the hall?  
(24b) V zale sideli (Topic)  v bol??instve svoem ?en??iny  
lit. ?in the hall were sitting (Topic) in majority the women?  
?most of those in the hall were women? 
5    Representation of adverbial derivatives in the lexicon 
From the viewpoint of the theory of phraseology developed in Mel??uk 1995, AdvDs belong to the 
class of collocations and should be represented in the dictionary within the entries of their nominal 
component ? the keyword of the derivation (Mel??uk 1995: 184). The entries of the keywords contain 
all the information on their argument frames. Based on this information, one can represent the 
argument frame of AdvD in a compact way.  
AdvD are to be described in the dictionary entry of the keyword, as a value of the Adv Lexical 
Function. The argument structure of the derivative is described by means of an index attached to the 
Adv symbol. We showed above (in 2.3) how the correlation between the arguments of the conversives 
can be stated by means of the numerical index attached to the symbol of the Conv Lexical Function. 
We are going to describe AdvDs along similar lines, but the index should be somewhat more 
elaborated. Namely, the argument index of the Lexical Function Adv is constructed as follows: 
? it consists of n positions, according to the number of valency slots of the keyword; the 1st 
position corresponds to the 1st slot of the keyword, the 2nd position corresponds to the 2nd slot 
etc. 
? each position contains information on whether the corresponding valency can be filled if the 
keyword is represented by its adverbial derivative and, if so, how it should be filled. This 
information is one of the following: 
o 0, if the slot cannot be filled, 
o i, if the slot is filled as the i-th slot of the keyword, 
o G, if the slot is filled by the syntactic governor of the AdvD, 
o i(G), if the slot is filled by a phrase that is the i-th actant of the syntactic governor of 
AdvD or has semantic role i with respect to this governor, 
o Topic, if the slot is filled by the Topic of the clause to which belongs AdvD.   
Let us show how the properties of AdvDs of different types can be represented using this notation. 
Each illustration consists of three parts: (a) the keyword and its argument frame, (b) an example 
containing AdvD, (c) representation of the argument frame of AdvD with a short comment. 
(25a) skorost? ?speed? (?what has the speed??, ?the value of the speed?) 
(25b) Avtomobil? m?alsja so skorostju 200 km/?as ?the car moved at the speed of 200 km/hour? 
(25c)  so skorostju ?at the speed of? = AdvG,2 [the 1st argument is the syntactic governor of AdvD 
(?moved?), and the 2nd is the 2nd argument of the keyword] 
(26a)  jarost? ?rage? (?who is in the state of rage??, ?what was the cause of this state??)  
(26b) On v jarosti razorval pis?mo na klo?ki ?in a rage, he tore the letter to pieces?.  
(26c) v jarosti ?in a rage? = Adv1(G),0 [the 1st argument is the 1st argument (?he?) of the syntactic 
governor (?tore?), the 2nd argument cannot be realized with AdvD] 
(27a) nasla?denie ?relish, enjoyment?(?who enjoys? ?, ?what does one enjoy??) 
(27b) On s nasla?deniem vykuril sigaru ?he smoked a cigar with relish?. 
(27c) s nasla?deniem ?with relish? = Adv1(G),G [the 1st argument is the 1st argument (?he?) of the 
syntactic governor (?smoked?), the 2nd argument is the syntactic governor itself. Note the important 
difference between (26c) and (27c): in case of ?with relish? the main predicate refers to the source of 
the emotional state: smoking a cigar is what made him feel relish; in (26b) the reason of feeling rage is 
not specified. This difference is reflected in different indices] 
(28a) somnenije ?doubt? (?who doubts??, ?what does one doubt??) 
????????????????????????????????????????????????????????
1 Of course, this is a simplification, the reality is more complicated, but this is a general rule.  
1078
(28b) Ona vrjad li pridet ?she will hardly come? 
(28c) vrjad li ?hardly, unlikely? = Adv0,G [the 1st argument cannot be realized with AdvD, the 2nd is 
expressed by the syntactic governor] 
(29a)  podarok ?a present? (?who gives??, ?what is given??, ?to whom??) 
(29b)  Otec privjoz Marii v podarok o?erelje ?Father brought Maria a necklace as a present?. 
(29c)  v podarok ?as a present? = AdvG(Agent),G(Theme),G(Recipient) [in (29b) all the three argument slots of 
AdvD are filled by the corresponding arguments of the predicate of transfer ? prvjoz ?brought?] 
(30a)  bol'?instvo ?majority? (?what constitutes the majority??, ?what is the whole??) 
(30b)  V zale sideli v bol??instve svoem ?en??iny lit. ?in the hall were sitting (Topic) the women?  
?most of those in the hall were women? 
(30c)  v bol??instve ?mostly? = Adv0,Topic [this AdvD is topic-sensitive; in (30b) the Topic is ?those 
who were in the hall?, therefore it is this meaning that fill the valency of the whole]. 
6    Conclusion 
The data presented above show that the argument frames of the adverbial derivatives of predicates are 
much more diverse than it was believed before. The number of the arguments and their roles are 
motivated by the semantics of the predicate they are derived from, but their syntactic realization is 
largely different. We showed a variety of syntactic, semantic and communicative positions the 
arguments of adverbial derivatives may take and how these positions can be described in the 
dictionary in a compact way. This information is needed in many semantics-related tasks but is not 
available in any of the existing lexicographic resources. We proposed a way to represent this 
information in the lexicon in a compact way. Supplied with this information, the lexicon will be able 
to support the extraction of propositions for a variety of applications2.  
References.  
Olga Boguslavskaya. 2003. ?????, ???????. In: ????? ?????????????? ??????? ?????????. ????? 
?????????? ????????. ???. 3. 
Igor Boguslavsky. 2005. ??????????? ?????????? ????. In: ???????????????? ?????? ?????. ??????, ???. 
139-165. 
Computational Linguistics. Special Issue on Semantic Role Labeling, 2008. 
Matthew Steven Gerber. 2011. Semantic Role Labeling of Implicit Arguments for Nominal Predicates.  A 
dissertation submitted to Michigan State University n partial fulfillment of the requirements for the 
degree of Doctor of Philosophy in Computer Science.  
Catherine Macleod, Adam Meyers, Ralph Grishman, Leslie Barrett, Ruth Reeves. 1997. Designing a Dictionary 
of Derived Nominals. Proceedings of Recent Advances in Natural Language Processing, Tzigov Chark, 
Bulgaria, September, 1997. 
Catherine Macleod, Ralph Grishman, Adam Meyers, Leslie Barrett, Ruth Reeves. 1998. NOMLEX: A Lexicon of 
Nominalizations. Proceedings of EURALEX'98, Liege, Belgium, August 1998. 
Lluis Marquez, Xavier Carreras, Kenneth C. Litkowski, Suzanne Stevenson. 2008. Semantic Role Labeling: An 
Introduction to the Special Issue. Computational Linguistics. Special Issue on Semantic Role Labeling, 
2008. 
Igor Mel??uk et al. 1984a. ??????? ?. ?., ?. ?. ??????????, ?. ?. ???????, ? ??. 1984. ???????-
????????????? ??????? ???????????? ???????? ?????. ????? ?????????-??????????????? 
???????? ??????? ???????. Wien.  
Igor Mel??uk et al. 1984b. DEC: Dictionnaire explicatif et combinatoire du fran?ais contemporain. Recherches 
lexico-s?mantiques I. Les Presses de l?Universit? de Montr?al, Montr?al (Qu?bec). 
Igor Mel??uk et al. 1988. DEC: Dictionnaire explicatif et combinatoire du fran?ais contemporain. Recherches 
lexico-s?mantiques II. Les Presses de l?Universit? de Montr?al, Montr?al (Qu?bec). 
Igor Mel??uk et al.  1992. DEC: Dictionnaire explicatif et combinatoire du fran?ais contemporain. Recherches 
lexico-s?mantiques III. Les Presses de l?Universit? de Montr?al, Montr?al (Qu?bec).  
Igor Mel??uk. 1995. Phrasemes in Language and Phraseology in Linguistics. In: M. Everaert, E.-J. van der 
Linden, A. Schenk and R. Schreuder (eds), Idioms. Structural and Psychological Perspectives, 1995, 
Hillsdale, N.J.?Hove: Lawrence Erlbaum Associates, 167-232. 
????????????????????????????????????????????????????????
2 This work was partly supported by the RBRF grant 12-07-00663 and the RFH grant 13-04-00343, which is gratefully 
acknowledged.  
 
1079
Igor Mel??uk. 1996.  Lexical Functions: A Tool for the Description of Lexical Relations in the Lexicon. In: L. 
Wanner (ed.), Lexical Functions in Lexicography and Natural Language Processing, 
Amsterdam/Philadelphia: Benjamins, 37-102. 
Adam Meyers, Catherine Macleod, Roman Yangarber, Ralph Grishman, Leslie Barrett, Ruth Reeves. 1998. 
Using NOMLEX to Produce Nominalization Patterns for Information Extraction. Coling-ACL98 
workshop Proceedings: the Computational Treatment of Nominals Montreal, Canada, August, 1998. 
Lucien Tesni?re.1959. ?l?ments de syntaxe structurale, Klincksieck, Paris.  
 
1080
Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 67?76,
Beijing, August 2010
Interfacing the Lexicon and the Ontology in a Semantic Analyzer 
 
Igor Boguslavsky 
Universidad Polit?cnica de Madrid 
Institute for Information Transmission 
Problems of the Russian Academy of Sci-
ences 
igor.m.boguslavsky@gmail.com
Leonid Iomdin 
Institute for Information Transmission 
Problems of the Russian Academy of Sci-
ences 
iomdin@iitp.ru 
Victor Sizov 
Institute for Information Transmission 
Problems of the Russian Academy of Sci-
ences 
sizov@iitp.ru 
Svetlana Timoshenko 
Institute for Information Transmission 
Problems of the Russian Academy of Sci-
ences 
nyrestein@gmail.com
 
Abstract 
We discuss the possibility to link the 
lexicon of an NLP system with a for-
mal ontology in an attempt to con-
struct a semantic analyzer of natural 
language texts. The work is carried 
out on the material of sports news 
published in Russian media.  
1 Introduction 
Many Semantic Web applications need a 
much deeper semantic analysis of the text 
than is used today. Not only should the 
ontology elements be extracted from the 
textual data but also it is important to 
interpret the text in terms of the ontology. It 
is essential that IE and QA systems should be 
able to discover semantic similarity between 
the texts if they express the meaning in 
different ways. Cf. synonymous sentences (1) 
? (3):  
(1) Real Madrid and Barcelona will meet in 
the semi-finals on Thursday. 
(2) The semi-final match between Real 
Madrid and Barcelona will take place on 
Thursday. 
(3) The adversary of Real Madrid in the semi-
finals on Thursday will be Barcelona.  
If we wish to extract the meaning from the 
text irrespective of the way it is conveyed, we 
should construct a semantic analyzer capable 
of producing identical semantic structures for 
sentences (1)-(3), or at least semantic 
structures whose equivalence can be easily 
demonstrated. 
The problem becomes much more difficult 
if text understanding includes access to text-
external world knowledge. For example, 
sentences (1)-(3) describe the same situation 
as (4).  
(4) The semi-finals on Thursday will see the 
champion of the UEFA Champions League 
2008-2009 and the team of Manuel Pelle-
grini.  
To account for this synonymy, the system 
should know that it was the football club 
Barcelona who won the UEFA Champions 
League in 2008-2009, and that Manuel Pelle-
grini is the coach of Real Madrid. This im-
plies that linguistic knowledge should be 
linked with ontological resources. The crea-
tion of a semantic analyzer of this type goes 
far beyond the task of assigning ontological 
classes to words occurring in the text. It re-
quires a powerful wide-coverage linguistic 
processor capable of building coherent se-
mantic structures, a knowledge-extensive 
lexicon, which contains different types of 
lexical information, an ontology, which de-
scribes objects in the domain and their prop-
erties, a repository of ground-level facts, and 
an inference engine.  
67
A project NOVOFUT aiming at the devel-
opment of a semantic analyzer of this type for 
Russian texts has started at the Institute for 
Information Transmission Problems of the 
Russian Academy of Sciences. It covers the 
domain of news about football. There are 
several reasons for this choice of domain. 
First, the news texts are written primarily for 
the general public, so that their understanding 
does not require specialized expert knowl-
edge. This is a major advantage since it sig-
nificantly facilitates the acquisition of the 
ontology. Second, the language typical of 
sports journalism is rich enough, which 
makes its interpretation linguistically non-
trivial. Last but not least, sports enjoy enor-
mous public interest. There are many sports 
portals publishing multifarious information 
on the daily (and sometimes hourly) basis and 
visited by a lot of people. Enhanced Ques-
tion-Answering and Information Extraction in 
this domain are likely to attract many users.     
The NOVOFUT semantic analyzer reuses 
many types of resources created or accumu-
lated by the team in previous work. In this 
paper we focus on the static resources used 
by the analyzer ? the lexicon and the ontol-
ogy. The plan of the presentation is as fol-
lows. In Section 2 we discuss related work. In 
Section 3 we will briefly describe the linguis-
tic processor we build on and its lexicon. Sec-
tion 4 outlines a small-scale ontology devel-
oped for the project. The correlation between 
natural language words as presented in the 
lexicon and the ontology is the main concern 
of Section 5. In Section 6 the interface be-
tween the ontology and the lexicon is dis-
cussed. Future work is outlined in Section 7.    
2 Related work 
The link between the ontologies and NL texts 
is investigated in two directions ? ?from the 
ontology towards NL texts? and ?from the 
texts towards the ontology?. In the first case 
written texts are used as a means for ontology 
extension and population. To name but a few 
authors, McDowell and Cafarella (2006) start 
from an ontology and specify web searches 
that identify in the texts possible semantic 
instances, relations, and taxonomic informa-
tion. In (Schutz and Buitelaar 2005) an inter-
esting attempt is made to extract ontological 
relations from texts. (Buitelaar et al 2008, 
Magnini et al 2006, Maynard & al. 2006) are 
further advances in the direction of ontology 
population.  
Finding NL equivalents to ontological 
elements and be monolingual or multilingual. 
A metamodel for linking conceptual knowl-
edge with its lexicalizations in various lan-
guages is proposed in (Montiel-Ponsoda et al 
2007).    
The second direction research starts from 
NL texts and tries to interpret them in terms 
of the ontology. In most cases, this takes the 
form of marking the text with ontological 
classes and instances. A typical example is 
(Sanfilippo et al 2006). One should also 
mention the work based on the Generalized 
Upper Model (GUM), which is meant for in-
terfacing between domain models and NL 
components (Bateman et al 1995) 
Our work belongs to this second direction, 
but our aim is not limited to finding ontologi-
cal correlates to words. In many aspects we 
were inspired by the ontological semantic 
approach developed in the Mikrokosmos 
framework (cf. Nirenburg and Raskin 2004). 
We share many of its postulates and concrete 
solutions. In particular, semantic analysis of 
the text should be based on both linguistic 
and extra-linguistic knowledge. Linguistic 
knowledge is implemented in language 
grammars and dictionaries, while extra-
linguistic knowledge is comprised in an on-
tology, which enumerates concepts, describes 
their properties and states interrelationships 
between them, and a fact repository which 
accumulates ground-level facts, such as, in 
our case, the data about concrete players, 
teams and matches. To a large extent, the on-
tology serves as the semantic language for 
meaning representation.  
At the same time, there exist some differ-
ences between our approaches determined by 
the linguistic model adopted.  Our work is 
based on the Meaning ? Text theory 
(Mel??uk 1974, 1996). In particular, we make 
extensive use of lexical functions, which con-
stitute one of the prominent features of this 
theory. Thanks to lexical functions it turns 
out possible to reduce a wider range of syn-
onymous sentences to the same semantic 
68
structure, and in many cases, improve the per-
formance of search engines (see e.g. Apresjan 
et al 2009). Another difference between the 
Mikrokosmos approach and ours concerns the 
fact that the Mikrokosmos ontology is written 
in a specific in-house formalism. Our empha-
sis is on using as far as possible standard on-
tology languages (OWL, SWRL), in order to 
obtain interoperability with a wide and ever 
growing range of semantic web resources and 
inference engines. 
3 The ETAP-3 Linguistic 
Processor and its Lexicon. 
The multifunctional ETAP-3 linguistic proc-
essor, developed by the Computational Lin-
guistics Laboratory of the Institute for Infor-
mation Transmission Problems, Russian 
Academy of Sciences, Moscow, (see e.g. 
Apresjan et al 2003), is the product of dec-
ades of research and development in the field 
of language modelling.  
At the moment, ETAP-3 consists of a 
number of options, including  
1) a rule-based machine translation system 
working both ways between Russian and 
English (plus several prototypes for other 
languages ? French, German, Spanish, Ko-
rean and Arabic);  
2) a system of synonymous and quasi-
synonymous paraphrasing of sentences;  
3) an environment for deep annotation of 
text corpora, in which SynTagRus, the only 
corpus of Russian texts tagged morphologi-
cally, syntactically (in the dependency tree 
formalism), and lexically was created, and  
4) a Universal Networking Language 
(UNL) module, responsible for automatic 
translation of natural language text into a  
semantic interlingua, UNL, and the other way 
around.  
The ETAP-3 processor is largely based on 
the general linguistic framework of the Mean-
ing ? Text theory by Mel??uk. An important 
complement to this theory was furnished by 
the theory of systematic lexicography and 
integrated description of language proposed 
by Jurij Apresjan (2000).  
One of the major resources used in ETAP-
3 is the combinatorial dictionary. It offers 
ample and diverse data for each lexical entry. 
In particular, the entry may list the word?s 
syntactic and semantic features, its subcate-
gorization frames, as well as rules (or refer-
ence to rules) of a dozen types, which make it 
possible to describe peculiar behavior of in-
dividual words and exceptions to general 
rules in a complete and consistent way. Many 
dictionary entries contain information on 
lexical functions, to be discussed below in 
some detail.  
The entry of the combinatorial dictionary 
has a number of zones, one of which provides 
the properties of the word that are manifested 
in the given language, while all the other 
zones contain information on the match be-
tween this word and its equivalent in a par-
ticular language. For example, the EN zone in 
the Russian combinatorial dictionary entry 
contains information on the translational 
equivalents of the respective Russian word 
into English. One field (TRANS) gives the 
default single-word translation (or several 
such translations) of this word in English. 
Other fields contain less trivial translation 
rules, or references to such rules. 
A newly introduced ONTO zone offers in-
formation underlying the match between the 
Russian word and its counterparts in the on-
tology. 
4 Ontology of football. 
The ontology we are working with focuses in 
the first place on football. It contains infor-
mation on teams, players, football field, sport 
events, and their properties. However, we 
want it to be extendable to other sports as 
well. That is why some classes are more gen-
eral than would be needed for football alone. 
For example, instead of having one class 
FootballPlayer, the ontology has a 
more general class Sportsman, of which 
FootballPlayer is a subclass. An 
equivalence restriction states that Foot-
ballPlayer is a Sportsman whose 
SportType is football. In this way, 
sportsmen doing different types of sports can 
be treated by the ontology in a uniform way.  
The football ontology is written in SWRL, 
which is OWL augmented with rules (Hor-
rocks et al 2004). In compiling it, we used 
some existing ontologies dealing with foot-
69
ball 
(http://www.lgi2p.ema.fr/~ranwezs/ontologie
s/soccerV2.0.daml). As usual, properties of 
classes are inherited by the subclasses. For 
example, the Match class is a subclass of 
SportEvent, which in its turn is a subclass 
of Event. Match inherits from Event the 
properties of having definite Time and 
Place. From SportEvent it inherits the 
fact that its participants should be Sport-
Agents. Its own properties are: the number 
of participants is 2 (as opposed to champion-
ships, which have more) and it has a definite 
sport type (as opposed to Olympics, which 
involve many sport types).  A subclass of 
Match is Derby, in which both participants 
should be from the same city or region. This 
property is implemented by means of a 
SWRL rule (cf. below). Another rule as-
signed to Match states that if its sport type is 
football (or any other team sport), then its 
participants should be teams and not individ-
ual sportsmen, as is the case in tennis or 
chess.  Sportsman is a subclass of two 
classes: Person, from which it inherits the 
property of having a name and a birth date, 
and SportAgent, which includes also 
Team and from which it inherits the property 
of having a definite sport type and a coach.  
 
5 Correlation between the 
words and the elements of the on-
tology.  
As mentioned above, the ontology plays a 
two-fold role. On the one hand, it is a reposi-
tory of domain-specific knowledge, and on 
the other hand, it is a semantic metalanguage 
used for representing the meaning of natural 
language texts. All meaningful natural lan-
guages elements (words, grammatical con-
structions, morphological features) must be 
interpreted in ontological terms. This makes 
the correlation between the lexicon and the 
ontology far from trivial. In this section, we 
will present several typical situations and il-
lustrate them with examples. 
5.1 One-to-one correspondence 
between NL words and ontology 
elements. 
The simplest situation occurs when a word 
directly corresponds to an ontology element ? 
a class, an individual, a relation, an attribute 
or its value.  Example:  
(5) Real Madrid pobedil Arsenal so s?etom 
3:1 ?Real Madrid defeated Arsenal 3 to 1?.  
Here Real Madrid and Arsenal are indi-
viduals ? instances of the Team class, the 
verb to defeat corresponds to the WinEvent 
class, and numbers 3 and 1 are values of at-
tributes scoreWinner and scoreLoser, 
respectively. In the semantic structure (SemS) 
classes are represented by instances supplied 
by a unique numeric identifier. SemS for sen-
tence (5) looks as follows: 
hasWinner(WinEvent01, Real-
Madrid)&hasLoser(WinEvent01, 
Arsenal)& scoreWinner 
(WinEvent01,3)& 
scoreLoser(WinEvent01,1) 
5.2 One ontology element ? 
several words (?multi-word con-
cepts?). 
This is a very typical situation, especially in 
the area of terminology. For example, in or-
dinary language ?eltaja karto?ka ?a yellow 
card? is a free noun phrase that simply de-
notes a card whose colour is yellow. In the 
sports domain, it is a single concept that re-
fers to one of several possible punishments a 
referee can mete out for a rules infraction. 
Therefore, it is represented as one element in 
the ontology. Some other examples of multi-
word sport concepts: uglovoj udar ?corner?, 
svobodnyj udar ?free kick?, pravyj po-
luza??itnik ?right tackle?. 
5.3 One word ? several onto-
logical elements. 
Many words that can be interpreted in terms 
of ontological elements do not correspond to 
any single class, relation or instance. Their 
definition consists in a configuration of these 
elements. Most often, it is a class with some 
of the properties instantiated. In principle, 
often there are two options: one can either 
postulate two different classes (e.g. 
70
Sportsman and FootballPlayer as its 
subclass), or only one class (Sportsman) 
and represent the football player as a 
Sportsman whose SportType property is 
football. There is no general solution to 
this alternative. In general, it is desirable to 
obtain a parsimonious ontology and refrain 
from introducing new classes, if one can de-
fine a concept in terms of existing classes and 
their properties. However, if a concept has 
important properties of its own, it is prefer-
able to present it in the ontology as a separate 
class. In our example, FootballPlayer 
has Specialization which other sports-
men do not have (goalkeeper, for-
ward, back, etc.) For this reason, it is pos-
tulated as a separate class of the ontology, 
with the indication of its equivalence to the 
anonymous class ?Sportsman and 
hasSportType football?.  
An interesting and typical case are adjecti-
val modifiers to nouns of the type ispanskij 
?Spanish?, francuzskij ?French?, moskovskij 
?of Moscow?, and the like. Usually, dictionar-
ies provide a very general definition of such 
words. For example, the COBUILD English 
dictionary gives only one meaning for the 
adjective Spanish: ?belonging or relating to 
Spain, or to its people, language or culture?. 
However, in real life situations this word is 
often interpreted by the speakers in a more 
specific way, according to the context. Onto-
logical description should try, as far as possi-
ble, to take contextual factors into account 
and make explicit the specific interpretation 
of the modifier in each particular case. Some-
times, it can be done by means of rules that 
refer to ontological categories. For example, 
the meaning of the adjective ispanskij ?Span-
ish? mentioned above, when applied to geo-
graphical objects (rivers, cities, mountains, 
roads, etc.), narrows down to (hasLoca-
tion Spain). If this adjective modifies a 
noun denoting an industrial or agricultural 
product (car, wine, olive oil, etc.), it is rather 
interpreted as (producedIn Spain). 
We will hardly understand the phrase Spanish 
wine as denoting the wine located in Spain. 
Textual objects (songs, literature, poetry, 
etc.) move the adjective towards denoting the 
Spanish language: (inLanguage Span-
ish). Of course, these rules are not always 
sufficient for disambiguation. If an object 
falls into more than one category, several in-
terpretations are possible. In particular, a 
book is both a textual and a consumer object. 
Therefore, a Spanish book can be interpreted 
as a book written in Spanish, and as a book 
published in Spain.  
In many cases, adjectives serve as argu-
ments of nouns. The semantic role of this ar-
gument may be different for different nouns 
(cf. (6-8)), and even for the same noun (cf. 
(9-11)):  
(6) presidential decree ? ?the president issued 
a decree?:  
hasAgent(decree,president); 
(7) presidential elections ? ?somebody elects 
the president?: hasOb-
ject(elect,president);  
(8) Babylonian invasion ? ?Babylon invaded 
some city or country?: 
hasAgent(invade,Babylon); but not 
?some city or country invaded Babylon?:  
hasObject(invade,Babylon);  
(9) economic advisor ? ?advises in the area of 
economics?: has-
Topic(advisor,economics);  
(10) American advisor:  hasNational-
ity(advisor,USA);  
(11) presidential advisor ? ?advises to the 
president?: hasAd-
dressee(advisor,president).  
5.4 A word is interpreted in on-
tological terms but does not have any 
fixed ontological equivalent. 
There is a large class of words that denote 
individuals which are in a certain relation to 
other individuals: brother, sister, uncle, wife, 
friend, enemy, classmate, co-author, co-
regent, coeval, adversary, ally, etc. Of 
course, these words can be easily represented 
as ontology properties: has-
Brother(John, Bill), hasSis-
ter(John, Mary). However, such rep-
resentation does not reveal the meaning of the 
concepts. Being a brother of somebody means 
being a male and having common parents 
with this person.  This meaning shares the 
second component (?having common par-
ents?) with the property of being a sister and 
71
differs from it in the first component (?being 
a male?). Such a definition of meanings re-
quires the use of variables. This is the point 
where the OWL expressive capacity is insuf-
ficient and one has to recur to SWRL rules:   
Person(?person1)&Gender (?per-
son1,male)&hasParent(?person1, 
?person3) &hasParent (?per-
son2,?person3)? 
brother(?person1, ?person2)  
Person(?person1)&Gender (?per-
son1,female)&hasParent(?person
1, ?person3)& hasParent (?per-
son2,?person3)?        sis-
ter(?person1,?person2) 
In a similar way one can define the concept 
of adversary (in sports), as used for example 
in sentence (3) above.  Adversary of Z is 
someone different from Z who plays in the 
same match as Z: 
SportAgent(?agent)& 
Match(?match)& hasPartici-
pant(?match,?agent)& hasPar-
ticipant(?match,?z)& differ-
entFrom(?agent,?z) ? adver-
sary(?agent,?z)  
Among the words that require variables for 
their ontological definition are not only rela-
tional nouns. There are many other words that 
cannot be translated into ontological catego-
ries without claiming identity (or difference) 
of the properties of some individuals. Here 
are some examples from the football domain.  
A derby is a match whose participants are 
from the same city or region. Our ontology 
defines the concept of derby as follows:   
hasParticipant(?match, ?par-
ticipant1)& hasParticipant 
(?match, ?partici-
pant2)&differentFrom (?par-
ticipant1,?participant2) 
&hasLocation (?participant1, 
?location) &hasLocation (?par-
ticipant2,?location) ? 
derby(?match) 
Pobednyj gol (?decisive goal?) is a goal 
which was scored when both teams had equal 
score and which was the last goal in the 
match. However, since having no subsequent 
goals cannot be expressed in SWRL we will 
convey this idea by saying that the goal 
brought the victory in the match to one of the 
teams. We will need the following classes 
and properties: 
GoalEvent, with the properties: 
hasAgent, atMinute, e.g. on the tenth 
minute, inMatch, hasResult (inher-
ited from the more general class Event).  
SituationInMatch (the score at a 
given moment), with the properties: in-
Match, atMinute, scorePartici-
pant1, scoreParticipant2. 
WinEvent, with the properties: 
hasWinner, hasLoser.  
Team, with the property hasPart, to be 
filled by instances of Sportsman.  
Besides that, we need the property 
timeImmediatelyBefore, inherited by 
moments of time from Time. 
We will describe the situation by means of 
two rules. Rule (12) says that the goal that 
brought a victory can be called a decisive 
goal. Rule (13) complements this description 
by saying that if a goal brings a victory, the 
winner is the team whose player scored it and 
this goal was scored at the moment when 
both teams had equal score.  
 (12) GoalEvent(?goal)&WinEvent 
(?victory)& hasRe-
sult(?goal,?victory) ?      
decisiveGoal(?goal) 
(13) hasResult(?goal,?victory) 
&hasAgent(?goal,?player)& has-
Part (?team,?player)&atMinute 
(?goal,?min0)&inMatch (?goal, 
?match)& timeImmediatelyBe-
fore(?min1,?min0)& Situation-
InMatch (?situation)&inMatch 
(?situation,?match)& atMinute 
(?situation,?min1) ?  
hasWinner(?victory,?team) 
&scoreParticipant1(?situation,
?n) &scoreParticipant2(?situa-
tion,?n). 
5.5 Ontology and Lexical Func-
tions. 
A lexical function (LF), in the Meaning ? 
Text theory (Mel'?uk 1996), has the basic 
properties of a multi-value mathematical 
72
function. A prototypical LF is a triple of ele-
ments {R, X, Y}, where R is a certain general 
semantic relation obtaining between the ar-
gument lexeme X (the keyword) and some 
other lexeme Y which is the value of R with 
regard to X (by a lexeme in this context we 
mean either a word in one of its lexical mean-
ings or some other lexical unit, such as a set 
expression). Here are some examples for  the 
Oper1 and Oper2 functions: Oper1 (control) = 
exercise (control), Oper1 (research) = do (re-
search), Oper1 (invitation) = issue (an invita-
tion), Oper1 (doubt) = have (doubts), Op-
er1 (defeat) = suffer (a defeat), Oper1 (victory) 
= gain (a victory), Oper1 (campaign) = wage 
(a campaign), Oper2 (control) = be  under 
(control), Oper2 (analysis) = undergo (an 
analysis), Oper2 (invitation) = receive (an 
invitation), Oper2 (resistance) = encounter 
(resistance), Oper2 (respect) = enjoy (re-
spect), Oper2 (obstacle) = face (an obstacle).  
Y is often represented by a set of synony-
mous lexemes Y1, Y2, ?., Yn, all of them 
being the values of the given LF R with re-
gard to X; e. g., Magn (desire) = strong / 
keen / intense / fervent / ardent / overwhelm-
ing. All the LF exponents for each word are 
listed in the lexicon. 
LFs have a strong potential for advanced 
NLP applications. Apresjan et al (2007) 
shows how LFs can be used in parsing, ma-
chine translation, paraphrasing. In parsing, 
LFs are used to resolve or reduce syntactic 
and lexical ambiguity. The MT system resorts 
to LFs to provide idiomatic target language 
equivalents for source sentences in which 
both the argument and the value of the same 
LF are present. The system of paraphrasing 
automatically produces one or several syn-
onymous transforms for a given sentence or 
phrase by means of universal LF-axioms; for 
example: He respects [X] his teachers ? He 
has [Oper1(S0 (X))] respect [S0 (X)] for his 
teachers ? He treats [Labor1-2(S0 (X))] his 
teachers with respect ? His teachers enjoy 
[Oper2(S0(X))] his respect. It can be used in a 
number of advanced NLP applications rang-
ing from machine translation to authoring and 
text planning. 
In ontologically-oriented semantic analysis 
different LFs are reflected in different ways. 
An LF corresponds to an ontological class. 
Many LFs represent bundles of words that are 
semantically identical or very close and there-
fore can serve as representatives of this com-
mon meaning. We illustrate this with two 
closely related LFs (Apresjan et al 2008).  
The meaning covered by LiquFunc0 is ?to 
cause to cease to exist or to be taking place?. 
This concept corresponds, in particular, to the 
following English verbs: to stop (the aggres-
sion), to lift (the blockade), to dispel (the 
clouds), to demolish (the building), to dis-
perse (the crowd), to avert (the danger), to 
cure (the disease), to close (the dispute), to 
break up (the family), to annul (the law), to 
dissolve (the parliament), to denounce (the 
treaty), to bridge (the gap). Another LF of the 
Liqu family ? LiquFact0 ? refers to a different 
kind of elimination. It means ?to cause to 
cease functioning according to its destina-
tion?. When somebody closes the eyes, they 
do not cease to exist, they only stop function-
ing. Some more examples: shut down (the 
factory), stop (the car), land (the airplane), 
depose (the king), switch off (the lamp), neu-
tralize (the poison), empty (the bucket).  
These LFs, along with several dozen oth-
ers, play a significant role not only in text 
understanding and generation. They contrib-
ute in an interesting way to one of the crucial 
functions of ontologies ? inference of implicit 
knowledge. Important inference rules can be 
easily formulated in terms of LFs: if the 
blockade is lifted (=LiquFunc0), it does not 
exist any more. Another example of the LF-
based inference (this time it is LF Real1): He 
fulfilled (= Real1) the promise to buy a bicycle 
? He bought a bicycle. 
It should be emphasized that, given a lexi-
con which contains LF data (which is the case 
of our ETAP dictionary), the acquisition of 
this part of the ontology is straightforward.   
An LF generates an ontological relation.  
This case can be illustrated by support verbs 
of the Oper-Func-Labor family that attach 
one of the arguments to the noun. For exam-
ple, in sentence Father gave me an advice the 
subject of the Oper1-support verb to give (fa-
ther) is the Agent of advice, while in The 
proposal received much attention the subject 
73
of the Oper2-support verb to receive (the pro-
posal) is the Object of attention. Other exam-
ples of Oper1 and Oper2 were given in 5.5 
above. Some examples of other LFs of this 
family:  
Func1: (fear) possesses (somebody), (ru-
mour) reaches (somebody), (the blame) falls 
on (somebody) / (the blame) lies with (some-
body), (control) belongs to (somebody), (re-
sponsibility) rests with (somebody). 
Func2: (proposal) consists in (something), 
(criticism) bears upon (something), (revenge) 
falls upon (somebody). 
Labor1-2: keep (something) under (control), 
submit (something) to (analysis), meet (some-
body) with (applause), put (somebody) under 
(arrest), hold (somebody) in (contempt), 
bring (something) into (comparison with 
something), take (something) into (considera-
tion).  
An LF has no ontological correlate. 
This is the case of Func0. This LF neither de-
notes a concept, nor attaches an argument to a 
concept. It only duplicates the meaning of its 
keyword and has no correlate in the SemS. 
For example, in sentence (2) above the phrase 
the match took place (= Func0) is only repre-
sented by the concept Match. Other exam-
ples of Func0: (the snow) falls, (the wind) 
blows, (the danger) exists, (the war) is on, 
(changes) occur. 
6 Lexicon ? Ontology in-
terface.  
For the purposes of semantic analysis, the 
Russian dictionary and the ontology are 
linked in the same way as dictionaries of dif-
ferent languages are linked in Machine Trans-
lation options of the ETAP-3 system. As 
noted in Section 2, if the system performs 
translations from language L to language L?, 
all dictionary entries of L contain a special 
zone (ZONE: L?) where all translation vari-
ants of the given word into L? are recorded. 
The semantic analysis option uses the ONTO 
zone of the Russian dictionary. In this zone, 
two types of information may be written: 
? Default translation. This is a one-
word equivalent of the given word, which is 
used if no translation rule is applicable.  
For example, Russian komanda ?team? has 
the Team class as its ontological counterpart. 
This is written in the ontological zone of ko-
manda as follows: 
ZONE: ONTO 
TRANS: Team 
Names of ontological individuals are also 
often translated by default.  
? Translation rules. A rule is written 
every time one needs to carry out an action 
which does not boil down to the default trans-
lation.  
Let us give several examples of translation 
rules written in the ONTO zone of the Rus-
sian lexicon. We will not give their formal 
representation and restrict ourselves to ex-
plaining what they are doing in plain words. 
Pobeditel? ?winner? is a SportAgent (i.e. a 
sportsman or a team) that won some contest: 
SportAgent(?x)&WinEvent(?y)& 
hasWinner(?y,?x). 
Phrases of the type komanda NN ?team of 
NN? (where NN is a proper human name in 
the genitive case) are translated in four differ-
ent ways depending on the ontological infor-
mation assigned to NN.  
(a) If NN is the name of a player, the 
phrase is represented as ?the team of which 
NN is a player?: komanda Arshavina ?Ar-
shavin?s team? = Team(?team) 
&hasPart(?team,Arshavin). 
(b) If NN is the name of a coach, the 
phrase is represented as ?the team of which 
NN is the coach?: komanda Pellegrini ?Pelle-
grini?s team? = Team(?team) 
&hasCoach(?team,Pellegrini) 
(c) If NN is the name of a captain, the 
phrase is represented as ?the team of which 
NN is the captain?: komanda Iraneka ?Ira-
nek?s team? = Team(?team)& hasCap-
tain(?team,Iranek) 
(d) If NN is neither a player, nor a coach, 
nor a captain, the phrase is represented as ?the 
team of which NN is a fan?: komanda Ivana 
?Ivan?s team? =  Team(?team)& has-
Fan(?team,Ivan) 
It is well-known that genitive noun phrases 
(or phrases ?N1 of N2? in English) are very 
vague semantically, and their interpretation is 
very much dependent on the context. This 
example shows that even within the 
74
part/whole interpretation such a phrase, para-
doxically, has two opposite varieties: either 
N2 is part of N1, as in the team of Ar-
shavin/Arshavin?s team, or N1 is part of N2, 
as in the leg of the table.  
The following examples involve the prop-
erty hasLocation, which characterizes 
both sport events (The match took place in 
Madrid), and sport agents (the Ukrainian 
sportsman, a London club).   
Frequently, a football match is played in a 
location, such that one of the teams is from 
that location while the other is not. This situa-
tion can be represented by the following 
SemS: 
(14) 
Match(?match)&hasLocation(?mat
ch,?place)&hasParticipant 
(?match, ?team1)&hasParicipant 
(?match,?team2)&differentFrom(
?team1,?team2)&hasLocation(?te
am1,?place)&?hasLocation(?team
2,?place) 
In the natural language this situation can be 
viewed from different angles and denoted by 
different words.  
Xozjaeva ?home team? denotes a team that 
plays a match in a place it is from, the adver-
sary being from a different place. Gosti ?visi-
tors? is a team that plays a match in a location 
different from the place it is from, the adver-
sary being the home team. Prinimat? ?to re-
ceive? means to play a match being a home 
team, to host it. Igrat? v gostjax lit.?to play 
being guests? means to play a match away.  
Although all these words correspond to the 
same situation (14), their translation rules 
cannot be identical. The rules should not only 
introduce SemS (14), but also assure correct 
amalgamation of this SemS with SemSs of 
other words. In particular, the rule for prini-
mat? ?receive? should guarantee that in (15) 
Real Madrid instantiates variable ?team1 of 
(14), and Barcelona ? variable ?team2.     
(15) Real Madrid prinimal Barcelonu ?Real 
Madrid hosted Barcelona? 
The rule for gosti ?visitors? should see to it 
that in (16) hasWinner property of 
WinEvent be filled by variable ?team2 of 
(14): 
(16) Gosti vyigrali 3:1 ?the visitors won 3 to 
1? 
This is assured due to marking ?team2 in 
the gosti ?visitors? rule as the head element of 
SemS (14). Naturally, in the xozjaeva ?home 
team? rule the same role is assigned to 
?team1. 
7 Future work.  
In the continuation, it is planned to enlarge 
both the ontology and ONTO zone of the 
Russian lexicon. We are investigating the 
possibility of merging our small football on-
tology with some existing larger upper level 
ontology. The difficult task will be to unify 
our semantic rules with the axioms of this 
ontology.  
A second direction of our future activity is 
connected with another component of the se-
mantic analyzer, which we did not touch 
upon in this paper. It is the set of semantic 
rules which are not incorporated into the lexi-
con due to their general character. This com-
ponent also requires significant enhancement.  
An important extension of this work con-
sists in introducing an inference component 
based on the SWRL rules. 
Acknowledgement  
This study has received partial funding from 
the Russian Foundation for Humanities (grant No. 
10-04-00040a), which is gratefully acknowledged.  
References 
Apresjan, Ju. D. Systematic Lexicography. 
Oxford University Press, 2000, XVIII p., 304 p.  
Apresjan, Jury, I. Boguslavsky, L. Iomdin, 
A. Lazursky, V.Sannikov, V.Sizov, L.Tsinman. 
ETAP-3 Linguistic Processor: a Full-Fledged 
NLP Implementation of the MTT // MTT 2003, 
First International Conference on Meaning ? 
Text Theory (June 16-18 2003). Paris: Ecole 
Normale Sup?rieure, 2003. P. 279-288. 
Apresjan, Jury, Igor Boguslavsky, Leonid 
Iomdin and Leonid Tsinman. Lexical Functions 
in Actual NLP Applications // Selected Lexical 
and Grammatical Issues in the Meaning?Text 
Theory. In honour of Igor Mel'?uk. (Ed. by Leo 
Wanner). John Benjamins, Studies in Language 
Companion Series 84. 2007. ?. 199-230.  
75
Apresjan, Ju.D., P.V. Djachenko, A.V. 
Lazursky, L.L. Tsinman. O kompjuternom 
uchebnike russkog? jazyka. [On a computer 
textbook of Russian.] Russkij jazyk v nauchnom 
osveshchenii. 2008, No. 2 (14). P. 48-112.  
Apresjan Ju., I. Boguslavsky, L.Iomdin, 
L.Cinman, S.Timoshenko. Semantic Paraphras-
ing for Information Retrieval and Extraction. 
In: T.Andreasen, R.Yager, H.Bulskov, 
H.Christiansen, H.Legind Larsen (eds.) Flexi-
ble Query Answering Systems. Proceedings, 
8th International Conference, 2009. Lecture 
Notes in Computer Science 5822. pp. 512-523 
Bateman J., B. Magnini and G. Fabris. The 
Generalized Upper Model Knowledge Base: 
Organization and Use. In: Towards Very Large 
Knowledge Bases, pp. 60-72, IOS Press. 1995.  
Buitelaar P., Ph. Cimiano, A.Frank, M. Har-
tung, S.Racioppa. (2008). "Ontology-based In-
formation Extraction and Integration from Het-
erogeneous Data Sources." In: International 
Journal of Human-Computer Studies, 66(11). 
Horrocks, I., P.F. Patel-Schneider, H. Boley, 
S. Tabet, B. Grosof and M. Dean (2004). 
SWRL: A Semantic Web Rule Language Com-
bining OWL and RuleML. // W3C Member 
Submission 21 May 2004.  
Magnini B., Emanuele Pianta, Octavian 
Popescu, and Manuela Speranza. (2006). 
"Ontology Population from Textual Mentions: 
Task Definition and Benchmark." In: Proceed-
ings of the Ontology Population and Learning 
Workshop at ACL/Coling 2006. 
Maynard D., Wim Peters, and Yaoyong Li. 
(2006). "Metrics for Evaluation of Ontology-
based Information Extraction." In: WWW 2006 
Workshop on Evaluation of Ontologies for the 
Web (EON 2006) 
McDowell Luke K.,  Michael Cafarella.  On-
tology-driven Information Extraction with On-
toSyphon. In: Proceedings of the 5th Interna-
tional Semantic Web Conference (ISWC 2006). 
Volume 4273 of LNCS., Athens, GA, Springer 
(2006) 428 ? 444 
Mel??uk, Igor. Opyt teorii lingvisiticheskix 
modelej ?Smysl ? Text? [The theory of lin-
guistic models of the Meaning ? Text type]. 
Moscow, 1974 (2nd edition 1999).  
Mel??uk, Igor. Lexical Functions: A Tool for 
the Description of Lexical Relations in Lexi-
con. L. Wanner (ed.), Lexical Functions in 
Lexicography and Natural Language Process-
ing. Amsterdam, 1996, 37-102.  
Montiel-Ponsoda, E., Aguado de Cea, G. y 
G?mez-P?rez, A. 2007 "Localizing ontologies 
in OWL. From Text to Knowledge: The Lexi-
con/Ontology Interface. WS 2. The 6th Interna-
tional Semantic Web Conference." 
Nirenburg, Sergei, and Victor Raskin. Onto-
logical Semantics. The MIT Press. Cambridge, 
Massachusetts. London, England, 2004. 
Sanfilippo A., Tratz S., Gregory M., Chap-
pell A.,  Whitney P., Posse Ch., Paulson P., 
Baddeley B.,  Hohimer R., White A. Automat-
ing Ontological Annotation with WordNet. In: 
Sojka P., Key-Sun Choi, Ch. Fellbaum, P. 
Vossen (Eds.): GWC 2006, Proceedings, pp. 
85?93. 
Schutz, A. and P. Buitelaar. RelExt: A Tool 
for Relation Extraction from Text in Ontology 
Extension. In: Y. Gil et al (Eds.), ISWC 2005, 
LNCS 3729, pp. 593?606, 2005. 
76
