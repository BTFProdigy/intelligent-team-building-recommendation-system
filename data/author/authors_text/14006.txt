Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1119?1127,
Beijing, August 2010
Syntax Based Reordering with Automatically Derived Rules for
Improved Statistical Machine Translation
Karthik Visweswariah
IBM Research
v-karthik@in.ibm.com
Jiri Navratil
IBM Research
jiri@us.ibm.com
Jeffrey Sorensen
Google, Inc.
sorenj@google.com
Vijil Chenthamarakshan
IBM Research
vijil.e.c@in.ibm.com
Nanda Kambhatla
IBM Research
kambhatla@in.ibm.com
Abstract
Syntax based reordering has been shown
to be an effective way of handling word
order differences between source and
target languages in Statistical Machine
Translation (SMT) systems. We present
a simple, automatic method to learn rules
that reorder source sentences to more
closely match the target language word or-
der using only a source side parse tree and
automatically generated alignments. The
resulting rules are applied to source lan-
guage inputs as a pre-processing step and
demonstrate significant improvements in
SMT systems across a variety of lan-
guages pairs including English to Hindi,
English to Spanish and English to French
as measured on a variety of internal test
sets as well as a public test set.
1 Introduction
Different languages arrange words in different or-
ders, whether due to grammatical constraints or
other conventions. Dealing with these word order
permutations is one of the fundamental challenges
of machine translation. Given an exceptionally
large training corpus, a phrase-based system can
learn these reordering on a case by case basis.
But, if our systems are to generalize to phrases not
seen in the training data, they must explicitly cap-
ture and model these reorderings. However, per-
mutations are difficult to model and impractical to
search.
Presently, approaches that handle reorderings
typically model word and phrase movements via
a distortion model and rely on the target language
model to produce words in the right order. Early
distortion models simply penalized longer jumps
more than shorter jumps (Koehn et al, 2003)
independent of the source or target phrases
in question. Other models (Tillman, 2004),
(Al-Onaizan and Papineni, 2006) generalize this
to include lexical dependencies on the source.
Another approach is to incorporate features,
based on the target syntax, during modeling and
decoding, and this is shown to be effective for var-
ious language pairs (Yamada and Knight, 2001),
(Zollmann and Venugopal, 2006). Hierarchical
phrase-based decoding (Chiang, 2005) also al-
lows for long range reordering without explic-
itly modeling syntax. While these approaches
have been shown to improve machine translation
performance (Zollmann et al, 2008) they usually
combine chart parsing with the decoding process,
and are significantly more computationally inten-
sive than phrase-based systems.
A third approach, one that has proved to be
useful for phrase-based SMT systems, is to re-
order each source-side sentence using a set of
rules applied to a parse tree of the source sen-
tence. The goal of these rules is to make the
word order of the source sentence more sim-
ilar to the expected target sentence word or-
der. With this approach, the reordering rules
are applied before training and testing with an
SMT system. The efficacy of these methods has
been shown on various language pairs including:
French to English (Xia and McCord, 2004), Ger-
man to English (Collins et al, 2005), English to
1119
Chinese, (Wang et al, 2007) and Hindi to English
(Ramanathan et al, 2008).
In this paper, we propose a simple model for re-
ordering conditioned on the source side parse tree.
The model is learned using a parallel corpus of
source-target sentence pairs, machine generated
word alignments, and source side parses. We ap-
ply the reordering model to both training and test
data, for four different language pairs: English
? Spanish, English ? French, English ? Hindi,
and English ? German. We show improvements
in machine translation performance for all of the
language pairs we consider except for English ?
German. We use this negative result to propose
extensions to our reordering model. We note that
the syntax based reordering we propose can be
combined with other approaches to handling re-
ordering and does not have to be followed by an
assumption of monotonicity. In fact, our phrase-
based model, trained upon reordered data, retains
its reordering models and search, but we expect
that these facilities are employed much more spar-
ingly with reordered inputs.
2 Related work
There is a significant quantity of work in syntax
based reordering employed to improve machine
translation systems. We summarize our contribu-
tions to be:
? Learning the reordering rules based on train-
ing data (without relying on linguistic knowl-
edge of the language pair)
? Requiring only source side parse trees
? Experimental results showing the efficacy for
multiple language pairs
? Using a lexicalized distortion model for our
baseline decoder
There have been several studies that have
demonstrated improvements with syntax
based reordering based upon hand-written
rules. There have also been studies inves-
tigating the sources of these improvements
(Zwarts and Dras, 2007). Hand-written rules
depend upon expert knowledge of the linguis-
tic properties of the particular language pair.
Initial efforts (Niessen and Ney, 2001) were
made at improving German-English translation
by handling two phenomena: question inver-
sion and detachable verb prefixes in German.
In (Collins et al, 2005), (Wang et al, 2007),
(Ramanathan et al, 2008), (Badr et al, 2009)
rules are developed for translation from Ger-
man to English, Chinese to English, English
to Hindi, and English to Arabic respectively.
(Xu et al, 2009) develop reordering rules based
upon a linguistic analysis of English and Korean
sentences and then apply those rules to trans-
lation from English into Korean and four other
languages: Japanese, Hindi, Urdu and Turkish.
Unlike this body of work, we automatically learn
the rules from the training data and show efficacy
on multiple language pairs.
There have been some studies that try to learn
rules from the data. (Habash, 2007) learns re-
ordering rules based on a dependency parse and
they report a negative result for Arabic to En-
glish translation. (Zhang et al, 2007) learn re-
ordering rules on chunks and part of speech
tags, but the rules they learn are not hierarchi-
cal and would require large amounts of training
data to learn rules for long sentences. Addition-
ally, we only keep a single best reordering (in-
stead of a lattice with possible reorderings) which
makes the decoding significantly more efficient.
(Xia and McCord, 2004) uses source and target
side parse trees to automatically learn rules to re-
order French sentences to match English order.
The requirement to have both source and target
side parse trees makes this method inapplicable
to any language that does not have adequate tree
bank resources. In addition, this work reports re-
sults using monotone decoding, since their exper-
iments using non-monotone decoding without a
distortion model were actually worse.
3 Reordering issues in specific languages
In this section we discuss the reordering issues
typical of translating between English and Hindi,
French, Spanish and German which are the four
language pairs we experiment on in this paper.
3.1 Spanish and French
Typical word ordering patterns common to these
two European languages relate to noun phrases in-
cluding groups of nouns and adjectives. In con-
1120
trast to English, French and Spanish adjectives
and adjunct nouns follow the main noun, i.e. we
typically observe a reversal of word order in noun
phrases, e.g., ?A beautiful red car? translates
into French as ?Une voiture rouge beau?, and as
?Un coche rojo bonito? into Spanish. Phrase-
based MT systems are capable of capturing these
patterns provided they occur with sufficient fre-
quency for each example in the training data. For
rare noun phrases, however, the MT may pro-
duce erroneous word order that can lead to seri-
ous distortions in the meaning. Particularly dif-
ficult are nominal phrases from specialized do-
mains that involve challenging terminology, for
example: ?group reference attribute? and ?valida-
tion checking code?. In both instances, the base-
line MT system generated translations with an in-
correct word order and, consequently, possibly a
different meaning. We will return to these two ex-
amples in Section 5.1 to compare the output of a
MT system with and without reordering.
3.2 German
Unlike French and Spanish, German poses a con-
siderably different challenge with respect to word
ordering. The most frequent reordering in German
relates to verbs, particularly verb groups consist-
ing of auxiliary and main verbs, as well as verbs
in relative clauses. Moreover, reordering patterns
between German and English tend to span large
portions of the sentence. We included German in
our investigations to determine whether our auto-
mated rule extraction procedure can capture such
long distance patterns.
3.3 Hindi
Hindi word order is significantly different than
English word order; the typical order followed
is Subject Object Verb (although Object Subject
Verb order can be used if nouns are followed by
appropriate case markers). This is in contrast to
English which has a Subject Verb Object order.
This can result in words that are close in English
moving arbitrarily far apart in Hindi depending on
the length of the noun phrase representing the ob-
ject and the length of the verb phrase. These long
range reorderings are generally hard for a phrase
based system to capture. Another way Hindi and
English differ is that prepositions in English be-
come postpositions in Hindi and appear after the
noun phrase. Again, this reordering can lead to
long distance movements of words. We include
Hindi in our investigation since it has significantly
different structure as compared to English.
4 Learning reordering rules
In this section we describe how we learn rules that
transform source parse trees so the leaf word order
is more like the target language. We restrict our-
selves to reorderings that can be obtained by per-
muting child nodes at various interior nodes in a
parse tree. With many reordering phenomena dis-
cussed in Section 3 this is a fairly strong assump-
tion about pairs of languages, and there are exam-
ples in English?Hindi where such an assumption
will not allow us to generate the right reordering.
As an example consider the English sentence ?I
do not want to play?. The sentence has a parse:
S
NP
PRP
I
VP
VBP
do
RB
not
VP
VB
want
S
VP
TO
to
VP
VB
play
The correct word order of the translation in Hindi
is ?I to play not want? In this case, the word not
breaks up the verb phrase want to play and hence
the right Hindi word order cannot be obtained by
the reordering allowed by our model. We found
such examples to be rare in English?Hindi, and
we impose this restriction for the simplicity of the
model. Experimental results on several languages
show benefits of reordering in spite of this simpli-
fying assumption.
Consider a source sentence s and its corre-
sponding constituency parse tree S1. We set up
the problem in a probabilistic framework, i.e. we
would like to build a probabilistic model P (T |S)
that assigns probabilities to trees such that the
1In this paper we work with constituency parse trees. Ini-
tial experiments, applying similar techniques to dependency
parse trees did not yield improvements.
1121
word order in trees T which are assigned higher
probability match the order of words in the target
language. A parse tree, S is a set of nodes. Inte-
rior nodes have an ordered list of children. Leaf
nodes in the tree are the words in the sentence
s, and interior nodes are labeled by the linguis-
tic constituent that they represent. Each word has
a parent node (with only one child) labeled by the
part-of-speech tag of the word.
Our model assigns non-zero probabilities to
trees that can be obtained by permuting the child
nodes at various interior nodes of the tree S. We
assume that children of a node are ordered inde-
pendently of all other nodes in the tree. Thus
P (T |S) =
?
n?I(S)
P (?(cn)|S, n, cn),
where I(S) is the set of interior nodes in the tree
S, cn is the list of children of node n and ? is a
permutation. We further assume that the reorder-
ing at a particular node is dependent only on the
labels of its children:
P (T |S) =
?
n?I(S)
P (?(cn)|cn).
We parameterize our model using a log-linear
model:
P (?(cn)|cn) =
1
Z(cn)
exp(?T f(?, cn)). (1)
We choose the simplest possible set of feature
functions: for each observed sequence of non-
terminals we have one boolean feature per per-
mutation of the sequence of non-terminals, with
the feature firing iff that particular sequence is ob-
served. Assuming, we have a training corpus C of
(T, S) tree pairs, we could optimize the parame-
ters of our model to maximize :
?
S?C P (T |S).
With the simple choice of feature functions de-
scribed above, this amounts to:
P (?(cn)|cn) =
count(?(cn))
count(cn)
,
where count(cn) is the number of times the se-
quences of nodes cn is observed in the training
data and count(?(cn)) is the number of times
that cn in S is permuted to ?(cn) in T . In Sec-
tion 6, we show considering more general fea-
ture functions and relaxing some of the indepen-
dence might yield improvements on certain lan-
guage pairs.
For each source sentence s with parse S we find
the tree T that makes the given alignment for that
sentence pair most monotone. For each node n in
the source tree S let Dn be the set of words that
are descendants of n. Let us denote by tpos(n) the
average position of words in the target sentence
that are aligned to words in Dn. Then
tpos(n) = 1|Dn|
?
w?Dn
a(w),
where a(w) is the index of the word on the target
side that w is aligned with. If a word w is not
aligned to any target word, we leave it out from
the mean position calculation above. If a word w
is aligned to many words we let a(w) be the mean
position of the words that w is aligned to. For each
node n in the tree we transform the tree by sorting
the list of children of n according to tpos. The
pairs of parse trees that we obtain (S, T ) in this
manner form our training corpus to estimate our
parameters.
In using our model, we once again go for the
simplest choice, we simply reorder the source side
sentences by choosing arg maxT P (T |S) both in
training and in testing; this amounts to reordering
each interior node based on the most frequent re-
ordering of the constituents seen in training. To
reduce the effect of noise in training alignments
we apply the reordering, only if we have seen the
constituent sequence often enough in our training
data (a count threshold parameter) and if the most
frequent reordering is sufficiently more frequent
than the next most frequent reordering (a signifi-
cance threshold).
5 Experiments
5.1 Results for French, Spanish, and German
In each language, the rule extraction was
performed using approximately 1.2M sen-
tence pairs aligned using a maxent aligner
(Ittycheriah and Roukos, 2005) trained using a
variety of domains (Europarl, computer manuals)
1122
and a maximum entropy parser for English
(Ratnaparkhi, 1999). With a significance thresh-
old of 1.2, we obtain about 1000 rules in the
eventual reordering process.
Phrase-based systems were trained for each lan-
guage pair using 11M sentence pairs spanning a
variety of publicly available (e.g. Europarl, UN
speeches) and internal corpora (IT technical and
news domains). The system phrase blocks were
extracted based on a union of HMM and max-
ent alignments with corpus-selective count prun-
ing. The lexicalized distortion model was used
as described in (Al-Onaizan and Papineni, 2006)
with a window width of up to 5 and a maximum
number of skipped (not covered) words during de-
coding of 2. The distortion model assigns a prob-
ability to a particular word to be observed with
a specific jump. The decoder uses a 5-gram in-
terpolated language model spanning the various
domains mentioned above. The baseline system
without reordering and a system with reordering
was trained and evaluated in contrastive experi-
ments. The evaluation was performed utilizing the
following (single-reference) test sets:
? News: 541 sentences from the news domain.
? TechA: 600 sentences from a computer-
related technical domain, this has been used
as a dev set.
? TechB: 1038 sentences from a similar do-
main as TechA used as a blind test.
? Dev09: 1026 sentences defined as the news-
dev2009b development set of the Workshop
on Statistical Machine Translation 2009 2.
This set provides a reference measurement
using a public data set. Previously published
results on this set can be found, for example,
in (Popovic et al, 2009).
In order to assess changes in word ordering pat-
terns prior to and after an application of the re-
ordering, we created histograms of word jumps
in the alignments obtained in the baseline as well
as in the reordered system. Given a source word
si at index i and the target word tj it is aligned
to at index j, a jump of 1 would correspond to
si+1 aligning to target word tj+1, while an align-
ment to tj?1 corresponds to a jump of -1, etc. A
2http://statmt.org/wmt09/
?8 ?6 ?4 ?2 0 2 4 6 8 10
?1
?0.5
0
0.5
1
1.5
2
x 105
C
nt
2 
? 
C
nt
1
Difference of histograms after and before reordering (EN?ES)
?8 ?6 ?4 ?2 0 2 4 6 8 10
?5000
0
5000
10000
15000
20000
Distance to next position
C
nt
2 
? 
C
nt
1
Difference of histograms after and before reordering (EN?FR)
Figure 1: Difference-histogram of word order
distortions for English?Spanish (upper), and
English?French (lower).
histogram over the jump values gives us a sum-
mary of word order distortion. If all of the jumps
were one, then there is no reordering between the
two languages. To gain insight into changes in-
troduced by our reordering we look at differences
of the two histograms i.e., counts after reordering
minus counts before reordering. We would hope
that after reordering most of the jumps are small
and concentrated around one. Figure 1 shows
such difference-histograms for the language pairs
English?Spanish and English?French, respec-
tively, on a sample of about 15k sentence pairs
held out of the system training data. Here, a pos-
itive difference value indicates an increased num-
ber after reordering. In both cases a consistent
trend toward monotonicity is observed, i.e more
jumps of size one and two, and fewer large jumps.
This confirms the intended reordering effect and
indicates that the reordering rules extracted gen-
eralize well.
Table 1 shows the resulting uncased BLEU
scores for English-Spanish and English-French.
In both cases the reordering has a consistent
positive effect on the BLEU scores across test sets.
In examining the sources of improvement, we no-
ticed that word order in several noun phrases that
1123
System News TechA TechB Dev09
Baseline 0.3849 0.3371 0.3483 0.2244
Sp
an
ish
Reordered 0.4031 0.3582 0.3605 0.2320
Baseline 0.5140 0.2971 0.3035 0.2014
Fr
en
ch
Reordered 0.5242 0.3152 0.3154 0.2092
Baseline 0.2580 0.1582 0.1697 0.1281
G
er
m
an
Reordered 0.2544 0.1606 0.1682 0.1271
Baseline 20.0
H
in
di
Reordered 21.7
Table 1: Uncased BLEU scores for phrase-based
machine translation.
were not common in the training data were fixed
by use of the reordering rules.
Table 1 shows the BLEU scores for the
English?German language pair, for which a
mixed result is observed. The difference-
histogram for English?German, shown in Figure
2, differs from those of the other languages with
several increases in jumps of large magnitude, in-
dicating failure of the extracted rules to general-
ize.
The failure of our simple method to gain con-
sistent improvements comparable to Spanish and
French, along with our preliminary finding that a
relatively few manually crafted reordering rules
(we describe these in Section 6.4) tend to outper-
form our method, leads us to believe that a more
refined approach is needed in this case and will be
subject of further discussion below.
5.2 Results for Hindi
Our Hindi-English experiments were run with
an internal parallel corpus of roughly 250k sen-
tence pairs (5.5M words) consisting of various
domains (including news). To learn reordering
rules we used HMM alignments and a maxent
parser (Ratnaparkhi, 1999), with a count thresh-
old of 100, and a significance threshold of 1.7
(these settings gave us roughly 200 rules). We also
experimented with other values of these thresh-
olds and found that the performance of our sys-
tems were not very sensitive to these thresholds.
We trained Direct Translation Model 2 (DTM)
?10 ?5 0 5 10
?600
?400
?200
0
200
400
600
800
1000
1200
Distance to next position
Cn
t2
 ?
 C
nt
1
Difference of histograms after and before reordering (EN?DE)
Figure 2: Difference-histogram of word order dis-
tortions for English?German.
systems (Ittycheriah and Roukos, 2007) with and
without source reordering and evaluated on a test
set of 357 sentences from the News domain.
We note that the DTM baseline includes features
(functions of target words and jump size) that al-
low it to model lexicalized reordering phenomena.
The reordering window size was set to +/- 8 words
for the baseline and system with reordered in-
puts. Table 1 shows the uncased BLEU scores for
English-Hindi, showing a gain from using the re-
ordering rules. For the reordered case, the HMM
alignments are rederived, but the accuracy of these
were no better than those of the unreordered in-
put and experiments showed that the gains in per-
formance were not due to the effect on the align-
ments.
Figure 3 shows difference-histograms for the
language pair English?Hindi, on a sample of
about 10k sentence pairs held out of the system
training data. The histogram indicates that our
reordering rules generalize and that the reordered
English is far more monotonic with respect to the
Hindi.
6 Analysis of errors and future
directions
In this section, we analyze some of the sources of
errors in reordering rules learned via our model, to
better understand directions for further improve-
ment.
1124
?8 ?6 ?4 ?2 0 2 4 6 8 10
?1.5
?1
?0.5
0
0.5
1
1.5
x 104
Distance to next position
Cn
t2
 ?
 C
nt
1
Difference of histograms after and before reordering (EN?HI)
Figure 3: Difference-histogram of word order dis-
tortions for English?Hindi.
6.1 Model weakness
In our initial experiments, we noticed that for the
most frequent reordering rules in English?Hindi
(e.g that IN NP or NP PP flips in Hindi) the prob-
ability of a reordering was roughly 65%. This
was concerning since it meant that on 35% of the
data we would be making wrong reordering deci-
sions by choosing the most likely reordering. To
get a better feel for whether we needed a stronger
model (e.g by lexicalization or by looking at larger
context in the tree rather than just the children),
we analyzed some of the cases in our training data
where (IN,NP), (NP, PP) pairs were left unaltered
in Hindi. In doing that analysis, we noticed exam-
ples involving negatives that our model does not
currently handle. The first issue was mentioned
in Section 4, where the assumption that we can
achieve the right word order by reordering con-
stituent phrases, is incorrect. The second issue
is illustrated by the following sentences: I have
some/no books, which have similar parse struc-
tures, the only difference being the determiner
some vs the determiner no. In Hindi, the order
of the fragments some books and the fragment
no books are different (in the first case the words
stay in order, in the second the flip). Handling
this example would need our model to be lexical-
ized. These issue of negatives requiring special
handling also came up in our analysis of German
(Section 6.4). Other than the negatives (which re-
quire a lexicalized model), the major reason for
the lack of sharpness of the reordering rule proba-
bility was alignment errors and parser issues. We
Aligner
Number of
Sentences fMeasure BLEU score
HMM 250k 62.4 21.7
MaxEnt 250k 76.6 21.4
Manual 5k - 21.3
Table 2: Using different alignments
look at these topics next.
6.2 Alignment accuracy
Since we rely on automatically generated align-
ments to learn the rules, low accuracy of
the alignments could impact the quality of
the rules learned. This is especially a con-
cern for English?Hindi since the quality of
HMM alignments are fairly low. To quan-
tify this effect, we learn reordering rules us-
ing three sets of alignments: HMM alignments,
alignments from a supervised MaxEnt aligner
(Ittycheriah and Roukos, 2005), and hand align-
ments. Table 2 summarizes our results using
aligners with differing alignment qualities for our
English?Hindi task and shows that quality of
alignments in learning the rules is not the driving
factor in affecting rule quality.
6.3 Parser accuracy
Accuracy of the parser in the source language is
a key requirement for our reordering method, be-
cause we choose the single best reordering based
on the most likely parse of the source sentence.
This would especially be an issue in translat-
ing from languages other than English, where the
parser would not be of quality comparable to the
English parser.
In examining some of the errors in reordering
we did observe a fair fraction attributable to
issues in parsing, as seen in the example sentence:
The rich of this country , corner almost 90% of
the wealth .
The second half of the sentence is parsed by the
Berkeley parser (Petrov et al, 2006) as:
FRAG
NP-SBJ
NN
corner
ADVP
RB
almost
NP-SBJ
NP
CD
90%
PP
IN
of
NP
DT
the
NN
wealth
1125
and by IBM?s maximum entropy
parser parser (Ratnaparkhi, 1999) as:
VP
VB
corner
NP
NP
QP
RB
almost
CD
90%
PP
IN
of
NP
DT
the
NN
wealth
With the first parse, we get the right Hindi order
for the second part of the sentence which is: the
wealth of almost 90% corner . To investigate the
effect of choice of parser we compared using the
Berkeley parser and the IBM parser for reorder-
ing, and we found the BLEU score essentially
unchanged: 21.6 for the Berkeley parser and
21.7 for the IBM parser. A potential source of
improvements might be to use alternative parses
(via different parsers or n-best parses) to generate
n-best reorderings both in training and at test.
6.4 Remarks on German reordering
Despite a common heritage, German word order is
distinct from English, particularly regarding verb
placement. This difference can be dramatic, if an
auxiliary (e.g. modal) verb is used in conjunction
with a full verb, or the sentence contains a subor-
dinate clause. In addition to our experiments with
automatically learned rules, a small set of hand-
crafted reordering rules was created and evalu-
ated. Our preliminary results indicate that the lat-
ter rules tend to outperform the automatically de-
rived ones by 0.5-1.0 BLEU points on average.
These rules are summarized as follows:
1. In a VP immediately following an NP, move
the negation particle to main verb.
2. Move a verb group away from a modal verb;
to the end the of a VP. Negation also moves
along with verb.
3. Move verb group to end of an embed-
ded/relative clause.
4. In a VP following a subject, move negation
to the end of VP (handling residual cases)
The above hand written rules show several weak-
nesses of our automatically learned rules for re-
ordering. Since our model is not lexicalized, nega-
tions are not handled properly as they are tagged
RB (along with other adverbs). Another limitation
apparent from the first rule above (the movement
of verbs in a verb phrase depends on the previous
phrase being a noun phrase) is that the automatic
reordering rule for a node?s children depends only
on the children of that node and not a larger con-
text. For instance, a full verb following a modal
verb is typically parsed as a VP child node of the
modal VP node, hence the automatic rule, as cur-
rently considered, will not take the modal verb
(being a sibling of the full-verb VP node) into ac-
count. We are currently investigating extensions
of the automatic rule extraction alorithm to ad-
dress these shortcomings.
6.5 Future directions
Based on our analysis of the errors and on the
hand designed German rules we would like to ex-
tend our model with more general feature func-
tions in Equation 1 by allowing features: that
are dependent on the constituent words (or head-
words), that examine a large context than just a
nodes children (see the first German rule above)
and that fire for all permutations when the con-
stituent X is moved to the end (or start). This
would allow us to generalize more easily to learn
rules of the type ?move X to the end of the
phrase?. Another direction that we feel should be
explored, is the use of multiple parses to obtain
multiple reorderings and combine these at a later
stage.
7 Conclusions
In this paper we presented a simple method to
automatically derive rules for reordering source
sentences to make it look more like target
language sentences. Experiments (on inter-
nal and public test sets) indicate performance
gains for English?French, English?Spanish,
and English?Hindi. For English?German we
did not see improvements with automatically
learned rules while a few hand designed rules did
give improvements, which motivated a few direc-
tions to explore.
1126
References
[Al-Onaizan and Papineni2006] Al-Onaizan, Yaser and
Kishore Papineni. 2006. Distortion models for sta-
tistical machine translation. In Proceedings of ACL.
[Badr et al2009] Badr, Ibrahim, Rabih Zbib, and
James Glass. 2009. Syntactic phrase reordering for
english-to-arabic statistical machine translation. In
Proceedings of EACL.
[Chiang2005] Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine transla-
tion. In Proceedings of ACL.
[Collins et al2005] Collins, Michael, Philipp Koehn,
and Ivona Kucerova. 2005. Clause restructuring
for statistical machine translation. In Proceedings
of ACL.
[Habash2007] Habash, Nizar. 2007. Syntactic prepro-
cessing for statistical machine translation. In MT
Summit.
[Ittycheriah and Roukos2005] Ittycheriah, Abraham
and Salim Roukos. 2005. A maximum entropy
word aligner for arabic-english machine translation.
In Proceedings of HLT/EMNLP.
[Ittycheriah and Roukos2007] Ittycheriah, Abraham
and Salim Roukos. 2007. Direct translation model
2. In Proceedings of HLT-NAACL, pages 57?64.
[Koehn et al2003] Koehn, Philipp, Franz Och, and
Daniel Marcu. 2003. Statistical phrase-based trans-
lation. In Proceedings of HLT-NAACL.
[Niessen and Ney2001] Niessen, Sonja and Hermann
Ney. 2001. Morpho-syntactic analysis for reorder-
ing in statistical machine translation. In Proc. MT
Summit VIII.
[Petrov et al2006] Petrov, Slav, Leon Barrett, Romain
Thibaux, and Dan Klein. 2006. Learning accu-
rate, compact, and interpretable tree annotation. In
COLING-ACL.
[Popovic et al2009] Popovic, Maja, David Vilar,
Daniel Stein, Evgeny Matusov, and Hermann Ney.
2009. The RWTH machine translation system for
WMT 2009. In Proceedings of WMT 2009.
[Ramanathan et al2008] Ramanathan, A., P. Bhat-
tacharyya, J. Hegde, R. M. Shah, and M. Sasikumar.
2008. Simple syntactic and morphological process-
ing can help english-hindi statistical machine trans-
lation. In Proceedings of International Joint Con-
ference on Natural Language Processing.
[Ratnaparkhi1999] Ratnaparkhi, Adwait. 1999. Learn-
ing to parse natural language with maximum en-
tropy models. Machine Learning, 34(1-3).
[Tillman2004] Tillman, Christoph. 2004. A unigram
orientation model for statistical machine translation.
In Proceedings of HLT-NAACL.
[Wang et al2007] Wang, Chao, Michael Collins, and
Philipp Koehn. 2007. Chinese syntactic reordering
for statistical machine translation. In Proceedings
of EMNLP-CoNLL.
[Xia and McCord2004] Xia, Fei and Michael McCord.
2004. Improving a statistical mt system with auto-
matically learned rewrite patterns. In Proceedings
of Coling.
[Xu et al2009] Xu, Peng, Jaeho Kang, Michael Ring-
gaard, and Franz Och. 2009. Using a dependency
parser to improve SMT for Subject-Object-Verb lan-
guages. In Proceedings of NAACL-HLT.
[Yamada and Knight2001] Yamada, Kenji and Kevin
Knight. 2001. A syntax-based statistical translation
model. In Proceedings of ACL.
[Zhang et al2007] Zhang, Yuqi, Richard Zens, and
Hermann Ney. 2007. Chunk-level reordering
of source language sentences with automatically
learned rules for statistical machine translation. In
NAACL-HLT AMTA Workshop on Syntax and Struc-
ture in Statistical Translation.
[Zollmann and Venugopal2006] Zollmann, Andreas
and Ashish Venugopal. 2006. Syntax augmented
machine translation via chart parsing. In Pro-
ceedings on the Workshop on Statistical Machine
Translation.
[Zollmann et al2008] Zollmann, Andreas, Ashish
Venugopal, Franz Och, and Jay Ponte. 2008. A
systematic comparison of phrase-based, hierar-
chical and syntax-augmented statistical MT. In
Proceedings of COLING.
[Zwarts and Dras2007] Zwarts, Simon and Mark Dras.
2007. Syntax-based word reordering in phrase-
based statistical machine translation: why does it
work? In Proc. MT Summit.
1127
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 486?496,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Word Reordering Model for Improved Machine Translation
Karthik Visweswariah
IBM Research India
Bangalore, India
v-karthik@in.ibm.com
Rajakrishnan Rajkumar
Dept. of Linguistics
Ohio State University
raja@ling.osu.edu
Ankur Gandhe
IBM Research India
Bangalore, India
ankugand@in.ibm.com
Ananthakrishnan Ramanathan
IBM Research India
Bangalore, India
aramana2@in.ibm.com
Jiri Navratil
IBM T.J. Watson Research Center
Yorktown Heights, New York
jiri@us.ibm.com
Abstract
Preordering of source side sentences has
proved to be useful in improving statistical
machine translation. Most work has used a
parser in the source language along with rules
to map the source language word order into
the target language word order. The require-
ment to have a source language parser is a ma-
jor drawback, which we seek to overcome in
this paper. Instead of using a parser and then
using rules to order the source side sentence
we learn a model that can directly reorder
source side sentences to match target word or-
der using a small parallel corpus with high-
quality word alignments. Our model learns
pairwise costs of a word immediately preced-
ing another word. We use the Lin-Kernighan
heuristic to find the best source reordering ef-
ficiently during training and testing and show
that it suffices to provide good quality reorder-
ing.
We show gains in translation performance
based on our reordering model for translating
from Hindi to English, Urdu to English (with
a public dataset), and English to Hindi. For
English to Hindi we show that our technique
achieves better performance than a method
that uses rules applied to the source side En-
glish parse.
1 Introduction
Languages differ in the way they order words to pro-
duce sentences representing the same meaning. Ma-
chine translation systems need to reorder words in
the source sentence to produce fluent output in the
target language that preserves the meaning of the
source sentence.
Current phrase based machine translation systems
can capture short range reorderings via the phrase
table. Even the capturing of these local reordering
phenomena is constrained by the amount of training
data available. For example, if adjectives precede
nouns in the source language and follow nouns in the
target language we still need to see a particular ad-
jective noun pair in the parallel corpus to handle the
reordering via the phrase table. Phrase based sys-
tems also rely on the target side language model to
produce the right target side order. This is known
to be inadequate (Al-Onaizan and Papineni, 2006),
and this inadequacy has spurred various attempts to
overcome the problem of handling differing word
order in languages.
One approach is through distortion models, that
try to model which reorderings are more likely
than others. The simplest models just penalize
long jumps in the source sentence when producing
the target sentence. These models have also been
generalized (Al-Onaizan and Papineni, 2006; Till-
man, 2004) to allow for lexical dependencies on the
source. While these models are simple, and can
be integrated with the decoder they are insufficient
to capture long-range reordering phenomena espe-
cially for language pairs that differ significantly.
The weakness of these simple distortion models
has been overcome using syntax of either the source
or target sentence (Yamada and Knight, 2002; Gal-
ley et al, 2006; Liu et al, 2006; Zollmann and Venu-
gopal, 2006). While these methods have shown to
be useful in improving machine translation perfor-
486
mance they generally involve joint parsing of the
source and target language which is significantly
more computationally expensive when compared to
phrase based translation systems. Another approach
that overcomes this weakness, is to to reorder the
source sentence based on rules applied on the source
parse (either hand written or learned from data) both
when training and testing (Collins et al, 2005; Gen-
zel, 2010; Visweswariah et al, 2010).
In this paper we propose a novel method for deal-
ing with the word order problem that is efficient and
does not rely on a source or target side parse being
available. We cast the word ordering problem as a
Traveling Salesman Problem (TSP) based on previ-
ous work on word-based and phrased-based statis-
tical machine translation (Tillmann and Ney, 2003;
Zaslavskiy et al, 2009). Words are the cities in the
TSP and the objective is to learn the distance be-
tween words so that the shortest tour corresponds to
the ordering of the words in the source sentence in
the target language. We show that the TSP distances
for reordering can be learned from a small amount
of high-quality word alignment data by means of
pairwise word comparisons and an informative fea-
ture set involving words and part-of-speech (POS)
tags adapted and extended from prior work on de-
pendency parsing (McDonald et al, 2005b). Ob-
taining high-quality word alignments that we require
for training is fairly easy compared with obtaining a
treebank required to obtain parses for use in syntax
based methods.
We show experimentally that our reordering
model, even when used to reorder sentences for
training and testing (rather than being used as an
additional score in the decoder) improves machine
translation performance for: Hindi ? English, En-
glish?Hindi, and Urdu? English. Although Urdu
is similar to Hindi from the point of reordering phe-
nomena we include it in our experiments since there
are publicly available datasets for Urdu-English. For
English ? Hindi we obtained better machine trans-
lation performance with our reordering model as
compared to a method that uses reordering rules ap-
plied to the source side parse.
The rest of the paper is organized as follows. Sec-
tion 2 reviews related work and places our work in
context. Section 3 outlines reordering issues due
to syntactic differences between Hindi and English.
Section 4 presents our reordering model, Section 5
presents experimental results and Section 6 presents
our conclusions and possible future work.
2 Related work
There have been several studies demonstrating im-
proved machine translation performance by reorder-
ing source side sentences based on rules applied to
the source side parse during training and decoding.
Much of this work has used hand written rules and
several language pairs have been studied e.g German
to English (Collins et al, 2005), Chinese to English
(Wang et al, 2007), English to Hindi (Ramanathan
et al, 2009), English to Arabic (Badr et al, 2009)
and Japanese to English (Lee et al, 2010). There
have also been some studies where the rules are
learned from the data (Genzel, 2010; Visweswariah
et al, 2010; Xia and McCord, 2004). In addition
there has been work (Yamada and Knight, 2002;
Zollmann and Venugopal, 2006; Galley et al, 2006;
Liu et al, 2006) which uses source and/or target
side syntax in a Context Free Grammar framework
which results in machine translation decoding being
considered as a parsing problem. In this paper we
propose a model that does not require either source
or target side syntax while also preserving the effi-
ciency of reordering techniques based on rules ap-
plied to the source side parse.
In work that is closely related to ours, (Tromble
and Eisner, 2009) formulated word reordering as a
Linear Ordering Problem (LOP), an NP-hard permu-
tation problem. They learned LOP model weights
capable of assigning a score to every possible per-
mutation of the source language sentence from an
aligned corpus by using a averaged perceptron learn-
ing model. The key difference between our model
and the model in (Tromble and Eisner, 2009) is that
while they learn costs of a word wi appearing any-
where before wj , we learn costs of wi immediately
preceding wj . This results in more compact models
and (as we show in Section 5) better models.
Our model results in us having to solve a TSP
instance. The relation between the TSP and ma-
chine translation decoding has been explored before.
(Knight, 1999) showed that TSP is a sub-class ofMT
decoding and thus established that the latter is NP-
hard. (Zaslavskiy et al, 2009) casts phrase-based
487
decoding as a TSP and they show favorable speed
performance trade-offs compared with Moses, an
existing state-of-the-art decoder. In (Tillmann and
Ney, 2003), a beam-search algorithm used for TSP
is adapted to work with an IBM-4 word-based model
and phrase-based model respectively. As opposed
to calculating TSP distances from existing machine
translation components ( viz. the translation, dis-
tortion and language model probabilities) we learn
model weights to reorder source sentences to match
target word order using an informative feature set
adapted from graph-based dependency parsing (Mc-
Donald et al, 2005a).
3 Hindi-English reordering issues
This section provides a brief survey of constructions
that the two languages in question differ as well as
have in common. (Ramanathan et al, 2009) notes
the following divergences:
? English follows SVO order while Hindi follows
SOV order
? English uses prepositions while Hindi uses
post-positions
? Hindi allows greater word order freedom
? Hindi has a relatively richer case-marking sys-
tem
In addition to these differences, (Visweswariah et
al., 2010) mention the similarity in word order in
the case of adjective noun sequences (some books
vs. kuch kitab).
4 Reordering model
Consider a source sentence w consisting of a se-
quence of n words w1, w2, ... wn that we would
like to reorder into the target language order. Given
a permutation pi of the indices 1..n, let the candi-
date reordering be wpi1 , wpi2 , ..., wpin . Thus, pii de-
notes the index of the word in the source sentence
that maps to position i in the candidate reordering.
Clearly there are n! such permutations. Our reorder-
ing model assigns costs to candidate permutations
as:
C(pi|w) =
?
i
c(pii?1, pii).
The cost c(m,n) can be thought of as the cost of the
word at index m immediately preceding the word
with index n in the candidate reordering. In this pa-
per, we parametrize the costs as:
c(m,n) = ?T?(w,m, n),
where ? is a learned vector of weights and ? is a
vector of feature functions.
Given a source sentence w we reorder it accord-
ing to the permutation pi that minimizes the cost
C(pi|w). Thus, we would like our cost function
C(pi|w) to be such that the correct reordering pi? has
the lowest cost of all possible reorderings pi. In Sec-
tion 4.1 we describe the features ? that we use, and
in Section 4.2 we describe how we train the weights
? to obtain a good reordering model.
Given our model structure, the minimization
problem that we need to solve is identical to solving
a Asymmetric Traveling Salesman Problem (ATSP)
with each word corresponding to a city, and the costs
c(m,n) representing the pairwise distances between
the cities. Consider the following example:
English input: John eats apples
Hindi: John seba(apples) khaataa hai(eats)
Desired reordered English: John apples eats
The ATSP that we need to solve is represented
pictorially in Figure 1 with sample costs. Note that
we have one extra node numbered 0. We start and
end the tour at node 0, and this determines the first
word in the reordered sentence. In this example the
minimum cost tour is:
Start ? John ? apple ? eats
recovering the right reordering for translation into
Hindi.
Solving the ATSP (which is a well known NP hard
problem) efficiently is crucial for the efficiency of
our reordering model. To solve the ATSP, we first
convert the ATSP to a symmetric TSP and then use
the Lin-Kernighan heuristic as implemented in Con-
corde, a state-of-the-art TSP solver (Applegate et al,
2005). We also experimented with using the exact
TSP solver in Concorde but since it was slower and
did not improve performance we preferred using the
Lin-Kernighan heuristic. To convert the ATSP to
a symmetric TSP we double the size of the orig-
inal problem creating a node N ? for every node
N in the original graph. Following (Hornik and
488
3apples
2eats1John
0Start
c(2,3)=3c(3,2)=1
c(0,3)=5c(3,0)=2
c(0,1)=-1c(1,0)= 5 c(1,3)=0
c(0,2)=5
c(1,2)=3c(2,1)=5
c(3,1)=5
c(2,0)=-2
Figure 1: Example of an ATSP for reordering the sen-
tence: John eats apples.
Hahsler, 2009), we then set new costs as follows:
c?(A,B) = ?, c?(A,B?) = c?(B? , A) = c(A,B)
and C(A,A?) = ??. Even with this doubling of
the number of nodes, we observed that solving the
TSPs with the Lin-Kernighan heuristic is very fast,
taking roughly 10 milliseconds per sentence on av-
erage. Overall, this means that our reordering model
is as fast as parsing and hence our model is compara-
ble in performance to techniques based on applying
rules to the parse tree.
4.1 Features
Since we would like to model reordering phenomena
which are largely related to analyzing the syntax of
the source sentence, we chose to use features based
on those that have in the past been used for parsing
(McDonald et al, 2005a). A subset of the features
we use was also used for reordering in (Tromble and
Eisner, 2009).
To be able to generalize from relatively small
amounts of data, we use features that in addition to
depending on the words in the input sentence w de-
pend on the part-of-speech (POS) tags of the words
in the input sentence. All features ?(w, i, j) we use
are binary features, that fire based on the identities
of the words and POS tags at or surrounding posi-
tions i and j in the source sentence. The first set of
feature templates we use are given in Table 1. These
features depend only on the identities of the word
and POS tag of the two positions i and j and we call
wi pi wj pj
? ? ? ?
?
?
?
?
? ?
? ?
? ?
? ?
? ?
? ?
? ? ?
Table 1: Bigram feature templates used to calculate the
cost that word at position i immediately precedes word at
position j in the target word order. wi (pi) denotes the
word (POS tag) at position i in the source sentence. Each
of the templates is also conjoined with i-j the signed dis-
tance between the two words in the source sentence.
these Bigram features.
The second set of feature templates we use are
given in Table 2. These features, in addition to ex-
amining positions i and j examine the surround-
ing positions. We instantiate these feature templates
separately for the POS tag sequence and for the
word sequence. We call these two feature sets Con-
textPOS and ContextWord respectively. When in-
stantiated with POS tags, the first row of Table 2
looks at all POS tags between positions i and j.
(Tromble and Eisner, 2009) use Bigram and Con-
textPOS features, while we extend their feature set
with the use of ContextWord features. Since Hindi
is verb final, in Hindi sentences with multiple verb
groups it is rare for words with a verb in between
to be placed together in the reordering to match En-
glish. Looking at the POS tags of words between
positions i and j allows us to penalize such reorder-
ings.
Each of the templates described in Table 1 and
Table 2 is also conjoined with i-j the signed dis-
tance between the two words in the source sentence.
The values of i-j between 5 and 10, and greater than
10 are quantized (negative values are similarly quan-
tized).
In Section 5.2 we report on experiments showing
the relative performance of these different feature
489
oi?1 oi oi+1 ob oj?1 oj oj+1
? ? ?
? ? ? ?
? ? ?
? ? ?
? ? ?
? ? ?
? ? ? ?
? ? ?
? ? ?
? ? ?
? ? ?
? ? ? ?
? ? ?
? ? ?
? ? ? ?
? ? ?
? ? ?
Table 2: Context feature templates used to calculate the
cost that word at position i immediately precedes word
at position j in the target word order. oi denotes the ob-
servation at position i in the source sentence and ob de-
notes an observation at a position between i and j (i.e
i + 1 ? b ? j ? 1). Each of the templates is instan-
tiated with the observation sequence o taken to be the
word sequence w and the POS tag sequence p. Each of
the templates is also conjoined with i-j the signed dis-
tance between the two positions in the source sentence.
types for the task of reordering Hindi sentences to
be in English word order.
4.2 Training
To train the weights ? in our model, we need a
collection of sentences, where we have the desired
reference reordering pi?(x) for each input sentence
x. To obtain these reference reorderings we use
word aligned source-target sentence pairs. The qual-
ity and consistency of these reference reorderings
will depend on the quality of the word alignments
that we use. Given word aligned source and tar-
get sentences, we drop the source words that are not
aligned. Let mi be the mean of the target word po-
sitions that the source word at index i is aligned to.
We then sort the source indices in increasing order
of mi. If mi = mj (for example, because wi and wj
are aligned to the same set of words) we keep them
in the same order that they occurred in the source
sentence. Obtaining the target ordering in this man-
ner, is certainly not the only possible way and we
would like to explore better treatment of this in fu-
ture work.
We used the single best Margin Infused Re-
laxed Algorithm (MIRA) ((McDonald et al, 2005b),
(Crammer and Singer, 2003)) with the online up-
dates to our parameters being given by
?i+1 = argmin
?
||? ? ?i||
s.t. C(pi?|w) < C(p?i|w)? L(pi?, p?i).
In the equation above,
p?i = argmin
pi
C(pi|x)
is the best reordering based on the current parameter
value and L is a loss function. We take the loss of
a reordering to be the number of words for which
the preceding word is wrong relative to the reference
target order.
We also experimented with the averaged percep-
tron algorithm (Collins, 2002), but found single best
MIRA to work slightly better and hence used MIRA
for all our experiments.
5 Experiments
In this section we report on experiments to evalu-
ate our reordering model. The first method we use
for evaluation (monolingual BLEU) is by generat-
ing the desired reordering of the source sentence (as
described in Section 4.2) and compare the reordered
output to this desired reordered sentence using the
BLEU metric. In addition, to these monolingual
BLEU results, we also evaluate (in Section 5.5) the
reordering by its effect on eventual machine transla-
tion performance.
We note that our reordering techniques uses POS
information for the input sentence. The POS taggers
used in this paper are Maximum Entropy Markov
models trained using manually annotated POS cor-
pora. For Hindi, we used roughly fifty thousand
words with twenty six tags from the corpus de-
scribed in (Dalal et al, 2007). For Urdu we used
roughly fifty thousand words and forty six tags from
the CRULP corpus (Hussain, 2008) and for English
we used the Wall Street Journal section of the Penn
Treebank.
490
5.1 Reordering model training data and
alignment quality
To train our reordering models we need training data
where we have the input source language sentence
and the desired reordering in the target language.
As described in Section 4.2 we derive the refer-
ence reordered sentence using word alignments. Ta-
ble 3 presents our monolingual BLEU results for
Hindi to English reordering as the source of the
word alignments is varied. All results in Table 3
are with Bigram and ContextPOS features. We have
word alignments from three sources: A small set
of hand aligned sentences, HMM alignments (Vo-
gel et al, 1996) and alignments obtained using a su-
pervised Maximum Entropy aligner (Ittycheriah and
Roukos, 2005) trained on the hand alignments. The
F-measure for the HMM alignments were 65% and
78% for the Maximum Entropy model alignments.
We see that the quality of the alignments is an im-
portant determiner of reordering performance. Row
1 shows the BLEU for unreordered (baseline) Hindi
compared with the Hindi sentences reordered in En-
glish Order. Using just HMM alignments to train
our model we do worse than unreordered Hindi. Al-
though using the Maximum Entropy alignments is
better than using HMM alignments, we do not im-
prove upon a small number of hand alignments by
using all the Maximum Entropy alignments.
To improve upon the model trained with only
hand alignments we selected a small number of snip-
pets of sentences from our Maximum Entropy align-
ments. The goal was to pick parts of sentences
where the alignment is reliable enough to use for
training. The heuristic we used in the selection of
snippets was to pick maximal snippets of at least
7 consecutive Hindi words with all Hindi words
aligned to a consecutive span of English words,
with no unaligned English words in the span and no
English words aligned to Hindi words outside the
span. Adding snippets selected with this heuristic
improves the reordering performance of our model
as seen in the last row of Table 3.
5.2 Feature set comparison
In this section we report on experiments to deter-
mine the performance of the different classes of fea-
tures (Bigram, ContextPos and ContextWord) dis-
HMM MaxEnt Hand BLEU
- - - 35.9
220K - - 35.4
- 220K - 47.0
- 220K 6K 48.4
- - 6K 49.0
- Good 17K 6K 51.3
Table 3: Monolingual BLEU scores for Hindi to English
reordering using models trained on different alignment
types and tested on a development set of 280 Hindi sen-
tences (5590 tokens).
Feature template
Bigram ContextPOS ContextWord BLEU
- - - 35.9
? - - 43.8
? ? - 49.0
? ? ? 51.3
Table 4: Monolingual BLEU scores for Hindi to En-
glish reordering using models trained with different fea-
ture sets and tested on a development set of 280 Hindi
sentences (5590 tokens).
cussed in Section 4.1. Table 4 shows monolingual
BLEU results for training with different features sets
for Hindi to English reordering. In all cases, we
use a set of 6000 sentence pairs which were hand
aligned to generate the training data. It is clear that
all three sets of features contribute to performance of
the reordering model, however the number of Con-
textWord features is larger than the number of Bi-
gram and ContextPOS features put together, and it
may be desirable to select from this set of features
especially when training on large amounts of data.
5.3 Monolingual reordering comparisons
Table 5 compares our reordering model with a reim-
plementation of the reordering model proposed in
(Tromble and Eisner, 2009). Both the models use
exactly the same features (bigram features and Con-
textPOS features) and are trained on the same data.
To generate our training data, for Hindi to English
and English to Hindi we use a set of 6000 hand
aligned sentences, for Urdu to English we use a set
of 8500 hand aligned sentences and for English to
French we use a set of 10000 hand aligned sentences
(a subset of Europarl and Hansards corpus). Our
491
Language pair Monolingual BLEU
Source Target Unreordered LOP TSP
Hindi English 35.9 36.6 49.0
English Hindi 34.4 48.4 56.7
Urdu English 35.6 39.5 49.9
English French 64.4 78.2 81.2
Table 5: Monolingual BLEU scores comparing the orig-
inal source order with desired target reorder without re-
ordering, and reordering using our model (TSP) and the
model proposed in (Tromble and Eisner, 2009) (LOP).
test data consisted of 280 sentences for Hindi to En-
glish and 400 sentences for all other language pairs
generated from hand aligned sentences. We include
English-French here to compare on a fairly similar
language pair with local reordering phenomena (the
main difference being that in French adjectives gen-
erally follow nouns). We note that our model outper-
forms the model proposed in (Tromble and Eisner,
2009) in all cases.
5.4 Analysis of reordering performance
To get a feel for the qualitative performance of our
reordering algorithm and the kind of phenomena it
is able to capture, we analyze the reordering per-
formance in terms of (i) whether the clause restruc-
turing is done correctly ? these can be thought of
as medium-to-long range reorderings, (ii) whether
clause boundaries are respected, and (iii) whether lo-
cal (short range) reordering is performed correctly.
The following analysis is for Hindi to English re-
ordering with the best model (this is also the model
used for Machine Translation experiments reported
on in Section 5.5).
? Clause structure: As discussed in Section 3,
the canonical clause order in Hindi is SOV,
while in English it is SVO. However, variations
on this structure are possible and quite frequent
(e.g., clauses with two objects). To evaluate
clause restructuring, we compared sequences
of subjects, objects and verbs in the output and
reference reorderings.
We had a set of 70 sentences annotated with
subject, direct object, indirect object and verb
information ? these annotations were made on
the head word of each phrase, and the compar-
isons were on sequences of these words alone
and not the entire constituent phrase. 52 sen-
tences were reordered by the model to match
the order of the corresponding reference. Eight
sentences were ordered correctly but differently
from the reference, because the reference was
expressed in non-canonical fashion (e.g., in the
passive) ? note that these cases negatively im-
pact the monolingual BLEU score. The follow-
ing example shows a sentence being reordered
correctly, where, however, the reference is ex-
pressed differently (note the position of the
subject ?policy? (niiti) in the reference and the
reordered output) 1:
Input: aba1 (now) taka2 (till) aisii3 (this) niiti4
(policy) kabhii5 (ever) nahii6 (not) rahii7 (has)
hai8 (been)
Reordered: taka2 (till) aba1 (now) aisii3 (this)
niiti4 (policy) hai8 (been) kabhii5 (ever) nahii6
(not) rahii7 (has)
Reference: taka2 (till) aba1 (now) aisii3 (this)
kabhii5 (ever) nahii6 (not) rahii7 (has) hai8
(been) niiti4 (policy)
English: Till now this never has been the policy
The remaining ten sentences were reordered in-
correctly. These errors are largely in clauses
which deviate from the SVO order in some
way ? clauses with multiple subjects or objects,
clauses with no object, etc.. For example, the
following sentence with two subjects and ob-
jects corresponding to the verb wearing has not
been reordered correctly.
Input: sabhii1 (all) purusha2 (men) safeda3
(white) evama4 (and) mahilaaen5 (women)
kesariyaa6 (saffron) vastra7 (clothes) dhaarana8
(wear) kiye9 hue10 (-ing) thiin11 (were)
Reordered: sabhii1 (all) purusha2 (men)
safeda3 (white) evama4 (and) mahilaaen5
(women) kesariyaa6 (saffron) vastra7 (clothes)
dhaarana8 (wear) thiin11 (were) kiye9 hue10 (-
ing)
Reference: sabhii1 (all) purusha2 (men)
thiin11 (were) dhaarana8 (wear) kiye9 hue10 (-
1The numeric subscripts in the examples indicate word po-
sitions in the input.
492
ing) safeda3 (white) evama4 (and) mahilaaen5
(women) kesariyaa6 (saffron)
English: All men were wearing white and the
women saffron
The model possibly needs more data with pat-
terns that deviate from the standard SOV order
to learn to reorder them correctly. We could
also add to the model, features pertaining to
subject, object, etc.
? Clause boundaries: Measured on a set of
844 sentences which were marked with clause
boundaries, 37 sentences (4.4 %) had reorder-
ings that violated these boundaries. An exam-
ple of such a clause-boundary violation is be-
low:
Input: main1 (I) sarakaara2 (government) kaa3
(of) dhyaana4 (attention) maananiiya5 (hon-
ourable) pradhaana6 (prime) mantri7 (min-
ister) dvaaraa8 (by) isa9 (this) sabhaa10
(house) me11 (in) kiye12 gaye13 (made) isa14
(this) vaade15 (promise) ki16 ora17 (towards)
dilaanaa18 (to bring) chaahuungaa19 (would
like)
Reordered: main1 (I) chahuungaa19 (would
like) dilaanaa18 (to bring) kii16 ora17 (to-
wards) isa9 (this) vaade15 (promise) kiye12
gaye13 (made) dvaaraa8 (by) maananiiya5
(honourable) mantri7 (minister) pradhaana6
(prime) dhyaana4 (attention) kaa3 (of)
sarakaara2 (government) men11 (in) isa14 (this)
sabhaa10 (house)
Reference: main1 (I) chahuungaa19 (would
like) dilaanaa18 (to bring) dhyaana4 (attention)
kaa3 (of) sarakaara2 (government) kii16 ora17
(towards) isa9 (this) vaade15 (promise) kiye12
gaye13 (made) dvaaraa8 (by) maananiiya5
(honourable) mantri7 (minister) pradhaana6
(prime) men11 (in) isa9 (this) sabhaa10 (house)
English I would like to bring the attention of
the government towards this promise made by
the honourable prime minister in this house.
Note how the italicized clause, which is kept
together in the reference, is split up incorrectly
in the reordered output. The proportion of such
boundary violations is, however, quite low, be-
cause Hindi being a verb-final language, most
clauses end with a verb and it is probably quite
straightforward for the model to keep clauses
separate. A clause boundary detection program
should make it possible to eliminate the re-
maining errors.
? Local reordering: To estimate the short range
reordering performance, we consider how of-
ten different POS bigrams in the input are re-
ordered correctly. Here, we expect the model
to reorder prepositions correctly, and to avoid
any reordering that moves apart nouns and their
adjectival pre-modifiers or components of com-
pound nouns (see Section 3). Table 6 sum-
marizes the reordering performance for these
categories for a set of 280 sentences (same as
the test set used in Section 5.1). Each row
in Table 6 indicates the total number of cor-
rect instances for the pair, i.e., the number of
instances of the pair in the reference (column
titled Total), the number of instances that al-
ready appear in the correct order in the input
(column Input), and the number that are or-
dered correctly by the reordering model (col-
umn Reordered). The first two rows show that
adjective-noun and noun-noun (compounds)
are in most cases correctly retained in the orig-
inal order by the model. The final row shows
that while many prepositions have been moved
into their correct positions, there are still quite a
few mismatches with the reference. An impor-
tant reason why this happens is that nouns mod-
ified by prepositional phrases can often also be
expressed as noun compounds. For example,
vidyuta (electricity) kii (of) aavashyakataaen
(requirements) in Hindi can be expressed either
as ?requirements of electricity? or ?electricity
requirements?. The latter expression results in
a match with the input (explaining many of the
104 correct orders in the input) and a mismatch
with the model?s reordering. The same problem
in the training data would also adversely impact
the learning of the preposition reordering rule.
493
POS pair Total Input Reordered
adj-noun 234 192 196
noun-noun 46 44 42
prep-noun 436 104 250
Table 6: An analyis of reordering for a few POS bigrams
5.5 Machine translation results
We now present experiments in incorporating the re-
ordering model in machine translation systems. For
all results presented here, we reorder the training and
test data using the single best reordering based on
our reordering model for each sentence. For each of
the language pairs we evaluated, we trained Direct
Translation Model 2 (DTM) systems (Ittycheriah
and Roukos, 2007) with and without reordering and
compared performance on test data. We note that the
DTM system includes features that allow it to model
lexicalized reordering phenomena. The reordering
window size was set to +/-8 words for both the base-
line and our reordered input. In our experiments, we
left the word alignments fixed, i.e we reordered the
existing word alignments rather than realigning the
sentences after reordering. Redoing the word align-
ments with the reordered data could potentially give
further small improvements. We note that we ob-
tained better baseline performance using DTM sys-
tems than the standard Moses/Giza++ pipeline (e.g
we obtained a BLEU of 14.9 for English to Hindi
with a standard Moses/Giza++ pipeline). For all of
our systems we used a combination of HMM (Vo-
gel et al, 1996) and MaxEnt alignments (Ittycheriah
and Roukos, 2005).
For our Hindi-English experiments we use a train-
ing set of roughly 250k sentences (5.5Mwords) con-
sisting of the Darpa-TIDES dataset (Bojar et al,
2010) and an internal dataset from several domains
but dominated by news. Our test set was roughly
1.2K sentences from the news domain with a sin-
gle reference. To train our reordering model, we
used roughly 6K alignments plus 17K snippets se-
lected from MaxEnt alignments as described in Sec-
tion 5.1 with bigram, ContextPOS and ContextWord
features. The monolingual reordering BLEU (on the
same data reported on in Section 5.3) was 54.0 for
Hindi to English and 60.8 for English to Hindi.
For our Urdu-English experiments we used 70k
Language pair BLEU
Source Target Unreordered Reordered
Hindi English 14.7 16.7
Urdu English 23.3 24.8
English Hindi 20.7 22.5
Table 7: Translation performance without reordering
(baseline) compared with performance after preordering
with our reordering model.
sentences from the NIST MT-08 training corpus
and used the MT-08 eval set for testing. We note
that the MT-08 eval set has four references as com-
pared to one reference for our Hindi-English test
set. This largely explains the improved baseline per-
formance for Urdu-English as compared to Hindi-
English. We present averaged results for the Web
and News part of the test sets. To train the reorder-
ing model we used 9K hand alignments and 11K
snippets extracted from MaxEnt alignments as de-
scribed in Section 5.1 with bigram, ContextPOS and
ContextWord context feature. The monolingual re-
ordering BLEU for the reordering model thus ob-
tained (on the same data reported on in Section 5.3)
was 52.7.
Table 7 shows that for Hindi to English, English
to Hindi and for Urdu to English we see a gain
of 1.5 - 2 BLEU points. For English ? Hindi
we also experimented with a system that uses rules
(learned from the data using the methods described
in (Visweswariah et al, 2010)) applied to a parse to
reorder source side English sentences. This system
had a BLEU score of 21.2, which is an improvement
over the baseline, but our reordering model is better
by 1.3 BLEU points.
An added benefit of our reordering model is that
the decoder can be run with a smaller search space
exploring only a small amount of reordering with-
out losing accuracy but running substantially faster.
Table 8 shows the variation in machine Hindi to En-
glish translation performance with varying skip size
(this parameter sets the maximum number of words
skipped during decoding, lower values are associ-
ated with a restricted decoder search space and in-
creased speed).
494
skip Unreordered Reordered
2 12.2 16.7
4 13.4 16.7
8 14.7 16.4
Table 8: Translation performance with/without reorder-
ing with varying decoder search space.
6 Conclusion and future work
In this paper we presented a reordering model to
reorder source language data to make it resemble
the target language word order without using either
a source or target parser. We showed consistent
gains of up to 2 BLEU points in machine transla-
tion performance using this model to preorder train-
ing and test data. We show better performance com-
pared to syntax based reordering rules for English
to Hindi translation. Our model used only a part of
speech tagger (sometimes trained with fairly small
amounts of data) and a small corpus of word align-
ments. Considering the fact that treebanks required
to build high quality parsers are costly to obtain, we
think that our reordering model is a viable alterna-
tive to using syntax for reordering. We also note,
that with the preordering based on our reordering
model we can achieve the best BLEU scores with
a much tighter search space in the decoder. Even ac-
counting for the cost of finding the best reordering
according to our model, this usually results in faster
processing than if we did not have the reordering in
place.
In future work we plan to explore using more data
from automatic alignments, perhaps by considering
a joint model for aligning and reordering. We would
also like to explore doing away with the requirement
of having a POS tagger, using completely unsuper-
vised methods to class words. We currently only
look at word pairs in calculating the loss function
used in MIRA updates. We would like to investigate
the use of other loss functions and their effect on re-
ordering performance. We also would like to explore
whether the use of scores from our reordering model
directly in machine translation systems can improve
performance relative to using just the single best re-
ordering.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of ACL, ACL-44, pages 529?536, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
David L. Applegate, Robert E. Bixby, Vasek Chvatal, and
William J. Cook. 2005. Concorde tsp solver. In
http://www.tsp.gatech.edu/.
Ibrahim Badr, Rabih Zbib, and James Glass. 2009. Syn-
tactic phrase reordering for English-to-Arabic statisti-
cal machine translation. In Proceedings of EACL.
Ondrej Bojar, Pavel Stranak, and Daniel Zeman. 2010.
Data issues in English-to-Hindi machine translation.
In LREC.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Morristown, NJ, USA. Association for Computational
Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. Journal of
Machine Learning Research.
Aniket Dalal, Kumar Nagaraj, Uma Sawant, Sandeep
Shelke, and Pushpak Bhattacharyya. 2007. Building
feature rich pos tagger for morphologically rich lan-
guages: Experiences in Hindi. In Proceedings of In-
ternational Conference on Natural Language Process-
ing.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and
training of context-rich syntactic translation models.
In Proceedings of ACL.
D. Genzel. 2010. Automatically learning source-side re-
ordering rules for large scale machine translation. In
Proceedings of the 23rd International Conference on
Computational Linguistics.
Kurt Hornik and Michael Hahsler. 2009. TSP?
infrastructure for the traveling salesperson problem.
Journal of Statistical Software, 23(i02).
Sarmad Hussain. 2008. Resources for Urdu language
processing. In Proceedings of the 6th Workshop on
Asian Language Resources, IJCNLP?08.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of HLT/EMNLP,
HLT ?05, pages 89?96, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
495
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In Proceedings of HLT-NAACL,
pages 57?64.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Comput. Linguist.,
25:607?615, December.
Young-Suk Lee, Bing Zhao, and Xiaoqian Luo. 2010.
Constituent reordering and syntax models for English-
to-Japanese statistical machine translation. In COL-
ING.
Y. Liu, Q. Liu, and S. Lin. 2006. Tree-to-String align-
ment template for statistical machine translation. In
Proceedings of ACL.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
HLT.
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and morphology: addressing the crux
of the fluency problem in English-Hindi smt. In
Proceedings of ACL-IJCNLP.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a dynamic programming beam search al-
gorithm for statistical machine translation. Computa-
tional Linguistics, 29(1):97?133.
Roy Tromble and Jason Eisner. 2009. Learning linear or-
dering problems for better translation. In Proceedings
of EMNLP.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with automat-
ically derived rules for improved statistical machine
translation. In Proceedings of the 23rd International
Conference on Computational Linguistics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational Linguistics.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of EMNLP-CoNLL.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical MT system with automatically learned rewrite
patterns. In COLING.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical mt. In Proceedings of ACL.
Mikhail Zaslavskiy, Marc Dymetman, and Nicola Can-
cedda. 2009. Phrase-based statistical machine transla-
tion as a traveling salesman problem. In Proceedings
of ACL-IJCNLP.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation.
496
Proceedings of the TextGraphs-8 Workshop, pages 29?38,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Graph-Based Unsupervised Learning of Word Similarities Using
Heterogeneous Feature Types
Avneesh Saluja?
Carnegie Mellon University
avneesh@cmu.edu
Jir??? Navra?til
IBM Research
jiri@us.ibm.com
Abstract
In this work, we propose a graph-based
approach to computing similarities between
words in an unsupervised manner, and take ad-
vantage of heterogeneous feature types in the
process. The approach is based on the creation
of two separate graphs, one for words and
one for features of different types (alignment-
based, orthographic, etc.). The graphs are con-
nected through edges that link nodes in the
feature graph to nodes in the word graph, the
edge weights representing the importance of a
particular feature for a particular word. High
quality graphs are learned during training, and
the proposed method outperforms experimen-
tal baselines.
1 Introduction
Data-driven approaches in natural language process-
ing (NLP) have resulted in a marked improvement
in a variety of NLP tasks, from machine translation
to part-of-speech tagging. Such methods however,
are generally only as good as the quality of the data
itself. This issue becomes highlighted when there
is a mismatch in domain between training and test
data, in that the number of out-of-vocabulary (OOV)
words increases, resulting in problems for language
modeling, machine translation, and other tasks. An
approach that specifically replaces OOV words with
their synonyms from a restricted vocabulary (i.e., the
words already contained in the training data) could
alleviate this OOV word problem.
?This work was done during the first author?s internship at
the IBM T.J. Watson Research Center, Yorktown Heights, NY
in 2012.
Vast ontologies that capture semantic similarities
between words, also known as WordNets, have been
carefully created and compiled by linguists for dif-
ferent languages. A WordNet-based solution could
be implemented to fill the gaps when an OOV word
is encountered, but this approach is not scalable in
that it requires significant human effort for a num-
ber of languages in which the WordNet is limited
or does not exist. Thus, a practical solution to this
problem should ideally require as little human su-
pervision and involvement as possible.
Additionally, words can be similar to each other
due to a variety of reasons. For example, the similar-
ity between the words optimize and optimal can be
captured via the high orthographical similarity be-
tween the words. However, relying too much on a
single feature type may result in false positives, e.g.,
suggestions of antonyms instead of synonyms. Valu-
able information can be gleaned from a variety of
feature types, both monolingual and bilingual. Thus,
any potential solution to an unsupervised or mildly
supervised word similarity algorithm should be able
to take into account heterogeneous feature types and
combine them in a globally effective manner when
yielding the final solution.
In this work, we present a graph-based approach
to impute word similarities in an unsupervised man-
ner and takes into account heterogeneous features.
The key idea is to maintain two graphs, one for
words and one for the all the features of different
types, and attempt to promote concurrence between
the two graphs in an effort to find a final solution.
The similarity graphs learned during training are
generally of high quality, and the testing approach
proposed outperforms the chosen baselines.
29
2 Approach
The eventual goal is to compute the most similar
word to a given OOV word from a restricted, pre-
existing vocabulary. We propose a graph-based so-
lution for this problem, relying on undirected graphs
to represent words and features as well as the simi-
larities between them. The solution can be broadly
divided into two distinct sub-problems, the training
and testing components.
2.1 Learning the Graph
The intuition of our approach is best expressed
through a small example problem. Figure 1 shows
an example graph of words (shaded) and features
(unshaded). For exposition, let v1 = optimize, v2 =
optimal, and v3 = ideal, while f1 = orth |opti, i.e.,
an orthographic feature corresponding to the sub-
string ?opti? at the beginning of a word, and f5 =
align ide?al, i.e., a bilingual feature corresponding to
the alignment of the word ?optimal? to the French
word ?ide?al? in the training data1.
v1	

v3	
v2	

v4	
 v5	

f1	

f2	

f5	

f3	

f4	

Zv1,f1
Wf1,f5
Wv1,v2
Figure 1: An example graph for explanatory purposes. The
nodes in red constitute the word graph, and the nodes in white
the feature graph.
There are three types of edges in this scenario.
Edges between word nodes (e.g., Wv1,v2) represent
word similarities, and edges between features (e.g.,
Wf1,f5) represent feature similarities. Edges be-
tween words and features (e.g., Zv1,f1 , the dashed
lines) represent pertinent or active features for a
given word when computing its similarity with other
words, with the edge weight reflecting the degree of
importance.
We restrict the values of all similarities to be be-
tween 0 and 1, as negative-valued edges in undi-
1such word alignments can be extracted through standard
word alignment algorithms applied to a parallel corpus in two
different languages.
rected graphs are significantly more complicated
and would make subsequent computations more in-
tricate. In an ideal situation, the similarity matrices
that represent the word and feature graphs should be
positive semi-definite, which provides a nice prob-
abilistic interpretation due to connections to covari-
ance matrices of multivariate distributions, but this
constraint is not enforced here. Future work will
focus on improved optimization techniques that re-
spect the positive semi-definiteness constraint.
2.1.1 Objective Function
To learn the graph, the following objective func-
tion is minimized:
?(WV ,WF ,Z) = ?0
?
fp,fq?F
(Wfp,fq ?W ?fp,fq )
2 (1)
+ ?1
?
vi?V
?
fp?F
(Zvi,fp ? Z?vi,fp)
2 (2)
+ ?2
?
vi,vj?V
?
fp,fq?F
Zvi,fpZvj ,fq (Wvi,vj ?Wfp,fq )2
(3)
+ ?3
?
vi,vj?V
?
fp,fq?F
Wvi,vjWfp,fq (Zvi,fp ? Zvj ,fq )2
(4)
where Wfp,fq is the current similarity between fea-
ture fp and feature fq (with corresponding initial
value W ?fp,fq ), Wvi,vj is the current similarity be-
tween word vi and word vj , Zvi,fp is the current
importance weight of feature fp for word vi (with
corresponding initial value Z?vi,fp), and ?0 to ?3 are
parameters (that sum to 1) which represent the im-
portance of a given term in the objective function.
The intuition of the objective function is straight-
forward. The first two terms correspond to minimiz-
ing the `2-norm between the initial and current val-
ues of Wfp,fq and Zvi,fp (for further details on ini-
tialization, see Section 2.1.2). The intuition behind
the third term is to minimize the difference between
the word similarity of words vi and vj and the fea-
ture similarity of features fp and fq in proportion to
how important those features are for words vi and vj
respectively. If two features have high importance
weights for two words, and those features are very
similar to each other, then the corresponding words
should also be similar. The fourth term has a simi-
lar rationale, in that it minimizes the difference be-
tween importance weights in proportion to the sim-
ilarities. In other words, we attempt to promote pa-
rameter concurrence between the word and feature
30
graphs, which in turn ensures smoothness over the
two graphs.
The basic idea of minimizing two quantities of the
graph in proportion to their link strength has been
used before, for example (but not limited to) graph-
based semi-supervised learning and label propaga-
tion (Zhu et al, 2003) where the concept is applied
to node labels (as opposed to edge weights as pre-
sented in this work). In such methods, the idea is
to ensure that the function varies smoothly over the
graph (Zhou et al, 2004), i.e., to promote parame-
ter concurrence within a graph, whereas we promote
parameter concurrence across two graphs. In that
sense, the ? parameters as control the trade-off be-
tween respecting initial values vs. achieving consis-
tency between the two graphs.
While not necessary, we decided to tie the param-
eters together, such that ?0 and ?2 (representing fea-
ture similarity preference for initial values vs. pref-
erence for consistency) sum to 0.5, and ?1 and ?3
sum to 0.5 as well, implicitly giving equal weight to
feature similarities and importance weights. In the
future, a more appropriate method of learning these
? parameters will be explored.
2.1.2 Initialization
In many unsupervised algorithms, e.g., EM, the
initialization of parameters is of paramount impor-
tance, as these initial values guide the algorithm in
its attempt to minimize a proposed objective func-
tion. In our problem, initial estimates for word simi-
larities do not exist (otherwise the problem would be
considerably easier!). Instead, word similarities are
seeded from the initial feature similarities and initial
importance weights, and all three quantities are then
iteratively refined.
The initial importance weight values are com-
puted from the co-occurrence statistics between
words and features, by taking the geometric mean
of the conditional probabilities (feature given word
and word given feature) in both directions: Z?vi,fp =?
P(vi|fp)P(fp|vi). For the initial feature similar-
ity values, the pointwise mutual information (PMI)
vector for each feature is first computed, by taking
the log ratio of the joint probability with each word
to the marginal probabilities of the feature and the
word (also done through the co-occurrence statis-
tics). Subsequently, the initial similarity is then
computed as the normalized dot product between
feature vectors:
PMIfp ?PMIfq
?PMIfp??PMIfq?
.
After computing the initial feature similarity
and weights matrices, we remove features that are
densely connected in the feature similarity graph by
trimming high entropy features (normalizing edge
weights and treating the resulting values as a prob-
ability distribution). This pruning was done in or-
der to speed up the optimization procedure, and we
found that results were not affected by pruning away
the top one percentile of features sorted by entropy.
2.1.3 Optimization
The objective function (Equations 1 to 4) is con-
vex and differentiable with respect to the individ-
ual variables Wvi,vj ,Wfp,fq , and Zvi,fp . Hence, one
way to minimize it is to evaluate the derivatives of
the objective function with respect to these variables,
set to 0 and solve. The final update equations are
provided in the Appendix.
The entire training pipeline is captured in Figure
2. We first compute the word similarities from the
initial feature similarities and importance weights,
and then update those values in turn, based on
the alternating minimization method (Csisza?r and
Tusna?dy, 1984). The process is repeated till con-
vergence.
Preprocessing	

Feature Extraction	

Initialization	

Update Word Sim	

Corpus	

Update Feature Sim	
Repeat for N iterations	

Update Weights	

Figure 2: Flowchart for the training pipeline described in Sec-
tion 2.1.3. The number of iterations N is determined before-
hand.
31
2.2 Link Prediction
Given a learned word similarity graph (along with
a learned feature similarity graph and the edges be-
tween the two graphs) and an OOV word with as-
sociated features, the proposed solution should also
generate a list of synonyms. In a graph-based set-
ting, this is analogous to the link prediction prob-
lem: given a graph and a new node that needs to be
embedded in the graph, which links, or edges, do we
add between the new node and all the existing ones?
We experimented with two different approaches
for link prediction. The first computes word sim-
ilarities in the same manner as in training, as per
Equation 5. However, since the learned importance
weights Zvi,fp (or Zvj ,fq ) are specific to a given
word, importance weights for the OOV word are ini-
tialized in the same manner as in Section 2.1.2 for
the words in the training data. Thus, for a given
OOV word, we obtain word similarities with all
words in the vocabulary through Equation 5, and
output the most similar words by this metric.
The second method is based on a random walk
approach, similar to (Kok and Brockett, 2010),
wherein a probabilistic interpretation is imposed on
the graphs by row-normalizing all of the matrices
involved (word similarity, feature similarity, and im-
portance weights), implying that the transition prob-
ability, say from node vi to vj , is proportional to
the similarity between the two nodes. For this ap-
proach, only the active features for a given OOV
word, i.e., the features that have at least one non-
zero Z edge between the feature and a word, are
used (see Section 2.3 for more details on active and
inactive features). First, M random walks are ini-
tialized from each active feature node, each walk of
maximum length T . For every walk, the number of
steps needed to hit a word node in the word simi-
larity graph for the first time is recorded. After av-
eraging across the M runs, we need to average the
hitting times across all of the active features, which
is done by weighting the hitting times of each ac-
tive feature f? by
?
vi
Zvi,f? , i.e., the sum across all
rows of a given feature (represented by a column) in
the importance weights matrix.
The random walk-based approach introduces
three new parameters: M , the number of random
walks per active feature, T , the maximum length
of each random walk, and ?, a parameter that con-
trols how often a random walk should take a Z
edge (thereby transitioning from one graph to the
other) or a W edge (thereby staying within the same
graph). If a node has both Z and W edges, then ?
is the parameter for a simple Bernoulli distribution
that samples whether to take one type of edge or the
other; if the node has only one type of edge, then the
walk traverses only that type.
2.3 Sparsification
There is a crucial point regarding Equations 1 to
4, namely that restricting the inputted values to be-
tween 0 and 1 does not guarantee that the resulting
similarity or weight value will also be between 0 and
1, due to the difference in terms in the numerator
of the equations. In order to bypass this problem,
a projection step is employed subsequent to an up-
date, wherein the value obtained is projected into the
correct part of the n-dimensional Euclidean space,
namely the positive orthant. Although slightly more
involved in the multidimensional case, i.e., where
n > 1, since the partial derivatives as computed
in Equations 5 to 7 are with respect to a single ele-
ment, orthant projection in the unidimensional case
amounts to nothing more than setting the value to 0
if it is less than 0. This effectively sparsifies the re-
sulting matrix, and is similar to the soft-thresholding
effect that comes about due to `1-norm regulariza-
tion. Further exploration of this link is left to future
work.
However, the sparsification of the graphs/matrices
is problematic for the random walk-based approach,
in that an OOV word may consist of features that are
all inactive, i.e., none of the features have a non-zero
Z edge to the word similarity graph. In this case,
we cannot compute which words in our vocabulary
are similar to the OOV word. One method to allevi-
ate this drawback is to add back Z edges that were
removed during training with their initial weights.
Yet, we found that adding back all of the features
for a test word was worse than filtering out the fea-
tures with the highest entropy (i.e., with the most
edges to other features) out of the features to add
back. The latter approach was thus adopted and is
the setup used in Section 3.5.
3 Experiments & Results
In our experiments, we looked at both the quality of
the similarity graphs learned from the data, as well
as the performance of the link prediction techniques.
32
Corpus Sentences Words
EuroParl+ NewsComm (Train) 1.64 million+ 40.6 million+
WMT2010 (Test) 2034 44,671
Table 1: Corpus statistics for the datasets used in evaluation.
3.1 Dataset
Table 1 summarizes the statistics of the training and
test sets used. We used the standard WMT 2010
evaluation dataset, and the training data consists of a
combination of European Parliament and news com-
mentary bitext, while the test set is from the news
domain. Note that a parallel corpus is not needed as
only the English side is used. While the current ex-
periment is restricted to English, any language can
be used in principle.
3.2 Features
During the feature extraction phase, we first filtered
the 30 most common words from the corpus and do
not extract features for those words. However, these
common words are still used when extracting distri-
butional features. The following features are used:
? Orthographic: all substrings of length 3, 4, and
5 for a given word are extracted. For exam-
ple, the feature ?orth |opt?, corresponding to
the substring ?opt? at the beginning of a word,
would be extracted from the word ?optimal?.
? Distributional (a.k.a., contextual): for a given
word, we extract the word immediately preced-
ing and succeeding it as well as words within
a window of 5. These features are extracted
from a corpus without the 30 most common
words filtered. An example of such a feature
is ?LR the+cost?, representing an instance of a
preceding and succeeding word for ?optimal?,
extracted from the phrase ?the optimal cost?.
Lastly, all distributional features that occur less
than 5 times are removed.
? Part-of-Speech (POS): for example, ?pos JJ? is
a POS feature extracted for the word ?optimal?.
? Alignment (a.k.a., bilingual): alignment fea-
tures are extracted from alignment matrices
across languages. For every word, we filter
all words in the target language (treating En-
glish, our working language, as the source)
that have a lexical probability less than half the
maximum lexical probability, and use the re-
sulting aligned words as features. For exam-
ple, ?align ide?al? would be a feature for the
word ?optimal?, since the French word ?ide?al?
is aligned (with high probability) to the word
?optimal?. Note that the assumption during test
time is that alignment features are not available
for OOV words; if they were, then the word
would not be OOV. Nonetheless, alignment in-
formation can be utilized indirectly in the link
prediction stage from random walk traversals
of in-vocabulary nodes.
Statistics on the number of features broken down by
type are presented in Table 2, for 3 different vocab-
ulary sizes. In the experiments, we concentrated on
the 10,000 and 50,000 size vocabularies.
3.3 Baselines
When selecting the baselines, we had two goals in
mind. Firstly, we wanted to compare the proposed
approach against simpler alternatives for generating
word similarities. The baselines were also chosen
so as to correspond in some way to the various fea-
ture types, since a main advantage of our approach
is that it effectively combines various feature types
to yield global word similarity scores. This choice
of baselines also provides insight into the impact of
the various feature types chosen; the idea is that a
baseline corresponding to a particular feature type
would be indicative of word similarity performance
using just that type. Three baselines were initially
selected:
? Distributional: a PMI vector is computed for
each word over the various distributional fea-
tures. The inner product of two PMI vectors
is computed to evaluate the similarity of two
words. We found that this baseline performed
poorly relative to the other ones, and thus de-
cided not to include it in the final evaluation.
? Orthographic: based on a simple edit distance-
based approach, where all words within an edit
distance of 25% of the length of the test word
are retrieved.
? Alignment: we compose the alignment matri-
ces in both directions to generate an English
to English matrix (using German as the pivot
language), from which the three most similar
33
Vocabulary Words Features Alignment Distributional Orthographic POS
Full 93,011 780,357 325,940 206,253 248,114 50
50k-vocab 50,000 569,890 222,701 204,266 142,873 50
10k-vocab 10,000 301,555 61,792 199,256 40,457 50
Table 2: Statistics on the number of features extracted based on the number of words, broken down by feature type. Note that the
distributional features are only those with count 5 and above.
words (as per the lexical probabilities in the
matrices) are extracted.
3.4 Evaluation
Automatic evaluation of an algorithm that computes
similarities between words is tricky. The judgment
on whether two words are synonyms is still done
best by a human, requiring significant manual effort.
Therefore, during the experimentation and parame-
ter selection process we developed an intermediate
form of evaluation wherein a human annotator as-
sisted in creating a pseudo ?ground truth?. Prior to
creating the ground truth, all OOV words in the test
set were identified (i.e., no match in our vocabulary),
resulting in 978 OOV words. Named entities were
then manually filtered, resulting in a final test set of
312 words for evaluation purposes.
To create the ground truth, we generated for each
test OOV word a set of possible synonyms using the
alignment and orthographic baselines, as per Section
3.3. Naturally, many of the words generated were
not legitimate synonyms; human evaluators thus re-
moved all words that were not synonyms or near
synonyms, ignoring mild grammatical inconsisten-
cies, like singular vs. plural. Generally, a synonym
was considered valid if substituting the word with
the synonym preserved meaning in a sentence.
The final evaluation was performed by a human
evaluator. The two baselines and the proposed ap-
proach generated the top three synonym candidates
for a given OOV test word and both 1-best and 3-
best results were evaluated (as in Table 3). Final
performance was evaluated using precision and re-
call. Recall is defined as the percentage of words
for which at least one synonym was generated, and
precision evaluates the number of correct synonyms
from the ones generated.
3.5 Results
Figure 3 looks at the neighborhood of words around
the word ?guardian?. Note that while only two dif-
ferent ? parameter configurations are compared in
Test Word Synonym 1 Synonym 2 Synonym 3
pubescent puberty adolescence nanotubes
sportswoman sportswomen athlete draftswoman
briny salty saline salinity
Table 3: Example of the annotation task. The suggested syn-
onyms are real output from our algorithm.
the figure, we investigated a variety of settings and
found that ?0 = 0.3, ?1 = 0.4, ?2 = 0.2, ?3 = 0.1
worked best from a final evaluation perspective.
The first point to note is that the graph in Fig-
ure 3b is generally more dense than that of Figure
guardian	

custodian	

angel	

guardians	

tutor	

0.13	
 0.22	

0.08	
 0.07	

guard	

guardia	

stickler	

0.17	

0.13	

0.12	

custodians	
 trainers	

tutors	

0.20	
 0.05	

0.43	

michelangelo	
 angelic	

0.09	
 0.04	

teachers	

0.06	

0.33	

0.10	

0.11	

(a) ?0 = 0.3, ?1 = 0.4, ?2 = 0.2, ?3 = 0.1
guardian	

custodian	

angel	

guardians	

tutor	

0.07	
 0.19	

0.06	
 0.09	

steward	

stickler	

0.09	

0.07	

custodians	
 keepers	

tutors	

0.15	
 0.04	

0.24	

michelangelo	
 angelic	

0.03	
 0.04	
 teachers	
0.03	

0.06	

0.19	

0.06	

rod	
 0.03	

0.004	

(b) ?0 = 0.4, ?1 = 0.4, ?2 = 0.1, ?3 = 0.1
Figure 3: A snapshot of a portion of the learned graph for two
different parameter settings. The graph in 3b is more dense.
34
1 2 3 4 50
1
2
3
4
5
6
7
8
9
10 x 105
 
Numb
er of E
lemen
ts
 Iteration
 10k Word Similarity
 
 HHLLHLLHLHHLNHNL
(a) Word similarity matrix sparsity
0 1 2 3 4 50
5
10
15 x 105
 
Numbe
r of Ele
ments
 Iteration
 10k Weights
 
 HHLLHLLHLHHLNHNL
(b) Weights matrix sparsity
Figure 4: Word similarity and weights matrices sparsities for
10,000-word vocabulary.
3a. For example, Figure 3b contains an edge be-
tween ?custodian? and ?custodians?, whereas Figure
3a does not. In the latter graph, there is a higher pref-
erence for smoothness over the graph and thus the
idea is that ?custodian? and ?custodians? are linked
via the smooth transition ?custodian?? ?guardian?
? ?guardians?? ?custodians?, whereas in the for-
mer, there is a higher preference to respect the ini-
tial values, which generates this additional edge. We
also observed weak edges between words like ?cus-
todian? and ?tutor? in Figure 3b but not in Figure
3a. The effect of the parameters on the sparsity of
the graph is definitely apparent, but generally the
learned graphs are of high quality. A further anal-
ysis reveals that for many of the words in the cor-
pus, the highest weighted features are usually align-
ment features; their heavy use allows the algorithm
to produce interesting synonym candidates, and em-
phasizes the importance of bilingual features.
To underscore the point regarding impact of pa-
rameters on graph sparsity, Figures 4 and 5 present
the number of elements in the resulting word sim-
ilarity and weights matrices (graphs) vs. iteration
for vocabulary sizes of 10,000 and 50,000 respec-
Configuration ?0 ?1 ?2 ?3
HHLL 0.4 0.4 0.1 0.1
NHNL 0.3 0.4 0.2 0.1
HLLH 0.4 0.1 0.1 0.4
LHHL 0.1 0.4 0.4 0.1
Table 4: Legend for the charts in Figures 4 and 5. H corre-
sponds to ?high?, L to ?low?, and N to ?neutral?.
1 2 3 4 50
5
10
15 x 105
 
Numb
er of E
lemen
ts
 Iteration
 50k Word Similarity
 
 HHLLNHNL
(a) Word similarity matrix sparsity
0 1 2 3 4 50
0.5
1
1.5
2
2.5 x 106
 
Numb
er of E
lemen
ts
 Iteration
 50k Weights
 
 HHLLNHNL
(b) Weights matrix sparsity
Figure 5: Word similarity and weights matrices sparsities for
50,000-word vocabulary.
tively, with Table 4 providing a legend to the curves
in those figures. Higher ? weights for terms 1 and
2 in the objective function result in less sparse solu-
tions. The density of the matrices also drops drasti-
cally after a few iterations and stabilizes thereafter.
Lastly, Tables 5 and 6 present the final results of
the evaluation, as assessed by a human evaluator, on
the 312 OOV words in the test set. While the re-
sults on the 1-best front are marginally better than
the edit distance-based baseline, 3-best the perfor-
mance of our approach is comfortably better than the
baselines. Testing was done with the word similarity
update method.
The performance of the random walk-based link
35
Method Precision Recall F-1
? matrix 31.1% 67.0% 42.5%
orthographic 37.5% 92.3% 53.3%
50k-nhnl 37.2% 100% 54.2%
Table 5: 1-best evaluation results on WMT 2010 OOV words
trained on a 50,000-word vocabulary. Our best approach (?50k-
nhnl?) is bolded
Method Precision Recall F-1
? matrix 96.7% 67.0% 79.1%
orthographic 89.9% 92.3% 91.1%
50k-nhnl 92.6% 100% 96.2%
Table 6: 3-best evaluation results on WMT 2010 OOV words
trained on a 50,000-word vocabulary. Our best approach (?50k-
nhnl?) is bolded
prediction approach was sub-optimal for several rea-
sons. Firstly, it was difficult to use the learned im-
portance weights as is, since the resulting weights
matrix was so sparse that many test words simply
did not have active features. This issue resulted
in the vanilla variant of the random walk approach
to have very low recall. Therefore, we adopted a
?mixed weights? strategy, where we selectively in-
troduced a number of features previously inactive
for a test word, not including the features that had
high entropy. Yet in this case, the random walks get
stuck traversing certain edges, and a good sampling
of similar words was not properly achievable.
A general issue that arose during link prediction
is that the orthographic features tend to dominate
the candidate synonyms list since alignment features
are not utilized. If instead we assume that align-
ment features are accessible during testing, then the
random walk-based approaches do marginally better
than the word similarity update method, but further
investigation is warranted before drawing any defini-
tive conclusions.
4 Related Work
We used the objective function and basic formula-
tion of (Muthukrishnan et al, 2011), but corrected
their derivation of the optimization and introduced
methods to handle the resulting complications. In
addition, (Muthukrishnan et al, 2011) implemented
their approach on just one feature type and with far
fewer nodes, since their word similarity graph was
actually over documents and their feature similar-
ity graph was over words. Recently, an alterna-
tive graph-based approach for the same problem was
presented in (Minkov and Cohen, 2012). However,
in addition to requiring a dependency parse of the
corpus, the emphasis of that work is more on the
testing side. Indeed, we can incorporate some of the
ideas presented in that work to improve our link pre-
diction during query time. The label propagation-
based approaches of (Tamura et al, 2012; Razmara
et al, 2013), wherein ?seed distributions? are ex-
tracted from bilingual corpora and are propagated
around a similarity graph, can also be easily inte-
grated into our approach as a downstream method
specific to machine translation.
Another approach to handle OOVs, particularly
in the translation domain, is (Zhang et al, 2005),
wherein the authors leveraged the web as an ex-
panded corpus for OOV mining. If web access is un-
available however, then this method would not work.
The general problem of combining multiple views
of similarity (i.e., across different feature types)
can also be tackled through multiple kernel learn-
ing (MKL) (Bach et al, 2004). However, most of
the work in this field has been on supervised MKL,
whereas we required an unsupervised approach.
An area that has seen a recent resurgence in popu-
larity is deep learning, especially in its applications
to continuous embeddings. Embeddings of word
distributions have been explored in (Mnih and Hin-
ton, 2007; Turian et al, 2010; Weston et al, 2008).
Lastly, while not directly relevant to our work, the
idea of using a graph-based framework to combine
both monolingual and bilingual features was also
presented in (Das and Petrov, 2011).
5 Conclusion & Future Work
In this work, we presented a graph-based approach
to computing word similarities, based on dual word
and feature similarity graphs, and the edges that
go between the graphs, representing importance
weights. We introduced an objective function that
promotes parameter concurrence between the two
graphs, and minimized this function with a simple
alternating minimization-based approach. The re-
sulting optimization recovers high quality word sim-
ilarity graphs, primarily due to the bilingual features,
and improves over the baselines during the link pre-
diction stage.
In the future, on the training side we would like
to optimize the proposed objective function in a
better manner, while enforcing the positive semi-
36
definiteness constraints. Other link prediction tech-
niques should be explored, as the current techniques
have pitfalls. Richer features that model more re-
fined aspects can be introduced. In particular, fea-
tures from a dependency parse of the data would be
very useful in this situation.
References
Francis R. Bach, Gert R. G. Lanckriet, and Michael I.
Jordan. 2004. Multiple kernel learning, conic duality,
and the smo algorithm. In Proceedings of the twenty-
first international conference on Machine learning,
ICML ?04.
I. Csisza?r and G. Tusna?dy. 1984. Information geome-
try and alternating minimization procedures. Statistics
and Decisions, Supplement Issue 1:205?237.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ?11,
pages 600?609.
Stanley Kok and Chris Brockett. 2010. Hitting the right
paraphrases in good time. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT ?10, pages 145?153.
Einat Minkov and William W. Cohen. 2012. Graph
based similarity measures for synonym extraction
from parsed text. In TextGraphs-7: Graph-based
Methods for Natural Language Processing.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling. In
Proceedings of the 24th international conference on
Machine learning, ICML ?07, pages 641?648.
Pradeep Muthukrishnan, Dragomir R. Radev, and
Qiaozhu Mei. 2011. Simultaneous similarity learning
and feature-weight learning for document clustering.
In TextGraphs-6: Graph-based Methods for Natural
Language Processing, pages 42?50.
Majid Razmara, Maryam Siahbani, Gholamreza Haffari,
and Anoop Sarkar. 2013. Graph propagation for para-
phrasing out-of-vocabulary words in statistical ma-
chine translation. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics, ACL ?13.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from comparable
corpora using label propagation. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, EMNLP-CoNLL ?12, pages 24?
36, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 384?394.
Jason Weston, Fre?de?ric Ratle, and Ronan Collobert.
2008. Deep learning via semi-supervised embedding.
In ICML, pages 1168?1175.
Ying Zhang, Fei Huang, and Stephan Vogel. 2005. Min-
ing translations of oov terms from the web through
cross-lingual query expansion. In Proceedings of the
28th annual international ACM SIGIR conference on
Research and development in information retrieval,
SIGIR ?05, pages 669?670.
Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal,
Jason Weston, and Bernhard Scho?lkopf. 2004. Learn-
ing with local and global consistency. In Sebastian
Thrun, Lawrence Saul, and Bernhard Scho?lkopf, edi-
tors, Advances in Neural Information Processing Sys-
tems 16. MIT Press, Cambridge, MA.
Xiaojin Zhu, Z. Ghahramani, and John Lafferty. 2003.
Semi-supervised learning using gaussian fields and
harmonic functions. In Proceedings of the Twenti-
eth International Conference on Machine Learning
(ICML-2003), volume 20, page 912.
A Final Equations for Parameter Updates
Wvi,vj =
1
C1
?
?
?
fp,fq?F
?2Zvi,fpZvj ,fqWfp,fq?
?3
2
Wfp,fq(Zvi,fp ? Zvj ,fq)2
)
(5)
Wfp,fq =
1
C2
?
?
?
vi,vj?V
(
?2Zvi,fpZvj ,fqWvi,vj?
?3
2
Wvi,vj (Zvi,fp ? Zvj ,fq)2
)
+ ?0W ?fp,fq
)
(6)
Zvi,fp =
1
C3
?
?
?
vi?V
?
fp?F
(
?3Zvj ,fqWvi,vjWfp,fq
?
?2
2
Zvj ,fq(Wvi,vj ?Wfp,fq)2
)
+ ?1Z?vi,fp
)
(7)
37
where
C1 = ?2
?
fp,fq?F
Zvi,fpZvj ,fq
C2 = ?0 + ?2
?
vi,vj?V
Zvi,fpZvj ,fq
C3 = ?1 + ?3
?
vi?V
?
fp?F
Wvi,vjWfp,fq
38
