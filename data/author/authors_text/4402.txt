Proceedings of the 43rd Annual Meeting of the ACL, pages 531?540,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Clause Restructuring for Statistical Machine Translation
Michael Collins
MIT CSAIL
mcollins@csail.mit.edu
Philipp Koehn
School of Informatics
University of Edinburgh
pkoehn@inf.ed.ac.uk
Ivona Kuc?erova?
MIT Linguistics Department
kucerova@mit.edu
Abstract
We describe a method for incorporating syntactic informa-
tion in statistical machine translation systems. The first step
of the method is to parse the source language string that is be-
ing translated. The second step is to apply a series of trans-
formations to the parse tree, effectively reordering the surface
string on the source language side of the translation system. The
goal of this step is to recover an underlying word order that is
closer to the target language word-order than the original string.
The reordering approach is applied as a pre-processing step in
both the training and decoding phases of a phrase-based statis-
tical MT system. We describe experiments on translation from
German to English, showing an improvement from 25.2% Bleu
score for a baseline system to 26.8% Bleu score for the system
with reordering, a statistically significant improvement.
1 Introduction
Recent research on statistical machine translation
(SMT) has lead to the development of phrase-
based systems (Och et al, 1999; Marcu and Wong,
2002; Koehn et al, 2003). These methods go be-
yond the original IBM machine translation models
(Brown et al, 1993), by allowing multi-word units
(?phrases?) in one language to be translated directly
into phrases in another language. A number of em-
pirical evaluations have suggested that phrase-based
systems currently represent the state?of?the?art in
statistical machine translation.
In spite of their success, a key limitation of
phrase-based systems is that they make little or no
direct use of syntactic information. It appears likely
that syntactic information will be crucial in accu-
rately modeling many phenomena during transla-
tion, for example systematic differences between the
word order of different languages. For this reason
there is currently a great deal of interest in meth-
ods which incorporate syntactic information within
statistical machine translation systems (e.g., see (Al-
shawi, 1996; Wu, 1997; Yamada and Knight, 2001;
Gildea, 2003; Melamed, 2004; Graehl and Knight,
2004; Och et al, 2004; Xia and McCord, 2004)).
In this paper we describe an approach for the use
of syntactic information within phrase-based SMT
systems. The approach constitutes a simple, direct
method for the incorporation of syntactic informa-
tion in a phrase?based system, which we will show
leads to significant improvements in translation ac-
curacy. The first step of the method is to parse the
source language string that is being translated. The
second step is to apply a series of transformations
to the resulting parse tree, effectively reordering the
surface string on the source language side of the
translation system. The goal of this step is to re-
cover an underlying word order that is closer to the
target language word-order than the original string.
Finally, we apply a phrase-based system to the re-
ordered string to give a translation into the target
language.
We describe experiments involving machine
translation from German to English. As an illustra-
tive example of our method, consider the following
German sentence, together with a ?translation? into
English that follows the original word order:
Original sentence: Ich werde Ihnen die entsprechenden An-
merkungen aushaendigen, damit Sie das eventuell bei der
Abstimmung uebernehmen koennen.
English translation: I will to you the corresponding comments
pass on, so that you them perhaps in the vote adopt can.
The German word order in this case is substan-
tially different from the word order that would be
seen in English. As we will show later in this pa-
per, translations of sentences of this type pose dif-
ficulties for phrase-based systems. In our approach
we reorder the constituents in a parse of the German
sentence to give the following word order, which is
much closer to the target English word order (words
which have been ?moved? are underlined):
Reordered sentence: Ich werde aushaendigen Ihnen die
entsprechenden Anmerkungen, damit Sie koennen
uebernehmen das eventuell bei der Abstimmung.
English translation: I will pass on to you the corresponding
comments, so that you can adopt them perhaps in the vote.
531
We applied our approach to translation from Ger-
man to English in the Europarl corpus. Source lan-
guage sentences are reordered in test data, and also
in training data that is used by the underlying phrase-
based system. Results using the method show an
improvement from 25.2% Bleu score to 26.8% Bleu
score (a statistically significant improvement), using
a phrase-based system (Koehn et al, 2003) which
has been shown in the past to be a highly competi-
tive SMT system.
2 Background
2.1 Previous Work
2.1.1 Research on Phrase-Based SMT
The original work on statistical machine transla-
tion was carried out by researchers at IBM (Brown
et al, 1993). More recently, phrase-based models
(Och et al, 1999; Marcu and Wong, 2002; Koehn
et al, 2003) have been proposed as a highly suc-
cessful alternative to the IBM models. Phrase-based
models generalize the original IBM models by al-
lowing multiple words in one language to corre-
spond to multiple words in another language. For
example, we might have a translation entry specify-
ing that I will in English is a likely translation for Ich
werde in German.
In this paper we use the phrase-based system
of (Koehn et al, 2003) as our underlying model.
This approach first uses the original IBM models
to derive word-to-word alignments in the corpus
of example translations. Heuristics are then used
to grow these alignments to encompass phrase-to-
phrase pairs. The end result of the training process is
a lexicon of phrase-to-phrase pairs, with associated
costs or probabilities. In translation with the sys-
tem, a beam search method with left-to-right search
is used to find a high scoring translation for an in-
put sentence. At each stage of the search, one or
more English words are added to the hypothesized
string, and one or more consecutive German words
are ?absorbed? (i.e., marked as having already been
translated?note that each word is absorbed at most
once). Each step of this kind has a number of costs:
for example, the log probability of the phrase-to-
phrase correspondance involved, the log probability
from a language model, and some ?distortion? score
indicating how likely it is for the proposed words in
the English string to be aligned to the corresponding
position in the German string.
2.1.2 Research on Syntax-Based SMT
A number of researchers (Alshawi, 1996; Wu,
1997; Yamada and Knight, 2001; Gildea, 2003;
Melamed, 2004; Graehl and Knight, 2004; Galley
et al, 2004) have proposed models where the trans-
lation process involves syntactic representations of
the source and/or target languages. One class of ap-
proaches make use of ?bitext? grammars which si-
multaneously parse both the source and target lan-
guages. Another class of approaches make use of
syntactic information in the target language alone,
effectively transforming the translation problem into
a parsing problem. Note that these models have radi-
cally different structures and parameterizations from
phrase?based models for SMT. As yet, these sys-
tems have not shown significant gains in accuracy
in comparison to phrase-based systems.
Reranking methods have also been proposed as a
method for using syntactic information (Koehn and
Knight, 2003; Och et al, 2004; Shen et al, 2004). In
these approaches a baseline system is used to gener-
ate
 
-best output. Syntactic features are then used
in a second model that reranks the   -best lists, in
an attempt to improve over the baseline approach.
(Koehn and Knight, 2003) apply a reranking ap-
proach to the sub-task of noun-phrase translation.
(Och et al, 2004; Shen et al, 2004) describe the
use of syntactic features in reranking the output of
a full translation system, but the syntactic features
give very small gains: for example the majority of
the gain in performance in the experiments in (Och
et al, 2004) was due to the addition of IBM Model
1 translation probabilities, a non-syntactic feature.
An alternative use of syntactic information is to
employ an existing statistical parsing model as a lan-
guage model within an SMT system. See (Charniak
et al, 2003) for an approach of this form, which
shows improvements in accuracy over a baseline
system.
2.1.3 Research on Preprocessing Approaches
Our approach involves a preprocessing step,
where sentences in the language being translated are
modified before being passed to an existing phrase-
based translation system. A number of other re-
532
searchers (Berger et al, 1996; Niessen and Ney,
2004; Xia and McCord, 2004) have described previ-
ous work on preprocessing methods. (Berger et al,
1996) describe an approach that targets translation
of French phrases of the form NOUN de NOUN
(e.g., conflit d?inte?re?t). This was a relatively lim-
ited study, concentrating on this one syntactic phe-
nomenon which involves relatively local transfor-
mations (a parser was not required in this study).
(Niessen and Ney, 2004) describe a method that
combines morphologically?split verbs in German,
and also reorders questions in English and German.
Our method goes beyond this approach in several
respects, for example considering phenomena such
as declarative (non-question) clauses, subordinate
clauses, negation, and so on.
(Xia and McCord, 2004) describe an approach for
translation from French to English, where reorder-
ing rules are acquired automatically. The reorder-
ing rules in their approach operate at the level of
context-free rules in the parse tree. Our method
differs from that of (Xia and McCord, 2004) in a
couple of important respects. First, we are consid-
ering German, which arguably has more challeng-
ing word order phenonema than French. German
has relatively free word order, in contrast to both
English and French: for example, there is consid-
erable flexibility in terms of which phrases can ap-
pear in the first position in a clause. Second, Xia
et. al?s (2004) use of reordering rules stated at the
context-free level differs from ours. As one exam-
ple, in our approach we use a single transformation
that moves an infinitival verb to the first position in
a verb phrase. Xia et. al?s approach would require
learning of a different rule transformation for every
production of the form VP => .... In practice the
German parser that we are using creates relatively
?flat? structures at the VP and clause levels, leading
to a huge number of context-free rules (the flatness
is one consequence of the relatively free word order
seen within VP?s and clauses in German). There are
clearly some advantages to learning reordering rules
automatically, as in Xia et. al?s approach. How-
ever, we note that our approach involves a hand-
ful of linguistically?motivated transformations and
achieves comparable improvements (albeit on a dif-
ferent language pair) to Xia et. al?s method, which
in contrast involves over 56,000 transformations.
S PPER-SB Ich
VAFIN-HD werde
VP PPER-DA Ihnen
NP-OA ART die
ADJA entsprechenden
NN Anmerkungen
VVINF-HD aushaendigen
, ,
S KOUS damit
PPER-SB Sie
VP PDS-OA das
ADJD eventuell
PP APPR bei
ART der
NN Abstimmung
VVINF-HD uebernehmen
VMFIN-HD koennen
Figure 1: An example parse tree. Key to non-terminals:
PPER = personal pronoun; VAFIN = finite verb; VVINF = in-
finitival verb; KOUS = complementizer; APPR = preposition;
ART = article; ADJA = adjective; ADJD = adverb; -SB = sub-
ject; -HD = head of a phrase; -DA = dative object; -OA = ac-
cusative object.
2.2 German Clause Structure
In this section we give a brief description of the syn-
tactic structure of German clauses. The character-
istics we describe motivate the reordering rules de-
scribed later in the paper.
Figure 1 gives an example parse tree for a German
sentence. This sentence contains two clauses:
Clause 1: Ich/I werde/will Ihnen/to you die/the
entsprechenden/corresponding
Anmerkungen/comments aushaendigen/pass on
Clause 2: damit/so that Sie/you das/them
eventuell/perhaps bei/in der/the Abstimmung/vote
uebernehmen/adopt koennen/can
These two clauses illustrate a number of syntactic
phenomena in German which lead to quite different
word order from English:
Position of finite verbs. In Clause 1, which is a
matrix clause, the finite verb werde is in the second
position in the clause. Finite verbs appear rigidly in
2nd position in matrix clauses. In contrast, in sub-
ordinate clauses, such as Clause 2, the finite verb
comes last in the clause. For example, note that
koennen is a finite verb which is the final element
of Clause 2.
Position of infinitival verbs. In German, infini-
tival verbs are final within their associated verb
533
phrase. For example, returning to Figure 1, no-
tice that aushaendigen is the last element in its verb
phrase, and that uebernehmen is the final element of
its verb phrase in the figure.
Relatively flexible word ordering. German has
substantially freer word order than English. In par-
ticular, note that while the verb comes second in ma-
trix clauses, essentially any element can be in the
first position. For example, in Clause 1, while the
subject Ich is seen in the first position, potentially
any of the other constituents (e.g., Ihnen) could also
appear in this position. Note that this often leads
to the subject following the finite verb, something
which happens very rarely in English.
There are many other phenomena which lead to
differing word order between German and English.
Two others that we focus on in this paper are nega-
tion (the differing placement of items such as not in
English and nicht in German), and also verb-particle
constructions. We describe our treatment of these
phenomena later in this paper.
2.3 Reordering with Phrase-Based SMT
We have seen in the last section that German syntax
has several characteristics that lead to significantly
different word order from that of English. We now
describe how these characteristics can lead to dif-
ficulties for phrase?based translation systems when
applied to German to English translation.
Typically, reordering models in phrase-based sys-
tems are based solely on movement distance. In par-
ticular, at each point in decoding a ?cost? is associ-
ated with skipping over 1 or more German words.
For example, assume that in translating
Ich werde Ihnen die entsprechenden An-
merkungen aushaendigen.
we have reached a state where ?Ich? and ?werde?
have been translated into ?I will? in English. A
potential decoding decision at this point is to add
the phrase ?pass on? to the English hypothesis, at
the same time absorbing ?aushaendigen? from the
German string. The cost of this decoding step
will involve a number of factors, including a cost
of skipping over a phrase of length 4 (i.e., Ihnen
die entsprechenden Anmerkungen) in the German
string.
The ability to penalise ?skips? of this type, and
the potential to model multi-word phrases, are es-
sentially the main strategies that the phrase-based
system is able to employ when modeling differing
word-order across different languages. In practice,
when training the parameters of an SMT system, for
example using the discriminative methods of (Och,
2003), the cost for skips of this kind is typically set
to a very high value. In experiments with the sys-
tem of (Koehn et al, 2003) we have found that in
practice a large number of complete translations are
completely monotonic (i.e., have   skips), suggest-
ing that the system has difficulty learning exactly
what points in the translation should allow reorder-
ing. In summary, phrase-based systems have rela-
tively limited potential to model word-order differ-
ences between different languages.
The reordering stage described in this paper at-
tempts to modify the source language (e.g., German)
in such a way that its word order is very similar to
that seen in the target language (e.g., English). In
an ideal approach, the resulting translation problem
that is passed on to the phrase-based system will be
solvable using a completely monotonic translation,
without any skips, and without requiring extremely
long phrases to be translated (for example a phrasal
translation corresponding to Ihnen die entsprechen-
den Anmerkungen aushaendigen).
Note than an additional benefit of the reordering
phase is that it may bring together groups of words
in German which have a natural correspondance to
phrases in English, but were unseen or rare in the
original German text. For example, in the previous
example, we might derive a correspondance between
werde aushaendigen and will pass on that was not
possible before reordering. Another example con-
cerns verb-particle constructions, for example in
Wir machen die Tuer auf
machen and auf form a verb-particle construction.
The reordering stage moves auf to precede machen,
allowing a phrasal entry that ?auf machen? is trans-
lated to to open in English. Without the reordering,
the particle can be arbitrarily far from the verb that
it modifies, and there is a danger in this example of
translating machen as to make, the natural transla-
tion when no particle is present.
534
Original sentence: Ich werde Ihnen die entsprechenden
Anmerkungen aushaendigen, damit Sie das eventuell bei
der Abstimmung uebernehmen koennen. (I will to you the
corresponding comments pass on, so that you them perhaps
in the vote adopt can.)
Reordered sentence: Ich werde aushaendigen Ihnen
die entsprechenden Anmerkungen, damit Sie koennen ue-
bernehmen das eventuell bei der Abstimmung.
(I will pass on to you the corresponding comments, so that you
can adopt them perhaps in the vote.)
Figure 2: An example of the reordering process, showing the
original German sentence and the sentence after reordering.
3 Clause Restructuring
We now describe the method we use for reordering
German sentences. As a first step in the reordering
process, we parse the sentence using the parser de-
scribed in (Dubey and Keller, 2003). The second
step is to apply a sequence of rules that reorder the
German sentence depending on the parse tree struc-
ture. See Figure 2 for an example German sentence
before and after the reordering step.
In the reordering phase, each of the following six
restructuring steps were applied to a German parse
tree, in sequence (see table 1 also, for examples of
the reordering steps):
[1] Verb initial In any verb phrase (i.e., phrase
with label VP-...) find the head of the phrase (i.e.,
the child with label -HD) and move it into the ini-
tial position within the verb phrase. For example,
in the parse tree in Figure 1, aushaendigen would be
moved to precede Ihnen in the first verb phrase (VP-
OC), and uebernehmen would be moved to precede
das in the second VP-OC. The subordinate clause
would have the following structure after this trans-
formation:
S-MO KOUS-CP damit
PPER-SB Sie
VP-OC VVINF-HD uebernehmen
PDS-OA das
ADJD-MO eventuell
PP-MO APPR-DA bei
ART-DA der
NN-NK Abstimmung
VMFIN-HD koennen
[2] Verb 2nd In any subordinate clause labelled
S-..., with a complementizer KOUS, PREL, PWS
or PWAV, find the head of the clause, and move it to
directly follow the complementizer.
For example, in the subordinate clause in Fig-
ure 1, the head of the clause koennen would be
moved to follow the complementizer damit, giving
the following structure:
S-MO KOUS-CP damit
VMFIN-HD koennen
PPER-SB Sie
VP-OC VVINF-HD uebernehmen
PDS-OA das
ADJD-MO eventuell
PP-MO APPR-DA bei
ART-DA der
NN-NK Abstimmung
[3] Move Subject For any clause (i.e., phrase with
label S...), move the subject to directly precede
the head. We define the subject to be the left-most
child of the clause with label ...-SB or PPER-
EP, and the head to be the leftmost child with label
...-HD.
For example, in the subordinate clause in Fig-
ure 1, the subject Sie would be moved to precede
koennen, giving the following structure:
S-MO KOUS-CP damit
PPER-SB Sie
VMFIN-HD koennen
VP-OC VVINF-HD uebernehmen
PDS-OA das
ADJD-MO eventuell
PP-MO APPR-DA bei
ART-DA der
NN-NK Abstimmung
[4] Particles In verb particle constructions, move
the particle to immediately precede the verb. More
specifically, if a finite verb (i.e., verb tagged as
VVFIN) and a particle (i.e., word tagged as PTKVZ)
are found in the same clause, move the particle to
precede the verb.
As one example, the following clause contains
both a verb (forden) as well as a particle (auf):
S PPER-SB Wir
VVFIN-HD fordern
NP-OA ART das
NN Praesidium
PTKVZ-SVP auf
After the transformation, the clause is altered to:
S PPER-SB Wir
PTKVZ-SVP auf
VVFIN-HD fordern
NP-OA ART das
NN Praesidium
535
Transformation Example
Verb Initial
Before: Ich werde Ihnen die entsprechenden Anmerkungen aushaendigen,    
After: Ich werde aushaendigen Ihnen die entsprechenden Anmerkungen,    
English: I shall be passing on to you some comments,    
Verb 2nd
Before:     damit Sie uebernehmen das eventuell bei der Abstimmung koennen.
After:     damit koennen Sie uebernehmen das eventuell bei der Abstimmung .
English:     so that could you adopt this perhaps in the voting.
Move Subject
Before:     damit koennen Sie uebernehmen das eventuell bei der Abstimmung.
After:     damit Sie koennen uebernehmen das eventuell bei der Abstimmung .
English:     so that you could adopt this perhaps in the voting.
Particles
Before: Wir fordern das Praesidium auf,    
After: Wir auf fordern das Praesidium,    
English: We ask the Bureau,    
Infinitives
Before: Ich werde der Sache nachgehen dann,    
After: Ich werde nachgehen der Sache dann,    
English: I will look into the matter then,    
Negation
Before: Wir konnten einreichen es nicht mehr rechtzeitig,    
After: Wir konnten nicht einreichen es mehr rechtzeitig,    
English: We could not hand it in in time,    
Table 1: Examples for each of the reordering steps. In each case the item that is moved is underlined.
[5] Infinitives In some cases, infinitival verbs are
still not in the correct position after transformations
[1]?[4]. For this reason we add a second step that
involves infinitives. First, we remove all internal VP
nodes within the parse tree. Second, for any clause
(i.e., phrase labeled S...), if the clause dominates
both a finite and infinitival verb, and there is an argu-
ment (i.e., a subject, or an object) between the two
verbs, then the infinitive is moved to directly follow
the finite verb.
As an example, the following clause contains an
infinitival (einreichen) that is separated from a finite
verb konnten by the direct object es:
S PPER-SB Wir
VMFIN-HD konnten
PPER-OA es
PTKNEG-NG nicht
VP-OC VVINF-HD einreichen
AP-MO ADV-MO mehr
ADJD-HD rechtzeitig
The transformation removes the VP-OC, and
moves the infinitive, giving:
S PPER-SB Wir
VMFIN-HD konnten
VVINF-HD einreichen
PPER-OA es
PTKNEG-NG nicht
AP-MO ADV-MO mehr
ADJD-HD rechtzeitig
[6] Negation As a final step, we move negative
particles. If a clause dominates both a finite and in-
finitival verb, as well as a negative particle (i.e., a
word tagged as PTKNEG), then the negative particle
is moved to directly follow the finite verb.
As an example, the previous example now has the
negative particle nicht moved, to give the following
clause structure:
S PPER-SB Wir
VMFIN-HD konnten
PTKNEG-NG nicht
VVINF-HD einreichen
PPER-OA es
AP-MO ADV-MO mehr
ADJD-HD rechtzeitig
4 Experiments
This section describes experiments with the reorder-
ing approach. Our baseline is the phrase-based
MT system of (Koehn et al, 2003). We trained
this system on the Europarl corpus, which consists
of 751,088 sentence pairs with 15,256,792 German
words and 16,052,269 English words. Translation
performance is measured on a 2000 sentence test set
from a different part of the Europarl corpus, with av-
erage sentence length of 28 words.
We use BLEU scores (Papineni et al, 2002) to
measure translation accuracy. We applied our re-
536
Annotator 2
Annotator 1 R B E
R 33 2 5
B 2 13 5
E 9 4 27
Table 2: Table showing the level of agreement between two
annotators on 100 translation judgements. R gives counts cor-
responding to translations where an annotator preferred the re-
ordered system; B signifies that the annotator preferred the
baseline system; E means an annotator judged the two systems
to give equal quality translations.
ordering method to both the training and test data,
and retrained the system on the reordered training
data. The BLEU score for the new system was
26.8%, an improvement from 25.2% BLEU for the
baseline system.
4.1 Human Translation Judgements
We also used human judgements of translation qual-
ity to evaluate the effectiveness of the reordering
rules. We randomly selected 100 sentences from the
test corpus where the English reference translation
was between 10 and 20 words in length.1 For each
of these 100 translations, we presented the two anno-
tators with three translations: the reference (human)
translation, the output from the baseline system, and
the output from the system with reordering. No in-
dication was given as to which system was the base-
line system, and the ordering in which the baseline
and reordered translations were presented was cho-
sen at random on each example, to prevent ordering
effects in the annotators? judgements. For each ex-
ample, we asked each of the annotators to make one
of two choices: 1) an indication that one translation
was an improvement over the other; or 2) an indica-
tion that the translations were of equal quality.
Annotator 1 judged 40 translations to be improved
by the reordered model; 40 translations to be of
equal quality; and 20 translations to be worse under
the reordered model. Annotator 2 judged 44 trans-
lations to be improved by the reordered model; 37
translations to be of equal quality; and 19 transla-
tions to be worse under the reordered model. Ta-
ble 2 gives figures indicating agreement rates be-
tween the annotators. Note that if we only consider
preferences where both annotators were in agree-
1We chose these shorter sentences for human evaluation be-
cause in general they include a single clause, which makes hu-
man judgements relatively straightforward.
ment (and consider all disagreements to fall into the
?equal? category), then 33 translations improved un-
der the reordering system, and 13 translations be-
came worse. Figure 3 shows a random selection
of the translations where annotator 1 judged the re-
ordered model to give an improvement; Figure 4
shows examples where the baseline system was pre-
ferred by annotator 1. We include these examples to
give a qualitative impression of the differences be-
tween the baseline and reordered system. Our (no
doubt subjective) impression is that the cases in fig-
ure 3 are more clear cut instances of translation im-
provements, but we leave the reader to make his/her
own judgement on this point.
4.2 Statistical Significance
We now describe statistical significance tests for our
results. We believe that applying significance tests
to Bleu scores is a subtle issue, for this reason we go
into some detail in this section.
We used the sign test (e.g., see page 166 of
(Lehmann, 1986)) to test the statistical significance
of our results. For a source sentence
 
, the sign test
requires a function 
 
that is defined as follows:

	















 If reordered system produces a better
translation for
	
than the baseline
 If baseline produces a better translation
for
	
than the reordered system.

If the two systems produce equal
quality translations on
	
We assume that sentences
 
are drawn from
some underlying distribution    , and that the test
set consists of independently, identically distributed
(IID) sentences from this distribution. We can define
the following probabilities:
 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 232?241,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Discriminative Model for Tree-to-Tree Translation
Brooke Cowan
MIT CSAIL
brooke@csail.mit.edu
Ivona Kuc?erova?
MIT Linguistics Department
kucerova@mit.edu
Michael Collins
MIT CSAIL
mcollins@csail.mit.edu
Abstract
This paper proposes a statistical, tree-
to-tree model for producing translations.
Two main contributions are as follows:
(1) a method for the extraction of syn-
tactic structures with alignment informa-
tion from a parallel corpus of translations,
and (2) use of a discriminative, feature-
based model for prediction of these target-
language syntactic structures?which we
call aligned extended projections, or
AEPs. An evaluation of the method on
translation from German to English shows
similar performance to the phrase-based
model of Koehn et al (2003).
1 Introduction
Phrase-based approaches (Och and Ney, 2004)
to statistical machine translation (SMT) have re-
cently achieved impressive results, leading to sig-
nificant improvements in accuracy over the origi-
nal IBM models (Brown et al, 1993). However,
phrase-based models lack a direct representation
of syntactic information in the source or target lan-
guages; this has prompted several researchers to
consider various approaches that make use of syn-
tactic information.
This paper describes a framework for tree-to-
tree based statistical translation. Our goal is to
learn a model that maps parse trees in the source
language to parse trees in the target language.
The model is learned from a corpus of transla-
tion pairs, where each sentence in the source or
target language has an associated parse tree. We
see two major benefits of tree-to-tree based trans-
lation. First, it is possible to explicitly model the
syntax of the target language, thereby improving
grammaticality. Second, we can build a detailed
model of the correspondence between the source
and target parse trees, with the aim of constructing
translations that preserve the meaning of source
language sentences.
Our translation framework involves a process
where the target-language parse tree is broken
down into a sequence of clauses, and each clause
is then translated separately. A central concept we
introduce in the translation of clauses is that of an
aligned extended projection (AEP). AEPs are de-
rived from the concept of an extended projection
in lexicalized tree adjoining grammars (LTAG)
(Frank, 2002), with the addition of alignment in-
formation that is based on work in synchronous
LTAG (Shieber and Schabes, 1990). A key con-
tribution of this paper is a method for learning
to map German clauses to AEPs using a feature-
based model with a perceptron learning algorithm.
We performed experiments on translation from
German to English on the Europarl data set. Eval-
uation in terms of both BLEU scores and human
judgments shows that our system performs sim-
ilarly to the phrase-based model of Koehn et al
(2003).
1.1 A Sketch of the Approach
This section provides an overview of the transla-
tion process. We will use the German sentence wir
wissen da? das haupthemmnis der vorhersehbare
widerstand der hersteller war as a running exam-
ple. For this example we take the desired transla-
tion to be we know that the main obstacle has been
the predictable resistance of manufacturers.
Translation of a German sentence proceeds in
the following four steps:
Step 1: The German sentence is parsed and then
broken down into separate parse structures for a
sequence of clauses. For example, the German ex-
ample above is broken into a parse structure for
the clause wir wissen followed by a parse struc-
ture for the subordinate clause da?. . .war. Each
of these clauses is then translated separately, using
steps 2?3 below.
Step 2: An aligned extended projection (AEP)
is predicted for each German clause. To illustrate
this step, consider translation of the second Ger-
man clause, which has the following parse struc-
ture:
232
s-oc kous-cp da?
np-sb 1 art das
nn haupthemmnis
np-pd 2 art der
adja vorhersehbare
nn widerstand
np-ag art der
nn hersteller
vafin-hd war
Note that we use the symbols 1 and 2 to identify
the two modifiers (arguments or adjuncts) in the
clause, in this case a subject and an object.
A major part of the AEP is a parse-tree frag-
ment, that is similar to a TAG elementary tree (see
also Figure 2):
SBAR
that S
NP VP
V
has
VP
V
been
NP
Following the work of Frank (2002), we will refer
to a structure like this as an extended projection
(EP). The EP encapsulates the core syntactic struc-
ture in the English clause. It contains the main
verb been, as well as the function words that and
has. It also contains a parse tree ?spine? which has
the main verb been as one of its leaves, and has the
clause label SBAR as its root. In addition, it spec-
ifies positions for arguments in the clause?in this
case NPs corresponding to the subject and object.
An AEP contains an EP, as well as alignment
information about where the German modifiers
should be placed in the extended projection. For
example, the AEP in this case would contain the
tree fragment shown above, together with an align-
ment specifying that the modifiers 1 and 2 from
the German parse will appear in the EP as subject
and object, respectively.
Step 3: The German modifiers are translated
and placed in the appropriate positions within the
AEP. For example, the modifiers das haupthemm-
nis and der vorhersehbare widerstand der her-
steller would be translated as the main obstacle,
and the predictable resistance of manufacturers,
respectively, and then placed into the subject and
object positions in the AEP.
Step 4: The individual clause translations are
combined to give a final translation. For example,
the translations we know and that the main obsta-
cle has been . . . would be concatenated to give we
know that the main obstacle has been . . ..
The main focus of this paper will be Step 2: the
prediction of AEPs from German clauses. AEPs
are detailed structural objects, and their relation-
ship to the source-language clause can be quite
complex. We use a discriminative feature-based
model, trained with the perceptron algorithm, to
incrementally predict the AEP in a sequence of
steps. At each step we define features that allow
the model to capture a wide variety of dependen-
cies within the AEP itself, or between the AEP and
the source-language clause.
1.2 Motivation for the Approach
Our approach to tree-to-tree translation is mo-
tivated by several observations. Breaking the
source-language tree into clauses (Step 1) consid-
erably simplifies the difficult problem of defining
an alignment between source and target trees. Our
impression is that high-quality translations can be
produced in a clause-by-clause fashion.1 The use
of a feature-based model for AEP prediction (Step
2) allows us to capture complex syntactic corre-
spondences between English and German, as well
as grammaticality constraints on the English side.
In this paper, we implement the translation of
modifiers (Step 3) with the phrase-based system
of Koehn et al (2003). The modifiers in our data
set are generally small chunks of text such as NPs,
PPs, and ADJPs, which by definition do not in-
clude clauses or verbs. In our approach, we use
the phrase-based system to generate n-best lists of
candidate translations and then rerank the trans-
lations based on grammaticality, i.e., using crite-
ria that judge how well they fit the position in the
AEP. In future work, we might use finite state ma-
chines in place of a reranking approach, or recur-
sively apply the AEP approach to the modifiers.
Stitching translated clauses back together (Step
4) is a relatively simple task: in a substantial ma-
jority of cases, the German clauses are not embed-
ded, but instead form a linear sequence that ac-
counts for the entire sentence. In these cases we
can simply concatenate the English clause trans-
lations to form the full translation. Embedded
clauses in German are slightly more complicated,
but it is not difficult to form embedded structures
in the English translations.
Section 5.2 of this paper describes the features
1Note that we do not assume that all of the translations
in the training data have been produced in a clause-by-clause
fashion. Rather, we assume that good translations for test
examples can be produced in this way.
233
we use for AEP prediction in translation from
German to English. Many of the features of the
AEP prediction model are specifically tuned to the
choice of German and English as the source and
target languages. However, it should be easy to
develop new feature sets to deal with other lan-
guages or treebanking styles. We see this as one
of the strengths of the feature-based approach.
In the work presented in this paper, we focus on
the prediction of clausal AEPs, i.e., AEPs associ-
ated with main verbs. One reason for this is that
clause structures are particularly rich and com-
plex from a syntactic perspective. This means that
there should be considerable potential in improv-
ing translation quality if we can accurately predict
these structures. It also means that clause-level
AEPs are a good test-bed for the discriminative
approach to AEP prediction; future work may con-
sider applying these methods to other structures
such as NPs, PPs, ADJPs, and so on.
2 Related Work
There has been a substantial amount of previous
work on approaches that make use of syntactic in-
formation in statistical machine translation. Wu
(1997) and Alshawi (1996) describe early work on
formalisms that make use of transductive gram-
mars; Graehl and Knight (2004) describe meth-
ods for training tree transducers. Melamed (2004)
establishes a theoretical framework for general-
ized synchronous parsing and translation. Eisner
(2003) discusses methods for learning synchro-
nized elementary tree pairs from a parallel corpus
of parsed sentences. Chiang (2005) has recently
shown significant improvements in translation ac-
curacy, using synchronous grammars. Riezler and
Maxwell (2006) describe a method for learning
a probabilistic model that maps LFG parse struc-
tures in German into LFG parse structures in En-
glish.
Yamada and Knight (2001) and Galley et al
(2004) describe methods that make use of syn-
tactic information in the target language alone;
Quirk et al (2005) describe similar methods that
make use of dependency representations. Syntac-
tic parsers in the target language have been used
as language models in translation, giving some
improvement in accuracy (Charniak et al, 2001).
The work of Gildea (2003) involves methods that
make use of syntactic information in both the
source and target languages.
Other work has attempted to incorporate syntac-
S
NP-A VP
V
know
SBAR-A
SBAR-A
IN
that
S
NP-A VP
V
has
VP
V
been
NP-A
NP
D
the
N
obstacle
Figure 1: Extended projections for the verbs know and been,
and for the noun obstacle. The EPs were taken from the parse
tree for the sentence We know that the main obstacle has been
the predictable resistance of manufacturers.
tic information through reranking approaches ap-
plied to n-best output from phrase-based systems
(Och et al, 2004). Another class of approaches
has shown improvements in translation through re-
ordering, where source language strings are parsed
and then reordered, in an attempt to recover a word
order that is closer to the target language (Collins
et al, 2005; Xia and McCord, 2004).
Our approach is closely related to previous
work on synchronous tree adjoining grammars
(Shieber and Schabes, 1990; Shieber, 2004), and
the work on TAG approaches to syntax described
by Frank (2002). A major departure from previous
work on synchronous TAGs is in our use of a dis-
criminative model that incrementally predicts the
information in the AEP. Note also that our model
may include features that take into account any
part of the German clause.
3 A Translation Architecture Based on
Aligned Extended Projections
3.1 Background: Extended Projections (EPs)
Extended projections (EPs) play a crucial role in
the lexicalized tree adjoining grammar (LTAG)
(Joshi, 1985) approach to syntax described by
Frank (2002). In this paper we focus almost ex-
clusively on extended projections associated with
main verbs; note, however, that EPs are typically
associated with all content words (nouns, adjec-
tives, etc.). As an example, a parse tree for the
sentence we know that the main obstacle has been
the predictable resistance of manufacturers would
make use of EPs for the words we, know, main, ob-
stacle, been, predictable, resistance, and manufac-
turers. Function words (in this sentence that, the,
has, and of) do not have EPs; instead, as we de-
scribe shortly, each function word is incorporated
in an EP of some content word.
Figure 1 has examples of EPs. Each one is
an LTAG elementary tree which contains a sin-
234
gle content word as one of its leaves. Substitution
nodes (such as NP-A or SBAR-A) in the elemen-
tary trees specify the positions of arguments of the
content words. Each EP may contain one or more
function words that are associated with the con-
tent word. For verbs, these function words include
items such as modal verbs and auxiliaries (e.g.,
should and has); complementizers (e.g., that);
and wh-words (e.g., which). For nouns, function
words include determiners and prepositions.
Elementary trees corresponding to EPs form the
basic units in the LTAG approach described by
Frank (2002). They are combined to form a full
parse tree for a sentence using the TAG operations
of substitution and adjunction. For example, the
EP for been in Figure 1 can be substituted into the
SBAR-A position in the EP for know; the EP for
obstacle can be substituted into the subject posi-
tion of the EP for been.
3.2 Aligned Extended Projections (AEPs)
We now build on the idea of extended projections
to give a detailed description of AEPs. Figure 2
shows examples of German clauses paired with the
AEPs found in training data.2 The German clause
is assumed to have n (where n ? 0) modifiers. For
example, the first German parse in Figure 2 has
two arguments, indexed as 1 and 2. Each of these
modifiers must either have a translation in the cor-
responding English clause, or must be deleted.
An AEP consists of the following parts:
STEM: A string specifying the stemmed form
of the main verb in the clause.
SPINE: A syntactic structure associated with
the main verb. The structure has the symbol V
as one of its leaf nodes; this is the position of
the main verb. It includes higher projections of
the verb such as VPs, Ss, and SBARs. It also in-
cludes leaf nodes NP-A in positions correspond-
ing to noun-phrase arguments (e.g., the subject
or object) of the main verb. In addition, it may
contain leaf nodes labeled with categories such
as WHNP or WHADVP where a wh-phrase may be
placed. It may include leaf nodes corresponding
to one or more complementizers (common exam-
ples being that, if, so that, and so on).
VOICE: One of two alternatives, active or
passive, specifying the voice of the main verb.
2Note that in this paper we consider translation from Ger-
man to English; in the remainder of the paper we take English
to be synonymous with the target language in translation and
German to be synonymous with the source language.
SUBJECT: This variable can be one of three
types. If there is no subject position in the SPINE
variable, then the value for SUBJECT is NULL.
Otherwise, SUBJECT can either be a string, for
example there,3 or an index of one of the n modi-
fiers in the German clause.
OBJECT: This variable is similar to SUBJECT,
and can also take three types: NULL, a specific
string, or an index of one of the n German modi-
fiers. It is always NULL if there is no object posi-
tion in the SPINE; it can never be a modifier index
that has already been assigned to SUBJECT.
WH: This variable is always NULL if there is no
wh-phrase position within the SPINE; it is always
a non-empty string (such as which, or in which) if
a wh-phrase position does exist.
MODALS: This is a string of verbs that consti-
tute the modals that appear within the clause. We
use NULL to signify an absence of modals.
INFL: The inflected form of the verb.
MOD(i): There are n modifier variables
MOD(1), MOD(2), . . ., MOD(n) that spec-
ify the positions for German arguments that have
not already been assigned to the SUBJECT or
OBJECT positions in the spine. Each variable
MOD(i) can take one of five possible values:
? null: This value is chosen if and only if
the modifier has already been assigned to the
subject or object position.
? deleted: This means that a translation of
the i?th German modifier is not present in the
English clause.
? pre-sub: The modifier appears after any
complementizers or wh-phrases, but before
the subject of the English clause.
? post-sub: The modifier appears after the
subject of the English clause, but before the
modals.
? in-modals: The modifier appears after the
first modal in the sequence of modals, but be-
fore the second modal or the main verb.
? post-verb: The modifier appears some-
where after the main verb.
3This happens in the case where there exists a subject in
the English clause which is not aligned to a modifier in the
German clause. See, for instance, the second example in Fig-
ure 2.
235
German Clause English AEP
s-oc kous-cp da?
np-sb 1 art das
nn haupthemmnis
np-pd 2 art der
adja vorhersehbare
nn widerstand
np-ag art der
nn hersteller
vafin-hd war
Paraphrase: that [np-sb the
main obstacle] [np-pd the
predictable resistance of man-
ufacturers] was
STEM: be
SPINE:
SBAR-A IN that
S NP-A
VP V
NP-A
VOICE: active
SUBJECT: 1
OBJECT: 2
WH: NULL
MODALS: has
INFL: been
MOD1: null
MOD2: null
s pp-mo 1 appr zwischen
piat beiden
nn gesetzen
vvfin-hd bestehen
adv-mo 2 also
np-sb 3 adja erhebliche
adja rechtliche
$, ,
adja praktische
kon und
adja wirtschaftliche
nn unterschiede
Paraphrase: [pp-mo between
the two pieces of legislation]
exist so [np-sb significant
legal, practical and economic
differences]
STEM: be
SPINE:
S NP-A
VP V
NP-A
VOICE: active
SUBJECT: ?there?
OBJECT: 3
WH: NULL
MODALS: NULL
INFL: are
MOD1: post-verb
MOD2: pre-sub
MOD3: null
s-rc prels-sb die
vp pp-mo 1 appr an
pdat jenem
nn tag
pp-mo 2 appr in
ne tschernobyl
vvpp-hd gezu?ndet
vafin-hd wurde
Paraphrase: which [pp-mo on
that day] [pp-mo in cher-
nobyl] released were
STEM: release
SPINE:
SBAR WHNP
SG-A VP V
VOICE: passive
SUBJECT: NULL
OBJECT: NULL
WH: which
MODALS: was
INFL: released
MOD1: post-verb
MOD2: post-verb
Figure 2: Three examples of German parse trees, together
with their aligned extended projections (AEPs) in the train-
ing data. Note that in the second example the correspondence
between the German clause and its English translation is not
entirely direct. The subject in the English is the expletive
there; the subject in the German clause becomes the object
in English. This is a typical pattern for the German verb
bestehen. The German PP zwischen ... appears at the start
of the clause in German, but is post-verbal in the English.
The modifier also?whose English translation is so?is in an
intermediate position in the German clause, but appears in the
pre-subject position in the English clause.
4 Extracting AEPs from a Corpus
A crucial step in our approach is the extraction
of training examples from a translation corpus.
Each training example consists of a German clause
paired with an English AEP (see Figure 2).
In our experiments, we used the Europarl cor-
pus (Koehn, 2005). For each sentence pair from
this data, we used a version of the German parser
described by Dubey (2005) to parse the German
component, and a version of the English parser
described by Collins (1999) to parse the English
component. To extract AEPs, we perform the fol-
lowing steps:
NP and PP Alignment To align NPs and PPs,
first all German and English nouns, personal
and possessive pronouns, numbers, and adjectives
are identified in each sentence and aligned using
GIZA++ (Och and Ney, 2003). Next, each NP in
an English tree is aligned to an NP or PP in the
corresponding German tree in a way that is consis-
tent with the word-alignment information. That is,
the words dominated by the English node must be
aligned only to words dominated by the German
node, and vice versa. Note that if there is more
than one German node that is consistent, then the
one rooted at the minimal subtree is selected.
Clause alignment, and AEP Extraction The
next step in the training process is to identify
German/English clause pairs which are transla-
tions of each other. We first break each English
or German parse tree into a set of clauses; see
Appendix A for a description of how we iden-
tify clauses. We retain only those training ex-
amples where the English and German sentences
have the same number of clauses. For these re-
tained examples, define the English sentence to
contain the clause sequence ?e1, e2, . . . , en?, and
the German sentence to contain the clause se-
quence ?g1, g2, . . . , gn?. The clauses are ordered
according to the position of their main verbs in
the original sentence. We create n candidate pairs
?(e1, g1), (e2, g2), . . . , (en, gn)? (i.e., force a one-
to-one correspondence between the two clause se-
quences). We then discard any clause pairs (e, g)
which are inconsistent with the NP/PP alignments
for that sentence.4
4A clause pair is inconsistent with the NP/PP alignments
if it contains an NP/PP on either the German or English side
which is aligned to another NP/PP which is not within the
clause pair.
236
Note that this method is deliberately conserva-
tive (i.e., high precision, but lower recall), in that it
discards sentence pairs where the English/German
sentences have different numbers of clauses. In
practice, we have found that the method yields a
large number of training examples, and that these
training examples are of relatively high quality.
Future work may consider improved methods for
identifying clause pairs, for example methods that
make use of labeled training examples.
An AEP can then be extracted from each
clause pair. The EP for the English clause is
first extracted, giving values for all variables ex-
cept for SUBJECT, OBJECT, and MOD(1), . . . ,
MOD(n). The values for the SUBJECT, OBJECT,
and MOD(i) variables are derived from the align-
ments between NPs/PPs, and an alignment of
other clauses (ADVPs, ADJPs, etc.) derived from
GIZA++ alignments. If the English clause has a
subject or object which is not aligned to a German
modifier, then the value for SUBJECT or OBJECT
is taken to be the full English string.
5 The Model
5.1 Beam search and the perceptron
In this section we describe linear history-based
models with beam search, and the perceptron al-
gorithm for learning in these models. These meth-
ods will form the basis for our model that maps
German clauses to AEPs.
We have a training set of n examples, (xi, yi)
for i = 1 . . . n, where each xi is a German parse
tree, and each yi is an AEP. We follow previous
work on history-based models, by representing
each yi as a series of N decisions ?d1, d2, . . . dN ?.
In our approach, N will be a fixed number for any
input x: we take the N decisions to correspond to
the sequence of variables STEM, SPINE, . . .,
MOD(1), MOD(2), . . ., MOD(n) described
in section 3. Each di is a member of a set Di
which specifies the set of allowable decisions at
the i?th point (for example, D2 would be the set
of all possible values for SPINE). We assume a
function ADVANCE(x, ?d1, d2, . . . , di?1?) which
maps an input x together with a prefix of decisions
d1 . . . di?1 to a subset ofDi. ADVANCE is a func-
tion that specifies which decisions are allowable
for a past history ?d1, . . . , di?1? and an input x. In
our case the ADVANCE function implements hard
constraints on AEPs (for example, the constraint
that the SUBJECT variable must be NULL if no
subject position exists in the SPINE). For any in-
put x, a well-formed decision sequence for x is a
sequence ?d1, . . . , dN ? such that for i = 1 . . . n,
di ? ADVANCE(x, ?d1, . . . , di?1?). We define
GEN(x) to be the set of all decision sequences (or
AEPs) which are well-formed for x.
The model that we will use is a
discriminatively-trained, feature-based model. A
significant advantage to feature-based mod-
els is their flexibility: it is very easy to
sensitize the model to dependencies in the
data by encoding new features. To define a
feature-based model, we assume a function
??(x, ?d1, . . . , di?1?, di) ? Rd which maps a deci-
sion di in context (x, ?d1, . . . , di?1?) to a feature
vector. We also assume a vector ?? ? Rd of param-
eter values. We define the score for any partial or
complete decision sequence y = ?d1, d2, . . . , dm?
paired with x as:
SCORE(x, y) = ?(x, y) ? ?? (1)
where ?(x, y) = ?mi=1 ??(x, ?d1, . . . , di?1?, di).
In particular, given the definitions above, the out-
put structure F (x) for an input x is the highest?
scoring well?formed structure for x:
F (x) = arg max
y?GEN(x)
SCORE(x, y) (2)
To decode with the model we use a beam-search
method. The method incrementally builds an AEP
in the decision order d1, d2, . . . , dN . At each
point, a beam contains the top M highest?scoring
partial paths for the first m decisions, where M
is taken to be a fixed number. The score for any
partial path is defined in Eq. 1. The ADVANCE
function is used to specify the set of possible deci-
sions that can extend any given path in the beam.
To train the model, we use the averaged per-
ceptron algorithm described by Collins (2002).
This combination of the perceptron algorithm with
beam-search is similar to that described by Collins
and Roark (2004).5 The perceptron algorithm is a
convenient choice because it converges quickly ?
usually taking only a few iterations over the train-
ing set (Collins, 2002; Collins and Roark, 2004).
5.2 The Features of the Model
The model?s features allow it to capture depen-
dencies between the AEP and the German clause,
as well as dependencies between different parts
of the AEP itself. The features included in ??
5Future work may consider alternative algorithms, such
as those described by Daume? and Marcu (2005).
237
1 main verb
2 any verb in the clause
3 all verbs, in sequence
4 spine
5 tree
6 preterminal label of left-most child of subject
7 terminal label of left-most child of subject
8 suffix of terminal label of right-most child of subject
9 preterminal label of left-most child of object
10 terminal label of left-most child of object
11 suffix of terminal label of right-most child of object
12 preterminal label of the negation word nicht (not)
13 is either of the strings es gibt (there is/are)
or es gab (there was/were) present?
14 complementizers and wh-words
15 labels of all wh-nonterminals
16 terminal labels of all wh-words
17 preterminal label of a verb in first position
18 terminal label of a verb in first position
19 terminal labels of all words in any relative pronoun
under a PP
20 are all of the verbs at the end?
21 nonterminal label of the root of the tree
22 terminal labels of all words constituting the subject
23 terminal labels of all words constituting the object
24 the leaves dominated by each node in the tree
25 each node in the context of a CFG rule
26 each node in the context of the RHS of a CFG rule
27 each node with its left and right sibling
28 the number of leaves dominated by each node
in the tree
Table 1: Functions of the German clause used for making
features in the AEP prediction model.
can consist of any function of the decision history
?d1, . . . , di?1?, the current decision di, or the Ger-
man clause. In defining features over AEP/clause
pairs, we make use of some basic functions which
look at the German clause and the AEP (see Ta-
bles 1 and 2). We use various combinations of
these basic functions in the prediction of each de-
cision di, as described below.
STEM: Features for the prediction of STEM
conjoin the value of this variable with each of the
functions in lines 1?13 of Table 1. For example,
one feature is the value of STEM conjoined with
the main verb of the German clause. In addition,
?? includes features sensitive to the rank of a can-
didate stem in an externally-compiled lexicon.6
SPINE: Spine prediction features make use of
the values of the variables SPINE and STEM from
the AEP, as well as functions of the spine in lines
1?7 of Table 2, conjoined in various ways with
the functions in lines 4, 12, and 14?21 of Table 1.
Note that the functions in Table 2 allow us to look
6The lexicon is derived from GIZA++ and provides, for a
large number of German main verbs, a ranked list of possible
English translations.
1 does the SPINE have a subject?
2 does the SPINE have an object?
3 does the SPINE have any wh-words?
4 the labels of any complementizer nonterminals
in the SPINE
5 the labels of any wh-nonterminals in the SPINE
6 the nonterminal labels SQ or SBARQ in the SPINE
7 the nonterminal label of the root of the SPINE
8 the grammatical category of the finite verbal form
INFL (i.e., infinitive, 1st-, 2nd-, or 3rd-person pres,
pres participle, sing past, plur past, past participle)
Table 2: Functions of the English AEP used for making fea-
tures in the AEP prediction model.
at substructure in the spine. For instance, one of
the features for SPINE is the label SBARQ or SQ,
if it exists in the candidate spine, conjoined with
a verbal preterminal label if there is a verb in the
first position of the German clause. This feature
captures the fact that German yes/no questions be-
gin with a verb in the first position.
VOICE: Voice features in general combine val-
ues of VOICE, SPINE, and STEM, with the func-
tions in lines 1?5, 22, and 23 of Table 1.
SUBJECT: Features used for subject prediction
make use of the AEP variables VOICE and STEM.
In addition, if the value of SUBJECT is an index
i (see section 3), then ?? looks at the nontermi-
nal label of the German node indexed by i as well
as the surrounding context in the German clausal
tree. Otherwise, ?? looks at the value of SUBJECT.
These basic features are combined with the func-
tions in lines 1, 3, and 24?27 of Table 1.
OBJECT: We make similar features to those for
the prediction of SUBJECT. In addition, ?? can
look at the value predicted for SUBJECT.
WH: Features for WH look at the values of WH
and SPINE, conjoined with the functions in lines
1, 15, and 19 of Table 1.
MODALS: For the prediction of MODALS, ??
looks at MODALS, SPINE, and STEM, conjoined
with the functions in lines 2?5 and 12 of Table 1.
INFL: The features for INFL include the values
of INFL, MODALS, and SUBJECT, and VOICE,
and the function in line 8 of Table 2.
MOD(i): For the MOD(i) variables, ?? looks
at the value of MODALS, SPINE and the current
MOD(i), as well as the nonterminal label of the
root node of the German modifier being placed,
and the functions in lines 24 and 28 of Table 1.
238
6 Deriving Full Translations
As we described in section 1.1, the translation of a
full German sentence proceeds in a series of steps:
a German parse tree is broken into a sequence of
clauses; each clause is individually translated; and
finally, the clause-level translations are combined
to form the translation for a full sentence. The first
and last steps are relatively straightforward. We
now show how the second step is achieved?i.e.,
how AEPs can be used to derive English clause
translations from German clauses.
We will again use the following translation
pair as an example: da? das haupthemmnis der
vorhersehbare widerstand der hersteller war./that
the main obstacle has been the predictable resis-
tance of manufacturers.
First, an AEP like the one at the top of Fig-
ure 2 is predicted. Then, for each German mod-
ifier which does not have the value deleted, an
English translation is predicted. In the example,
the modifiers das haupthemmnis and der vorherse-
hbare widerstand der hersteller would be trans-
lated to the main obstacle, and the predictable re-
sistance of manufacturers, respectively.
A number of methods could be used for trans-
lation of the modifiers. In this paper, we use the
phrase-based system of Koehn et al (2003) to
generate n-best translations for each of the mod-
ifiers, and we then use a discriminative rerank-
ing algorithm (Bartlett et al, 2004) to choose be-
tween these modifiers. The features in the rerank-
ing model can be sensitive to various properties of
the candidate English translation, for example the
words, the part-of-speech sequence or the parse
tree for the string. The reranker can also take into
account the original German string. Finally, the
features can be sensitive to properties of the AEP,
such as the main verb or the position in which the
modifier appears (e.g., subject, object, pre-sub,
post-verb, etc.) in the English clause. See
Appendix B for a full description of the features
used in the modifier translation model. Note that
the reranking stage allows us to filter translation
candidates which do not fit syntactically with the
position in the English tree. For example, we can
parse the members of the n-best list, and then learn
a feature which strongly disprefers prepositional
phrases if the modifier appears in subject position.
Finally, the full string is predicted. In our
example, the AEP variables SPINE, MODALS,
and INFL in Figure 2 give the ordering <that
SUBJECT has been OBJECT>. The AEP
and modifier translations would be combined
to give the final English string. In gen-
eral, any modifiers assigned to pre-sub,
post-sub, in-modals or post-verb are
placed in the corresponding position within the
spine. For example, the second AEP in Fig-
ure 2 has a spine with ordering <SUBJECT
are OBJECT>; modifiers 1 and 2 would be
placed in positions pre-sub and post-verb,
giving the ordering <MOD2 SUBJECT are
OBJECT MOD1>. Note that modifiers assigned
post-verb are placed after the object. If mul-
tiple modifiers appear in the same position (e.g.,
post-verb), then they are placed in the order
seen in the original German clause.
7 Experiments
We applied the approach to translation from Ger-
man to English, using the Europarl corpus (Koehn,
2005) for our training data. This corpus contains
over 750,000 training sentences; we extracted over
441,000 training examples for the AEP model
from this corpus, using the method described in
section 4. We reserved 35,000 of these training
examples as development data for the model. We
used a set of features derived from the those de-
scribed in section 5.2. This set was optimized us-
ing the development data through experimentation
with several different feature subsets.
Modifiers within German clauses were trans-
lated using the phrase-based model of Koehn et
al. (2003). We first generated n-best lists for each
modifier. We then built a reranking model?see
section 6?to choose between the elements in the
n-best lists. The reranker was trained using around
800 labeled examples from a development set.
The test data for the experiments consisted of
2,000 sentences, and was the same test set as that
used by Collins et al (2005). We use the model
of Koehn et al (2003) as a baseline for our ex-
periments. The AEP-driven model was used to
translate all test set sentences where all clauses
within the German parse tree contained at least
one verb and there was no embedding of clauses?
there were 1,335 sentences which met these crite-
ria. The remaining 665 sentences were translated
with the baseline system. This set of 2,000 trans-
lations had a BLEU score of 23.96. The baseline
system alone achieved a BLEU score of 25.26 on
the same set of 2,000 test sentences. We also ob-
tained judgments from two human annotators on
239
100 randomly-drawn sentences on which the base-
line and AEP-based outputs differed. For each ex-
ample the annotator viewed the reference transla-
tion, together with the two systems? translations
presented in a random order. Annotator 1 judged
62 translations to be equal in quality, 16 transla-
tions to be better under the AEP system, and 22
to be better for the baseline system. Annotator 2
judged 37 translations to be equal in quality, 32 to
be better under the baseline, and 31 to be better
under the AEP-based system.
8 Conclusions and Future Work
We have presented an approach to tree-to-
tree based translation which models a new
representation?aligned extended projections?
within a discriminative, feature-based framework.
Our model makes use of an explicit representation
of syntax in the target language, together with con-
straints on the alignments between source and tar-
get parse trees.
The current system presents many opportuni-
ties for future work. For example, improve-
ment in accuracy may come from a tighter in-
tegration of modifier translation into the over-
all translation process. The current method?
using an n-best reranking model to select the best
candidate?chooses each modifier independently
and then places it into the translation. We in-
tend to explore an alternative method that com-
bines finite-state machines representing the n-best
output from the phrase-based system with finite-
state machines representing the complementiz-
ers, verbs, modals, and other substrings of the
translation derived from the AEP. Selecting mod-
ifiers using this representation would correspond
to searching the finite-state network for the most
likely path. A finite-state representation has many
advantages, including the ability to easily incorpo-
rate an n-gram language model.
Future work may also consider expanded defi-
nitions of AEPs. For example, we might consider
AEPs that include larger chunks of phrase struc-
ture, or we might consider AEPs that contain more
detailed information about the relative ordering of
modifiers. There is certainly room for improve-
ment in the accuracy with which AEPs are pre-
dicted in our data; the feature-driven approach al-
lows a wide range of features to be tested. For ex-
ample, it would be relatively easy to incorporate a
syntactic language model (i.e., a prior distribution
over AEP structures) induced from a large amount
of English monolingual data.
Appendix A: Identification of Clauses
In the English parse trees, we identify clauses as
follows. Any non-terminal labeled by the parser
of (Collins, 1999) as SBAR or SBAR-A is labeled
as a clause root. Any node labeled by the parser as
S or S-A is also labeled as the root of a clause, un-
less it is directly dominated by a non-terminal la-
beled SBAR or SBAR-A. Any node labeled SG or
SG-A by the parser is labeled as a clause root, un-
less (1) the node is directly dominated by SBAR or
SBAR-A; or (2) the node is directly dominated by
a VP, and the node is directly preceded by a verb
(POS tag beginning with V) or modal (POS tag be-
ginning with M). Any node labeled VP is marked
as a clause root if (1) the node is not directly dom-
inated by a VP, S, S-A, SBAR, SBAR-A, SG, or
SG-A; or (2) the node is directly preceded by a
coordinating conjunction (i.e., a POS tag labeled
as CC).
In German parse trees, we identify any nodes
labeled as S or CS as clause roots. In addition,
we mark any node labeled as VP as a clause root,
provided that (1) it is preceded by a coordinating
conjunction, i.e., a POS tag labeled as KON; or (2)
it has one of the functional tags -mo, -re or -sb.
Appendix B: Reranking Modifier
Translations
The n-best reranking model for the translation of
modifiers considers a list of candidate translations.
We hand-labeled 800 examples, marking the ele-
ment in each list that would lead to the best trans-
lation. The features of the n-best reranking algo-
rithm are combinations of the basic features in Ta-
bles 3 and 4.
Each list contained the n-best translations pro-
duced by the phrase-based system of Koehn et al
(2003). The lists also contained a supplementary
candidate ?DELETED?, signifying that the mod-
ifier should be deleted from the English transla-
tion. In addition, each candidate derived from the
phrase-based system contributed one new candi-
date to the list signifying that the first word of
the candidate should be deleted. These additional
candidates were motivated by our observation that
the optimal candidate in the n-best list produced
by the phrase-based system often included an un-
wanted preposition at the beginning of the string.
240
1 candidate string
2 should the first word of the candidate be deleted?
3 POS tag of first word of candidate
4 POS tag of last word of candidate
5 top nonterminal of parse of candidate
6 modifier deleted from English translation?
7 first candidate on n-best list
8 first word of candidate
9 last word of candidate
10 rank of candidate in n-best list
11 is there punctuation at the beginning, middle,
or end of the string?
12 if the first word of the candidate should be deleted,
what is the string that is deleted?
13 if the first word of the candidate should be deleted,
what is the POS tag of the word that is deleted?
Table 3: Functions of the candidate modifier translations used
for making features in the n-best reranking model.
1 the position of the modifier (0?4) in AEP
2 main verb
3 voice
4 subject prediction
5 German input string
Table 4: Functions of the German input string and predicted
AEP output used for making features in the n-best reranking
model.
Acknowledgements
We would like to thank Luke Zettlemoyer, Regina Barzilay,
Ed Filisko, and Ben Snyder for their valuable comments and
help during the writing of this paper. Thanks also to Jason
Rennie and John Barnett for providing human judgments of
the translation output. This work was funded by NSF grants
IIS-0347631, IIS-0415030, and DMS-0434222, as well as a
grant from NTT, Agmt. Dtd. 6/21/1998.
References
H. Alshawi. 1996. Head automata and bilingual tiling: trans-
lation with minimal representations. ACL 96.
P. Bartlett, M. Collins, B. Taskar, and D. McAllester. 2004.
Exponentiated gradient algorithms for large-margin struc-
tured classification. Proceedings of NIPS 2004.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine translation.
Computational Linguistics, 22(1):39?69.
E. Charniak, K. Knight, and K. Yamada. 2001. Syntax-based
language models for statistical machine translation. ACL
01.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. ACL 05.
M. Collins. 1999. Head-Driven Statistical Models for Natu-
ral Language Parsing. University of Pennsylvania.
M. Collins. 2002. Discriminative training methods for hid-
den markov models: theory and experiments with percep-
tron algorithms. EMNLP 02.
M. Collins and B. Roark. 2004. Incremental parsing with the
perceptron algorithm. ACL 04.
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause restruc-
turing for statistical machine translation. ACL 05.
H. Daume? III and D. Marcu. 2005. Learning as search op-
timization: approximate large margin methods for struc-
tured prediction. ICML 05.
A. Dubey. 2005. What to do when lexicalization fails: pars-
ing German with suffix analysis and smoothing. ACL 05.
J. Eisner. 2003. Learning non-isomorphic tree mappings for
machine translation. ACL 03, Companion Volume.
R. Frank. 2002. Phrase Structure Composition and Syntactic
Dependencies. Cambridge, MA: MIT Press.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule? HLT-NAACL 04.
D. Gildea. 2003. Loosely tree-based alignment for machine
translation. ACL 03.
J. Graehl and K. Knight. 2004. Training tree transducers.
NAACL-HLT 04.
A. Joshi. 1985. How much context-sensitivity is necessary
for characterizing structural descriptions ? tree-adjoining
grammar. Cambridge University Press.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase
based translation. HLT-NAACL 03.
P. Koehn. 2005. Europarl: A parallel corpus for statistical
machine translation. MT Summit 05.
I. D. Melamed 2004. Statistical machine translation by pars-
ing. ACL 04.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,
A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain,
Z. Jin, D. Radev. 2004. A smorgasbord of features for
statistical machine translation. HLT/NAACL 04
F. J. Och and H. Ney. 2004. The alignment template ap-
proach to statistical machine translation. Computational
Linguistics, 30(4):417?449.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational Lin-
guistics, 29(1):19?51.
C. Quirk, A. Menezes, and C. Cherry. 2005. Depen-
dency tree translation: syntactically informed phrasal
SMT. EACL 05.
S. Riezler and J. Maxwell. 2006. Grammatical machine
translation. In NLT-NAACL 06.
S. Shieber. 2004. Synchronous grammars as tree transduc-
ers. In Proceedings of the Seventh International Workshop
on Tree Adjoining Grammar and Related Formalisms.
S. Shieber and Y. Schabes. 1990. Synchronous tree-
adjoining grammars. In Proceedings of the 13th Interna-
tional Conference on Computational Linguistics.
D. Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
F. Xia and M. McCord. 2004. Improving a statistical MT
system with automatically learned rewrite patterns. COL-
ING 04.
K. Yamada and K. Knight. 2001. A syntax-based statistical
translation model. ACL 01.
241
