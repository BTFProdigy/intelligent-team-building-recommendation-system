Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 42?54,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Joint Learning Model of Word Segmentation, Lexical Acquisition,
and Phonetic Variability
Micha Elsner
melsner0@gmail.com
Dept. of Linguistics
The Ohio State University
Sharon Goldwater
sgwater@inf.ed.ac.uk
ILCC, School of Informatics
University of Edinburgh
Naomi H. Feldman
nhf@umd.edu
Dept. of Linguistics
University of Maryland
Frank Wood
fwood@robots.ox.ac.uk
Dept. of Engineering
University of Oxford
Abstract
We present a cognitive model of early lexi-
cal acquisition which jointly performs word
segmentation and learns an explicit model of
phonetic variation. We define the model as a
Bayesian noisy channel; we sample segmen-
tations and word forms simultaneously from
the posterior, using beam sampling to control
the size of the search space. Compared to a
pipelined approach in which segmentation is
performed first, our model is qualitatively more
similar to human learners. On data with vari-
able pronunciations, the pipelined approach
learns to treat syllables or morphemes as words.
In contrast, our joint model, like infant learners,
tends to learn multiword collocations. We also
conduct analyses of the phonetic variations that
the model learns to accept and its patterns of
word recognition errors, and relate these to de-
velopmental evidence.
1 Introduction
By the end of their first year, infants have acquired
many of the basic elements of their native language.
Their sensitivity to phonetic contrasts has become
language-specific (Werker and Tees, 1984), and they
have begun detecting words in fluent speech (Jusczyk
and Aslin, 1995; Jusczyk et al, 1999) and learn-
ing word meanings (Bergelson and Swingley, 2012).
These developmental cooccurrences lead some re-
searchers to propose that phonetic and word learning
occur jointly, each one informing the other (Swingley,
2009; Feldman et al, 2013). Previous computational
models capture some aspects of this joint learning
problem, but typically simplify the problem consid-
erably, either by assuming an unrealistic degree of
phonetic regularity for word segmentation (Goldwa-
ter et al, 2009) or assuming pre-segmented input
for phonetic and lexical acquisition (Feldman et al,
2009; Feldman et al, in press; Elsner et al, 2012).
This paper presents, to our knowledge, the first broad-
coverage model that learns to segment phonetically
variable input into words, while simultaneously learn-
ing an explicit model of phonetic variation that allows
it to cluster together segmented tokens with different
phonetic realizations (e.g., [ju] and [jI]) into lexical
items (/ju/).
We base our model on the Bayesian word segmen-
tation model of Goldwater et al (2009) (henceforth
GGJ), using a noisy-channel setup where phonetic
variation is introduced by a finite-state transducer
(Neubig et al, 2010; Elsner et al, 2012). This in-
tegrated model allows us to examine how solving
the word segmentation problem should affect infants?
strategies for learning about phonetic variability and
how phonetic learning can allow word segmentation
to proceed in ways that mimic the idealized input
used in previous models.
In particular, although the GGJ model achieves
high segmentation accuracy on phonemic (non-
variable) input and makes errors that are qualitatively
similar to human learners (tending to undersegment
the input), its accuracy drops considerably on phonet-
ically noisy data and it tends to oversegment rather
than undersegment. Here, we demonstrate that when
the model is augmented to account for phonetic vari-
ability, it is able to learn common phonetic changes
42
and by doing so, its accuracy improves and its errors
return to the more human-like undersegmentation
pattern. In addition, we find small improvements
in lexicon accuracy over a pipeline model that seg-
ments first and then performs lexical-phonetic learn-
ing (Elsner et al, 2012). We analyze the model?s
phonetic and lexical representations in detail, draw-
ing comparisons to experimental results on adult and
infant speech processing. Taken together, our results
support the idea that a Bayesian model that jointly
performs word segmentation and phonetic learning
provides a plausible explanation for many aspects of
early phonetic and word learning in infants.
2 Related Work
Nearly all computational models used to explore the
problems addressed here have treated the learning
tasks in isolation. Examples include models of word
segmentation from phonemic input (Christiansen et
al., 1998; Brent, 1999; Venkataraman, 2001; Swing-
ley, 2005) or phonetic input (Fleck, 2008; Rytting,
2007; Daland and Pierrehumbert, 2011; Boruta et
al., 2011), models of phonetic clustering (Vallabha
et al, 2007; Varadarajan et al, 2008; Dupoux et al,
2011) and phonological rule learning (Peperkamp et
al., 2006; Martin et al, 2013).
Elsner et al (2012) present a model that is similar
to ours, using a noisy channel model implemented
with a finite-state transducer to learn about phonetic
variability while clustering distinct tokens into lexi-
cal items. However (like the earlier lexical-phonetic
learning model of Feldman et al (2009; in press))
their model assumes known word boundaries, so
to perform both segmentation and lexical-phonetic
learning, they use a pipeline that first segments using
GGJ and then applies their model to the results.
Neubig et al (2010) also present a transducer-
based noisy channel model that performs joint in-
ference on two out of the three tasks we consider
here; their model assumes fixed probabilities for pho-
netic changes (the noise model) and jointly infers
the word segmentation and lexical items, as in our
?oracle? model below (though unlike our system their
model learns from phone lattices rather than a single
transcription). They evaluate only on phone recogni-
tion, not scoring the inferred lexical items.
Recently, Bo?rschinger et al (2013) did present a
? Geom a, b, ..., ju, ... want, ... juwant, ...
Generator for possible words
Probabilities for each word(sparse)p(?i) = .1, p(a) = .05, p(want) = .01...
? contexts
Conditional probabilitiesfor each word after each wordp(?i | want) = .3, p(a | want) = .1, p(want | want) = .0001...
G
Gx?1
n utterances
x1 x2 ... Intended formsju want ? kukiju want ?t...s1 s2 ...
00
Surface formsj? wan ? kukiju wand ?t...T
GGJ09
Figure 1: The graphical model for our system (Eq. 1-
4). Note that the si are not distinct observations; they
are concatenated together into a continuous sequence of
characters which constitute the observations.
joint learner for segmentation, phonetic learning, and
lexical clustering, but the model and inference are
tailored to investigate word-final /t/-deletion, rather
than aiming for a broad coverage system as we do.
3 Model
We follow several previous models of lexical acquisi-
tion in adopting a Bayesian noisy channel framework
(Eq. 1-4; Fig. 1). The model has two components:
a source distribution P (X) over utterances without
phonetic variability X , i.e., intended forms (Elsner et
al., 2012) and a channel or noise distribution T (S|X)
that translates them into the observed surface forms
S. The boundaries between surface forms are then
deterministically removed so that the actual observa-
tions are just the unsegmented string of characters in
the surface forms.
G0|?0, pstop ? DP (?0, Geom(pstop)) (1)
Gx|G0, ?1 ? DP (?1, G0) (2)
Xi|Xi?1 ? GXi?1 (3)
S|X; ? ? T (S|X; ?) (4)
The source model is an exact copy of GGJ1: to
generate the intended-form word sequences X , we
1We use their best reported parameter values: ?0 =
3000, ?1 = 100, pstop = .2 and for unigrams, ?0 = 20.
43
sample a random language model from a hierarchi-
cal Dirichlet process (Teh et al, 2006) with char-
acter strings as atoms. To do so, we first draw a
unigram distribution G0 from a Dirichlet process
prior whose base distribution generates intended form
word strings by drawing each phone in turn until the
stop character is drawn (with probability pstop). Then,
for each possible context word x, we draw a condi-
tional distribution on words following that context
Gx = P (Xi = ?|Xi?1 = x) using G0 as a prior.
Finally, we sample word sequences x1 . . . xn from
the bigram model.
The channel model is a finite transducer with pa-
rameters ? which independently rewrites single char-
acters from the intended string into characters of the
surface string. We use MAP point estimates of these
parameters; single characters (without n-gram con-
text) are used for computational efficiency. Also for
efficiency, the transducer can insert characters into
the surface string, but cannot delete characters from
the intended string. As in several previous phonolog-
ical models (Dreyer et al, 2008; Hayes and Wilson,
2008), the probabilities are learned using a feature-
based log-linear model. For features, we use all the
unigram features from Elsner et al (2012), which
check faithfulness to voicing, place and manner of
articulation (for example, for k ? g, active features
are faith-manner, faith-place, output-g and voiceless-
to-voiced).
Below, we present two methods for learning the
transducer parameters ?. The oracle transducer is es-
timated using the gold-standard word segmentations
and intended forms for the dataset; it represents the
best possible approximation under our model of the
actual phonetics of the dataset. We can also estimate
the transducer using the EM algorithm. We first ini-
tialize a simple transducer by putting small weights
on the faithfulness features to encourage phonologi-
cally plausible changes. With this initial model, we
begin running the sampler used to learn word segmen-
tations. After several hundred sampler iterations, we
start re-estimating the transducer by maximum likeli-
hood after each iteration. We regularize our estimates
by adding 200 pseudocounts for the rewrite x ? x
during training (rather than regularizing the weights
for particular features). We also show segment only
results for a model without the transducer component
(i.e., S = X); this recovers the GGJ baseline.
4 Inference
Inference for this model is complicated for two rea-
sons. First, the hypothesis space is extremely large.
Since we allow the input string to be probabilistically
lengthened, we cannot be sure how long it is, nor
which characters it contains. Second, our hypothe-
ses about nearby characters are highly correlated due
to lexical effects. When deciding how to interpret
[w@nt], if we posit that the intended vowel is /2/, the
word is likely to be /w2n/ ?one? and the next word
begins with /t/ ; if instead we posit that the vowel
is /O/, the word is probably /wOnt/ ?want?. Thus,
inference methods that change only one character at
a time are unlikely to mix well. Since they cannot
simultaneously change the vowel and resegment the
/t/, they must pass through a low-probability inter-
mediate state to get from one state to the other, so
will tend to get stuck in a bad local minimum. A
Gibbs sampler which inserts or deletes a single seg-
ment boundary in each step (Goldwater et al, 2009)
suffers from this problem.
Mochihashi et al (2009) describe an inference
method with higher mobility: a block sampler for
the GGJ model that samples from the posterior over
analyses of a whole utterance at once. This method
encodes the model as a large HMM, using dynamic
programming to select an analysis. We encode our
own model in the same way, constructing the HMM
and composing it with the transducer (Mohri, 2004)
to form a larger finite-state machine which is still
amenable to forward-backward sampling.
4.1 Finite-state encoding
Following Mochihashi et al (2009) and Neubig et
al. (2010), we can write the original GGJ model
as a Hidden Semi-Markov model. States in the
HMM, written ST:[w][C], are labeled with the
previous word w and the sequence of characters C
which have so far been incorporated into the current
word. To produce a word boundary, we transition
from ST:[w][C] to ST:[C][] with probability
P (xi = C|xi?1 = w). We can also add the next
character s to the current word, transitioning from
ST:[w][C] to ST:[w][C : s], at no cost (since
the full cost of the word is paid at its boundary, there
44
? word j?p(j?|[s])d
j u word ju[s]
word j u word up(j|[s]) p(u|j)
p(ju|[s])j/jd/j ?/u
u/u
u/u
Figure 2: A fragment of the composed finite-state machine
for word segmentation and character replacement for the
surface string ju. The start state [s] is followed by a word
boundary (filled circle); the next intended character is
probably j but can be d or others with lower probability.
After j can be a word boundary (forming the intended
word j), or another character such as u, @ or other (not
shown) alternatives.
is no cost for the individual characters)2.
In addition to analyses using known words, we
can also encode the uniform-geometric prior over
unknown words using a finite-state machine. We
can choose to select a word from the prior by tran-
sitioning to a state ST:[Geom][] with probability
P (new word|xi?1 = w) immediately after a word
boundary. While inGeom, we can transition to a new
Geom state and produce any character with uniform
probability P (c) = (1?Pstop) 1|C| ; otherwise, we can
end the word, transitioning to ST:[unk .word][],
with probability Pstop.
This construction is also approximate; it ignores
the possibility that the prior will generate a known
word w, in which case our final transition ought to
be to ST:[w][] instead of ST:[unk .word][]. This
approximation means we do not need to add context
to the Geom state to remember the sequence of char-
acters it produced, which allows us to keep only a
single Geom state on the chart at each timestep.
When we compose this model with the channel
model, the number of states expands. Each state must
now keep track of the previous word, what intended
charactersC have been posited and what surface char-
acters S have been recognized, ST:[w][C][S].
2Though not mentioned by Mochihashi et al (2009) or Neu-
big et al (2010), this construction is not exact, since transitions
in a Bayesian HMM are exchangeable but not independent (Beal
et al, 2001): if a word occurs twice in an utterance, its probabil-
ity is slightly higher the second time. For single utterances, this
bias is small and easy to correct for using a Metropolis-Hastings
acceptance check (Bo?rschinger and Johnson, 2012) using the
path probability from the HMM as the proposal.
To recognize the current word, we transition to
ST:[C][][] with probability P (xi = C|xi?1 =
w). To parse a new surface character s by positing
intended character x (note that x might be ), we
transition to ST:[w][C : x][S : s] with probabil-
ity T (s|x). (As above, we pay no cost for our choice
of x, which is paid for when we recognize the word;
however, we must pay for s.) For efficiency, we do
not allow the G0 states to hypothesize different sur-
face and intended characters, so when we initially
propose an unknown word, it must surface as itself.3
4.2 Beam sampler
This machine has too many states to fully fill the chart
before backward sampling, so we restrict the set of
trajectories under consideration using beam sampling
(Van Gael et al, 2008) and simulated annealing.
The beam sampler is closely related to the standard
beam search technique, which uses a probability cut-
off to discard parts of the FST which are unlikely to
figure in the eventual solution. Unlike conventional
beam search, the sampler explores using stochastic
cutoffs, so that all trajectories are explored, but most
of the bad ones are explored infrequently, leading to
higher efficiency.
We design our beam sampler to restrict the set
of potential intended characters at each timestep.
In particular, given a stream of input characters
S = s1 . . . sn, we introduce a set of auxiliary cutoff
variables U = u1 . . . un. The ui variables represent
limits on the probability of the emission of surface
character si; we exclude any hypothesized xi whose
probability of generating si, T (si|xi), is less than
ui. To create a beam sampling scheme, we must de-
vise a distribution for U given a state sequence Q (as
discussed above, the sequence of states encodes the
intended character sequence and the segmentation
of the surface string), Pu(U |Q) and then incorporate
the probability of U into the forward messages.
If qi is the state in Q at which si is generated, and
xi the corresponding intended character, we require
that Pu < T (si|xi); that is, the cutoffs must not
exclude any states in the sequence Q. We define Pu
3Again, this approximation is corrected for by the Metropolis-
Hastings step.
45
as a ?-mixture of two distributions:
Pu(u|si, xi) = ?U [0,min(.05, T (si|xi))]+
(1? ?)T (si|xi)Beta(5, 1e? 5)
The former distribution is quite unrestrictive, while
the latter prefers to prune away nearly all the states.
Thus, for most characters in the string, we do not
permit radical changes, while for a fraction, we do.
We follow Huggins and Wood (2013), who ex-
tended Van Gael et al (2008) to the case of a non-
uniform Pu, to define our forward message ? as:
?(qi, i) ? P (qi, S0..i, U0..i) (5)
=
?
qi?1
Pu(ui|si, xi)T (si|xi)?(qi?1, i? 1)
This is the standard HMM forward message, aug-
mented with the probability of u. Since Pu(?|si, xi)
is required to be less than T (si|xi), it will be 0 when-
ever T (si|xi) < u; this is how the u variables func-
tion as cutoffs. In practice, we use the u variables to
filter the lexical items that begin at each position i
in advance, using a simple 0/1 edit distance Markov
model which runs faster than our full model. (For ex-
ample, we can quickly check if the current U allows
want as the intended form for wOlk at i; if not, we can
avoid constructing the prefix ST:[xi?1][wa][wO]
since the continuation will fail.)
The algorithm?s speed depends on the size and
uncertainty of the inferred LM: large numbers of
plausible words mean more states to explore. When
inference starts, and the system is highly uncertain
about word boundaries, it is therefore reasonable to
limit the exploration of the character sequence. We
do so by annealing in two ways: as in Goldwater
et al (2009), we raise P (X) (Eq. 3) to a power t
which increases linearly from .3. To sample from
the posterior, we would want to end with t = 1, but
as in previous noisy-channel models (Elsner et al,
2012; Bahl et al, 1980) we get better results when we
emphasize the LM at the expense of the channel and
so end at t = 2. Meanwhile, as t rises and we explore
fewer implausible lexical sequences, we can explore
the character sequence more. We begin by setting
the ? interpolation parameter of Pu to 0 to minimize
exploration and increase it linearly to .3 (allowing
the system to change about a third of the characters
on each sweep). This is similar to the scheme for
altering Pu in Huggins and Wood (2013).
4.3 Dataset and metrics
We use the corpus released by Elsner et al (2012),
which contains 9790 child-directed English utter-
ances originally from the Bernstein-Ratner corpus
(Bernstein-Ratner, 1987) and later transcribed phone-
mically (Brent, 1999). This standard word segmenta-
tion dataset was modified by Elsner et al (2012) to
include phonetic variation by assigning each token a
pronunciation independently selected from the empir-
ical distribution of pronunciations of that word type
in the closely-transcribed Buckeye Speech Corpus
(Pitt et al, 2007). Following previous work, we hold
out the last 1790 utterances as unseen test data during
development. In the results presented here, we run
the model on all 9790 utterances but score only these
1790. We average results over 5 runs of the model
with different random seeds.
We use standard metrics for segmentation and lex-
icon recovery. For segmentation, we report precision,
recall and F-score for word boundaries (bds), and for
the positions of word tokens in the surface string (srf ;
both boundaries must be correct).
For normalization of the pronunciation variation,
we follow Elsner et al (2012) in measuring how well
the system clusters together variant pronunciations
of the same lexical item, without insisting that the
intended form the system proposes for them match
the one in our corpus. For example, if the system
correctly clusters [ju] and [jI] together but assigns
them the incorrect intended form /jI/, we can still
give credit to this cluster if it is the one that overlaps
best with the gold-standard /ju/ cluster. To compute
these scores, we find the optimal one-to-one map-
ping between our clusters of pronunciations and the
true lexical entries, then report scores for mapped to-
kens (mtk; boundaries and mapping to gold standard
cluster must be correct) and mapped types4 (mlx).
4Elsner et al (2012) calls the mlx metric lexicon F, which
is possibly confusing. We map the clusters to a gold-standard
lexicon (plus potentially some words that don?t correspond to
anything in the gold standard) and compute a type-level F-score
on this lexicon.
46
Prec Rec F-score
Pipeline (segment, then cluster): (Elsner et al, 2012)
Bds 70.4 93.5 80.3
Srf 56.5 69.7 62.4
Mtk 44.2 54.5 48.8
Mlx 48.6 43.1 45.7
Bigram model, segment only
Bds 73.9 (-0.6:0.7) 91.0 (-0.6:0.4) 81.6 (-0.5:0.6)
Srf 60.8 (-0.7:1.1) 70.8 (-0.8:0.9) 65.4 (-0.6:1.0)
Mtk 41.6 (-0.6:1.2) 48.4 (-0.5:1.2) 44.8 (-0.6:1.2)
Mlx 36.6 (-0.7:0.8) 49.8 (-1.0:0.8) 42.2 (-0.9:0.8)
Unigram model, oracle transducer
Bds 81.4 (-0.8:0.4) 72.1 (-0.9:0.8) 76.4 (-0.5:0.7)
Srf 63.6 (-1.0:1.1) 58.5 (-1.2:1.2) 60.9 (-0.9:1.2)
Mtk 46.8 (-1.0:1.1) 43.0 (-1.1:1.2) 44.8 (-1.0:1.2)
Mlx 56.7 (-1.1:1.0) 47.6 (-1.4:0.8) 51.7 (-1.2:0.8)
Bigram model, oracle transducer
Bds 76.1 (-0.6:0.6) 83.8 (-0.9:1.0) 79.8 (-0.8:0.4)
Srf 62.2 (-0.9:1.0) 66.7 (-1.2:1.1) 64.4 (-1.1:0.8)
Mtk 47.2 (-0.7:0.9) 50.6 (-1.0:0.8) 48.8 (-0.8:0.7)
Mlx 40.1 (-1.0:1.2) 43.7 (-0.6:0.7) 41.8 (-0.8:0.6)
Bigram model, EM transducer
Bds 80.1 (-0.5:0.8) 83.0 (-1.4:1.3) 81.5 (-0.5:0.7)
Srf 66.1 (-0.8:1.4) 67.8 (-1.4:1.7) 66.9 (-0.9:1.4)
Mtk 49.0 (-0.9:0.7) 50.3 (-1.1:1.4) 49.6 (-1.0:1.0)
Mlx 43.0 (-1.0:1.4) 49.5 (-1.5:1.1) 46.0 (-1.0:1.3)
Table 1: Mean segmentation (bds, srf ) and normalization
(mtk, mlx) scores on the test set over 5 runs. Parentheses
show min and max scores as differences from the mean.
5 Results and discussion
In the following sections, we analyze how our model
with variability compares to GGJ on noisy data. We
give quantitative scores and also show that qualitative
patterns of errors are often similar to those of human
learners and listeners.
5.1 Clean versus variable input
We begin by evaluating our model as a word seg-
mentation system. (Table 1 gives segmentation and
normalization scores for various models and base-
lines on the 1790 test utterances.) We first confirm
that our inference method is reasonable. The bigram
model without variability (?segment only?) should
have the same segmentation performance as the stan-
dard dpseg implementation of GGJ. This is the case:
dpseg has boundary F of 80.3 and token F of 62.4;
we get 81.6 and 65.4. Thus, our sampler is finding
good solutions, at least for the no-variability model.
We compare segmentation scores between the
?segment only? system and the two bigram models
with transducers (?oracle? and ?EM?). While these
systems all achieve similar segmentation scores, they
do so in different ways. ?Segment only? finds a so-
lution with boundary precision 73.9% and boundary
recall 91.0% for a total F of 81.6%. The low pre-
cision and high recall here indicate a tendency to
oversegment; when the analysis of a given subse-
quence is unclear, the system prefers to chop it into
small chunks. The bigram models which incorporate
transducers score P : 76.1, R: 83.8 (oracle) and P :
80.1,R: 83.0 (EM), indicating that they prefer to find
longer sequences (undersegment) more.
In previous experiments on datasets without varia-
tion, GGJ also has a strong tendency to undersegment
the data (boundary P : 90.1, R: 80.3), which Gold-
water et al argue is rational behavior for an ideal
learner seeking a parsimonious explanation for the
data. Undersegmentation occurs especially when ig-
noring lexical context (a unigram model), but to some
extent even in bigram models. Human learners also
tend to learn collocations as single words (Peters,
1983; Tomasello, 2000), and the GGJ model has been
shown to capture several other effects seen in labora-
tory segmentation tasks (Frank et al, 2010). Together,
these findings support the idea that human learners
may behave in important respects like the Bayesian
ideal learners that Goldwater et al presented.
However, experiments on data with variation have
called these conclusions into question. In particu-
lar, GGJ has previously been shown to oversegment
rather than undersegment as the input grows noisier
(Fleck, 2008), and our results replicate this finding
(oversegmentation for the ?segment only? model).
In addition, the GGJ bigram model, which achieves
much higher segmentation accuracy than the unigram
model on clean data, actually performs worse on very
noisy data (Jansen et al, 2013). Infants are known to
track statistical dependencies across words (Go?mez
and Maye, 2005), so it is worrisome that these de-
pendencies hurt GGJ?s segmentation accuracy when
learning from noisy data.
Our results show that modeling phonetic variabil-
ity reverses the problematic trends described above.
Although the models with phonetic variability show
similar overall segmentation accuracy on noisy data
to the original GGJ model, the pattern of errors
changes, with less oversegmentation and more un-
47
dersegmentation. Thus, their qualitative performance
on variable data resembles GGJ?s on clean data, and
therefore the behavior of human learners.
5.2 Phonetic variability
We next analyze the model?s ability to normalize vari-
ations in the pronunciation of tokens, by inspecting
the mtk score. The ?segment only? baseline is pre-
dictably poor, F : 44.8. The pipeline model scores
48.8, and our oracle transducer model matches this
exactly. The EM transducer scores better, F : 49.6.
Although the confidence intervals overlap slightly,
the EM system also outperforms the pipeline on the
other F -measures; altogether, these results suggest
at least a weak learning synergy (Johnson, 2008) be-
tween segmentation and phonetic learning.
It is interesting that EM can perform better than
the oracle. However, EM is more conservative about
which sound changes it will allow, and thus tends to
avoid mistakes caused by the simplicity of the trans-
ducer model. Since the transducer works segment-
by-segment, it can apply rare contextual variations
out of context. EM benefits from not learning these
variations to begin with.
We can also compare the bigram and unigram ver-
sions of the model. The unigram model is a rea-
sonable segmenter, though not quite as good as the
bigram model, with boundary F of 76.4 and token
F of 60.9 (compared to 79.8 and 64.4 using the bi-
gram model). However, it is not good at normalizing
variation; its mtk score is comparable to the baseline
at 44.8%5. Although bigram context is only moder-
ately effective for telling where words are, the model
seems heavily reliant on lexical context to decide
what words it is hearing.
5.3 Error analysis
To gain more insight into the differing behavior of
our model versus a pipelined system, we inspect the
intended word strings X proposed by each one in
detail. Below, we categorize the kinds of intended
word strings that the model might propose to span a
given gold-standard word token:
Correct Correctly segmented, mapped to the correct
lexical item (e.g., gold intended /ju/, surface
5Elsner et al (2012) show a similar result for a unigram
version of their pipelined system.
EM-learned Segment only
Correct 49.88 47.61
Wrong form 17.96 23.73
Collocation 14.25 7.59
Split 8.26 15.18
One bound 7.11 15.18
Corr. colloc. 1.35 < 0.01
Other 0.75 0.22
Corr. split 0.43 0.66
Table 2: Distribution (%) of error types (see text) in a
single run on the full dataset.
segmentation [ju], intended /ju/)
Wrong form Correctly segmented, mapped to the
wrong lexical item (/ju/, surf. [ju], int. /jEs/)
Colloc Missegmented as part of a sequence whose
boundaries correspond to real word boundaries
(/ju?want/, surf. [juwant], int. /juwant/)
Corr. colloc As above, but proposed lexical item
maps to this word (/ar?ju/, surf. [arj@] int.
/ju/)
Split Missegmented with a word-internal boundary
(/dOgiz/, surf. [dO?giz], int. /dO?giz/)
Corr. split As above, but one proposed word maps
correctly (/dOgi/, surf. [dOg?i], int. /dOgi?@/)
One boundary One boundary correct, the other
wrong (/ju?wa. . ./, surf. [juw], int. /juw/)
Other Not a collocation, both boundaries are wrong
(/du?ju?wa. . ./, surf. [ujuw], int. /ujuw/)
Table 2 shows the distribution over intended word
strings proposed by the ?segment only? baseline and
the EM-learned transducer. Both systems propose
a large number of correct forms, and the most com-
mon error category is ?wrong form? (lexical error
without segmentation error), an error which could
potentially be repaired in a pipeline system. How-
ever, the remaining errors represent segmentation
mistakes which a pipeline could not repair. Here
the two systems behave quite differently. The EM-
learned transducer analyses 14% of real tokens as
parts of multiword collocations like ?doyou?; in an-
other 1.35%, the underlying content word is even
correctly detected. The non-variable system, on the
other hand, analyses 15% of real tokens by splitting
them into pieces. Since infant learners tend to learn
collocations, this supports our analysis that the model
with variation better models human behavior.
48
EM ju: 805, duju: 239, juwan: 88, jI: 58, e~ju: 54, judu:
47, j?: 39, jul2k: 39, Su: 30, u: 23, Zu: 18, j: 17,
je~: 16, tSu: 15, aj:15, Derjugo: 12, dZu: 12
GGJ ju: 498, jI: 280, j@: 165, ji: 119, duju: 106, dujI: 44,
kInju: 39, i: 32, u: 29, kInjI: 29, jul2k: 24, juwan:
23, j: 22, Su: 19, jU: 18, e~ju: 18, I:16, Zu: 15, dZ?u:
13, jE: 12, SI: 11, T?Nkju: 11
Table 3: Forms proposed with frequency > 10 for
gold-standard tokens of ?you? in one sample from EM-
transducer and segment-only (GGJ) system.
To illustrate this behavior anecdotally, we present
the distribution of intended word strings spanning
tokens whose gold intended form is /ju/ ?you? (Table
3). The EM-learned solution proposes 805 tokens
of /ju/, which is the correct analysis6; the ?segment
only? system instead finds varying forms like /jI/,
/j?/ etc. This is unsurprising and could be repaired
by a suitable pipelined system. However, the EM
system also proposes 239 instances of ?doyou?, 88
instances of ?youwant?, 54 instances of ?areyou? and
several other collocations. The ?segment only? sys-
tem finds some of these collocations, split into dif-
ferent versions: for instance 106 instances of /duju/
and 44 of /dujI/. In a pipelined system, we could
combine these variants to find 150 instances? but
this is still 89 instances short of the 239 found when
allowing for variability. The same pattern holds for
?youlike? and ?youwant?. Because the non-variable
system must learn each variant separately, it learns
only the most common instances of these long collo-
cations, and analyzes infrequent variants differently.
We also perform this analysis specifically for
words beginning with vowels. Infants show a delay
in their ability to segment these words from continu-
ous speech (Mattys and Jusczyk, 2001; Nazzi et al,
2005; Seidl and Johnson, 2008), and Seidl and John-
son (2008) suggest a perceptual explanation? initial
vowels can be hard to hear and often exhibit variation
due to coarticulation or resyllabification. Although
our dataset does not contain coarticulation as such, it
should show this pattern of greater variation, which
we hypothesize might lead to difficulty in segmenting
and recognizing vowel-initial words.
The model?s behavior is consistent with this hy-
pothesis (Table 4). Both the ?segment only? and
EM transducer models find approximately the same
6Not all the variants are merged, however. jI, j?, Su etc. are
still occasionally analyzed as separate lexical items.
Segment only Vow. init Cons. init
Correct 47.5 51.7
Wrong form 18.6 15.7
Collocation 14.6 12.2
Split 6.2 10.8
Right bd. corr. 5.8 3.6
Left bd. corr. 4.6 3.8
EM transducer Vow. init Cons. init
Correct 41.5 52.1
Wrong form 20.4 17.3
Collocation 19.2 12.5
Split 5.2 9.1
Right bd. corr. 6.2 2.7
Left bd. corr. 2.7 3.1
Table 4: Most common error types (%; see text) for in-
tended forms beginning with vowels or consonants. Rare
error types are not shown. ?One bound? errors are split up
by which boundary is correct.
proportion of vowel-initial tokens, and both systems
do somewhat better on consonant-initial words than
vowel-initial words. The advantage is stronger for
the transducer model, which gets only 41.5% of
vowel-initial tokens correct as opposed to 52.1% of
consonant-initial words. It proposes more colloca-
tions for vowel-initial words (19.2%) than for conso-
nants (12.5%). In cases where they do not propose a
collocation, both systems are somewhat more likely
to find the right boundary of a vowel-initial token
than the left boundary (although again this difference
is larger for the EM system); this suggests that the
problem is indeed caused by the initial segment.
5.4 Phonetic Learning
We next compare phonetic variations learned by the
model to characteristics of infant speech perception.
Infants show an asymmetry between consonants and
vowels, losing sensitivity to non-native vowel con-
trasts by eight months (Kuhl et al, 1992; Bosch
and Sebastia?n-Galle?s, 2003) but to non-native con-
sonant contrasts only by 10-12 months (Werker and
Tees, 1984). The observed ordering is somewhat
puzzling when one considers the availability for dis-
tributional information (Maye et al, 2002), which is
much stronger for stop consonants than for vowels
(Lisker and Abramson, 1964; Peterson and Barney,
1952). Infants are also conservative in generalizing
across phonetic variability, showing a delayed abil-
49
ity to generalize across talkers, affects, and dialects.
They have difficulty recognizing word tokens that are
spoken by a different talker or in a different tone of
voice until 11 months (Houston and Jusczyk, 2000;
Singh et al, 2004), and the ability to adapt to unfa-
miliar dialects appears to develop even later, between
15 and 19 months (Best et al, 2009; Heugten and
Johnson, in press; White and Aslin, 2011).
Similar to infants, our model shows both a vowel-
consonant asymmetry and a reluctance to accept the
full range of adult phonetic variability. Table 5 shows
some segment-to-segment alternations learned in var-
ious transducers. The oracle learns a large amount
of variation (u surfaces as itself only 68% of the
time) involving many different segments, whereas
EM is similar to infant learners in learning a more
conservative solution with fewer alternations over-
all. Moreover, EM appears to identify patterns of
variability in vowels before consonants. It learns a
similar range of alternations for u as in the oracle,
although it treats the sound as less variable than it
actually is. It learns much less variability for con-
sonants; it picks up the alternation of D with s and
z, but predicts that D will surface as itself 91% of
the time when the true figure is only 69%. And it
fails to learn any meaningful alternations involving
k. These results suggest that patterns of variability in
vowels are more evident than patterns of variability
in consonants when infants are beginning to solve the
word segmentation problem.
To investigate the effect of data size on this con-
servativism, we ran the system on 1000 utterances
instead of 9790. This leads to an even more conser-
vative solution, with variations for u but none of the
others (although i and D still vary more than k).
5.5 Segmentation and recognition errors
A particularly interesting set of errors are those that
involve both a missegmentation and a simultaneous
misrecognition, since the joint model is prone to
such errors while the pipelined model is not. Rel-
atively little is known about infants? misrecognitions
of words in fluent speech, although it is clear that they
find words in medial position harder (Plunkett, 2005;
Seidl and Johnson, 2006). However, adults make
missegmentation/misrecognition errors fairly often,
especially when listening to noisy audio (Butterfield
and Cutler, 1988). Such errors are more common
System x top 4 outputs s
Oracle
u u .68 @ .05 a .04 U .04
i i .85 I .03 @ .03 E .02
D D .69 s .07 [?] .07 z .04
k k .93 d .02 g .02
[?] r .21 h .11 d .01 @ .07
EM
(full)
u u .75 @ .08 I .04 U .03
i i .90 I .04 E .02
D D .91 s .03 z 0.1
k k .98
[?] @ .32 I .14 n .13 t .13
EM
(only
1000
utts)
u u .82 I .04 @ .04 a .02
i i .97
D D .95
k k .99
[?] @ .21 I .18 t .12 s .12
Table 5: Learned phonetic alternations: top 4 outputs s
with p > .001 for inputs x = uw (/u/ ), iy (/i/ ), dh (/D/ ),
k (/k/) and [?], the null character. Outputs from [?] are
insertions. The oracle allows [?] as an output (deletion)
but for computational reasons, the model does not.
when the misrecognized word belongs to a prosod-
ically rare class and when the incorrectly hypothe-
sized string contains frequent words (Cutler, 1990);
phonetically ambiguous words are also more com-
monly recognized as the more frequent of two op-
tions (Connine et al, 1993). For the indefinite article
?a? (often reduced to [@]), lexical context is the main
factor in deciding between ambiguous interpretations
(Kim et al, 2012). In rapid speech, listeners have few
phonetic cues to indicate whether it is present at all
(Dilley and Pitt, 2010). Below, we analyze various
misrecognitions made by our system (using the EM
transducer), and find some similar effects.
The easiest cases to analyze are those with no mis-
segmentation: the proposed boundaries are correct,
and the proposed lexical entry corresponds to a real
word7, but not the correct one. Most of them corre-
spond to homophones (Table 6).
Common cases with a missegmentation include it
and is, a and is, it?s and is, who, who?s and whose,
that?s and what?s, and there and there?s. In general,
these errors involve words which sometimes appear
7The one-to-one mapping can be misleading, as it may map
a large cluster to a real word on the basis of one or two tokens if
all other tokens correspond to a different word already used for
another cluster. We manually filter out a few cases like this.
50
Actual proposed count
/tu/ ?two? /t@/ ?to? 95
/kin/ ?can? /k?nt/ ?can?t? 67
/En/ ?and? /?n/ ?an? 61
/hIz/ ?his? /Iz/ ?is? 57
/D@/ ?the? /@/ ?ah? 51
/w@ts/ ?what?s? /wants/ ?wants? 40
/wan/ ?want? /won/ ?won?t? 39
/yu/ ?you? /y?/ ?yeah? 39
/f@~/ ?for? /fOr/ ?four? 30
/hir/ ?here? /hil/ ?he?ll? 28
Table 6: Top ten errors involving confusion between real,
correctly segmented words: the most common pronunci-
ation of the actual token and its orthographic form, the
same for the proposed token, and the frequency.
with a morpheme or clitic (which can easily be mis-
segmented as part of something else), words which
differ by one segment, and frequent function words
which often appear in similar contexts. These tenden-
cies match those shown by adult human listeners.
A particularly distinctive set of joint recognition
and segmentation errors are those where an entire
real token is treated as phonetic ?noise?? that is, it
is segmented along with an adjacent word, and the
system clusters the whole sequence as a token of
that word. The most common examples are ?that?s a?
identified as ?that?s?, ?have a? identified as ?have?,
?sees a? identified as ?sees? and other examples in-
volving ?a?, a word which also frequently confuses
humans (Kim et al, 2012; Dilley and Pitt, 2010).
However, there are also instances of ?who?s in? as
?who?s?, ?does it? as ?does?, and ?can you? as ?can?.
6 Conclusion
We have presented a model that jointly infers word
segmentation, lexical items, and a model of phonetic
variability; we believe this is the first model to do
so on a broad-coverage naturalistic corpus8. Our re-
sults show a small improvement in both segmentation
and normalization over a pipeline model, providing
evidence for a synergistic interaction between these
learning tasks and supporting claims of interactive
learning from the developmental literature on infants.
We also reproduced several experimental findings;
our results suggest that two vowel-consonant asym-
8Software is available from the ACL archive; updated
versions may be posted at https://bitbucket.org/
melsner/beamseg.
metries, one from the word segmentation literature
and another from the phonetic learning literature, are
linked to the large variability in vowels found in nat-
ural corpora. The model?s correspondence with hu-
man behavioral results is by no means exact, but we
believe these kinds of predictions might help guide
future research on infant phonetic and word learning.
Acknowledgements
Thanks to Mary Beckman for comments. This work
was supported by EPSRC grant EP/H050442/1 to the
second author.
References
Lalit Bahl, Raimo Bakis, Frederick Jelinek, and Robert
Mercer. 1980. Language-model/acoustic-channel-
model balance mechanism. Technical disclosure bul-
letin Vol. 23, No. 7b, IBM, December.
Matthew J. Beal, Zoubin Ghahramani, and Carl Edward
Rasmussen. 2001. The infinite Hidden Markov Model.
In NIPS, pages 577?584.
Elika Bergelson and Daniel Swingley. 2012. At 6-9
months, human infants know the meanings of many
common nouns. Proceedings of the National Academy
of Sciences, 109:3253?3258.
Nan Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck, editors,
Children?s Language, volume 6. Erlbaum, Hillsdale,
NJ.
Catherine T. Best, Michael D. Tyler, Tiffany N. Good-
ing, Corey B. Orlando, and Chelsea A. Quann. 2009.
Development of phonological constancy: Toddlers? per-
ception of native- and jamaican-accented words. Psy-
chological Science, 20(5):539?542.
Benjamin Bo?rschinger and Mark Johnson. 2012. Using
rejuvenation to improve particle filtering for Bayesian
word segmentation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 85?89, Jeju Island,
Korea, July. Association for Computational Linguistics.
Benjamin Bo?rschinger, Mark Johnson, and Katherine De-
muth. 2013. A joint model of word segmentation
and phonological variation for English word-final /t/-
deletion. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics, Sofia,
Bulgaria, August. Association for Computational Lin-
guistics.
Luc Boruta, Sharon Peperkamp, Beno??t Crabbe?, and Em-
manuel Dupoux. 2011. Testing the robustness of online
word segmentation: Effects of linguistic diversity and
51
phonetic variation. In Proceedings of the 2nd Workshop
on Cognitive Modeling and Computational Linguistics,
pages 1?9.
Laura Bosch and Nu?ria Sebastia?n-Galle?s. 2003. Simulta-
neous bilingualism and the perception of a language-
specific vowel contrast in the first year of life. Lan-
guage and Speech, 46(2-3):217?243.
Michael R. Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discovery.
Machine Learning, 34:71?105, February.
Sally Butterfield and Anne Cutler. 1988. Segmentation
errors by human listeners: Evidence for a prosodic
segmentation strategy. In Proceedings of SPEECH
?88: Seventh Symposium of the Federation of Acoustic
Societies of Europe, vol. 3, pages 827?833, Edinburgh.
Morten H. Christiansen, Joseph Allen, and Mark S. Sei-
denberg. 1998. Learning to Segment Speech Using
Multiple Cues: A Connectionist Model. Language and
Cognitive Processes, 13(2/3):221?269.
C. M. Connine, D. Titone, and J. Wang. 1993. Audi-
tory word recognition: Extrinsic and intrinsic effects of
word frequency. Journal of Experimental Psychology:
Learning, Memory and Cognition, 19:81?94.
Anne Cutler. 1990. Exploiting prosodic probabilities in
speech segmentation. In G. A. Altmann, editor, Cog-
nitive models of speech processing: Psycholinguistic
and computational perspectives, pages 105?121. MIT
Press, Cambridge, MA.
Robert Daland and Janet B. Pierrehumbert. 2011. Learn-
ing diphone-based segmentation. Cognitive Science,
35(1):119?155.
Laura C. Dilley and Mark Pitt. 2010. Altering context
speech rate can cause words to appear or disappear.
Psychological Science, 21(11):1664?1670.
Markus Dreyer, Jason R. Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions with
finite-state methods. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 1080?1089, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Emmanuel Dupoux, Guillaume Beraud-Sudreau, and
Shigeki Sagayama. 2011. Templatic features for mod-
eling phoneme acquisition. In Proceedings of the 33rd
Annual Cognitive Science Society.
Micha Elsner, Sharon Goldwater, and Jacob Eisenstein.
2012. Bootstrapping a unified model of lexical and pho-
netic acquisition. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 184?193, Jeju
Island, Korea, July. Association for Computational Lin-
guistics.
Naomi Feldman, Thomas Griffiths, and James Morgan.
2009. Learning phonetic categories by learning a lexi-
con. In Proceedings of the 31st Annual Conference of
the Cognitive Science Society.
Naomi H. Feldman, Emily B. Myers, Katherine S. White,
Thomas L. Griffiths, and James L. Morgan. 2013.
Word-level information influences phonetic learning
in adults and infants. Cognition, 127(3):427?438.
Naomi H. Feldman, Thomas L. Griffiths, Sharon Gold-
water, and James L. Morgan. in press. A role for the
developing lexicon in phonetic category acquisition.
Psychological Review.
Margaret M. Fleck. 2008. Lexicalized phonotactic word
segmentation. In Proceedings of ACL-08: HLT, pages
130?138, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Michael C. Frank, Sharon Goldwater, Thomas L. Griffiths,
and Joshua B. Tenenbaum. 2010. Modeling human per-
formance in statistical word segmentation. Cognition,
117(2):107?125.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21?54.
Rebecca Go?mez and Jessica Maye. 2005. The develop-
mental trajectory of nonadjacent dependency learning.
Infancy, 7:183?206.
Bruce Hayes and Colin Wilson. 2008. A maximum en-
tropy model of phonotactics and phonotactic learning.
Linguistic Inquiry, 39(3):379?440.
Marieke van Heugten and Elizabeth K. Johnson. in press.
Learning to contend with accents in infancy: Benefits
of brief speaker exposure. Journal of Experimental
Psychology: General.
Derek M. Houston and Peter W. Jusczyk. 2000. The role
of talker-specific information in word segmentation by
infants. Journal of Experimental Psychology: Human
Perception and Performance, 26:1570?1582.
Jonathan Huggins and Frank Wood. 2013. Infinite struc-
tured hidden semi-Markov models. Transactions on
Pattern Analysis and Machine Intelligence (TPAMI), to
appear, September.
Aren Jansen, Emmanuel Dupoux, Sharon Goldwater,
Mark Johnson, Sanjeev Khudanpur, Kenneth Church,
Naomi Feldman, Hynek Hermansky, Florian Metze,
Richard Rose, Mike Seltzer, Pascal Clark, Ian McGraw,
Balakrishnan Varadarajan, Erin Bennett, Benjamin
Borschinger, Justin Chiu, Ewan Dunbar, Abdellah Four-
tassi, David Harwath, Chia-ying Lee, Keith Levin,
Atta Norouzian, Vijay Peddinti, Rachael Richardson,
Thomas Schatz, and Samuel Thomas. 2013. A sum-
mary of the 2012 JHU CLSP workshop on zero re-
source speech technologies and early language acqui-
sition. Proceedings of the IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing.
52
Mark Johnson. 2008. Using adaptor grammars to identify
synergies in the unsupervised acquisition of linguis-
tic structure. In Proceedings of ACL-08: HLT, pages
398?406, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Peter W. Jusczyk and Richard N. Aslin. 1995. Infants? de-
tection of the sound patterns of words in fluent speech.
Cognitive Psychology, 29:1?23.
Peter W. Jusczyk, Derek M. Houston, and Mary Newsome.
1999. The beginnings of word segmentation in English-
learning infants. Cognitive Psychology, 39:159?207.
Dahee Kim, Joseph D.W. Stephens, and Mark A. Pitt.
2012. How does context play a part in splitting words
apart? Production and perception of word boundaries
in casual speech. Journal of Memory and Language,
66(4):509 ? 529.
Patricia K. Kuhl, Karen A. Williams, Francisco Lacerda,
Kenneth N. Stevens, and Bjorn Lindblom. 1992. Lin-
guistic experience alters phonetic perception in infants
by 6 months of age. Science, 255(5044):606?608.
Leigh Lisker and Arthur S. Abramson. 1964. A cross-
language study of voicing in initial stops: Acoustical
measurements. Word, 20:384?422.
Andrew Martin, Sharon Peperkamp, and Emmanuel
Dupoux. 2013. Learning phonemes with a proto-
lexicon. Cognitive Science, 37:103?124.
Sven L. Mattys and Peter W. Jusczyk. 2001. Do infants
segment words or recurring contiguous patterns? Jour-
nal of Experimental Psychology: Human Perception
and Performance, 27(3):644?655+.
Jessica Maye, Janet F. Werker, and LouAnn Gerken. 2002.
Infant sensitivity to distributional information can affect
phonetic discrimination. Cognition, 82(3):B101?11.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested pitman-yor language modeling. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
100?108, Suntec, Singapore, August. Association for
Computational Linguistics.
Mehryar Mohri, 2004. Weighted Finite-State Transducer
Algorithms: An Overview, chapter 29, pages 551?564.
Physica-Verlag.
Thierry Nazzi, Laura C. Dilley, Ann Marie Jusczyk, Ste-
fanie Shattuck-Hufnagel, and Peter W. Jusczyk. 2005.
English-learning infants? segmentation of verbs from
fluent speech. Language and Speech, 48(3):279?298+.
Graham Neubig, Masato Mimura, Shinsuke Mori, and
Tatsuya Kawahara. 2010. Learning a language model
from continuous speech. In 11th Annual Conference
of the International Speech Communication Associa-
tion (InterSpeech 2010), pages 1053?1056, Makuhari,
Japan, 9.
Sharon Peperkamp, Rozenn Le Calvez, Jean-Pierre Nadal,
and Emmanuel Dupoux. 2006. The acquisition of
allophonic rules: Statistical learning with linguistic
constraints. Cognition, 101(3):B31?B41.
Ann M. Peters. 1983. The Units of Language Acquisi-
tion. Cambridge Monographs and Texts in Applied
Psycholinguistics. Cambridge University Press.
Gordon E. Peterson and Harold L. Barney. 1952. Control
methods used in a study of the vowels. Journal of the
Acoustical Society of America, 24(2):175?184.
Mark A. Pitt, Laura Dilley, Keith Johnson, Scott Kies-
ling, William Raymond, Elizabeth Hume, and Eric
Fosler-Lussier. 2007. Buckeye corpus of conversa-
tional speech (2nd release).
Kim Plunkett. 2005. Learning how to be flexible with
words. Attention and Performance, XXI:233?248.
Anton Rytting. 2007. Preserving Subsegmental Varia-
tion in Modeling Word Segmentation (Or, the Raising
of Baby Mondegreen). Ph.D. thesis, The Ohio State
University.
Amanda Seidl and Elizabeth Johnson. 2006. Infant word
segmentation revisited: Edge alignment facilitates tar-
get extraction. Developmental Science, 9:565?573.
Amanda Seidl and Elizabeth Johnson. 2008. Perceptual
factors influence infants? extraction of onsetless words
from continuous speech. Journal of Child Language,
34.
Leher Singh, James Morgan, and Katherine White. 2004.
Preference and processing: The role of speech affect
in early spoken word recognition. Journal of Memory
and Language, 51:173?189.
Daniel Swingley. 2005. Statistical clustering and the con-
tents of the infant vocabulary. Cognitive Psychology,
50:86?132.
Daniel Swingley. 2009. Contributions of infant word
learning to language development. Philosophical
Transactions of the Royal Society B: Biological Sci-
ences, 364(1536):3617?3632, December.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Ameri-
can Statistical Association, 101(476):1566?1581.
Michael Tomasello. 2000. The item-based nature of chil-
dren?s early syntactic development. Trends in Cognitive
Sciences, 4(4):156 ? 163.
Gautam K. Vallabha, James L. McClelland, Ferran Pons,
Janet F. Werker, and Shigeaki Amano. 2007. Unsuper-
vised learning of vowel categories from infant-directed
speech. Proceedings of the National Academy of Sci-
ences, 104(33):13273?13278.
Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and
Zoubin Ghahramani. 2008. Beam sampling for the
infinite Hidden Markov model. In Proceedings of the
25th International Conference on Machine learning,
53
ICML ?08, pages 1088?1095, New York, NY, USA.
ACM.
Balakrishnan Varadarajan, Sanjeev Khudanpur, and Em-
manuel Dupoux. 2008. Unsupervised learning of
acoustic sub-word units. In Proceedings of the As-
sociation for Computational Linguistics: Short Papers,
pages 165?168.
Anand Venkataraman. 2001. A statistical model for word
discovery in transcribed speech. Computational Lin-
guistics, 27(3):351?372.
Janet F. Werker and Richard C. Tees. 1984. Cross-
language speech perception: Evidence for perceptual
reorganization during the first year of life. Infant Be-
havior and Development, 7(1):49 ? 63.
Katherine S. White and Richard N. Aslin. 2011. Adap-
tation to novel accents by toddlers. Developmental
Science, 14(2):372?384.
54
Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 63?70,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
Quantifying the role of discourse topicality
in speakers? choices of referring expressions
Naho Orita
Department of Linguistics
University of Maryland
naho@umd.edu
Eliana Vornov
Departments of Computer Science and Linguistics
University of Maryland
evornov@umd.edu
Naomi H. Feldman
Department of Linguistics
University of Maryland
nhf@umd.edu
Jordan Boyd-Graber
College of Information Studies and UMIACS
University of Maryland
jbg@umiacs.umd.edu
Abstract
The salience of an entity in the discourse
is correlated with the type of referring ex-
pression that speakers use to refer to that
entity. Speakers tend to use pronouns to
refer to salient entities, whereas they use
lexical noun phrases to refer to less salient
entities. We propose a novel approach to
formalize the interaction between salience
and choices of referring expressions us-
ing topic modeling, focusing specifically
on the notion of topicality. We show that
topic models can capture the observation
that topical referents are more likely to be
pronominalized. This lends support to the-
ories of discourse salience that appeal to
latent topic representations and suggests
that topic models can capture aspects of
speakers? cognitive representations of en-
tities in the discourse.
1 Introduction
Speakers? choices of referring expressions (pro-
nouns, demonstratives, full names, and so on) have
been used as a tool to understand cognitive rep-
resentations of entities in a discourse. Many re-
searchers have proposed a correlation between the
type of a referring form and saliency (or accessi-
bility, prominence, focus) of the entity in the dis-
course (Chafe, 1976; Gundel et al., 1993; Bren-
nan, 1995; Ariel, 1990). Because a pronoun car-
ries less information compared to more specified
forms (e.g., she vs. Hillary Clinton), theories pre-
dict that speakers tend to use pronouns when they
think that a referent is sufficiently salient in the
discourse. When the referent is less salient, more
specified forms are used. In other words, the like-
lihood of pronominalization increases as referents
become more salient.
Topic modeling (Blei et al., 2003; Griffiths et
al., 2007) uses a probabilistic model that recovers
a latent topic representation from observed words
in a document. The model assumes that words ap-
pearing in documents have been generated from a
mixture of latent topics. These latent topics have
been argued to provide a coarse semantic repre-
sentation of documents and to be in close corre-
spondence with many aspects of human seman-
tic cognition (Griffiths et al., 2007). This previ-
ous work has focused on semantic relationships
among words and documents. While it is often
assumed that the topics extracted by topic models
correspond to the gist of a document, and although
topic models have been used to capture discourse-
level properties in some settings (Nguyen et al.,
2013), the ability of topic models to capture cogni-
tive aspects of speakers? discourse representations
has not yet been tested.
In this paper we use topic modeling to formal-
ize the idea of salience in the discourse. We fo-
cus specifically on the idea of topicality as a pre-
dictor of salience (Ariel, 1990; Arnold, 1998) and
ask whether the latent topics that are recovered by
topic models can predict speakers? choices of re-
ferring expressions. Simulations show that the ref-
erents of pronouns belong, on average, to higher
probability topics than the referents of full noun
phrases, indicating that topical referents are more
likely to be pronominalized. This suggests that
63
the information recovered by topic models is rele-
vant to speakers? choices of referring expressions
and that topic models can provide a useful tool for
quantifying speakers? representations of entities in
the discourse.
The structure of this paper is as follows. Sec-
tion 2 briefly reviews studies that look at the cor-
relation between saliency and choices of refer-
ring expression, focusing on topicality, and intro-
duces our approach to this problem. Section 3 de-
scribes a model that learns a latent topic distribu-
tion and formalizes the notion of topicality within
this framework. Section 4 describes the data we
used for our simulation. Section 5 shows simula-
tion results. Section 6 discusses implications and
future directions.
2 Saliency and referring expressions
Various factors have been proposed to influence
referent salience (Arnold, 1998; Arnold, 2010).
These factors include giveness (Chafe, 1976; Gun-
del et al., 1993), grammatical position (Bren-
nan, 1995; Stevenson et al., 1994), order of men-
tion (J?arvikivi et al., 2005; Kaiser and Trueswell,
2008), recency (Giv?on, 1983; Arnold, 1998), syn-
tactic focus and syntactic topic (Cowles et al.,
2007; Foraker and McElree, 2007; Walker et al.,
1994), parallelism (Chambers and Smyth, 1998;
Arnold, 1998), thematic role (Stevenson et al.,
1994; Arnold, 2001; Rohde et al., 2007), coher-
ence relation (Kehler, 2002; Rohde et al., 2007)
and topicality (Ariel, 1990; Arnold, 1998; Arnold,
1999). Psycholinguistic experiments (Arnold,
1998; Arnold, 2001; Kaiser, 2006) show that de-
termining the salient referent is a complex process
which is affected by various sources of informa-
tion, and that these multiple factors have different
strengths of influence.
Among the numerous factors influencing the
salience of a referent, this study focuses on top-
icality. In contrast to surface-level factors such
as grammatical position, order of mention, and re-
cency, the representation of topicality is latent and
requires inference. Because of this latent repre-
sentation, it has been challenging to investigate the
role of topicality in discourse.
Many researchers have observed that there is a
correlation between a linguistic category ?topic?
and referent salience and have suggested that top-
ical referents are more likely to be pronominal-
ized (Ariel, 1990; Dahl and Fraurud, 1996). How-
ever, Arnold (2010) points out that examining the
relation between topicality and choices of refer-
ring expressions is difficult for two reasons. First,
identifying the topic is known to be hard. Arnold
(2010) shows that it is hard to determine what the
topic is even in a simple sentence like Andy brews
beer (Is the topic Andy, beer, or brewing?). Sec-
ond, researchers have defined the notion of ?topic?
differently as follows.
? The topic is often defined as what the sen-
tence is about (Reinhart, 1981).
? The topic can be defined as prominent
characters such as the protagonist (Francik,
1985).
? The topic is often associated with old infor-
mation (Gundel et al., 1993).
? The subject position is considered to be a top-
ical position (Chafe, 1976).
? Repeated mentions are topical (Kameyama,
1994).
? Psycholinguistic experiments define a dis-
course topic as a referent that has already
been mentioned in the preceding discourse
as a pronoun/the topic of a cleft (Arnold,
1999) or realized in subject position (Cowles,
2003).
? Centering theory (Grosz et al., 1995; Bren-
nan, 1995) formalizes the topic as a
backward-looking center that is a single en-
tity mentioned in the last sentence and in the
most salient grammatical position (the gram-
matical subject is the most salient, and fol-
lowed by the object and oblique object).
? Giv?on (1983) suggests that all discourse enti-
ties are topical but that topicality is defined by
a gradient/continuous property. Giv?on shows
that three measures of topicality ? recency
(the distance between the referent and the
referring expression), persistence (how long
the referent would remain in the subsequent
discourse), and potential interference (how
many other potential referents of the refer-
ring expression there are in the preceding dis-
course) ? correlate with the types of reference
expressions. Note that these scales measure
topicality of the referring expression, but not
the referent per se.
The variation in the literature seems to de-
rive from three fundamental properties. First, as
Arnold (2010) pointed out, there is variation in the
64
linguistic unit that bears the topic. For example,
Reinhart (1981) defines each sentence as having
a single topic, whereas Giv?on (1983) defines each
entity as having a single topic. Second, there is a
variation in type of variable. For example, Giv?on
(1983) defines topicality as a continuous property,
whereas Centering seems to treat topicality as cat-
egorical based on the grammatical position of the
referent. Third, many studies define ?topic? as a
combination of surface linguistic factors such as
grammatical position and recency. When topical-
ity is defined in terms of meaning, as in Reinhart
(1981), we face difficulty in identifying what the
topic is, as summarized in Arnold (1998). None of
the existing definitions/measures seem to provide
a way to capture latent topic representations, and
this makes it challenging to investigate their role in
discourse representations. It is this idea of latent
topic representations that we aim to formalize.
Our study investigates whether topic modeling
(Blei et al., 2003; Griffiths et al., 2007) can be
used to formalize the relationship between topi-
cality and choices of referring expressions. Be-
cause of their structured representations, consist-
ing of a set of topics as well as information about
which words belong to those topics, topic models
are able to capture topicality by means of semantic
associations. For example, observing a word Clin-
ton increases the topicality of other words associ-
ated with the topic that Clinton belongs to, e.g.,
president, Washington and so on. In other words,
topic models can capture not only the salience of
referents within a document, but also the salience
of referents via the structured topic representation
learned from multiple texts.
We use topic modeling to verify the prevailing
hypothesis that topical referents are more likely to
be pronominalized than lexical nouns. Examin-
ing the relationship between topicality and refer-
ring expressions using topic modeling provides an
opportunity to test how well the representation re-
covered by topic models corresponds to the cogni-
tive representation of entities in a discourse. If we
can recover the observation that topical referents
are more likely to be pronominalized than more
specified forms, this could indicate that topic mod-
els can capture not only aspects of human seman-
tic cognition (Griffiths et al., 2007), but also as-
pects of a higher level of linguistic representation,
discourse.
3 Model
3.1 Recovering latent topics
We formalize topicality of referents using topic
modeling. Each document is represented as a
probability distribution over topics. Each topic is
represented as a probability distribution over pos-
sible referents in the corpus. In training our topic
model, we assume that all lexical nouns in the dis-
course are potential referents. The topic model is
trained only on lexical nouns, excluding all other
words. This ensures that the latent topics capture
information about which referents typically occur
together in documents.
1
Rather than pre-specifying a number of latent
topics, we use the hierarchical Dirichlet process
(Teh et al., 2006), which learns a number of topics
to flexibly represent input data. The summary of
the generative process is as follows.
1. Draw a global topic distribution
G
0
? DP(?,H) (where ? is a hyperparame-
ter and H is a base distribution).
2. For each document d ? {1, . . . , D} (where
D denotes the number of documents in the
corpus),
(a) draw a document-topic distribution
G
d
? DP(?
0
, G
0
) (where ?
0
is a hyper-
parameter).
(b) For each referent r ? {1, . . . , N
d
}
(where N
d
denotes the number of refer-
ents in document d),
i. draw a topic parameter ?
d,r
? G
d
.
ii. draw a word x
d,r
? Mult(?
d,r
).
This process generates a distribution over topics
for each document, a distribution over referents for
each topic, and a topic assignment for each refer-
ent. The distribution over topics for each docu-
ment represents what the topics of the document
are. The distribution over referents for each topic
represents what the topic is about. An illustra-
tion of this representation is in Table 3.1. Top-
ics and words that appear in the second and third
columns are ordered from highest to lowest. We
can represent topicality of the referents using this
1
Excluding pronouns from the training set introduces a
confound, because it artificially lowers the probability of the
topics corresponding to those pronouns. However, in this pa-
per our predicted effect goes in the opposite direction: we
predict that topics corresponding to the referents of pronouns
will have higher probability than those corresponding to the
referents of lexical nouns. Excluding pronouns thus makes us
less likely to find support for our hypothesis.
65
probabilistic latent topic representation, measur-
ing which topics have high probability and assum-
ing that referents associated with high probability
topics are likely to be topical in the discourse.
Word Top 3 topic IDs Associated words in the 1st topic
Clinton 5, 26, 61 president, meeting, peace,
Washington, talks
FBI 148, 73, 67 Leung, charges, Katrina,
documents, indictment
oil 91, 145, 140 Burmah, Iraq, SHV, coda,
pipeline
Table 1: Illustration of the topic distribution
Given this generative process, we can use
Bayesian inference to recover the latent topic dis-
tribution. We use the Gibbs sampling algorithm
in Teh et al. (2006) to estimate the conditional
distribution of the latent structure, the distribu-
tions over topics associated with each document,
and the distributions over words associated with
each topic. The state space consists of latent vari-
ables for topic assignments, which we refer to as
z = {z
d,r
}. In each iteration we compute the con-
ditional distribution p(z
d,r
|x, z
?d,r
, ?), where the
subscript ?d, r denotes counts without consider-
ing z
d,r
and ? denotes all hyperparameters. Recov-
ering these latent variables allows us to determine
what the topic of the referent is and how likely that
topic is in a particular document. We use the latent
topic and its probability to represent topicality.
3.2 A measure of topicality
Discourse theories predict that topical referents
are more likely to be pronominalized than more
specified expressions.
2
We can quantify the effect
of topicality on choices of referring expressions
by comparing the topicality of the referents of two
types of referring expressions, pronouns and lexi-
cal nouns. If topical words are more likely to be
pronominalized, then the topicality of the referents
of pronouns should be higher than the topicality of
the referents of lexical nouns.
Annotated coreference chains in the corpus, de-
scribed below, are used to determine the referent
of each referring expression. We look at the topic
assigned to each referent r in document d by the
topic model, z
d,r
. We take the log probability
2
Although theories make more fine-grained predictions
on the choices of referring expressions with respect to
saliency, e.g., a full name is used to refer to less salient entity
compared to a definite description (c.f. accessibility mark-
ing scale in Ariel 1990), we focus here on the coarse contrast
between pronouns and lexical nouns.
of this topic within the document, log p(z
d,r
|G
d
),
as a measure of the topicality of the referent.
We take the expectation over a uniform distri-
bution of referents, where the uniform distribu-
tions are denoted u(lex) and u(pro), to obtain
an estimate of the average topicality of the ref-
erents of lexical nouns, E
u(lex)
[log p(z
d,r
|G
d
)],
and the average topicality of the referents of pro-
nouns, E
u(pro)
[log p(z
d,r
|G
d
)], within each docu-
ment. The expectation for the referents of the pro-
nouns in a document is computed as
E
u(pro)
[log p(z
d,r
|G
d
)] =
N
d,pro
?
r=1
log p(z
d,r
|G
d
)
N
d,pro
(1)
where N
d,pro
denotes the number of pronouns in
a document d. Replacing N
d,pro
with N
d,lex
(the
number of lexical nouns in a document d) gives us
the expectation for the referents of lexical nouns.
To obtain a single measure for each document of
the extent to which our measure of topicality pre-
dicts speakers? choices of referring expressions,
we subtract the average topicality for the referents
of lexical nouns from the average topicality for the
referents of pronouns within the document to ob-
tain a log likelihood ratio q
d
,
q
d
= E
u(pro)
[log p(z
d,r
|G
d
)]?E
u(lex)
[log p(z
d,r
|G
d
)]
(2)
A value of q
d
greater than zero indicates that the
referents of pronouns are more likely to be topical
than the referents of lexical nouns.
4 Annotated coreference data
Our simulations use a training set of the Ontonotes
corpus (Pradhan et al., 2007), which consists of
news texts. We use these data because each entity
in the corpus has a coreference annotation. We use
the coreference annotations in our evaluation, de-
scribed above. The training set in the corpus con-
sists of 229 documents, which contain 3,648 sen-
tences and 79,060 word tokens. We extract only
lexical nouns (23,084 tokens) and pronouns (2,867
tokens) from the corpus as input to the model.
3
Some preprocessing is necessary before using
these data as input to a topic model. This necessity
arises because some entities in the corpus are rep-
resented as phrases, such as in (1a) and (1b) below,
3
In particular, we extracted words that are tagged as NN,
NNS, NNP, NNPS, and for pronouns as PRP, PRP$.
66
where numbers following each expression repre-
sent the entity ID that is assigned to this expression
in the annotated corpus. However, topic models
use bag-of-words representations and therefore as-
sign latent topic structure only to individual words,
and not to entire phrases. We preprocessed these
entities as in (2). This enabled us to attribute entity
IDs to individual words, rather than entire phrases,
allowing us to establish a correspondence between
these ID numbers and the latent topics recovered
by our model for the same words.
1. Before preprocessing
(a) a tradition in Betsy?s family: 352
(b) Betsy?s family: 348
(c) Betsy: 184
2. After preprocessing
(a) tradition: 352
(b) family: 348
(c) Betsy: 184
Annotated coreference chains in the corpus were
used to determine the referent of each pronoun
and lexical noun. The annotations group all re-
ferring expressions in a document that refer to the
same entity together into one coreference chain,
with the order of expressions in the chain corre-
sponding to the order in which they appear in the
document. We assume that the referent for each
pronoun and lexical noun appears in its corefer-
ence chain. We further assume that the referent
needs to be a lexical noun, and thus exclude all
pronouns from consideration as referents. If a lex-
ical noun does not have any other words before it
in the coreference chain, i.e., that noun is the first
or the only word in that coreference chain, we as-
sume that this noun refers to itself (the noun itself
is the referent). Otherwise, if a coreference chain
has multiple referents, we take its referent to be
the lexical noun that is before and closest to the
target word.
5 Results
To recover the latent topic distribution, we ran 5
independent Gibbs sampling chains for 1000 iter-
ations.
4
Hyperparameters ?, ?
0
, and ? were fixed
at 1.0, 1.0, and 0.01, respectively.
5
The model re-
4
We used a Python version of the hierarchical Dirichlet
process implemented by Ke Zhai (http://github.com/
kzhai/PyNPB/tree/master/src/hdp).
5
Parameter ? controls how likely a new topic is to be cre-
ated in the corpus. If the value of ? is high, more topics are
covered an average of 161 topics (range: 160?163
topics).
We computed the log likelihood ratio q
d
(Equa-
tion 2) for each document and took the average of
this value across documents for each chain. The
formula to compute this average is as follows.
For each chain g,
1. get the final sample s in g.
2. For each document d in the corpus,
i. compute q
d
based on s.
3. Compute the average of all q
d
in the cor-
pus.
The average log likelihood ratio in each chain con-
sistently shows values greater than zero across
the 5 chains. The average log likelihood ratio
across chains is 1.0625 with standard deviation
0.7329. As an example, in one chain, the aver-
age of the expected values for the referents of pro-
nouns across documents is?1.1849 with standard
deviation 0.8796. In the same chain, the average
of the expected values for the referents of lexical
nouns across documents is?2.2356 with standard
deviation 0.5009.
We used the median test
6
to evaluate whether
the two groups of the referents are different with
respect to the expected values of the log probabil-
ities of topics. The test shows a significant differ-
ence between two groups (p < 0.0001).
We also computed the probability density p(q)
from the log likelihood ratio q
d
for each docu-
ment using the final samples from each chain.
Graph 1 shows the probability density p(q) from
each chain. The peak after zero confirms the ob-
served effect.
Table 2 shows examples of target pronouns and
lexical nouns, their referents, and the topic as-
signed to each referent from a document. Table 3
shows the distribution over topics in the document
obtained from one chain. Topics in Table 3 are
ordered from highest to lowest. Only four topics
were present in this document. The list of referents
associated with each topic in Table 3 is recovered
from the topic distribution over referents. This list
shows what the topic is about.
discovered in the corpus. Parameter ?
0
controls the sparse-
ness of the distribution over topics in a document, and param-
eter ? controls the sparseness of the distribution over words
in a topic.
6
The median test compares medians to test group differ-
ences (Siegel, 1956).
67
Topic ID Assciated words Probability
1 Milosevic, Kostunica, Slobodan, president, Belgrade, Serbia, Vojislav, Yugoslavia, crimes, parliament 0.64
2 president, Clinton, meeting, peace, Washington, talks, visit, negotiators, region, . . . , Alabanians 0.16
3 people, years, U.S., president, time, government, today, country, world, way, year 0.16
4 government, minister, party, Barak, today, prime, east, parliament, leader, opposition, peace, leadership 0.04
Table 3: The document-topic distribution
0.0
0.2
0.4
?1 0 1 2 3q
prob
abilit
y den
sity p
(q) Gibbs chain IDchain.01chain.02chain.03chain.04chain.05
Figure 1: The probability density of p(q)
Target Referent Referent?s Topic ID
his Spilanovic 1
he Spilanovic 1
its Belgrade 1
Goran Minister 4
Albanians Albanians 2
Kosovo Kosovo 1
Table 2: Target words, their corresponding refer-
ents, and the assigned topics of the referents
The topics associated with the pronouns his,
he and its have the highest probability in the
document-topic distribution, as shown in Table 3.
In contrast, although the topic associated with
the word Kosovo has the highest probability in
the document-topic distribution, the topics asso-
ciated with nouns Goran and Albanians do not
have high probability in the document-topic dis-
tribution. This is an example from one document,
but this tendency is observed in most of the docu-
ments in the corpus.
These results indicate that the referents of pro-
nouns are more topical than the referents of lexi-
cal nouns using our measure of topicality derived
from the topic model. This suggests that our mea-
sure of topicality captures aspects of salience that
influence choices of referring expressions.
However, there is a possibility that the effect
we observed is simply derived from referent fre-
quencies and that topic modeling structure does
not play a role beyond this. Tily and Piantadosi
(2009) found that the frequency of referents has a
significant effect on predicting the upcoming ref-
erent. Although their finding is about comprehen-
der?s ability to predict the upcoming referent (not
the type of referring expression), we conducted
an additional analysis to rule out the possibility
that referent frequencies alone were driving our re-
sults.
In order to quantify the effect of referent fre-
quency on choices of referring expressions, we
computed the same log likelihood ratio q
d
with
referent probabilities. The probability of a refer-
ent in a document was computed as follows:
p(r
i
|doc
d
) =
C
d,r
i
C
d,?
(3)
where C
d,r
i
denotes the number of mentions that
refer to referent r
i
in document d and C
d,?
denotes
the total number of mentions in document d. We
can directly compute this value by using the anno-
tated coreference chains in the corpus.
The log likelihood ratio for this measure is
2.3562. The average of the expected values for
the referents of pronouns across documents is
?1.1993 with standard deviation 0.6812. The av-
erage of the expected values for the referents of
lexical nouns across documents is ?3.5556 with
standard deviation 0.9742. The median test shows
a significant difference between two groups. (p <
0.0001). These results indicate that the frequency
of a referent captures aspects of its salience that
influence choices of referring expressions, raising
the question of whether our latent topic represen-
tations capture something that simple referent fre-
quencies do not.
In order to examine to what extent the relation-
ship between topicality and referring expressions
captures information that goes beyond simple ref-
erent frequencies, we compare two logistic regres-
68
sion models.
7
Both models are built to predict
whether a referent will be a full noun phrase or a
pronoun. The first model incorporates only the log
probability of the referent as a predictor, whereas
the second includes both the log probability of the
referent and our topicality measure as predictors.
8
The null hypothesis is that removing our topi-
cality measure from the second model makes no
difference for predicting the types of referring ex-
pressions. Under this null hypothesis, twice the
difference in the log likelihoods between the two
models should follow a ?
2
(1) distribution. We
find a significant difference in likelihood between
these two models (?
2
(1) = 118.38, p < 0.0001),
indicating that the latent measure of topicality de-
rived from the topic model predicts aspects of lis-
teners? choices of referring expressions that are
not predicted by the probabilities of individual ref-
erents.
6 Discussion
In this study we formalized the correlation be-
tween topicality and choices of referring expres-
sions using a latent topic representation obtained
through topic modeling. Both quantitative and
qualitative results showed that according to this la-
tent topic representation, the referents of pronouns
are more likely to be topical than the referents of
lexical nouns. This suggests that topic models can
capture aspects of discourse representations that
are relevant to the selection of referring expres-
sions. We also showed that this latent topic repre-
sentation has an independent contribution beyond
simple referent frequency.
This study examined only two independent fac-
tors: topicality and referent frequency. However,
discourse studies suggest that the salience of a ref-
erent is determined by various sources of informa-
tion and multiple discourse factors with different
strengths of influence (Arnold, 2010). Our frame-
work could eventually form part of a more com-
plex model that explicitly formalizes the interac-
tion of information source and various discourse
factors. Having a formal model would help by al-
lowing us to test different hypotheses and develop
a firm theory regarding cognitive representations
of entities in the discourse.
7
Models were fit using glm in R. For the log-likelihood
ratio test, lrtest in R package epicalc was used.
8
We also ran a version of this comparison in which fre-
quency of mention was included as a predictor in both mod-
els, and obtained similar results.
One possibility for exploring the role of vari-
ous discourse factors in our framework is to use
recent advances in topic modeling. For example,
TagLDA (Zhu et al., 2006) includes part-of-speech
as part of the model, and syntactic topic models
(Boyd-Graber and Blei, 2008) incorporate syntac-
tic information. Whereas simulations in our study
only used nouns as input, it has been observed that
the thematic role of the entity influences referent
salience (Stevenson et al., 1994; Arnold, 2001;
Rohde et al., 2007). Using part-of-speech and syn-
tactic information together with the topic informa-
tion could help us approximate the influence of the
thematic role and allow us to simulate how this
factor interacts with latent topic information and
other factors.
It has been challenging to quantify the influence
of latent factors such as topicality, and the simula-
tions in this paper represent only a first step toward
capturing these challenging factors. The simula-
tions nevertheless provide an example of how for-
mal models can help us validate theories of the re-
lationship between speakers? discourse represen-
tations and the language they produce.
Acknowledgments
We thank Ke Zhai, Viet-An Nguyen, and four
anonymous reviewers for helpful comments and
discussion.
References
Mira Ariel. 1990. Accessing noun-phrase antecedents.
Routledge, London.
Jennifer Arnold. 1998. Reference form and discourse
patterns. Ph.D. thesis, Stanford University Stanford,
CA.
Jennifer Arnold. 1999. Marking salience: The simi-
larity of topic and focus. Unpublished manuscript,
University of Pennsylvania.
Jennifer Arnold. 2001. The effect of thematic roles
on pronoun use and frequency of reference continu-
ation. Discourse Processes, 31(2):137?162.
Jennifer Arnold. 2010. How speakers refer: the role of
accessibility. Language and Linguistics Compass,
4(4):187?203.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jordan L Boyd-Graber and David M Blei. 2008. Syn-
tactic topic models. In Neural Information Process-
ing Systems, pages 185?192.
69
Susan E Brennan. 1995. Centering attention in
discourse. Language and Cognitive Processes,
10(2):137?167.
Wallace Chafe. 1976. Givenness, contrastiveness, def-
initeness, subjects, topics, and point of view. In
C. N. Li, editor, Subject and Topic. Academic Press,
New York.
Craig G Chambers and Ron Smyth. 1998. Structural
parallelism and discourse coherence: A test of Cen-
tering theory. Journal of Memory and Language,
39(4):593?608.
H Wind Cowles, Matthew Walenski, and Robert Klu-
ender. 2007. Linguistic and cognitive prominence
in anaphor resolution: topic, contrastive focus and
pronouns. Topoi, 26(1):3?18.
Heidi Wind Cowles. 2003. Processing information
structure: Evidence from comprehension and pro-
duction. Ph.D. thesis, University of California, San
Diego.
Osten Dahl and Kari Fraurud. 1996. Animacy in gram-
mar and discourse. Pragmatics and Beyond New Se-
ries, pages 47?64.
Stephani Foraker and Brian McElree. 2007. The role
of prominence in pronoun resolution: Active ver-
sus passive representations. Journal of Memory and
Language, 56(3):357?383.
Ellen Palmer Francik. 1985. Referential choice and
focus of attention in narratives (discourse anaphora,
topic continuity, language production). Ph.D. thesis,
Stanford University.
Talmy Giv?on. 1983. Topic continuity in discourse: A
quantitative cross-language study, volume 3. John
Benjamins Publishing.
Thomas L Griffiths, Mark Steyvers, and Joshua B
Tenenbaum. 2007. Topics in semantic representa-
tion. Psychological Review, 114(2):211.
Barbara J Grosz, Scott Weinstein, and Aravind K Joshi.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Jeanette K Gundel, Nancy Hedberg, and Ron
Zacharski. 1993. Cognitive status and the form of
referring expressions in discourse. Language, pages
274?307.
Juhani J?arvikivi, Roger PG van Gompel, Jukka Hy?on?a,
and Raymond Bertram. 2005. Ambiguous pro-
noun resolution contrasting the first-mention and
subject-preference accounts. Psychological Sci-
ence, 16(4):260?264.
Elsi Kaiser and John C Trueswell. 2008. Interpreting
pronouns and demonstratives in Finnish: Evidence
for a form-specific approach to reference resolution.
Language and Cognitive Processes, 23(5):709?748.
Elsi Kaiser. 2006. Effects of topic and focus on
salience. In Proceedings of Sinn und Bedeutung,
volume 10, pages 139?154. Citeseer.
Megumi Kameyama. 1994. Indefeasible semantics
and defeasible pragmatics. In CWI Report CS-
R9441 and SRI Technical Note 544. Citeseer.
Andrew Kehler. 2002. Coherence, reference, and the
theory of grammar. CSLI publications, Stanford,
CA.
Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik,
Deborah A Cai, Jennifer E Midberry, and Yuanxin
Wang. 2013. Modeling topic control to detect in-
fluence in conversations using nonparametric topic
models. Machine Learning, pages 1?41.
Sameer S Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. Ontonotes: A unified relational
semantic representation. International Journal of
Semantic Computing, 1(4):405?419.
Tanya Reinhart. 1981. Pragmatics and linguistics: An
analysis of sentence topics in pragmatics and philos-
ophy I. Philosophica, 27(1):53?94.
Hannah Rohde, Andrew Kehler, and Jeffrey L Elman.
2007. Pronoun interpretation as a side effect of dis-
course coherence. In Proceedings of the 29th An-
nual Conference of the Cognitive Science Society,
pages 617?622.
Sidney Siegel. 1956. Nonparametric statistics for the
behavioral sciences. McGraw-Hill.
Rosemary J Stevenson, Rosalind A Crawley, and David
Kleinman. 1994. Thematic roles, focus and the rep-
resentation of events. Language and Cognitive Pro-
cesses, 9(4):519?548.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.
2006. Hierarchical Dirichlet processes. Journal
of the American Statistical Association, 101:1566?
1581.
Harry Tily and Steven Piantadosi. 2009. Refer effi-
ciently: Use less informative expressions for more
predictable meanings. In Proceedings of the work-
shop on the production of referring expressions:
Bridging the gap between computational and empir-
ical approaches to reference.
Marilyn Walker, Sharon Cote, and Masayo Iida. 1994.
Japanese discourse and the process of centering.
Computational Linguistics, 20(2):193?232.
Xiaojin Zhu, David Blei, and John Lafferty. 2006.
TagLDA: Bringing document structure knowledge
into topic models. Technical report, Technical Re-
port TR-1553, University of Wisconsin.
70
