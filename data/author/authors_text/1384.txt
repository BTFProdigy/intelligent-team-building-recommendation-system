Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 869?876,
Beijing, August 2010
Text Summarization of Turkish Texts using                   
Latent Semantic Analysis  
Makbule Gulcin Ozsoy 
Dept. of Computer Eng. 
Middle East Tech. Univ. 
e1395383@ceng.metu.edu.tr 
Ilyas Cicekli 
Dept. of Computer Eng.     
Bilkent University 
ilyas@cs.bilkent.edu.tr 
Ferda Nur Alpaslan 
Dept. of Computer Eng.   
Middle East Tech. Univ. 
alpaslan@ceng.metu.edu.tr 
 
Abstract 
Text summarization solves the problem 
of extracting important information from 
huge amount of text data. There are vari-
ous methods in the literature that aim to 
find out well-formed summaries. One of 
the most commonly used methods is the 
Latent Semantic Analysis (LSA). In this 
paper, different LSA based summariza-
tion algorithms are explained and two 
new LSA based summarization algo-
rithms are proposed. The algorithms are 
evaluated on Turkish documents, and 
their performances are compared using 
their ROUGE-L scores. One of our algo-
rithms produces the best scores. 
1 Introduction 
The exponential growth in text documents brings 
the problem of finding out whether a text docu-
ment meets the needs of a user or not. In order to 
solve this problem, text summarization systems 
which extract brief information from a given text 
are created. By just looking at the summary of a 
document, a user can decide whether the docu-
ment is of interest to him/her without looking at 
the whole document. 
The aim of a text summarization system is to 
generate a summary for a given document such 
that the generated summary contains all neces-
sary information in the text, and it does not in-
clude redundant information. Summaries can 
have different forms (Hahn and Mani, 2000). 
Extractive summarization systems collect impor-
tant sentences from the input text in order to 
generate summaries. Abstractive summarization 
systems do not collect sentences from the input 
text, but they try to capture the main concepts in 
the text, and generate new sentences to represent 
these main concepts. Abstractive summarization 
approach is similar to the way that human sum-
marizers follow. Since creating abstractive 
summaries is a more complex task, most of 
automatic text summarization systems are ex-
tractive summarization systems. 
Summarization methods can be categorized 
according to what they generate and how they 
generate it (Hovy and Lin, 1999). A summary 
can be extracted from a single document or from 
multiple documents. If a summary is generated 
from a single document, it is known as single-
document summarization. On the other hand, if a 
single summary is generated from multiple 
documents on the same subject, this is known as 
multi-document summarization. Summaries are 
also categorized as generic summaries and 
query-based summaries. Generic summarization 
systems generate summaries containing main 
topics of documents. In query-based summariza-
tion, the generated summaries contain the sen-
tences that are related to the given queries.  
Extractive summarization systems determine 
the important sentences of the text in order to 
put them into the summary. The important sen-
tences of the text are the sentences that represent 
the main topics of the text. Summarization sys-
tems use different approaches to determine the 
important sentences (Hahn and Mani, 2000; 
Hovy and Lin, 1999). Some of them look surface 
clues such as the position of the sentence and the 
words that are contained in the sentence. Some 
summarization systems use more semantic ori-
ented analysis such as lexical chains in order to 
determine the important sentences. Lately, an 
algebraic method known as Latent Semantic 
Analysis (LSA) is used in the determination of 
869
the important sentences, and successful results 
are obtained (Gong and Liu, 2001).  
In this paper, we present a generic extractive 
Turkish text summarization system based on 
LSA. We applied the known text summarization 
approaches based on LSA in order to extract the 
summaries of Turkish texts. One of the main 
contributions of this paper is the introduction of 
two new summarization methods based on LSA. 
One of our methods produced much better re-
sults than the results of the other known methods.  
The rest of the paper is organized as follows. 
Section 2 presents the related work in summari-
zation. Section 3 explains the LSA approach in 
detail. Then, the existing algorithms that use dif-
ferent LSA approaches are presented (Gong and 
Liu, 2001; Steinberger and Jezek 2004; Murray 
et al, 2005), and two new algorithms are pro-
posed in Section 4. Section 5 presents the 
evaluation results of these algorithms, and Sec-
tion 6 presents the concluding remarks. 
2 Related Work 
Text summarization is an active research area 
of natural language processing. Its aim is to ex-
tract short representative information from input 
documents. Since the 1950s, various methods 
are proposed and evaluated. The first studies 
conducted on text summaries use simple features 
like terms from keywords/key phrases, terms 
from user queries, frequency of words, and posi-
tion of words/sentences (Luhn, 1958).  
The use of statistical methods is another ap-
proach used for summary extraction. The most 
well known project that uses statistical approach 
is the SUMMARIST (Hovy and Lin, 1999). In 
this project, natural language processing meth-
ods are used together with the concept relevance 
information. The concept relevance information 
is extracted from dictionaries and WordNet.  
Text connectivity is another approach used for 
summarization. The most well-known algorithm 
that uses text connectivity is the lexical chains 
method (Barzilay and Elhadad, 1997; Ercan and 
Cicekli, 2008). In lexical chains method, Word-
Net and dictionaries are used to determine se-
mantic relations between words where semanti-
cally related words construct lexical chains. 
Lexical chains are used in the determination of 
the important sentences of the text. 
TextRank (Mihalcea and Tarau, 2004) is a 
summarization algorithm which is based on 
graphs, where nodes are sentences and edges 
represent similarity between sentences. The 
similarity value is decided by using the overlap-
ping terms. Cluster Lexrank (Qazvinian and 
Radev, 2008) is another graph-based summariza-
tion algorithm, and it tries to find important sen-
tences in a graph in which nodes are sentences 
and edges are similarities.  
In recent years, algebraic methods are used 
for text summarization. Most well-known alge-
braic algorithm is Latent Semantic Analysis 
(LSA) (Landauer et al, 1998). This algorithm 
finds similarity of sentences and similarity of 
words using an algebraic method, namely Singu-
lar Value Decomposition (SVD). Besides text 
summarization, the LSA algorithm is also used 
for document clustering and information filter-
ing. 
3 Latent Semantic Analysis 
Latent Semantic Analysis (LSA) is an algebraic-
statistical method that extracts meaning of words 
and similarity of sentences using the information 
about the usage of the words in the context. It 
keeps information about which words are used 
in a sentence, while preserving information of 
common words among sentences. The more 
common words between sentences mean that 
those sentences are more semantically related. 
LSA method can represent the meaning of 
words and the meaning of sentences simultane-
ously. It averages the meaning of words that a 
sentence contains to find out the meaning of that 
sentence. It represents the meaning of words by 
averaging the meaning of sentences that contain 
this word. 
LSA method uses Singular Value Decomposi-
tion (SVD) for finding out semantically similar 
words and sentences. SVD is a method that 
models relationships among words and sen-
tences. It has the capability of noise reduction, 
which leads to an improvement in accuracy.  
LSA has three main limitations. The first limi-
tation is that it uses only the information in the 
input text, and it does not use the information of 
world knowledge. The second limitation is that it 
does not use the information of word order, syn-
tactic relations, or morphologies. Such informa-
tion is used for finding out the meaning of words 
870
and texts. The third limitation is that the per-
formance of the algorithm decreases with large 
and inhomogeneous data. The decrease in per-
formance is observed since SVD which is a very 
complex algorithm is used for finding out the 
similarities.  
All summarization methods based on LSA use 
three main steps. These steps are as follows: 
1. Input Matrix Creation: A matrix which 
represents the input text is created. The col-
umns of the matrix represent the sentences of 
the input text and the rows represent the 
words. The cells are filled out to represent the 
importance of words in sentences using dif-
ferent approaches, whose details are de-
scribed in the rest of this section. The created 
matrix is sparse.  
2. Singular Value Decomposition (SVD): Singu-
lar value decomposition is a mathematical 
method which models the relationships 
among terms and sentences. It decomposes 
the input matrix into three other matrices as 
follows:  
   A = U ? VT  
 where A is the input matrix with dimensions 
m x n, U is an m x n matrix which represents 
the description of the original rows of the in-
put matrix as a vector of extracted concepts, 
? is an n x n diagonal matrix containing scal-
ing values sorted in descending order, and V 
is an m x n matrix which represents the de-
scription of the original columns of input ma-
trix as a vector of the extracted concepts. 
3. Sentence Selection:  Different algorithms are 
proposed to select sentences from the input 
text for summarization using the results of 
SVD. The details of these algorithms are de-
scribed in Section 4. 
The creation of the input matrix is important 
for summarization, since it affects the resulting 
matrices of SVD. There are some ways to reduce 
the row size of the input matrix, such as elimi-
nating words seen in stop words list, or using the 
root words only. There are also different ap-
proaches to fill out the input matrix cell values, 
and each of them affects the performance of the 
summarization system differently. These ap-
proaches are as follows:  
1. Number of Occurrence: The cell is filled with 
the frequency of the word in the sentence. 
2. Binary Representation of Number of Occur-
rence: If the word is seen in the sentence, the 
cell is filled with 1; otherwise it is filled with 
0. 
3. TF-IDF (Term Frequency?Inverse Document 
Frequency): The cell is filled with TF-IDF 
value of the word. This method evaluates the 
importance of words in a sentence. The im-
portance of a word is high if it is frequent in 
the sentence, but less frequent in the docu-
ment. TF-IDF is equal to TF*IDF, and TF 
and IDF are computed as follows: 
   tf (i,j) = n(i,j)  /  ?k n(k,j) 
 where n(i,j) is the number of occurrences of 
the considered word i in sentence j, and    ?k 
n(k,j) is the sum of number of occurrences of 
all words in sentence j. 
   idf (i) = log( |D| / di) 
 where |D| is the total number of sentences in 
the input text, and di is the number of sen-
tences where the word i appears 
4. Log Entropy: The cell is filled with log-
entropy value of the word, and it is computed 
as follows. 
sum = ?j p(i,j) log2(p(i,j)) 
global(i) = 1 + (sum / log2(n)) 
local(i,j)= log2(1 + f(i,j)) 
log-entropy = global*local 
 where p(i,j) is the probability of word i that is 
appeared in sentence j, f(i,j) is the number of 
times word i appeared in sentence j, and n is 
the number of sentences in the document. 
5. Root Type: If the root type of the word is 
noun, the related cell is filled with the fre-
quency of the word in the sentence; otherwise 
the cell is filled with 0. 
6. Modified TF-IDF: First the matrix is filled 
with TF-IDF values. Then, the average TF-
IDF values in each row are calculated. If the 
value in the cell is less than or equal to the 
average value, the cell value is set to 0. This 
is our new approach which is proposed to 
eliminate the noise from the input matrix. 
871
4 Text Summarization 
The algorithms in the literature that use LSA for 
text summarization perform the first two steps of 
LSA algorithm in the same way. They differ in 
the way they fill out the input matrix cells. 
4.1 Sentence Selection Algorithms in Lit-
erature 
4.1.1. Gong & Liu (Gong and Liu, 2001) 
After performing the first two steps of the LSA 
algorithm, Gong & Liu summarization algorithm 
uses VT matrix for sentence selection. The col-
umns of VT matrix represent the sentences of the 
input matrix and the rows of it represent the 
concepts that are obtained from SVD method. 
The most important concept in the text is placed 
in the first row, and the row order indicates the 
importance of concepts. Cells of this matrix give 
information about how much the sentence is re-
lated to the given concept. A higher cell value 
means the sentence is more related to the con-
cept.  
In Gong & Liu summarization algorithm, the 
first concept is chosen, and then the sentence 
most related to this concept is chosen as a part of 
the resulting summary. Then the second concept 
is chosen, and the same step is executed. This 
repetition of choosing a concept and the sen-
tence most related to that concept is continued 
until a predefined number of sentences are ex-
tracted as a part of the summary. In Figure 1, an 
example VT matrix is given. First, the concept 
con0 is chosen, and then the sentence sent1 is 
chosen, since it has the highest cell value in that 
row. 
There are some disadvantages of this algo-
rithm, which are defined by Steinberger and 
Jezek (2004). First, the reduced dimension size 
has to be the same as the summary length. This 
approach may lead to the extraction of sentences 
from less significant concepts. Second, there 
exist some sentences that are related to the cho-
sen concept somehow, but do not have the high-
est cell value in the row of that concept. These 
kinds of sentences cannot be included in the re-
sulting summary by this algorithm. Third, all 
chosen concepts are thought to be in the same 
importance level, but some of those concepts 
may not be so important in the input text. 
 
 sent0 sent1 sent2 sent3 sent4 
con0 0,557 0,691 0,241 0,110 0,432
con1 0,345 0,674 0,742 0,212 0,567
con2 0,732 0,232 0,435 0,157 0,246
con3 0,628 0,836 0,783 0,265 0,343
Figure 1. Gong & Liu approach: From each row 
of VT matrix which represents a concept, the sen-
tence with the highest score is selected. This is 
repeated until a predefined number of sentences 
are collected. 
 
4.1.2.   Steinberger & Jezek (Steinberger and 
Jezek 2004)  
As in the Gong & Liu summarization algorithm, 
the first two steps of LSA algorithm are exe-
cuted before selecting sentences to be a part of 
the resulting summary. For sentence selection, 
both V and ? matrixes are used.  
The sentence selection step of this algorithm 
starts with the calculation of the length of each 
sentence vector which is represented by a row in 
V matrix. In order to find the length of a sen-
tence vector, only concepts whose indexes are 
less than or equal to the number of dimension in 
the new space is used. The dimension of a new 
space is given as a parameter to the algorithm. 
The concepts which are highly related to the text 
are given more importance by using the values 
in ? matrix as a multiplication parameter. If the 
dimension of the new space is n, the length of 
the sentence i is calculated as follows: 
 ?
=
?=
n
j
jjjii Vlength
1
*  
After the calculation of sentence lengths, the 
longest sentences are chosen as a part of the re-
sulting summary. In Figure 2, an example V ma-
trix is given, and the dimension of the new space 
is assumed to be 3. The lengths of the sentences 
are calculated using the first three concepts. 
Since the sentence sent2 has the highest length, 
it is extracted first as a part of the summary. 
The aim of this algorithm is to get rid of the 
disadvantages of Gong & Liu summarization 
algorithm, by choosing sentences which are re-
lated to all important concepts and at the same 
time choosing more than one sentence from an 
important topic. 
 
 
872
 con0 con1 con2 con3 length
sent0 0,846 0,334 0,231 0,210 0,432 
sent1 0,455 0,235 0,432 0,342 0,543 
sent2 0,562 0,632 0,735 0,857 0,723 
sent3 0,378 0,186 0,248 0,545 0,235 
Figure 2. Steinberger & Jezek approach: For 
each row of V matrix, the lengths of sentences 
using n concepts are calculated. The value n is 
given as an input parameter. ? matrix values are 
also used as importance parameters in the length 
calculations. 
 
 sent0 sent1 sent2 sent3 sent4 
con0 0,557 0,691 0,241 0,110 0,432
con1 0,345 0,674 0,742 0,212 0,567
con2 0,732 0,232 0,435 0,157 0,246
con3 0,628 0,836 0,783 0,265 0,343
Figure 3. Murray & Renals & Carletta ap-
proach: From each row of VT matrix, concepts, 
one or more sentences with the higher scores are 
selected. The number of sentences to be selected 
is decided by using ? matrix. 
4.1.3.   Murray & Renals & Carletta (Murray 
et al, 2005)  
The first two steps of the LSA algorithm are 
executed, as in the previous algorithms before 
the construction of the summary. VT and ? ma-
trices are used for sentence selection. 
In this approach, one or more sentences are 
collected from the topmost concepts in VT ma-
trix. The number of sentences to be selected de-
pends on the values in the ? matrix. The number 
of sentences to be collected for each topic is de-
termined by getting the percentage of the related 
singular value over the sum of all singular val-
ues, which are represented in the ? matrix. In 
Figure 3, an example VT matrix is given. Let?s 
choose two sentences from con0, and one sen-
tence from con1. Thus, the sentences sent1 and 
sent0 are selected from con0, and sent2 is se-
lected from con1 as a part of the summary. 
This approach tries to solve the problems of 
Gong & Liu?s approach. The reduced dimension 
has not to be same as the number of sentences in 
the resulting summary. Also, more than one sen-
tence can be chosen even they do not have the 
highest cell value in the row of the related con-
cept. 
4.2 Proposed Sentence Selection Algo-
rithms 
The analysis of input documents indicates that 
some sentences, especially the ones in the intro-
duction and conclusion parts of the documents, 
belong to more than one main topic. In order to 
observe whether these sentences are important or 
they cause noise in matrices of LSA, we propose 
a new method, named as Cross. 
Another concern about matrices in LSA is that 
the concepts that are found after the SVD step 
may represent main topics or subtopics. So, it is 
important to determine whether the found con-
cepts are main topics or subtopics. This causes 
the ambiguity that whether these concepts are 
subtopics of another main topic, or all the con-
cepts are main topics of the input document. We 
propose another new method, named as Topic, in 
order to distinguish main topics from subtopics 
and make sentence selections from main topics. 
4.2.1.   Cross Method 
In this approach, the first two steps of LSA are 
executed in the same way as the other ap-
proaches. As in the Steinberger and Jezek ap-
proach, the VT matrix is used for sentence selec-
tion. The proposed approach, however, preproc-
esses the VT matrix before selecting the sen-
tences. First, an average sentence score is calcu-
lated for each concept which is represented by a 
row of VT matrix. If the value of a cell in that 
row is less than the calculated average score of 
that row, the score in the cell is set to zero. The 
main idea is that there can be sentences such that 
they are not the core sentences representing the 
topic, but they are related to the topic in some 
way. The preprocessing step removes the overall 
effect of such sentences.  
After preprocessing, the steps of Steinberger 
and Jezek approach are followed with a modifi-
cation. In our Cross approach, first the cell val-
ues are multiplied with the values in the ? ma-
trix, and the total lengths of sentence vectors, 
which are represented by the columns of the VT 
matrix, are calculated. Then, the longest sen-
tence vectors are collected as a part of the result-
ing summary. 
In Figure 4, an example VT matrix is given. 
First, the average scores of all concepts are cal-
culated, and the cells whose values are less than 
the average value of their row are set to zero. 
873
The boldface numbers are below row averages 
in Figure 4, and they are set to zero before the 
calculation of the length scores of sentences. 
Then, the length score of each sentence is calcu-
lated by adding up the concept scores of sen-
tences in the updated matrix. In the end, the sen-
tence sent1 is chosen for the summary as the 
first sentence, since it has the highest length 
score. 
 
 sent0 sent1 sent2 sent3 average
con0 0,557 0,691 0,241 0,110 0,399 
con1 0,345 0,674 0,742 0,212 0,493 
con2 0,732 0,232 0,435 0,157 0,389 
con3 0,628 0,436 0,783 0,865 0,678 
con4 0,557 0,691 0,241 0,710 0,549 
length 1,846 2,056 1,960 1,575  
Figure 4. Cross approach: For each row of VT 
matrix, the cell values are set to zero if they are 
less than the row average. Then, the cell values 
are multiplied with the values in the ? matrix, 
and the lengths of sentence vectors are found, by 
summing up all concept values in columns of VT 
matrix, which represent the sentences. 
4.2.2. Topic Method 
The first two steps of LSA algorithm are exe-
cuted as in the other approaches. For sentence 
selection, the VT matrix is used. In the proposed 
approach, the main idea is to decide whether the 
concepts that are extracted from the matrix VT 
are really main topics of the input text, or they 
are subtopics. After deciding the main topics 
which may be a group of subtopics, the sen-
tences are collected as a part of the summary 
from the main topics.  
In the proposed algorithm, a preprocessing 
step is executed, as in the Cross approach. First, 
for each concept which is represented by a row 
of VT matrix, the average sentence score is cal-
culated and the values less than this score are set 
to zero. So, a sentence that is not highly related 
to a concept is removed from the concept in the 
VT matrix. Then, the main topics are found. In 
order to find out the main topics, a concept x 
concept matrix is created by summing up the cell 
values that are common between the concepts. 
After this step, the strength values of the con-
cepts are calculated. For this calculation, each 
concept is thought as a node, and the similarity 
values in concept x concept matrix are consid-
ered as edge scores. The strength value of each 
concept is calculated by summing up the values 
in each row in concept x concept matrix. The 
topics with the highest strength values are cho-
sen as the main topic of the input text. 
 
 sent0 sent1 sent2 sent3 average
con0 0,557 0,691 0,241 0,110 0,399 
con1 0,345 0,674 0,742 0,212 0,493 
con2 0,732 0,232 0,435 0,157 0,389 
con3 0,628 0,436 0,783 0,865 0,678 
con4 0,557 0,691 0,241 0,710 0,549 
 
 con0 con1 con2 con3 con4 strength
con0 1,248 1,365 1,289 0 2,496 6,398
con1 1,365 1,416 1,177 1,525 1,365 6,848
con2 1,289 1,177 0,732 1,218 1,289 5,705
con3 0 1,525 1,218 1,648 1,575 5,966
con4 2,496 1,365 1,289 1,575 1,958 8,683
 
 sent0 sent1 sent2 sent3 
con0 0,557 0.691 0 0 
con1 0 0,674 0,742 0 
con2 0,732 0 0,435 0 
con3 0 0 0,783 0,865 
con4 0,557 0.691 0 0,710 
Figure 5. Topic approach: From each row of VT 
matrix, concepts, the values are set to zero if 
they are less than the row average. Then concept 
x concept similarity matrix is created, and the 
strength values of concepts are calculated, which 
show how strong the concepts are related to the 
other concepts. Then the concept whose strength 
value is highest is chosen, and the sentence with 
the highest score from that concept is collected. 
The sentence selection s repeated until a prede-
fined number of sentences is collected. 
After the above steps, sentence selection is 
performed in a similar manner to Gong and Liu 
approach. For each main topic selected, the sen-
tence with the highest score is chosen. This se-
lection is done until predefined numbers of sen-
tences are collected. 
In Figure 5, an example VT matrix is given. 
First, the average scores of each concept is cal-
culated and shown in the last column of the ma-
874
trix. The cell values that are less than the row 
average value (boldface numbers in Figure 5) 
are set to zero. Then, a concept x concept matrix 
is created by filling a cell with the summation of 
the cell values that are common between those 
two concepts.  The strength values of the con-
cepts are calculated by summing up the concept 
values, and the strength values are shown in the 
last column of the related matrix. A higher 
strength value indicates that the concept is much 
more related to the other concepts, and it is one 
of the main topics of the input text. After finding 
out the main topic which is the concept con4 in 
this example, the sentence with the highest cell 
value which is sentence sent3 is chosen as a part 
of the summary. 
5 Evaluation 
Two different sets of scientific articles in Turk-
ish are used for the evaluation our summariza-
tion approach. The articles are chosen from dif-
ferent areas, such as medicine, sociology, psy-
chology, having fifty articles in each set. The 
second data set has longer articles than the first 
data set. The abstracts of these articles, which 
are human-generated summaries, are used for 
comparison. The sentences in the abstracts may 
not match with the sentences in the input text. 
The statistics about these data sets are given in 
Table 1. 
 
 DS1 DS2 
Number of documents 50 50 
Sentences per document 89,7 147,3 
Words per document 2302,2 3435 
Words per sentence 25,6 23,3 
Table 1. Statistics of datasets 
Evaluation of summaries is an active research 
area. Judgment of human evaluators is a com-
mon approach for the evaluation, but it is very 
time consuming and may not be objective. An-
other approach that is used for summarization 
evaluation is to use the ROUGE evaluation ap-
proach (Lin and Hovy, 2003), which is based on 
n-gram co-occurrence, longest common subse-
quence and weighted longest common subse-
quence between the ideal summary and the ex-
tracted summary. Although we obtained all 
ROUGE results (ROUGE-1, ROUGE-2, 
ROUGE-3, ROUGE-W and ROUGE-L) in our 
evaluations, we only report ROUGE-L results in 
this paper. The discussions that are made de-
pending on our ROUGE-L results are also appli-
cable to other ROUGE results. Different LSA 
approaches are executed using different matrix 
creation methods.  
 
 G&L S&J MRC Cross Topic
frequency 0,236 0,250 0,244 0,302 0,244
binary 0,272 0,275  0,274  0,313 0,274 
tf-idf 0,200 0,218 0,213 0,304 0,213
logentropy 0,230 0,250 0,235  0,302  0,235 
root type 0,283 0,282  0,289  0,320  0,289 
mod. tf-idf 0,195 0,221  0,223  0,290  0,223 
Table 2. ROUGE-L scores for the data set DS1  
In Table 2, it can be observed that the Cross 
method has the highest ROUGE scores for all 
matrix creation techniques. The Topic method 
has the same results with Murray & Renals & 
Carletta approach, and it is better than the Gong 
& Liu approach. 
Table 2 indicates that all algorithms give their 
best results when the input matrix is created us-
ing the root type of words. Binary and log-
entropy approaches also produced good results. 
Modified tf-idf approach, which is proposed in 
this paper, did not work well for this data set. 
The modified tf-idf approach lacks performance 
because it removes some of the sentences/words 
from the input matrix, assuming that they cause 
noise. The documents in the data set DS1 are 
shorter documents, and most of words/sentences 
in shorter documents are important and should 
be kept.  
Table 3 indicates that the best F-score is 
achieved for all when the log-entropy method is 
used for matrix creation. Modified tf-idf ap-
proach is in the third rank for all algorithms. We 
can also observe that, creating matrix according 
to the root types of words did not work well for 
this data set. 
Given the evaluation results it can be said that 
Cross method, which is proposed in this paper, 
is a promising approach. Also Cross approach is 
not affected from the method of matrix creation. 
It produces good results when it is compared 
against an abstractive summary which is created 
by a human summarizer. 
875
 G&L S&J MRC Cross Topic
frequency 0,256 0,251 0,259 0,264 0,259 
binary 0,191 0,220 0,189 0,274 0,189 
tf-idf 0,230 0,235 0,227 0,266 0,227 
logentropy 0,267 0,245 0,268 0,267 0,268 
root type 0,194 0,222 0,197 0,263 0,197 
mod. tf-idf 0,234 0,239 0,232 0,268 0,232 
Table 3. ROUGE-L scores for the data set DS2  
6 Conclusion 
The growth of text based resources brings the 
problem of getting the information matching 
needs of user. In order to solve this problem, text 
summarization methods are proposed and evalu-
ated. The research on summarization started 
with the extraction of simple features and im-
proved to use different methods, such as lexical 
chains, statistical approaches, graph based ap-
proaches, and algebraic solutions. One of the 
algebraic-statistical approaches is Latent Seman-
tic Analysis method. 
In this study, text summarization methods 
which use Latent Semantic Analysis are ex-
plained. Besides well-known Latent Semantic 
Analysis approaches of Gong & Liu, Steinberger 
& Jezek and Murray & Renals & Carletta, two 
new approaches, namely Cross and Topic, are 
proposed. 
Two approaches explained in this paper are 
evaluated using two different datasets that are in 
Turkish. The comparison of these approaches is 
done using the ROUGE-L F-measure score. The 
results show that the Cross method is better than 
all other approaches. Another important result of 
this approach is that it is not affected by differ-
ent input matrix creation methods.  
In future work, the proposed approaches will 
be improved and evaluated in English texts as 
well. Also, ideas that are used in other methods, 
such as graph based approaches, will be used 
together with the proposed approaches to im-
prove the performance of summarization. 
Acknowledgments 
This work is partially supported by The Scien-
tific and Technical Council of Turkey Grant 
?TUBITAK EEEAG-107E151?. 
References 
Barzilay, R. and Elhadad, M. 1997. Using Lexical 
Chains for Text Summarization. Proceedings of 
the ACL/EACL'97 Workshop on Intelligent Scal-
able Text Summarization, pages 10-17. 
Ercan G. and Cicekli, I. 2008. Lexical Cohesion 
based Topic Modeling for Summarization.  Pro-
ceedings of 9th Int. Conf. Intelligent Text Process-
ing and Computational Linguistics (CICLing-
2008), pages 582-592.  
Gong, Y. and Liu, X. 2001. Generic Text Summariza-
tion Using Relevance Measure and Latent Seman-
tic Analysis. Proceedings of SIGIR'01. 
Hahn, U. and Mani, I. 2000. The challenges of auto-
matic summarization. Computer, 33, 29?36. 
Hovy, E. and Lin, C-Y. 1999. Automated Text Sum-
marization in SUMMARIST. I. Mani and M.T. 
Maybury (eds.), Advances in Automatic Text 
Summarization, The MIT Press, pages 81-94.  
Landauer, T.K., Foltz, P.W. and Laham, D. 1998. An 
Introduction to Latent Semantic Analysis. Dis-
course Processes, 25, 259-284. 
Lin, C.Y. and Hovy, E.. 2003. Automatic Evaluation 
of Summaries Using N-gram Co-occurrence Sta-
tistics. Proceedings of 2003 Conf. North American 
Chapter of the Association for Computational Lin-
guistics on Human Language Technology (HLT-
NAACL-2003), pages 71-78.  
Luhn, H.P. 1958. The Automatic Creation of Litera-
ture Abstracts. IBM Journal of Research Devel-
opment 2(2), 159-165. 
Mihalcea, R. and Tarau, P. 2004. Text-rank - bringing 
order into texts. Proceeding of the Conference on 
Empirical Methods in Natural Language Process-
ing. 
Murray, G., Renals, S. and Carletta, J. 2005. Extrac-
tive summarization of meeting recordings. Pro-
ceedings of the 9th European Conference on 
Speech Communication and Technology. 
Qazvinian, V. and Radev, D.R. 2008. Scientific paper 
summarization using citation summary networks. 
Proceedings of COLING2008, Manchester, UK, 
pages 689-696. 
Steinberger,  J. and Jezek, K. 2004. Using Latent Se-
mantic Analysis in Text Summarization and Sum-
mary Evaluation. Proceedings of ISIM '04, pages 
93-100.
  
876
Proceedings of the 2014 Joint Meeting of SIGMORPHON and SIGFSM, pages 46?54,
Baltimore, Maryland USA, June 27 2014. c?2014 Association for Computational Linguistics
Rule Based Morphological Analyzer of Kazakh Language 
 
Gulshat Kessikbayeva 
Hacettepe University, Department of 
Computer Engineering, 
Ankara,Turkey 
shatik2030@gmail.com 
Ilyas Cicekli 
Hacettepe University, Department of 
Computer Engineering, 
Ankara,Turkey 
ilyas@cs.hacettepe.edu.tr 
 
 
 
Abstract 
Having a morphological analyzer is a very 
critical issue especially for NLP related 
tasks on agglutinative languages. This paper 
presents a detailed computational analysis 
of Kazakh language which is an 
agglutinative language. With a detailed 
analysis of Kazakh language morphology, 
the formalization of rules over all 
morphotactics of Kazakh language is 
worked out and a rule-based morphological 
analyzer is developed for Kazakh language. 
The morphological analyzer is constructed 
using two-level morphology approach with 
Xerox finite state tools and some 
implementation details of rule-based 
morphological analyzer have been presented 
in this paper.   
 
1 Introduction 
 
Kazakh language is a Turkic language which 
belongs to Kipchak branch of Ural-Altaic 
language family and it is spoken approximately 
by 8 million people. It is the official language 
of Kazakhstan and it has also speakers in 
Russia, China, Mongolia, Iran, Turkey, 
Afghanistan and Germany.  It is closely related 
to other Turkic languages and there exists 
mutual intelligibility among them. Words in 
Kazakh language can be generated from root 
words recursively by adding proper suffixes. 
Thus, Kazakh language has agglutinative form 
and has vowel harmony property except for 
loan-words from other languages such as 
Russian, Persian and Arabic.  
Having a morphological analyzer for an 
agglutinative language is a starting point for 
Natural Language Processing (NLP) related 
researches. An analysis of inflectional affixes of 
Kazakh language is studied within the work of 
a Kazakh segmentation system (Altenbek and 
Wang, 2010). A finite state approach for 
Kazakh nominals is presented (Kairakbay and 
Zaurbekov, 2013) and it only gives specific 
alternation rules without generalized forms of 
alternations. Here we present all generalized 
forms of all alternation rules. Moreover, many 
studies and researches have been done upon on 
morphological analysis of Turkic languages 
(Altintas and Cicekli, 2001; Oflazer, 1994; 
Coltekin, 2010; Tantug et al., 2006; Orhun et al, 
2009). However there is no complete work 
which provides a detailed computational 
analysis of Kazakh language morphology and 
this paper tries to do that.     
The organization of the rest of the paper is 
as follows. Next section gives a brief 
comparison of Kazakh language and Turkish 
morphologies. Section 3 presents Kazakh vowel 
and consonant harmony rules. Then, nouns with 
their inflections are presented in Section 4. 
Section 4 also presents morphotactic rules for 
nouns, pronouns, adjectives, adverbs and 
numerals. The detailed morphological structure 
of verbs is introduced in Section 5. Results of 
the performed tests are presented together with 
their analysis in Section 6. At last, conclusion 
and future work are described in Section 7. 
 
2 Comparison of Closely Related 
Languages 
 
There are many studies and researches prior 
made on closely related languages by 
comparing them for many purposes related with 
NLP such as Turkish?Crimean Tatar (Altintas 
and Cicekli, 2001), Turkish?Azerbaijani 
(Hamzao?lu, 1993), Turkish?Turkmen (Tantu? 
et al., 2007), Turkish-Uygur (Orhun et al, 2009) 
and Tatar-Kazakh (Salimzyanov et al, 2013). A 
deep comparison of Kazakh and Turkish 
languages from computational view is another 
study which is in out of scope for this work. 
However, in this study, a brief grammatical 
comparison of these languages is given in order 
to give a better analysis of Kazakh language. 
46
Kazakh and Turkish languages have many 
common parts due to being in same language 
family. Possible differences are mostly 
morpheme based rather than deep grammar 
differences. Distinct morphemes can be added 
in order to get same meaning. There exist some 
differences in their alphabets, their vowel and 
consonant harmony rules, their possessive 
forms of nouns, and inflections of verbs as 
given in Table 1. There are extra 9 letters in 
Kazakh alphabet, and Kazakh alphabet also has 
4 additional letters for Russian loan words. 
Both Kazakh language and Turkish employ 
vowel harmony rules when morphemes are 
added. Vowel harmony is defined according to 
last morpheme containing back or front vowel. 
In Kazakh language, if the last morpheme 
contains a back vowel then the vowel of next 
coming suffix is a or ?. If the last morpheme 
contains one of front vowels then the vowel of 
next coming suffix is e or i. In Turkish, suffixes 
with vowels a, ?, u follow morphemes with 
vowels a, o, u, ? and suffixes with vowels e, i, ? 
follow morphemes with vowels e, i, ?, ? 
depending on being rounded and unrounded 
vowels. Consonant harmony rule related with 
voiceless letters is similar in both languages.  
  
 Turkish Kazakh 
Language 
Alphabet Latin 
29 letters 
( 8 Vowels, 
21 Consonant ) 
Cyril 
42 letters 
( 10 Vowels, 
25 Consonants, 
3 Compound 
Letters, 
4 Russian 
Loan Word 
Letters ) 
Vowel & 
Consonant 
Harmony 
Synharmonism 
according to 
back, front,  
unrounded and 
rounded 
vowels 
Synharmonism 
according to 
back and front 
vowels 
Possessive 
Forms of 
Nouns 
6 types of 
possessive 
agreements  
8 types of 
possessive 
agreements 
Case 
Forms of 
Nouns 
7 Case Forms 7 Case Forms  
Verbs Similar Tenses Similar Tenses 
 
Table 1. Comparison of Kazakh and Turkish. 
 
In Kazakh language there are 8 types of 
personal possessive agreement morphemes as 
given in Table 2. Kazakh language has two 
additional possessive agreements for second 
person.  
There are some identical tenses and moods 
of verbs in both language such as definite past 
tense, present tense, imperative mood, optative 
mood and conditional mood. They have nearly 
same morphemes for tenses. On the other hand 
there are some tenses of verbs which are 
identical according to meaning and usage, but 
different morphemes are used. Moreover, in 
Kazakh language there are some tenses such as 
goal oriented future and present tenses which 
do not exist in Turkish language.  
 
Possessive 
Pronoun 
Representation 
Examples for 
Eke, ?father? 
None 
Possessive 
Pnon  Eke father 
My P1Sg 1 Eke-m 
my 
father 
Your P2Sg 2 Eke-N 
your 
father 
Your 
(Polite) 
P2PSg 2 
Eke-
Niz 
your 
father 
His/Her P3Sg 3 Eke-si 
his 
father 
Our P1Pl 1 
Eke-
miz 
our 
father 
Your Plural P2Pl 2 
Eke-
leriN 
your 
father 
Your Plural 
(Polite)  
P2PPl 2 
Eke-
leriNiz 
your 
father 
Their P3Pl 3 
Eke-
leri 
their 
father 
 
Table 2. Possessive Agreement of Nouns. 
 
3 Vowel and Consonant Harmony 
 
Kazakh is officially written in the Cyrillic 
alphabet. In its history, it was represented by 
Arabic, Latin and Cyrillic letters. Nowadays 
switching back to Latin alphabets in 20 years is 
planned by the Kazakh government. In the 
beginning stage of study, Latin transcription of 
Cyril version is used for convenience.  
 Two main issues of language such as 
morphotactics and alternations can be dealt 
with Xerox tools. First of all, morphotactic 
rules are represented by encoding a finite-state 
network. Then, a finite-state transducer for 
alternations is constructed. Then, the formed 
network and the transducer are composed into a 
47
single final network which cover all 
morphological aspects of the language such as 
morphemes, derivations, inflections, 
alternations and geminations (Beesley and 
Karttunen, 2003).  
Vowel harmony of Kazakh language obeys 
a rule such that vowels in each syllable should 
match according to being front or back vowel. 
It is called synharmonism and it is basic 
linguistic structure of nearly all Turkic 
languages (Demirci, 2006).  For example, a 
word qa-la-lar-dIN, ?of cities? has a stem qa-
la, ?city? and two syllables of containing back 
vowels according to the vowel harmony rule. 
Here ?lar is an affix of Plural form and ?dIN is 
an affix of Genitive case. However, as stated 
before, there are a lot of loan words from 
Persian and generally they do not obey vowel 
harmony rules. For example, a word mu-Ga-
lim, ?teacher? has first two syllables have back 
vowels and the last one has a front vowel. So 
suffixes to be added are defined according to 
the last syllable. For example, a word muGalim-
der-diN, ?of teachers? has suffixes with front 
vowels. On the other hand, there are 
morphemes with static front vowels which are 
independently from the type of last syllable can 
be added to all words such as Instrumental 
suffix ?men. In this case, all suffixes added 
after that should contain front vowels.  
 
 
Name XFST   Type 1 Type 2 
Sonorous 
Consonant 
SCons l r y w m n N 
Voiced 
Consonant 
VCons z Z b v g d 
Voiceless 
Consonant 
VLCons p f q k t s S C x c 
Consonant Cons 
b p t c x d r z Z s S 
C G f q k g N l m n 
h w y v   
Vowel Vowel a e E i I O o u U j 
Front 
Vowel 
FVowel e E i O U j 
Back 
Vowel 
BVowel a I o u 
Table 3. Groups of Kazakh letters according to 
their sound. Upper case letters are used for non-
Latin letters. 
 
In order to construct a finite-state transducer 
for alternation rules, there are some capital 
letters such as A, J, H, B, P, C, D, Q, K, T are 
defined in intermediate level and they are 
invisible by user.  These representations are 
used for substitution such as A is for a and e 
and J is for I and i. So if suffix dA should be 
added according to morphotactic rules, it means 
suffixes da or de should be considered. In Table 
3, there are group of letters defined according to 
their sounds and these groups are used in 
alternation rules (Valyaeva, 2007).  
Consonant harmony rules are varied 
according to the last letter of a word with in 
morphotactic rules. As in Table 3, different 
patterns are presented in order to visualize the 
relation between common valid rules and to 
generalize morphotactic rules. Thus, in each 
case according to morphotactic rules there are 
proper alternation rules for morphemes.  
 
GROUP 1 
Ablative Case Locative case Dative Case 
dAn    dA     TA     
tAn  tA  TA  
nAn  3 ndA 3 nA 3 
  A 1/2 
GROUP 2 
Genitive Case Accusative Case Poss. Affix-2 
dJN    dJ     diki    
tJN   tJ   tiki  
nJN   3   nJ   niki  
  n 3  
GROUP 3 
Plural Form of 
Noun 
Negative Form A1Pl 
dAr  l   bA   bJz   
tAr  pA  pJz  
lAr  r y w mA   mJz   
GROUP 4 
Instrumental 
Case 
 
A1Sg 
ben    bJn  
pen    pJn  
men     3   mJn    
 
Table 4. Alternation rules according to groups 
of letters. 
 
All alternation rules for suffixes depend on 
the last letter of a morpheme with in 
morphotactic rules and Table 4 gives some 
groupings that can be made in order to set some 
generalized rules overall. Patterns of last letters 
of morphemes in Table 4 are matched with 
groups of letters presented in Table 3. In Table 
4, Locative case affix is ?dA, if the last letter of 
a morpheme is one of Vowel, Sonorous 
48
Consonant or Voiced Consonant of Type 1 in 
Table 3. On the other hand, it is ?tA, if the last 
letter is Voiceless Consonant or Voiced 
Consonant of Type 2. Here A is for a or e 
according to last syllable of containing Front or 
Back Vowel.  
In Table 4, boxes presented by numbers 
such as 1, 2 and 3 are used for personal 
possessive agreements in Table 2. For example, 
word Eke, ?father? in Ablative case without a 
possessive agreement takes suffix ?den, 
because the word Eke ends with vowel e. 
However, in third person possessive agreement 
it takes suffix ?nen, because all words with 
third person possessive agreement in Ablative 
case always take suffix ?nen even though the 
third person possessive agreement morpheme 
ends with vowel.  
According to those similarities in Table 4, 
there are some generalized rules which are valid 
in many cases in grammar including verbs and 
derivations. Some of these generalized rules 
derived from close patterns given in Table 4, 
are given in Table 5. For example, Rule 12 in 
Table 5 represents rules for Locative and Dative 
cases in Group 1 in Table 4.  In Table 4, 
Locative and Dative suffix rules are nearly 
identical and have same patterns which can be 
observed visually. Also, Accusative and 
Possessive Pronouns of Type 2 are same. 
 
 1 2 
1 Rule 11 
Ablative Case 
Rule 12 Locative, 
Dative cases  
2 Rule 21 
Genitive case 
Rule 22 
Accusative case, 
Poss. Affix-2 
3 Rule 31 
Plural Form of 
Noun 
Rule 32 
Negation, Personal 
Agreement of A1Pl 
4 Rule 41 
Instrumental 
case 
Rule 42  
Personal 
Agreement of A1Sg 
 
Table 5. Generalized Rules. 
 
In Dative case of GROUP 1 in Table 4, if 
the last letter is Back Vowel then T is replaced 
by G and T is replaced by g if the last letter is 
Front Vowel. Thus, a word bala, ?child? 
becomes bala-Ga, ?to child? and a word Eke, 
?father? will be Eke-ge, ?to father?. If the last 
letter is Voiceless Consonant, T is replaced by q 
or k depending on whether the last syllable 
contains Back Vowel or Front Vowel. For 
example, a word kitap-qa, ?to book? has the 
last letter of Voiceless Consonant and the last 
syllable contains Back Vowel, thus T is 
replaced by q. A word mektep-ke, ?to school? 
has the last letter of Voiceless Consonant and 
the last syllable contains Front vowel, thus T is 
replaced by k.  
After detailed analysis of the language it 
can be seen that there are mainly common rules 
of alternations valid over all grammar. There 
are about 25 main alternation rules defined for 
all system together with generalized rules and 7 
exception rules for each case. All these rules are 
implemented with XFST tools (Beesley and 
Karttunen, 2003). For instance, some mainly 
used common rules are given below and they 
are called by capital letters defined only in 
intermediate level. As mentioned before they 
are invisible by user. Here 0 is for empty 
character.   
Rule H & Rule B: H is realized as 0 or J, B is 
realized as 0 or A. 
 [H->0,B->0||[Vowel]%+_[Cons]]     
[H->J,B->A]  
If the last letter of a morpheme is Vowel and 
the first letter of the following suffix is 
Consonant then H and B are realized as 0. 
Otherwise, they are realized as J and B. Some 
examples are: 
ana-Hm? ana-m, ?my mother? 
iS-Hm? iS-Jm?Rule J?iSim, ?my stomache? 
ege-Br? ege-r, ?will sharpen? 
bar-Br? bar-Ar? Rule A?bar-ar, ?will go?  
Rule J & Rule A: J is realized as I or i and A is 
realized as y, a or e. 
[A->y||[Vowel]%+_] 
[A->a,J->I||[BVowel](Cons)*%+?*_] 
[A->e,J->i||[FVowel](Cons)*%+?*_] 
If the last letter of a morpheme is Vowel then A 
is realized as y, and if the last syllable of a 
morpheme contains Back Vowel then A and J 
are realized as a and I. Otherwise, if the last 
syllable of a morpheme contains Front Vowel 
then A and J are realized as e and i. Some 
examples are: 
bas-Hm?bas-Jm?basIm, ?my head? 
dos-tAr?dos-tar, ?friends? 
dEpter-lAr? dEpter-ler, ?copybooks? 
barma-AmIn?barma-ymIn, ?I will not go? 
Rule T (a part of Rule 12 in Table 5): T is 
realized as q, G, k or g. 
[T->q||[BVowel](?)[VLCons]%+_]                     
[T->k||[FVowel](?)[VLCons]%+_]     
[T->G||[BVowel](?)[0|SCons|VCons1]%+_]       
[T->g||[FVowel](?)[0|SCons|VCons1]%+_] 
49
This rule is a part of Rule 12 given in Table 5 
for Dative case. It is one of generalized rules 
which are valid in many cases such as 
derivation of nouns, adjectives and verbs. Some 
examples are: 
     bala-Ta? bala-Ga, ?to child? (Noun in Dative) 
Zaz-TI? Zaz-GI, ?of summer? (Adjective) 
ZUr-Teli?ZUr-geli, ?since coming? (Verb) 
estit-Tiz? estit-kiz, ?make hear?(Causative 
Verb) 
 
4 Nouns 
  
Nouns in Kazakh Language take singular or 
plural (A3Sg, A3Pl) suffixes, Possessive 
suffixes, Case suffixes and Derivational 
suffixes. In addition, nouns can take Personal 
Agreement suffixes when they are derived into 
verbs. For example, kitap-tar-da-GI-lar-dIN  
which means ?of those which is in books? has 
the following morphological analysis 
kitap+Noun+A3Pl+Pnon+Loc^DB+Noun+Zer
o+A3Pl+Pnon+Gen.  
Every nominal root at least has form of 
Noun+A3Sg+Pnon+Nom. Therefore, a root 
noun kitap which means ?book? has the 
following morphological analysis 
kitap+Noun+A3Sg+Pnon+Nom. 
These inflections of noun are given in FST 
diagram in Figure 1.  
 
 
Figure 1. The FSA model of inflectional 
changes of a noun. 
 
It can be seen that nominal root can be in 
singular form by adding (+0) no suffix which is 
in fact third personal singular agreement 
(A3Sg) and by adding suffix (+PAr) in plural 
form which is in fact third personal plural 
agreement (A3Pl). Here P is an intermediate 
level representation letter for d, t or l in surface 
level. After, possessive affixes (+Pnon:0,  
+P1Sg:Hm, +P2Sg:HN, +P2PSg:HNJz, +P3Sg:sJ, 
+P1Pl:HmJz, +P2Pl:HN, +P2PPl:HNJz, +P3Pl:s) 
and case affixes (Nom, Dat, Abl, Loc, Acc, Gen, 
Ins) are added. Here H and J are intermediate 
letters. All morphotactic rules together with 
adjective, pronoun, adverb and numerals are 
given in Figure 2. It can be observed that every 
adjective can be derived to noun and nouns 
with relative affix can be derived to adjectives. 
There are other derivations which are produced 
by adding some specific suffixes between verbs 
and nouns, adjectives and adverbs, adjectives 
and nouns. In order to get rid of complex view 
those derivations are not explicitly shown in 
Figure 2. 
 
Figure 2. Morphotactic Rules for Nominal Roots. 
50
In our system, the root of a word is a 
starting point for morphemes defined in lexicon 
file, and other morphemes are added according 
to morphotactic rules. Thus, starting from a 
root the system checks for all possible 
following morphemes and if a word is matched 
it gives appropriate output and moves to next 
state. For example, a surface form of a word 
kitaptan, ?from a book? will have intermediate 
form of ?kitap+tan? after implemented 
alternation rules. First it will check and find a 
noun root from lexicon. Then after giving 
output as ?kitap+Noun?, continues with next 
state which is Singular/Plural. At this state it 
will go on with 0 input giving output of +A3Sg 
for singular form of noun. Then, the next state 
will be Possessive Affix state to determine the 
personal possessive suffix. Here it is 0, thus 
epsilon transition which gives output as +Pnon. 
Now the output is ?kitap+Noun+A3Sg+Pnon?. 
The next state is Case state in order to 
recognize the case of noun. Thus, for given 
input such as +tan, the output is determined as 
+Abl and this continues until the system 
reaches the final state or invalid state which is 
unacceptable state not returned to user. All 
possible morphemes are defined in the lexicon 
and all states are visualized in Figure 2.  
 
 
 
Figure 3. Tenses of Verbs in Kazakh 
Language. 
 
 
5 Verbs 
 
Verbs are terms which define actions and states. 
Mainly three tenses exist such as present, future 
and past as stated in Figure 3. Moreover, 
conditional, optative and imperative moods are 
also defined. However in detailed form there 
are thirteen tenses together with modals in 
Kazakh language. These tenses are worked out 
from many resources where presentation and 
naming have variance among each other 
according to their scholars (Tuymebayev, 1996; 
Mamanov, 2007;  Isaeva and Nurkina, 1996; 
Musaev, 2008). For example, according to 
Isaeva and Nurkina (1996) awIspalI keler Saq 
?Future Transitional Tense? denotes action in 
future and has same affix as Present Tense. 
However, Mamanov (2007) pointing out that 
awIspalI keler Saq, ?Future Transitional Tense? 
denotes present action. Additionally, there are 
large amount of auxiliary verbs which define 
tenses and some modal verbs. However in cases 
that auxiliary verbs are not used verbs become 
as deverbial adverbs or participles which define 
verb or noun (Demirci, 2006). In Figure 4, 
morphotactic rules of verbs and modals are 
given. Derivations of verbs to nouns and 
adverbs with specific suffixes are shown with 
asterisk in Figure 4. 
Verbs can be in reflexive, passive, collective 
and causative forms. For instance, verb tara-w 
means ?to comb?, tara-n-w in reflexive infinity 
form, tara-l-w in passive infinity form, tara-s-w 
in collective infinity and tara-tQJz-w and tara-
tTJr-w in causative infinity form. Here, Q, J 
and T are intermediate letters. However not all 
verbs can have all of these forms at the same 
time. 
Verbs in infinity form are generally formed 
with last letter w. For example: kelw which 
means ?to come?. The system is performing 
over generalization on verbs which take 
auxiliary verb on appropriate tenses. Those 
verbs are analyzed as derived adverbs or 
incomplete verbs on that tense since every verb 
of sentence should have personal agreement at 
the end and personal agreement affix added to 
the verb itself after the suffix of tense or to the 
auxiliary verb. In constructed morphological 
analyzer, we make analysis of every single 
word and for that reason generalization of some 
rules are made by giving more than one result.
51
 Figure 4. Morphotactic Rules of Verbs in Kazakh Language. 
For example, kel-geli tur-mIn means ?I am 
planning to come?. Here tur is an auxiliary 
form which actually defines the tense of the 
verb and takes personal agreement affix mIn. 
Without an auxiliary verb, the word kel-geli 
means ?since coming? and derived as an 
adverb. Thus compound verbs are examined 
separately. Some of tenses have different 
personal agreement endings and they are 
presented in Figure 4 
 
6 Tests and Analysis 
 
As mentioned before, the system is 
implemented using Xerox finite-state tools for 
NLP. Morphotactic rules and possible 
morphemes are defined in lexicon file and 
compiled with lexc tool. Alternation rules are 
defined in regex file and rule transducer is 
composed with lexicon file in one network with 
xfst tool. Loan words, proper names and 
technical terms are not included. System is 
working in two directions as in lexical and 
surface level. Due to the ambiguities in 
language there is no one-to-one mapping 
between surface and lexical forms of words and 
the system can produce more than one result.  
A large corpus of Kazakh words 
(Qazinform, 2010) not seen by the 
morphological analyzer before has been 
continually analyzed in order to enhance the 
system by adding new words to lexicon. There 
are approximately 1000 words randomly 
selected from web which exist in lexicon and 
analyzed with the system. The percentage of 
correctly analyzed words is approximately 
96%. Most of the errors are mainly the errors 
that occurred in the analysis of technical words 
which do not obey alternation rules of Kazakh 
Language. In Table 6, the w1.txt file has more 
technical words than w2.txt file. The results of 
the tests are given in Table 6. Errors due to 
Rules are exception errors which are not 
included in transducer yet. We hope in near 
future enhancing of the system will be 
performed by including all these rules. Also it 
can be seen in Table 6 that Kazakh words have 
2.1 morphologic parses on average. 
 
    
Files 
Total 
Words 
Correctly 
Analyzed 
Words 
Total Errors 
Rules Analyzer 
w1.txt 1000 962 30 8 
w2.txt 1010 978 26 6 
Morphologic Ambiguity is 2.1 
 
Table 6. Test Results. 
 
7 Conclusion 
 
Language is one of the main tools for 
communication. Thus its investigation provides 
52
better perspectives on all other aspects related 
with NLP. However, formalization and 
computational analysis of Kazakh language 
morphology is not widely worked out. In other 
words, there is a lack of tools for analysis of 
Kazakh language morphology from 
computational point of view. Moreover, 
grammar resources contain variances depending 
on scholars. For example, in some resources 
there are twelve tenses, whereas in others there 
are much less tenses of verbs. Naming of tenses 
can also vary from source to source. To 
summarize, building correctly working system 
of morphological analysis by combining all 
information is valuable for further researches 
on language.  
In this paper, a detailed morphological 
analysis of Kazakh language has been 
performed. Also, a formalization of rules over 
all morphotactics of Kazakh languages is 
worked out.  By combining all gained 
information, a morphological analyzer is 
constructed. For future work, enhancing of 
system by adding exception rules related with 
loan words and proper names should be 
performed. Having more stabilized system with 
lessened possible rule errors some internal 
details of character encoding will also be 
solved. Moreover, releasing the working system 
to users on the web and collecting feedbacks 
are intended. These feedbacks from users can 
help on improving the system capacity and 
lessen any possible errors. This is planned to be 
performed with using an open source 
environment which is alternative to Xerox 
XFST, namely Foma by Hulden (2009).  
 
 
Reference 
 
Altenbek G and Wang X. 2010. Kazakh 
Segmentation System of Inflectional 
Affixes. Proceedings of CIPS-SIGHAN 
Joint Conference on Chinese Language 
Processing (CLP2010), Beijing, China, 
p.183?190. 
 
Altintas K. and Cicekli I. 2001. A 
Morphological Analyser for Crimean Tatar. 
Proceedings of the 10th Turkish Symposium 
on Artificial Intelligence and Neural 
Networks (TAINN?2001), North Cyprus, 
p.180-189. 
 
Beesley R. K. and Karttunen L. 2003. Finite 
State Morphology. CSLI Publications, 
Stanford, CA, USA. 
 
Coltekin C. 2010. A Freely Available 
Morphological Analyzer for Turkish. 
Proceedings of the 7th International 
Conference on Language Resources and 
Evaluation (LREC?10), Valletta, Malta. 
 
Demirci K. 2006. Kazakh Verbal Structures 
and Descriptive Verbs. Dunwoody Press, 
Maryland, USA. 
 
Isaeva S, Nurkina G. 1996. Sopostavitelnaya 
tipologiya kazakhskogo i russkogo yazykov. 
Uchebnogo Posobie. Sanat publishers, 
Almaty, Kazakhstan. 
 
Hamzao?lu I. 1993. Machine translation from 
Turkish to other Turkic languages and an 
implementation for the Azeri language. 
Master?s thesis, Bogazici University, 
Turkey. 
 
Hulden M. 2009. Foma: a finite-state compiler 
and library. Proceedings of the 12th 
Conference of the European Chapter of the 
Association for Computational Linguistics: 
Demonstrations Session. Association for 
Computational Linguistics, pp. 29?32. 
 
Kairakbay M. B. and Zaurbekov D. L. 2013. 
Finite State Approach to the Kazakh 
Nominal Paradigm. Proceedings of the 11th 
International Conference on Finite State 
Methods and Natural Language Processing 
(FSMNLP 2013), Scotland. 
 
Mamanov I.E. 1961. Kazahskij jazyk. 
Uchebnogo posobie gumanitarnogo tipa. 
Almaty, Kazakhstan. 
 
Mamanov I.E. 2007. Qazaq til biliminin 
maseleleri. Aris publishers, Almaty, 
Kazakhstan 
 
Mussayev M. K. 2008. The Kazakh Language. 
Vostochnaya literatura publishers, Moscow, 
Russia. 
 
Oflazer K. 1994. Two-level Description of 
Turkish Morphology. Literary and 
Linguistic Computing, 9(2):137-148. 
 
53
Orhun M., Tantu? C., Adal? E., and S?nmez C. 
2009. Computational comparison of the 
Uyg-hur and Turkish Grammar. The 2nd 
IEEE International Conference on 
Computer Science and Information 
Technology, pp:338-342, Beijing, China. 
 
Qazinform. 2010. National news agency. 
http://www.inform.kz/qaz. 
 
Salimzyanov I, Washington J.  and  Tyers F, 
2013. A free/open-source Kazakh-Tatar 
machine translation system. Machine 
Translation Summit XIV, Nice, France. 
 
Tantug C., Adali E. and Oflazer K. 2006. 
Computer Analysis of the Turkmen 
Language Morphology. Lecture Notes in 
Computer Science, 4139:186-193. A. C.  
 
Tantug C., Adali E. and Oflazer K. 2007.  AMT 
system from Turkmen to Turkish 
employing finite state and statistical 
methods.  Proceedings of MT Summit XI. 
 
Tuymebayev Q. Zhanseyit 1996. Qazaq Tili: 
Grammatikaliq anaiqtagish. Almaty, 
Kazakhstan. 
 
Valyaeva T., 2014. Kazakhskii yazyk. 
http://www.kaz-tili.kz/.  
54
