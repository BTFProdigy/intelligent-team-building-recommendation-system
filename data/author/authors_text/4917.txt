Feature-based Pronunciation Modeling for Speech Recognition
Karen Livescu and James Glass
MIT Computer Science and Artificial Intelligence Laboratory
Cambridge, MA 02139, USA
{klivescu, glass}@csail.mit.edu
Abstract
We present an approach to pronunciation mod-
eling in which the evolution of multiple lin-
guistic feature streams is explicitly represented.
This differs from phone-based models in that
pronunciation variation is viewed as the result
of feature asynchrony and changes in feature
values, rather than phone substitutions, inser-
tions, and deletions. We have implemented a
flexible feature-based pronunciation model us-
ing dynamic Bayesian networks. In this paper,
we describe our approach and report on a pilot
experiment using phonetic transcriptions of ut-
terances from the Switchboard corpus. The ex-
perimental results, as well as the model?s quali-
tative behavior, suggest that this is a promising
way of accounting for the types of pronuncia-
tion variation often seen in spontaneous speech.
1 Introduction
Pronunciation variation in spontaneous speech has been
cited as a serious obstacle for automatic speech recog-
nition (McAllester et al, 1998). Typical pronunciation
models approach this problem by augmenting a phone-
mic dictionary with additional pronunciations, often re-
sulting from the application of phone substitution, inser-
tion, and deletion rules. By carefully constructing a rule
set (Hazen et al, 2002), or by deriving rules or variants
from data (Riley and Ljolje, 1996), many phenomena can
be accounted for. However, the recognition improvement
over a phonemic dictionary is typically modest, and some
types of variation remain awkward to represent.
These observations have motivated approaches to
speech recognition based on multiple streams of linguis-
tic features rather than a single stream of phones (e.g.,
King et al (1998); Metze and Waibel (2002); Livescu et
al. (2003)). Most of this work, however, has focused on
acoustic modeling, i.e. the mapping between the features
and acoustic observations. The pronunciation model is
typically still phone-based, limiting the feature values to
the target configurations of phones and forcing them to
behave as a synchronous ?bundle?. Some approaches
have begun to relax these constraints. For example, Deng
et al (1997) and Richardson et al (2000) model asyn-
chronous feature trajectories using hidden Markov mod-
els (HMMs), with each state corresponding to a vector
of feature values. This approach is powerful, but it can-
not represent independencies between features. Kirch-
hoff (1996), in contrast, models the feature streams as
independent, except for a requirement that they synchro-
nize at syllable boundaries. As pointed out by Osten-
dorf (2000), such independence assumptions may allow
for too much variability.
In this paper, we propose a more general feature-
based pronunciation model implemented using dynamic
Bayesian networks (Dean and Kanazawa, 1989), which
allow us to take advantage of inter-feature independen-
cies while avoiding overly strong independence assump-
tions. In the following sections, we describe the model
and present proof-of-concept experiments using phonetic
transcriptions of utterances from the Switchboard conver-
sational speech corpus (Greenberg et al, 1996).
2 Serval [sic] examples
To help ground the discussion, we first present several
examples of pronunciation variation. One common phe-
nomenon is the nasalization of vowels preceding nasal
consonants. This is a result of asynchrony: The velum is
lowered before the oral closure is made. In more extreme
cases, the nasal consonant is entirely absent, leaving only
a nasalized vowel, as in can?t ? [ k ae n t ] 1. All of the
feature values are still correct, although phonetically, this
would be described as a deletion.
Another example, taken from the Switchboard corpus,
is several ? [s eh r v ax l]. In this case, the tongue
and lips have desynchronized to the point that the tongue
1Here and throughout, we use the ARPAbet phonetic symbol
set with additional diacritics, such as ? n? for nasalization.
retroflexion for [r] starts and ends before the lip narrow-
ing gesture for [v]. Again, all of the feature streams
are produced correctly, but there is an apparent exchange
of two phones, which cannot be represented via single-
phone confusions conditioned on phonemic context.
A final example from Switchboard is everybody ? [eh
r uw ay]. It is difficult to imagine a set of phonetic trans-
formations that would predict this pronunciation without
allowing a host of other impossible pronunciations. How-
ever, when viewed in terms of features, the transforma-
tion from [eh v r iy bcl b ah dx iy] to [eh r uw ay] is
fairly simple. The tongue and lips desynchronize, caus-
ing the lips to start to close for the [bcl] during the previ-
ous vowel. In addition, the lip constrictions for [bcl] and
[v], and the tongue tip gesture for [dx], are reduced. We
will return to this example in the sections below.
3 Approach
A feature-based pronunciation model is one that explic-
itly models the evolution of multiple underlying linguis-
tic feature streams to predict the allowed realizations of
a word and their probabilities. Our approach begins with
the usual assumption that each word has one or more tar-
get phonemic pronunciations, or baseforms. Each base-
form is converted to a table of underlying feature values.
Table 1 shows what part of this table might look like for
the word everybody. The table may include ?unspecified?
values (?*? in the table). More generally, each table en-
try can be a distribution over the range of feature values.
For now, we assume that all of the features go through the
same sequence of indices (and therefore the same number
of targets) in a given word; e.g., in Table 1, LIP-OPEN
goes through the same indices as TT-LOC, although it
has the same target value for indices 2 and 3. In the first
time frame of speech, all of the features begin in index
0; in subsequent frames, each feature can either stay in
the same index or transition to the next one with some
probability.
The surface feature values?i.e., the ones that are ac-
tually produced by the speaker?can stray from the un-
derlying pronunciation in two ways, typically because of
articulatory inertia: substitution, in which a feature fails
to reach its target underlying value; and asynchrony, in
which different features proceed through their sequences
of indices at different rates. We define the degree of asyn-
chrony between two sets of features as the difference be-
tween the average index of one set relative to the average
index of the second. The degree of asynchrony is con-
strained: More ?synchronous? configurations are more
probable (soft constraints), and we make the further sim-
plifying assumption that there is an upper bound on the
degree of asynchrony (hard constraints).
A natural framework for such a model is provided by
dynamic Bayesian networks (DBNs), because of their
index 0 1 2 3 ...
phoneme eh v r iy ...
LIP-OPEN wide critical wide wide ...
TT-LOC alv. * ret. alv. ...
... ... ... ... ... ...
Table 1: Part of a target pronunciation for everybody.
In this feature set, LIP-OPEN is the lip opening degree;
TT-LOC is the location along the palate to which the
tongue tip is closest (alv. = alveolar; ret. = retroflex).
2 3
U UU
2 3S1 S S
sync =11;2
ind ind ind
lexEntry
sync =11,2;3
1 2 3
t
t t
t
tt
t t
t
1
t
t
twdTr
t
Figure 1: One frame of a DBN for recognition with
a feature-based pronunciation model. Nodes represent
variables; shaded nodes are observed. Edges repre-
sent dependencies between variables. Edges without par-
ents/children point from/to variables in adjacent frames
(see text).
ability to efficiently implement factored state representa-
tions. Figure 1 shows one frame of the type of DBN used
in our model (simplified somewhat for clarity of presenta-
tion). This example DBN assumes a feature set with three
features. The variables at time frame t are as follows:
lexEntryt ? entry in the lexicon corresponding to the cur-
rent word and baseform. Words with multiple base-
forms have one entry per baseform. lexEntryt?s par-
ents are lexEntryt?1 and wdTrt?1
indjt ? index of feature j into the underlying pronun-
ciation, as in Table 1. indj0 = 0; in subsequent
frames indjt is conditioned on lexEntryt?1, ind
j
t?1,
and wdTrt?1 (defined below).
Ujt ? underlying value of feature j. Its distribution
p(U jt |lexEntryt, indjt ) is determined by the target
feature table of lexEntryt.
Sjt ? observed surface value of feature j. p(Sjt |U jt ) en-
codes allowed feature substitutions.
wdTrt ? binary variable indicating whether this is the last
frame of the current word.
syncA;Bt ? binary variable that enforces a synchrony con-
straint between subsets A and B of the feature set.
It is observed with value 1; its distribution is con-
structed in such a way as to force its parent ind vari-
ables to obey the desired constraint. For example,
to enforce a constraint between the average index of
features 1 and 2 and the index of feature 3, we would
have P (sync1,2;3t = 1|ind1t , ind2t , ind3t ) = 0 when-
ever ind1t , ind2t , ind3t violate the constraint.
In an end-to-end recognizer, the acoustic observations
would depend on the Sjt , which would be unobserved.
However, to facilitate quick experimentation and isolate
the pronunciation model, we begin by testing how well
we can do when given observed surface feature values.
4 Experiments
We have performed a pilot experiment using the follow-
ing feature set, based on the vocal tract variables of artic-
ulatory phonology (Browman and Goldstein, 1992): de-
gree of lip opening; tongue tip location and opening de-
gree; tongue body location and opening degree; velum
state; and glottal (voicing) state. We imposed the follow-
ing synchrony constraints: (1) All four tongue features
are completely synchronized; (2) the lips can desynchro-
nize from the tongue by up to one index; and (3) the glot-
tis and velum are synchronized, and their index must be
within 2 of the mean index of the tongue and lips.
We used the Graphical Models Toolkit (Bilmes and
Zweig, 2002) to implement the model. The distri-
butions p(Sjt |U jt ) were constructed by hand based on
linguistic considerations, e.g. that features tend to go
from more ?constricted? values to less constricted ones,
but not vice versa. p(U jt |lexEntryt, indjt ) was de-
rived from manually-constructed phoneme-to-feature-
probability mappings. For these experiments, no param-
eter learning has been done.
The task was to recognize an isolated word, given a set
of observed surface feature sequences Sjt . To create the
observations, we used the detailed phonetic transcriptions
created at ICSI for the Switchboard corpus (Greenberg et
al., 1996). For each word, we converted its transcrip-
tion to a sequence of feature vectors, one vector per 10
ms frame. For this purpose, we divided diphthongs and
stops into pairs of feature configurations. Given the input
feature sequences, we computed a Viterbi score for each
lexical entry in a 3000+-word (5500+-lexEntry) vocabu-
lary, by ?observing? the lexEntry variable and finding
the most likely settings of all remaining variables. The
most likely variable settings can be thought of as a mul-
tistream alignment between the surface and underlying
feature streams. Finally, we output the word correspond-
ing to the highest-scoring lexical entry.
We performed this procedure on a development set of
165 word transcriptions, which was used to tune settings
such as synchronization constraints, and a test set of 236
transcriptions 2. We compared the performance of sev-
eral models, measured in terms of word error rate (WER)
and failure rate (FR), the percentage of inputs that had no
Viterbi alignment with the correct word. To get a sense of
the effect of feature asynchrony, we compared our asyn-
chronous model with a version in which all features are
forced to be synchronized, so that only feature substitu-
tion is allowed. This uses the same DBN, but with de-
generate distributions for the synchronization variables.
Also, since the Sj values are derived from phonetic tran-
scriptions, and are therefore constant over several frames
at a time, we also built a variant of the DBN in which Sj
is allowed to change value with non-zero probability only
when indj changes (by adding parents indjt , indjt?1, Sjt?1
to Sjt ); we refer to this DBN as ?segment-based?, and to
the original as ?frame-based?. We compared four vari-
ants, differing along the ?synchronous vs. asynchronous?
and ?frame-based vs. segment-based? dimensions. The
variant which is both synchronous and segment-based is
similar to a phone-based pronunciation model with only
context-independent phone substitutions.
dev set test set
model WER FR WER FR
baseforms only 63.6 61.2 69.5 66.9
phonological rules 50.3 47.9 59.7 55.5
sync. seg.-based 38.2 24.8 43.2 35.2
sync. fr.-based 35.2 23.0 46.2 31.4
async. seg.-based 32.7 19.4 41.1 31.4
async. fr.-based 29.7 16.4 42.7 26.3
Table 2: Results of Switchboard ranking experiment.
Table 2 shows the performance of these four models,
as well as of two ?baseline? models: one allowing only
the baseform pronunciations (on average 1.7 per word),
and another including all pronunciations produced by
an extensive set of context-dependent phonological rules
(about 4 per word), with no feature substitutions or asyn-
chrony in either case. The phonological rules are the ?full
rule set? described in Hazen et al (2002). We note that
they were not designed with Switchboard in mind.
The models that allow asynchrony outperform the ones
that do not, in terms of both WER and FR. Looking more
closely at the performance on the development set, the
inputs on which the synchronous models failed but the
asynchronous models succeeded were in fact the kinds of
pronunciations that we expect to arise from feature asyn-
chrony, including: nasals replaced by nasalization on a
preceding vowel; a /t r/ sequence realized as /ch/; and
everybody ? [eh r uw ay]. The relative merits of the
frame-based and segment-based models is less clear, as
2We required that words in the development and test sets
have phonemic pronunciations with at least 4 phonemes, so as
to limit context effects from adjacent words.
they have opposite relative performance on the develop-
ment and test sets. For 27 (16.4%) development utter-
ances, none of the models was able to find an alignment
with the correct word. Most of these were due to apparent
gesture deletions and context-dependent feature changes,
which are not yet included in the model.
Figure 2 shows a part of the Viterbi alignment of ev-
erybody with [eh r uw ay], produced by the segment-
based, asynchronous model. Using this model, everybody
was the top-ranked word. As expected, the asynchrony is
manifested in the [uw] region, and the lips do not close
but reach only a narrow (glide-like) configuration.
Figure 2: Spectrogram, transcription, and partial Viterbi
alignment, including the lip opening and tongue tip loca-
tion variables. Indices are relative to the underlying pro-
nunciation /eh v r iy bcl b ah dx iy/. Adjacent frames with
equal values have been merged for easier viewing. WI =
wide; NA = narrow; CR = critical; CL = closed; ALV
= alveolar; P-A = palato-alveolar; RET = retroflex.
5 Discussion
We have motivated our pronunciation model as part of
an overall strategy of feature-based speech recognition.
One way in which this model could fit into a complete
recognizer is, as mentioned above, by adding a variable
A representing the acoustic observations, with the Sj as
its parents. The modeling of p(A|S1, . . . , SM ) (where M
is the number of features) is a significant problem in its
own right. Alternatively, as this study suggests, there may
be some benefit to this type of model even if the acoustic
model is phone-based. One possible setup would be to
use a phonetic recognizer to produce a phone lattice, then
convert the phones into features and proceed as in our
Switchboard experiments.
Thus far we have not trained the variable distribu-
tions. With the exception of the sync variables, these
can be trained from feature transcriptions (i.e. Sj obser-
vations) using the Expectation-Maximization (EM) algo-
rithm (Dempster et al, 1977). In the absence of actual
feature transcriptions, they can be approximated by con-
verting detailed phonetic transcriptions, as we have done
in our decoding experiments above. The sync distribu-
tions cannot be trained via EM, since they are always
observed with value 1. They can either be treated as ex-
perimental parameters or trained discriminatively. We are
currently working on a new formulation in which the syn-
chronization constraints can be trained via EM.
In addition, we are currently investigating extensions
to the model, including context-dependent feature substi-
tutions. We also plan to extend this study to a larger data
set and to multi-word utterances.
References
J. Bilmes and G. Zweig, ?The Graphical Models Toolkit: An
open source software system for speech and time-series pro-
cessing,? ICASSP, Orlando, 2002.
C. P. Browman and L. Goldstein, ?Articulatory phonology: An
overview,? Phonetica, 49:155?180, 1992.
T. Dean and K. Kanazawa, ?A model for reasoning about per-
sistence and causation,? Computational Intelligence, 5:142?
150, 1989.
A. P. Dempster, N. M. Laird, and D. B. Rubin, ?Maximum Like-
lihood from Incomplete Data via the EM Algorithm,? Jour-
nal of the Royal Statistical Society, 39:1?38,1977.
L. Deng, G. Ramsay, and D. Sun, ?Production models as a struc-
tural basis for automatic speech recognition,? Speech Com-
munication, 33:93?111, 1997.
S. Greenberg, J. Hollenback, and D. Ellis, ?Insights into spoken
language gleaned from phonetic transcription of the Switch-
board corpus,? ICSLP, Philadelphia, 1996.
T. J. Hazen, I. L. Hetherington, H. Shu, and K. Livescu, ?Pro-
nunciation modeling using a finite-state transducer represen-
tation,? ITRW PMLA, Estes Park, CO, 2002.
S. King, T. Stephenson, S. Isard, P. Taylor, and A. Strachan,
?Speech recognition via phonetically featured syllables,? IC-
SLP, Sydney, 1998.
K. Kirchhoff, ?Syllable-level desynchronisation of phonetic
features for speech recognition,? ICSLP, Philadelphia, 1996.
K. Livescu, J. Glass, and J. Bilmes, ?Hidden feature models
for speech recognition using dynamic Bayesian networks,?
Eurospeech, Geneva, 2003.
D. McAllester, L. Gillick, F. Scattone, and M. Newman, ?Fab-
ricating conversational speech data with acoustic models: A
program to examine model-data mismatch,? ICSLP, Sydney,
1998.
F. Metze and A. Waibel, ?A flexible stream architecture for ASR
using articulatory features,? ICSLP, Denver, 2002.
M. Ostendorf, ?Incorporating linguistic theories of pronuncia-
tion variation into speech-recognition models,? Phil. Trans.
R. Soc. Lond. A, 358:1325-1338, 2000.
M. Richardson, J. Bilmes, and C. Diorio, ?Hidden-articulator
Markov models for speech recognition,? ITRW ASR2000,
Paris, 2000.
M. D. Riley and A. Ljolje, ?Automatic generation of detailed
pronunciation lexicons,? in C.-H. Lee, F. K. Soong, and K.
K. Paliwal (eds.), Automatic Speech and Speaker Recogni-
tion, Kluwer Academic Publishers, Boston, 1996.
Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, page 1,
Columbus, Ohio, USA June 2008. c?2008 Association for Computational Linguistics
Invited Talk:
Phonological Models in Automatic Speech Recognition
Karen Livescu
Toyota Technological Institute at Chicago
1427 E. 60th St., Chicago, IL 60637
klivescu@tti-c.org
Abstract
The performance of automatic speech recognition systems varies widely across different contexts. Very
good performance can be achieved on single-speaker, large-vocabulary dictation in a clean acoustic environ-
ment, as well as on very small vocabulary tasks with much fewer constraints on the speakers and acoustic
conditions. In other domains, speech recognition is still far from usable for real-world applications. One
domain that is still elusive is that of spontaneous conversational speech. This type of speech poses a number
of challenges, such as the presence of disfluencies, a mix of speech and non-speech sounds such as laughter,
and extreme variation in pronunciation. In this talk, I will focus on the challenge of pronunciation variation.
A number of analyses suggest that this variability is responsible for a large part of the drop in recognition
performance between read (dictated) speech and conversational speech.
I will describe efforts in the speech recognition community to characterize and model pronunciation
variation, both for conversational speech and in general. The work can be roughly divided into several types
of approaches, including: augmentation of a phonetic pronunciation lexicon with phonological rules; the use
of large (syllable- or word-sized) units instead of the more traditional phonetic ones; and the use of smaller
units, such as distinctive or articulatory features. Of these, the first is the most thoroughly studied and
also the most disappointing: Despite successes in a few domains, it has been difficult to obtain significant
recognition improvements by including in the lexicon those phonetic pronunciations that appear to exist in
the data. In part as a reaction to this, many have advocated the use of a ?null pronunciation model,? i.e. a
very limited lexicon including only canonical pronunciations. The assumption in this approach is that the
observation model?the distribution of the acoustics given phonetic units?will better model the ?noise?
introduced by pronunciation variability.
I will advocate an alternative view: that the phone unit may not be the most appropriate for modeling the
lexicon. When considering a variety of pronunciation phenomena, it becomes apparent that phonetic tran-
scription often obscures some of the fundamental processes that are at play. I will describe approaches using
both larger and ?smaller? units. Larger units are typically syllables or words, and allow greater freedom to
model the component states of each unit. In the class of ?smaller? unit models, ideas from articulatory and
autosegmental phonology motivate multi-tier models in which different features (or groups of features) have
semi-independent behavior. I will present a particular model in which articulatory features are represented
as variables in a dynamic Bayesian network.
Non-phonetic pronunciation models can involve significantly different model structures than those typi-
cally used in speech recognition, and as a result they may also entail modifications to other components such
as the observation model and training algorithms. At this point it is not clear what the ?winning? approach
will be. The success of a given approach may depend on the domain or on the amount and type of training
data available. I will describe some of the current challenges and ongoing work, with a particular focus on
the role of phonological theories in statistical models of pronunciation (and vice versa?).
1
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 194?203,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Discriminative Pronunciation Modeling:
A Large-Margin, Feature-Rich Approach
Hao Tang, Joseph Keshet, and Karen Livescu
Toyota Technological Institute at Chicago
Chicago, IL USA
{haotang,jkeshet,klivescu}@ttic.edu
Abstract
We address the problem of learning the map-
ping between words and their possible pro-
nunciations in terms of sub-word units. Most
previous approaches have involved genera-
tive modeling of the distribution of pronuncia-
tions, usually trained to maximize likelihood.
We propose a discriminative, feature-rich ap-
proach using large-margin learning. This ap-
proach allows us to optimize an objective
closely related to a discriminative task, to
incorporate a large number of complex fea-
tures, and still do inference efficiently. We
test the approach on the task of lexical access;
that is, the prediction of a word given a pho-
netic transcription. In experiments on a sub-
set of the Switchboard conversational speech
corpus, our models thus far improve classi-
fication error rates from a previously pub-
lished result of 29.1% to about 15%. We
find that large-margin approaches outperform
conditional random field learning, and that
the Passive-Aggressive algorithm for large-
margin learning is faster to converge than the
Pegasos algorithm.
1 Introduction
One of the problems faced by automatic speech
recognition, especially of conversational speech, is
that of modeling the mapping between words and
their possible pronunciations in terms of sub-word
units such as phones. While pronouncing dictionar-
ies provide each word?s canonical pronunciation(s)
in terms of phoneme strings, running speech of-
ten includes pronunciations that differ greatly from
the dictionary. For example, some pronunciations
of ?probably? in the Switchboard conversational
speech database are [p r aa b iy], [p r aa l iy], [p r
ay], and [p ow ih] (Greenberg et al, 1996). While
some words (e.g., common words) are more prone
to such variation than others, the effect is extremely
general: In the phonetically transcribed portion of
Switchboard, fewer than half of the word tokens
are pronounced canonically (Fosler-Lussier, 1999).
In addition, pronunciation variants sometimes in-
clude sounds not present in the dictionary at all,
such as nasalized vowels (?can?t? ? [k ae n n t])
or fricatives introduced due to incomplete consonant
closures (?legal? ? [l iy g fr ix l]).1 This varia-
tion makes pronunciation modeling one of the major
challenges facing speech recognition (McAllaster et
al., 1998; Jurafsky et al, 2001; Sarac?lar and Khu-
danpur, 2004; Bourlard et al, 1999). 2
Most efforts to address the problem have involved
either learning alternative pronunciations and/or
their probabilities (Holter and Svendsen, 1999) or
using phonetic transformation (substitution, inser-
tion, and deletion) rules, which can come from lin-
guistic knowledge or be learned from data (Riley
et al, 1999; Hazen et al, 2005; Hutchinson and
Droppo, 2011). These have produced some im-
provements in recognition performance. However,
they also tend to cause additional confusability due
to the introduction of additional homonyms (Fosler-
1We use the ARPAbet phonetic alphabet with additional di-
acritics, such as [ n] for nasalization and [ fr] for frication.
2This problem is separate from the grapheme-to-phoneme
problem, in which pronunciations are predicted from a word?s
spelling; here, we assume the availability of a dictionary of
canonical pronunciations as is usual in speech recognition.
194
Lussier et al, 2002). Some other alternatives are
articulatory pronunciation models, in which words
are represented as multiple parallel sequences of ar-
ticulatory features rather than single sequences of
phones, and which outperform phone-based models
on some tasks (Livescu and Glass, 2004; Jyothi et
al., 2011); and models for learning edit distances be-
tween dictionary and actual pronunciations (Ristad
and Yianilos, 1998; Filali and Bilmes, 2005).
All of these approaches are generative?i.e., they
provide distributions over possible pronunciations
given the canonical one(s)?and they are typically
trained by maximizing the likelihood over train-
ing data. In some recent work, discriminative ap-
proaches have been proposed, in which an objective
more closely related to the task at hand is optimized.
For example, (Vinyals et al, 2009; Korkmazskiy
and Juang, 1997) optimize a minimum classification
error (MCE) criterion to learn the weights (equiv-
alently, probabilities) of alternative pronunciations
for each word; (Schramm and Beyerlein, 2001) use
a similar approach with discriminative model com-
bination. In this work, the weighted alternatives are
then used in a standard (generative) speech recog-
nizer. In other words, these approaches optimize
generative models using discriminative criteria.
We propose a general, flexible discriminative ap-
proach to pronunciation modeling, rather than dis-
criminatively optimizing a generative model. We
formulate a linear model with a large number
of word-level and subword-level feature functions,
whose weights are learned by optimizing a discrim-
inative criterion. The approach is related to the re-
cently proposed segmental conditional random field
(SCRF) approach to speech recognition (Zweig et
al., 2011). The main differences are that we opti-
mize large-margin objective functions, which lead
to sparser, faster, and better-performing models than
conditional random field optimization in our exper-
iments; and we use a large set of different feature
functions tailored to pronunciation modeling.
In order to focus attention on the pronunciation
model alone, our experiments focus on a task that
measures only the mapping between words and sub-
word units. Pronunciation models have in the past
been tested using a variety of measures. For gener-
ative models, phonetic error rate of generated pro-
nunciations (Venkataramani and Byrne, 2001) and
phone- or frame-level perplexity (Riley et al, 1999;
Jyothi et al, 2011) are appropriate measures. For
our discriminative models, we consider the task
of lexical access; that is, prediction of a single
word given its pronunciation in terms of sub-word
units (Fissore et al, 1989; Jyothi et al, 2011). This
task is also sometimes referred to as ?pronunciation
recognition? (Ristad and Yianilos, 1998) or ?pro-
nunciation classification? (Filali and Bilmes, 2005).)
As we show below, our approach outperforms both
traditional phonetic rule-based models and the best
previously published results on our data set obtained
with generative articulatory approaches.
2 Problem setting
We define a pronunciation of a word as a representa-
tion of the way it is produced by a speaker in terms
of some set of linguistically meaningful sub-word
units. A pronunciation can be, for example, a se-
quence of phones or multiple sequences of articu-
latory features such as nasality, voicing, and tongue
and lip positions. For purposes of this paper, we will
assume that a pronunciation is a single sequence of
units, but the approach applies to other representa-
tions. We distinguish between two types of pronun-
ciations of a word: (i) canonical pronunciations, the
ones typically found in the dictionary, and (ii) sur-
face pronunciations, the ways a speaker may actu-
ally produce the word. In the task of lexical access
we are given a surface pronunciation of a word, and
our goal is to predict the word.
Formally, we define a pronunciation as a sequence
of sub-word units p = (p1, p2, . . . , pK), where pk ?
P for all 1 ? k ? K and P is the set of all sub-word
units. The index k can represent either a fixed-length
frame or a variable-length segment. P? denotes the
set of all finite-length sequences over P . We denote
a word by w ? V where V is the vocabulary. Our
goal is to find a function f : P? ? V that takes as
input a surface pronunciation and returns the word
from the vocabulary that was spoken.
In this paper we propose a discriminative super-
vised learning approach for learning the function f
from a training set of pairs (p, w). We aim to find a
function f that performs well on the training set as
well as on unseen examples. Let w? = f(p) be the
predicted word given the pronunciation p. We assess
the quality of the function f by the zero-one loss: if
195
w 6= w? then the error is one, otherwise the error is
zero. The goal of the learning process is to mini-
mize the expected zero-one loss, where the expec-
tation is taken with respect to a fixed but unknown
distribution over words and surface pronunciations.
In the next section we present a learning algorithm
that aims to minimize the expected zero-one loss.
3 Algorithm
Similarly to previous work in structured prediction
(Taskar et al, 2003; Tsochantaridis et al, 2005),
we construct the function f from a predefined set
of N feature functions, {?j}Nj=1, each of the form
?j : P??V ? R. Each feature function takes a sur-
face pronunciation p and a proposed word w and re-
turns a scalar which, intuitively, should be correlated
with whether the pronunciation p corresponds to the
word w. The feature functions map pronunciations
of different lengths along with a proposed word to a
vector of fixed dimension in RN . For example, one
feature function might measure the Levenshtein dis-
tance between the pronunciation p and the canonical
pronunciation of the word w. This feature function
counts the minimum number of edit operations (in-
sertions, deletions, and substitutions) that are needed
to convert the surface pronunciation to the canonical
pronunciation; it is low if the surface pronunciation
is close to the canonical one and high otherwise.
The function f maximizes a score relating the
word w to the pronunciation p. We restrict our-
selves to scores that are linear in the feature func-
tions, where each ?j is scaled by a weight ?j :
N?
j=1
?j?j(p, w) = ? ? ?(p, w),
where we have used vector notation for the feature
functions ? = (?1, . . . , ?N ) and for the weights
? = (?1, . . . , ?N ). Linearity is not a very strong
restriction, since the feature functions can be arbi-
trarily non-linear. The function f is defined as the
word w that maximizes the score,
f(p) = argmax
w?V
? ? ?(p, w).
Our goal in learning ? is to minimize the expected
zero-one loss:
?? = argmin
?
E(p,w)??
[
1w 6=f(p)
]
,
where 1pi is 1 if predicate pi holds and 0 other-
wise, and where ? is an (unknown) distribution from
which the examples in our training set are sampled
i.i.d. Let S = {(p1, w1), . . . , (pm, wm)} be the
training set. Instead of working directly with the
zero-one loss, which is non-smooth and non-convex,
we use the surrogate hinge loss, which upper-bounds
the zero-one loss:
L(?, pi, wi) = max
w?V
[
1wi 6=w
? ? ? ?(pi, wi) + ? ? ?(pi, w)
]
. (1)
Finding the weight vector ? that minimizes the
`2-regularized average of this loss function is the
structured support vector machine (SVM) problem
(Taskar et al, 2003; Tsochantaridis et al, 2005):
?? = argmin
?
?
2
???2 +
1
m
m?
i=1
L(?, pi, wi), (2)
where ? is a user-defined tuning parameter that bal-
ances between regularization and loss minimization.
In practice, we have found that solving the
quadratic optimization problem given in Eq. (2) con-
verges very slowly using standard methods such as
stochastic gradient descent (Shalev-Shwartz et al,
2007). We use a slightly different algorithm, the
Passive-Aggressive (PA) algorithm (Crammer et al,
2006), whose average loss is comparable to that of
the structured SVM solution (Keshet et al, 2007).
The Passive-Aggressive algorithm is an efficient
online algorithm that, under some conditions, can
be viewed as a dual-coordinate ascent minimizer of
Eq. (2) (The connection to dual-coordinate ascent
can be found in (Hsieh et al, 2008)). The algorithm
begins by setting ? = 0 and proceeds in rounds.
In the t-th round the algorithm picks an example
(pi, wi) from S at random uniformly without re-
placement. Denote by ?t?1 the value of the weight
vector before the t-th round. Let w?ti denote the pre-
dicted word for the i-th example according to ?t?1:
w?ti = argmax
w?V
?t?1 ? ?(pi, w) + 1wi 6=w.
Let ??ti = ?(pi, wi) ? ?(pi, w?
t
i). Then the algo-
rithm updates the weight vector ?t as follows:
?t = ?t?1 + ?ti??
t
i (3)
196
where
?ti = min
{
1
?m
,
1wi 6=w?ti
? ? ???ti
???ti?
}
.
In practice we iterate over the m examples in the
training set several times; each such iteration is an
epoch. The final weight vector is set to the average
over all weight vectors during training.
An alternative loss function that is often used to
solve structured prediction problems is the log-loss:
L(?, pi, wi) = ? logP?(wi|pi) (4)
where the probability is defined as
P?(wi|pi) =
e???(pi,wi)
?
w?V e
???(p,w)
.
Minimization of Eq. (2) under the log-loss results in
a probabilistic model commonly known as a condi-
tional random field (CRF) (Lafferty et al, 2001). By
taking the sub-gradient of Eq. (4), we can obtain an
update rule similar to the one shown in Eq. (3).
4 Feature functions
Before defining the feature functions, we define
some notation. Suppose p ? P? is a sequence of
sub-word units. We use p1:n to denote the n-gram
substring p1 . . . pn. The two substrings a and b are
said to be equal if they have the same length and
ai = bi for 1 ? i ? n. For a given sub-word unit n-
gram u ? Pn, we use the shorthand u ? p to mean
that we can find u in p; i.e., there exists an index i
such that pi:i+n = u. We use |p| to denote the length
of the sequence p.
We assume we have a pronunciation dictionary,
which is a set of words and their baseforms. We ac-
cess the dictionary through the function pron, which
takes a word w ? V and returns a set of baseforms.
4.1 TF-IDF feature functions
Term frequency (TF) and inverse document fre-
quency (IDF) are measures that have been heavily
used in information retrieval to search for documents
using word queries (Salton et al, 1975). Similarly to
(Zweig et al, 2010), we adapt TF and IDF by treat-
ing a sequence of sub-word units as a ?document?
and n-gram sub-sequences as ?words.? In this anal-
ogy, we use sub-sequences in surface pronunciations
to ?search? for baseforms in the dictionary. These
features measure the frequency of each n-gram in
observed pronunciations of a given word in the train-
ing set, along with the discriminative power of the n-
gram. These features are therefore only meaningful
for words actually observed in training.
The term frequency of a sub-word unit n-gram
u ? Pn in a sequence p is the length-normalized
frequency of the n-gram in the sequence:
TFu(p) =
1
|p| ? |u|+ 1
|p|?|u|+1?
i=1
1u=pi:i+|u|?1 .
Next, define the set of words in the training set that
contain the n-gram u as Vu = {w ? V | (p, w) ?
S, u ? p}. The inverse document frequency (IDF)
of an n-gram u is defined as
IDFu = log
|V|
|Vu|
.
IDF represents the discriminative power of an n-
gram: An n-gram that occurs in few words is better
at word discrimination than a very common n-gram.
Finally, we define word-specific features using TF
and IDF. Suppose the vocabulary is indexed: V =
{w1, . . . , wn}. Define ew as a binary vector with
elements
(ew)i = 1wi=w.
We define the TF-IDF feature function of u as
?u(p, w) = (TFu(p)? IDFu)? ew,
where ? : Ra?b ? Rc?d ? Rac?bd is the tensor
product. We therefore have as many TF-IDF feature
functions as we have n-grams. In practice, we only
consider n-grams of a certain order (e.g., bigrams).
The following toy example demonstrates how the
TF-IDF features are computed. Suppose we have
V = {problem, probably}. The dictionary maps
?problem? to /pcl p r aa bcl b l ax m/ and ?prob-
ably? to /pcl p r aa bcl b l iy/, and our input is
(p, w) = ([p r aa b l iy], problem). Then for the bi-
gram /l iy/, we have TF/l iy/(p) = 1/5 (one out of
five bigrams in p), and IDF/l iy/ = log(2/1) (one
word out of two in the dictionary). The indicator
vector is eproblem =
[
1 0
]>
, so the final feature is
?/l iy/(p, w) =
[1
5 log
2
1
0
]
.
197
4.2 Length feature function
The length feature functions measure how the length
of a word?s surface form tends to deviate from the
baseform. These functions are parameterized by a
and b and are defined as
?a??`<b(p, w) = 1a??`<b ? ew,
where ?` = |p| ? |v|, for some baseform v ?
pron(w). The parameters a and b can be either posi-
tive or negative, so the model can learn whether the
surface pronunciations of a word tend to be longer
or shorter than the baseform. Like the TF-IDF fea-
tures, this feature is only meaningful for words ac-
tually observed in training.
As an example, suppose we have V =
{problem, probably}, and the word ?probably? has
two baseforms, /pcl p r aa bcl b l iy/ (of length
eight) and /pcl p r aa bcl b ax bcl b l iy/ (of length
eleven). If we are given an input (p, w) =
([pcl p r aa bcl l ax m], probably), whose length of
the surface form is eight, then the length features for
the ranges 0 ? ?` < 1 and ?3 ? ?` < ?2 are
?0??`<1(p, w) =
[
0 1
]>
,
??3??`<?2(p, w) =
[
0 1
]>
,
respectively. Other length features are all zero.
4.3 Phonetic alignment feature functions
Beyond the length, we also measure specific pho-
netic deviations from the dictionary. We define pho-
netic alignment features that count the (normalized)
frequencies of phonetic insertions, phonetic dele-
tions, and substitutions of one surface phone for an-
other baseform phone. Given (p, w), we use dy-
namic programming to align the surface form p with
all of the baseforms of w. Following (Riley et al,
1999), we encode a phoneme/phone with a 4-tuple:
consonant manner, consonant place, vowel manner,
and vowel place. Let the dash symbol ??? be a
gap in the alignment (corresponding to an inser-
tion/deletion). Given p, q ? P ? {?}, we say that
a pair (p, q) is a deletion if p ? P and q = ?, is
an insertion if p = ? and q ? P , and is a substi-
tution if both p, q ? P . Given p, q ? P ? {?}, let
(s1, s2, s3, s4) and (t1, t2, t3, t4) be the correspond-
ing 4-tuple encoding of p and q, respectively. The
pcl p r aa pcl p er l iy
pcl p r aa bcl b ? l iy
pcl p r aa pcl p er ? ? l iy
pcl p r aa bcl b ax bcl b l iy
Table 1: Possible alignments of [p r aa pcl p er l iy] with
two baseforms of ?probably? in the dictionary.
similarity between p and q is defined as
s(p, q) =
{
1, if p = ? or q = ?;
?4
i=1 1si=ti , otherwise.
Consider aligning p with the Kw = |pron(w)|
baseforms of w. Define the length of the align-
ment with the k-th baseform as Lk, for 1 ? k ?
Kw. The resulting alignment is a sequence of pairs
(ak,1, bk,1), . . . , (ak,Lk , bk,Lk), where ak,i, bk,i ?
P ? {?} for 1 ? i ? Lk. Now we define the align-
ment features, given p, q ? P ? {?}, as
?p?q(p, w) =
1
Zp
Kw?
k=1
Lk?
i=1
1ak,i=p, bk,i=q,
where the normalization term is
Zp =
{?Kw
k=1
?Lk
i=1 1ak,i=p, if p ? P ;
|p| ?Kw if p = ?.
The normalization for insertions differs from the
normalization for substitutions and deletions, so that
the resulting values always lie between zero and one.
As an example, consider the input pair (p, w) =
([p r aa pcl p er l iy], probably) and suppose there
are two baseforms of the word ?probably? in the
dictionary. Let one possible alignments be the one
shown in Table 1. Since /p/ occurs four times in the
alignments and two of them are aligned to [b], the
feature for p? b is then ?p?b(p, w) = 2/4.
Unlike the TF-IDF feature functions and the
length feature functions, the alignment feature func-
tions can assign a non-zero score to words that are
not seen at training time (but are in the dictionary),
as long as there is a good alignment with their base-
forms. The weights given to the alignment fea-
tures are the analogue of substitution, insertion, and
deletion rule probabilities in traditional phone-based
pronunciation models such as (Riley et al, 1999);
they can also be seen as a generalized version of the
Levenshtein features of (Zweig et al, 2011).
198
4.4 Dictionary feature function
The dictionary feature is an indicator of whether
a pronunciation is an exact match to a baseform,
which also generalizes to words unseen in training.
We define the dictionary feature as
?dict(p, w) = 1p?pron(w).
For example, assume there is a baseform
/pcl p r aa bcl b l iy/ for the word ?probably? in
the dictionary, and p = /pcl p r aa bcl b l iy/. Then
?dict(p, probably) = 1, while ?dict(p, problem) = 0.
4.5 Articulatory feature functions
Articulatory models represented as dynamic
Bayesian networks (DBNs) have been successful
in the past on the lexical access task (Livescu
and Glass, 2004; Jyothi et al, 2011). In such
models, pronunciation variation is seen as the
result of asynchrony between the articulators (lips,
tongue, etc.) and deviations from the intended
articulatory positions. Given a sequence p and a
word w, we use the DBN to produce an alignment
at the articulatory level, which is a sequence of
7-tuples, representing the articulatory variables3 lip
opening, tongue tip location and opening, tongue
body location and opening, velum opening, and
glottis opening. We extract three kinds of features
from the output?substitutions, asynchrony, and
log-likelihood.
The substitution features are similar to the pho-
netic alignment features in Section 4.3, except that
the alignment is not a sequence of pairs but a se-
quence of 14-tuples (7 for the baseform and 7 for the
surface form). The DBN model is based on articu-
latory phonology (Browman and Goldstein, 1992),
in which there are no insertions and deletions, only
substitutions (apparent insertions and deletions are
accounted for by articulatory asynchrony). For-
mally, consider the seven sets of articulatory vari-
able values F1, . . . , F7. For example, F1 could be
all of the values of lip opening, F1 ={closed, crit-
ical, narrow, wide}. Let F = {F1, . . . , F7}. Con-
sider an articulatory variable F ? F . Suppose the
alignment for F is (a1, b1), . . . , (aL, bL), where L
3We use the term ?articulatory variable? for the ?articulatory
features? of (Livescu and Glass, 2004; Jyothi et al, 2011), in
order to avoid confusion with our feature functions.
is the length of the alignment and ai, bi ? F , for
1 ? i ? L. Here the ai are the intended articulatory
variable values according to the baseform, and the
bi are the corresponding realized values. For each
a, b ? F we define a substitution feature function:
?a?b(p, w) =
1
L
L?
i=1
1ai=a, bi=b.
The asynchrony features are also extracted from
the DBN alignments. Articulators are not always
synchronized, which is one cause of pronunciation
variation. We measure this by looking at the phones
that two articulators are aiming to produce, and find
the time difference between them. Formally, we
consider two articulatory variables Fh, Fk ? F .
Let the alignment between the two variables be
(a1, b1), . . . , (aL, bL), where now ai ? Fh and bi ?
Fk. Each ai and bi can be mapped back to the cor-
responding phone index th,i and tk,i, for 1 ? i ? L.
The average degree of asynchrony is then defined as
async(Fh, Fk) =
1
L
L?
i=1
(th,i ? tk,i) .
More generally, we compute the average asynchrony
between any two sets of variables F1,F2 ? F as
async(F1,F2) =
1
L
L?
i=1
?
?
1
|F1|
?
Fh?F1
th,i ?
1
|F2|
?
Fk?F2
tk,i
?
? .
We then define the asynchrony features as
?a?async(F1,F2)?b = 1a?async(F1,F2)?b.
Finally, the log-likelihood feature is the DBN
alignment score, shifted and scaled so that the value
lies between zero and one,
?dbn-LL(p, w) =
L(p, w)? h
c
,
where L is the log-likelihood function of the DBN,
h is the shift, and c is the scale.
Note that none of the DBN features are word-
specific, so that they generalize to words in the dic-
tionary that are unseen in the training set.
5 Experiments
All experiments are conducted on a subset of the
Switchboard conversational speech corpus that has
199
been labeled at a fine phonetic level (Greenberg et
al., 1996); these phonetic transcriptions are the input
to our lexical access models. The data subset, phone
set P , and dictionary are the same as ones previ-
ously used in (Livescu and Glass, 2004; Jyothi et al,
2011). The dictionary contains 3328 words, consist-
ing of the 5000 most frequent words in Switchboard,
excluding ones with fewer than four phones in their
baseforms. The baseforms use a similar, slightly
smaller phone set (lacking, e.g., nasalization). We
measure performance by error rate (ER), the propor-
tion of test examples predicted incorrectly.
The TF-IDF features used in the experiments
are based on phone bigrams. For all of the ar-
ticulatory DBN features, we use the DBN from
(Livescu, 2005) (the one in (Jyothi et al, 2011)
is more sophisticated and may be used in fu-
ture work). For the asynchrony features, the ar-
ticulatory pairs are (F1,F2) ? {({tongue tip},
{tongue body}), ({lip opening}, {tongue tip,
tongue body}), and ({lip opening, tongue tip,
tongue body}, {glottis, velum})}, as in (Livescu,
2005). The parameters (a, b) of the length and
asynchrony features are drawn from (a, b) ?
{(?3,?2), (?2,?1), . . . (2, 3)}.
We compare the CRF4, Passive-Aggressive (PA),
and Pegasos learning algorithms. The regularization
parameter ? is tuned on the development set. We run
all three algorithms for multiple epochs and pick the
best epoch based on development set performance.
For the first set of experiments, we use the same
division of the corpus as in (Livescu and Glass,
2004; Jyothi et al, 2011) into a 2492-word train-
ing set, a 165-word development set, and a 236-
word test set. To give a sense of the difficulty of
the task, we test two simple baselines. One is a lex-
icon lookup: If the surface form is found in the dic-
tionary, predict the corresponding word; otherwise,
guess randomly. For a second baseline, we calcu-
late the Levenshtein (0-1 edit) distance between the
input pronunciation and each dictionary baseform,
and predict the word corresponding to the baseform
closest to the input. The results are shown in the first
two rows of Table 2. We can see that, by adding just
the Levenshtein distance, the error rate drops signif-
4We use the term ?CRF? since the learning algorithm corre-
sponds to CRF learning, although the task is multiclass classifi-
cation rather than a sequence or structure prediction task.
Model ER
lexicon lookup (from (Livescu, 2005)) 59.3%
lexicon + Levenshtein distance 41.8%
(Jyothi et al, 2011) 29.1%
CRF/DP+ 21.5%
PA/DP+ 15.2%
Pegasos/DP+ 14.8%
PA/ALL 15.2%
Table 2: Lexical access error rates (ER) on the same data
split as in (Livescu and Glass, 2004; Jyothi et al, 2011).
Models labeled X/Y use learning algorithm X and feature
set Y. The feature set DP+ contains TF-IDF, DP align-
ment, dictionary, and length features. The set ALL con-
tains DP+ and the articulatory DBN features. The best
results are in bold; the differences among them are in-
significant (according to McNemar?s test with p = .05).
icantly. However, both baselines do quite poorly.
Table 2 shows the best previous result on this data
set from the articulatory model of Jyothi et al, which
greatly improves over our baselines as well as over
a much more complex phone-based model (Jyothi
et al, 2011). The remaining rows of Table 2 give
results with our feature functions and various learn-
ing algorithms. The best result for PA/DP+ (the PA
algorithm using all features besides the DBN fea-
tures) on the development set is with ? = 100 and 5
epochs. Tested on the test set, this model improves
over (Jyothi et al, 2011) by 13.9% absolute (47.8%
relative). The best result for Pegasos with the same
features on the development set is with ? = 0.01 and
10 epochs. On the test set, this model gives a 14.3%
absolute improvement (49.1% relative). CRF learn-
ing with the same features performs about 6% worse
than the corresponding PA and Pegasos models.
The single-threaded running time for PA/DP+ and
Pegasos/DP+ is about 40 minutes per epoch, mea-
sured on a dual-core AMD 2.4GHz CPU with 8GB
of memory; for CRF, it takes about 100 minutes for
each epoch, which is almost entirely because the
weight vector ? is less sparse with CRF learning.
In the PA and Pegasos algorithms, we only update ?
for the most confusable word, while in CRF learn-
ing, we sum over all words. In our case, the number
of non-zero entries in ? for PA and Pegasos is around
800,000; for CRF, it is over 4,000,000. Though PA
and Pegasos take roughly the same amount of time
per epoch, Pegasos tends to require more epochs to
200
Figure 1: 5-fold cross validation (CV) results. The lex-
icon lookup baseline is labeled lex; lex + lev = lexi-
con lookup with Levenshtein distance. Each point cor-
responds to the test set error rate for one of the 5 data
splits. The horizontal red line marks the mean of the re-
sults with means labeled, and the vertical red line indi-
cates the mean plus and minus one standard deviation.
achieve the same performance as PA.
For the second experiment, we perform 5-fold
cross-validation. We combine the training, devel-
opment, and test sets from the previous experiment,
and divide the data into five folds. We take three
folds for training, one fold for tuning ? and the best
epoch, and the remaining fold for testing. The re-
sults on the test fold are shown in Figure 1, which
compares the learning algorithms, and Figure 2,
which compares feature sets. Overall, the results
are consistent with our first experiment. The fea-
ture selection experiments in Figure 2 shows that
the TF-IDF features alone are quite weak, while the
dynamic programming alignment features alone are
quite good. Combining the two gives close to our
best result. Although the marginal improvement gets
smaller as we add more features, in general perfor-
mance keeps improving the more features we add.
6 Discussion
The results in Section 5 are the best obtained thus
far on the lexical access task on this conversational
data set. Large-margin learning, using the Passive-
Aggressive and Pegasos algorithms, has benefits
over CRF learning for our task: It produces sparser
models, is faster, and produces better lexical access
results. In addition, the PA algorithm is faster than
Pegasos on our task, as it requires fewer epochs.
Our ultimate goal is to incorporate such models
into complete speech recognizers, that is to predict
word sequences from acoustics. This requires (1)
Figure 2: Feature selection results for five-fold cross val-
idation. In the figure, phone bigram TF-IDF is labeled
p2; phonetic alignment with dynamic programming is la-
beled DP. The dots and lines are as defined in Figure 1.
extension of the model and learning algorithm to
word sequences and (2) feature functions that re-
late acoustic measurements to sub-word units. The
extension to sequences can be done analogously to
segmental conditional random fields (SCRFs). The
main difference between SCRFs and our approach
would be the large-margin learning, which can be
straightforwardly applied to sequences. To incorpo-
rate acoustics, we can use feature functions based on
classifiers of sub-word units, similarly to previous
work on CRF-based speech recognition (Gunawar-
dana et al, 2005; Morris and Fosler-Lussier, 2008;
Prabhavalkar et al, 2011). Richer, longer-span (e.g.,
word-level) feature functions are also possible.
Thus far we have restricted the pronunciation-to-
word score to linear combinations of feature func-
tions. This can be extended to non-linear combi-
nations using a kernel. This may be challenging in
a high-dimensional feature space. One possibility
is to approximate the kernels as in (Keshet et al,
2011). Additional extensions include new feature
functions, such as context-sensitive alignment fea-
tures, and joint inference and learning of the align-
ment models embedded in the feature functions.
Acknowledgments
We thank Raman Arora, Arild N?ss, and the anony-
mous reviewers for helpful suggestions. This re-
search was supported in part by NSF grant IIS-
0905633. The opinions expressed in this work are
those of the authors and do not necessarily reflect
the views of the funding agency.
201
References
H. Bourlard, S. Furui, N. Morgan, and H. Strik. 1999.
Special issue on modeling pronunciation variation for
automatic speech recognition. Speech Communica-
tion, 29(2-4).
C. P. Browman and L. Goldstein. 1992. Articulatory
phonology: an overview. Phonetica, 49(3-4).
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive aggressive al-
gorithms. Journal of Machine Learning Research, 7.
K. Filali and J. Bilmes. 2005. A dynamic Bayesian
framework to model context and memory in edit dis-
tance learning: An application to pronunciation classi-
fication. In Proc. Association for Computational Lin-
guistics (ACL).
L. Fissore, P. Laface, G. Micca, and R. Pieraccini. 1989.
Lexical access to large vocabularies for speech recog-
nition. IEEE Transactions on Acoustics, Speech, and
Signal Processing, 37(8).
E. Fosler-Lussier, I. Amdal, and H.-K. J. Kuo. 2002. On
the road to improved lexical confusability metrics. In
ISCA Tutorial and Research Workshop (ITRW) on Pro-
nunciation Modeling and Lexicon Adaptation for Spo-
ken Language Technology.
J. E. Fosler-Lussier. 1999. Dynamic Pronunciation Mod-
els for Automatic Speech Recognition. Ph.D. thesis, U.
C. Berkeley.
S. Greenberg, J. Hollenback, and D. Ellis. 1996. Insights
into spoken language gleaned from phonetic transcrip-
tion of the Switchboard corpus. In Proc. International
Conference on Spoken Language Processing (ICSLP).
A. Gunawardana, M. Mahajan, A. Acero, and J. Platt.
2005. Hidden conditional random fields for phone
classification. In Proc. Interspeech.
T. J. Hazen, I. L. Hetherington, H. Shu, and K. Livescu.
2005. Pronunciation modeling using a finite-state
transducer representation. Speech Communication,
46(2).
T. Holter and T. Svendsen. 1999. Maximum likelihood
modelling of pronunciation variation. Speech Commu-
nication.
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and
S. Sundararajan. 2008. A dual coordinate descent
method for large-scale linear SVM. In Proc. Interna-
tional Conference on Machine Learning (ICML).
B. Hutchinson and J. Droppo. 2011. Learning non-
parametric models of pronunciation. In Proc. Inter-
national Conference on Acoustics, Speech, and Signal
Processing (ICASSP).
D. Jurafsky, W. Ward, Z. Jianping, K. Herold, Y. Xi-
uyang, and Z. Sen. 2001. What kind of pronunciation
variation is hard for triphones to model? In Proc. In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing (ICASSP).
P. Jyothi, K. Livescu, and E. Fosler-Lussier. 2011. Lex-
ical access experiments with context-dependent artic-
ulatory feature-based models. In Proc. International
Conference on Acoustics, Speech, and Signal Process-
ing (ICASSP).
J. Keshet, S. Shalev-Shwartz, Y. Singer, and D. Chazan.
2007. A large margin algorithm for speech and au-
dio segmentation. IEEE Transactions on Acoustics,
Speech, and Language Processing, 15(8).
J. Keshet, D. McAllester, and T. Hazan. 2011. PAC-
Bayesian approach for minimization of phoneme error
rate. In Proc. International Conference on Acoustics,
Speech, and Signal Processing (ICASSP).
F. Korkmazskiy and B.-H. Juang. 1997. Discriminative
training of the pronunciation networks. In Proc. IEEE
Workshop on Automatic Speech Recognition and Un-
derstanding (ASRU).
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional Random Fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. Interna-
tional Conference on Machine Learning (ICML).
K. Livescu and J. Glass. 2004. Feature-based pronun-
ciation modeling with trainable asynchrony probabil-
ities. In Proc. International Conference on Spoken
Language Processing (ICSLP).
K. Livescu. 2005. Feature-based Pronunciation Model-
ing for Automatic Speech Recognition. Ph.D. thesis,
Massachusetts Institute of Technology.
D. McAllaster, L. Gillick, F. Scattone, and M. Newman.
1998. Fabricating conversational speech data with
acoustic models : A program to examine model-data
mismatch. In Proc. International Conference on Spo-
ken Language Processing (ICSLP).
J. Morris and E. Fosler-Lussier. 2008. Conditional ran-
dom fields for integrating local discriminative classi-
fiers. IEEE Transactions on Acoustics, Speech, and
Language Processing, 16(3).
R. Prabhavalkar, E. Fosler-Lussier, and K. Livescu. 2011.
A factored conditional random field model for artic-
ulatory feature forced transcription. In Proc. IEEE
Workshop on Automatic Speech Recognition and Un-
derstanding (ASRU).
M. Riley, W. Byrne, M. Finke, S. Khudanpur, A. Ljolje,
J. McDonough, H. Nock, M. Saraclar, C. Wooters, and
G. Zavaliagkos. 1999. Stochastic pronunciation mod-
elling from hand-labelled phonetic corpora. Speech
Communication, 29(2-4).
E. S. Ristad and P. N. Yianilos. 1998. Learning string
edit distance. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 20(2).
G. Salton, A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Commun. ACM,
18.
202
M. Sarac?lar and S. Khudanpur. 2004. Pronunciation
change in conversational speech and its implications
for automatic speech recognition. Computer Speech
and Language, 18(4).
H. Schramm and P. Beyerlein. 2001. Towards discrimi-
native lexicon optimization. In Proc. Eurospeech.
S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007. Pega-
sos: Primal Estimated sub-GrAdient SOlver for SVM.
In Proc. International Conference on Machine Learn-
ing (ICML).
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Advances in Neural Information
Processing Systems (NIPS) 17.
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. Journal of Machine
Learning Research, 6.
V. Venkataramani and W. Byrne. 2001. MLLR adap-
tation techniques for pronunciation modeling. In
Proc. IEEE Workshop on Automatic Speech Recogni-
tion and Understanding (ASRU).
O. Vinyals, L. Deng, D. Yu, and A. Acero. 2009. Dis-
criminative pronunciation learning using phonetic de-
coder and minimum-classification-error criterion. In
Proc. International Conference on Acoustics, Speech,
and Signal Processing (ICASSP).
G. Zweig, P. Nguyen, and A. Acero. 2010. Continuous
speech recognition with a TF-IDF acoustic model. In
Proc. Interspeech.
G. Zweig, P. Nguyen, D. Van Compernolle, K. De-
muynck, L. Atlas, P. Clark, G. Sell, M. Wang, F. Sha,
H. Hermansky, D. Karakos, A. Jansen, S. Thomas,
G.S.V.S. Sivaram, S. Bowman, and J. Kao. 2011.
Speech recognition with segmental conditional ran-
dom fields: A summary of the JHU CLSP 2010 sum-
mer workshop. In Proc. International Conference on
Acoustics, Speech, and Signal Processing (ICASSP).
203
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 809?815,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Tailoring Continuous Word Representations for Dependency Parsing
Mohit Bansal Kevin Gimpel Karen Livescu
Toyota Technological Institute at Chicago, IL 60637, USA
{mbansal,kgimpel,klivescu}@ttic.edu
Abstract
Word representations have proven useful
for many NLP tasks, e.g., Brown clusters
as features in dependency parsing (Koo et
al., 2008). In this paper, we investigate the
use of continuous word representations as
features for dependency parsing. We com-
pare several popular embeddings to Brown
clusters, via multiple types of features, in
both news and web domains. We find that
all embeddings yield significant parsing
gains, including some recent ones that can
be trained in a fraction of the time of oth-
ers. Explicitly tailoring the representations
for the task leads to further improvements.
Moreover, an ensemble of all representa-
tions achieves the best results, suggesting
their complementarity.
1 Introduction
Word representations derived from unlabeled text
have proven useful for many NLP tasks, e.g., part-
of-speech (POS) tagging (Huang et al, 2014),
named entity recognition (Miller et al, 2004),
chunking (Turian et al, 2010), and syntactic
parsing (Koo et al, 2008; Finkel et al, 2008;
T?ackstr?om et al, 2012). Most word representa-
tions fall into one of two categories. Discrete rep-
resentations consist of memberships in a (possibly
hierarchical) hard clustering of words, e.g., via k-
means or the Brown et al (1992) algorithm. Con-
tinuous representations (or distributed representa-
tions or embeddings) consist of low-dimensional,
real-valued vectors for each word, typically in-
duced via neural language models (Bengio et al,
2003; Mnih and Hinton, 2007) or spectral meth-
ods (Deerwester et al, 1990; Dhillon et al, 2011).
Koo et al (2008) found improvement on in-
domain dependency parsing using features based
on discrete Brown clusters. In this paper, we ex-
periment with parsing features derived from con-
tinuous representations. We find that simple at-
tempts based on discretization of individual word
vector dimensions do not improve parsing. We
see gains only after first performing a hierarchi-
cal clustering of the continuous word vectors and
then using features based on the hierarchy.
We compare several types of continuous rep-
resentations, including those made available by
other researchers (Turian et al, 2010; Collobert et
al., 2011; Huang et al, 2012), and embeddings we
have trained using the approach of Mikolov et al
(2013a), which is orders of magnitude faster than
the others. The representations exhibit different
characteristics, which we demonstrate using both
intrinsic metrics and extrinsic parsing evaluation.
We report significant improvements over our base-
line on both the Penn Treebank (PTB; Marcus et
al., 1993) and the English Web treebank (Petrov
and McDonald, 2012).
While all embeddings yield some parsing im-
provements, we find larger gains by tailoring them
to capture similarity in terms of context within
syntactic parses. To this end, we use two sim-
ple modifications to the models of Mikolov et al
(2013a): a smaller context window, and condition-
ing on syntactic context (dependency links and la-
bels). Interestingly, the Brown clusters of Koo et
al. (2008) prove to be difficult to beat, but we find
that our syntactic tailoring can lead to embeddings
that match the parsing performance of Brown (on
all test sets) in a fraction of the training time. Fi-
nally, a simple parser ensemble on all the represen-
tations achieves the best results, suggesting their
complementarity for dependency parsing.
2 Continuous Word Representations
There are many ways to train continuous represen-
tations; in this paper, we are primarily interested
in neural language models (Bengio et al, 2003),
which use neural networks and local context to
learn word vectors. Several researchers have
made their trained representations publicly avail-
809
Representation Source Corpus Types, Tokens V D Time
BROWN Koo et al (2008) BLLIP 317K, 43M 316,710 ? 2.5 days
?
SENNA Collobert et al (2011) Wikipedia 8.3M, 1.8B 130,000 50 2 months
?
TURIAN Turian et al (2010) RCV1 269K, 37M 268,810 50 few weeks
?
HUANG Huang et al (2012) Wikipedia 8.3M, 1.8B 100,232 50 ?
CBOW, SKIP, SKIP
DEP
Mikolov et al (2013a) BLLIP 317K, 43M 316,697 100 2-4 mins.
?
Table 1: Details of word representations used, including datasets, vocabulary size V , and dimensionality D. Continuous
representations require an additional 4 hours to run hierarchical clustering to generate features (?3.2). RCV1 = Reuters Corpus,
Volume 1. ? = time reported by authors. ? = run by us on a 3.50 GHz desktop, using a single thread.
able, which we use directly in our experiments.
In particular, we use the SENNA embeddings of
Collobert et al (2011); the scaled TURIAN em-
beddings (C&W) of Turian et al (2010); and the
HUANG global-context, single-prototype embed-
dings of Huang et al (2012). We also use the
BROWN clusters trained by Koo et al (2008). De-
tails are given in Table 1.
Below, we describe embeddings that we train
ourselves (?2.1), aiming to make them more useful
for parsing via smaller context windows (?2.1.1)
and conditioning on syntactic context (?2.1.2). We
then compare the representations using two intrin-
sic metrics (?2.2).
2.1 Syntactically-tailored Representations
We train word embeddings using the continu-
ous bag-of-words (CBOW) and skip-gram (SKIP)
models described in Mikolov et al (2013a;
2013b) as implemented in the open-source toolkit
word2vec. These models avoid hidden layers
in the neural network and hence can be trained
in only minutes, compared to days or even weeks
for the others, as shown in Table 1.
1
We adapt
these embeddings to be more useful for depen-
dency parsing in two ways, described next.
2.1.1 Smaller Context Windows
The CBOW model learns vectors to predict a
word given its set of surrounding context words
in a window of size w. The SKIP model learns
embeddings to predict each individual surround-
ing word given one particular word, using an anal-
ogous window size w. We find that w affects
the embeddings substantially: with large w, words
group with others that are topically-related; with
small w, grouped words tend to share the same
POS tag. We discuss this further in the intrinsic
evaluation presented in ?2.2.
1
We train both models on BLLIP (LDC2000T43) with
PTB removed, the same corpus used by Koo et al (2008) to
train their BROWN clusters. We created a special vector for
unknown words by averaging the vectors for the 50K least
frequent words; we did not use this vector for the SKIP
DEP
(?2.1.2) setting because it performs slightly better without it.
2.1.2 Syntactic Context
We expect embeddings to help dependency pars-
ing the most when words that have similar parents
and children are close in the embedding space. To
target this type of similarity, we train the SKIP
model on dependency context instead of the linear
context in raw text. When ordinarily training SKIP
embeddings, words v
?
are drawn from the neigh-
borhood of a target word v, and the sum of log-
probabilities of each v
?
given v is maximized. We
propose to instead choose v
?
from the set contain-
ing the grandparent, parent, and children words of
v in an automatic dependency parse.
A simple way to implement this idea is to train
the original SKIP model on a corpus of depen-
dency links and labels. For this, we parse the
BLLIP corpus (minus PTB) using our baseline de-
pendency parser, then build a corpus in which each
line contains a single child word c, its parent word
p, its grandparent g, and the dependency label ` of
the ?c, p? link:
?`
<L>
g
<G>
p c `
<L>
?,
that is, both the dependency label and grandparent
word are subscripted with a special token to avoid
collision with words.
2
We train the SKIP model on
this corpus of tuples with window size w = 1, de-
noting the result SKIP
DEP
. Note that this approach
needs a parsed corpus, but there also already ex-
ist such resources (Napoles et al, 2012; Goldberg
and Orwant, 2013).
2.2 Intrinsic Evaluation of Representations
Short of running end-to-end parsing experiments,
how can we choose which representations to use
for parsing tasks? Several methods have been pro-
posed for intrinsic evaluation of word representa-
2
We use a subscript on g so that it will be treated dif-
ferently from c when considering the context of p. We re-
moved all g
<G>
from the vocabulary after training. We also
tried adding information about POS tags. This increases M-1
(?2.2), but harms parsing performance, likely because the em-
beddings become too tag-like. Similar ideas have been used
for clustering (Sagae and Gordon, 2009; Haffari et al, 2011;
Grave et al, 2013), semantic space models (Pad?o and Lapata,
2007), and topic modeling (Boyd-Graber and Blei, 2008).
810
Representation SIM M-1
BROWN ? 89.3
SENNA 49.8 85.2
TURIAN 29.5 87.2
HUANG 62.6 78.1
CBOW, w = 2 34.7 84.8
SKIP, w = 1 37.8 86.6
SKIP, w = 2 43.1 85.8
SKIP, w = 5 44.4 81.1
SKIP, w = 10 44.6 71.5
SKIP
DEP
34.6 88.3
Table 2: Intrinsic evaluation of representations. SIM column
has Spearman?s ?? 100 for 353-pair word similarity dataset.
M-1 is our unsupervised POS tagging metric. For BROWN,
M-1 is simply many-to-one accuracy of the clusters. Best
score in each column is bold.
tions; we discuss two here:
Word similarity (SIM): One widely-used evalu-
ation compares distances in the continuous space
to human judgments of word similarity using the
353-pair dataset of Finkelstein et al (2002). We
compute cosine similarity between the two vectors
in each word pair, then order the word pairs by
similarity and compute Spearman?s rank correla-
tion coefficient (?) with the gold similarities. Em-
beddings with high ? capture similarity in terms of
paraphrase and topical relationships.
Clustering-based tagging accuracy (M-1): In-
tuitively, we expect embeddings to help parsing
the most if they can tell us when two words are
similar syntactically. To this end, we use a met-
ric based on unsupervised evaluation of POS tag-
gers. We perform clustering and map each cluster
to one POS tag so as to maximize tagging accu-
racy, where multiple clusters can map to the same
tag. We cluster vectors corresponding to the to-
kens in PTB WSJ sections 00-21.
3
Table 2 shows these metrics for representations
used in this paper. The BROWN clusters have
the highest M-1, indicating high cluster purity in
terms of POS tags. The HUANG embeddings have
the highest SIM score but low M-1, presumably
because they were trained with global context,
making them more tuned to capture topical sim-
ilarity. We compare several values for the win-
dow size (w) used when training the SKIP embed-
dings, finding that smallw leads to higher M-1 and
lower SIM. Table 3 shows examples of clusters
obtained by clustering SKIP embeddings of w = 1
versus w = 10, and we see that the former cor-
respond closely to POS tags, while the latter are
3
For clustering, we use k-means with k = 1000 and ini-
tialize by placing centroids on the 1000 most-frequent words.
w Example clusters
1 [Mr., Mrs., Ms., Prof., ...], [Jeffrey, Dan, Robert,
Peter, ...], [Johnson, Collins, Schmidt, Freedman,
...], [Portugal, Iran, Cuba, Ecuador, ...], [CST, 4:30,
9-10:30, CDT, ...], [his, your, her, its, ...], [truly,
wildly, politically, financially, ...]
10 [takeoff, altitude, airport, carry-on, airplane, flown,
landings, ...], [health-insurance, clinic, physician,
doctor, medical, health-care, ...], [financing, equity,
investors, firms, stock, fund, market, ...]
Table 3: Example clusters for SKIP embeddings with win-
dow size w = 1 (syntactic) and w = 10 (topical).
much more topically-coherent and contain mixed
POS tags.
4
For parsing experiments, we choose
w = 2 for CBOW and w = 1 for SKIP. Finally,
our SKIP
DEP
embeddings, trained with syntactic
context and w = 1 (?2.1.2), achieve the highest
M-1 of all continuous representations. In ?4, we
will relate these intrinsic metrics to extrinsic pars-
ing performance.
3 Dependency Parsing Features
We now discuss the features that we add to our
baseline dependency parser (second-order MST-
Parser; McDonald and Pereira, 2006) based on
discrete and continuous representations.
3.1 Brown Cluster Features
We start by replicating the features of Koo et al
(2008) using their BROWN clusters; each word is
represented by a 0-1 bit string indicating the path
from the root to the leaf in the binary merge tree.
We follow Koo et al in adding cluster versions of
the first- and second-order features in MSTParser,
using bit string prefixes of the head, argument,
sibling, intermediate words, etc., to augment or
replace the POS and lexical identity information.
We tried various sets of prefix lengths on the devel-
opment set and found the best setting to use pre-
fixes of length 4, 6, 8, and 12.
5
3.2 Continuous Representation Features
We tried two kinds of indicator features:
Bucket features: For both parent and child vec-
tors in a potential dependency, we fire one indi-
cator feature per dimension of each embedding
4
A similar effect, when changing distributional context
window sizes, was found by Lin and Wu (2009).
5
See Koo et al (2008) for the exact feature templates.
They used the full string in place of the length-12 prefixes,
but that setting worked slightly worse for us. Note that the
baseline parser used by Koo et al (2008) is different from the
second-order MSTParser that we use here; their parser allows
grandparent interactions in addition to the sibling interactions
in ours. We use their clusters, available at http://people.
csail.mit.edu/maestro/papers/bllip-clusters.gz.
811
vector, where the feature consists of the dimen-
sion index d and a bucketed version of the embed-
ding value in that dimension, i.e., bucket
k
(E
vd
)
for word index v and dimension d, where E is the
V ?D embedding matrix.
6
We also tried standard
conjunction variants of this feature consisting of
the bucket values of both the head and argument
along with their POS-tag or word information, and
the attachment distance and direction.
7
Cluster bit string features: To take into account
all dimensions simultaneously, we perform ag-
glomerative hierarchical clustering of the embed-
ding vectors. We use Ward?s minimum variance
algorithm (Ward, 1963) for cluster distance and
the Euclidean metric for vector distance (via MAT-
LAB?s linkage function with {method=ward,
metric=euclidean}). Next, we fire features on the
hierarchical clustering bit strings using templates
identical to those for BROWN, except that we use
longer prefixes as our clustering hierarchies tend
to be deeper.
8
4 Parsing Experiments
Setup: We use the publicly-available MST-
Parser for all experiments, specifically its second-
order projective model.
9
We remove all fea-
tures that occur only once in the training data.
For WSJ parsing, we use the standard train(02-
21)/dev(22)/test(23) split and apply the NP brack-
eting patch by Vadas and Curran (2007). For
Web parsing, we still train on WSJ 02-21, but
test on the five Web domains (answers, email,
newsgroup, reviews, and weblog) of the ?English
Web Treebank? (LDC2012T13), splitting each do-
main in half (in original order) for the develop-
ment and test sets.
10
For both treebanks, we con-
vert from constituent to dependency format us-
ing pennconverter (Johansson and Nugues,
2007), and generate POS tags using the MXPOST
tagger (Ratnaparkhi, 1996). To evaluate, we use
6
Our bucketing function bucket
k
(x) converts the real
value x to its closest multiple of k. We choose a k value
of around 1/5th of the embedding?s absolute range.
7
We initially experimented directly with real-valued fea-
tures (instead of bucketed indicator features) and similar con-
junction variants, but these did not perform well.
8
We use prefixes of length 4, 6, 8, 12, 16, 20, and full-
length, again tuned on the development set.
9
We use the recommended MSTParser settings: training-
k:5 iters:10 loss-type:nopunc decode-type:proj
10
Our setup is different from SANCL 2012 (Petrov and
McDonald, 2012) because the exact splits and test data were
only available to participants.
System Dev Test
Baseline 92.38 91.95
BROWN 93.18 92.69
SENNA (Buckets) 92.64 92.04
SENNA (Bit strings) 92.88 92.30
HUANG (Buckets) 92.44 91.86
HUANG (Bit strings) 92.55 92.36
CBOW (Buckets) 92.57 91.93
CBOW (Bit strings) 93.06 92.53
Table 4: Bucket vs. bit string features (UAS on WSJ).
System Dev Test
Baseline 92.38 91.95
BROWN 93.18 92.69
SENNA 92.88 92.30
TURIAN 92.84 92.26
HUANG 92.55 92.36
CBOW 93.06 92.53
SKIP 92.94 92.29
SKIP
DEP
93.33 92.69
Ensemble Results
ALL ? BROWN 93.46 92.90
ALL 93.54 92.98
Table 5: Full results with bit string features (UAS on WSJ).
unlabeled attachment score (UAS).
11
We report
statistical significance (p < 0.01, 100K sam-
ples) using the bootstrap test (Efron and Tibshi-
rani, 1994).
Comparing bucket and bit string features: In
Table 4, we find that bucket features based on in-
dividual embedding dimensions do not lead to im-
provements in test accuracy, while bit string fea-
tures generally do. This is likely because indi-
vidual embedding dimensions rarely correspond to
interpretable or useful distinctions among words,
whereas the hierarchical bit strings take into ac-
count all dimensions of the representations simul-
taneously. Their prefixes also naturally define fea-
tures at multiple levels of granularity.
WSJ results: Table 5 shows our main WSJ
results. Although BROWN yields one of the
highest individual gains, we also achieve statis-
tically significant gains over the baseline from
all embeddings. The CBOW embeddings per-
form as well as BROWN (i.e., no statistically
significant difference) but are orders of magni-
tude faster to train. Finally, the syntactically-
trained SKIP
DEP
embeddings are statistically indis-
tinguishable from BROWN and CBOW, and sig-
nificantly better than all other embeddings. This
suggests that targeting the similarity captured by
syntactic context is useful for dependency parsing.
11
We find similar improvements under labeled attachment
score (LAS). We ignore punctuation : , ? ? . in our evalua-
tion (Yamada and Matsumoto, 2003; McDonald et al, 2005).
812
System ans eml nwg rev blog Avg
Baseline 82.6 81.2 84.3 83.8 85.5 83.5
BROWN 83.4 81.7 85.2 84.5 86.1 84.2
SENNA 83.7 81.9 85.0 85.0 86.0 84.3
TURIAN 83.0 81.5 85.0 84.1 85.7 83.9
HUANG 83.1 81.8 85.1 84.7 85.9 84.1
CBOW 82.9 81.3 85.2 83.9 85.8 83.8
SKIP 83.1 81.1 84.7 84.1 85.4 83.7
SKIP
DEP
83.3 81.5 85.2 84.3 86.0 84.1
Ensemble Results
ALL?BR 83.9 82.2 85.9 85.0 86.6 84.7
ALL 84.2 82.3 85.9 85.1 86.8 84.9
Table 6: Main UAS test results on Web treebanks. Here,
ans=answers, eml=email, nwg=newsgroup, rev=reviews,
blog=weblog, BR=BROWN, Avg=Macro-average.
Web results: Table 6 shows our main Web re-
sults.
12
Here, we see that the SENNA, BROWN,
and SKIP
DEP
embeddings perform the best on av-
erage (and are statistically indistinguishable, ex-
cept SENNA vs. SKIP
DEP
on the reviews domain).
They yield statistically significant UAS improve-
ments over the baseline across all domains, except
weblog for SENNA (narrowly misses significance,
p=0.014) and email for SKIP
DEP
.
13
Ensemble results: When analyzing errors, we
see differences among the representations, e.g.,
BROWN does better at attaching proper nouns,
prepositions, and conjunctions, while CBOW
does better on plural common nouns and adverbs.
This suggests that the representations might be
complementary and could benefit from combina-
tion. To test this, we use a simple ensemble parser
that chooses the highest voted parent for each ar-
gument.
14
As shown in the last two rows of Ta-
bles 5 and 6, this leads to substantial gains. The
?ALL ? BROWN? ensemble combines votes from
all non-BROWN continuous representations, and
the ?ALL? ensemble also includes BROWN.
Characteristics of representations: We now re-
late the intrinsic metrics from ?2.2 to parsing
performance. The clearest correlation appears
when comparing variations of a single model,
e.g., for SKIP, the WSJ dev accuracies are 93.33
(SKIP
DEP
), 92.94 (w = 1), 92.86 (w = 5), and
92.70 (w = 10), which matches the M-1 score or-
der and is the reverse of the SIM score order.
12
We report individual domain results and macro-average
over domains. We do not tune any features/parameters on
Web dev sets; we only show the test results for brevity.
13
Note that SENNA and HUANG are trained on Wikipedia
which may explain why they work better on Web parsing as
compared to WSJ parsing.
14
This does not guarantee a valid tree. Combining features
from representations will allow training to weigh them appro-
priately and also guarantee a tree.
5 Related Work
In addition to work mentioned above, relevant
work that uses discrete representations exists for
POS tagging (Ritter et al, 2011; Owoputi et
al., 2013), named entity recognition (Ratinov
and Roth, 2009), supersense tagging (Grave et
al., 2013), grammar induction (Spitkovsky et al,
2011), constituency parsing (Finkel et al, 2008),
and dependency parsing (Tratz and Hovy, 2011).
Continuous representations in NLP have been
evaluated for their ability to capture syntactic and
semantic word similarity (Huang et al, 2012;
Mikolov et al, 2013a; Mikolov et al, 2013b) and
used for tasks like semantic role labeling, part-
of-speech tagging, NER, chunking, and sentiment
classification (Turian et al, 2010; Collobert et al,
2011; Dhillon et al, 2012; Al-Rfou? et al, 2013).
For dependency parsing, Hisamoto et al (2013)
also used embedding features, but there are several
differences between their work and ours. First,
they use only one set of pre-trained embeddings
(TURIAN) while we compare several and also train
our own, tailored to the task. Second, their em-
bedding features are simpler than ours, only us-
ing flat (non-hierarchical) cluster IDs and binary
strings obtained via sign quantization (1[x > 0])
of the vectors. They also compare to a first-order
baseline and only evaluate on the Web treebanks.
Concurrently, Andreas and Klein (2014) inves-
tigate the use of embeddings in constituent pars-
ing. There are several differences: we work on de-
pendency parsing, use clustering-based features,
and tailor our embeddings to dependency-style
syntax; their work additionally studies vocabulary
expansion and relating in-vocabulary words via
embeddings.
6 Conclusion
We showed that parsing features based on hierar-
chical bit strings work better than those based on
discretized individual embedding values. While
the Brown clusters prove to be well-suited to pars-
ing, we are able to match their performance with
our SKIP
DEP
embeddings that train much faster.
Finally, we found the various representations to
be complementary, enabling a simple ensemble
to perform best. Our SKIP
DEP
embeddings and
bit strings are available at ttic.edu/bansal/
data/syntacticEmbeddings.zip.
813
References
Rami Al-Rfou?, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual NLP. In Proceedings of CoNLL.
Jacob Andreas and Dan Klein. 2014. How much do
word embeddings encode about syntax? In Pro-
ceedings of ACL.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155, March.
Jordan L. Boyd-Graber and David M. Blei. 2008. Syn-
tactic topic models. In Proceedings of NIPS.
Peter F. Brown, Peter V. Desouza, Robert L. Mercer,
Vincent J. Della Pietra, and Jenifer C. Lai. 1992.
Class-based N-gram models of natural language.
Computational Linguistics, 18(4):467?479.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391?407.
Paramveer Dhillon, Dean P. Foster, and Lyle H. Ungar.
2011. Multi-view learning of word embeddings via
CCA. In Proceedings of NIPS.
Paramveer Dhillon, Jordan Rodu, Dean P. Foster, and
Lyle H. Ungar. 2012. Two Step CCA: A new spec-
tral method for estimating vector models of words.
In Proceedings of ICML.
Bradley Efron and Robert J. Tibshirani. 1994. An in-
troduction to the bootstrap, volume 57. CRC press.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. In Proceedings of ACL.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116?131.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of English books. In Second Joint Conference
on Lexical and Computational Semantics (* SEM),
volume 1, pages 241?247.
Edouard Grave, Guillaume Obozinski, and Francis
Bach. 2013. Hidden markov tree models for se-
mantic class induction. In Proceedings of CoNLL.
Gholamreza Haffari, Marzieh Razavi, and Anoop
Sarkar. 2011. An ensemble model that combines
syntactic and semantic clustering for discriminative
dependency parsing. In Proceedings of ACL.
Sorami Hisamoto, Kevin Duh, and Yuji Matsumoto.
2013. An empirical investigation of word repre-
sentations for parsing the web. In Proceedings of
ANLP.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. In Proceedings of ACL.
Fei Huang, Arun Ahuja, Doug Downey, Yi Yang,
Yuhong Guo, and Alexander Yates. 2014. Learning
representations for weakly supervised natural lan-
guage processing tasks. Computational Linguistics,
40(1).
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In 16th Nordic Conference of Computa-
tional Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of ACL-
IJCNLP.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational Linguistics, 19:313?330.
Ryan T. McDonald and Fernando C. Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In Proceedings of EACL.
Ryan T. McDonald, Koby Crammer, and Fernando C.
Pereira. 2005. Spanning tree methods for discrim-
inative training of dependency parsers. Technical
Report MS-CIS-05-11, University of Pennsylvania.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of ICLR.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013b. Distributed repre-
sentations of words and phrases and their composi-
tionality. In Proceedings of NIPS.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In Proceedings of HLT-NAACL.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of ICML.
814
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge
Extraction, AKBC-WEKEX ?12, pages 95?100,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL.
Sebastian Pad?o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of CoNLL.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of EMNLP.
Kenji Sagae and Andrew S. Gordon. 2009. Cluster-
ing words by syntactic similarity improves depen-
dency parsing of predicate-argument structures. In
Proceedings of the 11th International Conference on
Parsing Technologies.
Valentin I. Spitkovsky, Hiyan Alshawi, Angel X.
Chang, and Daniel Jurafsky. 2011. Unsupervised
dependency parsing without gold part-of-speech
tags. In Proceedings of EMNLP.
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
NAACL.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of EMNLP.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of ACL.
David Vadas and James R. Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proceed-
ings of ACL.
Joe H. Ward. 1963. Hierarchical grouping to optimize
an objective function. Journal of the American sta-
tistical association, 58(301):236?244.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of International Conference
on Parsing Technologies.
815
Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 45?52,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Domain Adaptation with Unlabeled Data for Dialog Act Tagging
Anna Margolis1,2 Karen Livescu2
1Department of Electrical Engineering, University of Washington, Seattle, WA, USA.
2TTI-Chicago, Chicago, IL, USA.
amargoli@ee.washington.edu, klivescu@ttic.edu, mo@ee.washington.edu
Mari Ostendorf1
Abstract
We investigate the classification of utter-
ances into high-level dialog act categories
using word-based features, under condi-
tions where the train and test data dif-
fer by genre and/or language. We han-
dle the cross-language cases with ma-
chine translation of the test utterances.
We analyze and compare two feature-
based approaches to using unlabeled data
in adaptation: restriction to a shared fea-
ture set, and an implementation of Blitzer
et al?s Structural Correspondence Learn-
ing. Both methods lead to increased detec-
tion of backchannels in the cross-language
cases by utilizing correlations between
backchannel words and utterance length.
1 Introduction
Dialog act (or speech act) tagging aims to label
abstract functions of utterances in conversations,
such as Request, Floorgrab, or Statement; poten-
tial applications include automatic conversation
analysis, punctuation transcription, and human-
computer dialog systems. Although some appli-
cations require domain-specific tag sets, it is often
useful to label utterances based on generic tags,
and several tag sets have been developed for this
purpose, e.g. DAMSL (Core and Allen, 1997).
Many approaches to automatic dialog act (DA)
tagging assume hand-labeled training data. How-
ever, when building a new system it may be diffi-
cult to find a labeled corpus that matches the tar-
get domain, or even the language. Even within the
same language, speech from different domains can
differ linguistically, and the same DA categories
might be characterized by different cues. The do-
main characteristics (face-to-face vs. telephone,
two-party vs. multi-party, informal vs. agenda-
driven, familiar vs. stranger) can influence both
the distribution of tags and word choice.
This work attempts to use unlabeled target do-
main data in order to improve cross-domain train-
ing performance, an approach referred to as both
unsupervised and semi-supervised domain adapta-
tion in the literature. We refer to the labeled train-
ing domain as the source domain. We compare
two adaptation approaches: a simple one based
on forcing the classifier to learn only on ?shared?
features that appear in both domains, and a more
complex one based on Structural Correspondence
Learning (SCL) from Blitzer et al (2007). The
shared feature approach has been investigated for
adaptation in other tasks, e.g. Aue and Gamon
(2005) for sentiment classification and Dredze et
al. (2007) for parsing. SCL has been used suc-
cessfully for sentiment classification and part-of-
speech tagging (Blitzer et al, 2006); here we in-
vestigate its applicability to the DA classification
task, using a multi-view learning implementation
as suggested by Blitzer et al (2009). In addition to
analyzing these two methods on a novel task, we
show an interesting comparison between them: in
this setting, both methods turn out to have a simi-
lar effect caused by correlating cues for a particu-
lar DA class (Backchannel) with length.
We classify pre-segmented utterances based on
their transcripts, and we consider only four high-
level classes: Statement, Question, Backchannel,
and Incomplete. Experiments are performed us-
ing all train/test pairs among three conversational
speech corpora : the Meeting Recorder Dialog Act
corpus (MRDA) (Shriberg et al, 2004), Switch-
board DAMSL (Swbd) (Jurafsky et al, 1997), and
the Spanish Callhome dialog act corpus (SpCH)
(Levin et al, 1998). The first is multi-party,
face-to-face meeting speech; the second is topic-
prompted telephone speech between strangers;
and the third is informal telephone speech between
friends and family members. The first two are in
English, while the third is in Spanish. When the
source and target domains differ in language, we
45
apply machine translation to the target domain to
convert it to the language of the source domain.
2 Related Work
Automatic DA tagging across domain has been
investigated by a handful of researchers. Webb
and Liu (2008) investigated cross-corpus train-
ing between Swbd and another corpus consist-
ing of task-oriented calls, although no adaptation
was attempted. Similarly, Rosset et al (2008)
reported on recognition of task-oriented DA tags
across domain and language (French to English)
by using utterances that had been pre-processed
to extract entities. Tur (2005) applied supervised
model adaptation to intent classification across
customer dialog systems, and Guz et al (2010)
applied supervised model adaptation methods for
DA segmentation and classification on MRDA us-
ing labeled data from both MRDA and Swbd.
Most similar to our work is that of Jeong et al
(2009), who compared two methods for semi-
supervised adaptation, using Swbd/MRDA as the
source training set and email or forums corpora as
the target domains. Both methods were based on
incorporating unlabeled target domain examples
into training. Success has also been reported for
self-training approaches on same-domain semi-
supervised learning (Venkataraman et al, 2003;
Tur et al, 2005). We are not aware of prior work
on cross-lingual DA tagging via machine transla-
tion, although a translation approach has been em-
ployed for cross-lingual text classification and in-
formation retrieval, e.g. Bel et al (2003).
In recent years there has been increasing in-
terest in domain adaptation methods based on
unlabeled target domain data. Several kinds of
approaches have been proposed, including self-
training (Roark and Bacchiani, 2003), instance
weighting (Huang et al, 2007), change of feature
representation (Pan et al, 2008), and clustering
methods (Xing et al, 2007). SCL (Blitzer et al,
2006) is one feature representation approach that
has been effective on certain high-dimensional
NLP problems, including part-of-speech tagging
and sentiment classification. SCL uses unlabeled
data to learn feature projections that tie together
source and target features via their correlations
with features shared between domains. It first se-
lects ?pivot features? that are common in both do-
mains; next, linear predictors for those features are
learned on all the other features. Finally, singular
value decomposition (SVD) is performed on the
collection of learned linear predictors correspond-
ing to different pivot features. Features that tend
to get similar weights in predicting pivot features
will be tied together in the SVD. By learning on
the SVD dimensions, the source-trained classifier
can put weight on target-only features.
3 Methods
Our four-class DA problem is similar to problems
studied in other work, such as Tur et al (2007)
who used five classes (ours plus Floorgrab/hold).
When defining a mapping from each corpus? tag
set to the four high-level classes, our goal was to
try to make the classes similarly defined across
corpora. Note that the Incomplete category is de-
fined in Swbd-DAMSL to include only utterances
too short to determine their DA label (e.g., just a
filler word). Thus, for our work the MRDA In-
complete category excludes utterances also tagged
as Statement or Question; it includes those con-
sisting of just a floor-grab, hold or filler word.
For classification we used an SVM with linear
kernel, with L2 regularization and L1 loss, as im-
plemented in the Liblinear package (Fan et al,
2008) which uses the one-vs.-rest configuration
for multiclass classification. SVMs have been suc-
cessful for supervised learning of DAs based on
words and other features (Surendran and Levow,
2006; Liu, 2006). Features are derived from the
hand transcripts, which are hand-segmented into
DA units. Punctuation and capitalization are re-
moved so that our setting corresponds to classifi-
cation based on (perfect) speech recognition out-
put. The features are counts of unigrams, bi-
grams, and trigrams that occur at least twice in
the train set, including beginning/end-of-utterance
tags (?s?, ?/s?), and a length feature (total num-
ber of words, z-normalized across the training
set). Note that some previous work on DA tag-
ging has used contextual features from surround-
ing utterances, or Markov models for the DA se-
quence. In addition, some work has used prosodic
or other acoustic features. The work of Stolcke
et al (2000) found benefits to using Markov se-
quence models and prosodic features in addition
to word features, but those benefits were relatively
small, so for simplicity our experiments here use
only word features and classify utterances in iso-
lation.
We used Google Translate to derive English
46
translations of the Spanish SpCH utterances, and
to derive Spanish translations of the English Swbd
and MRDA utterances. Of course, translations are
far from perfect; DA classification performance
could likely be improved by using a translation
system trained on spoken dialog. For instance,
Google Translate often failed on certain words like
?i? that are usually capitalized in text. Even so,
when training and testing on translated utterances,
the results with the generic system are surprisingly
good.
The results reported below used the standard
train/test splits provided with the corpora: MRDA
had 51 train meetings/11 test; Swbd had 1115 train
conversations/19 test; SpCH had 80 train conver-
sations/20 test. The SpCH train set is the smallest
at 29k utterances. To avoid issues of differing train
set size when comparing performance of different
models, we reduced the Swbd and MRDA train
sets to the same size as SpCH using randomly se-
lected examples from the full train sets. For each
adaptation experiment, we used the target domain
training set as the unlabeled data, and report per-
formance on the target domain test set. The test
sets contain 4525, 15180, and 3715 utterances for
Swbd, MRDA, and SpCH respectively.
4 Results
Table 1 shows the class proportions in the training
sets for each domain. MRDA has fewer Backchan-
nels than the the others, which is expected since
the meetings are face-to-face. SpCH has fewer In-
completes and more Questions than the others; the
reasons for this are unclear. Backchannels have
the shortest mean length (less than 2 words) in all
domains. Incompletes are also short, while State-
ments have the longest mean length. The mean
lengths of Statements and Questions are similar
in the English corpora, but are shorter in SpCH.
(This may point to differences in how the utter-
ances were segmented; for instance Swbd utter-
ances can span multiple turns, although 90% are
only one turn long.)
Because of the high class skew, we consider two
different schemes for training the classifiers, and
report different performance measures for each.
To optimize overall accuracy, we use basic un-
weighted training. To optimize average per-class
recall (weighted equally across all classes), we use
weighted training, where each training example is
weighted inversely to its class proportion. We op-
timize the regularization parameter using a source
domain development set corresponding to each
training set. Since the optimum values are close
for all three domains, we choose a single value for
all the accuracy classifiers and a single value for
all the per-class recall classifiers. (Different values
are chosen for different feature types correspond-
ing to the different adaptation methods.)
Inc. Stat. Quest. Back.
Swbd 8.1% 67.1% 5.8% 19.1%
MRDA 10.7% 67.9% 7.5% 14.0%
SpCH 5.7% 60.6% 12.1% 21.7%
Table 1: Proportion of utterances in each
DA category (Incomplete, Statement, Question,
Backchannel) in each domain?s training set.
Table 2 gives baseline performance for all train-
test pairs, using translated versions of the test set
when the train set differs in language. It also lists
the in-domain results using translated (train and
test) data, and results using the adaptation methods
(which we discuss below). Figure 1 shows details
of the contribution of each class to the average per-
class recall; bar height corresponds to the second
column in Table 2.
4.1 Baseline performance and analysis
We observe first that translation does not have a
large effect on in-domain performance; degrada-
tion occurs primarily in Incompletes and Ques-
tions, which depend most on word order and there-
fore might be most sensitive to ordering differ-
ences in the translations. We conclude that it is
possible to perform well on the translated test sets
when the training data is well matched. However,
cross-domain performance degradation is much
worse between pairs that differ in language than
between the two English corpora.
We now describe three kinds of issues contribut-
ing to cross-domain domain degradation, which
we observed anecdotally. First, some highly im-
portant words in one domain are sometimes miss-
ing entirely from another domain. This issue ap-
pears to have a dramatic effect on Backchannel
detection across languages: when optimizing for
average per-class recall, the English-trained clas-
sifiers detect about 20% of the Spanish translated
Backchannels and the Spanish classifier detects
a little over half of the English ones, while they
each detect more than 80% in their own domain.
47
train set Acc (%) Avg. Rec. (%)
Test on Swbd
Swbd 89.2 84.9
Swbd translated 86.7 80.4
MRDA baseline 86.4 78.0
MRDA shared only 85.7* 77.7
MRDA SCL 81.8* 69.6
MRDA length only 78.3* 51.4
SpCH baseline 74.5 57.2
SpCH shared only 77.4* 64.2
SpCH SCL 76.8* 64.8
SpCH length only 77.7* 48.2
majority 67.7 25.0
Test on MRDA
MRDA 83.8 80.5
MRDA translated 80.5 74.7
Swbd baseline 81.0 71.6
Swbd shared only 80.1* 72.1
Swbd SCL 75.6* 68.1
Swbd length only 68.6* 44.9
SpCH baseline 66.9 50.5
SpCH shared only 66.8 52.1
SpCH SCL 66.1* 58.4
SpCH length only 68.3* 44.6
majority 65.2 25.0
Test on SpCH
SpCH 83.1 72.8
SpCH translated 82.4 71.3
Swbd baseline 63.8 41.1
Swbd shared only 66.2* 50.9
Swbd SCL 68.2* 47.2
Swbd length only 72.6* 43.6
MRDA baseline 65.1 42.9
MRDA shared only 65.5 51.2
MRDA SCL 67.6* 50.9
MRDA length only 72.6* 44.7
majority 65.3 25.0
Table 2: Overall accuracy and average per-class
recall on each test set, using in-domain, in-domain
translated, and cross-domain training. Starred re-
sults under the accuracy column are significantly
different from the corresponding cross-domain
baseline under McNemar?s test (p < 0.05). (Sig-
nificance is not calculated for the average per-class
recall column.) ?Majority? classifies everything as
Statement.
The reason for the cross-domain drop is that many
backchannel words in the English corpora (uhhuh,
right, yeah) do not overlap with those in the Span-
train Swbd train MRDA train SpCH
0
20
40
60
80
Test on Swbd
a
v
e
. 
pe
r?
cl
as
s 
re
ca
ll 
(%
)
 
 in?dom
ain
in?dom
ain trans.
baseln
shared
SC
L
length
baseln
shared
SC
L
length
I
S
Q
B
train MRDA train Swbd train SpCH
0
20
40
60
80
Test on MRDA
in?dom
ain
in?dom
ain trans.
baseln
shared
SC
L
length
baseln
shared
SC
L
length
train SpCH train Swbd train MRDA
0
20
40
60
80
Test on SpCH
in?dom
ain
in?dom
ain trans.
baseln
shared
SC
L
length
baseln
shared
SC
L
length
Figure 1: Per-class recall of weighted classifiers
in column 2 of Table 2. Bar height represents
average per-class recall; colors indicate contribu-
tion of each class: I=incomplete, S=statement,
Q=question, B=backchannel. (Maximum possible
bar height is 100%, each color 25%).
ish corpora (mmm, s??, ya) even after translation?
for example, ?ya? becomes ?already?, ?s??? be-
comes ?yes?, ?right? becomes ?derecho?, and ?uh-
huh?, ?mmm? are unchanged.
A second issue has to do with different kinds
of utterances found in each domain, which some-
times lead to different relationships between fea-
tures and class label. This is sometimes caused
by the translation system; for example, utterances
starting with ?es que . . .? are usually statements
in SpCH, but without capitalization the translator
often gives ?is that . . .?. Since ??s??is?that? is
a cue feature for Question in English, these utter-
ances are usually labeled as Question by the En-
glish domain classifiers. The existence of differ-
ent types of utterances can result in sets of features
that are more highly correlated in one domain than
the other. In both Swbd and translated SpCH, ut-
terances containing the trigram ??s??but??/s?? are
most likely to be in the Incomplete class. In Swbd,
the bigram ?but??/s?? rarely occurs outside of that
trigram, but in SpCH it sometimes occurs at the
48
end of long (syntactically-incomplete) Statements,
so it corresponds to much lower likelihood for the
Incomplete class.
The last issue concerns utterances whose true
label probabilities given the word sequence are
not the same across domains. We distinguish two
such kinds utterances. The first are due to class
definition differences across domains and anno-
tators, e.g., long statements or questions that are
also incomplete are more often labeled Incomplete
in SpCH and Swbd than in MRDA. The second
kind are utterances whose class labels are not com-
pletely determined by their word sequence. To
minimize error rate the classifier should label an
utterance with its most frequent class, but that may
differ across domains. For example, ?yes? can be
either a Statement of Backchannel; in the English
corpora, it is most likely to be a Statement (?yeah?
is more commonly used for Backchannels). How-
ever, ?s??? is most likely to be a Backchannel in
SpCH. To measure the effect of differing label
probabilities across domains, we trained ?domain-
general? classifiers using concatenated training
sets for each pair of domains. We found that they
performed about the same or only slightly worse
than domain-specific models, so we conclude that
this issue is likely only a minor effect.
4.2 Adaptation using shared features only
In the cross-language domain pairs, some dis-
criminative features in one domain are missing
in the other. By removing all features from the
source domain training utterances that are not ob-
served (twice) in the target domain training data,
we force the classifier to learn only on features
that are present in both domains. As seen in
Figure 1, this had the effect of improving re-
call of Backchannels in the four cross-language
cases. Backchannels are the second-most frequent
class after Statements, and are typically short in
all domains. Many typical Backchannel words
are domain-specific; by removing them from the
source data, we force the classifier to attempt to
detect Backchannels based on length alone. The
resulting classifier has a better chance of recog-
nizing target domain Backchannels that lack the
source-only Backchannel words. At the same
time, it mistakes many other short utterances for
Backchannels, and does particularly worse on In-
completes, for which length is also strong cue.
Although average per-class recall improved in all
four cross-language cases, total accuracy only im-
proved significantly in two of those cases, and
for the Swbd/MRDA pair, accuracy got signifi-
cantly worse. The effect on the one-vs.-rest com-
ponent classifiers was mixed: for some (State-
ment and some Backchannel classifiers in the
cross-language cases), accuracy improved, while
in other cases it decreased.
As noted above, the shared feature approach
was investigated by Aue and Gamon (2005), who
argued that its success depends on the assump-
tion that class/feature relationships be the same
across domains. However, we argue here that the
success of this method requires stronger assump-
tions about both the relationship between domains
and the correlations between domain-specific and
shared features. Consider learning a linear model
on either the full source domain feature set or the
reduced shared feature set. In general, the co-
efficients for a given feature will be different in
each model?in the reduced case, the coefficients
incorporate correlation information and label pre-
dictive information for the removed (source-only)
features. This is potentially useful on the tar-
get domain, provided that there exist analogous,
target-only features that have similar correlations
with the shared features, and similar predictive co-
efficients.
For example, consider the discriminative source
and target features ?uhhuh? and ?mmm,? which
are both are correlated with a shared, noisier, fea-
ture (length). Forcing the model to learn only on
the shared, noisy feature incorporates correlation
information about ?uhhuh?, which is similar to
that of ?mmm?. Thus, the reduced model is poten-
tially more useful on the target domain, compared
to the full source domain model which might not
put weight on the noisy feature. On the other hand,
the approach is inappropriate in several other sce-
narios. For one, if the target domain utterances
actually represent samples from a subspace of the
source domain, the absence of features is informa-
tive: the fact that an utterance does not contain
??s??verdad??/s??, for instance, might mean that
it is less likely to be a Question, even if none of
the target domain utterances contain this feature.
4.3 Adaptation using SCL
The original formulation of SCL proposed predict-
ing pivot features using the entire feature set, ex-
cept for those features perfectly correlated with
49
the pivots (e.g., the pivots themselves). Our ex-
periments with this approach found it unsuitable
for our task, since even after removing the pivots
there are many features which remain highly cor-
related with the pivots due to overlapping n-grams
(i-love vs. love). The number of features that over-
lap with pivots is large, so removing these would
lead to few features being included in the projec-
tions. Therefore, we adopted the multi-view learn-
ing approach suggested by Blitzer et al (2009).
We split the utterances into two parts; pivot fea-
tures in the first part were predicted with all the
features in the second, and vice versa. We experi-
mented with splitting the utterances in the middle,
but found that since the number of words in the
first part (nearly) predicts the number in the sec-
ond part, all of the features in the first part were
positively predictive of pivots in the second part
so the main dimension learned was length. In the
results presented here, the first part consists of the
first word only, and the second part is the rest of
the utterance. (All utterances in our experiments
have at least one word.) Pivot features are selected
in each part and predicted using a least-squares
linear regression on all features in the other part.
We used the SCL-MI method of Blitzer et al
(2007) to select pivot features, which requires that
they be common in both domains and have high
mutual information (MI) with the class (according
to the source labels.) We selected features that oc-
curred at least 10 times in each domain and were
in the top 500 ranked MI features for any of the
four classes; this resulted in 78-99 first-part piv-
ots and 787-910 second-part pivots (depending on
the source-target pair). We performed SVD on
the learned prediction weights for each part sep-
arately, and the top (at most) 100 dimensions were
used to project utterances on each side.
In all train-test pairs, the first dimension of the
first part appeared to distinguish short utterance
words from long ones. Such short-utterance words
included backchannels from both domains, in ad-
dition to acknowledgments, exclamations, swear
words and greetings. An analogous dimension ex-
isted in the second part, which captured words cor-
related with short utterances greater than one word
(right, really, interesting). The other dimensions of
both domains were difficult to interpret.
We experimented with using the SCL fea-
tures together with the raw features (n-grams and
length), as suggested by (Blitzer et al, 2006). As
in (Blitzer et al, 2006), we found it necessary to
scale up the SCL features to increase their utiliza-
tion in the presence of the raw features; however,
it was difficult to guess the optimal scaling factor
without having access to labeled target data. The
results here use SCL features only, which also al-
lows us to more clearly investigate the utility of
those features and to compare them with the other
feature sets.
The most notable effect was an improvement
in Backchannel recall, which occurred under both
weighted and unweighted training. In addition,
there was high confusability between Statements
and the other classes, and more false detections
of Backchannels. When optimizing for accuracy,
SCL led to an improvement in accuracy in three
of the four cross-language cases. When optimiz-
ing for average per-class recall, it led to improve-
ment in all cross-language cases; however, re-
call of Statements went down dramatically in all
cases. In addition, while there was no clear ben-
efit of the SCL vs. the shared-feature method on
the cross-language cases, the SCL approach did
much worse than the shared-feature approach on
the Swbd/MRDA pair, causing large degradation
from the baseline.
As we have noted, utterance length appears
to underlie the improvement seen in the cross-
language performance for both the SCL and
shared-feature approaches. Therefore, we include
results for a classifier based only on the length
feature. Optimizing for accuracy, this method
achieves the highest accuracy of all methods in
the cross-language pairs. (It does so by classifying
everything as Statement or Backchannel, although
with weighted training, as shown in Figure 1, it
gets some Incompletes.) However, under weighted
class training, the average per-class recall of this
method is much worse than the shared-feature and
SCL approaches.
Comparison with other SCL tasks Although
we basically take a text classification approach to
the problem of dialog act tagging, our problem dif-
fers in several ways from the sentiment classifi-
cation task in Blitzer et al (2007). In particular,
utterances are much shorter than documents, and
we use position information via the start/end-of-
sentence tags. Some important DA cue features
(such as the value of the first word) are mutually
exclusive rather than correlated. In this way our
problem resembles the part-of-speech tagging task
50
(Blitzer et al, 2006), where the category of each
word is predicted using values of the left, right,
and current word token. In fact, that work used
a kind of multi-view learning for the SCL projec-
tion, with three views corresponding to the three
word categories. However, our problem essen-
tially uses a mix of bag-of-words and position-
based features, which poses a greater challenge
since there is no natural multi-view split. The ap-
proach described here suffers from the fact that it
cannot use all the features available to the base-
line classifier?bigrams and trigrams spanning the
first and second words are left out. It also suffers
from the fact that the first-word pivot feature set is
extremely small?a consequence of the small set
of first words that occur at least 10 times in the
29k-utterance corpora.
5 Conclusions
We have considered two approaches for domain
adaptation for DA tagging, and analyzed their
performance for source/target pairs drawn from
three different domains. For the English domains,
the baseline cross-domain performance was quite
good, and both adaptation methods generally led
to degradation over the baseline. For the cross-
language cases, both methods were effective at im-
proving average per-class recall, and particularly
Backchannel recall. SCL led to significant accu-
racy improvement in three cases, while the shared
feature approach did so in two cases. On the
other hand, SCL showed poor discrimination be-
tween Statements and other classes, and did worse
on the same-language pair that had little cross-
domain degradation. Both methods work by tak-
ing advantage of correlations between shared and
domain-specific class-discriminative features. Un-
fortunately in our task, membership in the rare
classes is often cued by features that are mutually
exclusive, e.g., the starting n-gram for Questions.
Both methods might therefore benefit from addi-
tional shared features that are correlated with these
n-grams, e.g., sentence-final intonation for Ques-
tions. (Indeed, other work on semi-supervised
DA tagging has used a richer feature set: Jeong
et al (2009) included parse, part-of-speech, and
speaker sequence information, and Venkataraman
et al (2003) used prosodic information, plus a
sequence-modeling framework.) From the task
perspective, an interesting result is that machine
translation appears to preserve most of the dialog-
act information, in that in-domain performance is
similar on original and translated text.
Acknowledgments
We thank Sham Kakade for suggesting the multi-
view SCL method based on utterance splits and
for many other helpful discussions, as well as John
Blitzer for helpful discussions. We thank the three
reviewers for their useful comments.
This research was funded by the Office of
the Director of National Intelligence (ODNI), In-
telligence Advanced Research Projects Activity
(IARPA). All statements of fact, opinion or con-
clusions contained herein are those of the authors
and should not be construed as representing the of-
ficial views or policies of IARPA, the ODNI or the
U.S. Government.
References
Anthony Aue and Michael Gamon. 2005. Customiz-ing sentiment classifiers to new domains: a casestudy. In Proc. International Conference on Recent
Advances in NLP.
Nuria Bel, Cornelis H. A. Koster, and Marta Ville-
gas. 2003. Cross-lingual text categorization. In
Research and Advanced Technology for Digital Li-
braries, pages 126?139. Springer Berlin / Heidel-
berg.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-dence learning. In Proc. of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, pages 120?128.
John Blitzer, Mark Dredze, and Fernando Pereira.2007. Biographies, Bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,pages 440?447.
John Blitzer, Dean P. Foster, and Sham M. Kakade.2009. Zero-shot domain adaptation: A multi-view
approach. Technical report, Toyota TechnologicalInstitute TTI-TR-2009-1.
Mark G. Core and James F. Allen. 1997. Coding di-alogs with the DAMSL annotation scheme. In Proc.
of the Working Notes of the AAAI Fall Symposium on
Communicative Action in Humans and Machines.
Mark Dredze, John Blitzer, Partha Pratim Taluk-
dar, Kuzman Ganchev, Joa?o Graca, and FernandoPereira. 2007. Frustratingly hard domain adapta-tion for dependency parsing. In Proc. of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages1051?1055.
51
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. Liblinear: Alibrary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Umit Guz, Gokhan Tur, Dilek Hakkani-Tu?r, andSe?bastien Cuendet. 2010. Cascaded model adapta-tion for dialog act segmentation and tagging. Com-
puter Speech & Language, 24(2):289?306.
Jiayuan Huang, Alexander J. Smola, Arthur Gretton,
Karsten M. Borgwardt, and Bernhard Scho?lkopf.2007. Correcting sample selection bias by unlabeleddata. In Advances in Neural Information Processing
Systems 19, pages 601?608.
Minwoo Jeong, Chin Y. Lin, and Gary G. Lee. 2009.
Semi-supervised speech act recognition in emailsand forums. In Proc. of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1250?1259.
Dan Jurafsky, Liz Shriberg, and Debra Biasca. 1997.Switchboard SWBD-DAMSL shallow-discourse-function annotation coders manual, draft 13. Tech-
nical report, University of Colorado at BoulderTechnical Report 97-02.
Lori Levin, Ann Thyme?-Gobbel, Alon Lavie, KlausRies, and Klaus Zechner. 1998. A discourse cod-ing scheme for conversational Spanish. In Proc. The
5th International Conference on Spoken Language
Processing, pages 2335?2338.
Yang Liu. 2006. Using SVM and error-correctingcodes for multiclass dialog act classification in meet-ing corpus. In Proc. Interspeech, pages 1938?1941.
Sinno J. Pan, James T. Kwok, and Qiang Yang. 2008.
Transfer learning via dimensionality reduction. In
Proc. of the Twenty-Third AAAI Conference on Arti-
ficial Intelligence.
Brian Roark and Michiel Bacchiani. 2003. Supervisedand unsupervised PCFG adaptation to novel do-
mains. In Proc. of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 126?133.
Sophie Rosset, Delphine Tribout, and Lori Lamel.2008. Multi-level information and automatic dia-log act detection in human?human spoken dialogs.
Speech Communication, 50(1):1?13.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The ICSI meetingrecorder dialog act (MRDA) corpus. In Proc. of the
5th SIGdial Workshop on Discourse and Dialogue,
pages 97?100.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, PaulTaylor, Rachel Martin, Carol Van Ess-Dykema, andMarie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversationalspeech. Computational Linguistics, 26:339?373.
Dinoj Surendran and Gina-Anne Levow. 2006. Dialogact tagging with support vector machines and hiddenMarkov models. In Proc. Interspeech, pages 1950?
1953.
Gokhan Tur, Dilek Hakkani-Tu?r, and Robert E.Schapire. 2005. Combining active and semi-supervised learning for spoken language under-
standing. Speech Communication, 45(2):171?186.
Gokhan Tur, Umit Guz, and Dilek Hakkani-Tu?r.2007. Model adaptation for dialog act tagging.In Proc. IEEE Spoken Language Technology Work-
shop, pages 94?97.
Gokhan Tur. 2005. Model adaptation for spoken lan-guage understanding. In Proc. IEEE International
Conference on Acoustics, Speech, and Signal Pro-
cessing, pages 41?44.
Anand Venkataraman, Luciana Ferrer, Andreas Stol-cke, and Elizabeth Shriberg. 2003. Traininga prosody-based dialog act tagger from unlabeled
data. In Proc. IEEE International Conference on
Acoustics, Speech, and Signal Processing, volume I,pages 272?275.
Nick Webb and Ting Liu. 2008. Investigating the
portability of corpus-derived cue phrases for dia-logue act classification. In Proc. of the 22nd Inter-
national Conference on Computational Linguistics
(Coling 2008), pages 977?984.
Dikan Xing, Wenyuan Dai, Gui-Rong Xue, and YongYu. 2007. Bridged refinement for transfer learn-ing. In Knowledge Discovery in Databases: PKDD
2007, pages 324?335. Springer Berlin / Heidelberg.
52
Proceedings of the 2014 Joint Meeting of SIGMORPHON and SIGFSM, pages 1?9,
Baltimore, Maryland USA, June 27 2014.
c?2014 Association for Computational Linguistics
Revisiting Word Neighborhoods for Speech Recognition
Preethi Jyothi
?
Beckman Institute
University of Illinois, Urbana, IL
pjyothi@illinois.edu
Karen Livescu
Toyota Technological Institute at Chicago
Chicago, IL
klivescu@ttic.edu
Abstract
Word neighborhoods have been suggested
but not thoroughly explored as an ex-
planatory variable for errors in automatic
speech recognition (ASR). We revisit the
definition of word neighborhoods, propose
new measures using a fine-grained artic-
ulatory representation of word pronuncia-
tions, and consider new neighbor weight-
ing functions. We analyze the signifi-
cance of our measures as predictors of er-
rors in an isolated-word ASR system and
a continuous-word ASR system. We find
that our measures are significantly better
predictors of ASR errors than previously
used neighborhood density measures.
1 Introduction
An important pursuit for both human and ma-
chine speech recognition research is to under-
stand the factors that affect word recognition ac-
curacy. In the substantial body of work on hu-
man word recognition, it has been shown that
it is harder to recognize words that have many
?similar? neighboring words than words with few
neighbors (Luce and Pisoni, 1998), and that fre-
quent words are recognized faster and more accu-
rately than are infrequent words (Marslen-Wilson,
1987; Luce and Pisoni, 1998; Vitevitch and Luce,
1999). In the ASR research community, prior
work has also investigated various factors that
benefit or disrupt recognition. Examples of such
factors include word frequency, speaking rate,
and prosodic factors (Fosler-Lussier and Morgan,
1999; Shinozaki and Furui, 2001; Hirschberg et
al., 2004; Goldwater et al., 2010). There has also
been prior work that uses word confusability mea-
sures to predict speech recognition errors (Fosler-
Lussier et al., 2005; Jyothi and Fosler-Lussier,
2009).
?
Supported by a Beckman Postdoctoral Fellowship.
Word neighborhood measures have been stud-
ied more heavily for human word recognition than
as predictors of ASR errors. Although not stud-
ied specifically in prior work (Fosler-Lussier et al.,
2005; Jyothi and Fosler-Lussier, 2009), word con-
fusability measures used in predicting ASR errors
could be utilized to build word neighborhoods.
Goldwater et al. (2010) examine the behavior of
certain standard neighborhood density measures
as predictors of ASR errors. To our knowledge,
this is the only study that explicitly considers word
neighborhoods as a potential factor in ASR.
In this work, we investigate word neighborhood
measures as predictors of ASR errors. We pro-
pose new neighborhood measures that we find to
be more well-suited to ASR than standard neigh-
borhood density measures. We also propose a
new mechanism to incorporate frequency weight-
ing within the measures. Finally, we analyze the
measures as predictors of errors in an isolated-
word recognition system and a continuous-word
recognition system for conversational speech.
2 Related Work: Neighborhood Density
Measures
In much of the prior work in the psycholinguistics
literature, the notion of word similarity is quanti-
fied by a simple one-phone-away rule: A word w
?
is a neighbor of wordw ifw andw
?
differ by a sin-
gle phone, via a substitution, deletion, or insertion.
We refer to this density measure as ?ND?.
ND =
?
w
?
?
ND
(w,w
?
)
where ?
ND
(w,w
?
) = 1 if w and w
?
differ by a
phone and 0 otherwise.
The frequencies of the neighbors are often ac-
counted for in the neighborhood density measure
by computing the sum of the raw (or log) frequen-
cies of a word?s neighbors (Luce and Pisoni, 1998;
Vitevitch and Luce, 1999); the word frequencies
1
are derived from a large corpus. We refer to this
frequency-weighted measure as ?wND?.
wND =
?
w
?
?
ND
(w,w
?
) ? pi(w
?
)
where pi(w
?
) is the frequency of the word w
?
.
1
Both ND and wND are popular measures for word
neighborhoods that we consider to be our base-
lines; Goldwater et al. (2010) also make use of
these two density measures.
2
Neither of these measures account for the fre-
quency of the word itself. In continuous ASR,
which uses a language model, frequent words are
more likely to be recognized correctly (Fosler-
Lussier and Morgan, 1999). To account for this,
instead of using absolute frequencies of the neigh-
boring words, we use their relative frequencies to
define a third baseline density measure,?rwND?
(relative-wND):
rwND =
?
w
?
?
ND
(w,w
?
) ?
pi(w
?
)
pi(w)
Relative frequencies have appeared in prior
work (Luce, 1986; Luce and Pisoni, 1998; Scar-
borough, 2012). In fact, the measure used by Scar-
borough (2012) is the reciprocal of rwND.
3 Proposed Neighborhood Measures
Our new neighborhood measures are defined in
terms of a distance function between a pair of
words, ?, and a weighting function, ?. The pro-
posed measures are not densities in the same sense
as ND, wND, rwND, but are scores that we may
expect to correlate with recognition errors. We de-
fine the neighborhood score for a word w as:
score(w) =
?
w
?
6=w
?(w,w
?
) ??(w,w
?
) (1)
Intuitively, ? is an averaging function that weighs
the importance of each neighboring word. For ex-
ample, Yarkoni et al. (2008) use a neighborhood
measure that gives equal importance to the top
1
Here we use raw rather than log frequencies. The base-
line density measures in this section perform better with raw
rather than log frequencies on our evaluation data. Our pro-
posed measures perform significantly better than the baseline
measures using both raw and log frequencies.
2
Goldwater et al. (2010) also consider the number of ho-
mophones (words that share a pronunciation with the tar-
get word) and frequency-weighted homophones as additional
neighborhood measures. In our data there is insufficient ho-
mophony for these measures to be significant, so we do not
report on experiments using them.
20 closest neighbors and rejects the others. The
rest of the section presents multiple choices for ?
and ? which will define our various neighborhood
measures via Equation 1.
3.1 Distance Functions
All of our distance functions are based on an edit
distance between a pair of words, i.e., the mini-
mum cost incurred in converting one word to the
other using substitutions, insertions and deletions
of the sub-word units in the word. In addition
to binary edit costs, we consider edit costs that
depend on sub-phonetic properties of the phones
rather than a uniform cost across all phones. Sec-
ond, instead of a single pronunciation for a word,
we consider a distribution over multiple pronun-
ciations. These distance functions can be easily
computed via finite-state transducer (FST) opera-
tions, as explained below (see also Figure 1).
Edit Distance (?
ED
): This is the simplest edit
distance function that incurs an equal cost of 1 for
any substitution, insertion, or deletion. To com-
pute the distance between a pair of words, each
word w is represented as a finite state acceptor,
F
w
, that accepts the pronunciations (phone se-
quences) of the word. We also introduce a memo-
ryless transducer, T , that maps an input phone to
any output phone, with arc weights equal to the
corresponding substitution costs (mapping to or
from epsilon indicates a deletion or an insertion).
The weight of the shortest path in the composed
FST, F
w
?T ?F
w
?
, gives the edit distance between
w and w
?
. When either w or w
?
has more than
one pronunciation, ?
ED
is the minimum edit dis-
tance among all pairs of pronunciations. This edit
distance function has been previously proposed
as a measure of phonological similarity between
words (Hahn and Bailey, 2005). Similar distance
functions have also been used for neighborhood
density measures in visual word recognition stud-
ies (Yarkoni et al., 2008).
Simple Articulatory Feature-based Edit Dis-
tance (?
AF
): The distance function ?
ED
pe-
nalizes an incorrect substitution equally regardless
of the phone identity; for example, the phone [p]
can be substituted with [b] or [aa] with equal cost
according to ?
ED
, although we know it is more
likely for [p] to be produced as [b] than as [aa]. To
account for this, we adopt a finer-grained repre-
sentation of the phone as a vector of discrete artic-
ulatory ?features?. Our features are derived from
2
?AF
:
0 1 2 3
?
0
?
0 1 2 3
k ah m k aa
p
k:/3.364
m:p/3.464
k:k/0
? ? ?
?
AFx
:
0 1
2
3
4 5
?
0
?
0 1
2
3
4 5
k
g
gcl
kcl
ah
ax
em
kcl
m
ah
ah n
ax n
k
g
gcl
kcl
aa
ao
pcl
p
pcl
ah n
ah
ax n
k:/3.364
m:p/3.464
k:k/0
? ? ?
Figure 1: Distance functions implemented using finite-state machines.
the vocal tract variables of articulatory phonol-
ogy (Browman and Goldstein, 1992), including
the constriction degrees and locations of the lips,
tongue tip, tongue body, velum and glottis.We bor-
row a particular feature set from (Livescu, 2005).
3
The substitution cost between two phones is de-
fined as the L1 distance between the articulatory
vectors corresponding to the phones. We set the
insertion and deletion costs to the mean substitu-
tion cost between the articulatory vectors for all
phone pairs. These new costs will appear as the arc
weights on the edit transducer T . This is shown
in Figure 1; apart from the difference in the arc
weights on T , ?
AF
is the same as ?
ED
.
Extended Articulatory Feature-based Edit Dis-
tance (?
AFx
): The words in our dictionary are
associated with one or more canonical pronuncia-
tions written as sequences of phones. The distance
functions ?
ED
and ?
AF
make use of this small set
of canonical pronunciations and do not capture the
various other ways in which a word can be pro-
nounced. An alternative, explored in some prior
work on pronunciation modeling (Deng and Sun,
1994; Richardson et al., 2003; Livescu and Glass,
2004; Mitra et al., 2011; Jyothi et al., 2011), is
to model the pronunciation of a word as multiple,
possibly asynchronous streams of fine-grained ar-
ticulatory features, again inspired by articulatory
phonology. Such a model can be implemented as
a dynamic Bayesian network (DBN) with multi-
ple variables representing the articulatory features
3
The mapping of phones to their articulatory feature val-
ues is defined in Appendix B of Livescu (2005). This map-
ping includes a probability distribution over feature values
for certain phones; in these cases, we choose the articulatory
feature value with the highest probability.
in each time frame; please refer to (Livescu and
Glass, 2004; Livescu, 2005; Jyothi et al., 2011)
for more details. In this approach, deviations from
a dictionary pronunciation are the result of either
asynchrony between the articulatory streams (ac-
counting for effects such as nasalization, round-
ing, and epenthetic stops) or the substitution of one
articulatory feature value for another (accounting
for many reduction phenomena).
Jyothi et al. (2012) describe an approach to
encode such a DBN model of pronunciation as
an FST that outputs an articulatory feature tu-
ple for each frame of speech. We modify this
FST by mapping each articulatory feature tuple
to a valid phone as per the phone-to-articulatory-
feature mapping used for ?
AF
(discarding arcs
whose labels do not correspond to a valid phone).
The resulting FSTs are used to define ?
AFx
by
composing with the edit transducer T as in the
definition of ?
AF
. For computational efficiency,
we prune these FSTs to retain only paths that are
within three times the weight of the shortest path.
The pruned FSTs have hundreds of arcs and ?50
states on average. A schematic diagram is used to
illustrate the computation of ?
AFx
in Figure 1.
3.2 Weighting Functions
Our weighting functions can be appropriately de-
fined to discount the contributions of words that
are infrequent or are very far away. We note here
that unlike the density measures in Section 2, the
lower the distance-based score for a word (from
Equation 1), the more confusable it would be with
its neighbors. One approach, as pursued in Nosof-
sky (1986) and Bailey and Hahn (2001), is to use
score(w) =
?
w
?
g(?(w,w
?
)) where g is an expo-
3
r1
r
2
0
0.2
0.4
0.6
0.8
1
?
(
r
)
Figure 2: Let w
1
and w
2
be the two closest
words to w. The area of the shaded region shows
?(w,w
2
) where r
i
= R
w
(w
i
) = i. In the
weighted case given in Equation 4, r
1
= R
?
w
(w
1
),
r
2
= R
?
w
(w
2
) and r
2
? r
1
= ?
w
(w
2
).
nentially decreasing function. This, however, has
the disadvantage of being very sensitive to the dis-
tance measure used: Slight changes in the distance
can alter the score significantly, even if the overall
ordering of the distances is preserved. We propose
an alternative approach that keeps the score as a
linear function of the distances as long as the or-
dering is fixed. For this, we introduce ?(w,w
?
) in
Equation 1 and let it be a (possibly exponentially)
decreasing function of the rank of w
?
.
Formally, we define the rank of w
?
with re-
spect to w, R
w
(w
?
), as follows: Fix an ordering
of all N ? 1 words in the vocabulary other than
w as (w
1
, w
2
, . . . , w
N?1
) such that ?(w,w
i
) ?
?(w,w
i+1
) for all i ? {1, . . . , N ? 2}. Then
R
w
(w
?
) = j if w
?
= w
j
in the above ordering.
We then define ? in terms of a ?decay? function ?:
?(w,w
?
) =
?
R
w
(w
?
)
R
w
(w
?
)?1
?(r)dr (2)
If ? is monotonically decreasing, Equation 2 en-
sures that neighbors with a higher rank (i.e., fur-
ther away) contribute less weight than neighbors
with a lower rank. For example, a measure
that gives equal weight to the k closest neigh-
bors (Yarkoni et al., 2008) corresponds to
?(r) =
{
1 if r ? k
0 otherwise
Instead of a step function that gives equal weight
to all k neighbors, we define ? as an exponen-
tially decreasing function of rank: ?(r) = e
?r
.
Then, from Equation 2, we obtain ?(w,w
?
) =
(e?1)e
?R
w
(w
?
)
. Figure 2 shows the exponentially
decreasing ?(r) and a sample ?(w,w
?
).
We know from prior work that it is also impor-
tant to distinguish among the neighbors depending
on how frequently they appear in the language. To
account for this, we define a frequency-weighted
rank function, R
?
w
(w
?
):
R
?
w
(w
?
) =
R
w
(w
?
)
?
i=1
?
w
(w
i
) (3)
where ?
w
is a suitably defined frequency function
(see below). We now redefine ? as:
?(w,w
?
) =
?
R
?
w
(w
?
)
R
?
w
(w
?
)??
w
(w
?
)
?(r)dr (4)
Note that when ?
w
(w
?
) = 1 for all w
?
, Equation 4
reduces to Equation 2. ?(w,w
?
) is robust in that
it is invariant to the ordering used to define rank,
R
?
w
, i.e. words with the same distance from w can
be arbitrarily ordered. Also, multiple words at the
same distance contribute to ? equally to a single
word at the same distance with a frequency that is
the sum of their frequencies.
We use three choices for ?
w
(w
?
):
1. The first choice is simply ?
w
(w
?
) = 1 for all
w
?
.
2. Let pi(w
?
) be the unigram probability of w
?
. We
then define ?
w
(w
?
) = P ? pi(w
?
) where P is
a scaling parameter. One natural choice for
P is the perplexity of the unigram probability
distribution, pi, i.e., 2
?
?
w
pi(w) log(pi(w))
. With
this choice of P , when pi is a uniform distribu-
tion over all words in the vocabulary, we have
?
w
(w
?
) = 1 for all w
?
, and R
?
w
(w
?
) = R
w
(w
?
).
3. As defined above, ?
w
(w
?
) does not depend on
w. Our third choice for the frequency func-
tion considers the frequency of w
?
relative to
w: ?
w
(w
?
) =
pi(w
?
)
/pi(w)
To summarize, Equation 1 gives the neighbor-
hood score for w in terms of ? and ?. We use
three choices for ? as specified in Section 3. ?
is defined by Equation 4 where R
?
w
is defined
by Equation 3 in terms of the frequency function
?
w
. We use the three choices described above for
?
w
. The resulting nine score functions are sum-
marized in Table 1. For completeness, we also
include the neighborhood density baseline mea-
sures and represent them using our notation with
a distance function defined as ?
ND
(w,w
?
) =
4
Measure ?(r) ?(w,w
?
) ?
w
(w
?
)
ND
1 ?
ND
1
wND pi(w
?
)
rwND
pi(w
?
)
pi(w)
ED
e
?r
?
ED
1
wED pi(w
?
) ? P
rwED
pi(w
?
)
pi(w)
AF
?
AF
1
wAF pi(w
?
) ? P
rwAF
pi(w
?
)
pi(w)
AFx
?
AFx
1
wAFx pi(w
?
) ? P
rwAFx
pi(w
?
)
pi(w)
Table 1: Summary of neighborhood measures.
1(?
ED
(w,w
?
) = 1) (i.e. ?
ND
(w,w
?
) = 1 if
?
ED
(w,w
?
) = 1 and 0 otherwise) and ? = 1.
With ? = 1 and ?(w,w
?
) = ?
w
(w
?
), the three
choices of ?
w
give us ND, wND and rwND, as
shown in Table 1. The notation ?
ND
(w,w
?
) is
to highlight the inverse relationship of the density
measures with our distance-based measures.
4 Experiments
We provide an individual analysis of each neigh-
borhood measure as it relates to recognition error
rate. We also present a matrix of pairwise com-
parisons among all of the neighborhood measures
with respect to their ability to predict recognition
errors. We study the relationship between neigh-
borhood measures and ASR errors in two settings:
? Isolated-word ASR: Psycholinguistic stud-
ies typically use isolated words as stimuli to study
the influence of neighborhood measures on recog-
nition (e.g., see Goldwater et al. (2010) and ref-
erences therein). Motivated by this, we build an
ASR system that recognizes words in isolation
and analyze the relationship between its errors and
each neighborhood measure. Further details of
this analysis are described in Section 4.1.
? Continuous-word ASR: ASR systems typ-
ically deal with continuous speech. However,
the usefulness of neighborhood measures for
continuous-word ASR has received little atten-
tion, with the notable exception of Goldwater et
al. (2010). We further this line of investigation in
our second set of experiments by analyzing the re-
lationship between errors made by a continuous-
word ASR system and our new measures. These
are described in more detail in Section 4.2.
4.1 Isolated-Word ASR
Experimental Setup: We extract isolated words
from a subset of the Switchboard-I conversational
speech corpus (Godfrey et al., 1992) called the
Switchboard Transcription Project, STP (Green-
berg et al., 1996; STP, 1996), which is phonet-
ically labeled at a fine-grained level. Isolated
words were excised from continuous utterances in
sets 20?22 in the STP corpus. We use a total of
401 word tokens (247 unique words) derived from
the 3500 most frequent words in Switchboard-I,
excluding non-speech events and partial words.
These words make up the development and eval-
uation sets used in prior related work on pronun-
ciation modeling (Livescu and Glass, 2004; Jyothi
et al., 2011; Jyothi et al., 2012). We use the dictio-
nary that accompanies the Switchboard-I corpus
consisting of 30,241 words; ?98% of these words
are associated with a single pronunciation.
The recognition system for this isolated word
dataset was built using the Kaldi toolkit (Povey
et al., 2011; Kal, 2011). We use an acous-
tic model that is trained on all of Switchboard-
I, excluding the sentences from which our 401-
word set was drawn. The ASR system uses stan-
dard mel frequency cepstral coefficients with their
first and second derivatives (deltas and double-
deltas) as acoustic features, with standard normal-
ization and adaptation techniques including cep-
stral mean and variance normalization and maxi-
mum likelihood linear regression. Linear discrim-
inant analysis (LDA) and maximum likelihood lin-
ear transform (MLLT) feature-space transforma-
tions were applied to reduce the feature-space di-
mensionality (Povey et al., 2011). The acous-
tic models are standard Gaussian mixture model-
Hidden Markov models (GMM-HMMs) for tied-
state triphones. The recognition vocabulary in-
cludes 3328 words, consisting of the 3500 most
frequent words from Switchboard excluding par-
tial and non-speech words.
4
Since this is an
isolated-word task, the ASR system does not use
any language model.
Results and Discussion: In order to individu-
ally analyze each of the neighborhood measures,
4
Large-vocabulary automatic recognition of isolated
words is a hard task due to the absence of constraints from
a language model. Using the entire Switchboard vocabulary
would greatly deteriorate the recognition performance on an
already hard task. Thus, we restrict the vocabulary to 1/10th
of the original size in order to obtain reasonable performance
from the isolated ASR system.
5
0 10 20 30 40 50 60
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
ND
E
R
0.000 0.010 0.020
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
wND
E
R
5 10 15
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
wAFx
E
R
(a) Neighborhood measures ND, wND and wAFx as predictors of isolated-word error rate (ER).
ND ED AF AFx wND wED wAF wAFx
ND - - - - - - - -
ED - - - - - - - -
AF - - - - - - - -
AFx - - - - - - - -
wND - - - - - - - -
wED - - - - - - - -
wAF - - - - - - - -
wAFx - - - - - - - -
null 5
?6
8
?5
5
?7
1
?7
8
?8
2
?8
3
?10
6
?11
0 0.0001 0.001 0.01 0.05 0.1 1
(b) Pairwise comparison of word neighborhood measures as predictors of errors from the isolated-word ASR system using
p-values. Many low p-values (darker cells) along a column implies the corresponding measure is a significant predictor of ER.
Figure 3: Analysis of neighborhood measures with isolated word ASR.
following Goldwater et al. (2010), we use a logis-
tic regression model implemented using the glm
function in R (R Development Core Team, 2005).
The logistic regression model fits the log-odds of
a binary response variable with a linear combina-
tion of one or more predictor variables. For our
isolated-word task, the response variable takes a
value of either 1 or 0 corresponding to the pres-
ence or absence of an error, respectively; we will
refer to it as ?ER?. We build a separate logis-
tic regression model for each neighborhood mea-
sure acting as the only predictor of ER. We use
restricted cubic splines, using the rcs (Harrell Jr.,
2012) function in R, to model non-linear predic-
tive relationships. In order to determine whether
a neighborhood measure is a significant predictor
of ER, we use a likelihood-ratio test (using the
anova function in R) that compares the fit of the
model including only that neighborhood measure
as a predictor against the fit of a baseline model in-
cluding only an intercept and no other predictors.
All of the neighborhood measures were found to
be significant predictors, with our measures wAF
and wAFx being most significant. The p-values
from this test are shown in a separate row under
the header ?null? in Figure 3(b); here, 5
?6
stands
for 5? 10
?6
and so forth. We note that the neigh-
borhood measures are significantly correlated with
ER as individual predictors, but classifiers built
with each individual measure as the only feature
are not good predictors of ASR errors. This is
unsurprising as we expect many other predictors
other than neighborhood measures, as outlined in
Goldwater et al. (2010), to influence ASR errors.
This paper focuses only on analyzing each neigh-
borhood measure as an individual predictor; joint
models will be explored as part of future work.
Figure 3(a) shows the relationship between er-
rors from the isolated ASR system and three
neighborhood measures: the best-performing
measure (wAFx) and the two standard density
measures (ND, wND). The feature values are ag-
gregated into roughly equal-sized bins and the
average error rate for each bin is plotted. The
6
0.000 0.005 0.010 0.015 0.020
0.
0
0.
2
0.
4
0.
6
wND
IW
ER
0 10 20 30 40
0.
0
0.
2
0.
4
0.
6
rwND
IW
ER
2 4 6 8 10
0.
0
0.
2
0.
4
0.
6
rwAFx
IW
ER
(a) Neighborhood measures wND, rwND and rwAFx as predictors of IWER.
ND ED AF AFx wND wED wAF wAFx rwND rwED rwAF rwAFx
ND - - - - - - - - - - - -
ED - - - - - - - - - - - -
AF - - - - - - - - - - - -
AFx - - - - - - - - - - - -
wND - - - - - - - - - - - -
wED - - - - - - - - - - - -
wAF - - - - - - - - - - - -
wAFx - - - - - - - - - - - -
rwND - - - - - - - - - - - -
rwED - - - - - - - - - - - -
rwAF - - - - - - - - - - - -
rwAFx - - - - - - - - - - - -
null 0.09 0.72 0.08 0.04 0.18 0.14 0.002 0.03 0.001 0.02 2
?5
2
?5
0 0.0001 0.001 0.01 0.05 0.1 1
(b) Pairwise comparison of all word neighborhood measures as predictors of IWER from the continuous-word ASR system.
Figure 4: Analysis of neighborhood measures with continuous-word ASR system.
solid line shows the probability of an error from
the corresponding logistic regression model and
the dashed lines show a 95% confidence interval.
The dotted line is the average error rate from the
entire data set of 401 words, 0.483. The plots
clearly show the inverse relationship between our
distance-based measure (wAFx) and the density
measures (ND and wND). The slope of the fitted
probabilities from the logistic regression model
for a measure is indicative of the usefulness of the
measure in predicting ER. All of the measures are
significant predictors having non-zero slope with a
slightly larger slope for wAFx than ND and wND.
ND and wND being significant predictors of errors
for isolated words is consistent with prior stud-
ies from human speech recognition. The proposed
measures, wAF and wAFx, stand out as the best
predictors of errors. We next analyze the differ-
ences between the measures more closely.
Figure 3(b) shows a pairwise comparison of the
word neighborhood measures. Each cell {i, j}
shows a p-value range from a likelihood-ratio test
that compares the fit of a logistic regression model
using only measure i as a predictor with the fit of a
model using both measures i and j as independent
predictors. Lower p-values (darker cells) indicate
that adding the measure in column j significantly
improves the ability of the model to predict ER, as
opposed to only using the measure along row i.
5
We use such nested models to compare the model
fits using likelihood-ratio significance tests. It is
clear from Figure 3(b) that our measures wAF and
wAFx are the most significant predictors.
5
The relative frequency-weighted measures (rwND,
rwED, rwAF, rwAFx) were omitted since (wND, wED, wAF,
wAFx) are significantly better predictors. This could be be-
cause the isolated-word system has no language model and is
thus unaffected by the target word frequency.
7
4.2 Continuous-word ASR
Experimental Setup: For the continuous-word
task, our evaluation data consists of full sentences
from Switchboard-I that were used to extract the
isolated words in Section 4.1. For our analysis, we
include all the words in the evaluation sentences
that are 3 or more phonemes long and occur 100
times or more in the training set. This gives us a
total of 1223 word tokens (459 word types).
The continuous-word ASR system uses an
acoustic model trained on all of Switchboard-
I excluding the above-mentioned evaluation sen-
tences. The acoustic models are GMM-HMMs for
tied-state triphones using MFCC + delta + double-
delta features with LDA and MLLT feature-space
transformations and speaker adaptation. They are
also trained discriminatively using boosted maxi-
mum mutual information training from the Kaldi
toolkit. We use the entire Switchboard vocabu-
lary of 30,241 words and a 3-gram language model
trained on all of the training sentences. The word
error rate on the evaluation sentences is 28.3%.
6
Results and Discussion: Unlike the isolated-
word task, the continuous-word ASR system gives
word error rates over full utterances. Since we
need to measure the errors associated with the in-
dividual words, we use the individual word er-
ror rate (IWER) metric proposed by Goldwater et
al. (2010). The IWER for wordw
i
is ??in
i
+del
i
+
sub
i
where in
i
is the number of insertions adja-
cent to w
i
; del
i
or sub
i
is 1 if w
i
is either deleted
or substituted, respectively. ? is chosen such that
? ?
?
i
in
i
= I where I is the total number of inser-
tions for the entire dataset.
As in the isolated-word task, we fit logistic re-
gression models to analyze the neighborhood mea-
sures as predictors of IWER. Figure 4(a) shows fit-
ted probabilities from a logistic regression model
for IWER built individually using each of the mea-
sures wND, rwND and rwAFx as predictors. The
number of frequency-weighted neighbors, wND
(as well as the number of neighbors, ND), was
not found to be a significant predictor of IWER.
This is consistent with the findings in Goldwater
et al. (2010) that show weak correlations between
6
The training set includes other utterances from the same
speakers in the STP evaluation utterances. This allows for
an additional boost in performance from the speaker adapted
acoustic models during recognition. Ideally, the training and
evaluation sets should not contain utterances from the same
speakers. We allow for this to get word error rates that are
more comparable to state-of-the-art results on this corpus.
the number of frequency-weighted neighbors and
the probability of misrecognizing a word. How-
ever, we find that using the number of frequency-
weighted neighbors relative to the frequency of
the word (rwND) improves the correlation with
the probability of error (seen in Figure 4(a) as an
increase in slope). Using our proposed distance
measures with relative frequency weighting im-
proves the correlation even further.
Figure 4(b) shows a pairwise comparison of all
measures in Table 1; the interpretation is sim-
ilar to Figure 3(b). We observe that the rela-
tive frequency-weighted measures (rwND, rwED,
rwAF, rwAFx) are consistently better than their
unweighted (ND, ED, AF, AFx) and frequency-
weighted (wND, wED, wAF, wAFx) counterparts,
with rwAF and rwAFx being most significant.
This suggests that the relative frequency-weighted
measures are taking precedence in the continuous-
word task as significant predictors of IWER (un-
like in the isolated-word task) due to the presence
of a strong language model.
5 Conclusion
In this work, we propose new word neighborhood
measures using distances between words that em-
ploy a fine-grained articulatory feature-based rep-
resentation of the word. We present a new rank-
based averaging method to aggregate the word dis-
tances into a single neighborhood score. We also
suggest multiple ways of incorporating frequency
weighting into this score. We analyze the signifi-
cance of our word neighborhood measures as pre-
dictors of errors from an isolated-word ASR sys-
tem and a continuous-word ASR system. In both
cases, our measures perform significantly better
than standard neighborhood density measures.
This work reopens the question of whether word
neighborhood measures are a useful variable for
ASR. There are many possible directions for fu-
ture work. Our measures could be refined fur-
ther, for example by exploring alternative distance
measures, different articulatory feature sets, dif-
ferent choices of ? and ? in the weighting func-
tion, or automatically learned costs and distances.
Also, our analysis currently looks at each neigh-
borhood measure as an individual predictor; we
could jointly analyze the measures to account for
possible correlations. Finally, it may be possible
to use neighborhood measures in ASR confidence
scoring or even directly in recognition as an addi-
tional feature in a discriminative model.
8
References
T. M. Bailey and U. Hahn. 2001. Determinants
of wordlikeness: Phonotactics or lexical neigh-
borhoods? Journal of Memory and Language,
44(4):568?591.
C. P. Browman and L. Goldstein. 1992. Articulatory
phonology: An overview. Phonetica, 49(3-4):155?
180.
L. Deng and D.X. Sun. 1994. A statistical approach
to automatic speech recognition using the atomic
speech units constructed from overlapping articula-
tory features. The Journal of the Acoustical Society
of America, 95(5):2702?2719.
E. Fosler-Lussier and N. Morgan. 1999. Effects of
speaking rate and word frequency on pronunciations
in conversational speech. Speech Communication,
29(2):137?158.
E. Fosler-Lussier, I. Amdal, and H-K. J. Kuo. 2005. A
framework for predicting speech recognition errors.
Speech Communication, 46(2):153?170.
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In Proc. of ICASSP.
S. Goldwater, D. Jurafsky, and C. D. Manning. 2010.
Which words are hard to recognize? Prosodic,
lexical, and disfluency factors that increase speech
recognition error rates. Speech Communication,
52(3):181?200.
S. Greenberg, J. Hollenback, and D. Ellis. 1996. In-
sights into spoken language gleaned from phonetic
transcription of the Switchboard corpus. In Proc. of
ICSLP.
U. Hahn and T. M. Bailey. 2005. What makes words
sound similar? Cognition, 97(3):227?267.
F. E. Harrell Jr. 2012. RMS: Regression Modeling
Strategies. R package version 3.5-0.
J. Hirschberg, D. Litman, and M. Swerts. 2004.
Prosodic and other cues to speech recognition fail-
ures. Speech Communication, 43(1):155?175.
P. Jyothi and E. Fosler-Lussier. 2009. A comparison of
audio-free speech recognition error prediction meth-
ods. In Proc. of Interspeech.
P. Jyothi, K. Livescu, and E. Fosler-Lussier. 2011.
Lexical access experiments with context-dependent
articulatory feature-based models. In Proc. of
ICASSP.
P. Jyothi, E. Fosler-Lussier, and K. Livescu. 2012. Dis-
criminatively learning factorized finite state pronun-
ciation models from dynamic Bayesian networks. In
Proc. of Interspeech.
2011. Kaldi. http://kaldi.sourceforge.
net/.
K. Livescu and J. Glass. 2004. Feature-based pronun-
ciation modeling with trainable asynchrony proba-
bilities. In Proc. of ICSLP.
K. Livescu. 2005. Feature-based Pronunciation Mod-
eling for Automatic Speech Recognition. PhD Dis-
sertation, MIT EECS department.
P. A. Luce and D. B. Pisoni. 1998. Recognizing spo-
ken words: The neighborhood activation model. Ear
and hearing, 19:1?36.
P. A. Luce. 1986. Neighborhoods of words in the men-
tal lexicon. Research on Speech Perception, (Tech-
nical Report No. 6.).
W. D. Marslen-Wilson. 1987. Functional parallelism
in spoken word-recognition. Cognition, 25(1):71?
102.
V. Mitra, H. Nam, C. Y. Espy-Wilson, E. Saltzman,
and L. Goldstein. 2011. Articulatory information
for noise robust speech recognition. IEEE Transac-
tions on Audio, Speech, and Language Processing,
19(7):1913?1924.
R. M. Nosofsky. 1986. Attention, similarity, and the
identification?categorization relationship. Journal
of Experimental Psychology: General, 115(1):39.
D. Povey, A. Ghoshal, et al. 2011. The Kaldi speech
recognition toolkit. Proc. of ASRU.
R Development Core Team. 2005. R: A language and
environment for statistical computing. R foundation
for Statistical Computing.
M. Richardson, J. Bilmes, and C. Diorio. 2003.
Hidden-articulator Markov models for speech recog-
nition. Speech Communication, 41(2-3):511?529.
R. A. Scarborough. 2012. Lexical confusability and
degree of coarticulation. In Proceedings of the An-
nual Meeting of the Berkeley Linguistics Society.
T. Shinozaki and S. Furui. 2001. Error analysis using
decision trees in spontaneous presentation speech
recognition. In Proc. of ASRU.
1996. The Switchboard Transcription Project.
http://www1.icsi.berkeley.edu/
Speech/stp/.
M. S. Vitevitch and P. A. Luce. 1999. Probabilis-
tic phonotactics and neighborhood activation in spo-
ken word recognition. Journal of Memory and Lan-
guage, 40(3):374?408.
T. Yarkoni, D. Balota, and M. Yap. 2008. Mov-
ing beyond Coltheart?s N: A new measure of ortho-
graphic similarity. Psychonomic Bulletin & Review,
15(5):971?979.
9
