A word-grammar based morl)hoh)gieal nalyzer 
for agglutinative languages 
Aduriz 1.+, Agirre E., Aldezabal I., Alegria I., Arregi X., Arriohl J. M., Artola X., Gojenola K., 
Marilxalar A., Sarasola K., Urkia M.+ 
l)ept, of Colllptiier 1Aulgtlages and Systems, University of lhe Basqtlo Cotlnlry, 64.9 P. K., 
E-20080 1)onostia, Basque Counh'y 
tUZEI, Aldapeta 20, E-20009 1)onostia, Basque Country 
+Universidad de Barcelona, Grin Vfii de Isis Cortes CalaiallaS, 585, E-08007 Flarcelona 
j ipgogak @ si.elm, es. 
Abst rac l  
Agglutinative languages presenl rich 
morphology and for sonic applications 
they lleed deep analysis at word level. 
Tile work here presenled proposes a 
model for designing a full nlorpho- 
logical analyzer. 
The model integrates lhe two-level 
fornlalisnl alld a ullificalion-I)asod 
fornialisni. In contrast to other works, 
we propose to separate the treatment of 
sequential and non-sequetTtial mou)ho- 
lactic constraints. Sequential constraints 
are applied in lhe seglllenlalion phase, 
and non-seqtlontial OlleS ill the filial 
feature-combination phase. Early appli- 
cation of sequential nlorpholactic 
coilsli'aiills during tile segnloillaiioi/ 
process nlakes feasible :,ill officienl 
iinplenleilialion of tile full morpho- 
logical analyzer. 
The result of lhis research has been tile 
design and imi)len~entation of a full 
nlorphosynlactic analysis procedure for 
each word in unrestricted Basque texts. 
I n t roduct ion  
Morphological analysis of woMs is a basic 
tool for automatic language processing, and 
indispensable when dealing willl highly 
agglutinative languages like Basque (Aduriz el 
al., 98b). In lhis conlext, some applications, 
like spelling corfeclion, do ilOI need illOl'e lhan 
the seglllOlltation of each word inlo its 
different COlllponenl nlorphellles alollg with 
their morphological information, ltowever, 
there are oiher applications such as lemnializa- 
tion, lagging, phrase recognition, and 
delernlinaiion of clause boundaries (Aduriz el 
al., 95), which need an additional global 
morphological i)arsing j of the whole word. 
Such a complete nlorphological analyzer has 
lo consider three main aspects (l~,ilchie et al, 
92; Sproal, 92): 
1 Morl)hographenfics (also called morpho- 
phonology). This ternl covers orthographic 
variations that occur when linking 
I l lOfphellleS. 
2) morpholactics. Specil'ication of which 
nlorphenles can or cannot combine with 
each other lo form wflid words. 
3) Feature-combination. Specification of how 
these lnorphemes can be grouped and how 
their nlorphosyntactic features can be 
comlfined. 
The system here presented adopts, oil the one 
hand, tile lwo-level fornlalisnl to deal with 
morphogralfilemics and sequential morl)ho- 
lactics (Alegria el al., 96) and, on the other 
hand, a unification-based woM-grammar 2 to 
combine the grammatical information defined 
in nlorphemes and to  tackle complex 
nlorphotactics. This design allowed us to 
develop a full coverage analyzer that processes 
efl'iciently unrestricted texts in Basque. 
The remainder of tills paper is organized sis 
follows. After a brief' description of Basque 
nlorphology, section 2 describes tile 
architecture for morphological processing, 
where the morphosynlactic omponent is 
included. Section 3 specifies tile plaenomena 
covered by the analyzer, explains its desigi~ 
criteria, alld presents implementation and 
ewthialion details. Section d compares file 
I This has also been called mo*7)hOSh,ntactic 
parsitlg. When we use lhc \[(fill #11017~\]lOSyltl~/X WC 
will always refer to il~c lficrarchical structure at 
woM level, conlbining morphology and synlax. 
2 '\]'\]lt3 \[IDl'll\] WOl'd-gF(lllllllUl" should not be confused 
with the synlaclic lilcory presented in (Hudson, 84). 
system with previous works. Finally, the paper 
ends with some concluding renmrks. 
1 Brief description of Basque 
morphology 
These are the most important features of 
Basque morphology (Alegria et al, 96): 
? As prepositional functions are realized by 
case suffixes inside word-fornls, Basque 
presents a relatively high power to generate 
inflected word-forms. For instance, froth a 
single noun a minimum of 135 inflected 
forms can be generated. Therefore, the 
number of simple word-forms covered by 
the current 70,000 dictionary entries woukl 
not be less than 10 million. 
? 77 of the inflected forms are simple 
combinations of number, determination, 
and case marks, not capable of further 
inflection, but the other 58 word-forms 
ending in one of the two possible genitives 
(possessive and locative) can be further 
inflected with the 135 morphemes. This 
kind of recursive construction reveals a 
noun ellipsis inside a noun phrase and 
could be theoretically exteuded ad 
infinitum; however, in practice it is not 
usual to fiud more than two levels of this 
kind of recursion in a word-form. Taking 
into account a single level of noun ellipsis, 
the number of word-forum coukl be 
estimated over half a billion. 
? Verbs offer a lot of grammatical 
information. A verb tbrln conveys informa- 
tion about the subject, the two objects, as 
well as the tense and aspect. For example: 
diotsut (Eng.: 1 am telling you something). 
o Word-formation is very productive in 
Basque. It is very usual to create new 
compounds as well as derivatives. 
As a result of this wealth of infornmtion 
contained within word-forms, complex struc- 
tures have to be built to represent complete 
morphological information at word level. 
2 An architecture for the full 
morphological ana lyzer  
The framework we propose for the 
morphological treatment is shown in Figure 1. 
The morphological nalyzer is the fiont-end to 
all present applications for the processing of 
Basque texts. It is composed of two modules: 
the segmentation module and the 
morphosyntactic analyzer. 
conformant .................. ~ U~atabas N TEZ-conf~ 
\[Segmentation module 
____~| HorphograDhemics 
Morphotactics I 
TEI-FS .............. ~ ~ ~ ~  ~ - p ~  
conformant Cegmented TexN 
Morphosyntactic 
analyzer 
Feature- combination 
Morphotactics II 
TEI-FS \] .............. ~ actically 
Lermnatization, linguistic Analysis tagging tools 
Figure 1. Architecture 1"o1" morphological processing. 
The segmentation ,nodule was previously 
implemented in (Alegria et al, 96). This 
system applies two-level morphology 
(Koskenniemi, 83) for the morphological 
description and obtains, for each word, its 
possible segmentations (one or many) into 
component morphemes. The two-level system 
has the following components: 
? A set of 24 morphograf~hemic rules, 
compiled into transducers (Karttunen, 94). 
? A lexicon made up of around 70,000 items, 
grouped into 120 sublexicons and stored in 
a general lexical database (Aduriz et al, 
98a). 
This module has full coverage of free-running 
texts in Basque, giving an average number of 
2.63 different analyses per word. The result is 
the set of possible morphological segmenta- 
tions of a word, where each morpheme is 
associated with its corresponding features in 
the lexicon: part of speech (POS), 
subcategory, declension case, number, 
definiteness, as well as syntactic function and 
some semantic features. Therefore, the output 
of the segmeutation phase is very rich, as 
shown in Figure 2 with the word amarengan 
(Eng.: on the mother). 
grammar 
mother) 
POS noun) 
subc~t common 
:count: +) 
(an imate  +) 
(nleasurable "-) 
aren 
(of life) 
(POS decl-suffix) 
(definite +) 
(number sing) 
(case genitive) 
(synt-f @nouncomp) 
J gan \] 
(o.1 / 
(POS decl-suf fix) I 
(case inossivo) \] 
(synt-f @adverbial)I 
=> 
amarengan 
(o. the mother) 
POS noun) 
subcat common) 
number sing) 
definite +) 
case inessive) 
count +) 
animate +) 
measurable -) 
synt-f @adverbial) 
iq:e, ure 2. Morphosynlactic analysis eof (unureugun (l{ng.: (m 
The architecture is a modular envhoument that 
allows different ypes of output depending on 
the desired level of analysis. The foundation of 
the architecture lies in the fact lhat TEI- 
confommnt SGML has been adopted for the 
comnmnication allloIlg modules (Ide and 
VCFOIIiS, 95). l~'eature shucluleS coded 
accoMing TIU are used to represent linguistic 
information, illcluding tile input mM outl)ut of 
the morplaological analyzer. This reprcscnta- 
tion rambles the use of SGML-aware parsers 
and tools, and Call he easily filtered into 
different formats (Artola et ill., 00). 
3 Word level morl)hosyntactic analysis 
This section Hrst presents the l~henomena lhat 
must be covered by the morphosyntactic 
analyzer, then explains ils design criteria, and 
finally shows implementation and ewfluation 
details. 
3.1 Phenomena covered by the analyzer 
There are several features that emphasized the 
need of morphosyntactic almlysis in order to 
build up word level information: 
I) Multiplicity of values for the same feature 
in successive morphemes. In the analysis 
of Figure 2 there are two different values 
for the POS (noun and declension suffix), 
two for the case (genitive and inessive), 
and two for the syntactic function 
(@nouncomp and @adverbial). Multiple 
values at moq~hemc-level will have to be 
merged to obtain the word level infer 
mation. 
2) Words with phrase structure. Although the 
segmentation is done for isolated words, 
independently of context, in several cases 
3 l?calurc wtlues starling with the "@" character 
correspond to syntactic functions, like @noullcomp 
(norm complement) or @adverbial. 
the mother) 
tile resulting structure is oquiwflent o the 
aualysis of a phrase, as can be seen i, 
Figure 2. 111 this case, although there are 
two different cases (genitive and inessive), 
lhe case of the full word-form is simply 
inessive. 
3) Noun ellipsis inside word-lbrms. A noun 
ellipsis can occur withi, the word 
(oceasi(mally more than once). This 
information must be made explicit in the 
resulting analysis. For example, Figure 3 
shows the analysis of a single word-forln 
like diotsudumtrel&z (Eng.: with what I am 
lelling you). The first line shows its 
segmentation into four morphemes 
(die tsut+en+ 0 +arekin). The feature 
compl ill tile final analysis conveys the 
information for the verb (l um lelliHg you), 
that carries information about pc'rson, 
number and case o1' subject, object and 
indirect object. The feature comp2 
represents an elided noun and its 
declension stfffix (with). 
4) l)erivation and composition are productive 
in Basque. There arc more than 80 deri- 
w/tion morphemes (especially suffixes) 
intensively used in word-fornlatioll. 
3.2 Design of the word-grammar 
The need to impose hierarchical structure upon 
sequences of morphemes and to build complex 
constructions from them forced us to choose a 
unil'ication mechanism. This task is currently 
unsolwlble using finite-state techniques, clue to 
the growth in size of the resulting network 
(Beesley, 98). We have developed a unifica- 
tion based word-grammar, where each rule 
combines information flom different 
mot+lJlemes giving as a result a feature 
structure for each interpretation of a word- 
fol'nl, treating the previously mentioned cases. 
3 
diotsut 
I am tellh,g you) 
POS verb) 
(tense present) 
(pers-ergative is)\[ 
(pets-dative 2s) 
(pers-absol 3s) 
en 
(what) 
(POS relation) 
(subcat subord) 
(relator relative 
(synt-f @rel-clause 
0 
() 
(POS ellipsis) 
arekin 
(wire) 
(POS declension-suffix)) 
(case sociative) 
(number sing) 
(definite +) 
(synt-f @adverbial) 
=> diotsudanarekin (wi~ what lamtel l ingyou) 
(POS verb-noun_ellipsis) 
(case sociative) 
(number sing) 
(definite +) 
(synt-f @adverbial) 
(compl (POS verb) 
(subcat subord) 
(relator relative) 
(synt-f @tel-clause) 
(tense present) 
(pers-ergative is) 
(pets-dative 2s) 
(pers-absol 3s)) 
(comp2 (POS noun) 
(subcat common) 
(number sing) 
(definite+) 
(synt-f @adverbial)) 
Figure 3. Morphosyntactic analysis of diotxudanarekin (Eng.: with what I am tellittg you) 
As a consequence of the rich naorphology of 
Basque we decided to control morphotactic 
phenomena, as much as possible, in the 
morphological segmentation phase. Alterna- 
tively, a model with minimal morphotactic 
treatment (Ritchie et al, 92) would produce 
too many possible analyses after segmentation, 
which should be reiected in a second phase. 
Therefore, we propose to separate sequential 
morphotactics (i.e., which sequences of 
morphemes can or cannot combine with each 
other to form valid words), which will be 
recognized by the two-level system by means 
of continuation classes, and non-sequential 
morphotactics like long-distance dependencies 
that will be controlled by the word-gmnunar. 
The general linguistic principles used to define 
unification equations in the word-grannnar 
rules are the following: 
1) Information risen from the lemma. The 
POS and semantic features are risen flom 
the lemnm. This principle is applied to 
common nouns, adjectives and adverbs. 
The lemma also gives the mnnber in 
proper nouns, pronouns and determiners 
(see Figure 2). 
2) lnfornmtion risen from case suffixes. 
Simple case suffixes provide information 
on declension case, number and syntactic 
function. For example, tile singular 
genitive case is given by the suffix -tell in 
ama+ren (Eng.: of the mother). For 
compound case suffixes the number and 
determination are taken from the first 
suffix and the case from the second one. 
First, both suffixes are joined and after 
that they are attached to the lemma. 
3) Noun ellipsis. When an ellipsis occurs, the 
POS of the whole word-form is expressed 
by a compound, which indicates both the 
presence of the ellipsis (always a noun) 
and the main POS of the word. 
For instance, the resulting POS is 
verb-noun_e l l ips is  when a noun- 
ellipsis occurs after a verb. All the 
information corresponding to both units, 
the explicit lemma and the elided one, is 
stored (see Figure 3). 
4) Subordination morl~hemes. When a 
subordination morpheme is attached to a 
verb, the verb POS and its featm'es are 
risen as well as the subordhmte relation 
and the syntactic fnnction conveyed by the 
naorpheme. 
5) Degree morphemes attached to adjectives, 
past participles and adverbs. The POS and 
diotsudan 
(diotsut + en) 
(POS verb) 
(tense present) 
(relator relative) 
/ \ / 
diotsut 
(POS verb) 
(tense present 
diotsudanarekin 
(diotsut + en -I 0 + arekin) 
(POS verb-noun_ell ipsis) 
(case sociative) 
arekin 
(0 + arekin) 
(POS noun ellipsis) 
(case sociative) 
en 
(pos 
? . . 
o 
(POS e l l ips i s  re la t ion)  
arekin 
(case sociative) 
Figure 4. Parse tree for diotmuhmarekitl (Eng.: with what I am lellittg yott) 
main features arc taken from the lemma 
and the features corresponding to the 
degrees of comparison (comparative, 
supcrhttive) aft taken from the degree 
morphemes. 
6) l)efiwttion. 1)miwttion suffixes select tile 
POS of the base-form to create the deriw> 
tive anti in most cases to change its POS. 
For instance, the suffix -garri (Eng.: -able) 
is applied to verbs and the derived word is 
an adjective. When the derived form is 
obtained by means o1' a prefix, it does not 
change the POS of the base-form. In both 
cases the morphosyntactic rules add a new 
feature representing the structure of tile 
word as a derivative (root and affixes). 
7) Composition. At the moment, we only 
treat the most freqttent kind of 
composition (noun-noun). Since Basque is 
syntactically characterized as a right-head 
hmguage, the main information of the 
compound is taken from the second 
element. 
8) Order of application of the mofphosyn- 
tactic phenomena. When several morpho- 
syntactic phenomena are applied to the 
same leml l la ,  so as to eliminate 
nonsensical readings, the natural order to 
consider them in Basque is the following: 
lemmas, derbation prefixes, deriwltion 
suffixes, composition and inflection (see 
Figure 4). 
9) Morl)hotactic constraints. Elimination of 
illegal sequences of morphemes, such as 
those due to long-distance dependencies, 
which are difficult to restrict by means of 
conti.uation classes. 
The first and second principles are defined lo 
combine information of previously recognized 
mOrl~hemcs, but all the other principles arc 
related to both feature-combination a d non- 
sequential moq~hotactics. 
3.3 Implementation 
We have chosen the PATR formalism 
(Shiebcr, 86) for the definition of the moqflm- 
syntactic rules. There were two main reasons 
for this choice: 
? The formalism is based o.  unification. 
Unification is adequate for the treatment of 
complex phenomena (e.g., agreement of 
conslituents in case, tmmber and definite- 
hess) and complex linguistic structures. 
? Simplicity. The grammar is not linked to a 
linguistic theory, e.g. GPSG in (Ritchie et 
al., 92)? The fact that PATR is simpler than 
more sophisticated formalisms will allow 
that in @e future the grammar could be 
adapted to any of them. 
25 rules have been defined, distributed in the 
following way: 
? 11 rules for the merging of declension 
morphemes and their combination with the 
main categories, 
? 9 rules for the description of verbal 
subordination morphenles, 
? 2 general fulcs for derivation, 
? 1 rule for each of the following 
phenomeml: ellipsis, degree of COlnpavison 
of adjectives (comparative and SUl)erlative) 
and noun composition. 
3.4 Evaluat ion 
As a cousequence of the size of the lexical 
database and tile extensive treatment of 
nlorphosyntax, the resulting analyzer offers 
full coverage when applied to real texts, 
capable of treating unknown words and non- 
standard forms (dialectal wtriants and typical 
errors). 
We performed four experilnents to ewtluate 
tile efficiency of the implemented analyzer 
(see Table 1). A 10,832-word text was 
randomly selected from newspapers. We 
measured tile number of words per second 
analyzed by the morphosyntactic analyzer and 
also by the whole morphological analyzer 
(results taken on a Sun Ultra 10). Ill the first 
experiment all tile word-t'ornls were analyzed 
one-by-one; while ill tile other three experi- 
ments words with more than one occurrence 
were analyzed only once. Ill the last two 
experimeuts a memory with the analysis of tile 
most frequent word-forms (MFW) in Basque 
was used, so that only word-forms not found 
in the MFW were analyzed. 
Test 
description 
All 
word forms 
Diffcrent 
word forms 
MFW 
10,000 words 
(I 5 Mb) 
MFW 
50,000 words 
(75 mb) 
# words/scc 
analyzed Morphosynt. 
words analyzer 
10,832 
3,692 
1,483 
533 
15,13 
44 40 
111 95 
308 270 
words/see 
Full 
morphological 
analyzer 
13,5 
Table 1. Evaluation results. 
Even when our language is agglutinative, and 
its morphological phenomena need more 
computational resources to build complex and 
deep structures, the results prove tile feasibility 
of implementiug efficiently a fifll 
morphological analyzer, although efficiency 
was not the main concern of our 
implementation. The system is currently being 
applied to unrestricted texts in real-time 
applications. 
4 Related work 
(Koskeniemmi, 83) defined the formalism 
named two-level morphology. Its main 
contributiou was the treatment of 
morl)hographemics and morphotactics. The 
formalisnl has been stmcessfully applied to a 
wide wlriety ot' languages. 
(Karttunen, 94) speeds the two-level model 
compiling two-level rules into lexical 
transducers, also increasing the expressiveness 
of the model 
The morphological analyzer created by 
(Ritchie et al, 92) does not adopt finite state 
mechanisms to control morphotactic 
phenomena. Their two-level implementation 
incorporates a straightforward morphotactics, 
reducing tile number of sublexicons to the 
indispensable (prefixes, lemmas and suffixes). 
This approximation would be highly 
inefficient for agglutinative languages, as it 
would create lnany nonsensical interpretatiolas 
that should be rejected by tile unification 
phase. They use the word-grammar for both 
morphotactics and feature-conlbination. 
ill a similar way, (Trost, 90) make a proposal 
to combine two-level morphology and non- 
sequential morphotactics. 
The PC-Kimmo-V2 system (Antworth, 94) 
presents an architecture similar to ours applied 
to English, using a finite-state segmentation 
phase before applying a unification-based 
grammar. 
(Pr6szdky and Kis, 99) describe a morpho- 
syntactic analyzer for Hungarian, an agglu- 
tinative language. The system clots not use the 
two-level model for segmentation, precom- 
piling suffix-sequences to improve efficiency. 
They claim the need of a word-grammar, 
giving a first outline of its design, although 
they do not describe it in detail. 
(Oflazer, 99) presents a different approach for 
the treatment of Turkish, an agglutinative 
language, applying directly a dependency 
parsing scheme to morpheme groups, that is, 
merging morphosyntax and syntax. Although 
we are currently using a similar model to 
Basque, there are several applications that are 
word-based and need full morphological 
parsing of each word-t'orm, like the word- 
oriented Constraint Graminar formalism for 
disambiguation (Karlsson et aI., 95). 
Conc lus ion  
We propose a model for fllll morphological 
analysis iutegrating two different components. 
On tile one hand, the two-level formalism 
deals with morphographenfics and sequential 
morphotactics and, on the other hand, a 
unil\]cation-based word-grammar combines lhe 
granlll-iatical in\['ornlatioli defined in illoi'- 
phelllOS alld also handles COlllplcx illori)ho- 
tactics. 
Early application of sCqtloniial I/lOrl)hotactic 
conslraints dtu-ing the segmentation process 
avoids all excessive laUlllber of nleaningless 
segmentation possibilities before the 
coulputationally lllOlO expensive unification 
process. Unification permits lhe resohition of a 
wide variety of morl)hological phenonlena, 
like ellipsis, thal force the definition of: 
complex and deep structures Io roprosenl the 
output of the analyzer. 
This design allowed us io develop a full 
coverage allalyzor that processes efficiently 
unrestricted loxis in Basque, a strongly 
agglulinafive langttage. 
The anaiyzcl" has bccll integrated ill a gCllOl'al 
franlework for the l)lOCessing of l~asquc, with 
all the linguistic inodulos communicating by 
l l leallS O\[: foattll'C stltlClll l 'eS ill accord  {o the 
principles of ihe Text Encoding Initiative. 
Acknowledgements  
This research was partially supported by the 
Basque Government, the University of the 
\]71aS(lUe Cotlntry {/lid the CICYq' (Cotllisidn 
lntcrministorial de Ciencia y Tecnologfil). 
References 
Aduriz 1., Aldczabal I., Ansa ()., Arlola X., I)faz de 
Ilarraza A., Insau.~li .I.M. (1998a) EI)BL: a 
Mttlli-l~ttrposed Lexica/ Sttl)l)c;rl .lot the 
Treatment of Ba,s'que. Proceedings of the l;irst 
Inlernational Confcncncc on l Auiguagc Resources 
and Ewduation, Granada. 
Aduriz I., Agirre E., Aldczabal 1., Alegria 1., Ansa 
O., Arrcgi X., Arriola J.M., ArtolaX., I)faz de 
lhu'raza A., Ezciza N., Gqicnola K., Maritxahu" 
A., Maritxalar M., Oronoz M., Sarasola K., 
Soroa A., Urizar R., Urkia M. (1998b) A 
Framework .for the Automatic Pmce.vsi#~g (if" 
Basqtte. Proceedings o1 the First Ii~ternational 
Con \[elel i te on Lall.gtlagc Resources turf 
Evaluation, Granada. 
Aduriz I., Alcgria I., Arriohl J.M., Artola X., l)faz 
do Ilarraza A., Ecciza N., Gojcnola K., 
Maritxalar M. (1995) Di\[.ferelt! Issues in the 
Design qf a lemmatizer/Tagger fo Ba,s'qtte. From 
Tcxls to Tags: Issues in Mullilingual Language 
Analysis. ACL SIGI)AT Workshop, l)ublin. 
Alcgria 1., Art(Ha X., Sarasoht K., Urkia M. (1996) 
Automatic moqdzological analysis of Basque. 
IAtcrary and IAnguistic Computing, 11 (4): 193- 
203. Oxford University. 
Aniworlh E. I.. (1994) Morphological Par, ffng with 
a lhl(fication-ba,s'ed Word Grcmmutr. Norlh 
Te, xas Natural l~anguage Processing Workshop, 
Texas. 
Arlola X., Dfaz de \]larraza A., Ezciza N., Oo.icnohi 
K., Marilxahu' A., Soma A. (2000) A proposal 
for the integration of NLP tools using SGML- 
lagged documeHls. Proceedings of ll~e Second 
Cotfforence or1 Language Resources and 
Evaltmfion (IA~,EC 2000). Athens, Greece 2000. 
Bcesl%, K. (1998)AraDic Morphological Analysis 
(m the lnlernet, l'rocccdings of the International 
Conference on Mulii-IAngual Computing (Arabic 
& lhlglish), Cambridge. 
Hudson R. (1990) English Word Grammmar. 
Oxford: Basil Blackwcll. 
ldc N., Vcronis J. K. (1995) Text-Ettcoding hHtia- 
tire, Bac:kgmtmd and Context. Kluwcr Academic 
Publishers. 
Karlsson F., Voulilaincn A., Heikkiht J., Anltila A. 
(1995) Constrai, t Gnmmmr: A lxm,?tmge- 
i#ldcpcndent System Jor Pm:ffng Um'estricled 
Text, Mouton do Gruyicr ed.. 
Kartmnen 1,. (1994) Con,s'tructin~ l,e.vical 
7)'ansdttcers. Proc. of CO13NG'94, 406-411. 
Koskcnniemi, K, (1983) Two-level Mc;qdlo\[ogy: A 
ge,eral Comptttational Model ./br Word-Form 
Recognition and Pmduclioth University of 
Ilclsinki, l)clmrtmcnt of General IAnguisiics. 
l~ublications " 11. 
()flazcr K (1999) l)epetMe/t O' Parsing, with a, 
E.rtended I:inite State Approac\]t. ACL'99, 
Maryland. 
Pr6sz6ky G., Kis B (1999)A Unificati(m-hascd 
Apl~roach to Moqdto-syntactic I'arsitl<~ of 
Agghttinative and Other (Highly) lnjlectional 
Languages. ACtd99, Ma,yhmd. 
Ritchie G., Pulhnan S. G., FJlack A. W., Russcl G. 
J. (1992) Comlmtational Moudu)logy: Practical 
Mechanism,s'.fi)r the l#lglish l,exico,. ACL-MIT 
Series on Natural Language Processing, MIT 
Press. 
Shicbcr S. M. (1986) At/ lntroductiotz to 
Unification-Based Approaches to Grammar. 
CSLI, Slanford. 
Sproat R. (1992) Morphology anU Computcaion. 
ACL-MIT Press series in Natural Language 
Processing. 
Trost It. (1990) The application of two-level 
morldzo/ogy to rzon-concatenative German 
moqgtology. COIANG'90, Hclsinki. 
7 
Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 59?64,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Strategies for sustainable MT for Basque:  
incremental design, reusability, standardization and open-source 
 I. Alegria, X. Arregi, X. Artola, A. Diaz de Ilarraza, G. Labaka,  
M. Lersundi, A. Mayor, K. Sarasola 
Ixa taldea.  
University of the Basque Country. 
i.alegria@ehu.es 
 
 
 
Abstract 
We present some Language Technology 
applications that have proven to be effec-
tive tools to promote the use of Basque, a 
European less privileged language. We also 
present the strategy we have followed for 
almost twenty years to develop those appli-
cations as the top of an integrated environ-
ment of language resources, language 
foundations, language tools and other ap-
plications. When we have faced a difficult 
task such as Machine Translation to 
Basque, our strategy has worked well. We 
have had good results in a short time just 
reusing previous works for Basque, reusing 
other open-source tools, and developing 
just a few new modules in collaboration 
with other groups. In addition, new reus-
able tools and formats have been produced.  
1 Introduction and Basque Language 
Basque is a highly inflected minority language 
with free order of sentence constituents. Machine 
Translation for Basque is thus both, a real need and 
a test bed for our strategy to develop NLP tools for 
Basque.          
Basque is an isolate language, and little is 
known of its origins. It is likely that an early form 
of the Basque language was already present in 
Western Europe before the arrival of the Indo-
European languages. 
Basque is an agglutinative language, with a rich 
flexional morphology. In fact for nouns, for 
example, at least 360 word forms are possible for 
each lemma. Each of the declension cases such as 
absolutive, dative, associative? has four different 
suffixes to be added to the last word of the noun 
phrase. These four suffix variants correspond to 
undetermined, determined singular, determined 
plural and ?close? determined plural.  
Basque is also an ergative-absolutive language. 
The subject of an intransitive verb is in the 
absolutive case (which is unmarked), and the same 
case is used for the direct object of a transitive 
verb. The subject of the transitive verb (that is, the 
agent) is marked differently, with the ergative case 
(shown by the suffix -k). This also triggers main 
and auxiliary verbal agreement. 
The auxiliary verb, which accompanies most 
main verbs, agrees not only with the subject, but 
with the direct object and the indirect object, if 
present. Among European languages, this 
polypersonal system (multiple verb agreement) is 
only found in Basque, some Caucasian languages, 
and Hungarian. The ergative-absolutive alignment 
is rare among European languages, but not 
worldwide. 
Although in last centuries Basque suffered 
continuous regression it still remains alive. The 
region in which Basque is spoken is smaller than 
what is known as the Basque Country, and the 
distribution of Basque speakers is not 
homogeneous there. The main reasons of this 
regression (Amorrortu, 2002) are that Basque was 
not an official language, and that it was out of 
educational system, out of media and out of 
industrial environments. Besides, the fact of being 
six different dialects made the wide development 
of written Basque difficult.  
However, after 1980, some of those features 
changed and many citizens and some local 
59
governments promote recovering of Basque 
Language.  
Today, Basque holds co-official language status 
in the Basque regions of Spain: the whole 
autonomous community of the Basque Country 
and some parts of Navarre. Basque has no official 
standing in the Northern Basque Country.   
In the past, Basque was associated with lack of 
education, stigmatized as uneducated, rural, or 
holding low economic and power resources. There 
is not such an association today; Basque speakers 
do not differ from Spanish or French monolinguals 
in any of these characteristics.  
Standard Basque, called Batua (unified) in 
Basque, was defined by the Academy of Basque 
Language (Euskaltzaindia) in 1968. At present, its 
morphology is completely standardized, but the 
lexical standardization process is still underway. 
Now this is the language model taught in most 
schools and used on some media and official 
papers published in Basque.  
Basque speakers are about 700,000, about 25% 
of the total population of the Basque Country, but 
they are not evenly distributed. Still the use of 
Basque in industry and specially in Information 
and Communication Technology is not 
widespread. A language that seeks to survive in the 
modern information society has to be present also 
in such field and this requires language technology 
products. Basque, as other minority languages, has 
to make a great effort to face this challenge (Petek, 
2000; Williams et al, 2001).  
2 Strategy to develop Human Language 
Technology (HLT) in Basque 
IXA group is a research Group created in 1986 by 
5 university lecturers in the computer science fac-
ulty of the University of the Basque Country with 
the aim of laying foundations for research and de-
velopment of NLP software mainly for Basque. 
We wanted to face the challenge of adapting 
Basque to language technology. 
Twenty one years later, now IXA is a group 
composed of 28 computer scientists, 13 linguists 
and 2 research assistants. It works in cooperation 
with more than 7 companies from Basque Country 
and 5 from abroad; it has been involved in the birth 
of two new spin-off companies; and it has devel-
oped more than seven language technology prod-
ucts. 
In recent years, several private companies and 
technology centers in the Basque Country have 
begun to get interested and to invest in this area. At 
the same time, more agents have come to be aware 
of the fact that collaboration is essential to the de-
velopment of language technologies for minority 
languages. One of the fruits of this collaboration 
are HIZKING21 (2002-2005) and ANHITZ (2006-
2008) projects. Both projects were accepted by the 
Government of the Basque Country in a new 
strategical research line called ?Language Infoen-
gineering?. 
At the very beginning, twenty years ago, our 
first goal was just to create a Spanish-Basque 
translation system, but after some preliminary 
work we realized that instead of wasting our time 
in creating an ad hoc MT system with small accu-
racy, we had to invest our effort in creating basic 
tools such as a morphological analyzer/generator 
for Basque, that could later be used to build not 
only a more robust MT system but also other ap-
plications. 
This thought was the seed to design our strategy 
to make progress in the adaptation of Basque to 
Language Technology. Basque language had to 
face up scarcity of resources and tools that could 
make possible its development in Language Tech-
nology at a reasonable and competitive rate. 
We presented an open proposal for making pro-
gress in Human Language Technology (Aduriz et 
al., 1998). Anyway, the steps proposed did not cor-
respond exactly with those observed in the history 
of the processing of English, because the high ca-
pacity and computational power of new computers 
allowed facing problems in a different way.  
Our strategy may be described in two points: 
1) The need for standardization of resources to 
be useful in different researches, tools and applica-
tions 
2) The need for incremental design and devel-
opment of language foundations, tools, and appli-
cations in a parallel and coordinated way in order 
to get the best benefit from them. Language foun-
dations and research are essential to create any tool 
or application; but in the same way tools and ap-
plications will be very helpful in the research and 
improvement of language foundations. 
Following this strategy, our steps on standardi-
zation of resources led us to adopt TEI and XML 
standards and also to define a methodology for 
60
stand-off corpus tagging based on TEI, feature 
structures and XML (Artola et al, 2005). 
In the same way, taking as reference our experi-
ence in incremental design and development we 
proposed four phases as a general strategy for lan-
guage processing. These are the phases defined 
with the products to be developed in each of them. 
1. Initial phase: Foundations. Corpus I (collection 
of raw text with no tagging mark). Lexical da-
tabase I (the first version could be a list of 
lemmas and affixes). Machine-readable dic-
tionaries. Morphological description.  
2. Second phase: Basic tools and applications. 
Statistical tools for the treatment of corpora. 
Morphological analyzer/generator. Lemma-
tizer/tagger. Spelling checker and corrector (al-
though in morphologically simple languages a 
word list could be enough). Speech processing 
at word level. Corpus II (word-forms are 
tagged with their part of speech and lemma). 
Lexical database II (lexical support for the con-
struction of general applications, including part 
of speech and morphological information). 
3. Third phase: Advanced tools and applications. 
An environment for tool integration. Web 
search engine.  A traditional search machine 
that integrates lemmatization and language 
identification. Surface syntax. Corpus III (syn-
tactically tagged text). Grammar and style 
checkers. Structured versions of dictionaries 
(they allow enhanced functionality not avail-
able for printed or raw electronic versions). 
Lexical database III (the previous version is en-
riched with multiword lexical units. Integration 
of dictionaries in text editors). Lexical-
semantic knowledge base. Creation of a con-
cept taxonomy (e.g.: Wordnet). Word-sense 
disambiguation. Speech processing at sentence 
level. Basic Computer Aided Language Learn-
ing (CALL) systems 
4. Fourth phase: Multilingualism and general 
applications. Information extraction. Transla-
tion aids (integrated use of multiple on-line 
dictionaries, translation of noun phrases and 
simple sentences). Corpus IV (semantically 
tagged text after word-sense disambiguation). 
Dialog systems. Knowledge base on multilin-
gual lexico-semantic relations and its applica-
tions.  
We will complete this strategy with some sug-
gestions about what shouldn?t be done when work-
ing on the treatment of minority languages. a) Do 
not start developing applications if linguistic foun-
dations are not defined previously; we recommend 
following the above given sequence: foundations, 
tools and applications. b) When a new system has 
to be planned, do not create ad hoc lexical or syn-
tactic resources; you should design those resources 
in a way that they could be easily extended to full 
coverage and reusable by any other tool or applica-
tion. c) If you complete a new resource or tool, do 
not keep it to yourself; there are many researchers 
working on English, but only a few on each minor-
ity language; thus, the few results should be public 
and shared for research purposes, for it is desirable 
to avoid needless and costly repetition of work. 
3 Machine Translation for Basque 
After years working on basic resources and tools 
we decided it was time to face  the MT task (Hut-
chins and Somers, 1992). Our general strategy was 
more specifically for Machine Translation defined 
bearing in mind the following concepts:  
? reusability of previous resources, specially 
lexical resources and morphology of Basque 
? standardization and collaboration: using a 
more general framework in collaboration 
with other groups working in NLP 
? open-source: this means that anyone having 
the necessary computational and linguistic 
skills will be able to adapt or enhance it to 
produce a new MT system,  
Due to the real necessity for translation in our 
environment the involved languages would be 
Basque, Spanish and English. 
From the beginning we wanted to combine the 
two basic approaches for MT (rule-based and cor-
pus-based) in order to build a hybrid system, be-
cause it is generally agreed that there are not 
enough corpora for a good corpus-based system in 
minority languages like Basque.  
Data-driven Machine Translation (example-
based or statistical) is nowadays the most prevalent 
trend in Machine Translation research. Translation 
results obtained with this approach have already 
reached a high level of accuracy, especially when 
the target language is English. But these Data-
driven MT systems base their knowledge on 
aligned bilingual corpora, and the accuracy of their 
61
output depends heavily on the quality and the size 
of these corpora. Large and reliable bilingual cor-
pora are unavailable for many language pairs. 
3.1 The rule-based approach 
First, we present the main architecture and the pro-
posed standards of an open source MT engine, the 
first implementation of which translates from 
Spanish into Basque using the traditional transfer 
model and based on shallow and dependency pars-
ing. 
The design and the programs are independent 
from the languages, so the software can be used for 
other projects in MT. Depending on the languages 
included in the adaptation, it will be necessary to 
add, reorder and change some modules, but this 
will not be difficult because a unique XML format 
is used for the communication among all the mod-
ules. 
The project has been integrated in the OpenTrad 
initiative (www.opentrad.com), a government-
funded project shared among different universities 
and small companies, which also include MT en-
gines for translation among the main languages in 
Spain. The main objective of this initiative is the 
construction of an open, reusable and interoperable 
framework. 
In the OpenTrad project, two different but coor-
dinated designs have been carried out: 
? A shallow-transfer machine translation en-
gine for similar languages (Spanish, Catalan 
and Galician by the the time being). The 
MT architecture uses finite-state transducers 
for lexical processing, hidden Markov mod-
els for part-of-speech tagging, and chunking 
based on finite-state for structural transfer. 
It is named Apertium and it can be 
downloaded from apertium.sourceforge.net. 
(Armentano-Oller et al, 2004) 
? A deeper-transfer engine for the Spanish-
Basque pair. It is named Matxin (Alegria et 
al., 2007) and it is stored in 
matxin.sourceforge.net. It is an extension of 
previous work in our group. In order to re-
use resources in this Spanish-Basque system 
the analysis module for similar languages 
was not included in Matxin; another open 
source engine, FreeLing (Carreras et al, 
2004), was used here, of course, and its out-
put had to be converted to the proposed in-
terchange format. 
Some of the components (modules, data formats 
and compilers) from the first architecture in Open-
Trad were used in the second one. Indeed, an im-
portant additional goal of this work was testing 
which modules from the first architecture could be 
integrated in deeper-transfer architectures for more 
difficult language pairs. 
The transfer module is also based on three main 
objects in the translation process: words or nodes, 
chunks or phrases, and sentences.  
? First, lexical transfer is carried out using a 
bilingual dictionary compiled into a finite-
state transducer. We use the XML specifica-
tion of Apertium engine.  
? Then, structural transfer at the sentence 
level is applied, and some information is 
transferred from some chunks to others, and 
some chunks may disappear. Grammars 
based on regular expressions are used to 
specify these changes. For example, in the 
Spanish-Basque transfer, the person and 
number information of the object and the 
type of subordination are imported from 
other chunks to the chunk corresponding to 
the verb chain. 
? Finally the structural transfer at the chunk 
level is carried out. This process can be 
quite simple (e.g. noun chains between 
Spanish and Basque) or more complex (e.g. 
verb chains between these same languages). 
The XML file coming from the transfer module 
is passed on the generation module. 
? In the first step, syntactic generation is per-
formed in order to decide the order of 
chunks in the sentence and the order of 
words in the chunks. Several grammars are 
used for this purpose.  
? Morphological generation is carried out in 
the last step. In the generation of Basque, 
the main inflection is added to the last word 
in the phrase (in Basque: the declension 
case, the article and other features are added 
to the whole noun phrase at the end of the 
last word), but in verb chains other words 
need morphological generation. A previous 
morphological analyzer/generator for 
Basque (Alegria et al, 1996) has been 
adapted and transformed to the format used 
in Apertium. 
The results for the Spanish/Basque system using 
FreeLing and Matxin are promising. The quantita-
62
tive evaluation uses the open source evaluation 
tool IQMT and figures are given using Bleu and 
NIST measures (Gim?nez et al, 2005). An user 
based evaluation has been carried out too. 
3.2 The corpus-based approach 
The corpus-based approach has been carried out in 
collaboration with the National Center for Lan-
guage Technology in Dublin.  
The system exploits both EBMT and SMT tech-
niques to extract a dataset of aligned chunks. We 
conducted Basque to English and Spanish to 
Basque translation experiments, evaluated on a 
large corpus (270, 000 sentence pairs).  
Some tools have been reused for this purpose: 
? GIZA++: for word/morpheme alignment we 
used the GIZA++ statistical word alignment 
toolkit, and following the ?refined? method 
of (Och and Ney, 2003), extracted a set of 
high-quality word/ morpheme alignments 
from the original unidirectional alignment 
sets. These along with the extracted chunk 
alignments were passed to the translation 
decoder.                                         
? Pharaoh/Moses decoder: the decoder is also 
a hybrid system which integrates EBMT 
and SMT. It is capable of retrieving already 
translated sentences and also provides a 
wrapper around the PHARAOH SMT de-
coder (Koehn, 2004). 
? MaTrEx: the MATREX (Machine Transla-
tion using Examples) system used in our 
experiments is a data-driven MT engine, 
built following an extremely modular de-
sign. It consists of a number of extensible 
and re-implementable modules (Way and 
Gough, 2005). 
   For this engine, we reuse a toolkit to chunk the 
Basque sentences. After this processing stage, a 
sentence is treated as a sequence of morphemes, in 
which chunk boundaries are clearly visible. Mor-
phemes denoting morphosyntactic features are re-
placed by conventional symbolic strings. After 
some adaptation, the chunks obtained in this man-
ner are actually very comparable to the English 
chunks obtained with the marker-based chunker. 
The experimental results have shown that our 
system significantly outperforms state-of-the-art 
approaches according to several common auto-
matic evaluation metrics: WER, Bleu and PER 
(Stroppa et al, 2006; Labaka et al, 2007). 
4 Conclusions 
A language that seeks to survive in the modern 
information society requires language technology 
products. "Minority" languages have to do a great 
effort to face this challenge. The Ixa group has 
been working since 1986 on adapting Basque to 
language technology, having developed several 
applications that are effective tools to promote the 
use of Basque. Now we are planning to define the 
BLARK for Basque (Krauwer, 2003).  
From our experience, we defend that research 
and development for a minority language should to 
be faced following these points: high standardiza-
tion,  reusing language foundations, tools, and ap-
plications, and their incremental design and devel-
opment. We know that any HLT project related to 
a less privileged language should follow those 
guidelines, but from our experience we know that 
in most cases they do not. We think that if Basque 
is now in an good position in HLT is because those 
guidelines have been applied even  when it was 
easier to define "toy" resources and tools useful to 
get good short term academic results, but not reus-
able in future developments.  
This strategy has been completely useful when 
we have created MT systems for Basque. Reusing 
previous works for Basque (that were defined fol-
lowing XML and TEI standards) and reusing other 
open-source tools have been the key to get satisfac-
tory results in a short time.  
Two results produced in the MT track are pub-
licly available:  
? matxin.sourceforge.net for the free code for 
the Spanish-Basque RBMT system 
? www.opentrad.org for the on-line demo  
Acknowledgments 
This work has been partially funded by the Spanish 
Ministry of Education and Science (OpenMT: 
Open Source Machine Translation using hybrid 
methods,TIN2006-15307-C03-01) and the Local 
Government of the Basque Country (AnHITZ 
2006: Language Technologies for Multingual In-
teraction in Intelligent Environments., IE06-185). 
Andy Way, Declan Groves and Nicolas Stroppa 
from National Centre for Language Technology in 
Dublin are kindly acknowledged for providing 
their expertise on the Matrex system and the 
evaluation of the output. 
63
References 
I. Aduriz, E. Agirre, I. Aldezabal, I. Alegria, O. Ansa, 
X. Arregi, J. Arriola, X. Artola, A. D?az de Ilarraza, 
N. Ezeiza, K.Gojenola, M. Maritxalar, M. Oronoz, K. 
Sarasola, A. Soroa, R. Urizar. 1998. A framework for 
the automatic processing of Basque. Proceedings of 
Workshop on Lexical Resources for Minority Lan-
guages.  
I. Alegria, X. Artola, K. Sarasola. 1996.Automatic mor-
phological analysis of Basque. Literary & Linguistic 
Computing Vol. 11, No. 4, 193-203. Oxford Univer-
sity Press. Oxford. 1996. 
I. Alegria, A. D?az de Ilarraza, G. Labaka, M Lersundi, 
A. Mayor, K. Sarasola.  2007. Transfer-based MT 
from Spanish into Basque: reusability, standardiza-
tion and open source. LNCS 4394. 374-384. Cicling 
2007.  
E. Amorrortu. 2002. Bilingual Education in the Basque 
Country: Achievements and Challenges after Four 
Decades of Acquisition Planning. Journal of Iberian 
and Latin American Literary and Cultural Stud-
ies.Volume 2 Number 2 (2002) 
C. Armentano-Oller, A. Corb?-Bellot, M. L. Forcada, 
M. Ginest?-Rosell, B. Bonev, S. Ortiz-Rojas, J. A. 
P?rez-Ortiz, G. Ram?rez-S?nchez, F. S?nchez-
Mart?nez, 2005. An open-source shallow-transfer 
machine translation toolbox: consequences of its re-
lease and availability. Proceedings of OSMaTran: 
Open-Source Machine Translation workshop, MT 
Summit X. 
X. Artola, A. D?az de Ilarraza, N. Ezeiza, K. Gojenola, 
G. Labaka, A. Sologaistoa, A. Soroa.  2005. A 
framework for representing and managing linguistic 
annotations based on typed feature structures. Proc. 
of RANLP 2005. 
X. Carreras,, I. Chao, L. Padr? and M. Padr?. 2004. 
FreeLing: An open source Suite of Language Ana-
lyzers, in  Proceedings of the 4th International Con-
ference on Language Resources and Evaluation 
(LREC'04).  
J. Gim?nez, E. Amig?, C. Hori. 2005. Machine 
Translation Evaluation Inside QARLA. In Proceed-
ings of the International Workshop on Spoken Lan-
guage Technology (IWSLT'05) 
W. Hutchins and H. Somers. 1992. An Introduction to 
Machine Translation. Academic Press. 
P. Koehn. 2004. Pharaoh: A Beam Search Decoder for 
Phrase-Based Statistical Machine Translation Mod-
els.  In Proceedings of AMTA-04, pages 115?124, 
Washington, District of Columbia. 
S. Krauwer. 2003. The Basic Language Resource Kit 
(BLARK) as the First Milestone for the Language 
Resources Roadmap. Proc. of the International 
Workshop  Speech and Computer. Moscow, Russia. 
G. Labaka, N. Stroppa, A. Way, K. Sarasola  2007 
Comparing Rule-Based and Data-Driven Approaches 
to Spanish-to-Basque Machine Translation Proc. of 
MT-Summit XI, Copenhagen 
F. Och and H. Ney. 2003. A Systematic Comparison of 
Various Statistical Alignment Models. Computa-
tional Linguistics, 29(1): 19?51. 
B. Petek. 2000. Funding for research into human lan-
guage technologies for less prevalent languages, Sec-
ond International Conference on Language Re-
sources and Evaluation (LREC 2000). Athens, 
Greece. 
N. Stroppa, D. Groves, A. Way, K. Sarasola K. 2006. 
Example-Based Machine Translation of the Basque 
Language. AMTA. 7th conference of the Association 
for Machine Translation in the Americas.. 
A. Way and N. Gough. 2005. Comparing Example-
Based and Statistical Machine Translation. Natural 
Language Engineering, 11(3):295?309. 
B. Williams, K. Sarasola, D. ??Cr?inin, B. Petek. 2001. 
Speech and Language Technology for Minority Lan-
guages. Proceedings of Eurospeech 2001 
 
 
64
Coling 2010: Poster Volume, pages 9?17,
Beijing, August 2010
Document Expansion Based on WordNet
for Robust IR
Eneko Agirre
IXA NLP Group
Univ. of the Basque Country
e.agirre@ehu.es
Xabier Arregi
IXA NLP Group
Univ. of the Basque Country
xabier.arregi@ehu.es
Arantxa Otegi
IXA NLP Group
Univ. of the Basque Country
arantza.otegi@ehu.es
Abstract
The use of semantic information to im-
prove IR is a long-standing goal. This pa-
per presents a novel Document Expansion
method based on a WordNet-based system
to find related concepts and words. Ex-
pansion words are indexed separately, and
when combined with the regular index,
they improve the results in three datasets
over a state-of-the-art IR engine. Consid-
ering that many IR systems are not robust
in the sense that they need careful fine-
tuning and optimization of their parame-
ters, we explored some parameter settings.
The results show that our method is spe-
cially effective for realistic, non-optimal
settings, adding robustness to the IR en-
gine. We also explored the effect of doc-
ument length, and show that our method
is specially successful with shorter docu-
ments.
1 Introduction
Since the earliest days of IR, researchers noted
the potential pitfalls of keyword retrieval, such
as synonymy, polysemy, hyponymy or anaphora.
Although in principle these linguistic phenom-
ena should be taken into account in order to ob-
tain high retrieval relevance, the lack of algo-
rithmic models prohibited any systematic study
of the effect of this phenomena in retrieval. In-
stead, researchers resorted to distributional se-
mantic models to try to improve retrieval rele-
vance, and overcome the brittleness of keyword
matches. Most research concentrated on Query
Expansion (QE) methods, which typically ana-
lyze term co-occurrence statistics in the corpus
and in the highest scored documents for the orig-
inal query in order to select terms for expanding
the query terms (Manning et al, 2009). Docu-
ment expansion (DE) is a natural alternative to
QE, but surprisingly it was not investigated un-
til very recently. Several researchers have used
distributional methods from similar documents in
the collection in order to expand the documents
with related terms that do not actually occur in the
document (Liu and Croft, 2004; Kurland and Lee,
2004; Tao et al, 2006; Mei et al, 2008; Huang
et al, 2009). The work presented here is com-
plementary, in that we also explore DE, but use
WordNet instead of distributional methods.
Lexical semantic resources such as WordNet
(Fellbaum, 1998) might provide a principled and
explicit remedy for the brittleness of keyword
matches. WordNet has been used with success
in psycholinguistic datasets of word similarity and
relatedness, where it often surpasses distributional
methods based on keyword matches (Agirre et al,
2009b). WordNet has been applied to IR before.
Some authors extended the query with related
terms (Voorhees, 1994; Liu et al, 2005), while
others have explicitly represented and indexed
word senses after performing word sense disam-
biguation (WSD) (Gonzalo et al, 1998; Stokoe
et al, 2003; Kim et al, 2004). More recently,
a CLEF task was organized (Agirre et al, 2008;
Agirre et al, 2009a) where queries and docu-
ments were semantically disambiguated, and par-
ticipants reported mixed results.
This paper proposes to use WordNet for docu-
ment expansion, proposing a new method: given
9
a full document, a random walk algorithm over
the WordNet graph ranks concepts closely related
to the words in the document. This is in con-
trast to previous WordNet-based work which fo-
cused on WSD to replace or supplement words
with their senses. Our method discovers impor-
tant concepts, even if they are not explicitly men-
tioned in the document. For instance, given a doc-
ument mentioning virus, software and DSL, our
method suggests related concepts and associated
words such us digital subscriber line, phone com-
pany and computer. Those expansion words are
indexed separately, and when combined with the
regular index, we show that they improve the re-
sults in three datasets over a state-of-the-art IR en-
gine (Boldi and Vigna, 2005). The three datasets
used in this study are ResPubliQA (Pen?as et al,
2009), Yahoo! Answers (Surdeanu et al, 2008)
and CLEF-Robust (Agirre et al, 2009a).
Considering that many IR systems are not ro-
bust in the sense that they need careful fine-tuning
and optimization of their parameters, we decided
to study the robustness of our method, explor-
ing some alternative settings, including default pa-
rameters, parameters optimized in development
data, and parameters optimized in other datasets.
The study reveals that the additional semantic ex-
pansion terms provide robustness in most cases.
We also hypothesized that semantic document
expansion could be most profitable when docu-
ments are shorter, and our algorithm would be
most effective for collections of short documents.
We artificially trimmed documents in the Robust
dataset. The results, together with the analysis of
document lengths of the three datasets, show that
document expansion is specially effective for very
short documents, but other factors could also play
a role.
The paper is structured as follows. We first in-
troduce the document expansion technique. Sec-
tion 3 introduces the method to include the expan-
sions in a retrieval system. Section 4 presents the
experimental setup. Section 5 shows our main re-
sults. Sections 6 and 7 analyze the robustness and
relation to document length. Section 8 compares
to related work. Finally, the conclusions and fu-
ture work are mentioned.
2 Document Expansion Using WordNet
Our key insight is to expand the document with
related words according to the background infor-
mation in WordNet (Fellbaum, 1998), which pro-
vides generic information about general vocabu-
lary terms. WordNet groups nouns, verbs, adjec-
tives and adverbs into sets of synonyms (synsets),
each expressing a distinct concept. Synsets are in-
terlinked with conceptual-semantic and lexical re-
lations, including hypernymy, meronymy, causal-
ity, etc.
In contrast with previous work, we select those
concepts that are most closely related to the doc-
ument as a whole. For that, we use a technique
based on random walks over the graph represen-
tation of WordNet concepts and relations.
We represent WordNet as a graph as fol-
lows: graph nodes represent WordNet concepts
(synsets) and dictionary words; relations among
synsets are represented by undirected edges; and
dictionary words are linked to the synsets asso-
ciated to them by directed edges. We used ver-
sion 3.0, with all relations provided, including the
gloss relations. This was the setting obtaining the
best results in a word similarity dataset as reported
by Agirre et al (2009b).
Given a document and the graph-based repre-
sentation of WordNet, we obtain a ranked list of
WordNet concepts as follows:
1. We first pre-process the document to obtain
the lemmas and parts of speech of the open
category words.
2. We then assign a uniform probability distri-
bution to the terms found in the document.
The rest of nodes are initialized to zero.
3. We compute personalized PageR-
ank (Haveliwala, 2002) over the graph,
using the previous distribution as the reset
distribution, and producing a probability
distribution over WordNet concepts The
higher the probability for a concept, the
more related it is to the given document.
Basically, personalized PageRank is computed
by modifying the random jump distribution vec-
tor in the traditional PageRank equation. In our
case, we concentrate all probability mass in the
concepts corresponding to the words in the docu-
10
ment.
Let G be a graph with N vertices v1, . . . , vN
and di be the outdegree of node i; let M be a N ?
N transition probability matrix, where Mji = 1diif a link from i to j exists, and zero otherwise.
Then, the calculation of the PageRank vector Pr
over G is equivalent to resolving Equation (1).
Pr = cMPr + (1? c)v (1)
In the equation, v is a N ? 1 vector and c is the
so called damping factor, a scalar value between
0 and 1. The first term of the sum on the equa-
tion models the voting scheme described in the
beginning of the section. The second term repre-
sents, loosely speaking, the probability of a surfer
randomly jumping to any node, e.g. without fol-
lowing any paths on the graph. The damping fac-
tor, usually set in the [0.85..0.95] range, models
the way in which these two terms are combined at
each step.
The second term on Eq. (1) can also be seen as a
smoothing factor that makes any graph fulfill the
property of being aperiodic and irreducible, and
thus guarantees that PageRank calculation con-
verges to a unique stationary distribution.
In the traditional PageRank formulation the
vector v is a stochastic normalized vector whose
element values are all 1N , thus assigning equalprobabilities to all nodes in the graph in case of
random jumps. In the case of personalized PageR-
ank as used here, v is initialized with uniform
probabilities for the terms in the document, and
0 for the rest of terms.
PageRank is actually calculated by applying an
iterative algorithm which computes Eq. (1) suc-
cessively until a fixed number of iterations are
executed. In our case, we used a publicly avail-
able implementation1, with default values for the
damping value (0.85) and the number of iterations
(30). In order to select the expansion terms, we
chose the 100 highest scoring concepts, and get
all the words that lexicalize the given concept.
Figure 1 exemplifies the expansion. Given the
short document from Yahoo! Answers (cf. Sec-
tion 4) shown in the top, our algorithm produces
the set of related concepts and words shown in the
1http://ixa2.si.ehu.es/ukb/
bottom. Note that the expansion produces syn-
onyms, but also other words related to concepts
that are not mentioned in the document.
3 Including Expansions in a Retrieval
System
Once we have the list of words for document ex-
pansion, we create one index for the words in the
original documents and another index with the ex-
pansion terms. This way, we are able to use the
original words only, or to also include the expan-
sion words during the retrieval.
The retrieval system was implemented using
MG4J (Boldi and Vigna, 2005), as it provides
state-of-the-art results and allows to combine sev-
eral indices over the same document collection.
We conducted different runs, by using only the in-
dex made of original words (baseline) and also by
using the index with the expansion terms of the
related concepts.
BM25 was the scoring function of choice. It is
one of the most relevant and robust scoring func-
tions available (Robertson and Zaragoza, 2009).
wBM25Dt := (2)
tfDt
k1
(
(1? b) + b dlDavdlD
)
+ tfDt
idft
where tfDt is the term frequency of term t in doc-
ument D, dlD is the document length, idft is the
inverted document frequency (or more specifically
the RSJ weight, (Robertson and Zaragoza, 2009)),
and k1 and b are free parameters.
The two indices were combined linearly, as fol-
lows (Robertson and Zaragoza, 2009):
score(d, e, q) := (3)
?
t?q?d
wBM25Dt + ?
?
t?q?e
wBM25Et
where D and E are the original and expanded in-
dices, d, e and q are the original document, the
expansion of the document and the query respec-
tively, t is a term, and ? is a free parameter for the
relative weight of the expanded index.
11
You should only need to turn off virus and anti-spy not uninstall. And that?s
done within each of the softwares themselves. Then turn them back on later after
installing any DSL softwares.
06566077-n? computer software, package, software, software package, software program, software system
03196990-n? digital subscriber line, dsl
01569566-v? instal, install, put in, set up
04402057-n? line, phone line, suscriber line, telephone circuit, telephone line
08186221-n? phone company, phone service, telco, telephone company, telephone service
03082979-n? computer, computing device, computing machine, data processor, electronic computer
Figure 1: Example of a document expansion, with original document on top, and some of the relevant
WordNet concepts identified by our algorithm, together with the words that lexicalize them. Words in
the original document are shown in bold, synonyms in italics, and other related words underlined.
4 Experimental Setup
We chose three data collections. The first is based
on a traditional news collection. DE could be
specially interesting for datasets with short docu-
ments, which lead our choice of the other datasets:
the second was chosen because it contains shorter
documents, and the third is a passage retrieval task
which works on even shorter paragraphs. Table 1
shows some statistics about the datasets.
One of the collections is the English dataset
of the Robust task at CLEF 2009 (Agirre et al,
2009a). The documents are news collections from
LA Times 94 and Glasgow Herald 95. The top-
ics are statements representing information needs,
consisting of three parts: a brief title statement; a
one-sentence description; a more complex narra-
tive describing the relevance assessment criteria.
We use only the title and the description parts of
the topics in our experiments.
The Yahoo! Answers corpus is a subset of a
dump of the Yahoo! Answers web site2 (Surdeanu
et al, 2008), where people post questions and
answers, all of which are public to any web user
willing to browse them. The dataset is a small
subset of the questions, selected for their linguis-
tic properties (for example they all start with ?how
{to?do?did?does?can?would?could?should}?).
Additionally, questions and answers of obvious
low quality were removed. The document set was
created with the best answer of each question
(only one for each question).
2Yahoo! Webscope dataset ?ydata-yanswers-manner-
questions-v1 0? http://webscope.sandbox.yahoo.com/
docs length q. train q. test
Robust 166,754 532 150 160
Yahoo! 89610 104 1000 88610
ResPubliQA 1,379,011 20 100 500
Table 1: Number of documents, average docu-
ment length, number of queries for train and test
in each collection.
The other collection is the English dataset of
ResPubliQA exercise at the Multilingual Ques-
tion Answering Track at CLEF 2009 (Pen?as et al,
2009). The exercise is aimed at retrieving para-
graphs that contain answers to a set of 500 natu-
ral language questions. The document collection
is a subset of the JRC-Acquis Multilingual Paral-
lel Corpus, and consists of 21,426 documents for
English which are aligned to a similar number of
documents in other languages3. For evaluation,
we used the gold standard released by the orga-
nizers, which contains a single correct passage for
each query. As the retrieval unit is the passage,
we split the document collection into paragraphs.
We applied the expansion strategy only to pas-
sages which had more than 10 words (half of the
passages), for two reasons: the first one was that
most of these passages were found not to contain
relevant information for the task (e.g. ?Article 2?
or ?Having regard to the proposal from the Com-
mission?), and the second was that we thus saved
some computation time.
In order to evaluate the quality of our expansion
in practical retrieval settings, the next Section re-
3Note that Table 1 shows the number of paragraphs,
which conform the units we indexed.
12
base. expa. ?
Robust MAP .3781 .3835*** 1.43%
Yahoo! MRR .2900 .2950*** 1.72%P@1 .2142 .2183*** 1.91%
ResPubliQA MRR .3931 .4077*** 3.72%P@1 .2860 .3000** 4.90%
Table 2: Results using default parameters.
port results with respect to several parameter set-
tings. Parameter optimization is often neglected
in retrieval with linguistic features, but we think it
is crucial since it can have a large effect on rele-
vance performance and therefore invalidate claims
of improvements over the baseline. In each setting
we assign different values to the free parameters in
the previous section, k1, b and ?.
5 Results
The main evaluation measure for Robust is mean
Average Precision (MAP), as customary. In two of
the datasets (Yahoo! and ResPubliQA) there is a
single correct answer per query, and therefore we
use Mean Reciprocal Rank (MRR) and Mean Pre-
cision at rank 1 (P@1) for evaluation. Note that in
this setting MAP is identical to MRR. Statistical
significance was computed using Paired Random-
ization Test (Smucker et al, 2007). In the tables
throughout the paper, we use * to indicate statis-
tical significance at 90% confidence level, ** for
95% and *** for 99%. Unless noted otherwise,
base. refers to MG4J with the standard index, and
expa. refers to MG4J using both indices. Best
results per row are in bold when significant. ? re-
ports relative improvement respect to the baseline.
5.1 Default Parameter Setting
The values for k1 and b are the default values as
provided in the wBM25 implementation of MG4J,
1.2 and 0.5 respectively. We could not think of a
straightforward value for ?. A value of 1 would
mean that we are assigning equal importance to
original and expanded terms, which seemed an
overestimation, so we used 0.1. Table 2 shows
the results when using the default setting of pa-
rameters. The use of expansion is beneficial in all
datasets, with relative improvements ranging from
1.43% to 4.90%.
base. expa. ?
Robust MAP .3740 .3823** 2.20%
Yahoo! MRR .3070 .3100*** 0.98%P@1 .2293 .2317* 1.05%
ResPubliQA MRR .4970 .4942 -0.56%P@1 .3980 .3940 -1.01%
Table 3: Results using optimized parameters.
Setting System k1 b ?
Default base. 1.20 0.50 -expa. 1.20 0.50 0.100
Robust base. 1.80 0.64 -expa. 1.66 0.55 0.075
Yahoo! basel. 0.99 0.82 -expa. 0.84 0.87 0.146
ResPubliQA base. 0.09 0.56 -expa. 0.13 0.65 0.090
Table 4: Parameters as in the default setting or as
optimized in each dataset. The ? parameter is not
used in the baseline systems.
5.2 Optimized Parameter Setting
We next optimized all three parameters using the
train part of each collection. The optimization of
the parameters followed a greedy method called
?promising directions? (Robertson and Zaragoza,
2009). The comparison between the baseline and
expansion systems in Table 3 shows that expan-
sion helps in Yahoo! and Robust, with statistical
significance. The differences in ResPubliQA are
not significant, and indicate that expansion terms
were not helpful in this setting.
Note that the optimization of the parameters
yields interesting effects in the baseline for each
of the datasets. If we compare the results of the
baseline with default settings (Table 2) and with
optimized setting (Table 3), the baseline improves
MRR dramatically in ResPubliQA (26% relative
improvement), significantly in Yahoo! (5.8%) and
decreases MAP in Robust (-0.01%). This dis-
parity of effects could be explained by the fact
that the default values are often approximated us-
ing TREC-style news collections, which is exactly
the genre of the Robust documents, while Yahoo
uses shorter documents, and ResPubliQA has the
shortest documents.
Table 4 summarizes the values of the parame-
ters in both default and optimized settings. For k1,
the optimization yields very different values. In
Robust the value is similar to the default value, but
13
base. expa. ? ?
Rob MAP .3781 .3881*** 2.64% 0.18
Y! MRR .2900 .2980*** 2.76% 0.27P@1 .2142 .2212*** 3.27%
ResP. MRR .3931 .4221*** 7.39% 0.61P@1 .2860 .3180** 11.19%
Table 5: Results obtained using the ? optimized
setting, including actual values of ?.
in ResPubliQA the optimization pushes it down
below the typical values cited in the literature
(Robertson and Zaragoza, 2009), which might ex-
plain the boost in performance for the baseline in
the case of ResPubliQA. When all three param-
eters are optimized together, the values ? in the
table range from 0.075 to 0.146. The values of the
optimized ? can be seem as an indication of the
usefulness of the expanded terms, so we explored
this farther.
5.3 Exploring ?
As an additional analysis experiment, we wanted
to know the effect of varying ? keeping k1 and b
constant at their default values. Table 5 shows the
best values in each dataset, which that the weight
of the expanded terms and the relative improve-
ment are highly correlated.
5.4 Exploring Number of Expansion
Concepts
One of the free parameters of our system is the
number of concepts to be included in the docu-
ment expansion. We have performed a limited
study with the default parameter setting on the
Robust setting, using 100, 500 and 750 concepts,
but the variations were not statistically significant.
Note that with 100 concepts we were actually ex-
panding with 268 words, with 500 concepts we
add 1247 words and with 750 concepts we add
1831 words.
6 Robustness
The results in the previous section indicate that
optimization is very important, but unfortunately
real applications usually lack training data. In this
Section we wanted to study whether the param-
eters can be carried over from one dataset to the
other, and if not, whether the extra terms found by
train base. expa. ?
Rob.
def. MAP .3781 .3835*** 1.43%
Rob. MAP .3740 .3823** 2.20%
Y! MAP .3786 .3759 -0.72%
Res. MAP .3146 .3346*** 6.35%
Y!
def. MRR .2900 .2950*** 1.72%
Rob. MRR .2920 .2920 0.0%
Y! MRR .3070 .3100** 0.98%
Res. MRR .2600 .2750*** 5.77%
ResP.
def. MRR .3931 .4077*** 3.72%
Rob. MRR .3066 .3655*** 19.22%
Y! MRR .3010 .3459*** 14.93%
Res. MRR .4970 .4942 -0.56%
Table 6: Results optimizing parameters with train-
ing from other datasets. We also include default
and optimization on the same dataset for compar-
ison. Only MRR and MAP results are given.
DE would make the system more robust to those
sub-optimal parameters.
Table 6 includes a range of parameter set-
tings, including defaults, and optimized parame-
ters coming from the same and different datasets.
The values of the parameters are those in Table
4. The results show that when the parameters are
optimized in other datasets, DE provides improve-
ment with statistical significance in all cases, ex-
cept for the Robust dataset when using parameters
optimized from Yahoo! and vice-versa.
Overall, the table shows that our DE method ei-
ther improves the results significantly or does not
affect performance, and that it provides robustness
across different parameter settings, even with sub-
optimal values.
7 Exploring Document Length
The results in Table 6 show that the perfor-
mance improvements are best in the collection
with shortest documents (ResPubliQA). But the
results for Robust and Yahoo! do not show any re-
lation to document length. We thus decided to do
an additional experiment artificially shrinking the
document in Robust to a certain percentage of its
original length. We create new pseudo-collection
with the shrinkage factors of 2.5%, 10%, 20% and
50%, keeping the first N% words in the document
and discarding the rest. In all cases we used the
same parameters, as optimized for Robust.
Table 7 shows the results (MAP), with some
clear indication that the best improvements are ob-
14
tained for the shortest documents.
length base. expa. ?
2.5% 13 .0794 .0851 7.18%
10% 53 .1757 .1833 4.33%
20% 107 .2292 .2329 1.61%
50% 266 .3063 .3098 1.14%
100% 531 .3740 .3823 2.22%
Table 7: Results (MAP) on Robust when arti-
ficially shrinking documents to a percentage of
their length. In addition to the shrinking rate we
show the average lengths of documents.
8 Related Work
Given the brittleness of keyword matches, most
research has concentrated on Query Expansion
(QE) methods. These methods analyze the user
query terms and select automatically new related
query terms. Most QE methods use statistical
(or distributional) techniques to select terms for
expansion. They do this by analyzing term co-
occurrence statistics in the corpus and in the high-
est scored documents of the original query (Man-
ning et al, 2009). These methods seemed to im-
prove slightly retrieval relevance on average, but
at the cost of greatly decreasing the relevance of
difficult queries. But more recent studies seem
to overcome some of these problems (Collins-
Thompson, 2009).
An alternative to QE is to perform the expan-
sion in the document. Document Expansion (DE)
was first proposed in the speech retrieval commu-
nity (Singhal and Pereira, 1999), where the task
is to retrieve speech transcriptions which are quite
noisy. Singhal and Pereira propose to enhance the
representation of a noisy document by adding to
the document vector a linearly weighted mixture
of related documents. In order to determine re-
lated documents, the original document is used as
a query into the collection, and the ten most rele-
vant documents are selected.
Two related papers (Liu and Croft, 2004; Kur-
land and Lee, 2004) followed a similar approach
on the TREC ad-hoc document retrieval task.
They use document clustering to determine simi-
lar documents, and document expansion is carried
out with respect to these. Both papers report sig-
nificant improvements over non-expanded base-
lines. Instead of clustering, more recent work (Tao
et al, 2006; Mei et al, 2008; Huang et al, 2009)
use language models and graph representations of
the similarity between documents in the collec-
tion to smooth language models with some suc-
cess. The work presented here is complementary,
in that we also explore DE, but use WordNet in-
stead of distributional methods. They use a tighter
integration of their expansion model (compared to
our simple two-index model), which coupled with
our expansion method could help improve results
further. We plan to explore this in the future.
An alternative to statistical expansion methods
is to use lexical semantic knowledge bases such as
WordNet. Most of the work has focused on query
expansion and the use of synonyms from Word-
Net after performing word sense disambiguation
(WSD) with some success (Voorhees, 1994; Liu
et al, 2005). The short context available in
the query when performing WSD is an impor-
tant problems of these techniques. In contrast,
we use full document context, and related words
beyond synonyms. Another strand of WordNet
based work has explicitly represented and indexed
word senses after performing WSD (Gonzalo et
al., 1998; Stokoe et al, 2003; Kim et al, 2004).
The word senses conform a different space for
document representation, but contrary to us, these
works incorporate concepts for all words in the
documents, and are not able to incorporate con-
cepts that are not explicitly mentioned in the doc-
ument. More recently, a CLEF task was orga-
nized (Agirre et al, 2009a) where terms were se-
mantically disambiguated to see the improvement
that this would have on retrieval; the conclusions
were mixed, with some participants slightly im-
proving results with information from WordNet.
To the best of our knowledge our paper is the first
on the topic of document expansion using lexical-
semantic resources.
We would like to also compare our performance
to those of other systems as tested on the same
datasets. The systems which performed best in
the Robust evaluation campaign (Agirre et al,
2009a) report 0.4509 MAP, but note that they de-
ployed a complex system combining probabilis-
tic and monolingual translation-based models. In
ResPubliQA (Pen?as et al, 2009), the official eval-
15
uation included manual assessment, and we can-
not therefore reproduce those results. Fortunately,
the organizers released all runs, but only the first
ranked document for each query was included, so
we could only compute P@1. The P@1 of best
run was 0.40. Finally (Surdeanu et al, 2008) re-
port MRR figure around 0.68, but they evaluate
only in the questions where the correct answer
is retrieved by answer retrieval in the top 50 an-
swers, and is thus not comparable to our setting.
Regarding the WordNet expansion technique
we use here, it is implemented on top of publicly
available software4, which has been successfully
used in word similarity (Agirre et al, 2009b) and
word sense disambiguation (Agirre and Soroa,
2009). In the first work, a single word was in-
put to the random walk algorithm, obtaining the
probability distribution over all WordNet synsets.
The similarity of two words was computed as the
similarity of the distribution of each word, obtain-
ing the best results for WordNet-based systems on
the word similarity dataset, and comparable to the
results of a distributional similarity method which
used a crawl of the entire web. Agirre et al (2009)
used the context of occurrence of a target word to
start the random walk, and obtained very good re-
sults for WordNet WSD methods.
9 Conclusions and Future Work
This paper presents a novel Document Expan-
sion method based on a WordNet-based system
to find related concepts and words. The docu-
ments in three datasets were thus expanded with
related words, which were fed into a separate in-
dex. When combined with the regular index we
report improvements over MG4J usingwBM25 for
those three datasets across several parameter set-
tings, including default values, optimized param-
eters and parameters optimized in other datasets.
In most of the cases the improvements are sta-
tistically significant, indicating that the informa-
tion in the document expansion is useful. Similar
to other expansion methods, parameter optimiza-
tion has a stronger effect than our expansion strat-
egy. The problem with parameter optimization is
that in most real cases there is no tuning dataset
4http://ixa2.si.ehu.es/ukb
available. Our analysis shows that our expansion
method is more effective for sub-optimal param-
eter settings, which is the case for most real-live
IR applications. A comparison across the three
datasets and using artificially trimmed documents
indicates that our method is particularly effective
for short documents.
As document expansion is done at indexing
time, it avoids any overhead at query time. It
also has the advantage of leveraging full document
context, in contrast to query expansion methods,
which use the scarce information present in the
much shorter queries. Compared to WSD-based
methods, our method has the advantage of not
having to disambiguate all words in the document.
Besides, our algorithm picks the most relevant
concepts, and thus is able to expand to concepts
which are not explicitly mentioned in the docu-
ment. The successful use of background informa-
tion such as the one in WordNet could help close
the gap between semantic web technologies and
IR, and opens the possibility to include other re-
sources like Wikipedia or domain ontologies like
those in the Unified Medical Language System.
Our method to integrate expanded terms using
an additional index is simple and straightforward,
and there is still ample room for improvement.
A tighter integration of the document expansion
technique in the retrieval model should yield bet-
ter results, and the smoothed language models of
(Mei et al, 2008; Huang et al, 2009) seem a
natural choice. We would also like to compare
with other existing query and document expan-
sion techniques and study whether our technique
is complementary to query expansion approaches.
Acknowledgments
This work has been supported by KNOW2
(TIN2009-14715-C04-01) and KYOTO (ICT-
2007-211423) projects. Arantxa Otegi?s work is
funded by a PhD grant from the Basque Govern-
ment. Part of this work was done while Arantxa
Otegi was visiting Yahoo! Research Barcelona.
References
Agirre, E. and A. Soroa. 2009. Personalizing PageR-
ank for Word Sense Disambiguation. In Proc. of
16
EACL 2009, Athens, Greece.
Agirre, E., G. M. Di Nunzio, N. Ferro, T. Mandl,
and C. Peters. 2008. CLEF 2008: Ad-Hoc Track
Overview. In Working Notes of the Cross-Lingual
Evaluation Forum.
Agirre, E., G. M. Di Nunzio, T. Mandl, and A. Otegi.
2009a. CLEF 2009 Ad Hoc Track Overview: Ro-
bust - WSD Task. In Working Notes of the Cross-
Lingual Evaluation Forum.
Agirre, E., A. Soroa, E. Alfonseca, K. Hall, J. Kraval-
ova, and M. Pasca. 2009b. A Study on Similarity
and Relatedness Using Distributional and WordNet-
based Approaches. In Proc. of NAACL, Boulder,
USA.
Boldi, P. and S. Vigna. 2005. MG4J at TREC 2005.
In The Fourteenth Text REtrieval Conference (TREC
2005) Proceedings, number SP 500-266 in Special
Publications. NIST.
Collins-Thompson, Kevyn. 2009. Reducing the risk
of query expansion via robust constrained optimiza-
tion. In Proceedings of CIKM ?09, pages 837?846.
Fellbaum, C., editor. 1998. WordNet: An Elec-
tronic Lexical Database and Some of its Applica-
tions. MIT Press, Cambridge, Mass.
Gonzalo, J., F. Verdejo, I. Chugur, and J. Cigarran.
1998. Indexing with WordNet synsets can improve
text retrieval. In Proceedings ACL/COLING Work-
shop on Usage of WordNet for Natural Language
Processing.
Haveliwala, T. H. 2002. Topic-sensitive PageRank. In
Proceedings of WWW ?02, pages 517?526.
Huang, Yunping, Le Sun, and Jian-Yun Nie. 2009.
Smoothing document language model with local
word graph. In Proceedings of CIKM ?09, pages
1943?1946.
Kim, S. B., H. C. Seo, and H. C. Rim. 2004. Informa-
tion retrieval using word senses: root sense tagging
approach. In Proceedings of SIGIR ?04, pages 258?
265.
Kurland, O. and L. Lee. 2004. Corpus structure, lan-
guage models, and ad hoc information retrieval. In
Proceedings of SIGIR ?04, pages 194?201.
Liu, X. and W. B. Croft. 2004. Cluster-based retrieval
using language models. In Proceedings of SIGIR
?04, pages 186?193.
Liu, S., C. Yu, and W. Meng. 2005. Word sense dis-
ambiguation in queries. In Proceedings of CIKM
?05, pages 525?532.
Manning, C. D., P. Raghavan, and H. Schu?tze. 2009.
An introduction to information retrieval. Cam-
bridge University Press, UK.
Mei, Qiaozhu, Duo Zhang, and ChengXiang Zhai.
2008. A general optimization framework for
smoothing language models on graph structures. In
Proceedings of SIGIR ?08, pages 611?618.
Pen?as, A., P. Forner, R. Sutcliffe, A. Rodrigo,
C. Fora?scu, I. Alegria, D. Giampiccolo, N. Moreau,
and P. Osenova. 2009. Overview of ResPubliQA
2009: Question Answering Evaluation over Euro-
pean Legislation. In Working Notes of the Cross-
Lingual Evaluation Forum.
Robertson, S. and H. Zaragoza. 2009. The Proba-
bilistic Relevance Framework: BM25 and Beyond.
Foundations and Trends in Information Retrieval,
3(4):333?389.
Singhal, A. and F. Pereira. 1999. Document expansion
for speech retrieval. In Proceedings of SIGIR ?99,
pages 34?41, New York, NY, USA. ACM.
Smucker, M. D., J. Allan, and B. Carterette. 2007. A
comparison of statistical significance tests for infor-
mation retrieval evaluation. In Proc. of CIKM 2007,
Lisboa, Portugal.
Stokoe, C., M. P. Oakes, and J. Tait. 2003. Word sense
disambiguation in information retrieval revisited. In
Proceedings of SIGIR ?03, page 166.
Surdeanu, M., M. Ciaramita, and H. Zaragoza. 2008.
Learning to Rank Answers on Large Online QA
Collections. In Proceedings of ACL 2008.
Tao, T., X. Wang, Q. Mei, and C. Zhai. 2006. Lan-
guage model information retrieval with document
expansion. In Proceedings of HLT/NAACL, pages
407?414, June.
Voorhees, E. M. 1994. Query expansion using lexical-
semantic relations. In Proceedings of SIGIR ?94,
page 69.
17
