Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 839?847,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Complexity of finding the BLEU-optimal hypothesis in a confusion network
Gregor Leusch and Evgeny Matusov and Hermann Ney
RWTH Aachen University, Germany
{leusch,matusov,ney}@cs.rwth-aachen.de
Abstract
Confusion networks are a simple representa-
tion of multiple speech recognition or transla-
tion hypotheses in a machine translation sys-
tem. A typical operation on a confusion net-
work is to find the path which minimizes or
maximizes a certain evaluation metric. In this
article, we show that this problem is gener-
ally NP-hard for the popular BLEU metric,
as well as for smaller variants of BLEU. This
also holds for more complex representations
like generic word graphs. In addition, we give
an efficient polynomial-time algorithm to cal-
culate unigram BLEU on confusion networks,
but show that even small generalizations of
this data structure render the problem to be
NP-hard again.
Since finding the optimal solution is thus not
always feasible, we introduce an approximat-
ing algorithm based on a multi-stack decoder,
which finds a (not necessarily optimal) solu-
tion for n-gram BLEU in polynomial time.
1 Introduction
In machine translation (MT), confusion networks
(CNs) are commonly used to represent alternative
versions of sentences. Typical applications include
translation of different speech recognition hypothe-
ses (Bertoldi et al, 2007) or system combination
(Fiscus, 1997; Matusov et al, 2006).
A typical operation on a given CN is to find the
path which minimizes or maximizes a certain eval-
uation metric. This operation can be used in ap-
plications like Minimum Error Rate Training (Och,
2003), or optimizing system combination as de-
scribed by Hillard et al (2007). Whereas this is
easily achievable for simple metrics like the Word
Error Rate (WER) as described by Mohri and Riley
(2002), current research in MT uses more sophisti-
cated measures, like the BLEU score (Papineni et
al., 2001). Zens and Ney (2005) first described this
task on general word graphs, and sketched a com-
plete algorithm for calculating the maximum BLEU
score in a word graph. While they do not give an
estimate on the complexity of their algorithm, they
note that already a simpler algorithm for calculating
the Position independent Error Rate (PER) has an
exponential worst-case complexity. The same can
be expected for their BLEU algorithm. Dreyer et
al (2007) examined a special class of word graphs,
namely those that denote constrained reorderings of
single sentences. These word graphs have some
properties which simplify the calculation; for exam-
ple, no edge is labeled with the empty word, and
all paths have the same length and end in the same
node. Even then, their decoder does not optimize
the true BLEU score, but an approximate version
which uses a language-model-like unmodified pre-
cision. We give a very short introduction to CNs and
the BLEU score in Section 2.
In Section 3 we show that finding the best BLEU
score is an NP-hard problem, even for a simplified
variant of BLEU which only scores unigrams and
bigrams. The main reason for this problem to be-
come NP-hard is that by looking at bigrams, we al-
low for one decision to also influence the following
decision, which itself can influence the decisions af-
ter that. We also show that this also holds for uni-
gram BLEU and the position independent error rate
(PER) on a slightly augmented variant of CNs which
allows for edges to carry multiple symbols. The con-
catenation of symbols corresponds to the interde-
pendency of decisions in the case of bigram matches
above.
NP-hard problems are quite common in machine
839
translation; for example, Knight (1999) has shown
that even for a simple form of statistical MT mod-
els, the decoding problem is NP-complete. More
recently, DeNero and Klein (2008) have proven the
NP-completeness of the phrase alignment problem.
But even a simple, common procedure as BLEU
scoring, which can be performed in linear time on
single sentences, becomes a potentially intractable
problem as soon as it has to be performed on a
slightly more powerful representation, such as con-
fusion networks. This rather surprising result is the
motivation of this paper.
The problem of finding the best unigram BLEU
score in an unaugmented variant of CNs is not NP-
complete, as we show in Section 4. We present an
algorithm that finds such a unigram BLEU-best path
in polynomial time.
An important corollary of this work is that calcu-
lating the BLEU-best path on general word graphs
is also NP-complete, as CNs are a true subclass
of word graphs. It is still desirable to calculate a
?good? path in terms of the BLEU score in a CN,
even if calculating the best path is infeasible. In Sec-
tion 5, we present an algorithm which can calculate
?good? solutions for CNs in polynomial time. This
algorithm can easily be extended to handle arbitrary
word graphs. We assess the algorithm experimen-
tally on real-world MT data in Section 6, and draw
some conclusions from the results in this article in
Section 7.
2 Confusion networks
A confusion network (CN) is a word graph where
each edge is labeled with exactly zero or one sym-
bol, and each path from the start node to the end
node visits each node of the graph in canonical or-
der. Usually, we represent unlabeled edges by label-
ing them with the empty word ?.
Within this paper, we represent a CN by a list of
lists of words {wi,j}, where each wi,j corresponds
to a symbol on an edge between nodes i and i + 1.
A path in this CN can be written as a string of inte-
gers, an1 = a1, . . . , an, such that the path is labeled
w1,a1w2,a2 . . . wn,an . Note that there can be a differ-
ent number of possible words, j, for different posi-
tions i.
2.1 BLEU and variants
The BLEU score, as defined by Papineni et al
(2001), is the modified n-gram precision of a hy-
pothesis, with 1 ? n ? N , given a set of reference
translations R. ?Modified precision? here means
that for each n-gram, its maximum number of oc-
currences within the reference sentences is counted,
and only up to that many occurrences in the hypothe-
sis are considered to be correct. The geometric mean
over the precisions for all n is calculated, and mul-
tiplied by a brevity penalty bp. This brevity penalty
is 1.0 if the hypothesis sentence is at least as long as
the reference sentence (special cases occur if multi-
ple reference sentences with different length exists),
and less than 1.0 otherwise. The exact formulation
can be found in the cited paper; for the proofs in
our paper it is enough to note that the BLEU score
is 1.0 exactly if all n-grams in the hypothesis oc-
cur at least that many times in a reference sentence,
and if there is a reference sentence which is as long
as or shorter than the hypothesis. Assuming that
we can always provide a dummy reference sentence
shorter than this length, we do not need to regard
the brevity penalty in these proofs. Within the fol-
lowing proofs of NP-hardness, we will only require
confusion networks (and word graphs) which do not
contain empty words, and where all paths from the
start node to the end node have the same length.
Usually, in the definition of the BLEU score, N is
set to 4; within this article we denote this metric as
4BLEU. We can also restrict the calculations to un-
igrams only, which would be 1BLEU, or to bigrams
and unigrams, which we denote as 2BLEU.
Similar to the 1BLEU metric is the Position in-
dependent Error Rate PER (Tillmann et al, 1997),
which counts the number of substitutions, insertions,
and deletions that have to be performed on the uni-
gram counts to have the hypothesis counts match the
reference counts. Unlike 1BLEU, for PER to be op-
timal (here, 0.0), the reference counts must match
the candidate counts exactly.
Given a CN {wi,j} and a set of reference sen-
tences R, we define the optimization problem
Definition 1 (CN-2BLEU-OPTIMIZE) Among
all paths aI1 through the CN, what is the path with
the highest 2BLEU score?
Related to this is the decision problem
Definition 2 (CN-2BLEU-DECIDE) Among all
paths aI1 through the CN, is there a path with a
2BLEU score of 1.0?
Similarly we define CN-4BLEU-DECIDE, CN-
PER-DECIDE, etc.
840
3 CN-2BLEU-DECIDE is NP-complete
We now show that CN-2BLEU-DECIDE is NP-
complete. It is obvious that the problem is in NP:
Given a path aI1, which is polynomial in size to the
problem, we can decide in polynomial time whether
aI1 is a solution to the problem ? namely by calcu-
lating the BLEU score. We now show that there is
a problem known to be NP-complete which can be
polynomially reduced to CN-2BLEU-DECIDE. For
our proof, we choose 3SAT.
3.1 3SAT
Consider the following problem:
Definition 3 (3SAT) Let X = {x1, . . . , xn}
be a set of Boolean variables, let F =?k
i=1 (Li,1?Li,2?Li,3) be a Boolean formula,
where each literal Li,j is either a variable x or its
negate x. Is there a assignment ? : X ? {0, 1}
such that ? |= F? In other words, if we replace
each x in F by ?(x), and each x by 1? ?(x), does
F become true?
It has been shown by Karp (1972) that 3SAT is
NP-complete. Consequently, if for another problem
in NP there is polynomial-size and -time reduction
of an arbitrary instance of 3SAT to an instance of this
new problem, this new problem is also NP-complete.
3.2 Reduction of 3SAT to
CN-2BLEU-DECIDE
Let F be a Boolean formula in 3CNF, and let k be
its size, as in Definition 3. We will now reduce it to a
corresponding CN-2BLEU-DECIDE problem. This
means that we create an alphabet ?, a confusion net-
work C, and a set of reference sentencesR, such that
there is a path through C with a BLEU score of 1.0
exactly if F is solvable:
Create an alphabet ? based on F as ? :=
{x1, . . . , xn} ? {x1, . . . , xn} ? {}. Here, the xi
and xi symbols will correspond to the variable with
the same name or their negate, respectively, whereas
 will serve as an ?isolator symbol?, to avoid un-
wanted bigram matches or mismatches between sep-
arate parts of the constructed CN or sentences.
Consider the CN C from Figure 1.
Consider the following set of reference sentences:
R := {  (x1)
k(x2)
k . . . (xn)
k
 (x1)
k(x2)
k . . . (xn)
k,
(x1)
k  (x1)
k  . . . (xn)
k  (xn)
k  ()k+n }
where (x)k denotes k subsequent occurrences of x.
Clearly, both C and R are of polynomial size in n
and k, and can be constructed in polynomial time.
Then,
There is an assignment ? such that ? |= F
?
There is a path aI1 through C such that
BLEU(aI1, R) = 1.0.
Proof: ???
Let ? be an assignment under which F becomes
true. Create a path aI1 as follows: Within A, for
each set of edges Li,1, Li,2, Li,3, choose the path
through an x where ?(x) = 1, or through an x
where ?(x) = 0. Note that there must be such an
x, because otherwise the clause Li,1 ? Li,2 ? Li,3
would not be true under ?. Within B, select the path
always through xi if ?(xi) = 0, and through xi if
?(xi) = 1.
Then, aI1 consists of, for each i,
? At most k occurrences of both xi and xi
? At most k occurrences of each of the bigrams
xi, xi, xi, xi, xixi, and xixi
? No other bigram than those listed above.
For all of these unigram and bigram counts, there is
a reference sentence in R which contains at least as
many of those unigrams/bigrams as the path. Thus,
the unigram and bigram precision of aI1 is 1.0. In ad-
dition, there is always a reference sentence whose
length is shorter than that of aI1, such that the brevity
penalty is also 1.0. As a result, BLEU(aI1, R) =
1.0.
???
Let aI1 be a path through C such that
BLEU(aI1, R) = 1.0. Because there is no bi-
gram xixi or xixi in R, we can assume that for each
xi, either only xi edges, or only xi edges appear
in the B part of aI1, each at most k times. As no
unigram xi and xi appears more than k times in R,
we can assume that, if the xi edges are passed in
B, then only the xi edges are passed in A, and vice
versa. Now, create an assignment ? as follows:
? :=
{
0 ifxi edges are passed inB
1 otherwise
Then, ? |= F . Proof: Assume that F? = 0. Then
there must be a clause i such that Li,1?Li,2?Li,3 =
841
A :=          
   




L1,1
L1,2
L1,3
   
   
   
   





   
   
   
   




L2,1
L2,2
L2,3
   
   
   
   




 ? ? ?
   
   
   
   





   
   
   
   




Lk,1
Lk,2
Lk,3
   
   
   
   





   
   
   
   




B :=             
x1
x1
   
   
   
   




x1
x1
? ? ?
   
   
   
   




x1
x1? ?? ?
k times
   
   
   
   





   
   
   
   




x2
x2
? ? ?
? ?? ?
k
   
   
   
   




 ? ? ?
   
   
   
   




xn
xn
? ? ?
? ?? ?
k
   
   
   
   





C := A             

B
Figure 1: CN constructed from a 3SAT formula F . C is the concatenation of the left part A, and the right path B,
separated by an isolating .
0. At least one of the edges Li,j associated with the
literals of this clause must have been passed by aK1
in A. This literal, though, can not have been passed
in B. As a consequence, ?(Li,j) = 1. But this
means that Li,1 ? Li,2 ? Li,3 = 1 ; contra-
diction.
Because CN-2BLEU-DECIDE is in NP, and we
can reduce an NP-complete problem (3SAT) in poly-
nomial time to a CN-2BLEU-DECIDE problem, this
means that CN-2BLEU-DECIDE is NP-complete.
3.3 CN-4BLEU-DECIDE
It is straightforward to modify the construction
above to create an equivalent CN-4BLEU-DECIDE
problem instead: Replace each occurrence of the
isolating symbol  in A,B, C, R by three consecu-
tive isolating symbols . Then, everything said
about unigrams still holds, and bi-, tri- and four-
grams are handled equivalently: Previous unigram
matches on  correspond to uni-, bi-, and trigram
matches on , , . Bigram matches on x corre-
spond to bi-, tri-, and fourgram matches on x, x,
x, and similar holds for bigram matches x, x,
x. Unigram matches x, x, and bigram matches
xx etc. stay the same. Consequently, CN-4BLEU-
DECIDE is also an NP-complete problem.
3.4 CN*-1BLEU-DECIDE
Is it possible to get rid of the necessity for bi-
gram counts in this proof? One possibility might be
to look at slightly more powerful graph structures,
CN*. In these graphs, each edge can be labeled
by arbitrarily many symbols (instead of just zero or
one). Then, consider a CN* graph C? := A            

B?,
B? :=             
(x1)k
(x1)k
   
   
   
   





   
   
   
   




x1
x1
?? ?? ?
k times
? ? ?
   
   
   
   




(xn)k
(xn)k
   
   
   
   




 ? ? ?
Figure 2: Right part of a CN* constructed from a 3SAT
formula F .
with B? as in Figure 2.
With
R? := {(x1)
k(x1)
k . . . (xn)
k(xn)
k()k}
we can again assume that either xi or xi ap-
pears k times in the B?-part of a path aK1 with
1BLEU(aK1 , R
?) = 1.0, and that for every solution
? to F there is a corresponding path aK1 through C
?
and vice versa. In this construction, we also have
exact matches of the counts, so we can also use PER
in the decision problem.
While CN* are generally not word graphs by
themselves due to the multiple symbols on edges,
it is straightforward to create an equivalent word
graph from a given CN*, as demonstrated in Fig-
ure 3. Consequently, deciding unigram BLEU and
unigram PER are NP-complete problems for general
word graphs as well.
4 Solving CN-1BLEU-DECIDE in
polynomial time
It is not a coincidence that we had to resort to
bigrams or to edges with multiple symbols for
NP-completeness: It turns out that CN-1BLEU-
DECIDE, where the order of the words does not
842
CN*: ? ? ?             
(x1)k
(x1)k
   
   
   
   




 ? ? ?
;
WG: ? ? ?          
x1
x1
  
  
  



  
  
  
  




x1
x1
? ? ?   
  
  



  
  
  
  




x1
x1? ?? ?
k times
   
   
   
   




 ? ? ?
Figure 3: Construction of a word graph from a CN* as in
B?.
matter at all, can be decided in polynomial time
using the following algorithm, which disregards a
brevity penalty for the sake of simplicity:
Given a vocabulary X , a CN {wi,j}, and a set of
reference sentences R together with their unigram
BLEU counts c(x) : X ? N and C :=
?
x?X c(x),
1. Remove all parts fromw where there is an edge
labeled with the empty word ?. This step will
always increase unigram precision, and can not
hurt any higher n-gram precision here, because
n = 1. In the example in Figure 4, the edges
labeled very and ? respectively are affected in
this step.
2. Create nodes A0 := {1, . . . , n}, one for each
node with edges in the CN. In the example in
Figure 5, the three leftmost column heads cor-
respond to these nodes.
3. Create nodes B := {x.j |x ? X, 1?j?c(x)}.
In other words, create a unique node for each
?running? word in R ? e.g. if the first and
second reference sentence contain x once each,
and the third reference contains x twice, create
exactly x.1 and x.2. In Figure 5, those are the
row heads to the right.
4. Fill A with empty nodes to match the total
length: A := A0 ? {?.j | 1 ? j ? C ? n}.
If n > C, the BLEU precision can not be 1.0.
The five rightmost columns in Figure 5 corre-
spond to those.
5. Create edges
E := {(i, wi,j .k) | 1? i?n, all j, 1?c(wi,j)}
? {(i, ?.j) | 1 ? i ? n, all j}. These edges are
denoted as ? or ? in Figure 5.
C :=             
on
at
   
   
   
   




the
that
   
   
   
   




very
?
   
   
   
   




day
time
R := { on the same day,
at the time and the day }
Figure 4: Example for CN-1BLEU-DECIDE.
1 2 3 ?.1 ?.2 ?.3 ?.4 ?.5
? ? ? ? ? ? on
? ? ? ? ? ? the.1
? ? ? ? ? ? the.2
? ? ? ? ? same
? ? ? ? ? ? day
? ? ? ? ? ? at
? ? ? ? ? ? time
? ? ? ? ? and
Figure 5: Bipartite graph constructed to find the optimal
1BLEU path in Figure 4. One possible maximum bipar-
tite matching is marked with ?.
6. Find the maximum bipartite matching M be-
tween A and B given E. Figure 5 shows such
a matching with ?.
7. If all nodes in A and B are covered by M ,
then 1BLEU({wi,j}, R) = 1.0. The words that
are matched to A0 then form the solution path
through {wi,j}.
Figure 4 gives an example of a CN and a set of ref-
erences R, for which the best 1BLEU path can be
constructed by the algorithm above. The bipartite
graph constructed in Step 1 to Step 4 for this exam-
ple, given in matrix form, can be found in Figure 5.
Such a solution to Step 6, if found, corresponds
exactly to a path through the confusion network with
1BLEU=1.0, and vice versa: for each position 1 ?
i ? n, the matched word corresponds to the word
that is selected for the position of the path; ?surplus?
counts are matched with ?s.
Step 6 can be performed in polynomial time
(Hopcroft and Karp, 1973) O((C +n)5/2); all other
steps in linear time O(C + n). Consequently, CN-
1BLEU can be decided in polynomial time O((C +
n)5/2). Similarly, an actual optimum 1BLEU score
843
can be calculated in O((C + n)5/2).
It should be noted that the only alterations in the
hypothesis length, and as a result the only alterations
in the brevity penalty, will come from Step 1. Con-
sequently, the brevity penalty can be taken into ac-
count as follows: Consider that there are M nodes
with an empty edge in {wi,j}. Instead of remov-
ing them in Step 1, keep them in, but for each
1 ? m ? M , run through steps 2 to 6, but add
m nodes ?.1, . . . , ?.m to B in Step 3, and add corre-
sponding edges to these nodes to E in Step 5. After
each iteration (which leads to a constant hypothesis
length), calculate precision and brevity penalty. Se-
lect the best product of precision and brevity penalty
in the end. The overall time complexity now is in
M ?O((C + n)5/2).
A PER score can be calculated in a similar fash-
ion.
5 Finding approximating solutions for
CN-4BLEU in polynomial time
Knowing that the problem of finding the BLEU-best
path is an NP-complete problem is an unsatisfactory
answer in practice ? in many cases, having a good,
but not necessarily optimum path is preferable to
having no good path at all.
A simple approach would be to walk the CN from
the start node to the end node, keeping track of n-
grams visited so far, and choosing the word next
which maximizes the n-gram precision up to this
word. Track is kept by keeping n-gram count vec-
tors for the hypothesis path and the reference sen-
tences, and update those in each step.
The main problem with this approach is that of-
ten the local optimum is suboptimal on the global
scale, for example if a word occurs on a later posi-
tion again.
Zens and Ney (2005) on the other hand propose
to keep all n-gram count vectors instead, and only
recombine path hypotheses with identical count vec-
tors. As they suspect, the search space can become
exponentially large.
In this paper, we suggest a compromise between
these two extremes, namely keeping active a suffi-
ciently large number of ?path hypotheses? in terms
of n-gram precision, instead of only the first best,
or of all. But even then, edges with empty words
pose a problem, as stepping along an empty edge
will never decrease the precision of the local path.
In certain cases, steps along empty edges may affect
the n-gram precision for higher n-grams. But this
will only take effect after the next non-empty step, it
does not influence the local decision in a node. Step-
ping along a non-empty edge will often decrease the
local precision, though. As a consequence, a simple
algorithm will prefer paths with shorter hypotheses,
which leads to a suboptimal total BLEU score, be-
cause of the brevity penalty. One can counter this
problem for example by using a brevity penalty al-
ready during the search. But this is problematic as
well, because it is difficult to define a proper partial
reference length in this case.
The approach we propose is to compare only par-
tial path hypotheses with the same number of empty
edges, and ending in the same position in the confu-
sion network. This idea is illustrated in Figure 6: We
compare only the partial precision of path hypothe-
ses ending in the same node. Due to the simple na-
ture of this search graph, it can easily be traversed in
a left-to-right, top-to-bottom manner. With regard to
a node currently being expanded, only the next node
in the same row, and the corresponding columns in
the next row need to be kept active. When imple-
menting this algorithm, Hypotheses should be com-
pared on the modified BLEUS precision by Lin and
Och (2004) because the original BLEU precision
equals zero as long as there are no higher n-gram
matches in the partial hypotheses, which renders
meaningful comparison hard or impossible.
In the rightmost column, all path hypotheses
within a node have the same hypothesis length. Con-
sequently, we can select the hypothesis with the best
(brevity-penalized) BLEU score by multiplying the
appropriate brevity penalty to the precision of the
best path ending in each of these nodes. If we al-
ways expand all possible path hypotheses within the
nodes, and basically run a full search, we will al-
ways find the BLEU-best path this way. From the
proof above, it follows that the number of path hy-
pothesis we would have to keep can become expo-
nentially large. Fortunately, if a ?good? solution is
good enough, we do not have to keep all possible
path hypotheses, but only the S best ones for a given
constant S, or those with a precision not worse than
c times the precision of the best hypothesis within
the node. Assuming that adding and removing an
element to/from a size-limited stack of size S takes
timeO(logS), that we allow at mostE empty edges
in a solution, and that there are j edges in each of the
n positions, this algorithm has a time complexity of
844
Figure 6: Principle of the multi-stack decoder used to find
a path with a good BLEU score. The first row shows
the original confusion network, the following rows show
the search graph. Duplicate edges were removed, but no
word was considered ?unknown?.
O(E ? n ? j ? S logS).
To reduce redundant duplicated path hypotheses,
and by this to speed up the algorithm and reduce the
risk that good path hypotheses are pruned, the confu-
sion network should be simplified before the search,
as shown in Figure 6:
1. Remove all words in the CN which do not ap-
pear in any reference sentence, if there at least
one ?known? non-empty word at the same po-
sition. If there is no such ?known? word, re-
place them all by a single token denoting the
?unknown word?.
2. Remove all duplicate edges in a position, that
is, if there are two or more edges carrying the
same label in one position, remove all but one
for them.
These two steps will keep at least one of the BLEU-
best paths intact. But they can remove the average
branching factor (j) of the CN significantly, which
leads to a significantly lower number of duplicate
path hypotheses during the search.
Table 1: Statistics of the (Chinese?)English MT corpora
used for the experiments
NIST NIST
2003 2006
number of systems 4 4
number of ref. 4 4 per sent.
sentences 919 249
system length 28.4 33.2 words?
ref. length 27.5 34.2 words?
best path 24.4 33.9 words?
CN length 40.7 39.5 nodes?
best single system 29.3 52.5 BLEU
30.5 51.6 BLEUS?
?average per sentence
Our algorithm can easily be extended to handle ar-
bitrary word graphs instead of confusion networks.
In this case, each ?row? in Figure 6 will reflect the
structure of the word graph instead of the ?linear?
structure of the CN.
While this algorithm searches for the best path
for a single sentence only, a common task is to
find the best BLEU score over a whole test set ?
which can mean suboptimal BLEU scores for in-
dividual sentences. This adds an additional com-
binatorial problem over the sentences to the actual
decoding process. Both Zens and Ney (2005) and
Dreyer et al(2007) use a greedy approach here; the
latter estimated the impact of this to be insignifi-
cant in random sampling experiments. In our exper-
iments, we used the per-sentence BLEUS score as
(greedy) decision criterion, as this is also the prun-
ing criterion. One possibility to adapt this approach
to Zens?s/Dreyer?s greedy approach for system-level
BLEU scores might be to initialize n-gram counts
and hypothesis length not to zero at the beginning
of each sentence, but to those of the corpus so far.
But as this diverts from our goal to optimize the
sentence-level scores, we have not implemented it
so far.
6 Experimental assessment of the
algorithm
The question arises how many path hypotheses we
need to retain in each step to obtain optimal paths.
To examine this, we created confusion networks out
of the translations of the four best MT systems of
845
ll l
l l l l l l l l l l l l
5 10 15 20
0.32
0.34
0.36
0.38
0.40
# path hyps
BL
EU
, BL
EU
S
l
sys?BLEU
avg. seg?BLEUS
avg. seg?BLEU
Figure 7: Average of the sentence-wise BLEU and
BLEUS score and the system-wide BLEU score versus
the number of path hypotheses kept per node during the
search. NIST MT03 corpus.
the NIST 2003 and 2006 Chinese?English evalu-
ation campaigns, as available from the Linguistic
Data Consortium (LDC). The hypotheses of the best
single system served as skeleton, those of the three
remaining systems were reordered and aligned to the
skeleton hypothesis. This corpus is described in Ta-
ble 1. Figures 7 and 8 show the measured BLEU
scores in three different definitions, versus the max-
imum number of path hypotheses that are kept in
each node of the search graph. Shown are the av-
erage sentence-wise BLEUS score, which is what
the algorithm actually optimizes, for comparison the
average sentence-wise BLEU score, and the total
document-wise BLEU score.
All scores increase with increasing number of re-
tained hypotheses, but stabilize around a total of 15
hypotheses per node. The difference over a greedy
approach, which corresponds to a maximum of one
hypothesis per node if we leave out the separation by
path length, is quite significant. No further improve-
ments can be expected for a higher number of hy-
potheses, as experiments up to 100 hypotheses show.
7 Conclusions
In this paper, we showed that deciding whether a
given CN contains a path with a BLEU score of 1.0
is an NP-complete problem for n-gram lengths ? 2.
l
l l l
l l l l l l l l l l l l
5 10 15 20 250.
65
0.67
0.69
0.71
# path hyps
BL
EU
, BL
EU
S
l
sys?BLEU
avg. seg?BLEUS
avg. seg?BLEU
Figure 8: Average of the sentence-wise BLEU and
BLEUS score and the system-wide BLEU score versus
the number of path hypotheses kept per node during the
search. NIST MT06 corpus.
The problem is also NP-complete if we only look
at unigram BLEU, but allow for CNs where edges
may contain multiple symbols, or for arbitrary word
graphs. As a corollary, any proposed algorithm to
find the path with an optimal BLEU score in a CN,
even more in an arbitrary word graph, which runs
in worst case polynomial time can only deliver an
approximation1.
We gave an efficient polynomial time algorithm
for the simplest variant, namely deciding on a uni-
gram BLEU score for a CN. This algorithm can eas-
ily be modified to decide on the PER score as well,
or to calculate an actual unigram BLEU score for the
hypothesis CN.
Comparing these results, we conclude that the
ability to take bi- or higher n-grams into account,
be it in the scoring (as in 2BLEU), or in the graph
structure (as in CN*), is the key to render the prob-
lem NP-hard. Doing so creates long-range depen-
dencies, which oppose local decisions.
We also gave an efficient approximating algo-
rithm for higher-order BLEU scores. This algorithm
is based on a multi-stack decoder, taking into ac-
count the empty arcs within a path. Experimental
results on real-world data show that our method is
indeed able to find paths with a significantly better
1provided that P 6= NP , of course.
846
BLEU score than that of a greedy search. The re-
sulting BLEUS score stabilizes already on a quite
restricted search space, showing that despite the
proven NP-hardness of the exact problem, our al-
gorithm can give useful approximations in reason-
able time. It is yet an open problem in how far
the problems of finding the best paths regarding a
sentence-level BLEU score, and regarding a system-
level BLEU score correlate. Our experiments here
suggest a good correspondence.
8 Acknowledgments
This paper is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
The proofs and algorithms in this paper emerged
while the first author was visiting researcher at
the Interactive Language Technologies Group of
the National Research Council (NRC) of Canada,
Gatineau. The author wishes to thank NRC and
Aachen University for the opportunity to jointly
work on this project.
References
Nicola Bertoldi, Richard Zens, and Marcello Federico.
2007. Speech translation by confusion network de-
coding. In IEEE International Conference on Acous-
tics, Speech, and Signal Processing, pages 1297?1300,
Honululu, HI, USA, April.
John DeNero and Dan Klein. 2008. The complexity
of phrase alignment problems. In Human Language
Technologies 2008: The Conference of the Association
for Computational Linguistics, Short Papers, pages
25?28, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Markus Dreyer, Keith Hall, and Sanjeev Khudanpur.
2007. Comparing Reordering Constraints for SMT
Using Efficient BLEU Oracle Computation. In AMTA
Workshop on Syntax and Structure in Statistical Trans-
lation (SSST) at the Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics (NAACL-HLT), pages 103?110,
Rochester, NY, USA, April.
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates: Recogniser output vot-
ing error reduction (ROVER). In Proceedings 1997
IEEE Workshop on Automatic Speech Recognition and
Understanding, pages 347?352, Santa Barbara, CA.
Dustin Hillard, Bjo?rn Hoffmeister, Mari Ostendorf, Ralf
Schlu?ter, and Hermann Ney. 2007. iROVER: Improv-
ing system combination with classification. In Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Companion Volume, Short Pa-
pers, pages 65?68, Rochester, New York, April.
John E. Hopcroft and Richard M. Karp. 1973. An n5/2
algorithm for maximum matchings in bipartite graphs.
SIAM Journal on Computing, 2(4):225?231.
Richard M. Karp. 1972. Reducibility among combina-
torial problems. In R. E. Miller and J. W. Thatcher,
editors, Complexity of Computer Computations, pages
85?103. Plenum Press.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607?615, December.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluation automatic evaluation metrics for
machine translation. In Proc. COLING 2004, pages
501?507, Geneva, Switzerland, August.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 33?40, Trento, Italy, April.
Mehryar Mohri and Michael Riley. 2002. An efficient
algorithm for the n-best-strings problem. In Proc. of
the 7th Int. Conf. on Spoken Language Processing (IC-
SLP?02), pages 1313?1316, Denver, CO, September.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of the 41th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 160?167, Sapporo, Japan,
July.
Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. Bleu: a method for auto-
matic evaluation of machine translation. Technical Re-
port RC22176 (W0109-022), IBM Research Division,
Thomas J. Watson Research Center, September.
Christoph Tillmann, Stephan Vogel, Hermann Ney, Alex
Zubiaga, and Hassan Sawaf. 1997. Accelerated
DP based search for statistical translation. In Euro-
pean Conf. on Speech Communication and Technol-
ogy, pages 2667?2670, Rhodes, Greece, September.
Richard Zens and Hermann Ney. 2005. Word graphs
for statistical machine translation. In 43rd Annual
Meeting of the Assoc. for Computational Linguistics:
Proc. Workshop on Building and Using Parallel Texts:
Data-Driven Machine Translation and Beyond, pages
191?198, Ann Arbor, MI, June.
847
CDER: Efficient MT Evaluation Using Block Movements
Gregor Leusch and Nicola Ueffing and Hermann Ney
Lehrstuhl fu?r Informatik VI, Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
{leusch,ueffing,ney}@i6.informatik.rwth-aachen.de
Abstract
Most state-of-the-art evaluation measures
for machine translation assign high costs
to movements of word blocks. In many
cases though such movements still result
in correct or almost correct sentences. In
this paper, we will present a new eval-
uation measure which explicitly models
block reordering as an edit operation.
Our measure can be exactly calculated in
quadratic time.
Furthermore, we will show how some
evaluation measures can be improved
by the introduction of word-dependent
substitution costs. The correlation of the
new measure with human judgment has
been investigated systematically on two
different language pairs. The experimental
results will show that it significantly
outperforms state-of-the-art approaches in
sentence-level correlation. Results from
experiments with word dependent substi-
tution costs will demonstrate an additional
increase of correlation between automatic
evaluation measures and human judgment.
1 Introduction
Research in machine translation (MT) depends
heavily on the evaluation of its results. Espe-
cially for the development of an MT system,
an evaluation measure is needed which reliably
assesses the quality of MT output. Such a measure
will help analyze the strengths and weaknesses of
different translation systems or different versions
of the same system by comparing output at
the sentence level. In most applications of
MT, understandability for humans in terms of
readability as well as semantical correctness
should be the evaluation criterion. But as human
evaluation is tedious and cost-intensive, automatic
evaluation measures are used in most MT research
tasks. A high correlation between these automatic
evaluation measures and human evaluation is thus
desirable.
State-of-the-art measures such as BLEU (Pap-
ineni et al, 2002) or NIST (Doddington, 2002)
aim at measuring the translation quality rather
on the document level1 than on the level of
single sentences. They are thus not well-suited
for sentence-level evaluation. The introduction
of smoothing (Lin and Och, 2004) solves this
problem only partially.
In this paper, we will present a new automatic
error measure for MT ? the CDER ? which is
designed for assessing MT quality on the sentence
level. It is based on edit distance ? such as the
well-known word error rate (WER) ? but allows
for reordering of blocks. Nevertheless, by defining
reordering costs, the ordering of the words in
a sentence is still relevant for the measure. In
this, the new measure differs significantly from
the position independent error rate (PER) by
(Tillmann et al, 1997). Generally, finding an
optimal solution for such a reordering problem is
NP hard, as is shown in (Lopresti and Tomkins,
1997). In previous work, researchers have tried to
reduce the complexity, for example by restricting
the possible permutations on the block-level, or by
approximation or heuristics during the calculation.
Nevertheless, most of the resulting algorithms still
have high run times and are hardly applied in
practice, or give only a rough approximation. An
overview of some better-known measures can be
found in Section 3.1. In contrast to this, our new
measure can be calculated very efficiently. This
is achieved by requiring complete and disjoint
coverage of the blocks only for the reference
sentence, and not for the candidate translation. We
will present an algorithm which computes the new
error measure in quadratic time.
The new evaluation measure will be investi-
gated and compared to state-of-the-art methods
on two translation tasks. The correlation with
human assessment will be measured for several
different statistical MT systems. We will see
that the new measure significantly outperforms the
existing approaches.
1The n-gram precisions are measured at the sentence level
and then combined into a score over the whole document.
241
As a further improvement, we will introduce
word dependent substitution costs. This method
will be applicable to the new measure as well
as to established measures like WER and PER.
Starting from the observation that the substitution
of a word with a similar one is likely to affect
translation quality less than the substitution with
a completely different word, we will show how
the similarity of words can be accounted for in
automatic evaluation measures.
This paper is organized as follows: In Section 2,
we will present the state of the art in MT
evaluation and discuss the problem of block
reordering. Section 3 will introduce the new
error measure CDER and will show how it can
be calculated efficiently. The concept of word-
dependent substitution costs will be explained in
Section 4. In Section 5, experimental results on
the correlation of human judgment with the CDER
and other well-known evaluation measures will be
presented. Section 6 will conclude the paper and
give an outlook on possible future work.
2 MT Evaluation
2.1 Block Reordering and State of the Art
In MT ? as opposed to other natural language
processing tasks like speech recognition ? there
is usually more than one correct outcome of a
task. In many cases, alternative translations of
a sentence differ from each other mostly by the
ordering of blocks of words. Consequently, an
evaluation measure for MT should be able to
detect and allow for block reordering. Neverthe-
less, a higher ?amount? of reordering between a
candidate translation and a reference translation
should still be reflected in a worse evaluation
score. In other words, the more blocks there are
to be reordered between reference and candidate
sentence, the higher we want the measure to
evaluate the distance between these sentences.
State-of-the-art evaluation measures for MT
penalize movement of blocks rather severely: n-
gram based scores such as BLEU or NIST still
yield a high unigram precision if blocks are
reordered. For higher-order n-grams, though, the
precision drops. As a consequence, this affects the
overall score significantly. WER, which is based
on Levenshtein distance, penalizes the reordering
of blocks even more heavily. It measures the
distance by substitution, deletion and insertion
operations for each word in a relocated block.
PER, on the other hand, ignores the ordering
of the words in the sentences completely. This
often leads to an overly optimistic assessment of
translation quality.
2.2 Long Jumps
The approach we pursue in this paper is to
extend the Levenshtein distance by an additional
operation, namely block movement. The number
of blocks in a sentence is equal to the number
of gaps among the blocks plus one. Thus, the
block movements can equivalently be expressed
as long jump operations that jump over the
gaps between two blocks. The costs of a
long jump are constant. The blocks are read
in the order of one of the sentences. These
long jumps are combined with the ?classical?
Levenshtein edit operations, namely insertion,
deletion, substitution, and the zero-cost operation
identity. The resulting long jump distance dLJ
gives the minimum number of operations which
are necessary to transform the candidate sentence
into the reference sentence. Like the Levenshtein
distance, the long jump distance can be depicted
using an alignment grid as shown in Figure 1:
Here, each grid point corresponds to a pair of
inter-word positions in candidate and reference
sentence, respectively. dLJ is the minimum cost of
a path between the lower left (first) and the upper
right (last) alignment grid point which covers all
reference and candidate words. Deletions and
insertions correspond to horizontal and vertical
edges, respectively. Substitutions and identity
operations correspond to diagonal edges. Edges
between arbitrary grid points from the same row
correspond to long jump operations. It is easy to
see that dLJ is symmetrical.
In the example, the best path contains one dele-
tion edge, one substitution edge, and three long
jump edges. Therefore, the long jump distance
between the sentences is five. In contrast, the
best Levenshtein path contains one deletion edge,
four identity and five consecutive substitution
edges; the Levenshtein distance between the two
sentences is six. The effect of reordering on the
BLEU measure is even higher in this example:
Whereas 8 of the 10 unigrams from the candidate
sentence can be found in the reference sentence,
this holds for only 4 bigrams, and 1 trigram. Not a
single one of the 7 candidate four-grams occurs in
the reference sentence.
3 CDER: A New Evaluation Measure
3.1 Approach
(Lopresti and Tomkins, 1997) showed that finding
an optimal path in a long jump alignment grid is
an NP-hard problem. Our experiments showed
that the calculation of exact long jump distances
becomes impractical for sentences longer than 20
words.
242
we
met
at
the
airport
at
seven
o?clock
.
we met
at seven
o?clock
on the
airport
.have
candidate
re
fe
re
nc
e
deletion
insertion
substitution
identity best path
start/
end node
long jump
block
Figure 1: Example of a long jump alignment
grid. All possible deletion, insertion, identity and
substitution operations are depicted. Only long
jump edges from the best path are drawn.
A possible way to achieve polynomial run-
time is to restrict the number of admissible block
permutations. This has been implemented by
(Leusch et al, 2003) in the inversion word error
rate. Alternatively, a heuristic or approximative
distance can be calculated, as in GTM by (Turian et
al., 2003). An implementation of both approaches
at the same time can be found in TER by (Snover
et al, 2005). In this paper, we will present another
approach which has a suitable run-time, while
still maintaining completeness of the calculated
measure. The idea of the proposed method is to
drop some restrictions on the alignment path.
The long jump distance as well as the Lev-
enshtein distance require both reference and
candidate translation to be covered completely
and disjointly. When extending the metric by
block movements, we drop this constraint for the
candidate translation. That is, only the words
in the reference sentence have to be covered
exactly once, whereas those in the candidate
sentence can be covered zero, one, or multiple
times. Dropping the constraints makes an efficient
computation of the distance possible. We drop
the constraints for the candidate sentence and not
for the reference sentence because we do not want
any information contained in the reference to be
omitted. Moreover, the reference translation will
not contain unnecessary repetitions of blocks.
The new measure ? which will be called
CDER in the following ? can thus be seen as a
measure oriented towards recall, while measures
like BLEU are guided by precision. The CDER
is based on the CDCD distance2 introduced
in (Lopresti and Tomkins, 1997). The authors
show there that the problem of finding the optimal
solution can be solved in O(I2 ? L) time, where
I is the length of the candidate sentence and L
the length of the reference sentence. Within this
paper, we will refer to this distance as dCD . In
the next subsection, we will show how it can be
computed in O(I ?L) time using a modification of
the Levenshtein algorithm.
We also studied the reverse direction of the
described measure; that is, we dropped the
coverage constraints for the reference sentence
instead of the candidate sentence. Addition-
ally, the maximum of both directions has been
considered as distance measure. The results in
Section 5.2 will show that the measure using the
originally proposed direction has a significantly
higher correlation with human evaluation than the
other directions.
3.2 Algorithm
Our algorithm for calculating dCD is based
on the dynamic programming algorithm for the
Levenshtein distance (Levenshtein, 1966). The
Levenshtein distance dLev(eI1, e?L1
)
between two
strings eI1 and e?L1 can be calculated in con-
stant time if the Levenshtein distances of the
substrings, dLev(eI?11 , e?L1
)
, dLev(eI1, e?L?11
)
, and
dLev(eI?11 , e?L?11
)
, are known.
Consequently, an auxiliary quantity
DLev(i, l) := dLev
(
ei1, e?l1
)
is stored in an I ?L table. This auxiliary quantity
can then be calculated recursively from DLev(i ?
1, l), DLev(i, l ? 1), and DLev(i ? 1, l ? 1).
Consequently, the Levenshtein distance can be
calculated in time O(I ? L).
This algorithm can easily be extended for the
calculation of dCD as follows: Again we define
an auxiliary quantity D(i, l) as
D(i, l) := dCD
(
ei1, e?l1
)
Insertions, deletions, and substitutions are
handled the same way as in the Levenshtein
algorithm. Now assume that an optimal dCD path
has been found: Then, each long jump edge within
2C stands for cover and D for disjoint. We adopted this
notion for our measures.
243
...
i
l
deletion insertion subst/id long jump
l-1
i-1
Figure 2: Predecessors of a grid point (i, l) in
Equation 1
this path will always start at a node with the lowest
D value in its row3.
Consequently, we use the following modifica-
tion of the Levenshtein recursion:
D(0, 0) = 0
D(i, l) = min
?
?
?
?
?
?
?
D(i?1, l?1) + (1??(ei, e?l)) ,
D(i? 1, l) + 1,
D(i, l ? 1) + 1,
min
i?
D(i?, l) + 1
?
?
?
?
?
?
?
(1)
where ? is the Kronecker delta. Figure 2 shows the
possible predecessors of a grid point.
The calculation of D(i, l) requires all values of
D(i?, l) to be known, even for i? > i. Thus, the
calculation takes three steps for each l:
1. For each i, calculate the minimum of the first
three terms.
2. Calculate min
i?
D(i?, l).
3. For each i, calculate the minimum according
to Equation 1.
Each of these steps can be done in time O(I).
Therefore, this algorithm calculates dCD in time
O(I ? L) and space O(I).
3.3 Hypothesis Length and Penalties
As the CDER does not penalize candidate trans-
lations which are too long, we studied the use
of a length penalty or miscoverage penalty. This
determines the difference in sentence lengths
between candidate and reference. Two definitions
of such a penalty have been studied for this work.
3Proof: Assume that the long jump edge goes from (i?, l)
to (i, l), and that there exists an i?? such that D(i??, l) <
D(i?, l). This means that the path from (0, 0) to (i??, l) is
less expensive than the path from (0, 0) to (i?, l). Thus, the
path from (0, 0) through (i??, l) to (i, l) is less expensive than
the path through (i?, l). This contradicts the assumption.
Length Difference
There is always an optimal dCD alignment path
that does not contain any deletion edges, because
each deletion can be replaced by a long jump, at
the same costs. This is different for a dLJ path,
because here each candidate word must be covered
exactly once. Assume now that the candidate
sentence consists of I words and the reference
sentence consists of L words, with I > L.
Then, at most L candidate words can be covered
by substitution or identity edges. Therefore, the
remaining candidate words (at least I ? L) must
be covered by deletion edges. This means that at
least I?L deletion edges will be found in any dLJ
path, which leads to dLJ ? dCD ? I ? L in this
case.
Consequently, the length difference between
the two sentences gives us a useful miscoverage
penalty lplen:
lplen := max
(
I ? L, 0
)
This penalty is independent of the dCD alignment
path. Thus, an optimal dCD alignment path
is optimal for dCD + lplen as well. Therefore
the search algorithm in Section 3.2 will find the
optimum for this sum.
Absolute Miscoverage
Let coverage(i) be the number of substitution,
identity, and deletion edges that cover a candidate
word ei in a dCD path. If we had a complete and
disjoint alignment for the candidate word (i.e., a
dLJ path), coverage(i) would be 1 for each i.
In general this is not the case. We can use the
absolute miscoverage as a penalty lpmisc for dCD:
lpmisc :=
?
i
|1 ? coverage(i)|
This miscoverage penalty is not independent of
the alignment path. Consequently, the proposed
search algorithm will not necessarily find an
optimal solution for the sum of dCD and lpmisc.
The idea behind the absolute miscoverage is
that one can construct a valid ? but not necessarily
optimal ? dLJ path from a given dCD path. This
procedure is illustrated in Figure 3 and takes place
in two steps:
1. For each block of over-covered candidate
words, replace the aligned substitution and/or
identity edges by insertion edges; move the
long jump at the beginning of the block
accordingly.
2. For each block of under-covered candidate
words, add the corresponding number of
244
candidate
re
fe
re
nc
e
2 2 0coverage
candidate
1 1 1 1 1 1 1 1 1 1 1
dCD dLJ
deletion insertion subst/id long jump
Figure 3: Transformation of a dCD path into a dLJ
path.
deletion edges; move the long jump at the
beginning of the block accordingly.
This also shows that there cannot be4 a
polynomial time algorithm that calculates the
minimum of dCD + lpmisc for arbitrary pairs of
sentences, because this minimum is equal to dLJ.
With these miscoverage penalties, inexpensive
lower and upper bounds for dLJ can be calculated,
because the following inequality holds:
(2) dCD + lplen ? dLJ ? dCD + lpmisc
4 Word-dependent Substitution Costs
4.1 Idea
All automatic error measures which are based
on the edit distance (i.e. WER, PER, and
CDER) apply fixed costs for the substitution
of words. However, this is counter-intuitive,
as replacing a word with another one which
has a similar meaning will rarely change the
meaning of a sentence significantly. On the other
hand, replacing the same word with a completely
different one probably will. Therefore, it seems
advisable to make substitution costs dependent on
the semantical and/or syntactical dissimilarity of
the words.
To avoid awkward case distinctions, we assume
that a substitution cost function cSUB for two
words e, e? meets the following requirements:
1. cSUB depends only on e and e?.
2. cSUB is a metric; especially
(a) The costs are zero if e = e?, and larger
than zero otherwise.
(b) The triangular inequation holds: it is
always cheaper to replace e by e? than to
replace e by e? and then e? by e?.
4provided that P 6= NP , of course.
3. The costs of substituting a word e by e? are
always equal or lower than those of deleting
e and then inserting e?. In short, cSUB ? 2.
Under these conditions the algorithms for
WER and CDER can easily be modified to use
word-dependent substitution costs. For example,
the only necessary modification in the CDER
algorithm in Equation 1 is to replace 1 ? ?(e, e?)
by cSUB(e, e?).
For the PER, it is no longer possible to use a
linear time algorithm in the general case. Instead,
a modification of the Hungarian algorithm (Knuth,
1993) can be used.
The question is now how to define the word-
dependent substitution costs. We have studied two
different approaches.
4.2 Character-based Levenshtein Distance
A pragmatic approach is to compare the spelling
of the words to be substituted with each other.
The more similar the spelling is, the more similar
we consider the words to be, and the lower we
want the substitution costs between them. In
English, this works well with similar tenses of
the same verb, or with genitives or plurals of the
same noun. Nevertheless, a similar spelling is no
guarantee for a similar meaning, because prefixes
such as ?mis-?, ?in-?, or ?un-? can change the
meaning of a word significantly.
An obvious way of comparing the spelling is the
Levenshtein distance. Here, words are compared
on character level. To normalize this distance
into a range from 0 (for identical words) to 1
(for completely different words), we divide the
absolute distance by the length of the Levenshtein
alignment path.
4.3 Common Prefix Length
Another character-based substitution cost function
we studied is based on the common prefix length
of both words. In English, different tenses of
the same verb share the same prefix; which is
usually the stem. The same holds for different
cases, numbers and genders of most nouns and
adjectives. However, it does not hold if verb
prefixes are changed or removed. On the other
hand, the common prefix length is sensitive to
critical prefixes such as ?mis-? for the same
reason. Consequently, the common prefix length,
normalized by the average length of both words,
gives a reasonable measure for the similarity of
two words. To transform the normalized common
prefix length into costs, this fraction is then
subtracted from 1.
Table 1 gives an example of these two word-
dependent substitution costs.
245
Table 1: Example of word-dependent substitution costs.
Levenshtein prefix
e e? distance substitution cost similarity substitution cost
usual unusual 2 27 = 0.29 1 1 ? 16 = 0.83
understanding misunderstanding 3 316 = 0.19 0 1.00
talk talks 1 15 = 0.20 4 1 ? 44.5 = 0.11
4.4 Perspectives
More sophisticated methods could be considered
for word-dependent substitution costs as well.
Examples of such methods are the introduction of
information weights as in the NIST measure or the
comparison of stems or synonyms, as in METEOR
(Banerjee and Lavie, 2005).
5 Experimental Results
5.1 Experimental Setting
The different evaluation measures were assessed
experimentally on data from the Chinese?English
and the Arabic?English task of the NIST 2004
evaluation workshop (Przybocki, 2004). In this
evaluation campaign, 4460 and 1735 candidate
translations, respectively, generated by different
research MT systems were evaluated by human
judges with regard to fluency and adequacy.
Four reference translations are provided for each
candidate translation. Detailed corpus statistics
are listed in Table 2.
For the experiments in this study, the candidate
translations from these tasks were evaluated using
different automatic evaluation measures. Pear-
son?s correlation coefficient r between automatic
evaluation and the sum of fluency and adequacy
was calculated. As it could be arguable whether
Pearson?s r is meaningful for categorical data like
human MT evaluation, we have also calculated
Kendall?s correlation coefficient ? . Because of
the high number of samples (= sentences, 4460)
versus the low number of categories (= out-
comes of adequacy+fluency, 9), we calculated
? separately for each source sentence. These
experiments showed that Kendall?s ? reflects the
same tendencies as Pearson?s r regarding the
ranking of the evaluation measures. But only
the latter allows for an efficient calculation of
confidence intervals. Consequently, figures of ?
are omitted in this paper.
Due to the small number of samples for eval-
uation on system level (10 and 5, respectively),
all correlation coefficients between automatic
and human evaluation on system level are very
close to 1. Therefore, they do not show any
significant differences for the different evaluation
Table 2: Corpus statistics. TIDES corpora,
NIST 2004 evaluation.
Source language Chinese Arabic
Target language English English
Sentences 446 347
Running words 13 016 10 892
Ref. translations 4 4
Avg. ref. length 29.2 31.4
Candidate systems 10 5
measures. Additional experiments on data from
the NIST 2002 and 2003 workshops and from
the IWSLT 2004 evaluation workshop confirm
the findings from the NIST 2004 experiments;
for the sake of clarity they are not included
here. All correlation coefficients presented here
were calculated for sentence level evaluation.
For comparison with state-of-the-art evaluation
measures, we have also calculated the correlation
between human evaluation and WER and BLEU,
which were both measures of choice in several
international MT evaluation campaigns. Further-
more, we included TER (Snover et al, 2005) as
a recent heuristic block movement measure in
some of our experiments for comparison with our
measure. As the BLEU score is unsuitable for
sentence level evaluation in its original definition,
BLEU-S smoothing as described by (Lin and
Och, 2004) is performed. Additionally, we
added sentence boundary symbols for BLEU, and
a different reference length calculation scheme
for TER, because these changes improved the
correlation between human evaluation and the two
automatic measures. Details on this have been
described in (Leusch et al, 2005).
5.2 CDER
Table 3 presents the correlation of BLEU, WER,
and CDER with human assessment. It can be
seen that CDER shows better correlation than
BLEU and WER on both corpora. On the
Chinese?English task, the smoothed BLEU score
has a higher sentence-level correlation than WER.
However, this is not the case for the Arabic?
246
Table 3: Correlation (r) between human evalua-
tion (adequacy + fluency) and automatic evalu-
ation with BLEU, WER, and CDER (NIST 2004
evaluation; sentence level).
Automatic measure Chin.?E. Arab.?E.
BLEU 0.615 0.603
WER 0.559 0.589
CDER 0.625 0.623
CDER reverseda 0.222 0.393
CDER maximumb 0.594 0.599
aCD constraints for candidate instead of reference.
bSentence-wise maximum of normal and reversed CDER
Table 4: Correlation (r) between human evalua-
tion (adequacy + fluency) and automatic evalua-
tion for CDER with different penalties.
Penalty Chin.?E. Arab.?E.
? 0.625 0.623
lplen 0.581 0.567
lpmisc 0.466 0.528
(lplen + lpmisc)/2 0.534 0.557
English task. So none of these two measures
is superior to the other one, but they are both
outperformed by CDER.
If the direction of CDER is reversed (i.e, the
CD constraints are required for the candidate
instead of the reference, such that the measure
has precision instead of recall characteristics), the
correlation with human evaluation is much lower.
Additionally we studied the use of the maxi-
mum of the distances in both directions. This has
a lower correlation than taking the original CDER,
as Table 3 shows. Nevertheless, the maximum still
performs slightly better than BLEU and WER.
5.3 Hypothesis Length and Penalties
The problem of how to avoid a preference of
overly long candidate sentences by CDER remains
unsolved, as can be found in Table 4: Each of
the proposed penalties infers a significant decrease
of correlation between the (extended) CDER and
human evaluation. Future research will aim at
finding a suitable length penalty. Especially
if CDER is applied in system development,
such a penalty will be needed, as preliminary
optimization experiments have shown.
5.4 Substitution Costs
Table 5 reveals that the inclusion of word-
dependent substitution costs yields a raise by more
than 1% absolute in the correlation of CDER
with human evaluation. The same is true for
Table 5: Correlation (r) between human evalua-
tion (adequacy + fluency) and automatic evalu-
ation for WER and CDER with word-dependent
substitution costs.
Measure Subst. costs Chin.?E. Arab.?E.
WER const (1) 0.559 0.589
prefix 0.571 0.605
Levenshtein 0.580 0.611
CDER const (1) 0.625 0.623
prefix 0.637 0.634
Levenshtein 0.638 0.637
WER: the correlation with human judgment is
increased by about 2% absolute on both language
pairs. The Levenshtein-based substitution costs
are better suited for WER than the scheme based
on common prefix length. For CDER, there is
hardly any difference between the two methods.
Experiments on five more corpora did not give any
significant evidence which of the two substitution
costs correlates better with human evaluation. But
as the prefix-based substitution costs improved
correlation more consistently across all corpora,
we employed this method in our next experiment.
5.5 Combination of CDER and PER
An interesting topic in MT evaluation research
is the question whether a linear combination of
two MT evaluation measures can improve the
correlation between automatic and human evalu-
ation. Particularly, we expected the combination
of CDER and PER to have a significantly higher
correlation with human evaluation than the mea-
sures alone. CDER (as opposed to PER) has the
ability to reward correct local ordering, whereas
PER (as opposed to CDER) penalizes overly long
candidate sentences. The two measures were
combined with linear interpolation. In order
to determine the weights, we performed data
analysis on seven different corpora. The result was
consistent across all different data collections and
language pairs: a linear combination of about 60%
CDER and 40% PER has a significantly higher
correlation with human evaluation than each of
the measures alone. For the two corpora studied
here, the results of the combination can be found
in Table 6: On the Chinese?English task, there is
an additional gain of more than 1% absolute in
correlation over CDER alone. The combined error
measure is the best method in both cases.
The last line in Table 6 shows the 95%-
confidence interval for the correlation. We see
that the new measure CDER, combined with PER,
has a significantly higher correlation with human
evaluation than the existing measures BLEU, TER,
247
Table 6: Correlation (r) between human evalua-
tion (adequacy + fluency) and automatic evalua-
tion for different automatic evaluation measures.
Automatic measure Chin.?E. Arab.?E.
BLEU 0.615 0.603
TER 0.548 0.582
WER 0.559 0.589
WER + Lev. subst. 0.580 0.611
CDER 0.625 0.623
CDER +prefix subst. 0.637 0.634
CDER +prefix+PER 0.649 0.635
95%-confidence ?0.018 ?0.028
and WER on both corpora.
6 Conclusion and Outlook
We presented CDER, a new automatic evalua-
tion measure for MT, which is based on edit
distance extended by block movements. CDER
allows for reordering blocks of words at constant
cost. Unlike previous block movement measures,
CDER can be exactly calculated in quadratic
time. Experimental evaluation on two different
translation tasks shows a significantly improved
correlation with human judgment in comparison
with state-of-the-art measures such as BLEU.
Additionally, we showed how word-dependent
substitution costs can be applied to enhance the
new error measure as well as existing approaches.
The highest correlation with human assessment
was achieved through linear interpolation of the
new CDER with PER.
Future work will aim at finding a suitable length
penalty for CDER. In addition, more sophisticated
definitions of the word-dependent substitution
costs will be investigated. Furthermore, it will
be interesting to see how this new error measure
affects system development: We expect it to
allow for a better sentence-wise error analysis.
For system optimization, preliminary experiments
have shown the need for a suitable length penalty.
Acknowledgement
This material is partly based upon work supported
by the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-
06-C-0023, and was partly funded by the Euro-
pean Union under the integrated project TC-STAR
? Technology and Corpora for Speech to Speech
Translation
References
S. Banerjee and A. Lavie. 2005. METEOR: An
automatic metric for MT evaluation with improved
correlation with human judgments. ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages
65?72, Ann Arbor, MI, Jun.
G. Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. ARPA Workshop on Human
Language Technology.
D. E. Knuth, 1993. The Stanford GraphBase: a
platform for combinatorial computing, pages 74?87.
ACM Press, New York, NY.
G. Leusch, N. Ueffing, and H. Ney. 2003. A novel
string-to-string distance measure with applications
to machine translation evaluation. MT Summit IX,
pages 240?247, New Orleans, LA, Sep.
G. Leusch, N. Ueffing, D. Vilar, and H. Ney. 2005.
Preprocessing and normalization for automatic eval-
uation of machine translation. ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages
17?24, Ann Arbor, MI, Jun.
V. I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. Soviet
Physics Doklady, 10(8):707?710, Feb.
C.-Y. Lin and F. J. Och. 2004. Orange: a
method for evaluation automatic evaluation metrics
for machine translation. COLING 2004, pages 501?
507, Geneva, Switzerland, Aug.
D. Lopresti and A. Tomkins. 1997. Block edit models
for approximate string matching. Theoretical
Computer Science, 181(1):159?179, Jul.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu.
2002. BLEU: a method for automatic evaluation
of machine translation. 40th Annual Meeting of the
ACL, pages 311?318, Philadelphia, PA, Jul.
M. Przybocki. 2004. NIST machine translation 2004
evaluation: Summary of results. DARPA Machine
Translation Evaluation Workshop, Alexandria, VA.
M. Snover, B. J. Dorr, R. Schwartz, J. Makhoul,
L. Micciulla, and R. Weischedel. 2005. A
study of translation error rate with targeted human
annotation. Technical Report LAMP-TR-126,
CS-TR-4755, UMIACS-TR-2005-58, University of
Maryland, College Park, MD.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and
H. Sawaf. 1997. Accelerated DP based search for
statistical translation. European Conf. on Speech
Communication and Technology, pages 2667?2670,
Rhodes, Greece, Sep.
J. P. Turian, L. Shen, and I. D. Melamed. 2003.
Evaluation of machine translation and its evaluation.
MT Summit IX, pages 23?28, New Orleans, LA, Sep.
248
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 17?24, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Preprocessing and Normalization
for Automatic Evaluation of Machine Translation
Gregor Leusch and Nicola Ueffing and David Vilar and Hermann Ney
Lehrstuhl fu?r Informatik VI
RWTH Aachen University
D-52056 Aachen, Germany,
{leusch,ueffing,vilar,ney}@i6.informatik.rwth-aachen.de
Abstract
Evaluation measures for machine trans-
lation depend on several common meth-
ods, such as preprocessing, tokenization,
handling of sentence boundaries, and the
choice of a reference length. In this
paper, we describe and review some
new approaches to them and compare
these to state-of-the-art methods. We
experimentally look into their impact on
four established evaluation measures. For
this purpose, we study the correlation
between automatic and human evaluation
scores on three MT evaluation corpora.
These experiments confirm that the to-
kenization method, the reference length
selection scheme, and the use of sentence
boundaries we introduce will increase the
correlation between automatic and human
evaluation scores. We find that ignoring
case information and normalizing evalu-
ator scores has a positive effect on the
sentence level correlation as well.
1 Introduction
Machine translation (MT), as any other natural lan-
guage processing (NLP) research subject, depends
on the evaluation of its results. Unfortunately,
human evaluation of MT system output is a time
consuming and expensive task. This is why auto-
matic evaluation is preferred to human evaluation in
the research community.
Over the last years, a manifold of automatic evalu-
ation measures has been proposed and studied. This
underlines the importance, but also the complexity
of finding a suitable evaluation measure for MT.
We will give a short overview of some measures in
section 2 of this paper.
Although most of these measures share similar
ideas and foundation, we observe that researchers
tend to approach problems common to several
measures differently from each other. A noteworthy
example here is the determination of a translation
reference length.
In section 3, we will have a look onto structural
similarities and differences among several measures,
focussing on common steps. We will show that
decisions taken about them can be as important to
the outcome of an evaluation, as the choice of the
evaluation measure itself.
To this end, we will study the performance
of each error measure and setting by comparison
with human evaluation on three different evaluation
tasks in section 4. These experiments will show
that sophisticated tokenization as well as adding
sentence boundaries and a good choice for the
reference lengths will improve correlation between
automatic and human evaluation significantly. Case
normalization and evaluator normalization are help-
ful only when evaluating on sentence level; system
level evaluation is not affected by these methods.
After a discussion of these results in section 5, we
will conclude this paper in section 6.
2 Automatic evaluation measures
The majority of MT evaluation approaches are based
on the distance or similarity of MT candidate output
to a set of reference translations, i.e. to sentences
which are known to be correct. The lower this
distance is, or the higher the similarity, the better the
17
candidate translations are considered to be, and thus
the better the MT system.
2.1 Evaluation measures studied
Out of the vast amount of measures, we will focus
on the following measures that are widely used in
research and in evaluation campaigns: WER, PER,
BLEU, and NIST.
Let a test set consist of k = 1, . . . ,K candidate
sentences Ek generated by an MT system. For
each candidate sentence Ek, we have a set of r =
1, . . . , Rk reference sentences E?r,k. Let Ik denote
the length, and I?k the reference length for each
sentence Ek. We will explain in section 3.3 how the
reference length is calculated.
With this, we write the total candidate length over
the corpus as I? :=
?
k Ik, and the total reference
length as I?? :=
?
k I
?
k .
Let nem1 ,k denote the count of the m-gram e
m
1
within the candidate sentence Ek; similarly let
n?em1 ,r,k denote the same count within the reference
sentence E?r,k. The total m-gram count over the
corpus is then n?m :=
?
k
?
em1 ?Ek
nem1 ,k.
2.1.1 WER
The word error rate is defined as the Levenshtein
distance dL(Ek, E?r,k) between a candidate sentence
Ek and a reference sentence E?r,k, divided by the
reference length I?k for normalization.
For a whole candidate corpus with multiple
references, we define the WER to be:
WER :=
1
I??
?
k
min
r
dL
(
Ek, E?r,k
)
Note that the WER of a single sentence can be
calculated as the WER for a corpus of size K = 1.
2.1.2 PER
The position independent error rate (Tillmann et
al., 1997) ignores the ordering of the words within
a sentence. Independent of the word position, the
minimum number of deletions, insertions, and
substitutions to transform the candidate sentence
into the reference sentence is calculated. Using
the counts ne,r, n?e,r,k of a word e in the candidate
sentence Ek, and the reference sentence E?r,k, we
can calculate this distance as
dPER
(
Ek, E?r,k
)
:=
1
2
(?
?Ik?I?k
?
?+
?
e
?
?ne,k?n?e,r,k
?
?
)
This distance is then normalized into an error rate,
the PER, as described in section 2.1.1.
A promising approach is to compare bigram or
arbitrary m-gram count vectors instead of unigram
count vectors only. This will take into account the
ordering of the words within a sentence implicitly,
although not as strong as the WER does.
2.1.3 BLEU
BLEU (Papineni et al, 2001) is a precision
measure based on m-gram count vectors. The
precision is modified such that multiple references
are combined into a single m-gram count vector,
n?e,k := maxr n?e,r,k. Multiple occurrences of an
m-gram in the candidate sentence are counted as
correct only up to the maximum occurrence count
within the reference sentences. Typically, m =
1, . . . , 4.
To avoid a bias towards short candidate sentences
consisting of ?safe guesses? only, sentences shorter
than the reference length will be penalized with a
brevity penalty.
BLEU := lpBLEU ? gm
m
{
1
sm+n?m
?
(
sm+
?
k
?
em1 ?Ek
min
(
nem1 ,k , n?em1 ,k
))
}
with the geometric mean gm and a brevity penalty
lpBLEU := min
(
1 , exp
(
1 ?
I??
I?
))
In the original BLEU definition, the smoothing
term sm is zero. To allow for sentence-wise
evaluation, Lin and Och (2004) define the BLEU-S
measure with s1 := 1 and sm>1 := 0. We have
adopted this technique for this study.
2.1.4 NIST
The NIST score (Doddington, 2002) extends
the BLEU score by taking information weights of
the m-grams into account. The NIST information
weight is defined as
Info(em1 ) := ?
(
log2 ??nem1 ? log2
??nem?11
)
with ??nem1 :=
?
k,r
n?en1 ,k,r.
Note that the weight of a phrase occurring
in many references sentence for a candidate is
considered to be lower than the weight of a phrase
occurring only once!
18
The NIST score is the sum over all information
counts of the co-occurring m-grams, summed up
separately for each m = 1, . . . , 5 and normalized
by the total m-gram count.
NIST := lpNIST ?
?
m
(
1
n?m
?
?
k
?
em1 ?Ek
min
(
nem1 ,k , n?em1 ,k
)
? Info(em1 )
)
As in BLEU, there is a brevity penalty to avoid a
bias towards short candidates:
lpNIST := exp
(
? ? log22 min
(
1 ,
I?
I??
))
where ? := ? log2 2
log22 3
Due to the information weights, the value of the
NIST score depends highly on the selection of the
reference corpus. This must be taken into account
when comparing NIST scores of different evaluation
campaigns.
2.2 Other measures
Lin and Och (2004) introduce a family of three
measures named ROUGE. ROUGE-S is a skip-
bigram F-measure. ROUGE-L and ROUGE-W are
measures based on the length of the longest common
subsequence of the sentences. ROUGE-S has a
structure similar to the bigram PER presented here.
We expect ROUGE-L and ROUGE-W to have similar
properties to WER.
In (Leusch et al, 2003), we have described
INVWER, a word error rate enhanced by block
transposition edit operations. As structure and
scores of INVWER are similar to WER, we have
omitted INVWER experiments in this paper.
3 Preprocessing and normalization
Although the general idea is clear, there are still
several details to be specified when implementing
and using an automatic evaluation measure. We are
going to investigate the following problems:
The first detail we have to state more precisely is
the term ?word? in the above formulae. A common
approach for western languages is to consider spaces
as separators of words. The role of punctuation
marks in tokenization is arguable though. A
punctuation mark can separate words, it can be part
of a word, and it can be a word of its own. Equally
it can be irrelevant at all for evaluation.
On the same lines it is to be specified whether
we consider words to be equal if they differ only
with respect to upper and lower case. For the
IWSLT evaluation, (Paul et al, 2004) give an
introduction to how the handling of punctuation
and case information may affect automatic MT
evaluation.
Also, a method to calculate the ?reference
length? must specified if there are multiple reference
sentences of different length.
Since we want to compare automatic evaluation
with human evaluation, we have to clarify some
questions about assessing human evaluation as well:
Large evaluation tasks are usually distributed to
several human evaluators. To smooth evaluation
noise, it is common practice to have each candidate
sentence evaluated by at least two human judges in-
dependently. Therefore there are several evaluation
scores for each candidate sentence. We require a
single score for each system, though. Consequently,
we have to specify how to combine the evaluator
scores into sentence scores and then the sentence
scores into a system score.
Different definitions of this will have a significant
impact on automatic and human evaluation scores.
3.1 Tokenization and punctuation
The importance of punctuation as well as the
strictness of punctuation rules depends on the
language. In most western languages, correct
punctuation can vastly improve the legibility of
texts. Marks like full stop or comma separate words.
Other marks like apostrophes and hyphens can be
used to join words, forming new words by this. For
example, the spelling ?There?s? is a contraction of
?There is?.
Similar phenomena can be found in other lan-
guages, although the set of critical characters may
vary. Even when evaluating English translations, the
candidate sentences may contain source language
parts like proper names which should thus be treated
according to the source language.
From the viewpoint of an automatic evaluation
measure, we have to decide which units we would
consider to be words of their own.
We have studied four tokenization methods. The
simplest method is keeping the original sentences,
and considering only spaces as word separators.
Moreover, we can consider all punctuation marks to
separate words but remove them completely then.
The mteval tool (Papineni, 2002) improves this
19
Table 1: Tokenization methods studied
? Original candidate
Powell said: "We?d not be
alone; that?s for sure."
? Remove punctuation
Powell said We d not be alone
that s for sure
? Tokenization of punctuation (mteval)
Powell said : " We?d not be
alone ; that?s for sure . "
? Tokenization and treatment of abbreviations
and contractions
Powell said : " we would not be
alone ; that is for sure . "
scheme by keeping all punctuation marks as separate
words except for decimal points and hyphens
joining composita. We have extended this scheme
by implementing a treatment of common English
contractions. Table 1 illustrates these methods.
3.2 Case sensitivity
In western languages, maintaining correct upper
and lower case can improve the readability of a
text. Unfortunately, though the case of a word
depends on the word class, classification is not
always unambiguous. What is more, the first word
in a sentence is always written in upper case. This
lowers the significance of case information in MT
evaluation, as even a valid reordering of words
between candidate and reference sentence may lead
to conflicting cases. Consequently, we investigated
if and how case information can be exploited for
automatic evaluation.
3.3 Reference length
Each automatic evaluation measure we have taken
into account depends on the calculation of a refer-
ence length: WER, PER, and ROUGE are normalized
by it, whereas NIST or BLEU incorporate it for
the determination of the brevity penalty. In MT
evaluation practise, there are multiple reference
sentences for each candidate sentence, with different
lengths each. It is thus not intuitively clear what the
?reference length? is.
A simple choice here is the average length of the
reference sentences. Though this is modus operandi
for NIST, it is problematic with brevity penalty or F-
measure based scores, as even candidate sentences
that are identical to a shorter-than-average reference
sentence ? which we would intuitively consider to be
?optimal? ? will then receive a sub-optimal score.
BLEU incorporates a different method for the
determination of the reference length in its default
implementation: Reference length here is the
reference sentence length which is closest to the
candidate length. If there is more than one the
shortest of them is chosen.
For measures based on the comparison of single
sentences such as WER, PER, and ROUGE, at least
two more methods deserve consideration:
? The average length of the sentences with the
lowest absolute distance or highest similarity
to the candidate sentence. We call this method
?average nearest-sentence length?.
? The length of the sentence with the lowest
relative error rate or the highest relative
similarity. We call this method ?best length?.
Note that when using this method, not the
minimum absolute distance is used for the
error rate, but the distance that leads to
minimum relative error.
Other strategies studied by us, e.g. minimum
length of the reference sentences, did not show
any theoretical or experimental advantage over the
methods mentioned here. Thus we will not discuss
them in this paper.
3.4 Sentence boundaries
The position of a word within a sentence can be quite
significant for the correctness of the sentence.
WER, INVWER, and ROUGE-L take into account
the ordering explicitly. This is not the case with n-
PER, BLEU, or NIST, although the positions of inner
words are regarded implicitly by m-gram overlap.
To model the position of words at the initial or the
end of a sentence, one can enclose the sentence with
artificial sentence boundary words. Although this
is a common approach in language modelling, it
has to our knowledge not yet been applied to MT
evaluation.
3.5 Evaluator normalization
For human evaluation, it has to be specified how to
handle evaluator bias, and how to combine sentence
scores into system scores.
Regarding evaluator bias, even accurate evalua-
tion guidelines will not prevent a measurable dis-
crepancy between the scores assigned by different
human evaluators.
The 2003 TIDES/MT evaluation may serve as
an example here: Since the candidate sentences of
20
54321
0.0
0.2
0.4
0.6
0.8
1.0
Rel
ativ
e as
sess
men
t co
unt
Evaluator
Figure 1: Distribution of adequacy assessments for
each human evaluator. TIDES CE corpus.
the participating systems were randomly distributed
among ten human evaluators, one would expect the
assessed scores to be independent of the evaluator.
Figure 1 indicates that this is indeed not the case,
as the evaluators can clearly be distinguished by the
amount of good and bad marks they assessed.
(0, 1) evaluator normalization overcomes this
bias: For each human evaluator the average sentence
score given by him or her and its variance are
calculated. These assignments are then normalized
to (0, 1) expectation and standard deviation (Dod-
dington, 2003), separately for each evaluator.
Evaluator normalization should be unnecessary
for system evaluation, as the evaluator biases
tend to cancel out over the large amount of
candidate sentences if the alignment of evaluators
and systems is random enough. Moreover, with
(0, 1) normalization the calculated system scores are
relative, not absolute scores. As such they can only
be compared with scores out of the same evaluation.
Whereas the assessments by the human evaluators
are given on the sentence level, our interest may
lie on the evaluation of whole candidate systems.
Depending on the number of assessments per
candidate sentence, different combination methods
for the sentence scores can be considered for this,
e.g. mean or median. As our data consisted only
of two or three human assessments per sentence, we
have only applied the mean in our experiments.
It has to be defined how a system score is
calculated from the sentence scores. All of the
automatic evaluation measures implicitly weight the
candidate sentences by their length. Consequently,
we applied for the human evaluation scores a
weighting by length on sentence level as well.
Table 2: Corpus statistics
TIDES CE TIDES AE BTEC CE
Source language Chinese Arabic Chinese
Target language English English English
Sentences 919 663 500
Running words 25784 17763 3632
Punctuation marks 3760 2698 610
Ref. translations 4 4 16
Avg. ref. length 28.1 26.8 7.3
Candidate systems 7 6 11
4 Experimental results
To assess the impact of the mentioned preprocessing
steps, we calculated scores for several automatic
evaluation measures with varying preprocessing,
reference length calculation, etc. on three eval-
uation test sets from international MT evaluation
campaigns. We then compared these automatic eval-
uation results with human evaluation of adequacy
and fluency by determining a correlation coefficient
between human and automatic evaluation. We
chose Pearson?s r for this. Although all evaluation
measures were calculated using length weighting,
we did not do any weighting when calculating the
sentence level correlation.
Regarding the m-gram PER, we had studied m-
gram lengths of up to 8 both separately and in com-
bination with shorter m-gram lengths in previous
experiments. However, an m-gram length of greater
than 4 did not show noteworthy correlation. For this,
we will leave out these results in this paper.
For the sake of clarity, we will also leave
out measures that behave very similarly to akin
measures e.g. INVWER and WER, 2-PER and 1-
PER, or BLEU and BLEU-S.
Since WER and PER are error measures, whereas
BLEU and NIST are similarity measures, the
correlation coefficients with human evaluation will
have opposite signs. For convenience, we will look
at the absolute coefficients only.
4.1 Corpora
From the 2003 TIDES evaluation campaign we
included both the Chinese-English and the Arabic-
English test corpus in our experiments. Both were
provided with adequacy and fluency scores between
1 and 5 for seven and six candidate sets respectively.
As we wanted to perform experiments on a corpus
with a larger amount of MT systems, we also
included the IWSLT BTEC 2004 Chinese-English
21
evaluation (Akiba et al, 2004). We restricted our
experiments to the eleven MT systems that had been
trained on a common training corpus.
Corpus statistics can be found in table 2.
4.2 Experimental baseline
In our first experiment we studied the correlation
of the different evaluation measures with human
evaluation at ?baseline? conditions. These included
no sentence boundaries, but tokenization with
treatment of abbreviations, see table 1. For
sentence evaluation, conditions included evaluator
normalization. Case information was removed. We
used these settings in the other experiments, too, if
not stated otherwise.
Figure 2 shows the correlation between automatic
and human scores. On the TIDES corpora the
system level correlation is particularly high, at a
moderate sentence level correlation. We assume
the latter is due to the poor sentence inter-annotator
agreement on these corpora, which is then smoothed
out on system level. On the BTEC corpus
a high sentence level correlation accompanies a
significantly lower system level correlation. Note
that due to the much lower number of samples on
the system level (e.g. 5 vs. 5500), small changes
in the sentence level correlation are more likely to
be significant than such changes on system level.
We have verified these effects by inspecting the rank
correlation on both levels, as well as by experiments
on other corpora. Although these experiments
support our findings, we have omitted results here
WERPER BLEUSNIST l AdequacyFluency
TIDESCE TIDESAE BTECCE0.0
0.2
0.4
0.6
l l l l
l l l l
l
l
l
l
TIDESCE TIDESAE BTECCE0.0
0.20
.40.6
0.81
.0
l l l l
l l l l l
l
l
l
Figure 2: Pearson correlation coefficient between
automatic and human evaluation. Bars indicate
correlation with adequacy, circles with fluency
score.
Left: sentence, right: system level correlation.
W WERP PER B BLEUS ll no normalizationnormalization
0.0
0.2
0.4
0.6
l l l l l
l l l l l l l
TIDESCE TIDESAE
W P B W P B
0.00
.20.4
0.60
.81.0 l l l l l l l l l l l l
TIDESCE TIDESAE
W P B W P B
Figure 3: Effect of evaluator normalization.
Left: sentence, right: system level correlation.
W WERP PER B BLEUS ll use caseignore case
0.0
0.2
0.4
0.6
ll ll ll
ll ll l
l l
l
l
l l
l
TIDESCE TIDESAE BTECCE
W P B W P B W P B
0.00
.20.4
0.60
.81.0 l
l
ll
ll
ll ll ll ll
ll
ll
TIDESCE TIDESAE BTECCE
W P B W P B W P B
Figure 4: Effect of case normalization.
Left: sentence, right: system level correlation.
for the sake of clarity.
4.3 Evaluator normalization
We studied the effect of (0, 1)-normalization of
scores assigned by human evaluators. The NIST
measure showed a behavior very similar to that of
the other measures and is thus left out in the graph.
The correlation of all automatic measures both with
fluency and with adequacy increases significantly
at sentence level (figure 3). We do not notice a
positive effect on system level, which confirms the
assumption stated in section 3.5.
4.4 Tokenization and case normalization
The impact of case information was analyzed in our
next experiment. Figure 4 (again without the NIST
measure as it shows a similar behavior to the other
measures) indicates that it is advisable to disregard
case information when looking into adequacy on
sentence level. Surprisingly, this also holds for
22
W WERB BLEUS l AdequacyFluency ll llkeepremove tokenizetok+treat.
0.0
0.2
0.4
0.6
llll llll lll
l llll
TIDESCE TIDESAE
W B W B
0.00
.20.4
0.60
.81.0 llll llll llll llll
TIDESCE TIDESAE
W B W B
Figure 5: Effect of different tokenization steps.
Left: sentence, right: system level correlation.
fluency. We do no find a clear tendency on whether
or not to regard case information at system level.
Figure 5 indicates that the way of handling
punctuation we proposed does pay off when eval-
uating adequacy. For fluency our results were
contradictory: A slight decrease on the Arabic-
English corpus is accompanied by a slight decay on
the Chinese-English corpus. We did not investigate
the BTEC corpus here as most systems sticked to the
tokenization guidelines for this evaluation.
4.5 Reference length
The dependency of evaluation measures on the
selection of reference lengths is rarely covered in
the literature. However, as we can see in figure 6,
our experiments indicate a significant impact. The
selected three methods here are the default for
WER/PER, NIST, and BLEU, respectively. For the
distance based evaluation measures, represented by
W WERB BLEUS N NIST ll laveragenearest best
0.0
0.2
0.4
0.6
lllll
 
ll
 
lllll
 
ll
 
lllll
 
l
l
 TIDESCE TIDESAE BTECCE
W B N W B N W B N
0.00
.20.
40.6
0.81
.0
lllll
 
ll
 
lllll
 
ll
 
lllll
 
l
l
 TIDESCE TIDESAE BTECCE
W B N W B N W B N
Figure 6: Effect of different reference lengths.
Left: sentence, right: system level correlation.
P 2PERB BLEUS N NIST ll llnoneinitial endboth
0.0
0.2
0.4
0.6
llllllllllll lll
lllllllll
TIDESCE TIDESAE
P B N P B N
0.00
.20.4
0.60
.81.0 llllllllllll llllllllllll
TIDESCE TIDESAE
P B N P B N
Figure 7: Effect of sentence boundaries.
Left: sentence, right: system level correlation.
WER here, taking the length of the sentence leading
to the best score leads to the best correlation with
both fluency and adequacy. Taking the average
length instead seems to be the worst choice.
For brevity penalty based measures, the effect is
not as clear: On both TIDES corpora there is no
significant difference in correlation between using
the average length and the nearest length. On
the BTEC corpus, choosing the nearest sentence
length leads to a significantly higher correlation than
choosing the average length. We assume this is due
to the high number of reference sentences on this
corpus.
4.6 Sentence boundaries
As sentence boundaries will only influence m-gram
count vector based measures, we have restricted
our experiments to bigram PER, BLEU-S, and NIST
here. Including sentence boundaries (figure 7)
has a positive effect on correlation with fluency
and adequacy for both bigram PER and BLEU-S.
Sentence initials seem to be more important than
sentence ends here. For the NIST measure, we do
not find any significant effect.
5 Discussion
In a perfect MT world, any dependency of an
evaluation on case information or tokenization
should be inexistent, as MT systems already have
to deal with both in the translation process, and
could be designed to produce output according to
evaluation campaign guidelines. Once all translation
systems stick to the same specifications, no further
preprocessing steps should be necessary.
In practice there will be some systems that step
23
out of line. If we then choose strict rules regarding
case information and punctuation, automatic error
measures will penalize these systems rather hard,
whereas penalty is rather low if we choose lax ones.
In this situation case information will have a
large effect on the correlation between automatic
and human evaluation, depending on whether the
involved candidate systems will have a good or a bad
human evaluation. It is vital to keep this in mind
when drawing conclusions here regarding system
evaluation, despite the obvious importance of case
information in natural languages.
These considerations also hold for the treatment
of punctuation marks, as a special care should be
unnecessary if all systems sticked to tokenization
specifications. In practise, MT systems differ
in the way they generate and handle punctuation
marks. Therefore, appropriate preprocessing steps
are advisable.
Our experiments suggest that sentence boundaries
increase correlation between automatic scores and
adequacy both on sentence and on system level.
For fluency, the improvement is less significant, and
mainly depends on the sentence initials.
For length penalty based measures, we have found
that choosing the nearest sentence length yields the
highest correlation with human evaluation. For
distance based measures instead, it seems advisable
to choose the sentence that leads to the best relative
score as the one that determines the reference length.
6 Conclusion
We have described several MT evaluation measures.
We have pointed out common preprocessing steps
and auxiliary methods which have not been studied
in detail so far in spite of their importance for
the MT evaluation process. Particularly, we have
introduced a novel method for determining the
reference length of an evaluation candidate sentence,
and a simple method to incorporate sentence
boundary information to m-gram based evaluation
measures.
We then have performed several experiments
on these methods on three evaluation corpora.
The results indicate that both our new reference
length algorithm and the use of sentence boundaries
improve the correlation of the studied automatic
evaluation measures with human evaluation. Fur-
thermore, we have learned that case information
should be removed when performing automatic
sentence evaluation. On sentence level, evaluator
normalization can improve the correlation between
automatic and human evaluation.
Acknowledgements
This work was partially funded by the Deutsche
Forschungsgemeinschaft (DFG) under the project
?Statistische Textu?bersetzung? (Ne572/5) and by the
European Union under the integrated project TC-
STAR ? Technology and Corpora for Speech to
Speech Translation (IST-2002-FP6-506738).
References
Y. Akiba, M. Federico, N. Kando, H. Nakaiwa, M. Paul,
and J. Tsujii. 2004. Overview of the IWSLT04
evaluation campaign. In Proc. IWSLT, pp. 1?12,
Kyoto, Japan, September.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proc. ARPA Workshop on Human Language
Technology.
G. Doddington. 2003. NIST MT Evaluation Workshop.
Personal communication, July.
G. Leusch, N. Ueffing, and H. Ney. 2003. A novel
string-to-string distance measure with applications to
machine translation evaluation. In Proc. MT Summit
IX, pp. 240?247, New Orleans, LA, September.
C. Y. Lin and F. J. Och. 2004. Orange: a method for
evaluation automatic evaluation metrics for machine
translation. In Proc. COLING 2004, pp. 501?507,
Geneva, Switzerland, August.
K. A. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical Report RC22176 (W0109-022),
IBM Research Division, Thomas J. Watson Research
Center, September.
K. A. Papineni. 2002. The NIST mteval scor-
ing software. http://www.itl.nist.gov/iad/
894.01/tests/mt/resources/scoring.htm.
M. Paul, H. Nakaiwa, and M. Federico. 2004. Towards
innovative evaluation methodologies for speech trans-
lation. In Working Notes of the NTCIR-4 Meeting,
volume 2, pp. 17?21.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and
H. Sawaf. 1997. Accelerated DP based search for
statistical translation. In European Conf. on Speech
Communication and Technology, pp. 2667?2670,
Rhodes, Greece, September.
24
Proceedings of the Second Workshop on Statistical Machine Translation, pages 96?103,
Prague, June 2007. c?2007 Association for Computational Linguistics
Human Evaluation of Machine Translation Through Binary System
Comparisons
David Vilar, Gregor Leusch
and Hermann Ney
Lehrstuhl fu?r Informatik 6
RWTH Aachen University
D-52056 Aachen, Germany
{vilar,leusch,ney}@cs.rwth-aachen.de
Rafael E. Banchs
D. of Signal Theory and Communications
Universitat Polite`cnica de Catalunya
08034 Barcelona, Spain
rbanchs@gps.tsc.upc.edu
Abstract
We introduce a novel evaluation scheme for
the human evaluation of different machine
translation systems. Our method is based
on direct comparison of two sentences at a
time by human judges. These binary judg-
ments are then used to decide between all
possible rankings of the systems. The ad-
vantages of this new method are the lower
dependency on extensive evaluation guide-
lines, and a tighter focus on a typical eval-
uation task, namely the ranking of systems.
Furthermore we argue that machine transla-
tion evaluations should be regarded as sta-
tistical processes, both for human and au-
tomatic evaluation. We show how confi-
dence ranges for state-of-the-art evaluation
measures such as WER and TER can be
computed accurately and efficiently without
having to resort to Monte Carlo estimates.
We give an example of our new evaluation
scheme, as well as a comparison with classi-
cal automatic and human evaluation on data
from a recent international evaluation cam-
paign.
1 Introduction
Evaluation of machine translation (MT) output is a
difficult and still open problem. As in other natu-
ral language processing tasks, automatic measures
which try to asses the quality of the translation
can be computed. The most widely known are the
Word Error Rate (WER), the Position independent
word Error Rate (PER), the NIST score (Dodding-
ton, 2002) and, especially in recent years, the BLEU
score (Papineni et al, 2002) and the Translation Er-
ror Rate (TER) (Snover et al, 2005). All of the-
ses measures compare the system output with one
or more gold standard references and produce a nu-
merical value (score or error rate) which measures
the similarity between the machine translation and a
human produced one. Once such reference transla-
tions are available, the evaluation can be carried out
in a quick, efficient and reproducible manner.
However, automatic measures also have big dis-
advantages; (Callison-Burch et al, 2006) describes
some of them. A major problem is that a given sen-
tence in one language can have several correct trans-
lations in another language and thus, the measure of
similarity with one or even a small amount of ref-
erence translations will never be flexible enough to
truly reflect the wide range of correct possibilities of
a translation. 1 This holds in particular for long sen-
tences and wide- or open-domain tasks like the ones
dealt with in current MT projects and evaluations.
If the actual quality of a translation in terms of
usefulness for human users is to be evaluated, human
evaluation needs to be carried out. This is however
a costly and very time-consuming process. In this
work we present a novel approach to human evalu-
ation that simplifies the task for human judges. In-
stead of having to assign numerical scores to each
sentence to be evaluated, as is done in current evalu-
ation procedures, human judges choose the best one
out of two candidate translations. We show how this
method can be used to rank an arbitrary number of
systems and present a detailed analysis of the statis-
tical significance of the method.
1Compare this with speech recognition, where apart from
orthographic variance there is only one correct reference.
96
2 State-of-the-art
The standard procedure for carrying out a human
evaluation of machine translation output is based on
the manual scoring of each sentence with two nu-
merical values between 1 and 5. The first one mea-
sures the fluency of the sentence, that is its readabil-
ity and understandability. This is a monolingual fea-
ture which does not take the source sentence into
account. The second one reflects the adequacy, that
is whether the translated sentence is a correct trans-
lation of the original sentence in the sense that the
meaning is transferred. Since humans will be the
end users of the generated output,2 it can be ex-
pected that these human-produced measures will re-
flect the usability and appropriateness of MT output
better than any automatic measure.
This kind of human evaluation has however addi-
tional problems. It is much more time consuming
than the automatic evaluation, and because it is sub-
jective, results are not reproducible, even from the
same group of evaluators. Furthermore, there can
be biases among the human judges. Large amounts
of sentences must therefore be evaluated and proce-
dures like evaluation normalization must be carried
out before significant conclusions from the evalua-
tion can be drawn. Another important drawback,
which is also one of the causes of the aforemen-
tioned problems, is that it is very difficult to define
the meaning of the numerical scores precisely. Even
if human judges have explicit evaluation guidelines
at hand, they still find it difficult to assign a numeri-
cal value which represents the quality of the transla-
tion for many sentences (Koehn and Monz, 2006).
In this paper we present an alternative to this eval-
uation scheme. Our method starts from the obser-
vation that normally the final objective of a human
evaluation is to find a ?ranking? of different systems,
and the absolute score for each system is not relevant
(and it can even not be comparable between differ-
ent evaluations). We focus on a method that aims to
simplify the task of the judges and allows to rank the
systems according to their translation quality.
3 Binary System Comparisons
The main idea of our method relies in the fact
that a human evaluator, when presented two differ-
ent translations of the same sentence, can normally
choose the best one out of them in a more or less
2With the exception of cross-language information retrieval
and similar tasks.
definite way. In social sciences, a similar method
has been proposed by (Thurstone, 1927).
3.1 Comparison of Two Systems
For the comparison of two MT systems, a set of
translated sentence pairs is selected. Each of these
pairs consists of the translations of a particular
source sentence from the two systems. The human
judge is then asked to select the ?best? translation of
these two, or to mark the translations to be equally
good. We are aware that the definition of ?best? here
is fuzzy. In our experiments, we made a point of not
giving the evaluators explicit guidelines on how to
decide between both translations. As a consequence,
the judges were not to make a distinction between
fluency and adequacy of the translation. This has a
two-fold purpose: on the one hand it simplifies the
decision procedure for the judges, as in most of the
cases the decision is quite natural and they do not
need to think explicitly in terms of fluency and ade-
quacy. On the other hand, one should keep in mind
that the final goal of an MT system is its usefulness
for a human user, which is why we do not want to
impose artificial constraints on the evaluation proce-
dure. If only certain quality aspects of the systems
are relevant for the ranking, for example if we want
to focus on the fluency of the translations, explicit
guidelines can be given to the judges. If the evalua-
tors are bilingual they can use the original sentences
to judge whether the information was preserved in
the translation.
After our experiment, the human judges provided
feedback on the evaluation process. We learned
that the evaluators normally selected the translation
which preserved most of the information from the
original sentence. Thus, we expect to have a slight
preference for adequacy over fluency in this evalu-
ation process. Note however that adequacy and flu-
ency have shown a high correlation3 in previous ex-
periments. This can be explained by noting that a
low fluency renders the text incomprehensible and
thus the adequacy score will also be low.
The difference in the amount of selected sen-
tences of each system is an indicator of the differ-
ence in quality between the systems. Statistics can
be carried out in order to decide whether this differ-
ence is statistically significant; we will describe this
in more detail in Section 3.4.
3At least for ?sensible? translation systems. Academic
counter-examples could easily be constructed.
97
3.2 Evaluation of Multiple Systems
We can generalize our method to find a ranking of
several systems as follows: In this setting, we have
a set of n systems. Furthermore, we have defined an
order relationship ?is better than? between pairs of
these systems. Our goal now is to find an ordering
of the systems, such that each system is better than
its predecessor. In other words, this is just a sorting
problem ? as widely known in computer science.
Several efficient sorting algorithms can be found
in the literature. Generally, the efficiency of sort-
ing algorithms is measured in terms of the number
of comparisons carried out. State-of-the-art sort-
ing algorithms have a worst-case running time of
O(n log n), where n is the number of elements to
sort. In our case, because such binary comparisons
are very time consuming, we want to minimize the
absolute number of comparisons needed. This mini-
mization should be carried out in the strict sense, not
just in an asymptotic manner.
(Knuth, 1973) discusses this issue in detail. It is
relatively straightforward to show that, in the worst
case, the minimum number of comparisons to be
carried out to sort n elements is at least dlog n!e
(for which n log n is an approximation). It is not
always possible to reach this minimum, however, as
was proven e.g. for the case n = 12 in (Wells, 1971)
and for n = 13 in (Peczarski, 2002). (Ford Jr and
Johnson, 1959) propose an algorithm called merge
insertion which comes very close to the theoretical
limit. This algorithm is sketched in Figure 1. There
are also algorithms with a better asymptotic runtime
(Bui and Thanh, 1985), but they only take effect for
values of n too large for our purposes (e.g., more
than 100). Thus, using the algorithm from Figure 1
we can obtain the ordering of the systems with a
(nearly) optimal number of comparisons.
3.3 Further Considerations
In Section 3.1 we described how to carry out the
comparison between two systems when there is only
one human judge carrying out this comparison. The
comparison of systems is a very time consuming
task. Therefore it is hardly possible for one judge
to carry out the evaluation on a whole test corpus.
Usually, subsets of these test corpora are selected
for human evaluations instead. In order to obtain
a better coverage of the test corpus, but also to try
to alleviate the possible bias of a single evaluator, it
is advantageous to have several evaluators carrying
out the comparison between two systems. However,
there are two points that must be considered.
The first one is the selection of sentences each hu-
man judge should evaluate. Assume that we have al-
ready decided the amount of sentences m each eval-
uator has to work with (in our case m = 100). One
possibility is that all human judges evaluate the same
set of sentences, which presumably will cancel pos-
sible biases of the evaluators. A second possibility is
to give each judge a disjunct set of sentences. In this
way we benefit from a higher coverage of the corpus,
but do not have an explicit bias compensation.
In our experiments, we decided for a middle
course: Each evaluator receives a randomly selected
set of sentences. There are no restrictions on the se-
lection process. This implicitly produces some over-
lap while at the same time allowing for a larger set
of sentences to be evaluated. To maintain the same
conditions for each comparison, we also decided
that each human judge should evaluate the same set
of sentences for each system pair.
The other point to consider is how the evaluation
results of each of the human judges should be com-
bined into a decision for the whole system. One
possibility would be to take only a ?majority vote?
among the evaluators to decide which system is the
best. By doing this, however, possible quantitative
information on the quality difference of the systems
is not taken into account. Consequently, the output is
strongly influenced by statistical fluctuations of the
data and/or of the selected set of sentences to eval-
uate. Thus, in order to combine the evaluations we
just summed over all decisions to get a total count of
sentences for each system.
3.4 Statistical Significance
The evaluation of MT systems by evaluating trans-
lations of test sentences ? be it automatic evaluation
or human evaluation ? must always be regarded as
a statistical process: Whereas the outcome, or score
R, of an evaluation is considered to hold for ?all?
possible sentences from a given domain, a test cor-
pus naturally consists of only a sample from all these
sentences. Consequently, R depends on that sam-
ple of test sentences. Furthermore, both a human
evaluation score and an automatic evaluation score
for a hypothesis sentence are by itself noisy: Hu-
man evaluation is subjective, and as such is subject
to ?human noise?, as described in Section 2. Each
automatic score, on the other hand, depends heavily
on the ambiguous selection of reference translations.
Accordingly, evaluation scores underly a probability
98
1. Make pairwise comparisons of bn/2c disjoint pairs of elements. (If n is odd, leave one element out).
2. Sort the bn/2c larger elements found in step 1, recursively by merge insertion.
3. Name the bn/2c elements found in step 2 a1, a2, . . . , abn/2c and the rest b1, b2, . . . , bdn/2e, such that
a1 ? a2 ? ? ? ? ? abn/2c and bi ? ai for 1 ? i ? bn/2c. Call b1 and the a?s the ?main chain?.
4. Insert the remaining b?s into the main chain, using binary insertion, in the following order (ignore the
bj such that j > dn/2e): b3, b2; b5, b4; b11, . . . , b6; . . . ; btk , . . . , btk?1+1; . . . with tk =
2k+1+(?1)k
3 .
Figure 1: The merge insertion algorithm as presented in (Knuth, 1973).
distribution, and each evaluation result we achieve
must be considered as a sample from that distribu-
tion. Consequently, both human and automatic eval-
uation results must undergo statistical analysis be-
fore conclusions can be drawn from them.
A typical application of MT evaluation ? for ex-
ample in the method described in this paper ? is to
decide whether a given MT system X , represented
by a set of translated sentences, is significantly better
than another system Y with respect to a given eval-
uation measure. This outcome is traditionally called
the alternative hypothesis. The opposite outcome,
namely that the two systems are equal, is known
as the null hypothesis. We say that certain values
of RX , RY confirm the alternative hypothesis if the
null hypothesis can be rejected with a given level
of certainty, e.g. 95%. In the case of comparing
two MT systems, the null hypothesis would be ?both
systems are equal with regard to the evaluation mea-
sure; that is, both evaluation scoresR, R? come from
the same distribution R0?.
As R is randomly distributed, it has an expecta-
tion E[R] and a standard error se[R]. Assuming a
normal distribution for R, we can reject the null hy-
pothesis with a confidence of 95% if the sampled
score R is more than 1.96 times the standard error
away from the null hypothesis expectation:
R significant ? |E[R0] ? R| > 1.96 se[R0] (1)
The question we have to solve is: How can we es-
timate E[R0] and se[R0]? The first step is that we
consider R and R0 to share the same standard error
se[R0] = se[R]. This value can then be estimated
from the test data. In a second step, we give an es-
timate for E[R0], either inherent in the evaluation
measure (see below), or from the estimate for the
comparison system R?.
A universal estimation method is the bootstrap
estimate: The core idea is to create replications of
R by random sampling from the data set (Bisani
and Ney, 2004). Bootstrapping is generally possi-
ble for all evaluation measures. With a high number
of replicates, se[R] and E[R0] can be estimated with
satisfactory precision.
For a certain class of evaluation measures, these
parameters can be estimated more accurately and ef-
ficiently from the evaluation data without resorting
to Monte Carlo estimates. This is the class of er-
rors based on the arithmetic mean over a sentence-
wise score: In our binary comparison experiments,
each judge was given hypothesis translations ei,X ,
ei,Y . She could then judge ei,X to be better than,
equal to, or worse than ei,Y . All these judgments
were counted over the systems. We define a sentence
score ri,X,Y for this evaluation method as follows:
ri,X,Y :=
?
??
??
+1 ei,X is better than ei,Y
0 ei,X is equal to ei,Y
?1 ei,X is worse than ei,Y
. (2)
Then, the total evaluation score for a binary com-
parison of systems X and Y is
RX,Y :=
1
m
m?
i=1
ri,X,Y , (3)
with m the number of evaluated sentences.
For this case, namelyR being an arithmetic mean,
(Efron and Tibshirani, 1993) gives an explicit for-
mula for the estimated standard error of the score
RX,Y . To simplify the notation, we will use R in-
stead of RX,Y from now on, and ri instead of ri,X,Y .
se[R] =
1
m ? 1
?
?
?
?
m?
i=1
(ri ? R)2 . (4)
With x denoting the number of sentences where
ri = 1, and y denoting the number of sentences
99
where ri = ?1,
R =
x ? y
m
(5)
and with basic algebra
se[R] =
1
m ? 1
?
x + y ?
(x ? y)2
m
. (6)
Moreover, we can explicitly give an estimate for
E[R0]: The null hypothesis is that both systems are
?equally good?. Then, we should expect as many
sentences where X is better than Y as vice versa,
i.e. x = y. Thus, E[R0] = 0.
Using Equation 4, we calculate se[R] and thus a
significance range for adequacy and fluency judg-
ments. When comparing two systems X and Y ,
we assume for the null hypothesis that se[R0] =
se[RX ] and E[R0] = E[RY ] (or vice versa).
A very useful (and to our knowledge new) result
for MT evaluation is that se[R] can also be explic-
itly estimated for weighted means ? such as WER,
PER, and TER. These measures are defined as fol-
lows: Let di, i = 1, . . . ,m denote the number of ?er-
rors? (edit operations) of the translation candidate ei
with regard to a reference translation with length li.
Then, the total error rate will be computed as
R :=
1
L
m?
i=1
di (7)
where
L :=
m?
i=1
li (8)
As a result, each sentence ei affects the overall score
with weight li ? the effect of leaving out a sen-
tence with length 40 is four times higher than that
of leaving out one with length 10. Consequently,
these weights must be considered when estimating
the standard error of R:
se[R] =
?
?
?
? 1
(m ? 1)(L ? 1)
m?
i=1
(
di
li
? R
)2
? li
(9)
With this Equation, Monte-Carlo-estimates are no
longer necessary for examining the significance of
WER, PER, TER, etc. Unfortunately, we do not ex-
pect such a short explicit formula to exist for the
standard BLEU score. Still, a confidence range
for BLEU can be estimated by bootstrapping (Och,
2003; Zhang and Vogel, 2004).
Spanish English
Train Sentences 1.2M
Words 32M 31M
Vocabulary 159K 111K
Singletons 63K 46K
Test Sentences 1 117
Words 26K
OOV Words 72
Table 1: Statistics of the EPPS Corpus.
4 Evaluation Setup
The evaluation procedure was carried out on the data
generated in the second evaluation campaign of the
TC-STAR project4. The goal of this project is to
build a speech-to-speech translation system that can
deal with real life data. Three translation directions
are dealt with in the project: Spanish to English, En-
glish to Spanish and Chinese to English. For the sys-
tem comparison we concentrated only in the English
to Spanish direction.
The corpus for the Spanish?English language pair
consists of the official version of the speeches held in
the European Parliament Plenary Sessions (EPPS),
as available on the web page of the European Parlia-
ment. A more detailed description of the EPPS data
can be found in (Vilar et al, 2005). Table 1 shows
the statistics of the corpus.
A total of 9 different MT systems participated in
this condition in the evaluation campaign that took
place in February 2006. We selected five representa-
tive systems for our study. Henceforth we shall refer
to these systems as System A through System E. We
restricted the number of systems in order to keep the
evaluation effort manageable for a first experimental
setup to test the feasibility of our method. The rank-
ing of 5 systems can be carried out with as few as 7
comparisons, but the ranking of 9 systems requires
19 comparisons.
5 Evaluation Results
Seven human bilingual evaluators (6 native speakers
and one near-native speaker of Spanish) carried out
the evaluation. 100 sentences were randomly cho-
sen and assigned to each of the evaluators for every
system comparison, as discussed in Section 3.3. The
results can be seen in Table 2 and Figure 2. Counts
4http://www.tc-star.org/
100
0 10 20 30 40 50 60 70
0
10
20
30
40
50
60
70
l
l
l
l
ll
l
# "First system better"
# "S
eco
nd s
yste
m b
ette
r" l
B?AD?CA?CE?AE?BB?DD?A
(a) Each judge.
0 100 200 300 400
0
100
200
300
400
# "First system better"
# "S
eco
nd s
yste
m b
ette
r"
lB?AD?C
A?C E?A
E?B
B?D D?A
(b) All judges.
Figure 2: Results of the binary comparisons. Number of times the winning system was really judged ?better?
vs. number of times it was judged ?worse?. Results in hatched area can not reject null hypothesis, i.e. would
be considered insignificant.
missing to 100 and 700 respectively denote ?same
quality? decisions.
As can be seen from the results, in most of the
cases the judges clearly favor one of the systems.
The most notable exception is found when compar-
ing systems A and C, where a difference of only 3
sentences is clearly not enough to decide between
the two. Thus, the two bottom positions in the final
ranking could be swapped.
Figure 2(a) shows the outcome for the binary
comparisons separately for each judge, together with
an analysis of the statistical significance of the re-
sults. As can be seen, the number of samples (100)
would have been too low to show significant re-
sults in many experiments (data points in the hatched
area). In some cases, the evaluator even judged bet-
ter the system which was scored to be worse by the
majority of the other evaluators (data points above
the bisector). As Figure 2(b) shows, ?the only thing
better than data is more data?: When we summarize
R over all judges, we see a significant difference
(with a confidence of 95%) at all comparisons but
two (A vs. C, and E vs. B). It is interesting to note
that exactly these two pairs do not show a significant
difference when using a majority vote strategy.
Table 3 shows also the standard evaluation met-
rics. Three BLEU scores are given in this table, the
one computed on the whole corpus, the one com-
puted on the set used for standard adequacy and flu-
ency computations and the ones on the set we se-
lected for this task5. It can be seen that the BLEU
scores are consistent across all data subsets. In this
case the ranking according to this automatic measure
matches exactly the ranking found by our method.
When comparing with the adequacy and fluency
scores, however, the ranking of the systems changes
considerably: B D E C A. However, the difference
between the three top systems is quite small. This
can be seen in Figure 3, which shows some auto-
matic and human scores for the five systems in our
experiments, along with the estimated 95% confi-
dence range. The bigger difference is found when
comparing the bottom systems, namely System A
and System C. While our method produces nearly
no difference the adequacy and fluency scores indi-
cate System C as clearly superior to System A. It is
worth noting that the both groups use quite different
translation approaches (statistical vs. rule-based).
5Regretfully these two last sets were not the same. This is
due to the fact that the ?AF Test Set? was further used for eval-
uating Text-to-Speech systems, and thus a targeted subset of
sentences was selected.
101
Sys E1 E2 E3 E4 E5 E6 E7
?
A 29 19 38 17 32 29 41 205
B 40 59 48 53 63 64 45 372
C 32 22 29 23 32 34 42 214
D 39 61 59 50 64 58 46 377
A 32 31 31 31 47 38 40 250
C 37 29 32 22 39 45 43 247
A 36 28 17 28 34 37 31 211
E 41 47 44 43 53 45 58 331
B 26 29 18 24 43 36 33 209
E 34 33 28 27 32 29 43 226
B 34 28 30 31 40 41 48 252
D 23 17 23 17 24 28 38 170
A 36 14 27 9 31 30 34 181
D 34 50 40 50 57 61 57 349
Final ranking (best?worst): E B D A C
Table 2: Result of the binary system comparison.
Numbers of sentences for which each system was
judged better by each evaluator (E1-E7).
Subset: Whole A+F Binary
Sys BLEU BLEU A F BLEU
A 36.3 36.2 2.93 2.46 36.3
B 49.4 49.3 3.74 3.58 49.2
C 36.3 36.2 3.53 3.31 36.1
D 48.2 46.8 3.68 3.48 47.7
E 49.8 49.6 3.67 3.46 49.4
Table 3: BLEU scores and Adequacy and Fluency
scores for the different systems and subsets of the
whole test set. BLEU values in %, Adequacy (A)
and Fluency (F) from 1 (worst) to 5 (best).
6 Discussion
In this section we will review the main drawbacks of
the human evaluation listed in Section 2 and analyze
how our approach deals with them. The first one
was the use of explicit numerical scores, which are
difficult to define exactly. Our system was mainly
designed for the elimination of this issue.
Our evaluation continues to be time consuming.
Even more, the number of individual comparisons
needed is in the order of log(n!), in contrast with the
standard adequacy-fluency evaluation which needs
2n individual evaluations (two evaluations per sys-
tem, one for fluency, another one for adequacy). For
n in the range of 1 up to 20 (a realistic number of
systems for current evaluation campaigns) these two
quantities are comparable. And actually each of our
CA
DB
E
CA
DB
E
CA
DB
E
CA
DB
E
ll
ll
l
ll
ll
l
ll
ll
l
ll
l l
lFluency
Adequacy
1?WER
BLEU
0.3 0.4 0.5 0.6 0.7
          worse <?  normalized score  ?> better
Me
asu
re &
 Sys
tem
Figure 3: Normalized evaluation scores. Higher
scores are better. Solid lines show the 95% con-
fidence range. Automatic scores calculated on the
whole test set, human scores on the A+F subset.
evaluations should be simpler than the standard ad-
equacy and fluency ones. Therefore the time needed
for both evaluation procedures is probably similar.
Reproducibility of the evaluation is also an impor-
tant concern. We computed the number of ?errors?
in the evaluation process, i.e. the number of sen-
tences evaluated by two or more evaluators where
the evaluators? judgement was different. Only in
10% of the cases the evaluation was contradictory,
in the sense that one evaluator chose one sentence as
better than the other, while the other evaluator chose
the other one. In 30% of the cases, however, one
evaluator estimated both sentences to be of the same
quality while the other judged one sentence as supe-
rior to the other one. As comparison, for the fluency-
adequacy judgement nearly one third of the com-
mon evaluations have a difference in score greater or
equal than two (where the maximum would be four),
and another third a score difference of one point6.
With respect to biases, we feel that it is almost im-
possible to eliminate them if humans are involved. If
one of the judges prefers one kind of structure, there
will a bias for a system producing such output, in-
dependently of the evaluation procedure. However,
the suppression of explicit numerical scores elimi-
nates an additional bias of evaluators. It has been
observed that human judges often give scores within
6Note however that possible evaluator biases can have a
great influence in these statistics.
102
a certain range (e.g. in the mid-range or only ex-
treme values), which constitute an additional diffi-
culty when carrying out the evaluation (Leusch et
al., 2005). Our method suppresses this kind of bias.
Another advantage of our method is the possibil-
ity of assessing improvements within one system.
With one evaluation we can decide if some modi-
fications actually improve performance. This eval-
uation even gives us a confidence interval to weight
the significance of an improvement. Carrying out
a full adequacy-fluency analysis would require a lot
more effort, without giving more useful results.
7 Conclusion
We presented a novel human evaluation technique
that simplifies the task of the evaluators. Our method
relies on two basic observations. The first one is that
in most evaluations the final goal is to find a ranking
of different systems, the absolute scores are usually
not so relevant. Especially when considering human
evaluation, the scores are not even comparable be-
tween two evaluation campaigns. The second one
is the fact that a human judge can normally choose
the best one out of two translations, and this is a
much easier process than the assessment of numeri-
cal scores whose definition is not at all clear. Taking
this into consideration we suggested a method that
aims at finding a ranking of different MT systems
based on the comparison of pairs of translation can-
didates for a set of sentences to be evaluated.
A detailed analysis of the statistical significance
of the method is presented and also applied to some
wide-spread automatic measures. The evaluation
methodology was applied for the ranking of 5 sys-
tems that participated in the second evaluation cam-
paign of the TC-STAR project and comparison with
standard evaluation measures was performed.
8 Acknowledgements
We would like to thank the human judges who par-
ticipated in the evaluation. This work has been
funded by the integrated project TC-STAR? Tech-
nology and Corpora for Speech-to-Speech Transla-
tion ? (IST-2002-FP6-506738).
References
M. Bisani and H. Ney. 2004. Bootstrap estimates for
confidence intervals in ASR performance evaluationx.
IEEE ICASSP, pages 409?412, Montreal, Canada,
May.
T. Bui and M. Thanh. 1985. Significant improvements to
the Ford-Johnson algorithm for sorting. BIT Numeri-
cal Mathematics, 25(1):70?75.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Re-
evaluating the role of BLEU in machine translation re-
search. Proceeding of the 11th Conference of the Eu-
ropean Chapter of the ACL: EACL 2006, pages 249?
256, Trento, Italy, Apr.
G. Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. Proc. ARPA Workshop on Human Language
Technology.
B. Efron and R. J. Tibshirani. 1993. An Introduction
to the Bootstrap. Chapman & Hall, New York and
London.
L. Ford Jr and S. Johnson. 1959. A Tournament Problem.
The American Mathematical Monthly, 66(5):387?389.
D. E. Knuth. 1973. The Art of Computer Programming,
volume 3. Addison-Wesley, 1st edition. Sorting and
Searching.
P. Koehn and C. Monz. 2006. Manual and automatic
evaluation of machine translation between european
languages. Proceedings of the Workshop on Statisti-
cal Machine Translation, pages 102?121, New York
City, Jun.
G. Leusch, N. Ueffing, D. Vilar, and H. Ney. 2005.
Preprocessing and normalization for automatic evalu-
ation of machine translation. 43rd ACL: Proc. Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for MT and/or Summarization, pages 17?24, Ann Ar-
bor, Michigan, Jun.
F. J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. Proc. of the 41st ACL, pages
160?167, Sapporo, Japan, Jul.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. Proc. of the 40th ACL, pages 311?318,
Philadelphia, PA, Jul.
M. Peczarski. 2002. Sorting 13 elements requires 34
comparisons. LNCS, 2461/2002:785?794, Sep.
M. Snover, B. J. Dorr, R. Schwartz, J. Makhoul, L. Micci-
ulla, and R. Weischedel. 2005. A study of translation
error rate with targeted human annotation. Technical
Report LAMP-TR-126, CS-TR-4755, UMIACS-TR-
2005-58, University of Maryland, College Park, MD.
L. Thurstone. 1927. The method of paired comparisons
for social values. Journal of Abnormal and Social Psy-
chology, 21:384?400.
D. Vilar, E. Matusov, S. Hasan, R. Zens, and H. Ney.
2005. Statistical Machine Translation of European
Parliamentary Speeches. Proceedings of MT Summit
X, pages 259?266, Phuket, Thailand, Sep.
M. Wells. 1971. Elements of combinatorial computing.
Pergamon Press.
Y. Zhang and S. Vogel. 2004. Measuring confidence
intervals for the machine translation evaluation met-
rics. Proceedings of the 10th International Conference
on Theoretical and Methodological Issues in Machine
Translation, pages 4?6, Baltimore, MD.
103
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 51?55,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
The RWTH System Combination System for WMT 2009
Gregor Leusch, Evgeny Matusov, and Hermann Ney
RWTH Aachen University
Aachen, Germany
Abstract
RWTH participated in the System Combi-
nation task of the Fourth Workshop on Sta-
tistical Machine Translation (WMT 2009).
Hypotheses from 9 German?English MT
systems were combined into a consen-
sus translation. This consensus transla-
tion scored 2.1% better in BLEU and 2.3%
better in TER (abs.) than the best sin-
gle system. In addition, cross-lingual
output from 10 French, German, and
Spanish?English systems was combined
into a consensus translation, which gave
an improvement of 2.0% in BLEU/3.5% in
TER (abs.) over the best single system.
1 Introduction
The RWTH approach to MT system combination
is a refined version of the ROVER approach in
ASR (Fiscus, 1997), with additional steps to cope
with reordering between different hypotheses, and
to use true casing information from the input hy-
potheses. The basic concept of the approach has
been described by Matusov et al (2006). Several
improvements have been added later (Matusov et
al., 2008). This approach includes an enhanced
alignment and reordering framework. In con-
trast to existing approaches (Jayaraman and Lavie,
2005; Rosti et al, 2007), the context of the whole
corpus rather than a single sentence is considered
in this iterative, unsupervised procedure, yielding
a more reliable alignment. Majority voting on the
generated lattice is performed using the prior prob-
abilities for each system as well as other statistical
models such as a special n-gram language model.
2 System Combination Algorithm
In this section we present the details of our system
combination method. Figure 1 gives an overview
of the system combination architecture described
in this section. After preprocessing the MT hy-
potheses, pairwise alignments between the hy-
potheses are calculated. The hypotheses are then
reordered to match the word order of a selected
primary hypothesis. From this, we create a confu-
sion network (CN), which we then rescore using
Figure 1: The system combination architecture.
system prior weights and a language model (LM).
The single best path in this CN then constitutes the
consensus translation.
2.1 Word Alignment
The proposed alignment approach is a statistical
one. It takes advantage of multiple translations for
a whole corpus to compute a consensus translation
for each sentence in this corpus. It also takes ad-
vantage of the fact that the sentences to be aligned
are in the same language.
For each source sentence F in the test corpus,
we select one of its translations En, n=1, . . . ,M,
as the primary hypothesis. Then we align the sec-
ondary hypotheses Em(m = 1, . . . ,M ;n 6= m)
with En to match the word order in En. Since it is
not clear which hypothesis should be primary, i. e.
has the ?best? word order, we let every hypothesis
play the role of the primary translation, and align
all pairs of hypotheses (En, Em); n 6= m.
The word alignment is trained in analogy to
the alignment training procedure in statistical MT.
The difference is that the two sentences that have
to be aligned are in the same language. We use the
IBM Model 1 (Brown et al, 1993) and the Hid-
den Markov Model (HMM, (Vogel et al, 1996))
to estimate the alignment model.
The alignment training corpus is created from a
test corpus1 of effectively M ? (M ? 1) ? N sen-
tences translated by the involved MT engines. The
single-word based lexicon probabilities p(e|e?) are
initialized from normalized lexicon counts col-
lected over the sentence pairs (Em, En) on this
corpus. Since all of the hypotheses are in the same
language, we count co-occurring identical words,
i. e. whether em,j is the same word as en,i for some
i and j. In addition, we add a fraction of a count
for words with identical prefixes.
1A test corpus can be used directly because the align-
ment training is unsupervised and only automatically pro-
duced translations are considered.
51
The model parameters are trained iteratively us-
ing the GIZA++ toolkit (Och and Ney, 2003). The
training is performed in the directions Em ? En
and En ? Em. After each iteration, the updated
lexicon tables from the two directions are interpo-
lated. The final alignments are determined using
a cost matrix C for each sentence pair (Em, En).
Elements of this matrix are the local costs C(j, i)
of aligning a word em,j from Em to a word en,i
from En. Following Matusov et al (2004), we
compute these local costs by interpolating the
negated logarithms of the state occupation proba-
bilities from the ?source-to-target? and ?target-to-
source? training of the HMM model. Two differ-
ent alignments are computed using the cost matrix
C: the alignment a? used for reordering each sec-
ondary translation Em, and the alignment a? used
to build the confusion network.
In addition to the GIZA++ alignments, we have
also conducted preliminary experiments follow-
ing He et al (2008) to exploit character-based
similarity, as well as estimating p(e|e?) :=?
f p(e|f)p(f |e
?) directly from a bilingual lexi-
con. But we were not able to find improvements
over the GIZA++ alignments so far.
2.2 Word Reordering and Confusion
Network Generation
After reordering each secondary hypothesis Em
and the rows of the corresponding alignment cost
matrix according to a?, we determine M?1 mono-
tone one-to-one alignments between En as the pri-
mary translation and Em,m = 1, . . . ,M ;m 6= n.
We then construct the confusion network. In case
of many-to-one connections in a? of words in Em
to a single word from En, we only keep the con-
nection with the lowest alignment costs.
The use of the one-to-one alignment a? implies
that some words in the secondary translation will
not have a correspondence in the primary transla-
tion and vice versa. We consider these words to
have a null alignment with the empty word ?. In
the corresponding confusion network, the empty
word will be transformed to an ?-arc.
M ? 1 monotone one-to-one alignments can
then be transformed into a confusion network. We
follow the approach of Bangalore et al (2001)
with some extensions. Multiple insertions with re-
gard to the primary hypothesis are sub-aligned to
each other, as described by Matusov et al (2008).
Figure 2 gives an example for the alignment.
2.3 Voting in the confusion network
Instead of choosing a fixed sentence to define the
word order for the consensus translation, we gen-
erate confusion networks for all hypotheses as pri-
mary, and unite them into a single lattice. In our
experience, this approach is advantageous in terms
of translation quality, e.g. by 0.7% in BLEU com-
pared to a minimum Bayes risk primary (Rosti et
al., 2007). Weighted majority voting on a single
confusion network is straightforward and analo-
gous to ROVER (Fiscus, 1997). We sum up the
probabilities of the arcs which are labeled with the
same word and have the same start state and the
same end state. To exploit the true casing abilities
of the input MT systems, we sum up the scores of
arcs bearing the same word but in different cases.
Here, we leave the decision about upper or lower
case to the language model.
2.4 Language Models
The lattice representing a union of several confu-
sion networks can then be directly rescored with
an n-gram language model (LM). A transforma-
tion of the lattice is required, since LM history has
to be memorized.
We train a trigram LM on the outputs of the sys-
tems involved in system combination. For LM
training, we took the system hypotheses for the
same test corpus for which the consensus trans-
lations are to be produced. Using this ?adapted?
LM for lattice rescoring thus gives bonus to n-
grams from the original system hypotheses, in
most cases from the original phrases. Presum-
ably, many of these phrases have a correct word or-
der, since they are extracted from the training data.
Previous experimental results show that using this
LM in rescoring together with a word penalty (to
counteract any bias towards short sentences) no-
tably improves translation quality. This even re-
sults in better translations than using a ?classical?
LM trained on a monolingual training corpus. We
attribute this to the fact that most of the systems
we combine are phrase-based systems, which al-
ready include such general LMs. Since we are us-
ing a true-cased LM trained on the hypotheses, we
can exploit true casing information from the in-
put systems by using this LM to disambiguate be-
tween the separate arcs generated for the variants
(see Section 2.3).
After LM rescoring, we add the probabilities of
identical partial paths to improve the estimation
of the score for the best hypothesis. This is done
through determinization of the lattice.
2.5 Extracting Consensus Translations
To generate our consensus translation, we extract
the single-best path within the rescored confusion
network. With our approach, we could also extract
N -best hypotheses. In a subsequent step, these N -
best lists could be rescored with additional statis-
tical models (Matusov et al, 2008). But as we did
not have the resources in the WMT 2009 evalua-
tion, this step was dropped for our submission.
3 Tuning system weights
System weights, LM factor, and word penalty
need to be tuned to produce good consensus trans-
lations. We optimize these parameters using the
52
0.25 would your like coffee or tea
system 0.35 have you tea or Coffee
hypotheses 0.10 would like your coffee or
0.30 I have some coffee tea would you like
alignment have|would you|your $|like Coffee|coffee or|or tea|tea
and would|would your|your like|like coffee|coffee or|or $|tea
reordering I|$ would|would you|your like|like have|$ some|$ coffee|coffee $|or tea|tea
$ would your like $ $ coffee or tea
confusion $ have you $ $ $ Coffee or tea
network $ would your like $ $ coffee or $
I would you like have some coffee $ tea
$ would you $ $ $ coffee or tea
voting 0.7 0.65 0.65 0.35 0.7 0.7 0.5 0.7 0.9
(normalized) I have your like have some Coffee $ $
0.3 0.35 0.35 0.65 0.3 0.3 0.5 0.3 0.1
consensus translation would you like coffee or tea
Figure 2: Example of creating a confusion network from monotone one-to-one word alignments (denoted
with symbol |). The words of the primary hypothesis are printed in bold. The symbol $ denotes a null
alignment or an ?-arc in the corresponding part of the confusion network.
Table 1: Systems combined for the WMT 2009
task. Systems written in oblique were also used in
the Cross Lingual task (rbmt3 for FR?EN).
DE?EN google, liu, rbmt3, rwth, stutt-
gart, systran, uedin, uka, umd
ES?EN google, nict, rbmt4, rwth,
talp-upc, uedin
FR?EN dcu, google, jhu, limsi, lium-
systran, rbmt4, rwth, uedin, uka
publicly available CONDOR optimization toolkit
(Berghen and Bersini, 2005). For the WMT
2009 Workshop, we selected a linear combina-
tion of BLEU (Papineni et al, 2002) and TER
(Snover et al, 2006) as optimization criterion,
?? := argmax? {(2 ? BLEU)? TER}, based on
previous experience (Mauser et al, 2008). We
used the whole dev set as a tuning set. For more
stable results, we used the case-insensitive variants
for both measures, despite the explicit use of case
information in our approach.
4 Experimental results
Due to the large number of submissions (71 in
total for the language pairs DE?EN, ES?EN,
FR?EN), we had to select a reasonable number
of systems to be able to tune the parameters in
a reliable way. Based on previous experience,
we manually selected the systems with the best
BLEU/TER score, and tried different variations of
this selection, e.g. by removing systems which
had low weights after optimization, or by adding
promising systems, like rule based systems.
Table 1 lists the systems which made it into
our final submission. In our experience, if a large
number of systems is available, using n-best trans-
lations does not give better results than using sin-
gle best translations, but raises optimization time
significantly. Consequently, we only used single
best translations from all systems.
The results also confirm another observation:
Even though rule-based systems by itself may
have significantly lower automatic evaluation
scores (e.g. by 2% or more in BLEU on DE?EN),
they are often very important in system combina-
tion, and can improve the consensus translation
e.g. by 0.5% in BLEU.
Having submitted our translations to the WMT
workshop, we calculated scores on the WMT 2009
test set, to verify the results on the tuning data.
Both the results on the tuning set and on the test
set can be found in the following tables.
4.1 The Google Problem
One particular thing we noticed is that in the lan-
guage pairs of FR?EN and ES?EN, the trans-
lations from one provided single system (Google)
were much better in terms of BLEU and TER than
those of all other systems ? in the former case
by more than 4% in BLEU. In our experience,
our system combination approach requires at least
three ?comparably good? systems to be able to
achieve significant improvements. This was con-
firmed in the WMT 2009 task as well: Neither in
FR?EN nor in ES?EN we were able to achieve
an improvement over the Google system. For this
reason, we did not submit consensus translations
for these two language pairs. On the other hand,
we would have achieved significant improvements
over all (remaining) systems leaving out Google.
4.2 German?English (DE?EN)
Table 2 lists the scores on the tuning and test set
for the DE?EN task. We can see that the best
systems are rather close to each other in terms
of BLEU. Also, the rule-based translation system
(RBMT), here SYSTRAN, scores rather well. As
a consequence, we find a large improvement using
system combination: 2.9%/2.7% abs. on the tun-
ing set, and still 2.1%/2.3% on test, which means
that system combination generalizes well here.
4.3 Spanish?English (ES?EN),
French?English (FR?EN)
In Table 3, we see that on the ES?EN and
FR?EN tasks, a single system ? Google ? scores
significantly better on the TUNE set than any other
53
Table 2: German?English task: case-insensitive
scores. Best single system was Google, second
best UKA, best RBMT Systran. SC stands for sys-
tem combination output.
TUNE TEST
German?English BLEU TER BLEU TER
Best single 23.2 59.5 21.3 61.3
Second best single 23.0 58.8 21.0 61.7
Best RBMT 21.3 61.3 18.9 63.7
SC (9 systems) 26.1 56.8 23.4 59.0
w/o RBMT 24.5 57.3 22.5 59.2
w/o Google 24.9 57.4 23.0 59.1
Table 3: Spanish?English and French?English
task: scores on the tuning set after system combi-
nation weight tuning (case-insensitive). Best sin-
gle system was Google, second best was Uedin
(Spanish) and UKA (French). No results on TEST
were generated.
ES?EN FR?EN
Spanish?English BLEU TER BLEU TER
Best single 29.5 53.6 32.2 50.1
Second best single 26.9 56.1 28.0 54.6
SC (6/9 systems) 28.7 53.6 30.7 52.5
w/o Google 27.5 55.6 30.0 52.8
system, namely by 2.6%/4.2% resp. in BLEU. As
a result, a combination of these systems scores
better than any other system, even when leaving
out the Google system. But it gives worse scores
than the single best system. This is explainable,
because system combination is trying to find a
consensus translation. For example, in one case,
the majority of the systems leave the French term
?wagon-lit? untranslated; spurious translations in-
clude ?baggage car?, ?sleeping car?, and ?alive?.
As a result, the consensus translation also contains
?wagon-lit?, not the correct translation ?sleeper?
which only the Google system provides. Even tun-
ing all other system weights to zero would not re-
sult in pure Google translations, as these weights
neither affect the LM nor the selection of the pri-
mary hypothesis in our approach.
4.4 Cross-Lingual?English (XX?EN)
Finally, we have conducted experiments on cross-
lingual system combination, namely combining
the output from DE?EN, ES?EN, and FR?EN
systems to a single English consensus transla-
tion. Some interesting results can be found in
Table 4. We see that this consensus translation
scores 2.0%/3.5% better than the best single sys-
tem, and 4.4%/5.6% better than the second best
single system. While this is only 0.8%/2.5% bet-
ter than the combination of only the three Google
systems, the combination of the non-Google sys-
Table 4: Cross-lingual task: combination
of German?English, Spanish?English, and
French?English. Case-insensitive scores. Best
single system was Google for all language pairs.
Cross-lingual TUNE TEST
? English BLEU TER BLEU TER
Best single German 23.2 59.5 21.3 61.3
Best single Spanish 29.5 53.6 28.7 53.8
Best single French 32.2 50.1 31.1 51.7
SC (10 systems) 35.5 46.4 33.1 48.2
w/o RBMT 35.1 46.5 32.7 48.3
w/o Google 32.3 48.8 29.9 50.5
3 Google systems 34.2 48.0 32.3 49.2
w/o German 34.0 49.3 31.5 50.9
w/o Spanish 33.4 49.8 31.0 51.9
w/o French 30.5 51.4 28.6 52.3
tems leads to translations that could compete with
the FR?EN Google system. Again, we see that
RBMT systems lead to a small improvement of
0.4% in BLEU, although their scores are signif-
icantly worse than those of the competing SMT
systems.
Regarding languages, we see that despite the
large differences in the quality of the systems (10
points between DE?EN and FR?EN), all lan-
guages seem to provide significant information to
the consensus translation: While FR?EN cer-
tainly has the largest influence (?4.5% in BLEU
when left out), even DE?EN ?contributes? 1.6
BLEU points to the final submission.
5 Conclusions
We have shown that our system combination sys-
tem can lead to significant improvements over
single best MT output where a significant num-
ber of comparably good translations is available
on a single language pair. For cross-lingual sys-
tem combination, we observe even larger improve-
ments, even if the quality in terms of BLEU or
TER between the systems of different language
pairs varies significantly. While the input of high-
quality SMT systems has the largest weight for the
consensus translation quality, we find that RBMT
systems can give important additional information
leading to better translations.
Acknowledgments
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. This work was
partly supported by the Defense Advanced Re-
search Projects Agency (DARPA) under Contract
No. HR0011-06-C-0023.
54
References
S. Bangalore, G. Bordel, and G. Riccardi. 2001.
Computing consensus translation from multiple ma-
chine translation systems. In IEEE Automatic
Speech Recognition and Understanding Workshop,
Madonna di Campiglio, Italy, December.
F. V. Berghen and H. Bersini. 2005. CONDOR,
a new parallel, constrained extension of Powell?s
UOBYQA algorithm: Experimental results and
comparison with the DFO algorithm. Journal of
Computational and Applied Mathematics, 181:157?
175.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263?311, June.
J. Fiscus. 1997. A post-processing system to yield re-
duced word error rates: Recognizer output voting er-
ror reduction (ROVER). In IEEE Workshop on Au-
tomatic Speech Recognition and Understanding.
X. He, M. Yang, J. Gao, P. Nguyen, and R. Moore.
2008. Indirect-HMM-based hypothesis alignment
for combining outputs from machine translation sys-
tems. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 98?107, Honolulu, Hawaii, October.
S. Jayaraman and A. Lavie. 2005. Multi-engine ma-
chine translation guided by explicit word matching.
In Proc. of the 10th Annual Conf. of the European
Association for Machine Translation (EAMT), pages
143?152, Budapest, Hungary, May.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
word alignments for statistical machine translation.
In COLING ?04: The 20th Int. Conf. on Computa-
tional Linguistics, pages 219?225, Geneva, Switzer-
land, August.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine trans-
lation systems using enhanced hypotheses align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40, Trento, Italy, April.
E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y. S. Lee,
J. B. Marino, M. Paulik, S. Roukos, H. Schwenk,
and H. Ney. 2008. System combination for machine
translation of spoken and written language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237, September.
A. Mauser, S. Hasan, and H. Ney. 2008. Automatic
evaluation measures for statistical machine transla-
tion system optimization. In International Confer-
ence on Language Resources and Evaluation, Mar-
rakech, Morocco, May.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51, March.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311?318, Philadelphia, PA, July.
A. V. Rosti, S. Matsoukas, and R. Schwartz. 2007.
Improved word-level system combination for ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics (ACL), pages 312?319, Prague, Czech Re-
public, June.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Error
Rate with Targeted Human Annotation. In Proc. of
the 7th Conf. of the Association for Machine Trans-
lation in the Americas (AMTA), pages 223?231,
Boston, MA, August.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
COLING ?96: The 16th Int. Conf. on Computational
Linguistics, pages 836?841, Copenhagen, Denmark,
August.
55
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412?418,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Language Independent Connectivity Strength Features
for Phrase Pivot Statistical Machine Translation
Ahmed El Kholy, Nizar Habash
Center for Computational Learning Systems, Columbia University
{akholy,habash}@ccls.columbia.edu
Gregor Leusch, Evgeny Matusov
Science Applications International Corporation
{gregor.leusch,evgeny.matusov}@saic.com
Hassan Sawaf
eBay Inc.
hsawaf@ebay.com
Abstract
An important challenge to statistical ma-
chine translation (SMT) is the lack of par-
allel data for many language pairs. One
common solution is to pivot through a
third language for which there exist par-
allel corpora with the source and target
languages. Although pivoting is a robust
technique, it introduces some low quality
translations. In this paper, we present two
language-independent features to improve
the quality of phrase-pivot based SMT.
The features, source connectivity strength
and target connectivity strength reflect the
quality of projected alignments between
the source and target phrases in the pivot
phrase table. We show positive results (0.6
BLEU points) on Persian-Arabic SMT as
a case study.
1 Introduction
One of the main issues in statistical machine trans-
lation (SMT) is the scarcity of parallel data for
many language pairs especially when the source
and target languages are morphologically rich. A
common SMT solution to the lack of parallel data
is to pivot the translation through a third language
(called pivot or bridge language) for which there
exist abundant parallel corpora with the source
and target languages. The literature covers many
pivoting techniques. One of the best performing
techniques, phrase pivoting (Utiyama and Isahara,
2007), builds an induced new phrase table between
the source and target. One of the main issues of
this technique is that the size of the newly cre-
ated pivot phrase table is very large (Utiyama and
Isahara, 2007). Moreover, many of the produced
phrase pairs are of low quality which affects the
translation choices during decoding and the over-
all translation quality. In this paper, we introduce
language independent features to determine the
quality of the pivot phrase pairs between source
and target. We show positive results (0.6 BLEU
points) on Persian-Arabic SMT.
Next, we briefly discuss some related work. We
then review two common pivoting strategies and
how we use them in Section 3. This is followed by
our approach to using connectivity strength fea-
tures in Section 4. We present our experimental
results in Section 5.
2 Related Work
Many researchers have investigated the use of piv-
oting (or bridging) approaches to solve the data
scarcity issue (Utiyama and Isahara, 2007; Wu and
Wang, 2009; Khalilov et al, 2008; Bertoldi et al,
2008; Habash and Hu, 2009). The main idea is to
introduce a pivot language, for which there exist
large source-pivot and pivot-target bilingual cor-
pora. Pivoting has been explored for closely re-
lated languages (Hajic? et al, 2000) as well as un-
related languages (Koehn et al, 2009; Habash and
Hu, 2009). Many different pivot strategies have
been presented in the literature. The following
three are perhaps the most common.
The first strategy is the sentence translation
technique in which we first translate the source
sentence to the pivot language, and then translate
the pivot language sentence to the target language
412
(Khalilov et al, 2008).
The second strategy is based on phrase pivot-
ing (Utiyama and Isahara, 2007; Cohn and Lap-
ata, 2007; Wu and Wang, 2009). In phrase pivot-
ing, a new source-target phrase table (translation
model) is induced from source-pivot and pivot-
target phrase tables. Lexical weights and transla-
tion probabilities are computed from the two trans-
lation models.
The third strategy is to create a synthetic source-
target corpus by translating the pivot side of
source-pivot corpus to the target language using an
existing pivot-target model (Bertoldi et al, 2008).
In this paper, we build on the phrase pivoting
approach, which has been shown to be the best
with comparable settings (Utiyama and Isahara,
2007). We extend phrase table scores with two
other features that are language independent.
Since both Persian and Arabic are morphologi-
cally rich, we should mention that there has been
a lot of work on translation to and from morpho-
logically rich languages (Yeniterzi and Oflazer,
2010; Elming and Habash, 2009; El Kholy and
Habash, 2010a; Habash and Sadat, 2006; Kathol
and Zheng, 2008). Most of these efforts are fo-
cused on syntactic and morphological processing
to improve the quality of translation.
To our knowledge, there hasn?t been a lot of
work on Persian and Arabic as a language pair.
The only effort that we are aware of is based
on improving the reordering models for Persian-
Arabic SMT (Matusov and Ko?pru?, 2010).
3 Pivoting Strategies
In this section, we review the two pivoting strate-
gies that are our baselines. We also discuss how
we overcome the large expansion of source-to-
target phrase pairs in the process of creating a
pivot phrase table.
3.1 Sentence Pivoting
In sentence pivoting, English is used as an inter-
face between two separate phrase-based MT sys-
tems; Persian-English direct system and English-
Arabic direct system. Given a Persian sentence,
we first translate the Persian sentence from Per-
sian to English, and then from English to Arabic.
3.2 Phrase Pivoting
In phrase pivoting (sometimes called triangulation
or phrase table multiplication), we train a Persian-
to-Arabic and an English-Arabic translation mod-
els, such as those used in the sentence pivoting
technique. Based on these two models, we induce
a new Persian-Arabic translation model.
Since we build our models on top of Moses
phrase-based SMT (Koehn et al, 2007), we need
to provide the same set of phrase translation prob-
ability distributions.1 We follow Utiyama and Isa-
hara (2007) in computing the probability distribu-
tions. The following are the set of equations used
to compute the lexical probabilities (?) and the
phrase probabilities (pw)
?(f |a) =?
e
?(f |e)?(e|a)
?(a|f) =?
e
?(a|e)?(e|f)
pw(f |a) =
?
e
pw(f |e)pw(e|a)
pw(a|f) =
?
e
pw(a|e)pw(e|f)
where f is the Persian source phrase. e is
the English pivot phrase that is common in both
Persian-English translation model and English-
Arabic translation model. a is the Arabic target
phrase.
We also build a Persian-Arabic reordering table
using the same technique but we compute the re-
ordering weights in a similar manner to Henriquez
et al (2010).
As discussed earlier, the induced Persian-
Arabic phrase and reordering tables are very large.
Table 1 shows the amount of parallel corpora
used to train the Persian-English and the English-
Arabic and the equivalent phrase table sizes com-
pared to the induced Persian-Arabic phrase table.2
We introduce a basic filtering technique dis-
cussed next to address this issue and present some
baseline experiments to test its performance in
Section 5.3.
3.3 Filtering for Phrase Pivoting
The main idea of the filtering process is to select
the top [n] English candidate phrases for each Per-
sian phrase from the Persian-English phrase ta-
ble and similarly select the top [n] Arabic target
phrases for each English phrase from the English-
Arabic phrase table and then perform the pivot-
ing process described earlier to create a pivoted
1Four different phrase translation scores are computed in
Moses? phrase tables: two lexical weighting scores and two
phrase translation probabilities.
2The size of the induced phrase table size is computed but
not created.
413
Training Corpora Phrase Table
Translation Model Size # Phrase Pairs Size
Persian-English ?4M words 96,04,103 1.1GB
English-Arabic ?60M words 111,702,225 14GB
Pivot Persian-Arabic N/A 39,199,269,195 ?2.5TB
Table 1: Translation Models Phrase Table comparison in terms of number of line and sizes.
Persian-Arabic phrase table. To select the top can-
didates, we first rank all the candidates based on
the log linear scores computed from the phrase
translation probabilities and lexical weights mul-
tiplied by the optimized decoding weights then we
pick the top [n] pairs.
We compare the different pivoting strategies
and various filtering thresholds in Section 5.3.
4 Approach
One of the main challenges in phrase pivoting is
the very large size of the induced phrase table.
It becomes even more challenging if either the
source or target language is morphologically rich.
The number of translation candidates (fanout) in-
creases due to ambiguity and richness (discussed
in more details in Section 5.2) which in return
increases the number of combinations between
source and target phrases. Since the only criteria
of matching between the source and target phrase
is through a pivot phrase, many of the induced
phrase pairs are of low quality. These phrase pairs
unnecessarily increase the search space and hurt
the overall quality of translation.
To solve this problem, we introduce two
language-independent features which are added to
the log linear space of features in order to deter-
mine the quality of the pivot phrase pairs. We call
these features connectivity strength features.
Connectivity Strength Features provide two
scores, Source Connectivity Strength (SCS) and
Target Connectivity Strength (TCS). These two
scores are similar to precision and recall metrics.
They depend on the number of alignment links be-
tween words in the source phrase to words of the
target phrase. SCS and TSC are defined in equa-
tions 1 and 2 where S = {i : 1 ? i ? S} is the
set of source words in a given phrase pair in the
pivot phrase table and T = {j : 1 ? j ? T}
is the set of the equivalent target words. The
word alignment between S and T is defined as
A = {(i, j) : i ? S and j ? T }.
SCS = |A||S| (1)
TCS = |A||T | (2)
We get the alignment links by projecting the
alignments of source-pivot to the pivot-target
phrase pairs used in pivoting. If the source-target
phrase pair are connected through more than one
pivot phrase, we take the union of the alignments.
In contrast to the aggregated values represented
in the lexical weights and the phrase probabilities,
connectivity strength features provide additional
information by counting the actual links between
the source and target phrases. They provide an
independent and direct approach to measure how
good or bad a given phrase pair are connected.
Figure 1 and 2 are two examples (one good, one
bad) Persian-Arabic phrase pairs in a pivot phrase
table induced by pivoting through English.3 In the
first example, each Persian word is aligned to an
Arabic word. The meaning is preserved in both
phrases which is reflected in the SCS and TCS
scores. In the second example, only one Persian
word in aligned to one Arabic word in the equiv-
alent phrase and the two phrases conveys two dif-
ferent meanings. The English phrase is not a good
translation for either, which leads to this bad pair-
ing. This is reflected in the SCS and TCS scores.
5 Experiments
In this section, we present a set of baseline ex-
periments including a simple filtering technique to
overcome the huge expansion of the pivot phrase
table. Then we present our results in using connec-
tivity strength features to improve Persian-Arabic
pivot translation quality.
3We use the Habash-Soudi-Buckwalter Arabic transliter-
ation (Habash et al, 2007) in the figures with extensions for
Persian as suggested by Habash (2010).
414
Persian: "A?tmAd"myAn"dw"k?wr " " "? ?"??"()"?"?%$#"?,-. ?"" " " " " " " " " " " " "?trust"between"the"two"countries?"English: "trust"between"the"two"countries"
Arabic:" "Al?q?"byn"Aldwltyn " " " "? /012?52?2$3"34"? ?"" " " " " " " " " " " " "?the"trust"between"the"two"countries?"
Figure 1: An example of strongly connected Persian-Arabic phrase pair through English. All Persian
words are connected to one or more Arabic words. SCS=1.0 and TCS=1.0.
Persian: "AyjAd"cnd"?rkt"m?trk " " " "? 0/.+?",+*(")'&"?$#"? ?"" " " " " " " " " " " " "?Establish"few"joint"companies?"English: "joint"ventures"
Arabic:" "b?D"?rkAt"AlmqAwlAt"fy"Albld" "? 123"?<=>&";:"?89"?6?",+5"? ?"" " " " " " " " " " " " "?Some"construcBon"companies"in"the"country?"
Figure 2: An example of weakly connected Persian-Arabic phrase pairs through English. Only one
Persian word is connected to an Arabic word. SCS=0.25 and TCS=0.2.
5.1 Experimental Setup
In our pivoting experiments, we build two SMT
models. One model to translate from Persian to
English and another model to translate from En-
glish to Arabic. The English-Arabic parallel cor-
pus is about 2.8M sentences (?60M words) avail-
able from LDC4 and GALE5 constrained data. We
use an in-house Persian-English parallel corpus of
about 170K sentences and 4M words.
Word alignment is done using GIZA++ (Och
and Ney, 2003). For Arabic language model-
ing, we use 200M words from the Arabic Giga-
word Corpus (Graff, 2007) together with the Ara-
bic side of our training data. We use 5-grams
for all language models (LMs) implemented us-
ing the SRILM toolkit (Stolcke, 2002). For En-
glish language modeling, we use English Giga-
word Corpus with 5-gram LM using the KenLM
toolkit (Heafield, 2011).
All experiments are conducted using the Moses
phrase-based SMT system (Koehn et al, 2007).
We use MERT (Och, 2003) for decoding weight
4LDC Catalog IDs: LDC2005E83, LDC2006E24,
LDC2006E34, LDC2006E85, LDC2006E92, LDC2006G05,
LDC2007E06, LDC2007E101, LDC2007E103,
LDC2007E46, LDC2007E86, LDC2008E40, LDC2008E56,
LDC2008G05, LDC2009E16, LDC2009G01.
5Global Autonomous Language Exploitation, or GALE,
is a DARPA-funded research project.
optimization. For Persian-English translation
model, weights are optimized using a set 1000 sen-
tences randomly sampled from the parallel cor-
pus while the English-Arabic translation model
weights are optimized using a set of 500 sen-
tences from the 2004 NIST MT evaluation test
set (MT04). The optimized weights are used for
ranking and filtering (discussed in Section 3.3).
We use a maximum phrase length of size 8
across all models. We report results on an in-
house Persian-Arabic evaluation set of 536 sen-
tences with three references. We evaluate using
BLEU-4 (Papineni et al, 2002) and METEOR
(Lavie and Agarwal, 2007).
5.2 Linguistic Preprocessing
In this section we present our motivation and
choice for preprocessing Arabic, Persian, English
data. Both Arabic and Persian are morphologi-
cally complex languages but they belong to two
different language families. They both express
richness and linguistic complexities in different
ways.
One aspect of Arabic?s complexity is its vari-
ous attachable clitics and numerous morphologi-
cal features (Habash, 2010). We follow El
Kholy and Habash (2010a) and use the PATB to-
kenization scheme (Maamouri et al, 2004) in our
415
experiments. We use MADA v3.1 (Habash and
Rambow, 2005; Habash et al, 2009) to tokenize
the Arabic text. We only evaluate on detokenized
and orthographically correct (enriched) output fol-
lowing the work of El Kholy and Habash (2010b).
Persian on the other hand has a relatively sim-
ple nominal system. There is no case system and
words do not inflect with gender except for a few
animate Arabic loanwords. Unlike Arabic, Persian
shows only two values for number, just singular
and plural (no dual), which are usually marked by
either the suffix A?+ +hA and sometimes 	?@+ +An,
or one of the Arabic plural markers. Verbal mor-
phology is very complex in Persian. Each verb
has a past and present root and many verbs have
attached prefix that is regarded part of the root.
A verb in Persian inflects for 14 different tense,
mood, aspect, person, number and voice combina-
tion values (Rasooli et al, 2013). We use Perstem
(Jadidinejad et al, 2010) for segmenting Persian
text.
English, our pivot language, is quite different
from both Arabic and Persian. English is poor
in morphology and barely inflects for number and
tense, and for person in a limited context. English
preprocessing simply includes down-casing, sepa-
rating punctuation and splitting off ??s?.
5.3 Baseline Evaluation
We compare the performance of sentence pivot-
ing against phrase pivoting with different filtering
thresholds. The results are presented in Table 2. In
general, the phrase pivoting outperforms the sen-
tence pivoting even when we use a small filtering
threshold of size 100. Moreover, the higher the
threshold the better the performance but with a di-
minishing gain.
Pivot Scheme BLEU METEOR
Sentence Pivoting 19.2 36.4
Phrase Pivot F100 19.4 37.4
Phrase Pivot F500 20.1 38.1
Phrase Pivot F1K 20.5 38.6
Table 2: Sentence pivoting versus phrase pivoting
with different filtering thresholds (100/500/1000).
We use the best performing setup across the rest
of the experiments.
5.4 Connectivity Strength Features
Evaluation
In this experiment, we test the performance of
adding the connectivity strength features (+Conn)
to the best performing phrase pivoting model
(Phrase Pivot F1K).
Model BLEU METEOR
Sentence Pivoting 19.2 36.4
Phrase Pivot F1K 20.5 38.6
Phrase Pivot F1K+Conn 21.1 38.9
Table 3: Connectivity strength features experi-
ment result.
The results in Table 3 show that we get a
nice improvement of ?0.6/0.5 (BLEU/METEOR)
points by adding the connectivity strength fea-
tures. The differences in BLEU scores between
this setup and all other systems are statistically
significant above the 95% level. Statistical signif-
icance is computed using paired bootstrap resam-
pling (Koehn, 2004).
6 Conclusion and Future Work
We presented an experiment showing the effect of
using two language independent features, source
connectivity score and target connectivity score,
to improve the quality of pivot-based SMT. We
showed that these features help improving the
overall translation quality. In the future, we plan
to explore other features, e.g., the number of the
pivot phases used in connecting the source and tar-
get phrase pair and the similarity between these
pivot phrases. We also plan to explore language
specific features which could be extracted from
some seed parallel data, e.g., syntactic and mor-
phological compatibility of the source and target
phrase pairs.
Acknowledgments
The work presented in this paper was possible
thanks to a generous research grant from Science
Applications International Corporation (SAIC).
The last author (Sawaf) contributed to the effort
while he was at SAIC. We would like to thank M.
Sadegh Rasooli and Jon Dehdari for helpful dis-
cussions and insights into Persian. We also thank
the anonymous reviewers for their insightful com-
ments.
416
References
Nicola Bertoldi, Madalina Barbaiani, Marcello Fed-
erico, and Roldano Cattoni. 2008. Phrase-based
statistical machine translation with pivot languages.
Proceeding of IWSLT, pages 143?149.
Trevor Cohn and Mirella Lapata. 2007. Ma-
chine translation by triangulation: Making ef-
fective use of multi-parallel corpora. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 728.
Ahmed El Kholy and Nizar Habash. 2010a. Ortho-
graphic and Morphological Processing for English-
Arabic Statistical Machine Translation. In Proceed-
ings of Traitement Automatique du Langage Naturel
(TALN-10). Montre?al, Canada.
Ahmed El Kholy and Nizar Habash. 2010b. Tech-
niques for Arabic Morphological Detokenization
and Orthographic Denormalization. In Proceed-
ings of the seventh International Conference on Lan-
guage Resources and Evaluation (LREC), Valletta,
Malta.
Jakob Elming and Nizar Habash. 2009. Syntactic
Reordering for English-Arabic Phrase-Based Ma-
chine Translation. In Proceedings of the EACL 2009
Workshop on Computational Approaches to Semitic
Languages, pages 69?77, Athens, Greece, March.
David Graff. 2007. Arabic Gigaword 3, LDC Cat-
alog No.: LDC2003T40. Linguistic Data Consor-
tium, University of Pennsylvania.
Nizar Habash and Jun Hu. 2009. Improving Arabic-
Chinese Statistical Machine Translation using En-
glish as Pivot Language. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 173?181, Athens, Greece, March.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphologi-
cal Disambiguation in One Fell Swoop. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL?05), pages 573?
580, Ann Arbor, Michigan.
Nizar Habash and Fatiha Sadat. 2006. Arabic Pre-
processing Schemes for Statistical Machine Transla-
tion. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Companion Vol-
ume: Short Papers, pages 49?52, New York City,
USA.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den
Bosch and A. Soudi, editors, Arabic Computa-
tional Morphology: Knowledge-based and Empiri-
cal Methods. Springer.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Khalid
Choukri and Bente Maegaard, editors, Proceedings
of the Second International Conference on Arabic
Language Resources and Tools. The MEDAR Con-
sortium, April.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Jan Hajic?, Jan Hric, and Vladislav Kubon. 2000. Ma-
chine Translation of Very Close Languages. In Pro-
ceedings of the 6th Applied Natural Language Pro-
cessing Conference (ANLP?2000), pages 7?12, Seat-
tle.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, UK.
Carlos Henriquez, Rafael E. Banchs, and Jose? B.
Marin?o. 2010. Learning reordering models for sta-
tistical machine translation with a pivot language.
Amir Hossein Jadidinejad, Fariborz Mahmoudi, and
Jon Dehdari. 2010. Evaluation of PerStem: a sim-
ple and efficient stemming algorithm for Persian. In
Multilingual Information Access Evaluation I. Text
Retrieval Experiments, pages 98?101.
Andreas Kathol and Jing Zheng. 2008. Strategies for
building a Farsi-English smt system from limited re-
sources. In Proceedings of the 9th Annual Confer-
ence of the International Speech Communication As-
sociation (INTERSPEECH2008), pages 2731?2734,
Brisbane, Australia.
M. Khalilov, Marta R. Costa-juss, Jos A. R. Fonollosa,
Rafael E. Banchs, B. Chen, M. Zhang, A. Aw, H. Li,
Jos B. Mario, Adolfo Hernndez, and Carlos A. Hen-
rquez Q. 2008. The talp & i2r smt systems for iwslt
2008. In International Workshop on Spoken Lan-
guage Translation. IWSLT 2008, pg. 116?123.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-
pher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Christopher Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public.
Philipp Koehn, Alexandra Birch, and Ralf Steinberger.
2009. 462 machine translation systems for europe.
Proceedings of MT Summit XII, pages 65?72.
Philipp Koehn. 2004. Statistical significance tests for-
machine translation evaluation. In Proceedings of
the Empirical Methods in Natural Language Pro-
cessing Conference (EMNLP?04), Barcelona, Spain.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 228?231, Prague, Czech Repub-
lic.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus.
417
In NEMLAR Conference on Arabic Language Re-
sources and Tools, pages 102?109, Cairo, Egypt.
Evgeny Matusov and Selc?uk Ko?pru?. 2010. Improv-
ing reordering in statistical machine translation from
farsi. In AMTA The Ninth Conference of the Associ-
ation for Machine Translation in the Americas, Den-
ver, Colorado, USA.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?52.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160?167. As-
sociation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?318,
Philadelphia, PA.
Mohammad Sadegh Rasooli, Manouchehr Kouhestani,
and Amirsaeid Moloodi. 2013. Development of
a Persian syntactic dependency treebank. In The
2013 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies (NAACL HLT), At-
lanta, USA.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), volume 2, pages 901?904, Denver,
CO.
Masao Utiyama and Hitoshi Isahara. 2007. A com-
parison of pivot methods for phrase-based statistical
machine translation. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
484?491, Rochester, New York, April. Association
for Computational Linguistics.
Hua Wu and Haifeng Wang. 2009. Revisiting pivot
language approach for machine translation. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 154?162, Suntec, Singapore, August.
Association for Computational Linguistics.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based sta-
tistical machine translation from english to turkish.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 454?
464, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
418
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 93?97,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The RWTH Aachen Machine Translation System for WMT 2010
Carmen Heger, Joern Wuebker, Matthias Huck, Gregor Leusch,
Saab Mansour, Daniel Stein and Hermann Ney
RWTH Aachen University
Aachen, Germany
surname@cs.rwth-aachen.de
Abstract
In this paper we describe the statisti-
cal machine translation system of the
RWTH Aachen University developed for
the translation task of the Fifth Workshop
on Statistical Machine Translation. State-
of-the-art phrase-based and hierarchical
statistical MT systems are augmented
with appropriate morpho-syntactic en-
hancements, as well as alternative phrase
training methods and extended lexicon
models. For some tasks, a system combi-
nation of the best systems was used to gen-
erate a final hypothesis. We participated
in the constrained condition of German-
English and French-English in each trans-
lation direction.
1 Introduction
This paper describes the statistical MT system
used for our participation in the WMT 2010 shared
translation task. We used it as an opportunity to in-
corporate novel methods which have been investi-
gated at RWTH over the last year and which have
proven to be successful in other evaluations.
For all tasks we used standard alignment and
training tools as well as our in-house phrase-
based and hierarchical statistical MT decoders.
When German was involved, morpho-syntactic
preprocessing was applied. An alternative phrase-
training method and additional models were tested
and investigated with respect to their effect for the
different language pairs. For two of the language
pairs we could improve performance by system
combination.
An overview of the systems and models will fol-
low in Section 2 and 3, which describe the base-
line architecture, followed by descriptions of the
additional system components. Morpho-syntactic
analysis and other preprocessing issues are cov-
ered by Section 4. Finally, translation results for
the different languages and system variants are
presented in Section 5.
2 Translation Systems
For the WMT 2010 Evaluation we used stan-
dard phrase-based and hierarchical translation sys-
tems. Alignments were trained with a variant of
GIZA++. Target language models are 4-gram lan-
guage models trained with the SRI toolkit, using
Kneser-Ney discounting with interpolation.
2.1 Phrase-Based System
Our phrase-based translation system is similar to
the one described in (Zens and Ney, 2008). Phrase
pairs are extracted from a word-aligned bilingual
corpus and their translation probability in both di-
rections is estimated by relative frequencies. Ad-
ditional models include a standard n-gram lan-
guage model, phrase-level IBM1, word-, phrase-
and distortion-penalties and a discriminative re-
ordering model as described in (Zens and Ney,
2006).
2.2 Hierarchical System
Our hierarchical phrase-based system is similar to
the one described in (Chiang, 2007). It allows for
gaps in the phrases by employing a context-free
grammar and a CYK-like parsing during the de-
coding step. It has similar features as the phrase-
based system mentioned above. For some sys-
tems, we only allowed the non-terminals in hierar-
chical phrases to be substituted with initial phrases
as in (Iglesias et al, 2009), which gave better re-
sults on some language pairs. We will refer to this
as ?shallow rules?.
2.3 System Combination
The RWTH approach to MT system combination
of the French?English systems as well as the
German?English systems is a refined version of
the ROVER approach in ASR (Fiscus, 1997) with
93
German?English French?English English?French
BLEU # Phrases BLEU # Phrases BLEU # Phrases
Standard 19.7 128M 25.5 225M 23.7 261M
FA 20.0 12M 25.9 35M 24.0 33M
Table 1: BLEU scores on Test and phrase table sizes with and without forced alignment (FA). For
German?English and English?French phrase table interpolation was applied.
additional steps to cope with reordering between
different hypotheses, and to use true casing infor-
mation from the input hypotheses. The basic con-
cept of the approach has been described by Ma-
tusov et al (2006). Several improvements have
been added later (Matusov et al, 2008). This ap-
proach includes an enhanced alignment and re-
ordering framework. Alignments between the sys-
tems are learned by GIZA++, a one-to-one align-
ment is generated from the learned state occupa-
tion probabilities.
From these alignments, a confusion network
(CN) is then built using one of the hypotheses as
?skeleton? or ?primary? hypothesis. We do not
make a hard decision on which of the hypothe-
ses to use for that, but instead combine all pos-
sible CNs into a single lattice. Majority voting on
the generated lattice is performed using the prior
probabilities for each system as well as other sta-
tistical models such as a special trigram language
model. This language model is also learned on
the input hypotheses. The intention is to favor
longer phrases contained in individual hypotheses.
The translation with the best total score within this
lattice is selected as consensus translation. Scal-
ing factors of these models are optimized similar
to MERT using the Downhill Simplex algorithm.
As the objective function for this optimization, we
selected a linear combination of BLEU and TER
with a weight of 2 on the former; a combination
that has proven to deliver stable results on sev-
eral MT evaluation measures in preceding experi-
ments.
In contrast to previous years, we now include a
separate consensus true casing step to exploit the
true casing capabilities of some of the input sys-
tems: After generating a (lower cased) consensus
translation from the CN, we sum up the counts of
different casing variants of each word in a sen-
tence over the input hypotheses, and use the ma-
jority casing over those. In previous experiments,
this showed to work significantly better than us-
ing a fixed non-consensus true caser, and main-
tains flexibility on the input systems.
3 New Additional Models
3.1 Forced Alignment
For the German?English, French?English and
English?French language tasks we applied a
forced alignment procedure to train the phrase
translation model with the EM algorithm, sim-
ilar to the one described in (DeNero et al,
2006). Here, the phrase translation probabil-
ities are estimated from their relative frequen-
cies in the phrase-aligned training data. The
phrase alignment is produced by a modified
version of the translation decoder. In addi-
tion to providing a statistically well-founded
phrase model, this has the benefit of produc-
ing smaller phrase tables and thus allowing
more rapid experiments. For the language pairs
German?English and English?French the best
results were achieved by log-linear interpolation
of the standard phrase table with the generative
model. For French?English we directly used the
model trained by forced alignment. A detailed
description of the training procedure is given in
(Wuebker et al, 2010). Table 1 shows the system
performances and phrase table sizes with the stan-
dard phrase table and the one trained with forced
alignment after the first EM iteration. We can see
that the generative model reduces the phrase table
size by 85-90% while increasing performance by
0.3% to 0.4% BLEU.
3.2 Extended Lexicon Models
In previous work, RWTH was able to show the
positive impact of extended lexicon models that
cope with lexical context beyond the limited hori-
zon of phrase pairs and n-gram language models.
Mauser et al (2009) report improvements of
up to +1% in BLEU on large-scale systems for
Chinese?English and Arabic?English by incor-
porating discriminative and trigger-based lexicon
models into a state-of-the-art phrase-based de-
coder. They discuss how the two types of lexicon
94
models help to select content words by capturing
long-distance effects.
The triplet model is a straightforward extension
of the IBM model 1 with a second trigger, and like
the former is trained iteratively using the EM al-
gorithm. In search, the triggers are usually on the
source side, i.e., p(e|f, f ?) is modeled. The path-
constrained triplet model restricts the first source
trigger to the aligned target word, whereas the sec-
ond trigger can move along the whole source sen-
tence. See (Hasan et al, 2008) for a detailed de-
scription and variants of the model and its training.
For the WMT 2010 evaluation, triplets mod-
eling p(e|f, f ?) were trained and applied di-
rectly in search for all relevant language pairs.
Path-constrained models were trained on the in-
domain news-commentary data only and on the
news-commentary plus the Europarl data. Al-
though experience from similar setups indicates
that triplet lexicon models can be beneficial for
machine translation between the languages En-
glish, French, and German, on this year?s WMT
translation tasks slight improvements on the devel-
opment sets did not or only partially carry over to
the held-out test sets. Nevertheless, systems with
triplets were used for system combination, as ex-
tended lexicon models often help to predict con-
tent words and to capture long-range dependen-
cies. Thus they can help to find a strong consensus
hypothesis.
3.3 Unsupervised Training
Due to the small size of the English?German re-
sources available for language modeling as well as
for lexicon extraction, we decided to apply the un-
supervised adaptation suggested in (Schwenk and
Senellart, 2009). We use a baseline SMT system to
translate in-domain monolingual source data, fil-
ter the translations according to a decoder score
normalized by sentence length, add this synthetic
bilingual data to the original one and rebuild the
SMT system from scratch.
The motivation behind the method is that the
phrase table will adapt to the genre, and thus
let phrases which are domain related have higher
probabilities. Two phenomena are observed from
phrase tables and the corresponding translations:
? Phrase translation probabilities are changed,
making the system choose better phrase
translation candidates.
Running Words
English German
Bilingual 44.3M 43.4M
Dict. 1.4M 1.2M
AFP 610.7M
AFP unsup. 152.0M 157.3M
Table 2: Overview on data for unsupervised train-
ing.
BLEU
Dev Test
baseline 15.0 14.7
+dict. 15.1 14.6
+unsup.+dict 15.4 14.9
Table 3: Results for unsupervised training method.
? Phrases which appear repeatedly in the do-
main get higher probabilities, so that the de-
coder can better segment the sentence.
To implement this idea, we translate the AFP part
of the English LDC Gigaword v4.0 and obtain the
synthetic data.
To decrease the number of OOV words, we use
dictionaries from the stardict directory as addi-
tional bilingual data to translate the AFP corpus.
We filter sentences with OOV words and sentences
longer than 100 tokens. A summary of the addi-
tional data used is shown in Table 2.
We tried to use the best 10%, 20% and 40% of
the synthetic data, where the 40% option worked
best. A summary of the results is given in Table 3.
Although this is our best result for the
English?German task, it was not submitted, be-
cause the use of the dictionary is not allowed in
the constrained track.
4 Preprocessing
4.1 Large Parallel Data
In addition to the provided parallel Europarl and
news-commentary corpora, also the large French-
English news corpus (about 22.5 Mio. sentence
pairs) and the French-English UN corpus (about
7.2 Mio. sentence pairs) were available. Since
model training and tuning with such large cor-
pora takes a very long time, we extracted about
2 Mio. sentence pairs of both of these corpora. We
filter sentences with the following properties:
95
? Only sentences of minimum length of 4 to-
kens were considered.
? At least 92% of the vocabulary of each sen-
tence occur in the development set.
? The ratio of the vocabulary size of a sen-
tence and the number of its tokens is mini-
mum 80%.
4.2 Morpho-Syntactic Analysis
German, as a flexible and morphologically rich
language, raises a couple of problems in machine
translation. We picked two major problems and
tackled them with morpho-syntactic pre- and post-
processing: compound splitting and long-range
verb reordering.
For the translation from German into English,
German compound words were split using the
frequency-based method described in (Koehn and
Knight, 2003). Thereby, we forbid certain words
and syllables to be split. For the other trans-
lation direction, the English text was first trans-
lated into the modified German language with
split compounds. The generated output was then
postprocessed by re-merging the previously gen-
erated components using the method described in
(Popovic? et al, 2006).
Additionally, for the German?English phrase-
based system, the long-range POS-based reorder-
ing rules described in (Popovic? and Ney, 2006)
were applied on the training and test corpora as a
preprocessing step. Thereby, German verbs which
occur at the end of a clause, like infinitives and
past participles, are moved towards the beginning
of that clause. With this, we improved our baseline
phrase-based system by 0.6% BLEU.
5 Experimental Results
For all translation directions, we used the provided
parallel corpora (Europarl, news) to train the trans-
lation models and the monolingual corpora to train
BLEU
Dev Test
phrase-based baseline 19.9 19.2
phrase-based (+POS+mero+giga) 21.0 20.3
hierarchical baseline 20.2 19.6
hierarchical (+giga) 20.5 20.1
system combination 21.4 20.4
Table 4: Results for the German?English task.
the language models. We improved the French-
English systems by enriching the data with parts of
the large addional data, extracted with the method
described in Section 4.1. Depending on the sys-
tem this gave an improvement of 0.2-0.7% BLEU.
We also made use of the large giga-news as well
as the LDC Gigaword corpora for the French and
English language models. All systems were opti-
mized for BLEU score on the development data,
newstest2008. The newstest2009 data is
used as a blind test set.
In the following, we will give the BLEU scores
for all language tasks of the baseline system and
the best setup for both, the phrase-based and the
hierarchical system. We will use the following
notations to indicate the several methods we used:
(+POS) POS-based verb reordering
(+mero) maximum entropy reordering
(+giga) including giga-news and
LDC Gigaword in LM
(fa) trained by forced alignment
(shallow) allow only shallow rules
We applied system combination of up to 6 sys-
tems with several setups. The submitted systems
are marked in tables 4-7.
6 Conclusion
For the participation in the WMT 2010 shared
translation task, RWTH used state-of-the-art
phrase-based and hierarchical translation systems.
To deal with the rich morphology and word or-
der differences in German, compound splitting
and long range verb reordering were applied in a
preprocessing step. For the French-English lan-
guage pairs, RWTH extracted parts of the large
news corpus and the UN corpus as additional
training data. Further, training the phrase trans-
lation model with forced alignment yielded im-
provements in BLEU. To obtain the final hypothe-
sis for the French?English and German?English
BLEU
Dev Test
phrase-based baseline 14.8 14.5
phrase-based (+mero) 15.0 14.7
hierarchical baseline 14.2 13.9
hierarchical (shallow) 14.5 14.3
Table 5: Results for the English?German task.
96
BLEU
Dev Test
phrase-based baseline 21.8 25.1
phrase-based (fa+giga) 23.0 26.1
hierarchical baseline 21.9 25.0
hierarchical (shallow+giga) 22.7 25.6
system combination 23.1 26.1
Table 6: Results for the French?English task.
BLEU
Dev Test
phrase-based baseline 20.9 23.2
phrase-based (fa+mero+giga) 23.0 24.6
hierarchical baseline 20.6 22.5
hierarchical (shallow,+giga) 22.4 24.3
Table 7: Results for the English?French task.
language pairs, RWTH applied system combina-
tion. Altogether, by application of these meth-
ods RWTH was able to increase performance in
BLEU by 0.8% for German?English, 0.2% for
English?German, 1.0% for French?English and
1.4% for English?French on the test set over the
respective baseline systems.
Acknowledgments
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006.
Why Generative Phrase Models Underperform Sur-
face Heuristics. In Proceedings of the Workshop on
Statistical Machine Translation, pages 31?38.
J.G. Fiscus. 1997. A Post-Processing System to Yield
Reduced Word Error Rates: Recognizer Output Vot-
ing Error Reduction (ROVER). In IEEE Workshop
on Automatic Speech Recognition and Understand-
ing.
S. Hasan, J. Ganitkevitch, H. Ney, and J. Andre?s-
Ferrer. 2008. Triplet Lexicon Models for Statisti-
cal Machine Translation. In Proceedings of Emperi-
cal Methods of Natural Language Processing, pages
372?381.
G. Iglesias, A. de Gispert, E.R. Banga, and W. Byrne.
2009. Rule Filtering by Pattern for Efficient Hierar-
chical Translation. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL (EACL
2009), pages 380?388.
P. Koehn and K. Knight. 2003. Empirical Methods for
Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine trans-
lation systems using enhanced hypotheses align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Marino, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
A. Mauser, S. Hasan, and H. Ney. 2009. Extend-
ing Statistical Machine Translation with Discrimi-
native and Trigger-Based Lexicon Models. In Con-
ference on Empirical Methods in Natural Language
Processing, pages 210?217.
M. Popovic? and H. Ney. 2006. POS-based Word Re-
orderings for Statistical Machine Translation. In In-
ternational Conference on Language Resources and
Evaluation, pages 1278?1283.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Nat-
ural Language Processing, Springer Verlag, LNCS,
pages 616?624.
H. Schwenk and J. Senellart. 2009. Translation Model
Adaptation for an Arabic/French News Translation
System by Lightly-Supervised Training. In MT
Summit XII.
J. Wuebker, A. Mauser, and H. Ney. 2010. Training
Phrase Translation Models with Leaving-One-Out.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics. To ap-
pear.
R. Zens and H. Ney. 2006. Discriminative Reorder-
ing Models for Statistical Machine Translation. In
Workshop on Statistical Machine Translation, pages
55?63.
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statis-
tical Machine Translation. In International Work-
shop on Spoken Language Translation.
97
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 315?320,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The RWTH System Combination System for WMT 2010
Gregor Leusch and Hermann Ney
RWTH Aachen University
Aachen, Germany
{leusch,ney}@cs.rwth-aachen.de
Abstract
RWTH participated in the System Combi-
nation task of the Fifth Workshop on Sta-
tistical Machine Translation (WMT 2010).
For 7 of the 8 language pairs, we com-
bine 5 to 13 systems into a single con-
sensus translation, using additional n-best
reranking techniques in two of these lan-
guage pairs. Depending on the language
pair, improvements versus the best sin-
gle system are in the range of +0.5 and
+1.7 on BLEU, and between ?0.4 and
?2.3 on TER. Novel techniques compared
with RWTH?s submission to WMT 2009
include the utilization of n-best reranking
techniques, a consensus true casing ap-
proach, a different tuning algorithm, and
the separate selection of input systems
for CN construction, primary/skeleton hy-
potheses, HypLM, and true casing.
1 Introduction
The RWTH approach to MT system combination
is a refined version of the ROVER approach in
ASR (Fiscus, 1997), with additional steps to cope
with reordering between different hypotheses, and
to use true casing information from the input hy-
potheses. The basic concept of the approach has
been described by Matusov et al (2006). Several
improvements have been added later (Matusov et
al., 2008). This approach includes an enhanced
alignment and reordering framework. In con-
trast to existing approaches (Jayaraman and Lavie,
2005; Rosti et al, 2007), the context of the whole
corpus rather than a single sentence is considered
in this iterative, unsupervised procedure, yielding
a more reliable alignment. Majority voting on the
generated lattice is performed using prior weights
for each system as well as other statistical mod-
els such as a special n-gram language model. In
addition to lattice rescoring, n-best list reranking
techniques can be applied to n best paths of this
lattice. True casing is considered a separate step
in RWTH?s approach, which also takes the input
hypotheses into account.
The pipeline, and consequently the description
of the pipeline given in this paper, is based on our
pipeline for WMT 2009 (Leusch et al, 2009), with
several extensions as described.
2 System Combination Algorithm
In this section we present the details of our system
combination method. Figure 1 gives an overview
of the system combination architecture described
in this section. After preprocessing the MT hy-
potheses, pairwise alignments between the hy-
potheses are calculated. The hypotheses are then
reordered to match the word order of a selected
primary or skeleton hypothesis. From this, we
create a lattice which we then rescore using sys-
tem prior weights and a language model (LM).
The single best path in this CN then constitutes
the consensus translation; alternatively the n best
paths are generated and reranked using additional
statistical models. The consensus translation is
then true cased and postprocessed.
2.1 Word Alignment
The proposed alignment approach is a statistical
one. It takes advantage of multiple translations for
a whole corpus to compute a consensus translation
for each sentence in this corpus. It also takes ad-
vantage of the fact that the sentences to be aligned
are in the same language.
For each of the K source sentences in the
test corpus, we select one of its translations
En, n = 1, . . . ,M, as the primary hypothesis.
Then we align the secondary hypotheses Em(m=
1, . . . ,M ;n 6= m) with En to match the word or-
der in En. Since it is not clear which hypothesis
should be primary, i. e. has the ?best? word order,
we let several or all hypothesis play the role of the
primary translation, and align all pairs of hypothe-
ses (En, Em); n 6= m. In this paper, we denote
the number of possible primary hypotheses by N .
The word alignment is trained in analogy to
the alignment training procedure in statistical MT.
The difference is that the two sentences that have
to be aligned are in the same language. We use the
IBM Model 1 (Brown et al, 1993) and the Hid-
den Markov Model (HMM, (Vogel et al, 1996))
315
alignmentGIZA++- Network generation Weighting&RescoringReordering 200-bestlist
Hyp 1
Hyp k... ConsensusTranslation
nbestrescoring(Triplets,LM, ...)
Figure 1: The system combination architecture.
to estimate the alignment model.
The alignment training corpus is created from a
test corpus of effectively N ?(M?1)?K sentences
translated by the involved MT engines. Model pa-
rameters are trained iteratively using the GIZA++
toolkit (Och and Ney, 2003). The training is per-
formed in the directions Em ? En and En ?
Em. The final alignments are determined using
a cost matrix C for each sentence pair (Em, En).
Elements of this matrix are the local costs C(j, i)
of aligning a word em,j from Em to a word en,i
from En. Following Matusov et al (2004), we
compute these local costs by interpolating the
negated logarithms of the state occupation proba-
bilities from the ?source-to-target? and ?target-to-
source? training of the HMM model.
2.2 Word Reordering and Confusion
Network Generation
After reordering each secondary hypothesis Em
and the rows of the corresponding alignment cost
matrix, we determine M?1 monotone one-to-one
alignments between En as the primary translation
and Em,m = 1, . . . ,M ;m 6= n. We then con-
struct the confusion network.
We consider words without a correspondence to
the primary translation (and vice versa) to have a
null alignment with the empty word ?, which will
be transformed to an ?-arc in the corresponding
confusion network.
The M?1 monotone one-to-one alignments can
then be transformed into a confusion network, as
described by Matusov et al (2008).
2.3 Voting in the Confusion Network
Instead of choosing a fixed sentence to define the
word order for the consensus translation, we gen-
erate confusion networks for N possible hypothe-
ses as primary, and unite them into a single lattice.
In our experience, this approach is advantageous
in terms of translation quality compared to a min-
imum Bayes risk primary (Rosti et al, 2007).
Weighted majority voting on a single confu-
sion network is straightforward and analogous to
ROVER (Fiscus, 1997). We sum up the probabil-
ities of the arcs which are labeled with the same
word and have the same start state and the same
end state. This can also be regarded as having a
binary system feature in a log-linear model.
2.4 Language Models
The lattice representing a union of several confu-
sion networks can then be directly rescored with
an n-gram language model (LM). A transforma-
tion of the lattice is required, since LM history has
to be memorized.
We train a trigram LM on the outputs of the sys-
tems involved in system combination. For LM
training, we take the system hypotheses for the
same test corpus for which the consensus transla-
tions are to be produced. Using this ?adapted? LM
for lattice rescoring thus gives bonus to n-grams
from the original system hypotheses, in most cases
from the original phrases. Presumably, many of
these phrases have a correct word order. Previous
experimental results show that using this LM in
rescoring together with a word penalty notably im-
proves translation quality. This even results in bet-
ter translations than using a ?classical? LM trained
on a monolingual training corpus. We attribute
this to the fact that most of the systems we com-
bine already include such general LMs.
2.5 Extracting Consensus Translations
To generate our consensus translation, we extract
the single-best path from the rescored lattice, us-
ing ?classical? decoding as in MT. Alternatively,
we can extract the n best paths for n-best list
rescoring.
2.6 n-best-List Reranking
If n-best lists were generated in the previous steps,
additional sentence-based features can be calcu-
lated on these sentences, and combined in a log-
linear way. These scores can then be used to re-
rank the sentences.
For the WMT 2010 FR?EN and the DE?EN
task, we generated 200-best lists, and calculated
the following features:
1. Total score from the lattice rescoring
2. NGram posterior weights on those (Zens and
Ney, 2006)
3. Word Penalty
4. HypLM trained on a different set of hypothe-
ses (FR?EN only)
5. Large fourgram model trained on Gigaword
(DE?EN) or Europarl (FR?EN)
6. IBM1 scores and deletion counts based on a
word lexicon trained on WMT training data
316
7. Discriminative word lexicon score (Mauser et
al., 2009)
8. Triplet lexicon score (Hasan et al, 2008)
Other features were also calculated, but did not
seem to give an improvement on the DEV set.
2.7 Consensus True Casing
Previous approaches to achieve true cased output
in system combination operated on true-cased lat-
tices, used a separate input-independent true caser,
or used a general true-cased LM to differenti-
ate between alternative arcs in the lattice, as in
(Leusch et al, 2009). For WMT 2010, we use
per-sentence information from the input systems
to determine the consensus case of each output
word. Lattice generation, rescoring, and rerank-
ing are performed on lower-cased input, with a
lower-cased consensus hypothesis as their result.
For each word in this hypothesis, we count how
often each casing variant occurs in the input hy-
potheses for this sentence. We then use the vari-
ant with the highest support for the final consen-
sus output. One advantage is that the set of sys-
tems used to determine the consensus case does
not have to be identical to those used for building
the lattice: Assuming that each word from the con-
sensus hypothesis also occurs in one or several of
the true casing input hypotheses, we can focus on
systems that show a good true casing performance.
3 Tuning
3.1 Tuning Weights for Lattice and n-best
Rescoring
For lattice rescoring, we need to tune system
weights, LM factor, and word penalty to produce
good consensus translations. The same holds for
the log-linear weights in n-best reranking.
For the WMT 2010 Workshop, we selected
a linear combination of BLEU (Papineni et al,
2002) and TER (Snover et al, 2006) as optimiza-
tion criterion, ?? := argmax? {BLEU ? TER},
based on previous experience (Mauser et al,
2008). For more stable results, we use the case-
insensitive variants for both measures, despite the
explicit use of case information in the pipeline.
System weights were tuned to this criterion us-
ing the Downhill Simplex method. Because we
considered the number of segments in the tuning
set to be too small to allow for a further split into
an actual tuning and a control (dev) part, we went
for a method closely related to 5-fold cross valida-
tion: We randomly split the tuning set into 5 equal-
sized parts, and tune parameters on four fifth of
the set, measuring progress on the remaining fifth.
This was repeated for the other four choices for the
?dev? part. Only settings which reliably showed
progress on these five different versions were used
later on the test set. For the actual weights and
numerical parameters to be used on the test set,
we calculate the median of the five variants, which
lowered the risk of outliers and overfitting.
3.2 System Selection
With the large numbers of input systems ? e.g., 17
for DE?EN ? and their large spread in translation
quality ? e.g. 10% abs. in BLEU ? not all sys-
tems should participate in the system combination
process. For the generation of lattices, we con-
sidered several variants of systems, often starting
from the top, and either replacing some of the sys-
tems very similar to others with systems further
down the list, or not considering those as primary,
adding further systems as additional secondaries.
For true casing, and the additional HypLM for
FR?EN, we selected a set of 8 to 12 promising
systems, and ran an exhaustive search on all com-
binations of those to optimize the LM perplexity
on the dev set (LM) or the true case BLEU/TER
score on a consensus translation (TC). Further re-
search may include a weighted combination here,
followed by an optimization of the weights as de-
scribed in the previous paragraph.
4 Experimental Results
Each language pair and each direction in
WMT 2010 had its own set of systems, so we se-
lected and tuned for each direction separately. Af-
ter submission of our system combination output
to WMT 2010, we also calculated scores on the
test set (TEST), to validate our results, and as a
preparation for this report. Note that the scores re-
ported for DEV are calculated on the full DEV set,
but not on any combination of the one-fifth ?cross
validation? subcorpora.
4.1 FR?EN and EN?FR
For French?English, we selected a set of eight
systems for the primary submission, and eleven
systems for the contrastive system, of which six
served as skeleton. Six different systems were
used for an additional HypLM, five for consen-
sus true casing. Table 1 shows the distribution of
these systems. We see the results of system com-
bination on DEV and TEST (the latter calculated
after submission) in Table 2. System combination
itself turns out to have the largest improvement,
+0.5 in BLEU and -0.7 in TER on TEST over the
best single system. n-best reranking improves this
result even more, by +0.3/-0.3. The influence of
tuning and of TC selection is measurable on DEV,
but rather small on TEST.
For English?French, 13 systems were used to
construct the lattice, 5 serving as skeleton. Five
different systems were used for true casing. No
n-best list reranking was performed here, as pre-
liminary experiments did not show any significant
317
Table 1: Overview of systems used for FR/EN.
System FR?EN EN?FR
A B A B
cambridge P L C p P p
cu-zeman S
cmu-statxfer L s
dfki S
eu S
geneva S
huicong s
jhu P L p S p
koc S
lig s
limsi P C p S C p
lium P L C s P C p
nrc P C s S p
rali P L p P C p
rwth P p P C p
uedin P L C p P C p
?A? is the primary, ?B? the contrastive submission.
?P? denotes a system that served as skeleton.
?S? a system that was only aligned to others.
?L? denotes a system used for a larger HypLM-n-best-
rescoring.
?C? is a system used for consensus true casing.
Table 2: Results for FR?EN.
TUNE TEST
BLEU TER BLEU TER
Best single 27.9 55.4 28.5 54.0
Lattice SC 28.4 55.0 29.0 53.3
+ tuning 28.8 54.5 29.1 53.3
+ CV tuning 28.6 54.7 29.1 53.3
+ nbest rerank. 29.0 54.4 29.4 53.0
+ sel. for TC 29.1 54.3 29.3 53.0
Contrast. SC 28.9 54.3 28.8 53.4
?SC? stands for System Combination output.
?CV? denotes the split into five different tuning and valida-
tion parts.
?sel. TC? is the separate selection for consensus true casing.
Systems in bold were submitted for WMT 2010.
Table 3: Results for EN?FR.
TUNE TEST
BLEU TER BLEU TER
Best single 27.1 55.7 26.5 56.1
Primary SC 28.3 55.2 28.2 54.7
Contrast. SC 28.5 54.7 28.1 54.6
Table 4: Overview of systems used for DE/EN.
System DE?EN EN?DE
A B A B
cu-zeman S
cmu C P
dfki S p
fbk P C p P
jhu p
kit P C p P C p
koc S C p
limsi P p P C p
liu C S C p
rwth P p P C p
sfu S
uedin P C p P C p
umd P p
uppsala p S
For abbreviations see Table 1.
Table 5: Results for DE?EN.
TUNE TEST
BLEU TER BLEU TER
Best single 23.8 59.7 23.5 59.7
Lattice SC 24.7 58.5 25.0 57.9
+ tuning 25.1 57.6 25.0 57.6
+ CV tuning 24.8 58.0 24.9 57.8
+ nbest rerank. 25.3 57.6 24.9 57.6
+ sel. for TC 25.5 57.5 24.9 57.6
Contrast. SC 25.2 57.7 24.8 57.7
For abbreviations see Table 2.
gain in this direction. As a contrastive submission,
we submitted the consensus of 8 systems. These
are also listed in Table 1. The results can be found
in Table 3. Note that the contrastive system was
not tuned using the ?cross validation? approach;
as a result, we expected it to be sensitive to over-
fitting. We see improvements around +1.7/-1.4 on
TEST.
4.2 DE?EN and EN?DE
In the German?English language pair, 17 systems
were available, but incorporating only six of them
turned out to deliver optimal results on DEV. As
shown in Table 4, we used a combination of seven
systems in the contrastive submission. While a
Table 6: Results for EN?DE.
TUNE TEST
BLEU TER BLEU TER
Best single 16.1 66.3 16.4 65.7
Primary SC 16.4 64.9 17.0 63.7
Contrast. SC 16.4 64.9 17.3 63.4
318
Table 7: Overview of systems used for CZ/EN.
System CZ?EN EN?CZ
aalto P
cmu P C
cu-bojar P P
cu-tecto S
cu-zeman P S C
dcu P
eurotrans S
google P C P C
koc P C
pc-trans S
potsdam P C
sfu S
uedin P C P C
For abbreviations see Table 1.
No contrastive systems were built for this language pair.
Table 8: Results for CZ?EN and EN?CZ.
TUNE TEST
BLEU TER BLEU TER
CZ?EN
Best single 21.8 58.4 22.9 57.5
Primary SC 22.4 59.1 23.4 57.9
EN?CZ
Best single 17.0 67.1 16.6 66.4
Primary SC 16.7 65.4 17.4 63.6
different set of five systems was used for consen-
sus true casing, it turned out that using the same
six systems for the ?additional? HypLM as for
the lattice seemed to be optimal in our approach.
Table 5 shows the outcome of our experiments:
Again, we see that the largest effect on TEST re-
sults from system combination as such (+1.5/-1.8).
The other steps, in particular tuning and selection
for TC, seem to help on DEV, but make hardly
a difference on TEST. n-best reranking brings an
improvement of -0.2 in TER, but at a minor dete-
rioration (-0.1) in BLEU.
In the opposite direction, English?German, we
combined all twelve systems, five of them serv-
ing as skeleton. The contrastive submission con-
sists of a combination of eight systems. Six sys-
tems were used for true casing. Again, n-best
list rescoring did not result in any improvement
in preliminary experiments, and was skipped. Re-
sults are shown in Table 6: We see that even
though both versions perform equally well on
DEV (+0.4/-1.4), the contrastive system performs
better by +0.3/-0.3 on TEST (+0.9/-2.3).
4.3 CZ?EN and EN?CZ
In both directions involving Czech, the number of
systems was rather limited, so no additional se-
Table 9: Overview of systems used for ES/EN.
System EN?ES
A B
cambridge P C p
dcu P p
dfki P C p
jhu P C p
sfu P C p
uedin P C p
upv p
upv-nnlm P p
Table 10: Results for EN?ES.
TUNE TEST
BLEU TER BLEU TER
ES?EN
Best single 28.7 53.6 ? ?
SC 29.0 53.3 ? ?
EN?ES
Best single 27.8 55.2 28.7 54.0
Primary SC 29.5 52.9 30.0 51.4
Contrast. SC 29.6 52.8 30.1 51.7
lection turned out to be necessary, and we did not
build a contrastive system. For Czech?English, all
six systems were used; three of them for true cas-
ing. For English?Czech, all eleven systems were
used in building the lattice, six of them also as
skeleton. Five systems were used in the true cas-
ing step. Table 7 lists these systems. From the
results in Table 8, we see that for CZ?EN, system
combination gains around +0.5 in BLEU, but at
costs of +0.4 to +0.7 in TER. For EN?CZ, the re-
sults look more positive: While we see only -0.3/-
1.7 on DEV, there is a significant improvement of
+1.2/-2.8 on TEST.
4.4 ES?EN and EN?ES
In the Spanish?English language pair, we did not
see any improvement at all on the direction with
English as target in preliminary experiments. Con-
sequently, and given the time constraints, we did
not further investigate on this language pair. Post-
eval experiments revealed that improvements of
+0.3/-0.3 are possible, with far off-center weights
favoring the top three systems.
On English?Spanish, where these preliminary
experiments showed a gain, we used seven out of
the available ten systems in building the lattice
for the primary system, eight for the contrastive.
Five of those were uses for consensus true cas-
ing. Table 9 lists these systems. Table 10 shows
the results on this language pair: For both the pri-
mary and the contrastive systems we see improve-
319
ments of around +1.7/-2.3 on DEV, and +1.3/-2.6
on TEST. Except for the TER on TEST, these two
submissions differ only by ?0.1 from each other.
5 Conclusions
We have shown that our system combination sys-
tem can lead to significant improvements over sin-
gle best MT output where a significant number of
comparably good translations is available on a sin-
gle language pair. n-best reranking can further
improve the quality of the consensus translation;
results vary though. While consensus true casing
turned out to be very useful despite of its simplic-
ity, we were unable to find significant improve-
ments on TEST from the selection of a separate
set of true casing input systems.
Acknowledgments
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. This work was
partly supported by the Defense Advanced Re-
search Projects Agency (DARPA) under Contract
No. HR0011-06-C-0023.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263?311, June.
J. Fiscus. 1997. A post-processing system to yield re-
duced word error rates: Recognizer output voting er-
ror reduction (ROVER). In IEEE Workshop on Au-
tomatic Speech Recognition and Understanding.
S. Hasan, J. Ganitkevitch, H. Ney, and J. Andre?s-Ferrer.
2008. Triplet lexicon models for statistical machine
translation. In Conference on Empirical Methods in
Natural Language Processing, pages 372?381, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
S. Jayaraman and A. Lavie. 2005. Multi-engine ma-
chine translation guided by explicit word matching.
In Proc. of the 10th Annual Conf. of the European
Association for Machine Translation (EAMT), pages
143?152, Budapest, Hungary, May.
G. Leusch, E. Matusov, and H. Ney. 2009. The
RWTH system combination system for WMT 2009.
In Fourth Workshop on Statistical Machine Transla-
tion, pages 56?60, Athens, Greece, March. Associa-
tion for Computational Linguistics.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
word alignments for statistical machine translation.
In COLING ?04: The 20th Int. Conf. on Computa-
tional Linguistics, pages 219?225, Geneva, Switzer-
land, August.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine trans-
lation systems using enhanced hypotheses align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40, Trento, Italy, April.
E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y. S. Lee,
J. B. Marino, M. Paulik, S. Roukos, H. Schwenk,
and H. Ney. 2008. System combination for machine
translation of spoken and written language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237, September.
A. Mauser, S. Hasan, and H. Ney. 2008. Automatic
evaluation measures for statistical machine transla-
tion system optimization. In International Confer-
ence on Language Resources and Evaluation, Mar-
rakech, Morocco, May.
A. Mauser, S. Hasan, and H. Ney. 2009. Extending sta-
tistical machine translation with discriminative and
trigger-based lexicon models. In Conference on Em-
pirical Methods in Natural Language Processing,
pages 210?217, Singapore, August.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51, March.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311?318, Philadelphia, PA, July.
A. V. Rosti, S. Matsoukas, and R. Schwartz. 2007.
Improved word-level system combination for ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics (ACL), pages 312?319, Prague, Czech Re-
public, June.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Error
Rate with Targeted Human Annotation. In Proc. of
the 7th Conf. of the Association for Machine Trans-
lation in the Americas (AMTA), pages 223?231,
Boston, MA, August.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
COLING ?96: The 16th Int. Conf. on Computational
Linguistics, pages 836?841, Copenhagen, Denmark,
August.
R. Zens and H. Ney. 2006. N-gram posterior prob-
abilities for statistical machine translation. In Hu-
man Language Technology Conf. / North American
Chapter of the Assoc. for Computational Linguistics
Annual Meeting (HLT-NAACL), Workshop on Statis-
tical Machine Translation, pages 72?77, New York
City, June.
320
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 152?158,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The RWTH System Combination System for WMT 2011
Gregor Leusch, Markus Freitag, and Hermann Ney
RWTH Aachen University
Aachen, Germany
{leusch,freitag,ney}@cs.rwth-aachen.de
Abstract
RWTH participated in the System Combi-
nation task of the Sixth Workshop on Sta-
tistical Machine Translation (WMT 2011).
For three language pairs, we combined
6 to 14 systems into a single consen-
sus translation. A three-level meta-
combination scheme combining six dif-
ferent system combination setups with
three different engines was applied on the
French?English language pair. Depend-
ing on the language pair, improvements
versus the best single system are in the
range of +1.9% and +2.5% abs. on
BLEU, and between ?1.8% and ?2.4%
abs. on TER. Novel techniques compared
with RWTH?s submission to WMT 2010
include two additional system combina-
tion engines, an additional word alignment
technique, meta combination, and addi-
tional optimization techniques.
1 Introduction
RWTH?s main approach to System Combination
(SC) for Machine Translation (MT) is a refined
version of the ROVER approach in Automatic
Speech Recognition (ASR) (Fiscus, 1997), with
additional steps to cope with reordering between
different hypotheses, and to use true casing infor-
mation from the input hypotheses. The basic con-
cept of the approach has been described by Ma-
tusov et al (2006). Several improvements have
been added later (Matusov et al, 2008). This ap-
proach includes an enhanced alignment and re-
ordering framework. In contrast to existing ap-
proaches (Jayaraman and Lavie, 2005; Rosti et
al., 2007b), the context of the whole corpus rather
than a single sentence is considered in this itera-
tive, unsupervised procedure, yielding a more reli-
able alignment. Majority voting on the generated
lattice is performed using prior weights for each
system as well as other statistical models such
as a special n-gram language model. True cas-
ing is considered a separate step in RWTH?s ap-
proach, which also takes the input hypotheses into
account. The pipeline, and consequently the de-
scription of the main pipeline given in this paper, is
based on our pipeline for WMT 2010 (Leusch and
Ney, 2010), with extensions as described. When
necessary, we denote this pipeline as Align-to-
Lattice, or A2L .
For the French?English task, we used two ad-
ditional system combination engines for the first
time: The first one uses the same alignments as
A2L, but generates lattices in the OpenFST frame-
work (Allauzen et al, 2007). The OpenFST de-
coder (fstshortestpath) is then used to find
the best path (consensus translation) in this lattice.
Analogously, we call this engine A2FST . The sec-
ond additional engine, which we call SCUNC, uses
a TER-based alignment, similar to the approach by
Rosti et al (2007b). Instead of a lattice rescor-
ing, finding the consensus translation is consid-
ered a per-node classification problem: For each
slot, which one is the ?correct? one (i.e. will give
the ?best? output)? This approach is inspired by
iROVER (Hillard et al, 2007). Consensus trans-
lations from different settings of these approaches
could then be combined again by an additional ap-
plication of system combination ? which we refer
to as meta combination (Rosti et al, 2007a). These
three approaches are described in more detail in
Section 2. In Section 3 we describe how we tuned
the parameters and decisions of our system combi-
nation approaches for WMT 2011. Section 4 then
lists our experimental setup as well as the experi-
mental results we obtained on the WMT 2011 sys-
tem combination track. We conclude this paper in
Section 5.
2 System Combination Algorithm (A2L)
In this section we present the details of our main
system combination method, A2L. The upper part
of Figure 1 gives an overview of the system combi-
nation architecture described in this section. After
preprocessing the MT hypotheses, pairwise align-
152
ments between the hypotheses are calculated. The
hypotheses are then reordered to match the word
order of a selected primary (skeleton) hypothesis.
From this, we create a confusion network (CN)
which we then rescore using system prior weights
and a language model (LM). The single best path
in this CN then constitutes the consensus transla-
tion. The consensus translation is then true cased
and post processed.
2.1 Word Alignment
The main proposed alignment approach is a statis-
tical one. It takes advantage of multiple transla-
tions for a whole corpus to compute a consensus
translation for each sentence in this corpus. It also
takes advantage of the fact that the sentences to be
aligned are in the same language.
For each of the K source sentences in the test
corpus, we select one of its N translations from
different MT systems E,m=1, . . . , N, as the pri-
mary hypothesis. Then we align the secondary hy-
potheses En(n=1, . . . , ;n 6=m) with En to match
the word order in En. Since it is not clear which
hypothesis should be primary, i. e. has the ?best?
word order, we let several or all hypothesis play
the role of the primary translation, and align all
pairs of hypotheses (En, Em); n 6= m.
The word alignment is trained in analogy to
the alignment training procedure in statistical MT.
The difference is that the two sentences that have
to be aligned are in the same language. We use the
IBM Model 1 (Brown et al, 1993) and the Hid-
den Markov Model (HMM, (Vogel et al, 1996))
to estimate the alignment model.
The alignment training corpus is created from a
test corpus of effectively N ?(N?1) ?K sentences
translated by the involved MT engines. Model pa-
rameters are trained iteratively using the GIZA++
toolkit (Och and Ney, 2003). The training is per-
formed in the directions Em ? En and En ?
Em. The final alignments are determined using
a cost matrix C for each sentence pair (Em, En).
Elements of this matrix are the local costs C(j, i)
of aligning a word em,j from Em to a word en,i
from En. Following Matusov et al (2004), we
compute these local costs by interpolating the
negated logarithms of the state occupation proba-
bilities from the ?source-to-target? and ?target-to-
source? training of the HMM model.
A different approach that has e.g. been pro-
posed by Rosti et al (2007b) is the utilization of a
TER alignment (Snover et al, 2006) for this pur-
pose. Because the original TER is insensitive to
small changes in spellings, synonyms etc., it has
been proposed to use more complex variants, e.g.
TERp. For our purposes, we utilized ?poor-man?s-
stemming?, i.e. shortening each word to its first
four characters when calculating the TER align-
ment. Since a TER alignment already implies a
reordering between the primary and the secondary
hypothesis, an explicit reordering step is not nec-
essary.
2.2 Word Reordering and Confusion
Network Generation
After reordering each secondary hypothesis Em
and the rows of the corresponding alignment cost
matrix, we determine N ? 1 monotone one-to-one
alignments between En as the primary translation
and Em,m = 1, . . . , N ;m 6= n. We then con-
struct the confusion network.
We consider words without a correspondence to
the primary translation (and vice versa) to have a
null alignment with the empty word ?, which will
be transformed to an ?-arc in the corresponding
confusion network.
The N?1 monotone one-to-one alignments can
then be transformed into a confusion network, as
described by Matusov et al (2008).
2.3 Voting in the Confusion Network (A2L,
A2FST)
Instead of choosing a fixed sentence to define the
word order for the consensus translation, we gen-
erate confusion networks for N possible hypothe-
ses as primary, and unite them into a single lattice.
In our experience, this approach is advantageous
in terms of translation quality compared to a min-
imum Bayes risk primary (Rosti et al, 2007b).
Weighted majority voting on a single confu-
sion network is straightforward and analogous to
ROVER (Fiscus, 1997). We sum up the probabil-
ities of the arcs which are labeled with the same
word and have the same start state and the same
end state.
Compared to A2L, our new A2FST engine al-
lows for a higher number of features for each arc.
Consequently, we add a binary system feature for
each system in addition to the logarithm of the sum
of system weights, as before. The advantage of
these features is that the weights are linear within
a log-linear model, as opposed to be part of a loga-
rithmic sum. Consequently they can later be opti-
mized using techniques designed for linear feature
weights, such as MERT, or MIRA.
2.4 Language Models
The lattice representing a union of several confu-
sion networks can then be directly rescored with
an n-gram language model (LM). When regarding
153
alignmentGIZA++-/TER- Network generation Weighting&Rescoring& Reordering
Hyp 1
Hyp k... ConsensusTranslation
CreatingClassificationProblem& Features
Classificationwithin eachslot ConsensusTranslation
A2L, A2FST
SCUNC
ShortestPath
Path of"recognized"arcs
Figure 1: The system combination architecture.
the lattice as a weighted Finite State Transducer
(FST), this can be regarded (and implemented) as
composition with a LM FST.
In our approach, we train a trigram LM on the
outputs of the systems involved in system combi-
nation. For LM training, we take the system hy-
potheses for the same test corpus for which the
consensus translations are to be produced. Using
this ?adapted? LM for lattice rescoring thus gives
bonus to n-grams from the original system hy-
potheses, in most cases from the original phrases.
Presumably, many of these phrases have a correct
word order. Previous experimental results show
that using this LM in rescoring together with a
word penalty notably improves translation quality.
This even results in better translations than using
a ?classical? LM trained on a monolingual train-
ing corpus. We attribute this to the fact that most
of the systems we combine already include such
general LMs. Nevertheless, one of the SC systems
we use for the French?English task (IV in Sec-
tion 4.1) uses a filtered fourgram LM trained on
GigaWord and other constrained training data sets
for this WMT tasks as an additional LM.
2.5 Extracting Consensus Translations
To generate our consensus translation, we ex-
tract the single-best path from the rescored lat-
tice, using ?classical? decoding as in MT. In A2L,
this is implemented as shortest-path decoder on a
pruned lattice. In A2FST, we use the OpenFST
fstshortestpath decoder, which does not re-
quire a pruning step for lattices of the size and den-
sity produced here.
2.6 Classification in the Confusion Network
(SCUNC)
Instead of considering the selection of the con-
sensus problem as a shortest-path problem in a
rescored confusion network, we can treat it instead
as a classification problem: For each slot (set of
outgoing arcs from one node in a CN), we consider
one or more arcs to be ?correct?, and train a clas-
sifier to identify these certain arcs. This is the idea
of the iROVER approach in ASR (Hillard et al,
2007). We call our implementation System Com-
bination Using N-gram Classifiers, or SCUNC.
For the WMT evaluation, we used the ICSI-
Boost framework (Favre et al, 2007) as classifier
(in binary mode, i.e. giving a yes/no-decision for
each single arc). We generated 109 features from
8 families: Pairwise equality of words from dif-
ferent systems, Number of votes for a word, word
that would win a simple majority voting, empty
word (also in previous two arcs), position at be-
ginning or end of sentence, cross-BLEU-S score
of hypothesis, equality of system with system of
last slot, and SRILM uni- to trigram scores. As
this approach requires strict CN instead of lattices,
a union of CNs for different primary hypotheses
was no longer possible. We decided to select
a fixed single primary system; other approaches
would have been to train an additional classifier
for this purpose, or to select a minimum-Bayes-
risk (MBR) skeleton.
2.7 Consensus True Casing
Previous approaches to achieve true cased output
in system combination operated on true-cased lat-
tices, used a separate input-independent true caser,
or used a general true-cased LM to differentiate
between alternative arcs in the lattice, as described
by Leusch et al (2009). For WMT 2011, we use
per-sentence information from the input systems
to determine the consensus case of each output
word. Lattice generation, rescoring, and rerank-
ing are performed on lower-cased input, with a
lower-cased consensus hypothesis as their result.
For each word in this hypothesis, we count how
often each casing variant occurs in the input hy-
potheses for this sentence. We then use the vari-
ant with the highest support for the final consensus
output.
154
Table 1: Corpus and Task statistics.
avg. # words #sys
TUNE DEV TEST
FR?EN 15670 11410 49832 25
DE?EN 15508 10878 49395 24
ES?EN 15989 11234 50612 15
# sent 609 394 2000
3 Tuning
3.1 Feature weights
For lattice rescoring, we selected a linear combi-
nation of BLEU (Papineni et al, 2002) and TER
(Snover et al, 2006) as optimization criterion,
?? := argmax? {BLEU ? TER} for the A2L
engine, based on previous experience (Mauser et
al., 2008). To achieve more stable results, we use
the case-insensitive variants for both measures, de-
spite the explicit use of case information in the
pipeline. System weights were tuned to this cri-
terion using the Downhill Simplex method.
In the A2FST setup, we were able to generate
full lattices, with separate costs for each individual
feature on all arcs (Power Semiring). This allowed
us to run Lattice MERT (Macherey et al, 2008)
on the full lattice, with no need for pruning (and
thus additional outer iterations for re-generating
lattices). We tried different strategies ? random
lines vs axis-parallel lines, regularization, random
restarts, etc, and selected the most stable results
on TUNE and DEV for this engine. Optimization
criterion here was BLEU.
3.2 Training a classifier for SCUNC
In MT system combination, even with given refer-
ence translations, there is no simple way to iden-
tify the ?correct? arc in a slot. This renders a
classifier-based approach even more difficult than
iROVER in ASR. The problem is even aggravated
because both the alignment of words, and their or-
der, can be incorrect already in the CN. We thus
consider an arc to be ?correct? within this task ex-
actly if it gives us the best possible total BLEU-S
score.1 These ?correct? arcs, which lie on such an
?oracle path? for BLEU-S, were therefore used as
reference classes when training the classifier.
3.3 System Selection
With the large numbers of input systems ? e.g.,
25 for FR?EN ? and their large spread in transla-
tion quality ? e.g. from 22.2 to 31.4% in BLEU
? not all systems should participate in the system
1We are looking at the sentence level, so we use BLEU-
S (Lin and Och, 2004) instead of BLEU
combination process. This is especially the case
since several of these e.g. 25 systems are often
only small variants of each other (contrastive vs.
primary submissions), which leads to a low vari-
ability of these translations. We considered several
variants of the set of input systems, often starting
from the top, and either replacing some of the sys-
tems very similar to others with systems further
down the list, or not considering those as primary,
adding further systems as additional secondaries.
Depending on the engine we were using, we se-
lected between 6 and 14 different systems as input.
4 Experimental Results
Each language pair in WMT 2011 had its own set
of systems, so we selected and tuned separately for
each language pair . Due to time constraints, we
only participated in tasks with English as the target
language. In preliminary experiments, it turned
out that System Combination was not able to get
a better result than the best single system on the
Czech?English task. Consequently, we focused
on the language pairs French?English, German?
English, and Spanish?English.
We split the available tuning data document-
wise into a 609-line TUNE set (for tuning), and a
394-line DEV set (to verify tuning results). More
statistics on these sets can be found in Table 1.
Unfortunately, late in the evaluation campaign
it turned out that the quality of several reference
sentences used in TUNE and DEV was rather low:
Many reference sentences contained spelling er-
rors, a few dozen lines even contained French
phrases or sentences within or after the English
text. We corrected many of these errors manually
in the references. In total 101 of 690 lines (16.6%)
in TUNE and 58 of 394 lines (14.7%) in DEV
were affected by this. While it was too late to re-
run all of the optimization runs, we re-optimized
at least a few final systems. All scores within this
section were calculated on the corrected reference
translations.
4.1 FR?EN
For French?English, we built in total seven differ-
ent system combination setups to generate a single
consensus translation and two contrastive transla-
tions. Figure 2 shows the structure and the data
flow of our setup for FR?EN. Table 2 lists more
details about the individual engines.
Our primary submission was focused on our ex-
perience that while rule-based MT systems (such
as RBMT-1..5 and systran) tend to have
lower BLEU scores than statistical (SMT) sys-
tems, they usually give considerable improve-
155
aalignm aaalietGIt aZlignAe+-i/T Zlignm ZalignAe+
cmu-denkowskicmu-hannemancu-zemanjhukitlia-liglimsiliumonline-Aonline-Brwth-hucksystranudeinrbmt-1rbmt-2rbmt-3rbmt-4rbmt-5
alignm
Zaalignm
primary contrastive 2contrastive 1
Bold arrows denote a system that is always considered as skeleton.
Note that there are two variants of setup II, see text.
Figure 2: System combination pipelines for FR?EN
Table 2: Engines and input systems for FR?EN.
Engine # Input submitted?
I A2L 6 RBMT
II A2L I + 6 primary
II? A2L fix I + 6 for VII
III SCUNC 6
IV A2FST GW, 8
V A2L 10 contrastive-2
VI A2FST 14
VII A2L II??VI contrastive-1
?GW? means a 4-gram LM trained on GigaWord.
II uses all skeletons, II? uses I as fixed skeleton.
Table 3: Results for FR?EN.
TUNE DEV
BLEU TER BLEU TER
kit 31.56 50.15 30.25 52.88
systran 28.18 53.32 26.50 56.07
I 27.37 54.73 26.72 57.73
II 33.69 48.47 32.45 51.09
II? 33.39 48.77 31.81 51.57
III 32.74 48.06 31.88 50.87
IV 34.16 48.31 31.95 51.64
V 33.17 48.95 32.60 51.14
VI 33.86 48.69 31.56 52.25
VII 34.41 48.20 32.15 51.49
kit is the best single system.
systran is the best single rule-based system.
All scores are case insensitive, and were calculated on the
corrected reference translations.
ments to the latter in a SC setup. Here, though,
the number of such systems was too high to sim-
ply add them to a reasonable set of SMT systems.
Consequently, we first built a SC system (I) com-
bining all RBMT/Systran systems, and then a sec-
ond SC system (II) which combines the output
of I, and 6 SMT systems. As further experi-
ments showed, allowing all hypotheses as primary
(or skeleton) gave significantly better scores than
forcing SC to use the output of I as primary only.
But vice versa, when looking at the meta combi-
nation scheme, VII, using I as primary only (a
setup which we will now denote as II?) gave
measurable improvements in the overall transla-
tion quality. We assume this is due to the similarity
of the output of II with that of the other setups.
Setup III is a SCUNC setup, that is, we built
a single CN for each sentence using poor-man?s-
stemming-TER, with rwth-huck as primary hy-
pothesis. We then generated a large number of fea-
tures for each arc, and trained an ICSIBoost clas-
sifier to recognize the arc (or system) that gave the
best BLEU-S score. This then gave us the consen-
sus translation.
For IV, we built an OpenFST lattice out of eight
systems, and rescored it with both the Hypothe-
sis LM (3-gram), and a 4-gram LM trained on Gi-
gaWord and other WMT constrained training data
for this task. The log-linear weights were trained
using lattice MERT for BLEU. Setup V is a clas-
sical A2L setup, using ten different input systems.
This setup was tuned on BLEU ? TER using the
Downhill-Simplex algorithm. In setup VI, again
the A2FST engine was used, this time using the
Hyp LM only, without an additional LM. Tuning
156
Table 4: Results for DE?EN.
TUNE DEV
BLEU TER BLEU TER
online-B 23.13 60.15 26.20 57.20
Primary 24.57 58.51 28.11 54.83
4 best sys 23.85 58.22 27.47 54.96
6 best sys 24.46 57.74 27.82 54.50
online-B is the best single system.
was also performed using lattice MERT towards
BLEU. And finally, setup VII combines the out-
put of II? to IV using the A2L engine again.
All the results of system combination on TUNE
and DEV are listed in Table 3. It turns out that
with the exception of I, all system combination
approaches were able to achieve a significant im-
provement of at least +1.8% abs. in BLEU com-
pared to the best input system. For I, we need
to keep in mind that all other systems were sev-
eral BLEU points worse than the best one ? a sce-
nario where we can expect system combination,
which is based on the consensus translation after
all, to underperform. We also see that both A2FST
and SCUNC, with their large number of features,
show a tendency to overfitting ? we see large im-
provements on TUNE, but significantly smaller
improvements on DEV. This tendency is, unfortu-
nately, also the case for meta combination: While
we see an additional +0.3% abs. in BLEU over
the best first-level system combination on TUNE,
this improvement does not reflect in the scores on
DEV: While we still see a +0.2% abs. improve-
ment in BLEU over the setup that performed best
on TUNE, there is even a small deterioration of
?0.4% in BLEU over the setup that performed
best on DEV. Because of this effect, we decided to
submit our meta combination output only as first
contrastive, and the output that performed well
both on TUNE and DEV as our primary submis-
sion for WMT. As second contrastive submission,
we selected the setup that performed best on DEV.
4.2 DE?EN
24 systems were available in the German?English
language pair, but incorporating only 7 of them
turned out to deliver optimal results on DEV. We
ran experiments on several settings of systems,
but only in our tried and tested A2L framework.
We settled for a combination of seven systems
(online-B,cmu-dyer,dfki-xu,limsi,
online-A,rwth-wuebker,kit) as primary
submission. Table 4 also lists two different set-
tings. One setting consists of the four best systems
Table 5: Results for ES?EN.
TUNE DEV
BLEU TER BLEU TER
online-A 30.58 51.69 30.77 51.95
Primary 34.29 48.47 33.41 49.71
Contrastive 34.23 48.27 33.30 49.51
online-A is the best single system.
(online-B,cmu-dyer,rwth-wuebker,
kit) and the other setting contains the six best
systems (online-B,cmu-dyer,dfki-xu,
rwth-wuebker,online-A,kit). When we
added more systems to system combination, we
lost performance in both TUNE and DEV.
4.3 ES?EN
For Spanish?English, we tried several settings
of systems. We sticked to our tried and tested
A2L framework. We settled for a combination
of six systems (alacant,koc,online-A,
online-B,rbmt-1,systran) as contrastive
submission, and a combination of ten systems
(+rbmt-2,rbmt-3,rbmt-4,udein) as pri-
mary submission. Table 5 lists the results for this
task. The difference between our primary setup
(10 systems) and our contrastive setup (6 systems)
is rather small, less than 0.1% abs. in BLEU. Nev-
ertheless, we see significant improvements over
the best single system of +2.4% abs. in BLEU,
and ?2.2% in TER.
5 Conclusions
We have shown that our system combination ap-
proach leads to significant improvements over sin-
gle best MT output where a significant number of
comparably good translations is available on a sin-
gle language pair. A meta combination can give
additional improvement, but can be sensitive to
overfitting; so in some cases, using one of its in-
put system combination hypothesis may be a bet-
ter choice. In any way, both of our new engines
have shown that they can compete with our present
approach, so we hope to make good use of the new
possibilities they may offer.
Acknowledgments
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. This work was
partly supported by the Defense Advanced Re-
search Projects Agency (DARPA) under Contract
No. HR0011-06-C-0023.
157
References
C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, and
M. Mohri. 2007. OpenFst: A general and efficient
weighted finite-state transducer library. In Proc. of
the Twelfth International Conference on Implemen-
tation and Application of Automata (CIAA), volume
4783 of Lecture Notes in Computer Science, pages
11?23. Springer.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263?311, June.
B. Favre, D. Hakkani-Tu?r, and S. Cuendet. 2007.
Icsiboost. http://code.google.come/p/
icsiboost.
J. Fiscus. 1997. A post-processing system to yield re-
duced word error rates: Recognizer output voting er-
ror reduction (ROVER). In IEEE Workshop on Au-
tomatic Speech Recognition and Understanding.
D. Hillard, B. Hoffmeister, M. Ostendorf, R. Schlu?ter,
and H. Ney. 2007. iROVER: improving sys-
tem combination with classification. In Human
Language Technologies 2007: The Conference of
the North American Chapter of the Association
for Computational Linguistics; Companion Volume,
Short Papers, NAACL-Short ?07, pages 65?68. As-
sociation for Computational Linguistics.
S. Jayaraman and A. Lavie. 2005. Multi-engine ma-
chine translation guided by explicit word matching.
In Proc. of the 10th Annual Conf. of the European
Association for Machine Translation (EAMT), pages
143?152, Budapest, Hungary, May.
G. Leusch and H. Ney. 2010. The rwth system com-
bination system for wmt 2010. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation
and Metrics MATR, pages 315?320, Uppsala, Swe-
den, July.
G. Leusch, E. Matusov, and H. Ney. 2009. The
RWTH system combination system for WMT 2009.
In Fourth Workshop on Statistical Machine Transla-
tion, pages 56?60, Athens, Greece, March. Associa-
tion for Computational Linguistics.
C. Y. Lin and F. J. Och. 2004. Orange: a method for
evaluation automatic evaluation metrics for machine
translation. In Proc. COLING 2004, pages 501?507,
Geneva, Switzerland, August.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit.
2008. Lattice-based minimum error rate training for
statistical machine translation. In Proc. of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 725?734. Asso-
ciation for Computational Linguistics.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
word alignments for statistical machine translation.
In COLING ?04: The 20th Int. Conf. on Computa-
tional Linguistics, pages 219?225, Geneva, Switzer-
land, August.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine trans-
lation systems using enhanced hypotheses align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40, Trento, Italy, April.
E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y. S. Lee,
J. B. Marino, M. Paulik, S. Roukos, H. Schwenk,
and H. Ney. 2008. System combination for machine
translation of spoken and written language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237, September.
A. Mauser, S. Hasan, and H. Ney. 2008. Automatic
evaluation measures for statistical machine transla-
tion system optimization. In International Confer-
ence on Language Resources and Evaluation, Mar-
rakech, Morocco, May.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51, March.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311?318, Philadelphia, PA, July.
A. V. Rosti, N. F. Ayan, B. Xiang, S. Matsoukas, R. M.
Schwartz, and B. J. Dorr. 2007a. Combining out-
puts from multiple machine translation systems. In
HLT-NAACL?07, pages 228?235.
A. V. Rosti, S. Matsoukas, and R. Schwartz. 2007b.
Improved word-level system combination for ma-
chine translation. In Proc. of the 45th Annual Meet-
ing of the Association of Computational Linguis-
tics (ACL), pages 312?319, Prague, Czech Republic,
June.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Error
Rate with Targeted Human Annotation. In Proc. of
the 7th Conf. of the Association for Machine Trans-
lation in the Americas (AMTA), pages 223?231,
Boston, MA, August.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
COLING ?96: The 16th Int. Conf. on Computational
Linguistics, pages 836?841, Copenhagen, Denmark,
August.
158
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358?364,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Joint WMT Submission of the QUAERO Project
?Markus Freitag, ?Gregor Leusch, ?Joern Wuebker, ?Stephan Peitz, ?Hermann Ney,
?Teresa Herrmann, ?Jan Niehues, ?Alex Waibel,
?Alexandre Allauzen, ?Gilles Adda,?Josep Maria Crego,
?Bianka Buschbeck, ?Tonio Wandmacher, ?Jean Senellart
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint QUAERO sub-
mission to the WMT 2011 machine transla-
tion evaluation. Four groups (RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy, LIMSI-CNRS, and SYSTRAN) of the
QUAERO project submitted a joint translation
for the WMT German?English task. Each
group translated the data sets with their own
systems. Then RWTH system combination
combines these translations to a better one. In
this paper, we describe the single systems of
each group. Before we present the results of
the system combination, we give a short de-
scription of the RWTH Aachen system com-
bination approach.
1 Overview
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in machine
translation is mainly assigned to the four groups
participating in this joint submission. The aim of
this WMT submission was to show the quality of a
joint translation by combining the knowledge of the
four project partners. Each group develop and main-
tain their own different machine translation system.
These single systems differ not only in their general
approach, but also in the preprocessing of training
and test data. To take the advantage of these dif-
ferences of each translation system, we combined
all hypotheses of the different systems, using the
RWTH system combination approach.
1.1 Data Sets
For WMT 2011 each QUAERO partner trained their
systems on the parallel Europarl and News Com-
mentary corpora. All single systems were tuned on
the newstest2009 dev set. The newstest2008 dev set
was used to train the system combination parame-
ters. Finally the newstest2010 dev set was used to
compare the results of the different system combi-
nation approaches and settings.
2 Translation Systems
2.1 RWTH Aachen Single Systems
For the WMT 2011 evaluation the RWTH utilized
RWTH?s state-of-the-art phrase-based and hierar-
chical translation systems. GIZA++ (Och and Ney,
2003) was employed to train word alignments, lan-
guage models have been created with the SRILM
toolkit (Stolcke, 2002).
2.1.1 Phrase-Based System
The phrase-based translation (PBT) system is
similar to the one described in Zens and Ney (2008).
After phrase pair extraction from the word-aligned
bilingual corpus, the translation probabilities are es-
timated by relative frequencies. The standard feature
set alo includes an n-gram language model, phrase-
level IBM-1 and word-, phrase- and distortion-
penalties, which are combined in log-linear fash-
ion. Parameters are optimized with the Downhill-
Simplex algorithm (Nelder and Mead, 1965) on the
word graph.
358
2.1.2 Hierarchical System
For the hierarchical setups described in this pa-
per, the open source Jane toolkit (Vilar et al, 2010)
is employed. Jane has been developed at RWTH
and implements the hierarchical approach as intro-
duced by Chiang (2007) with some state-of-the-art
extensions. In hierarchical phrase-based translation,
a weighted synchronous context-free grammar is in-
duced from parallel text. In addition to contiguous
lexical phrases, hierarchical phrases with up to two
gaps are extracted. The search is typically carried
out using the cube pruning algorithm (Huang and
Chiang, 2007). The model weights are optimized
with standard MERT (Och, 2003) on 100-best lists.
2.1.3 Phrase Model Training
For some PBT systems a forced alignment pro-
cedure was applied to train the phrase translation
model as described in Wuebker et al (2010). A
modified version of the translation decoder is used
to produce a phrase alignment on the bilingual train-
ing data. The phrase translation probabilities are es-
timated from their relative frequencies in the phrase-
aligned training data. In addition to providing a sta-
tistically well-founded phrase model, this has the
benefit of producing smaller phrase tables and thus
allowing more rapid and less memory consuming
experiments with a better translation quality.
2.1.4 Final Systems
For the German?English task, RWTH conducted
experiments comparing the standard phrase extrac-
tion with the phrase training technique described in
Section 2.1.3. Further experiments included the use
of additional language model training data, rerank-
ing of n-best lists generated by the phrase-based sys-
tem, and different optimization criteria.
A considerable increase in translation quality can
be achieved by application of German compound
splitting (Koehn and Knight, 2003). In comparison
to standard heuristic phrase extraction techniques,
performing force alignment phrase training (FA)
gives an improvement in BLEU on newstest2008
and newstest2009, but a degradation in TER. The
addition of LDC Gigaword corpora (+GW) to the
language model training data shows improvements
in both BLEU and TER. Reranking was done on
1000-best lists generated by the the best available
system (PBT (FA)+GW). Following models were
applied: n-gram posteriors (Zens and Ney, 2006),
sentence length model, a 6-gram LM and IBM-1 lex-
icon models in both normal and inverse direction.
These models are combined in a log-linear fashion
and the scaling factors are tuned in the same man-
ner as the baseline system (using TER?4BLEU on
newstest2009).
The final table includes two identical Jane sys-
tems which are optimized on different criteria. The
one optimized on TER?BLEU yields a much lower
TER.
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
We preprocess the training data prior to training
the system, first by normalizing symbols such as
quotes, dashes and apostrophes. Then smart-casing
of the first words of each sentence is performed. For
the German part of the training corpus we use the
hunspell1 lexicon to learn a mapping from old Ger-
man spelling to new German spelling to obtain a cor-
pus with homogeneous spelling. In addition, we per-
form compound splitting as described in (Koehn and
Knight, 2003). Finally, we remove very long sen-
tences, empty lines, and sentences that probably are
not parallel due to length mismatch.
2.2.2 System Overview
The KIT system uses an in-house phrase-based
decoder (Vogel, 2003) to perform translation. Op-
timization with regard to the BLEU score is done
using Minimum Error Rate Training as described
by Venugopal et al (2005). The translation model
is trained on the Europarl and News Commentary
Corpus and the phrase table is based on a GIZA++
Word Alignment. We use two 4-gram SRI language
models, one trained on the News Shuffle corpus and
one trained on the Gigaword corpus. Reordering is
performed based on continuous and non-continuous
POS rules to cover short and long-range reorder-
ings. The long-range reordering rules were also ap-
plied to the training corpus and phrase extraction
was performed on the resulting reordering lattices.
Part-of-speech tags are obtained using the TreeTag-
1http://hunspell.sourceforge.net/
359
ger (Schmid, 1994). In addition, the system applies
a bilingual language model to extend the context of
source language words available for translation. The
individual models are described briefly in the fol-
lowing.
2.2.3 POS-based Reordering Model
We use a reordering model that is based on parts-
of-speech (POS) and learn probabilistic rules from
the POS tags of the words in the training corpus and
the alignment information. In addition to continu-
ous reordering rules that model short-range reorder-
ing (Rottmann and Vogel, 2007), we apply non-
continuous rules to address long-range reorderings
as typical for German-English translation (Niehues
and Kolss, 2009). The reordering rules are applied
to the source sentences and the reordered sentence
variants as well as the original sequence are encoded
in a word lattice which is used as input to the de-
coder.
2.2.4 Lattice Phrase Extraction
For the test sentences, the POS-based reordering
allows us to change the word order in the source sen-
tence so that the sentence can be translated more eas-
ily. If we apply this also to the training sentences, we
would be able to extract also phrase pairs for origi-
nally discontinuous phrases and could apply them
during translation of reordered test sentences.
Therefore, we build reordering lattices for all
training sentences and then extract phrase pairs from
the monotone source path as well as from the re-
ordered paths. To limit the number of extracted
phrase pairs, we extract a source phrase only once
per sentence, even if it is found in different paths and
we only use long-range reordering rules to generate
the lattices for the training corpus.
2.2.5 Bilingual Language Model
In phrase-based systems the source sentence is
segmented by the decoder during the search pro-
cess. This segmentation into phrases leads to the
loss of context information at the phrase boundaries.
The language model can make use of more target
side context. To make also source language context
available we use a bilingual language model, an ad-
ditional language model in the phrase-based system
in which each token consist of a target word and all
source words it is aligned to. The bilingual tokens
enter the translation process as an additional target
factor.
2.3 LIMSI-CNRS Single System
2.3.1 System overview
The LIMSI system is built with n-code2, an open
source statistical machine translation system based
on bilingual n-grams.
2.3.2 n-code Overview
In a nutshell, the translation model is im-
plemented as a stochastic finite-state transducer
trained using a n-gram model of (source,target)
pairs (Casacuberta and Vidal, 2004). Training this
model requires to reorder source sentences so as to
match the target word order. This is performed by a
stochastic finite-state reordering model, which uses
part-of-speech information3 to generalize reordering
patterns beyond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized reorder-
ing models (Tillmann, 2004) aiming at predicting
the orientation of the next translation unit; a weak
distance-based distortion model; and finally a word-
bonus model and a tuple-bonus model which com-
pensate for the system preference for short transla-
tions. The four lexicon models are similar to the ones
use in a standard phrase based system: two scores
correspond to the relative frequencies of the tuples
and two lexical weights estimated from the automat-
ically generated word alignments. The weights asso-
ciated to feature functions are optimally combined
using a discriminative training framework (Och,
2003), using the newstest2009 data as development
set.
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the tu-
ple extraction process. The resulting reordering hy-
potheses are passed to the decoder in the form of
word lattices (Crego and Marin?o, 2007).
2http://www.limsi.fr/Individu/jmcrego/n-code
3Part-of-speech information for English and German is com-
puted using the TreeTagger.
360
2.3.3 Data Preprocessing
Based on previous experiments which have
demonstrated that better normalization tools provide
better BLEU scores (K. Papineni and Zhu, 2002),
all the English texts are tokenized and detokenized
with in-house text processing tools (De?chelotte et
al., 2008). For German, the standard tokenizer sup-
plied by evaluation organizers is used.
2.3.4 Target n-gram Language Models
The English language model is trained assuming
that the test set consists in a selection of news texts
dating from the end of 2010 to the beginning of
2011. This assumption is based on what was done
for the 2010 evaluation. Thus, a development cor-
pus is built in order to create a vocabulary and to
optimize the target language model.
Development Set and Vocabulary In order to
cover different period, two development sets are
used. The first one is newstest2008. However, this
corpus is two years older than the targeted time pe-
riod. Thus a second development corpus is gath-
ered by randomly sampling bunches of 5 consecu-
tive sentences from the provided news data of 2010
and 2011.
To estimate a LM, the English vocabulary is first
defined by including all tokens observed in the Eu-
roparl and news-commentary corpora. This vocabu-
lary is then expanded with all words that occur more
that 5 times in the French-English giga-corpus, and
with the most frequent proper names taken from the
monolingual news data of 2010 and 2011. This pro-
cedure results in a vocabulary around 500k words.
Language Model Training All the training data
allowed in the constrained task are divided into 9
sets based on dates on genres. On each set, a
standard 4-gram LM is estimated from the 500k
word vocabulary with in-house tools using abso-
lute discounting interpolated with lower order mod-
els (Kneser and Ney, 1995; Chen and Goodman,
1998).
All LMs except the one trained on the news cor-
pora from 2010-2011 are first linearly interpolated.
The associated coefficients are estimated so as to
minimize the perplexity evaluated on the dev2010-
2011. The resulting LM and the 2010-2011 LM are
finally interpolated with newstest2008 as develop-
ment data. This two steps interpolation aims to avoid
an overestimate of the weight associated to the 2010-
2011 LM.
2.4 SYSTRAN Software, Inc. Single System
The data submitted by SYSTRAN were obtained by
the SYSTRAN baseline system in combination with
a statistical post editing (SPE) component.
The SYSTRAN system is traditionally classi-
fied as a rule-based system. However, over the
decades, its development has always been driven by
pragmatic considerations, progressively integrating
many of the most efficient MT approaches and tech-
niques. Nowadays, the baseline engine can be con-
sidered as a linguistic-oriented system making use of
dependency analysis, general transfer rules as well
as of large manually encoded dictionaries (100k ?
800k entries per language pair).
The basic setup of the SPE component is identi-
cal to the one described in (L. Dugast and Koehn,
2007). A statistical translation model is trained on
the rule-based translation of the source and the tar-
get side of the parallel corpus. This is done sepa-
rately for each parallel corpus. Language models are
trained on each target half of the parallel corpora and
also on additional in-domain corpora. Moreover, the
following measures ? limiting unwanted statistical
effects ? were applied:
? Named entities are replaced by special tokens
on both sides. This usually improves word
alignment, since the vocabulary size is signif-
icantly reduced. In addition, entity translation
is handled more reliably by the rule-based en-
gine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the ref-
erence translation) is used to produce an addi-
tional parallel corpus (whose target is identical
to the source). This was added to the parallel
text in order to improve word alignment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side are
also discarded.
361
? Phrase pairs appearing less than 2 times were
pruned.
The SPE language model was trained 15M
phrases from the news/europarl corpora, provided
as training data for WMT 2011. Weights for these
separate models were tuned by the MERT algorithm
provided in the Moses toolkit (P. Koehn et al, 2007),
using the provided news development set.
3 RWTH Aachen System Combination
System combination is used to produce consensus
translations from multiple hypotheses produced with
different translation engines that are better in terms
of translation quality than any of the individual hy-
potheses. The basic concept of RWTH?s approach
to machine translation system combination has been
described by Matusov et al (2006; 2008). This ap-
proach includes an enhanced alignment and reorder-
ing framework. A lattice is built from the input hy-
potheses. The translation with the best score within
the lattice according to a couple of statistical mod-
els is selected as consensus translation. A deeper
description will be also given in the WMT11 sys-
tem combination paper of RWTH Aachen Univer-
sity. For this task only the A2L framework has been
used.
4 Experiments
We tried different system combinations with differ-
ent sets of single systems and different optimiza-
tion criteria. As RWTH has two different transla-
tion systems, we put the output of both systems into
system combination. Although both systems have
the same preprocessing, their hypotheses differ. Fi-
nally, we added for both RWTH systems two addi-
tional hypotheses to the system combination. The
two hypotheses of Jane were optimized on differ-
ent criteria. The first hypothesis was optimized on
BLEU and the second one on TER?BLEU. The first
RWTH phrase-based hypothesis was generated with
force alignment, the second RWTH phrase-based
hypothesis is a reranked version of the first one as
described in 2.1.4. Compared to the other systems,
the system by SYSTRAN has a completely different
approach (see section 2.4). It is mainly based on a
rule-based system. For the German?English pair,
SYSTRAN achieves a lower BLEU score in each
test set compared to the other groups. But since the
SYSTRAN system is very different to the others, we
still obtain an improvement when we add it also to
system combination.
We obtain the best result from system combina-
tion of all seven systems, optimizing the parameters
on BLEU. This system was the system we submitted
to the WMT 2011 evaluation.
For each dev set we obtain an improvement com-
pared to the best single systems. For newstest2008
and newstest2009 we get an improvement of 0.5
points in BLEU and 1.8 points in TER compared to
the best single system of Karlsruhe Institute of Tech-
nology. For newstest2010 we get an improvement
of 1.8 points in BLEU and 2.7 points in TER com-
pared to the best single system of RWTH. The sys-
tem combination weights optimized for the best run
are listed in Table 2. We see that although the single
system of SYSTRAN has the lowest BLEU scores,
it gets the second highest system weight. This high
value shows the influence of a completely different
system. On the other hand, all RWTH systems are
very similar, because of their same preprocessing
and their small variations. Therefor the system com-
bination parameter of all four systems by themselves
are relatively small. The summarized ?RWTH ap-
proach? system weight, though, is again on par with
the other systems.
5 Conclusion
The four statistical machine translation systems of
Karlsruhe Institute of Technology, RWTH Aachen
and LIMSI and the very structural approach of SYS-
TRAN produce hypotheses with a huge variability
compared to the others. Finally the RWTH Aachen
system combination combined all single system hy-
potheses to one hypothesis with a higher BLEU
compared to each single system. If the system
combination implementation can handle enough sin-
gle systems we would recommend to add all single
systems to the system combination. Although the
single system of SYSTRAN has the lowest BLEU
scores and the RWTH single systems are similar we
achieved the best result in using all single systems.
362
newstest2008 newstest2009 newstest2010 description
BLEU TER BLEU TER BLEU TER
22.73 60.73 22.50 59.82 25.26 57.37 sc (all systems) BLEU opt
22.61 60.60 22.28 59.39 25.07 56.95 sc (all systems - (1)) TER?BLEU opt
22.50 60.41 22.52 59.61 25.23 57.40 sc (all systems) TER?BLEU opt
22.19 60.09 22.05 59.31 24.74 56.89 sc (all systems - (4)) TER?BLEU opt
22.21 60.71 21.89 59.95 24.72 57.58 sc (all systems - (4,7)) TER?BLEU opt
22.22 60.45 21.79 59.72 24.32 57.59 sc (all systems - (3,4)) TER?BLEU opt
22.27 60.60 21.75 59.92 24.35 57.64 sc (all systems - (3,4)) BLEU opt
22.10 62.59 22.01 61.64 23.34 60.35 (1) Karlsruhe Institute of Technology
21.41 62.77 21.12 61.91 23.44 60.06 (2) RWTH PBT (FA) rerank +GW
21.11 62.96 21.06 62.16 23.29 60.26 (3) RWTH PBT (FA)
21.47 63.89 21.00 63.33 22.93 61.71 (4) RWTH jane + GW BLEU opt
20.89 61.05 20.36 60.47 23.42 58.31 (5) RWTH jane + GW TER?BLEU opt
20.33 64.50 19.79 64.91 21.97 61.44 (6) Limsi-CNRS
17.06 69.48 17.52 67.34 18.68 66.37 (7) SYSTRAN Software
Table 1: All systems for the WMT 2011 German?English translation task (truecase). BLEU and TER results are in
percentage. FA denotes systems with phrase training, +GW the use of LDC data for the language model. sc denotes
system combination.
system weight
Karlsruhe Institute of Technology 0.350
RWTH PBT (FA) rerank +GW 0.001
RWTH PBT (FA) 0.046
RWTH jane + GW BLEU opt 0.023
RWTH jane + GW TER?BLEU opt 0.034
Limsi-CNRS 0.219
SYSTRAN Software 0.328
Table 2: Optimized systems weights for each system of the best system combination result.
Acknowledgments
This work was achieved as part of the QUAERO
Programme, funded by OSEO, French State agency
for innovation.
References
F. Casacuberta and E. Vidal. 2004. Machine translation
with inferred stochastic finite-state transducers. Com-
putational Linguistics, 30(3):205?225.
S.F. Chen and J.T. Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Computer Science Group,
Harvard University.
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J.M. Crego and J.B. Marin?o. 2007. Improving statistical
MT by coupling reordering and decoding. Machine
Translation, 20(3):199?215.
D. De?chelotte, O. Galibert G. Adda, A. Allauzen, J. Gau-
vain, H. Meynard, and F. Yvon. 2008. LIMSI?s statis-
tical translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
Annual Meeting of the Association for Computational
Linguistics, pages 144?151, Prague, Czech Republic,
June.
T. Ward K. Papineni, S. Roukos and W. Zhu. 2002. Bleu:
363
a method for automatic evaluation of machine transla-
tion. In ACL ?02: Proc. of the 40th Annual Meeting
on Association for Computational Linguistics, pages
311?318. Association for Computational Linguistics.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing, ICASSP?95, pages 181?184, Detroit,
MI.
P. Koehn and K. Knight. 2003. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
J. Senellart L. Dugast and P. Koehn. 2007. Statistical
post-editing on systran?s rule-based translation system.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, StatMT ?07, pages 220?223,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Mari no, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
J.A. Nelder and R. Mead. 1965. The Downhill Simplex
Method. Computer Journal, 7:308.
J. Niehues and M. Kolss. 2009. A POS-Based Model for
Long-Range Reorderings in SMT. In Fourth Work-
shop on Statistical Machine Translation (WMT 2009),
Athens, Greece.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training for Statis-
tical Machine Translation. In Proc. Annual Meeting of
the Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July.
A. Birch P. Koehn, H. Hoang, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and
E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, ACL ?07, pages 177?
180, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
K. Rottmann and S. Vogel. 2007. Word Reordering in
Statistical Machine Translation with a POS-Based Dis-
tortion Model. In TMI, Sko?vde, Sweden.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In International Conference
on NewMethods in Language Processing, Manchester,
UK.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901?904, Denver, Col-
orado, USA, September.
C. Tillmann. 2004. A unigram orientation model for sta-
tistical machine translation. In Proceedings of HLT-
NAACL 2004, pages 101?104. Association for Com-
putational Linguistics.
A. Venugopal, A. Zollman, and A. Waibel. 2005. Train-
ing and Evaluation Error Minimization Rules for Sta-
tistical Machine Translation. In Workshop on Data-
drive Machine Translation and Beyond (WPT-05), Ann
Arbor, MI.
D. Vilar, S. Stein, M. Huck, and H. Ney. 2010. Jane:
Open Source Hierarchical Translation, Extended with
Reordering and Lexicon Models. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 262?270, Uppsala, Sweden,
July.
S. Vogel. 2003. SMT Decoder Dissected: Word Re-
ordering. In Int. Conf. on Natural Language Process-
ing and Knowledge Engineering, Beijing, China.
J. Wuebker, A. Mauser, and H. Ney. 2010. Training
Phrase Translation Models with Leaving-One-Out. In
Proceedings of the 48th Annual Meeting of the Assoc.
for Computational Linguistics, pages 475?484, Upp-
sala, Sweden, July.
R. Zens and H. Ney. 2006. N-gram Posterior Proba-
bilities for Statistical Machine Translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), Workshop on Statistical Ma-
chine Translation, pages 72?77, New York City, June.
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statisti-
cal Machine Translation. In Proc. of the Int. Workshop
on Spoken Language Translation (IWSLT), Honolulu,
Hawaii, October.
364
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 405?412,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The RWTH Aachen Machine Translation System for WMT 2011
Matthias Huck, Joern Wuebker, Christoph Schmidt, Markus Freitag, Stephan Peitz,
Daniel Stein, Arnaud Dagnelies, Saab Mansour, Gregor Leusch and Hermann Ney
RWTH Aachen University
Aachen, Germany
surname@cs.rwth-aachen.de
Abstract
This paper describes the statistical machine
translation (SMT) systems developed by
RWTH Aachen University for the translation
task of the EMNLP 2011 Sixth Workshop on
Statistical Machine Translation. Both phrase-
based and hierarchical SMT systems were
trained for the constrained German-English
and French-English tasks in all directions. Ex-
periments were conducted to compare differ-
ent training data sets, training methods and op-
timization criteria, as well as additional mod-
els on dependency structure and phrase re-
ordering. Further, we applied a system com-
bination technique to create a consensus hy-
pothesis from several different systems.
1 Overview
We sketch the baseline architecture of RWTH?s se-
tups for the WMT 2011 shared translation task by
providing an overview of our translation systems in
Section 2. In addition to the baseline features, we
adopted several novel methods, which will be pre-
sented in Section 3. Details on the respective se-
tups and translation results for the French-English
and German-English language pairs (in both trans-
lation directions) are given in Sections 4 and 5. We
finally conclude the paper in Section 6.
2 Translation Systems
For the WMT 2011 evaluation we utilized RWTH?s
state-of-the-art phrase-based and hierarchical trans-
lation systems as well as our in-house system com-
bination framework. GIZA++ (Och and Ney, 2003)
was employed to train word alignments, language
models have been created with the SRILM toolkit
(Stolcke, 2002).
2.1 Phrase-Based System
We applied a phrase-based translation (PBT) system
similar to the one described in (Zens and Ney, 2008).
Phrase pairs are extracted from a word-aligned bilin-
gual corpus and their translation probability in both
directions is estimated by relative frequencies. The
standard feature set moreover includes an n-gram
language model, phrase-level single-word lexicons
and word-, phrase- and distortion-penalties. To lexi-
calize reordering, a discriminative reordering model
(Zens and Ney, 2006a) is used. Parameters are opti-
mized with the Downhill-Simplex algorithm (Nelder
and Mead, 1965) on the word graph.
2.2 Hierarchical System
For the hierarchical setups described in this paper,
the open source Jane toolkit (Vilar et al, 2010) was
employed. Jane has been developed at RWTH and
implements the hierarchical approach as introduced
by Chiang (2007) with some state-of-the-art exten-
sions. In hierarchical phrase-based translation, a
weighted synchronous context-free grammar is in-
duced from parallel text. In addition to contiguous
lexical phrases, hierarchical phrases with up to two
gaps are extracted. The search is typically carried
out using the cube pruning algorithm (Huang and
Chiang, 2007). The standard models integrated into
our Jane systems are: phrase translation probabil-
ities and lexical translation probabilities on phrase
level, each for both translation directions, length
405
penalties on word and phrase level, three binary fea-
tures marking hierarchical phrases, glue rule, and
rules with non-terminals at the boundaries, source-
to-target and target-to-source phrase length ratios,
four binary count features and an n-gram language
model. The model weights are optimized with stan-
dard MERT (Och, 2003) on 100-best lists.
2.3 System Combination
System combination is used to produce consensus
translations from multiple hypotheses produced with
different translation engines that are better in terms
of translation quality than any of the individual hy-
potheses. The basic concept of RWTH?s approach
to machine translation system combination has been
described by Matusov et al (Matusov et al, 2006;
Matusov et al, 2008). This approach includes an
enhanced alignment and reordering framework. A
lattice is built from the input hypotheses. The trans-
lation with the best score within the lattice according
to a couple of statistical models is selected as con-
sensus translation.
3 Translation Modeling
We incorporated several novel methods into our sys-
tems for the WMT 2011 evaluation. This section
provides a short survey of three of the methods
which we suppose to be of particular interest.
3.1 Language Model Data Selection
For the English and German language models,
we applied the data selection method proposed in
(Moore and Lewis, 2010). Each sentence is scored
by the difference in cross-entropy between a lan-
guage model trained from in-domain data and a lan-
guage model trained from a similar-sized sample of
the out-of-domain data. As in-domain data we used
the news-commentary corpus. The out-of-domain
data from which the data was selected are the news
crawl corpus for both languages and for English the
109 corpus and the LDC Gigaword data. We used a
3-gram trained with the SRI toolkit to compute the
cross-entropy. For the news crawl corpus, only 1/8
of the sentences were discarded. Of the 109 corpus
we retained 1/2 and of the LDC Gigaword data we
retained 1/4 of the sentences to train the language
models.
3.2 Phrase Model Training
For the German?English and French?English
translation tasks we applied a forced alignment pro-
cedure to train the phrase translation model with the
EM algorithm, similar to the one described in (DeN-
ero et al, 2006). Here, the phrase translation prob-
abilities are estimated from their relative frequen-
cies in the phrase-aligned training data. The phrase
alignment is produced by a modified version of the
translation decoder. In addition to providing a statis-
tically well-founded phrase model, this has the ben-
efit of producing smaller phrase tables and thus al-
lowing more rapid experiments. A detailed descrip-
tion of the training procedure is given in (Wuebker
et al, 2010).
3.3 Soft String-to-Dependency
Given a dependency tree of the target language,
we are able to introduce language models that span
over longer distances than the usual n-grams, as in
(Shen et al, 2008). To obtain dependency structures,
we apply the Stanford parser (Klein and Manning,
2003) on the target side of the training material.
RWTH?s open source hierarchical translation toolkit
Jane has been extended to include dependency infor-
mation in the phrase table and to build dependency
trees on the output hypotheses at decoding time from
this information.
Shen et al (2008) use only phrases that meet cer-
tain restrictions. The first possibility is what the au-
thors call a fixed dependency structure. With the
exception of one word within this phrase, called
the head, no outside word may have a dependency
within this phrase. Also, all inner words may only
depend on each other or on the head. For a second
structure, called a floating dependency structure, the
head dependency word may also exist outside the
phrase. If the dependency structure of a phrase con-
forms to these restrictions, it is denoted as valid.
In our phrase table, we mark those phrases that
possess a valid dependency structure with a binary
feature, but all phrases are retained as translation op-
tions. In addition to storing the dependency informa-
tion, we also memorize for all hierarchical phrases
if the content of gaps has been dependent on the left
or on the right side. We utilize the dependency in-
formation during the search process by adding three
406
French English
Sentences 3 710 985
Running Words 98 352 916 87 689 253
Vocabulary 179 548 216 765
Table 1: Corpus statistics of the preprocessed high-
quality training data (Europarl, news-commentary, and
selected parts of the 109 and UN corpora) for the
RWTH systems for the WMT 2011 French?English and
English?French translation tasks. Numerical quantities
are replaced by a single category symbol.
features to the log-linear model: merging errors to
the left, merging errors to the right, and the ratio of
valid vs. non-valid dependency structures. The de-
coder computes the corresponding costs when it tries
to construct a dependency tree of a (partial) hypothe-
sis on-the-fly by merging the dependency structures
of the used phrase pairs.
In an n-best reranking step, we compute depen-
dency language model scores on the dependencies
which were assembled on the hypotheses by the
search procedure. We apply one language model
for left-side dependencies and one for right-side de-
pendencies. For head structures, we also compute
their scores by exploiting a simple unigram language
model. We furthermore include a language count
feature that is incremented each time we compute
a dependency language model score. As trees with
few dependencies have less individual costs to be
computed, they tend to obtain lower overall costs
than trees with more complex structures in other
sentences. The intention behind this feature is thus
comparable to the word penalty in combination with
a normal n-gram language model.
4 French-English Setups
We set up both hierarchical and standard phrase-
based systems for the constrained condition of the
WMT 2011 French?English and English?French
translation tasks. The English?French RWTH pri-
mary submission was produced with a single hierar-
chical system, while a system combination of three
systems was used to generate a final hypothesis for
the French?English primary submission.
Besides the Europarl and news-commentary cor-
pora, the provided parallel data also comprehends
French English
Sentences 29 996 228
Running Words 916 347 538 778 544 843
Vocabulary 1 568 089 1 585 093
Table 2: Corpus statistics of the preprocessed full training
data for the RWTH primary system for the WMT 2011
English?French translation task. Numerical quantities
are replaced by a single category symbol.
the large French-English 109 corpus and the French-
English UN corpus. Since model training with
such a huge amount of data requires a consider-
able computational effort, RWTH decided to select
a high-quality part of altogether about 2 Mio. sen-
tence pairs from the latter two corpora. The selec-
tion of parallel sentences was carried out according
to three criteria: (1) Only sentences of minimum
length of 4 tokens are considered, (2) at least 92%
of the vocabulary of each sentence occurs in new-
stest2008, and (3) the ratio of the vocabulary size
of a sentence and the number of its tokens is mini-
mum 80%. Word alignments in both directions were
trained with GIZA++ and symmetrized according to
the refined method that was proposed in (Och and
Ney, 2003). The phrase tables of the translation
systems are extracted from the Europarl and news-
commentary parallel training data as well as the se-
lected high-quality parts the 109 and UN corpora
only. The only exception is the hierarchical system
used for the English?French RWTH primary sub-
mission which comprehends a second phrase table
with lexical (i.e. non-hierarchical) phrases extracted
from the full parallel data (approximately 30 Mio.
sentence pairs).
Detailed statistics of the high-quality parallel
training data (Europarl, news-commentary, and the
selected parts of the 109 and UN corpora) are given
in Table 1, the corpus statistics of the full parallel
data from which the second phrase table with lexi-
cal phrases for the English?French RWTH primary
system was created are presented in Table 2.
The translation systems use large 4-gram lan-
guage models with modified Kneser-Ney smooth-
ing. The French language model was trained on
most of the provided French data including the
monolingual LDC Gigaword corpora, the English
407
newstest2009 newstest2010
French?English BLEU TER BLEU TER
System combination of ? systems (primary) 26.7 56.0 27.4 54.9
PBT with triplet lexicon, no forced alignment (contrastive) ? 26.2 56.7 27.2 55.3
Jane as below + improved LM (contrastive) 26.3 57.4 26.7 56.2
Jane with parse match + syntactic labels + dependency ? 26.2 57.5 26.5 56.4
PBT with forced alignment phrase training ? 26.0 57.1 26.3 56.0
Table 3: RWTH systems for the WMT 2011 French?English translation task (truecase). BLEU and TER results are
in percentage.
newstest2009 newstest2010
English?French BLEU TER BLEU TER
Jane shallow + in-domain TM + lexical phrases from full data 25.3 60.1 27.1 57.2
Jane shallow + in-domain TM + triplets + DWL + parse match 24.8 60.5 26.6 57.5
PBT with triplets, DWL, sentence-level word lexicon, discrim. reord. 24.8 60.1 26.5 57.3
Table 4: RWTH systems for the WMT 2011 English?French translation task (truecase). BLEU and TER results are
in percentage.
language model was trained on automatically se-
lected English data (cf. Section 3.1) from the pro-
vided resources including the 109 corpus and LDC
Gigaword.
The scaling factors of the log-linear model com-
bination are optimized towards BLEU on new-
stest2009, newstest2010 is used as an unseen test set.
4.1 Experimental Results French?English
The results for the French?English task are given in
Table 3. RWTH?s three submissions ? one primary
and two contrastive ? are labeled accordingly in the
table. The first contrastive submission is a phrase-
based system with a standard feature set plus an ad-
ditional triplet lexicon model (Mauser et al, 2009).
The triplet lexicon model was trained on in-domain
news commentary data only. The second contrastive
submission is a hierarchical Jane system with three
syntax-based extensions: A parse match model (Vi-
lar et al, 2008), soft syntactic labels (Stein et al,
2010), and the soft string-to-dependency extension
as described in Section 3.3. The primary submis-
sion combines the phrase-based contrastive system,
a hierarchical system that is very similar to the Jane
contrastive submission but with a slightly worse lan-
guage model, and an additional PBT system that has
been trained with forced alignment (Wuebker et al,
2010) on WMT 2010 data only.
4.2 Experimental Results English?French
The results for the English?French task are given
in Table 4. We likewise submitted two contrastive
systems for this translation direction. The first con-
trastive submission is a phrase-based system, en-
hanced with a triplet lexicon model and a discrim-
inative word lexicon model (Mauser et al, 2009) ?
both trained on in-domain news commentary data
only ? as well as a sentence-level single-word lex-
icon model and a discriminative reordering model
(Zens and Ney, 2006a). The second contrastive sub-
mission is a hierarchical Jane system with shallow
rules (Iglesias et al, 2009), a triplet lexicon model, a
discriminative word lexicon, the parse match model,
and a second phrase table extracted from in-domain
data only. Our primary submission is very similar
to the latter Jane setup. It does not comprise the ex-
tended lexicon models and the parse match exten-
sion, but instead includes lexical phrases from the
full 30 Mio. sentence corpus as described above.
5 German-English Setups
We trained phrase-based and hierarchical transla-
tion systems for both translation directions of the
German-English language pair. The corpus statis-
408
German English
Sentences 1 857 745
Running Words 48 449 977 50 559 217
Vocabulary 387 593 123 470
Table 5: Corpus statistics of the preprocessed train-
ing data for the WMT 2011 German?English and
English?German translation tasks. Numerical quantities
are replaced by a single category symbol.
tics can be found in Table 5. Word alignments were
generated with GIZA++ and symmetrized as for the
French-English setups.
The language models are 4-grams trained on the
bilingual data as well as the provided News crawl
corpus. For the English language model the 109
French-English and LDC Gigaword corpora were
used additionally. For the 109 French-English and
LDC Gigaword corpora RWTH applied the data se-
lection technique described in Section 3.1. We ex-
amined two different language models, one with
LDC data and one without.
Systems were optimized on the newstest2009 data
set, newstest2008 was used as test set. The scores
for newstest2010 are included for completeness.
5.1 Morpho-Syntactic Analysis
In order to reduce the source vocabulary size for
the German?English translation, the source side
was preprocessed by splitting German compound
words with the frequency-based method described
in (Koehn and Knight, 2003). To further reduce
translation complexity, we performed the long-range
part-of-speech based reordering rules proposed by
(Popovic? et al, 2006). For additional experiments
we used the TreeTagger (Schmid, 1995) to produce
a lemmatized version of the German source.
5.2 Optimization Criterion
We studied the impact of different optimization cri-
teria on tranlsation performance. The usual prac-
tice is to optimize the scaling factors to maximize
BLEU. We also experimented with two different
combinations of BLEU and Translation Edit Rate
(TER): TER?BLEU and TER?4BLEU. The first
denotes the equally weighted combination, while for
the latter BLEU is weighted 4 times as strong as
TER.
5.3 Experimental Results German?English
For the German?English task we conducted ex-
periments comparing the standard phrase extraction
with the phrase training technique described in Sec-
tion 3.2. For the latter we applied log-linear phrase-
table interpolation as proposed in (Wuebker et al,
2010). Further experiments included the use of addi-
tional language model training data, reranking of n-
best lists generated by the phrase-based system, and
different optimization criteria. We also carried out
a system combination of several systems, including
phrase-based systems on lemmatized German and
on source data without compound splitting and two
hierarchical systems optimized for different criteria.
The results are given in Table 6.
A considerable increase in translation quality can
be achieved by application of German compound
splitting. The system that operates on German
surface forms without compound splitting (SUR)
clearly underperforms the baseline system with mor-
phological preprocessing. The system on lemma-
tized German (LEM) is at about the same level as
the system on surface forms.
In comparison to the standard heuristic phrase ex-
traction technique, performing phrase training (FA)
gives an improvement in BLEU on newstest2008
and newstest2009, but a degradation in TER. The
addition of LDC Gigaword corpora (+GW) to the
language model training data shows improvements
in both BLEU and TER. Reranking was done on
1000-best lists generated by the the best available
system (PBT (FA)+GW). Following models were
applied: n-gram posteriors (Zens and Ney, 2006b),
sentence length model, a 6-gram LM and single-
word lexicon models in both normal and inverse di-
rection. These models are combined in a log-linear
fashion and the scaling factors are tuned in the same
manner as the baseline system (using TER?4BLEU
on newstest2009).
The table includes three identical Jane systems
which are optimized for different criteria. The one
optimized for TER?4BLEU offers the best balance
between BLEU and TER, but was not finished in
time for submission. As primary submission we
chose the reranked PBT system, as secondary the
system combination.
409
newstest2008 newstest2009 newstest2010
German?English opt criterion BLEU TER BLEU TER BLEU TER
Syscombi of ? (secondary) TER?BLEU 21.1 62.1 20.8 61.2 23.7 59.2
Jane +GW ? BLEU 21.5 63.9 21.0 63.3 22.9 61.7
Jane +GW TER?4BLEU 21.4 62.6 21.1 62.0 23.5 60.3
PBT (FA) rerank +GW (primary) ? TER?4BLEU 21.4 62.8 21.1 61.9 23.4 60.1
PBT (FA) +GW ? TER?4BLEU 21.1 63.0 21.1 62.2 23.3 60.3
Jane +GW ? TER?BLEU 20.9 61.1 20.4 60.5 23.4 58.3
PBT (FA) TER?4BLEU 21.1 63.2 20.6 62.4 23.2 60.4
PBT TER?4BLEU 20.6 62.7 20.3 61.9 23.3 59.7
PBT (SUR) ? TER?4BLEU 19.5 66.5 18.9 65.8 21.0 64.9
PBT (LEM) ? TER?4BLEU 19.2 66.1 18.9 65.4 21.0 63.5
Table 6: RWTH systems for the WMT 2011 German?English translation task (truecase). BLEU and TER results
are in percentage. FA denotes systems with phrase training, +GW the use of LDC data for the language model.
SUR and LEM denote the systems without compound splitting and on the lemmatized source, respectively. The three
hierarchical Jane systems are identical, but used different parameter optimization criterea.
newstest2008 newstest2009 newstest2010
English?German opt criterion BLEU TER BLEU TER BLEU TER
PBT + discrim. reord. (primary) TER?4BLEU 15.3 70.2 15.1 69.8 16.2 65.6
PBT + discrim. reord. BLEU 15.2 70.6 15.2 70.1 16.2 66.0
PBT TER?4BLEU 15.2 70.7 15.2 70.2 16.2 66.1
Jane BLEU 15.1 72.1 15.4 71.2 16.4 67.4
Jane TER?4BLEU 15.1 68.4 14.6 69.5 14.6 65.9
Table 7: RWTH systems for the WMT 2011 English?German translation task (truecase). BLEU and TER results are
in percentage.
5.4 Experimental Results English?German
We likewise studied the effect of using BLEU only
versus using TER?4BLEU as optimization crite-
rion in the English?German translation direction.
Moreover, we tested the impact of the discriminative
reordering model (Zens and Ney, 2006a). The re-
sults can be found in Table 7. For the phrase-based
system, optimizing towards TER?4BLEU leads to
slightly better results both in BLEU and TER than
optimizing towards BLEU. Using the discriminative
reordering model yields some improvements both on
newstest2008 and newstest2010. In the case of the
hierarchical system, the effect of the optimization
criterion is more pronounced than for the phrase-
based system. However, in this case it clearly leads
to a tradeoff between BLEU and TER, as the choice
of TER?4BLEU harms the translation results of
test2010 with respect to BLEU.
6 Conclusion
For the participation in the WMT 2011 shared trans-
lation task, RWTH experimented with both phrase-
based and hierarchical translation systems. We used
all bilingual and monolingual data provided for the
constrained track. To limit the size of the lan-
guage model, a data selection technique was applied.
Several techniques yielded improvements over the
baseline, including three syntactic models, extended
lexicon models, a discriminative reordering model,
forced alignment training, reranking methods and
different optimization criteria.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
410
References
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006.
Why Generative Phrase Models Underperform Surface
Heuristics. In Proceedings of the Workshop on Statis-
tical Machine Translation, pages 31?38.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
Annual Meeting of the Association for Computational
Linguistics, pages 144?151, Prague, Czech Republic,
June.
G. Iglesias, A. de Gispert, E.R. Banga, and W. Byrne.
2009. Rule Filtering by Pattern for Efficient Hierar-
chical Translation. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL (EACL
2009), pages 380?388.
D. Klein and C.D. Manning. 2003. Accurate Unlexi-
calized Parsing. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics
- Volume 1, ACL ?03, pages 423?430.
P. Koehn and K. Knight. 2003. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Marino, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
A. Mauser, S. Hasan, and H. Ney. 2009. Extending Sta-
tistical Machine Translation with Discriminative and
Trigger-Based Lexicon Models. In Conference on
Empirical Methods in Natural Language Processing,
pages 210?217.
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In ACL (Short Pa-
pers), pages 220?224, Uppsala, Sweden, July.
J.A. Nelder and R. Mead. 1965. The Downhill Simplex
Method. Computer Journal, 7:308.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training for Statis-
tical Machine Translation. In Proc. Annual Meeting of
the Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Natural
Language Processing, Springer Verlag, LNCS, pages
616?624.
H. Schmid. 1995. Improvements in Part-of-Speech Tag-
ging with an Application to German. In Proceedings
of the ACL SIGDAT-Workshop, pages 47?50, Dublin,
Ireland, March.
L. Shen, J. Xu, and R. Weischedel. 2008. A New String-
to-Dependency Machine Translation Algorithm with a
Target Dependency Language Model. In Proceedings
of ACL-08: HLT. Association for Computational Lin-
guistics, pages 577?585, June.
D. Stein, S. Peitz, D. Vilar, and H. Ney. 2010. A Cocktail
of Deep Syntactic Features for Hierarchical Machine
Translation. In Conference of the Association for Ma-
chine Translation in the Americas 2010, page 9, Den-
ver, USA, October.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901 ? 904, Denver, Col-
orado, USA, September.
D. Vilar, D. Stein, and H. Ney. 2008. Analysing Soft
Syntax Features and Heuristics for Hierarchical Phrase
Based Machine Translation. In Proc. of the Int. Work-
shop on Spoken Language Translation (IWSLT), pages
190?197, Waikiki, Hawaii, October.
D. Vilar, S. Stein, M. Huck, and H. Ney. 2010. Jane:
Open Source Hierarchical Translation, Extended with
Reordering and Lexicon Models. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 262?270, Uppsala, Sweden,
July.
J. Wuebker, A. Mauser, and H. Ney. 2010. Training
Phrase Translation Models with Leaving-One-Out. In
Proceedings of the 48th Annual Meeting of the Assoc.
for Computational Linguistics, pages 475?484, Upp-
sala, Sweden, July.
R. Zens and H. Ney. 2006a. Discriminative Reordering
Models for Statistical Machine Translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), Workshop on Statistical Ma-
chine Translation, pages 55?63, New York City, June.
R. Zens and H. Ney. 2006b. N-gram Posterior Proba-
bilities for Statistical Machine Translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), Workshop on Statistical Ma-
chine Translation, pages 72?77, New York City, June.
411
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statisti-
cal Machine Translation. In Proc. of the Int. Workshop
on Spoken Language Translation (IWSLT), Honolulu,
Hawaii, October.
412
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 191?199,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Review of Hypothesis Alignment Algorithms for MT System Combination
via Confusion Network Decoding
Antti-Veikko I. Rostia?, Xiaodong Heb, Damianos Karakosc, Gregor Leuschd?, Yuan Caoc,
Markus Freitage, Spyros Matsoukasf , Hermann Neye, Jason R. Smithc and Bing Zhangf
aApple Inc., Cupertino, CA 95014
arosti@apple.com
bMicrosoft Research, Redmond, WA 98052
xiaohe@microsoft.com
cJohns Hopkins University, Baltimore, MD 21218
{damianos,yuan.cao,jrsmith}@jhu.edu
dSAIC, Monheimsallee 22, D-52062 Aachen, Germany
gregor.leusch@saic.com
eRWTH Aachen University, D-52056 Aachen, Germany
{freitag,ney}@cs.rwth-aachen.de
fRaytheon BBN Technologies, 10 Moulton Street, Cambridge, MA 02138
{smatsouk,bzhang}@bbn.com
Abstract
Confusion network decoding has proven to
be one of the most successful approaches
to machine translation system combination.
The hypothesis alignment algorithm is a cru-
cial part of building the confusion networks
and many alternatives have been proposed in
the literature. This paper describes a sys-
tematic comparison of five well known hy-
pothesis alignment algorithms for MT sys-
tem combination via confusion network de-
coding. Controlled experiments using identi-
cal pre-processing, decoding, and weight tun-
ing methods on standard system combina-
tion evaluation sets are presented. Transla-
tion quality is assessed using case insensitive
BLEU scores and bootstrapping is used to es-
tablish statistical significance of the score dif-
ferences. All aligners yield significant BLEU
score gains over the best individual system in-
cluded in the combination. Incremental indi-
rect hidden Markov model and a novel incre-
mental inversion transduction grammar with
flexible matching consistently yield the best
translation quality, though keeping all things
equal, the differences between aligners are rel-
atively small.
?The work reported in this paper was carried out while the
authors were at Raytheon BBN Technologies and
?RWTH Aachen University.
1 Introduction
Current machine translation (MT) systems are based
on different paradigms, such as rule-based, phrase-
based, hierarchical, and syntax-based. Due to the
complexity of the problem, systems make various
assumptions at different levels of processing and
modeling. Many of these assumptions may be
suboptimal and complementary. The complemen-
tary information in the outputs from multiple MT
systems may be exploited by system combination.
Availability of multiple system outputs within the
DARPA GALE program as well as NIST Open MT
and Workshop on Statistical Machine Translation
evaluations has led to extensive research in combin-
ing the strengths of diverse MT systems, resulting in
significant gains in translation quality.
System combination methods proposed in the lit-
erature can be roughly divided into three categories:
(i) hypothesis selection (Rosti et al, 2007b; Hilde-
brand and Vogel, 2008), (ii) re-decoding (Frederking
and Nirenburg, 1994; Jayaraman and Lavie, 2005;
Rosti et al, 2007b; He and Toutanova, 2009; De-
vlin et al, 2011), and (iii) confusion network de-
coding. Confusion network decoding has proven to
be the most popular as it does not require deep N -
best lists1 and operates on the surface strings. It has
1N -best lists of around N = 10 have been used in confu-
sion network decoding yielding small gains over using 1-best
191
also been shown to be very successful in combining
speech recognition outputs (Fiscus, 1997; Mangu et
al., 2000). The first application of confusion net-
work decoding in MT system combination appeared
in (Bangalore et al, 2001) where a multiple string
alignment (MSA), made popular in biological se-
quence analysis, was applied to the MT system out-
puts. Matusov et al (2006) proposed an alignment
based on GIZA++ Toolkit which introduced word
reordering not present in MSA, and Sim et al (2007)
used the alignments produced by the translation edit
rate (TER) (Snover et al, 2006) scoring. Extensions
of the last two are included in this study together
with alignments based on hidden Markov model
(HMM) (Vogel et al, 1996) and inversion transduc-
tion grammars (ITG) (Wu, 1997).
System combinations produced via confusion net-
work decoding using different hypothesis alignment
algorithms have been entered into open evalua-
tions, most recently in 2011 Workshop on Statistical
Machine Translation (Callison-Burch et al, 2011).
However, there has not been a comparison of the
most popular hypothesis alignment algorithms us-
ing the same sets of MT system outputs and other-
wise identical combination pipelines. This paper at-
tempts to systematically compare the quality of five
hypothesis alignment algorithms. Alignments were
produced for the same system outputs from three
common test sets used in the 2009 NIST Open MT
Evaluation and the 2011 Workshop on Statistical
Machine Translation. Identical pre-processing, de-
coding, and weight tuning algorithms were used to
quantitatively evaluate the alignment quality. Case
insensitive BLEU score (Papineni et al, 2002) was
used as the translation quality metric.
2 Confusion Network Decoding
A confusion network is a linear graph where all
paths visit all nodes. Two consecutive nodes may be
connected by one or more arcs. Given the arcs repre-
sent words in hypotheses, multiple arcs connecting
two consecutive nodes can be viewed as alternative
words in that position of a set of hypotheses encoded
by the network. A special NULL token represents
a skipped word and will not appear in the system
combination output. For example, three hypotheses
outputs (Rosti et al, 2011).
?twelve big cars?, ?twelve cars?, and ?dozen cars?
may be aligned as follows:
twelve big blue cars
twelve NULL NULL cars
dozen NULL blue cars
This alignment may be represented compactly as the
confusion network in Figure 1 which encodes a total
of eight unique hypotheses.
40 1twelve(2)dozen(1) 2big(1)NULL(2) 3blue(2)NULL(1) cars(3)
Figure 1: Confusion network from three strings ?twelve
big blue cars?, ?twelve cars?, and ?dozen blue cars? us-
ing the first as the skeleton. The numbers in parentheses
represent counts of words aligned to the corresponding
arc.
Building confusion networks from multiple ma-
chine translation system outputs has two main prob-
lems. First, one output has to be chosen as the skele-
ton hypothesis which defines the final word order of
the system combination output. Second, MT system
outputs may have very different word orders which
complicates the alignment process. For skeleton se-
lection, Sim et al (2007) proposed choosing the out-
put closest to all other hypotheses when using each
as the reference string in TER. Alternatively, Ma-
tusov et al (2006) proposed leaving the decision to
decoding time by connecting networks built using
each output as a skeleton into a large lattice. The
subnetworks in the latter approach may be weighted
by prior probabilities estimated from the alignment
statistics (Rosti et al, 2007a). Since different align-
ment algorithm produce different statistics and the
gain from the weights is relatively small (Rosti et al,
2011), weights for the subnetworks were not used
in this work. The hypothesis alignment algorithms
used in this work are briefly described in the follow-
ing section.
The confusion networks in this work were repre-
sented in a text lattice format shown in Figure 2.
Each line corresponds to an arc, where J is the arc
index, S is the start node index, E is the end node in-
dex, SC is the score vector, and W is the word label.
The score vector has as many elements as there are
input systems. The elements correspond to each sys-
tem and indicate whether a word from a particular
192
J=0 S=0 E=1 SC=(1,1,0) W=twelve
J=1 S=0 E=1 SC=(0,0,1) W=dozen
J=2 S=1 E=2 SC=(1,0,0) W=big
J=3 S=1 E=2 SC=(0,1,1) W=NULL
J=4 S=2 E=3 SC=(1,0,1) W=blue
J=5 S=2 E=3 SC=(0,1,0) W=NULL
J=6 S=3 E=4 SC=(1,1,1) W=cars
Figure 2: A lattice in text format representing the con-
fusion network in Figure 1. J is the arc index, S and E
are the start and end node indexes, SC is a vector of arc
scores, and W is the word label.
system was aligned to a given link2. These may be
viewed as system specific word confidences, which
are binary when aligning 1-best system outputs. If
no word from a hypothesis is aligned to a given link,
a NULL word token is generated provided one does
not already exist, and the corresponding element in
the NULL word token is set to one. The system
specific word scores are kept separate in order to
exploit system weights in decoding. Given system
weights wn, which sum to one, and system specific
word scores snj for each arc j (the SC elements), the
weighted word scores are defined as:
sj =
Ns?
n=1
wnsnj (1)
where Ns is the number of input systems. The hy-
pothesis score is defined as the sum of the log-word-
scores along the path, which is linearly interpolated
with a logarithm of the language model (LM) score
and a non-NULL word count:
S(E|F ) =
?
j?J (E)
log sj + ?SLM (E) + ?Nw(E)
(2)
where J (E) is the sequence of arcs generating the
hypothesis E for the source sentence F , SLM (E)
is the LM score, and Nw(E) is the number of
non-NULL words. The set of weights ? =
{w1, . . . , wNs , ?, ?} can be tuned so as to optimize
an evaluation metric on a development set.
Decoding with an n-gram language model re-
quires expanding the lattice to distinguish paths with
2A link is used as a synonym to the set of arcs between two
consecutive nodes. The name refers to the confusion network
structure?s resemblance to a sausage.
unique n-gram contexts before LM scores can be as-
signed the arcs. Using long n-gram context may re-
quire pruning to reduce memory usage. Given uni-
form initial system weights, pruning may remove
desirable paths. In this work, the lattices were ex-
panded to bi-gram context and no pruning was per-
formed. A set of bi-gram decoding weights were
tuned directly on the expanded lattices using a dis-
tributed optimizer (Rosti et al, 2010). Since the
score in Equation 2 is not a simple log-linear inter-
polation, the standard minimum error rate training
(Och, 2003) with exact line search cannot be used.
Instead, downhill simplex (Press et al, 2007) was
used in the optimizer client. After bi-gram decod-
ing weight optimization, another set of 5-gram re-
scoring weights were tuned on 300-best lists gener-
ated from the bi-gram expanded lattices.
3 Hypothesis Alignment Algorithms
Two different methods have been proposed for
building confusion networks: pairwise and incre-
mental alignment. In pairwise alignment, each
hypothesis corresponding to a source sentence is
aligned independently with the skeleton hypothe-
sis. This set of alignments is consolidated using the
skeleton words as anchors to form the confusion net-
work (Matusov et al, 2006; Sim et al, 2007). The
same word in two hypotheses may be aligned with a
different word in the skeleton resulting in repetition
in the network. A two-pass alignment algorithm to
improve pairwise TER alignments was introduced in
(Ayan et al, 2008). In incremental alignment (Rosti
et al, 2008), the confusion network is initialized by
forming a simple graph with one word per link from
the skeleton hypothesis. Each remaining hypothesis
is aligned with the partial confusion network, which
allows words from all previous hypotheses be con-
sidered as matches. The order in which the hypothe-
ses are aligned may influence the alignment qual-
ity. Rosti et al (2009) proposed a sentence specific
alignment order by choosing the unaligned hypoth-
esis closest to the partial confusion network accord-
ing to TER. The following five alignment algorithms
were used in this study.
193
3.1 Pairwise GIZA++ Enhanced Hypothesis
Alignment
Matusov et al (2006) proposed using the GIZA++
Toolkit (Och and Ney, 2003) to align a set of tar-
get language translations. A parallel corpus where
each system output acting as a skeleton appears as
a translation of all system outputs corresponding to
the same source sentence. The IBM Model 1 (Brown
et al, 1993) and hidden Markov model (HMM) (Vo-
gel et al, 1996) are used to estimate the alignment.
Alignments from both ?translation? directions are
used to obtain symmetrized alignments by interpo-
lating the HMM occupation statistics (Matusov et
al., 2004). The algorithm may benefit from the fact
that it considers the entire test set when estimating
the alignment model parameters; i.e., word align-
ment links from all output sentences influence the
estimation, whereas other alignment algorithms only
consider words within a pair of sentences (pairwise
alignment) or all outputs corresponding to a single
source sentence (incremental alignment). However,
it does not naturally extend to incremental align-
ment. The monotone one-to-one alignments are then
transformed into a confusion network. This aligner
is referred to as GIZA later in this paper.
3.2 Incremental Indirect Hidden Markov
Model Alignment
He et al (2008) proposed using an indirect hidden
Markov model (IHMM) for pairwise alignment of
system outputs. The parameters of the IHMM are
estimated indirectly from a variety of sources in-
cluding semantic word similarity, surface word sim-
ilarity, and a distance-based distortion penalty. The
alignment between two target language outputs are
treated as the hidden states. A standard Viterbi al-
gorithm is used to infer the alignment. The pair-
wise IHMM was extended to operate incrementally
in (Li et al, 2009). Sentence specific alignment or-
der is not used by this aligner, which is referred to
as iIHMM later in this paper.
3.3 Incremental Inversion Transduction
Grammar Alignment with Flexible
Matching
Karakos et al (2008) proposed using inversion trans-
duction grammars (ITG) (Wu, 1997) for pairwise
alignment of system outputs. ITGs form an edit
distance, invWER (Leusch et al, 2003), that per-
mits properly nested block movements of substrings.
For well-formed sentences, this may be more nat-
ural than allowing arbitrary shifts. The ITG algo-
rithm is very expensive due to its O(n6) complexity.
The search algorithm for the best ITG alignment, a
best-first chart parsing (Charniak et al, 1998), was
augmented with an A? search heuristic of quadratic
complexity (Klein and Manning, 2003), resulting in
significant reduction in computational complexity.
The finite state-machine heuristic computes a lower
bound to the alignment cost of two strings by allow-
ing arbitrary word re-orderings. The ITG hypothesis
alignment algorithm was extended to operate incre-
mentally in (Karakos et al, 2010) and a novel ver-
sion where the cost function is computed based on
the stem/synonym similarity of (Snover et al, 2009)
was used in this work. Also, a sentence specific
alignment order was used. This aligner is referred
to as iITGp later in this paper.
3.4 Incremental Translation Edit Rate
Alignment with Flexible Matching
Sim et al (2007) proposed using translation edit rate
scorer3 to obtain pairwise alignment of system out-
puts. The TER scorer tries to find shifts of blocks
of words that minimize the edit distance between
the shifted reference and a hypothesis. Due to the
computational complexity, a set of heuristics is used
to reduce the run time (Snover et al, 2006). The
pairwise TER hypothesis alignment algorithm was
extended to operate incrementally in (Rosti et al,
2008) and also extended to consider synonym and
stem matches in (Rosti et al, 2009). The shift
heuristics were relaxed for flexible matching to al-
low shifts of blocks of words as long as the edit dis-
tance is decreased even if there is no exact match in
the new position. A sentence specific alignment or-
der was used by this aligner, which is referred to as
iTER later in this paper.
3.5 Incremental Translation Edit Rate Plus
Alignment
Snover et al (2009) extended TER scoring to con-
sider synonyms and paraphrase matches, called
3http://www.cs.umd.edu/?snover/tercom/
194
TER-plus (TERp). The shift heuristics in TERp
were also relaxed relative to TER. Shifts are allowed
if the words being shifted are: (i) exactly the same,
(ii) synonyms, stems or paraphrases of the corre-
sponding reference words, or (iii) any such combina-
tion. Xu et al (2011) proposed using an incremental
version of TERp for building consensus networks. A
sentence specific alignment order was used by this
aligner, which is referred to as iTERp later in this
paper.
4 Experimental Evaluation
Combination experiments were performed on (i)
Arabic-English, from the informal system combi-
nation track of the 2009 NIST Open MT Evalua-
tion4; (ii) German-English from the system com-
bination evaluation of the 2011 Workshop on Sta-
tistical Machine Translation (Callison-Burch et al,
2011) (WMT11) and (iii) Spanish-English, again
from WMT11. Eight top-performing systems (as
evaluated using case-insensitive BLEU) were used
in each language pair. Case insensitive BLEU scores
for the individual system outputs on the tuning and
test sets are shown in Table 1. About 300 and
800 sentences with four reference translations were
available for Arabic-English tune and test sets, re-
spectively, and about 500 and 2500 sentences with a
single reference translation were available for both
German-English and Spanish-English tune and test
sets. The system outputs were lower-cased and to-
kenized before building confusion networks using
the five hypothesis alignment algorithms described
above. Unpruned English bi-gram and 5-gram lan-
guage models were trained with about 6 billion
words available for these evaluations. Multiple com-
ponent language models were trained after dividing
the monolingual corpora by source. Separate sets
of interpolation weights were tuned for the NIST
and WMT experiments to minimize perplexity on
the English reference translations of the previous
evaluations, NIST MT08 and WMT10. The sys-
tem combination weights, both bi-gram lattice de-
coding and 5-gram 300-best list re-scoring weights,
were tuned separately for lattices build with each hy-
pothesis alignment algorithm. The final re-scoring
4http://www.itl.nist.gov/iad/mig/tests/
mt/2009/ResultsRelease/indexISC.html
outputs were detokenized before computing case in-
sensitive BLEU scores. Statistical significance was
computed for each pairwise comparison using boot-
strapping (Koehn, 2004).
Decode Oracle
Aligner tune test tune test
GIZA 60.06 57.95 75.06 74.47
iTER 59.74 58.63? 73.84 73.20
iTERp 60.18 59.05? 76.43 75.58
iIHMM 60.51 59.27?? 76.50 76.17
iITGp 60.65 59.37?? 76.53 76.05
Table 2: Case insensitive BLEU scores for NIST MT09
Arabic-English system combination outputs. Note, four
reference translations were available. Decode corre-
sponds to results after weight tuning and Oracle corre-
sponds to graph TER oracle. Dagger (?) denotes statisti-
cally significant difference compared to GIZA and double
dagger (?) compared to iTERp and the aligners above it.
The BLEU scores for Arabic-English system
combination outputs are shown in Table 2. The first
column (Decode) shows the scores on tune and test
sets for the decoding outputs. The second column
(Oracle) shows the scores for oracle hypotheses ob-
tained by aligning the reference translations with the
confusion networks and choosing the path with low-
est graph TER (Rosti et al, 2008). The rows rep-
resenting different aligners are sorted according to
the test set decoding scores. The order of the BLEU
scores for the oracle translations do not always fol-
low the order for the decoding outputs. This may be
due to differences in the compactness of the confu-
sion networks. A more compact network has fewer
paths and is therefore less likely to contain signif-
icant parts of the reference translation, whereas a
reference translation may be generated from a less
compact network. On Arabic-English, all incremen-
tal alignment algorithms are significantly better than
the pairwise GIZA, incremental IHMM and ITG
with flexible matching are significantly better than
all other algorithms, but not significantly different
from each other. The incremental TER and TERp
were statistically indistinguishable. Without flexi-
ble matching, iITG yields a BLEU score of 58.85 on
test. The absolute BLEU gain over the best individ-
ual system was between 6.2 and 7.6 points on the
test set.
195
Arabic German Spanish
System tune test tune test tune test
A 48.84 48.54 21.96 21.41 27.71 27.13
B 49.15 48.97 22.61 21.80 28.42 27.90
C 49.30 49.50 22.77 21.99 28.57 28.23
D 49.38 49.59 22.90 22.41 29.00 28.41
E 49.42 49.75 22.90 22.65 29.15 28.50
F 50.28 50.69 22.98 22.65 29.53 28.61
G 51.49 50.81 23.41 23.06 29.89 29.82
H 51.72 51.74 24.28 24.16 30.55 30.14
Table 1: Case insensitive BLEU scores for the individual system outputs on the tune and test sets for all three source
languages.
Decode Oracle
Aligner tune test tune test
GIZA 25.93 26.02 37.32 38.22
iTERp 26.46 26.10 38.16 38.76
iTER 26.27 26.39? 37.00 37.66
iIHMM 26.34 26.40? 37.87 38.48
iITGp 26.47 26.50? 37.99 38.60
Table 3: Case insensitive BLEU scores for WMT11
German-English system combination outputs. Note, only
a single reference translation per segment was available.
Decode corresponds to results after weight tuning and
Oracle corresponds to graph TER oracle. Dagger (?)
denotes statistically significant difference compared to
iTERp and GIZA.
The BLEU scores for German-English system
combination outputs are shown in Table 3. Again,
the graph TER oracle scores do not follow the same
order as the decoding scores. The scores for GIZA
and iTERp are statistically indistinguishable, and
iTER, iIHMM, and iITGp are significantly better
than the first two. However, they are not statistically
different from each other. Without flexible match-
ing, iITG yields a BLEU score of 26.47 on test. The
absolute BLEU gain over the best individual system
was between 1.9 and 2.3 points on the test set.
The BLEU scores for Spanish-English system
combination outputs are shown in Table 4. All align-
ers but iIHMM are statistically indistinguishable and
iIHMM is significantly better than all other align-
ers. Without flexible matching, iITG yields a BLEU
score of 33.62 on test. The absolute BLEU gain over
the best individual system was between 3.5 and 3.9
Decode Oracle
Aligner tune test tune test
iTERp 34.20 33.61 50.45 51.28
GIZA 34.02 33.62 50.23 51.20
iTER 34.44 33.79 50.39 50.39
iITGp 34.41 33.85 50.55 51.33
iIHMM 34.61 34.05? 50.48 51.27
Table 4: Case insensitive BLEU scores for WMT11
Spanish-English system combination outputs. Note, only
a single reference translation per segment was available.
Decode corresponds to results after weight tuning and
Oracle corresponds to graph TER oracle. Dagger (?)
denotes statistically significant difference compared to
aligners above iIHMM.
points on the test set.
5 Error Analysis
Error analysis was performed to better understand
the gains from system combination. Specifically, (i)
how the different types of translation errors are af-
fected by system combination was investigated; and
(ii) an attempt to quantify the correlation between
the word agreement that results from the different
aligners and the translation error, as measured by
TER (Snover et al, 2006), was made.
5.1 Influence on Error Types
For each one of the individual systems, and for each
one of the three language pairs, the per-sentence er-
rors that resulted from that system, as well as from
each one of the the different aligners studied in this
paper, were computed. The errors were broken
196
down into insertions/deletions/substitutions/shifts
based on the TER scorer.
The error counts at the document level were ag-
gregated. For each document in each collection, the
number of errors of each type that resulted from each
individual system as well as each system combina-
tion were measured, and their difference was com-
puted. If the differences are mostly positive, then
it can be said (with some confidence) that system
combination has a significant impact in reducing the
error of that type. A paired Wilcoxon test was per-
formed and the p-value that quantifies the probabil-
ity that the measured error reduction was achieved
under the null hypothesis that the system combina-
tion performs as well as the best system was com-
puted.
Table 5 shows all conditions under consideration.
All cases where the p-value is below 10?2 are con-
sidered statistically significant. Two observations
are in order: (i) all alignment schemes significantly
reduce the number of substitution/shift errors; (ii)
in the case of insertions/deletions, there is no clear
trend; there are cases where the system combination
increases the number of insertions/deletions, com-
pared to the individual systems.
5.2 Relationship between Word Agreement
and Translation Error
This set of experiments aimed to quantify the rela-
tionship between the translation error rate and the
amount of agreement that resulted from each align-
ment scheme. The amount of system agreement at
a level x is measured by the number of cases (con-
fusion network arcs) where x system outputs con-
tribute the same word in a confusion network bin.
For example, the agreement at level 2 is equal to 2
in Figure 1 because there are exactly 2 arcs (with
words ?twelve? and ?blue?) that resulted from the
agreement of 2 systems. Similarly, the agreement at
level 3 is 1, because there is only 1 arc (with word
?cars?) that resulted from the agreement of 3 sys-
tems. It is hypothesized that a sufficiently high level
of agreement should be indicative of the correctness
of a word (and thus indicative of lower TER). The
agreement statistics were grouped into two values:
the ?weak? agreement statistic, where at most half
of the combined systems contribute a word, and the
?strong? agreement statistic, where more than half
non-NULL words NULL words
weak strong weak strong
Arabic 0.087 -0.068 0.192 0.094
German 0.117 -0.067 0.206 0.147
Spanish 0.085 -0.134 0.323 0.102
Table 6: Regression coefficients of the ?strong? and
?weak? agreement features, as computed with a gener-
alized linear model, using TER as the target variable.
of the combined systems contribute a word. To sig-
nify the fact that real words and ?NULL? tokens
have different roles and should be treated separately,
two sets of agreement statistics were computed.
A regression with a generalized linear model
(glm) that computed the coefficients of the agree-
ment quantities (as explained above) for each align-
ment scheme, using TER as the target variable, was
performed. Table 6 shows the regression coeffi-
cients; they are all significant at p-value < 0.001.
As is clear from this table, the negative coefficient of
the ?strong? agreement quantity for the non-NULL
words points to the fact that good aligners tend to
result in reductions in translation error. Further-
more, increasing agreements on NULL tokens does
not seem to reduce TER.
6 Conclusions
This paper presented a systematic comparison of
five different hypothesis alignment algorithms for
MT system combination via confusion network de-
coding. Pre-processing, decoding, and weight tun-
ing were controlled and only the alignment algo-
rithm was varied. Translation quality was compared
qualitatively using case insensitive BLEU scores.
The results showed that confusion network decod-
ing yields a significant gain over the best individ-
ual system irrespective of the alignment algorithm.
Differences between the combination output using
different alignment algorithms were relatively small,
but incremental alignment consistently yielded bet-
ter translation quality compared to pairwise align-
ment based on these experiments and previously
published literature. Incremental IHMM and a novel
incremental ITG with flexible matching consistently
yield highest quality combination outputs. Further-
more, an error analysis shows that most of the per-
197
Language Aligner ins del sub shft
GIZA 2.2e-16 0.9999 2.2e-16 2.2e-16
iHMM 2.2e-16 0.433 2.2e-16 2.2e-16
Arabic iITGp 0.8279 2.2e-16 2.2e-16 2.2e-16
iTER 4.994e-07 3.424e-11 2.2e-16 2.2e-16
iTERp 2.2e-16 1 2.2e-16 2.2e-16
GIZA 7.017e-12 2.588e-06 2.2e-16 2.2e-16
iHMM 6.858e-07 0.4208 2.2e-16 2.2e-16
German iITGp 0.8551 0.2848 2.2e-16 2.2e-16
iTER 0.2491 1.233e-07 2.2e-16 2.2e-16
iTERp 0.9997 0.007489 2.2e-16 2.2e-16
GIZA 2.2e-16 0.8804 2.2e-16 2.2e-16
iHMM 2.2e-16 1 2.2e-16 2.2e-16
Spanish iITGp 2.2e-16 0.9999 2.2e-16 2.2e-16
iTER 2.2e-16 1 2.2e-16 2.2e-16
iTERp 3.335e-16 1 2.2e-16 2.2e-16
Table 5: p-values which show which error types are statistically significantly improved for each language and aligner.
formance gains from system combination can be at-
tributed to reductions in substitution errors and word
re-ordering errors. Finally, better alignments of sys-
tem outputs, which tend to cause higher agreement
rates on words, correlate with reductions in transla-
tion error.
References
Necip Fazil Ayan, Jing Zheng, and Wen Wang. 2008.
Improving alignments for better confusion networks
for combining machine translation systems. In Proc.
Coling, pages 33?40.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proc. ASRU,
pages 351?354.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar F. Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Proc.
WMT, pages 22?64.
Eugene Charniak, Sharon Goldwater, and Mark Johnson.
1998. Edge-based best-first chart parsing. In Proc.
Sixth Workshop on Very Large Corpora, pages 127?
133. Morgan Kaufmann.
Jacob Devlin, Antti-Veikko I. Rosti, Shankar Ananthakr-
ishnan, and Spyros Matsoukas. 2011. System combi-
nation using discriminative cross-adaptation. In Proc.
IJCNLP, pages 667?675.
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates: Recognizer output vot-
ing error reduction (ROVER). In Proc. ASRU, pages
347?354.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proc. ANLP, pages 95?
100.
Xiaodong He and Kristina Toutanova. 2009. Joint opti-
mization for machine translation system combination.
In Proc. EMNLP, pages 1202?1211.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proc. EMNLP, pages 98?
107.
Almut S. Hildebrand and Stephan Vogel. 2008. Combi-
nation of machine translation systems via hypothesis
selection from combined n-best lists. In AMTA, pages
254?261.
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word
matching. In Proc. EAMT.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation sys-
tem combination using ITG-based alignments. In
Proc. ACL, pages 81?84.
Damianos Karakos, Jason R. Smith, and Sanjeev Khu-
danpur. 2010. Hypothesis ranking and two-pass ap-
proaches for machine translation system combination.
In Proc. ICASSP.
198
Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact Viterbi parse selection. In Proc.
NAACL, pages 40?47.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP,
pages 388?395.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2003.
A novel string-to-string distance measure with appli-
cations to machine translation evaluation. In Proc. MT
Summit 2003, pages 240?247, September.
Chi-Ho Li, Xiaodong He, Yupeng Liu, and Ning Xi.
2009. Incremental hmm alignment for mt system com-
bination. In Proc. ACL/IJCNLP, pages 949?957.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373?
400.
Evgeny Matusov, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical ma-
chine translation. In Proc. COLING, pages 219?225.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proc. EACL, pages 33?40.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. ACL, pages
311?318.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2007. Numerical recipes:
the art of scientific computing. Cambridge University
Press, 3rd edition.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Rirchard
Schwartz. 2007a. Improved word-level system com-
bination for machine translation. In Proc. ACL, pages
312?319.
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,
Richard Schwartz, Necip Fazil Ayan, and Bonnie J.
Dorr. 2007b. Combining outputs from multiple
machine translation systems. In Proc. NAACL-HLT,
pages 228?235.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translation system combination. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 183?186.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hypothesis
alignment with flexible matching for building confu-
sion networks: BBN system description for WMT09
system combination task. In Proc. WMT, pages 61?
65.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN system descrip-
tion for WMT10 system combination task. In Proc.
WMT, pages 321?326.
Antti-Veikko I. Rosti, Evgeny Matusov, Jason Smith,
Necip Fazil Ayan, Jason Eisner, Damianos Karakos,
Sanjeev Khudanpur, Gregor Leusch, Zhifei Li, Spy-
ros Matsoukas, Hermann Ney, Richard Schwartz, Bing
Zhang, and Jing Zheng. 2011. Confusion network de-
coding for MT system combination. In Joseph Olive,
Caitlin Christianson, and John McCary, editors, Hand-
book of Natural Language Processing and Machine
Translation: DARPA Global Autonomous Language
Exploitation, pages 333?361. Springer.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007. Con-
sensus network decoding for statistical machine trans-
lation system combination. In Proc. ICASSP.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. AMTA, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy or
HTER? exploring different human judgments with a
tunable MT metric. In Proc. WMT, pages 259?268.
Stephan Vogel, Hermann Ney, and Christoph Tillman.
1996. HMM-based word alignment in statistical trans-
lation. In Proc. ICCL, pages 836?841.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403, Septem-
ber.
Daguang Xu, Yuan Cao, and Damianos Karakos. 2011.
Description of the JHU system combination scheme
for WMT 2011. In Proc. WMT, pages 171?176.
199
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 158?163,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
OmnifluentTM English-to-French and Russian-to-English Systems for the
2013 Workshop on Statistical Machine Translation
Evgeny Matusov, Gregor Leusch
Science Applications International Corporation (SAIC)
7990 Science Applications Ct.
Vienna, VA, USA
{evgeny.matusov,gregor.leusch}@saic.com
Abstract
This paper describes OmnifluentTM Trans-
late ? a state-of-the-art hybrid MT sys-
tem capable of high-quality, high-speed
translations of text and speech. The sys-
tem participated in the English-to-French
and Russian-to-English WMT evaluation
tasks with competitive results. The
features which contributed the most to
high translation quality were training data
sub-sampling methods, document-specific
models, as well as rule-based morpholog-
ical normalization for Russian. The latter
improved the baseline Russian-to-English
BLEU score from 30.1 to 31.3% on a held-
out test set.
1 Introduction
Omnifluent Translate is a comprehensive multilin-
gual translation platform developed at SAIC that
automatically translates both text and audio con-
tent. SAIC?s technology leverages hybrid machine
translation, combining features of both rule-based
machine and statistical machine translation for im-
proved consistency, fluency, and accuracy of trans-
lation output.
In the WMT 2013 evaluation campaign, we
trained and tested the Omnifluent system on the
English-to-French and Russian-to-English tasks.
We chose the En?Fr task because Omnifluent En?
Fr systems are already extensively used by SAIC?s
commercial customers: large human translation
service providers, as well as a leading fashion de-
signer company (Matusov, 2012). Our Russian-to-
English system also produces high-quality transla-
tions and is currently used by a US federal govern-
ment customer of SAIC.
Our experimental efforts focused mainly on the
effective use of the provided parallel and monolin-
gual data, document-level models, as well using
rules to cope with the morphological complexity
of the Russian language. While striving for the
best possible translation quality, our goal was to
avoid those steps in the translation pipeline which
would make a real-time use of the Omnifluent sys-
tem impossible. For example, we did not integrate
re-scoring of N-best lists with huge computation-
ally expensive models, nor did we perform system
combination of different system variants. This al-
lowed us to create a MT system that produced our
primary evaluation submission with the translation
speed of 18 words per second1. This submission
had a BLEU score of 24.2% on the Russian-to-
English task2, and 27.3% on the English-to-French
task. In contrast to many other submissions from
university research groups, our evaluation system
can be turned into a fully functional, commer-
cially deployable on-line system with the same
high level of translation quality and speed within
a single work day.
The rest of the paper is organized as follows. In
the next section, we describe the core capabilities
of the Omnifluent Translate systems. Section 3
explains our data selection and filtering strategy.
In Section 4 we present the document-level trans-
lation and language models. Section 5 describes
morphological transformations of Russian. In sec-
tions 6 we present an extension to the system that
allows for automatic spelling correction. In Sec-
tion 7, we discuss the experiments and their evalu-
ation. Finally, we conclude the paper in Section 8.
2 Core System Capabilities
The Omnifluent system is a state-of-the-art hybrid
MT system that originates from the AppTek tech-
nology acquired by SAIC (Matusov and Ko?pru?,
2010a). The core of the system is a statistical
search that employs a combination of multiple
1Using a single core of a 2.8 GHz Intel Xeon CPU.
2The highest score obtained in the evaluation was 25.9%
158
probabilistic translation models, including phrase-
based and word-based lexicons, as well as reorder-
ing models and target n-gram language models.
The retrieval of matching phrase pairs given an
input sentence is done efficiently using an algo-
rithm based on the work of (Zens, 2008). The
main search algorithm is the source cardinality-
synchronous search. The goal of the search is to
find the most probable segmentation of the source
sentence into non-empty non-overlapping contigu-
ous blocks, select the most probable permutation
of those blocks, and choose the best phrasal trans-
lations for each of the blocks at the same time. The
concatenation of the translations of the permuted
blocks yields a translation of the whole sentence.
In practice, the permutations are limited to allow
for a maximum of M ?gaps? (contiguous regions
of uncovered word positions) at any time during
the translation process. We set M to 2 for the
English-to-French translation to model the most
frequent type of reordering which is the reorder-
ing of an adjective-noun group. The value of M
for the Russian-to-English translation is 3.
The main differences of Omnifluent Trans-
late as compared to the open-source MT sys-
tem Moses (Koehn et al, 2007) is a reordering
model that penalizes each deviation from mono-
tonic translation instead of assigning costs propor-
tional to the jump distance (4 features as described
by Matusov and Ko?pru? (2010b)) and a lexicaliza-
tion of this model when such deviations depend on
words or part-of-speech (POS) tags of the last cov-
ered and current word (2 features, see (Matusov
and Ko?pru?, 2010a)). Also, the whole input doc-
ument is always visible to the system, which al-
lows the use of document-specific translation and
language models. In translation, multiple phrase
tables can be interpolated linearly on the count
level, as the phrasal probabilities are computed
on-the-fly. Finally, various novel phrase-level fea-
tures have been implemented, including binary
topic/genre/phrase type indicators and translation
memory match features (Matusov, 2012).
The Omnifluent system also allows for partial
or full rule-based translations. Specific source lan-
guage entities can be identified prior to the search,
and rule-based translations of these entities can
be either forced to be chosen by the MT system,
or can compete with phrase translation candidates
from the phrase translation model. In both cases,
the language model context at the boundaries of
the rule-based translations is taken into account.
Omnifluent Translate identifies numbers, dates,
URLs, e-mail addresses, smileys, etc. with manu-
ally crafted regular expressions and uses rules to
convert them to the appropriate target language
form. In addition, it is possible to add manual
translation rules to the statistical phrase table of
the system.
3 Training Data Selection and Filtering
We participated in the constrained data track of
the evaluation in order to obtain results which are
comparable to the majority of the other submis-
sions. This means that we trained our systems only
on the provided parallel and monolingual data.
3.1 TrueCasing
Instead of using a separate truecasing module, we
apply an algorithm for finding the true case of the
first word of each sentence in the target training
data and train truecased phrase tables and a true-
cased language model3. Thus, the MT search de-
cides on the right case of a word when ambiguities
exist. Also, the Omnifluent Translate system has
an optional feature to transfer the case of an input
source word to the word in the translation output
to which it is aligned. Although this approach is
not always error-free, there is an advantage to it
when the input contains previously unseen named
entities which use common words that have to be
capitalized. We used this feature for our English-
to-French submission only.
3.2 Monolingual Data
For the French language model, we trained sepa-
rate 5-gram models on the two GigaWord corpora
AFP and APW, on the provided StatMT data for
2007?2012 (3 models), on the EuroParl data, and
on the French side of the bilingual data. LMs were
estimated and pruned using the IRSTLM toolkit
(Federico et al, 2008). We then tuned a linear
combination of these seven individual parts to op-
timum perplexity on WMT test sets 2009 and 2010
and converted them for use with the KenLM li-
brary (Heafield, 2011). Similarly, our English LM
was a linear combination of separate LMs built for
GigaWord AFP, APW, NYT, and the other parts,
StatMT 2007?2012, Europarl/News Commentary,
and the Yandex data, which was tuned for best per-
plexity on the WMT 2010-2013 test sets.
3Source sentences were lowercased.
159
3.3 Parallel Data
Since the provided parallel corpora had differ-
ent levels of noise and quality of sentence align-
ment, we followed a two-step procedure for fil-
tering the data. First, we trained a baseline sys-
tem on the ?good-quality? data (Europarl and
News Commentary corpora) and used it to trans-
late the French side of the Common Crawl data
into English. Then, we computed the position-
independent word error rate (PER) between the
automatic translation and the target side on the
segment level and only kept those original seg-
ment pairs, the PER for which was between 10%
and 60%. With this criterion, we kept 48% of the
original 3.2M sentence pairs of the common-crawl
data.
To leverage the significantly larger Multi-UN
parallel corpus, we performed perplexity-based
data sub-sampling, similarly to the method de-
scribed e. g. by Axelrod et al (2011). First, we
trained a relatively small 4-gram LM on the source
(English) side of our development data and evalu-
ation data. Then, we used this model to compute
the perplexity of each Multi-UN source segment.
We kept the 700K segments with the lowest per-
plexity (normalized by the segment length), so that
the size of the Multi-UN corpus does not exceed
30% of the total parallel corpus size. This proce-
dure is the only part of the translation pipeline for
which we currently do not have a real-time solu-
tion. Yet such a real-time algorithm can be imple-
mented without problems: we word-align the orig-
inal corpora using GIZA++ahead of time, so that af-
ter sub-sampling we only need to perform a quick
phrase extraction. To obtain additional data for
the document-level models only (see Section 4),
we also applied this procedure to the even larger
Gigaword corpus and thus selected 1M sentence
pairs from this corpus.
We used the PER-based procedure as described
above to filter the Russian-English Common-
crawl corpus to 47% of its original size. The base-
line system used to obtain automatic translation
for the PER-based filtering was trained on News
Commentary, Yandex, and Wiki headlines data.
4 Document-level Models
As mentioned in the introduction, the Omnifluent
system loads a whole source document at once.
Thus, it is possible to leverage document context
by using document-level models which score the
phrasal translations of sentences from a specific
document only and are unloaded after processing
of this document.
To train a document-level model for a specific
document from the development, test, or evalua-
tion data, we automatically extract those source
sentences from the background parallel training
data which have (many) n-grams (n=2...7) in com-
mon with the source sentences of the document.
Then, to train the document-level LM we take the
target language counterparts of the extracted sen-
tences and train a standard 3-gram LM on them.
To train the document-level phrase table, we take
the corresponding word alignments for the ex-
tracted source sentences and their target counter-
parts, and extract the phrase table as usual. To
keep the additional computational overhead min-
imal yet have enough data for model estimation,
we set the parameters of the n-gram matching
in such a way that the number of sentences ex-
tracted for document-level training is around 20K
for document-level phrase tables and 100K for
document-level LMs.
In the search, the counts from the document-
level phrase table are linearly combined with the
counts from the background phrase table trained
on the whole training data. The document-level
LM is combined log-linearly with the general LM
and all the other models and features. The scal-
ing factors for the document-level LMs and phrase
tables are not document-specific; neither is the
linear interpolation factor for a document-level
phrase table which we tuned manually on a devel-
opment set. The scaling factor for the document-
level LM was optimized together with the other
scaling factors using Minimum Error Rate Train-
ing (MERT, see (Och, 2003)).
For English-to-French translation, we used both
document-level phrase tables and document-level
LMs; the background data for them contained the
sub-sampled Gigaword corpus (see Section 3.3).
We used only the document-level LMs for the
Russian-to-English translation. They were ex-
tracted from the same data that was used to train
the background phrase table.
5 Morphological Transformations of
Russian
Russian is a morphologically rich language. Even
for large vocabulary MT systems this leads to data
sparseness and high out-of-vocabulary rate. To
160
mitigate this problem, we developed rules for re-
ducing the morphological complexity of the lan-
guage, making it closer to English in terms of the
used word forms. Another goal was to ease the
translation of some morphological and syntactic
phenomena in Russian by simplifying them; this
included adding artificial function words.
We used the pymorphy morphological analyzer4
to analyze Russian words in the input text. The
output of pymorphy is one or more alternative
analyses for each word, each of which includes
the POS tag plus morphological categories such
as gender, tense, etc. The analyses are generated
based on a manual dictionary, do not depend on
the context, and are not ordered by probability of
any kind. However, to make some functional mod-
ifications to the input sentences, we applied the
tool not to the vocabulary, but to the actual input
text; thus, in some cases, we introduced a context
dependency. To deterministically select one of the
pymorphy?s analyses, we defined a POS priority
list. Nouns had a higher priority than adjectives,
and adjectives higher priority than verbs. Other-
wise we relied on the first analysis for each POS.
The main idea behind our hand-crafted rules
was to normalize any ending/suffix which does not
carry information necessary for correct translation
into English. Under normalization we mean the
restoration of some ?base? form. The pymorphy
analyzer API provides inflection functions so that
each word could be changed into a particular form
(case, tense, etc.). We came up with the following
normalization rules:
? convert all adjectives and participles to first-
person masculine singular, nominative case;
? convert all nouns to the nominative case
keeping the plural/singular distinction;
? for nouns in genitive case, add the artificial
function word ?of ? after the last noun before
the current one, if the last noun is not more
than 4 positions away;
? for each verb infinitive, add the artificial
function word ?to ? in front of it;
? convert all present-tense verbs to their infini-
tive form;
? convert all past-tense verbs to their past-tense
first-person masculine singular form;
? convert all future-tense verbs to the artificial
function word ?will ? + the infinitive;
4https://bitbucket.org/kmike/pymorphy
? For verbs ending with reflexive suffixes
??/??, add the artificial function word ?sya ?
in front of the verb and remove the suf-
fix. This is done to model the reflexion (e.g.
??? ????????? ??? sya_ ??????? ? ?he
washed himself?, here ?sya ? corresonds to
?himself?), as well as, in other cases, the pas-
sive mood (e.g. ??? ???????????  ??
sya_ ?????????? ?it is inserted?).
An example that is characteristic of all these mod-
ifications is given in Figure 1.
It is worth noting that not all of these transfor-
mations are error-free because the analysis is also
not always error-free. Also, sometimes there is in-
formation loss (as in case of the instrumental noun
case, for example, which we currently drop instead
of finding the right artificial preposition to express
it). Nevertheless, our experiments show that this is
a successful morphological normalization strategy
for a statistical MT system.
6 Automatic Spelling Correction
Machine translation input texts, even if prepared
for evaluations such as WMT, still contain spelling
errors, which lead to serious translation errors. We
extended the Omnifluent system by a spelling cor-
rection module based on Hunspell5 ? an open-
source spelling correction software and dictionar-
ies. For each input word that is unknown both to
the Omnifluent MT system and to Hunspell, we
add those Hunspell?s spelling correction sugges-
tions to the input which are in the vocabulary of
the MT system. They are encoded in a lattice and
assigned weights. The weight of a suggestion is
inversely proportional to its rank in the Hunspell?s
list (the first suggestions are considered to be more
probable) and proportional to the unigram proba-
bility of the word(s) in the suggestion. To avoid
errors related to unknown names, we do not apply
spelling correction to words which begin with an
uppercase letter.
The lattice is translated by the decoder using
the method described in (Matusov et al, 2008);
the globally optimal suggestion is selected in the
translation process. On the English-to-French
task, 77 out of 3000 evaluation data sentences
were translated differently because of automatic
spelling correction. The BLEU score on these
sentences improved from 22.4 to 22.6%. Man-
ual analysis of the results shows that in around
5http://hunspell.sourceforge.net
161
source ???? ?????????? ? ????? ????????? ?????? ????????? ????? ????? ????????? ???? ?? ????
prep ???? sya_ ???????? ? ????? ????????? ?????? ????????? ???? ????? ????????? of_ ??? ?? ????
ref The dinner was held at a Washington hotel a few hours after the conference of the court over the case
Figure 1: Example of the proposed morphological normalization rules and insertion of artificial function
words for Russian.
System BLEU PER
[%] [%]
baseline 31.3 41.1
+ extended features 31.7 41.0
+ alignment combination 32.1 40.6
+ doc-level models 32.7 39.3
+ common-crawl/UN data 33.0 39.9
Table 1: English-to-French translation results
(newstest-2012-part2 progress test set).
70% of the cases the MT system picks the right
or almost right correction. We applied automatic
spelling correction also to the Russian-to-English
evaluation submissions. Here, the spelling correc-
tion was applied to words which remained out-of-
vocabulary after applying the morphological nor-
malization rules.
7 Experiments
7.1 Development Data and Evaluation
Criteria
For our experiments, we divided the 3000-
sentence newstest-2012 test set from the WMT
2012 evaluation in two roughly equal parts, re-
specting document boundaries. The first part we
used as a tuning set for N-best list MERT opti-
mization (Och, 2003). We used the second part
as a test set to measure progress; the results on it
are reported below. We computed case-insensitive
BLEU score (Papineni et al, 2002) for optimiza-
tion and evaluation. Only one reference translation
was available.
7.2 English-to-French System
The baseline system for the English-to-French
translation direction was trained on Europarl and
News Commentary corpora. The word align-
ment was obtained by training HMM and IBM
Model 3 alignment models and combining their
two directions using the ?grow-diag-final? heuris-
tic (Koehn, 2004). The first line in Table 1 shows
the result for this system when we only use the
standard features (phrase translation and word lex-
icon costs in both directions, the base reorder-
System BLEU PER
[%] [%]
baseline (full forms) 30.1 38.9
morph. reduction 31.3 38.1
+ extended features 32.4 37.3
+ doc-level LMs 32.3 37.4
+ common-crawl data 32.9 37.1
Table 2: Russian-to-English translation results
(newstest-2012-part2 progress test set).
ing features as described in (Matusov and Ko?pru?,
2010b) and the 5-gram target LM). When we
also optimize the scaling factors for extended fea-
tures, including the word-based and POS-based
lexicalized reordering models described in (Ma-
tusov and Ko?pru?, 2010a), we improve the BLEU
score by 0.4% absolute. Extracting phrase pairs
from three different, equally weighted alignment
heuristics improves the score by another 0.3%.
The next big improvement comes from using
document-level language models and phrase ta-
bles, which include Gigaword data. Especially the
PER decreases significantly, which indicates that
the document-level models help, in most cases, to
select the right word translations. Another signifi-
cant improvement comes from adding parts of the
Common-crawl and Multi-UN data, sub-sampled
with the perplexity-based method as described in
Section 3.3. The settings corresponding to the last
line of Table 1 were used to produce the Omniflu-
ent primary submission, which resulted in a BLEU
score of 27.3 on the WMT 2013 test set.
After the deadline for submission, we discov-
ered a bug in the extraction of the phrase table
which had reduced the positive impact of the ex-
tended phrase-level features. We re-ran the opti-
mization on our tuning set and obtained a BLEU
score of 27.7% on the WMT 2013 evaluation set.
7.3 Russian-to-English System
The first experiment with the Russian-to-English
system was to show the positive effect of the
morphological transformations described in Sec-
tion 5. Table 2 shows the result of the baseline
system, trained using full forms of the Russian
162
words on the News Commentary, truecased Yan-
dex and Wiki Headlines data. When applying the
morphological transformations described in Sec-
tion 5 both in training and translation, we obtain
a significant improvement in BLEU of 1.3% ab-
solute. The out-of-vocabulary rate was reduced
from 0.9 to 0.5%. This shows that the morpholog-
ical reduction actually helps to alleviate the data
sparseness problem and translate structurally com-
plex constructs in Russian.
Significant improvements are obtained for Ru?
En through the use of extended features, including
the lexicalized and ?POS?-based reordering mod-
els. As the ?POS? tags for the Russian words we
used the pymorphy POS tag selected deterministi-
cally based on our priority list, together with the
codes for additional morphological features such
as tense, case, and gender. In contrast to the En?
Fr task, document-level models did not help here,
most probably because we used only LMs and
only trained on sub-sampled data that was already
part of the background phrase table. The last boost
in translation quality was obtained by adding those
segments of the cleaned Common-crawl data to
the phrase table training which are similar to the
development and evaluation data in terms of LM
perplexity. The BLEU score in the last line of Ta-
ble 2 corresponds to Omnifluent?s BLEU score of
24.2% on the WMT 2013 evaluation data. This is
only 1.7% less than the score of the best BLEU-
ranked system in the evaluation.
8 Summary and Future Work
In this paper we described the Omnifluent hybrid
MT system and its use for the English-to-French
and Russian-to-English WMT tasks. We showed
that it is important for good translation quality to
perform careful data filtering and selection, as well
as use document-specific phrase tables and LMs.
We also proposed and evaluated rule-based mor-
phological normalizations for Russian. They sig-
nificantly improved the Russian-to-English trans-
lation quality. In contrast to some evaluation par-
ticipants, the presented high-quality system is fast
and can be quickly turned into a real-time system.
In the future, we intend to improve the rule-based
component of the system, allowing users to add
and delete translation rules on-the-fly.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain Adaptation via Pseudo In-Domain
Data Selection. In International Conference on Em-
perical Methods in Natural Language Processing,
Edinburgh, UK, July.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Proceed-
ings of Interspeech, pages 1618?1621.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, Scotland, United Kingdom,
July.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), Prague, Czech Repub-
lic. Association for Computational Linguistics.
Philipp Koehn. 2004. Pharaoh: a beam search
decoder for phrase-based statistical machine trans-
lation models. In 6th Conference of the As-
sociation for Machine Translation in the Ameri-
cas (AMTA 04), pages 115?124, Washington DC,
September/October.
Evgeny Matusov and Selc?uk Ko?pru?. 2010a. AppTek?s
APT Machine Translation System for IWSLT 2010.
In Proc. of the International Workshop on Spoken
Language Translation, Paris, France, December.
Evgeny Matusov and Selc?uk Ko?pru?. 2010b. Improv-
ing Reordering in Statistical Machine Translation
from Farsi. In AMTA 2010: The Ninth Conference
of the Association for Machine Translation in the
Americas, Denver, Colorado, USA, November.
Evgeny Matusov, Bjo?rn Hoffmeister, and Hermann
Ney. 2008. ASR word lattice translation with
exhaustive reordering is possible. In Interspeech,
pages 2342?2345, Brisbane, Australia, September.
Evgeny Matusov. 2012. Incremental Re-training of a
Hybrid English-French MT System with Customer
Translation Memory Data. In 10th Conference of the
Association for Machine Translation in the Amer-
icas (AMTA 12), San Diego, CA, USA, October-
November.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In 41st Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 160?167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL ?02: Pro-
ceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 311?318,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Richard Zens. 2008. Phrase-based Statistical Machine
Translation: Models, Search, Training. Ph.D. the-
sis, RWTH Aachen University, Aachen, Germany,
February.
163
