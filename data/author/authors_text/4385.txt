Proceedings of the 12th European Workshop on Natural Language Generation, pages 126?129,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
A Situated Context Model for
Resolution and Generation of Referring Expressions
Hendrik Zender and Geert-Jan M. Kruijff and Ivana Kruijff-Korbayova?
Language Technology Lab, German Research Center for Artificial Intelligence (DFKI)
Saarbru?cken, Germany
{zender, gj, ivana.kruijff}@dfki.de
Abstract
The background for this paper is the aim
to build robotic assistants that can ?natu-
rally? interact with humans. One prereq-
uisite for this is that the robot can cor-
rectly identify objects or places a user
refers to, and produce comprehensible ref-
erences itself. As robots typically act
in environments that are larger than what
is immediately perceivable, the problem
arises how to identify the appropriate con-
text, against which to resolve or produce
a referring expression (RE). Existing al-
gorithms for generating REs generally by-
pass this problem by assuming a given
context. In this paper, we explicitly ad-
dress this problem, proposing a method for
context determination in large-scale space.
We show how it can be applied both for re-
solving and producing REs.
1 Introduction
The past years have seen an extraordinary increase
in research on robotic assistants that help users
perform daily chores. Autonomous vacuum clean-
ers have already found their way into people?s
homes, but it will still take a while before fully
conversational robot ?gophers? will assist people
in more demanding everyday tasks. Imagine a
robot that can deliver objects, and give directions
to visitors on a university campus. This robot must
be able to verbalize its knowledge in a way that is
understandable by humans.
A conversational robot will inevitably face sit-
uations in which it needs to refer to an entity (an
object, a locality, or even an event) that is located
somewhere outside the current scene, as Figure 1
illustrates. There are conceivably many ways in
which a robot might refer to things in the world,
but many such expressions are unsuitable in most
Where is the 
IT Help desk?
It is on the 
1st floor in 
building 3b.
it is at
<45.56, -3.92, 10.45>
Where i  the 
IT hel  desk?
It is on the 1st 
floor in building 
3B.
It is at
Figure 1: Situated dialogue with a service robot
human-robot dialogues. Consider the following
set of examples:
1. ?position P = ?45.56,?3.92, 10.45??
2. ?Peter?s office no. 200 at the end of the cor-
ridor on the third floor of the Acme Corp.
building 3 in the Acme Corp. complex, 47
Evergreen Terrace, Calisota, Earth, (...)?
3. ?the area?
These REs are valid descriptions of their respec-
tive referents. Still they fail to achieve their com-
municative goal, which is to specify the right
amount of information that the hearer needs to
uniquely identify the referent. The next REs might
serve as more appropriate variants of the previous
examples (in certain contexts! ):
1. ?the IT help desk?
2. ?Peter?s office?
3. ?the large hall on the first floor?
The first example highlights a requirement on the
knowledge representation to which an algorithm
for generating referring expressions (GRE) has ac-
cess. Although the robot needs a robot-centric rep-
resentation of its surrounding space that allows it
to safely perform actions and navigate its world,
it should use human-centric qualitative descrip-
tions when talking about things in the world. We
126
do not address this issue here, but refer the inter-
ested reader to our recent work on multi-layered
spatial maps for robots, bridging the gap between
robot-centric and human-centric spatial represen-
tations (Zender et al, 2008).
The other examples point out another impor-
tant consideration: howmuch information does the
human need to single out the intended referent
among the possible entities that the robot could be
referring to? According to the seminal work on
GRE by Dale and Reiter (1995), one needs to dis-
tinguish whether the intended referent is already
in the hearer?s focus of attention or not. This focus
of attention can consist of a local visual scene (vi-
sual context) or a shared workspace (spatial con-
text), but also contains recently mentioned entities
(dialogue context). If the referent is already part
of the current context, the GRE task merely con-
sists of singling it out among the other members
of the context, which act as distractors. In this
case the generated RE contains discriminatory in-
formation, e.g. ?the red ball? if several kinds of ob-
jects with different colors are in the context. If, on
the other hand, the referent is not in the hearer?s fo-
cus of attention, an RE needs to contain what Dale
and Reiter call navigational, or attention-directing
information. The example they give is ?the black
power supply in the equipment rack,? where ?the
equipment rack? is supposed to direct the hearers
attention to the rack and its contents.
In the following we propose an approach for
context determination and extension that allows a
mobile robot to produce and interpret REs to enti-
ties outside the current visual context.
2 Background
Most GRE approaches are applied to very lim-
ited, visual scenes ? so-called small-scale space.
The domain of such systems is usually a small vi-
sual scene, e.g. a number of objects, such as cups
and tables, located in the same room), or other
closed-context scenarios (Dale and Reiter, 1995;
Horacek, 1997; Krahmer and Theune, 2002). Re-
cently, Kelleher and Kruijff (2006) have presented
an incremental GRE algorithm for situated di-
alogue with a robot about a table-top setting,
i.e. also about small-scale space. In all these cases,
the context set is assumed to be identical to the
visual scene that is shared between the interlocu-
tors. The intended referent is thus already in the
hearer?s focus of attention.
In contrast, robots typically act in large-scale
space, i.e. space ?larger than what can be per-
ceived at once? (Kuipers, 1977). They need the
ability to understand and produce references to
things that are beyond the current visual and spa-
tial context. In any situated dialogue that involves
entities beyond the current focus of attention, the
task of extending the context becomes key.
Paraboni et al (2007) present an algorithm for
context determination in hierarchically ordered
domains, e.g. a university campus or a document
structure. Their approach is mainly targeted at
producing textual references to entities in written
documents (e.g. figures, tables in book chapters).
Consequently they do not address the challenges
that arise in physically and perceptually situated
dialogues. Still, the approach presents a num-
ber of good contributions towards GRE for situ-
ated dialogue in large-scale space. An appropriate
context, as a subset of the full domain, is deter-
mined through Ancestral Search. This search for
the intended referent is rooted in the ?position of
the speaker and the hearer in the domain? (repre-
sented as d), a crucial first step towards situated-
ness. Their approach suffers from the shortcom-
ing that spatial relationships are treated as one-
place attributes by their GRE algorithm. For ex-
ample they transform the spatial containment re-
lation that holds between a room entity and a
building entity (?the library in the Cockroft build-
ing?) into a property of the room entity (BUILDING
NAME = COCKROFT) and not a two-place relation
(in(library,Cockroft)). Thus they avoid
recursive calls to the algorithm, which would be
needed if the intended referent is related to another
entity that needs to be properly referred to.
However, according to Dale and Reiter (1995),
these related entities do not necessarily serve as
discriminatory information. At least in large-scale
space, in contrast to a document structure that is
conceivably transparent to a reader, they function
as attention-directing elements that are introduced
to build up common ground by incrementally ex-
tending the hearer?s focus of attention. Moreover,
representing some spatial relations as two-place
predicates between two entities and some as one-
place predicates is an arbitrary decision.
We present an approach for context determina-
tion (or extension), that imposes less restrictions
on its knowledge base, and which can be used as a
sub-routine in existing GRE algorithms.
127
3 Situated Dialogue in Large-Scale Space
Imagine the situation in Figure 1 did not take place
somewhere on campus, but rather inside building
3B. Certainly the robot would not have said ?the
IT help desk is on the 1st floor in building 3B.?
To avoid confusing the human, an utterance like
?the IT help desk is on the 1st floor? would have
been appropriate. Likewise, if the IT help desk
happened to be located on another site of the uni-
versity, the robot would have had to identify its lo-
cation as being ?on the 1st floor in building 3B on
the new campus.? The hierarchical representation
of space that people are known to assume (Cohn
and Hazarika, 2001), reflects upon the choice of
an appropriate context when producing REs.
In the above example the physical and spatial
situatedness of the dialogue participants play an
important role in determining which related parts
of space come into consideration as potential dis-
tractors. Another important observation concerns
the verbal behavior of humans when talking about
remote objects and places during a complex dia-
logue (i.e. more than just a question and a reply).
Consider the following example dialogue:
Person A: ?Where is the exit??
Person B: ?You first go down this corridor.
Then you turn right. After a few steps you
will see the big glass doors.?
Person A: ?And the bus station? Is it to the
left??
The dialogue illustrates how utterances become
grounded in previously introduced discourse ref-
erents, both temporally and spatially. Initially,
the physical surroundings of the dialogue partners
form the context for anchoring references. As a di-
alogue unfolds, this point can conceptually move
to other locations that have been explicitly intro-
duced. Discourse markers denoting spatial or tem-
poral cohesion (e.g. ?then? or ?there?) can make
this move to a new anchor explicit, leading to a
?mental tour? through large-scale space.
We propose a general principle of Topological
Abstraction (TA) for context extension which is
rooted in what we will call the Referential Anchor
a.1 TA is designed for a multiple abstraction hier-
archy (e.g. represented as a lattice structure rather
than a simple tree). The Referential Anchor a, cor-
responding to the current focus of attention, forms
the nucleus of the context. In the simple case, a
1similar to Ancestral Search (Paraboni et al, 2007)
loc1 loc2 loc3
room1 room2
floor1_1 floor1_2
building1
loc4 (a) loc5 loc7 loc8loc6
room3 room4 room5 (r)
floor2_1 floor2_2
building2
1
2
3
4
Figure 2: Incremental TA in large-scale space
corresponds to the hearer?s physical location. As
illustrated above, a can also move along the ?spa-
tial progression? of the most salient discourse en-
tity during a dialogue. If the intended referent is
outside the current context, TA extends the context
by incrementally ascending the spatial abstraction
hierarchy until the intended referent is an element
of the resulting sub-hierarchy, as illustrated in Fig-
ure 2. Below we describe two instantiations of the
TA principle, a TA algorithm for reference gener-
ation (TAA1) and TAA2 for reference resolution.
Context Determination for GRE TAA1 con-
structs a set of entities dominated by the Referen-
tial Anchor a (and a itself). If this set contains the
intended referent r, it is taken as the current utter-
ance context set. Else TAA1 moves up one level
of abstraction and adds the set of all child nodes to
the context set. This loop continues until r is in the
context set. At that point TAA1 stops and returns
the constructed context set (cf. Algorithm 1).
TAA1 is formulated to be neutral to the kind of
GRE algorithm that it is used for. It can be used
with the original Incremental Algorithm (Dale and
Reiter, 1995), augmented by a recursive call if a
relation to another entity is selected as a discrim-
inatory feature. It could in principle also be used
with the standard approach to GRE involving re-
lations (Dale and Haddock, 1991), but we agree
with Paraboni et al (2007) that the mutually qual-
ified references that it can produce2 are not easily
resolvable if they pertain to circumstances where
a confirmatory search is costly (such as in large-
scale space). More recent approaches to avoid-
ing infinite loops when using relations in GRE
make use of a graph-based knowledge represen-
tation (Krahmer et al, 2003; Croitoru and van
Deemter, 2007). TAA1 is compatible with these
approaches, as well as with the salience based ap-
proach of (Krahmer and Theune, 2002).
2An example for such a phenomenon is the expression
?the ball on the table? in a context with several tables and
several balls, but of which only one is on a table. Humans
find such REs natural and easy to resolve in visual scenes.
128
Algorithm 1 TAA1 (for reference generation)
Require: a = referential anchor; r = intended referent
Initialize context: C = {}
C = C ? topologicalChildren(a) ? {a}
if r ? C then
return C
else
Initialize: SUPERNODES = {a}
for each n ? SUPERNODES do
for each p ? topologicalParents(n) do
SUPERNODES = SUPERNODES ? {p}
C = C ? topologicalChildren(p)
end for
if r ? C then
return C
end if
end for
return failure
end if
Algorithm 2 TAA2 (for reference resolution)
Require: a = ref. anchor; desc(x) = description of referent
Initialize context: C = {}
Initialize possible referents: R = {}
C = C ? topologicalChildren(a) ? {a}
R = desc(x) ? C
if R 6= {} then
return R
else
Initialize: SUPERNODES = {a}
for each n ? SUPERNODES do
for each p ? topologicalParents(n) do
SUPERNODES = SUPERNODES ? {p}
C = C ? topologicalChildren(p)
end for
R = desc(x) ? C
if R 6= {} then
return R
end if
end for
return failure
end if
Resolving References to Elsewhere Analogous
to the GRE task, a conversational robot must be
able to understand verbal descriptions by its users.
In order to avoid overgenerating possible refer-
ents, we propose TAA2 (cf. Algorithm 2) which
tries to select an appropriate referent from a rel-
evant subset of the full knowledge base. It is ini-
tialized with a given semantic representation of the
referential expression, desc(x), in a format com-
patible with the knowledge base. Then, an appro-
priate entity satisfying this description is searched
for in the knowledge base. Similarly to TAA1,
the description is first matched against the current
context set C consisting of a and its child nodes. If
this set does not contain any instances that match
desc(x), TAA2 increases the context set alng the
spatial abstraction axis until at least one possible
referent can be identified within the context.
4 Conclusions and Future Work
We have presented two algorithms for context de-
termination that can be used both for resolving and
generating REs in large-scale space.
We are currently planning a user study to evalu-
ate the performance of the TA algorithms. Another
important item for future work is the exact nature
of the spatial progression, modeled by ?moving?
the referential anchor, in a situated dialogue.
Acknowledgments
This work was supported by the EU FP7 ICT
Project ?CogX? (FP7-ICT-215181).
References
A. G. Cohn and S. M. Hazarika. 2001. Qualitative
spatial representation and reasoning: An overview.
Fundamenta Informaticae, 46:1?29.
M. Croitoru and K. van Deemter. 2007. A conceptual
graph approach to the generation of referring expres-
sions. In Proc. IJCAI-2007, Hyderabad, India.
R. Dale and N. Haddock. 1991. Generating referring
expressions involving relations. In Proc. of the 5th
Meeting of the EACL, Berlin, Germany, April.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the Gricean Maxims in the generation of re-
ferring expressions. Cognitive Science, 19(2):233?
263.
H. Horacek. 1997. An algorithm for generating ref-
erential descriptions with flexible interfaces. In
Proc. of the 35th Annual Meeting of the ACL and
8th Conf. of the EACL, Madrid, Spain.
J. Kelleher and G.-J. Kruijff. 2006. Incremental gener-
ation of spatial referring expressions in situated di-
alogue. In In Proc. Coling-ACL 06, Sydney, Aus-
tralia.
E. Krahmer and M. Theune. 2002. Efficient context-
sensitive generation of referring expressions. In
K. van Deemter and R.Kibble, editors, Information
Sharing: Givenness and Newness in Language Pro-
cessing. CSLI Publications, Stanford, CA, USA.
E. Krahmer, S. van Erk, and A. Verleg. 2003. Graph-
based generation of referring expressions. Compu-
tational Linguistics, 29(1).
B. Kuipers. 1977. Representing Knowledge of Large-
scale Space. Ph.D. thesis, Massachusetts Institute of
Technology, Cambridge, MA, USA.
I. Paraboni, K. van Deemter, and J. Masthoff. 2007.
Generating referring expressions: Making refer-
ents easy to identify. Computational Linguistics,
33(2):229?254, June.
H. Zender, O. Mart??nez Mozos, P. Jensfelt, G.-J. Krui-
jff, and W. Burgard. 2008. Conceptual spatial rep-
resentations for indoor mobile robots. Robotics and
Autonomous Systems, 56(6):493?502, June.
129
227
228
229
230
231
232
233
234
199
200
201
202
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 9?16, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Data-driven Approaches for Information Structure Identification
Oana Postolache,
Ivana Kruijff-Korbayova?
University of Saarland,
Saarbru?cken, Germany
{oana,korbay}@coli.uni-saarland.de
Geert-Jan M. Kruijff
German Research Center for
Artificial Intelligence (DFKI GmbH)
Saarbru?cken, Germany
gj@dfki.de
Abstract
This paper investigates automatic identi-
fication of Information Structure (IS) in
texts. The experiments use the Prague
Dependency Treebank which is annotated
with IS following the Praguian approach
of Topic Focus Articulation. We auto-
matically detect t(opic) and f(ocus), us-
ing node attributes from the treebank as
basic features and derived features in-
spired by the annotation guidelines. We
present the performance of decision trees
(C4.5), maximum entropy, and rule in-
duction (RIPPER) classifiers on all tec-
togrammatical nodes. We compare the re-
sults against a baseline system that always
assigns f(ocus) and against a rule-based
system. The best system achieves an ac-
curacy of 90.69%, which is a 44.73% im-
provement over the baseline (62.66%).
1 Introduction
Information Structure (IS) is a partitioning of the
content of a sentence according to its relation to
the discourse context. There are numerous theo-
retical approaches describing IS and its semantics
(Halliday, 1967; Sgall, 1967; Vallduv??, 1990; Steed-
man, 2000) and the terminology used is diverse ?
see (Kruijff-Korbayova? and Steedman, 2003) for an
overview. However, all theories consider at least one
of the following two distinctions: (i) a Topic/Focus1
distinction that divides the linguistic meaning of the
sentence into parts that link the sentence content
? We use the Praguian terminology for this distinction.
to the discourse context, and other parts that ad-
vance the discourse, i.e., add or modify informa-
tion; and (ii) a background/kontrast2 distinction be-
tween parts of the utterance which contribute to dis-
tinguishing its actual content from alternatives the
context makes available.
Information Structure is an important factor in de-
termining the felicity of a sentence in a given con-
text. Applications in which IS is crucial are text-
to-speech systems, where IS helps to improve the
quality of the speech output (Prevost and Steedman,
1994; Kruijff-Korbayova? et al, 2003; Moore et al,
2004), and machine translation, where IS improves
target word order, especially that of free word order
languages (Stys and Zemke, 1995).
Existing theories, however, state their principles
using carefully selected illustrative examples. Be-
cause of this, they fail to adequately explain how
different linguistic dimensions cooperate to realize
Information Structure.
In this paper we describe data-driven, machine
learning approaches for automatic identification of
Information Structure; we describe what aspects of
IS we deal with and report results of the performance
of our systems and make an error analysis. For our
experiments, we use the Prague Dependency Tree-
bank (PDT) (Hajic?, 1998). PDT follows the theory
of Topic-Focus Articulation (Hajic?ova? et al, 1998)
and to date is the only corpus annotated with IS.
Each node of the underlying structure of sentences
in PDT is annotated with a TFA value: t(opic), dif-
ferentiated in contrastive and non-contrastive, and
f(ocus). Our system identifies these two TFA val-
ues automatically. We trained three different clas-
? The notion ?kontrast? with a ?k? has been introduced in (Vall-
duv?? and Vilkuna, 1998) to replace what Steedman calls ?fo-
cus?, and to avoid confusion with other definitions of focus.
9
sifiers, C4.5, RIPPER and MaxEnt using basic fea-
tures from the treebank and derived features inspired
by the annotation guidelines. We evaluated the per-
formance of the classifiers against a baseline sys-
tem that simulates the preprocessing procedure that
preceded the manual annotation of PDT, by always
assigning f(ocus), and against a rule-based system
which we implemented following the annotation in-
structions. Our best system achieves a 90.69% accu-
racy, which is a 44.73% improvement over the base-
line (62.66%).
The organization of the paper is as follows.
Section 2 describes the Prague Dependency Tree-
bank and the Praguian approach of Topic-Focus Ar-
ticulation, from two perspectives: of the theoreti-
cal definition and of the annotation guidelines that
have been followed to annotate the PDT. Section 3
presents our experiments, the data settings, results
and error analysis. The paper closes with conclu-
sions and issues for future research (Section 4).
2 Prague Dependency Treebank
The Prague Dependency Treebank (PDT) consists of
newspaper articles from the Czech National Corpus
( ?Cerma?k, 1997) and includes three layers of annota-
tion:
1. The morphological layer gives a full mor-
phemic analysis in which 13 categories are
marked for all sentence tokens (including punc-
tuation marks).
2. The analytical layer, on which the ?surface?
syntax (Hajic?, 1998) is annotated, contains an-
alytical tree structures, in which every token
from the surface shape of the sentence has a
corresponding node labeled with main syntac-
tic functions like SUBJ, PRED, OBJ, ADV.
3. The tectogrammatical layer renders the deep
(underlying) structure of the sentence (Sgall et
al., 1986; Hajic?ova? et al, 1998). Tectogram-
matical tree structures (TGTSs) contain nodes
corresponding only to the autosemantic words
of the sentence (e.g., no preposition nodes) and
to deletions on the surface level; the condi-
tion of projectivity is obeyed, i.e., no cross-
ing edges are allowed; each node of the tree is
assigned a functor such as ACTOR, PATIENT,
ADDRESSEE, ORIGIN, EFFECT, the repertoire
of which is very rich; elementary coreference
links are annotated for pronouns.
2.1 Topic-Focus Articulation (TFA)
The tectogrammatical level of the PDT was moti-
vated by the ever increasing need for large corpora to
include not only morphological and syntactic infor-
mation but also semantic and discourse-related phe-
nomena. Thus, the tectogrammatical trees have been
enriched with features indicating the information
structure of sentences which is a means of showing
their contextual potential.
In the Praguian approach to IS, the content of the
sentence is divided into two parts: the Topic is ?what
the sentence is about? and the Focus represents the
information asserted about the Topic. A prototypical
declarative sentence asserts that its Focus holds (or
does not hold) about its Topic: Focus(Topic) or not-
Focus(Topic).
The TFA definition uses the distinction between
Context-Bound (CB) and Non-Bound (NB) parts of
the sentence. To distinguish which items are CB and
which are NB, the question test is applied, (i.e., the
question for which a given sentence is the appropri-
ate answer is considered). In this framework, weak
and zero pronouns and those items in the answer
which reproduce expressions present in the question
(or associated to those present) are CB. Other items
are NB.
In example (1), (b) is the sentence under investi-
gation, in which CB and NB items are marked. Sen-
tence (a) is the context in which the sentence (b) is
uttered, and sentence (c) is the question for which
the sentence (b) is an appropriate answer:
(1) (a) Tom and Mary both came to John?s party.
(b) John
CB
invited
CB
only
NB
her
NB
.
(c) Whom did John invite?
It should be noted that the CB/NB distinction is
not equivalent to the given/new distinction, as the
pronoun ?her? is NB although the cognitive entity,
Mary, has already been mentioned in the discourse
(therefore is given).
The following rules determine which lexical items
(CB or NB) belong to the Topic or to the Focus of the
sentence (Hajic?ova? et al, 1998; Hajic?ova? and Sgall,
2001):
10
1. The main verb and any of its direct dependents
belong to the Focus if they are NB;
2. Every item that does not depend directly on the
main verb and is subordinated to a Focus el-
ement belongs to the Focus (where ?subordi-
nated to? is defined as the irreflexive transitive
closure of ?depend on?);
3. If the main verb and all its dependents are CB,
then those dependents di of the verb which
have subordinated items sm that are NB are
called ?proxi foci?; the items sm together with
all items subordinated to them belong to the Fo-
cus (i,m > 1);
4. Every item not belonging to the Focus accord-
ing to 1 ? 3 belongs to the Topic.
Applying these rules for the sentence (b) in exam-
ple (1) we find the Topic and the Focus of the sen-
tence: [John invited]Topic [only her]Focus.
It is worth mentioning that although most of the
time, CB items belong to the Topic and NB items
belong to the Focus (as it happens in our exam-
ple too), there may be cases when the Focus con-
tains some NB items and/or the Topic contains some
CB items. Figure 1 shows such configurations: in
the top-left corner the tectogrammatical representa-
tion of sentence (1) (b) is presented together with
its Topic-Focus partitioning. The other three con-
figurations are other possible tectogrammatical trees
with their Topic-Focus partitionings; the top-right
one corresponds to the example (2), the bottom-left
to (3), and bottom-right to (4).
(2) Q: Which teacher did Tom meet?
A: Tom
CB
met
CB
the teacher
CB
of chemistry
NB
.
(3) Q: What did he think about the teachers?
A: He
CB
liked
NB
the teacher
CB
of chemistry
NB
.
(4) Q: What did the teachers do?
A: The teacher
CB
of chemistry
NB
met
NB
his
CB
pupils
NB
.
2.2 TFA annotation
Within PDT, the TFA attribute has been annotated
for all nodes (including the restored ones) from the
tectogrammatical level. Instructions for the assign-
ment of the TFA attribute have been specified in
Figure 1: Topic-Focus partitionings of tectogram-
matical trees.
(Bura?n?ova? et al, 2000) and are summarized in Ta-
ble 1. These instructions are based on the surface
word order, the position of the sentence stress (into-
nation center ? IC)3 and the canonical order of the
dependents.
The TFA attribute has three values:
1. t ? for non-contrastive CB items;
2. f ? for NB items;
3. c ? for contrastive CB items.
In this paper, we do not distinguish between con-
trastive and non-contrastive items, considering both
of them as being just t. In the PDT annotation, the
notation t (from topic) and f (from focus) was chosen
to be used because, as we mentioned earlier, in the
most common cases and in prototypical sentences,
t-items belong to the Topic and f-items to the Focus.
Prior the manual annotation, the PDT corpus was
preprocessed to mark all nodes with the TFA at-
tribute of f, as it is the most common value. Then
the annotators corrected the value according to the
guidelines in Table 1.
Figure 2 illustrates the tectogramatical tree struc-
ture of the following sentence:
(5) Sebeve?dom??m
self-confidence
votroku?
bastards
to
it
ale
but
neotr?a?slo.
not shake
?But it did not shake the self-confidence of those bas-
tards?.
? In the PDT the intonation center is not annotated. However,
the annotators were instructed to use their judgement where
the IC would be if they uttered the sentence.
11
1. The bearer of the IC (typically, the rightmost child of the verb) f
2. If IC is not on the rightmost child, everything after IC t
3. A left-side child of the verb (unless it carries IC) t
4. The verb and the right children of the verb before the f-node (cf. 1) that are canon-
ically ordered
f
5. Embedded attributes (unless repeated or restored) f
6. Restored nodes t
7. Indexical expressions (ja? I, ty you, te?d now, tady here), weak pronouns, pronominal
expressions with a general meaning (ne?kdo somebody, jednou once) (unless they
carry IC)
t
8. Strong forms of pronouns not preceded by a preposition (unless they carry IC) t
Table 1: Annotation guidelines; IC = Intonation Center.
Each node is labeled with the corresponding word?s
lemma, the TFA attribute, and the functor attribute.
For example, votroku? has lemma votrok, the TFA at-
tribute f, and the functor APP (appurtenance).
Figure 2: Tectogramatical tree annotated with t/f.
In order to measure the consistency of the annota-
tion, Interannotator Agreement has been measured
(Vesela? et al, 2004).4 During the annotation pro-
cess, there were four phases in which parallel anno-
tations have been performed; a sample of data was
chosen and annotated in parallel by three annotators.
AGREEMENT 1 2 3 4 AVG
t/c/f 81.32 81.89 76.21 89.57 82.24
t/f 85.42 83.94 84.18 92.15 86.42
Table 2: Interannotator Agreement for TFA assign-
ment in PDT 2.0.
The agreement for each of the four phases, as well
as an average agreement, is shown in Table 2. The
second row of the table displays the percentage of
nodes for which all three annotators assigned the
? In their paper the authors don?t give Kappa values, nor the
complete information needed to compute a Kappa statistics
ourselves.
same TFA value (be it t, c or f). Because in our
experiments we do not differentiate between t and c,
considering both as t, we computed, in the last row
of the table, the agreement between the three anno-
tators after replacing the TFA value c with t.5
3 Identification of topic and focus
In this section we present data-driven, machine
learning approaches for automatic identification of
Information Structure. For each tectogrammatical
node we detect the TFA value t(opic) or f(ocus) (that
is CB or NB). With these values one can apply the
rules presented in Subsection 2.1 in order to find the
Topic-Focus partitioning of each sentence.
3.1 Experimental settings
Our experiments use the tectogrammatical trees
from The Prague Dependency Treebank 2.0.6 Statis-
tics of the experimental data are shown in Table 3.
Our goal is to automatically label the tectogram-
matical nodes with topic or focus. We built ma-
chine learning models based on three different well
known techniques, decision trees (C4.5), rule induc-
tion (RIPPER) and maximum entropy (MaxEnt), in
order to find out which approach is the most suitable
for our task. For C4.5 and RIPPER we use the Weka
implementations (Witten and Frank, 2000) and for
MaxEnt we use the openNLP package.7
? In (Vesela? et al, 2004), the number of cases when the anno-
tators disagreed when labeling t or c is reported; this allowed
us to compute the t/f agreement, by disregarding this number.
? We are grateful to the researchers at the Charles University in
Prague for providing us the data before the PDT 2.0 official
release.
? http://maxent.sourceforge.net/
12
PDT DATA TRAIN DEV EVAL TOTAL
#files 2,53680%
316
10%
316
10%
3,168
100%
#sentences 38,73778.3%
5,228
10.6%
5,477
11.1%
49,442
100%
#tokens 652,70078.3%
87,988
10.6%
92,669
11.1%
833,356
100%
#tecto-nodes 494,75978.3%
66,711
10.5%
70,323
11.2%
631,793
100%
Table 3: PDT data: Statistics for the training, devel-
opment and evaluation sets.
All our models use the same set of 35 features (pre-
sented in detail in Appendix A), divided in two
types:
1. Basic features, consisting of attributes of the
tectogrammatical nodes whose values were
taken directly from the treebank annotation.
We used a total of 25 basic features, that may
have between 2 and 61 values.
2. Derived features, inspired by the annotation
guidelines. The derived features are computed
using the dependency information from the tec-
togrammatical level of the treebank and the
surface order of the words corresponding to
the nodes.8 We also used lists of forms of
Czech pronouns that are used as weak pro-
nouns, indexical expressions, pronouns with
general meaning, or strong pronouns. All the
derived features have boolean values.
3.2 Results
The classifiers were trained on 494,759 instances
(78.3%) (cf. Table 3) (tectogrammatical nodes) from
the training set. The performance of the classifiers
was evaluated on 70,323 instances (11.2%) from the
evaluation set. We compared our models against a
baseline system that assigns focus to all nodes (as it
is the most common value) and against a determinis-
tic, rule-based system, that implements the instruc-
tions from the annotation guidelines.
Table 4 shows the percentages of correctly classi-
fied instances for our models. We also performed a
? In the tectogramatical level in the PDT, the order of the nodes
has been changed during the annotation process of the TFA
attribute, so that all t items precede all f items. Our fea-
tures use the surface order of the words corresponding to the
nodes.
10-fold cross validation, which for C4.5 gives accu-
racy of 90.62%.
BASELINE RULE-BASED C4.5 RIPPER MAXENT
62.66 58.92 90.69 88.46? 88.97
Table 4: Correctly classified instances (the numbers
are given as percentages). ?The RIPPER classifier
was trained with only 40% of the training data.
The baseline value is considerably high due to the
topic/focus distribution in the test set (a similar dis-
tribution characterizes the training set as well). The
rule-based system performs very poorly, although it
follows the guidelines according to which the data
was annotated. This anomaly is due to the fact that
the intonation center of the sentence, which plays a
very important role in the annotation, is not marked
in the corpus, thus the rule-based system doesn?t
have access to this information.
The results show that all three models perform
much better than the baseline and the rule-based sys-
tem. We used the ?? test to examine if the dif-
ference between the three classifiers is statistically
significant. The C4.5 model significantly outper-
forms the MaxEnt model (?? = 113.9, p < 0.001)
and the MaxEnt model significantly outperforms the
RIPPER model although with a lower level of confi-
dence (?? = 9.1, p < 0.01).
The top of the decision tree generated by C4.5 in
the training phase looks like this:
coref = true
| is_member = true
| | POS = ...
| is_member = false
| | is_rightmost = ...
coref = false
| is_generated = true
| | nodetype = ...
| is_generated = false
| | iterativeness = ...
It is worth mentioning that the RIPPER classifier
was built with only 40% of the training set (with
more data, the system crashes due to insufficient
memory). Interestingly and quite surprisingly, the
values of all three classifiers are actually greater than
the interannotator agreement which has an average
of 86.42%.
What is the cause of the classifiers? success? How
come that they perform better than the annotators
themselves? Is it because they take advantage of a
13
large amount of training data? To answer this ques-
tion we have computed the learning curves. They
are shown in the figure 3, which shows that, actu-
ally, after using only 1% of the training data (4,947
instances), the classifiers already perform very well,
and adding more training data improves the results
only slightly. On the other hand, for RIPPER,
adding more data causes a decrease in performance,
and as we mentioned earlier, even an impossibility
of building a classifier.
 0.82
 0.83
 0.84
 0.85
 0.86
 0.87
 0.88
 0.89
 0.9
 0.91
 0  10  20  30  40  50  60  70  80  90
Co
rre
ct
ly 
Cl
as
sif
ie
d 
In
st
an
ce
s
% of Training Data
Figure 3: Learning curves for C4.5 (+),
RIPPER(?), MaxEnt(?) and a na??ve predictor
(2) (introduced in Section 3.3).
3.3 Error Analysis
If errors don?t come from the lack of training data,
then where do they come from? To answer this ques-
tion we performed an error analysis. For each in-
stance (tectogrammatical node), we considered its
context as being the set of values for the features pre-
sented in Appendix A. Table 5 displays in the second
column the number of all contexts. The last three
columns divide the contexts in three groups:
1. Only t ? all instances having these contexts are
assigned t;
2. Only f ? all instances having these contexts
are assigned f;
3. Ambiguous ? some instances that have these
contexts are assigned t and some other are as-
signed f.
The last row of the table shows the number of in-
stances for each type of context, in the training data.
All Only t Only f Ambiguous
#contexts 27,901 9,901 13,009 4,991
#instances 494,759100%
94,056
19.01%
42,048
8.49%
358,655
72.49%
Table 5: Contexts & Instances in the training set.
Table 5 shows that the source of ambiguity (and
therefore of errors) stays in 4,991 contexts that cor-
respond to nodes that have been assigned both t and
f. Moreover these contexts yield the largest amount
of instances (72.49%). We investigated further these
ambiguous contexts and we counted how many of
them correspond to a set of nodes that are mostly as-
signed t (#t > #f), respectively f (#t < #f), and how
many are highly ambiguous (half of the correspond-
ing instances are assigned t and the other half f (#t =
#f)). The numbers, shown in Table 6, suggest that in
the training data there are 41,851 instances (8.45%)
(the sum of highlighted numbers in the third row of
the Table 6) that are exceptions, meaning they have
contexts that usually correspond to instances that are
assigned the other TFA value. There are two ex-
planations for these exceptions: either they are part
of the annotators disagreement, or they have some
characteristics that our set of features fail to capture.
#t > #f #t = #f #t < #f
#ambiguous
contexts 998 833 3,155
#instances
t=50,722
f=4,854
all=55,576
11.23%
t=602
f=602
all=1,204
0.24%
t=35,793
f=266,082
all=301,875
61.01%
Table 6: Ambiguous contexts in the training data.
The error analysis led us to the idea of implementing
a na??ve predictor. This predictor trains on the train-
ing set, and divides the contexts into five groups. Ta-
ble 7 describes these five types of contexts and dis-
plays the TFA value assigned by the na??ve predictor
for each type.
If an instance has a context of type #t = #f, we
decide to assign f because this is the most common
value. Also, for the same reason, new contexts in
the test set that don?t appear in the training set are
assigned f.
The performance of the na??ve predictor on the
evaluation set is 89.88% (correctly classified in-
stances), a value which is significantly higher than
14
Context Type In the training set, instances with
a context of this type are:
Predicted
TFA value
Only t all t t
Only f all f f
#t > #f more t than f t
#t = #f half t, half f f
#t < #f more f than t f
unseen not seen f
Table 7: Na??ve Predictor: its TFA prediction for
each type of context.
the one obtained by the MaxEnt and RIPPER clas-
sifiers (?? = 30.7, p < 0.001 and respectively ??
= 73.3, p < 0.001), and comparable with the C4.5
value, although the C4.5 classifier still performs sig-
nificantly better (?? = 26.3, p < 0.001).
To find out whether the na??ve predictor would im-
prove if we added more data, we computed the learn-
ing curve, shown in Figure 3. Although the curve
is slightly more abrupt than the ones of the other
classifiers, we do not have enough evidence to be-
lieve that more data in the training set would bring
a significant improvement. We calculated the num-
ber of new contexts in the development set, and al-
though the number is high (2,043 contexts), they
correspond to only 2,125 instances. This suggests
that the new contexts that may appear are very rare,
therefore they cannot yield a big improvement.
4 Conclusions
In this paper we investigated the problem of learn-
ing Information Structure from annotated data. The
contribution of this research is to show for the first
time that IS can be successfuly recovered using
mostly syntactic features. We used the Prague De-
pendency Treebank which is annotated with Infor-
mation Structure following the Praguian theory of
Topic Focus Articulation. The results show that we
can reliably identify t(opic) and f(ocus) with over
90% accuracy while the baseline is at 62%.
Issues for further research include, on the one
hand, a deeper investigation of the Topic-Focus Ar-
ticulation in the Prague Dependency Treebank of
Czech, by improving the feature set, considering
also the distinction between contrastive and non-
contrastive t items and, most importantly, by inves-
tigating how we can use the t/f annotation in PDT
(and respectively our results) in order to detect the
Topic/Focus partitioning of the whole sentence.
We also want to benefit from our experience with
the Czech data in order to create an English corpus
annotated with Information Structure. We have al-
ready started to exploit a parallel English-Czech cor-
pus, in order to transfer to the English version the
topic/focus labels identified by our systems.
References
Eva Bura?n?ova?, Eva Hajic?ova?, and Petr Sgall. 2000. Tagging of very large corpora:
Topic-Focus Articulation. In Proceedings of the 18th International Confer-
ence on Computational Linguistics (COLING 2000), pages 139?144.
Jan Hajic?. 1998. Building a syntactically annotated corpus: The Prague Depen-
dency Treebank. In Eva Hajic?ova?, editor, Issues of valency and Meaning.
Studies in Honor of Jarmila Panevova?. Karolinum, Prague.
Eva Hajic?ova? and Petr Sgall. 2001. Topic-focus and salience. In Proceedings
of the 39th Annual Meeting of the Association for Computational Linguistics
(ACL 2001), pages 268?273, Toulose, France.
Eva Hajic?ova?, Barbara Partee, and Petr Sgall. 1998. Topic-focus articulation,
tripartite structures, and semantic content. In Studies in Linguistics and Phi-
losophy, number 71. Dordrecht: Kluwer.
M. Halliday. 1967. Notes on transitivity and theme in english, part ii. Journal of
Linguistic, (3):199?244.
Ivana Kruijff-Korbayova? and Mark Steedman. 2003. Discourse and Information
Structure. Journal of Logic, Language and Information, (12):249?259.
Ivana Kruijff-Korbayova?, Stina Erricson, Kepa J. Rodr??gues, and Elena
Karagjosova. 2003. Producing Contextually Appropriate Intonation in an
Information-State Based Dialog System. In Proceeding of European Chapter
of the Association for Computational Linguistics, Budapest, Hungary.
Johanna Moore, Mary Ellen Foster, Oliver Lemon, and Michael White. 2004.
Generating Tailored, Comparative Description in Spoken Dialogue. In Pro-
ceedings of the Seventeenth International Florida Artificial Intelligence Re-
search Sociey Conference.
Scott Prevost and Mark Steedman. 1994. Information Based Intonation Synthe-
sis. In Proceedings of the ARPA Workshop on Human Language Technology,
Princeton, USA.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986. The Meaning of the Sen-
tence in Its Semantic and Pragmatic Aspects. Reidel, Dordrecht.
Petr Sgall. 1967. Functional sentence perspective in a generative description.
Prague Studies in Mathematical Linguistics, (2):203?225.
Mark Steedman. 2000. Information Structure and the syntax-phonology inter-
face. Linguistic Inquiry, (34):649?689.
Malgorzata Stys and Stefan Zemke. 1995. Incorporating Discourse Aspects in
English-Polish MT: Towards Robust Implementation. In Recent Advances in
NLP, Velingrad, Bulgaria.
Enrich Vallduv?? and Maria Vilkuna. 1998. On rheme and kontrast. In P. Culicover
and L. McNally, editors, Syntax and Semantics Vol 29: The Limits of Syntax.
Academic Press, San Diego.
Enrich Vallduv??. 1990. The information component. Ph.D. thesis, University of
Pennsylvania.
Frantis?ek ?Cerma?k. 1997. Czech National Corpus: A Case in Many Contexts.
International Journal of Corpus Linguistics, (2):181?197.
Kater?ina Vesela?, Jir??? Havelka, and Eva Hajic?ova. 2004. Annotators? Agreement:
The Case of Topic-Focus Articulation. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC 2004).
Ian H. Witten and Eibe Frank. 2000. Practical Machine Learning Tools and
Techniques with Java Implementations. Morgan Kaufmann, San Francisco.
15
Appendix A
In this appendix we provide a full list of the feature names and the values they take (a feature for MaxEnt being a
combination of the name, value and the prediction).
BASIC FEATURE POSSIBLE VALUES
nodetype complex, atom, dphr, list, qcomplex
is generated true, false
functor ACT, LOC, DENOM, APP, PAT, DIR1, MAT, RSTR, THL, TWHEN, REG,
CPHR, COMPL, MEANS, ADDR, CRIT, TFHL, BEN, ORIG, DIR3, TTILL,
TSIN, MANN, EFF, ID, CAUS, CPR, DPHR, AIM, EXT, ACMP, THO, DIR2,
RESTR, TPAR, PAR, COND, CNCS, DIFF, SUBS, AUTH, INTT, VOCAT,
TOWH, ATT, RHEM, TFRWH, INTF, RESL, PREC, PRED, PARTL, HER,
MOD, CONTRD
coref true, false
afun Pred, Pnom, AuxV, Sb, Obj, Atr, Adv, AtrAdv, AdvAtr, Coord, AtrObj, ObjAtr,
AtrAtr, AuxT, AuxR, AuxP, Apos, ExD, AuxC, Atv, AtvV, AuxO, AuxZ, AuxY,
AuxG, AuxK, NA
POS N, A, R, V, D, C, P, J, T, Z, I, NA
SUBPOS NN, AA, NA, RR, VB, Db, Vp, C=, Dg, PD, Vf, J, J
?
, P7, P4, PS, Cl, TT, RV, PP,
P8, Vs, Cr, AG, Cn, PL, PZ, Vc, AU, PH, Z:, PW, AC, NX, Ca, PQ, P5, PJ, Cv,
PK, PE, P1, Vi, P9, A2, CC, P6, Cy, C?, RF, Co, Ve, II, Cd, Ch, J*, AM, Cw,
AO, Vt, Vm
is member true, false
is parenthesis true, false
sempos n.denot, n.denot.neg, n.pron.def.demon, n.pron.def.pers, n.pron.indef,
n.quant.def, adj.denot, adj.pron.def.demon, adj.pron.indef, adj.quant.def,
adj.quant.indef, adj.quant.grad, adv.denot.grad.nneg, adv.denot.ngrad.nneg,
adv.denot.grad.neg, adv.denot.ngrad.neg, adv.pron.def, adv.pron.indef, v, NA
number sg, pl, inher, nr, NA
gender anim, inan, fem, neut, inher, nr, NA
person 1, 2, 3, inher, NA
degcmp pos, comp, acomp, sup, nr, NA
verbmod ind, imp, cdn, nr, NA
aspect proc, cpl, nr, NA
tense sim, ant, post, nil, NA
numertype basic, set, kind, ord, frac, NA
indeftype relat, indef1, indef2, indef3, indef4, indef5, indef6, inter, negat, total1, total2,
NA
negation neg0, neg1, NA
politeness polite, basic, inher, NA
deontmod deb, hrt, vol, poss, perm, fac, decl, NA
dispmod disp1, disp0, nil, NA
resultative res1, res0, NA
iterativeness it1, it0, NA
DERIVED FEATURE POSSIBLE VALUES
is rightmost true, false
is rightside from verb true, false
is leftside dependent true, false
is embedded attribute true, false
has repeated lemma true, false
is in canonical order true, false
is weak pronoun true, false
is indexical expression true, false
is pronoun with general meaning true, false
is strong pronoun with no prep true, false
16
Analysis of Mixed Natural and Symbolic Language Input
in Mathematical Dialogs
Magdalena Wolska Ivana Kruijff-Korbayova?
Fachrichtung Computerlinguistik
Universita?t des Saarlandes, Postfach 15 11 50
66041 Saarbru?cken, Germany
 
magda,korbay  @coli.uni-sb.de
Abstract
Discourse in formal domains, such as mathemat-
ics, is characterized by a mixture of telegraphic nat-
ural language and embedded (semi-)formal sym-
bolic mathematical expressions. We present lan-
guage phenomena observed in a corpus of dialogs
with a simulated tutorial system for proving theo-
rems as evidence for the need for deep syntactic and
semantic analysis. We propose an approach to input
understanding in this setting. Our goal is a uniform
analysis of inputs of different degree of verbaliza-
tion: ranging from symbolic alone to fully worded
mathematical expressions.
1 Introduction
Our goal is to develop a language understanding
module for a flexible dialog system tutoring math-
ematical problem solving, in particular, theorem
proving (Benzm u?ller et al, 2003a).1 As empirical
findings in the area of intelligent tutoring show, flex-
ible natural language dialog supports active learn-
ing (Moore, 1993). However, little is known about
the use of natural language in dialog setting in for-
mal domains, such as mathematics, due to the lack
of empirical data. To fill this gap, we collected a
corpus of dialogs with a simulated tutorial dialog
system for teaching proofs in naive set theory.
An investigation of the corpus reveals various
phenomena that present challenges for such input
understanding techniques as shallow syntactic anal-
ysis combined with keyword spotting, or statistical
methods, e.g., Latent Semantic Analysis, which are
commonly employed in (tutorial) dialog systems.
The prominent characteristics of the language in our
corpus include: (i) tight interleaving of natural and
symbolic language, (ii) varying degree of natural
language verbalization of the formal mathematical
1This work is carried out within the DIALOG project: a col-
laboration between the Computer Science and Computational
Linguistics departments of the Saarland University, within
the Collaborative Research Center on Resource-Adaptive
Cognitive Processes, SFB 378 (www.coli.uni-sb.de/
sfb378).
content, and (iii) informal and/or imprecise refer-
ence to mathematical concepts and relations.
These phenomena motivate the need for deep
syntactic and semantic analysis in order to ensure
correct mapping of the surface input to the under-
lying proof representation. An additional method-
ological desideratum is to provide a uniform treat-
ment of the different degrees of verbalization of the
mathematical content. By designing one grammar
which allows a uniform treatment of the linguistic
content on a par with the mathematical content, one
can aim at achieving a consistent analysis void of
example-based heuristics. We present such an ap-
proach to analysis here.
The paper is organized as follows: In Section 2,
we summarize relevant existing approaches to in-
put analysis in (tutorial) dialog systems on the one
hand and analysis of mathematical discourse on the
other. Their shortcomings with respect to our set-
ting become clear in Section 3 where we show ex-
amples of language phenomena from our dialogs.
In Section 4, we propose an analysis methodology
that allows us to capture any mixture of natural and
mathematical language in a uniform way. We show
example analyses in Section 5. In Section 6, we
conclude and point out future work issues.
2 Related work
Language understanding in dialog systems, be it
with text or speech interface, is commonly per-
formed using shallow syntactic analysis combined
with keyword spotting. Tutorial systems also suc-
cessfully employ statistical methods which com-
pare student responses to a model built from pre-
constructed gold-standard answers (Graesser et al,
2000). This is impossible for our dialogs, due to
the presence of symbolic mathematical expressions.
Moreover, the shallow techniques also remain obliv-
ious of such aspects of discourse meaning as causal
relations, modality, negation, or scope of quanti-
fiers which are of crucial importance in our setting.
When precise understanding is needed, tutorial sys-
tems either use menu- or template-based input, or
use closed-questions to elicit short answers of lit-
tle syntactic variation (Glass, 2001). However, this
conflicts with the preference for flexible dialog in
active learning (Moore, 1993).
With regard to interpreting mathematical
texts, (Zinn, 2003) and (Baur, 1999) present DRT
analyses of course-book proofs. However, the
language in our dialogs is more informal: natural
language and symbolic mathematical expressions
are mixed more freely, there is a higher degree and
more variety of verbalization, and mathematical
objects are not properly introduced. Moreover, both
above approaches rely on typesetting and additional
information that identifies mathematical symbols,
formulae, and proof steps, whereas our input does
not contain any such information. Forcing the user
to delimit formulae would reduce the flexibility
of the system, make the interface harder to use,
and might not guarantee a clean separation of the
natural language and the non-linguistic content
anyway.
3 Linguistic data
In this section, we first briefly describe the corpus
collection experiment and then present the common
language phenomena found in the corpus.
3.1 Corpus collection
24 subjects with varying educational background
and little to fair prior mathematical knowledge par-
ticipated in a Wizard-of-Oz experiment (Benzm u?ller
et al, 2003b). In the tutoring session, they were
asked to prove 3 theorems2:
(i)  	


 
 


 
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 57?60,
Sydney, July 2006. c?2006 Association for Computational Linguistics
The SAMMIE System: Multimodal In-Car Dialogue
Tilman Becker, Peter Poller,
Jan Schehl
DFKI
First.Last@dfki.de
Nate Blaylock, Ciprian Gerstenberger,
Ivana Kruijff-Korbayova?
Saarland University
talk-mit@coli.uni-sb.de
Abstract
The SAMMIE1 system is an in-car multi-
modal dialogue system for an MP3 ap-
plication. It is used as a testing environ-
ment for our research in natural, intuitive
mixed-initiative interaction, with particu-
lar emphasis on multimodal output plan-
ning and realization aimed to produce out-
put adapted to the context, including the
driver?s attention state w.r.t. the primary
driving task.
1 Introduction
The SAMMIE system, developed in the TALK
project in cooperation between several academic
and industrial partners, employs the Information
State Update paradigm, extended to model collab-
orative problem solving, multimodal context and
the driver?s attention state. We performed exten-
sive user studies in a WOZ setup to guide the sys-
tem design. A formal usability evaluation of the
system?s baseline version in a laboratory environ-
ment has been carried out with overall positive re-
sults. An enhanced version of the system will be
integrated and evaluated in a research car.
In the following sections, we describe the func-
tionality and architecture of the system, point out
its special features in comparison to existing work,
and give more details on the modules that are in
the focus of our research interests. Finally, we
summarize our experiments and evaluation results.
2 Functionality
The SAMMIE system provides a multi-modal inter-
face to an in-car MP3 player (see Fig. 1) through
speech and haptic input with a BMW iDrive input
device, a button which can be turned, pushed down
and sideways in four directions (see Fig. 2 left).
System output is provided by speech and a graphi-
cal display integrated into the car?s dashboard. An
example of the system display is shown in Fig. 2.
1SAMMIE stands for Saarbru?cken Multimodal MP3 Player
Interaction Experiment.
Figure 1: User environment in laboratory setup.
The MP3 player application offers a wide range
of functions: The user can control the currently
playing song, search and browse an MP3 database
by looking for any of the fields (song, artist, al-
bum, year, etc.), search and select playlists and
even construct and edit playlists.
The user of SAMMIE has complete freedom in
interacting with the system. Input can be through
any modality and is not restricted to answers to
system queries. On the contrary, the user can give
new tasks as well as any information relevant to
the current task at any time. This is achieved by
modeling the interaction as a collaborative prob-
lem solving process, and multi-modal interpreta-
tion that fits user input into the context of the
current task. The user is also free in their use
of multimodality: SAMMIE handles deictic refer-
ences (e.g., Play this title while pushing the iDrive
button) and also cross-modal references, e.g., Play
the third song (on the list). Table 1 shows a typ-
ical interaction with the SAMMIE system; the dis-
played song list is in Fig. 2. SAMMIE supports in-
teraction in German and English.
3 Architecture
Our system architecture follows the classical ap-
proach (Bunt et al, 2005) of a pipelined architec-
ture with multimodal interpretation (fusion) and
57
U: Show me the Beatles albums.
S: I have these four Beatles albums.
[shows a list of album names]
U: Which songs are on this one?
[selects the Red Album]
S: The Red Album contains these songs
[shows a list of the songs]
U: Play the third one.
S: [music plays]
Table 1: A typical interaction with SAMMIE.
fission modules encapsulating the dialogue man-
ager. Fig. 2 shows the modules and their inter-
action: Modality-specific recognizers and analyz-
ers provide semantically interpreted input to the
multimodal fusion module that interprets them in
the context of the other modalities and the cur-
rent dialogue context. The dialogue manager de-
cides on the next system move, based on its model
of the tasks as collaborative problem solving, the
current context and also the results from calls to
the MP3 database. The turn planning module then
determines an appropriate message to the user by
planning the content, distributing it over the avail-
able output modalities and finally co-ordinating
and synchronizing the output. Modality-specific
output modules generate spoken output and graph-
ical display update. All modules interact with the
extended information state which stores all context
information.
Figure 2: SAMMIE system architecture.
Many tasks in the SAMMIE system are mod-
eled by a plan-based approach. Discourse mod-
eling, interpretation management, dialogue man-
agement and linguistic planning, and turn plan-
ning are all based on the production rule system
PATE2 (Pfleger, 2004). It is based on some con-
cepts of the ACT-R 4.0 system, in particular the
goal-oriented application of production rules, the
2Short for (P)roduction rule system based on (A)ctivation
and (T)yped feature structure (E)lements.
activation of working memory elements, and the
weighting of production rules. In processing typed
feature structures, PATE provides two operations
that both integrate data and also are suitable for
condition matching in production rule systems,
namely a slightly extended version of the general
unification, but also the discourse-oriented opera-
tion overlay (Alexandersson and Becker, 2001).
4 Related Work and Novel Aspects
Many dialogue systems deployed today follow a
state-based approach that explicitly models the
full (finite) set of dialogue states and all possible
transitions between them. The VoiceXML3 stan-
dard is a prominent example of this approach. This
has two drawbacks: on the one hand, this approach
is not very flexible and typically allows only so-
called system controlled dialogues where the user
is restricted to choosing their input from provided
menu-like lists and answering specific questions.
The user never is in control of the dialogue. For
restricted tasks with a clear structure, such an ap-
proach is often sufficient and has been applied suc-
cessfully. On the other hand, building such appli-
cations requires a fully specified model of all pos-
sible states and transitions, making larger applica-
tions expensive to build and difficult to test.
In SAMMIE we adopt an approach that mod-
els the interaction on an abstract level as collab-
orative problem solving and adds application spe-
cific knowledge on the possible tasks, available re-
sources and known recipes for achieving the goals.
In addition, all relevant context information is
administered in an Extended Information State.
This is an extension of the Information State Up-
date approach (Traum and Larsson, 2003) to the
multi-modal setting.
Novel aspects in turn planning and realization
include the comprehensive modeling in a sin-
gle, OWL-based ontology and an extended range
of context-sensitive variation, including system
alignment to the user on multiple levels.
5 Flexible Multi-modal Interaction
5.1 Extended Information State
The information state of a multimodal system
needs to contain a representation of contextual in-
formation about discourse, but also a represen-
tation of modality-specific information and user-
specific information which can be used to plan
system output suited to a given context. The over-
3http://www.w3.org/TR/voicexml20
58
all information state (IS) of the SAMMIE system is
shown in Fig. 3.
The contextual information partition of the IS
represents the multimodal discourse context. It
contains a record of the latest user utterance and
preceding discourse history representing in a uni-
form way the salient discourse entities introduced
in the different modalities. We adopt the three-
tiered multimodal context representation used in
the SmartKom system (Pfleger et al, 2003). The
contents of the task partition are explained in the
next section.
5.2 Collaborative Problem Solving
Our dialogue manager is based on an
agent-based model which views dialogue
as collaborative problem-solving (CPS)
(Blaylock and Allen, 2005). The basic building
blocks of the formal CPS model are problem-
solving (PS) objects, which we represent as
typed feature structures. PS object types form a
single-inheritance hierarchy. In our CPS model,
we define types for the upper level of an ontology
of PS objects, which we term abstract PS objects.
There are six abstract PS objects in our model
from which all other domain-specific PS objects
inherit: objective, recipe, constraint, evaluation,
situation, and resource. These are used to model
problem-solving at a domain-independent level
and are taken as arguments by all update opera-
tors of the dialogue manager which implement
conversation acts (Blaylock and Allen, 2005).
The model is then specialized to a domain by
inheriting and instantiating domain-specific types
and instances of the PS objects.
5.3 Adaptive Turn Planning
The fission component comprises detailed con-
tent planning, media allocation and coordination
and synchronization. Turn planning takes a set
of CPS-specific conversational acts generated by
the dialogue manager and maps them to modality-
specific communicative acts.
Information on how content should be dis-
tributed over the available modalities (speech or
graphics) is obtained from Pastis, a module which
stores discourse-specific information. Pastis pro-
vides information about (i) the modality on which
the user is currently focused, derived by the cur-
rent discourse context; (ii) the user?s current cog-
nitive load when system interaction becomes a
secondary task (e.g., system interaction while
driving); (iii) the user?s expertise, which is rep-
resented as a state variable. Pastis also contains
information about factors that influence the prepa-
ration of output rendering for a modality, like the
currently used language (German or English) or
the display capabilities (e.g., maximum number of
displayable objects within a table). Together with
the dialogue manager?s embedded part of the in-
formation state, the information stored by Pastis
forms the Extended Information State of the SAM-
MIE system (Fig. 3).
Planning is then executed through a set of pro-
duction rules that determine which kind of infor-
mation should be presented through which of the
available modalities. The rule set is divided in two
subsets, domain-specific and domain-independent
rules which together form the system?s multi-
modal plan library.
contextual-info:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
last-user-utterance:
:
[
interp : set(grounding-acts)
modality-requested : modality
modalities-used : set(msInput)
]
discourse-history:
: list(discourse-objects)
modality-info:
:
[
speech : speechInfo
graphic : graphicInfo
]
user-info:
:
[
cognitive-load : cogLoadInfo
user-expertise : expertiseInfo
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
task-info:
[
cps-state : c-situation (see below for details)
pending-sys-utt : list(grounding-acts)
]
Figure 3: SAMMIE Information State structure.
5.4 Spoken Natural Language Output
Generation
Our goal is to produce output that varies in the sur-
face realization form and is adapted to the con-
text. A template-based module has been devel-
oped and is sufficient for classes of system output
that do not need fine-tuned context-driven varia-
tion. Our template-based generator can also de-
liver alternative realizations, e.g., alternative syn-
tactic constructions, referring expressions, or lexi-
cal items. It is implemented by a set of straightfor-
ward sentence planning rules in the PATE system
to build the templates, and a set of XSLT trans-
formations to yield the output strings. Output in
German and English is produced by accessing dif-
ferent dictionaries in a uniform way.
In order to facilitate incremental development
of the whole system, our template-based mod-
ule has a full coverage wrt. the classes of sys-
59
tem output that are needed. In parallel, we are
experimenting with a linguistically more power-
ful grammar-based generator using OpenCCG4,
an open-source natural language processing en-
vironment (Baldridge and Kruijff, 2003). This al-
lows for more fine-grained and controlled choices
between linguistic expressions in order to achieve
contextually appropriate output.
5.5 Modeling with an Ontology
We use a full model in OWL as the knowledge rep-
resentation format in the dialogue manager, turn
planner and sentence planner. This model in-
cludes the entities, properties and relations of the
MP3 domain?including the player, data base and
playlists. Also, all possible tasks that the user may
perform are modeled explicitly. This task model
is user centered and not simply a model of the
application?s API.The OWL-based model is trans-
formed automatically to the internal format used
in the PATE rule-interpreter.
We use multiple inheritance to model different
views of concepts and the corresponding presen-
tation possibilities; e.g., a song is a browsable-
object as well as a media-object and thus allows
for very different presentations, depending on con-
text. Thereby PATE provides an efficient and ele-
gant way to create more generic presentation plan-
ning rules.
6 Experiments and Evaluation
So far we conducted two WOZ data collection
experiments and one evaluation experiment with
a baseline version of the SAMMIE system. The
SAMMIE-1 WOZ experiment involved only spo-
ken interaction, SAMMIE-2 was multimodal, with
speech and haptic input, and the subjects had
to perform a primary driving task using a Lane
Change simulator (Mattes, 2003) in a half of their
experiment session. The wizard was simulating
an MP3 player application with access to a large
database of information (but not actual music) of
more than 150,000 music albums (almost 1 mil-
lion songs). In order to collect data with a variety
of interaction strategies, we used multiple wizards
and gave them freedom to decide about their re-
sponse and its realization. In the multimodal setup
in SAMMIE-2, the wizards could also freely de-
cide between mono-modal and multimodal output.
(See (Kruijff-Korbayova? et al, 2005) for details.)
We have just completed a user evaluation to
explore the user-acceptance, usability, and per-
formance of the baseline implementation of the
4http://openccg.sourceforge.net
SAMMIE multimodal dialogue system. The users
were asked to perform tasks which tested the sys-
tem functionality. The evaluation analyzed the
user?s interaction with the baseline system and
combined objective measurements like task com-
pletion (89%) and subjective ratings from the test
subjects (80% positive).
Acknowledgments This work has been carried
out in the TALK project, funded by the EU 6th
Framework Program, project No. IST-507802.
References
[Alexandersson and Becker2001] J. Alexandersson and
T. Becker. 2001. Overlay as the basic operation for
discourse processing in a multimodal dialogue system. In
Proceedings of the 2nd IJCAI Workshop on Knowledge
and Reasoning in Practical Dialogue Systems, Seattle,
Washington, August.
[Baldridge and Kruijff2003] J.M. Baldridge and G.J.M. Krui-
jff. 2003. Multi-Modal Combinatory Categorial Gram-
mar. In Proceedings of the 10th Annual Meeting of the
European Chapter of the Association for Computational
Linguistics (EACL?03), Budapest, Hungary, April.
[Blaylock and Allen2005] N. Blaylock and J. Allen. 2005. A
collaborative problem-solving model of dialogue. In Laila
Dybkj?r and Wolfgang Minker, editors, Proceedings of
the 6th SIGdial Workshop on Discourse and Dialogue,
pages 200?211, Lisbon, September 2?3.
[Bunt et al2005] H. Bunt, M. Kipp, M. Maybury, and
W. Wahlster. 2005. Fusion and coordination for multi-
modal interactive information presentation: Roadmap, ar-
chitecture, tools, semantics. In O. Stock and M. Zanca-
naro, editors, Multimodal Intelligent Information Presen-
tation, volume 27 of Text, Speech and Language Technol-
ogy, pages 325?340. Kluwer Academic.
[Kruijff-Korbayova? et al2005] I. Kruijff-Korbayova?,
T. Becker, N. Blaylock, C. Gerstenberger, M. Kai?er,
P. Poller, J. Schehl, and V. Rieser. 2005. An experiment
setup for collecting data for adaptive output planning in
a multimodal dialogue system. In Proc. of ENLG, pages
191?196.
[Mattes2003] S. Mattes. 2003. The lane-change-task as a tool
for driver distraction evaluation. In Proc. of IGfA.
[Pfleger et al2003] N. Pfleger, J. Alexandersson, and
T. Becker. 2003. A robust and generic discourse model
for multimodal dialogue. In Proceedings of the 3rd
Workshop on Knowledge and Reasoning in Practical
Dialogue Systems, Acapulco.
[Pfleger2004] N. Pfleger. 2004. Context based multimodal
fusion. In ICMI ?04: Proceedings of the 6th interna-
tional conference on Multimodal interfaces, pages 265?
272, New York, NY, USA. ACM Press.
[Traum and Larsson2003] David R. Traum and Staffan Lars-
son. 2003. The information state approach to dialog man-
agement. In Current and New Directions in Discourse and
Dialog. Kluwer.
60
Linear order as higher-level decision:
Information Structure in strategic and tactical generation
Geert-Jan M. Kruijff,
Ivana Kruijff-Korbayova?
Computational Linguistics
University of the Saarland
Saarbru?cken, Germany
 
gj,korbay  @coli.uni-sb.de 
John Bateman
Applied Linguistics
University of Bremen
Bremen, Germany
 
bateman@uni-bremen.de 
Elke Teich
Applied Linguistics
University of the Saarland
Saarbru?cken, Germany
 
E.Teich@mx.uni-saarland.de 
Abstract
We propose a multilingual approach to
characterizing word order at the clause
level as a means to realize informa-
tion structure. We illustrate the prob-
lem with three languages which differ
in the degree of word order freedom
they exhibit: Czech, a free word or-
der language in which word order vari-
ation is pragmatically determined; En-
glish, a fixed word order language in
which word order is primarily gram-
matically determined; and German, a
language which is between Czech and
English on the scale of word order free-
dom. Our work is theoretically rooted
in previous work on information struc-
turing and word order in the Prague
School framework as well as on the
systemic-functional notion of Theme.
The approach we present has been im-
plemented in KPML.
1 Introduction
The aim of this paper is to describe an architecture
that addresses how information structure can be
integrated into strategic and tactical generation.
We focus primarily here on the tactical aspect of
how word order (henceforth: WO) may function
as a means of realizing information structure. The
approach we take is multilingually applicable. It
is implemented in KPML (Bateman, 1997b; Bate-
man, 1997a) and has been tested for Czech, Bul-
garian and Russian as three Slavonic languages
with different WO properties, as well as for En-
glish. The algorithm itself is not KPML-specific:
it combines the idea of WO constraints posed by
the grammar, with a complementary mechanism
of default ordering based on information struc-
ture. The algorithm could thus be applied in other
systems wich allow multiple sources of ordering
constraints.
Information structure is a means that a speaker
employs to indicate that some parts of a sen-
tence meaning are context-dependent (?given?),
and that others are context-affecting (?new?). In-
formation structure is therefore an inherent aspect
of sentence meaning, and it contributes in an im-
portant way to the overall coherence of a text.
While it is commonly accepted that information
structuring is a major source of constraints for the
organization of a given content in a particular lin-
ear order in many languages, there is very little
work in Natural Language Generation that explic-
itly models this relation.
From a practical perspective, in the most com-
monly employed generation systems such as
KPML, FUF (Elhadad, 1993; Elhadad and Robin,
1997) or REALPRO (Lavoie and Rambow, 1997),
linear ordering comes as a by-product of other
grammatical choices. This is fine for tactical
generation components and it is sufficient for
languages with grammatically determined WO
(?fixed? WO languages), such as English or Chi-
nese. However, most languages have some WO
variability and this variation usually reflects infor-
mation structure. When languages in which linear
order is primarily pragmatically determined are
involved, such as the Slavonic languages we have
dealt with, a number of problems become imme-
diately apparent.
A comprehensive account of WO variation for
natural language generation that is reusable across
languages is thus required. Such an account needs
to represent linearization as an explicit decision-
making process that involves both the representa-
tion of the language-specific linear ordering pos-
sibilites and the representation of the language-
specific (and possibly cross-linguistically valid)
motivations for particular linearizations. Again,
while the former is catered for in most tactical
generation systems, only selected aspects of the
latter have been dealt with and only for selected
languages (e.g., (Hoffman, 1994; Hoffman, 1995;
Hakkani et al, 1996)).
For example, (Hoffman, 1994) proposes a
treatment of WO in Turkish using a categorial
grammar framework (CCG, (Steedman, 2000))
and relating this to Steedman?s (earlier) account
of information structure (Steedman, 1991). How-
ever, the most important issue, that of providing
an integrated account of how information struc-
ture guides the choice of (or, is realized by) linear
ordering, is left unsolved (Kruijff, 2001).
Given that in many languages, information
structure is the major driving force for WO vari-
ation, it is indeed the most straighforward idea to
couple an account of information structure with
the choice of linear ordering. However, for mul-
tilingual application, the particular challenge is to
develop a solution that can be applied, no mat-
ter at which point on the free-to-fixed WO cline a
language is located.
The approach to WO proposed in this paper is a
move in exactly this direction. We start in  2 with
presenting data from Czech, German and English
that motivate the perspective we take on informa-
tion structure, and its role in generating coher-
ent discourse. In  3 we introduce the linguistic
notions employed in the present account. In  4
we discuss how information structure fits into a
general system architecture, and we discuss the
implementation of the strategic generation com-
ponent on the basis of KPML. We continue with
an elaboration of the role of information structure
in tactical generation, presenting an algorithm for
generating contextually appropriate linearization,
given a sentence?s information structure, and il-
lustrate its implementation on Czech and English
examples (  5). We conclude the paper with a
summary (  6).
2 Linguistic motivation
There are a number of factors commonly ac-
knowledged to play an important role in express-
ing a given content in a specific linear form.
The inventory of these factors contains at least
the following: information structure, syntactic
structure, intonation, rhythm and style. Cross-
linguistically, these factors may be involved in
constraining linear ordering to varying degrees.
English, for instance, is an example of a lan-
guage in which WO is rather rigid, i.e., strongly
constrained by syntactic structure. In such lan-
guages, differences in information structure are
often reflected by varying the intonation pat-
tern or by the choice of particular types of
grammatical constructions, such as clefting and
pseudo-clefting, or definiteness/indefiniteness of
the nominal group. Czech, in contrast, which has
a rich case system and no definite or indefinite
article, belongs to the so-called ?free word order?
languages, where the same effects are achieved by
varying WO. Finally, German lies between En-
glish and Czech in the spectrum between fixed
and free WO. We illustrate the general point that
WO selections are related to information structure
by appropriateness judgements of some examples
of instructions in Czech, German and English.1
(1) Otevr?eme
open-1PL
pr???kazem
command-INS
Open
Open
soubor.
file-ACC
Sie
You
o?ffnen
open
eine
a
Datei
file
mit
with
dem
the
Befehl
command
Open.
Open.
Open a file with the Open command.
The ordering in (1) is neutral in that no partic-
ular contextual constraints hold with respect to
the newsworthiness of any of the elements ex-
pressed in this clause. This kind of ordering can
1The English examples use imperative mood, while the
Czech and the German examples use indicative mood as
the most common way of conveying instructions of the dis-
cussed type. Alternatively, both Czech and German can use
also imperatives or infinitives for instructions, but these are
considered less polite than the indicative versions. Last but
not least, instructions can also be formulated in indicative
mood with passive voice in both Czech and German.
be elicited by the question What should we do?.2
We follow Prague School accounts (Firbas, 1992;
Sgall et al, 1986) in calling this neutral ordering
the systemic ordering (cf. also  5). Alternatively,
(1) could be used in a context characterized by
the question What should we open by the Open
command?, when the Open command is not be-
ing contrasted with some other entity.
(2) Otevr?eme
open-1PL
soubor
file-ACC
pr???kazem
command-INS
Open.
Open
Sie
you
o?ffnen
open
die
the
Datei
file
mit
with
dem
the
Befehl
command
Open.
Open.
?Open the file with the Open command.?
(3) Soubor
file-ACC
otevr?eme
open-1PL
pr???kazem
command-INS
Open.
Open
Die
the
Datei
file
o?ffnen
open
Sie
you
mit
with
dem
the
Befehl
command
Open.
Open.
?Open the file with the Open command.?
The word order variants illustrated in (2) and (3)
are appropriate when some file is active in the
context (Chafe, 1976), for instance when the user
is working with a file. In (2), the action of open-
ing is also active; in (3) it can, but does not have
to be active, too. The contexts in which (2) and
(3) can be appropriately used can be character-
ized by the questions What should we do with the
file? or How should we open the file?. Unlike
(2), example (3) can be used if file is contrasted
with another entity. In German, this contrast is
required, whereas in Czech it is optional. In En-
glish, intonation could mark whether contrast is
required.
(4) Pr???kazem
command-INS
Open
Open
otevr?eme
open-1PL
soubor.
file-ACC
Mit
with
dem
the
Befehl
command
Open
Open
o?ffnen
open
Sie
you
eine
a
Datei.
file.
With the Open command, open a file.
(5) Pr???kazem
command-INS
Open
Open
soubor
fileACC
otevr?eme.
open-1PL
Mit
with
dem
the
Befehl
command
Open
Open
o?ffnen
open
Sie
you
die
a
Datei.
file.
With the Open command, open the file.
2We use questions for presentational purposes to indicate
which contexts would be appropriate for uttering sentences
with particular WO variants. Such question-answer pairs are
known as question tests (Sgall et al, 1986).
The contexts in which (4) can be used are char-
acterized by What should we do with the Open
command?. While (4) does not refer to a spe-
cific file, in (5) an activated file is presumed. (5)
is appropriate in contexts characterized by What
should we do to the file with the Open command?.
It is also possible to use (4) in a context charac-
terized by What should we do?, and (5) in a con-
text characterized by What should we do to the
file?, if it is presumed that we are talking about
using various commands (or various means or in-
struments) to do various things. In the latter type
of context, the Open command does not have to
be activated.
(6) Soubor
file-ACC
pr???kazem
command-INS
Open
Open
otevr?ete.
open-I2PL
Die
the
Datei
file
o?ffnen
open
Sie
you
mit
with
dem
the
Befehl
command
Open.
Open
Open the file with the Open command.
Example (6) is like (5) in that it is appropriate
when both a file and the Open command are acti-
vated. The contexts in which (6) can be appropri-
ately used can be characterized by What should
we do to the file with the Open command?. Un-
like (5), (6) can also be used when file is con-
trasted with another entity. In German, there is
no difference in word order between (6) and (3)
(they differ only in intonation). This is a result of
the strong ordering constraint in German to place
the finite verb as second (in independent, declara-
tive clauses). In Czech verb secondness also plays
a role, but it is much weaker.
Analogous judgements concerning contextual
appropriateness apply to WO variants in differ-
ent mood and/or voice (when available in the in-
dividual languages). The orders in which the verb
is first do not presume the activation of either a
file or a command. The orders in which ?file?
precedes the verb appear to presume an active
file, the orders in which ?command? precedes the
verb appear to presume the activation of a com-
mand. When both ?file? and ?command? precede
the verb, the activation of both a file and a com-
mand appears to be presumed.
These judgements show that differences in WO
(in languages with a more flexible WO then En-
glish, e.g., Czech and German) very often corre-
spond to differences in how the speaker presents
the information status of the entities and pro-
cesses that are referred to in a text, in particu-
lar, whether they are assumed to be already fa-
miliar or not, and whether they are assumed to
be activated in the context. Note that in English,
the same distinction is expressed by the use of a
definite vs. an indefinite nominal expression, i.e.
?a  the file?.
To summarize: Since sentences which differ
only in WO (and not in the syntactic realizations
of clause elements) are not freely interchangable
in a given context, we have to be able to gen-
erate contextually appropriate WOs. In order to
achieve this, we need to be able to capture not
only the structural restrictions specific to individ-
ual languages, but also the restrictions reflecting
the information status of the entities (and pro-
cesses) being referred to.
3 Underlying notions
In order to provide constraints for WO decisions
within our generation architecture, we require
mechanisms through which particular patterns of
information structuring can constrain the choice
among the WO variants available. These patterns
are provided by our text planning component. We
have found two complementary approaches to the
relationship between aspects of information struc-
turing and WO to be ripe for application in the
generation of extended texts; these approaches are
briefly introduced below.
In order to clarify the complementary nature of
the approaches that we have adopted, it is neces-
sary first to distinguish between two dimensions
of organization that are often confused or whose
difference is contested: in his Systemic Func-
tional Grammar (SFG), (Halliday, 1970; Hall-
iday, 1985) distinguishes between the thematic
structure of a clause and its information struc-
ture: Whereas the Theme is ?the starting point
for the message, it is the ground from which the
clause is taking off? (Halliday, 1985, 38), infor-
mation structure concerns the distinction between
the Given as ?what is presented as being already
known to the listener? (Halliday, 1985, 59), and
the New as ?what the listener is being invited to
attend to as new, or unexpected, or important?
(ibid).
3.1 Information structure and ordering
In Halliday?s original approach (Halliday, 1967),
the basic assumption for English and also for
other languages is that ordering, apart from being
grammatically constrained, is iconic with respect
to ?newsworthiness?. So on a scale from Given
to New information, the ?newer? elements would
come towards the end of the information unit, the
?newest? element bearing the nuclear stress. This
approach relies on the possibility of giving a com-
plete ordering of all clause elements with respect
to their newsworthiness.
The notion of ordering by newsworthiness in
Halliday?s approach is parallel to the notion of
communicative dynamism (CD) introduced in the
early works of Firbas (for a recent formulation
see (Firbas, 1992)) and used also within the Func-
tional Generative Description (FGD, (Sgall et al,
1986)). Also from the viewpoint of CD, the pro-
totypical ordering of clause elements from left
to right respects newsworthiness: In prototypical
cases, WO corresponds to CD. However, textu-
ally motivated thematization or grammatical con-
straints may force WO to diverge from CD.
The FGD approach differs from Halliday?s in
that, in addition to CD, it works with a de-
fault (canonical) ordering, called systemic order-
ing (SO). SO is the language specific canonical
ordering of clause elements (complements and
adjuncts), as well as of elements of lower syntac-
tic levels, with respect to one another.
For the current purposes we concentrate on the
SO for a subset of the clause elements that are dis-
cerned in FGD. We use the following SOs for the
Slavonic languages and for English and German:3
SO for Czech, Russian, Bulgarian:
Actor  TemporalLocative  Purpose  Space-
Locative  Means  Addressee  Patient 
Source  Destination
SO for English: Actor  Addressee  Pa-
tient  SpaceLocative  TemporalLocative 
Means  Source  Destination  Purpose-
dependent
SO for German: Actor  TemporalLocative
 SpaceLocative  Means  Addressee  Pa-
tient  Source  Destination  Purpose
3The labels we use for the various types of elements are
a mixture of FGD and SFG terminology.
The SO for the Slavonic languages is based on
the one for Czech (Sgall et al, 1986); the only
difference is that we have placed Patient before
Source (?from where?). We follow (Sgall et al,
1986) in considering the SOs for the main types
of complementations in Russian and Bulgarian to
be similar to the Czech one, though there can be
slight differences (cf. the observations reported in
(Adonova et al 1999)). The SO for English com-
bines the suggestions made by (Sgall et al, 1986)
and the ordering defaults of the NIGEL grammar
of English (cf. Section 5.2). The SO for German
is based on (Heidolph et al, 1981, p.704).
The informational status of elements is estab-
lished through deviation of CD from the SO. This
leads us to the distinction FGD makes between
contextually bound (CB) and contextually non-
bound (NB) items in a sentence (Sgall et al,
1986). A CB item is assumed to convey some
content that bears on the preceding discourse con-
text. It may refer to an entity already explic-
itly referred to in the discourse, or an ?implicitly
evoked? entity. At each level of syntactic struc-
ture, CB items are ranked lower than NB items in
the CD ordering. The motivation behind and the
meaning of the CB/NB distinction in FGD cor-
responds to those underlying the Given/New di-
chotomy in SFG.
Contextual boundness can be used to constrain
WO (at the clause level) as follows:
 The CB elements (if there are any) typically
precede the NB elements.
 The mutual ordering of multiple CB items in
a clause corresponds to communicative dy-
namism, and the mutual ordering of mul-
tiple NB items in a clause follows the SO
(with the exceptions required by grammati-
cally constrained ordering as described be-
low). The default for communicative dy-
namism is SO.
 The main verb of a clause is ordered at the
boundary between the CB elements and the
NB elements, unless the grammar specifies
otherwise (verb secondness).
It is the above abstract ordering principles that
underly the algorithm we present in  5.
3.2 Thematic structure
In all languages we looked at so far, there are also
orders we cannot explain solely on the basis of
the CB/NB distinction along with SO and gram-
matical constraints. On the one hand, it has been
claimed that the ordering of CB elements follows
CD rather than SO, and that CD is determined
by contextual factors (Sgall et al, 1986). On the
other hand, cases where an NB element appears at
the beginning of a clause are far from rare. While
we currently do not have more to add to the for-
mer issue, the latter can be readily addressed us-
ing the notion of Theme. For illustration, consider
(8) in Czech, German and English, appearing in a
context where it is preceded only by (7).
(7) First open the Multiline styles dialog box using one
of the following methods.
(8) Z
From Data
menu
menu
Data
choose 	

vybereme
Style.
Style.
Im
In
Menu?
menu
Data
Data
wa?hlen
choose
Sie
you
Style.
Style.
In the Data menu, choose Style.
The preceding context does not refer to the ?Data
menu? or make it active in any way. Working
only with the notion of information structure dis-
cerning CB (Given) and NB (New) elements, one
is thus unable to explain this ordering. On the
other hand, the notion of thematic structure as
a reflection of a global text organization strategy
makes such explanation possible. In Halliday?s
approach, Theme has a particular textual function,
that of signposting the intended development or
?scaffolding? that a writer employs for structuring
an extended text. In software instruction manuals,
for example, we encounter regular thematization
of (i) the location where actions are performed,
(ii) the particular action that the user is instructed
to perform, or (iii) the goal that the user wants to
achieve (cf. (Kruijff-Korbayova? et al, in prep) for
a more detailed discussion).
4 Information structure and strategic
planning
In this section we briefly describe how we in-
tegrate information structure into strategic gen-
eration, i.e. text- and sentence-planning. The
Figure 1: A text plan. In our system, a text plan organizes content into a linear fashion, showing
where (and how) content might be aggregated syntactically (e.g. conjunction) or discursively (e.g.
RST-relations). In the example above, the text plan specifies a text consisting of an overall goal (the
title) and five substeps to resolve that goal (the tasks). The first task is a simple one, the second task
is a complex formed around an RST-purpose relation, after which follows a conjunction of tasks. (The
CONJOINED-INSTRUCTION-TASKS nodes indicate that the left-daughter node (a task) and the task
dominated by the immediate non-terminal node above a CONJOINED-INSTRUCTION-TASKS node,
are to be related by a conjunction.) The content to be realized is identified by the leaves of the text
plan. Whenever a leaf is introduced in the text plan, the discourse model is updated with the content?s
(A-box) concepts. The sentence planner decends through the text plan depth-first. Thereby it gathers
the leaves? content into sentence-specifications, following any indications of aggregation. It makes use
of the discourse model to specify whether content should be realized as contextually bound (or not).
principle idea is that during text-planning, a dis-
course model is built that is then used in sentence-
planning to determine a sentence?s information
structure.
We have developed a system using KPML. In
KPML, generation resources are divided into in-
teracting modules called regions. For the purpose
of text-planning we have constructed a region that
defines an additional level of linguistic resources
for the level of genre. The region facilitates the
composition of text structures in a way that is very
similar to the way the lexico-grammar builds up
grammatical structures. This enables us to have a
close interaction between global level text gener-
ation and lexico-grammatical expression, with the
possibility to accommodate and propagate con-
straints on output realization. While constructing
a text plan, the text planner constructs a (rudimen-
tary) discourse model that keeps track of the dis-
course entities introduced.
Text planning results in a text plan and a dis-
course model that serve as input to the sentence
planner. The text plan is a hierarchical structure,
organizing the content into a more linear fashion
(see Figure 3.2). The sentence planner creates
the input to the tactical generation phase as for-
mulas of the Sentence planning Language (SPL,
(Kasper, 1989)). The SPL formulas express the
bits of content identified by the text plan?s leaves,
and can also group one or more leaves together
(aggregation) depending on decisions taken by
the text planner concerning discourse relations.
Most importantly, during this phase of planning
what content is to be realized by a sentence, the
underlying information structure of that content
is determined: Whenever the sentence planner
encounters a piece of content that the discourse
model notes as previously used, it marks the cor-
responding item in the SPL formula as contextu-
ally bound (note that we are hereby making a sim-
plifying assumption that in the current version of
the sentence planner we equate contextual bound-
ness with previous mention).
The text planner can also choose a particular
textual organization and determine the element
which should become the Theme. If no particu-
lar element is chosen as the Theme, the grammar
chooses some element as the default Theme. This
can be the Subject (as in English), the least com-
municatively dynamic element (as in Czech); the
choice of the default Theme in German is freer
than in English, but more restricted than in Czech
(cf. (Steiner and Ramm, 1995) for a discussion).
The Theme is then placed at the beginning of the
clause, although not necessarily at the very first
position, as this might be occupied, e.g., by a con-
nective. The placement of the Theme is also re-
solved by the grammar.
5 Realizing information structure
through linearization
It is in the setting described in  4 that the issue of
generating contextually appropriate sentences re-
ally arises. In this section we describe the word
ordering algorithm (  5.1) and its application to
Czech and English (  5.2).
5.1 Flexible word order algorithm
As discussed, constraints from various sources
need to be combined in order to determine gram-
matically well-formed and contextually appropri-
ate WO. Contextual boundness is used to con-
strain WO at the clause level as specified above.
We combine the following two phases in which
information structure (CB/NB) is taken into ac-
count during tactical generation:
 information structure can determine partic-
ular realization choices made in the gram-
mar; for example, when inserting and plac-
ing the particle of a phrasal verb, when in-
serting and ordering the Source and Destina-
tion for a motion process;
 information structure can determine the or-
dering of elements whose placement has not
been sufficiently constrained by the gram-
mar.
For a multilingual resource, this allows each
language to establish its own balance between the
two phases. To show our approach in a nutshell,
we present an abstract WO algorithm in Figure 2.
Given:
a set GC of ordering constraints
imposed by the grammar
a list L1 of constituents
that are to be ordered,
a list D giving ordering of CB
constituents (default is SO)
Create two lists LC and LN of de-
fault orders:
Create empty lists LC (for CB items)
and LN (for NB items)
Repeat for each element E in L1
if E is CB,
then add E into LC,
else add E into LN.
Order all elements in LC
according to D
Order all elements in LN
according to SO
if the Verb is yet unordered then
Order the Verb at
the beginning of LN
Order the elements of L1
if GC is not empty then
use the contraints in GC, and
if the contraints in GC are
insufficient,
apply first the default
orders in LC and then those in LN
Figure 2: Abstract ordering algorithm
The ordering constraints posed by the gram-
mar have the highest priority. Note that this in-
cludes the ordering of the textually determined
Theme. Then, elements which are not ordered by
the grammar are subject to the ordering according
to information structure, i.e. systemic ordering in
combination with the CB/NB distinction. The or-
dering of the NB elements (i) is restricted by the
syntactic structure or (ii) follows SO. The order-
ing of the CB elements can be (i) specified on the
basis of the context, (ii) restricted by the syntactic
structure, or (iii) follow SO.
The ordering algorithm as such is not language
specific, and could be usefully applied in the gen-
eration of any language. What differs across lan-
guages is first of all the extent to which the gram-
mar of a particular language constrains ordering,
i.e. which elements are subject to ordering re-
quirements posed by the syntactic structure, and
which elements can be ordered according to infor-
mation structure. Also, it is desirable (and our al-
gorithm allows it) to specify different systemic or-
derings for different languages. And, even within
a single language, our algorithm allows the spec-
ification of different systemic orderings in differ-
ent grammatical contexts (just by adding a real-
ization statement that (partially) defines the SO
during strategic generation).
The algorithm is applicable in platforms other
than KPML. In the first place, any grammar
can modify its decisions to take information
structure into account. In addition, those tacti-
cal generators allows multiple sources of order-
ing constraints, e.g., a combination of grammar-
determined choices and defaults, as long as such
that the default ordering based on information
structure can be applied.
5.2 Algorithm application
The algorithm described above has been imple-
mented and used for generation of Czech and En-
glish instructional texts. The Czech grammar re-
sources used in tactical generation have been built
up along with Bulgarian and Russian grammar
resources as described in (Kruijff et al, 2000),
reusing the NIGEL grammar for English. The
original NIGEL grammar itself already combines
the specification of ordering constraints in the
grammar with the application of defaults. If an or-
dering is underspecified by the grammar, the de-
faults are applied. The defaults are ?static?, i.e.
specified once and for all. The algorithm we have
described replaces these ?static? defaults with a
?dynamic? construction of ordering constraints.
Two separate sets of ?dynamic? defaults are com-
puted on the basis of the SO for the CB and the
NB elements in each sentence/clause.
We use the SOs for Czech and English
specified above (cf.  3.1). For each ele-
ment in the input SPL we specify whether it
is CB (:contextual-boundness yes) or
NB (:contextual-boundness no); in ad-
dition, we can specify the textual Theme in the
SPL (theme <id>). The SPL in Figure 3 illus-
trates this.
Note that the information structure distinction
between CB vs. NB elements on the one hand,
and the informational status of referents as iden-
tifiable vs. non-identifiable on the other hand, are
orthogonal. Whereas CB/NB has to do with the
(R / RST-purpose
:speechact assertion
:DOMAIN (ch/DM::choose
:actor (a1/DM::user
:identifiability-q identifiable
:contextual-boundness yes)
:actee (a2/object :name gui-open
:identifiability-q identifiable
:contextual-boundness no)
:instrumental (mea/DM::mouse
:identifiability-q identifiable
:contextual-boundness no)
:spatial-locating (loc/DM::menu
:identifiability-q identifiable
:contextual-boundness yes
:class-ascription (label/object
:name gui-file))
:RANGE (open/DM::open
:contextual-boundness no
:actee (f/DM::file
:contextual-boundness no)))
:theme open)
Generated output:
Pro
for
otevr?en??
opening-GEN
souboru
file-GEN
uz?ivatel
user-NOM
v
in
menu
menu-LOC
vybere
choose-3SG
mys???
mouse-INS
Open.
Open
To open a file, the user chooses Open in the menu with the
mouse.
Figure 3: Sample input SPL for English and
Czech and generated outputs
speaker?s presenting an element as either bearing
on the context or context-affecting, identifiability
reflects whether the speaker assumes the hearer
to pick out the intended referent. These two di-
mensions are independent, though correlated (cf.
the discussion of activation vs. identifiability in
(Lambrecht, 1994)). What is encountered most
often is the correlation of CB with identifiable
and NB with non-identifiable. The correlation of
NB with identifiable corresponds is found, e.g., in
cases of ?reintroducing? an element talked about
before, or in cases like There is a square and a
circle. Delete the circle. ?in the second sentence,
the same ordering would be used also in German
(Lo?schen Sie den Kreis) and in Czech (Vymaz?te
kruh.).
What is hard to find is the correlation of CB
with non-identifiable, but it is the way we would
analyze a dollar bill in example (9) (Gregory
Ward, p.c.)4
(9) (What do you do if you see money laying on the
ground?)
Dolarovou
Dollar
bankovku
note
bych
would 	
zvedla.
pick-up 	
Eine
a
Dollarnote
dollarnote
wu?rde
would
ich
I
aufheben.
pick-up
A dollar bill I would pick up.
The CB/NB assignments can be varied to ob-
tain different WO variants. The examples below
show some of the CB/NB assignment combina-
tions and the outputs generated using the Czech
and English grammars.
(10) user
Actor-NB
Uz?ivatel
choose
(Finite-Verb)
vybere
Open
Purpose-NB
pro
menu
SpaceLoc.-NB
otevr?en??
mouse
Means-NB
souboru
open file
Patient-NB
v
The user chooses Open in the menu with the mouse
to open a file.
(11) user
Actor-CB
Uz?ivatel
choose
v
Open
SpaceLoc.-CB
menu
menu
(Finite-Verb)
vybere
mouse
Purpose-NB
pro
open file
Means-NB
otevr?en??
Patient-NB
souboru mys???
The user chooses Open in the menu with the mouse
to open a file.
(12) user
Purpose-CB
Pro
choose
otevr?en??
Open
Actor-CB
souboru
menu
SpaceLoc.-CB
uz?ivatel
mouse
Means-CB
v
open file
(Finite-Verb)
menu
Patient-NB
mys??? vybere
To open a file the user chooses Open in the menu
with the mouse.
As mentioned above, we preserve the notion of
textual Theme. An SPL can contain a specifica-
tion of a Theme, and the corresponding element
is then ordered at the front of the sentence, as de-
termined by the grammar. The WO of the rest of
the sentence is determined as described.
4Regarding intonation: in English, there are two into-
nation phrases, the first containing dollar bill with a L+H*
pitch accent on dollar, and the second with a H* pitch accent
on pick up. In Czech and German it seems that a contrastive
pitch accent on dolarovou bankovku is optional, and the rest
can have neutral intonation with nuclear stress on the last
word.
6 Summary and conclusions
We have presented a flexible word ordering al-
gorithm for natural language generation. The
novel contribution consists in offering one way
of implementing information structure as the ma-
jor source of constraints on word order varia-
tion for languages with pragmatically-determined
word order. Apart from that, the special feature of
the word order algorithm proposed is that it can
also be applied to languages with grammatically-
determined word order. We have illustrated the
application of the algorithm for Czech and En-
glish, Czech being a language in which word or-
der is primarily pragmatically determined and En-
glish being a grammatically-determined word or-
der language. We have thus provided evidence
that the algorithm can flexibly be applied to ?free?
word order languages as well as ?fixed? word or-
der languages.
From a linguistic theoretical point of view, the
most important precondition for achieving this
has been to take seriously the linguistic observa-
tion that in many languages information structure
is the driving force for word order variation. For
the modeling of information structure for strate-
gic generation, we have drawn upon two well es-
tablished linguistic frameworks, in both of which
the discourse-linguistic and pragmatic constraints
on grammatical realization are a focal interest, the
Prague School and Systemic Functional Linguis-
tics. From a technical point of view, we have
based the implementation on the KPML system,
integrating the proposed word order algorithm
with existing multilingual grammatical resources
and re-using KPML?s mechanisms for word or-
der realization as well as its systemic-functionally
based notion of Theme. The algorithm is not
KPML-specific, though, and could be applied in
other frameworks as well, especially if they allow
the combination of linearization constraints com-
ing from different sources.
Acknowledgements
The work presented here folows up on our
earlier work carried out partially within AG-
ILE (Automatic Generation of Instructions
in Languages of Eastern Europe), a project
funded by the European Community within
the INCO-COPERNICUS programme (grant
No. PL96114). We would also like to thank
the anonymous reviewers of this workshop for
valuable comments.
References
John A. Bateman. 1997a. Enabling technology for multilin-
gual natural language generation: The kpml development
environment. Natural Language Engineering, 3:15 ? 55.
John A. Bateman. 1997b. KPML Development Environ-
ment: multilingual linguistic resource development and
sentence generation. Darmstadt, Germany, March. (Re-
lease 1.0).
Wallae Chafe. 1976. Givenness, contrastiveness, definite-
ness, subjects, topics and point of view. Subject and
Topic. Charles Li (ed.). New York: Academic Press. p.
25 ? 56.
Michael Elhadad and Jacques Robin. 1997. Surge: A com-
prehensive plug-in syntactic realisation component for
text generation. Technical report, Department of Com-
puter Science, Ben Gurion University, Beer Shava, Israel.
Michael Elhadad. 1993. Fuf: The universal unifier user
manual 5.2. Technical report, Department of Computer
Science, Ben Gurion University, Beer Shava, Israel.
Jan Firbas. 1992. Functional Sentence Perspective in Writ-
ten and Spoken Communication. Studies in English Lan-
guage. Cambridge University Press, Cambridge.
Dilek Zeynep Hakkani, Kemal Oflazer, and Ilyas Cicekli.
1996. Tactical generation in a free constituent order lan-
guage. In Proceedings of the International Workshop on
Natural Language Generation, Herstmonceux, Sussex,
UK.
Michael A. K. Halliday. 1967. Notes on transitivity and
theme in English ? parts 1 and 2. Journal of Linguistics,
3(1 and 2):37?81 and 199?244.
Michael A.K. Halliday. 1970. A Course in Spoken English:
Intonation. Oxford Uniersity Press, Oxford.
Michael A.K. Halliday. 1985. Introduction to Functional
Grammar. Edward Arnold, London, U.K.
K. Heidolph, W. Fla?mig, and W. Motsch. 1981. Grundzu?ge
einer deutschen Grammatik. Akademie-Verlag.
Beryl Hoffman. 1994. Generating context-appropriate
word orders in turkish. In Proceedings of the Internati-
nal Workshop on Natural Language Generation, Kenneb-
unkport, Maine.
Beryl Hoffman. 1995. Integrating ?free? word order syntax
and information structure. In Proceedings of the Euro-
pean Chapter of the Association for computational Lin-
guistics (EACL), Dublin, Ireland.
Robert T. Kasper. 1989. A flexible interface for linking
applications to PENMAN?s sentence generator. In Pro-
ceedings of the DARPA Workshop on Speech and Natural
Language.
Geert-Jan M. Kruijff. 2001. A Categorial-Modal Logi-
cal Architecture of Informativity: Dependency Grammar
Logic & Information Structure. Ph.D. thesis, Faculty
of Mathematics and Physics, Charles University, Prague,
Czech Republic, April.
Geert-Jan M. Kruijff, Elke Teich, John Bateman, Ivana
Kruijff-Korbayova?, Hana Skoumalova?, Serge Sharoff,
Lena Sokolova, Tony Hartley, Kamy Staykova and Jir???
Hana. 2000. Multilingual generation for three slavic lan-
guages. In Proceedings COLING 2000.
Ivana Kruijff-Korbayova?, John Bateman, and Geert-Jan M.
Kruijff. in prep. Generation of contextually appropriate
word order. In Kees van Deemter and Rodger Kibble,
editors, Information Sharing, Lecture Notes. CSLI.
Knud Lambrecht. 1994. Information Structure and Sen-
tence Form. Cambridge Studies in Linguistics. Cam-
bridge University Press.
Benoit Lavoie and Owen Rambow. 1997. A fast and
portable realizer for text generation. In Proceedings of
the Fifth Conference on Applied Natural Language Pro-
cessing (ANLP), Washington DC.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Semantic and Prag-
matic Aspects. D. Reidel Publishing Company, Dor-
drecht, Boston, London.
Mark J. Steedman. 1991. Structure and intonation. Lan-
guage, 68:260 ? 296.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge Massachusetts.
Erich Steiner and Wiebke Ramm. 1995. On Theme as a
grammatical notion for German. Functions of Language,
2(1):57?93.
Conditional Responses in Information-Seeking Dialogues
Elena Karagjosova and Ivana Kruijff-Korbayova?
Computational Linguistics, Saarland University, Saarbru?cken, Germany
{elka,korbay}@coli.uni-sb.de
Abstract
The paper deals with conditional responses of the
form ?Not if c/Yes if c? in reply to a question ??q?
in the context of information-seeking dialogues. A
conditional response is triggered if the obtainability
of q depends on whether c holds: The response in-
dicates a possible need to find alternative solutions,
opening a negotiation in the dialogue. The paper
discusses the conditions under which conditional
responses are appropriate, and proposes a uniform
approach to their generation and interpretation.
1 Introduction
The goal of this paper is to provide a basic account
of conditional yes/no responses (CRs): We describe
the conditions under which CRs are appropriate, and
how these conditions translate into a uniform ap-
proach to understanding and producing CRs.1
We focus on information-seeking dialogues be-
tween a human user and a dialogue system in the
travel domain. We allow for mixed initiative and ne-
gotiation to let a dialogue be more collaborative than
?quizzing?. In this context CRs arise naturally (1).
(1) U.1: Do I need a visa to enter the U.S.?
S.1: Not if you are an EU citizen.
(2) S.1?: Yes, if you are not an EU citizen.
(1:S.1) is an example of a negative CR, asserting If
you?re an EU citizen, then you do not need a visa to
enter the U.S. An alternative, positive CR is (2:S.1?),
asserting If you?re not an EU citizen, then you do
need a visa to enter the U.S..
In both cases, the system answers the question
(1:U.1), but it makes the answer conditional on the
value of a particular attribute (here, citizenship).
1This work was done in SIRIDUS (Specification, Interac-
tion and Reconfiguration in Dialogue Understanding Systems),
EC Project IST-1999-10516. We would like to thank Geert-Jan
Kruijff for detailed discussion and comments .
Moreover, the CR suggests that, for another value,
the answer may be different (2).
The CRs in (1:S.1) and (2:S.1?) are elliptical utter-
ances. Intuitively, they can be expanded to the com-
plete propositions in (3) and (3?). The material for
resolving the ellipsis comes from the immediately
preceding context. In the approach we work with,
ellipsis is resolved with respect to the current ques-
tion under discussion (QUD, (Ginzburg, 1996)).
(3) No, you don?t need a visa to enter the U.S. if you are an
EU citizen.
(3?) Yes, you do need a visa to enter the U.S. if you are not
an EU citizen.
The dialogue move of a CR depends on the con-
text. Consider (4) and (5). Similarly to (1), in (4)
the system does not know an attribute-value (A/V)
on which the positive or the negative answer to the
yes/no question is contingent (here, whether the user
wants a business or economy class flight).2
(4) U.1: A flight from Ko?ln to Paris on Sunday.
S.1: I?m sorry, there are no flights from Ko?ln to Paris
on Sunday.
U.2: Can I fly on Monday?
S.2: Not if you want business class.
S.2?: Yes, if you want economy class.
(5) U.1: I want a business class flight from Ko?ln to Paris
on Sunday.
S.1: I?m sorry, there are no business flights from Ko?ln
to Paris on Sunday.
U.2: Can I fly on Monday?
S.2: Not if you want business class.
S.2?: Yes, if you want economy class.
The system?s CR (4:S.2) is a request for further in-
formation: whether the user wants a business flight
(Monday is out), or does not (she is able to fly on
Monday). Likewise, (4:S.2?) is a request for fur-
ther information whether the user wants an economy
flight (Monday is available), or not (Monday is out).
Dialogue (5) is different. Now the user indi-
cates that she is interested in a business class flight
2We realize that intonation might play a role. However,
given space restrictions we cannot address this issue here.
       Philadelphia, July 2002, pp. 84-87.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
(5:U.1). The system by default assumes that this re-
mains unchanged for another day of travel.
What both the negative and positive CR in (5)
do is to start a negotiation to either confirm or re-
vise the user?s decision for business class. The sys-
tem?s response (5:S.2) or (5:S.2?) indirectly proposes
a change (to economy class) to achieve the higher-
level goal of finding a flight from Ko?ln to Paris on
Monday. If the user insists on business class, this
goal cannot be achieved.
If we want a dialogue system to understand and
appropriately produce CRs, we need to describe
their semantics in terms of the contextual condi-
tions and communicative goals under which these
responses occur, and the effects they have on the di-
alogue context. We aim at providing the basis of
an account that can be implementated in the GoDiS
dialogue system. GoDis is an experimental sys-
tem in the travel domain, using the information-state
approach to dialogue developed the TRINDI and
SIRIDUS projects (Cooper et al, 1999; Lewin et al,
2000). We focus on aspects that can improve its flex-
ibility and functionality.
Overview. In ?2 we discuss the uses of positive
and negative CRs in terms of their appropriateness
conditions and their interpretation. In ?3 we discuss
dialogue moves. We end the paper with conclusions.
2 Uses of conditional responses
In this section we present two different types of CRs
and discuss in what contexts they are appropriate.
CRs can be used as answers to yes/no-questions.3
A CR does not provide a yes/no answer simpliciter,
though: It provides an answer that is contingent on
the value of some attribute. Consider (1). The sys-
tem?s reply (1:S.1) provides an answer that is con-
tingent on the value of the citizenship attribute. If
the value is (or implies) EU citizenship, the answer
is negative: If the user is an EU citizen, she does not
need a visa to enter the U.S.
The CR in (1) also seems to suggest the contra-
positive that if the value is ?non-EU-citizen?, the
answer is positive. (2) illustrates the opposite case.
We consider this additional suggestion an implica-
ture. The assertions and implicatures that arise from
3Corpora show also occurrences of CRs in response to state-
ments, cf. (Karagjosova and Kruijff-Korbayova?, 2002).
CRs are summarized in Figure 1.
Green & Carberry (1999) characterize CRs in
terms of the speaker?s motivation to provide infor-
mation ?about conditions that could affect the ve-
racity of the response?. However, they only consider
cases like (4) in which the A/V on which the CR is
contingent has not yet been determined in the pre-
ceding context (or cannot be assumed). Cases like
(5) where the A/V has been determined are left un-
noticed.4 We discuss each of the cases below.
Not-determined A/V. The A/V on which a CR is
contingent can be one that has not yet been deter-
mined in the preceding context, as in (1) and (4).
We call this type of CR a non-determined A/V CR
(NDCR). Besides the assertion and the implicature
that answer the yes/no question as specified in Fig-
ure 1, the NDCR amounts to indirectly giving rise to
the question ?whether c holds?.
Consider the user?s utterances in (6) as continu-
ations of (1). They show that the implicitly raised
question cannot be answered just by ?yes? or ?no?.
Rather, it requires some content that matches with c.
(6) U.2: Yes. | No.
U.2?: Yes, I am. | No, I am not.
U.2??: Yes, I have German citizenship. |
No, I have Czech citizenship.
The responses in (6:U.2) could be interpreted as
acknowledgments, but certainly not as answers to
whether the user is an EU citizen. This is corrobo-
rated by the following continuation of (6:U2?) where
the system does answer the pending question.
(7) S.2: Then you do (not).
S.2?: Then you do (not) need a visa.
(7:S.2) is elliptical for (7:S.2?). Correct resolution of
the ellipsis is possible only if the question whether
the user needs a visa is the topmost QUD.
The need to answer the implicitly raised ques-
tion depends on what goals the participants try to
achieve. ?Do I need a visa?? in (1) is satisfactorily
answered with either a yes or a no, or when enough
information is provided so the asker can find out the
answer herself. On the other hand, consider (8).
(8) U.1 Can I fly to Paris tomorrow?
S.1 Not if you want to fly economy class.
4Both cases are attested in corpora (Karagjosova and
Kruijff-Korbayova?, 2002).
Y/N-Question ?q ?q
Response Not if c Yes if c
Assertion If c, not-q If c, then q
Implicature Possibly, if not-c, then q Possibly, if not-c, then not-q
Figure 1: Patterns of conditional responses
In (8) the response is contingent on whether the
user wants to fly economy class. Before flight se-
lection can proceed further, the question whether
c holds must be answered. In order to satisfy its
goal of finding a flight which satisfies the user re-
quirements, the system does need to know whether
c holds to find out whether q holds. This is a differ-
ence between (8) and (1). In (1), the system?s goal
is merely to answer the user?s question.
To summarize, the interpretation of a CR in re-
sponse to a question whether q in a context where c
has not been established is that (i) it is still not de-
termined whether q, because (ii) the answer (speci-
fied in Figure 1) is contingent on c, and thus (iii) the
question whether c holds is implicitly raised.
As for production, it is appropriate for the system
to produce a NDCR when (i) answering a yes/no-
question whether q, where (ii) the answer is either q
or not-q, depending on some additional A/V c which
has not yet been established in the context. We con-
jecture that whether a positive or a negative CR is
more cooperative in a particular context depends on
what the preferred answer to the question ?whether
q? is assumed to be.
Contextually-determined A/V. Another context in
which a CR is appropriate is when an answer to a
yes/no-question is contingent on an A/V that has
already been established in the preceding context,
as in (5). We call this type of CR a contextually-
determined A/V CR (CDCR).
What does a CDCR communicate besides the as-
sertion and implicature that answer the question as
specified in Figure 1? We suggested in ?1 that it
initiates a negotiation about the already established
A/V. However, this cannot happen by simply rais-
ing the question whether c holds, because c has al-
ready been established. We suggest that a CDCR
implicitly proposes to consider changing the A/V: It
re-raises the question whether c holds. Re-raising
c differs from raising a ?new? question at least in
two aspects: c must be negotiable, and re-raising
c means it cannot be answered simply by provid-
ing a sufficiently discriminative positive or negative
response. To see the difference, consider (5) with
(5:S.2) continued by the following utterances.
(9) U.3: Yes. | No.
U.3?: Yes, I do. | No, I don?t.
U.3??: Yes, I want business class. |
No, I don?t want business class.
(10) U.3: OK, I can fly economy.
U.3?: But I do want business class.
(11) U.3: How about Tuesday?
Like the responses in (6), the response in (9:U.3)
cannot be interpreted as answers to whether the user
wants to change her mind from business to economy
class. It seems hard to interpret even as acknowledg-
ment. But then we observe a number of differences
from the NDCR in (6):
The responses in (9:U.3?) and (9:U.3??) are not ap-
propriate as answers to the implicitly re-raised c, be-
cause a revision of an A/V is involved. Hence, some
kind of acknowledgment of the revision is needed in
addition to answering whether or not the A/V is to
be revised (and how). Such acknowledgments are
present in (10). In (10:U.3), ?OK? can be seen as ac-
knowledging the revision from business to economy
class. In (10:U.3?), ?but? acknowledges the contrast
between the proposed revision and the actual preser-
vation of the A/V (here, business class). The con-
tinuation in (11), on the other hand, refuses the pro-
posed revision only implicitly by proposing instead
to check the flight possibilities on another day.
Another observation concerning a CDCR is that
it cannot immediately follow after an utterance in
which the value is established, as the inappropriate-
ness of (12:S.1) and (12:S.1?) shows.
(12) U.1: Can I fly business class from Ko?ln to Paris on
Sunday?
S.1: Not if you want business class.
S.1?: Yes if you want economy class.
Intuitively, the reason for this is that there needs
to be some degree of uncertainty (in the sense of
being assumed but not known to be shared) about
the A/V. For example, in (5), the business class re-
quirement is assumed to be maintained when the day
is revised. The inappropriateness of (12:S.1) and
(12:S.1?) can also be explained on purely semantic
grounds. When both the assertion and the implica-
ture as specified in Figure 1 are taken into account,
a contradiction arises: Given that the elliptical an-
swer is resolved to the previous utterance, (12:S.1)
asserts If user wants business class, then a business
flight from SB to Paris on Sunday is not available,
and implicates If user does not want business class,
then a business flight from SB to Paris on Sunday is
available. Similarly for (12:S.1?).
Thus, the interpretation of a CDCR is that (i) it is
now determined whether q or not-q holds, because
(ii) the answer (specified in Figure 1) is contingent
on c and c is established. Also, (iii) the CDCR in-
dicates the reason for the answer, and (iv) proposes
to reconsider the earlier made decision by implicitly
re-raising the question whether c holds, and (v) mak-
ing a suggestion for it to be revised. A negotiation is
started in which the conflicting A/V is either revised
or confirmed. In the latter case a different solution
to the overall goal must be sought.
As for production, the system may produce
a CDCR when (i) answering a yes/no-question
whether q, where (ii) the answer is either q or not-
q, depending on some A/V c which has been estab-
lished in the context prior to the question whether q.
Again, what polarity of CR is more cooperative in
a particular context depends on what the preferred
answer to the question whether q is assumed to be.
3 Conditional response dialogue moves
According to the dialogue annotation scheme of
(Allen and Core, 1997), utterances in which ?the
participant does not address the proposal but per-
forms an act that leaves the decision open pending
further discussion? are called hold moves. The di-
alogue moves of a NDCR seem similar to hold in
that the answer to q remains pending due to its con-
tingency on an unknown A/V c. Once c is deter-
mined, q is answered. Hence, we propose to char-
acterize a NDCR as a dialogue move combining
the backward-looking function of a partial yes/no-
answer and hold, and the forward-looking function
of a yes/no question whether the condition holds. A
CDCR is different in that it proposes to reconsider a
contexually-determined c. Allen & Core provide no
suitable characterization of this. We propose to char-
acterize a CDCR as a dialogue move that combines
the backward-looking function of a yes/no-answer
with the forward-looking function of an alternative
question whether c is preserved or revised.
4 Conclusions
We proposed an approach to dealing with condi-
tional responses (CRs), which arise naturally in dia-
logues allowing for mixed initiative and negotiation.
We proposed two types of CRs. One type describes
the case where the answer is contingent on an at-
tribute/value that has not yet been determined in the
context (NDCRs). The other type deals with an at-
tribute/value that has already been set in the context,
and which now needs to reconsidered (CDCRs). The
distinction properly clarifies the different effects on
dialogue context CRs may have. We are currently
developing an implementation of CRs in the GoDiS
system (Kruijff-Korbayova? et al, 2002).
References
James Allen and Mark Core. 1997. Draft of
damsl: Dialogue Act Markup in Several Layers.
http://www.cs.rochester.edu/research/cisd/resources/damsl.
Robin Cooper, Staffan Larsson, Colin Matheson, Massimo Poe-
sio, and David Traum. 1999. Coding Instructional Dialogue
for Information States. http://www.ling.gu.se/projekt/trindi/,
February.
Jonathan Ginzburg. 1996. Interrogatives: Questions, Facts and
Dialogue. In Shalom Lappin, editor, The Handbook of Con-
temporary Semantic Theory, pages 385?422. Blackwell, Ox-
ford, UK/Cambridge, USA.
Nancy Green and Sandra Carberry. 1999. A Computational
Mechanism for Initiative in Answer Generation. User Mod-
eling and User-Adapted Interaction, 9(1/2):93?132.
Elena Karagjosova and Ivana Kruijff-Korbayova?. 2002. An
Analysis of Conditional Responses in Dialogue. In Proceed-
ings of the 5th International Conference on TEXT, SPEECH
and DIALOGUE, Brno, Czech Republic. forthcoming.
Ivana Kruijff-Korbayova?, Elena Karagjosova, and Staffan Lars-
son. 2002. Enhancing collaboration with conditional re-
sponses in information seeking dialogues. Under review.
Ian Lewin, C.J.Rupp, Jim Hieronymus, David Milward,
Staffan Larsson, and Alexander Berman. 2000. Siridus
System Architecture and Interface Report (Baseline).
http://www.ling.gu.se/projekt/siridus/.
Discourse-Level Annotation
for Investigating Information Structure
Ivana Kruijff-Korbayova? and Geert-Jan M. Kruijff
Computational Linguistics, Saarland University, Saarbru?cken, Germany
{korbay,gj}@coli.uni-sb.de
Abstract
We present discourse-level annotation of newspa-
per texts in German and English, as part of an
ongoing project aimed at investigating information
structure from a cross-linguistic perspective. Rather
than annotating some specific notion of information
structure, we propose a theory-neutral annotation
of basic features at the levels of syntax, prosody
and discourse, using treebank data as a starting
point. Our discourse-level annotation scheme cov-
ers properties of discourse referents (e.g., semantic
sort, delimitation, quantification, familiarity status)
and anaphoric links (coreference and bridging). We
illustrate what investigations this data serves and
discuss some integration issues involved in combin-
ing different levels of stand-off annotations, created
by using different tools.
1 Introduction
The goal of this paper is to present a discourse-
level annotation scheme developed for the pur-
pose of investigating information distribution in
text from a cross-linguistic perspective, with a
particular focus on the interplay of various fac-
tors pertaining to the realization of information
structure. Information Structure (IS) concerns
utterance-internal structural and semantic proper-
ties reflecting the speaker?s/writer?s communica-
tive intentions and the relation of the utterance
to the discourse context, in terms of the dis-
course status of the content, the actual and at-
tributed attentional states of the discourse partici-
pants, and the participants? prior and changing atti-
tudes (knowledge, beliefs, intentions, expectations,
etc.) (Kruijff-Korbayova? and Steedman, 2003). In
many (if not all) languages, differences in IS moti-
vate variations in surface realization of utterances,
such as syntactic structure, word order and intona-
tion. But languages differ in the extent to which
they employ various combinations of IS-realization
means (Vallduv?? and Engdahl, 1996; Kruijff, 2001).
Modeling these phenomena and their interaction re-
quires understanding IS and its role in discourse.
IS is therefore an important aspect of meaning at
the interface between utterance and discourse, which
computational models of discourse processing should
take into account. Unfortunately, there exists no
theory that provides a comprehensive picture of IS,
explaining its realization cross-linguistically, its rep-
resentation at the level of linguistic meaning, and its
interpretation in context. Employing corpora can
help to deepen our intuitive understanding of IS, in
order to construct explanatorily more adequate the-
ories.
While the phenomena involved in discourse and
IS are themselves complex and not yet fully un-
derstood, studying and modeling their interaction
is made difficult by proliferating and often under-
formalized terminologies, especially for IS (cf. the
diverging dichotomies, e.g., Theme-Rheme, Topic-
Comment, Topic-Focus, Background-Focus, Given-
New, Contextually Bound-Nonbound). What is
needed is further systematization of terminologies,
formalization and computational modeling, and em-
pirical and corpus-based studies.
The goal of the MULI (MUltilingual Informa-
tion structure) project is to contribute to this effort
by empirically analyzing IS in German and English
newspaper texts. For this, we designed annotation
schemes for enriching existing linguistically inter-
preted language resources with information at the
levels of syntax, discourse semantics and prosody.
The MULI corpus consists of extracts
from the Tiger treebank for German
(Brants et al, to appear)1 and the Penn treebank
for English (Marcus et al, 1994)2. It comprises
250 sentences in German (app. 3,500 tokens) and
320 sentences in English (app. 7,000 tokens). The
MULI corpus has been created by extracting a
continuous stretch of 21 relatively short texts from
the Tiger treebank, and a set of 10 texts from the
Penn Treebank. The selection was made so that
the texts would be comparable in genre (financial
news/announcements).
The morphological, part-of-speech and syntactic
information encoded in the treebanks can be re-
used for our purposes. We add annotations of
syntactically marked constructions, prosodic fea-
tures and discourse semantics. Our approach
to annotation at the levels of syntax, prosody
and discourse is outlined in (Bauman et al, 2004a;
Bauman et al, 2004b). In this paper, we provide
1http://www.coli.uni-sb.de/cl/projects/tiger/
2http://www.cis.upenn.edu/~treebank/home.html
more details about the discourse-level annotation.
In ?2 we overview the methodological concerns
and desiderata we adhere to in designing our anno-
tation schemes. In ?3 we present the discourse-level
annotation scheme in detail. In ?4 we illustrate the
multi-level investigation perspective. ?5 we briefly
describe the annotation tools we use. In ?6 we con-
clude and sketch future work.
2 Methodology
Text samples of varying origin, genre, language and
size have been previously annotated with theory-
specific notions of IS by various authors. Such data
are typically not publicly available, and even if they
can be obtained, it is very hard if not impossible
to compare and reuse different annotations. More
promising in this respect are annotations that in-
clude or add some aspect(s) of IS to an existing
corpus or treebank. The most systematic effort of
this kind that we are familiar with is the Topic-
Focus annotation in the Prague Dependency Tree-
bank (Bura?n?ova? et al, 2000).
In contrast to other projects in which IS is
annotated and investigated, we do not annotate
theory-biased abstract categories like Topic-Focus
or Theme-Rheme. Since we are particularly inter-
ested in the correlations and co-occurrences of fea-
tures on different linguistic levels that can be inter-
preted as indicators of the abstract IS categories,
we needed an annotation scheme to be as theory-
neutral as possible: It should allow for a descrip-
tion of the phenomena, from which ?any? theory-
specific explanatory mechanisms can subsequently
be derived (Skut et al, 1997). We therefore con-
centrate instead on features pertaining, on the one
hand, to the surface realization of linguistic expres-
sions (the levels of syntax and prosody), and, on
the other hand, to the semantic character of the dis-
course referents (the discourse level).
In designing our annotation schemes, we fol-
lowed the guidelines of the Text Encoding Ini-
tiative3 and the Discourse Resource Initiative
(Carletta et al, 1997). In line with these standards,
we define for each annotation level (i) the markable
expressions, (ii) the attributes of markables, and (iii)
the links between markables (if any).
Syntax The Tiger treebank and the Penn tree-
bank we use as the starting point already con-
tain syntactic information. The additional syntac-
tic features annotated in the MULI project per-
tain to clauses as markable units, and encode the
presence of structures with noncanonical word order
that typically serve to put the focus on certain syn-
tactic elements. We include cleft, pseudo-cleft, re-
versed pseudo-cleft, extraposition, fronting and ex-
pletives, as well as voice distinctions (active, medio-
passive and passive). We annotate these features
explicitly (when not already present in the tree-
3http://www.tei-c.org/
bank annotation), to be able to correlate them di-
rectly with features at other levels. The annotation
scheme draws on accounts of the analysed features in
(Eisenberg, 1994) and (Weinrich, 1993) for German
and in (Quirk et al, 1985) and (Biber et al, 1999)
for English.
Prosody For the prosodic annotation, we
recorded one German and one English native
speaker reading aloud the texts of the MULI
corpus.4,5 The recordings were digitised and
annotated using the EMU Speech Database
System ((Cassidy and Harrington, 2001b);
http://emu.sourceforge.net/).
The markables at the prosody level are into-
nation phrases, intermediate phrases and words.
Their attributes encode the position and strength
of phrase breaks, and the position and type of
pitch accents and boundary tones, following the
conventions of ToBI (Tones and Break Indices
(Beckmann and Hirschberg, 1994)) for English and
GToBI6 (Grice et al, in press) for German, which
are regarded as standards for describing the into-
nation of these languages within the framework of
autosegmental-metrical phonology.
Discourse At the discourse level, we define as
markable those linguistic expressions that introduce
or access discourse entities (i.e., discourse referents
in the sense used in DRT and alike) (Webber, 1983;
Kamp and Reyle, 1993). Currently we consider
primarily the discourse entities introduced by
?nominal-like? expressions (Passoneau, 1996). We
include other kinds of expressions as markable only
when they participate in an anaphoric relation
with a ?nominal-like? expression. For example, a
sentence is a markable when it serves as an an-
tecedent of a discourse-deictic anaphoric expres-
sion (Webber, 1991); the main verb of a sentence
is a markable when the subject of the sentence
is a ?zero-anaphor?, etc. Our annotation instruc-
tions for identifying markables are an amalgamation
and extension of those of the MUC-7 Coreference
Task Definition7, the DRAMA annotation manual
(Passoneau, 1996), and (Wind, 2002).
The attributes of markables in our discourse-
level annotation scheme are designed to capture
a range of properties that semantically character-
ize the discourse entities evoked by linguistic ex-
4We are aware that using recorded speech is not ideal. We
nevertheless decided for this approach, as we wanted to work
on top of existing treebanks. As far as we are aware, there
does not exist a treebank for any of the publicly available
speech corpora.
5Since prosodic annotation is very time-consuming, we had
to concentrate mainly on one language. Thus, we analysed
all German texts and restricted ourselves to some English
examples. Since individual speaking preferences may vary
from speaker to speaker, we will have to record additional
speakers in order to be able to come up with generalizable
results.
6http://www.coli.uni-sb.de/phonetik/projects/Tobi/gtobi.html
7http://www.itl.nist.gov/iaui/894.02/related_projects/
muc/proceedings/co_task.html
pressions. Thereby we differ from most existing
discourse-level annotation efforts, which concentrate
on the linguistic expressions and on identifying
anaphoric relations between them (i.e., identifying
anaphors and their antecedents). A notable ex-
ception is the GNOME project annotation scheme
(Poesio et al, 1999): In GNOME, the aim was to
annotate a corpus with information relevant for noun
phrase generation. This included syntactic, seman-
tic and discourse attributes of nominal expressions.
The semantic attributes include, among others, an-
imacy, ontological status, countability, quantifica-
tion and generic vs. specific reference, which reflect
similar distinctions as we make in our annotation
scheme.
Besides the semantic properties that characterize
discourse entities individually, our annotation
scheme of course also covers referential rela-
tions between discourse entities, including both
identity and bridging. We build on and ex-
tend the MUC-7 coreference specification and
the coreference/bridging classifications described
in (Passoneau, 1996), (Carletta et al, 1997),
(Poesio, 2000) and (Mu?ller and Strube, 2001). We
represent anaphoric relations between linguistic ex-
pressions through links between the corresponding
markables. The type of relation is annotated as
an attribute of the markable corresponding to the
anaphor.
3 Discourse-Level Annotation
Information structure theories describe the phenom-
ena at hand at a surface level, at a semantic level,
or at both levels simultaneously, i.e., an expres-
sion belongs to some IS partition, in virtue of some
information-status of the corresponding discourse
entity. For the investigation of IS at the (discourse)
semantic level, we thus need more information about
the character of the discourse entities introduced by
linguistic expressions. We therefore annotated ex-
pressions with their discourse referents and their fol-
lowing properties:
Semantic type/sort reflects ontological charac-
ter of a discourse entity: object, property, even-
tuality or textual entity. Since the primary fo-
cus of our current annotation are discourse enti-
ties evoked by nominal-like expressions, most of
them denote objects. Objects are further classi-
fied according to semantic sorts: human/person, of-
fice/profession, organization, animal, plant, physical
object, quantity/amount, date/time, location/place,
group/collection, abstract entity, other. Proper-
ties are classified into either temporal or perma-
nent. Eventuality has sub-classes phase (habit
or state) and process (activity, accomplishment,
achievement). Textual entities are for now not fur-
ther classified.
Denotation characteristics of a discourse en-
tity are captured by a combination of attributes,
inspired by (Hlavsa, 1975). First, we distinguish
between denotational (extensional, referential) and
non-denotational (intensional, attributive) uses of
linguistic expressions. Denotationally used expres-
sions pick out (specify) some instance(s) of the des-
ignated concept(s). The instance(s) can be uniquely
specified (=identifiable to the hearer), or specific
but not identifiable, or even unspecific (arbitrary,
generic ? so any instance will do). Generic refer-
ences are seen as denoting types. An expression is
used non-denotationally when it attribute or qual-
ifies, i.e., evokes the characteristic properties of a
concept, without actually instantiating it. A typical
example of a non-denotationally used expression is
a predicative NP, as in ?He was a painter?.
The annotation of a group of denotation proper-
ties is motivated by the need to have a language-
independent characterization of the referents as
such, rather then the properties of the referring ex-
pression, such as (in)definiteness. The latter is a
surface reflex of a combination of denotation char-
acteristics, and sometimes may not even be overtly
indicated by articles or other determiners.
For the denotationally used expressions, we then
analyze what part of the domain designated by
the expression is actualy included in the extension.
These aspects are annotated in the determination,
delimitation and quantification attributes.
Determination characterizes the specificity of
the denoted concept instance. Unique determina-
tion means that the entity is uniquely specified, i.e.,
the hearer can (or is assumed to be able to) iden-
tify the entity/instance intended by the speaker.
There may be just one such entity, e.g., as with
proper names, or there are possibly more entities
that satisfy the description, but the speaker means
a particular one and assumes that the hearer can
identify it. Anaphoric pronouns are also typically
used as unique denotators. Finally, an entity can
be uniquely specified through a relation to another
entity, or through a relation between expressions in
the text. In (Hlavsa, 1975) this is called relational
uniqueness ; it seems to correspond to Loebner?s no-
tion of NPs as functions, used in the GNOME an-
notation scheme.
Existential determination is assigned to entities
that are not uniquely specified, that is, the speaker
does not assume the hearer to be able to identify a
particular entity, but in principle the speaker would
be able to identify one. Maybe such unique identifi-
cation by the hearer is not important for the inter-
action, it is enough to take ?some instance?.
Variable determination is assigned when an ex-
pression not only does not uniquely specify an en-
tity, but a particular entity cannot in principle be
identified, rather, the speaker means an arbitrary
(?any?) instance. Typical examples are generics, or
references to type.
Delimitation characterizes the extent of the de-
noted concept instance with respect to the domain
designated by the expression. The posible values are
total and partial, indicating the entire domain desig-
nated by the expression is included in the extension,
or only a part.
Quantification captures the countability of the
denotated concept instance, and if countable, the
quantity of the individual objects included in the
extension:
 uncountable is assigned when it is impossible
to decompose the extension into countable dis-
tinguishable individual objects, e.g., with mass
nouns;
 specific-single means quantity of one, e.g., ?one
x?, ?the other x?;
 specific-multiple means a concrete quantity
larger than one, e.g., ?two x?, ?both x?, ?a
dozen?;
 unspecific-multiple means an unspecified num-
ber larger than one, e.g., ?some x?, ?many x?,
?most x?.
Familiarity Status is a notion that most ap-
proaches to IS use as one dimension or level
of the IS-partitioning, for example Given/New
in (Halliday, 1985), Background/Focus in
(Steedman, 2000), or as the basis for deriving
a higher level of partitioning (Sgall et al, 1986).
It is therefore important to capture it in our anno-
tation as an independent feature, so that we can cor-
relate it with other features at the discourse level and
at other levels. We apply the familiarity status tax-
onomy from (Prince, 1981), distinguishing between
new, unused, inferrable, textually and situationally
evoked entities. We are aware that operationalizing
Prince?s taxonomy is a tough issue. For the time be-
ing, our annotation guidelines give intuitive descrip-
tions of the different statuses, roughly as follows:
 brand new : create a new discourse referent for
a previously unknown object;
 unused : create a new discourse referent for a
known object;
 inferable: create a new discourse referent for an
inferable object;
 evoked (textually or situationally): access an
available discourse referent.
Annotators? uncertainty or discrepancies between
annotators help us to identify problematic cases, and
to revise the guidelines where necessary.8
Linguistic form encodes the syntactic category
of the markable expression. This is not an attribute
encoding a semantic property of a discourse entity.
We have found it useful to distinguish the following
categories:
8Our reason for applying the familiarity taxonomy from
(Prince, 1981) is that it addresses the status of discourse en-
tities as such, not other referential properties. For example,
the givenness hierarchy in (Gundel et al, 1993) interleaves in-
formation status with uniqueness and specificity.
 nominal group is a ?normal? NP with a head
noun;
 pronominal subsumes expressions headed by a
personal, demonstrative, interrogative or rela-
tive pronoun;
 possessive covers possessive premodifiers (typ-
ically a possessive pronoun, e.g., ?our view?,
or possessive adjective, e.g., ?the Treasury?s
threat? or in German ?newyorker Burse?;
 pronominal adverb in German, e.g. ?daraus?
(from that);
 apposition and coordination;
 clitic is used for clitics and in those cases when
an expression contains a clitic affix (though
not frequent in English and German newspaper
text);
 ellipsis is used for elliptical (reduced) expres-
sions, which function as nominal-like groups,
but contain no nominal head (e.g., ?the first?);
in case a discourse entity is evoked by a zero ar-
gument, e.g., in case of subject- or object pro-
drop, a markable is created on a surrogate non-
nominal expression, labeled as zero-arg; finally,
clause or text are used for markables which are
clause and simple sentences, or text segments,
respectively (note that these are only mark-
able, when they serve as antecedents to nominal
anaphors).
These categories classify the linguistic forms of ex-
pressions independently of the categories employed
in the syntactic-level annotation. There are also
technical reasons for introducing a form-feature, e.g.,
when some other expression serves as a markable to
annotate the attributes of the discourse entity cor-
responding to a ?zero-anaphor? or to a clitic affix.
Referential link encodes the type of relation
between the discourse entity corresponding to an
anaphoric expression, and the one corresponding to
the (most likely) antecedent. The referential links we
distinguish are identity (representing coreference)
and bridging, further classified into set-membership,
set-containment, part-whole composition, property-
attribution, generalized possession, causal link and
lexical-argument-filling.
The attributes of information status and referen-
tial link are related, but we include them both, be-
cause the former is a property of a discourse entity,
while the latter directly reflects anaphoricity as a
property of an expression (the size of it ranging, ul-
timately, from a word to a segment of a discourse).
The relation between anaphoricity and IS is not a
straightforward one, and needs further investigation,
enabled by an annotation like ours.
4 Multi-level Investigation of IS
We illustrate the different levels of annotation and
analysis with an example sequence taken from our
English corpus (Figure 1). We considered the syn-
tactic annotation as a suitable starting point for the
analysis. Where relevant features are detected, we
compare the annotation at other levels.
(1) In the 1987 crash, remember, the market
was shaken by a Danny Rostenkowski pro-
posal to tax takeovers out of existence. (2)
Even more important, in our view, was the
Treasury?s threat to thrash the dollar. (3) The
Treasury is doing the same thing today; (4)
thankfully, the dollar is not under 1987-style
pressure.
Figure 1: Example from the English corpus
Of the four clauses in the example sequence, three
show noncanonical word orders. In (1), the temporal
adjunct is fronted, followed by the main predicate
remember (in imperative mood). Additionally, (1)
contains a passive construction bringing the patient
in subject position. In (2), subject complement and
adjunct (marking stance) are fronted. In (4), an
adjunct (againmarking stance) is fronted.
The discourse entity (DE) introduced in the
fronted temporal phrase the 1987 crash in (1) is ex-
tensional, abstract, unique, specific singular, and has
the information status of unused (also indicated by
remember). The DE introduced in the unmarked
subject position is extensional, abstract, unique,
specific singular, but has the status of inferrable:
the market can be seen as a bridging anaphor to the
crash, by means of an argument filling (crash of the
market). The DEs introduced by the sentence-final
expressions in (1) and (2) are also extensional, ab-
stract, unique, specific singular, and both have the
information status of new.9 What appears sentence-
final in (1) and (2) are thus two negative things that
happened during the 1987 crash. The fronted ex-
pression(s) in (2) are not annotated as a DE. The
DEs in the unmarked subject positions in (3) and (4)
both have the information status of textually evoked,
as both expressions are coreferential anaphors to
parts of the Treasury?s threat to thrash the dollar.
While the DE referred to by the Treasury is an ex-
tensional, office, unique, specific singular, that of the
dollar is intensional, abstract, unique, uncountable.
The expression the same thing in (3) is anaphoric to
the Treasury?s threat . . . in (2), but it introduces
a new DE of the same type; its information status
is that of inferrable. Finally, the DE introduced in
the sentence-final expression 1987-style pressure in
(4) is intensional, abstract, existential, uncountable,
and also has the information status of inferable; it
is however hard to code it as a bridging anaphor,
because it is not clear what relation it would have
to what antecedent: if anything, then a Danny Ros-
tenkowski proposal . . . in (1).
The prosodic analysis shows that the fronted
phrase in (2) is not only syntactically but also
9We assume a layman reader. For an economy expert,
these entities may have the status of unused.
prosodically prominent (cf. Figure 2): Two peak ac-
cents on even and more highlight these words (with
the more pronounced accent on more expressing a
contrast), whereas the word important is deaccented,
since the concept of ?importance? is inferable from
the context. Furthermore, the adjective construc-
tion forms a phrase of its own, delimited by an in-
tonation phrase boundary, which is in turn signalled
by a falling-rising contour plus a short pause. The
following parenthesis in our view also constitutes a
single intonation phrase. Here again, our is assigned
a contrastive accent, while view is unaccented.
All remaining content words of the clause re-
ceive accents. However, the most ?newsworthy?
word, threat, is the only one marked by a ris-
ing pitch accent (L+H*), indicating its higher de-
gree of importance for the speaker. This interpre-
tation is further supported by the insertion of a
phrase break directly after this word. Finally, the
high-downstepped nuclear accent (H+!H*) on dollar
marks this item as being accessible by speaker and
hearer (Pierrehumbert and Hirschberg, 1990).
5 Technical Realization
Above we presented a multi-level view on IS anno-
tation, where each layer is to be annotated indepen-
dently, to enable us to investigate interactions across
the different levels. Such investigations involve ei-
ther exploration of the integrated data (i.e., simul-
taneous viewing of the different levels and searching
across levels) or integrated processing, e.g., in order
to discover or test correlations across levels. There
are two crucial technical requirements that must be
satisfied to make this possible: (i) stand-off anno-
tation at each level and (ii) alignment of base data
across the levels. Without the first, we would not be
able to keep the levels separate and perform annota-
tion at each level independently, without the latter
we would not be able to align the separate levels.
We have chosen XML for the representation and
maintenance of annotations. Each level of anno-
tation is represented as a separate XML file, re-
ferring to (sequences of) tokens in a common base
file containing the actual text data. We keep inde-
pendent levels of annotation separate, even if they
can in principle be merged into a single hierarchy.
Parallel aligned texts (e.g., the written and spo-
ken versions of our corpus) are also represented via
shared token IDs. A related issue is that of annota-
tion tools. We are not using one generic tool for
all levels for the simple reason that we have not
found a tool that would support the needs of all
levels and still be efficient (Bauman et al, 2004b;
Mu?ller and Strube, 2001). Therefore, we prefer to
use tools specifically designed for the task at hand.
We describe the tools of our choice below.
Prosodic Level The speech data was anno-
tated with the EMU Speech Database System10
(Cassidy and Harrington, 2001a), which produces
10http://emu.sourceforge.net/
Figure 2: Prosodic annotation of example sentence (2) in EMU
several files in which time stamps are associated with
the respective annotated labels.
Syntactic Level For the syntactic annotation, we
used the XML editor XML-Spy11. The annotation
scheme is defined in a DTD, which is used to check
the well-formedness and the validity of the annota-
tion.
Discourse Level The discourse-level annotation
is done with the MMAX annotation tool developed
at EML, Heidelberg (Mu?ller and Strube, 2003).
MMAX is a light-weight tool written in Java that
runs under both Windows and Unix/Linux. It sup-
ports multilevel annotation of XML-encoded data
using annotation schemes defined as DTDs. MMAX
implements the above-mentioned general concepts of
markables with attributes and standing in link rela-
tions to one another. To exploit and reuse annotated
data in the MMAX format, there is the MMAX
XML Discourse API.
Integration The tools inevitably employ differ-
ent data formats: on the prosodic level data is stored
in the EMU data format, on the syntactic level in
Tiger XML and on the discourse level in MMAX
XML format.
The EMU files have to be converted into stand-off
XML format. To be able to align the prosodic an-
notation with the syntax and the discourse level, we
chose the word as common basic unit. This poses
several problems. First, punctuation marks count
as separate words, but are not realised in spoken
language. To be able to correlate prosodic phras-
ing and punctuation marks, we store the punctua-
tion marks as attributes of the respective preceding
word. Second, pauses occur very often in speech, but
as they are not part of the written texts, they do not
count as words. Because they are an important fea-
ture for phrasing and rhythm, we also code them
as attributes of the preceding word. Third, in some
cases a single word carries more than one accent, e.g.
11http://www.xmlspy.com/
long compounds (Getra?nkedosenhersteller), or num-
bers. In these cases, it would be interesting to know
which part(s) of the word get accented, which re-
quires some way of annotating parts of words (e.g.,
syllables). Finally, for some multi-word units, e.g.
18,50 Mark, the spoken realisation (achtzehn Mark
fu?nfzig) cannot be aligned with the orthographic
form, because spoken and orthographic form differ
in number and order of words.
6 Conclusions and Perspectives
We presented the details of the discourse-level anno-
tation scheme that we developed within the MULI
project. This project is a pilot project: As such, the
annotation has so far been restricted to a relatively
small amount of data, since the experimental design
of the study required testing of tools as well as man-
ual annotation. We plan to extend the size of the
corpus by manual and semi-automatic annotation in
a follow-up project.
The challenge in the MULI project has been to
define theory-neutral and language-independent an-
notation schemes for annotating linguistic data with
information that pertains to the realisation and in-
terpretation of information structure. An important
characteristic of the MULI corpus, arising from its
theory-neutrality, is that it is descriptive. The cor-
pus annotation is not based on explanatory mecha-
nisms: We have to derive such explanations from the
data. (See (Skut et al, 1997) for related methodol-
ogy pertaining to syntactic treebanks.)
The MULI corpus facilitates linguistic investiga-
tion of how phenomena at different annotation levels
interact. For example, how do syntactic structure
and intonation interact to realize information struc-
ture? Or, how does information structure interact
with anaphoric relationships? Such linguistic inves-
tigations can help to extend existing accounts of in-
formation structure, and can also be used to verify
(or falsify) predictions made by such accounts. The
corpus also makes it possible to construct computa-
tional models from the corpus data.
Theory-neutrality enhances reusability of linguis-
tic resources, because it facilitates the integration
with other, theory-neutral resources. To some ex-
tent we have already explored this in MULI, com-
bining e.g. Tiger annotation with discourse-level
annotation. Another possibility to explore is the to
integrate MULI annotation with, e.g., the SALSA
corpus (Erk et al, 2003), which provides more de-
tailed semantico-pragmatic information in the style
of FrameNet.
Our initial investigation also reveals where addi-
tional annotation would be needed. For instance,
the text example discussed above constitutes a con-
cession scheme, which we cannot identify without
annotating discourse/rhetorical relations. This in
turn requires extending the annotation scheme to
non-nominal markables.
Acknowledgements
We would like to thank Saarland University for fund-
ing the MULI pilot project. Thanks also to Stella
Neumann, Erich Steiner, Elke Teich, Stefan Bau-
mann, Caren Brinckmann, Silvia Hansen-Schirra
and Hans Uszkoreit for discussions.
References
S. Bauman, C. Brinckmann, S. Hansen-Schirra, G.-J.
Kruijff, I. Kruijff-Korbayova?, S. Neumann, and E. Te-
ich. 2004a. Multi-dimensional annotation of linguis-
tic corpora for investigating information structure. In
Proc. of the Workshop on Frontiers in Corpus Anno-
tation, held at the NAACL-HLT 2004 Conference.
S. Bauman, C. Brinckmann, S. Hansen-Schirra, G.-J.
Kruijff, I. Kruijff-Korbayova?, S. Neumann, E. Te-
ich, E. Steiner, and H. Uszkoreit. 2004b. The muli
project: Annotation and analysis of information struc-
ture in German and English. In Proc. of the LREC
2004 Conference.
M. E. Beckmann and J. Hirschberg. 1994. The ToBI an-
notation conventions. Ms. and accompanying speech
materials, Ohio State University.
D. Biber, S. Johansson, G. Leech, S. Conrad, and E.
Finegan. 1999. The Longman Grammar of Spoken
and Written English. Longman, Harlow.
S. Bird and M. Liberman. 2001. A formal framework for
linguistic annotation. Speech Communication, 33(1-
2):23?60.
S. Brants, S. Dipper, P. Eisenberg, S. Hansen, E. Ko?nig,
W. Lezius, C. Rohrer, G. Smith, and H. Uszkoreit. to
appear. TIGER: Linguistic interpretation of a Ger-
man corpus. Journal of Language and Computation
(JLAC), Special Issue.
E. Bura?n?ova?, E. Hajic?ova?, and P. Sgall. 2000. Tagging of
very large corpora: Topic-focus articulation. In Proc.
of the 18th Conference on Computational Linguistics
(COLING?2000), July 31 - August 4 2000, pages 139?
144. Universita?t des Saarlandes, Saarbru?cken, Ger-
many.
J. Carletta, N. Dahlba?ck, N. Reithinger, and M. A.
Walker. 1997. Standards for dialogue coding in natu-
ral language processing. Report on the dagstuhl sem-
inar, Discourse Resource Initiative, February 3?7.
S. Cassidy and J. Harrington. 2001a. Multi-level anno-
tation in the emu speech database management sys-
tem. Speech Communication, 33(1-2):61?78.
S. Cassidy and J. Harrington. 2001b. Multi-level an-
notation in the EMU speech database management
system. Speech Communication, 33(1-2):61?78.
P. Eisenberg. 1994. Grundriss der deutschen Gram-
matik, 3. Aufl. Metzler, Stuttgart, Weimar.
K. Erk, A. Kowalski, S. Pado?, and M. Pinkal. 2003. To-
wards a resource for lexical semantics: A large german
corpus with extensive semantic annotation. In Proc.
of ACL 2003, Sapporo, Japan.
M. Grice, S. Baumann, and R. Benzmu?ller. in press.
German intonation in autosegmental-metrical phonol-
ogy. In Sun-Ah Jun, editor, Prosodic Typology:
Through Intonational Phonology and Transcription.
OUP.
J. Gundel, N. Hedberg, and R. Zacharski. 1993. Cog-
nitive status and the form of referring expressions in
discourse. Language, (69):274?307.
M. A.K. Halliday. 1985. Introduction to Functional
Grammar. Edward Arnold, London, U.K.
Z. Hlavsa. 1975. Denotace objektu a jej?? prostr?edky v
souc?asne? c?es?tine? [Denotating of objects and its means
in contemporary Czech], volume 10 of Studie a pra?ce
lingvisticke? [Linguistic studies and works]. Academia.
N. Ide, P. Bonhomme, and L. Romary. 2000. Xces: An
xml-based standard for linguistic corpora. pages 825?
830, Athens, Greece.
H. Kamp and U. Reyle. 1993. From discourse to logic.
Kluwer Academic Publishers, Dordrecht, the Nether-
lands.
Geert-Jan M. Kruijff 2001. A Categorial-Modal Logical
Architecture of Informativity: Dependency Grammar
Logic & Information Structure, Faculty of Mathemat-
ics and Physics, Charles University. Prague, Czech Re-
public.
I. Kruijff-Korbayova? and M. Steedman. 2003. Discourse
and information structure. Journal of Logic, Lan-
guage and Information: Special Issue on Discourse
and Information Structure, 12(3):249?259.
M. Marcus, G. Kim, M. Ann Marcinkiewicz, R. MacIn-
tyre, A. Bies, M. Ferguson, K. Katz, and B. Schas-
berger? 1994. The Penn treebank: Annotating predi-
cate argument structure. In Proc. of the Human Lan-
guage Technology Workshop, San Francisco, Morgan
Kaufmann.
D. McKelvie, A. Isard, A. Mengel, M.B. Moller,
M. Grosse, and M. Klein. 2001. The MATE work-
bench ? an annotation tool for XML coded speech
corpora. Speech Communication, 33(1-2):97?112.
C. Mu?ller and M. Strube. 2001. Annotating anaphoric
and bridging relations with MMAX. In Proc. of the
2nd SIGdial Workshop on Discourse and Dialogue,
pages 90?95, Aalborg, Denmark, 1?2 September.
http://www.eml.villa-bosch.de/english/Research/NLP/sigdial
C. Mu?ller and M. Strube. 2003. Multi-level an-
notation in mmax. In Proc. of the 4th SIG-
dial Workshop on Discourse and Dialogue, Sap-
poro, Japan, 4-5 July. http://www.eml.villa-
bosch.de/english/Research/NLP/Publications.
R. Passoneau. 1996. Instructions for applying dis-
course reference annotation for multiple applications
(DRAMA). draft, December 20.
J. Pierrehumbert and J. Hirschberg. 1990. The meaning
of intonational contours in the interpretation of dis-
course. In P.R. Cohen, J. Morgan, and M.E. Pollack,
editors, Intentions in Communication, pages 271?311.
MIT press.
Massimo Poesio, Renate Henschel, Janet Hitzeman,
Rodger Kibble, Shane Montague, and Kees van
Deemter 1999. Towards An Annotation Scheme For
Noun Phrase Generation In Proc. of the EACL Work-
shop on Linguistically Interpreted Corpora. Bergen,
Norway.
Massimo Poesio 2000. The GNOME An-
notation Scheme Manual Available online
http://www.hcrc.ed.ac.uk/~gnome/anno manual.html
E. Prince. 1981. Toward a taxonomy of given-new infor-
mation. In P. Cole, editor, Radical Pragmatics, pages
223?256. Academic Press.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartik. 1985.
A comprehensive grammar of the English language.
Longman, London.
P. Sgall, E. Hajic?ova?, and J. Panevova?. 1986. The mean-
ing of the sentence in its semantic and pragmatic as-
pects. Reidel, Dordrecht, The Netherlands.
W. Skut, B. Krenn, T. Brants, and H. Uszkoreit. 1997.
An annotation scheme for free word order languages.
In Applied Natural Language Processing 1997, pages
88?95.
M. Steedman. 2000. Information structure and
the syntax-phonology interface. Linguistic Inquiry,
31(4):649?689.
E. Teich, S. Hansen, and P. Fankhauser. 2001. Repre-
senting and querying multi-layer annotated corpora.
pages 228?237, Philadelphia.
E. Vallduv?? and E. Engdahl. 1996. The linguistic reali-
sation of information packaging. Linguistics, 34:459?
519.
B. L. Webber. 1983. So what can we talk about now?
M.I.T. Press.
B. L. Webber. 1991. Structure and ostension in the in-
terpretation of discourse deixis. Language and Cogni-
tive Processes, 6(2):107?135.
H. Weinrich. 1993. Textgrammatik der deutschen
Sprache. Dudenverlag, Mannheim u.a.
L. Wind. 2002. Manual zur Annotation von anapho-
rischen und Bridging-relationen. European Media
Laboratory GmbH, August 9.
Lexical-Semantic Interpretation of Language Input
in Mathematical Dialogs
Magdalena Wolska1 Ivana Kruijff-Korbayova?1 Helmut Horacek2
1Fachrichtung Computerlinguistik 2Fachrichtung Informatik
Universita?t des Saarlandes, Postfach 15 11 50
66041 Saarbru?cken, Germany
{magda,korbay}@coli.uni-sb.de, horacek@ags.uni-sb.de
Abstract
Discourse in formal domains, such as mathematics,
is characterized by a mixture of telegraphic natu-
ral language and embedded (semi-)formal symbolic
mathematical expressions. Due to the lack of em-
pirical data, little is known about the suitability of
input analysis methods for mathematical discourse
in a dialog setting. We present an input understand-
ing method for a tutoring system teaching mathe-
matical theorem proving. The adopted deep anal-
ysis strategy is motivated by the complexity of the
language phenomena observed in a corpus collected
in a Wizard-of-Oz experiment. Our goal is a uni-
form input interpretation, in particular, considering
different degrees of formality of natural language
verbalizations.
1 Introduction
In the DIALOG1 project (Benzm u?ller et al, 2003a),
we are investigating and modeling semantic and
pragmatic phenomena in tutorial dialogs focused on
problem solving skills in mathematics. Our goal is
(i) to empirically investigate the use of flexible natu-
ral language dialog in tutoring mathematics, and (ii)
to develop a dialog-based tutoring system for teach-
ing mathematical theorem proving. The experimen-
tal system will engage in a dialog in written natural
language to help a student understand and construct
mathematical proofs. In this paper, we address a
strategy for user input interpretation in our setting.
Because of the lack of empirical dialog-data on
the use of natural language in formal domains, such
as mathematics, we conducted a Wizard-of-Oz ex-
periment to collect a corpus of dialogs with a sim-
ulated system teaching proofs in naive set theory.
An investigation of the corpus reveals language phe-
nomena that present challenges to the existing com-
monly used input understanding methods. The chal-
1The DIALOG project is a collaboration between the Com-
puter Science and Computational Linguistics departments of
University of the Saarland, and is a part of the Collaborative
Research Center on Resource-Adaptive Cognitive Processes,
SFB 378 (www.coli.uni-sb.de/sfb378).
lenges lie in (i) the tight interleaving of natural and
symbolic language, (ii) varying degree of natural
language verbalization of the formal mathematical
content, and (iii) informal and/or imprecise refer-
ence to mathematical concepts and relations.
These phenomena motivate the use of deep syn-
tactic and semantic analysis of user input. We de-
veloped a grammar that allows a uniform treatment
of the linguistic content on a par with the math-
ematical content and thus supports analysis of in-
puts of different degrees of verbalization. We em-
ploy a domain-motivated semantic lexicon to medi-
ate between the domain-independent semantic rep-
resentation obtained through semantic construction
during parsing and domain-specific interpretation.
This serves to achieve a consistent semantic anal-
ysis while avoiding example-based heuristics.
The paper is organized as follows: In Sect. 2,
we present the setup of the system and the corpus
collection experiment. In Sect. 3 and show exam-
ples of language phenomena from our dialogs. In
Sect. 4, we first summarize the approach to pars-
ing the mixed symbolic and natural language in-
put and then present a lexical-semantic interface to
a domain-specific interpretation of the input. We
show example analyzes in Sect. 5. In Sect. 6,
we summarize relevant existing approaches to input
analysis in (tutorial) dialog systems on the one hand
and analysis of mathematical discourse on the other.
Sect. 7 is the conclusion.
2 System setup and corpus collection
Our system scenario is illustrated in Fig. 1:
? Learning Environment: Students take an inter-
active course in the relevant subfield of mathe-
matics.
? Mathematical Proof Assistant (MPA): Checks
the appropriateness of user specified inference
steps with respect to the problem-solving goal;
based on ?MEGA.
? Proof Manager (PM): In the course of tutor-
ing session the student may explore alternative
PEDAGOGICAL
KNOWLEDGE
US
ER
MO
DE
L
LEARNING
ENVIRONMENT
MATHEMATICAL
PROOF ASSISTANT
DIALOG MANAGERG
EN
ER
A
TI
O
N
PRO
O
F M
A
N
A
G
ER
A
N
A
LY
SIS
MATHEMATICAL
KNOWLEDGE
(MBASE)
ACTIVEMATH OMEGA
RESOURCES
LINGUISTIC DIALOG
RESOURCES
TUTORING
RESOURCES /
MANAGER
U
SE
R
Figure 1: DIALOG project scenario.
proofs. The PM builds and maintains a repre-
sentation of constructed proofs and communi-
cates with the MPA to evaluate the appropriate-
ness of the student?s contributions for the proof
construction.
? Dialog Manager: We employ the Information-
State (IS) Update approach developed in the
TRINDI project2
? Tutorial Manager (TM): This component
incorporates extensions to handle tutorial-
specific dialog moves, such as hinting.
? Knowledge Resources: This includes peda-
gogical knowledge (teaching strategies), and
mathematical knowledge.
In order to empirically investigate the use of nat-
ural language in mathematics tutoring, we collected
and analyzed a corpus of dialogs with a simulated
tutoring system.
24 subjects with varying educational background
and little/fair prior mathematical knowledge partic-
ipated in a Wizard-of-Oz experiment (Benzm u?ller et
al., 2003b). The experiment consisted of 3 phases:
(i) preparation and pre-test (on paper), (ii) tutor-
ing session (mediated by a WOz tool (Fiedler and
Gabsdil, 2002)), (iii) post-test and evaluation ques-
tionnaire (on paper). At the tutoring session, they
were asked to prove 3 theorems3: (i) K((A ? B) ?
(C ? D)) = (K(A) ? K(B)) ? (K(C) ? K(D));
(ii) A ? B ? P ((A ? C) ? (B ? C)); (iii) If
A ? K(B), then B ? K(A). The subjects were
instructed to enter proof steps, rather than complete
proofs at once, to encourage interaction with the
system. The subjects and the tutor were free in for-
mulating their turns.4
2http://www.ling.gu.se/research/projects/trindi/
3K stands for set complement and P for power set.
4Buttons were available in the interface for inserting math-
ematical symbols, while literals were typed on the keyboard.
The collected corpus consists of 66 dialog log-
files, containing on average 12 turns. The total num-
ber of sentences is 1115, of which 393 are student
sentences. The students? turns consisted on aver-
age of 1 sentence, the tutor?s of 2. More details on
the corpus itself and annotation efforts that guide
the development of the system components can be
found in (Wolska et al, 2004).
3 Linguistic data
In this section, we present an overview of the lan-
guage phenomena prominent in the collected di-
alogs to indicate the overall complexity of input un-
derstanding in our setting.5
Interleaved natural language and formulas The
following examples illustrate how the mathematical
language, often semi-formal, is interleaved with the
natural language informally verbalizing proof steps.
A auch ? K(B) [Aalso ? K (B)]
A?B ist ? von C?(A?B) [... is ? of ...]
(da ja A?B=?) [(because A?B=?)]
B enthaelt kein x?A [B contains no x?A]
The mixture affects the way parsing needs to be
conducted: mathematical content has to be identi-
fied before it is interpreted within the utterance. In
particular, mathematical objects (or parts thereof)
may lie within the scope of quantifiers or negation
expressed in natural language (as in the last example
above).
Imprecise or informal naming Domain relations
and concepts are described informally using impre-
cise and/or ambiguous expressions.
A enthaelt B [A contains B]
A muss in B sein [A must be in B]
B vollstaendig ausserhalb von A liegen muss, also im
Komplement von A
[B has to be entirely outside of A, so in the complement of A]
dann sind A und B (vollkommen) verschieden, haben keine
gemeinsamen Elemente
[then A and B are (completely) different, have no common
elements]
In the above examples, contain and be in can ex-
press domain relations of (strict) subset or element,
while be outside of and be different are informal
descriptions of the empty intersection of sets.
To handle imprecision and informality, we have
designed an ontological knowledge base that in-
cludes domain-specific interpretations of concep-
tual relations that have corresponding formal coun-
terparts in the domain of naive set theory.
The dialogs were typed in German.
5As the tutor was also free in wording his turns, we include
observations from both student and tutor language behavior.
Metonymy Metonymic expressions are used to
refer to structural sub-parts of formulas, resulting
in predicate structures acceptable informally, yet in-
compatible in terms of selection restrictions.
Dann gilt fuer die linke Seite, wenn
C ? (A ? B) = (A ? C) ? (B ?C), der Begriff A ? B dann ja
schon dadrin und ist somit auch Element davon
[Then for the left hand side it is valid that..., the term A ? B is already
there, and so an element of it]
where the predicate be valid for, in this domain,
normally takes an argument of sort CONSTANT,
TERM or FORMULA, rather than LOCATION;
de morgan regel 2 auf beide komplemente angewendet
[de morgan rule 2 applied to both complements]
where the predicate apply takes two arguments: one
of sort RULE and the other of sort TERM or FOR-
MULA, rather than OPERATION ON SETS.
Informal descriptions of proof-step actions
Wende zweimal die DeMorgan-Regel an
[I?m applying DeMorgan rule twice]
damit kann ich den oberen Ausdruck wie folgt schreiben:. . .
[given this I can write the upper term as follows:. . . ]
Sometimes, ?actions? involving terms, formulae
or parts thereof are verbalized before the appropri-
ate formal operation is performed. The meaning of
the ?action verbs? is needed for the interpretation of
the intended proof-step.
Discourse deixis
der obere Ausdruck [the above term]
der letzte Satz [the last sentence]
Folgerung aus dem Obigen [conclusion from the above]
aus der regel in der zweiten Zeile
[from the rule in the second line]
This class of referring expressions includes also
references to structural parts of terms and formu-
las such as ?the left side? or ?the inner parenthe-
sis? which are incomplete specifications: the former
refers to a part of a formula, the latter, metonymic,
to an expression enclosed in parenthesis. More-
over, they require discourse referents for sub-parts
of mathematical expressions to be available.
Generic vs. specific reference
Potenzmenge enthaelt alle Teilmengen, also auch (A?B)
[A power set contains all subsets, hence also(A?B)]
Generic and specific references can appear within
one utterance as above, where ?a power set? is a
generic reference, whereas ?A?B? is a specific ref-
erence to a subset of a specific instance of a power
set introduced earlier.
Co-reference6
Da, wenn Ai?K(Bj) sein soll, Ai Element von K(Bj) sein
muss. Und wenn Bk?K(Al) sein soll, muss esk auch
Element von K(Al) sein.
[Because if it should be that Ai?K(Bj), Ai must be an
element of K(Bj). And if it should be that Bk?K(Al), it
must be an element of K(Al) as well.]
DeMorgan-Regel-2 besagt: K(Ai ? Bj) = K(Ai) ? K(Bj)
In diesem Fall: z.B. K(Ai) = dem Begriff
K(Ak ? Bl) K(Bj) = dem Begriff K(C ? D)[DeMorgan-Regel-2 means:
K(Ai ? Bj) = K(Ai) ? K(Bj) In this case: e.g. K(Ai) =
the term K(Ak ? Bl) K(Bj) = the term K(C ?D)]
Co-reference phenomena specific to informal
mathematical discourse involve (parts of) mathe-
matical expressions within text. In particular, enti-
ties denoted with the same literals may not co-refer,
as in the second utterance.
In the next section, we present the input interpre-
tation procedure up to the level of lexical-semantic
interpretation. We concentrate on the interface be-
tween the linguistic meaning representation (ob-
tained from the parser) and the representation of
domain-knowledge (encoded in a domain ontol-
ogy), which we realize through a domain-motivated
semantic lexicon.
4 Interpretation strategy
The task of the input interpretation component is
two-fold. Firstly, it is to construct a representation
of the utterance?s linguistic meaning. Secondly, it is
to identify within the utterance, separate, and con-
struct interpretations of:
(i) parts which constitute meta-communication
with the tutor (e.g., ?Ich habe die Aufgaben-
stellung nicht verstanden.? [I don?t understand
what the task is.] that are not to be processed
by the domain reasoner; and
(ii) parts which convey domain knowledge that
should be verified by a domain reasoner; for
example, the entire utterance ?K((A ? B)) ist
laut deMorgan-1 K(A) ? K(B)? [... is, ac-
cording to deMorgan-1,...] can be evaluated
in the context of the proof being constructed;
on the other hand, the reasoner?s knowledge
base does not contain appropriate representa-
tions to evaluate the appropriateness of the fo-
cusing particle ?also? in ?Wenn A = B, dann ist
A auch ? K(B) und B ? K(A).? [If A = B,
then A is also ? K(B) and B ? K(A).].
Domain-specific interpretation(s) of the proof-
relevant parts of the input are further processed by
6To indicate co-referential entities, we inserted the indices
which are not present in the dialog logfiles.
Proof Manager, a component that directly commu-
nicates with a domain-reasoner7 . The task of the
Proof Manager is to: (i) build and maintain a repre-
sentation of the proof constructed by the student;8
(ii) check appropriateness of the interpretation(s)
found by the input understanding module with the
state of the proof constructed so far; (iii) given the
current proof state, evaluate the utterance with re-
spect to soundness, relevance, and completeness.
The semantic analysis proceeds in 2 stages:
(i) After standard pre-processing9 , mathematical
expressions are identified, analyzed, catego-
rized, and substituted with default lexicon en-
tries encoded in the grammar. The input is then
syntactically parsed, and an formal abstract
representation of its meaning is constructed
compositionally along with the parse;
(ii) The obtained meaning representation is subse-
quently merged with discourse context and in-
terpreted by consulting a semantic lexicon of
the domain and a domain-specific ontology.
In the next sections, we first briefly summa-
rize the syntactic and semantic parsing part of the
input understanding process10 and show the for-
mat of meaning encoding constructed at this stage
(Sect. 4.1). Then, we show the lexical-semantic in-
terface to the domain ontology (Sect. 4.2).
4.1 Linguistic Meaning
By linguistic meaning (LM), we understand the
dependency-based deep semantics in the sense of
the Prague School sentence meaning as employed in
the Functional Generative Description (FGD) (Sgall
et al, 1986; Kruijff, 2001). It represents the lit-
eral meaning of the utterance rather than a domain-
specific interpretation.11 In FGD, the central frame
unit of a sentence/clause is the head verb which
specifies the tectogrammatical relations (TRs) of
7We are using a version of ?MEGA adapted for assertion-
level proving (Vo et al, 2003)
8The discourse content representation is separated from the
proof representation, however, the corresponding entities must
be co-indexed in both.
9Standard pre-processing includes sentence and word to-
kenization, (spelling correction and) morphological analysis,
part-of-speech tagging.
10We are concentrating on syntactically well-formed utter-
ances. In this paper, we are not discussing ways of combin-
ing deep and shallow processing techniques for handling mal-
formed input.
11LM is conceptually related to logical form, however, dif-
fers in coverage: while it does operate on the level of deep
semantic roles, such aspects of meaning as the scope of quan-
tifiers or interpretation of plurals, synonymy, or ambiguity are
not resolved.
its dependents (participants). Further distinction is
drawn into inner participants, such as Actor, Pa-
tient, Addressee, and free modifications, such as Lo-
cation, Means, Direction. Using TRs rather than
surface grammatical roles provides a generalized
view of the correlations between the conceptual
content of an utterance and its linguistic realization.
At the pre-processing stage, mathematical ex-
pressions embedded within input are identified, ver-
ified as to syntactic validity, categorized, and sub-
stituted with default lexical entries encoded in the
parser grammar for mathematical expression cate-
gories. For example, the expression K((A ? B) ?
(C ?D)) = (K(A?B)?K(C ?D)) given its top
node operator, =, is of type formula, its ?left side?
is the expression K((A ? B) ? (C ? D)), the list
of bracketed sub-expressions includes: A?B, C?D,
(A ? B) ? (C ? D), etc.
Next, the pre-processed input is parsed with
a lexically-based syntactic/semantic parser built
on Multi-Modal Combinatory Categorial Gram-
mar (Baldridge, 2002; Baldridge and Kruijff, 2003).
The task of the deep parser is to produce an FGD-
based linguistic meaning representation of syntac-
tically well-formed sentences and fragments. The
linguistic meaning is represented in the formalism
of Hybrid Logic Dependency Semantics. Details on
the semantic construction in this formalism can be
found in (Baldridge and Kruijff, 2002).
To derive our set of TRs we generalize and sim-
plify the collection of Praguian tectogrammatical
relations from (Hajic?ova? et al, 2000). One rea-
son for simplification is to distinguish which re-
lations are to be understood metaphorically given
the domain-specific sub-language. The most com-
monly occurring relations in our context (aside from
the roles of Actor and Patient) are Cause, Condi-
tion, and Result-Conclusion (which coincide with
the rhetorical relations in the argumentative struc-
ture of the proof):
Da [A ? K(B) gilt]<CAUSE>, alle x, die in A sind sind nicht in B
[As A?K(B) applies, all x that are in A are not in B]
Wenn [A ? K(B)]<COND>, dann A ? B=?
[If A?K(B), then A?B=?]
For example, in one of the readings of ?B en-
thaelt x ? A?, the verb ?enthaelten? represents
Figure 2: TRs in ?B contains x ? A?.
contain
FORMULA:B
<ACT>
FORMULA:x ? A
<PAT>
the meaning contain and in this frame takes de-
pendents in the relations Actor and Patient, shown
schematically in Fig. 2 (FORMULA represents the
default lexical entry for the identified mathematical
expressions categorized as formulas). The linguis-
tic meaning of this utterance returned by the parser
obtains the following representation:
@h1(contain ? <ACT>(f1 ? FORMULA:B) ? <PAT>(f2 ?
FORMULA: x ? A)
where h1 is the state where the proposition contain
is true, and the nominals f1 and f2 represent depen-
dents of the head contain, in the relations Actor and
Patient, respectively.
More details on our approach to parsing inter-
leaved natural and symbolic expressions can be
found in (Wolska and Kruijff-Korbayova?, 2004a)
and more information on investigation into tec-
togrammatical relations that build up linguistic
meaning of informal mathematical text can be found
in (Wolska and Kruijff-Korbayova?, 2004b).
4.2 Conceptual Semantics
At the final stage of input understanding, the lin-
guistic meaning representations obtained from the
parser are interpreted with respect to the given
domain. We encode information on the domain-
specific concepts and relations in a domain ontol-
ogy that reflects the knowledge base of the domain-
reasoner, and which is augmented to allow res-
olution of ambiguities introduced by natural lan-
guage (Horacek and Wolska, 2004). We interface
to the domain ontology through an upper-level on-
tology of concepts at the lexical-semantics level.
Domain specializations of conceptual relations
are encoded in the domain ontology, while a seman-
tic lexicon assigns conceptually-oriented semantics
in terms of linguistic meaning frames and provides a
link to the domain interpretation(s) through the do-
main ontology. Lexical semantics in combination
with the knowledge encoded in the ontology allows
us to identify those parts of utterances that have an
interpretation in the given domain. Moreover, pro-
ductive rules for treatment of metonymic expres-
sions are encoded through instantiation of type com-
patible concepts. If more than one lexical-semantic
interpretation is plausible, no disambiguation is per-
formed. Alternative conceptual representations are
further interpreted using the domain ontology, and
passed on to the Proof Manager for evaluation. Be-
low we explain some of the entries the semantic lex-
icon encodes:
Containment The Containment relation special-
izes into the domain relations of (strict) SUB-
SET and ELEMENT. Linguistically, it can be re-
alized, among others, with the verb ?enthalten?
(?contain?). The tectogrammatical frame of
?enthalten? involves the roles of Actor (ACT)
and Patient (PAT):
contain(ACTtype:F ORMULA, PATtype:F ORMULA) ?
(SUBFORMULAP AT , embeddingACT )
contain(ACTtype:OBJECT , PATtype:OBJECT ) ?
CONTAINMENT(containerACT , containeeP AT )
Location The Location relation, realized linguisti-
cally by the prepositional phrase introduced by
?in?, involves the tectogrammatical relations
HasProperty-Location (LOC) and the Actor of
the predicate ?sein?. We consider Location
in our domain as synonymous with Contain-
ment. Another realization of this relation, dual
to the above, occurs with the adverbial phrase
?au?erhalb von ...(liegen)? (?lie outside of?)
and is defined as negation of Containment:
in(ACTtype:OBJECT ,LOCtype:OBJECT )
? CONTAINMENT(containerLOC , containeeACT )
outside(ACTtype:OBJECT ,LOCtype:OBJECT )
? not(in(ACTtype:OBJECT ,LOCtype:OBJECT ))
Common property A general notion of ?common
property? we define as follows:
common(Property, ACTplural(A:SET,B:SET))
? Property(p1, A) ? Property(p1, B)
Property is a meta-object that can be instanti-
ated with any relational predicate, for example
as in ?(A und B)<ACT> haben (gemeinsame
Elemente)<PAT>? (?A and B have common
elements?):
common(ELEMENT, ACTplural(A:SET,B:SET))
? ELEMENT(p1 ,A) ? ELEMENT(p1 , B)
Difference The Difference relation, realized
linguistically by the predicates ?verschieden
(sein)? (?be different?; for COLLECTION or
STRUCTURED OBJECTS) and ?disjunkt (sein)?
(?be disjoint?; for objects of type COLLEC-
TION) involves a plural Actor (e.g. coordinated
noun phrases) and a HasProperty TRs. De-
pending on the type of the entity in the Actor
relation, the interpretations are:
different(ACTplural(A:SET,B:SET)) ? A 6= B
different(ACTplural(A:SET,B:SET))
? (e1 ELEMENT A ? e2 ELEMENT B ? e1 6= e2)
different(ACTplural(A:ST RUCTUREDOBJECT
,B:STRUCT UREDOBJECT ))
? (Property1(p1, A) ? Property2(p2, B) ?
Property1 = Property2 ? p1 6= p2)
Mereological relations Here, we encode part-
of relations between domain objects. These
concern both physical surface and ontologi-
cal properties of objects. Commonly occurring
part-of relations in our domain are:
hasComponent(STRUCTURED OBJECTterm,formula ,
STRUCTURED OBJECTSUBT ERM,SUBF ORMULA)
hasComponent(STRUCTURED OBJECTterm,formula ,
STRUCTURED
OBJECTENCLOSEDT ERM,ENCLOSEDF ORMULA)
hasComponent(STRUCTURED OBJECTterm,formula ,
STRUCTURED
OBJECTT ERMCOMP ONENT,FORMULACOMP ONENT )
Moreover, from the ontology we have:
Property(STRUCTURED OBJECTterm,formula ,
componentterm?side,formula?side)
Using these definitions and polysemy rules
such as polysemous(Object, Property), we can
obtain interpretation of utterances such as
?Dann gilt f u?r die linke Seite, . . . ? (?Then
for the left side it holds that . . . ?) where the
predicate ?gilt? normally takes two arguments
of types STRUCTURED OBJECTterm,formula ,
rather than an argument of type Property.
For example, the previously mentioned predicate
contain (Fig. 2) represents the semantic relation of
Containment which, in the domain of naive set the-
ory, is ambiguous between the domain relations EL-
EMENT, SUBSET, and PROPER SUBSET. The al-
ternative specializations are encoded in the domain
ontology, while the semantic lexicon provides the
conceptual structure of the head predicate. At the
domain interpretation stage, the semantic lexicon is
consulted to translate the tectogrammatical frame of
the predicate into a semantic relation represented
in the domain ontology. For the predicate contain,
from the semantic lexicon, we obtain:
contain(ACTtype:F ORMULA, PATtype:F ORMULA)
? (SUBFORMULAP AT , embeddingACT )
[?a Patient of type FORMULA is a subformula embedded within a
FORMULA in the Actor relation with respect to the head contain?]
contain(ACTtype:OBJECT , PATtype:OBJECT )
? CONTAINMENT(containerACT , containeeP AT )
[?the Containment relation involves a predicate contain and its Actor
and Patient dependents, where the Actor and Patient are the container
and containee parameters respectively?]
Translation rules that consult the domain ontology
expand the conceptual structure representation into
alternative domain-specific interpretations preserv-
ing argument structure. As it is in the capacity of
neither sentence-level nor discourse-level analysis
to evaluate the appropriateness of the alternative in-
terpretations in the proof context, this task is dele-
gated to the Proof Manager.
5 Example analysis
In this section, we illustrate the mechanics of the
approach on the following example:
A enthaelt keinesfalls Elemente, die auch in B sind.
[A contains no elements that are also in B]
The analysis proceeds as follows.
The mathematical expression tagger first iden-
tifies the expressions A and B. If there was no
prior discourse entity for ?A? and ?B? to verify
their types, they are ambiguous between constant,
term, and formula12 . The expressions are substi-
tuted with generic entries FORMULA, TERM, CONST
represented in the parser grammar. The sentence is
assigned alternative readings: ?CONST contains no
elements that are also in CONST?, ?CONST contains
no elements that are also in TERM?, ?CONST con-
tains no elements that are also in FORMULA?, etc.
Here, we continue only with ?CONST contains no
elements that are also in CONST?; the other readings
would be discarded at later stages of processing be-
cause of sortal incompatibilities.
The linguistic meaning of the utterance obtained
from the parser is represented by the following for-
mula13:
@n1(no ? <Restr>e1 ?
<Body>(p1 ? contain ? <ACT>(a1 ? A) ? <PAT> e1)) ?
@e1(element ?
<GenRel>(b1 ? be ? <ACT>e1 ? <HasProp-Loc>(b2 ? B)))
[?(set) A contains no elements that are in (set) B?]
Next, the semantic lexicon is consulted to trans-
late the linguistic meaning representation into a con-
ceptual structure. The relevant lexical semantic en-
tries are Containment and Location (see Sect. 4.2).
The transformation is presented schematically be-
low:
contain(ACTOBJECT :A, PATOBJECT :element) ?
CONTAINMENT(containerA , containeeelement)
(ACTOBJECT :element, HasProp-LocOBJECT :B )
? CONTAINMENT(containerB , containeeelement)
Finally, in the domain ontology, we find that the
conceptual relation of Containment, in naive set the-
ory, specializes into the domain relations of ELE-
MENT, SUBSET, STRICT SUBSET. Using the lin-
guistic meaning, the semantic lexicon, and the do-
main ontology, we obtain all the combinations of
interpretations, including the target one paraphrased
below:
?it is not the case that there exist elements e, such that e ? A and e ? B?,
Using translation rules the final interpretations
are translated into first-order logic formulas and
passed on for evaluation to the Proof Manager.
6 Related work
Language understanding in dialog systems, be it
with speech or text interface, is commonly per-
formed using shallow syntactic analysis combined
12In prior discourse, there may have been an assignment
A := ?, where ? is a formula, in which case, A would be known
from discourse context to be of type FORMULA (similarly for
term assignment); by CONST we mean a set or element variable
such as A, x denoting a set A or an element x respectively.
13Irrelevant parts of the meaning representation are omitted;
glosses of the formula are provided.
with keyword spotting. Tutorial systems also suc-
cessfully employ statistical methods which com-
pare student responses to a model built from pre-
constructed gold-standard answers (Graesser et al,
2000). This is impossible for our dialogs, due to
the presence of symbolic mathematical expressions
and because of such aspects of discourse meaning
as causal relations, modality, negation, or scope
of quantifiers which are of crucial importance in
our setting, but of which shallow techniques remain
oblivious (or handle them in a rudimentary way).
When precise understanding is needed, tutorial sys-
tems use closed-questions to elicit short answers of
little syntactic variation (Glass, 2001) or restricted
format of input is allowed. However, this conflicts
with the preference for flexible dialog do achieve
active learning (Moore, 1993).
With regard to interpreting mathematical
texts, (Zinn, 1999) and (Baur, 1999) present DRT
analyzes of course-book proofs. The language in
our dialogs is more informal: natural language and
symbolic mathematical expressions are mixed more
freely, there is a higher degree and more variety
of verbalization, and mathematical objects are not
properly introduced. Both above approaches rely on
typesetting information that identifies mathematical
symbols, formulas, and proof steps, whereas our
input does not contain any such information.
Forcing the user to delimit formulas would not
guarantee a clean separation of the natural language
and the non-linguistic content, while might reduce
the flexibility of the system by making the interface
harder to use.
7 Conclusion and Further Work
In this paper, we reported on the use of deep syn-
tactic and semantic analysis in the interpretation
of mathematical discourse in a dialog setting. We
presented an approach that uses domain-motivated
semantic lexicon to mediate between a domain-
independent representation of linguistic meaning of
utterances and their domain-specific interpretation.
We are incrementally extending the coverage of
the deep analysis components. Our current parser
grammar and upper-level ontology cover most of
the constructions and concepts that occur most fre-
quently in our corpus. The module will be evaluated
as part of the next Wizard-of-Oz experiment.
We are planning to investigate the possibility
of using FrameNet resources developed within the
SALSA project (Erk et al, 2003) at the intermedi-
ate interpretation stage between the linguistic mean-
ing and domain-specific interpretation. Presently,
the semantic lexicon we have constructed encodes,
for instance, a general conceptual relation of CON-
TAINMENT evoked by the verb ?enthalten? (?con-
tain?), with dependents in relations Actor and Pa-
tient, which corresponds to the FrameNet CON-
TAINING domain with frame elements CONTAINER
and CONTENTS. In the course of further work, we
would like to investigate ways of establishing inter-
face between the linguistic meaning TRs and frame
elements, and attempt to use FrameNet to interpret
predicates unknown to our semantic lexicon. Tak-
ing a hypothetical example, if our parser grammar
encoded the meaning of the verb ?beinhalten? (with
the intended meaning contain) in the same linguis-
tic meaning frame as ?enthalten? (contain), while
the sense of ?beinhalten? were not explicitly defined
in the semantic lexicon, we could attempt to inter-
pret it using the FrameNet CONTAINING domain
and the existing lexical semantic entry for ?enthal-
ten?.
References
J. Baldridge. 2002. Lexically Specified Derivational Control
in Combinatory Categorial Grammar. Ph.D. Thesis, Uni-
versity of Edinburgh, Edinburgh.
J. M. Baldridge and G.J. M. Kruijff. 2002. Coupling CCG with
hybrid logic dependency semantics. In Proc. of the 40th An-
nual Meeting of the Association for Computational Linguis-
tics (ACL?02), Philadelphia PA.
J. M. Baldridge and G.J. M. Kruijff. 2003. Multi-modal com-
binatory categorial grammar. In Proc. of the 10th Annual
Meeting of the European Chapter of the Association for
Computational Linguistics (EACL?03), Budapest.
J. Baur. 1999. Syntax und Semantik mathematischer Texte.
Diplomarbeit, Fachrichtung Computerlinguistik, Universit a?t
des Saarlandes, Saarbr u?cken, Germany.
C. Benzm u?ller, A. Fiedler, M. Gabsdil, H. Horacek, I. Kruijff-
Korbayov a?, M. Pinkal, J. Siekmann, D. Tsovaltzi, B. Q. Vo,
and M. Wolska. 2003a. Tutorial dialogs on mathematical
proofs. In Proc. of IJCAI?03 Workshop on Knowledge Rep-
resentation and Automated Reasoning for E-Learning Sys-
tems, Acapulco, Mexico.
C. Benzm u?ller, A. Fiedler, M. Gabsdil, H. Horacek, I. Kruijff-
Korbayov a?, M. Pinkal, J. Siekmann, D. Tsovaltzi, B. Q. Vo,
and M. Wolska. 2003b. A Wizard-of-Oz experiment for tu-
torial dialogues in mathematics. In Proc. of the AIED?03
Workshop on Advanced Technologies for Mathematics Edu-
cation, Sydney, Australia.
K. Erk, A. Kowalski, and M. Pinkal. 2003. A corpus re-
source for lexical semantics. In Proc. of the 5th Interna-
tional Workshop on Computational Semantics, Tilburg, The
Netherlands.
A. Fiedler and M. Gabsdil. 2002. Supporting Progressive Re-
finement of Wizard-of-Oz Experiments. In Proc. of the
ITS?02 Workshop on Empirical Methods for Tutorial Dia-
logue, San Sebastian, Spain.
M. Glass. 2001. Processing language input in the CIRCSIM-
Tutor intelligent tutoring system. In Proc. of the 10th Con-
ference on Artificial Intelligence in Education (AIED?01),
San Antonio.
A. Graesser, P. Wiemer-Hastings, K. Wiemer-Hastings, D. Har-
ter, and N. Person. 2000. Using latent semantic analysis to
evaluate the contributions of students in autotutor. Interac-
tive Learning Environments, 8.
E. Hajic?ov a?, J. Panevov a?, and P. Sgall. 2000. A manual for tec-
togrammatical tagging of the Prague Dependency Treebank.
TR-2000-09, Charles University, Prague, Czech Republic.
H. Horacek and M. Wolska. 2004. Interpreting Semi-Formal
Utterances in Dialogs about Mathematical Proofs. In Proc.
of the 9th International Conference on Application of Nat-
ural Language to Information Systems (NLDB?04), Salford,
Manchester, Springer. To appear.
G.J.M. Kruijff. 2001. A Categorial-Modal Logical Architec-
ture of Informativity: Dependency Grammar Logic & In-
formation Structure. Ph.D. Thesis, Institute of Formal and
Applied Linguistics ( ?UFAL), Faculty of Mathematics and
Physics, Charles University, Prague, Czech Republic.
J. Moore. 1993. What makes human explanations effective?
In Proc. of the 15th Annual Conference of the Cognitive Sci-
ence Society, Hillsdale, NJ.
P. Sgall, E. Hajic?ov a?, and J. Panevov a?. 1986. The meaning of
the sentence in its semantic and pragmatic aspects. Reidel
Publishing Company, Dordrecht, The Netherlands.
Q.B. Vo, C. Benzm u?ller, and S. Autexier. 2003. An approach
to assertion application via generalized resolution. SEKI
Report SR-03-01, Fachrichtung Informatik, Universit a?t des
Saarlandes, Saarbr u?cken, Germany.
M. Wolska and I. Kruijff-Korbayov a?. 2004. Analysis of mixed
natural and symbolic language input in mathematical di-
alogs. In Proc.of the 42nd Meeting of the Association for
Computational Linguistics (ACL), Barcelona, Spain. To ap-
pear.
M. Wolska and I. Kruijff-Korbayov a?. 2004. Building a
dependency-based grammar for parsing informal mathemat-
ical discourse. In Proc. of the 7th International Conference
on Text, Speech and Dialogue (TSD?04), Brno, Czech Re-
public, Springer. To appear.
M. Wolska, B. Q. Vo, D. Tsovaltzi, I. Kruijff-Korbayov a?,
E. Karagjosova, H. Horacek, M. Gabsdil, A. Fiedler,
C. Benzm u?ller, 2004. An annotated corpus of tutorial di-
alogs on mathematical theorem proving. In Proc. of 4th In-
ternational Conference On Language Resources and Evalu-
ation (LREC?04), Lisbon, Portugal. To appear.
C. Zinn. 1999. Understanding mathematical discourse. In
Proc. of the 3rd Workshop on the Semantics and Pragmat-
ics of Dialogue (Amstelogue?99), Amsterdam, The Nether-
lands.
  	
An Experiment Setup for Collecting Data for Adaptive Output Planning
in a Multimodal Dialogue System
Ivana Kruijff-Korbayova?, Nate Blaylock,
Ciprian Gerstenberger, Verena Rieser
Saarland University, Saarbru?cken, Germany
korbay@coli.uni-sb.de
Tilman Becker, Michael Kai?er,
Peter Poller, Jan Schehl
DFKI, Saarbru?cken, Germany
tilman.becker@dfki.de
Abstract
We describe a Wizard-of-Oz experiment setup for
the collection of multimodal interaction data for a
Music Player application. This setup was devel-
oped and used to collect experimental data as part
of a project aimed at building a flexible multimodal
dialogue system which provides an interface to an
MP3 player, combining speech and screen input
and output. Besides the usual goal of WOZ data
collection to get realistic examples of the behav-
ior and expectations of the users, an equally im-
portant goal for us was to observe natural behavior
of multiple wizards in order to guide our system
development. The wizards? responses were there-
fore not constrained by a script. One of the chal-
lenges we had to address was to allow the wizards
to produce varied screen output a in real time. Our
setup includes a preliminary screen output planning
module, which prepares several versions of possi-
ble screen output. The wizards were free to speak,
and/or to select a screen output.
1 Introduction
In the larger context of the TALK project1 we are develop-
ing a multimodal dialogue system for a Music Player appli-
cation for in-car and in-home use, which should support nat-
ural, flexible interaction and collaborative behavior. The sys-
tem functionalities include playback control, manipulation of
playlists, and searching a large MP3 database. We believe
that in order to achieve this goal, the system needs to provide
advanced adaptive multimodal output.
We are conducting Wizard-of-Oz experiments
[Bernsen et al, 1998] in order to guide the development
of our system. On the one hand, the experiments should
give us data on how the potential users interact with such
an application. But we also need data on the multimodal
interaction strategies that the system should employ to
achieve the desired naturalness, flexibility and collaboration.
We therefore need a setup where the wizard has freedom of
1TALK (Talk and Look: Tools for Ambient Linguistic Knowl-
edge; www.talk-project.org) is funded by the EU as project
No. IST-507802 within the 6th Framework program.
choice w.r.t. their response and its realization through single
or multiple modalities. This makes it different from previous
multimodal experiments, e.g., in the SmartKom project
[Tu?rk, 2001], where the wizard(s) followed a strict script.
But what we need is also different in several aspects from
taking recordings of straight human-human interactions: the
wizard does not hear the user?s input directly, but only gets a
transcription, parts of which are sometimes randomly deleted
(in order to approximate imperfect speech recognition);
the user does not hear the wizard?s spoken output directly
either, as the latter is transcribed and re-synthesized (to
produce system-like sounding output). The interactions
should thus more realistically approximate an interaction
with a system, and thereby contain similar phenomena (cf.
[Duran et al, 2001]).
The wizard should be able to present different screen out-
puts in different context, depending on the search results and
other aspects. However, the wizard cannot design screens on
the fly, because that would take too long. Therefore, we de-
veloped a setup which includes modules that support the wiz-
ard by providing automatically calculated screen output op-
tions the wizard can select from if s/he want to present some
screen output.
Outline In this paper we describe our experiment setup and
the first experiences with it. In Section 2 we overview the
research goals that our setup was designed to address. The
actual setup is presented in detail in Section 3. In Section 4
we describe the collected data, and we summarize the lessons
we learnt on the basis of interviewing the experiment partici-
pants. We briefly discuss possible improvements of the setup
and our future plans with the data in Section 5.
2 Goals of the Multimodal Experiment
Our aim was to gather interactions where the wizard can com-
bine spoken and visual feedback, namely, displaying (com-
plete or partial) results of a database search, and the user can
speak or select on the screen.
Multimodal Presentation Strategies The main aim was to
identify strategies for the screen output, and for the multi-
modal output presentation. In particular, we want to learn
Figure 1: Multimodal Wizard-of-Oz data collection setup for
an in-car music player application, using the Lane Change
driving simulator. Top right: User, Top left: Wizard, Bottom:
transcribers.
when and what content is presented (i) verbally, (ii) graphi-
cally or (iii) by some combination of both modes. We expect
that when both modalities are used, they do not convey the
same content or use the same level of granularity. These are
important questions for multimodal fission and for turn plan-
ning in each modality.
We also plan to investigate how the presentation strategies
influence the responses of the user, in particular w.r.t. what
further criteria the user specifies, and how she conveys them.
Multimodal Clarification Strategies The experiments
should also serve to identify potential strategies for multi-
modal clarification behavior and investigate individual strat-
egy performance. The wizards? behavior will give us an ini-
tial model how to react when faced with several sources of
interpretation uncertainty. In particular we are interested in
what medium the wizard chooses for the clarification request,
what kind of grounding level he addresses, and what ?sever-
ity? he indicates. 2 In order to invoke clarification behavior
we introduced uncertainties on several levels, for example,
multiple matches in the database, lexical ambiguities (e.g., ti-
tles that can be interpreted denoting a song or an album), and
errors on the acoustic level. To simulate non-understanding
on the acoustic level we corrupted some of the user utterances
by randomly deleting parts of them.
3 Experiment Setup
We describe here some of the details of the experiment. The
experimental setup is shown schematically in Figure 1. There
are five people involved in each session of the experiment: an
experiment leader, two transcribers, a user and a wizard.
The wizards play the role of an MP3 player application
and are given access to a database of information (but not
actual music) of more than 150,000 music albums (almost 1
2Severity describes the number of hypotheses indicated by the
wizard: having no interpretation, an uncertain interpretation, or sev-
eral ambiguous interpretations.
Figure 2: Screenshot from the FreeDB-based database appli-
cation, as seen by the wizard. First-level of choice what to
display.
million songs), extracted from the FreeDB database.3 Fig-
ure 2 shows an example screen shot of the music database
as it is presented to the wizard. Subjects are given a set of
predefined tasks and are told to accomplish them by using
an MP3 player with a multimodal interface. Tasks include
playing songs/albums and building playlists, where the sub-
ject is given varying amounts of information to help them
find/decide on which song to play or add to the playlist. In
a part of the session the users also get a primary driving task,
using a Lane Change driving simulator [Mattes, 2003]. This
enabled us to test the viability of combining primary and sec-
ondary task in our experiment setup. We also aimed to gain
initial insight regarding the difference in interaction flow un-
der such conditions, particularly with regard to multimodal-
ity.
The wizards can speak freely and display the search result
or the playlist on the screen. The users can also speak as well
as make selections on the screen.
The user?s utterances are immediately transcribed by a typ-
ist and also recorded. The transcription is then presented to
the wizard.4 We did this for two reasons: (1) To deprive
the wizards of information encoded in the intonation of utter-
ances, because our system will not have access to it either. (2)
To be able to corrupt the user input in a controlled way, sim-
ulating understanding problems at the acoustic level. Unlike
[Stuttle et al, 2004], who simulate automatic speech recogni-
tion errors using phone-confusion models, we used a tool that
?deletes? parts of the transcribed utterances, replacing them
by three dots. Word deletion was triggered by the experiment
leader. The word deletion rate varied: 20% of the utterances
got weakly and 20% strongly corrupted. In 60% of the cases
the wizard saw the transcribed speech uncorrupted.
The wizard?s utterances are also transcribed (and recorded)
3Freely available at http://www.freedb.org
4We were not able to use a real speech recognition system, be-
cause we do not have one trained for this domain. This is one of the
purposes the collected data will be used for.
Figure 3: Screenshot from the display presentation tool offer-
ing options for screen output to the wizard for second-level
of choice what to display an how.
and presented to the user via a speech synthesizer. There are
two reasons for doing this: One is to maintain the illusion for
the subjects that they are actually interacting with a system,
since it is known that there are differences between human-
human and human-computer dialogue [Duran et al, 2001],
and we want to elicit behavior in the latter condition; the
other has to do with the fact that synthesized speech is imper-
fect and sometimes difficult to understand, and we wanted to
reproduce this condition.
The transcription is also supported by a typing and spelling
correction module to minimize speech synthesis errors and
thus help maintain the illusion of a working system.
Since it would be impossible for the wizard to construct
layouts for screen output on the fly, he gets support for his
task from the WOZ system: When the wizard performs a
database query, a graphical interface presents him a first level
of output alternatives, as shown in Figure 2. The choices are
found (i) albums, (ii) songs, or (iii) artists. For a second level
of choice, the system automatically computes four possible
screens, as shown in Figure 3. The wizard can chose one of
the offered options to display to the user, or decide to clear
the user?s screen. Otherwise, the user?s screen remains un-
changed. It is therefore up to the wizard to decide whether
to use speech only, display only, or to combine speech and
display.
The types of screen output are (i) a simple text-message
conveying how many results were found, (ii) output of a list
of just the names (of albums, songs or artists) with the cor-
responding number of matches (for songs) or length (for al-
bums), (iii) a table of the complete search results, and (iv) a
table of the complete search results, but only displaying a sub-
set of columns. For each screen output type, the system uses
heuristics based on the search to decide, e.g., which columns
should be displayed. These four screens are presented to the
wizard in different quadrants on a monitor (cf. Figure 3),
allowing for selection with a simple mouse click. The heuris-
tics for the decision what to display implement preliminary
strategies we designed for our system. We are aware that due
to the use of these heuristics, the wizard?s output realization
may not be always ideal. We have collected feedback from
both the wizards and the users in order to evaluate whether
the output options were satisfactory (cf. Section 4 for more
details).
Technical Setup To keep our experimental system modu-
lar and flexible we implemented it on the basis of the Open
Agent Architecture (OAA) [Martin et al, 1999], which is a
framework for integrating a community of software agents in
a distributed environment. Each system module is encapsu-
lated by an OAA wrapper to form an OAA agent, which is
able to communicate with the OAA community. The exper-
imental system consists of 12 agents, all of them written in
Java. We made use of an OAA monitor agent which comes
with the current OAA distribution to trace all communication
events within the system for logging purposes.
The setup ran distributed over six PCs running different
versions of Windows and Linux.5
4 Collected Data and Experience
The SAMMIE-26 corpus collected in this experiment contains
data from 24 different subjects, who each participated in one
session with one of our six wizards. Each subject worked on
four tasks, first two without driving and then two with driving.
The duration was restricted to twice 15 minutes. Tasks were
of two types: searching for a title either in the database or in
an existing playlist, building a playlist satisfying a number of
constraints. Each of the two sets for each subject contained
one task of each type. The tasks again differed in how specific
information was provided. We aimed to keep the difficulty
level constant across users. The interactions were carried out
in German.7
The data for each session consists of a video and audio
recording and a logfile. Besides the transcriptions of the spo-
ken utterances, a number of other features have been anno-
tated automatically in the log files of the experiment, e.g.,
the wizard?s database query and the number of found results,
the type and form of the presentation screen chosen by the
wizard, etc. The gathered logging information for a single
experiment session consists of the communication events in
chronological order, each marked by a timestamp. Based on
this information, we can recapitulate the number of turns and
the specific times that were necessary to accomplish a user
task. We expect to use this data to analyze correlations be-
5We would like to thank our colleagues from CLT Sprachtech-
nologie http://www.clt-st.de/ for helping us to set up the
laboratory.
6SAMMIE stands for Saarbru?cken Multimodal MP3 Player In-
teraction Experiment. We have so far conducted two series of data-
collection experiments: SAMMIE-1 involved only spoken interaction
(cf. [Kruijff-Korbayova? et al, 2005] for more details), SAMMIE-2 is
the multimodal experiment described in this paper.
7However, most of the titles and artist names in the music
database are in English.
tween queries, numbers of results, and spoken and graphical
presentation strategies.
Whenever the wizard made a clarification request, the
experiment leader invoked a questionnaire window on the
screen, where the wizard had to classify his clarification re-
quest according to the primary source of the understanding
problem. At the end of each task, users were asked to what
extent they believed they accomplished their tasks and how
satisfied they were with the results. Similar to methods used
by [Skantze, 2003] and [Williams and Young, 2004], we plan
to include subjective measures of task completion and cor-
rectness of results in our evaluation matrix, as task descrip-
tions can be interpreted differently by different users.
Each subject was interviewed immediately after the ses-
sion. The wizards were interviewed once the whole experi-
ment was over. The interviews were carried out verbally, fol-
lowing a prepared list of questions. We present below some
of the points gathered through these interviews.
Wizard Interviews All 6 wizards rated the overall under-
standing as good, i.e., that communication completed suc-
cessfully. However, they reported difficulties due to delays in
utterance transmission in both directions, which caused un-
necessary repetitions due to unintended turn overlap.
There were differences in how different wizards rated and
used the different screen output options: The table containing
most of the information about the queried song(s) or album(s)
was rated best and shown most often by some wizards, while
others thought it contained too much information and would
not be clear at first glance for the users and hence they used
it less or never. The screen option containing the least infor-
mation in tabular form, namely only a list of songs/albums
with their length, received complementary judgments: some
of the wizards found it useless because it contained too little
information, and they thus did not use it, and others found it
very useful because it would not confuse the user by present-
ing too much information, and they thus used it frequently.
Finally, the screen containing a text message conveying only
the number of matches, if any, has been hardly used by the
wizards. The differences in the wizards? opinions about what
the users would find useful or not clearly indicate the need
for evaluation of the usefulness of the different screen output
options in particular contexts from the users? view point.
When showing screen output, the most common pattern
used by the wizards was to tell the user what was shown (e.g.,
I?ll show you the songs by Prince), and to display the screen.
Some wizards adapted to the user?s requests: if asked to show
something (e.g., Show me the songs by Prince), they would
show it without verbal comments; but if asked a question
(e.g., What songs by Prince are there? or What did you find?),
they would show the screen output and answer in speech.
Concerning the adaptation of multimodal presentation
strategies w.r.t. whether the user was driving or not, four
of the six wizards reported that they consciously used speech
instead of screen output if possible when the user was driving.
The remaining two wizards did not adapt their strategy.
On the whole, interviewing the wizards brought valuable
information on presentation strategies and the use of modal-
ities, but we expect to gain even more insight after the an-
notation and evaluation of the collected data. Besides ob-
servations about the interaction with the users, the wizards
also gave us various suggestions concerning the software used
in the experiment, e.g., the database interface (e.g., the pos-
sibility to decide between strict search and search for par-
tial matches, and fuzzy search looking for items with similar
spelling when no hits are found), the screen options presenter
(e.g., ordering of columns w.r.t. their order in the database in-
terface, the possibility to highlight some of the listed items),
and the speech synthesis system.
Subject Interviews In order to use the wizards? behavior as
a model for interaction design, we need to evaluate the wiz-
ards? strategies. We used user satisfaction, task experience,
and multi-modal feedback behavior as evaluation metrics.
The 24 experimental subjects were all native speakers of
German with good English skills. They were all students
(equally spread across subject areas), half of them male and
half female, and most of them were between 20 to 30 years
old.
In order to calculate user satisfaction, users were inter-
viewed to evaluate the system?s performance with a user sat-
isfaction survey. The survey probed different aspects of the
users? perception of their interaction with the system. We
asked the users to evaluate a set of five core metrics on a
5-point Likert scale. We followed [Walker et al, 2002] def-
inition of the overall user satisfaction as the sum of text-to-
speech synthesis performance, task ease, user expertise, over-
all difficulty and future use. The mean for user satisfaction
across all dialogues was 15.0 (with a standard derivation of
2.9). 8 A one-way ANOVA for user satisfaction between wiz-
ards (df=5, F=1.52 p=0.05) shows no significant difference
across wizards, meaning that the system performance was
judged to be about equally good for all wizards.
To measure task experience we elicited data on perceived
task success and satisfaction on a 5-point Likert scale after
each task was completed. For all the subjects the final per-
ceived task success was 4.4 and task satisfaction 3.9 across
the 4 tasks each subject had to complete. For task success
as well as for task satisfaction no significant variance across
wizards was detected.
Furthermore the subjects were asked about the employed
multi-modal presentation and clarification strategies.
The clarification strategies employed by the wizards
seemed to be successful: From the subjects? point of view,
mutual understanding was very good and the few misunder-
standings could be easily resolved. Nevertheless, in the case
of disambiguation requests and when grounding an utterance,
subjects ask for more display feedback. It is interesting to
note that subjects judged understanding difficulties on higher
levels of interpretation (especially reference resolution prob-
lems and problems with interpreting the intention) to be more
costly than problems on lower levels of understanding (like
the acoustic understanding). For the clarification strategy this
8[Walker et al, 2002] reported an average user satisfaction of
16.2 for 9 Communicator systems.
implies that the system should engage in clarification at the
lowest level a error was detected.9
Multi-modal presentation strategies were perceived to be
helpful in general, having a mean of 3.1 on a 5-point Lik-
ert scale. However, the subjects reported that too much in-
formation was being displayed especially for the tasks with
driving. 85.7% of the subjects reported that the screen out-
put was sometimes distracting them. 76.2% of the sub-
jects would prefer to more verbal feedback, especially while
driving. On a 3-point Likert scale subjects evaluated the
amount of the information presented verbally to be about
right (mean of 1.8), whereas they found the information pre-
sented on the screen to be too much (mean of 2.3). Stud-
ies by [Bernsen and Dybkjaer, 2001] on the appropriateness
of using verbal vs. graphical feedback for in-car dialogues
indicate that the need for text output is very limited. Some
subjects in that study, as well subjects in our study report that
they would prefer to not have to use the display at all while
driving. On the other hand subjects in our study perceived the
screen output to be very helpful in less stressful driving situa-
tions and when not driving (e.g. for memory assistance, clari-
fications etc.). Especially when they want to verify whether a
complex task was finally completed (e.g. building a playlist),
they ask for a displayed proof. For modality selection in in-
car dialogues the driver?s mental workload on primary and
secondary task has to be carefully evaluated with respect to a
situation model.
With respect to multi-modality subjects also asked for
more personalized data presentation. We therefore need to
develop intelligent ways to reduce the amount of data being
displayed. This could build on prior work on the generation
of ?tailored? responses in spoken dialogue according to a user
model [Moore et al, 2004].
The results for multi-modal feedback behavior showed no
significant variations across wizards except for the general
helpfulness of multi-modal strategies. An ANOVA Planned
Comparison of the wizard with the lowest mean against the
other wizards showed that his behavior was significantly
worse. It is interesting to note, that this wizard was using
the display less than the others. We might consider not to in-
clude the 4 sessions with this wizard in our output generation
model.
We also tried to analyze in more detail how the wizards?
presentation strategies influenced the results. The option
which was chosen most of the time was to present a table
with the search results (78.6%); to present a list was only cho-
sen in 17.5% of the cases and text only 0.04%. The wizards?
choices varied significantly only for presenting the table op-
tion. The wizard who was rated lowest for multimodality was
using the table option less, indicating that this option should
be used more often. This is also supported by the fact that the
show table option is the only presentation strategy which is
positively correlated to how the user evaluated multimodality
(Spearman?s r = 0.436*). We also could find a 2-tailed corre-
9Note that engaging at the lowest level just helps to save dialogue
?costs?. Other studies have shown that user satisfaction is higher
for strategies that would ?hide? the understanding error by asking
questions on higher levels [Skantze, 2003], [Raux et al, 2005]
lation between user satisfaction and multimodality judgment
(Spearman?s r = 0.658**). This indicates the importance of
good multimodal presentation strategies for user satisfaction.
Finally, the subjects were asked for own comments. They
liked to be able to provide vague information, e.g., ask for ?an
oldie?, and were expecting collaborative suggestions. They
also appreciated collaborative proposals based on inferences
made from previous conversations.
In sum, as the measures for user satisfaction, task experi-
ence, and multi-modal feedback strategies, the subjects? judg-
ments show a positive trend. The dialogue strategies em-
ployed by most of the wizards seem to be a good starting
point for building a baseline system. Furthermore, the results
indicate that intelligent multi-modal generation needs to be
adaptive to user and situation models.
5 Conclusions and Future Steps
We have presented an experiment setup that enables us to
gather multimodal interaction data aimed at studying not only
the behavior of the users of the simulated system, but also
that of the wizards. In order to simulate a dialogue system in-
teraction, the wizards were only shown transcriptions of the
user utterances, sometimes corrupted, to simulate automatic
speech recognition problems. The wizard?s utterances were
also transcribed and presented to the user through a speech
synthesizer. In order to make it possible for the wizards to
produce contextually varied screen output in real time, we
have included a screen output planning module which auto-
matically calculated several screen output versions every time
the wizard ran a database query. The wizards were free to
speak and/or display screen output. The users were free to
speak or select on the screen. In a part of each session, the
user was occupied by a primary driving task.
The main challenge for an experiment setup as described
here is the considerable delay between user input and wizard
response. This is due partly to the transcription and spelling
correction step and partly due to the time it takes the wizard to
decide on and enter a query to the database, then select a pre-
sentation and in parallel speak to the user. We have yet to ana-
lyze the exact distribution of time needed for these tasks. Sev-
eral ways can be chosen to speed up the process. Transcrip-
tion can be eliminated either by using speech recognition and
dealing with its errors, or instead applying signal processing
software, e.g., to filter out prosodic information from the user
utterance and/or to transform the wizard?s utterance into syn-
thetically sounding speech (e.g., using a vocoder). Database
search can be sped up in a number of ways too, ranging from
allowing selection directly from the transcribed text to auto-
matically preparing default searches by analyzing the user?s
utterance. Note, however, that the latter will most likely prej-
udice the wizard to stick to the proposed search.
We plan to annotate the corpus, most importantly w.r.t.
wizard presentation strategies and context features relevant
for the choice between them. We also plan to compare the
presentation strategies to the strategies in speech-only mode,
for which we collected data in an earlier experiment (cf.
[Kruijff-Korbayova? et al, 2005]).
For clarification strategies previous studies already showed
that the decision process needs to be highly dynamic by tak-
ing into account various features such as interpretation uncer-
tainties and local utility [Paek and Horvitz, 2000]. We plan
to use the wizard data to learn an initial multi-modal clarifi-
cation policy and later on apply reinforcement learning meth-
ods to the problem in order to account for long-term dialogue
goals, such as task success and user satisfaction.
The screen output options used in the experiment will also
be employed in the baseline system we are currently imple-
menting. The challenges involved there are to decide (i) when
to produce screen output, (ii) what (and how) to display and
(iii) what the corresponding speech output should be. We will
analyze the corpus in order to determine what the suitable
strategies are.
References
[Bernsen and Dybkjaer, 2001] Niels Ole Bernsen and Laila
Dybkjaer. Exploring natural interaction in the car. In
CLASS Workshop on Natural Interactivity and Intelligent
Interactive Information Representation, 2001.
[Bernsen et al, 1998] N. O. Bernsen, H. Dybkj?r, and
L. Dybkj?r. Designing Interactive Speech Systems ?
From First Ideas to User Testing. Springer, 1998.
[Duran et al, 2001] Christine Duran, John Aberdeen, Laurie
Damianos, and Lynette Hirschman. Comparing several as-
pects of human-computer and human-human dialogues. In
Proceedings of the 2nd SIGDIAL Workshop on Discourse
and Dialogue, Aalborg, 1-2 September 2001, pages 48?57,
2001.
[Kruijff-Korbayova? et al, 2005] Ivana Kruijff-Korbayova?,
Tilman Becker, Nate Blaylock, Ciprian Gerstenberger,
Michael Kai?er, Peter Poler, Jan Schehl, and Verena
Rieser. Presentation strategies for flexible multimodal
interaction with a music player. In Proceedings of
DIALOR?05 (The 9th workshop on the semantics and
pragmatics of dialogue (SEMDIAL), 2005.
[Martin et al, 1999] D. L. Martin, A. J. Cheyer, and D. B.
Moran. The open agent architecture: A framework for
building distributed software systems. Applied Artificial
Intelligence: An International Journal, 13(1?2):91?128,
Jan?Mar 1999.
[Mattes, 2003] Stefan Mattes. The lane-change-task as a tool
for driver distraction evaluation. In Proceedings of IGfA,
2003.
[Moore et al, 2004] Johanna D. Moore, Mary Ellen Foster,
Oliver Lemon, and Michael White. Generating tailored,
comparative descriptions in spoken dialogue. In Proceed-
ings of the Seventeenth International Florida Artificial In-
telligence Research Sociey Conference, AAAI Press, 2004.
[Paek and Horvitz, 2000] Tim Paek and Eric Horvitz. Con-
versation as action under uncertainty. In Proceedings of
the Sixteenth Conference on Uncertainty in Artificial In-
telligence, 2000.
[Raux et al, 2005] Antoine Raux, Brian Langner, Dan Bo-
hus, Allan W. Black, and Maxine Eskenazi. Let?s go pub-
lic! taking a spoken dialog system to the real world. 2005.
[Skantze, 2003] Gabriel Skantze. Exploring human error
handling strategies: Implications for spoken dialogue sys-
tems. In Proceedings of the ISCA Tutorial and Research
Workshop on Error Handling in Spoken Dialogue Systems,
2003.
[Stuttle et al, 2004] Matthew Stuttle, Jason Williams, and
Steve Young. A framework for dialogue data collection
with a simulated asr channel. In Proceedings of the IC-
SLP, 2004.
[Tu?rk, 2001] Ulrich Tu?rk. The technical processing in
smartkom data collection: a case study. In Proceedings
of Eurospeech2001, Aalborg, Denmark, 2001.
[Walker et al, 2002] Marylin Walker, R. Passonneau, J. Ab-
erdeen, J. Boland, E. Bratt, J. Garofolo, L. Hirschman,
A. Le, S. Lee, S. Narayanan, K. Papineni, B. Pellom,
J. Polifroni, A. Potamianos, P. Prabhu, A. Rudnicky,
G. Sandersa, S. Seneff, D. Stallard, and S. Whittaker.
Cross-site evaluation in darpa communicator: The june
2000 data collection. 2002.
[Williams and Young, 2004] Jason D. Williams and Steve
Young. Characterizing task-oriented dialog using a sim-
ulated asr channel. In Proceedings of the ICSLP, 2004.
The SAMMIE Multimodal Dialogue Corpus Meets the Nite XML Toolkit
Ivana Kruijff-Korbayova?, Verena Rieser,
Ciprian Gerstenberger
Saarland University, Saarbru?cken, Germany
vrieser@coli.uni-sb.de
Jan Schehl, Tilman Becker
DFKI, Saarbru?cken, Germany
jan.schehl@dfki.de
Abstract
We demonstrate work in progress1 us-
ing the Nite XML Toolkit on a cor-
pus of multimodal dialogues with an
MP3 player collected in a Wizard-of-Oz
(WOZ) experiments and annotated with
a rich feature set at several layers. We
designed an NXT data model, converted
experiment log file data and manual tran-
scriptions into NXT, and are building an-
notation tools using NXT libraries.
1 Introduction
In the TALK project2 we are developing a mul-
timodal dialogue system for an MP3 application
for in-car and in-home use. The system should
support natural, flexible interaction and collabo-
rative behavior. To achieve this, it needs to pro-
vide advanced adaptive multimodal output.
To determine the interaction strategies and
range of linguistic behavior naturally occurring
in this scenario, we conducted two WOZ exper-
iments: SAMMIE-1 involved only spoken inter-
action, SAMMIE-2 was multimodal, with speech
and screen input and output.3
We have been annotating the corpus on sev-
eral layers, representing linguistic, multimodal
and context information. The annotated corpus
will be used (i) to investigate various aspects of
1Our demonstration results from the efforts of a larger
team including also N. Blaylock, B. Fromkorth, M. Gra?c,
M. Kai?er, A. Moos, P. Poller and M. Wirth.
2TALK (Talk and Look: Tools for Ambient Linguis-
tic Knowledge; http://www.talk-project.org), funded by the
EU 6th Framework Program, project No. IST-507802.
3SAMMIE stands for Saarbru?cken Multimodal MP3
Player Interaction Experiment.
multimodal presentation and interaction strate-
gies both within and across the annotation lay-
ers; (ii) to design an initial policy for reinforce-
ment learning of multimodal clarifications.4 We
use the Nite XML Toolkit (NXT) (Carletta et al,
2003) to represent and browse the data and to de-
velop annotation tools.
Below we briefly describe our experiment
setup, the collected data and the annotation lay-
ers; we comment on methods and tools for data
representation and annotation, and then present
our NXT data model.
2 Experiment Setup
24 subjects in SAMMIE-1 and 35 in SAMMIE-2
performed several tasks with an MP3 player ap-
plication simulated by a wizard. For SAMMIE-
1 we had two, for SAMMIE-2 six wizards. The
tasks involved searching for titles and building
playlists satisfying various constraints. Each ses-
sion was 30 minutes long. Both users and wiz-
ards could speak freely. The interactions were
in German (although most of the titles and artist
names in the database were English).
SAMMIE-2 had a more complex setup. The
tasks the subjects had to fulfill were divided in
two classes: with vs. without operating a driv-
ing simulator. When presenting the search re-
sults, the wizards were free to produce mono-
or multimodal output as they saw fit; they could
speak freely and/or select one of four automati-
cally generated screen outputs, which contained
tables and lists of found songs/albums. The
users also had free choice between unconstrained
4See (Kruijff-Korbayova? et al, 2006) for more details
about the annotation goals and further usage of the corpus.
69
natural language and/or selecting items on the
screen. Both wizard and user utterances were im-
mediately transcribed. The wizard?s utterances
were presented to the user via a speech synthe-
sizer. To simulate acoustic understanding prob-
lems, the wizard sometimes received only part
of the transcribed user?s utterance, to elicit CRs.
(See (Kruijff-Korbayova? et al, 2005) for details.)
3 Collected Data
The SAMMIE-2 data for each session consists of
a video and audio recording and a log file.5 The
gathered logging information per session con-
sists of Open Agent Architecture (Martin et al,
1999) (OAA) messages in chronological order,
each marked by a timestamp. The log files con-
tain various information, e.g., the transcriptions
of the spoken utterances, the wizard?s database
query and the number of results, the screen op-
tion chosen by the wizard, classification of clari-
fication requests (CRs), etc.
4 Annotation Methods and Tools
The rich set of features we are interested in nat-
urally gives rise to a multi-layered view of the
corpus, where each layer is to be annotated inde-
pendently, but subsequent investigations involve
exploration and automatic processing of the inte-
grated data across layers.
There are two crucial technical requirements
that must be satisfied to make this possible: (i)
stand-off annotation at each layer and (ii) align-
ment of base data across layers. Without the for-
mer, we could not keep the layers separate, with-
out the latter we would not be able to align the
separate layers. An additional equally important
requirement is that elements at different layers
of annotation should be allowed to have overlap-
ping spans; this is crucial because, e.g., prosodic
units and syntactic phrases need not coincide.
Among the existing toolkits that support
multi-layer annotation, it was decided to use
NXT (Carletta et al, 2003)6 in the TALK
project. The NXT-based SAMMIE-2 corpus we
5For 19 sessions the full set of data files exists.
6http://www.ltg.ed.ac.uk/NITE/
are demonstrating has been created in several
steps: (1) The speech data was manually tran-
scribed using the Transcriber tool.7 (2) We auto-
matically extracted features at various annotation
layers by parsing the OAA messages in the log
files. (3) We automatically converted the tran-
scriptions and the information from the log files
into our NXT-based data representation format;
features annotated in the transcriptions and fea-
tures automatically extracted from the log files
were assigned to elements at the appropriate lay-
ers of representation in this step.
Manual annotation: We use tools specifi-
cally designed to support the particular annota-
tion tasks. We describe them below.
As already mentioned, we used Transcriber for
the manual transcriptions. We also performed
certain relatively simple annotations directly on
the transcriptions and coded them in-line by us-
ing special notation. This includes the identifica-
tion of self-speech, the identification of expres-
sions referring to domain objects (e.g., songs,
artists and albums) and the identification of utter-
ances that convey the results of database queries.
For other manual annotation tasks (the annota-
tion of CRs, task segmentation and completion,
referring expressions and the relations between
them) we have been building specialized tools
based on the NXT library of routines for build-
ing displays and interfaces based on Java Swing
(Carletta et al, 2003). Although NXT comes
with a number of example applications, these are
tightly coupled with the architecture of the cor-
pora they were built for. We therefore developed
a core basic tool for our own corpus; we mod-
ify this tool to suite each annotation task. To fa-
cilitate tool development, NXT provides GUI el-
ements linked directly to corpora elements and
support for handling complex multi-layer cor-
pora. This proved very helpful.
Figure 4 shows a screenshot of our CR anno-
tation tool. It allows one to select an utterance
in the left-hand side of the display by clicking
on it, and then choose the attribute values from
the pop-down lists on the right-hand side. Cre-
7http://trans.sourceforge.net/
70
ating relations between elements and creating el-
ements on top of other elements (e.g., words or
utterances) are extensions we are currently im-
plementing (and will complete by the time of the
workshop). First experiences using the tool to
identify CRs are promising.8 When demonstrat-
ing the system we will report the reliability of
other manual annotation tasks.
Automatic annotation using indexing: NXT
also provides a facility for automatic annotation
based on NiteQL query matches (Carletta et al,
2003). Some of our features, e.g., the dialogue
history ones, can be easily derived via queries.
5 The SAMMIE NXT Data Model
NXT uses a stand-off XML data format that con-
sist of several XML files that point to each other.
The NXT data model is a multi-rooted tree with
arbitrary graph structure. Each node has one set
of children, and can have multiple parents.
Our corpus consists of the following layers.
Two base layers: words and graphical output
events; both are time-aligned. On top of these,
structural layers correspond to one session per
subject, divided into task sections, which con-
sist of turns, and these consist of individual ut-
terances, containing words. Graphical output
events will be linked to turns at a featural layer.
Further structural layers are defined for CRs
and dialogue acts (units are utterances), domain
objects and discourse entities (units are expres-
sions consisting of words). We keep independent
layers of annotation separate, even when they can
in principle be merged into a single hierarchy.
Figure 2 shows a screenshot made with Ami-
gram (Lauer et al, 2005), a generic tool for
browsing and searching NXT data. On the left-
hand side one can see the dependencies between
the layers. The elements at the respective layers
are displayed on the right-hand side.
Below we indicate the features per layer:
? Words: Time-stamped words and other
sounds; we mark self-speech, pronuncia-
tion, deletion status, lemma and POS.
8Inter-annotator agreement of 0.788 (? corrected for
prevalence).
? Graphical output: The type and amount of
information displayed, the option selected
by the wizard, and the user?s choices.
? Utterances: Error rates due to word dele-
tion, and various features describing the
syntactic structure, e.g., mood, polarity,
diathesis, complexity and taxis, the pres-
ence of marked syntactic constructions such
as ellipsis, fronting, extraposition, cleft, etc.
? Turns: Time delay, dialogue duration so
far, and other dialogue history features, i.e.
values which accumulate over time.
? Domain objects and discourse entities:
Properties of referring expressions reflect-
ing the type and information status of dis-
course entities, and coreference/bridging
links between them.
? Dialogue acts: DAs based on an agent-
based approach to dialogue as collaborative
problem-solving (Blaylock et al, 2003),
e.g., determining joint objectives, find-
ing and instantiating recipes to accomplish
them, executing recipes and monitoring for
success. We also annotate propositional
content and the database queries.
? CRs: Additional features including the
source and degree of uncertainty, and char-
acteristics of the CRs strategy.
? Tasks: A set of features for estimating user
satisfaction online for reinforcement learn-
ing (Rieser et al, 2005).
? Session: Subject and wizard information,
user questionnaire aswers, and accumulat-
ing attribute values from other layers.
6 Summary
We described a multi-layered corpus of multi-
modal dialogues represented and annotated us-
ing NXT-based tools. Our data model relates lin-
guistic and graphical realization to a rich set of
context features and represents structural, hierar-
chical interactions between different annotation
layers. We combined different annotation meth-
ods to construct the corpus. Manual annotation
and annotation evaluation is on-going. The cor-
pus will be used (i) investigate multimodal pre-
sentation and interaction strategies with respect
71
Figure 1: NXT-based tool for annotating CRs
Figure 2: SAMMIE-2 corpus displayed in Amigram
to dialogue context and (ii) to design an initial
policy for reinforcement learning of multimodal
clarification strategies.
References
[Blaylock et al2003] N. Blaylock, J. Allen, and G. Fergu-
son. 2003. Managing communicative intentions with
collaborative problem solving. In Current and New
Directions in Discourse and Dialogue, pages 63?84.
Kluwer, Dordrecht.
[Carletta et al2003] J. Carletta, S. Evert, U. Heid, J. Kil-
gour, J. Robertson, and H. Voormann. 2003. The NITE
XML Toolkit: flexible annotation for multi-modal lan-
guage data. Behavior Research Methods, Instruments,
and Computers, special issue on Measuring Behavior.
Submitted.
[Kruijff-Korbayova? et al2005] I. Kruijff-Korbayova?,
T. Becker, N. Blaylock, C. Gerstenberger, M. Kai?er,
P. Poller, J. Schehl, and V. Rieser. 2005. An experiment
setup for collecting data for adaptive output planning in
a multimodal dialogue system. In Proc. of ENLG.
[Kruijff-Korbayova? et al2006] I. Kruijff-Korbayova?,
T. Becker, N. Blaylock, C. Gerstenberger, M. Kai?er,
P. Poller, V. Rieser, and J. Schehl. 2006. The SAMMIE
corpus of multimodal dialogues with an mp3 player. In
Proc. of LREC (to appear).
[Lauer et al2005] C. Lauer, J. Frey, B. Lang, T. Becker,
T. Kleinbauer, and J. Alexandersson. 2005. Amigram
- a general-purpose tool for multimodal corpus annota-
tion. In Proc. of MLMI.
[Martin et al1999] D. L. Martin, A. J. Cheyer, and D. B.
Moran. 1999. The open agent architecture: A frame-
work for building distributed software systems. Applied
Artificial Intelligence: An International Journal, 13(1?
2):91?128, Jan?Mar.
[Rieser et al2005] V. Rieser, I. Kruijff-Korbayova?, and
O. Lemon. 2005. A corpus collection and annotation
framework for learning multimodal clarification strate-
gies. In Proc. of SIGdial.
72
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 190?197,
Columbus, June 2008. c?2008 Association for Computational Linguistics
The Effect of Dialogue System Output Style Variation
on Users? Evaluation Judgments and Input Style
Ivana Kruijff-Korbayova? and Olga Kukina
Department of Computational Linguistics
Saarland University, Germany
{korbay|olgak}@coli.uni-sb.de
Abstract
A dialogue system can present itself and/or
address the user as an active agent by means
of linguistic constructions in personal style, or
suppress agentivity by using impersonal style.
We compare system evaluation judgments and
input style alignment of users interacting with
an in-car dialogue system generating output in
personal vs. impersonal style. Although our
results are consistent with earlier findings ob-
tained with simulated systems, the effects are
weaker.
1 Introduction
One of the goals in developing dialogue systems that
users find appealing and natural is to endow the sys-
tems with natural and contextually appropriate out-
put. This encompasses a broad range of research
issues. The one we address in this paper pertains
to style in the interpersonal dimension: does using
personal vs. impersonal style of system output have
an effect on dialogue system users, in particular, on
their judgments about the system and on the way
they formulate their input to the system?
We define the personal/impersonal style di-
chotomy as reflecting primarily a distinction with
respect to agentivity: personal style involves the ex-
plicit realization of an agent, whereas impersonal
style avoids it. In the simplest way it is manifested
by the presence of explicit reference to the dialogue
participants (typically by means of personal pro-
nouns) vs. its absence, respectively. More generally,
active voice and finite verb forms are typical for per-
sonal style, whereas impersonal style often, though
not exclusively, employs passive constructions or in-
finite verb forms:
(1) Typical personal style constructions:
a. I found 20 albums.
b. You have 20 albums.
c. Please search for albums by The Beatles.
(2) Typical impersonal style constructions:
a. 20 albums have been found.
b. There are 20 albums.
c. The database contains 20 albums.
d. 20 albums found.
The designer of a dialogue system has the choice
to make it manifest (its own and the user?s) agen-
tivity linguistically through the use of personal con-
structions or not.
Previous experiments with simulated systems
have shown that a natural language interface with
a synthesized voice should not say ?I? (Nass and
Brave, 2005) and that users align the style of their
input to that of the system output (Brennan and
Ohaeri, 1994). (See Section 2 for more detail.)
The dialogue system SAMMIE developed in the
TALK project (Becker et al, 2007) can use either per-
sonal or impersonal output style. In personal style, it
generates constructions making explicit reference to
the agent (both the user and the system itself), such
as (1a?1c); in impersonal style, it avoids explicit ref-
erence to any agent, as in (2a?2d). The system can
be set either to use one style consistently throughout
a dialogue session, or to align to the user?s style, i.e.,
mimic the user?s style on a turn-by-turn basis.
Inspired by the earlier results obtained with sim-
ulated systems (Nass and Brave, 2005; Brennan and
190
Ohaeri, 1994), we ran an experiment to test the ef-
fects of style manipulation in the SAMMIE system.
In this paper, we compare two versions of the sys-
tem, one using consistently the personal output style
and the other the impersonal style. We designed
our experiment to test (i) whether the users? judg-
ments of the system?s usability and performance dif-
fer among the system versions using the personal vs.
impersonal style, and (ii) whether users align to the
system style.
In Section 2 we review previous experiments con-
cerning the effect of system output style on users?
judgments and style. We describe our own experi-
ment in Section 3, present the results in Section 4,
and provide a discussion and conclusions in Sec-
tion 5.
2 Previous Work
(Nass and Brave, 2005) address the issue whether a
voice interface should say ?I? by investigating sev-
eral dimensions of user attitudes to their simulated
system with a synthetic vs. recorded voice. Gen-
erally, agents that use ?I? are perceived more like
a person than those that do not. However, systems
tend to be more positively rated when consistent
with respect to such parameters as personality, gen-
der, ontology (human vs. machine), etc. A system
with a recorded voice is perceived as more human-
like and thus entitled to the use of ?I?, whereas a
synthetic-voice interface is not perceived as human
enough to use ?I? to refer to itself (Nass et al, 2006).
Another question is whether system output style
influences users? input formulation, as would be ex-
pected due to the phenomenon of alignment, which
is generally considered a basic principle in natural
language dialogue (Garrod and Pickering, 2004).1
Experiments targeting human-human conversa-
tion show that in spite of the variety of linguistic
expressions available, speakers in spontaneous dia-
logues tend to express themselves in similar ways at
lexical and syntactic levels. For example, the sur-
face form of a question can affect the format of the
answer: the question ?What time do you close?? will
more likely get the response ?Five o?clock? than?At
1This dialogue phenomenon goes under a variety of terms in
the literature, besides alignment, e.g., accommodation, adapta-
tion, convergence, entrainment or shaping (used, e.g., by (Bren-
nan and Ohaeri, 1994)).
five o?clock?. On the other hand, ?At five o?clock?
is a more probable answer to ?At what time do you
close?? (Levelt and Kelter, 1982). There is evi-
dence that alignment happens automatically as a re-
sult of priming, e.g., (Hadelich et al, 2004) for lexi-
cal alignment.
Lexical and syntactic alignment is present in
human-computer interaction, too. (Brennan, 1996)
suggested that users adopt system?s terms to avoid
errors, expecting the system to be inflexible. How-
ever, recent experiments show that alignment in
human-computer interaction is also automatic and
its strength is comparable to that in human-human
communication (Branigan et al, 2003; Pearson et
al., 2006).
Early results concerning users? alignment to sys-
tem output style in the interpersonal dimension are
reported in (Brennan and Ohaeri, 1994): They dis-
tinguish three styles: anthropomorphic (the system
refers to itself using first person pronouns, like in
(1a) above, fluent (complete sentences, but no self-
reference) and telegraphic, like (2d). They found no
difference in users? perception of the system?s in-
telligence across the different conditions. However,
they observed that the anthropomorphic group was
more than twice as likely to refer to the computer
using the second person pronoun ?you? and it used
more indirect requests and conventional politeness
then the other groups. They concluded that the an-
thropomorphic style is undesirable for dialogue sys-
tems because it encourages more complex user input
which is harder to recognize and interpret.
The described experiments used either the
Wizard-of-Oz paradigm (Brennan, 1996) or prepro-
grammed system output (Branigan et al, 2003; Nass
and Brave, 2005) and involved written communica-
tion. Such methods allow one to test assumptions
about idealized human-computer interaction. The
purpose of our experiment was to test whether sim-
ilar effects arise in an interaction with an actual di-
alogue system, which may be plagued, among other
factors, by speech recognition problems.
3 Experiment
Dialogue System We used the SAMMIE in-car sys-
tem developed in the TALK project (Becker et al,
2006; Becker et al, 2007). SAMMIE provides a mul-
191
timodal interface to an MP3 player through speech
and haptic input with a button which can be turned,
pushed down and pushed sideways in four direc-
tions. System output is by speech and a graphical
display. The user can perform a range of tasks: con-
trol the MP3 player (play/stop/pause playing song,
next/previous/go-to track, turn shuffle mode on/off),
search and browse by looking for various fields in
the MP3 database (song, artist, album, etc.), search
and select playlists, edit them or construct new ones.
The SAMMIE system was designed with the aim
to support natural, intuitive mixed-initiative interac-
tion. Input can be given through any modality at
any point and is not restricted to answers to sys-
tem queries: the user can initiate new tasks as well
as give any information relevant to the current task
at any time. A sample interaction is shown below
(Becker et al, 2006).
(3) U: Show me the Beatles albums.
S: I have these four Beatles albums. [shows a list
of album names]
U: Which songs are on this one? [selects the Red
Album]
S: The Red Album contains these songs [shows a
list of the songs]
U: Play the third one.
S: [song ?From Me To You? plays]
The SAMMIE system has a German and an En-
glish version which both provide the same function-
ality. The experiment employed the German ver-
sion. See (Kruijff-Korbayova? et al, 2008) for a de-
scription of the natural language generation module.
Setup Figure 1 shows a picture of the experiment
setup. To simulate the driving situation, we used
the ?3D-Fahrschule? software.2 The driving simu-
lator visuals were projected on a wall-sized back-
projection screen. The graphical interface of the
SAMMIE system was shown on a display next to the
steering wheel. Participants wore headphones with
a microphone for the spoken input and output. The
button for manual input was positioned to the right
of their chair. The experimenter was sitting in an ad-
jacent room and could see and hear everything hap-
pening in the experiment lab. The subjects could not
2http://www.3d-fahrschule.de/index.htm
Figure 1: Experiment setup
see the experimenter, but heard her instructions, in-
cluding the task assignments, from loudspeakers. If
necessary, the subjects were able to talk to the ex-
perimenter.
Participants A total of 28 participants were paid
to take part in the experiment. All were native Ger-
man speakers, 22 female and 6 male, 22 students of
the Saarland University and 6 employees. All but
two participants had a driver?s license and 20 partic-
ipants reported driving more than 500km a year. 10
participants had previous experience with a driving
simulation and 6 had used a dialogue system before.
Each participant was assigned to one style condition,
14 to personal and 14 to impersonal style. To ensure
as even a distribution as possible, there were 11 fe-
male and 3 male participants in each style condition,
one of whom was a non-driver. There were 4 em-
ployees in impersonal style condition and 2 in the
personal one.
Procedure Each participant was welcomed by the
experimenter, seated in the experiment lab, and
given brief written instructions concerning the driv-
ing simulator, the SAMMIE system and the evalua-
tion procedure. The participants were instructed to
use mainly spoken input to accomplish the tasks, al-
though they were allowed to use manual input, too.
The participants first made a ca. 2-minute drive
to get familiar with the driving simulator. Then they
were asked to chose a destination city (Amsterdam,
Madrid or London) and drive there on a highway.
During the driving, the experimenter successively
192
read to the participant 2 trial tasks and 11 experi-
mental tasks to be solved using the SAMMIE system.
The tasks involved exploring the contents of a
database of about 25 music albums and were of four
types: (1) finding some specified title(s); (2) select-
ing some title(s) satisfying certain constraints; (3)
manipulating the playlists by adding or removing
songs and (4) free-use of the system.
The experimental tasks were presented to each
participant in randomized order apart from the free
use of the system, which was always the last task.
To avoid priming by the style of the task formula-
tion, and to help the participants memorize the task,
the experimenter (E) repeated each task assignment
twice to the participant, once in personal and once
in impersonal style, as shown in the example below.
(4) E: Bitte frage das System nach den Liedern von
?Pur?. Du willst also wissen welche Lieder von
?Pur? es gibt.
E: Please ask the the system about the songs by
?Pur?. You would like to know which songs by
?Pur? there are.
The time the participants spent completing the in-
dividual tasks was not constrained. It took them
about 40 minutes to complete all the tasks.
Afterwards, each participant was asked to fill in a
questionnaire about their attitudes towards the sys-
tem, consisting of questions with a 6-point scale
ranging from 1 (low grade) to 6 (high grade). The
questions were a subset of those used in (Nass and
Brave, 2005) and (Mutschler et al, 2007), for ex-
ample: How do you assess the system in general:
technical (1) ? human-like (6); Communication with
the system seemed to you: boring (1) ? exciting (6);
In terms of usability, the system is: inefficient (1)
?efficient(6).
Upon completing the questionnaire, the partici-
pant was paid and discharged.
Collected data The questionnaire responses have
been tabulated and the dialogues of the subjects with
the system have been recorded and transcribed.3
The utterances of the participants (on average 95
per session) were subsequently manually anno-
tated with the following features for further analysis:
3We did not record the data from the driving simulator.
? Construction type:
Personal (+/-) Is the utterance a complete sen-
tence in active voice or imperative form
Impersonal (+/-) Is the utterance expressed
by passive voice, infinite verb form (e.g.,
?Lied abspielen? (lit. ?song play?)), or ex-
pletive ?es-gibt? (?there-is?) construction
Telegraphic (+/-) Is the utterance expressed
by a phrase, e.g., ?weiter? (?next?)
? Personal pronouns: (+/-) Does the utterance
contain a first or second person pronoun
? Politeness marking: (+/-) Does the utterance
contain a politeness marker, such as ?bitte?
(?please?), ?danke? (?thanks?) and verbs in
subjunctive mood (eg. ?ich ha?tte gerne?)
4 Results
4.1 Style and Users? Attitudes
The first issue addressed in the experiment was
whether the users have different judgments of the
personal vs. impersonal version of the system. Since
the system used a synthetic voice, the judgments
were expected to be more positive in the impersonal
style condition (Nass and Brave, 2005). Based on
factor analysis performed on attitudinal data from
the user questionnaires we created the six indices
listed below. All indices were meaningful and re-
liable
1. General satisfaction with the communication
with the system was composed of 3 pairs of
adjectives describing communication with the
system: disappointing/motivating, uninterest-
ing/interesting and boring/exciting (Cronbach?s
?=0.86; t(26)=0.29, p=0.39 (one-tailed))
2. Ease of communication with the system com-
prised 5 parameters: naturalness of the commu-
nication with the system, formality/informality
and indifference/sympathy of the system?s
communicative style, participants feelings dur-
ing the conversation: tensed/relaxed and pleas-
ant/unpleasant (?=0.83; t(26)=0.00, p=0.5
(one-tailed))
3. Usability of the system consisted of 1
pair of adjectives referring to the success
193
Figure 2: Perceived humanness of the system depending
on system output style
of communication with the system: un-
successful/successful, and 4 pairs of adjec-
tives describing the usability of the sys-
tem: unpractical/practical, inefficient/efficient,
complicated/simple, inconvenient/convenient
(?=0.76; t(26)=0.08, p=0.47 (one-tailed))
4. Clarity of the system?s speech comprised 2
pairs of adjectives describing the system?s
speech: unpredictable/predictable and confus-
ing/clear (?=0.88; t(25)=0.87, p=0.2 (one-
tailed))
5. Perceived ?humanness? of the system was
composed of 3 parameters: perceived tech-
nicality/humanness, perceived unfriend-
liness/friendliness and attributed conser-
vatism/innovation (?=0.69; t(25)=1.64, p=0.06
(one-tailed))
6. System?s perceived flexibility and creativity
comprised 3 parameters: rigidness/flexibility
of system?s speech, perceived creativity of the
system and intelligence attributed to the system
(?=0.78; t(26)=0.40, p=0.35 (one-tailed))
We did not find any significant influence of sys-
tem output style on users? attitudes. The only in-
dex with a weak tendency in the predicted direction
is perceived humanness of the system (t(25)=1.64,
p=.06 (one-tailed); see Figure 2). This goes in line
with the earlier observation that an interface that
refers to itself by means of a personal pronoun is
perceived to be more human-like than one that does
Figure 3: Distribution chart for syntactic construction
types in user utterances depending on system output style
not (Nass and Brave, 2005).
4.2 Style and Alignment
The next issue we investigated was whether the users
formulated their input differently with the personal
vs. impersonal system version. For each dialogue
session, we calculated the percentage of utterances
containing the feature of interest relative to the total
number of user utterances in the session.
First we analyzed the distribution of personal,
impersonal and telegraphic constructions across the
personal and impersonal style conditions. (The rea-
son we separated telegraphic constructions is be-
cause they seem to be neutral with respect to style.)
We compared the means of the obtained numbers be-
tween the two style conditions. Figure 3 shows the
distribution of the types of syntactic constructions
across the system output style conditions.
1. We expected the participants to use more per-
sonal constructions with the personal style ver-
sion of the system. Independent samples t-
test showed a significant result in the predicted
direction (t(19)=1.8, p=0.05 (one-tailed); see
Figure 3).
2. We expected to find the reverse effect with
regard to the proportion of impersonal verb
forms: participants using the personal style
194
version of the system were expected to have
less infinite, passive and ?es-gibt? forms than
those in the impersonal style condition. How-
ever, we did not find any significant difference
between the two style conditions (t(26)=1.0,
p=0.17 (one-tailed)).
3. According to expectation we also did not find
any significant difference in the proportion of
telegraphic constructions per style condition
(t(26)=1.4, p=0.09 (one-tailed)).
4. In the impersonal style condition we found
a significantly lower proportion of verb-
containing utterances than utterances in tele-
graphic form (t(13)=3.5, p=0.00 (one-tailed)).
But in the personal style condition there was no
statistically significant difference (t(13)=0.7,
p=0.25 (one-tailed)).
Next we analyzed the distribution of first and sec-
ond person pronouns across style conditions. We ex-
pected to find more personal pronouns in personal
than in impersonal style condition (Brennan and
Ohaeri, 1994). However, the results showed no sta-
tistically significant difference (t(26)=0.67, p=0.25
(one-tailed)).
Another prediction based on (Brennan and
Ohaeri, 1994) was to find more politeness markers
in the personal style. However, the analysis showed
that participants in the personal style condition did
not use significantly more politeness markers than
those in the impersonal style condition (t(20)=1.06,
p=0.15 (one-tailed)).
Finally, (Brennan and Ohaeri, 1994) predicted
that personal style, being more flexible, might cause
more speech recognition problems than input in im-
personal style. We checked whether participants in
the personal style condition had a higher rate of un-
recognized utterances than those in the impersonal
style condition and found no significant difference
(t(26)=0.60, p = 0.28 (one-tailed)).
To summarize, we observed a significant differ-
ence in the number of personal constructions across
style conditions, in accordance with the expectation
based on style alignment in terms of agentivity. But
we did not find a significant difference in the distri-
bution of impersonal constructions across style con-
ditions. Not surprisingly, there was also no signifi-
cant difference in the distribution of telegraphic con-
structions. An unexpected finding was the higher
proportion of telegraphic constructions than verb-
containing ones within the impersonal style condi-
tion. However, the personal style condition showed
no significant effect. Contrary to expectations, we
also did not find any significant effect of style-
manipulation on the number of personal pronouns,
nor on the number of politeness markers.
4.3 Style Alignment over Time
Since alignment can also be seen as a process of
gradual adjustment among dialogue participants in
the course of their interaction, we were interested
in whether participants tended to converge to using
particular constructions as their session with the sys-
tem progressed. For each participant we divided the
transcribed conversation in two halves. Using paired
samples t-test, we compared the proportion of per-
sonal, impersonal and telegraphic constructions in
the first and second halves of the conversations for
both style conditions.
In the personal style condition, we found no sig-
nificant change in the usage of construction types
between the first and the second half of the dialogue.
In the impersonal style condition, we did not find
any significant difference in the distribution of im-
personal and telegraphic constructions either. How-
ever, we found a significant change in the number
of personal constructions (t(13)=2.5, p=0.02 (one-
tailed)): The participants cut down on the use of per-
sonal constructions in the second half.
5 Discussion and Conclusions
We presented the results of an experiment with the
in-car multimodal dialogue system SAMMIE, aimed
to test whether we obtain effects similar to earlier
findings concerning the influence of system output
style in the interpersonal dimension on the users?
subjective judgments of a system (Nass and Brave,
2005) as well as their formulation of input (Bren-
nan and Ohaeri, 1994). Although our results are not
conclusive, they point at a range of issues for further
research.
Regarding users? attitudes to the system, we
found no significant difference among the styles.
This is similar to (Brennan and Ohaeri, 1994) who
195
found no difference in intelligence attributed to the
system by the users, but it is at odds with the earlier
finding that a synthetic voice interface was judged
to be more useful when avoiding self-reference by
personal pronouns (Nass and Brave, 2005).
Whereas (Brennan and Ohaeri, 1994) used a flight
reservation dialogue system, (Nass and Brave, 2005)
used a phone-based auction system which read out
an introduction and five object descriptions. There
are two points to note: First, the subjects were ex-
posed to system output that was a read out contin-
uous text rather than turns in an interaction. This
may have reinforced the activation of particular style
features. Second, the auction task may have sensi-
bilized the subjects to the distinction between sub-
jective (the system?s) vs. objective information pre-
sentation, and thus make them more sensitive to
whether the system presents itself as an active agent
or not.
Regarding the question whether users align their
style to that of the system, where previous experi-
ments showed strong effects of alignment (Brennan
and Ohaeri, 1994), our experiment shows some ef-
fects, but some of the results seem conflicting. On
the one hand, subjects interacting with the personal
style version of the system used more personal con-
structions than those interacting with the impersonal
style version. However, subjects in either condi-
tion did not show any significant difference with re-
spect to the use of impersonal constructions or tele-
graphic forms. We also found a higher proportion of
telegraphic constructions than verb-containing ones
within the impersonal style condition, but no such
difference in the personal style. Finally, when we
consider alignment over time, we find no change in
construction usage in the personal style, whereas we
find a decrease in the use of personal constructions
in the impersonal style.
That there is no difference in the use of tele-
graphic constructions across conditions is not sur-
prising. Being just phrasal sentence fragments, these
constructions are neutral with respect to style. But
why does there seem to be an alignment effect for
personal constructions and not for others? One way
of explaining this is that (some of) the constructions
that we counted as impersonal are common in both
styles. Besides their deliberate use as means to avoid
explicit reference to oneself, the constructions typi-
cal for impersonal style also have their normal, neu-
tral usage, and therefore, some of the utterances that
we have classified as impersonal style might just be
neutral formulations, rather than cases of distancing
or ?de-agentivization?. However, we could not test
this hypothesis, because we have not found a way
to reliably distinguish between neutral and marked,
truly impersonal utterances. This is an issue requir-
ing further work.
The difference between our results concerning
alignment and those of (Brennan and Ohaeri, 1994)
is not likely to be due to a difference in the degree
of interactivity (as with (Nass and Brave, 2005)).
We now comment on other differences between our
systems, which might have contributed to the differ-
ences in results.
One aspect where we differ concerns our distinc-
tion between personal and impersonal style, both in
the implementation of the SAMMIE system and in
the experiment: We include the presence/absence
of agentivity not only in the system?s reference to
itself (akin to (Nass and Brave, 2005) and (Bren-
nan and Ohaeri, 1994)), but also in addressing the
user. This concept of the personal/impersonal dis-
tinction was inspired by such differences observed
in a study of instructional texts in several languages
(Kruijff et al, 1999), where the latter dimension is
predominant. The present experiment results make
it pertinent that more research into the motives be-
hind expressing or suppressing agentivity in both di-
mensions is needed.
Apart from the linguistic design of the system?s
output, other factors influence users? behavior and
perception of the system, and thus might confound
experiment results, e.g., functionality, design, er-
gonomics, speech synthesis and speech recognition.
Earlier experiments reported in (Nass and Brave,
2005) suggest that a system with synthesized speech
should be more positively rated when it does not
refer to itself as an active agent by personal con-
structions. Whereas the system used by (Brennan
and Ohaeri, 1994) used written interaction, we used
the MARY text-to-speech synthesis system (Schro?der
and Trouvain, 2003) with an MBROLA diphone
synthesizer, which produces an acceptable though
not outstanding output quality. But as discussed ear-
lier, contrary to (Nass and Brave, 2005) we have not
observed a difference in the users? attitudes depend-
196
ing on style. It thus remains an open issue what ef-
fect speech output quality has on on the users? atti-
tudes and alignment behavior.
Regarding a possible influence of speech recogni-
tion on our results, we performed a post-hoc analysis
(Kruijff-Korbayova? et al, 2008), which did not re-
veal significant differences in user attitudes or align-
ment behavior depending on better or worse speech
recognition performance experienced by the users.
A future experiment should address the possibility
of an interaction between system style and speech
recognition performance as both factors might be in-
fluencing the user simultaneously.
One radical difference between our experiment
and the earlier ones is that the users of our system
are occupied by the driving task, and therefore only
have a limited cognitive capacity left to devote to the
interaction with the system. This may make them
less susceptible to the subtleties of style manipula-
tion than would be the case if they were free of other
tasks. A possible future experiment could address
this issue by including a non-driving condition.
Finally, as we pointed out in the introduction,
the SAMMIE system can also be used in an style-
alignment mode, where it mimics the user?s style on
turn-to-turn basis. We plan to present experimental
results comparing the alignment-mode with the fixed
personal/impersonal style in a future publication.
Acknowledgments
This work was carried out in the TALK project
(www.talk-project.org) funded by the EU as project
No. IST-507802 within the 6th Framework Program.
References
T. Becker, N. Blaylock, C. Gerstenberger, I. Kruijff-
Korbayova?, A. Korthauer, M. Pinkal, M. Pitz, P. Poller,
and J. Schehl. 2006. Natural and intuitive multimodal
dialogue for in-car applications: The SAMMIE system.
In Proceedings of ECAI, PAIS Special section.
T. Becker, N. Blaylock, C. Gerstenberger, A. Korthauer,
M. Pitz, P. Poller, J. Schehl, F. Steffens, R. Stegmann,
and J. Steigner. 2007. Deliverable D5.3: In-car
showcase based on TALK libraries. Technical report,
TALK Project, EU FP6, IST-507802.
H. Branigan, M. Pickering, J. Pearson, J. F. McLean, and
C. Nass. 2003. Syntactic alignment between com-
puter and people: the role of belief about mental states.
In Proceedings of the Annual Conference of the Cog-
nitive Science Society.
S. Brennan and J.O. Ohaeri. 1994. Effects of mes-
sage style on user?s attribution toward agents. In Pro-
ceedings of CHI?94 Conference Companion Human
Factors in Computing Systems, pages 281?282. ACM
Press.
S. Brennan. 1996. Lexical entrainment in spontaneous
dialogue. In Proceedings of the International Sympo-
sium on Spoken Dialogue (ISSD-96), pages 41?44.
S. Garrod and M. Pickering. 2004. Why is conversation
so easy? TRENDS in Cognitive Sciences, 8.
K. Hadelich, H. Branigan, M. Pickering, and M. Crocker.
2004. Alignment in dialogue: Effects of feedback
on lexical overlap within and between participants.
In Proceedings of the AMLaP Conference. Aix en
Provence, France.
G.J.M. Kruijff, I. Kruijff-Korbayova?, J. Bateman,
D. Dochev, N. Gromova, T. Hartley, E. Teich,
S. Sharoff, L. Sokolova, and K. Staykova. 1999.
Deliverable TEXS2: Specification of elaborated text
structures. Technical report, AGILE Project, EU
INCO COPERNICUS PL961104.
I. Kruijff-Korbayova?, C. Gerstenberger, O. Kukina, and
J. Schehl. 2008. Generation of output style variation
in the SAMMIE dialogue system. In Proceedings of
INLG?08, Salt Fork Resort, Ohio.
W.J.M. Levelt and S. Kelter. 1982. Surface form and
memory in question answering. Cognitive Psychol-
ogy, 14:78?106.
H. Mutschler, F. Steffens, and A. Korthauer. 2007. De-
liverable D6.4: Final report on multimodal experi-
ments Part I: Evaluation of the SAMMIE system. Tech-
nical report, TALK Project, EU FP6, IST-507802.
C. Nass and S. Brave, 2005. Should voice interfaces say
?I?? Recorded and synthetic voice interfaces? claims
to humanity, chapter 10, pages 113?124. The MIT
Press, Cambridge.
C. Nass, S. Brave, and L. Takayama. 2006. Socializing
consistency: from technical homogeneity to human
epitome. In P. Zhang & D. Galletta (Eds.), Human-
computer interaction in management information sys-
tems: Foundations, pages 373?390. Armonk, NY: M.
E. Sharpe.
J. Pearson, J. Hu, H. Branigan, M. J. Pickering, and C. I.
Nass. 2006. Adaptive language behavior in HCI: how
expectations and beliefs about a system affect users?
word choice. In CHI ?06: Proceedings of the SIGCHI
conference on Human Factors in computing systems,
pages 1177?1180, New York, NY, USA. ACM.
M. Schro?der and J. Trouvain. 2003. The German text-to-
speech synthesis system MARY: A tool for research,
development and teaching. International Journal of
Speech Technology, 6:365?377.
197
Mul t i l i ngua l i ty  in a Text  Generat ion  System 
For Three  Slavic Languages  
Geert-Jan Kruijff a, Elke Teich t', John Bateman ~, Ivana Kruijit;Korbayovfi", 
Hana Skoumalovg ~,Serge Sharoff 'l, Lena Sokolova d, Tony Hartley ~, 
Kamenka Staykova/, Ji~'~ Hana" 
?Charles University, Prague; ~University of the Saarland; ~University of Bremen; 
aRRIAI, Moscow; ?University of Brighton; /IIT, BAS, Sofia 
http://www.itri.brighton.ac.uk/projects/agile/ 
Abstract 
This paper describes a lnultilingual text generation 
system in the domain of CAD/CAM software in-- 
structions tbr Bulgarian, Czech and l:\[ussian. Start- 
ing from a language-independent semantic represen- 
tation, the system drafts natural, continuous text 
as typically found in software inammls. The core 
modules for strategic and tactical gene,'ation are im- 
plemented using the KPML platform for linguistic 
resource development and generation. Prominent 
characteristics of the approach implemented a.re a 
treatment of multilinguality that makes maximal use 
of the cominonalities between languages while also 
accounting for their differences and a common repre- 
sentational strategy for both text planning and sen- 
tence generation. 
1 In t roduct ion  
This paper describes the Agile system I tbr the 
multilingual generation of instructional texts as 
found in soft;ware user-manuals in Bulgarian, 
Czech and Russian. The current prototype fo- 
cuses on the automatic drafting of CAD/CAM 
software documentation; routine passages as 
found in the AutoCAD user-manual have been 
taken as target texts. The application sce- 
nario of the Agile system is as follows. First, 
a user constructs, with the help of a GUI, 
language-independent task models that spec- 
ify the contents of the documentation to be 
generated. The user additionally specifies the 
language (currently Bulgarian, Czech or Rus- 
sian) and the register of the text to be gen- 
erated. The Agile system then produces con- 
tinuous instructional texts realizing the speci- 
fied content and conforming to the style of soft- 
ware user-manuals. The texts produced are 
1EU Inco-Copernicus project PL961004: 'Automatic 
Generation of Instructional Texts in the Languages of 
Eastern Europe' 
intended to serve as drafts for final revision; 
this ~drafting' scenario is therefbre analogous to 
that first explored within the Drafter project. 
Within the Agile project, however, we have ex- 
plored a more thoroughly nmltilingual architec- 
ture, making substantial use of existing linguis- 
tic resources and components. 
The design of the Agile system overall re, sts 
on the following three assumI)tions. 
First, the input of the system should be spec- 
ified irrespective of any particular output lan- 
guage. This means that the user must be able to 
express the content that she wants the texts to 
convey, irrespective of what natural language(s) 
she masters and in what language(s) the out- 
put text shouM be realized. Such language- 
independent content specification can take the 
form of some knowledge representation pertain- 
ing to the application domain. 
Second, the texts generated as the outtmt of 
the system should be well-formulated with re- 
spect to the expectations of natiw. ? speakers of 
each particular language covered by the system. 
Since differences among languages may appear 
at any level, language-sensitive d cisions about 
the realization of the specified content must be 
possible throughout he generation process. 
And third, the notion of multilinguality em- 
ployed in the system should be recursive, in 
the sense that the modules responsible tbr the 
generation should themselves be multilingual. 
The text generation tasks which are common 
to the languages under consideration should be 
pertbrmed only once. Ideally, there should be 
one process of generation yielding output in 
multiple languages rather than a sequence of 
monolingual processes. This view of 'intrin- 
sic multilinguality' builds on the approach set 
out in Bateman et al (1999). Each module of 
the system is fnlly multilingual in that it simul- 
474 
taneously enables both integration of linguistic 
resources, defining commonalities bel;ween lan- 
guages, and resource integrity, in |;bat the in- 
dividuality of each of the language-speeitic re- 
sources of a multilingnM ensemble is always pre- 
served. 
We consider these assuml)l;ions an(l the view 
of multilinguality entailed by |;hem to be cru- 
cial for the design of efli;ctive multilingual text 
generation systems. The results so far a(:hicved 
by the Agile system SUl)port this and also ofl'er 
a ~soli(l experiential basis tbr the develot)mcnt of 
fllrther multilingnal generation systems. 
The overall operation of 1;t1(; Agile sysl;em is 
as tbllows. Al/tcr the us(u' has Sl)ecilied some 
inl;en(led text (;OlltenI; (described in Section 2) 
via the Agile GUI, the system i)ro(:eeds to gen- 
eral;e the texts required. To do this, a text 
t)lammr (Section 3) first assigns parts of the, 
task model to text elements and arranges l;h(;m 
in a hierarchical fashion a text t)lan. Then, a 
sentence plammr organizes I;he content of the 
text elements into sentence-sized elml~ks and 
ere~,tes the corresponding input fin' l;he tacti- 
ca,1 generator, expressed in standard sentence 
l)lamfing language (SPI,) lbrmulae. Finally, 1;11(; 
tactical g(meral;or generates t;he linguistic real- 
izations corresponding 1;o these Sl)l~s the text 
(Sect;ion 4). In the stage of the l)rojccI; rt}l)orte(l 
here, we, conceal;rated i)arl;icularly on \])roccdu- 
ral texts. These otlhr sl;el)-by-st;e t) des(:rit)t;ions 
of how to perlbrm domain tasks using the given 
software tools. A simplified version of one such 
procedural text is given (tbr English) in Fig- 
ure 1. This architectm:e mirrors the reference 
architecture for generation diseusse(t in I/,eiter 
8z Dale (1.997). The modules of the system are 
1)ipelined so that a continuous text is generated 
realizing the intended meaning of the inlmt se- 
mantic representation without backtracking or 
revision. 
Several important properties have ('haracter- 
ized the method of development leading to the 
Agile system. These are to a large extent re- 
sponsible for the eflhetiveness of the system. 
These include: 
Re-use  and  adaptat ion  o f  ava i lab le  re-  
sources .  We have re-used snt)stantial bodics 
of e, xisting linguistic resources at all levels rel- 
evant for the system; this t)laye(l a (:rueial role 
in achieving the Sol)histieatcd generation capa- 
7b d~nw a polylinc 
First start the PLINE command using one of these meth- 
ods: 
Windows From the Polylinc tlyout on the, l)raw tool~ 
lmr, choose Polylinc. 
DOS and UNIX  lqom the Draw menu, choose Poly- 
line. 
1.. Spccit~y the start point of the polyline. 
2. S1)ecil~y tim next point of the 1)olylinc. 
3. Press ll,cturn t;o end the polyline. 
Figure l: Example "To draw a polyline" 
bilities now displayed by the system in each of 
its languages of expertise prior to the project 
t\]m'l.'e were 11o substantial ~mtomatic generation 
systenls fi)r any of the languages covered. The 
core modules for strategic and ta(:ti('al gener- 
ation were all imt)lemcnted using the Kernel- 
Penman Multilingual system (KPML: ef Bate- 
man et al, \]999) a Common l,isp base(t gram- 
mar development environment, in addition, 
we adopted the Pemnan Upt)er Model as used 
within Pemnan/KPMl~ as the basis tbr our 
linguistic semantics; a more rcstri(:ted domain 
model (DM) rclewmt o the CAD/CAM-domain 
was &',lined as a st)e('ialization of l;he UM con- 
(:epts. The I)M was iuspired by the domain 
me(tel of the Drafter l)rojet:t, but l)res(ml;s a 
g(m(',ralizati()n ()f the latter in that it allows for 
eml)(;d(ling t:asks and illsLrut'|;ions t:o any arlfi- 
l;rm:y re(:ursive depth (i.e., more complex l;cxt; 
plans). Ah'eady existing lexical resom:ces and 
morphological modules availabh; to the 1)ro.j(',ct 
were re-used tbr Bulgarian, Czech and l~.ussian: 
the Czech and Bulgarian components were mo(t- 
ules written in C (e.g., IIaji(: L; Hla(lk~, 1997, 
tbr Czech) that were interfimed with KPMI, us- 
ing a standard set of API-methods (of. Bate- 
man & Sharoff, 1998). Finally, because no 
grammars uitable for generation in Bulgarian, 
Czech and l/.ussia,n existed, a grammar tbr En- 
glish (NIGEL: Mann & Matthiessen, 1985) was 
re-used to lmild them; tbr the theoretical basis 
of this technique see Teich (1995). 
Combinat ion  o f  two  methods  o f  resources  
deve lopment .  Two methods  were com- 
bined to enable us to develop basic general- 
language grammars and sublanguage grammars 
fin: CAD/CAM instructional texts at; |;11(; same 
time. One nmthod is the system-oriented one 
aimed at lmildiug a computational resource 
475 
with a view of the whole language system: this 
is a method strongly supported by the KPML 
development environment. The other method 
is instance-oriented, and is guided by a detailed 
register analysis. The latter method was partic- 
ularly important given the Agile goal of being 
able to generate texts belonging to rather di- 
verse text types- -  e.g., impersonal vs. personal; 
procedural, flmetional descriptions, overviews 
etc. 
Cross-linguistic resource-sharing. A cross- 
linguistic approach to linguistic specifications 
and implementation was taken by maximizing 
resource sharing, i.e. taking into account sim- 
ilarities and differences among the treated lan- 
guages o that development tasks have been dis- 
tributed across different languages and re-used 
wherever possible. 
2 Language- independent  Content 
Specif icat ions 
The content constructed by a user via the Ag- 
ile GUI is specified in terms of Assertion-bozes 
or A-boxes. These A-boxes are considered to 
be entirely neutral with respect o the language 
that will be used to express the A-box's con- 
tent. Thus individual A-boxes can be used for 
generating multiple languages. A-boxes spec- 
i(y content by instantiating concepts from ~,he 
DM or UM, and placing these concepts in rela- 
tion to one another by means of configurational 
concepts. The configurational concepts define 
adnfissible ways in which content can be struc- 
tured. Figure 2 gives the configurational con- 
cepts distinguished within Agile. 
Procedure A procedure has three slots: 
(i) GOAL (obligatory,filled by a USER-AcTION), 
(ii) METIIODS (optional, filled by a METHOD-LIsT), 
(iii) SIDE-EPFECT (optional, filled by a USER- 
EVENT). 
Method A method has three slots: 
(i) CONSTRAINT (optionM, filled by an OPERATING- 
SYSTEM), 
(ii) PaEeONDITION (optional, filled by a PROCE- 
DURE), 
(iii) SUUSTEPS (obligatory, filled by a PI~OCEDUI/E- 
LIST). 
Method-List A METIIOD-LIST is a list of h/IETIIOD'S. 
Procedure-List A PROCEI)URE-LIST is a list of 
PROCEDURE:S. 
Figure 2: Configurational concepts 
Configurational concepts are devoid of actual 
content. Tile content is provided by inst, antia- 
tions of concepts that represent various user ac- 
tions, interface events, and interface modalities 
and functions. Taken together, these instanti- 
ations provide the basic propositional content 
tbr instructional texts and are taken as input 
tbr the text planning process. 
3 Strategic Generat ion: From 
Content  Specif icat ions to Sentence 
Plans 
To realize an A-box as a text, we go through suc- 
cessive stages of text planning, sentence plan- 
ning, and lexico-grammatical generation (cf 
also Reiter & Dale, 1997). At each stage there 
is an increase in sensitivity to, or dependency 
on, the target language in which output will 
be generated. Although the text planner itself 
is language-independent, the text; plamfing re- 
sources may (lifter fl'om language to language 
as much as is required. This is exactly analo- 
gous to the situation we find within the individ- 
ual language grammars as represented within 
KPML: we therefore represent the text planning 
resources in the same fashion. For the text type 
and languages of concern here, however, w~ria- 
lion across languages at the text planning stage 
turned out to be minimal. 
The organization of an A-box is used to guide 
the text planning process. Itere, we draw a dis- 
tinction between text structure elements (TSEs) 
as the elements from which a (task-oriented) 
text, is built ut), and text templates', which con- 
dition the way TSEs are to be realized linguis- 
tically. We locus on the relation between con- 
cepts on the one hand, and TSEs on the other. 
We are specifically interested in the configura- 
tional concepts that are used to configure the 
content specified in an A-box because we want 
to maintain a close connection between how the 
content can be defined in an A-box and how 
that content is to be spelled out in text. 
3.1 Structuring and Styling 
A text structure element is a predefined com- 
ponent that needs to be filled by one or more 
specific parts of the user's content definition. 
Using the reader-oriented terminology common 
in technical authoring guides, we distinguish 
a small (recursively defined) set of text TSEs; 
these are listed in Figure 3. 
476 
Task-Docmnent  A TASK-\])OCUMFNT has tWO slots: 
(i) TASK-TFI'I,E (ol)ligatory), 
(ii) TASK-INSTI{U(ITIONS (obligatory), being a list 
of" at least one ~\[NSTRUCTION. 
Instruction An INSTRUCTION has three slots: 
(i) TASKS (obligatory), being a list of at least one 
TASK~ 
(ii) CONSTRAINT (optional), 
(iii) Pm,ZCONDITION (optional). 
Task  A TASK has two slots: 
(i) INSTRUCTIONS (ol)tional), 
(ii) SII)I';-EI,'I"I,:C'I' (ol)tional). 
Figure 3: Text Structure Elements (TSEs) 
The TSEs are placed in correspondence with 
the configurational concet)ts of the DM (cf. Fig- 
ure 2); this enat)les us to lmild a text stru('ture 
l;hat folh)ws the structuring of the content in an 
A-1)ox (cf. Figure 4). 
Orthogonal to the notion of text structure l- 
ement is the notion of text temt)late. Whereas 
TSEs capture what needs to be realized, the 
text template (:al)tures how that content is to 
1)e realized. Thus, a feint)late defines a style 
for expressing the content. Am we discuss be- 
low, we define text templates in terms of con- 
straints on the realization of si)e(:iti(" (in(tivid- 
ual) TSEs. D)r examt)le, whereas in Bulgarian 
and Czech headings (to which the '\]'ASK-TITLE 
element corresponds: of. Figure 4) are usually 
realized as nominal groups, in the Russian Au- 
toCAD ulallnal headings are realized as nonii- 
nile purpose clauses as they are ill English. 
3.2 Tex~ P lann ing  g~ Sentence  P lann ing  
The major component of the text pbmner is 
fi)rnmd by a systemic network fi)r text struc- 
turing; this network, called the text structur- 
ing region, defines an additional level of linguis- 
tic resources for the level of genre. This region 
constructs text structures in a way that is very 
similar to the way in which the systemic net- 
works of the grammars of the tactical genera- 
|or build up grammatical structures. In fact, 
by using KPML to implement his means for 
text structuring, the interaction between global 
level text generation (strategic generation) and 
lexico-grammatical expression (tactical genera- 
tion) is greatly facilitated. Moreover, this al)- 
t)roach has the advantage |;tint constraints on 
output realization can 1)e easily accmnulated 
and propagated: for example, the text plan- 
ner can iml)ose constraints on the output lexico- 
grammatical realization of particular text t)lan 
elements, such am the realization of text head- 
ings by a nominalization ill Czech and Bulgar- 
|an or by an infinite purpose clause in Rus- 
sian. This is one contribution to overcoming the 
notorious generation gap prol)leln caused when 
a text planning module lacks control over the 
line-grained istinctions that m'e available in a 
grmmnar. Ill our case, both text plamfing and 
sentence planning are integrated into one and 
the same system and are distinguished by strat- 
ification. 
TASK-TITLE ~-} GOAl, of topmost  PROCEDURE 
TASK-INSTRUCTIONS ~-} METIIODS of PROCEDUI/E 
Sll)E-EIq,'ECT ~ SIDhl-EFFI~CT of PROCEDUII.I\] 
TASK /-~ GOAL of PROCEI)IHtI,; 
(-JONSTRA1NT <-} CONSTRAINT of ~41,VI'IIO1) 
PRECONI)ITION ~ PIH?COND1TION of ~,4ETI1OI) 
1NSTIIUCTI(IN-TAsKS 1--} SUBSTH)S of a METIIOD 
INST1HJCTION +5 MI,TI'IIOD 
Figure 4: Mapping TSEs and configurational 
concepts defined in the DM 
Following on from the orthogomflity of text 
t/;mplates and text structure elements, the text 
structuring region consists of two parts. One 
1)arl; deals wil;h interpreting the A-box in terms 
of TSEs: traversing l;he network of this part of 
the region produces a text structure for the A- 
b/lx contbrufing to the definitions above. The 
second part of the region imposes constraints 
on the realization of the TSEs introduced by 
the first part. Divers(; constraints can be ira- 
posed depending on the user's choice of style, 
e.g., personal (featuring ppredominantly imper- 
atives) vs. impersonal (tbaturing indicatives). 
Tile result of text plmming is a text plan. 
This can be thought of as a hierarchical struc- 
ture (built by TSEs) with lilts of A-box content 
at; its leaves together with additional constraints 
imposed by the text planning process: e.g., that 
the Title segment of the document should not be 
realized as a full (:lause but; rather as a nominal 
phrase or a lmrt)osive det)endent clause. The 
text plan may also include constraints on pre- 
ferred layout of the docmnent elements: this 
ilflbrmation is passed on via HTML annotations. 
The sentence plmmer then takes this text plan 
as intmt, and creates SPL tbrmulae to express 
477 
the content identified by the text plan's leaves. 
The resulting SPLs can also group one or more 
leaves together (aggregation) det)ending on de- 
cisions taken by the text planner concerning dis- 
course relations. Furthennore, constraints on 
realization that were introduced by the text- 
planner are also included into the SPLs at this 
stage. 
Of particular interest multilingually is the 
way concepts may require different kinds of re- 
alizations ill different languages. For example, 
languages need not of course realize concepts 
as single words: in Czech the concept Mcn,t 
gets realized as "menu" but the interface modal- 
ity Dialogboz is realized as a multiword expres- 
sion "dialogovd okno" (whose compofients i.e., 
an adjective and a nominal head may undergo 
various grammatical operations independently). 
The Agile system sentence plammr handles uch 
cases by inserting SPL fbrms corresponding to 
the literal semantics of the complex expressions 
required; these are then expressed via the tac- 
tical generator in the usual way. The result- 
ing SPL formulas thus represent the language- 
specitic semantics of the sentences to be gener- 
ated. Otherwise, if a concept maps to a single 
word, the sentence planner leaves the fnrther 
specification of how the concept should be re- 
alized to the lexico-grammar nd its concept- 
to-word mapI)ings. More extensive diflb.rences 
between languages are handled by conditional- 
izing the text and sentence planner resources 
fltrther according to language. 
4 Tactical Generat ion:  From 
Sentence P lans  to Sentences 
The tactical generation component hat colt- 
structs sentences (and other grammatical units) 
fl'om the SPL tbrmulae specified in the text 
plan relies on linguistic resources tbr Bulgarian, 
Czech and Russian. The necessary grammars 
and lexicons have been constrncted employing 
the methods described in Section 1. As ,toted 
there, the crucial characteristic of this model 
of nmltilingual representation is that it allows 
tbr the representation f both, commonalities and 
differences between languages, as required to 
cover the observable ontrastive-linguistic phe- 
nomena. This can be applied even among typo- 
logically rather distant languages. 
We first illustrate this with respect o some 
of the contrastive-linguistic t)henomena that are 
covered by this model employing exami)les ti'om 
English, Bulgarian, Czech and Russian. We 
then show the organization of the lexicons and 
briefly describe lexical dloice. 
4.1 Semantic and grammatical 
cross-linguistic variation 
One. of the tenets of our model of cross-linguistic 
variation is that languages have a rather high 
degree of similarity semantically attd tend to 
differ syntactically. We can thus expect o have 
identical SPL expressions for Bulgarian, Czech 
and Russian in many cases, although these may 
be realized by diverging syntactic structures. 
However, we also allow for the case in which 
there is no commonality at; this level and even 
the SPL expressions diverge. 2 Example 1 illus- 
trates the latter case (high semantic divergence, 
plus grammatical divergence), and example 2 
the former (semantic ommonality, plus gram- 
matical divergence). 
Example 1: English and Russian spa- 
tial PPs .  The major lexico-grammatical d i f  
ference l)etween English and Russian preposi- 
tional phrases is that the relation expressed by 
the PP is realized by the choice of the prepo- 
sition in English, whereas in Russian, it; is in 
addition realized by case-government. In the 
are.a of spatial PPs, the choice of a particular 
preI)osition in English corresl)onds to a distinc- 
tion in the dimensionality of the object that re- 
alizes the range of the relation expressed by the 
PP. For both PPs expressing a location and PPs 
expressing movement, English distinguishes be- 
tween three-dimensional objects (in, into), one- 
or-two-dimensional objects (on, onto) and zero- 
dimensional objects (at, to). 
In Russian, in contrast, zero-or-three dimen- 
sional objects (preposition: v) are opposed 
to one-or-two-dimensional objects (preposition: 
ha). A fnrther difference between the expres- 
sion of static location vs. movement is expressed 
by case selection: na/v+locative case expresses 
static location, v/na+accusative case expresses 
inovement (entering or reaching an object) and 
the preposition k+dative case expresses move- 
inent towards an object (,lot quite reaching or 
2This distinguishes our approach fl'om interlingua- 
based systems, which typically require a common seman- 
tic (or conceptual) input. 
478 
entering it). In the {-onverse relation, motion 
away from an object, s is sele, eted tbr move- 
ment from within an oh.joel;, and ot fbr move- 
men| away from the vicinity of an ot).jeet. Her(;, 
both prel)ositions govern genitive case. The di- 
mensionality of the object is only relevant for 
the distinction between v/na and s/ot, 1)ut not 
for h. Since the concel)tualizations of spatial re- 
lations are ditf'erent across \]'3nglish and Russian, 
the input SPL expressions diverge, as shown in 
Figure 5); rather than using domain model con- 
cepts, these SPL ext)ressions restrict hemselves 
to Ut)pe, r Model concepts in order to highlight 
the cross-linguistic contrast. This examl)le illus- 
trates well how it is (}ften ne{:e, ssary t{} 'semanti- 
{:ize,' eve, nts differently in (tilt'ere|d; languages in 
order 1;o achieve the most natural results. Not;{; 
that Cze, ch is here very similar to l/nssian. 
a. SPL Russian 
(example 
:name DO-Textl-Ku 
:targetform "Pomestite fragment v bufer." 
:logicalform 
(s / dispositive-material-action 
:lex pomestitj 
:speech-act-id command 
:actee (a / object :lex fragment) 
:destination (d / THREE-D-0BJECT 
:lex bufer))) 
1}. SPL Rn: English 
(example  
:name D0-Textl-En 
:targetform "Put the selection on the clipboard." 
:iogicalform 
(s / dispositive-material-action 
:lex put 
:speech-act-id command 
:actee (a / object :lex selection) 
:destination (d / ONE-0R-TW0-D-0BJECT 
:lex clipboard))) 
Figure 5: SPI, ext}ressions 
Example 2: English, Bulgar ian and Czech 
headers in CAD/CAM texts. Grammatical 
ullits (1) (4) below show all ex~?tIllt, e of ,:r,,ss- 
linguistic commonality at the level of sen|anti{: 
int}ut and divergence at the le, vel of grammar. 
These units all time|ion as selfsutficient Task- 
titles tbr the deseril}tions of particular actions 
that can be t)erformed with the given s{}t'tware. 
(1) En: T{} draw a polyline 
(2) BU: qepTaene na IlOJII4MI4IIFIH 
Drawing- of polylineqNDEF 
NOMINAL  
(3) Cz: Kreslenl kf'ivky 
drawing-NOMINAL \]ine-GEN 
(d) \]/,ll: LIwo6I,I Hal)I4COBaTI, IIO,KHJIIIIIHIO 
in-order draw-INF l)olyline-AcC 
There are two major dit  re,,,ces (:,) (4) 
that need to 1)e accounte, d for: (i) they exhibit 
divergent grammatieal  ranks in that (1) and 
(4) are clauses (uontinite), while (2) and (3) are 
nomil,al groul,s (nominalizations); and ( i i )they 
show divergent syntact ic realizations: (2) 
and (3) ditl'er in that in Bulgarian, wlfich does 
not have (:as(',, the relation 1)etween the syntactic 
head Met)q_'aelte (ch, crtacnc) and the modifier lie- 
:mamma (polilinia) is (;xt)ressed by a t)re, position 
na (ha), whereas in Cze, ch, which has cast, this 
relation is expressed by genitive case, (k?ivky). 
\])espite these (litferen(:es, only the first diver- 
gen(:e has any (;onsequen(:{;s for the S\])L ext)res- 
sions rcquir(;d; I;hc l)asie semantic ommona\]- 
ity among (1)(4)  is 1)reserve, d. This is shown 
in Figm:e 6 t)y me, ans of the standard linguis- 
tic conditionalization 1)rovided 1)y KPML l'or all 
levels of linguistic des(:ription. The COll(tition- 
alization shows that both the English (1.) and 
the Russian (4) ar(' nontinite clauses while, the 
\]hdgarian (2) and the Czech (3) are nominM- 
izations. These S\])l, ext)ressions also show the 
use of (lom~dn ('onc(;1)ts as i)rodu('e(l by the text 
tfl~mner rathe, r than Ut)lmr model concepts as in 
the SPLs in Figure, 5. 
(example 
:name DO-Textl 
:logicalform 
(s / DM::draw 
:en :ru :PROPOSAL-Q & PROPOSAL 
:bu :cz :EXIST-SPEECHACT-Q & NOSPEECHACT 
:actee (d / DM::polyline))) 
Figure 6: Multilingual SPL e, xpression tbr the 
header examlfles 
The second differen('e is handled by the gener- 
ation grmmnars internally. Here, Bulgarian and 
Czech share the basic tractional-grammatical 
description of t)ostmotlifie, rs tbr nomilmlizati(ms 
(Figm:e 7). The ditl'erence in structure only 
479 
shows in syntagmatic realization and is separate 
from the functional description: For Bulgarian, 
the postmodifier marker Ha (ha: %f') is inserted, 
and tbr Czech, the nominal group realizing the 
Postmodifier is attr ibuted genitive ease. a
(gate 
:name MEDIUM-QUALIFIER 
:inputs processual-mediated 
:outputs 
((i.0 medium-qualifier 
(:bu :cz preselect Medium nominal-group) 
(:cz preselect Medium noun-gen-case) 
(:bu insert Mediumqualifiermarker) 
(:bu lexify Mediumqualifiermarker na))) 
:region QUALIFICATION) 
Figure 7: Shared system tbr Bulgarian and 
Czech 
4.2 Lexical choice and lexicons 
The lexical items tbr each language are selected 
from the lexicon via the domain model. A DM 
concept is annotated with one or more lexical 
items from each language. If there is more than 
one item per language, the choice is constrained 
by features imposed by the gralnmar. 
For example the concept DN::draw is anno- 
tated with two lexical items which are the im- 
perfective and perfective forms of the verb draw 
in Czech, Bulgarian and Russian. If the gram- 
mar selects imperfective aspect, tim first is cho- 
sen; if the grammar selects perfective aspect, 
the second is chosen. This mechanism is used 
also fbr the choice between a verb and its nom- 
inalization, among others. With the help of the 
lexicon, the inflectional properties collected tbr 
a particular lexical item during generation are 
translated into a format suitable tbr external 
morphological modules, which are then called. 
The result of the external module, the inflected 
tbrm, is passed back to the KPML system and 
inserted into the grammatical structure. 
5 Eva luat ion  and Conc lus ions  
A first round of evaluation has been carried 
out on the Agile prototype. This directly as- 
sessed the ability of users to control multilin- 
3This description is also valid for Russian, which has a 
nominal group structure similar to Czech. The 13ulgarian 
one is more like English. 
gual generation in tile three languages, as well 
as the design and robustness of the system eom- 
1}onents. Groups of users were given a brief 
training period and then asked to construct 
A-boxes expressing iven content. Texts were 
cross-generated: i.e., the languages were w~ried 
across the A-boxes independently of the native 
languages of the subjects who created them. Er- 
rors were then classified and recommendations 
for the next and final Agile prototype collected. 
The generated texts were then evaluated by ex- 
pert technical authors. They were generally 
judged to be of a broadly similar quality to 
the texts originating from manuals, and both 
kinds of texts received similar criticism. The 
main source of criticism and errors was the de- 
sign of the GUI which is now being improved 
for the final prototype. The overall design of 
the system has theretbre shown itself to offer an 
etfective approach tbr multilingual generation. 
We are now extending the system to cover a 
broader ange of text types as well as the further 
grammatical and semantic variation required by 
the evaluators as well as by the additional text 
types. 
Re ferences  
Bateman, J. A., Matthiessen, C. M. I. M., & Zeng, L. 
(1999). Multilingual natural anguage generation 
for multilingual software: a flmctional inguistic 
approach. Applied Artificial hdelligencc, 13(6), 
607-639. 
Bateman, J. A. & Sharoff, S. (1998). Mult, ilingual 
grammars and multilingual lexicons for nmltilin- 
gual text; generation. In Mnltilinguality in the Icz- 
icon II, ECAI'98 Workshop 13, (pp. 1-8). 
Hajie, J. 8; Hladk?, B. (1997). Probabilistic and 
rule-based tagger of an inflective language a 
comparison. In Proceedings of ANLP'97, (pp. 
111-118). 
Mmm, W. C. & Matthiessen, C. M. I. M. (1985). 
Demonstration of the Nigel text generation com- 
puter progrmn. In J. D. Benson 8: W. S. Greaves 
(Eds.), Systemic Perspectives on Discourse, Vol- 
ume 1 (pp. 50-83). Ablex. 
Reiter, E. & Dale, R. (1997). Building applied natu- 
ral language generation systems. Journal of Nat- 
ural Language Engineering, 3, 57-87. 
Tcich, E. (1995). Towards a methodology for the 
construction of multilingual resources tbr multi- 
lingual text generation. In Proceedings of the I J- 
CAI'95 workshop on multilingual generation, (pp. 
136-148). 
480 
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 17?20,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
An Interactive Humanoid Robot Exhibiting Flexible Sub-Dialogues?
Heriberto Cuaya?huitl
DFKI GmbH
hecu01@dfki.de
Ivana Kruijff-Korbayova?
DFKI GmbH
ivana.kruijff@dfki.de
Abstract
We demonstrate a conversational humanoid
robot that allows users to follow their own
dialogue structures. Our system uses a hi-
erarchy of reinforcement learning dialogue
agents, which support transitions across
sub-dialogues in order to relax the strict-
ness of hierarchical control and therefore
support flexible interactions. We demon-
strate our system with the Nao robot play-
ing two versions of a Quiz game. Whilst
language input and dialogue control is au-
tonomous or wizarded, language output is
provided by the robot combining verbal and
non-verbal contributions. The novel fea-
tures in our system are (a) the flexibility
given to users to navigate flexibly in the in-
teraction; and (b) a framework for investi-
gating adaptive and flexible dialogues.
1 Introduction
Hierarchical Dialogue Control (HDC) consists of
behaviours or discourse segments at different lev-
els of granularity executed from higher to lower
level. For example, a dialogue agent can invoke a
sub-dialogue agent, which can also invoke a sub-
sub-dialogue agent, and so on. Task-oriented di-
alogues have shown evidence of following hierar-
chical structures (Grosz and Sidner, 1986; Litman
and Allen, 1987; Clark, 1996). Practically speak-
ing, HDC offers the following benefits. First,
modularity helps to specify sub-dialogues that
may be easier to specify than the entire full dia-
logues. Second, sub-dialogues may include only
relevant dialogue knowledge (e.g. subsets of dia-
logue acts), thus reducing significantly their com-
?*Funding by the EU-FP7 project ALIZ-E (ICT-248116)
is gratefully acknowledged.
(a) strict hierachical 
     dialogue control
Dialogue
Sub-dialogue1 Sub-dialogue2
(b) flexible hierachical 
  dialogue control
Dialogue
Sub-dialogue1 Sub-dialogue2
Figure 1: Hierarchies of dialogue agents with strict
(top down) and flexible control (partial top down).
plexity. Third, sub-dialogues can be reused when
dealing with new behaviours. In this paper we dis-
tinguish two types of hierarchical dialogue con-
trol: strict and flexible. These two forms of dia-
logue control are shown in Figure 1. It can be ob-
served that strict HDC is based on a pure top down
execution, and flexible HDC is based on a com-
bined hierarchical and graph-based execution.
The main limitation of strict HDC is that
human-machine interactions are rigid, i.e. the
user cannot change the imposed dialogue struc-
ture. A more natural way of interaction is by re-
laxing the dialogue structure imposed by the con-
versational machine. The advantage of flexible
HDC is that interactions become less rigid be-
cause it follows a partially specified hierarchical
control, i.e. the user is allowed to navigate across
the available sub-dialogues. In addition, another
important property of the latter form of HDC is
that we can model flexible dialogue structures not
only driven by the user but also by the machine.
The latter requires the machine to learn the dia-
logue structure in order to behave in an adaptive
way. The rest of the paper describes a demo sys-
tem exhibiting both types of behaviour, based on
a reinforcement learning dialogue framework.
17
2 Hierarchical Reinforcement Learning
Dialogue Agents with Flexible Control
Our dialogue controllers use hierarchical rein-
forcement learning as in (Cuaya?huitl et al, 2010).
We extend such a formalization through a hierar-
chy of dialogue agents defined with the following
tuples: M ij = <Sij , Aij , T ij , Rij , Lij , U ij , ?ij , ?ij>,
where Sij is a set of states, Aij is a set of actions,
T ij is a stochastic state transition function, Rij is
a reward function, Lij is a grammar that specifies
tree-based state representations, U ij is a finite set
of user actions (e.g. user dialogue acts), ?ij is a
finite set of models that subtask M ij is being al-
lowed to transition to, and ?ij = P (m? ? ?ij |m ?
?ij , u ? U ij) is a stochastic model transition func-
tion1 that specifies the next model m? given model
m and user action u. Although the hierarchy of
agents can be fully-connected when all models
are allowed to transition from a given particu-
lar model (avoiding self-transitions), in practice,
we may want our hierarchy of agents partially-
connected, i.e. when ?ij is a subset of subtasks
that agent M ij is allowed to transition to.
We implemented a modified version of the
HSMQ-Learning algorithm (Dietterich, 2000) to
simultaneously learn a hierarchy of policies piij .
This algorithm uses a stack of subtasks and op-
erates as illustrated in Figure 2. If during the ex-
ecution of a subtask the user decides to jump to
another subtask, i.e. to change to another sub-
dialogue, the flexible execution of subtasks allows
each subtask to be interrupted in two ways. In the
first case, we check whether the new (active) sub-
task is already on the stack of subtasks to execute.
This would be the case if it was a parent of the
current subtask. In this case, we terminate exe-
cution of all intervening subtasks until we reach
the parent subtask, which would be the new ac-
tive subtask. Notice that termination of all inter-
vening subtasks prevents the stack from growing
infinitely. In the second case, the current subtask
is put on hold, and if the new active subtask is
not already on the stack of subtasks to execute, it
is pushed onto the stack and control is passed to
it. Once the new subtask terminates its execution,
control is transferred back to the subtask on hold.
1This is a very relevant feature in dialogue agents in order
to allow users to say and/or do anything at anytime, and the
learning agents have to behave accordingly.
Initial
stack
Pushing
'dialogue'
Pushing
'sub-dialogue1'
Pushing
'sub-dialogue2'
(two siblings 
in the stack)
Popping
'sub-dialogue2'
Popping
'sub-dialogue1'
Popping
'dialogue'
dialogue dialogue dialogue dialogue dialogue
sub-
dialogue1
sub-
dialogue1
sub-
dialogue2
sub-
dialogue1
Figure 2: Hypothetical operations of stack-based hier-
archical dialogue controllers. Whilst the fourth opera-
tion from left to right is not allowed in strict HDC, all
stack operations are allowed in flexible HDC.
These kinds of transitions can be seen as high-
level transitions in the state space. They can also
be seen as the mechanism to transition from any
state to any other in the hierarchy. To do that we
maintain an activity status for each subtask M ij ,
where only one subtask is allowed to be active at
a time. We maintain a knowledge-rich state that
keeps the dialogue history in order to initialize
or reinitialize states of each subtask accordingly.
Since there is learning when new subtasks are in-
voked and no learning when they are interrupted,
this algorithm maintains its convergence proper-
ties to optimal context-independent policies.
3 A Hierarchy of Dialogue Agents for
Playing Quiz Games
We use a small hierarchy of dialogue agents?
for illustration purposes?with one parent agent
and two children agents (?robot asks? and ?user
asks?). Thus, the hierarchy of agents can ask the
user questions, and vice-versa, the user can ask
the robot questions (described in the next section).
Both conversants can play multiple rounds with a
predefined number of questions.
Due to space restrictions, we describe the hi-
erarchy of agents only briefly. The set of states
and actions use relational representations (they
can be seen as trees) in order to specify the
state-action space compactly, which can grow as
more features or games are integrated. Dialogue
and game features are included so as to inform
the agents of possible situations in the interac-
tion. The action sets use constrained spaces, i.e.
only a subset of actions is available at each state
based on the relational representations. For ex-
ample, the action Request(PlayGame) ? x0
is valid for the dialogue state x0 expressed as
Salutation(greeting)?UserName(known)?
PlayGame(unknown). The sets of primitive
actions (80 in total) assume verbal behaviours
18
with a mapping to non-verbal ones, some sam-
ple dialogue act types are as follows: requests,
apologies, confirmations, provide information,
acknowledgements, feedback, non-verbal expres-
sions, game-related actions. The transition func-
tions use pre-defined parameters, their training
from data is left as future work. The reward func-
tion addresses efficient and effective interactions
by penalizing dialogue length and encouraging to
continue playing. The dialogue agents learnt their
behaviour by interacting with a stochastic simu-
lated user, where the user responses eventually
required transitions across agents. A sample dia-
logue with flexible interaction is shown in Fig. 3.
4 A Humanoid Robot Integrated System
Figure 4 shows the robot?s integrated system,
which equips the robot with the following capa-
bilities: listening, talking, seeing and moving.2 A
sample interaction assuming wizarded behaviour
is as follows. The user says something to the robot
(e.g. ?ask me a question?) and the wizard selects
a user dialogue act for the corresponding utter-
ance such as ?Request(Question)?. Then the di-
alogue manager selects and executes the learned
action by querying/updating the game move gen-
erator and user model components, and by pass-
ing a dialogue act to the language generation and
motor control components. The former provides
the corresponding text to the speech synthesizer
and the latter provides the corresponding non-
verbal behaviour. Then the dialogue manager ob-
serves its new dialogue state, and suggests the
next actions?based on its dialogue policies and
user simulations?in the wizard-of-Oz GUI, see
Figure 5. From this setting, if we remove the
wizard then the system can behave autonomously,
where user inputs are derived from the language
and gesture understanding components.
The attractiveness of this kind of set-up is the
smooth transition from purely wizarded conver-
sational behaviour to fully autonomous behaviour,
where the dialogue policies can be tested and eval-
2We use off-the-shelf technologies such as Sphinx /
Julius / HTK for speech recognition, OpenCV for ges-
ture recognition, Acapela and Mary for speech synthe-
sis, HRL-based dialogue management (as described above),
OpenCCG for language parsing and language generation,
Weka and JavaBayes for maintaining a probabilistic and per-
sonalized user profile of asked and answered questions, the
URBI middleware (www.gostai.com), and the NAO hu-
manoid robot (www.aldebaran-robotics.com).
SYS Hi, my name is NAO, and you?
USR I am Peter
SYS Nice to meet you Peter
Do you want to Play a Quiz game?
USR Ask me questions [jump to ?robot asks?]
SYS The first question is.
What is a typical magic instrument?
Your choices are: One, guitar. Two, drums.
Three ,violin. Four, flute. What is your choice?
USR Number two
SYS That is not quite right. Do you have another choice?
USR Maybe number three
SYS The correct answer is number four, flute.
The next question is: which one of these is
an important artificial channel of the central America?
Your choices are: One, channel five. Two, Panama canal.
Three, Suez canal. Four, English channel.
What is your choice?
USR Now I want to ask you questions [jump to ?user asks?]
SYS Ask me the first question.
USR What does water become at the temperature of
zero degrees?
SYS What are my choices?
USR Your choices are:
One, condensation. Two, ice. Three, steam.
SYS I choose number one
USR That is incorrect.
SYS So what is the right answer?
USR The correct answer is number two.
SYS Okay, ask me another question.
USR The next question is: through which breach did
the Italian army entered into Rome in 1870?
Your choices are: One, porta venezia. Two, porta pia.
Three, porta pazienza. Four, porta nuova.
What is your choice?
SYS Paris
USR Let?s stop playing. [jump to ?root?]
SYS Did you like playing the Quiz Game?
USR Yes
SYS I am glad to hear that.
SYS It was nice playing with you, see you soon. Bye!
Figure 3: Sample dialogue with non-strict hierarchical
control. The groups of utterances represent activity
in different sub-dialogues in the hierarchy of agents.
Note that the user is allowed to override the imposed
dialogue structure. For instance, when the user says
?Ask me questions? the agent interrupts the execution
of the root subtask and transitions to the subtask ?robot
asks?. Similarly, the sub-dialogues do not need to follow
their imposed structure and the user is allowed to talk
about previous and unadressed sub-dialogues such as
the sudden switch from ?robot asks? to ?user asks?.
Middleware
Speech Recognizer,
Voice Act. Detector,
Audio Front End
Gesture
Recognizer
Motor
Control
Speech
Synthesizer
Game Move
Generator
Dialogue Manager
Parser,
Dialogue Act
Classifier
Language
Generator
User
Model
Wizard-of-Oz
GUI
ASR
result
Gesture
act
System
Dialogue
act
Text
ASR
result
Gesture
Act
System
dialogue
Act
Text
User
dialogue
act
Dialogue acts System
dialogue
act
user,
game
results
query,
questions,
answers
Figure 4: High-level architecture of our talking robot.
19
Figure 5: Screen shot of the wizard-of-Oz GUI, where
the dialogue policies and user simulations suggest
highlighted actions to the wizard. This setting allows
fully-wizarded and (semi-) autonomous behaviour.
Figure 6: The Nao robot greeting a user prior to play-
ing a Quiz game. The pieces of paper on the table are
the Quiz questions the child asks the robot.
uated with (semi-) autonomous behaviour. We use
this framework to investigate long-term human-
robot interaction, in particular child-robot inter-
actions for educational purposes. Figure 6 shows
a scene from a pilot evaluation, where the robot
and a child are visibly engaged with each other. A
complete evaluation with simulated and real dia-
logues will be reported in a forthcoming paper.
5 Discussion and Summary
Typically, conversational interfaces impose a di-
alogue structure on the user. Even in dialogue
systems with mixed-initiative interaction that give
flexibility to the user in terms of providing more
than one piece of information at a time, the
user is hardly allowed to navigate flexibly during
the interaction. Notable exceptions without dia-
logue optimization are (Rudnicky and Wu, 1999;
Lemon et al, 2001; Larsson, 2002; Foster et al,
2006). We believe that Hierarchical Reinforce-
ment Learning with global state transitions is an
interesting method to optimize (sub-) dialogues at
different levels of granularity, where the design of
action selection might not be easy to hand-craft.
On the one hand, our HDCs can be applied to
dialogues with user-driven topic shift, where the
user can take control of the interaction by navigat-
ing across sub-dialogues and the system has to re-
spond accordingly. On the other hand, our HDCs
can be applied to dialogues with system-driven
topic shift, where the system can itself terminate a
sub-dialogue, perhaps by inferring the user?s emo-
tional and/or situational state, and the system has
to switch itself to another sub-dialogue.
We have described a conversational humanoid
robot that allows users to follow their own dia-
logue structures. The novelty in our system is
its flexible hierarchical dialogue controller, which
extends strict hierarchical control with transitions
across sub-controllers. Suggested future work
consists in training and evaluating our humanoid
robot from real interactions using either partially
specified or fully learnt dialogue structures.
References
H. Clark. 1996. Using Language. Cambridge Univer-
sity Press.
H. Cuaya?huitl, S. Renals, O. Lemon, and H. Shi-
modaira. 2010. Evaluation of a hierarchical rein-
forcement learning spoken dialogue system. Com-
puter Speech and Language, 24(2):395?429.
T. Dietterich. 2000. An overview of MAXQ hi-
erarchical reinforcement learning. In Symposium
on Abstraction, Reformulation, and Approximation
(SARA), pages 26?44.
M. E. Foster, T. By, M. Rickert, and A. Knoll. 2006.
Human-robot dialogue for joint construction tasks.
In ICMI, pages 68?71.
B. Grosz and C. Sidner. 1986. Attention, intentions
and the structure of discourse. Computational Lin-
guistics, 12(3):175?204.
S. Larsson. 2002. Issue-Based Dialogue Manage-
ment. Ph.D. thesis, University of Goteborg.
O. Lemon, A. Bracy, A. Gruenstein, and S. Peters.
2001. The WITAS multi-modal dialogue system I.
In EUROSPEECH, Aalborg, Denmark.
D. Litman and J. Allen. 1987. A plan recognition
model for subdialogues in conversations. Cognitive
Science, 11:163?200.
A. Rudnicky and W. Wu. 1999. An agenda-based
dialogue management architecture for spoken lan-
guage systems. In IEEE Workshop on Automatic
Speech Recognition and Understanding (ASRU),
pages 337?340, Keystone, Colorado, USA, Dec.
20
Generation of Output Style Variation in the SAMMIE Dialogue System
Ivana Kruijff-Korbayova?, Ciprian Gerstenberger
Olga Kukina
Saarland University, Germany
{korbay|gerstenb|olgak}@coli.uni-sb.de
Jan Schehl
DFKI, Germany
jan.schehl@dfki.de
Abstract
A dialogue system can present itself and/or
address the user as an active agent by means
of linguistic constructions in personal style, or
suppress agentivity by using impersonal style.
We describe how we generate and control per-
sonal and impersonal style variation in the out-
put of SAMMIE, a multimodal in-car dialogue
system for an MP3 player. We carried out an
experiment to compare subjective evaluation
judgments and input style alignment behavior
of users interacting with versions of the sys-
tem generating output in personal vs. imper-
sonal style. Although our results are consis-
tent with earlier findings obtained with simu-
lated systems, the effects are weaker.
1 Introduction
One of the goals in developing dialogue systems that
users find appealing and natural is to endow the sys-
tems with contextually appropriate output. This en-
compasses a broad range of research issues. Our
present contribution concerns the generation of per-
sonal and impersonal style.
We define the personal/impersonal style di-
chotomy as reflecting primarily a distinction with
respect to agentivity: personal style involves the ex-
plicit realization of an agent, whereas impersonal
style avoids it. In the simplest way this is mani-
fested by the presence of explicit reference to the di-
alogue participants (typically by means of personal
pronouns) vs. its absence, respectively. More gen-
erally, active voice and finite verb forms are typical
for personal style, whereas impersonal style often,
though not exclusively, employs passive construc-
tions or infinite verb forms:
(1) Typical personal style constructions:
a. I found 20 albums.
b. You have 20 albums.
c. Please search for albums by The Beatles.
(2) Typical impersonal style constructions:
a. 20 albums have been found.
b. There are 20 albums.
c. The database contains 20 albums.
d. 20 albums found.
The dialogue system SAMMIE developed in the
TALK project uses either personal or impersonal out-
put style, employing constructions such as (1a?1c)
and (2a?2d), respectively, to manifest its own and
the user?s agentivity linguistically. We ran an ex-
periment to assess the effects of the system output
style on users? judgments of the system?s usability
and performance and on their input formulation.
In Section 2 we review related work on system
output adaptation and previous experiments con-
cerning the effect of system output style on users?
judgments and style. We describe the SAMMIE sys-
tem and the generation of style variation in Sec-
tion 3. In Section 4 we describe our experiment and
in Section 5 present the results. In Section 6 we pro-
vide a discussion and conclusions.
2 Previous Work
Although recently developed dialogue systems
adapt their output to the users in various ways, this
129
usually concerns content selection rather than sur-
face realization. There is to our knowledge no sys-
tem that varies the style of its output in the in-
terpersonal dimension as we have done in SAM-
MIE. Work on animated conversational agents has
addressed various issues concerning agents display-
ing their personality, but this usually concerns emo-
tional states and personality traits, rather than the
personal/impersonal alteration. (Isard et al, 2006)
model personality and alignment in generated dia-
logues between pairs of agents using OpenCCG and
an over-generation and ranking approach, guided by
a set of language models. Their approach probably
could produce the personal/impersonal style varia-
tion as an effect of personality or a side-effect of
syntactic alignment.
The question whether a system should generate
output in personal or impersonal style has been ad-
dressed by (Nass and Brave, 2005): They observe
that agents that use ?I? are generally perceived more
like a person than those that do not. However, sys-
tems tend to be more positively rated when consis-
tent with respect to such parameters as personality,
gender, ontology (human vs. machine), etc. On
the basis of an investigation of a range of user atti-
tudes to their simulated system with a synthetic vs. a
recorded voice, they conclude that a recorded voice
system is perceived as more human-like and thus en-
titled to use ?I?, whereas a synthetic-voice system is
not perceived as human enough to use ?I? to refer to
itself (Nass et al, 2006).
Another question is whether system output style
influences users? input formulation, as would be ex-
pected due to the phenomenon of alignment, which
is generally considered a basic principle in natural
language dialogue (Garrod and Pickering, 2004).1
Experiments targeting human-human conversa-
tion show that speakers in spontaneous dialogues
tend to express themselves in similar ways at lexi-
cal and syntactic levels (e.g., (Hadelich et al, 2004;
Garrod and Pickering, 2004). Lexical and syntactic
alignment is present in human-computer interaction,
too. (Brennan, 1996) suggested that users adopt
system?s terms to avoid errors, expecting the sys-
1This dialogue phenomenon goes under a variety of terms in
the literature, besides alignment, e.g., accommodation, adapta-
tion, convergence, entrainment or shaping (used, e.g., by (Bren-
nan and Ohaeri, 1994)).
tem to be inflexible. However, recent experiments
show that alignment in human-computer interaction
is also automatic and its strength is comparable to
that in human-human communication (Branigan et
al., 2003; Pearson et al, 2006).
Early results concerning users? alignment to sys-
tem output style in the interpersonal dimension are
reported in (Brennan and Ohaeri, 1994): They dis-
tinguish three styles: anthropomorphic (the system
refers to itself using first person pronouns, like in
(1a) above, fluent (complete sentences, but no self-
reference) and telegraphic, like (2d). They found no
difference in users? perception of the system?s in-
telligence across the different conditions. However,
they observed that the anthropomorphic group was
more than twice as likely to refer to the computer
using the second person pronoun ?you? and it used
more indirect requests and conventional politeness
than the other groups. They conclude that the an-
thropomorphic style is undesirable for dialogue sys-
tems because it encourages more complex user input
which is harder to recognize and interpret.
The described experiments used either the
Wizard-of-Oz paradigm (Brennan and Ohaeri, 1994)
or preprogrammed system output (Branigan et al,
2003; Nass and Brave, 2005) and involved written
communication. Such methods allow one to test as-
sumptions about idealized human-computer interac-
tion. Experimenting with the SAMMIE system al-
lows us to test whether similar effects arise in an in-
teraction with an actual dialogue system, which is
plagued, among other factors, by speech recognition
problems.
3 The SAMMIE System
SAMMIE is a multimodal dialogue system developed
in the TALK project with particular emphasis on mul-
timodal turn-planning and natural language genera-
tion to support intuitive mixed-initiative interaction.
The SAMMIE system provides a multimodal in-
terface to an in-car MP3 player through speech and
haptic input with a BMW iDrive input device, a but-
ton which can be turned, pushed down and sideways
in four directions. System output is by speech and a
graphical display integrated into the car?s dashboard.
SAMMIE has a German and an English version with
the same functionality.
130
The MP3 player application offers a wide range
of tasks: The user can control the currently playing
song, search and browse by looking for fields in the
MP3 database (song, artist, album, etc.), search and
select playlists and construct and edit them. A sam-
ple interaction is shown below (Becker et al, 2006).
(3) U: Show me the Beatles albums.
S: I have these four Beatles albums. [shows a list
of album names]
U: Which songs are on this one? [selects the Red
Album]
S: The Red Album contains these songs [shows a
list of the songs]
U: Play the third one.
S: [song ?From Me To You? plays]
The system puts the user in control of the inter-
action. Input can be given through any modality
and is not restricted to answers to system queries.
On the contrary, the user can provide new tasks as
well as any information relevant to the current task
at any time. This is achieved through modeling the
interaction as a collaborative problem solving (CPS)
process, modeling the tasks and their progression as
recipes and a multimodal interpretation that fits any
user input into the context of the current task (Blay-
lock and Allen, 2005). To support dialogue flexibil-
ity, we model discourse context, the CPS state and
the driver?s attention state by an enriched informa-
tion state (Kruijff-Korbayova? et al, 2006a).
3.1 System Architecture
The SAMMIE system architecture follows the classi-
cal approach of a pipelined architecture with mul-
timodal fusion and fission modules encapsulating
the dialogue manager (Bunt et al, 2005). Figure 1
shows the modules and their interaction: Modality-
specific recognizers and analysers provide seman-
tically interpreted input to the multimodal fusion
module (interpretation manager in Figure 1), that in-
terprets them in the context of the other modalities
and the current dialog context. The dialogue man-
ager decides on the next system move, based on its
CPS encoded task model, on the current context and
also on the results from calls to the MP3 database.
The multimodal fission component then generates
the system reaction on a modality-dependent level
Figure 1: SAMMIE system architecture.
by selecting the content to present, distributing it ap-
propriately over the available output modalities and
finally co-ordinating and synchronizing the output.
Modality-specific output modules generate spoken
output and an update of the graphical display. All
modules interact with the extended information state
in which all context information is stored.
Many tasks in the SAMMIE system are modeled by
a rule-based approach. Discourse modeling, inter-
pretation management, dialogue management, turn
planning and linguistic planning are all based on
the production rule system PATE (Pfleger, 2004;
Kempe, 2004). For speech recognition, we use Nu-
ance. The spoken output is synthesized with the
Mary TTS (Schro?der and Trouvain, 2003).2
3.2 Generation of Natural Language Output
with Variation
To generate natural language output in SAMMIE, we
developed a template-based generator. It is imple-
mented by a set of sentence planning rules in PATE
to build the templates, and a set of XSLT transforma-
tions for sentence realization, which yield the out-
put strings. German and English output is produced
by accessing different dictionaries in a uniform way.
The output is either plain text, if it is to be displayed
in the graphical user interface (e.g., captions in ta-
bles, written messages to the user) or it is text with
mark-up for speech synthesis using the MaryXML
format (Schro?der and Trouvain, 2003), if it is to be
spoken by a speech synthesizer.
2http://mary.dfki.de/
131
The SAMMIE generator can produce alternative
realizations for a given content that it receives as in-
put from the turn planner. The implemented range
of system output variation involves the following as-
pects, which have been determined by an analysis
of a corpus of dialogues collected in a Wizard-of-
Oz experiment using several wizards who were free
to formulate their responses to the users (Kruijff-
Korbayova? et al, 2006b):
1. Personal vs. impersonal style: Ich habe 3 Lieder ge-
funden (I?ve found three songs) vs. 3 Lieder wurden
gefunden (Three songs have been found);
2. Telegraphic vs. non-telegraphic style: 23 Alben ge-
funden (23 albums found) vs. Ich habe 23 Alben
gefunden (I found 23 albums)
3. Reduced vs. non-reduced referring expressions: der
Song ?Kinder An Die Macht? (the song ?Kinder An
Die Macht?) vs. der Song (the song) vs. ?Kinder
An Die Macht? (?Kinder An Die Macht?);
4. Lexical choice for (quasi-)synonyms: Song vs. Lied
vs. Titel (song vs. track)
5. Presence vs. absence of adverbs/adverbials: Ich
spiele jetzt den Song (I?ll now play the song) vs. Ich
spiele den Song (I?ll play the song).
The generation of alternatives is achieved by con-
ditioning the sentence planning and realization de-
cisions. The system can be set either to use one
style consistently throughout a dialogue, or to align
to the user, i.e., mimic the user?s style on a turn-
by-turn basis. For the purpose of experimenting
with system output variation, the generator supports
three sources of control for the available choices:
(a) global (default) parameter settings (resulting in
no variation); (b) random selection (resulting in ran-
dom variation); (c) contextual information (resulting
in variation based on the dialogue context).
The contextual information used by the genera-
tor to control realization includes (i) the grounding
status of the content to be communicated (e.g., to
decide for vs. against reducing a referring expres-
sion); and (ii) linguistic features extracted from the
recognized user input (e.g., to make the correspond-
ing syntactic and lexical choices in the output).
3.3 Personal/Impersonal Style Variation
The style variation in SAMMIE amounts to varying
between active voice for personal style and passive
voice or the ?es-gibt? (?there is?) construction for
impersonal style whenever applicable, as illustrated
for several typical dialogue moves below (where (i)
always shows the impersonal, and (ii) the personal
version).
(4) Search result:3
i. Es gibt 20 Alben.
There are 20 albums.
ii. Ich habe 20 Alben gefunden.
I found 20 albums.
Sie haben 20 Alben. / Du hast 20 Alben.
You have 20 albums
Wir haben 20 Alben.
We have 20 albums.
(5) Song addition:
i. Der Titel Bittersweet Symphony wurde zu
der Playliste 2 hinzugefu?gt.
The track Bittersweet Symphony has been
added to Playlist 2.
ii. Ich habe den Titel Bittersweet Symphony zu
der Playliste 2 hinzugefu?gt.
I added the track Bittersweer Symphony to
Playlist 2.
(6) Song playback:
i. Der Titel Ma?nner von Herbert Gro?nemeyer
wird gespielt.
The track Ma?nner by Herbert Gro?nemeyer is
playing.
ii. Ich spiele den Titel Ma?nner von Herbert
Gro?nemeyer.
I am playing the track Ma?nner by Herbert
Gro?nemeyer.
(7) Non-understanding:
i. Das wurde leider nicht verstanden.
That has unfortunately not been understood.
ii. Das habe ich leider nicht verstanden.
I have unfortunately not understood that.
(8) Clarification request:
i. Welches von diesen acht Liedern?/Welches
von diesen acht Liedern wird gewu?nscht?
Which of these eight songs? / Which of these
eight songs is desired?
ii. Welches von diesen acht Liedern mo?chtest du
/ mo?chten Sie ho?ren?
Which of these eight songs would you like to
hear?
3When referring to the user, personal style has several vari-
ants which differ in formality (formal and informal address) and
first vs. second person reference.
132
Figure 2: Experiment setup
The personal/impersonal style variation is not ap-
plicable for some dialogue moves, e.g., (9), and for
output in telegraphic style.
(9) Song interpreter:
Der Titel Bongo Girl ist von Nena.
The track Bongo Girl is by Nena.
4 Experiment
In order to assess the effects of style manipulation in
the SAMMIE system, we ran an experiment in simu-
lated driving conditions, comparing two versions of
the system: one consistently using personal and the
other impersonal style output.4 The experiment em-
ployed the German version of SAMMIE. The setup
(see Figure 2), participants, procedure and collected
data are described in detail in (Kruijff-Korbayova?
and Kukina, 2008), and summarized below.
There were 28 participants, all native speakers
of German. We balanced gender and background
when assigning them to the style conditions. The
experiment followed a fixed script for each partici-
pant: welcome, instruction, warm-up driving, 2 trial
and 11 experimental tasks, evaluation questionnaire,
payment and farewell. The participants were in-
structed to use mainly spoken input, although they
could also use the iDrive button. It took them about
40 minutes to complete all the tasks. The tasks in-
volved exploring the contents of a database of about
25 music albums and were of four types: (1) find-
ing some specified title(s); (2) selecting some title(s)
4For the time being we have not evaluated the version of the
system aligning to the user?s style.
satisfying certain constraints; (3) manipulating the
playlists by adding or removing songs and (4) free-
use of the system.
The experimental tasks were presented to each
participant in randomized order apart from the free
use of the system, which was always the last task.
The experimenter (E) repeated each task assignment
twice to the participant, once in personal and once
in impersonal style, as shown in the example below.
(10) E: Bitte frage das System nach den Liedern von
?Pur?. Du willst also wissen welche Lieder von
?Pur? es gibt.
E: Please ask the the system about the songs by
?Pur?. You would like to know which songs by
?Pur? there are.
The questionnaire was based on (Nass and Brave,
2005) and (Mutschler et al, 2007). It contained
questions with a 6-point scale ranging from 1 (low
grade) to 6 (high grade), such as How do you assess
the system in general: technical (1) ? human-like
(6); Communication with the system seemed to you:
boring (1) ? exciting (6); In terms of usability, the
system is: inefficient (1) ?efficient(6).
The recorded dialogues have been transcribed, the
questionnaire responses tabulated. We manually an-
notated the participants? utterances (on average 95
per session) with the following features for further
analysis:
? Construction type:
Personal (+/-) Is the utterance a complete sen-
tence in active voice or imperative form
Impersonal (+/-) Is the utterance expressed
by passive voice, infinite verb form (e.g.,
?Lied abspielen? (lit. ?song play?)), or ex-
pletive ?es-gibt? (?there-is?) construction
Telegraphic (+/-) Is the utterance expressed
by a phrase, e.g., ?weiter? (?next?)
? Personal pronouns: (+/-) Does the utterance
contain a first or second person pronoun
? Politeness marking: (+/-) Does the utterance
contain a politeness marker, such as ?bitte?
(?please?), ?danke? (?thanks?) and verbs in
subjunctive mood (eg. ?ich ha?tte gerne?)
133
5 Results
The results concerning users? attitudes and align-
ment are presented in detail in (Kruijff-Korbayova?
and Kukina, 2008). Here we summarize the signif-
icant findings and provide an additional analysis of
the influence of speech recognition problems.
5.1 Style and Users? Attitudes
The first issue addressed in the experiment was
whether the users have different judgments of the
personal vs. impersonal version of the system. Since
the system used a synthetic voice, the judgments
were expected to be more positive in the impersonal
style condition (Nass and Brave, 2005). Based on
factor analysis performed on attitudinal data from
the user questionnaires we created the six indices
listed below. All indices were meaningful and reli-
able. (A detailed description of the indices including
the contributing factors from the questionnaires can
be found in (Kruijff-Korbayova? and Kukina, 2008).)
1. General satisfaction with the communication
with the system (Cronbach?s ?=0.86)
2. Easiness of communication with the system
(?=0.83)
3. Usability of the system (?=0.76)
4. Clarity of the system?s speech (?=0.88)
5. Perceived ?humanness? of the system (?=0.69)
6. System?s perceived flexibility and creativity
(?=0.78)
We did not find any significant influence of sys-
tem output style on users? attitudes. Only for per-
ceived humanness of the system we found a weak
tendency in the predicted direction (independent
samples test: t(25)=1.64, p=0.06 (one-tailed)), in
line with the earlier observation that an interface that
refers to itself by a personal pronoun is perceived to
be more human-like than one that does not (Nass and
Brave, 2005).
5.2 Style and Alignment
The next issue we investigated was whether the users
formulated their input differently in the personal vs.
impersonal system version. For each dialogue ses-
sion, we calculated the percentage of utterances con-
taining the feature of interest relative to the total
number of user utterances in the session.
In accordance with the expectation based on style
alignment in terms of agentivity, we observed a sig-
nificant difference in the number of personal con-
structions across style conditions (t(19)=1.8, p=0.05
(one-tailed)). But we did not find a significant dif-
ference in the distribution of impersonal construc-
tions. Not surprisingly, there was also no signifi-
cant difference in the distribution of telegraphic con-
structions. An unexpected finding was the higher
proportion of telegraphic constructions than verb-
containing ones within the impersonal style condi-
tion (t(13)=3.5, p<0.001 (one-tailed)). However, no
such difference was found in the personal style con-
dition. Contrary to expectations, we also did not find
any significant effect of style-manipulation on the
number of personal pronouns, nor on the number of
politeness markers.
Since alignment can also be seen as a process
of gradual adjustment among dialogue participants
over time we compared the proportion of personal,
impersonal and telegraphic constructions in the first
and second halves of the conversations for both style
conditions. The only significant effect we found was
a decrease in the number of personal constructions
in the second halves of the impersonal style interac-
tions (t(13)=2.5, p=0.02 (one-tailed)).
5.3 Influence of Speech Recognition Problems
Unlike an interaction in a Wizard-of-Oz simulation
or similar, an interaction with a real system is bound
to suffer from speech recognition problems. There-
fore, we made a post-hoc analysis with respect to
how much speech recognition difficulty the partici-
pants experienced, in terms of the proportion of par-
ticipant utterances not recognized by the system rel-
ative to the total number of participant utterances in
a session.
On average, around 33% of participant utterances
were not understood by the system.5 We classi-
fied the participants into three groups according to
the performance of speech recognition they expe-
rienced: the good group with less than 27% of in-
put not understood (7 participants); the poor group
5This is admittedly rather bad performance, nevertheless it
mostly does not prevent the participants from getting their tasks
successfully completed within a reasonable time, as was shown
in an rigorous usability evaluation of the system in normal driv-
ing conditions (Mutschler et al, 2007).
134
Figure 3: Judgments of the system by the ?good? and ?poor? speech recognition group
with more than 37% of input not uderstood (7 par-
ticipants); the average group (the remaining 14 par-
ticipants).
Speech Recognition and Attitudinal Data We
suspected that speech recognition problems might
be neutralizing a potential influence of style. There-
fore we contrasted the judgments on all six factors
between the good and the poor speech recognition
group (see Figure 3). The ?good? speech recognition
group showed higher satisfaction with the communi-
cation (t(16)=1.9, p=0.04 (one-tailed)) and evaluated
the clarity of the system?s speech better (t(16)= 2.0,
p=0.03 (one-tailed)). The good speech recognition
group also showed a tendency to assess the usabil-
ity and flexibility of the system higher than the poor
speech recognition group (t(16)=1.71, p=0.05 and
t(16)=1.61, p=0.06, respectively (marginally signif-
icant results)). The two groups did not differ with
respect to their judgments of the ease of commu-
nication and perceived humanness of the system
(t(16)=0.45, p=0.66 and t(16)=0.90, p=0.38). These
results are not surprising. They confirm that speech
recognition does have an effect on the user?s percep-
tion of the system.
Speech Recognition and Style Alignment We
also checked post-hoc whether differences in the ex-
perienced speech recognition performance had an
influence on the style employed by the participants,
again in terms of the proportion of utterances with
personal, impersonal and telegraphic constructions,
personal pronouns and politeness marking. How-
ever, we found no significant effect on the linguistic
structure of the participant input across the groups
(politeness marking: F(2)=1.5, p=0.24; all other
Fs<1 (ANOVA)).
6 Discussion and Conclusions
We presented the generation of personal/impersonal
style variation in the SAMMIE multimodal dialogue
system, and the results of an experiment evaluating
the influence of the system output style on the users?
subjective judgments and their formulation of input.
Although our results are not conclusive, they point
at a range of issues for further research.
Regarding users? attitudes to the system, we
found no significant difference among the styles.
This is similar to (Brennan and Ohaeri, 1994) who
found no difference in intelligence attributed to the
system by the users, but it is at odds with the earlier
finding that a synthetic voice interface was judged
to be more useful when avoiding self-reference by
personal pronouns (Nass and Brave, 2005).
Whereas (Brennan and Ohaeri, 1994) used a flight
reservation dialogue system, (Nass and Brave, 2005)
used a phone-based auction system which read out
an introduction and five object descriptions. There
are two points to note: First, the subjects heard
system output that was a read out continuous text
rather than turns in an interaction. This may have
reinforced the activation of particular style features.
Second, the auction task may have sensibilized the
subjects to the distinction between subjective (the
system?s) vs. objective information presentation,
and thus make them more sensitive to whether the
system presents itself as an active agent or not.
Regarding the question whether users align their
style to that of the system, where previous experi-
ments showed strong effects of alignment (Brennan
and Ohaeri, 1994), our experiment shows some ef-
fects, but some of the results are conflicting. On
the one hand, subjects interacting with the personal
style version of the system used more personal con-
structions than those interacting with the impersonal
style version. However, subjects in either condi-
135
tion did not show any significant difference with re-
spect to the use of impersonal constructions or tele-
graphic forms. We also found a higher proportion of
telegraphic constructions than verb-containing ones
within the impersonal style condition, but no such
difference in the personal style. Finally, when we
considered alignment over time, we found no change
in construction use in the personal style, whereas we
found a decrease in the use of personal constructions
in the impersonal style. It is possible that divid-
ing the interactions into three parts and comparing
alignment in the first and the last part might lead to
stronger results.
That there is no difference in the use of tele-
graphic constructions across conditions is not sur-
prising. Being just phrasal sentence fragments, these
constructions are neutral with respect to style. But
why does there seem to be an alignment effect for
personal constructions and not for others? One way
of explaining this is that (some of) the construc-
tions that we counted as impersonal are common in
both styles. Besides their deliberate use as means
to avoid explicit reference to oneself, they also have
their normal, neutral usage, and therefore, some of
the utterances that we classified as impersonal style
may just be neutral formulations, rather than cases
of distancing or ?de-agentivization?. However, we
could not test this hypothesis, because we have not
found a way to reliably distinguish between neutral
and marked, truly impersonal utterances. This is an
issue for future work.
The difference between our results concerning
alignment and those of (Brennan and Ohaeri, 1994)
is not likely to be due to a difference in the degree
of interactivity (as with (Nass and Brave, 2005)).
We now comment on other differences between our
systems, which might have contributed to the differ-
ences in results.
One aspect where we differ concerns our distinc-
tion between personal and impersonal style, both in
the implementation of the SAMMIE system and in
the experiment: We include the presence/absence
of agentivity not only in the system?s reference to
itself (akin to (Nass and Brave, 2005) and (Bren-
nan and Ohaeri, 1994)), but also in addressing the
user. This concept of the personal/impersonal dis-
tinction was inspired by such differences observed
in a study of instructional texts in several languages
(Kruijff et al, 1999), where the latter dimension is
predominant. The present experiment results make
it pertinent that more research into the motives be-
hind expressing or suppressing agentivity in both di-
mensions is needed.
Apart from the linguistic design of the system?s
output, other factors influence users? behavior and
perception of the system, and thus might confound
experiment results, e.g., functionality, design, er-
gonomics, speech synthesis and speech recognition.
A system with synthesized speech should be more
positively rated when it does not refer to itself as an
active agent by personal pronouns (Nass and Brave,
2005). (Brennan and Ohaeri, 1994) used a sys-
tem with written interaction, the SAMMIE system
employs the MARY text-to-speech synthesis system
(Schro?der and Trouvain, 2003) with an MBROLA
diphone synthesiser, which produces an acceptable
though not outstanding output quality. Our post-hoc
analysis showed a tendency towards better judge-
ments of the system by the participants experienc-
ing less speech recognition problems. This is as
expected. We did not find any statistically signif-
icant effect regarding the style-related features we
analyzed. A future experiment should address the
possibility of an interaction between system style
and speech recognition performance as both factors
might be influencing the user simultaneously.
One radical difference between our experiment
and the earlier ones is that the users of the SAMMIE
system are occupied by the driving task, and thus
only have a limited cognitive capacity left for the
interaction with the system. This may make them
less susceptible to the subtleties of style manipula-
tion than would be the case if they were free of other
tasks. A possible future experiment could address
this issue by including a non-driving condition.
Finally, the SAMMIE system has also the style-
alignment mode, where it mimics the user?s style on
turn-to-turn basis. We plan to present experimental
results comparing the alignment-mode with the fixed
personal/impersonal style in a future publication.
Acknowledgments
This work was carried out in the TALK project
(www.talk-project.org) funded by the EU as project
No. IST-507802 within the 6th Framework Program.
136
References
T. Becker, N. Blaylock, C. Gerstenberger, I. Kruijff-
Korbayova?, A. Korthauer, M. Pinkal, M. Pitz, P. Poller,
and J. Schehl. 2006. Natural and intuitive multimodal
dialogue for in-car applications: The SAMMIE system.
In Proceedings of ECAI, PAIS Special section.
N. Blaylock and J. Allen. 2005. A collaborative
problem-solving model of dialogue. In L. Dybkj?r
and W. Minker, editors, Proceedings of the 6th SIGdial
Workshop on Discourse and Dialogue, pages 200?211,
Lisbon, September 2?3.
H. Branigan, M. Pickering, J. Pearson, J. F. McLean, and
C. Nass. 2003. Syntactic alignment between com-
puter and people: the role of belief about mental states.
In Proceedings of the Annual Conference of the Cog-
nitive Science Society.
S. Brennan and J.O. Ohaeri. 1994. Effects of mes-
sage style on user?s attribution toward agents. In Pro-
ceedings of CHI?94 Conference Companion Human
Factors in Computing Systems, pages 281?282. ACM
Press.
S. Brennan. 1996. Lexical entrainment in spontaneous
dialogue. In Proceedings of the International Sympo-
sium on Spoken Dialogue (ISSD-96), pages 41?44.
H. Bunt, M. Kipp, M. Maybury, and W. Wahlster. 2005.
Fusion and coordination for multimodal interactive in-
formation presentation: Roadmap, architecture, tools,
semantics. In O. Stock and M. Zancanaro, editors,
Multimodal Intelligent Information Presentation, vol-
ume 27 of Text, Speech and Language Technology,
pages 325?340. Kluwer Academic.
S. Garrod and M. Pickering. 2004. Why is conversation
so easy? TRENDS in Cognitive Sciences, 8.
K. Hadelich, H. Branigan, M. Pickering, and M. Crocker.
2004. Alignment in dialogue: Effects of feedback
on lexical overlap within and between participants.
In Proceedings of the AMLaP Conference. Aix en
Provence, France.
Amy Isard, Carsten Brockmann, and Jon Oberlander.
2006. Individuality and alignment in generated di-
alogues. In Proceedings of the 4th International
Natural Language Generation Conference (INLG-06),
pages 22?29, Sydney, Australia.
Benjamin Kempe. 2004. PATE a production rule sys-
tem based on activation and typed feature structure ele-
ments. Bachelor Thesis, Saarland University, August.
G.J.M. Kruijff, I. Kruijff-Korbayova?, J. Bateman,
D. Dochev, N. Gromova, T. Hartley, E. Teich,
S. Sharoff, L. Sokolova, and K. Staykova. 1999.
Deliverable TEXS2: Specification of elaborated text
structures. Technical report, AGILE Project, EU
INCO COPERNICUS PL961104.
I. Kruijff-Korbayova? and O. Kukina. 2008. The effect of
dialogue system output style variation on users? eval-
uation judgements and input style. In Proceedings of
SigDial?08, Columbus, Ohio.
I. Kruijff-Korbayova?, G. Amores, N. Blaylock, S. Eric-
sson, G. Pe?rez, K. Georgila, M. Kaisser, S. Larsson,
O. Lemon, P. Mancho?n, and J. Schehl. 2006a. De-
liverable D3.1: Extended information state modeling.
Technical report, TALK Project, EU FP6, IST-507802.
Ivana Kruijff-Korbayova?, Tilman Becker, Nate Blaylock,
Ciprian Gerstenberger, Michael Kaisser, Peter Poller,
Verena Rieser, and Jan Schehl. 2006b. The SAMMIE
corpus of multimodal dialogues with an MP3 player.
In Proceedings of LREC, Genova, Italy.
H. Mutschler, F. Steffens, and A. Korthauer. 2007. De-
liverable D6.4: Final report on multimodal experi-
ments Part I: Evaluation of the SAMMIE system. Tech-
nical report, TALK Project, EU FP6, IST-507802.
C. Nass and S. Brave, 2005. Should voice interfaces say
?I?? Recorded and synthetic voice interfaces? claims
to humanity, chapter 10, pages 113?124. The MIT
Press, Cambridge.
C. Nass, S. Brave, and L. Takayama. 2006. Socializing
consistency: from technical homogeneity to human
epitome. In P. Zhang & D. Galletta (Eds.), Human-
computer interaction in management information sys-
tems: Foundations, pages 373?390. Armonk, NY: M.
E. Sharpe.
J. Pearson, J. Hu, H. Branigan, M. J. Pickering, and C. I.
Nass. 2006. Adaptive language behavior in HCI: how
expectations and beliefs about a system affect users?
word choice. In CHI ?06: Proceedings of the SIGCHI
conference on Human Factors in computing systems,
pages 1177?1180, New York, NY, USA. ACM.
N. Pfleger. 2004. Context based multimodal fusion. In
ICMI ?04: Proceedings of the 6th international confer-
ence on Multimodal interfaces, pages 265?272, New
York, NY, USA. ACM Press.
M. Schro?der and J. Trouvain. 2003. The German text-to-
speech synthesis system MARY: A tool for research,
development and teaching. International Journal of
Speech Technology, 6:365?377.
137
