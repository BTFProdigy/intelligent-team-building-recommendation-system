Proceedings of the 12th Conference of the European Chapter of the ACL, pages 406?414,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Parsing Coordinations
Sandra Ku?bler
Indiana University
skuebler@indiana.edu
Erhard Hinrichs
Universita?t Tu?bingen
eh@sfs.uni-tuebingen.de
Wolfgang Maier
Unversita?t Tu?bingen
wo.maier@uni-tuebingen.de
Eva Klett
Universita?t Tu?bingen
eklett@sfs.uni-tuebingen.de
Abstract
The present paper is concerned with sta-
tistical parsing of constituent structures
in German. The paper presents four ex-
periments that aim at improving parsing
performance of coordinate structure: 1)
reranking the n-best parses of a PCFG
parser, 2) enriching the input to a PCFG
parser by gold scopes for any conjunct, 3)
reranking the parser output for all possi-
ble scopes for conjuncts that are permissi-
ble with regard to clause structure. Exper-
iment 4 reranks a combination of parses
from experiments 1 and 3.
The experiments presented show that n-
best parsing combined with reranking im-
proves results by a large margin. Provid-
ing the parser with different scope possi-
bilities and reranking the resulting parses
results in an increase in F-score from
69.76 for the baseline to 74.69. While the
F-score is similar to the one of the first ex-
periment (n-best parsing and reranking),
the first experiment results in higher re-
call (75.48% vs. 73.69%) and the third one
in higher precision (75.43% vs. 73.26%).
Combining the two methods results in the
best result with an F-score of 76.69.
1 Introduction
The present paper is concerned with statistical
parsing of constituent structures in German. Ger-
man is a language with relatively flexible phrasal
ordering, especially of verbal complements and
adjuncts. This makes processing complex cases
of coordination particularly challenging and error-
prone. The paper presents four experiments that
aim at improving parsing performance of coor-
dinate structures: the first experiment involves
reranking of n-best parses produced by a PCFG
parser, the second experiment enriches the input
to a PCFG parser by offering gold pre-bracketings
for any coordinate structures that occur in the sen-
tence. In the third experiment, the reranker is
given all possible pre-bracketed candidate struc-
tures for coordinated constituents that are permis-
sible with regard to clause macro- and microstruc-
ture. The parsed candidates are then reranked.
The final experiment combines the parses from the
first and the third experiment and reranks them.
Improvements in this final experiment corroborate
our hypothesis that forcing the parser to work with
pre-bracketed conjuncts provides parsing alterna-
tives that are not present in the n-best parses.
Coordinate structures have been a central is-
sue in both computational and theoretical linguis-
tics for quite some time. Coordination is one of
those phenomena where the simple cases can be
accounted for by straightforward empirical gen-
eralizations and computational techniques. More
specifically, it is the observation that coordination
involves two or more constituents of the same cat-
egories. However, there are a significant number
of more complex cases of coordination that defy
this generalization and that make the parsing task
of detecting the right scope of individual conjuncts
and correctly delineating the correct scope of the
coordinate structure as a whole difficult. (1) shows
some classical examples of this kind from English.
(1) a. Sandy is a Republican and proud of it.
b. Bob voted, but Sandy did not.
c. Bob supports him and Sandy me.
In (1a), unlike categories (NP and adjective) are
conjoined. (1b) and (1c) are instances of ellipsis
(VP ellipsis and gapping). Yet another difficult set
of examples present cases of non-constituent con-
junction, as in (2), where the direct and indirect
object of a ditransitive verb are conjoined.
(2) Bob gave a book to Sam and a record to Jo.
406
2 Coordination in German
The above phenomena have direct analogues in
German.1 Due to the flexible ordering of phrases,
their variability is even higher. For example, due
to constituent fronting to clause-initial position in
German verb-second main clauses, cases of non-
constituent conjunction can involve any two NPs
(including the subject) of a ditransitive verb to the
exclusion of the third NP complement that appears
in clause-initial position. In addition, German ex-
hibits cases of asymmetric coordination first dis-
cussed by Ho?hle (1983; 1990; 1991) and illus-
trated in (3).2
(3) In
Into
den
the
Wald
woods
ging
went
ein
a
Ja?ger
hunter
und
and
schoss
shot
einen
a
Hasen.
hare.
Such cases of subject gap coordination are fre-
quently found in text corpora (cf. (4) below) and
involve conjunction of a full verb-second clause
with a VP whose subject is identical to the subject
in the first conjunct.
3 Experimental Setup and Baseline
3.1 The Treebank
The data source used for the experiments is the
Tu?bingen Treebank of Written German (Tu?Ba-
D/Z) (Telljohann et al, 2005). Tu?Ba-D/Z uses
the newspaper ?die tageszeitung? (taz) as its data
source, version 3 comprises approximately 27 000
sentences. The treebank annotation scheme dis-
tinguishes four levels of syntactic constituency:
the lexical level, the phrasal level, the level of
topological fields, and the clausal level. The pri-
mary ordering principle of a clause is the inventory
of topological fields (VF, LK, MF, VC, and NF),
which characterize the word order regularities
among different clause types of German. Tu?Ba-
D/Z annotation relies on a context-free backbone
(i.e. proper trees without crossing branches) of
phrase structure combined with edge labels that
specify the grammatical function of the phrase in
question. Conjuncts are generally marked with the
1To avoid having to gloss German examples, they were
illustrated for English.
2Yet, another case of such asymmetric coordination dis-
cussed by Ho?hle involves cases of conjunction of different
clause types: [V?final Wenn du nach Hause kommst ] und
[V?2nd da warten Polizeibeamte vor der Tu?r. ?If you come
home and there are policemen waiting in front of the door ] .?
function label KONJ. Figure 1 shows the anno-
tation that sentence (4) received in the treebank.
Syntactic categories are displayed as nodes, gram-
matical functions as edge labels in gray (e.g. OA:
direct object, PRED: predicate). This is an exam-
ple of a subject-gap coordination, in which both
conjuncts (FKONJ) share the subject (ON) that is
realized in the first conjunct.
(4) Damit
So
hat
has
sich
itself
der
the
Bevo?lkerungs-
decline in
ru?ckgang
population
zwar
though
abgeschwa?cht,
lessened,
ist
is
aber
however
noch
still
doppelt
double
so
so
gro?
big
wie
as
1996.
1996.
?For this reason, although the decline in
population has lessened, it is still twice as
big as in 1996.?
The syntactic annotation scheme of the Tu?Ba-
D/Z is described in more detail in Telljohann et al
(2004; 2005).
All experiments reported here are based on a
data split of 90% training data and 10% test data.
3.2 The Parsers and the Reranker
Two parsers were used to investigate the influ-
ence of scope information on parser performance
on coordinate structures: BitPar (Schmid, 2004)
and LoPar (Schmid, 2000). BitPar is an effi-
cient implementation of an Earley style parser that
uses bit vectors. However, BitPar cannot han-
dle pre-bracketed input. For this reason, we used
LoPar for the experiments where such input was
required. LoPar, as it is used here, is a pure
PCFG parser, which allows the input to be par-
tially bracketed. We are aware that the results
that can be obtained by pure PCFG parsers are
not state of the art as reported in the shared task
of the ACL 2008 Workshop on Parsing German
(Ku?bler, 2008). While BitPar reaches an F-score
of 69.76 (see next section), the best performing
parser (Petrov and Klein, 2008) reaches an F-
score of 83.97 on Tu?Ba-D/Z (but with a different
split of training and test data). However, our ex-
periments require certain features in the parsers,
namely the capability to provide n-best analyses
and to parse pre-bracketed input. To our knowl-
edge, the parsers that took part in the shared task
do not provide these features. Should they become
available, the methods presented here could be ap-
plied to such parsers. We see no reason why our
407
Figure 1: A tree with coordination.
methods should not be able to improve the results
of these parsers further.
Since we are interested in parsing coordina-
tions, all experiments are conducted with gold
POS tags, so as to abstract away from POS tag-
ging errors. Although the treebank contains mor-
phological information, this type of information is
not used in the experiments presented here.
The reranking experiments were conducted us-
ing the reranker by Collins and Koo (2005). This
reranker uses a set of candidate parses for a sen-
tence and reranks them based on a set of features
that are extracted from the trees. The reranker uses
a boosting method based on the approach by Fre-
und et al (1998). We used a similar feature set
to the one Collins and Koo used; the following
types of features were included: rules, bigrams,
grandparent rules, grandparent bigrams, lexical
bigrams, two-level rules, two-level bigrams, tri-
grams, head-modifiers, PPs, and distance for head-
modifier relations, as well as all feature types in-
volving rules extended by closed class lexicaliza-
tion. For a more detailed description of the rules,
the interested reader is referred to Collins and
Koo (2005). For coordination, these features give
a wider context than the original parser has and
should thus result in improvements for this phe-
nomenon.
3.3 The Baseline
When trained on 90% of the approximately 27,000
sentences of the Tu?Ba-D/Z treebank, BitPar
reaches an F-Score of 69.73 (precision: 68.63%,
recall: 70.93%) on the full test set of 2611 sen-
tences. These results as well as all further re-
sults presented here are labeled results, including
grammatical functions. Since German has a rela-
tively free word order, it is impossible to deduce
the grammatical function of a noun phrase from
the configuration of the sentence. Consequently,
an evaluation based solely on syntactic constituent
labels would be meaningless (cf. (Ku?bler, 2008)
for a discussion of this point). The inclusion of
grammatical labels in the trees, makes the parsing
process significantly more complex.
Looking at sentences with coordination (i.e.
sentences that contain a conjunction which is not
in sentence-initial position), we find that 34.9%
of the 2611 test sentences contain coordinations.
An evaluation of only sentences with coordina-
tion shows that there is a noticeable difference: the
F-score reaches 67.28 (precision: 66.36%, recall:
68.23%) as compared to 69.73 for the full test set.
The example of a wrong parse shown below il-
lustrates why parsing of complex coordinations is
so hard. Complex coordinations can take up a con-
siderable part of the input string and accordingly
of the overall sentence structure. Such global phe-
nomena are particularly hard for pure PCFG pars-
ing, due to the independence assumption inherent
in the statistical models for PCFGs.
Sentence (4) has the following Viterbi parse:
(VROOT
(SIMPX
(VF
(SIMPX-OS
(VF (PX-MOD (PROP-HD Damit)))
(LK
(VXFIN-HD (VAFIN-HD hat)))
(MF
408
(NX-OA (PRF-HD sich))
(NX-ON (ART der)
(NN-HD Bevo?lkerungsru?ckgang))
(ADVX-MOD (ADV-HD zwar)))
(VC (VXINF-OV
(VVPP-HD abgeschwa?cht)))))
($, ,)
(LK
(VXFIN-HD (VAFIN-HD ist)))
(MF
(ADVX-MOD (ADV-HD aber))
(ADVX-MOD (ADV-HD noch))
(ADJX-PRED
(ADJX-HD (ADVX (ADV-HD mehr))
(ADJX (KOKOM als)
(ADJD-HD doppelt))
(ADVX (ADV-HD so))
(ADJD-HD gro?))
(NX (KOKOM wie)
(CARD-HD 1996)))))
($. .))
The parse shows that the parser did not
recognize the coordination. Instead, the first con-
junct including the fronted constituent, Damit
hat sich der Bevo?lkerungsru?ckgang
zwar abgeschwa?cht, is treated as a fronted
subordinate clause.
4 Experiment 1: n-Best Parsing and
Reranking
The first hypothesis for improving coordination
parsing is based on the assumption that the correct
parse may not be the most probable one in Viterbi
parsing but may be recovered by n-best parsing
and reranking, a technique that has become stan-
dard in the last few years. If this hypothesis holds,
we should find the correct parse among the n-best
parses. In order to test this hypothesis, we con-
ducted an experiment with BitPar (Schmid, 2004).
We parsed the test sentences in a 50-best setting.
A closer look at the 50-best parses shows that of
the 2611 sentences, 195 (7.5%) were assigned the
correct parse as the best parse. For 325 more sen-
tences (12.4%), the correct parse could be found
under the 50 best analyses. What is more, in
90.2% of these 520 sentences, for which the cor-
rect parse was among the 50 best parses, the best
parse was among the first 10 parses. Additionally,
only in 4 cases were the correct analyses among
the 40-best to 50-best parses, an indication that in-
creasing n may not result in improving the results
significantly. These findings resulted in the deci-
sion not to conduct experiments with higher n.
That the 50 best analyses contain valuable infor-
mation can be seen from an evaluation in which an
oracle chooses from the 50 parses. In this case, we
reach an F-score of 80.28. However, this F-score
is also the upper limit for improvement that can be
achieved by reranking the 50-best parses.
For reranking, the features of Collins and
Koo (2005) were extended in the following way:
Since the German treebank used for our exper-
iments includes grammatical function informa-
tion on almost all levels in the tree, all feature
types were also included with grammatical func-
tions attached: All nodes except the root node
of the subtree in question were annotated with
their grammatical information. Thus, for the noun
phrase (NX) rule with grandparent prepositional
phrase (PX) PXGP NX? ART ADJX NN, we add
an additional rule PXGP NX-HD ? ART ADJX
NN-HD.
After pruning all features that occurred in the
training data with a frequency lower than 5, the ex-
tractions produced more than 5 mio. different fea-
tures. The reranker was optimized on the training
data, the 50-best parses were produced in a 5-fold
cross-validation setting. A non-exhaustive search
for the best value for the ? parameter showed that
Collins and Koo?s value of 0.0025 produced the
best results. The row for exp. 1 in Table 1 shows
the results of this experiment. The evaluation of
the full data set shows an improvement of 4.77
points in the F-score, which reached 74.53. This is
a relative reduction in error rate of 18.73%, which
is slightly higher that the error rate reduction re-
ported by Collins and Koo for the Penn Treebank
(13%). However, the results for Collins and Koo?s
original parses were higher, and they did not eval-
uate on grammatical functions.
The evaluation of coordination sentences shows
that such sentences profit from reranking to the
same degree. These results prove that while coor-
dination structures profit from reranking, they do
not profit more than other phenomena. We thus
conclude that reranking is no cure-all for solving
the problem of accurate coordination parsing.
5 Experiment 2: Gold Scope
The results of experiment 1 lead to the conclusion
that reranking the n-best parses can only result
in restricted improvements on coordinations. The
fact that the correct parse often cannot be found
in the 50-best analyses suggests that the different
possible scopes of a coordination are so different
in their probability distribution that not all of the
possible scopes are present in the 50-best analyses.
409
all sentences coord. sentences
precision recall F-score precision recall F-score
baseline: 68.63 70.93 69.76 66.36 68.23 67.28
exp. 1: 50-best reranking: 73.26 75.84 74.53 70.67 72.72 71.68
exp. 2: with gold scope: 76.12 72.87 74.46 75.78 72.22 73.96
exp. 3: automatic scope: 75.43 73.96 74.69 72.88 71.42 72.14
exp. 4: comb. 1 and 3: 76.15 77.23 76.69 73.79 74.73 74.26
Table 1: The results of parsing all sentences and coordinated sentences only
If this hypothesis holds, forcing the parser to con-
sider the different scope readings should increase
the accuracy of coordination parsing. In order to
force the parser to use the different scope readings,
we first extract these scope readings, and then for
each of these scope readings generate a new sen-
tence with partial bracketing that represents the
corresponding scope (see below for an example).
LoPar is equipped to parse partially-bracketed in-
put. Given input sentences with partial brackets,
the parser restricts analyses to such cases that do
not contradict the brackets in the input.
(5) Was
Which
stimmt,
is correct,
weil
because
sie
they
unterhaltsam
entertaining
sind,
are,
aber
but
auch
also
falsche
wrong
Assoziationen
associations
weckt.
wakes.
?Which is correct because they are enter-
taining, but also triggers wrong associa-
tions.?
In order to test the validity of this hypothe-
sis, we conducted an experiment with coordination
scopes extracted from the treebank trees. These
scopes were translated into partial brackets that
were included in the input sentences. For the sen-
tence in (5) from the treebank (sic), the input for
LoPar would be the following:
Was/PWS stimmt/VVFIN ,/$, weil/
KOUS ( sie/PPER unterhalt-
sam/ADJD sind/VAFIN ) ,/$,
aber/KON ( auch/ADV falsche/ADJA
Assoziationen/NN weckt/VVFIN )
The round parentheses delineate the conjuncts.
LoPar was then forced to parse sentences contain-
ing coordination with the correct scope for the co-
ordination. The results for this experiment are
shown in Table 1 as exp. 2.
The introduction of partial brackets that delimit
the scope of the coordination improve overall re-
sults on the full test set by 4.7 percent points, a
rather significant improvement when we consider
that only approximately one third of the test sen-
tences were modified. The evaluation of the set
of sentences that contain coordination shows that
here, the difference is even higher: 6.7 percent
points. It is also worth noticing that provided with
scope information, the parser parses such sen-
tences with the same accuracy as other sentences.
The difference in F-scores between all sentences
and only sentences with coordination in this ex-
periment is much lower (0.5 percent points) than
for all other experiments (2.5?3.0 percent points).
When comparing the results of experiment 1 (n-
best parsing) with the present one, it is evident that
the F-scores are very similar: 74.53 for the 50-best
reranking setting, and 74.46 for the one where we
provided the gold scope. However, a comparison
of precision and recall shows that there are differ-
ences: 50-best reranking results in higher recall,
providing gold scope for coordinations in higher
precision. The lower recall in the latter experiment
indicates that the provided brackets in some cases
are not covered by the grammar. This is corrob-
orated by the fact that in n-best parsing, only 1
sentence could not be parsed; but in parsing with
gold scope, 8 sentences could not be parsed.
6 Experiment 3: Extracting Scope
The previous experiment has shown that providing
the scope of a coordination drastically improves
results for sentences with coordination as well as
for the complete test set (although to a lower de-
gree). The question that remains to be answered is
whether automatically generated possible scopes
can provide enough information for the reranker
to improve results.
The first question that needs to be answered is
how to find the possible scopes for a coordina-
tion. One possibility is to access the parse forest
of a chart parser such as LoPar and extract infor-
410
mation about all the possible scope analyses that
the parser found. If the same parser is used for
this step and for the final parse, we can be cer-
tain that only scopes are extracted that are com-
patible with the grammar of the final parser. How-
ever, parse forests are generally stored in a highly
packed format so that an exhaustive search of the
structures is very inefficient and proved impossi-
ble with present day computing power.
(6) ?Es
?There
gibt
are
zwar
indeed
ein
a
paar
few
Niederflurbusse,
low-floor buses,
aber
but
das
that
reicht
suffices
ja
part.
nicht?,
not?,
sagt
says
er.
he.
??There are indeed a few low-floor buses,
but that isn?t enough?, he says.
Another solution consists of generating all pos-
sible scopes around the coordination. Thus, for
the sentence in (6), the conjunction is aber. The
shortest possible left conjunct is Niederflurbusse,
the next one paar Niederflurbusse, etc. Clearly,
many of these possibilities, such as the last exam-
ple, are nonsensical, especially when the proposed
conjunct crosses into or out of base phrase bound-
aries. Another type of boundary that should not
be crossed is a clause boundary. Since the con-
junction is part of the subordinated clause in the
present example, the right conjunct cannot extend
beyond the end of the clause, i.e. beyond nicht.
For this reason, we used KaRoPars (Mu?ller and
Ule, 2002), a partial parser for German, to parse
the sentences. From the partial parses, we ex-
tracted base phrases and clauses. For (6), the rel-
evant bracketing provided by KaRoPars is the fol-
lowing:
( " Es gibt zwar { ein paar
Niederflurbusse } , ) aber ( das
reicht ja nicht ) " , sagt er .
The round parentheses mark clause boundaries,
the curly braces the one base phrase that is longer
than one word. In the creation of possible con-
juncts, only such conjuncts are listed that do not
cross base phrase or clause boundaries. In order to
avoid unreasonably high numbers of pre-bracketed
versions, we also use higher level phrases, such as
coordinated noun phrases. KaRoPars groups such
higher level phrases only in contexts that allow
a reliable decision. While a small percentage of
such decisions is wrong, the heuristic used turns
out to be reliable and efficient.
For each scope, a partially bracketed version
of the input sentence is created, in which only
the brackets for the suggested conjuncts are in-
serted. Each pre-bracketed version of the sentence
is parsed with LoPar. Then all versions for one
sentence are reranked. The reranker was trained
on the data from experiment 1 (n-best parsing).
The results of the reranker show that our restric-
tions based on the partial parser may have been
too restrictive. Only 375 sentences had more than
one pre-bracketed version, and only 328 sentence
resulted in more than one parse. Only the latter set
could then profit from reranking.
The results of this experiment are shown in Ta-
ble 1 as exp. 3. They show that extracting pos-
sible scopes for conjuncts from a partial parse
is possible. The difference in F-score between
this experiment and the baseline reaches 5.93 per-
cent points. The F-score is also minimally higher
than the F-score for experiment 2 (gold scope),
and recall is increased by approximately 1 per-
cent point (even though only 12.5% of the sen-
tences were reranked). This can be attributed to
two factors: First, we provide different scope pos-
sibilities. This means that if the correct scope is
not covered by the grammar, the parser may still
be able to parse the next closest possibility in-
stead of failing completely. Second, reranking is
not specifically geared towards improving coordi-
nated structures. Thus, it is possible that a parse is
reranked higher because of some other feature. It
is, however, not the case that the improvement re-
sults completely from reranking. This can be de-
duced from two points: First, while the F-score
for experiment 1 (50-best analyses plus reranking)
and the present experiment are very close (74.53
vs. 74.69), there are again differences in precision
and recall: In experiment 1, recall is higher, and in
the present experiment precision. Second, a look
at the evaluation on only sentences with coordi-
nation shows that the F-score for the present ex-
periment is higher than the one for experiment 1
(72.14 vs. 71.68). Additionally, precision for the
present experiment is more than 2 percent points
higher.
7 Experiment 4: Combining n-Best
Parses and Extracted Scope Parses
As described above, the results for reranking the
50-best analyses and for reranking the versions
411
with automatically extracted scope readings are
very close. This raises the question whether the
two methods produce similar improvements in the
parse trees. One indicator that this is not the case
can be found in the differences in precision and re-
call. Another possibility of verifying our assump-
tion that the improvements do not overlap lies in
the combination of the 50-best parses with the
parses resulting from the automatically extracted
scopes. This increases the number of parses be-
tween which the reranker can choose. In effect,
this means a combination of the methods of exper-
iments 1 (n-best) and 3 (automatic scope). Con-
sequently, if the results from this experiment are
very close to the results from experiment 1 (n-
best), we can conclude that adding the parses with
automatic scope readings does not add new infor-
mation. If, however, adding these parses improves
results, we can conclude that new information was
present in the parses with automatic scope that
was not covered in the 50-best parses. Note that
the combination of the two types of input for the
reranker should not be regarded as a parser ensem-
ble but rather as a resampling of the n-best search
space since both parsers use the same grammar,
parsing model, and probability model. The only
difference is that LoPar can accept partially brack-
eted input, and BitPar can list the n-best analyses.
The results of this experiment are shown in Ta-
ble 1 as exp. 4. For all sentences, both precision
and recall are higher than for experiment 1 and 3,
resulting in an F-score of 76.69. This is more than
2 percent points higher than for the 50-best parses.
This is a very clear indication that the parses con-
tributed by the automatically extracted scopes pro-
vide parses that were not present in the 50 best
parses from experiment 1 (n-best). The same trend
can be seen in the evaluation of the sentences con-
taining coordination: Here, the improvement in F-
score is higher than for the whole set, a clear in-
dication that this method is suitable for improving
coordination parsing. A comparison of the results
of the present experiment and experiment 3 (with
automatic scope only) shows that the gain in pre-
cision is rather small, but the combination clearly
improves recall, from 73.96% to 77.23%. We can
conclude that adding the 50 best parses remedies
the lacking coverage that was the problem of ex-
periment 3. More generally, experiment 4 suggests
that for the notoriously difficult problem of pars-
ing coordination structures, a hybrid approach that
combines parse selection of n best analyses with
pre-bracketed scope in the input results in a con-
siderable reduction in error rate compared to each
of these methods used in isolation.
8 Related Work
Parsing of coordinate structures for English has
received considerable attention in computational
linguistics. Collins (1999), among many other au-
thors, reports in the error analysis of his WSJ pars-
ing results that coordination is one of the most fre-
quent cases of incorrect parses, particularly if the
conjuncts involved are complex. He manages to
reduce errors for simple cases of NP coordination
by introducing a special phrasal category of base
NPs. In the experiments presented above, no ex-
plicit distinction is made between simple and com-
plex cases of coordination, and no transformations
are performed on the treebank annotations used for
training.
Our experiment 1, reranking 50-best parses, is
similar to the approaches of Charniak and John-
son (2005) and of Hogan (2007). However, it dif-
fers from their experiments in two crucial ways: 1)
Compared to Charniak and Johnson, who use 1.1
mio. features, our feature set is appr. five times
larger (more than 5 mio. features), with the same
threshold of at least five occurrences in the training
set. 2) Both Hogan and Charniak and Johnson use
special features for coordinate structures, such as a
Boolean feature for marking parallelism (Charniak
and Johnson) or for distinguishing between coor-
dination of base NPs and coordination of complex
conjuncts (Hogan), while our approach refrains
from such special-purpose features.
Our experiments using scope information are
similar to the approaches of Kurohashi and Na-
gao (1994) and Agarwal and Bogges (1992) in that
they try to identify coordinate structure bracket-
ings. However, the techniques used by Agarwal
and Bogges and in the present paper are quite dif-
ferent. Agarwal and Bogges and Kurohashi and
Nagao rely on shallow parsing techniques to de-
tect parallelism of conjuncts while we use a par-
tial parser only for suggesting possible scopes of
conjuncts. Both of these approaches are limited
to coordinate structures with two conjuncts only,
while our approach has no such limitation. More-
over, the goal of Agarwal and Bogges is quite dif-
ferent from ours. Their goal is robust detection of
coordinate structures only (with the intended ap-
412
plication of term extraction), while our goal is to
improve the performance of a parser that assigns a
complete sentence structure to an input sentence.
Finally, our approach at present is restricted to
purely syntactic structural properties. This is in
contrast to approaches that incorporate semantic
information. Hogan (2007) uses bi-lexical head-
head co-occurrences in order to identify nominal
heads of conjuncts more reliably than by syntactic
information alone. Chantree et al (2005) resolve
attachment ambiguities in coordinate structures, as
in (7a) and (7b), by using word frequency informa-
tion obtained from generic corpora as an effective
estimate of the semantic compatibility of a modi-
fier vis-a`-vis the candidate heads.
(7) a. Project managers and designers
b. Old shoes and boots
We view the work by Hogan and by Chantree
et al as largely complementary to, but at the same
time as quite compatible with our approach. We
must leave the integration of structural syntac-
tic and lexical semantic information to future re-
search.
9 Conclusion and Future Work
We have presented a study on improving the treat-
ment of coordinated structures in PCFG parsing.
While we presented experiments for German, the
methods are applicable for any language. We have
chosen German because it is a language with rel-
atively flexible phrasal ordering (cf. Section 2)
which makes parsing coordinations particularly
challenging. The experiments presented show that
n-best parsing combined with reranking improves
results by a large margin. However, the number
of cases in which the correct parse is present in
the n-best parses is rather low. This led us to the
assumption that the n-best analyses often do not
cover the whole range of different scope possibil-
ities but rather present minor variations of parses
with few differences in coordination scope. The
experiments in which the parser was forced to as-
sume predefined scopes show that the scope infor-
mation is important for parsing quality. Provid-
ing the parser with different scope possibilities and
reranking the resulting parses results in an increase
in F-score from 69.76 for the baseline to 74.69.
One of the major challenges for this approach lies
in extracting a list of possible conjuncts. Forc-
ing the parser to parse all possible sequences re-
sults in a prohibitively large number of possibili-
ties, especially for sentences with 3 or more con-
junctions. For this reason, we used chunks above
base phases, such as coordinated noun chunks, to
restrict the space. However, an inspection of the
lists of bracketed versions of the sentences shows
that the definition of base phrases is one of the ar-
eas that must be refined. As mentioned above, the
partial parser groups sequences of ?NP KON NP?
into a single base phrase. This may be correct in
many cases, but there are exceptions such as (8).
(8) Die
The
31ja?hrige
31-year-old
Gewerkschaftsmitarbei-
union staff member
terin und
and
ausgebildete
trained
Industriekauffrau
industrial clerk
aus
from
Oldenburg
Oldenburg
bereitet
is preparing
nun
now
ihre
her
erste
first
eigene
own
CD
CD
vor.
part..
For (8), the partial parser groups Die 31ja?hrige
Gewerkschaftsmitarbeiterin und ausgebildete In-
dustriekauffrau as one noun chunk. Since our
proposed conjuncts cannot cross these boundaries,
the correct second conjunct, ausgebildete Indus-
triekauffrau aus Oldenburg, cannot be suggested.
However, if we remove these chunk boundaries,
the number of possible conjuncts increases dra-
matically, and parsing times become prohibitive.
As a consequence, we will need to find a good bal-
ance between these two needs. Our plan is to in-
crease flexibility very selectively, for example by
enabling the use of wider scopes in cases where
the conjunction is preceded and followed by base
noun phrases. For the future, we are planning to
repeat experiment 3 (automatic scope) with differ-
ent phrasal boundaries extracted from the partial
parser. It will be interesting to see if improvements
in this experiment will still improve results in ex-
periment 4 (combining 50-best parses with exp. 3).
Another area of improvement is the list of fea-
tures used for reranking. At present, we use a fea-
ture set that is similar to the one used by Collins
and Koo (2005). However, this feature set does
not contain any coordination specific features. We
are planning to extend the feature set by features
on structural parallelism as well as on lexical sim-
ilarity of the conjunct heads.
413
References
Rajeev Agarwal and Lois Boggess. 1992. A simple but
useful approach to conjunct identification. In Pro-
ceedings of the 30th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-92), pages
15?21, Newark, DE.
Francis Chantree, Adam Kilgarriff, Anne de Roeck,
and Alistair Willis. 2005. Disambiguating coordi-
nations using word distribution information. In Pro-
ceedings of Recent Advances in NLP (RANLP 2005),
pages 144?151, Borovets, Bulgaria.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 173?180, Ann Arbor, MI.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25?69.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Anette Frank. 2002. A (discourse) functional analysis
of asymmetric coordination. In Proceedings of the
LFG-02 Conference, Athens, Greece.
Yoav Freund, Ray Iyer, Robert Shapire, and Yoram
Singer. 1998. An efficient boosting algorithm
for combining preferences. In Proceedings of the
15th International Conference on Machine Learn-
ing, Madison, WI.
Deirdre Hogan. 2007. Coordinate noun phrase disam-
biguation in a generative parsing model. In Proceed-
ings of the 45th Annual Meeting of the Association of
Computational Linguistics, pages 680?687, Prague,
Czech Republic.
Tilman Ho?hle. 1983. Subjektlu?cken in Koordinatio-
nen. Universita?t Tu?bingen.
Tilman Ho?hle. 1990. Assumptions about asymmetric
coordination in German. In Joan Mascaro? and Ma-
rina Nespor, editors, Grammar in Progress. Glow
Essays for Henk van Riemsdijk, pages 221?235.
Foris, Dordrecht.
Tilman Ho?hle. 1991. On reconstruction and coor-
dination. In Hubert Haider and Klaus Netter, ed-
itors, Representation and Derivation in the The-
ory of Grammar, volume 22 of Studies in Natural
Language and Linguistic Theory, pages 139?197.
Kluwer, Dordrecht.
Andreas Kathol. 1990. Linearization vs. phrase struc-
ture in German coordination constructions. Cogni-
tive Linguistics, 10(4):303?342.
Sandra Ku?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the ACL Work-
shop on Parsing German, pages 55?63, Columbus,
Ohio.
Sadao Kurohashi and Makoto Nagao. 1994. A syn-
tactic analysis method of long Japanese sentences
based on the detection of conjunctive structures.
Computational Linguistics, 20(4):507?534.
Frank Henrik Mu?ller and Tylman Ule. 2002. Annotat-
ing topological fields and chunks?and revising POS
tags at the same time. In Proceedings of the 19th In-
ternational Conference on Computational Linguis-
tics, COLING?02, pages 695?701, Taipei, Taiwan.
Slav Petrov and Dan Klein. 2008. Parsing German
with latent variable grammars. In Proceedings of
the ACL Workshop on Parsing German, pages 33?
39, Columbus, Ohio.
Helmut Schmid. 2000. LoPar: Design and implemen-
tation. Technical report, Universita?t Stuttgart.
Helmut Schmid. 2004. Efficient parsing of highly
ambiguous context-free grammars with bit vectors.
In Proceedings of the 20th International Confer-
ence on Computational Linguistics (COLING 2004),
Geneva, Switzerland.
Heike Telljohann, Erhard Hinrichs, and Sandra Ku?bler.
2004. The Tu?Ba-D/Z treebank: Annotating German
with a context-free backbone. In Proceedings of the
Fourth International Conference on Language Re-
sources and Evaluation (LREC 2004), pages 2229?
2235, Lisbon, Portugal.
Heike Telljohann, Erhard W. Hinrichs, Sandra Ku?bler,
and Heike Zinsmeister, 2005. Stylebook for the
Tu?bingen Treebank of Written German (Tu?Ba-
D/Z). Seminar fu?r Sprachwissenschaft, Universita?t
Tu?bingen, Tu?bingen, Germany.
Dieter Wunderlich. 1988. Some problems of coor-
dination in German. In Uwe Reyle and Christian
Rohrer, editors, Natural Language Parsing and Lin-
guistic Theories, Studies in Linguistics and Philoso-
phy, pages 289?316. Reidel, Dordrecht.
414
Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 19?24,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Annotation Schemes and their Influence on Parsing Results
Wolfgang Maier
Seminar fu?r Sprachwissenschaft, Universita?t Tu?bingen
Wilhelmstr. 19, 72074 Tu?bingen, Germany
wmaier@sfs.uni-tuebingen.de
Abstract
Most of the work on treebank-based sta-
tistical parsing exclusively uses the Wall-
Street-Journal part of the Penn treebank
for evaluation purposes. Due to the pres-
ence of this quasi-standard, the question of
to which degree parsing results depend on
the properties of treebanks was often ig-
nored. In this paper, we use two similar
German treebanks, Tu?Ba-D/Z and NeGra,
and investigate the role that different an-
notation decisions play for parsing. For
these purposes, we approximate the two
treebanks by gradually taking out or in-
serting the corresponding annotation com-
ponents and test the performance of a stan-
dard PCFG parser on all treebank versions.
Our results give an indication of which
structures are favorable for parsing and
which ones are not.
1 Introduction
The Wall-Street-Journal part (WSJ) of the Penn
Treebank (Marcus et al, 1994) plays a central role
in research on statistical treebank-based parsing.
It has not only become a standard for parser eval-
uation, but also the foundation for the develop-
ment of new parsing models. For the English WSJ,
high accuracy parsing models have been created,
some of them using extensions to classical PCFG
parsing such as lexicalization and markovization
(Collins, 1999; Charniak, 2000; Klein and Man-
ning, 2003). However, since most research has
been limited to a single language (English) and
to a single treebank (WSJ), the question of how
portable the parsers and their extensions are across
languages and across treebanks often remained
open.
Only recently, there have been attempts to eval-
uate parsing results with respect to the proper-
ties and the language of the treebank that is used.
Gildea (2001) investigates the effects that cer-
tain treebank characteristics have on parsing re-
sults, such as the distribution of verb subcatego-
rization frames. He conducts experiments on the
WSJ and the Brown Corpus, parsing one of the
treebanks while having trained on the other one.
He draws the conclusion that a small amount of
matched training data is better than a large amount
of unmatched training data. Dubey and Keller
(2003) analyze the difficulties that German im-
poses on parsing. They use the NeGra treebank
for their experiments and show that lexicalization,
while highly effective for English, has no bene-
fit for German. This result motivates them to cre-
ate a parsing model for German based on sister-
head-dependencies. Corazza et al (2004) con-
duct experiments with model 2 of Collins? parser
(Collins, 1999) and the Stanford parser (Klein and
Manning, 2003) on two Italian treebanks. They re-
port disappointing results which they trace back to
the different difficulties of different parsing tasks
in Italian and English and to differences in anno-
tation styles across treebanks.
In the present paper, our goal is to determine
the effects of different annotation decisions on
the results of plain PCFG parsing without exten-
sions. Our motivation is two-fold: first, we want
to present research on a language different from
English, second, we want to investigate the influ-
ences of annotation schemes via a realistic com-
parison, i.e. use two different annotation schemes.
Therefore, we take advantage of the availability
of two similar treebanks of German, Tu?Ba-D/Z
(Telljohann et al, 2003) and NeGra (Skut et al,
1997). The strategy we adopt extends Ku?bler
19
(2005). Treebanks and their annotation schemes
respectively are compared using a stepwise ap-
proximation. Annotation components correspond-
ing to certain annotation decisions are taken out or
inserted, submitting each time the resulting mod-
ified treebank to the parser. This method allows
us to investigate the role of single annotation deci-
sions in two different environments.
In section 2, we describe the annotation of
both treebanks in detail. Section 3 introduces the
methodology used. In section 4, we describe our
experimental setup and discuss the results. Section
5 presents a conclusion and plans for future work.
2 The Treebanks: Tu?Ba-D/Z and NeGra
With respect to treebanks, German is in a priv-
ileged position. Various treebanks are avail-
able, among them are two similar ones: Ne-
Gra (Skut et al, 1997), from Saarland University
at Saarbru?cken and Tu?Ba-D/Z (Telljohann et al,
2003), from the University of Tu?bingen. NeGra
contains about 20,000 sentences, Tu?Ba-D/Z about
15,000, both consist of newspaper text. In both
treebanks, predicate argument structure is anno-
tated, the core principle of the annotation being its
theory independence. Terminal nodes are labeled
with part-of-speech tags and morphological labels,
non-terminal nodes with phrase labels. All edges
are labeled with grammatical functions. Anno-
tation was accomplished semi-automatically with
the same software tools.
The main difference between the treebanks is
rooted in the partial free word order of Ger-
man sentences: the positions of complements
and adjuncts are of great variability. This leads
to a high number of discontinuous constituents,
even in short sentences. An annotation scheme
for German must account for that. NeGra al-
lows for crossing branches, thereby giving up the
context-free backbone of the annotation. With
crossing branches, discontinuous constituents are
not a problem anymore: all children of every
constituent, discontinuous or not, can always be
grouped under the same node. The inconvenience
of this method is that the crossing branches must
be resolved before the treebank can be used with
a (PCFG) parser. However, this can be accom-
plished easily by reattaching children of discon-
tinuous constituents to higher nodes.
Tu?Ba-D/Z uses another mechanism to account
for the free word order. Above the phrase level,
an additional layer of annotation is introduced. It
consists of topological fields (Drach, 1937; Ho?hle,
1986). The concept of topological fields is widely
accepted among German grammarians. It reflects
the empirical observation that German has three
possible sentence configurations with respect to
the position of the finite verb. In its five fields
(initial field, left sentence bracket, middle field,
right sentence bracket, final field), verbal mate-
rial generally resides in the two sentence brackets,
while the initial field and the middle field contain
all other elements. The final field contains mostly
extraposed material. Since word order variations
generally do not cross field boundaries, with the
model of topological fields, the free word order of
German can be accounted for in a natural way.
On the phrase level, the treebanks show great
differences, too. NeGra does not allow for any in-
termediate (?bar?) phrasal projections. Addition-
ally, no unary productions are allowed. This re-
sults in very flat phrases: pre- and postmodifiers
are attached directly to the phrase, nominal sub-
jects are attached directly to the sentence, nominal
material within PPs doesn?t project to NPs, com-
plex (non-coordinated) NPs remain flat. Tu?Ba-
D/Z, on the contrary, allows for ?deep? annota-
tion. Intermediate productions and unary produc-
tions are allowed and extensively used.
To illustrate the annotation principles, the fig-
ures 1 and 2 show the annotation of the sentences
(1) and (2) respectively.
(1) Daru?ber
About-that
mu?
must
nachgedacht
tought
werden.
be
?This must be tought about.?
(2) Schillen
Schillen
wies
rejected
dies
that
gestern
yesterday
zuru?ck:
VPART
?Schillen rejected that yesterday.?
0 1 2 3 4
500
501
502
Dar?ber
PROAV
??
mu?
VMFIN
3.Sg.Pres.Ind
nachgedacht
VVPP
??
werden
VAINF
??
.
$.
??
MO HD
VP
OC HD
HD
VP
OC
S
Figure 1: A NeGra tree
20
0 1 2 3 4 5
500 501 502 503 504
505 506 507
508
Schillen
NE
nsf
wies
VVFIN
3sit
dies
PDS
asn
gestern
ADV
??
zur?ck
PTKVZ
??
:
$.
??
HD HD HD HD VPT
NX
ON
VXFIN
HD
NX
OA
ADVX
V?MOD
VF
?
LK
?
MF
?
VC
?
SIMPX
Figure 2: A Tu?Ba-D/Z tree
3 Treebanks, Parsing, and Comparisons
Our goal is to determine which components of
the annotation schemes of Tu?Ba-D/Z and NeGra
have which influence on parsing results. A direct
comparison of the parsing results shows that the
Tu?Ba-D/Z annotation scheme is more appropriate
for PCFG parsing than NeGra?s (see tables 2 and
3). However, this doesn?t tell us anything about
the role of the subparts of the annotation schemes.
A first idea for a more detailed comparison
could be to compare the results for different phrase
types. The problem is that this would not give
meaningful results. NeGra noun phrases, e.g.,
cover a different set of constituents than Tu?Ba-D/Z
noun phrases, due to NeGra?s flat annotation and
avoidance of annotation of unary NPs. Further-
more, both annotation schemes contain categories
not contained in the other one. There are, e.g.,
no categories in NeGra that correspond to Tu?Ba-
D/Z?s field categories, while in Tu?Ba-D/Z, there
are no categories equivalent to NeGra?s categories
for coordinated phrases or verb phrases.
We therefore pursue another approach. We use
a method introduced by Ku?bler (2005) to investi-
gate the usefulness of different annotation compo-
nents for parsing. We gradually modify the tree-
bank annotations in order to approximate the an-
notation style of the treebanks to one another. This
is accomplished by taking out or inserting cer-
tain components of the annotation. For our tree-
banks, this generally results in reduced structures
for Tu?Ba-D/Z and augmented structures for Ne-
Gra. Table 1 presents three measures that cap-
ture the changes between each of the modifica-
tions. The average number of child nodes of non-
terminal nodes shows the degree of flatness of the
annotation on phrase level. Here, the unmodi-
fied NeGra consequently shows the highest values.
The average tree height relates directly to the num-
ber of annotation hierarchies in the tree. Here, the
unmodified Tu?Ba-D/Z has the highest values.
4 Experimental Setup
For our experiments, we use lopar (Schmid,
2000), a standard PCFG parser. We read the gram-
mar and the lexicon directly off the trees together
with their frequencies. The parser is given the
gold POS tagging to avoid parsing errors that are
caused by wrong POS tags. Only sentences up to a
length of 40 words are considered due to memory
limitations.
Traditionally, most of the work on WSJ uses the
same section of the treebank for testing. How-
ever, for our aims, this method has a shortcom-
ing: since both treebanks consist of text created
by different authors, linguistic phenomena are not
evenly distributed over the treebank. When using
a whole section as test set, some phenomena may
only occur there and thus not occur in the gram-
mar. To reduce data sparseness, we use another
test/training-set split for the treebanks and their
variations. Each 10th sentence is put into the test
set, all other sentences go into the training set.
4.1 Preprocessing the Treebanks
Since we want to read the grammars for our parser
directly off the treebanks, preprocessing of the
treebanks is necessary due to the non-context-free
nature of the original annotation. In both tree-
banks, punctuation is not included in the trees,
furthermore, sentence splitting in both treebanks
does not always coincide with the linguistic no-
tion of a sentence. This leads to sentences con-
sisting of several unconnected trees. All nodes in
a sentence, i.e. the roots and the punctation, are
grouped by a virtual root node, which may cause
crossing branches. Furthermore, the NeGra anno-
tation scheme allows for crossing branches for lin-
guistic reasons, as described in section 2. All of
the crossing branches have to be removed before
parsing.
The crossing branches caused by the NeGra an-
notation scheme are removed with a small pro-
gram by Thorsten Brants. It attaches some of the
children of discontinuous constituents to higher
nodes. The virtual root node is made continu-
ous by attaching all punctuation to the highest
possible location in the tree. Pairs of parenthe-
sis and quotation marks are preferably attached to
21
NeGra NE fi. NE NP NE tr. Tu?Ba Tu? NF Tu? NU Tu? f Tu? f NU Tu? f NU NF
N/T 0.41 0.70 0.50 0.41 1.21 0.89 0.54 1.00 0.42 0.35
? D/N 2.92 2.22 2.59 2.92 1.61 1.89 2.53 1.83 2.93 3.35
? H(T) 4.86 5.81 5.16 4.68 6.88 5.68 5.45 5.94 4.72 4.15
Table 1: Properties of the treebank modifications1
the same node, to avoid low-frequent productions
in the grammar that only differ by the position of
parenthesis marks on their right hand side.
4.2 Results of the Comparison
We use the standard parseval measures for the
evaluation of parser output. They measure the per-
centage of correctly parsed constituents, in terms
of precision, recall, and F-Measure. The parser
output of each modified treebank version is evalu-
ated against the correspondingly modified test set.
Unparsed sentences are fully included in the eval-
uation.
NeGra. Along with the unmodified treebank,
two modifications of NeGra are tested. Both of
them introduce annotation components present in
Tu?Ba-D/Z but not in NeGra. In the first one,
NE fi, we add an annotation layer of topologi-
cal fields2, as existing in Tu?Ba-D/Z. The precision
value benefits the most from this modification.
When parsing without grammatical functions, it
increases about 6,5%. When parsing with gram-
matical functions, it increases about 14%. Thus,
the additional rules provided by a topological field
level that groups phrases below the clausal level
are favorable for parsing. The average number of
crossing brackets per sentence increases, which is
due to the fact that there are simply more brackets
to create.
A detailed evaluation of the results for node
categories shows that the new field categories are
easy to recognize (e.g. LF gets 97.79 F-Measure).
Nearly all categories have a better precision value.
However, the F-Measure for VPs is low (only
26.70 while 59.41 in the unmodified treebank),
while verb phrases in the unmodified Tu?Ba-D/Z
(see below) are recognized with nearly 100 points
F-Measure. The problem here is the following. In
the original NeGra annotation, a verb and its com-
plements are grouped under the same VP. To pre-
1explanation: N/T = node/token ratio, ? D/N = average
number of daughters of non-terminal nodes, ? H(T) = average
tree height
2We are grateful to the DFKI Saarbru?cken for providing
us with the topological field annotation.
serve as much of the annotation as possible, the
topological fields are inserted below the VP (com-
plements are grouped by a middle field node, the
verb complex by the right sentence bracket). Since
this way, the phrase node VP resides above the
field level, it becomes difficult to recognize.
In the second modification, NE NP, we approx-
imate NeGra?s PPs to Tu?Ba-D/Z?s by grouping
all nominal material below the PPs to separate
NPs. This modification gives us a small bene-
fit in terms of precision and recall (about 2-3%).
Although there are more brackets to place, the
number of crossing parents increases only slightly,
which can be attributed to the fact that below PPs,
there is no room to get brackets wrong.
We finally parse a version of NeGra where
for each node movement during the resolution of
crossing edges, a trace label was created in the
corresponding edge (NE tr). Although this brings
the treebank closer to the format of Tu?Ba-D/Z, the
results get even worse than in the version without
traces. However, the high number of unparsed sen-
tences indicates that the result is not reliable due to
data sparseness.
NeGra NE fi. NE NP NE tr.
without grammatical functions
cross. br. 1.10 1.67 1.14 ?
lab. prec. 68.14% 74.96% 70.43% ?
lab. rec. 69.98% 70.37% 72.81% ?
lab. F1 69.05 72.59 71.60 ?
not parsed 1.00% 0.10% 0.15% ?
with grammatical functions
cross. br. 1.10 1.21 1.27 1.05
lab. prec. 52.67% 67.90% 59.77% 51.81%
lab. rec. 52.17% 65.18% 60.36% 49.19%
lab. F1 52.42 66.51 60.06 50.47
not parsed 12.90% 1.66% 9.88% 16.01%
Table 2: Parsing NeGra: Results
Tu?Ba-D/Z. Apart from the original treebank,
we test six modifications of Tu?Ba-D/Z. In each
of the modifications, annotation material is re-
moved in order to obtain NeGra-like structures.
Since they are equally absent in NeGra, we delete
the annotation of topological fields in the first
modification, Tu? NF. This results in small losses.
22
Tu?Ba Tu? NF Tu? NU Tu? flat Tu? f NU Tu? f NU NF
without grammatical functions
crossing brackets 2.21 1.82 1.67 1.04 0.80 1.03
labeled precision 87.39% 86.31% 79.97% 86.22% 75.18% 63.05%
labeled recall 83.57% 83.43% 78.52% 85.41% 76.11% 66.86%
labeled F-Measure 85.44 84.85 79.24 85.81 75.64 64.90
not parsed 0.07% 0.07% 2.45% 0.07% 2.99% 6.87%
with grammatical functions
crossing brackets 1.84 1.82 1.79 0.98 1.01 1.12
labeled precision 76.99% 68.55% 63.71% 76.93% 58.91% 45.15%
labeled recall 75.30% 68.40% 62.79% 77.21% 58.92% 44.76%
labeled F-Measure 76.14 68.47 63.25 77.07 58.92 44.96
not parsed 0.07% 0.27% 4.49% 0.07% 7.21% 17.76%
Table 3: Parsing Tu?Ba-D/Z: Results
A closer look at category results shows that
losses are mainly due to categories on the clausal
level; structures within fields do not deteriorate.
Field categories are thus especially helpful for the
clausal level.
In the second modification of Tu?Ba-D/Z,
Tu? NU, unary nodes are collapsed with the goal
to get structures comparable to NeGra?s. As the
figures show, the unary nodes are very helpful,
the F-Measure drops about 6 points without them.
The number of crossing brackets also drops, along
with the total number of nodes. When parsing
with grammatical functions, taking out unary pro-
ductions has a detrimental effect, F-Measure drops
about 13 points. A plausible explanation could be
data sparseness. 32.78% of the rules that the parser
needs to produce a correct parse don?t occur in the
training set.
An evaluation of the results for the different
categories shows that all major phrase categories
loose both in precision and recall. Since field
nodes are mostly unary, many of them disappear,
but most of the middle field nodes stay because
they generally contain more than one element.
However, their recall drops about 10%. Suppos-
edly it is more difficult for the parser to annotate
the middle field ?alone? without the other field cat-
egories.
We also test a version of Tu?Ba-D/Z with flat-
tened phrases that mimic NeGra?s flat phrases,
Tu? flat. With this treebank version, we get results
very similar to those of the unmodified treebank.
The F-Measure values are slightly higher and the
parser produces less crossing brackets. A single
category benefits the most from this treebank mod-
ification: EN-ADD, its F-Measure rising about 45
points. It was originally introduced as a marker
for named entities, which means that it has no spe-
cific syntactic function. In the Tu?Ba-D/Z version
with flattened phrases, many of the nominal nodes
below EN-ADD are taken out, bringing EN-ADD
closer to the lexical level. This way, the category
has more meaningful context and therefore pro-
duces better results.
Furthermore, we test combinations of the mod-
ifications. Apart from the average tree height, the
dimensions of Tu?Ba-D/Z with flattened phrases
and without unary productions (Tu? f NU) re-
semble those of the unmodified NeGra treebank,
which indicates their similarity. Nevertheless,
parser results are worse on NeGra. This indicates
that Tu?Ba-D/Z still benefits from the remaining
field nodes. The number of crossing branches is
the lowest in this treebank version.
In the last modification that combines all mod-
ifications made before (T ?U f NU NF), as ex-
pected, all values drop dramatically. F-Measure
is about 5 points worse than with the unmodified
NeGra treebank.
POS tagging. In a second round, we investigate
the benefits that gold POS tags have when making
them available in the parser input. We repeat all
experiments without giving the parser the perfect
tagging.
This leads to higher time and space require-
ments during parsing, caused by the additional
tagging step. With Tu?Ba-D/Z, NeGra, and all their
modifications, the F-Measure results are about 3-
5 points worse when parsing with grammatical
functions. When parsing without them, they drop
3-6 points. We can determine two exceptions:
Tu?Ba-D/Z with flattened phrases, where the F-
Score drops more than 9 points when parsing with
grammatical functions, and the Tu?Ba-D/Z version
with all modifications combined, where F-Score
drops only a little less than 2 points. The behavior
23
of the flattened Tu?Ba-D/Z relates directly to the
fact that the categories that loose the most with-
out gold POS tags are phrase categories (partic-
ularly infinite VPs and APs). They are directly
conditioned on the POS tagging and thus behave
accordingly to its quality. For the Tu?Ba-D/Z ver-
sion with all modifications combined, one could
argue that the results are not reliable because of
data sparseness, which is confirmed by the high
number of unparsed sentences in this treebank ver-
sion. However, in all cases, less crossing brackets
are produced.
To sum up, obviously, it is more difficult for the
parser to build a parse tree onto an already exist-
ing layer of POS-tagging. This explains the bigger
number of unparsed sentences. Nevertheless, in
terms of F-Score, the parsing results profit visibly
from the gold POS tagging.
5 Conclusions and Outlook
We presented an analysis of the influences of the
particularities of annotation schemes on parsing
results via a comparison of two German tree-
banks, NeGra and Tu?Ba-D/Z, based on a step-
wise approximation of both treebanks. The exper-
iments show that as treebanks are approximated,
the parsing results also get closer. When annota-
tion structure is deleted in Tu?Ba-D/Z, the number
of crossing brackets drops, but F-Measure drops,
too. When annotation structure is added in Ne-
Gra, the contrary happens. We can conclude that,
being interested in good F-Measure results, the
deep Tu?Ba-D/Z structures are more appropriate
for parsing than NeGra?s flat structures. Moreover,
we have observed that it is beneficial to provide
the parser with the gold POS tags at parsing time.
However, we see that especially when parsing with
grammatical functions, data sparseness becomes a
serious problem, making the results less reliable.
Seen in the context of a parse tree, the expansion
probability of a PCFG rule just covers a subtree of
height 1. This is a clear deficiency of PCFGs since
this way, e.g., the expansion probability of a VP is
independent of the choice of the verb. Our future
work will start at this point. We will conduct fur-
ther experiments with the Stanford Parser (Klein
and Manning, 2003) which considers broader con-
texts in its probability. It uses markovization to re-
duce horizontal context (right hand sides of rules
are broken up) and add vertical context (rule prob-
abilities are conditioned on (grand-)parent-node
information). This way, we expect further insights
in NeGra?s an Tu?Ba-D/Z?s annotation schemes.
References
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL 2000.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Anna Corazza, Alberto Lavelli, Giorgio Satta, and
Roberto Zanoli. 2004. Analyzing an Italian tree-
bank with state-of-the-art statistical parsers. In Pro-
ceedings of the 3rd Workshop on Treebanks and Lin-
guistic Theories (TLT 2004).
Erich Drach. 1937. Grundgedanken der deutschen
Satzlehre. Diesterweg, Frankfurt/Main.
Amit Dubey and Frank Keller. 2003. Probabilistic
parsing for German using sisterhead dependencies.
In Proceedings of ACL 2003.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of EMNLP 2001.
Tilman Ho?hle. 1986. Der Begriff ?Mittelfeld?,
Anmerkungen ber die Theorie der topologischen
Felder. In Akten des Siebten Internationalen Ger-
manistenkongresses 1985, Go?ttingen, Germany.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL
2003.
Sandra Ku?bler. 2005. How do treebank annotation
schemes influence parsing results? Or how not to
compare apples and oranges. In Proceedings of
RANLP 2005.
Mitchell P. Marcus, Grace Kim, Marry Ann
Marcinkiewicz, Robert MacIntyre, Ann Biew,
Mark Freguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proceedings of the
1994 Human Language Technology Workshop, HLT
94, Plainsboro, NJ.
Helmut Schmid. 2000. LoPar: Design and implemen-
tation. Technical report, Universita?t Stuttgart, Ger-
many.
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and
Hans Uszkoreit. 1997. An annotation scheme for
free word order languages. In Proceedings of ANLP
1997.
Heike Telljohann, Erhard W. Hinrichs, and Sandra
Ku?bler, 2003. Stylebook for the Tu?bingen Tree-
bank of Written German (Tu?Ba-D/Z). Seminar
fu?r Sprachwissenschaft, Universita?t Tu?bingen, Ger-
many.
24
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 9?12,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
An Earley Parsing Algorithm for Range Concatenation Grammars
Laura Kallmeyer
SFB 441
Universit?at T?ubingen
72074 T?ubingen, Germany
lk@sfs.uni-tuebingen.de
Wolfgang Maier
SFB 441
Universit?at T?ubingen
72074 T?ubingen, Germany
wo.maier@uni-tuebingen.de
Yannick Parmentier
CNRS - LORIA
Nancy Universit?e
54506 Vand?uvre, France
parmenti@loria.fr
Abstract
We present a CYK and an Earley-style
algorithm for parsing Range Concatena-
tion Grammar (RCG), using the deduc-
tive parsing framework. The characteris-
tic property of the Earley parser is that we
use a technique of range boundary con-
straint propagation to compute the yields
of non-terminals as late as possible. Ex-
periments show that, compared to previ-
ous approaches, the constraint propagation
helps to considerably decrease the number
of items in the chart.
1 Introduction
RCGs (Boullier, 2000) have recently received a
growing interest in natural language processing
(S?gaard, 2008; Sagot, 2005; Kallmeyer et al,
2008; Maier and S?gaard, 2008). RCGs gener-
ate exactly the class of languages parsable in de-
terministic polynomial time (Bertsch and Neder-
hof, 2001). They are in particular more pow-
erful than linear context-free rewriting systems
(LCFRS) (Vijay-Shanker et al, 1987). LCFRS is
unable to describe certain natural language phe-
nomena that RCGs actually can deal with. One
example are long-distance scrambling phenom-
ena (Becker et al, 1991; Becker et al, 1992).
Other examples are non-semilinear constructions
such as case stacking in Old Georgian (Michaelis
and Kracht, 1996) and Chinese number names
(Radzinski, 1991). Boullier (1999) shows that
RCGs can describe the permutations occurring
with scrambling and the construction of Chinese
number names.
Parsing algorithms for RCG have been intro-
duced by Boullier (2000), who presents a di-
rectional top-down parsing algorithm using pseu-
docode, and Barth?elemy et al (2001), who add an
oracle to Boullier?s algorithm. The more restricted
class of LCFRS has received more attention con-
cerning parsing (Villemonte de la Clergerie, 2002;
Burden and Ljungl?of, 2005). This article proposes
new CYK and Earley parsers for RCG, formulat-
ing them in the framework of parsing as deduction
(Shieber et al, 1995). The second section intro-
duces necessary definitions. Section 3 presents a
CYK-style algorithm and Section 4 extends this
with an Earley-style prediction.
2 Preliminaries
The rules (clauses) of RCGs
1
rewrite predicates
ranging over parts of the input by other predicates.
E.g., a clause S(aXb)? S(X) signifies that S is
true for a part of the input if this part starts with an
a, ends with a b, and if, furthermore, S is also true
for the part between a and b.
Definition 1. A RCG G = ?N,T, V, P, S? con-
sists of a) a finite set of predicates N with an arity
function dim: N ? N \ {0} where S ? N is
the start predicate with dim(S) = 1, b) disjoint fi-
nite sets of terminals T and variables V , c) a finite
set P of clauses ?
0
? ?
1
. . . ?
m
, where m ? 0
and each of the ?
i
, 0 ? i ? m, is a predicate of
the form A
i
(?
1
, . . . , ?
dim(A
i
)
) with A
i
? N and
?
j
? (T ? V )
?
for 1 ? j ? dim(A
i
).
Central to RCGs is the notion of ranges on
strings.
Definition 2. For every w = w
1
. . . w
n
with
w
i
? T (1 ? i ? n), we define a) Pos(w) =
{0, . . . , n}. b) ?l, r? ? Pos(w) ? Pos(w) with
l ? r is a range in w. Its yield ?l, r?(w) is the
substring w
l+1
. . . w
r
. c) For two ranges ?
1
=
?l
1
, r
1
?, ?
2
= ?l
2
, r
2
?: if r
1
= l
2
, then ?
1
? ?
2
=
?l
1
, r
2
?; otherwise ?
1
? ?
2
is undefined. d) A vec-
tor ? = (?x
1
, y
1
?, . . . , ?x
k
, y
k
?) is a range vector
of dimension k in w if ?x
i
, y
i
? is a range in w for
1 ? i ? k. ?(i).l (resp. ?(i).r) denotes then the
1
In this paper, by RCG, we always mean positive RCG,
see Boullier (2000) for details.
9
first (resp. second) component of the ith element
of ?, that is x
i
(resp. y
i
).
In order to instantiate a clause of the grammar,
we need to find ranges for all variables in the
clause and for all occurrences of terminals. For
convenience, we assume the variables in a clause
and the occurrences of terminals to be equipped
with distinct subscript indices, starting with 1 and
ordered from left to right (where for variables,
only the first occurrence is relevant for this order).
We introduce a function ? : P ? N that gives the
maximal index in a clause, and we define ?(c, x)
for a given clause c and x a variable or an occur-
rence of a terminal as the index of x in c.
Definition 3. An instantiation of a c ? P with
?(c) = j w.r.t. to some string w is given by a
range vector ? of dimension j. Applying ? to
a predicate A(~?) in c maps all occurrences of
x ? (T ? V ) with ?(c, x) = i in ~? to ?(i). If
the result is defined (i.e., the images of adjacent
variables can be concatenated), it is called an in-
stantiated predicate and the result of applying ? to
all predicates in c, if defined, is called an instanti-
ated clause.
We also introduce range constraint vectors, vec-
tors of pairs of range boundary variables together
with a set of constraints on these variables.
Definition 4. Let V
r
= {r
1
, r
2
, . . . } be a set
of range boundary variables. A range constraint
vector of dimension k is a pair ?~?, C? where a)
~? ? (V
2
r
)
k
; we define V
r
(~?) as the set of range
boundary variables occurring in ~?. b) C is a set
of constraints c
r
that have one of the following
forms: r
1
= r
2
, k = r
1
, r
1
+ k = r
2
,
k ? r
1
, r
1
? k, r
1
? r
2
or r
1
+ k ? r
2
for r
1
, r
2
? V
r
(~?) and k ? N.
We say that a range vector ? satisfies a range
constraint vector ??, C? iff ? and ? are of the same
dimension k and there is a function f : V
r
? N
that maps ?(i).l to ?(i).l and ?(i).r to ?(i).r for
all 1 ? i ? k such that all constraints in C are sat-
isfied. Furthermore, we say that a range constraint
vector ??, C? is satisfiable iff there exists a range
vector ? that satisfies it.
Definition 5. For every clause c, we define its
range constraint vector ??, C? w.r.t. aw with |w| =
n as follows: a) ? has dimension ?(c) and all
range boundary variables in ? are pairwise differ-
ent. b) For all ?r
1
, r
2
? ? ?: 0 ? r
1
, r
1
? r
2
,
r
2
? n ? C. For all occurrences x of terminals
in cwith i = ?(c, x): ?(i).l+1 = ?(i).r ? C. For
all x, y that are variables or occurrences of termi-
nals in c such that xy is a substring of one of the
arguments in c: ?(?(c, x)).r = ?(?(c, y)).l ? C.
These are all constraints in C.
The range constraint vector of a clause c cap-
tures all information about boundaries forming a
range, ranges containing only a single terminal,
and adjacent variables/terminal occurrences in c.
An RCG derivation consists of rewriting in-
stantiated predicates applying instantiated clauses,
i.e. in every derivation step ?
1
?
w
?
2
, we re-
place the lefthand side of an instantiated clause
with its righthand side (w.r.t. a word w). The lan-
guage of an RCG G is the set of strings that can
be reduced to the empty word: L(G) = {w |
S(?0, |w|?)
+
?
G,w
?}.
The expressive power of RCG lies beyond mild
context-sensitivity. As an example, consider the
RCG from Fig. 3 that generates a language that is
not semilinear.
For simplicity, we assume in the following with-
out loss of generality that empty arguments (?)
occur only in clauses whose righthand sides are
empty.
2
3 Directional Bottom-Up Chart Parsing
In our directional CYK algorithm, we move a dot
through the righthand side of a clause. We there-
fore have passive items [A, ?] where A is a pred-
icate and ? a range vector of dimension dim(A)
and active items. In the latter, while traversing
the righthand side of the clause, we keep a record
of the left and right boundaries already found
for variables and terminal occurrences. This is
achieved by subsequently enriching the range con-
straint vector of the clause. Active items have the
form [A(~x)? ? ??, ??, C?] with A(~x)? ?? a
clause, ?? 6= ?, ?(A(~x ? ??)) = j and ??, C?
a range constraint vector of dimension j. We re-
quire that ??, C? be satisfiable.
3
2
Any RCG can be easily transformed into an RCG satis-
fying this condition: Introduce a new unary predicate Eps
with a clause Eps(?) ? ?. Then, for every clause c with
righthand side not ?, replace every argument ? that occurs in
c with a new variable X (each time a distinct one) and add
the predicate Eps(X) to the righthand side of c.
3
Items that are distinguished from each other only by a bi-
jection of the range variables are considered equivalent. I.e.,
if the application of a rule yields a new item such that an
equivalent one has already been generated, this new one is
not added to the set of partial results.
10
Scan:
[A, ?]
A(~x)? ? ? P with instantiation ?
such that ?(A(~x)) = A(?)
Initialize:
[A(~x)? ??, ??,C?]
A(~x)? ? ? P with
range constraint vector
??,C?,? 6= ?
Complete:
[B,?
B
],
[A(~x)? ? ?B(x
1
...y
1
, ..., x
k
...y
k
)?, ??,C?]
[A(~x)? ?B(x
1
...y
1
, ..., x
k
...y
k
) ??, ??,C
?
?]
where C
?
= C ? {?
B
(j).l = ?(?(x
j
)).l, ?
B
(j).r =
?(?(y
j
)).r | 1 ? j ? k}.
Convert:
[A(~x)? ??, ??,C?]
[A, ?]
A(~x)? ? ? P with
an instantiation ? that
satisfies ??,C?,
?(A(~x)) = A(?)
Goal: [S, (?0, n?)]
Figure 1: CYK deduction rules
The deduction rules are shown in Fig. 1. The
first rule scans the yields of terminating clauses.
Initialize introduces clauses with the dot on the
left of the righthand side. Complete moves the dot
over a predicate provided a corresponding passive
item has been found. Convert turns an active item
with the dot at the end into a passive item.
4 The Earley Algorithm
We now add top-down prediction to our algorithm.
Active items are as above. Passive items have
an additional flag p or c depending on whether
the item is predicted or completed, i.e., they ei-
ther have the form [A, ??, C?, p] where ??, C? is a
range constraint vector of dimension dim(A), or
the form [A, ?, c] where ? is a range vector of di-
mension dim(A).
Initialize:
[S, ?(?r
1
, r
2
?), {0 = r
1
, n = r
2
}?, p]
Predict-rule:
[A, ??,C?, p]
[A(x
1
. . . y
1
, . . . , x
k
. . . y
k
)? ??, ??
?
, C
?
?]
where ??
?
, C
?
? is obtained from the range constraint vector
of the clause A(x
1
. . . y
1
, . . . , x
k
. . . y
k
) ? ? by taking all
constraints from C, mapping all ?(i).l to ?
?
(?(x
i
)).l and
all ?(i).r to ?
?
(?(y
i
)).r, and then adding the resulting con-
straints to the range constraint vector of the clause.
Predict-pred:
[A(...)? ? ?B(x
1
...y
1
, ..., x
k
...y
k
)?, ??,C?]
[B, ??
?
, C
?
?, p]
where ?
?
(i).l = ?(?(x
i
)).l, ?
?
(i).r = ?(?(y
i
)).r for all
1 ? i ? k and C
?
= {c | c ? C, c contains only range
variables from ?
?
}.
Scan:
[A, ??,C?, p]
[A, ?, c]
A(~x)? ? ? P with an
instantiation ? satisfying ??,C?
such that ?(A(~x)) = A(?)
Figure 2: Earley deduction rules
The deduction rules are listed in Fig. 2. The
axiom is the prediction of an S ranging over the
entire input (initialize). We have two predict op-
erations: Predict-rule predicts active items with
the dot on the left of the righthand side, for a
given predicted passive item. Predict-pred pre-
dicts a passive item for the predicate following the
dot in an active item. Scan is applied whenever a
predicted predicate can be derived by an ?-clause.
The rules complete and convert are the ones from
the CYK algorithm except that we add flags c to
the passive items occurring in these rules. The
goal is again [S, (?0, n?), c].
To understand how this algorithm works, con-
sider the example in Fig. 3. The crucial property of
this algorithm, in contrast to previous approaches,
is the dynamic updating of a set of constraints on
range boundaries. We can leave range boundaries
unspecified and compute their values in a more in-
cremental fashion instead of guessing all ranges of
a clause at once at prediction.
4
For evaluation, we have implemented a direc-
tional top-down algorithm where range bound-
aries are guessed at prediction (this is essentially
the algorithm described in Boullier (2000)), and
the new Earley-style algorithm. The algorithms
were tested on different words of the language
L = {a
2
n
|n ? 0}. Table 1 shows the number
of generated items.
Word Earley TD
a
2
15 21
a
4
30 55
a
8
55 164
a
9
59 199
Word Earley TD
a
16
100 539
a
30
155 1666
a
32
185 1894
a
64
350 6969
Table 1: Items generated by both algorithms
Clearly, range boundary constraint propagation
increases the amount of information transported
in single items and thereby decreases considerably
the number of generated items.
5 Conclusion and future work
We have presented a new CYK and Earley pars-
ing algorithms for the full class of RCG. The cru-
cial difference between previously proposed top-
down RCG parsers and the new Earley-style algo-
rithm is that while the former compute all clause
instantiations during predict operations, the latter
4
Of course, the use of constraints makes comparisons be-
tween items more complex and more expensive which means
that for an efficient implementation, an integer-based repre-
sentation of the constraints and adequate techniques for con-
straint solving are required.
11
Grammar for {a
2
n
|n > 0}: S(XY )? S(X)eq(X,Y ), S(a
1
)? ?, eq(a
1
X, a
2
Y )? eq(X,Y ), eq(a
1
, a
2
)? ?
Parsing trace for w = aa:
Item Rule
1 [S, ?(?r
1
, r
2
?), {0 = r
1
, r
1
? r
2
, 2 = r
2
}?, p] initialize
2 [S(XY )? ?S(X)eq(X,Y ), {X.l ? X.r,X.r = Y.l, Y.l ? Y.r, 0 = X.l, 2 = Y.r}] predict-rule from 1
3 [S, ?(?r
1
, r
2
?), {0 = r
1
, r
1
? r
2
}?, p] predict-pred from 2
4 [S, (?0, 1?), c] scan from 3
5 [S(XY )? ?S(X)eq(X,Y ), {X.l ? X.r,X.r = Y.l, Y.l ? Y.r, 0 = X.l, }] predict-rule from 3
6 [S(XY )? S(X) ? eq(X,Y ), {. . . , 0 = X.l, 2 = Y.r, 1 = X.r}] complete 2 with 4
7 [S(XY )? S(X) ? eq(X,Y ), {X.l ? X.r,X.r = Y.l, Y.l ? Y.r, 0 = X.l, 1 = X.r}] complete 5 with 4
8 [eq, ?(?r
1
, r
2
?, ?r
3
, r
4
?), {r
1
? r
2
, r
2
= r
3
, r
3
? r
4
, 0 = r
1
, 2 = r
4
, 1 = r
2
}?] predict-pred from 6
9 [eq(a
1
X, a
2
Y )? ?eq(X,Y ), {a
1
.l + 1 = a
1
.r, a
1
.r = X.l,X.l ? X.r,
a
2
.l + 1 = a
2
.r, a
2
.r = Y.l, Y.l ? Y.r,X.r = a
2
.l, 0 = a
1
.l, 1 = X.r, 2 = Y.r}] predict-rule from 8
. . .
10 [eq, (?0, 1?, ?1, 2?), c] scan 8
11 [S(XY )? S(X)eq(X,Y )?, {. . . , 0 = X.l, 2 = Y.r, 1 = X.r, 1 = Y.l}] complete 6 with 10
12 [S, (?0, 2?), c] convert 11
Figure 3: Trace of a sample Earley parse
avoids this using a technique of dynamic updating
of a set of constraints on range boundaries. Exper-
iments show that this significantly decreases the
number of generated items, which confirms that
range boundary constraint propagation is a viable
method for a lazy computation of ranges.
The Earley parser could be improved by allow-
ing to process the predicates of the righthand sides
of clauses in any order, not necessarily from left
to right. This way, one could process predicates
whose range boundaries are better known first. We
plan to include this strategy in future work.
References
Franc?ois Barth?elemy, Pierre Boullier, Philippe De-
schamp, and
?
Eric de la Clergerie. 2001. Guided
parsing of Range Concatenation Languages. In Pro-
ceedings of ACL, pages 42?49.
Tilman Becker, Aravind K. Joshi, and Owen Rambow.
1991. Long-distance scrambling and tree adjoining
grammars. In Proceedings of EACL.
Tilman Becker, Owen Rambow, and Michael Niv.
1992. The Derivationel Generative Power of Formal
Systems or Scrambling is Beyond LCFRS. Tech-
nical Report IRCS-92-38, Institute for Research in
Cognitive Science, University of Pennsylvania.
E. Bertsch and M.-J. Nederhof. 2001. On the complex-
ity of some extensions of RCG parsing. In Proceed-
ings of IWPT 2001, pages 66?77, Beijing, China.
Pierre Boullier. 1999. Chinese numbers, mix, scram-
bling, and range concatenation grammars. In Pro-
ceedings of EACL, pages 53?60, Bergen, Norway.
Pierre Boullier. 2000. Range concatenation grammars.
In Proceedings of IWPT 2000, pages 53?64, Trento.
H?akan Burden and Peter Ljungl?of. 2005. Parsing lin-
ear context-free rewriting systems. In Proceedings
of IWPT 2005, pages 11?17, Vancouver.
Laura Kallmeyer, Timm Lichte, Wolfgang Maier, Yan-
nick Parmentier, and Johannes Dellert. 2008. De-
veloping an MCTAG for German with an RCG-
based parser. In Proceedings of LREC-2008, Mar-
rakech, Morocco.
Wolfgang Maier and Anders S?gaard. 2008. Tree-
banks and mild context-sensitivity. In Proceedings
of the 13th Conference on Formal Grammar 2008,
Hamburg, Germany.
Jens Michaelis and Marcus Kracht. 1996. Semilinear-
ity as a Syntactic Invariant. In Logical Aspects of
Computational Linguistics, Nancy.
Daniel Radzinski. 1991. Chinese number-names, tree
adjoining languages, and mild context-sensitivity.
Computational Linguistics, 17:277?299.
Beno??t Sagot. 2005. Linguistic facts as predicates over
ranges of the sentence. In Proceedings of LACL 05,
number 3492 in Lecture Notes in Computer Science,
pages 271?286, Bordeaux, France. Springer.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24(1& 2):3?36.
Anders S?gaard. 2008. Range concatenation gram-
mars for translation. In Proceedings of COLING,
Manchester, England.
K. Vijay-Shanker, David Weir, and Aravind Joshi.
1987. Characterising structural descriptions used by
various formalisms. In Proceedings of ACL.
Eric Villemonte de la Clergerie. 2002. Parsing mildly
context-sensitive languages with thread automata.
In Proceedings of COLING, Taipei, Taiwan.
12
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 111?119,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Is it Really that Difficult to Parse German?
Sandra Ku?bler, Erhard W. Hinrichs, Wolfgang Maier
SfS-CL, SFB 441, University of Tu?bingen
Wilhelmstr. 19
72074 Tu?bingen, Germany
 
kuebler,eh,wmaier@sfs.uni-tuebingen.de
Abstract
This paper presents a comparative study
of probabilistic treebank parsing of Ger-
man, using the Negra and Tu?Ba-D/Z tree-
banks. Experiments with the Stanford
parser, which uses a factored PCFG and
dependency model, show that, contrary to
previous claims for other parsers, lexical-
ization of PCFG models boosts parsing
performance for both treebanks. The ex-
periments also show that there is a big
difference in parsing performance, when
trained on the Negra and on the Tu?Ba-
D/Z treebanks. Parser performance for the
models trained on Tu?Ba-D/Z are compara-
ble to parsing results for English with the
Stanford parser, when trained on the Penn
treebank. This comparison at least sug-
gests that German is not harder to parse
than its West-Germanic neighbor language
English.
1 Introduction
There have been a number of recent studies on
probabilistic treebank parsing of German (Dubey,
2005; Dubey and Keller, 2003; Schiehlen, 2004;
Schulte im Walde, 2003), using the Negra tree-
bank (Skut et al, 1997) as their underlying data
source. A common theme that has emerged from
this research is the claim that lexicalization of
PCFGs, which has been proven highly beneficial
for other languages1 , is detrimental for parsing
accuracy of German. In fact, this assumption
is by now so widely held that Schiehlen (2004)
does not even consider lexicalization as a possible
1For English, see Collins (1999).
parameter and concentrates instead only on tree-
bank transformations of various sorts in his exper-
iments.
Another striking feature of all studies men-
tioned above are the relatively low parsing F-
scores achieved for German by comparison to the
scores reported for English, its West-Germanic
neighbor, using similar parsers. This naturally
raises the question whether German is just harder
to parse or whether it is just hard to parse the Ne-
gra treebank.2
The purpose of this paper is to address pre-
cisely this question by training the Stanford parser
(Klein and Manning, 2003b) and the LoPar parser
(Schmid, 2000) on the two major treebanks
available for German, Negra and Tu?Ba-D/Z, the
Tu?bingen treebank of written German (Telljohann
et al, 2005). A series of comparative parsing
experiments that utilize different parameter set-
tings of the parsers is conducted, including lexi-
calization and markovization. These experiments
show striking differences in performance between
the two treebanks. What makes this comparison
interesting is that the treebanks are of compara-
ble size and are both based on a newspaper cor-
pus. However, both treebanks differ significantly
in their syntactic annotation scheme. Note, how-
ever, that our experiments concentrate on the orig-
inal (context-free) annotations of the treebank.
The structure of this paper is as follows: sec-
tion 2 discusses three characteristic grammatical
features of German that need to be taken into ac-
count in syntactic annotation and in choosing an
appropriate parsing model for German. Section 3
introduces the Negra and Tu?Ba-D/Z treebanks and
2German is not the first language for which this question
has been raised. See Levy and Manning (2003) for a similar
discussion of Chinese and the Penn Chinese Treebank.
111
discusses the main differences between their anno-
tation schemes. Section 4 explains the experimen-
tal setup, sections 5-7 the experiments, and section
8 discusses the results.
2 Grammatical Features of German
There are three distinctive grammatical features
that make syntactic annotation and parsing of Ger-
man particularly challenging: its placement of the
finite verb, its flexible phrasal ordering, and the
presence of discontinuous constituents. These fea-
tures will be discussed in the following subsec-
tions.
2.1 Finite Verb Placement
In German, the placement of finite verbs depends
on the clause type. In non-embedded assertion
clauses, the finite verb occupies the second posi-
tion in the clause, as in (1a). In yes/no questions,
as in (1b), the finite verb appears clause-initially,
whereas in embedded clauses it appears clause fi-
nally, as in (1c).
(1) a. Peter
Peter
wird
will
das
the
Buch
book
gelesen
read
haben.
have
?Peter will have read the book.?
b. Wird
Will
Peter
Peter
das
the
Buch
book
gelesen
have
haben?
read
?Will Peter have read the book??
c. dass
that
Peter
Peter
das
the
Buch
book
gelesen
read
haben
have
wird.
will
?... that Peter will have read the book.?
Regardless of the particular clause type, any
cluster of non-finite verbs, such as gelesen haben
in (1a) and (1b) or gelesen haben wird in (1c), ap-
pears at the right periphery of the clause.
The discontinuous positioning of the verbal el-
ements in verb-first and verb-second clauses is the
traditional reason for structuring German clauses
into so-called topological fields (Drach, 1937;
Erdmann, 1886; Ho?hle, 1986). The positions of
the verbal elements form the Satzklammer (sen-
tence bracket) which divides the sentence into a
Vorfeld (initial field), a Mittelfeld (middle field),
and a Nachfeld (final field). The Vorfeld and the
Mittelfeld are divided by the linke Satzklammer
(left sentence bracket), which is realized by the
finite verb or (in verb-final clauses) by a comple-
mentizer field. The rechte Satzklammer (right sen-
tence bracket) is realized by the verb complex and
consists of verbal particles or sequences of verbs.
This right sentence bracket is positioned between
the Mittelfeld and the Nachfeld. Thus, the theory
of topological fields states the fundamental regu-
larities of German word order.
The topological field structures in (2) for the ex-
amples in (1) illustrate the assignment of topolog-
ical fields for different clause types.
(2) a.       Peter     wird        das
Buch    	   
 gelesen haben.  
b.   Wird        Peter     das Buch  
 	   
 gelesen haben?  
c.    
  dass         Peter     das
Buch    	   
 gelesen haben wird.  
(2a) and (2b) are made up of the following
fields: LK (for: linke Satzklammer) is occupied
by the finite verb. MF (for: Mittelfeld) contains
adjuncts and complements of the main verb. RK
(for: rechte Satzklammer) is realized by the ver-
bal complex (VC). Additionally, (2a) realizes the
topological field VF (for: Vorfeld), which contains
the sentence-initial constituent. The left sentence
bracket (LK) in (2c) is realized by a complemen-
tizer field (CF) and the right sentence bracket (RK)
by a verbal complex (VC) that contains the finite
verb wird.
2.2 Flexible Phrase Ordering
The second noteworthy grammatical feature of
German concerns its flexible phrase ordering. In
(3), any of the three complements and adjuncts
of the main verb (ge)lesen can appear sentence-
initially.
(3) a. Der
The
Mann
man
hat
has
gestern
yesterday
den
the
Roman
novel
gelesen.
read
?The man read the novel yesterday.?
b. Gestern hat der Mann den Roman gelesen
c. Den Roman hat der Mann gestern gelesen
In addition, the ordering of the elements that oc-
cur in the Mittelfeld is also free so that there are
two possible linearizations for each of the exam-
ples in (3a) - (3b), yielding a total of six distinct
orderings for the three complements and adjuncts.
Due to this flexible phrase ordering, the gram-
matical functions of constituents in German, un-
like for English, cannot be deduced from the con-
stituents? location in the tree. As a consequence,
parsing approaches to German need to be based on
treebank data which contain a combination of con-
stituent structure and grammatical functions ? for
parsing and evaluation.
112
0 1 2 3 4 5 6 7 8 9 10 11
500 501 502
503
504
Diese
PDAT
Metapher
NN
kann
VMFIN
die
ART
Freizeitmalerin
NN
durchaus
ADV
auch
ADV
auf
APPR
ihr
PPOSAT
Leben
NN
anwenden
VVINF
.
$.
NK NK NK NK MO AC NK NK
NP
OA
PP
MO HD
HD
NP
SB MO
VP
OC
S
Figure 1: A sample tree from Negra.
2.3 Discontinuous Constituents
A third characteristic feature of German syntax
that is a challenge for syntactic annotation and
for parsing is the treatment of discontinuous con-
stituents.
(4) Der
The
Mann
man
hat
has
gestern
yesterday
den
the
Roman
novel
gelesen,
read
den
which
ihm
him
Peter
Peter
empfahl.
recommended
?Yesterday the man read the novel which Peter rec-
ommended to him.?
(5) Peter
Peter
soll
is to
dem
the
Mann
man
empfohlen
recommended
haben,
have
den
the
Roman
novel
zu
to
lesen.
read
?Peter is said to have recommended to the man to
read the novel.?
(4) shows an extraposed relative clause which
is separated from its head noun den Roman by the
non-finite verb gelesen. (5) is an example of an
extraposed non-finite VP complement that forms a
discontinuous constituent with its governing verb
empfohlen because of the intervening non-finite
auxiliary haben. Such discontinuous structures
occur frequently in both treebanks and are handled
differently in the two annotation schemes, as will
be discussed in more detail in the next section.
3 The Negra and the Tu?Ba-D/Z
Treebanks
Both treebanks use German newspapers as their
data source: the Frankfurter Rundschau news-
paper for Negra and the ?die tageszeitung? (taz)
newspaper for Tu?Ba-D/Z. Negra comprises 20 000
sentences, Tu?Ba-D/Z 15 000 sentences. There is
evidence that the complexity of sentences in both
treebanks is comparable: sentence length as well
as the percentage of clause nodes per sentence is
comparable. In Negra, a sentence is 17.2 words
long, in Tu?ba-D/Z, 17.5 words. Negra has an av-
erage of 1.4 clause nodes per sentence, Tu?Ba-D/Z
1.5 clause nodes.
Both treebanks use an annotation framework
that is based on phrase structure grammar and that
is enhanced by a level of predicate-argument struc-
ture. Annotation for both was performed semi-
automatically. Despite all these similarities, the
treebank annotations differ in four important as-
pects: 1) Negra does not allow unary branching
whereas Tu?Ba-D/Z does; 2) in Negra, phrases re-
ceive a flat annotation whereas Tu?Ba-D/Z uses
phrase internal structure; 3) Negra uses crossing
branches to represent long-distance relationships
whereas Tu?Ba-D/Z uses a pure tree structure com-
bined with functional labels to encode this infor-
mation; 4) Negra encodes grammatical functions
in a combination of structural and functional la-
beling whereas Tu?Ba-D/Z uses a combination of
topological fields functional labels, which results
in a flatter structure on the clausal level. The two
treebanks also use different notions of grammat-
ical functions: Tu?Ba-D/Z defines 36 grammati-
cal functions covering head and non-head infor-
mation, as well as subcategorization for comple-
ments and modifiers. Negra utilizes 48 grammat-
ical functions. Apart from commonly accepted
grammatical functions, such as SB (subject) or
OA (accusative object), Negra grammatical func-
tions comprise a more extended notion, e.g. RE
(repeated element) or RC (relative clause).
(6) Diese
This
Metapher
metaphor
kann
can
die
the
Freizeitmalerin
amateur painter
durchaus
by all means
auch
also
auf
to
ihr
her
Leben
life
anwenden.
apply.
?The amateur painter can by all means apply this
metaphor also to her life.?
Figure 1 shows a typical tree from the Negra
treebank for sentence (6). The syntactic categories
are shown in circular nodes, the grammatical func-
tions as edge labels in square boxes. A major
113
0 1 2 3 4 5 6 7 8 9 10 11
500 501 502
503
504
Diese
PDAT
Metapher
NN
kann
VMFIN
die
ART
Freizeitmalerin
NN
durchaus
ADV
auch
ADV
auf
APPR
ihr
PPOSAT
Leben
NN
anwenden
VVINF
.
$.
NK NK NK NK MO AC NK NK
PP
MO HD
NP
OA HD
NP
SB MO
VP
OC
S
Figure 2: A Negra tree with resolved crossing branches.
0 1 2 3 4 5 6 7 8 9 10 11 12 13
500 501 502 503 504 505
506 507 508 509 510
511 512
513
Den
ART
vorigen
ADJA
Sonntag
NN
h?tte
VAFIN
Frank
NE
Michael
NE
Nehr
NE
am
PTKA
liebsten
ADJD
aus
APPR
dem
ART
Kalender
NN
gestrichen
VVPP
.
$.
HD HD ? ? ? ? HD ? HD HD
?
ADJX
? HD
VXFIN
HD
NX
? ?
NX
HD
VXINF
OV
NX
OA
EN?ADD
ON
ADJX
MOD
PX
FOPP
VF
?
LK
?
MF
?
VC
?
SIMPX
Figure 3: A sample tree from Tu?ba-D/Z.
phrasal category that serves to structure the sen-
tence as a whole is the verb phrase (VP). It con-
tains non-finite verbs (here: anwenden) together
with their complements (here: the accusative ob-
ject Diese Metapher) and adjuncts (here: the ad-
verb durchaus and the PP modifier auch auf ihr
Leben). The subject NP (here: die Freizeitma-
lerin) stands outside the VP and, depending on its
linear position, leads to crossing branches with the
VP. This happens in all cases where the subject
follows the finite verb as in Figure 1. Notice also
that the PP is completely flat and does not contain
an internal NP.
Another phenomenon that leads to the introduc-
tion of crossing branches in the Negra treebank are
discontinuous constituents of the kind illustrated
in section 2.3. Extraposed relative clauses, as in
(4), are analyzed in such a way that the relative
clause constituent is a sister of its head noun in the
Negra tree and crosses the branch that dominates
the intervening non-finite verb gelesen.
The crossing branches in the Negra treebank
cannot be processed by most probabilistic parsing
models since such parsers all presuppose a strictly
context-free tree structure. Therefore the Negra
trees must be transformed into proper trees prior
to training such parsers. The standard approach
for this transformation is to re-attach crossing non-
head constituents as sisters of the lowest mother
node that dominates all constituents in question in
the original Negra tree.
Figure 2 shows the result of this transformation
of the tree in Figure 1. Here, the fronted accusative
object Diese Metapher is reattached on the clause
level. Crossing branches do not only arise with re-
spect to the subject at the sentence level but also in
cases of extraposition and fronting of partial con-
stituents. As a result, approximately 30% of all
Negra trees contain at least one crossing branch.
Thus, tree transformations have a major impact
on the type of constituent structures that are used
for training probabilistic parsing models. Previous
work, such as Dubey (2005), Dubey and Keller
(2003), and Schiehlen (2004), uses the version of
Negra in which the standard approach to resolving
crossing branches has been applied.
(7) Den
The
vorigen
previous
Sonntag
Sunday
ha?tte
would have
Frank
Frank
Michael
Michael
Nehr
Nehr
am liebsten
preferably
aus
from
dem
the
Kalender
calendar
gestrichen.
deleted.
?Frank Michael Nehr would rather have deleted the
previous Sunday from the calendar.?
Figure 3 shows the Tu?Ba-D/Z annotation for
sentence (7), a sentence with almost identi-
cal phrasal ordering to sentence (6). Crossing
branches are avoided by the introduction of topo-
114
0 1 2 3 4 5 6 7 8 9
500 501 502 503 504 505
506 507 508 509
510
511
F?r
APPR
diese
PDAT
Behauptung
NN
hat
VAFIN
Beckmeyer
NE
bisher
ADV
keinen
PIAT
Nachweis
NN
geliefert
VVPP
.
$.
? HD HD HD HD ? HD HD
?
NX
HD
VXFIN
HD
NX
ON
ADVX
MOD
NX
OA
VXINF
OV
PX
OA?MOD
VF
?
LK
?
MF
?
VC
?
SIMPX
Figure 4: Tu?Ba-D/Z annotation without crossing branches.
logical structures (here: VF, MF and VC) into the
tree. Notice also that compared to the Negra anno-
tation, Tu?Ba-D/Z introduces more internal struc-
ture into NPs and PPs.
(8) Fu?r
For
diese
this
Behauptung
claim
hat
has
Beckmeyer
Beckmeyer
bisher
yet
keinen
no
Nachweis
evidence
geliefert.
provided.
?For this claim, Beckmeyer has not provided evi-
dence yet.?
In Tu?Ba-D/Z, long-distance relationships are
represented by a pure tree structure and specific
functional labels. Figure 4 shows the Tu?Ba-D/Z
annotation for sentence (8). In this sentence,
the prepositional phrase Fu?r diese Behauptung is
fronted. Its functional label (OA-MOD ) provides
the information that it modifies the accusative ob-
ject (OA ) keinen Nachweis.
4 Experimental Setup
The main goals behind our experiments were
twofold: (1) to re-investigate the claim that lex-
icalization is detrimental for treebank parsing of
German, and (2) to compare the parsing results for
the two German treebanks.
To investigate the first issue, the Stanford Parser
(Klein and Manning, 2003b), a state-of-the-art
probabilistic parser, was trained with both lexical-
ized and unlexicalized versions of the two tree-
banks (Experiment I). For lexicalized parsing, the
Stanford Parser provides a factored probabilistic
model that combines a PCFG model with a depen-
dency model.
For the comparison between the two treebanks,
two types of experiments were performed: a
purely constituent-based comparison using both
the Stanford parser and the pure PCFG parser
LoPar (Schmid, 2000) (Experiment II), and an in-
depth evaluation of the three major grammatical
functions subject, accusative object, and dative
object, using the Stanford parser (Experiment III).
All three experiments use gold POS tags ex-
tracted from the treebanks as parser input. All
parsing results shown below are averaged over a
ten-fold cross-validation of the test data. Experi-
ments I and II used versions of the treebanks that
excluded grammatical information, thus only con-
tained constituent labeling. For Experiment III,
all syntactic labels were extended by their gram-
matical function (e.g NX-ON for a subject NP in
Tu?Ba-D/Z or NP-SB for a Negra subject). Experi-
ments I and II included all sentences of a maximal
length of 40 words. Due to memory limitations
(7 GB), Experiment III had to be restricted to sen-
tences of a maximal length of 35 words.
5 Experiment I: Lexicalization
Experiment I investigates the effect of lexicaliza-
tion on parser performance for the Stanford Parser.
The results, summarized in Table 1, show that lex-
icalization improves parser performance for both
the Negra and the Tu?Ba-D/Z treebank in compar-
ison to unlexicalized counterpart models: for la-
beled bracketing, an F-score improvement from
86.48 to 88.88 for Tu?Ba-D/Z and an improve-
ment from 66.92 to 67.13 for Negra. This di-
rectly contradicts the findings reported by Dubey
and Keller (2003) that lexicalization has a nega-
tive effect on probabilistic parsing models for Ger-
man. We therefore conclude that these previous
claims, while valid for particular configurations of
115
Negra Tu?Ba-D/Z
precision recall F-score precision recall F-score
Stanford PCFG unlabeled 71.24 72.68 71.95 93.07 89.41 91.20
labeled 66.26 67.59 66.92 88.25 84.78 86.48
Stanford lexicalized unlabeled 71.31 73.12 72.20 91.60 91.21 91.36
labeled 66.30 67.99 67.13 89.12 88.65 88.88
Table 1: The results of lexicalizing German.
Negra Tu?Ba-D/Z
precision recall F-score precision recall F-score
LoPar unlabeled 70.84 72.51 71.67 92.62 88.58 90.56
labeled 65.86 67.41 66.62 87.39 83.57 85.44
Stanford unlabeled 71.24 72.68 71.95 93.07 89.41 91.20
labeled 66.26 67.59 66.92 88.25 84.78 86.48
Stanford + markov unlabeled 74.13 74.12 74.12 92.28 90.90 91.58
labeled 69.96 69.95 69.95 89.86 88.51 89.18
Table 2: A comparison of unlexicalized parsing of Negra and Tu?Ba-D/Z.
parsers and parameters, should not be generalized
to claims about probabilistic parsing of German in
general.
Experiment I also shows considerable differ-
ences in the overall scores between the two tree-
banks, with the F-scores for Tu?Ba-D/Z parsing ap-
proximating scores reported for English, but with
Negra scores lagging behind by an average mar-
gin of appr. 20 points. Of course, it is impor-
tant to note that such direct comparisons with En-
glish are hardly possible due to different annota-
tion schemes, different underlying text corpora,
etc. Nevertheless, the striking difference in parser
performance between the two German treebanks
warrants further attention. Experiments II and III
will investigate this matter in more depth.
6 Experiment II: Different Parsers
The purpose of Experiment II is to rule out the pos-
sibility that the differences in parser performance
for the two German treebanks produced by Ex-
periment I may just be due to using a particular
parser ? in this particular case the hybrid PCFG
and dependency model of the Stanford parser. Af-
ter all, Experiment I also yielded different results
concerning the received wisdom about the utility
of lexicalization from previously reported results.
In order to obtain a broader experimental base, un-
lexicalized models of the Stanford parser and the
pure PCFG parser LoPar were trained on both tree-
banks. In addition we experimented with two dif-
ferent parameter settings of the Stanford parser,
one with and one without markovization. The ex-
periment with markovization used parent informa-
tion (v=1) and a second order Markov model for
horizontal markovization (h=2). The results, sum-
marized in Table 2, show that parsing results for all
unlexicalized experiments show roughly the same
20 point difference in F-score that were obtained
for the lexicalized models in Experiment I. We
can therefore conclude that the difference in pars-
ing performance is robust across two parsers with
different parameter settings, such as lexicalization
and markovization.
Experiment II also confirms the finding of Klein
and Manning (2003a) and of Schiehlen (2004) that
horizontal and vertical markovization has a pos-
itive effect on parser performance. Notice also
that markovization with unlexicalized grammars
yields almost the same improvement as lexicaliza-
tion does in Experiment I.
7 Experiment III: Grammatical
Functions
In Experiments I and II, only constituent structure
was evaluated, which is highly annotation depen-
dent. It could simply be the case that the Tu?Ba-
D/Z annotation scheme contains many local struc-
tures that can be easily parsed by a PCFG model
or the hybrid Stanford model. Moreover, such
easy to parse structures may not be of great im-
portance when it comes to determining the cor-
rect macrostructure of a sentence. To empirically
verify such a conjecture, a separate evaluation of
116
0 1 2 3 4
500
Moran
NE
ist
VAFIN
l?ngst
ADV
weiter
ADJD
.
$.
SB HD MO PD
S
Figure 5: Negra annotation without unary nodes.
Negra Tu?Ba-D/Z
lab. prec. lab. rec. lab. F-score lab. prec. lab. rec. lab. F-score
without gramm. functions 69.96 69.95 69.95 89.86 88.51 89.18
all gramm. functions 47.20 56.43 51.41 75.73 74.93 75.33
subjects 52.50 58.02 55.12 66.82 75.93 71.08
accusative objects 35.14 36.30 35.71 43.84 47.31 45.50
dative objects 8.38 3.58 5.00 24.46 9.96 14.07
Table 3: A comparison of unlexicalized, markovized parsing of constituent structure and grammatical
functions in Negra and Tu?Ba-D/Z.
parser performance for different constituent types
would be necessary. However, even such an eval-
uation would only be meaningful if the annotation
schemes agree on the defining characteristics of
such constituent types. Unfortunately, this is not
the case for the two treebanks under considera-
tion. Even for arguably theory-neutral constituents
such as NPs, the two treebanks differ considerably.
In the Negra annotation scheme, single word NPs
directly project from the POS level to the clausal
level, while in Tu?Ba-D/Z, they project by a unary
rule first to an NP. An extreme case of this Negra
annotation is shown in Figure 5 for sentence (9).
Here, all the phrases are one word phrases and are
thus projected directly to the clause level.
(9) Moran
Moran
ist
is
la?ngst
already
weiter.
further
?Moran is already one step ahead.?
There is an even more important motivation
for not focusing on the standard constituent-based
parseval measures ? at least when parsing Ger-
man. As discussed earlier in section 2.2, obtain-
ing the correct constituent structure for a German
sentence will often not be sufficient for determin-
ing its intended meaning. Due to the word order
freeness of phrases, a given NP in any one po-
sition may in principle fulfill different grammat-
ical functions in the sentence as a whole. There-
fore grammatical functions need to be explicitly
marked in the treebank and correctly assigned dur-
ing parsing. Since both treebanks encode gram-
matical functions, this information is available for
parsing and can ultimately lead to a more mean-
ingful comparison of the two treebanks when used
for parsing.
The purpose of Experiment III is to investigate
parser performance on the treebanks when gram-
matical functions are included in the trees. For
these experiments, the unlexicalized, markovized
PCFG version of the Stanford parser was used,
with markovization parameters v=1 and h=2, as
in Experiment II. The results of this experiment
are shown in Table 3. The comparison of the ex-
periments with (line 2) and without grammatical
functions (line 1) confirms the findings of Dubey
and Keller (2003) that the task of assigning cor-
rect grammatical functions is harder than mere
constituent-based parsing. When evaluating on all
grammatical functions, the results for Negra de-
crease from 69.95 to 51.41, and for Tu?Ba-D/Z
from 89.18 to 75.33. Notice however, that the rela-
tive differences between Negra and Tu?Ba-D/Z that
were true for Experiments I and II remain more or
less constant for this experiment as well.
In order to get a clearer picture of the quality
of the parser output for each treebank, it is im-
portant to consider individual grammatical func-
tions. As discussed in section 3, the overall in-
ventory of grammatical functions is different for
the two treebanks. We therefore evaluated those
grammatical functions separately that are crucial
for determining function-argument structure and
117
that are at the same time the most comparable for
the two treebanks. These are the functions of sub-
ject (encoded as SB in Negra and as ON in Tu?Ba-
D/Z), accusative object (OA ), and dative object
(DA in Negra and OD in Tu?Ba-D/Z). Once again,
the results are consistently better for Tu?Ba-D/Z
(cf. lines 3-5 in Table 3), with subjects yielding
the highest results (71.08 vs. 55.12 F-score) and
dative objects the lowest results (14.07 vs. 5.00).
The latter results must be attributed to data sparse-
ness, dative object occur only appr. 1 000 times
in each treebank while subjects occur more than
15 000 times.
8 Discussion
The experiments presented in sections 5-7 show
that there is a difference in results of appr. 20%
between Negra and Tu?Ba-D/Z. This difference is
consistent throughout, i.e. with different parsers,
under lexicalization and markovization. These re-
sults lead to the conjecture that the reasons for
these differences must be sought in the differences
in the annotation schemes of the two treebanks.
In section 3, we showed that one of the ma-
jor differences in annotation is the treatment of
discontinuous constituents. In Negra, such con-
stituents are annotated via crossing branches,
which have to be resolved before parsing. In such
cases, constituents are extracted from their mother
constituents and reattached at higher constituents.
In the case of the discontinuous VP in Figure 1,
it leads to a VP rule with the following daugh-
ters: head (HD ) and modifier (MO ), while the
accusative object is directly attached at the sen-
tence level as a sister of the VP. This conversion
leads to inconsistencies in the training data since
the annotation scheme requires that object NPs are
daughters of the VP rather than of S. The incon-
sistency introduced by tree conversion are con-
siderable since they cover appr. 30% of all Ne-
gra trees (cf. section 3). One possible explana-
tion for the better performance of Tu?ba-D/Z might
be that it has more information about the correct
attachment site of extraposed constituents, which
is completely lacking in the context-free version
of Negra. For this reason, Ku?bler (2005) and
Maier (2006) tested a version of Negra which con-
tained information of the original attachment site
of these discontinuous constituents. In this ver-
sion of Negra, the grammatical function OA in
Figure 2 would be changed to OA VP to show
that it was originally attached to the VP. Experi-
ments with this version showed a decrease in F-
score from 52.30 to 49.75. Consequently, adding
this information in a similar way to the encoding
of discontinuous constituents in Tu?ba-D/Z harms
performance.
By contrast, Tu?Ba-D/Z uses topological fields
as the primary structuring principle, which leads to
a purely context-free annotation of discontinuous
structures. There is evidence that the use of topo-
logical fields is advantageous also for other pars-
ing approaches (Frank et al, 2003; Ku?bler, 2005;
Maier, 2006).
Another difference in the annotation schemes
concerns the treatment of phrases. Negra phrases
are flat, and unary projections are not annotated.
Tu?Ba-D/Z always projects to the phrasal category
and annotates more phrase-internal structure. The
deeper structures in Tu?Ba-D/Z lead to fewer rules
for phrasal categories, which allows the parser a
more consistent treatment of such phrases. For ex-
ample, the direct attachment of one word subjects
on the clausal level in Negra leads to a high num-
ber of different S rules with different POS tags for
the subject phrase. An empirical proof for the as-
sumption that flat phrase structures and the omis-
sion of unary nodes decrease parsing results is pre-
sented by Ku?bler (2005) and Maier (2006).
We want to emphasize that our experiments
concentrate on the original context-free annota-
tions of the treebanks. We did not investigate
the influence of treebank refinement in this study.
However, we would like to note that by a com-
bination of suffix analysis and smoothing, Dubey
(2005) was able to obtain an F-score of 85.2 for
Negra. For other work in the area of treebank re-
finement using the German treebanks see Ku?bler
(2005), Maier (2006), and Ule (2003).
9 Conclusion and Future Work
We have presented a comparative study of proba-
bilistic treebank parsing of German, using the Ne-
gra and Tu?Ba-D/Z treebanks. Experiments with
the Stanford parser, which uses a factored PCFG
and dependency model, show that, contrary to
previous claims for other parsers, lexicalization
of PCFG models boosts parsing performance for
both treebanks. The experiments also show that
there is a big difference in parsing performance,
when trained on the Negra and on the Tu?Ba-D/Z
treebanks. This difference remains constant across
118
lexicalized, unlexicalized (also using the LoPar
parser), and markovized models and also extends
to parsing of major grammatical functions. Parser
performance for the models trained on Tu?Ba-D/Z
are comparable to parsing results for English with
the Stanford parser, when trained on the Penn tree-
bank. This comparison at least suggests that Ger-
man is not harder to parse than its West-Germanic
neighbor language English.
Additional experiments with the Tu?Ba-D/Z
treebank are planned in future work. A new re-
lease of the Tu?Ba-D/Z treebank has become avail-
able that includes appr. 22 000 trees, instead of
the release with 15 000 sentences used for the ex-
periments reported in this paper. This new re-
lease also contains morphological information at
the POS level, including case and number. With
this additional information, we expect consider-
able improvement in grammatical function assign-
ment for the functions subject, accusative object,
and dative object, which are marked by nomina-
tive, accusative, and dative case, respectively.
Acknowledgments
We are grateful to Helmut Schmid and to Chris
Manning and his group for making their parsers
publicly available as well as to Tylman Ule for
providing the evaluation scripts. We are also grate-
ful to the anonymous reviewers for many help-
ful comments. And we are especially grateful to
Roger Levy for all the help he gave us in creating
the language pack for Tu?Ba-D/Z in the Stanford
parser.
References
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Erich Drach. 1937. Grundgedanken der Deutschen
Satzlehre. Diesterweg, Frankfurt/M.
Amit Dubey and Frank Keller. 2003. Probabilistic
parsing for German using sister-head dependencies.
In Proceedings of ACL 2003, pages 96?103, Sap-
poro, Japan.
Amit Dubey. 2005. What to do when lexicaliza-
tion fails: Parsing German with suffix analysis and
smoothing. In Proceedings of ACL 2005, Ann Ar-
bor, MI.
Oskar Erdmann. 1886. Grundzu?ge der deutschen
Syntax nach ihrer geschichtlichen Entwicklung
dargestellt. Verlag der Cotta?schen Buchhandlung,
Stuttgart, Germany.
Anette Frank, Markus Becker, Berthold Crysmann,
Bernd Kiefer, and Ulrich Scha?fer. 2003. Integrated
shallow and deep parsing: TopP meets HPSG. In
Proceedings of ACL 2003, Sapporo, Japan.
Tilman Ho?hle. 1986. Der Begriff ?Mittel-
feld?, Anmerkungen u?ber die Theorie der topo-
logischen Felder. In Akten des Siebten Interna-
tionalen Germanistenkongresses 1985, pages 329?
340, Go?ttingen, Germany.
Dan Klein and Christopher Manning. 2003a. Accurate
unlexicalized parsing. In Proceedings of ACL 2003,
pages 423?430, Sapporo, Japan.
Dan Klein and Christopher Manning. 2003b. Fast ex-
act inference with a factored model for natural lan-
guage parsing. In Advances in Neural Information
Processing Systems 15 (NIPS 2002), pages 3?10,
Vancouver, Canada.
Sandra Ku?bler. 2005. How do treebank annotation
schemes influence parsing results? Or how not to
compare apples and oranges. In Proceedings of
RANLP 2005, Borovets, Bulgaria.
Roger Levy and Christopher Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank? In
Proceedings of ACL 2003, pages 439?446, Sapporo,
Japan.
Wolfgang Maier. 2006. Annotation schemes and
their influence on parsing results. In Proceedings of
the ACL-2006 Student Research Workshop, Sydney,
Australia.
Michael Schiehlen. 2004. Annotation strategies for
probabilistic parsing in German. In Proceedings of
COLING 2004, Geneva, Switzerland.
Helmut Schmid. 2000. LoPar: Design and implemen-
tation. Technical report, Universita?t Stuttgart, Ger-
many.
Sabine Schulte im Walde. 2003. Experiments on
the Automatic Induction of German Semantic Verb
Classes. Ph.D. thesis, Institut fu?r Maschinelle
Sprachverarbeitung, Universita?t Stuttgart.
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and
Hans Uszkoreit. 1997. An annotation scheme for
free word order languages. In Proceedings of ANLP
1997, Washington, D.C.
Heike Telljohann, Erhard W. Hinrichs, Sandra Ku?bler,
and Heike Zinsmeister, 2005. Stylebook for the
Tu?bingen Treebank of Written German (Tu?Ba-
D/Z). Seminar fu?r Sprachwissenschaft, Universita?t
Tu?bingen, Germany.
Tylman Ule. 2003. Directed treebank refinement for
PCFG parsing. In Proceedings of TLT 2003, Va?xjo?,
Sweden.
119
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 61?64,
Paris, October 2009. c?2009 Association for Computational Linguistics
An Incremental Earley Parser for Simple Range Concatenation Grammar
Laura Kallmeyer and Wolfgang Maier
Collaborative Research Center 833
University of Tu?bingen
Tu?bingen, Germany
{lk,wmaier}@sfs.uni-tuebingen.de
Abstract
We present an Earley-style parser for
simple range concatenation grammar, a
formalism strongly equivalent to linear
context-free rewriting systems. Further-
more, we present different filters which
reduce the number of items in the pars-
ing chart. An implementation shows that
parses can be obtained in a reasonable
time.
1 Introduction
Linear context-free rewriting systems (LCFRS)
(Vijay-Shanker et al, 1987), the equivalent mul-
tiple context-free grammars (MCFG) (Seki et al,
1991) and simple range concatenation grammars
(sRCG) (Boullier, 1998) have recently attracted
an increasing interest in the context of natu-
ral language processing. For example, Maier
and S?gaard (2008) propose to extract simple
RCGs from constituency treebanks with crossing
branches while Kuhlmann and Satta (2009) pro-
pose to extract LCFRS from non-projective depen-
dency treebanks. Another application area of this
class of formalisms is biological computing (Kato
et al, 2006).
This paper addresses the symbolic parsing of
sRCG/LCFRS. Starting from the parsing algo-
rithms presented in Burden and Ljunglo?f (2005)
and Villemonte de la Clergerie (2002), we pro-
pose an incremental Earley algorithm for simple
RCG. The strategy is roughly like the one pur-
sued in Villemonte de la Clergerie (2002). How-
ever, instead of the automaton-based formalization
in Villemonte de la Clergerie?s work, we give a
general formulation of an incremental Earley al-
gorithm, using the framework of parsing as de-
duction. In order to reduce the search space, we
introduce different types of filters on our items.
We have implemented this algorithm and tested it
on simple RCGs extracted from the German tree-
banks Negra and Tiger.
In the following section, we introduce simple
RCG and in section 3, we present an algorithm for
symbolic parsing of simple RCG. Section 4 then
presents different filtering techniques to reduce the
number of items. We close discussing future work.
2 Grammar Formalism
A range concatenation grammar (RCG) is a 5-
tupleG = (N,T, V, P, S). N is a finite set of non-
terminals (predicate names) with an arity function
dim: N ? N+, T and V are disjoint finite sets of
terminals and variables. P is a finite set of clauses
of the form ?0 ? ?1 . . . ?m, where m ? 0 and
each of the ?i, 0 ? i ? m, is a predicate of the
form Ai(?i1, . . . , ?idim(A)). Each ?ij ? (T ? V )?,
1 ? j ? dim(A) and 0 ? i ? k, is an argument.
As a shorthand notation for Ai(?1, . . . , ?dim(A)),
we use Ai(~?). S ? N is the start predicate name
with dim(S) = 1.
Note that the order of right-hand side (RHS)
predicates in a clause is of no importance. Sub-
classes of RCGs are introduced for further ref-
erence: An RCG G = (N,T, V, P, S) is sim-
ple if for all c ? P , it holds that every variable
X occurring in c occurs exactly once in the left-
hand side (LHS) and exactly once in the RHS, and
each argument in the RHS of c contains exactly
one variable. A simple RCG is ordered if for all
?0 ? ?1 ? ? ??m ? P , it holds that if a variable X1
precedes a variable X2 in a ?i, 1 ? i ? m, then
X1 also precedes X2 in ?0. The ordering require-
ment does not change the expressive power, i.e.,
ordered simple RCG is equivalent to simple RCG
(Villemonte de la Clergerie, 2002). An RCG is
?-free if it either contains no ?-rules or there is ex-
actly one rule S(?) ? ? and S does not appear in
any of the righthand sides of the rules in the gram-
mar. A rule is an ?-rule if one of the arguments
61
of the lefthand side is the empty string ?. (Boul-
lier, 1998) shows that for every simple RCG, one
can construct an equivalent ?-free simple RCG. An
RCG G = (N,T, V, P, S) is a k-RCG if for all
A ? N, dim(A) ? k.
The language of RCGs is based on the notion
of range. For a string w1 ? ? ?wn a range is a pair
of indices ?i, j? with 0 ? i ? j ? n, i.e., a
string span, which denotes a substring wi+1 ? ? ?wj
in the source string or a substring vi+1 ? ? ? vj in
the target string. Only consecutive ranges can be
concatenated into new ranges. Terminals, vari-
ables and arguments in a clause are bound to
ranges by a substitution mechanism. An instan-
tiated clause is a clause in which variables and ar-
guments are consistently replaced by ranges; its
components are instantiated predicates. For ex-
ample A(?g ? ? ?h?) ? B(?g + 1 ? ? ? h?) is an in-
stantiation of the clause A(aX1) ? B(X1) if
the target string is such that wg+1 = a. A de-
rive relation ? is defined on strings of instanti-
ated predicates. If an instantiated predicate is the
LHS of some instantiated clause, it can be replaced
by the RHS of that instantiated clause. The lan-
guage of an RCG G = (N,T, V, P, S) is the set
L(G) = {w1 ? ? ?wn | S(?0, n?) ?? ?}, i.e., an in-
put string w1 ? ? ?wn is recognized if and only if the
empty string can be derived from S(?0, n?). In this
paper, we are dealing only with ordered simple
RCGs. The ordering requirement does not change
the expressive power (Villemonte de la Clergerie,
2002). Furthermore, without loss of generality, we
assume that for every clause, there is a k ? 0 such
that the variables occurring in the clause are ex-
actly X1, . . . ,Xk.
We define derivation trees for simple RCGs as
unordered trees whose internal nodes are labelled
with predicate names and whose leaves are la-
belled with ranges such that all internal nodes
are licensed by RCG clause instantiations: given
a simple RCG G and a string w, a tree D =
?V,E, r? is a derivation tree of w = a1 . . . an
iff 1. there are exactly n leaves in D labelled
?0, 1?, . . . , ?n ? 1, n? and 2. for all v0 ? V with
v1, . . . , vn ? V , n ? 1 being all vertices with
?v0, vi? ? E (1 ? i ? n) such that the leftmost
range dominated by vi precedes the leftmost range
dominated by vi+1 (1 ? i < n): there is a clause
instantiation A0(~?0) ? A1(~?1) . . . An( ~?n) such
that a) l(vi) = Ai for 0 ? i ? n and b) the yield
of the leaves dominates by vi is ~?i.
3 Parsing
Our parsing algorithm is a modification of the
?incremental algorithm? of Burden and Ljunglo?f
(2005) with a strategy very similar to the strategy
adopted by Thread Automata (Villemonte de la
Clergerie, 2002). It assumes the grammar to be
ordered and ?-free. We refrain from supporting
non-?-free grammars since the treebank grammars
used with our implementation are all ?-free. How-
ever, note that only minor modifications would be
necessary in order to support non-?-free grammars
(see below).
We process the arguments of LHS of clauses in-
crementally, starting from an S-clause. Whenever
we reach a variable, we move into the clause of
the corresponding RHS predicate (predict or re-
sume). Whenever we reach the end of an argu-
ment, we suspend this clause and move into the
parent clause that has called the current one. In
addition, we treat the case where we reach the end
of the last argument and move into the parent as a
special case. Here, we first convert the item into
a passive one and then complete the parent item
with this passive item. This allows for some addi-
tional factorization.
The item form for passive items is [A, ~?] where
A a predicate of some arity k, ~? is a range vector of
arity k. The item form for active items: [A(~?) ?
A1( ~?1) . . . Am( ~?m), pos, ?i, j?, ~?] where A(~?) ?
A1( ~?1) . . . Am( ~?m) ? P ; pos ? {0, . . . , n} is the
position up to which we have processed the input;
?i, j? ? N2 marks the position of our dot in the
arguments of the predicate A: ?i, j? indicates that
we have processed the arguments up to the jth ele-
ment of the ith argument; ~? is an range vector con-
taining the bindings of the variables and terminals
occurring in the lefthand side of the clause (~?(i)
is the range the ith element is bound to). When
first predicting a clause, it is initialized with a vec-
tor containing only symbols ??? for ?unknown?.
We call such a vector (of appropriate arity) ~?init.
We introduce an additional piece of notation. We
write ~?(X) for the range bound to the variable X
in ~?. Furthermore, we write ~?(?i, j?) for the range
bound to the jth element in the ith argument of the
clause lefthand side.
Applying a range vector ~? containing variable
bindings for a given clause c to the argument vec-
tor of the lefthand side of c means mapping the ith
element in the arguments to ~?(i) and concatenat-
ing adjacent ranges. The result is defined iff every
62
argument is thereby mapped to a range.
We start by predicting the S-predicate:
[S(~?) ? ~?, 0, ?1, 0?, ~?init] S(
~?) ? ~? ? P
Scan: Whenever the next symbol after the dot
is the next terminal in the input, we can scan it:
[A(~?) ? ~?, pos, ?i, j?, ~?]
[A(~?) ? ~?, pos+ 1, ?i, j + 1?, ~??]
~?(i, j+1) = wpos+1
where ~?? is ~? updated with ~?(i, j + 1) =
?pos, pos+ 1?.
In order to support ?-free grammars, one would
need to store the pair of indices a ? is mapped to
in the range vector, along with the mappings of
terminals and variables. The indices could be ob-
tained through a Scan-? operation, parallel to the
Scan operation.
Predict: Whenever our dot is left of a variable
that is the first argument of some RHS predicate
B, we predict new B-clauses:
[A(~?) ? . . . B(X, . . . ) . . . , pos, ?i, j?, ~?A]
[B(~?) ? ~?, pos, ?1, 0?, ~?init]
with the side condition ~?(i, j + 1) = X,B(~?) ?
~? ? P .
Suspend: Whenever we arrive at the end of an
argument that is not the last argument, we suspend
the processing of this clause and we go back to the
item that was used to predict it.
[B(~?) ? ~?, pos?, ?i, j?, ~?B ],
[A(~?) ? . . . B(~?) . . . , pos, ?k, l?, ~?A]
[A(~?) ? . . . B(~?) . . . , pos?, ?k, l + 1?, ~?]
where the dot in the antecedent A-item precedes
the variable ~?(i), |~?(i)| = j (the ith argument has
length j and has therefore been completely pro-
cessed), |~?| < i (the ith argument is not the last
argument of B), ~?B(~?(i)) = ?pos, pos?? and for
all 1 ? m < i: ~?B(~?(m)) = ~?A(~?(m)). ~? is ~?A
updated with ~?A(~?(i)) = ?pos, pos??.
Convert: Whenever we arrive at the end of the
last argument, we convert the item into a passive
one:
[B(~?) ? ~?, pos, ?i, j?, ~?B ]
[B, ?]
|~?(i)| = j, |~?| = i,
~?B(~?) = ?
Complete: Whenever we have a passive B item
we can use it to move the dot over the variable of
the last argument of B in a parent A-clause that
was used to predict it.
[B, ~?B], [A(~?) ? . . . B(~?) . . . , pos, ?k, l?, ~?A]
[A(~?) ? . . . B(~?) . . . , pos?, ?k, l + 1?, ~?]
where the dot in the antecedent A-item precedes
the variable ~?(|~?B |), the last range in ~?B is
?pos, pos??, and for all 1 ? m < |~?B |: ~?B(m) =
~?A(~?(m)). ~? is ~?A updated with ~?A(~?(|~?B |)) =
?pos, pos??.
Resume: Whenever we are left of a variable
that is not the first argument of one of the RHS
predicates, we resume the clause of the RHS pred-
icate.
[A(~?) ? . . . B(~?) . . . , pos, ?i, j?, ~?A],
[B(~?) ? ~?, pos?, ?k ? 1, l?, ~?B]
[B(~?) ? ~?, pos, ?k, 0?, ~?B]
where ~?(i)(j + 1) = ~?(k), k > 1 (the next el-
ement is a variable that is the kth element in ~?,
i.e., the kth argument of B), |~?(k ? 1)| = l, and
~?A(~?(m)) = ~?B(~?)(m) for all 1 ? m ? k ? 1.
The goal item has the form [S, ?0, n?].
Note that, in contrast to a purely bottom-up
CYK algorithm, the Earley algorithm presented
here is prefix valid, provided that the grammar
does not contain useless symbols.
4 Filters
During parsing, various optimizations known from
(P)CFG parsing can be applied. More concretely,
because of the particular form of our simple
RCGs, we can use several filters to reject items
very early that cannot lead to a valid parse tree for
a given input w = w1 . . . wn.
Since our grammars are ?-free, we know that
each variable or occurrence of a terminal in the
clause must cover at least one terminal in the in-
put. Furthermore, since separations between ar-
guments are generated only in cases where be-
tween two terminals belonging to the yield of a
non-terminal, there is at least one other terminals
that is not part of the yield, we know that between
different arguments of a predicate, there must be at
least one terminal in the input. Consequently, we
obtain as a filtering condition on the validity of an
active item that the length of the remaining input
must be greater or equal to the number of variables
and terminal occurrences plus the number of argu-
ment separations to the right of the dot in the left-
hand side of the clause. More formally, an active
item [A(~?) ? A1( ~?1) . . . Am( ~?m), pos, ?i, j?, ~?]
satisfies the length filter iff
(n? pos)
? (|~?(i)| ? j) + ?dim(A)k=i+1 |~?(k)| + (dim(A) ? i)
The length filter is applied to results of predict,
resume, suspend and complete.
A second filter, first proposed in Klein and
Manning (2003), checks for the presence of re-
quired preterminals. In our case, we assume the
63
preterminals to be treated as terminals, so this fil-
ter amounts to checking for the presence of all
terminals in the predicted part of a clause (the
part to the right of the dot) in the remaining in-
put. Furthermore, we check that the terminals
appear in the predicted order and that the dis-
tance between two of them is at least the num-
ber of variables/terminals and argument separa-
tions in between. In other words, an active item
[A(~?) ? A1( ~?1) . . . Am( ~?m), pos, ?i, j?, ~?] satis-
fies the terminal filter iff we can find an injec-
tive mapping fT : Term = {?k, l? | ~?(k)(l) ? T
and either k > i or (k = i and l > j)} ?
{pos+ 1, . . . , n} such that
1. wfT (?k,l?) = ~?(k)(l) for all ?k, l? ? Term;
2. for all ?k1, l1?, ?k2, l2? ? Term with k1 = k2
and l1 < l2: fT (?k2, l2?) ? fT (?k1, l1?) +
(l2 ? l1);
3. for all ?k1, l1?, ?k2, l2? ? Term with k1 <
k2: fT (?k2, l2?) ? fT (?k1, l1?) + (|~?(k1)| ?
l1) + ?k2?1k=k1+1|~?(k)| + l2 + (k2 ? k1).
Checking this filter amounts to a linear traversal
of the part of the lefthand side of the clause that
is to the right of the dot. We start with index i =
pos + 1, for every variable or gap we increment
i by 1. For every terminal a, we search the next
a in the input, starting at position i. If it occurs
at position j, then we set i = j and continue our
traversal of the remaining parts of the lefthand side
of the clause.
The preterminal filter is applied to results of the
predict and resume operations.
We have implemented the incremental Earley
parser with the filtering conditions on items. In
order to test it, we have extracted simple RCGs
from the first 1000 sentences of Negra and Tiger
(with removed punctuation) using the algorithm
described in Maier and S?gaard (2008) and parsed
the sentences 1001-1100 with it. The grammars
contained 2474 clauses (Negra) and 2554 clauses
(Tiger). The following table contains the to-
tal number of sentences for different length and
resp. the number of sentences for which a parse
was found, along with the average parsing times
of those that had a parse:
Negra Tiger
parse/tot av. t. parse/tot av. t.
|w| ? 20 73/84 0.40 sec. 50/79 0.32
20 ?
|w| ? 35 14/16 2.14 sec. 10/19 2.16
5 Conclusion and Future Work
We have presented an Earley-style algorithm for
simple range concatenation grammar, formulated
as deduction system. Furthermore, we have pre-
sented a set of filters on the chart reducing the
number of items. An implementation and a test
with grammars extracted from treebanks showed
that reasonable parsing times can be achieved.
We are currently working on a probabilistic
k-best extension of our parser which resumes
comparable work for PCFG (Huang and Chiang,
2005). Unfortunately, experiments with the Ear-
ley algorithm have shown that with grammars of a
reasonable size for data-driven parsing (> 15, 000
clauses), an exhaustive parsing is no longer ef-
ficient, due to the highly ambiguous grammars.
Algorithms using only passive items seem more
promising in this context since they facilitate the
application of A? parsing techniques.
References
Pierre Boullier. 1998. Proposal for a natural lan-
guage processing syntactic backbone. Rapport de
Recherche RR-3342, INRIA.
Ha?kan Burden and Peter Ljunglo?f. 2005. Parsing lin-
ear context-free rewriting systems. In Proceedings
of IWPT 2005.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT 2005.
Yuki Kato, Hiroyuki Seki, and Tadao Kasami. 2006.
Stochastic multiple context-free grammar for RNA
pseudoknot modeling. In Proceedings of TAG+8.
Dan Klein and Christopher D. Manning. 2003. A*
Parsing: Fast Exact Viterbi Parse Selection. In Pro-
ceedings of HLT-NAACL.
Marco Kuhlmann and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Proceedings of EACL.
Wolfgang Maier and Anders S?gaard. 2008. Tree-
banks and mild context-sensitivity. In Proceedings
of Formal Grammar 2008.
Hiroyuki Seki, Takahashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On multiple context-free
grammars. Theoretical Computer Science.
K. Vijay-Shanker, David Weir, and Aravind Joshi.
1987. Characterising structural descriptions used by
various formalisms. In Proceedings of ACL.
Eric Villemonte de la Clergerie. 2002. Parsing mildly
context-sensitive languages with thread automata.
In Proceedings of COLING.
64
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 69?72,
Paris, October 2009. c?2009 Association for Computational Linguistics
Synchronous Rewriting in Treebanks
Laura Kallmeyer
University of Tu?bingen
Tu?bingen, Germany
lk@sfs.uni-tuebingen.de
Wolfgang Maier
University of Tu?bingen
Tu?bingen, Germany
wo.maier@uni-tuebingen.de
Giorgio Satta
University of Padua
Padova, Italy
satta@dei.unipd.it
Abstract
Several formalisms have been proposed
for modeling trees with discontinuous
phrases. Some of these formalisms allow
for synchronous rewriting. However, it
is unclear whether synchronous rewriting
is a necessary feature. This is an impor-
tant question, since synchronous rewrit-
ing greatly increases parsing complexity.
We present a characterization of recursive
synchronous rewriting in constituent tree-
banks with discontinuous annotation. An
empirical investigation reveals that syn-
chronous rewriting is actually a neces-
sary feature. Furthermore, we transfer this
property to grammars extracted from tree-
banks.
1 Introduction
Discontinuous phrases are frequent in natural
language, particularly in languages with a rela-
tively free word order. Several formalisms have
been proposed in the literature for modeling trees
containing such phrases. These include non-
projective dependency grammar (Nivre, 2006),
discontinuous phrase structure grammar (DPSG)
(Bunt et al, 1987), as well as linear context-
free rewriting systems (LCFRS) (Vijay-Shanker et
al., 1987) and the equivalent formalism of sim-
ple range concatenation grammar (sRCG) (Boul-
lier, 2000). Kuhlmann (2007) uses LCFRS for
non-projective dependency trees. DPSG have
been used in Plaehn (2004) for data-driven pars-
ing of treebanks with discontinuous constituent
annotation. Maier and S?gaard (2008) extract
sRCGs from treebanks with discontinuous con-
stituent structures.
Both LCFRS and sRCG can model discontinu-
ities and allow for synchronous rewriting as well.
We speak of synchronous rewriting when two or
more context-free derivation processes are instan-
tiated in a synchronous way. DPSG, which has
also been proposed for modeling discontinuities,
does not allow for synchronous rewriting because
the different discontinuous parts of the yield of a
non-terminal are treated locally, i.e., their deriva-
tions are independent from each other. So far, syn-
chronous rewriting has not been empirically mo-
tivated by linguistic data from treebanks. In this
paper, we fill this gap by investigating the exis-
tence of structures indicating synchronous rewrit-
ing in treebanks with discontinuous annotations.
The question of whether we can find evidence for
synchronous rewriting has consequences for the
complexity of parsing. In fact, parsing with syn-
chronous formalisms can be carried out in time
polynomial in the length of the input string, with
a polynomial degree depending on the maximum
number of synchronous branches one can find in
derivations (Seki et al, 1991).
In this paper, we characterize synchronous
rewriting as a property of trees with crossing
branches and in an empirical evaluation, we con-
firm that treebanks do contain recursive syn-
chronous rewriting which can be linguistically
motivated. Furthermore, we show how this char-
acterization transfers to the simple RCGs describ-
ing these trees.
2 Synchronous Rewriting Trees in
German treebanks
By synchronous rewriting we indicate the syn-
chronous instantiation of two or more context-free
derivation processes. As an example, consider the
language L = {anbncndn | n ? 1}. Each
of the two halves of some w ? L can be ob-
tained through a stand-alone context-free deriva-
tion, but for w to be in L the two derivations must
be synchronized somehow. For certain tasks, syn-
chronous rewriting is a desired property for a for-
malism. In machine translation, e.g., synchronous
69
rewriting is extensively used to model the syn-
chronous dependence between the source and tar-
get languages (Chiang, 2007). The question we
are concerned with in this paper is whether we can
find instances of recursive synchronous rewriting
in treebanks that show discontinuous phrases.
We make the assumption that, if the annota-
tion of a treebank allows to express synchronous
rewriting, then all cases of synchronous rewriting
are present in the annotation. This means that, on
the one hand, there are no cases of synchronous
rewriting that the annotator ?forgot? to encode.
Therefore unrelated cases of parallel iterations in
different parts of a tree are taken to be truly unre-
lated. On the other hand, if synchronous rewrit-
ing is annotated explicitely, then we take it to be a
case of true synchronous rewriting, even if, based
on the string, it would be possible to find an anal-
ysis that does not require synchronous rewriting.
This assumption allows us to concentrate only on
explicit cases of synchronous rewriting .
We concentrate on German treebanks annotated
with trees with crossing branches. In such trees,
synchronous rewriting amounts to cases where dif-
ferent components of a non-terminal category de-
velop in parallel. In particular, we search for cases
where the parallelism can be iterated. An exam-
ple is the relative clause in (1), found in TIGER.
Fig. 1 gives the annotation. As can be seen in
the annotation, we have two VP nodes, each of
which has a discontinuous span consisting of two
parts. The two parts are separated by lexical ma-
terial not belonging to the VPs. The two com-
ponents of the second VP (Pop-Idol and werden)
are included in the two components of the first,
higher, VP (genausogut auch Pop-Idol and wer-
den ko?nnen). In other words, the two VP compo-
nents are rewritten in parallel containing again two
smaller VP components.
(1) . . . der
. . . who
genausogut
as well
auch
also
Pop-Idol
pop-star
ha?tte
AUX
werden
become
ko?nnen
could
?who could as well also become a pop-star?
Let us assume the following definitions: We
map the elements of a string to their positions. We
then say that the yield ? of a node n in a tree is
the set of all indices i such that n dominates the
leaf labeled with the ith terminal. A yield ? has a
gap if there are i1 < i2 < i3 such that i1, i3 ? ?
and i2 /? ?. For all i, j ? ? with i < j, the set
??i,j? = {k | i ? k ? j} is a component of ? if
??i,j? ? ? and i?1 /? ? and j+1 /? ?. We order
the components of ? such that ??i1,j1? < ??i2,j2?
if i1 < i2.
Trees showing recursive synchronous rewrit-
ing can be characterized as follows: We have a
non-terminal node n1 with label A whose yield
has a gap. n1 dominates another node n2 with la-
bel A such that for some i 6= j, the ith component
of the yield of n2 is contained in the ith component
of the yield of n1 and similar for the jth compo-
nent. We call the path from n1 to n2 a recursive
synchronous rewriting segment (RSRS).
Table 1 shows the results obtained from search-
ing for recursive synchronous rewriting in the Ger-
man TIGER and NeGra treebanks. In a prepro-
cessing step, punctuation has been removed, since
it is directly attached to the root node and therefore
not included in the annotation.
TIGER NeGra
number of trees 40,013 20,597
total num. of RSRS in all trees 1476 600
av. RSRS length in all trees 2.13 2.12
max. RSRS length in all trees 5 4
Table 1: Synchronous rewriting in treebanks
Example (1) shows that we find instances of re-
cursive synchronous rewriting where each of the
rewriting steps adds something to both of the par-
allel components. (1) was not an isolated case.
The annotation of (1) in Fig. 1 could be turned
into a context-free structure if the lowest node
dominating the material in the gap while not
dominating the synchronous rewriting nodes (here
VAFIN) is attached lower, namely below the lower
VP node. (Note however that there is good linguis-
tic motivation for attaching it high.) Besides such
cases, we even encountered cases where the dis-
continuity cannot be removed this way. An exam-
ple is (2) (resp. Fig. 2) where we have a gap con-
taining an NP such that the lowest node dominat-
ing this NP while not dominating the synchronous
rewriting nodes has a daughter to the right of the
yields of the synchronous rewriting nodes, namely
the extraposed relative clause. This structure is of
the type ancbnd, where a and b depend on each
other in a left-to-right order and can be nested,
and c and d also depend on each other and must
be generated together. This is a structure that re-
quires synchronous rewriting, even on the basis of
the string language. Note that the nesting of VPs
can be iterated, as can be seen in (3).
(2) . . . ob
. . . whether
auf
on
deren
their
Gela?nde
premises
der
the
Typ
type
von
of
70
S
VP
VP
PRELS ADV ADV NN VAFIN VAINF VMINF
der genausogut auch Pop-Idol ha?tte werden ko?nnen
Figure 1: Example for recursive synchronous rewriting
Abstellanlage
parking facility
gebaut
built
werden
be
ko?nne,
could,
der
which
. . .
. . .
?whether on their premises precisely the type of parking
facility could be built, which . . . ?
(3) . . . ob
. . . whether
auf
on
deren
their
Gela?nde
premises
der
the
Typ
type
von
of
Abstellanlage
parking facility
eigentlich
actually
ha?tte
had
schon
already
gebaut
built
werden
be
sollen,
should,
der
which
. . .
. . .
?whether on their premises precisely the type of parking
facility should actually already have been built, which
. . . ?
As a conclusion from these empirical results,
we state that to account for the data we can find in
treebanks with discontinuities, i.e., with crossing
branches, we need a formalism that can express
synchronous rewriting.
3 Synchronous Rewriting in Grammars
Extracted from Treebanks
In the following, we will use simple RCG (which
are equivalent to LCFRS) to model our treebank
annotations. We extract simple RCG rewriting
rules from NeGra and TIGER and check them for
the possibility to generate recursive synchronous
rewriting.
A simple RCG (Boullier, 2000) is a tuple G =
(N,T, V, P, S) where a) N is a finite set of pred-
icate names with an arity function dim: N ? N,
b) T and V are disjoint finite sets of terminals and
variables, c) P is a finite set of clauses of the form
A(?1, . . . , ?dim(A)) ? A1(X(1)1 , . . . ,X(1)dim(A1))
? ? ?Am(X(m)1 , . . . ,X(m)dim(Am))
for m ? 0 where A,A1, . . . , Am ? N , X(i)j ?
V for 1 ? i ? m, 1 ? j ? dim(Ai) and ?i ?
(T ? V )? for 1 ? i ? dim(A), and e) S ? N is
the start predicate name with dim(S) = 1. For all
c ? P , it holds that every variable X occurring in
c occurs exactly once in the left-hand side (LHS)
and exactly once in the RHS. A simple RCG G =
(N,T, V, P, S) is a simple k-RCG if for all A ?
N, dim(A) ? k.
For the definition of the language of a simple
RCG, we borrow the LCFRS definitions here: Let
G = ?N,T, V, P, S? be a simple RCG. For every
A ? N , we define the yield of A, yield(A) as
follows:
a) For every A(~?) ? ?, ~? ? yield(A);
b) For every clause
A(?1, . . . , ?dim(A)) ? A1(X(1)1 , . . . ,X(1)dim(A1))
? ? ?Am(X(m)1 , . . . ,X(m)dim(Am))
and all ~?i ? yield(Ai) for 1 ? i ? m,
?f(?1), . . . , f(?dim(A))? ? yield(A) where
f is defined as follows:
(i) f(t) = t for all t ? T ,
(ii) f(X(i)j ) = ~?i(j) for all 1 ? i ? m, 1 ?
j ? dim(Ai) and
(iii) f(xy) = f(x)f(y) for all x, y ? (T ?
V )+.
c) Nothing else is in yield(A).
The language is then {w | ?w? ? yield(S)}.
We are using the algorithm from Maier and
S?gaard (2008) to extract simple RCGs from Ne-
Gra and TIGER. For the tree in Fig. 1, the algo-
rithm produces for instance the following clauses:
PRELS(der) ? ?
ADV(genausogut) ? ?
. . .
S(X1X2X3X4) ? PRELS(X1)VP2(X1,X4) VAFIN(X3)
VP2(X1X2X3,X4X5) ? ADV(X1) ADV(X2)
VP2(X3,X4) VMINF(X5)
VP2(X1,X2) ? NN(X1) VAINF(X2)
We distinguish different usages of the same cat-
egory depending on their numbers of yield com-
ponents. E.g., we distinguish non-terminals VP1,
VP2, . . . depending on the arity of the VP. We de-
fine cat(A) for A ? N as the category of A, inde-
pendent from the arity, e.g., cat(VP2) =VP.
In terms of simple RCG, synchronous rewrit-
ing means that in a single clause distinct variables
occurring in two different arguments of the LHS
predicate are passed to two different arguments of
the same RHS predicate. We call this recursive
71
S
NP
VP
VP
VP
PP NP
ob auf dem Gela?nde der Typ von Abstellanlage . . . ha?tte . . . gebaut werden sollen, der. . .
Figure 2: Iterable treebank example for synchronous rewriting
if, by a sequence of synchronous rewriting steps,
we can reach the same two arguments of the same
predicate again. Derivations using such cycles of
synchronous rewriting lead exactly to the recursive
synchronous rewriting trees characterized in sec-
tion 2. In the following, we check to which extent
the extracted simple RCG allows for such cycles.
In order to detect synchronous rewriting in a
simple k-RCG G, we build a labeled directed
graph G = (VG , EG , l) from the grammar with
VG a set of nodes, EG a set of arcs and l :
VG ? N ? ? {0, . . . , k} ? {0, . . . , k} where N ? =
{cat(A) |A ? N} a labeling function. G is con-
structed as follows. For each clause A0(~?) ?
A1( ~?1) . . . Am( ~?m) ? P we consider all pairs of
variables Xs,Xt for which the following condi-
tions hold: (i) Xs and Xt occur in different argu-
ments i and j of A0, 1 ? i < j ? dim(A0); and
(ii) Xs and Xt occur in different arguments q and
r of the same occurrence of predicate Ap in the
RHS, 1 ? q < r ? dim(Ap) and 1 ? p ? m.
For each of these pairs, two nodes with labels
[cat(A0), i, j] and [cat(Ap), q, r], respectively, are
added to VG (if they do not yet exist, otherwise we
take the already existing nodes) and a directed arc
from the first node to the second node is added to
EG . The intuition is that an arc in G represents
one or more clauses from the grammar in which
a gap between two variables in the LHS predicate
is transferred to the same RHS predicate. To de-
tect recursive synchronous rewriting, we then need
to discover all elementary cycles in G, i.e., all cy-
cles in which no vertex appears twice. In order to
accomplish this task efficiently, we exploit the al-
gorithm presented in Johnson (1975). On a gram-
mar extracted from NeGra (19,100 clauses), the
algorithm yields a graph with 28 nodes containing
206,403 cycles of an average length of 12.86 and
a maximal length of 28.
4 Conclusion
The starting point of this paper was the question
whether synchronous rewriting is a necessary fea-
ture of grammer formalisms for modelling natu-
ral languages. In order to answer this question,
we have characterized synchronous rewriting in
terms of properties of treebank trees with crossing
branches. Experiments have shown that recursive
cases of synchronous rewriting occur in treebanks
for German which leads to the conclusion that,
in order to model these data, we need formalisms
that allow for synchronous rewriting. In a second
part, we have extracted a simple RCG from these
treebanks and we have characterized the grammar
properties that are necessary to obtain recursive
synchronous rewriting. We then have investigated
the extent to which a grammar extracted from Ne-
Gra allows for recursive synchronous rewriting.
References
Pierre Boullier. 2000. Range concatenation grammars.
In Proceedings of IWPT.
Harry Bunt, Jan Thesingh, and Ko van der Sloot. 1987.
Discontinuous constituents in trees, rules and pars-
ing. In Proceedings of EACL.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics.
Donald B. Johnson. 1975. Finding all the elementary
circuits of a directed graph. SIAM Journal on Com-
puting.
Marco Kuhlmann. 2007. Dependency Structures and
Lexicalized Grammars. Dissertation, Saarland Uni-
versity.
Wolfgang Maier and Anders S?gaard. 2008. Tree-
banks and mild context-sensitivity. In Proceedings
of Formal Grammar.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
Oliver Plaehn. 2004. Computing the most probable
parse for a discontinuous phrase-structure grammar.
In New developments in parsing technology. Kluwer.
H. Seki, T. Matsumura, M. Fujii, and T. Kasami. 1991.
On multiple context-free grammars. Theoretical
Computer Science.
K. Vijay-Shanker, David Weir, and Aravind Joshi.
1987. Characterising structural descriptions used by
various formalisms. In Proceedings of ACL.
72
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 537?545,
Beijing, August 2010
Data-Driven Parsing with Probabilistic Linear Context-Free Rewriting
Systems
Laura Kallmeyer and Wolfgang Maier
SFB 833, University of Tu?bingen
{lk,wmaier}@sfs.uni-tuebingen.de
Abstract
This paper presents a first efficient imple-
mentation of a weighted deductive CYK
parser for Probabilistic Linear Context-
Free Rewriting Systems (PLCFRS), to-
gether with context-summary estimates
for parse items used to speed up pars-
ing. LCFRS, an extension of CFG, can de-
scribe discontinuities both in constituency
and dependency structures in a straight-
forward way and is therefore a natural
candidate to be used for data-driven pars-
ing. We evaluate our parser with a gram-
mar extracted from the German NeGra
treebank. Our experiments show that data-
driven LCFRS parsing is feasible with
a reasonable speed and yields output of
competitive quality.
1 Introduction
Data-driven parsing has largely been dominated
by Probabilistic Context-Free Grammar (PCFG).
The use of PCFG is tied to the annotation princi-
ples of popular treebanks, such as the Penn Tree-
bank (PTB) (Marcus et al, 1994), which are used
as a data source for grammar extraction. Their an-
notation generally relies on the use of trees with-
out crossing branches, augmented with a mech-
anism that accounts for non-local dependencies.
In the PTB, e.g., labeling conventions and trace
nodes are used which establish additional implicit
edges in the tree beyond the overt phrase struc-
ture. In contrast, some other treebanks, such as the
German NeGra and TIGER treebanks allow anno-
tation with crossing branches (Skut et al, 1997).
Non-local dependencies can then be expressed di-
rectly by grouping all dependent elements under a
single node.
However, given the expressivity restrictions of
PCFG, work on data-driven parsing has mostly
excluded non-local dependencies. When us-
ing treebanks with PTB-like annotation, label-
ing conventions and trace nodes are often dis-
carded, while in NeGra, resp. TIGER, tree trans-
formations are applied which resolve the crossing
branches (Ku?bler, 2005; Boyd, 2007, e.g.). Espe-
cially for these treebanks, such a transformation is
questionable, since it is non-reversible and implies
information loss.
Some research has gone into incorporating non-
local information into data-driven parsing. Levy
and Manning (2004) distinguish three approaches:
1. Non-local information can be incorporated di-
rectly into the PCFG model (Collins, 1999), or
can be reconstructed in a post-processing step af-
ter PCFG parsing (Johnson, 2002; Levy and Man-
ning, 2004). 2. Non-local information can be
incorporated into complex labels (Hockenmaier,
2003). 3. A formalism can be used which accom-
modates the direct encoding of non-local informa-
tion (Plaehn, 2004). This paper pursues the third
approach.
Our work is motivated by the following re-
cent developments: Linear Context-Free Rewrit-
ing Systems (LCFRS) (Vijay-Shanker et al, 1987)
have been established as a candidate for mod-
eling both discontinuous constituents and non-
projective dependency trees as they occur in tree-
banks (Kuhlmann and Satta, 2009; Maier and
Lichte, 2009). LCFRS extend CFG such that
non-terminals can span tuples of possibly non-
537
CFG:
A
?
LCFRS: ?
A
? ?
?1 ?2 ?3
Figure 1: Different domains of locality
adjacent strings (see Fig. 1). PCFG techniques,
such as Best-First Parsing (Charniak and Cara-
ballo, 1998), Weighted Deductive Parsing (Neder-
hof, 2003) and A? parsing (Klein and Manning,
2003a), can be transferred to LCFRS. Finally,
German has attracted the interest of the parsing
community due to the challenges arising from its
frequent discontinuous constituents (Ku?bler and
Penn, 2008).
We bring together these developments by pre-
senting a parser for probabilistic LCFRS. While
parsers for subclasses of PLCFRS have been pre-
sented before (Kato et al, 2006), to our knowl-
edge, our parser is the first for the entire class of
PLCFRS. We have already presented an applica-
tion of the parser on constituency and dependency
treebanks together with an extensive evaluation
(Maier, 2010; Maier and Kallmeyer, 2010). This
article is mainly dedicated to the presentation of
several methods for context summary estimation
of parse items, and to an experimental evaluation
of their usefulness. The estimates either act as
figures-of-merit in a best-first parsing context or
as estimates for A? parsing. Our evaluation shows
that while our parser achieves a reasonable speed
already without estimates, the estimates lead to a
great reduction of the number of produced items,
all while preserving the output quality.
Sect. 2 and 3 of the paper introduce probabilis-
tic LCFRS and the parsing algorithm. Sect. 4
presents different context summary estimates. In
Sect. 5, the implementation and evaluation of the
work is discussed.
2 Probabilistic LCFRS
LCFRS are an extension of CFG where the non-
terminals can span not only single strings but, in-
stead, tuples of strings. We will notate LCFRS
with the syntax of simple Range Concatenation
Grammars (SRCG) (Boullier, 1998), a formalism
that is equivalent to LCFRS.
A LCFRS (Vijay-Shanker et al, 1987) is a tu-
ple ?N,T, V, P, S? where a) N is a finite set of
non-terminals with a function dim: N ? N that
determines the fan-out of each A ? N ; b) T and V
are disjoint finite sets of terminals and variables;
c) S ? N is the start symbol with dim(S) = 1; d)
P is a finite set of rules
A(?1, . . . , ?dim(A)) ? A1(X(1)1 , . . . ,X(1)dim(A1))
? ? ?Am(X(m)1 , . . . ,X(m)dim(Am))
for m ? 0 where A,A1, . . . , Am ? N , X(i)j ?
V for 1 ? i ? m, 1 ? j ? dim(Ai) and
?i ? (T ? V )? for 1 ? i ? dim(A). For all
r ? P , it holds that every variable X occurring in
r occurs exactly once in the left-hand side (LHS)
and exactly once in the right-hand side (RHS).
A rewriting rule describes how the yield of
the LHS non-terminal can be computed from
the yields of the RHS non-terminals. The rules
A(ab, cd) ? ? and A(aXb, cY d) ? A(X,Y )
for instance specify that 1. ?ab, cd? is in the yield
of A and 2. one can compute a new tuple in the
yield of A from an already existing one by wrap-
ping a and b around the first component and c and
d around the second.
For every A ? N in a LCFRS G, we define the
yield of A, yield(A) as follows:
a) For every A(~?) ? ?, ~? ? yield(A);
b) For every rule
A(?1, . . . , ?dim(A)) ? A1(X(1)1 , . . . ,X(1)dim(A1))
? ? ?Am(X(m)1 , . . . ,X(m)dim(Am))
and all ~?i ? yield(Ai) for 1 ? i ? m,
?f(?1), . . . , f(?dim(A))? ? yield(A) where f
is defined as follows: (i) f(t) = t for all t ? T ,
(ii) f(X(i)j ) = ~?i(j) for all 1 ? i ? m, 1 ?
j ? dim(Ai) and (iii) f(xy) = f(x)f(y) for
all x, y ? (T ?V )+. f is the composition func-
tion of the rule.
c) Nothing else is in yield(A).
The language is then {w | ?w? ? yield(S)}.
The fan-out of an LCFRS G is the maximal fan-
out of all non-terminals in G. Furthermore, the
RHS length of a rewriting rules r ? P is called the
rank of r and the maximal rank of all rules in P
is called the rank of G. We call a LCFRS ordered
if for every r ? P and every RHS non-terminal A
in r and each pair X1, X2 of arguments of A in
538
the RHS of r, X1 precedes X2 in the RHS iff X1
precedes X2 in the LHS.
A probabilistic LCFRS (PLCFRS) (Kato et
al., 2006) is a tuple ?N,T, V, P, S, p? such that
?N,T, V, P, S? is a LCFRS and p : P ?
[0..1] a function such that for all A ? N :
?A(~x)?~??Pp(A(~x) ? ~?) = 1.
3 The CYK Parser
We use a probabilistic version of the CYK parser
from (Seki et al, 1991), applying techniques of
weighted deductive parsing (Nederhof, 2003).
LCFRS can be binarized (Go?mez-Rodr??guez et
al., 2009) and ?-components in the LHS of rules
can be removed (Boullier, 1998). We can there-
fore assume that all rules are of rank 2 and do not
contain ? components in their LHS. Furthermore,
we assume POS tagging to be done before pars-
ing. POS tags are non-terminals of fan-out 1. The
rules are then either of the form A(a) ? ? with A
a POS tag and a ? T or of the form A(~?) ? B(~x)
or A(~?) ? B(~x)C(~y) where ~? ? (V +)dim(A),
i.e., only the rules for POS tags contain terminals
in their LHSs.
For every w ? T ?, where w = w1 . . . wn with
wi ? T for 1 ? i ? n, we define: Pos(w) :=
{0, . . . , n}. A pair ?l, r? ? Pos(w) ? Pos(w)
with l ? r is a range in w. Its yield ?l, r?(w) is
the string wl+1 . . . wr. The yield ~?(w) of a vec-
tor of ranges ~? is the vector of the yields of the
single ranges. For two ranges ?1 = ?l1, r1?, ?2 =
?l2, r2?: if r1 = l2, then ?1 ? ?2 = ?l1, r2?; other-
wise ?1 ? ?2 is undefined.
For a given rule p : A(?1, . . . , ?dim(A)) ?
B(X1, . . . ,Xdim(B))C(Y1, . . . ,Xdim(C)) we
now extend the composition function f to ranges,
given an input w: for all range vectors ~?B and
~?C of dimensions dim(B) and dim(C) respec-
tively, fr( ~?B , ~?C) = ?g(?1), . . . , g(?dim(A))?
is defined as follows: g(Xi) = ~?B(i) for all
1 ? i ? dim(B), g(Yi) = ~?C(i) for all
1 ? i ? dim(C) and g(xy) = g(x) ? g(y) for all
x, y ? V +. p : A(fr( ~?B , ~?C)) ? B( ~?B)C( ~?C)
is then called an instantiated rule.
For a given input w, our items have the
form [A, ~?] where A ? N , ~? ? (Pos(w) ?
Pos(w))dim(A). The vector ~? characterizes the
span of A. We specify the set of weighted parse
Scan: 0 : [A, ??i, i + 1??] A POS tag of wi+1
Unary: in : [B, ~?]in + |log(p)| : [A, ~?] p : A(~?) ? B(~?) ? P
Binary: inB : [B, ~?B], inC : [C, ~?C ]inB + inC + log(p) : [A, ~?A]
where p : A( ~?A) ? B( ~?B)C( ~?C) is an instantiated rule.
Goal: [S, ??0, n??]
Figure 2: Weighted CYK deduction system
add SCAN results to A
while A 6= ?
remove best item x : I from A
add x : I to C
if I goal item
then stop and output true
else
for all y : I ? deduced from x : I and items in C:
if there is no z with z : I ? ? C ? A
then add y : I ? to A
else if z : I ? ? A for some z
then update weight of I ? in A to max (y, z)
Figure 3: Weighted deductive parsing
items via the deduction rules in Fig. 2. Our parser
performs a weighted deductive parsing (Nederhof,
2003), based on this deduction system. We use a
chart C and an agenda A, both initially empty, and
we proceed as in Fig. 3.
4 Outside Estimates
In order to speed up parsing, we add an estimate of
the log of the outside probabilities of the items to
their weights in the agenda. All our outside esti-
mates are admissible (Klein and Manning, 2003a)
which means that they never underestimate the ac-
tual outside probability of an item. However, most
of them are not monotonic. In other words, it can
happen that we deduce an item I2 from an item I1
where the weight of I2 is greater than the weight
of I1. The parser can therefore end up in a local
maximum that is not the global maximum we are
searching for. In other words, our outside weights
are only figures of merit (FOM). Only for the full
SX estimate, the monotonicity is guaranteed and
we can do true A? parsing as described in (Klein
and Manning, 2003a) that always finds the best
parse.
All outside estimates are computed for a certain
maximal sentence length lenmax.
539
POS tags: 0 : [A, ?1?] A a POS tag
Unary: in : [B,
~l]
in + log(p) : [A,~l] p : A(~?) ? B(~?) ? P
Binary: inB : [B,
~lB], inC : [C,~lC ]
inB + inC + log(p) : [A,~lA]
where p : A( ~?A) ? B( ~?B)C( ~?C) ? P and the follow-
ing holds: we define B(i) as {1 ? j ? dim(B) | ~?B(j)
occurs in ~?A(i)} and C(i) as {1 ? j ? dim(C) | ~?C(j)
occurs in ~?A(i)}. Then for all i, 1 ? i ? dim(A):
~lA(i) = ?j?B(i)~lB(j) + ?j?C(i)~lC(j).
Figure 4: Inside estimate
4.1 Full SX estimate
The full SX estimate, for a given sentence length
n, is supposed to give the minimal costs (maxi-
mal probability) of completing a category X with
a span ? into an S with span ??0, n??.
For the computation, we need an estimate of
the inside probability of a category C with a span
?, regardless of the actual terminals in our in-
put. This inside estimate is computed as shown
in Fig. 4. Here, we do not need to consider the
number of terminals outside the span of C (to
the left or right or in the gaps), they are not rel-
evant for the inside probability. Therefore the
items have the form [A, ?l1, . . . , ldim(A)?], where
A is a non-terminal and li gives the length of its
ith component. It holds that ?1?i?dim(A)li ?
lenmax ? dim(A) + 1.
A straight-forward extension of the CFG algo-
rithm from (Klein and Manning, 2003a) for com-
puting the SX estimate is given in Fig. 5. For a
given range vector ? = ??l1, r1?, . . . , ?lk, rk?? and
a sentence length n, we define its inside length
vector lin(?) as ?r1 ? l1, . . . , rk ? lk? and its
outside length vector lout(?) as ?l1, r1 ? l1, l2 ?
r1, . . . , lk ? rk?1, rk ? lk, n? rk?.
This algorithm has two major problems: Since
it proceeds top-down, in the Binary rules, we must
compute all splits of the antecedent X span into
the spans of A and B which is very expensive.
Furthermore, for a category A with a certain num-
ber of terminals in the components and the gaps,
we compute the lower part of the outside estimate
several times, namely for every combination of
number of terminals to the left and to the right
(first and last element in the outside length vec-
Axiom : 0 : [S, ?0, len, 0?] 1 ? len ? lenmax
Unary: w : [A,
~l]
w + log(p) : [B,~l] p : A(~?) ? B(~?) ? P
Binary-right:
w : [X,~lX ]
w + in(A,~l?A) + log(p) : [B,~lB]
Binary-left:
w : [X,~lX ]
w + in(B,~l?B) + log(p) : [A,~lA]
where, for both rules, there is an instantiated rule p :
X(~?) ? A( ~?A)B( ~?B) such that ~lX = lout(?), ~lA =
lout(?A),~l?A = lin(?A), ~lB = lout(?B,~lB = lin(?B .
Figure 5: Full SX estimate top-down
tor). In order to avoid these problems, we now
abstract away from the lengths of the part to the
left and the right, modifying our items such as to
allow a bottom-up strategy.
The idea is to compute the weights of items rep-
resenting the derivations from a certain lower C
up to some A (C is a kind of ?gap? in the yield of
A) while summing up the inside costs of off-spine
nodes and the log of the probabilities of the corre-
sponding rules. We use items [A,C, ?A, ?C , shift ]
where A,C ? N and ?A, ?C are range vectors,
both with a first component starting at position 0.
The integer shift ? lenmax tells us how many po-
sitions to the right the C span is shifted, compared
to the starting position of the A. ?A and ?C repre-
sent the spans of C and A while disregarding the
number of terminals to the left the right. I.e., only
the lengths of the components and of the gaps are
encoded. This means in particular that the length
n of the sentence does not play a role here. The
right boundary of the last range in the vectors is
limited to lenmax. For any i, 0 ? i ? lenmax,
and any range vector ?, we define shift(?, i) as the
range vector one obtains from adding i to all range
boundaries in ? and shift(?,?i) as the range vec-
tor one obtains from subtracting i from all bound-
aries in ?.
The weight of [A,C, ?A, ?C , i] estimates the
costs for completing a C tree with yield ?C into
an A tree with yield ?A such that, if the span of A
starts at position j, the span of C starts at position
i + j. Fig. 6 gives the computation. The value of
in(A,~l) is the inside estimate of [A,~l].
The SX-estimate for some predicate C with
540
POS tags: 0 : [C,C, ?0, 1?, ?0, 1?, 0] C a POS tag
Unary: 0 : [B,B, ?B, ?B, 0]log(p) : [A,B, ?B, ?B, 0] p : A(~?) ? B(~?) ? P
Binary-right:
0 : [A,A, ?A, ?A, 0], 0 : [B,B, ?B, ?B , 0]
in(A, l(?A)) + log(p) : [X,B, ?X , ?B, i]
Binary-left:
0 : [A,A, ?A, ?A, 0], 0 : [B,B, ?B, ?B, 0]
in(B, l(?B)) + log(p) : [X,A, ?X , ?A, 0]
where i is such that for shift(?B, i) = ??B p : X(?X) ?
A(?A)B(??B) is an instantiated rule.
Starting sub-trees with larger gaps:
w : [B,C, ?B, ?C , i]
0 : [B,B, ?B, ?B, 0]
Transitive closure of sub-tree combination:
w1 : [A,B, ?A, ?B, i], w2 : [B,C, ?B, ?C , j]
w1 + w2 : [A,C, ?A, ?C , i + j]
Figure 6: Full SX estimate bottom-up
span ? where i is the left boundary of the
first component of ? and with sentence length
n is then given by the maximal weight of
[S,C, ?0, n?, shift (?i, ?), i]. Among our esti-
mates, the full SX estimate is the only one that
is monotonic and that allows for true A? parsing.
4.2 SX with Left, Gaps, Right, Length
A problem of the previous estimate is that with
a large number of non-terminals the computation
of the estimate requires too much space. Our ex-
periments have shown that for treebank parsing
where we have, after binarization and markoviza-
tion, appr. 12,000 non-terminals, its computation
is not feasible. We therefore turn to simpler es-
timates with only a single non-terminal per item.
We now estimate the outside probability of a non-
terminal A with a span of a length length (the
sum of the lengths of all the components of the
span), with left terminals to the left of the first
component, right terminals to the right of the
last component and gaps terminals in between the
components of the A span, i.e., filling the gaps.
Our items have the form [X, len , left , right , gaps ]
with X ? N , len+ left +right +gaps ? lenmax,
len ? dim(X), gaps ? dim(X) ? 1.
Let us assume that, in the rule X(~?) ?
A( ~?A)B( ~?B), when looking at the vector ~?, we
have leftA variables for A-components preceding
the first variable of a B component, rightA vari-
ables for A-components following the last vari-
Axiom : 0 : [S, len, 0, 0, 0] 1 ? len ? lenmax
Unary: w : [X, len, l, r, g]w + log(p) : [A, len, l, r, g]
where p : X(~?) ? A(~?) ? P .
Binary-right:
w : [X, len, l, r, g]
w + in(A, len ? lenB) + log(p) : [B, lenB , lB, rB, gB]
Binary-left:
w : [X, len, l, r, g]
w + in(B, len ? lenA) + log(p) : [A, lenA, lA, rA, gA]
where, for both rules, p : X(~?) ? A( ~?A)B( ~?B) ? P .
Figure 7: SX with length, left, right, gaps
POS tags: 0 : [A, 1] A a POS tag
Unary: in : [B, l]in + log(p) : [A, l] p : A(~?) ? B(~?) ? P
Binary: inB : [B, lB], inC : [C, lC ]inB + inC + log(p) : [A, lB + lC ]
where either p : A( ~?A) ? B( ~?B)C( ~?C) ? P or p :
A( ~?A) ? C( ~?C)B( ~?B) ? P .
Figure 8: Inside estimate with total span length
able of a B component and rightB variables for
B-components following the last variable of a A
component. (In our grammars, the first LHS argu-
ment always starts with the first variable from A.)
Furthermore, gapsA = dim(A)?leftA?rightA,
gapsB = dim(B) ? rightB .
Fig. 7 gives the computation of the estimate.
The following side conditions must hold: For
Binary-right to apply, the following constraints
must be satisfied: a) len + l + r + g = lenB +
lB+rB+gB , b) lB ? l+ leftA, c) if rightA > 0,
then rB ? r+rightA, else (rightA = 0), rB = r,
d) gB ? gapsA. Similarly, for Binary-left to ap-
ply, the following constraints must be satisfied: a)
len + l+ r+ g = lenA + lA + rA + gA, b) lA = l,
c) if rightB > 0, then rA ? r + rightB , else
(rightB = 0), rA = r d) gA ? gapsB.
The value in(X, l) for a non-terminal X and a
length l, 0 ? l ? lenmax is an estimate of the
probability of an X category with a span of length
l. Its computation is specified in Fig. 8.
The SX-estimate for a sentence length n and
for some predicate C with a range characterized
by ~? = ??l1, r1?, . . . , ?ldim(C), rdim(C)?? where
len = ?dim(C)i=1 (ri ? li) and r = n ? rdim(C)
is then given by the maximal weight of the item
[C, len , l1, r, n ? len? l1 ? r].
541
Axiom : 0 : [S, len, 0, 0] 1 ? len ? lenmax
Unary: w : [X, len, lr , g]w + log(p) : [A, len, lr , g]
where p : X(~?) ? A(~?) ? P .
Binary-right:
w : [X, len, lr , g]
w + in(A, len ? lenB) + log(p) : [B, lenB, lrB, gB ]
Binary-left:
w : [X, len, lr , g]
w + in(B, len ? lenA) + log(p) : [A, lenA, lrA, gA]
where, for both rules, p : X(~?) ? A( ~?A)B( ~?B) ? P .
Figure 9: SX estimate with length, LR, gaps
4.3 SX with LR, Gaps, Length
In order to further decrease the space complex-
ity, we can simplify the previous estimate by sub-
suming the two lengths left and right in a sin-
gle length lr . I.e., the items now have the form
[X, len , lr , gaps ] with X ? N , len + lr +gaps ?
lenmax, len ? dim(X), gaps ? dim(X) ? 1.
The computation is given in Fig. 9. Again, we
define leftA, gapsA, rightA and gapsB , rightB
for a rule X(~?) ? A( ~?A)B( ~?B) as above. The
side conditions are as follows: For Binary-right to
apply, the following constraints must be satisfied:
a) len + lr + g = lenB + lrB + gB , b) lr < lrB,
and c) gB ? gapsA. For Binary-left to apply, the
following must hold: a) len + lr + g = lenA +
lrA + gA, b) if rightB = 0 then lr = lrA, else
lr < lrA and c) gA ? gapsB.
The SX-estimate for a sentence length n
and for some predicate C with a span ~? =
??l1, r1?, . . . , ?ldim(C), rdim(C)?? where len =
?dim(C)i=1 (ri ? li) and r = n ? rdim(C) is then the
maximal weight of [C, len , l1+r, n?len?l1?r].
5 Evaluation
The goal of our evaluation of our parser is to
show that, firstly, reasonable parser speed can be
achieved and, secondly, the parser output is of
promising quality.
5.1 Data
Our data source is the German NeGra treebank
(Skut et al, 1997). In a preprocessing step,
following common practice (Ku?bler and Penn,
2008), we attach punctuation (not included in the
NeGra annotation) as follows: In a first pass, us-
ing heuristics, we attach punctuation as high as
possible while avoiding to introduce new crossing
branches. In a second pass, parentheses and quo-
tation marks preferably attach to the same node.
Grammatical function labels on the edges are dis-
carded.
We create data sets of different sizes in order
to see how the size of the training set relates to
the gain using context summary estimates and to
the output quality of the parser. The first set uses
the first 4000 sentences and the second one all
sentences of NeGra. Due to memory limitations,
in both sets, we limit ourselves to sentences of a
maximal length of 25 words. We use the first 90%
of both sets as training set and the remaining 10%
as test set. Tab. 1 shows the resulting sizes.
NeGra-small NeGra
training test training test
size 2839 316 14858 1651
Table 1: Test and training sets
5.2 Treebank Grammar Extraction
S
VP
VP
PROAV VMFIN VVPP VAINF
daru?ber mu? nachgedacht werden
about it must thought be
?It must be thought about it?
Figure 10: A sample tree from NeGra
As already mentioned, in NeGra, discontinu-
ous phrases are annotated with crossing branches
(see Fig. 10 for an example with two discontin-
uous VPs). Such discontinuities can be straight-
forwardly modelled with LCFRS. We use the al-
gorithm from Maier and S?gaard (2008) to extract
LCFRS rules from NeGra and TIGER. It first cre-
ates rules of the form P (a) ? ? for each pre-
terminal P dominating some terminal a. Then
for all other nonterminals A0 with the children
A1 ? ? ?Am, a clause A0 ? A1 ? ? ?Am is cre-
ated. The arguments of the A1 ? ? ?Am are sin-
gle variables where the number of arguments is
the number of discontinuous parts in the yield of
a predicate. The arguments of A0 are concate-
nations of these variables that describe how the
542
discontinuous parts of the yield of A0 are ob-
tained from the yields of its daughters. Differ-
ent occurrences of the same non-terminal, only
with different fan-outs, are distinguished by corre-
sponding subscripts. Note that this extraction al-
gorithm yields only monotone LCFRS (equivalent
to ordered simple RCG). See Maier and S?gaard
(2008) for further details. For Fig. 10, we obtain
for instance the rules in Fig. 11.
PROAV(Daru?ber) ? ? VMFIN(mu?) ? ?
VVPP(nachgedacht) ? ? VAINF(werden) ? ?
S1(X1X2X3) ? VP2(X1, X3) VMFIN(X2)
VP2(X1, X2X3) ? VP2(X1, X2) VAINF(X3)
VP2(X1, X2) ? PROAV(X1) VVPP(X2)
Figure 11: LCFRS rules for the tree in Fig. 10
5.3 Binarization and Markovization
Before parsing, we binarize the extracted LCFRS.
For this we first apply Collins-style head rules,
based on the rules the Stanford parser (Klein and
Manning, 2003b) uses for NeGra, to mark the
resp. head daughters of all non-terminal nodes.
Then, we reorder the RHSs such that the sequence
? of elements to the right of the head daughter is
reversed and moved to the beginning of the RHS.
We then perform a binarization that proceeds from
left to right. The binarization works like the trans-
formation into Chomsky Normal Form for CFGs
in the sense that for RHSs longer than 2, we in-
troduce a new non-terminal that covers the RHS
without the first element. The rightmost new rule,
which covers the head daughter, is binarized to
unary. We do not use a unique new non-terminal
for every new rule. Instead, to the new symbols
introduced during the binarization (VPbin in the
example), a variable number of symbols from the
vertical and horizontal context of the original rule
is added in order to achieve markovization. Fol-
lowing the literature, we call the respective quan-
tities v and h. For reasons of space we restrict
ourselves here to the example in Fig. 12. Refer to
Maier and Kallmeyer (2010) for a detailed presen-
tation of the binarization and markovization.
The probabilities are then computed based on
the rule frequencies in the transformed treebank,
using a Maximum Likelihood estimator.
S
VP
PDS VMFIN PIS AD V VVINF
das mu? man jetzt machen
that must one now do
?One has to do that now?
Tree after binarization:
S
Sbin
VP
VPbin
Sbin VPbin
PDS VMFIN PIS ADV VVINF
Figure 12: Sample binarization
5.4 Evaluation of Parsing Results
In order to assess the quality of the output of
our parser, we choose an EVALB-style metric,
i.e., we compare phrase boundaries. In the con-
text of LCFRS, we compare sets of items [A, ~?]
that characterize the span of a non-terminal A in
a derivation tree. One set is obtained from the
parser output, and one from the corresponding
treebank trees. Using these item sets, we compute
labeled and unlabeled recall (LR/UR), precision
(LP/UP), and the F1 measure (LF1/UF1). Note
that if k = 1, our metric is identical to its PCFG
equivalent.We are aware of the recent discussion
about the shortcomings of EVALB. A discussion
of this issue is presented in Maier (2010).
5.5 Experiments
In all experiments, we provide the parser with
gold part-of-speech tags. For the experi-
ments with NeGra-small, the parser is given the
markovization settings v = 1 and h = 1. We com-
pare the parser performance without estimates
(OFF) with its performance with the estimates de-
scribed in 4.2 (SIMPLE) and 4.3 (LR). Tab. 2
shows the results. Fig. 13 shows the number of
items produced by the parser, indicating that the
estimates have the desired effect of preventing un-
necessary items from being produced. Note that it
is even the case that the parser produces less items
for the big set with LR than for the small set with-
out estimate.
We can see that the estimates lead to a slightly
543
OFF SIMPLE LR
UP/UR 72.29/72.40 70.49/71.81 72.10/72.60
UF1 72.35 71.14 72.35
LP/LR 68.31/68.41 64.93/66.14 67.35/66.14
LF1 68.36 65.53 65.53
Parsed 313 (99.05%) 313 (99.05%) 313 (99.05%)
Table 2: Experiments with NeGra-small
 50
 100
 150
 200
 250
 300
 350
 400
 450
 500
 16  18  20  22  24
N
o.
 o
f i
te
m
s 
(in
 10
00
)
Sentence length
OFF (NeGra)
LR (NeGra)
OFF (NeGra-small)
SIMPLE (NeGra-small)
LR (NeGra-small)
Figure 13: Items produced by the parser
lower F-score. However, while the losses in terms
of F1 are small, the gains in parsing time are sub-
stantial, as Fig. 13 shows.
Tab. 3 shows the results of experiments with
NeGra, with the markovization settings v = 2
and h = 1 which have proven to be successful
for PCFG parsing of NeGra (Rafferty and Man-
ning, 2008). Unfortunately, due to memory re-
strictions, we were not able to compute SIMPLE
for the large data set.1 Resp. LR, the findings
are comparable to the ones for NeGra-short. The
speedup is paid with a lower F1.
OFF LR
UP/UR 76.89/77.35 75.22/75.99
UF1 77.12 75.60
LP/LR 73.03/73.46 70.98/71.70
LF1 73.25 71.33
Parsed 1642 (99.45%) 1642 (99.45%)
Table 3: Experiments with NeGra
Our results are not directly comparable with
PCFG parsing results, since LCFRS parsing is a
1SIMPLE also proved to be infeasible to compute for the
small set for the markovization settings v = 2 and h = 1
due to the greatly increased label set with this settings.
harder task. However, since the EVALB met-
ric coincides for constituents without crossing
branches, in order to place our results in the con-
text of previous work on parsing NeGra, we cite
some of the results from the literature which were
obtained using PCFG parsers2: Ku?bler (2005)
(Tab. 1, plain PCFG) obtains 69.4, Dubey and
Keller (2003) (Tab. 5, sister-head PCFG model)
71.12, Rafferty and Manning (2008) (Tab. 2, Stan-
ford parser with markovization v = 2 and h = 1)
77.2, and Petrov and Klein (2007) (Tab. 1, Berke-
ley parser) 80.1. Plaehn (2004) obtains 73.16 La-
beled F1 using Probabilistic Discontinuous Phrase
Structure Grammar (DPSG), albeit only on sen-
tences with a length of up to 15 words. On those
sentences, we obtain 81.27.
The comparison shows that our system deliv-
ers competitive results. Additionally, when com-
paring this to PCFG parsing results, one has
to keep in mind that LCFRS parse trees con-
tain non-context-free information about disconti-
nuities. Therefore, a correct parse with our gram-
mar is actually better than a correct CFG parse,
evaluated with respect to a transformation of Ne-
Gra into a context-free treebank where precisely
this information gets lost.
6 Conclusion
We have presented the first parser for unrestricted
Probabilistic Linear Context-Free Rewriting Sys-
tems (PLCFRS), implemented as a CYK parser
with weighted deductive parsing. To speed up
parsing, we use context summary estimates for
parse items. An evaluation on the NeGra treebank,
both in terms of output quality and speed, shows
that data-driven parsing using PLCFRS is feasi-
ble. Already in this first attempt with a straight-
forward binarization, we obtain results that are
comparable to state-of-the-art PCFG results in
terms of F1, while yielding parse trees that are
richer than context-free trees since they describe
discontinuities. Therefore, our approach demon-
strates convincingly that PLCFRS is a natural and
tractable alternative for data-driven parsing which
takes non-local dependencies into consideration.
2Note that these results were obtained on sentences with
a length of ? 40 words and that those parser possibly would
deliver better results if tested on our test set.
544
References
Boullier, Pierre. 1998. A Proposal for a Natural Lan-
guage Processing Syntactic Backbone. Technical
Report 3342, INRIA.
Boyd, Adriane. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations.
In The Linguistic Annotation Workshop at ACL
2007.
Charniak, Eugene and Sharon A. Caraballo. 1998.
New figures of merit for best-first probabilistic chart
parsing. Computational Linguistics, 24.
Collins, Michael. 1999. Head-driven statistical mod-
els for natural language parsing. Ph.D. thesis, Uni-
versity of Pennsylvania.
Dubey, Amit and Frank Keller. 2003. Probabilistic
parsing for German using sisterhead dependencies.
In Proceedings of ACL.
Go?mez-Rodr??guez, Carlos, Marco Kuhlmann, Giorgio
Satta, and David Weir. 2009. Optimal reduction of
rule length in linear context-free rewriting systems.
In Proceedings of NAACL-HLT.
Hockenmaier, Julia. 2003. Data and models for Statis-
tical Parsing with Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
Johnson, Mark. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of ACL.
Kato, Yuki, Hiroyuki Seki, and Tadao Kasami. 2006.
Stochastic multiple context-free grammar for rna
pseudoknot modeling. In Proceedings of TAG+8.
Klein, Dan and Christopher D. Manning. 2003a. A*
Parsing: Fast Exact Viterbi Parse Selection. In Pro-
ceedings of NAACL-HLT.
Klein, Dan and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for natural
language parsing. In In Advances in Neural Infor-
mation Processing Systems 15 (NIPS).
Ku?bler, Sandra and Gerald Penn, editors. 2008. Pro-
ceedings of the Workshop on Parsing German at
ACL 2008.
Ku?bler, Sandra. 2005. How do treebank annotation
schemes influence parsing results? Or how not to
compare apples and oranges. In Proceedings of
RANLP 2005.
Kuhlmann, Marco and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Proceedings of EACL.
Levy, Roger and Christopher D. Manning. 2004. Deep
dependencies from context-free statistical parsers:
correcting the surface dependency approximation.
In Proceedings of ACL.
Maier, Wolfgang and Laura Kallmeyer. 2010. Discon-
tinuity and non-projectivity: Using mildly context-
sensitive formalisms for data-driven parsing. In
Proceedings of TAG+10.
Maier, Wolfgang and Timm Lichte. 2009. Charac-
terizing Discontinuity in Constituent Treebanks. In
Proceedings of Formal Grammar 2009.
Maier, Wolfgang and Anders S?gaard. 2008. Tree-
banks and mild context-sensitivity. In Proceedings
of Formal Grammar 2008.
Maier, Wolfgang. 2010. Direct parsing of discontin-
uous constituents in german. In Proceedings of the
SPMRL workshop at NAACL HLT 2010.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proceedings of
HLT.
Nederhof, Mark-Jan. 2003. Weighted Deductive Pars-
ing and Knuth?s Algorithm. Computational Lin-
guistics, 29(1).
Petrov, Slav and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL 2007.
Plaehn, Oliver. 2004. Computing the most proba-
ble parse for a discontinuous phrase-structure gram-
mar. In New developments in parsing technology.
Kluwer.
Rafferty, Anna and Christopher D. Manning, 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Ku?bler and Penn
(2008).
Seki, Hiroyuki, Takahashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On multiple context-free
grammars. Theoretical Computer Science, 88(2).
Skut, Wojciech, Brigitte Krenn, Thorten Brants, and
Hans Uszkoreit. 1997. An Annotation Scheme
for Free Word Order Languages. In Proceedings of
ANLP.
Vijay-Shanker, K., David J. Weir, and Aravind K.
Joshi. 1987. Characterizing structural descriptions
produced by various grammatical formalisms. In
Proceedings of ACL.
545
Data-Driven Parsing using Probabilistic
Linear Context-Free Rewriting Systems
Laura Kallmeyer?
Heinrich-Heine-Universita?t Du?sseldorf
Wolfgang Maier??
Heinrich-Heine-Universita?t Du?sseldorf
This paper presents the first efficient implementation of a weighted deductive CYK parser for
Probabilistic Linear Context-Free Rewriting Systems (PLCFRSs). LCFRS, an extension of CFG,
can describe discontinuities in a straightforward way and is therefore a natural candidate to be
used for data-driven parsing. To speed up parsing, we use different context-summary estimates
of parse items, some of them allowing for A? parsing. We evaluate our parser with grammars
extracted from the German NeGra treebank. Our experiments show that data-driven LCFRS
parsing is feasible and yields output of competitive quality.
1. Introduction
Recently, the challenges that a rich morphology poses for data-driven parsing have
received growing interest. A direct effect of morphological richness is, for instance, data
sparseness on a lexical level (Candito and Seddah 2010). A rather indirect effect is that
morphological richness often relaxes word order constraints. The principal intuition is
that a rich morphology encodes information that otherwise has to be conveyed by a
particular word order. If, for instance, the case of a nominal complement is not provided
by morphology, it has to be provided by the position of the complement relative to other
complements in the sentence. Example (1) provides an example of case marking and free
word order in German. In turn, in free word order languages, word order can encode
information structure (Hoffman 1995).
(1) a. der
the
kleine
little
Jungenom
boy
schickt
sends
seiner
his
Schwesterdat
sister
den
the
Briefacc
letter
b. Other possible word orders:
(i) der kleine Jungenom schickt den Briefacc seiner Schwesterdat
(ii) seiner Schwesterdat schickt der kleine Jungenom den Briefacc
(iii) den Briefacc schickt der kleine Jungenom seiner Schwesterdat
? Institut fu?r Sprache und Information, Universita?tsstr. 1, D-40225 Du?sseldorf, Germany.
E-mail: kallmeyer@phil.uni-duesseldorf.de.
?? Institut fu?r Sprache und Information, Universita?tsstr. 1, D-40225 Du?sseldorf, Germany.
E-mail: maierw@hhu.de.
Submission received: September 29, 2011; revised submission received: May 20, 2012; accepted for publication:
August 3, 2012.
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 1
It is assumed that this relation between a rich morphology and free word order does
not hold in both directions. Although it is generally the case that languages with a rich
morphology exhibit a high degree of freedom in word order, languages with a free word
order do not necessarily have a rich morphology. Two examples for languages with a
very free word order are Turkish and Bulgarian. The former has a very rich and the
latter a sparse morphology. See Mu?ller (2002) for a survey of the linguistics literature on
this discussion.
With a rather free word order, constituents and single parts of them can be displaced
freely within the sentence. German, for instance, has a rich inflectional system and
allows for a free word order, as we have already seen in Example (1): Arguments can
be scrambled, and topicalizations and extrapositions underlie few restrictions. Conse-
quently, discontinuous constituents occur frequently. This is challenging for syntactic
description in general (Uszkoreit 1986; Becker, Joshi, and Rambow 1991; Bunt 1996;
Mu?ller 2004), and for treebank annotation in particular (Skut et al 1997).
In this paper, we address the problem of data-driven parsing of discontinuous constit-
uents on the basis of German. In this section, we inspect the type of data we have to deal
with, and we describe the way such data are annotated in treebanks. We briefly discuss
different parsing strategies for the data in question and motivate our own approach.
1.1 Discontinuous Constituents
Consider the sentences in Example (2) as examples for discontinuous constituents
(taken from the German NeGra [Skut et al 1997] and TIGER [Brants et al 2002] tree-
banks). Example (2a) shows several instances of discontinuous VPs and Example (2b)
shows a discontinuous NP. The relevant constituent is printed in italics.
(2) a. Fronting:
(i) Daru?ber
Thereof
muss
must
nachgedacht
thought
werden.
be
(NeGra)
?One must think of that?
(ii) Ohne internationalen Schaden
Without international damage
ko?nne
could
sich
itself
Bonn
Bonn
von dem Denkmal
from the monument
nicht
not
distanzieren,
distance
... (TIGER)
?Bonn could not distance itself from the monument without international
damage.?
(iii) Auch
Also
wu?rden
would
durch die Regelung
through the regulation
nur
only
?sta?ndig
?constantly
neue
new
Altfa?lle
old cases
entstehen?. (TIGER)
emerge?
?Apart from that, the regulation would only constantly produce new old cases.?
b. Extraposed relative clauses:
(i) . . . ob
. . . whether
auf
on
deren
their
Gela?nde
terrain
der
the
Typ von Abstellanlage
type of parking facility
gebaut
built
werden
get
ko?nne,
could,
der ...
which . . .
(NeGra)
?. . . whether one could build on their premises the type of parking facility,
which . . . ?
88
Kallmeyer and Maier PLCFRS Parsing
Examples of other such languages are Bulgarian and Korean. Both show discontin-
uous constituents as well. Example (3a) is a Bulgarian example of a PP extracted out of
an NP, taken from the BulTreebank (Osenova and Simov 2004), and Example (3b) is an
example of fronting in Korean, taken from the Penn Korean Treebank (Han, Han, and
Ko 2001).
(3) a. Na kyshtata
Of house-DET
toi
he
popravi
repaired
pokriva.
roof.
?It is the roof of the house he repairs.?
b. Gwon.han-u?l
Authority-OBJ
nu.ga
who
ka.ji.go
has
iss.ji?
not?
?Who has no authority??
Discontinuous constituents are by no means limited to languages with freedom
in word order. They also occur in languages with a rather fixed word order such
as English, resulting from, for instance, long-distance movements. Examples (4a) and
(4b) are examples from the Penn Treebank for long extractions resulting in discontin-
uous S categories and for discontinuous NPs arising from extraposed relative clauses,
respectively (Marcus et al 1994).
(4) a. Long Extraction in English:
(i) Those chains include Bloomingdale?s, which Campeau recently said it
will sell.
(ii) What should I do.
b. Extraposed nominal modifiers (relative clauses and PPs) in English:
(i) They sow a row of male-fertile plants nearby, which then pollinate the male-
sterile plants.
(ii) Prices fell marginally for fuel and electricity.
1.2 Treebank Annotation and Data-Driven Parsing
Most constituency treebanks rely on an annotation backbone based on Context-Free
Grammar (CFG). Discontinuities cannot be modeled with CFG, because they require a
larger domain of locality than the one offered by CFG. Therefore, the annotation back-
bone based on CFG is generally augmented with a separate mechanism that accounts
for the non-local dependencies. In the Penn Treebank (PTB), for example, trace nodes
and co-indexation markers are used in order to establish additional implicit edges in the
tree beyond the overt phrase structure. In Tu?Ba-D/Z (Telljohann et al 2012), a German
Treebank, non-local dependencies are expressed via an annotation of topological fields
(Ho?hle 1986) and special edge labels. In contrast, some other treebanks, among them
NeGra and TIGER, give up the annotation backbone based on CFG and allow annota-
tion with crossing branches (Skut et al 1997). In such an annotation, non-local depen-
dencies can be expressed directly by grouping all dependent elements under a single
node. Note that both crossing branches and traces annotate long-distance dependencies
in a linguistically meaningful way. A difference is, however, that crossing branches
are less theory-dependent because they do not make any assumptions about the base
positions of ?moved? elements.
Examples for the different approaches of annotating discontinuities are given in
Figures 1 and 2. Figure 1 shows the NeGra annotation of Example (2a-i) (left), and an
89
Computational Linguistics Volume 39, Number 1
Figure 1
A discontinuous constituent. Original NeGra annotation (left) and a Tu?Ba-D/Z-style annotation
(right).
What
WP
should
MD
I
PRP
do
VB
*T*
-NONE-
?
.
WHNP NP NP
VP
SBJ
SQ
SBARQ
*T*
What
WP
should
MD
I
PRP
do
VB
?
.
WHNP NP
VP
SBJ
SQ
SBARQ
Figure 2
A discontinuous wh-movement. Original PTB annotation (left) and NeGra-style annotation
(right).
annotation of the same sentence in the style of the Tu?Ba-D/Z treebank (right). Figure 2
shows the PTB annotation of Example (4a-ii) (on the left, note that the directed edge
from the trace to the WHNP element visualizes the co-indexation) together with a
NeGra-style annotation of the same sentence (right).
In the past, data-driven parsing has largely been dominated by Probabilistic
Context-Free Grammar (PCFG). In order to extract a PCFG from a treebank, the trees
need to be interpretable as CFG derivations. Consequently, most work has excluded
non-local dependencies; either (in PTB-like treebanks) by discarding labeling conven-
tions such as the co-indexation of the trace nodes in the PTB, or (in NeGra/TIGER-like
treebanks) by applying tree transformations, which resolve the crossing branches (e.g.,
Ku?bler 2005; Boyd 2007). Especially for the latter treebanks, such a transformation is
problematic, because it generally is non-reversible and implies information loss.
Discontinuities are no minor phenomenon: Approximately 25% of all sentences
in NeGra and TIGER have crossing branches (Maier and Lichte 2011). In the Penn
Treebank, this holds for approximately 20% of all sentences (Evang and Kallmeyer
2011). This shows that it is important to properly treat such structures.
1.3 Extending the Domain of Locality
In the literature, different methods have been explored that allow for the use of non-
local information in data-driven parsing. We distinguish two classes of approaches.
The first class consists of approaches that aim at using formalisms which produce
trees without crossing branches but provide a larger domain of locality than CFG?
for instance, through complex labels (Hockenmaier 2003) or through the derivation
90
Kallmeyer and Maier PLCFRS Parsing
CFG:
A
?
LCFRS: ?
A
? ?
?1 ?2 ?3
Figure 3
Different domains of locality.
mechanism (Chiang 2003). The second class, to which we contribute in this paper,
consists of approaches that aim at producing trees which contain non-local information.
Some methods realize the reconstruction of non-local information in a post- or pre-
processing step to PCFG parsing (Johnson 2002; Dienes 2003; Levy and Manning 2004;
Cai, Chiang, and Goldberg 2011). Other work uses formalisms that accommodate the
direct encoding of non-local information (Plaehn 2004; Levy 2005). We pursue the latter
approach.
Our work is motivated by the following recent developments. Linear Context-Free
Rewriting Systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987) have been estab-
lished as a candidate for modeling both discontinuous constituents and non-projective
dependency trees as they occur in treebanks (Maier and S?gaard 2008; Kuhlmann and
Satta 2009; Maier and Lichte 2011). LCFRSs are a natural extension of CFGs where
the non-terminals can span tuples of possibly non-adjacent strings (see Figure 3). Be-
cause LCFRSs allow for binarization and CYK chart parsing in a way similar to CFGs,
PCFG techniques, such as best-first parsing (Caraballo and Charniak 1998), weighted
deductive parsing (Nederhof 2003), and A? parsing (Klein and Manning 2003a) can
be transferred to LCFRS. Finally, as mentioned before, languages such as German
have recently attracted the interest of the parsing community (Ku?bler and Penn 2008;
Seddah, Ku?bler, and Tsarfaty 2010).
We bring together these developments by presenting a parser for Probabilistic
LCFRS (PLCFRS), continuing the promising work of Levy (2005). Our parser pro-
duces trees with crossing branches and thereby accounts for syntactic long-distance
dependencies while not making any additional assumptions concerning the position
of hypothetical traces. We have implemented a CYK parser and we present several
methods for context summary estimation of parse items. The estimates either act as
figures-of-merit in a best-first parsing context or as estimates for A? parsing. A test on
a real-world-sized data set shows that our parser achieves competitive results. To our
knowledge, our parser is the first for the entire class of PLCFRS that has successfully
been used for data-driven parsing.1
The paper is structured as follows. Section 2 introduces probabilistic LCFRS. Sec-
tions 3 and 4 present the binarization algorithm, the parser, and the outside estimates
which we use to speed up parsing. In Section 5 we explain how to extract an LCFRS from
a treebank and we present grammar refinement methods for these specific treebank
grammars. Finally, Section 6 presents evaluation results and Section 7 compares our
work to other approaches.
1 Parts of the results presented in this paper have been presented earlier. More precisely, in Kallmeyer and
Maier (2010), we presented the general architecture of the parser and all outside estimates except the LN
estimate from Section 4.4 which is presented in Maier, Kaeshammer, and Kallmeyer (2012). In Maier and
Kallmeyer (2010) we have presented experiments with the relative clause split from Section 3.2. Finally,
Maier (2010) contains the evaluation of the baseline (together with an evaluation using other metrics).
91
Computational Linguistics Volume 39, Number 1
2. Probabilistic Linear Context-Free Rewriting Systems
2.1 Definition of PLCFRS
LCFRS (Vijay-Shanker, Weir, and Joshi 1987) is an extension of CFG in which a non-
terminal can span not only a single string but a tuple of strings of size k ? 1. k is thereby
called its fan-out. We will notate LCFRS with the syntax of Simple Range Concate-
nation Grammars (SRCG) (Boullier 1998b), a formalism that is equivalent to LCFRS.
A third formalism that is equivalent to LCFRS is Multiple Context-Free Grammar
(MCFG) (Seki et al 1991).
Definition 1 (LCFRS)
A Linear Context-Free Rewriting System (LCFRS) is a tuple ?N, T, V, P, S? where
a) N is a finite set of non-terminals with a function dim: N ? N that
determines the fan-out of each A ? N;
b) T and V are disjoint finite sets of terminals and variables;
c) S ? N is the start symbol with dim(S) = 1;
d) P is a finite set of rules
A(?1, . . . ,?dim(A) ) ? A1(X(1)1 , . . . , X
(1)
dim(A1)
) ? ? ?Am(X(m)1 , . . . , X
(m)
dim(Am )
)
for m ? 0 where A, A1, . . . , Am ? N, X(i)j ? V for 1 ? i ? m, 1 ? j ? dim(Ai)
and ?i ? (T ? V)? for 1 ? i ? dim(A). For all r ? P, it holds that every
variable X occurring in r occurs exactly once in the left-hand side and
exactly once in the right-hand side of r.
A rewriting rule describes how the yield of the left-hand side non-terminal can be
computed from the yields of the right-hand side non-terminals. The rules A(ab, cd) ? ?
and A(aXb, cYd) ? A(X, Y) from Figure 4 for instance specify that (1) ?ab, cd? is in the
yield of A and (2) one can compute a new tuple in the yield of A from an already existing
one by wrapping a and b around the first component and c and d around the second.
A CFG rule A ? BC would be written A(XY) ? B(X)C(Y) as an LCFRS rule.
Definition 2 (Yield, language)
Let G = ?N, T, V, P, S? be an LCFRS.
1. For every A ? N, we define the yield of A, yield(A) as follows:
a) For every rule A(?) ? ?, ? ? yield(A);
A(ab, cd) ? ?
A(aXb, cYd) ? A(X, Y)
S(XY) ? A(X, Y)
Figure 4
Sample LCFRS for {anbncndn | n ? 1}.
92
Kallmeyer and Maier PLCFRS Parsing
b) For every rule A(?1, . . . ,?dim(A) ) ? A1(X(1)1 , . . . , X
(1)
dim(A1)
) ? ? ?
Am(X
(m)
1 , . . . , X
(m)
dim(Am )
) and for all ?i ? yield(Ai) (1 ? i ? m):
? f (?1), . . . , f (?dim(A) )? ? yield(A) where f is defined as follows:
(i) f (t) = t for all t ? T,
(ii) f (X(i)j ) = ?i(j) for all 1 ? i ? m, 1 ? j ? dim(Ai) and
(iii) f (xy) = f (x)f (y) for all x, y ? (T ? V)+.
We call f the composition function of the rule.
c) Nothing else is in yield(A).
2. The language of G is then L(G) = {w | ?w? ? yield(S)}.
As an example, consider again the LCFRS in Figure 4. The last rule tells us that,
given a pair in the yield of A, we can obtain an element in the yield of S by concate-
nating the two components. Consequently, the language generated by this grammar is
{anbncndn |n ? 1}.
The terms of grammar fan-out and rank and the properties of monotonicity and
?-freeness will be referred to later and are therefore introduced in the following defini-
tion. They are taken from the LCFRS/MCFG terminology; the SRCG term for fan-out is
arity and the property of being monotone is called ordered in the context of SRCG.
Definition 3
Let G = ?N, T, V, P, S? be an LCFRS.
1. The fan-out of G is the maximal fan-out of all non-terminals in G.
2. Furthermore, the right-hand side length of a rewriting rule r ? P is called
the rank of r and the maximal rank of all rules in P is called the rank of G.
3. G is monotone if for every r ? P and every right-hand side non-terminal A
in r and each pair X1, X2 of arguments of A in the right-hand side of r, X1
precedes X2 in the right-hand side iff X1 precedes X2 in the left-hand side.
4. A rule r ? P is called an ?-rule if one of the left-hand side components of r
is ?.
G is ?-free if it either contains no ?-rules or there is exactly one ?-rule
S(?) ? ? and S does not appear in any of the right-hand sides of the rules
in the grammar.
For every LCFRS there exists an equivalent LCFRS that is ?-free (Seki et al 1991;
Boullier 1998a) and monotone (Michaelis 2001; Kracht 2003; Kallmeyer 2010).
The definition of a probabilistic LCFRS is a straightforward extension of the defini-
tion of PCFG and thus it follows (Levy 2005; Kato, Seki, and Kasami 2006) that:
Definition 4 (PLCFRS)
A probabilistic LCFRS (PLCFRS) is a tuple ?N, T, V, P, S, p? such that ?N, T, V, P, S? is an
LCFRS and p : P ? [0..1] a function such that for all A ? N:
?A(x)???Pp(A(x) ? ?) = 1
93
Computational Linguistics Volume 39, Number 1
PLCFRS with non-terminals {S, A, B}, terminals {a} and start symbol S:
0.2 : S(X) ? A(X) 0.8 : S(XY) ? B(X, Y)
0.7 : A(aX) ? A(X) 0.3 : A(a) ? ?
0.8 : B(aX, aY) ? B(X, Y) 0.2 : B(a, a) ? ?
Figure 5
Sample PLCFRS.
As an example, consider the PLCFRS in Figure 5. This grammar simply generates
a+. Words with an even number of as and nested dependencies are more probable
than words with a right-linear dependency structure. For instance, the word aa receives
the two analyses in Figure 6. The analysis (a) displaying nested dependencies has
probability 0.16 and (b) (right-linear dependencies) has probability 0.042.
3. Parsing PLCFRS
3.1 Binarization
Similarly to the transformation of a CFG into Chomsky normal form, an LCFRS can be
binarized, resulting in an LCFRS of rank 2. As in the CFG case, in the transformation,
we introduce a non-terminal for each right-hand side longer than 2 and split the rule
into two rules, using this new intermediate non-terminal. This is repeated until all
right-hand sides are of length 2. The transformation algorithm is inspired by Go?mez-
Rodr??guez et al (2009) and it is also specified in Kallmeyer (2010).
3.1.1 General Binarization. In order to give the algorithm for this transformation, we
need the notion of a reduction of a vector ? ? [(T ? V)?]i by a vector x ? Vj where all
variables in x occur in ?. A reduction is, roughly, obtained by keeping all variables in ?
that are not in x. This is defined as follows:
Definition 5 (Reduction)
Let ?N, T, V, P, S? be an LCFRS, ? ? [(T ? V)?]i and x ? Vj for some i, j ? IN.
Let w = ?1$ . . . $?i be the string obtained from concatenating the components of ?,
separated by a new symbol $ /? (V ? T).
Let w? be the image of w under a homomorphism h defined as follows: h(a) = $ for
all a ? T, h(X) = $ for all X ? {x1, . . .xj} and h(y) = y in all other cases.
Let y1, . . . ym ? V+ such that w? ? $?y1$+y2$+ . . . $+ym$?. Then the vector
?y1, . . . ym? is the reduction of ? by x.
For instance, ?aX1, X2, bX3? reduced with ?X2? yields ?X1, X3? and ?aX1X2bX3? re-
duced with ?X2? yields ?X1, X3? as well.
S
B
a a
(a)
S
A
a A
a
(b)
Figure 6
The two derivations of aa.
94
Kallmeyer and Maier PLCFRS Parsing
for all rules r = A(?) ? A0( ?0) . . .Am( ?m) in P with m > 1 do
remove r from P
R := ?
pick new non-terminals C1, . . . , Cm?1
add the rule A(?) ? A0( ?0)C1( ?1) to R where ?1 is obtained by reducing ? with ?0
for all i, 1 ? i ? m ? 2 do
add the rule Ci(?i) ? Ai(?i)Ci+1( ?i+1) to R where ?i+1 is obtained by reducing ?i with ?i
end for
add the rule Cm?1( ?m?2) ? Am?1( ?m?1)Am( ?m) to R
for every rule r? ? R do
replace right-hand side arguments of length > 1 with new variables (in both sides) and
add the result to P
end for
end for
Figure 7
Algorithm for binarizing an LCFRS.
The binarization algorithm is given in Figure 7. As already mentioned, it proceeds
like the CFG binarization algorithm in the sense that for right-hand sides longer than
2, we introduce a new non-terminal that covers the right-hand side without the first
element. Figure 8 shows an example. In this example, there is only one rule with a right-
hand side longer than 2. In a first step, we introduce the new non-terminals and rules
that binarize the right-hand side. This leads to the set R. In a second step, before adding
the rules from R to the grammar, whenever a right-hand side argument contains several
variables, these are collapsed into a single new variable.
The equivalence of the original LCFRS and the binarized grammar is rather straight-
forward. Note, however, that the fan-out of the LCFRS can increase.
The binarization depicted in Figure 7 is deterministic in the sense that for every rule
that needs to be binarized, we choose unique new non-terminals. Later, in Section 5.3.1,
we will introduce additional factorization into the grammar rules that reduces the set
of new non-terminals.
3.1.2 Minimizing Fan-Out and Number of Variables. In LCFRS, in contrast to CFG, the order
of the right-hand side elements of a rule does not matter for the result of a derivation.
Original LCFRS:
S(XYZUVW) ? A(X, U)B(Y, V)C(Z, W)
A(aX, aY) ? A(X, Y) A(a, a) ? ?
B(bX, bY) ? B(X, Y) B(b, b) ? ?
C(cX, cY) ? C(X, Y) C(c, c) ? ?
Rule with right-hand side of length > 2: S(XYZUVW) ? A(X, U)B(Y, V)C(Z, W)
For this rule, we obtain
R = {S(XYZUVW) ? A(X, U)C1(YZ, VW), C1(YZ, VW) ? B(Y, V)C(Z, W)}
Equivalent binarized LCFRS:
S(XPUQ) ? A(X, U)C1(P, Q)
C1(YZ, VW) ? B(Y, V)C(Z, W)
A(aX, aY) ? A(X, Y) A(a, a) ? ?
B(bX, bY) ? B(X, Y) B(b, b) ? ?
C(cX, cY) ? C(X, Y) C(c, c) ? ?
Figure 8
Sample binarization of an LCFRS.
95
Computational Linguistics Volume 39, Number 1
Therefore, we can reorder the right-hand side of a rule before binarizing it. In the
following, we present a binarization order that yields a minimal fan-out and a minimal
variable number per production and binarization step. The algorithm is inspired by
Go?mez-Rodr??guez et al (2009) and has first been published in this version in Kallmeyer
(2010). We assume that we are only considering partitions of right-hand sides where one
of the sets contains only a single non-terminal.
For a given rule c = A0(x0) ? A1(x1) . . .Ak(xk), we define the characteristic string
s(c, Ai) of the Ai-reduction of c as follows: Concatenate the elements of x0, separated with
new additional symbols $ while replacing every component from xi with a $. We then
define the arity of the characteristic string, dim(s(c, Ai)), as the number of maximal sub-
strings x ? V+ in s(Ai). Take, for example, a rule c = VP(X, YZU) ? VP(X, Z)V(Y)N(U).
Then s(c, VP) =$$Y$U, s(c, V) = X$$ZU.
Figure 9 shows how in a first step, for a given rule r with right-hand side length > 2,
we determine the optimal candidate for binarization based on the characteristic string
s(r, B) of some right-hand side non-terminal B and on the fan-out of B: On all right-
hand side predicates B we check for the maximal fan-out (given by dim(s(r, B))) and the
number of variables (dim(s(r, B)) + dim(B)) we would obtain when binarizing with this
predicate. This check provides the optimal candidate. In a second step we then perform
the same binarization as before, except that we use the optimal candidate now instead
of the first element of the right-hand side.
3.2 The Parser
We can assume without loss of generality that our grammars are ?-free and monotone
(the treebank grammars with which we are concerned all have these properties) and that
they contain only binary and unary rules. Furthermore, we assume POS tagging to be
done before parsing. POS tags are non-terminals of fan-out 1. Finally, according to our
grammar extraction algorithm (see Section 5.1), a separation between two components
always means that there is actually a non-empty gap in between them. Consequently,
two different components in a right-hand side can never be adjacent in the same
component of the left-hand side. The rules are then either of the form A(a) ? ? with A a
POS tag and a ? T or of the form A(x) ? B(x) or A(?) ? B(x)C(y) where ? ? (V+)dim(A),
x ? Vdim(B), y ? Vdim(C), that is, only the rules for POS tags contain terminals in their left-
hand sides.
cand = 0
fan-out = number of variables in r
vars = number of variables in r
for all i = 0 to m do
cand-fan-out = dim(s(r, Ai));
if cand-fan-out < fan-out and dim(Ai) < fan-out then
fan-out = max({cand-fan-out, dim(Ai)});
vars = cand-fan-out + dim(Ai);
cand = i;
else if cand-fan-out ? fan-out, dim(Ai) ? fan-out and cand-fan-out + dim(Ai) < vars then
fan-out = max({cand-fan-out, dim(Ai)});
vars = cand-fan-out + dim(Ai);
cand = i
end if
end for
Figure 9
Optimized version of the binarization algorithm, determining binarization order.
96
Kallmeyer and Maier PLCFRS Parsing
During parsing we have to link the terminals and variables in our LCFRS rules
to portions of the input string. For this purpose we need the notions of ranges, range
vectors, and rule instantiations. A range is a pair of indices that characterizes the span
of a component within the input. A range vector characterizes a tuple in the yield of a
non-terminal. A rule instantiation specifies the computation of an element from the left-
hand side yield from elements in the yields of the right-hand side non-terminals based
on the corresponding range vectors.
Definition 6 (Range)
Let w ? T? with w = w1 . . .wn where wi ? T for 1 ? i ? n.
1. Pos(w) := {0, . . . , n}.
2. We call a pair ?l, r? ? Pos(w) ? Pos(w) with l ? r a range in w. Its yield
?l, r?(w) is the substring wl+1 . . .wr.
3. For two ranges ?1 = ?l1, r1?,?2 = ?l2, r2?, if r1 = l2, then the concatenation
of ?1 and ?2 is ?1 ? ?2 = ?l1, r2?; otherwise ?1 ? ?2 is undefined.
4. A ? ? (Pos(w) ? Pos(w))k is a k-dimensional range vector for w iff
? = ??l1, r1?, . . . , ?lk, rk?? where ?li, ri? is a range in w for 1 ? i ? k.
We now define instantiations of rules with respect to a given input string. This
definition follows the definition of clause instantiations from Boullier (2000). An in-
stantiated rule is a rule in which variables are consistently replaced by ranges. Because
we need this definition only for parsing our specific grammars, we restrict ourselves to
?-free rules containing only variables.
Definition 7 (Rule instantiation)
Let G = (N, T, V, P, S) be an ?-free monotone LCFRS. For a given rule r = A(?) ?
A1(x1) ? ? ?Am( xm) ? P (0 < m) that does not contain any terminals,
1. an instantiation with respect to a string w = t1 . . . tn consists of a function
f : V ? {?i, j? | 1 ? i ? j ? |w|} such that for all x, y adjacent in one of the
elements of ?, f (x) ? f (y) must be defined; we then define f (xy) = f (x) ? f (y),
2. if f is an instantiation of r, then A( f (?)) ? A1( f (x1)) ? ? ?Am( f ( xm)) is an
instantiated rule where f (?x1, . . . , xk?) = ? f (x1), . . . , f (xk)?.
We use a probabilistic version of the CYK parser from Seki et al (1991). The algo-
rithm is formulated using the framework of parsing as deduction (Pereira and Warren
1983; Shieber, Schabes, and Pereira 1995; Sikkel 1997), extended with weights (Nederhof
2003). In this framework, a set of weighted items representing partial parsing results is
characterized via a set of deduction rules, and certain items (the goal items) represent
successful parses.
During parsing, we have to match components in the rules we use with portions of
the input string. For a given input w, our items have the form [A, ?] where A ? N and ?
is a range vector that characterizes the span of A. Each item has a weight in that encodes
the Viterbi inside score of its best parse tree. More precisely, we use the log probability
log(p) where p is the probability.
The first rule (scan) tells us that the POS tags that we receive as inputs are given.
Consequently, they are axioms; their probability is 1 and their weight therefore 0. The
97
Computational Linguistics Volume 39, Number 1
Scan: 0 : [A, ??i, i + 1??] A is the POS tag of wi+1
Unary:
in : [B, ?]
in + log(p) : [A, ?] p : A(?) ? B(?) ? P
Binary:
inB : [B, ?B], inC : [C, ?C]
inB + inC + log(p) : [A, ?A]
p : A( ?A ) ? B( ?B )C( ?C )
is an instantiated rule
Goal: [S, ??0, n??]
Figure 10
Weighted CYK deduction system.
second rule, unary, is applied whenever we have found the right-hand side of an
instantiation of a unary rule. In our grammar, terminals only occur in rules with POS
tags and the grammar is ordered and ?-free. Therefore, the components of the yield of
the right-hand side non-terminal and of the left-hand side terminals are the same. The
rule binary applies an instantiated rule of rank 2. If we already have the two elements
of the right-hand side, we can infer the left-hand side element. In both cases, unary
and binary, the probability p of the new rule is multiplied with the probabilities of the
antecedent items (which amounts to summing up the antecedent weights and log(p)).
We perform weighted deductive parsing, based on the deduction system from
Figure 10. We use a chart C and an agenda A, both initially empty, and we proceed
as in Figure 11. Because for all our deduction rules, the weight functions f that compute
the weight of a consequent item from the weights of the antecedent items are monotone
non-increasing in each variable, the algorithm will always find the best parse without
the need of exhaustive parsing. All new items that we deduce involve at least one of
the agenda items as an antecedent item. Therefore, whenever an item is the best in the
agenda, we can be sure that we will never find an item with a better (i.e., higher) weight.
Consequently, we can safely store this item in the chart and, if it is a goal item, we have
found the best parse.
As an example consider the development of the agenda and the chart in Figure 12
when parsing aa with the PLCFRS from Figure 5, transformed into a PLCFRS with
pre-terminals and binarization (i.e., with a POS tag Ta and a new binarization non-
terminal B?). The new PLCFRS is given in Figure 13.
In this example, we find a first analysis for the input (a goal item) when combining
an A with span ??0, 2?? into an S. This S has however a rather low probability and is
therefore not on top of the agenda. Later, when finding the better analysis, the weight
add SCAN results to A
while A = ?
remove best item x : I from A
add x : I to C
if I goal item
then stop and output true
else
for all y : I? deduced from x : I and items in C:
if there is no z with z : I? ? C ? A
then add y : I? to A
else if z : I? ? A for some z
then update weight of I? in A to max(y, z)
Figure 11
Weighted deductive parsing.
98
Kallmeyer and Maier PLCFRS Parsing
chart agenda
0 : [Ta, ?0, 1?], 0 : [Ta, ?1, 2?]
0 : [Ta, ?0, 1?] 0 : [Ta, ?1, 2?],?0.5 : [A, ?0, 1?]
0 : [Ta, ?0, 1?], 0 : [Ta, ?1, 2?] ?0.5 : [A, ?0, 1?],?0.5 : [A, ?1, 2?],
?0.7 : [B, ?0, 1?, ?1, 2?]
0 : [Ta, ?0, 1?], 0 : [Ta, ?1, 2?], ?0.5 : [A, ?1, 2?],?0.7 : [B, ?0, 1?, ?1, 2?],
?0.5 : [A, ?0, 1?] ?1.2 : [S, ?0, 1?]
0 : [Ta, ?0, 1?], 0 : [Ta, ?1, 2?], ?0.65 : [A, ?0, 2?],?0.7 : [B, ?0, 1?, ?1, 2?],
0.5 : [A, ?0, 1?],?0.5 : [A, ?1, 2?] ?1.2 : [S, ?0, 1?],?1.2 : [S, ?1, 2?]
0 : [Ta, ?0, 1?], 0 : [Ta, ?1, 2?], ?0.7 : [B, ?0, 1?, ?1, 2?],?1.2 : [S, ?0, 1?],
?0.5 : [A, ?0, 1?],?0.5 : [A, ?1, 2?], ?1.2 : [S, ?1, 2?],?1.35 : [S, ?0, 2?]
?0.65 : [A, ?0, 2?]
0 : [Ta, ?0, 1?], 0 : [Ta, ?1, 2?], ?0.8 : [S, ?0, 2?],?1.2 : [S, ?0, 1?],
?0.5 : [A, ?0, 1?],?0.5 : [A, ?1, 2?], ?1.2 : [S, ?1, 2?]
?0.65 : [A, ?0, 2?],?0.7 : [B, ?0, 1?, ?1, 2?]
Figure 12
Parsing of aa with the grammar from Figure 5.
PLCFRS with non-terminals {S, A, B, B?, Ta}, terminals {a} and start symbol S:
0.2 : S(X) ? A(X) 0.8 : S(XY) ? B(X, Y)
0.7 : A(XY) ? Ta(X)A(Y) 0.3 : A(X) ? Ta(X)
0.8 : B(ZX, Y) ? Ta(Z)B?(X, Y) 1 : B?(X, UY) ? B(X, Y)Ta(U)
0.2 : B(X, Y) ? Ta(X)Ta(Y) 1 : Ta(a) ? ?
Figure 13
Sample binarized PLCFRS (with pre-terminal Ta).
of the S item in the agenda is updated and then the goal item is the top agenda item and
therefore parsing has been successful.
Note that, so far, we have only presented the recognizer. In order to extend it to a
parser, we do the following: Whenever we generate a new item, we store it not only with
its weight but also with backpointers to its antecedent items. Furthermore, whenever
we update the weight of an item in the agenda, we also update the backpointers. In
order to read off the best parse tree, we have to start from the goal item and follow the
backpointers.
4. Outside Estimates
So far, the weights we use give us only the Viterbi inside score of an item. In order
to speed up parsing, we add the estimate of the costs for completing the item into a
goal item to its weight?that is, to the weight of each item in the agenda, we add an
estimate of its Viterbi outside score2 (i.e., the logarithm of the estimate). We use context
summary estimates. A context summary is an equivalence class of items for which we
can compute the actual outside scores. Those scores are then used as estimates. The
challenge is to choose the estimate general enough to be efficiently computable and
specific enough to be helpful for discriminating items in the agenda.
2 Note that just as Klein and Manning (2003a), we use the terms inside score and outside score to
denote the Viterbi inside and outside scores. They are not to be confused with the actual inside or
outside probability.
99
Computational Linguistics Volume 39, Number 1
Admissibility and monotonicity are two important conditions on estimates. All
our outside estimates are admissible (Klein and Manning 2003a), which means that
they never underestimate the actual outside score of an item. In other words, they
are too optimistic about the costs of completing the item into an S item spanning the
entire input. For the full SX estimate described in Section 4.1 and the SX estimate with
span and sentence length in Section 4.4, the monotonicity is guaranteed and we can do
true A? parsing as described by Klein and Manning. Monotonicity means that for each
antecedent item of a rule it holds that its weight is greater than or equal to the weight
of the consequent item. The estimates from Sections 4.2 and 4.3 are not monotonic. This
means that it can happen that we deduce an item I2 from an item I1 where the weight of
I2 is greater than the weight of I1. The parser can therefore end up in a local maximum
that is not the global maximum we are searching for. In other words, those estimates are
only figures of merit (FOM).
All outside estimates are computed off-line for a certain maximal sentence length
lenmax.
4.1 Full SX Estimate
The full SX estimate is a PLCFRS adaption of the SX estimate of Klein and Manning
(2003a) (hence the name). For a given sentence length n, the estimate gives the maximal
probability of completing a category X with a span ? into an S with span ??0, n??.
For its computation, we need an estimate of the inside score of a category C with a
span ?, regardless of the actual terminals in our input. This inside estimate is computed
as shown in Figure 14. Here, we do not need to consider the number of terminals outside
the span of C (to the left or right or in the gaps), because they are not relevant for the
inside score. Therefore the items have the form [A, ?l1, . . . , ldim(A)?], where A is a non-
terminal and li gives the length of its ith component. It holds that
?1?i?dim(A)li ? lenmax ? dim(A) + 1
because our grammar extraction algorithm ensures that the different components in
the yield of a non-terminal are never adjacent. There is always at least one terminal in
between two different components that does not belong to the yield of the non-terminal.
The first rule in Figure 14 tells us that POS tags always have a single component
of length 1; therefore this case has probability 1 (weight 0). The rules unary and binary
are roughly like the ones in the CYK parser, except that they combine items with length
information. The rule unary for instance tells us that if the log of the probability of
building [B,l] is greater or equal to in and if there is a rule that allows to deduce an
POS tags: 0 : [A, ?1?] A a POS tag Unary:
in : [B,l]
in + log(p) : [A,l]
p : A(?) ? B(?) ? P
Binary:
inB : [B,lB], inC : [C,lC]
inB + inC + log(p) : [A,lA]
where p : A( ?A) ? B( ?B)C( ?C) ? P and the following holds: we define B(i) as
{1 ? j ? dim(B) | ?B( j) occurs in ?A(i)} and C(i) as {1 ? j ? dim(C) | ?C( j) occurs in ?A(i)}.
Then for all i, 1 ? i ? dim(A):lA(i) = ?j?B(i)lB( j) +?j?C(i)lC( j).
Figure 14
Estimate of the Viterbi inside score.
100
Kallmeyer and Maier PLCFRS Parsing
Axiom : 0 : [S, ?0, len, 0?] 1 ? len ? lenmax Unary:
out : [A,l]
out + log(p) : [B,l]
p : A(?) ? B(?) ? P
Binary-right:
out : [X,lX]
out + in(A,l?A) + log(p) : [B,lB]
Binary-left:
out : [X,lX]
out + in(B,l?B) + log(p) : [A,lA]
where, for both binary rules, there is an instantiated rule p : X(?) ? A( ?A)B( ?B) such that
lX = lout(?),lA = lout(?A),l?A = lin(?A),lB = lout(?B),l
?
B = lin(?B).
Figure 15
Full SX estimate first version (top?down).
A item from [B,l] with probability p, then the log of the probability of [A,l] is greater
or equal to in + log(p). For each item, we record its maximal weight (i.e., its maximal
probability). The rule binary is slightly more complicated because we have to compute
the length vector of the left-hand side of the rule from the right-hand side length vectors.
A straightforward extension of the CFG algorithm from Klein and Manning (2003a)
for computing the SX estimate is given in Figure 15. Here, the items have the form [A,l]
where the vectorl tells us about the lengths of the string to the left of the first component,
the first component, the string in between the first and second component, and so on.
The algorithm proceeds top?down. The outside estimate of completing an S with
component length len and no terminals to the left or to the right of the S component
(item [S, ?0, len, 0?]) is 0. If we expand with a unary rule (unary), then the outside
estimate of the right-hand side item is greater or equal to the outside estimate of the
left-hand side item plus the log of the probability of the rule. In the case of binary rules,
we have to further add the inside estimate of the other daughter. For this, we need a
different length vector (without the lengths of the parts in between the components).
Therefore, for a given range vector ? = ??l1, r1?, . . . , ?lk, rk?? and a sentence length n,
we distinguish between the inside length vector lin(?) = ?r1 ? l1, . . . , rk ? lk? and the
outside length vector lout(?) = ?l1, r1 ? l1, l2 ? r1, . . . , lk ? rk?1, rk ? lk, n ? rk?.
This algorithm has two major problems: Because it proceeds top?down, in the
binary rules we must compute all splits of the antecedent X span into the spans of
A and B, which is very expensive. Furthermore, for a category A with a certain number
of terminals in the components and the gaps, we compute the lower part of the outside
estimate several times, namely, for every combination of number of terminals to the left
and to the right (first and last element in the outside length vector). In order to avoid
these problems, we now abstract away from the lengths of the part to the left and the
right, modifying our items such as to allow a bottom?up strategy.
The idea is to compute the weights of items representing the derivations from a
certain lower C up to some A (C is a kind of ?gap? in the yield of A) while summing up
the inside costs of off-spine nodes and the log of the probabilities of the corresponding
rules. We use items [A, C,?A,?C, shift] where A, C ? N and ?A,?C are range vectors, both
with a first component starting at position 0. The integer shift ? lenmax tells us how many
positions to the right the C span is shifted, compared to the starting position of the A.
?A and ?C represent the spans of C and A while disregarding the number of terminals
to the left and the right (i.e., only the lengths of the components and of the gaps are
encoded). This means in particular that the length n of the sentence does not play a
role here. The right boundary of the last range in the vectors is limited to lenmax. For
101
Computational Linguistics Volume 39, Number 1
any i, 0 ? i ? lenmax, and any range vector ?, we define shift(?, i) as the range vector one
obtains from adding i to all range boundaries in ? and shift(?,?i) as the range vector
one obtains from subtracting i from all boundaries in ?.
The weight of [A, C,?A,?C, i] estimates the log of the probability of completing a
C tree with yield ?C into an A tree with yield ?A such that, if the span of A starts at
position j, the span of C starts at position i + j. Figure 16 gives the computation. The
value of in(A,l) is the inside estimate of [A,l].
The SX-estimate for some predicate C with span ? where i is the left boundary of the
first component of ? and with sentence length n is then given by the maximal weight of
[S, C, ?0, n?, shift(?,?i), i].
4.2 SX with Left, Gaps, Right, Length
A problem of the previous estimate is that with a large number of non-terminals (for
treebank parsing, approximately 12,000 after binarization and markovization), the com-
putation of the estimate requires too much space. We therefore turn to simpler estimates
with only a single non-terminal per item. We now estimate the outside score of a non-
terminal A with a span of a length length (the sum of the lengths of all the components
of the span), with left terminals to the left of the first component, right terminals to the
right of the last component, and gaps terminals in between the components of the A
span (i.e., filling the gaps). Our items have the form [X, len, left, right, gaps] with X ? N,
len + left + right + gaps ? lenmax, len ? dim(X), gaps ? dim(X) ? 1.
Let us assume that, in the rule X(?) ? A( ?A)B( ?B), when looking at the vector ?,
we have leftA variables for A-components preceding the first variable of a B component,
rightA variables for A-components following the last variable of a B component, and
rightB variables for B-components following the last variable of an A component. (In our
grammars, the first left-hand side argument always starts with the first variable from A.)
Furthermore, we set gapsA = dim(A) ? leftA ? rightA and gapsB = dim(B) ? rightB.
Figure 17 gives the computation of the estimate. It proceeds top?down, as the
computation of the full SX estimate in Figure 15, except that now the items are simpler.
POS tags: 0 : [C, C, ?0, 1?, ?0, 1?, 0] C a POS tag
Unary:
0 : [B, B,?B,?B, 0]
log(p) : [A, B,?B,?B, 0]
p : A(?) ? B(?) ? P
Binary-right:
0 : [A, A,?A,?A, 0], 0 : [B, B,?B,?B, 0]
in(A, lin(?A)) + log(p) : [X, B,?X,?B, i]
Binary-left:
0 : [A, A,?A,?A, 0], 0 : [B, B,?B,?B, 0]
in(B, lin(?B)) + log(p) : [X, A,?X,?A, i]
where i is such that for shift(?B, i) = ??B p : X(?X ) ? A(?A)B(??B) is an instantiated rule.
Starting sub-trees with larger gaps:
out : [B, C,?B,?C, i]
0 : [B, B,?B,?B, 0]
Transitive closure of sub-tree combination:
out1 : [A, B,?A,?B, i], out2 : [B, C,?B,?C, j]
out1 + out2 : [A, C,?A,?C, i + j]
Figure 16
Full SX estimate second version (bottom?up).
102
Kallmeyer and Maier PLCFRS Parsing
Axiom : 0 : [S, len, 0, 0, 0] 1 ? len ? lenmax
Unary:
out : [X, len, l, r, g]
out + log(p) : [A, len, l, r, g] p : X(?) ? A(?) ? P
Binary-right:
out : [X, len, l, r, g]
out + in(A, len ? lenB) + log(p) : [B, lenB, lB, rB, gB]
Binary-left:
out : [X, len, l, r, g]
out + in(B, len ? lenA) + log(p) : [A, lenA, lA, rA, gA]
where, for both binary rules, p : X(?) ? A( ?A)B( ?B) ? P.
Further side conditions for Binary-right:
a) len + l + r + g = lenB + lB + rB + gB, b) lB ? l + leftA,
c) if rightA > 0, then rB ? r + rightA, else (rightA = 0), rB = r, d) gB ? gapsA.
Further side conditions for Binary-left:
a) len + l + r + g = lenA + lA + rA + gA, b) lA = l,
c) if rightB > 0, then rA ? r + rightB, else (rightB = 0), rA = r d) gA ? gapsB.
Figure 17
SX estimate depending on length, left, right, gaps.
The value in(X, l) for a non-terminal X and a length l, 0 ? l ? lenmax is an estimate
of the probability of an X category with a span of length l. Its computation is specified
in Figure 18.
The SX-estimate for a sentence length n and for some predicate C with a range
characterized by ? = ??l1, r1?, . . . , ?ldim(C), rdim(C)?? where len = ?dim(C)i=1 (ri ? li) and r =
n ? rdim(C) is then given by the maximal weight of the item [C, len, l1, r, n ? len ? l1 ? r].
4.3 SX with LR, Gaps, Length
In order to further decrease the space complexity of the computation of the outside
estimate, we can simplify the previous estimate by subsuming the two lengths left and
right in a single length lr. The items now have the form [X, len, lr, gaps] with X ? N,
len + lr + gaps ? lenmax, len ? dim(X), gaps ? dim(X) ? 1.
The computation is given in Figure 19. Again, we define leftA, gapsA, rightA and
gapsB, rightB for a rule X(?) ? A( ?A)B( ?B) as before. Furthermore, in both Binary-left
and Binary-right, we have limited lr in the consequent item to the lr of the antecedent
plus the length of the sister (lenB, resp. lenA). This results in a further reduction of the
number of items while having only little effect on the parsing results.
The SX-estimate for a sentence length n and for some predicate C with a span
? = ??l1, r1?, . . . , ?ldim(C), rdim(C)?? where len = ?dim(C)i=1 (ri ? li) and r = n ? rdim(C) is then the
maximal weight of [C, len, l1 + r, n ? len ? l1 ? r].
POS tags: 0 : [A, 1] A a POS tag Unary:
in : [B, l]
in + log(p) : [A, l] p : A(?) ? B(?) ? P
Binary:
inB : [B, lB], inC : [C, lC]
inB + inC + log(p) : [A, lB + lC]
where either p : A( ?A) ? B( ?B)C( ?C) ? P or p : A( ?A) ? C( ?C)B( ?B) ? P.
Figure 18
Estimate of the inside score with total span length.
103
Computational Linguistics Volume 39, Number 1
Axiom : 0 : [S, len, 0, 0] 1 ? len ? lenmax
Unary:
out : [X, len, lr, g]
out + log(p) : [A, len, lr, g] p : X(?) ? A(?) ? P
Binary-right:
out : [X, len, lr, g]
out + in(A, len ? lenB) + log(p) : [B, lenB, lrB, gB] p : X(?) ? A( ?A )B( ?B ) ? P
Binary-left:
out : [X, len, lr, g]
out + in(B, len ? lenA) + log(p) : [A, lenA, lrA, gA] p : X(?) ? A( ?A )B( ?B ) ? P
Further side conditions for Binary-right:
a) len + lr + g = lenB + lrB + gB b) lr < lrB c) gB ? gapsA
Further side conditions for Binary-left:
a) len + lr + g = lenA + lrA + gA b) if rightB = 0 then lr = lrA, else lr < lrA c) gA ? gapsB
Figure 19
SX estimate depending on length, LR, gaps.
4.4 SX with Span and Sentence Length
We will now present a further simplification of the last estimate that records only the
span length and the length of the entire sentence. The items have the form [X, len, slen]
with X ? N, dim(X) ? len ? slen. The computation is given in Figure 20. This last esti-
mate is actually monotonic and allows for true A? parsing.
The SX-estimate for a sentence length n and for some predicate C with a span
? = ??l1, r1?, . . . , ?ldim(C), rdim(C)?? where len = ?dim(C)i=1 (ri ? li) is then the maximal weight
of [C, len, n].
In order to prove that this estimate allows for monotonic weighted deductive pars-
ing and therefore guarantees that the best parse will be found, let us have a look at the
CYK deduction rules when being augmented with the estimate. Only Unary and Binary
are relevant because Scan does not have antecedent items. The two rules, augmented
with the outside estimate, are shown in Figure 21.
We have to show that for every rule, if this rule has an antecedent item with weight
w and a consequent item with weight w?, then w ? w?.
Let us start with Unary. To show: inB + outB ? inB + log(p) + outA. Because of the
Unary rule for computing the outside estimate and because of the unary production,
Axiom : 0 : [S, len, len] 1 ? len ? lenmax
Unary:
out : [X, lX, slen]
out + log(p) : [A, lX, slen]
p : X(?) ? A(?) ? P
Binary-right:
out : [X, lX, slen]
out + in(A, lX ? lB) + log(p) : [B, lB, slen] p : X(?) ? A( ?A )B( ?B ) ? P
Binary-left:
out : [X, lX, slen]
out + in(B, lX ? lA) + log(p) : [A, lA, slen] p : X(?) ? A( ?A )B( ?B ) ? P
Figure 20
SX estimate depending on span and sentence length.
104
Kallmeyer and Maier PLCFRS Parsing
Unary:
inB + outB : [B, ?]
inB + log(p) + outA : [A, ?]
p : A(?) ? B(?) ? P
Binary:
inB + outB : [B, ?B], inC + outC : [C, ?C]
inB + inC + log(p) + outA : [A, ?A]
p : A( ?A ) ? B( ?B )C( ?C )
is an instantiated rule
(Here, outA, outB, and outC are the respective outside estimates of [A, ?A], [B, ?B] and [C, ?C].)
Figure 21
Parsing rules including outside estimate.
we obtain that, given the outside estimate outA of [A, ?] the outside estimate outB of the
item [B, ?] is at least outA + log(p), namely, outB ? log(p) + outA.
Now let us consider the rule Binary. We treat only the relation between the weight
of the C antecedent item and the consequent. The treatment of the antecedent B is
symmetric. To show: inC + outC ? inB + inC + log(p) + outA. Assume that lB is the length
of the components of the B item and n is the sentence length. Then, because of the
Binary-right rule in the computation of the outside estimate and because of our in-
stantiated rule p : A( ?A) ? B( ?B)C( ?C), we have that the outside estimate outC of the
C-item is at least outA + in(B, lB) + log(p). Furthermore, in(B, lB) ? inB. Consequently
outC ? inB + log(p) + outA.
4.5 Integration into the Parser
Before parsing, the outside estimates of all items up to a certain maximal sentence length
lenmax are precomputed. Then, when performing the weighted deductive parsing as
explained in Section 3.2, whenever a new item is stored in the agenda, we add its outside
estimate to its weight.
Because the outside estimate is always greater than or equal to the actual outside
score, given the input, the weight of an item in the agenda is always greater than or
equal to the log of the actual product of the inside and outside score of the item. In this
sense, the outside estimates given earlier are admissible.
Additionally, as already mentioned, note that the full SX estimate and the SX esti-
mate with span and sentence length are monotonic and allow for A? parsing. The other
two estimates, which are both not monotonic, act as FOMs in a best-first parsing context.
Consequently, they contribute to speeding up parsing but they decrease the quality of
the parsing output. For further evaluation details see Section 6.
5. Grammars for Discontinuous Constituents
5.1 Grammar Extraction
The algorithm we use for extracting an LCFRS from a constituency treebank with cross-
ing branches has originally been presented in Maier and S?gaard (2008). It interprets
the treebank trees as LCFRS derivation trees. Consider for instance the tree in Figure 22.
The S node has two daughters, a VMFIN node and a VP node. This yields a rule
S ? VP VMFIN. The VP is discontinuous with two components that wrap around the
yield of the VMFIN. Consequently, the LCFRS rule is S(XYZ) ? VP(X, Z) VMFIN(Y).
The extraction of an LCFRS from treebanks with crossing branches is almost im-
mediate, except for the fan-out of the non-terminal categories: In the treebank, we can
have the same non-terminal with different fan-outs, for instance a VP without a gap
(fan-out 1), a VP with a single gap (fan-out 2), and so on. In the corresponding LCFRS,
105
Computational Linguistics Volume 39, Number 1
S
VP
VP
PROAV VMFIN VVPP VAINF
daru?ber mu? nachgedacht werden
about it must thought be
?It must be thought about it?
Figure 22
A sample tree from NeGra.
we have to distinguish these different non-terminals by mapping them to different
predicates.
The algorithm first creates a so-called lexical clause P(a) ? ? for each pre-terminal
P dominating some terminal a. Then for all other non-terminals A0 with the children
A1 ? ? ?Am, a clause A0 ? A1 ? ? ?Am is created. The number of components of the A1 ? ? ?Am
is the number of discontinuous parts in their yields. The components of A0 are concate-
nations of variables that describe how the discontinuous parts of the yield of A0 are
obtained from the yields of its daughters.
More precisely, the non-terminals in our LCFRS are all Ak where A is a non-terminal
label in the treebank and k is a possible fan-out for A. For a given treebank tree ?V, E, r, l?
where V is the set of nodes, E ? V ? V the set of immediate dominance edges, r ? V
the root node, and l : V ? N ? T the labeling function, the algorithm constructs the
following rules. Let us assume that w1, . . . , wn are the terminal labels of the leaves
in ?V, E, r? with a linear precedence relation wi ? wj for 1 ? i < j ? n. We introduce a
variable Xi for every wi, 1 ? i ? n.
 For every pair of nodes v1, v2 ? V with ?v2, v2? ? E, l(v2) ? T, we add
l(v1)(l(v2)) ? ? to the rules of the grammar. (We omit the fan-out subscript
here because pre-terminals are always of fan-out 1.)
 For every node v ? V with l(v) = A0 /? T such that there are exactly m
nodes v1, . . . , vm ? V (m ? 1) with ?v, vi? ? E and l(vi) = Ai /? T for all
1 ? i ? m, we now create a rule
A0(x
(0)
1 , . . . , x
(0)
dim(A0 )
)
? A1(x(1)1 , . . . , x
(1)
dim(A1 )
) . . .Am(x
(m)
1 , . . . , x
(m)
dim(Am )
)
where for the predicate Ai, 0 ? i ? m, the following must hold:
1. The concatenation of all arguments of Ai, x
(i)
1 . . . x
(i)
dim(Ai )
is the
concatenation of all X ? {Xi | ?vi, v?i? ? E? with l(v?i ) = wi} such that
Xi precedes Xj if i < j, and
2. a variable Xj with 1 ? j < n is the right boundary of an argument of
Ai if and only if Xj+1 /? {Xi | ?vi, v?i? ? E? with l(v?i ) = wi}, that is, an
argument boundary is introduced at each discontinuity.
As a further step, in this new rule, all right-hand side arguments of length
> 1 are replaced in both sides of the rule with a single new variable.
Finally, all non-terminals A in the rule are equipped with an additional
subscript dim(A), which gives us the final non-terminal in our LCFRS.
106
Kallmeyer and Maier PLCFRS Parsing
PROAV(Daru?ber) ? ?
VMFIN(mu?) ? ?
VVPP(nachgedacht) ? ?
VAINF(werden) ? ?
S1(X1X2X3) ? VP2(X1, X3)VMFIN(X2)
VP2(X1, X2X3) ? VP2(X1, X2)VAINF(X3)
VP2(X1, X2) ? PROAV(X1)VVPP(X2)
Figure 23
LCFRS rules extracted from the tree in Figure 22.
For the tree in Figure 22, the algorithm produces for instance the rules in Figure 23.
As standard for PCFG, the probabilities are computed using Maximum Likelihood
Estimation.
5.2 Head-Outward Binarization
As previously mentioned, in contrast to CFG the order of the right-hand side elements
of a rule does not matter for the result of an LCFRS derivation. Therefore, we can reorder
the right-hand side of a rule before binarizing it.
The following, treebank-specific reordering results in a head-outward binarization
where the head is the lowest subtree and it is extended by adding first all sisters to its
left and then all sisters to its right. It consists of reordering the right-hand side of the
rules extracted from the treebank such that first, all elements to the right of the head are
listed in reverse order, then all elements to the left of the head in their original order, and
then the head itself. Figure 24 shows the effect this reordering and binarization has on
the form of the syntactic trees. In addition to this, we also use a variant of this reordering
Tree in NeGra format:
S
VP
NN VMFIN NN AV VAINF
das mu? man jetzt machen
that must one now do
?One has to do that now?
Rule extracted for the S node: S(XYZU) ? VP(X, U)VMFIN(Y)NN(Z)
Reordering for head-outward binarization: S(XYZU) ? NN(Z)VP(X, U)VMFIN(Y)
New rules resulting from binarizing this rule:
S(XYZ) ? Sbin1(X, Z)NN(Y) Sbin1(XY, Z) ? VP(X, Z)VMFIN(Y)
Rule extracted for the VP node: VP(X, YZ) ? NN(X)AV(Y)VAINF(Z)
New rules resulting from binarizing this rule:
VP(X, Y) ? NN(X)VPbin1(Y) VPbin1(XY) ? AV(X)VAINF(Y)
Tree after binarization:
S
Sbin1
VP
VPbin1
NN VMFIN NN AV VAINF
Figure 24
Sample head-outward binarization.
107
Computational Linguistics Volume 39, Number 1
where we add first the sisters to the right and then the ones to the left. This is what Klein
and Manning (2003b) do. To mark the heads of phrases, we use the head rules that the
Stanford parser (Klein and Manning 2003c) uses for NeGra.
In all binarizations, there exists the possibility of adding additional unary rules
when deriving the head. This allows for a further factorization. In the experiments,
however, we do not insert unary rules, neither at the highest nor at the lowest new
binarization non-terminal, because this was neither beneficial for parsing times nor for
the parsing results.
5.3 Incorporating Additional Context
5.3.1 Markovization. As already mentioned in Section 3.1, a binarization that introduces
unique new non-terminals for every single rule that needs to be binarized produces
a large amount of non-terminals and fails to capture certain generalizations. For this
reason, we introduce markovization (Collins 1999; Klein and Manning 2003b).
Markovization is achieved by introducing only a single new non-terminal for the
new rules introduced during binarization and adding vertical and horizontal context
from the original trees to each occurrence of this new non-terminal. As vertical context,
we add the first v labels on the path from the root node of the tree that we want to
binarize to the root of the entire treebank tree. The vertical context is collected during
grammar extraction and then taken into account during binarization of the rules. As
horizontal context, during binarization of a rule A(?) ? A0( ?0) . . .Am( ?m), for the new
non-terminal that comprises the right-hand side elements Ai . . .Am (for some 1 ? i ?
m), we add the first h elements of Ai, Ai?1, . . . , A0.
Figure 25 shows an example of a markovization of the tree from Figure 24 with v = 1
and h = 2. Here, the superscript is the vertical context and the subscript the horizontal
context of the new non-terminal X. Note that in this example we have disregarded the
fan-out of the context categories. The VP, for instance, is actually a VP2 because it has
fan-out 2. For the context symbols, one can either use the categories from the original
treebank (without fan-out) or the ones from the LCFRS rules (with fan-out). We chose
the latter approach because it delivered better parsing results.
5.3.2 Further Category Splitting. Grammar annotation (i.e., manual enhancement of an-
notation information through category splitting) has previously been successfully used
in parsing German (Versley 2005). In order to see if such modifications can have a
beneficial effect in PLCFRS parsing as well, we perform different category splits on the
(unbinarized) NeGra constituency data.
We split the category S (?sentence?) into SRC (?relative clause?) and S (all other
categories S). Relative clauses mostly occur in a very specific context, namely, as the
S
XSVP,NN
VP
XVPADV,NN
NN VMFIN NN ADV VAINF
Figure 25
Sample markovization with v = 1, h = 2.
108
Kallmeyer and Maier PLCFRS Parsing
Table 1
NeGra: Properties of the data with crossing branches.
training test
number of sentences 16,502 1,833
average sentence length 14.56 14.62
average tree height 4.62 4.72
average children per node 2.96 2.94
sentences without gaps 12,481 (75.63%) 1,361 (74.25%)
sentences with one gap 3,320 (20.12%) 387 (21.11%)
sentences with ? 2 gaps 701 (4.25%) 85 (4.64%)
maximum gap degree 6 5
right part of an NP or a PP. This splitting should therefore speed up parsing and increase
precision. Furthermore, we distinguish NPs by their case. More precisely, to all nodes
with categories N, we append the grammatical function label to the category label. We
finally experiment with the combination of both splits.
6. Experiments
6.1 Data
Our data source is the NeGra treebank (Skut et al 1997). We create two different data
sets for constituency parsing. For the first one, we start out with the unmodified NeGra
treebank and remove all sentences with a length of more than 30 words. We pre-process
the treebank following common practice (Ku?bler and Penn 2008), attaching all nodes
which are attached to the virtual root node to nodes within the tree such that, ideally,
no new crossing edges are created. In a second pass, we attach punctuation which
comes in pairs (parentheses, quotation marks) to the same nodes. For the second data
set we create a copy of the pre-processed first data set, in which we apply the usual
tree transformations for NeGra PCFG parsing (i.e., moving nodes to higher positions
until all crossing branches are resolved). The first 90% of both data sets are used as the
training set and the remaining 10% as test set. The first data set is called NeGraLCFRS
and the second is called NeGraCFG.
Table 1 lists some properties of the training and test (respectively, gold) parts of
NeGraLCFRS, namely, the total number of sentences, the average sentence length, the
average tree height (the height of a tree being the length of the longest of all paths
from the terminals to the root node), and the average number of children per node
(excluding terminals). Furthermore, gap degrees (i.e., the number of gaps in the spans
of non-terminal nodes) are listed (Maier and Lichte 2011).
Our findings correspond to those of Maier and Lichte except for small differences
due to the fact that, unlike us, they removed the punctuation from the trees.
6.2 Parser Implementation
We have implemented the CYK parser described in the previous section in a system
called rparse. The implementation is realized in Java.3
3 rparse is available under the GNU General Public License 2.0 at http://www.phil.hhu.de/rparse.
109
Computational Linguistics Volume 39, Number 1
Table 2
NeGraLCFRS: PLCFRS parsing results for different binarizations.
Head-driven KM L-to-R Optimal Deterministic
LP 74.00 74.00 75.08 74.92 72.40
LR 74.24 74.13 74.69 74.88 71.80
LF1 74.12 74.07 74.88 74.90 72.10
UP 77.09 77.20 77.95 77.77 75.67
UR 77.34 77.33 77.54 77.73 75.04
UF1 77.22 77.26 77.75 77.75 75.35
6.3 Evaluation
For the evaluation of the constituency parses, we use an EVALB-style metric. For a tree
over a string w, a single constituency is represented by a tuple ?A, ?? with A being a
node label and ? ? (Pos(w) ? Pos(w))dim(A). We compute precision, recall, and F1 based
on these tuples from gold and de-binarized parsed test data from which all category
splits have been removed. This metric is equivalent to the corresponding PCFG metric
for dim(A) = 1. Despite the shortcomings of such a measure (Rehbein and van Genabith
2007), it still allows to some extent a comparison to previous work in PCFG parsing (see
also Section 7). Note that we provide the parser with gold POS tags in all experiments.
6.4 Markovization and Binarization
We use the markovization settings v = 1 and h = 2 for all further experiments. The
setting which has been reported to yield the best results for PCFG parsing of NeGra,
v = 2 and h = 1 (Rafferty and Manning 2008), required a parsing time which was too
high.4
Table 2 contains the parsing results for NeGraLCFRS using five different binariza-
tions: Head-driven and KM are the two head-outward binarizations that use a head
chosen on linguistic grounds (described in Section 5.2); L-to-R is another variant in
which we always choose the rightmost daughter of a node as its head.5 Optimal reorders
the left-hand side such that the fan-out of the binarized rules is optimized (described in
Section 3.1.2). Finally, we also try a deterministic binarization (Deterministic) in which
we binarize strictly from left to right (i.e., we do not reorder the right-hand sides of
productions, and choose unique binarization labels).
The results of the head-driven binarizations and the optimal binarization lie close
together; the results for the deterministic binarization are worse. This indicates that the
presence or absence of markovization has more impact on parsing results than the actual
binarization order. Furthermore, the non-optimal binarizations did not yield a binarized
grammar of a higher fan-out than the optimal binarization: For all five binarizations,
the fan-out was 7 (caused by a VP interrupted by punctuation).
4 Older versions of rparse contained a bug that kept the priority queue from being updated correctly
(i.e., during an update, the corresponding node in the priority queue was not moved to its top, and
therefore the best parse was not guaranteed to be found); however, higher parsing speeds were achieved.
The current version of rparse implements the update operation correctly, using a Fibonacci queue to
ensure efficiency (Cormen et al 2003). Thanks to Andreas van Cranenburgh for pointing this out.
5 The term head is not used in its proper linguistic sense here.
110
Kallmeyer and Maier PLCFRS Parsing
 0
 100
 200
 300
 400
 500
 600
 700
 15  17  19  21  23  25  27  29
ite
m
s 
(in
 10
00
)
sentence length
Head-driven
KM
L-to-R
Optimal
Deterministic
 50
 100
 150
 200
 250
 300
 350
 400
 15  17  19  21  23  25  27  29
ite
m
s 
(in
 10
00
)
sentence length
Baseline
NP
S
NP+S
 50
 100
 150
 200
 250
 300
 350
 400
 15  17  19  21  23  25  27  29
ite
m
s 
(in
 10
00
)
sentence length
OFF
LR
LN
Figure 26
NeGraLCFRS: Items for PLCFRS parsing (left-to-right): binarizations, baseline and category splits,
and estimates.
The different binarizations result in different numbers of items, and therefore allow
for different parsing speeds. The respective leftmost graph in Figures 26 and 27 show
a visual representation of the number of items produced by all binarizations, and the
corresponding parsing times. Note that when choosing the head with head rules the
number of items is almost not affected by the choice of adding first the children to
the left of the head and then to the right of the head or vice versa. The optimal bina-
rization produces the best results. Therefore we will use it in all further experiments,
in spite of its higher parsing time.
6.5 Baseline Evaluation and Category Splits
Table 3 presents the constituency parsing results for NeGraLCFRS and NeGraCFG, both
with and without the different category splits. Recall that NeGraLCFRS has crossing
branches and consequently leads to a PLCFRS of fan-out > 1 whereas NeGraCFG does
not contain crossing branches and consequently leads to a 1-PLCFRS?in other words,
 0.1
 1
 10
 100
 15  17  19  21  23  25  27  29
tim
e 
(se
c.)
sentence length
Head-driven
KM
L-to-R
Optimal
Deterministic
 0.1
 1
 10
 100
 15  17  19  21  23  25  27  29
tim
e 
(se
c.)
sentence length
Baseline
NP
S
NP+S
 0.1
 1
 10
 100
 15  17  19  21  23  25  27  29
tim
e 
(se
c.)
sentence length
OFF
LR
LN
Figure 27
NeGraLCFRS: Parsing times for PLCFRS parsing (left-to-right): binarizations, baseline and
category splits, and estimates (log scale).
111
Computational Linguistics Volume 39, Number 1
Table 3
NeGraLCFRS and NeGraCFG: baseline and category splits.
w/ category splits w/ category splits
NeGraLCFRS NP S NP ? S NeGraCFG NP S NP ? S
LP 74.92 75.21 75.81 75.93 76.32 76.79 77.39 77.58
LR 74.88 74.95 75.65 75.57 76.36 77.23 77.35 77.99
LF1 74.90 75.08 75.73 75.75 76.34 77.01 77.37 77.79
UP 77.77 78.16 78.31 78.60 79.12 79.62 79.84 80.09
UR 78.73 77.88 78.15 78.22 79.17 80.08 79.80 80.52
UF1 77.75 78.02 78.23 78.41 79.14 79.85 79.82 80.30
a PCFG. We evaluate the parser output against the unmodified gold data; that is,
before we evaluate the experiments with category splits, we replace all split labels in
the parser output with the corresponding original labels.
We take a closer look at the properties of the trees in the parser output for
NeGraLCFRS. Twenty-nine sentences had no parse, therefore, the parser output has 1,804
sentences. The average tree height is 4.72, and the average number of children per node
(excluding terminals) is 2.91. These values are almost identical to the values for the gold
data. As for the gap degree, we get 1,401 sentences with no gaps (1,361 in the gold set),
334 with gap degree 1 (387 in the gold set), and 69 with 2 or 3 gaps (85 in the gold set).
Even though the difference is only small, one can see that fewer gaps are preferred. This
is not surprising, since constituents with many gaps are rare events and therefore end
up with a probability which is too low.
We see that the quality of the PLCFRS parser output on NeGraLCFRS (which contains
more information than the output of a PCFG parser) does not lag far behind the quality
of the PCFG parsing results on NeGraCFG. With respect to the category splits, the results
show furthermore that category splitting is indeed beneficial for the quality of the
PLCFRS parser output. The gains in speed are particularly visible for sentences with
a length greater than 20 words (cf. the number of produced items and parsing times in
Figures 26 and 27 [middle]).
6.6 Evaluating Outside Estimates
We compare the parser performance without estimates (OFF) with its performance
with the estimates described in Sections 4.3 (LR) and 4.4 (LN).
Unfortunately, the full estimates seem to be only of theoretical interest because they
were too expensive to compute both in terms of time and space, given the restrictions
imposed by our hardware. We could, however, compute the LN and the LR estimate.
Unlike the LN estimate, which allows for true A? parsing, the LR estimate lets the
quality of the parsing results deteriorate: Compared with the baseline, labeled F1 drops
from 74.90 to 73.76 and unlabeled F1 drops from 77.91 to 76.89. The respective rightmost
graphs in Figures 26 and 27 show the average number of items produced by the
parser and the parsing times for different sentence lengths. The results indicate that the
estimates have the desired effect of preventing unnecessary items from being produced.
This is reflected in a significantly lower parsing time.
The different behavior of the LR and the LN estimate raises the question of the
trade-off between maintaining optimality and obtaining a higher parsing speed. In
112
Kallmeyer and Maier PLCFRS Parsing
other words, it raises the question of whether techniques such as pruning or coarse-
to-fine parsing (Charniak et al 2006) would probably be superior to A? parsing. A first
implementation of a coarse-to-fine approach has been presented by van Cranenburgh
(2012). He generates a CFG from the treebank PLCFRS, based on the idea of Barthe?lemy
et al (2001). This grammar, which can be seen as a coarser version of the actual PLCFRS,
is then used for pruning of the search space. The problem that van Cranenburgh tackles
is specific to PLCFRS: His PCFG stage generalizes over the distinction of labels by their
fan-out. The merit of his work is an enormous increase in efficiency: Sentences with a
length of up to 40 words can now be parsed in a reasonable time. For a comparison of
the results of van Cranenburgh (2012) with our work, the same version of evaluation
parameters would have to be used. The applicability and effectiveness of other coarse-
to-fine approaches (Charniak et al 2006; Petrov and Klein 2007) on PLCFRS remain to
be seen.
7. Comparison to Other Approaches
Comparing our results with results from the literature is a difficult endeavor, because
PLCFRS parsing of NeGra is an entirely new task that has no direct equivalent in
previous work. In particular, it is a harder task than PCFG parsing. What we can
provide in this section is a comparison of the performance of our parser on NeGraCFG
to the performance of previously presented PCFG parsers on the same data set and
an overview on previous work on parsing which aims at reconstructing crossing
branches.
For the comparison of the performance of our parser on NeGraCFG, we have per-
formed experiments with Helmut Schmid?s LoPar (Schmid 2000) and with the Stanford
Parser (Klein and Manning 2003c) on NeGraCFG.6 For the experiments both parsers
were provided with gold POS tags. Recall that our parser produced labeled precision,
recall, and F1 of 76.32, 76.46, and 76.34, respectively. The plain PCFG provided by LoPar
delivers lower results (LP 72.86, LR 74.43, and LF1 73.63). The Stanford Parser results
(markovization setting v = 2, h = 1 [Rafferty and Manning 2008], otherwise default
parameters) lie in the vicinity of the results of our parser (LP 74.27, LR 76.19, LF1 75.45).
Although the results for LoPar are no surprise, given the similarity of the models
implemented by our parser and the Stanford parser, it remains to be investigated why
the lexicalization component of the Stanford parser does not lead to better results. In
any case the comparison shows that on a data set without crossing branches, our parser
obtains the results one would expect. A further data set to which we can provide a
comparison is the PaGe workshop experimental data (Ku?bler and Penn 2008).7 Table 4
lists the results of some of the papers in Ku?bler and Penn (2008) on TIGER, namely,
for Petrov and Klein (2008) (P&K), who use the Berkeley Parser (Petrov and Klein
2007); Rafferty and Manning (2008) (R&M), who use the Stanford parser (see above);
and Hall and Nivre (2008) (H&N), who use a dependency-based approach (see next
paragraph). The comparison again shows that our system produces good results. Again
the performance gap between the Stanford parser and our parser warrants further
investigation.
6 We have obtained the former parser from http://www.ims.uni-stuttgart.de/tcl/SOFTWARE/
LoPar.html and the latter (Version 2.0.1) from http://nlp.stanford.edu/software/lex-parser.shtml.
7 Thanks to Sandra Ku?bler for providing us with the experimental data.
113
Computational Linguistics Volume 39, Number 1
Table 4
PaGe workshop data.
here P&K R&M H&N
LP 66.93 69.23 58.52 67.06
LR 60.79 70.41 57.63 58.07
LF1 63.71 69.81 58.07 65.18
As for the work that aims to create crossing branches, Plaehn (2004) obtains 73.16
Labeled F1 using Probabilistic Discontinuous Phrase Structure Grammar (DPSG), albeit
only on sentences with a length of up to 15 words. On those sentences, we obtain 83.97.
The crucial difference between DPSG rules and LCFRS rules is that the former explicitly
specify the material that can occur in gaps whereas LCFRS does not. Levy (2005), like us,
proposes to use LCFRS but does not provide any evaluation results of his work. Very
recently, Evang and Kallmeyer (2011) followed up on our work. They transform the
Penn Treebank such that the trace nodes and co-indexations are converted into crossing
branches and parse them with the parser presented in this article, obtaining promising
results. Furthermore, van Cranenburgh, Scha, and Sangati (2011) and van Cranenburgh
(2012) have also followed up on our work, introducing an integration of our approach
with Data-Oriented Parsing (DOP). The former article introduces an LCFRS adaption
of Goodman?s PCFG-DOP (Goodman 2003). For their evaluation, the authors use the
same data as we do in Maier (2010), and obtain an improvement of roughly 1.5 points
F-measure. They are also confronted with the same efficiency issues, however, and
encounter a bottleneck in terms of parsing time. In van Cranenburgh (2012), a coarse-
to-fine approach is presented (see Section 6.6). With this approach much faster parsing
is made possible and sentences with a length of up to 40 words can be parsed. The cost
of the speed, however, is that the results lie well below the baseline results for standard
PLCFRS parsing.
A comparison with non-projective dependency parsers (McDonald et al 2005;
Nivre et al 2007) might be interesting as well, given that non-projectivity is the
dependency-counterpart to discontinuity in constituency parsing. A meaningful com-
parison is difficult to do for the following reasons, however. Firstly, dependency parsing
deals with relations between words, whereas in our case words are not considered in
the parsing task. Our grammars take POS tags for a given and construct syntactic trees.
Also, dependency conversion algorithms generally depend on the correct identification
of linguistic head words (Lin 1995). We cannot rely on grammatical function labels, such
as, for example, Boyd and Meurers (2008). Therefore we would have to use heuristics for
the dependency conversion of the parser output. This would introduce additional noise.
Secondly, the resources one obtains from our PLCFRS parser and from dependency
parsers (the probabilistic LCFRS and the trained dependency parser) are quite different
because the former contains non-lexicalized internal phrase structure identifying mean-
ingful syntactic categories such as VP or NP while the latter is only concerned with rela-
tions between lexical items. A comparison would concentrate only on relations between
lexical items and the rich phrase structure provided by a constituency parser would
not be taken into account. To achieve some comparison, one could of course transform
the discontinuous constituency trees into dependency trees with dependencies between
heads and with edge labels that encode enough of the syntactic structure to retrieve
the original constituency tree (Hall and Nivre 2008). The result could then be used for
114
Kallmeyer and Maier PLCFRS Parsing
a dependency evaluation. It is not clear what is to gain by this evaluation because
the head-to-head dependencies one would obtain are not necessarily the predicate-
argument dependencies one would aim at when doing direct dependency parsing
(Rambow 2010).8
8. Conclusion
We have presented the first efficient implementation of a weighted deductive CYK
parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS), showing
that LCFRS indeed allows for data-driven parsing while modeling discontinuities in
a straightforward way. To speed up parsing, we have introduced different context-
summary estimates of parse items, some acting as figures-of-merit, others allowing for
A? parsing. We have implemented the parser and we have evaluated it with grammars
extracted from the German NeGra treebank. Our experiments show that data-driven
LCFRS parsing is feasible and yields output of competitive quality.
There are three main directions for future work on this subject.
 On the symbolic side, LCFRS seems to offer more power than necessary.
By removing symbolic expressivity, a lower parsing complexity can be
achieved. One possibility is to disallow the use of so-called ill-nested
LCFRS rules. These are rules where, roughly, the spans of two right-hand
side non-terminals interleave in a cross-serial way. See the parsing
algorithm in Go?mez-Rodr??guez, Kuhlmann, and Satta (2010).
Nevertheless, this seems to be too restrictive for linguistic modeling
(Chen-Main and Joshi 2010; Maier and Lichte 2011). Our goal for future
work is therefore to define reduced forms of ill-nested rules with which we
get a lower parsing complexity.
Another possibility is to reduce the fan-out of the extracted grammar. We
have pursued the question whether the fan-out of the trees in the treebank
can be reduced in a linguistically meaningful way in Maier, Kaeshammer,
and Kallmeyer (2012).
 On the side of the probabilistic model, there are certain independence
assumptions made in our model that are too strong. The main problem in
respect is that, due to the definition of LCFRS, we have to distinguish
between occurrences of the same category with different fan-outs. For
instance, VP1 (no gaps), VP2 (one gap), and so on, are different
non-terminals. Consequently, the way they expand are considered
independent from each other. This is of course not true, however.
Furthermore, some of these non-terminals are rather rare; we therefore
have a sparse data problem here. This leads to the idea to separate the
development of a category (independent from its fan-out) and the fan-out
and position of gaps. We plan to integrate this into our probabilistic model
in future work.
8 A way to overcome this difference in the content of the dependency annotation would be to use
an evaluation along the lines of Tsarfaty, Nivre, and Andersson (2011); this is not available yet for
annotations with crossing branches, however.
115
Computational Linguistics Volume 39, Number 1
 Last, it is clear that a more informative evaluation of the parser output is
still necessary, particularly with respect to its performance at the task of
finding long distance dependencies and with respect to its behavior when
not provided with gold POS tags.
Acknowledgments
We are particularly grateful to Giorgio Satta
for extensive discussions of the details of the
probabilistic treebank model presented in
this paper. Furthermore, we owe a debt to
Kilian Evang who participated in the
implementation of the parser. Thanks to
Andreas van Cranenburgh for helpful
feedback on the parser implementation.
Finally, we are grateful to our three
anonymous reviewers for many valuable
and helpful comments and suggestions.
A part of the work on this paper was funded
by the German Research Foundation DFG
(Deutsche Forschungsgemeinschaft) in the
form of an Emmy Noether Grant and a
subsequent DFG research project.
References
Barthe?lemy, Franc?ois, Pierre Boullier,
Philippe Deschamp, and E?ric Villemonte
de la Clergerie. 2001. Guided parsing
of range concatenation languages.
In Proceedings of the 39th Annual Meeting
of the Association for Computational
Linguistics, pages 42?49, Toulouse.
Becker, Tilman, Aravind K. Joshi, and
Owen Rambow. 1991. Long-distance
scrambling and tree-adjoining grammars.
In Proceedings of the Fifth Conference of the
European Chapter of the Association for
Computational Linguistics, pages 21?26,
Berlin.
Boullier, Pierre. 1998a. A generalization of
mildly context-sensitive formalisms.
In Proceedings of the Fourth International
Workshop on Tree Adjoining Grammars and
Related Formalisms (TAG+4), pages 17?20,
Philadelphia, PA.
Boullier, Pierre. 1998b. A proposal for a
natural language processing syntactic
backbone. Technical Report 3342, INRIA,
Roquencourt.
Boullier, Pierre. 2000. Range concatenation
grammars. In Proceedings of the Sixth
International Workshop on Parsing
Technologies (IWPT2000), pages 53?64,
Trento.
Boyd, Adriane. 2007. Discontinuity revisited:
An improved conversion to context-free
representations. In the Linguistic
Annotation Workshop at ACL 2007,
pages 41?44, Prague.
Boyd, Adriane and Detmar Meurers.
2008. Revisiting the impact of different
annotation schemes on PCFG parsing:
A grammatical dependency evaluation.
In Proceedings of the Workshop on Parsing
German at ACL 2008, pages 24?32,
Columbus, OH.
Brants, Sabine, Stefanie Dipper, Silvia
Hansen, Wolfgang Lezius, and George
Smith. 2002. The TIGER Treebank.
In Proceedings of the 1st Workshop on
Treebanks and Linguistic Theories,
pages 24?42, Sozopol.
Bunt, Harry. 1996. Formal tools for
describing and processing discontinuous
constituency structure. In Harry Bunt and
Arthur van Horck, editors, Discontinuous
Constituency, volume 6 of Natural Language
Processing. Mouton de Gruyter, Berlin,
pages 63?83.
Cai, Shu, David Chiang, and Yoav Goldberg.
2011. Language-independent parsing
with empty elements. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies, pages 212?216, Portland, OR.
Candito, Marie and Djame? Seddah. 2010.
Parsing word clusters. In Proceedings of the
First Workshop on Statistical Parsing of
Morphologically-Rich Languages at NAACL
HLT 2010, pages 76?84, Los Angeles, CA.
Caraballo, Sharon A. and Eugene Charniak.
1998. New figures of merit for best-first
probabilistic chart parsing. Computational
Linguistics, 24(2):275?298.
Charniak, Eugene, Mark Johnson, Micha
Elsner, Joseph Austerweil, David Ellis,
Isaac Haxton, Catherine Hill, R. Shrivaths,
Jeremy Moore, Michael Pozar, and Theresa
Vu. 2006. Multilevel coarse-to-fine PCFG
parsing. In Proceedings of the Human
Language Technology Conference of the
NAACL, Main Conference, pages 168?175,
New York, NY.
Chen-Main, Joan and Aravind Joshi. 2010.
Unavoidable ill-nestedness in natural
language and the adequacy of tree
local-MCTAG induced dependency
structures. In Proceedings of the Tenth
116
Kallmeyer and Maier PLCFRS Parsing
International Workshop on Tree Adjoining
Grammar and Related Formalisms (TAG+10),
pages 119?126, New Haven, CT.
Chiang, David. 2003. Statistical parsing with
an automatically extracted tree adjoining
grammar. In Rens Bod, Remko Scha, and
Khalil Sima?an, editors, Data-Oriented
Parsing, CSLI Studies in Computational
Linguistics. CSLI Publications, Stanford,
CA, pages 299?316.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania.
Cormen, Thomas H., Charles E. Leiserson,
Ronald L. Rivest, and Clifford Stein. 2003.
Introduction to Algorithms. MIT Press,
Cambridge, 2nd edition.
Dienes, Pe?ter. 2003. Statistical Parsing with
Non-local Dependencies. Ph.D. thesis,
Saarland University.
Evang, Kilian and Laura Kallmeyer. 2011.
PLCFRS parsing of English discontinuous
constituents. In Proceedings of the 12th
International Conference on Parsing
Technologies, pages 104?116, Dublin.
Go?mez-Rodr??guez, Carlos, Marco
Kuhlmann, and Giorgio Satta. 2010.
Efficient parsing of well-nested linear
context-free rewriting systems. In Human
Language Technologies: The 2010 Annual
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 276?284, Los Angeles, CA.
Go?mez-Rodr??guez, Carlos, Marco
Kuhlmann, Giorgio Satta, and David Weir.
2009. Optimal reduction of rule length in
linear context-free rewriting systems. In
Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 539?547,
Boulder, CO.
Goodman, Joshua. 2003. Efficient parsing of
DOP with PCFG-reductions. In Rens Bod,
Remko Scha, and Khalil Sima?an, editors,
Data-Oriented Parsing, CSLI Studies in
Computational Linguistics. CSLI
Publications, Stanford, CA, pages 125?146.
Hall, Johan and Joakim Nivre. 2008.
A dependency-driven parser for
German dependency and constituency
representations. In Proceedings of the
Workshop on Parsing German at ACL 2008,
pages 47?54, Columbus, OH.
Han, Chung-hye, Na-Rae Han, and
Eon-Suk Ko. 2001. Bracketing guidelines
for Penn Korean TreeBank. Technical
Report 01-10, IRCS, University of
Pennsylvania, Philadelphia, PA.
Hockenmaier, Julia. 2003. Data and models
for Statistical Parsing with Combinatory
Categorial Grammar. Ph.D. thesis,
University of Edinburgh.
Hoffman, Beryl. 1995. Integrating ?free?
word order syntax and information
structure. In Seventh Conference of the
European Chapter of the Association for
Computational Linguistics, pages 245?251,
Dublin.
Ho?hle, Tilman. 1986. Der Begriff
?Mittelfeld??Anmerkungen u?ber die
Theorie der topologischen Felder.
In Akten des Siebten Internationalen
Germanistenkongresses 1985, Go?ttingen,
Germany.
Johnson, Mark. 2002. A simple
pattern-matching algorithm for recovering
empty nodes and their antecedents. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics,
pages 136?143, Philadelphia, PA.
Kallmeyer, Laura. 2010. Parsing Beyond
Context-Free Grammars. Springer, Berlin.
Kallmeyer, Laura and Wolfgang Maier. 2010.
Data-driven parsing with probabilistic
linear context-free rewriting systems.
In Proceedings of the 23rd International
Conference on Computational Linguistics
(COLING 2010), pages 537?545, Beijing.
Kato, Yuki, Hiroyuki Seki, and Tadao
Kasami. 2006. Stochastic multiple
context-free grammar for RNA
pseudoknot modeling. In Proceedings
of the Eighth International Workshop
on Tree Adjoining Grammar and Related
Formalisms (TAG+8), pages 57?64,
Sydney.
Klein, Dan and Christopher D. Manning.
2003a. A* Parsing: Fast exact viterbi parse
selection. In Proceedings of the 2003 Human
Language Technology Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 40?47,
Edmonton.
Klein, Dan and Christopher D. Manning.
2003b. Accurate unlexicalized parsing.
In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics,
pages 423?430, Sapporo.
Klein, Dan and Christopher D. Manning.
2003c. Fast exact inference with a factored
model for natural language parsing. In
Advances in Neural Information Processing
Systems 15 (NIPS), pages 3?10, Vancouver.
Kracht, Marcus. 2003. The Mathematics
of Language. Number 63 in Studies in
Generative Grammar. Mouton de Gruyter,
Berlin.
117
Computational Linguistics Volume 39, Number 1
Ku?bler, Sandra. 2005. How do treebank
annotation schemes influence parsing
results? Or how not to compare apples
and oranges. In Recent Advances in Natural
Language Processing 2005 (RANLP 2005),
pages 293?300, Borovets.
Ku?bler, Sandra and Gerald Penn, editors.
2008. Proceedings of the Workshop on
Parsing German at ACL 2008. Association
for Computational Linguistics,
Columbus, OH.
Kuhlmann, Marco and Giorgio Satta.
2009. Treebank grammar techniques for
non-projective dependency parsing.
In Proceedings of the 12th Conference
of the European Chapter of the Association
for Computational Linguistics,
pages 478?486, Athens.
Levy, Roger. 2005. Probabilistic Models of
Word Order and Syntactic Discontinuity.
Ph.D. thesis, Stanford University.
Levy, Roger and Christopher D. Manning.
2004. Deep dependencies from context-free
statistical parsers: Correcting the surface
dependency approximation. In Proceedings
of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main
Volume, pages 328?335, Barcelona.
Lin, Dekang. 1995. A dependency-based
method for evaluating broad-coverage
parsers. In Proceedings of the 14th
International Joint Conference on Artificial
Intelligence (IJCAI 95), pages 1420?1427,
Montreal.
Maier, Wolfgang. 2010. Direct parsing of
discontinuous constituents in German.
In Proceedings of the First Workshop on
Statistical Parsing of Morphologically-Rich
Languages at NAACL HLT 2010,
pages 58?66, Los Angeles, CA.
Maier, Wolfgang, Miriam Kaeshammer,
and Laura Kallmeyer. 2012. Data-driven
PLCFRS parsing revisited: Restricting
the fan-out to two. In Proceedings of the
Eleventh International Conference on Tree
Adjoining Grammars and Related Formalisms
(TAG+11), pages 126?134, Paris.
Maier, Wolfgang and Laura Kallmeyer. 2010.
Discontinuity and non-projectivity: Using
mildly context-sensitive formalisms for
data-driven parsing. In Proceedings of the
Tenth International Workshop on Tree
Adjoining Grammars and Related
Formalisms (TAG+10), New Haven, CT.
Maier, Wolfgang and Timm Lichte. 2011.
Characterizing discontinuity in constituent
treebanks. In Formal Grammar. 14th
International Conference, FG 2009. Bordeaux,
France, July 25-26, 2009. Revised Selected
Papers, volume 5591 of Lecture Notes in
Artificial Intelligence, pages 167?182,
Springer-Verlag, Berlin/Heidelberg/
New York.
Maier, Wolfgang and Anders S?gaard. 2008.
Treebanks and mild context-sensitivity.
In Proceedings of the 13th Conference on
Formal Grammar (FG-2008), pages 61?76,
Hamburg.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre,
Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The Penn
Treebank: Annotating predicate argument
structure. In Proceedings of the Human
Language Technology Conference,
pages 114?119.
McDonald, Ryan, Fernando Pereira,
Kiril Ribarov, and Jan Hajic?. 2005.
Non-projective dependency parsing using
spanning tree algorithms. In Proceedings of
Human Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing (HLT/EMNLP),
pages 523?530, Vancouver.
Michaelis, Jens. 2001. On Formal Properties
of Minimalist Grammars. Ph.D. thesis,
Universita?t Potsdam.
Mu?ller, Gereon. 2002. Free word order,
morphological case, and sympathy theory.
In Gisbert Fanselow and Caroline Fery,
editors, Resolving Conflicts in Grammars:
Optimality Theory in Syntax, Morphology,
and Phonology. Buske Verlag, Hamburg,
pages 265?397.
Mu?ller, Stefan. 2004. Continuous or
discontinuous constituents? Research on
Language & Computation, 2(2):209?257.
Nederhof, Mark-Jan. 2003. Weighted
deductive parsing and knuth?s algorithm.
Computational Linguistics, 29(1):135?143.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Atanas Chanev, Gu?lsen Eryigit,
Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser:
A language-independent system for
data-driven dependency parsing. Natural
Language Engineering, 13(2):95?135.
Osenova, Petya and Kiril Simov. 2004.
BTB-TR05: BulTreebank Stylebook.
Technical Report 05, BulTreeBank
Project, Sofia, Bulgaria.
Pereira, Fernando C. N. and David Warren.
1983. Parsing as deduction. In Proceedings
of the 21st Annual Meeting of the Association
for Computational Linguistics, pages 137?144,
Cambridge, MA.
Petrov, Slav and Dan Klein. 2007. Improved
inference for unlexicalized parsing.
118
Kallmeyer and Maier PLCFRS Parsing
In Human Language Technologies 2007: The
Conference of the North American Chapter of
the Association for Computational Linguistics;
Proceedings of the Main Conference,
pages 404?411, Rochester, NY.
Petrov, Slav and Dan Klein. 2008. Parsing
German with latent variable grammars.
In Proceedings of the Workshop on Parsing
German at ACL 2008, pages 24?32,
Columbus, OH.
Plaehn, Oliver. 2004. Computing the most
probable parse for a discontinuous
phrase-structure grammar. In Harry Bunt,
John Carroll, and Giorgio Satta, editors,
New Developments in Parsing Technology,
volume 23 of Text, Speech And Language
Technology. Kluwer, Dordrecht,
pages 91?106.
Rafferty, Anna and Christopher D. Manning.
2008. Parsing three German treebanks:
Lexicalized and unlexicalized baselines.
In Proceedings of the Workshop on Parsing
German at ACL 2008, pages 40?46,
Columbus, OH.
Rambow, Owen. 2010. The simple
truth about dependency and phrase
structure representations: An opinion
piece. In Human Language Technologies:
The 2010 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 337?340,
Los Angeles, CA.
Rehbein, Ines and Josef van Genabith.
2007. Evaluating evaluation measures.
In Proceedings of the 16th Nordic
Conference of Computational Linguistics,
pages 372?379, Tartu.
Schmid, Helmut. 2000. LoPar: Design and
implementation. Arbeitspapiere des
Sonderforschungsbereiches 340 149,
IMS, University of Stuttgart, Stuttgart,
Germany.
Seddah, Djame, Sandra Ku?bler, and Reut
Tsarfaty, editors. 2010. Proceedings of the
First Workshop on Statistical Parsing of
Morphologically-Rich Languages at
NAACL HLT 2010. Association for
Computational Linguistics,
Los Angeles, CA.
Seki, Hiroyuki, Takahashi Matsumura,
Mamoru Fujii, and Tadao Kasami. 1991.
On multiple context-free grammars.
Theoretical Computer Science, 88(2):191?229.
Shieber, Stuart M., Yves Schabes, and
Fernando C. N. Pereira. 1995. Principles
and implementation of deductive parsing.
Journal of Logic Programming, 24(1?2):3?36.
Sikkel, Klaas. 1997. Parsing Schemata. Texts in
Theoretical Computer Science. Springer,
Berlin, Heidelberg, New York.
Skut, Wojciech, Brigitte Krenn, Thorten
Brants, and Hans Uszkoreit. 1997. An
annotation scheme for free word order
languages. In Proceedings of the Fifth
Conference on Applied Natural Language
Processing (ANLP), pages 88?95,
Washington, DC.
Telljohann, Heike, Erhard W. Hinrichs,
Sandra Ku?bler, Heike Zinsmeister, and
Kathrin Beck. 2012. Stylebook for the
Tu?bingen Treebank of Written German
(Tu?Ba-D/Z). Technical report, Seminar fu?r
Sprachwissenschaft, Universita?t Tu?bingen,
Tu?bingen, Germany. http://www.sfs.
uni.tuebingen.de/resources/tuebadz-
stylebook-1201.pdf.
Tsarfaty, Reut, Joakim Nivre, and Evelina
Andersson. 2011. Evaluating dependency
parsing: Robust and heuristics-free
cross-annotation evaluation. In Proceedings
of the 2011 Conference on Empirical Methods
in Natural Language Processing,
pages 385?396, Edinburgh.
Uszkoreit, Hans. 1986. Linear precedence
in discontinuous constituents: Complex
fronting in German. CSLI report
CSLI-86-47, Center for the Study of
Language and Information, Stanford
University, Stanford, CA.
van Cranenburgh, Andreas. 2012. Efficient
parsing with linear context-free rewriting
systems. In Proceedings of the 13th
Conference of the European Chapter of the
Association for Computational Linguistics,
pages 460?470, Avignon.
van Cranenburgh, Andreas, Remko Scha,
and Federico Sangati. 2011. Discontinuous
data-oriented parsing: A mildly
context-sensitive all-fragments grammar.
In Proceedings of the Second Workshop on
Statistical Parsing of Morphologically Rich
Languages (SPMRL 2011), pages 34?44,
Dublin.
Versley, Yannick. 2005. Parser evaluation
across text types. In Proceedings of the
Fourth Workshop on Treebanks and Linguistic
Theories, pages 209?220, Barcelona, Spain.
Vijay-Shanker, K., David J. Weir, and
Aravind K. Joshi. 1987. Characterizing
structural descriptions produced by
various grammatical formalisms. In
Proceedings of the 25th Annual Meeting of the
Association for Computational Linguistics,
pages 104?111, Stanford, CA.
119

Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 1?8
Manchester, August 2008
TuLiPA: Towards a Multi-Formalism Parsing Environment for
Grammar Engineering
Laura Kallmeyer
SFB 441
Universita?t Tu?bingen
D-72074, Tu?bingen, Germany
lk@sfs.uni-tuebingen.de
Yannick Parmentier
CNRS - LORIA
Nancy Universite?
F-54506, Vand?uvre, France
parmenti@loria.fr
Timm Lichte
SFB 441
Universita?t Tu?bingen
D-72074, Tu?bingen, Germany
timm.lichte@uni-tuebingen.de
Johannes Dellert
SFB 441 - SfS
Universita?t Tu?bingen
D-72074, Tu?bingen, Germany
{jdellert,kevang}@sfs.uni-tuebingen.de
Wolfgang Maier
SFB 441
Universita?t Tu?bingen
D-72074, Tu?bingen, Germany
wo.maier@uni-tuebingen.de
Kilian Evang
SFB 441 - SfS
Universita?t Tu?bingen
D-72074, Tu?bingen, Germany
Abstract
In this paper, we present an open-source
parsing environment (Tu?bingen Linguistic
Parsing Architecture, TuLiPA) which uses
Range Concatenation Grammar (RCG)
as a pivot formalism, thus opening the
way to the parsing of several mildly
context-sensitive formalisms. This en-
vironment currently supports tree-based
grammars (namely Tree-Adjoining Gram-
mars (TAG) and Multi-Component Tree-
Adjoining Grammars with Tree Tuples
(TT-MCTAG)) and allows computation not
only of syntactic structures, but also of the
corresponding semantic representations. It
is used for the development of a tree-based
grammar for German.
1 Introduction
Grammars and lexicons represent important lin-
guistic resources for many NLP applications,
among which one may cite dialog systems, auto-
matic summarization or machine translation. De-
veloping such resources is known to be a complex
task that needs useful tools such as parsers and
generators (Erbach, 1992).
Furthermore, there is a lack of a common frame-
work allowing for multi-formalism grammar engi-
neering. Thus, many formalisms have been pro-
posed to model natural language, each coming
with specific implementations. Having a com-
mon framework would facilitate the comparison
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
between formalisms (e.g., in terms of parsing com-
plexity in practice), and would allow for a better
sharing of resources (e.g., having a common lex-
icon, from which different features would be ex-
tracted depending on the target formalism).
In this context, we present a parsing environ-
ment relying on a general architecture that can
be used for parsing with mildly context-sensitive
(MCS) formalisms1 (Joshi, 1987). Its underly-
ing idea is to use Range Concatenation Grammar
(RCG) as a pivot formalism, for RCG has been
shown to strictly include MCS languages while be-
ing parsable in polynomial time (Boullier, 2000).
Currently, this architecture supports tree-based
grammars (Tree-Adjoining Grammars and Multi-
Component Tree-Adjoining Grammars with Tree
Tuples (Lichte, 2007)). More precisely, tree-
based grammars are first converted into equivalent
RCGs, which are then used for parsing. The result
of RCG parsing is finally interpreted to extract a
derivation structure for the input grammar, as well
as to perform additional processings (e.g., seman-
tic calculus, extraction of dependency views).
The paper is structured as follows. In section 2,
we present the architecture of the TuLiPA parsing
environment and show how the use of RCG as a
pivot formalism makes it easier to design a modu-
lar system that can be extended to support several
dimensions (syntax, semantics) and/or formalisms.
In section 3, we give some desiderata for gram-
mar engineering and present TuLiPA?s current state
1A formalism is said to be mildly context sensitive (MCS)
iff (i) it generates limited cross-serial dependencies, (ii) it is
polynomially parsable, and (iii) the string languages gener-
ated by the formalism have the constant growth property (e.g.,
{a
2
n
|n ? 0} does not have this property). Examples of MCS
formalisms include Tree-Adjoining Grammars, Combinatory
Categorial Grammars and Linear Indexed Grammars.
1
with respect to these. In section 4, we compare
this system with existing approaches for parsing
and more generally for grammar engineering. Fi-
nally, in section 5, we conclude by presenting fu-
ture work.
2 Range Concatenation Grammar as a
pivot formalism
The main idea underlying TuLiPA is to use RCG
as a pivot formalism for RCG has appealing for-
mal properties (e.g., a generative capacity lying be-
yond Linear Context Free Rewriting Systems and
a polynomial parsing complexity) and there ex-
ist efficient algorithms, for RCG parsing (Boullier,
2000) and for grammar transformation into RCG
(Boullier, 1998; Boullier, 1999).
Parsing with TuLiPA is thus a 3-step process:
1. The input tree-based grammar is converted
into an RCG (using the algorithm of
Kallmeyer and Parmentier (2008) when deal-
ing with TT-MCTAG).
2. The resulting RCG is used for parsing the in-
put string using an extension of the parsing
algorithm of Boullier (2000).
3. The RCG derivation structure is interpreted to
extract the derivation and derived trees with
respect to the input grammar.
The use of RCG as a pivot formalism, and thus
of an RCG parser as a core component of the sys-
tem, leads to a modular architecture. In turns, this
makes TuLiPA more easily extensible, either in
terms of functionalities, or in terms of formalisms.
2.1 Adding functionalities to the parsing
environment
As an illustration of TuLiPA?s extensibility, one
may consider two extensions applied to the system
recently.
First, a semantic calculus using the syn-
tax/semantics interface for TAG proposed by Gar-
dent and Kallmeyer (2003) has been added. This
interface associates each tree with flat semantic
formulas. The arguments of these formulas are
unification variables, which are co-indexed with
features labelling the nodes of the syntactic tree.
During classical TAG derivation, trees are com-
bined, triggering unifications of the feature struc-
tures labelling nodes. As a result of these unifica-
tions, the arguments of the semantic formulas are
unified (see Fig. 1).
S
NP?x VP
NP
j
V NP?y NP
m
John loves Mary
name(j,john) love(x,y) name(m,mary)
; love(j,m),name(j,john),name(m,mary)
Figure 1: Semantic calculus in Feature-Based
TAG.
In our system, the semantic support has been in-
tegrated by (i) extending the internal tree objects to
include semantic formulas (the RCG-conversion is
kept unchanged), and (ii) extending the construc-
tion of the derived tree (step 3) so that during the
interpretation of the RCG derivation in terms of
tree combinations, the semantic formulas are car-
ried and updated with respect to the feature unifi-
cations performed.
Secondly, let us consider lexical disambigua-
tion. Because of the high redundancy lying within
lexicalized formalisms such as lexicalized TAG,
it is common to consider tree schemata having a
frontier node marked for anchoring (i.e., lexical-
ization). At parsing time, the tree schemata are
anchored according to the input string. This an-
choring selects a subgrammar supposed to cover
the input string. Unfortunately, this subgrammar
may contain many trees that either do not lead to
a parse or for which we know a priori that they
cannot be combined within the same derivation
(so we should not predict a derivation from one
of these trees to another during parsing). As a re-
sult, the parser could have poor performance be-
cause of the many derivation paths that have to
be explored. Bonfante et al (2004) proposed to
polarize the structures of the grammar, and to ap-
ply an automaton-based filtering of the compatible
structures. The idea is the following. One compute
polarities representing the needs/resources brought
by a given tree (or tree tuple for TT-MCTAG).
A substitution or foot node with category NP re-
flects a need for an NP (written NP-). In the same
way, an NP root node reflects a resource of type
NP (written NP+). Then you build an automaton
whose edges correspond to trees, and states to po-
larities brought by trees along the path. The au-
tomaton is then traversed to extract all paths lead-
ing to a final state with a neutral polarity for each
category and +1 for the axiom (see Fig. 2, the state
2
7 is the only valid state and {proper., trans., det.,
noun.} the only compatible set of trees).
0
John
1 1
eats
2 2
a
3 3
cake
4
0 1
NP+
2
S+
3
S+ NP-
4
S+
5
S+ NP-
6
S+ NP+
7
S+
proper.
intrans.
trans.
det.
det.
noun.
noun.
Figure 2: Polarity-based lexical disambiguation.
In our context, this polarity filtering has been
added before step 1, leaving untouched the core
RCG conversion and parsing steps. The idea is
to compute the sets of compatible trees (or tree
tuples for TT-MCTAG) and to convert these sets
separately. Indeed the RCG has to encode only
valid adjunctions/substitutions. Thanks to this
automaton-based ?clustering? of the compatible
tree (or tree tuples), we avoid predicting incompat-
ible derivations. Note that the time saved by using
a polarity-based filter is not negligible, especially
when parsing long sentences.2
2.2 Adding formalisms to the parsing
environment
Of course, the two extensions introduced in the
previous section may have been added to other
modular architectures as well. The main gain
brought by RCG is the possibility to parse not
only tree-based grammars, but other formalisms
provided they can be encoded into RCG. In our
system, only TAG and TT-MCTAG have been
considered so far. Nonetheless, Boullier (1998)
and S?gaard (2007) have defined transformations
into RCG for other mildly context-sensitive for-
malisms.3
To sum up, the idea would be to keep the core
RCG parser, and to extend TuLiPA with a specific
conversion module for each targeted formalism.
On top of these conversion modules, one should
also provide interpretation modules allowing to de-
code the RCG derivation forest in terms of the in-
put formalism (see Fig. 3).
2An evaluation of the gain brought by this technique when
using Interaction Grammar is given by Bonfante et al (2004).
3These include Multi-Component Tree-Adjoining Gram-
mar, Linear Indexed Grammar, Head Grammar, Coupled
Context Free Grammar, Right Linear Unification Grammar
and Synchronous Unification Grammar.
Figure 3: Towards a multi-formalism parsing envi-
ronment.
An important point remains to be discussed. It
concerns the role of lexicalization with respect to
the formalism used. Indeed, the tree-based gram-
mar formalisms currently supported (TAG and TT-
MCTAG) both share the same lexicalization pro-
cess (i.e., tree anchoring). Thus the lexicon format
is common to these formalisms. As we will see
below, it corresponds to a 2-layer lexicon made of
inflected forms and lemma respectively, the latter
selecting specific grammatical structures. When
parsing other formalisms, it is still unclear whether
one can use the same lexicon format, and if not
what kind of general lexicon management module
should be added to the parser (in particular to deal
with morphology).
3 Towards a complete grammar
engineering environment
So far, we have seen how to use a generic parsing
architecture relying on RCG to parse different for-
malisms. In this section, we adopt a broader view
and enumerate some requirements for a linguistic
resource development environment. We also see
to what extent these requirements are fulfilled (or
partially fulfilled) within the TuLiPA system.
3.1 Grammar engineering with TuLiPA
As advocated by Erbach (1992), grammar en-
gineering needs ?tools for testing the grammar
with respect to consistency, coverage, overgener-
ation and accuracy?. These characteristics may
be taken into account by different interacting soft-
ware. Thus, consistency can be checked by a semi-
automatic grammar production device, such as the
XMG system of Duchier et al (2004). Overgen-
eration is mainly checked by a generator (or by
a parser with adequate test suites), and coverage
and accuracy by a parser. In our case, the TuLiPA
system provides an entry point for using a gram-
mar production system (and a lexicon conversion
3
tool introduced below), while including a parser.
Note that TuLiPA does not include any generator,
nonetheless it uses the same lexicon format as the
GenI surface realizer for TAG4.
TuLiPA?s input grammar is designed using
XMG, which is a metagrammar compiler for tree-
based formalisms. In other terms, the linguist de-
fines a factorized description of the grammar (the
so-called metagrammar) in the XMG language.
Briefly, an XMG metagrammar consists of (i) ele-
mentary tree fragments represented as tree descrip-
tion logic formulas, and (ii) conjunctive and dis-
junctive combinations of these tree fragments to
describe actual TAG tree schemata.5 This meta-
grammar is then compiled by the XMG system to
produce a tree grammar in an XML format. Note
that the resulting grammar contains tree schemata
(i.e., unlexicalized trees). To lexicalize these, the
linguist defines a lexicon mapping words with cor-
responding sets of trees. Following XTAG (2001),
this lexicon is a 2-layer lexicon made of morpho-
logical and lemma specifications. The motivation
of this 2-layer format is (i) to express linguistic
generalizations at the lexicon level, and (ii) to al-
low the parser to only select a subgrammar accord-
ing to a given sentence, thus reducing parsing com-
plexity. TuLiPA comes with a lexicon conversion
tool (namely lexConverter) allowing to write a lex-
icon in a user-friendly text format and to convert it
into XML. An example of an entry of such a lexi-
con is given in Fig. 4.
The morphological specification consists of a
word, the corresponding lemma and morphologi-
cal features. The main pieces of information con-
tained in the lemma specification are the ?ENTRY
field, which refers to the lemma, the ?CAT field
referring to the syntactic category of the anchor
node, the ?SEM field containing some semantic in-
formation allowing for semantic instantiation, the
?FAM field, which contains the name of the tree
family to be anchored, the ?FILTERS field which
consists of a feature structure constraining by uni-
fication the trees of a given family that can be
anchored by the given lemma (used for instance
for non-passivable verbs), the ?EQUATIONS field
allowing for the definition of equations targeting
named nodes of the trees, and the ?COANCHORS
field, which allows for the specification of co-
anchors (such as by in the verb to come by).
4http://trac.loria.fr/?geni
5See (Crabbe?, 2005) for a presentation on how to use the
XMG formalism for describing a core TAG for French.
Morphological specification:
vergisst vergessen [pos=v,num=sg,per=3]
Lemma specification:
?ENTRY: vergessen
?CAT: v
?SEM: BinaryRel[pred=vergessen]
?ACC: 1
?FAM: Vnp2
?FILTERS: []
?EX:
?EQUATIONS:
NParg1 ? cas = nom
NParg2 ? cas = acc
?COANCHORS:
Figure 4: Morphological and lemma specification
of vergisst.
From these XML resources, TuLiPA parses a
string, corresponding either to a sentence or a con-
stituent (noun phrase, prepositional phrase, etc.),
and computes several output pieces of informa-
tion, namely (for TAG and TT-MCTAG): deriva-
tion/derived trees, semantic representations (com-
puted from underspecified representations using
the utool software6, or dependency views of the
derivation trees (using the DTool software7).
3.2 Grammar debugging
The engineering process introduced in the preced-
ing section belongs to a development cycle, where
one first designs a grammar and corresponding
lexicons using XMG, then checks these with the
parser, fixes them, parses again, and so on.
To facilitate grammar debugging, TuLiPA in-
cludes both a verbose and a robust mode allow-
ing respectively to (i) produce a log of the RCG-
conversion, RCG-parsing and RCG-derivation in-
terpretation, and (ii) display mismatching features
leading to incomplete derivations. More precisely,
in robust mode, the parser displays derivations step
by step, highlighting feature unification failures.
TuLiPA?s options can be activated via an intu-
itive Graphical User Interface (see Fig. 5).
6See http://www.coli.uni-saarland.de/
projects/chorus/utool/, with courtesy of Alexander
Koller.
7With courtesy of Marco Kuhlmann.
4
Figure 5: TuLiPA?s Graphical User Interface.
3.3 Towards a functional common interface
Unfortunately, as mentioned above, the linguist
has to move back-and-forth from the gram-
mar/lexicon descriptions to the parser, i.e., each
time the parser reports grammar errors, the linguist
fixes these and then recomputes the XML files and
then parses again. To avoid this tedious task of re-
sources re-compilation, we started developing an
Eclipse8 plug-in for the TuLiPA system. Thus, the
linguist will be able to manage all these resources,
and to call the parser, the metagrammar compiler,
and the lexConverter from a common interface (see
Fig. 6).
Figure 6: TuLiPA?s eclipse plug-in.
The motivation for this plug-in comes from
the observation that designing electronic gram-
mars is a task comparable to designing source
8See http://www.eclipse.org
code. A powerful grammar engineering environ-
ment should thus come with development facili-
ties such as precise debugging information, syntax
highlighting, etc. Using the Eclipse open-source
development platform allows for reusing several
components inherited from the software develop-
ment community, such as plug-ins for version con-
trol, editors coupled with explorers, etc.
Eventually, one point worth considering in the
context of grammar development concerns data en-
coding. To our knowledge, only few environments
provide support for UTF-8 encoding, thus guaran-
tying the coverage of a wide set of charsets and
languages. In TuLiPA, we added an UTF-8 sup-
port (in the lexConverter), thus allowing to design
a TAG for Korean (work in progress).
3.4 Usability of the TuLiPA system
As mentioned above, the TuLiPA system is made
of several interacting components, that one cur-
rently has to install separately. Nonetheless, much
attention has been paid to make this installation
process as easy as possible and compatible with
all major platforms.9
XMG and lexConverter can be installed by com-
piling their sources (using a make command).
TuLiPA is developed in Java and released as an ex-
ecutable jar. No compilation is needed for it, the
only requirement is the Gecode/GecodeJ library10
(available as a binary package for many platforms).
Finally, the TuLiPA eclipse plug-in can be installed
easily from eclipse itself. All these tools are re-
leased under Free software licenses (either GNU
GPL or Eclipse Public License).
This environment is being used (i) at the Univer-
sity of Tu?bingen, in the context of the development
of a TT-MCTAG for German describing both syn-
tax and semantics, and (ii) at LORIA Nancy, in the
development of an XTAG-based metagrammar for
English. The German grammar, called GerTT (for
German Tree Tuples), is released under a LGPL li-
cense for Linguistic Resources11 and is presented
in (Kallmeyer et al, 2008). The test-suite cur-
rently used to check the grammar is hand-crafted.
A more systematic evaluation of the grammar is in
preparation, using the Test Suite for Natural Lan-
guage Processing (Lehmann et al, 1996).
9See http://sourcesup.cru.fr/tulipa.
10See http://www.gecode.org/gecodej.
11See http://infolingu.univ-mlv.
fr/DonneesLinguistiques/
Lexiques-Grammaires/lgpllr.html
5
4 Comparison with existing approaches
4.1 Engineering environments for tree-based
grammar formalisms
To our knowledge, there is currently no available
parsing environment for multi-component TAG.
Existing grammar engineering environments for
TAG include the DyALog system12 described in
Villemonte de la Clergerie (2005). DyALog is a
compiler for a logic programming language using
tabulation and dynamic programming techniques.
This compiler has been used to implement efficient
parsing algorithms for several formalisms, includ-
ing TAG and RCG. Unfortunately, it does not in-
clude any built-in GUI and requires a good know-
ledge of the GNU build tools to compile parsers.
This makes it relatively difficult to use. DyALog?s
main quality lies in its efficiency in terms of pars-
ing time and its capacity to handle very large re-
sources. Unlike TuLiPA, it does not compute se-
mantic representations.
The closest approach to TuLiPA corresponds to
the SemTAG system13, which extends TAG parsers
compiled with DyALog with a semantic calculus
module (Gardent and Parmentier, 2007). Unlike
TuLiPA, this system only supports TAG, and does
not provide any graphical output allowing to easily
check the result of parsing.
Note that, for grammar designers mainly inter-
ested in TAG, SemTAG and TuLiPA can be seen
as complementary tools. Indeed, one may use
TuLiPA to develop the grammar and check spe-
cific syntactic structures thanks to its intuitive pars-
ing environment. Once the grammar is stable, one
may use SemTAG in batch processing to parse
corpuses and build semantic representations using
large grammars. This combination of these 2 sys-
tems is made easier by the fact that both use the
same input formats (a metagrammar in the XMG
language and a text-based lexicon). This approach
is the one being adopted for the development of a
French TAG equipped with semantics.
For Interaction Grammar (Perrier, 2000), there
exists an engineering environment gathering the
XMG metagrammar compiler and an eLEtrOstatic
PARser (LEOPAR).14 This environment is be-
ing used to develop an Interaction Grammar for
French. TuLiPA?s lexical disambiguation module
12See http://dyalog.gforge.inria.fr
13See http://trac.loria.fr/?semconst
14See http://www.loria.fr/equipes/
calligramme/leopar/
reuses techniques introduced by LEOPAR. Unlike
TuLiPA, LEOPAR does not currently support se-
mantic information.
4.2 Engineering environments for other
grammar formalisms
For other formalisms, there exist state-of-the-art
grammar engineering environments that have been
used for many years to design large deep grammars
for several languages.
For Lexical Functional Grammar, one may cite
the Xerox Linguistic Environment (XLE).15 For
Head-driven Phrase Structure Grammar, the main
available systems are the Linguistic Knowledge
Base (LKB)16 and the TRALE system.17 For
Combinatory Categorial Grammar, one may cite
the OpenCCG library18 and the C&C parser.19
These environments have been used to develop
broad-coverage resources equipped with semantics
and include both a generator and a parser. Un-
like TuLiPA, they represent advanced projects, that
have been used for dialog and machine translation
applications. They are mainly tailored for a spe-
cific formalism.20
5 Future work
In this section, we give some prospective views
concerning engineering environments in general,
and TuLiPA in particular. We first distinguish be-
tween 2 main usages of grammar engineering en-
vironments, namely a pedagogical usage and an
application-oriented usage, and finally give some
comments about multi-formalism.
5.1 Pedagogical usage
Developing grammars in a pedagogical context
needs facilities allowing for inspection of the struc-
tures of the grammar, step-by-step parsing (or gen-
eration), along with an intuitive interface. The idea
is to abstract away from technical aspects related to
implementation (intermediate data structures, opti-
mizations, etc.).
15See http://www2.parc.com/isl/groups/
nltt/xle/
16See http://wiki.delph-in.net/moin
17See http://milca.sfs.uni-tuebingen.de/
A4/Course/trale/
18See http://openccg.sourceforge.net/
19See http://svn.ask.it.usyd.edu.au/trac/
candc/wiki
20Nonetheless, Beavers (2002) encoded a CCG in the
LKB?s Type Description Language.
6
The question whether to provide graphical or
text-based editors can be discussed. As advo-
cated by Baldridge et al (2007), a low-level text-
based specification can offer more flexibility and
bring less frustration to the grammar designer, es-
pecially when such a specification can be graph-
ically interpreted. This is the approach chosen
by XMG, where the grammar is defined via an
(advanced or not) editor such as gedit or emacs.
Within TuLiPA, we chose to go further by using
the Eclipse platform. Currently, it allows for dis-
playing a summary of the content of a metagram-
mar or lexicon on a side panel, while editing these
on a middle panel. These two panels are linked
via a jump functionality. The next steps concern
(i) the plugging of a graphical viewer to display
the (meta)grammar structures independently from
a given parse, and (ii) the extension of the eclipse
plug-in so that one can easily consistently modify
entries of the metagrammar or lexicon (especially
when these are split over several files).
5.2 Application-oriented usage
When dealing with applications, one may demand
more from the grammar engineering environment,
especially in terms of efficiency and robustness
(support for larger resources, partial parsing, etc.).
Efficiency needs optimizations in the parsing
engine making it possible to support grammars
containing several thousands of structures. One
interesting question concerns the compilation of a
grammar either off-line or on-line. In DyALog?s
approach, the grammar is compiled off-line into
a logical automaton encoding all possible deriva-
tions. This off-line compilation can take some
minutes with a TAG having 6000 trees, but the re-
sulting parser can parse sentences within a second.
In TuLiPA?s approach, the grammar is compiled
into an RCG on-line. While giving satisfactory re-
sults on reduced resources21 , it may lead to trou-
bles when scaling up. This is especially true for
TAG (the TT-MCTAG formalism is by definition a
factorized formalism compared with TAG). In the
future, it would be useful to look for a way to pre-
compile a TAG into an RCG off-line, thus saving
the conversion time.
Another important feature of grammar engineer-
ing environments consists of its debugging func-
21For a TT-MCTAG counting about 300 sets of trees and an
and-crafted lexicon made of about 300 of words, a 10-word
sentence is parsed (and a semantic representation computed)
within seconds.
tionalities. Among these, one may cite unit and
integration testing. It would be useful to extend
the TuLiPA system to provide a module for gen-
erating test-suites for a given grammar. The idea
would be to record the coverage and analyses of
a grammar at a given time. Once the grammar is
further developed, these snapshots would allow for
regression testing.
5.3 About multi-formalism
We already mentioned that TuLiPA was opening
a way towards multi-formalism by relying on an
RCG core. It is worth noticing that the XMG
system was also designed to be further extensi-
ble. Indeed, a metagrammar in XMG corresponds
to the combination of elementary structures. One
may think of designing a library of such structures,
these would be dependent on the target gram-
mar formalism. The combinations may represent
general linguistic concepts and would be shared
by different grammar implementations, following
ideas presented by Bender et al (2005).
6 Conclusion
In this paper, we have presented a multi-formalism
parsing architecture using RCG as a pivot formal-
ism to parse mildly context-sensitive formalisms
(currently TAG and TT-MCTAG). This system has
been designed to facilitate grammar development
by providing user-friendly interfaces, along with
several functionalities (e.g., dependency extrac-
tion, derivation/derived tree display and semantic
calculus). It is currently used for developing a core
grammar for German.
At the moment, we are working on the extension
of this architecture to include a fully functional
Eclipse plug-in. Other current tasks concern op-
timizations to support large scale parsing and the
extension of the syntactic and semantic coverage
of the German grammar under development.
In a near future, we plan to evaluate the parser
and the German grammar (parsing time, correction
of syntactic and semantic outputs) with respect to
a standard test-suite such as the TSNLP (Lehmann
et al, 1996).
Acknowledgments
This work has been supported by the Deutsche
Forschungsgemeinschaft (DFG) and the Deutscher
Akademischer Austausch Dienst (DAAD, grant
7
A/06/71039). We are grateful to three anonymous
reviewers for valuable comments on this work.
References
Baldridge, Jason, Sudipta Chatterjee, Alexis Palmer,
and Ben Wing. 2007. DotCCG and VisCCG: Wiki
and programming paradigms for improved grammar
engineering with OpenCCG. In King, Tracy Hol-
loway and Emily M. Bender, editors, Proceedings of
the GEAF07 workshop, pages 5?25, Stanford, CA.
CSLI.
Beavers, John. 2002. Documentation: A CCG Imple-
mentation for the LKB. LinGO Working Paper No.
2002-08, CSLI, Stanford University, Stanford, CA.
Bender, Emily, Dan Flickinger, Frederik Fouvry, and
Melanie Siegel. 2005. Shared representation in mul-
tilingual grammar engineering. Research on Lan-
guage & Computation, 3(2):131?138.
Bonfante, Guillaume, Bruno Guillaume, and Guy Per-
rier. 2004. Polarization and abstraction of grammat-
ical formalisms as methods for lexical disambigua-
tion. In Proceedings of the International Conference
on Computational Linguistics (CoLing 2004), pages
303?309, Geneva, Switzerland.
Boullier, Pierre. 1998. Proposal for a natural lan-
guage processing syntactic backbone. Rapport de
Recherche 3342, INRIA.
Boullier, Pierre. 1999. On TAG and Multicomponent
TAG Parsing. Rapport de Recherche 3668, INRIA.
Boullier, Pierre. 2000. Range concatenation gram-
mars. In Proceedings of the International Workshop
on Parsing Technologies (IWPT 2000), pages 53?64,
Trento, Italy.
Crabbe?, Benoit. 2005. Grammatical development with
XMG. In Proceedings of the conference on Logical
Aspects of Computational Linguistics 2005 (LACL
05), pages 84?100, Bordeaux, France.
Duchier, Denys, Joseph Le Roux, and Yannick Parmen-
tier. 2004. The Metagrammar Compiler: An NLP
Application with a Multi-paradigm Architecture. In
Proceedings of the 2nd International Mozart/Oz
Conference (MOZ?2004), pages 175?187, Charleroi,
Belgium.
Erbach, Gregor. 1992. Tools for grammar engineer-
ing. In 3rd Conference on Applied Natural Lan-
guage Processing, pages 243?244, Trento, Italy.
Gardent, Claire and Laura Kallmeyer. 2003. Semantic
Construction in FTAG. In Proceedings of the Con-
ference of the European chapter of the Association
for Computational Linguistics (EACL 2003), pages
123?130, Budapest, Hungary.
Gardent, Claire and Yannick Parmentier. 2007. Sem-
tag: a platform for specifying tree adjoining gram-
mars and performing tag-based semantic construc-
tion. In Proceedings of the International Confer-
ence of the Association for Computational Linguis-
tics (ACL 2007), Companion Volume Proceedings of
the Demo and Poster Sessions, pages 13?16, Prague,
Czech Republic.
Joshi, Aravind K. 1987. An introduction to Tree Ad-
joining Grammars. In Manaster-Ramer, A., editor,
Mathematics of Language, pages 87?114. John Ben-
jamins, Amsterdam.
Kallmeyer, Laura and Yannick Parmentier. 2008. On
the relation between Multicomponent Tree Adjoin-
ing Grammars with Tree Tuples (TT-MCTAG) and
Range Concatenation Grammars (RCG). In Pro-
ceedings of the 2nd International Conference on
Language and Automata Theories and Applications
(LATA 2008), pages 277?288, Tarragona, Spain.
Kallmeyer, Laura, Timm Lichte, Wolfgang Maier, Yan-
nick Parmentier, and Johannes Dellert. 2008. De-
velopping an MCTAG for German with an RCG-
based Parser. In Proceedings of the Language, Re-
source and Evaluation Conference (LREC 2008),
Marrakech, Morocco.
Lehmann, Sabine, Stephan Oepen, Sylvie Regnier-
Prost, Klaus Netter, Veronika Lux, Judith Klein,
Kirsten Falkedal, Frederik Fouvry, Dominique Esti-
val, Eva Dauphin, Herve? Compagnion, Judith Baur,
Lorna Balkan, and Doug Arnold. 1996. TSNLP ?
Test Suites for Natural Language Processing. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics (Coling 1996), volume 2, pages
711?716, Copenhagen, Denmark.
Lichte, Timm. 2007. An MCTAG with tuples for co-
herent constructions in German. In Proceedings of
the 12th Conference on Formal Grammar, Dublin,
Ireland.
Perrier, Guy. 2000. Interaction grammars. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics (CoLing 2000), pages 600?606,
Saarbruecken, Germany.
S?gaard, Anders. 2007. Complexity, expressivity and
logic of linguistic theories. Ph.D. thesis, University
of Copenhagen, Copenhagen, Denmark.
Villemonte de la Clergerie, ?Eric. 2005. DyALog: a
tabular logic programming based environment for
NLP. In Proceedings of the workshop on Constraint
Satisfaction for Language Processing (CSLP 2005),
pages 18?33, Barcelona, Spain.
XTAG-Research-Group. 2001. A lexicalized tree
adjoining grammar for english. Technical Re-
port IRCS-01-03, IRCS, University of Pennsylva-
nia. Available at http://www.cis.upenn.
edu/?xtag/gramrelease.html.
8
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 58?66,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Direct Parsing of Discontinuous Constituents in German
Wolfgang Maier
SFB 833, University of Tu?bingen
Nauklerstr. 35
72074 Tu?bingen, Germany
wmaier@sfs.uni-tuebingen.de
Abstract
Discontinuities occur especially frequently in
languages with a relatively free word order,
such as German. Generally, due to the long-
distance dependencies they induce, they lie
beyond the expressivity of Probabilistic CFG,
i.e., they cannot be directly reconstructed by
a PCFG parser. In this paper, we use a
parser for Probabilistic Linear Context-Free
Rewriting Systems (PLCFRS), a formalism
with high expressivity, to directly parse the
German NeGra and TIGER treebanks. In both
treebanks, discontinuities are annotated with
crossing branches. Based on an evaluation us-
ing different metrics, we show that an output
quality can be achieved which is comparable
to the output quality of PCFG-based systems.
1 Introduction
Languages with a rather free word order, like Ger-
man, display discontinuous constituents particularly
frequently. In (1), the discontinuity is caused by an
extraposed relative clause.
(1) wieder
again
treffen
match
alle
all
Attribute
attributes
zu,
VPART
die
which
auch
also
sonst
otherwise
immer
always
passen
fit
?Again, the same attributes as always apply.?
Another language with a rather free word order is
Bulgarian. In (2), the discontinuity is caused by top-
icalization.
(2) Himikali1
Pens1
az
I
kupuvam
buy
samo
only
evtini
expensive
t1
t1
?As for pens, I only buy expensive ones.?
In most constituency treebanks, sentence annota-
tion is restricted to having the shape of trees with-
out crossing branches, and the non-local dependen-
cies induced by the discontinuities are modeled by
an additional mechanism. In the Penn Treebank
(PTB) (Marcus et al, 1994), e.g., this mechanism
is a combination of special labels and empty nodes,
establishing implicit additional edges. In the Ger-
man Tu?Ba-D/Z (Telljohann et al, 2006), additional
edges are established by a combination of topolog-
ical field annotation and special edge labels. As an
example, Fig. 1 shows a tree from Tu?Ba-D/Z with
the annotation of (1). Note here the edge label ON-
MOD on the relative clause which indicates that the
subject of the sentence (alle Attribute) is modified.
Figure 1: A tree from Tu?Ba-D/Z
However, in a few other treebanks, such as the
German NeGra and TIGER treebanks (Skut et al,
1997; Brants et al, 2002), crossing branches are al-
lowed. This way, all dependents of a long-distance
dependency can be grouped under a single node.
58
Fig. 2 shows a tree from NeGra with the annotation
of (3).
(3) Noch
Yet
nie
never
habe
have
ich
I
so
so
viel
much
gewa?hlt
chosen
?Never have I had that much choice.?
Note the direct annotation of the discontinuous VP.
Noch
ADV
nie
ADV
habe
VAFIN
1.Sg.Pres.Ind
ich
PPER
1.Sg.*.Nom
so
ADV
viel
ADV
gew?hlt
VVPP
.
$.
MO HD
AVP
MO HD
AVP
MO MO HD
VP
OCHD SB
S
Figure 2: A tree from NeGra
Since in general, the annotation mechanisms for
non-local dependencies lie beyond the expressivity
of Context-Free Grammar, non-local information is
inaccessible for PCFG parsing and therefore gener-
ally discarded. In NeGra/TIGER annotation, e.g.,
tree transformation algorithms are applied before
parsing in order to resolve the crossing branches.
See, e.g., Ku?bler et al (2008) and Boyd (2007) for
details. If one wants to avoid the loss of annotation
information which is implied with such transforma-
tions, one possibility is to use a probabilistic parser
for a formalism which is more expressive than CFG.
In this paper, we tackle the question if qualita-
tively good results can be achieved when parsing
German with such a parser. Concretely, we use a
parser for Probabilistic Linear Context-Free Rewrit-
ing Systems (PLCFRS) (Kallmeyer and Maier,
2010). LCFRS (Vijay-Shanker et al, 1987) are a
natural extension of CFG in which a single non-
terminal node can dominate more than one contin-
uous span of terminals. We can directly interpret
NeGra-style trees as its derivation structures, i.e., we
can extract grammars without making further lin-
guistic assumptions (Maier and Lichte, 2009) (see
Sect. 2.3), as it is necessary for other formalisms
such as Probabilistic Tree Adjoining Grammars
(Chiang, 2003). Since the non-local dependencies
are immediately accessible in NeGra and TIGER,
we choose these treebanks as our data source. In
order to judge parser output quality, we use four dif-
ferent evaluation types. We use an EVALB-style
measure, adapted for LCFRS, in order to compare
our parser to previous work on parsing German tree-
banks. In order to address the known shortcomings
of EVALB, we perform an additional evaluation us-
ing the tree distance metric of Zhang and Shasha
(1989), which works independently of the fact if
there are crossing branches in the trees or not, and a
dependency evaluation (Lin, 1995), which has also
be applied before in the context of parsing German
(Ku?bler et al, 2008). Last, we evaluate certain diffi-
cult phenomena by hand on TePaCoC (Ku?bler et al,
2008), a set of sentences hand-picked from TIGER.
The evaluations show that with a PLCFRS parser,
competitive results can be achieved.
The remainder of the article is structured as fol-
lows. In Sect. 2, we present the formalism, the
parser, and how we obtain our grammars. In
Sect. 3, we discuss the evaluation methods we em-
ploy. Sect. 4 contains our experimental results.
Sect. 5 is dedicated to related work. Sect. 6 con-
tains the conclusion and presents some possible fu-
ture work.
2 A Parser for PLCFRS
2.1 Probabilistic Linear Context-Free
Rewriting Systems
LCFRS are an extension of CFG where the non-
terminals can span not only single strings but, in-
stead, tuples of strings. We will notate LCFRS with
the syntax of simple Range Concatenation Gram-
mars (SRCG) (Boullier, 1998), a formalism that is
equivalent to LCFRS.
A LCFRS (Vijay-Shanker et al, 1987) is a tuple
G = (N,T, V, P, S) where
a) N is a finite set of non-terminals with a func-
tion dim: N ? N that determines the fan-out
of each A ? N ;
b) T and V are disjoint finite sets of terminals and
variables;
c) S ? N is the start symbol with dim(S) = 1;
d) P is a finite set of rewriting rules
A(?1, . . . , ?dim(A)) ? A1(X(1)1 , . . . , X
(1)
dim(A1))
? ? ?Am(X(m)1 , . . . , X
(m)
dim(Am))
59
for m ? 0 where A,A1, . . . , Am ? N , X(i)j ? V
for 1 ? i ? m, 1 ? j ? dim(Ai) and ?i ? (T ?
V )? for 1 ? i ? dim(A). For all r ? P , it holds
that every variable X occurring in r occurs exactly
once in the left-hand side (LHS) and exactly once in
the right-hand side (RHS).
The fan-out of an LCFRS G is the maximal fan-
out of all non-terminals in G. Furthermore, the RHS
length of a rewriting rules r ? P is called the rank
of r and the maximal rank of all rules in P is called
the rank of G. An LCFRS is called ordered if for
every r ? P and every RHS non-terminal A in r and
each pair X1, X2 of arguments of A in the RHS of
r, X1 precedes X2 in the RHS iff X1 precedes X2
in the LHS.
Borrowed from SRCG, we specify the language
of an LCFRS based on the notion of ranges. For
some input word w = w1 ? ? ?wn, a range is a pair
?i, j? of integers with 0 ? i ? n denoting the sub-
string wi+1 ? ? ?wj . Note that a range denotes ? iff
i = j. Only consecutive ranges can be concatenated
into new ranges. We can replace the variables and
terminals of the rewriting rules with ranges. E.g.,
A(?g, h?) ? B(?g + 1, h ? 1?) is a replacement of
the clauseA(aX1b) ? B(X1) if the input wordw is
such that wg+1 = a and wh = b. A rewriting rule in
which all elements of all arguments have been con-
sistently replaced by ranges is called an instantiated
rule. A derivation is built by successively rewriting
the LHSs of instantiated rules with its RHSs. The
language L(G) of some LCFRS G consists of all
words w = w1 ? ? ?wn for which it holds that there is
a rule with the start symbol on the LHS which can
be instantiated to ?0, n? and rewritten to ?.
A probabilistic LCFRS (PLCFRS) is a tu-
ple ?N,T, V, P, S, p? such that ?N,T, V, P, S? is a
LCFRS and p : P ? [0..1] a function such that for
all A ? N : ?A(~x)?~??P p(A(~x) ? ~?) = 1. There
are possibly other ways to extend LCFRS with prob-
abilities. This definition is supported by the fact that
probabilistic MCFGs1 have been defined in the same
way (Kato et al, 2006).
1MCFGs are equivalent to LCFRSs and SRCGs (Boullier,
1998).
Scan: 0 : [A, ??i, i+ 1??] A POS tag ofwi+1
Unary: in : [B, ~?]in+ |log(p)| : [A, ~?] p : A(~?) ? B(~?) ? P
Binary: inB : [B, ~?B], inC : [C, ~?C ]inB + inC + log(p) : [A, ~?A]
where p : A( ~?A) ? B( ~?B)C( ~?C) is an instantiated rule.
Goal: [S, ??0, n??]
Figure 3: Weighted CYK deduction system
2.2 A CYK Parser for PLCFRS
We use the parser of Kallmeyer and Maier (2010).
It is a probabilistic CYK parser (Seki et al, 1991),
using the technique of weighted deductive parsing
(Nederhof, 2003). While for symbolic parsing, other
elaborate algorithms exist (Kallmeyer and Maier,
2009), for probabilistic parsing, CYK is a natural
choice.
It is assumed for the parser that our LCFRSs are
of rank 2 and do not contain rules where some of the
LHS components are ?. Both assumptions can be
made without loss of generality since every LCFRS
can be binarized (Go?mez-Rodr??guez et al, 2009)
and ?-components on LHS of rules can be removed
(Boullier, 1998). We make the assumption that POS
tagging is done before parsing. The POS tags are
special non-terminals of fan-out 1. Consequently,
the rules are either of the form A(a) ? ? where A
is a POS tag and a ? T or of the formA(~?) ? B(~x)
or A(~?) ? B(~x)C(~y) where ~? ? (V +)dim(A), i.e.,
only the rules for POS tags contain terminals in their
LHSs.
The parser items have the form [A, ~?], with A ?
N and ~? a vector of ranges characterizing all com-
ponents of the span of A. We specify the set
of weighted parse items via the deduction rules in
Fig. 3.
Parsing time can be reduced by reordering the
agenda during parsing such that those items are pro-
cessed first which lead to a complete parse more
quickly than others (Klein and Manning, 2003a).
The parser uses for this purpose an admissible, but
not monotonic estimate called LR estimate. It gives
(relative to a sentence length) an estimate of the out-
side probability of some non-terminal A with a span
of a certain length (the sum of the lengths of all the
60
components of the span), a certain number of ter-
minals to the left of the first and to the right of the
last component and a certain number of terminals
gaps in between the components of the A span, i.e.,
filling the gaps. A discussion of other estimates is
presented at length in Kallmeyer and Maier (2010).
2.3 LCFRS for Modeling Discontinuities
We use the algorithm from Maier and S?gaard
(2008) to extract LCFRS rules from our data sets.
For all nonterminals A0 with the children A1 ? ? ?Am
(i.e., for all non-terminals which are not pretermi-
nals), we create a clause ?0 ? ?1 ? ? ??m with ?i,
0 ? i ? m, labeled Ai. The arguments of each
?i, 1 ? i ? m, are single variables, one for each
of the continuous yield part dominated by the node
Ai. The arguments of ?0 are concatenations of these
variables that describe how the discontinuous parts
of the yield of A0 are obtained from the yields of its
daughters. For all preterminals A dominating some
terminal a, we extract a production A(a) ? ?. Since
by definition, a label is associated with a certain
fan-out, we distinguish the labels by correspond-
ing subscripts. Note that this extraction algorithm
yields only ordered LCFRS. Furthermore, note that
for trees without crossing branches, this algorithm
yields a PLCFRS with fan-out 1, i.e., a PCFG.
As mentioned before, the advantage of using
LCFRS is that grammar extraction is straight-
forward and that no separate assumptions must be
made. Note that unlike, e.g., Range Concatenation
Grammar (RCG) (Boullier, 1998), LCFRS cannot
model re-entrancies, i.e., nodes with more than one
incoming edge. While those do not occur in NeGra-
style annotation, some of the annotation in the PTB,
e.g., the annotation for right node raising, can be in-
terpreted as re-entrancies. This topic is left for fu-
ture work. See Maier and Lichte (2009) for further
details, especially on how treebank properties relate
to properties of extracted grammars.
Before parsing, we binarize our grammar. We first
mark the head daughters of all non-terminal nodes
using Collins-style head rules based on the NeGra
rules of the Stanford Parser (Klein and Manning,
2003b) and the reorder the RHSs of all LCFRS rules
such that sequence of elements to the right of the
head daughter is reversed and moved to the begin-
ning of the RHS. From this point, the binarization
works like the transformation into Chomsky Normal
Form for CFGs. For each rule with an RHS of length
? 3, we introduce a new non-terminal which cov-
ers the RHS without the first element and continue
successively from left to right. The rightmost new
rule, which covers the head daughter, is binarized to
unary.
We markovize the grammar as in the CFG case.
To the new symbols introduced during the binariza-
tion, a variable number of symbols from the vertical
and horizontal context of the original rule is added.
Following the literature, we call the respective quan-
tities v and h. As an example, Fig. 4 shows the out-
put for the production for the VP in the left tree in
Fig. 2.
After extraction and head marking:
VP2(X1,X2X3) ? AVP1(X1) AVP1(X2) VVPP1?(X3)
After binarization and markovization with v = 1, h = 2:
VP2(X1,X2) ? AVP1(X1) @-VP2v-AVP1h-VVPP1h(X2)
@-VP2v-AVP1h-VVPP1h(X1X2)
? AVP1(X1) @-VP2v-VVPP1h(X2)
@-VP2v-VVPP1h(X1) ? VVPP1(X1)
After binarization and markovization with v = 2, h = 1:
VP2(X1,X2) ? AVP1(X1) @-VP2v-S2v-AVP1h(X2)
@-VP2v-S2v-AVP1h(X1X2)
? AVP1(X1) @-VP2v-S2v-VVPP1h(X2)
@-VP2v-S2v-VVPP1h(X1) ? VVPP1(X1)
Figure 4: Grammar extraction and binarization example
The probabilities are then computed based on the
number of occurrences of rules in the transformed
treebank, using a Maximum Likelihood estimator.
3 Evaluation methods
We assess the quality of our parser output using dif-
ferent methods.
The first is an EVALB-style metric (henceforth
EVALB), i.e., we compare phrase boundaries. In
spite of its shortcomings (Rehbein and van Gen-
abith, 2007), it allows us to compare to previ-
ous work on parsing NeGra. In the context of
LCFRS, we compare sets of tuples of the form
[A, (i1l , i1r), . . . , (ikl , ikr )], where A is a non-terminal
in some derivation tree with dim(A) = k and each
(iml , imr ), 1 ? m ? k, is a tuple of indices denot-
ing a continuous sequence of terminals dominated
byA. One set is obtained from the parser output, and
61
B<
B C
B t3 B >B t4
t1 t2 z t1 t2 z
x y x y
Figure 5: TDIST example
one from the corresponding treebank trees. Using
these tuple sets, we compute labeled and unlabeled
recall (LR/UR), precision (LP/UP), and the F1 mea-
sure (LF1/UF1) in the usual way. Note that if k = 1,
our metric is identical to its PCFG version.
EVALB does not necessarily reflect parser output
quality (Rehbein and van Genabith, 2007; Emms,
2008; Ku?bler et al, 2008). One of its major prob-
lems is that attachment errors are penalized too
hard. As the second evaluation method, we there-
fore choose the tree-distance measure (henceforth
TDIST) (Zhang and Shasha, 1989), which levitates
this problem. It has been proposed for parser evalu-
ation by Emms (2008). TDIST is an ideal candidate
for evaluation of the output of a PLCFRS, since it the
fact if trees have crossing branches or not is not rel-
evant to it. Two trees ?k and ?A are compared on the
basis of T -mappings from ?k to ?A. A T -mapping
is a partial mapping ? of nodes of ?k to nodes of ?A
where all node mappings preserve left-to-right or-
der and ancestry. Within the mappings, node inser-
tion, node deletion, and label swap operations are
identified, represented resp. by the sets I , D and
S . Furthermore, we consider the set M represent-
ing the matched (i.e., unchanged) nodes. The cost of
a T -mapping is the total number of operations, i.e.
|I|+ |D|+ |S|. The tree distance between two trees
?K and ?A is the cost of the cheapest T -mapping.
Fig. 5, borrowed from Emms, shows an example for
a T -mapping. Inserted nodes are prefixed with >,
deleted nodes are suffixed with <, and nodes with
swapped labels are linked with arrows. Since in to-
tal, four operations are involved, to this T -mapping,
a cost of 4 is assigned. For more details, especially
on algorithms which compute TDIST, refer to Bille
(2005). In order to convert the tree distance measure
into a similarity measure like EVALB, we use the
macro-averaged Dice and Jaccard normalizations as
defined by Emms. Let ?K and ?A be two trees with
|?K | and |?A| nodes, respectively. For a T -mapping
? from ?K to ?A with the sets D, I , S and M, we
compute them as follows.
dice(?) = 1? |D|+ |I|+ |S||?K |+ |?A|
jaccard (?) = 1? |D|+ |I|+ |S||D|+ |I|+ |S|+ |M|
where, in order to achieve macro-averaging, we sum
the numerators and denominators over all tree pairs
before dividing. See Emms (2008) for further de-
tails.
The third method is dependency evaluation
(henceforth DEP), as described by Lin (1995). It
consists of comparing dependency graphs extracted
from the gold data and from the parser output. The
dependency extraction algorithm as given by Lin
does also not rely on trees to be free of crossing
branches. It only relies on a method to identify the
head of each phrase. We use our own implementa-
tion of the algorithm which is described in Sect. 4
of Lin (1995), combined with the head finding algo-
rithm of the parser. Dependency evaluation abstracts
away from another bias of EVALB. Concretely, it
does not prefer trees with a high node/token ratio,
since two dependency graphs to be compared neces-
sarily have the same number of (terminal) nodes. In
the context of parsing German, this evaluation has
been employed previously by Ku?bler et al (2008).
Last, we evaluate on TePaCoC (Testing
Parser Performance on Complex Grammatical
Constructions), a set of particularly difficult sen-
tences hand-picked from TIGER (Ku?bler et al,
2008).
4 Experiments
Our data sources are the German NeGra (Skut et
al., 1997) and TIGER (Brants et al, 2002) tree-
banks. In a preprocessing step, following common
practice, we attach all punctuation to nodes within
the tree, since it is not included in the NeGra an-
notation. In a first pass, using heuristics, we at-
tach all nodes to the in each case highest available
phrasal node such that ideally, we do not introduce
new crossing branches. In a second pass, paren-
theses and quotation marks are preferably attached
to the same node. Grammatical function labels are
62
discarded. After this preprocessing step, we create
a separate version of the data set, in which we re-
solve the crossing branches in the trees, using the
common approach of re-attaching nodes to higher
constituents. We use the first 90% of our data sets
for training and the remaining 10% for testing. Due
to memory limitations, we restrict ourselves to sen-
tences of a maximal length of 30 words. Our TIGER
data sets (TIGER and T-CF) have 31,568 sentences
of an average length of 14.81, splitted into 31,568
sentences for training and 3,508 sentences for test-
ing. Our NeGra data sets (NeGra and N-CF) have
18,335 sentences, splitted into 16,501 sentences for
training and 1,834 sentences for testing.
We parse the data sets described above with acti-
vated LR estimate. For all our experiments, we use
the markovization settings v = 2 and h = 1, which
have proven to be successful in previous work on
parsing NeGra (Rafferty and Manning, 2008). We
provide the parser with the gold tagging. Fig. 6
shows the average parsing times for all data sets on
an AMD Opteron node with 8GB of RAM (pure
Java implementation), Tab. 1 shows the percentage
of parsed sentences.
 0.01
 0.1
 1
 10
 100
 5  10  15  20  25
tim
e 
in
 s
ec
. (l
og
 sc
ale
)
Sentence length
NeGra
TIGER
N-CF
T-CF
Figure 6: Parsing times
NeGra TIGER N-CF T-CF
total 1834 3508 1834 3508
parsed 1779
(97.0%)
3462
(98.7%)
1804
(98.4%)
3462
(98.7%)
Table 1: Parsed sentences
4.1 Evaluation Using EVALB
Tab. 2 shows the evaluation of the parser output us-
ing EVALB, as described in the previous section.
We report labeled and unlabeled precision, recall
and F1 measure.
LP LR LF1 UP UR UF1
NeGra 72.39 70.68 71.52 76.01 74.22 75.10
TIGER 74.97 71.95 73.43 78.58 75.42 76.97
N-CF 74.85 73.26 74.04 78.11 76.45 77.28
T-CF 77.51 73.73 75.57 80.59 76.66 78.57
Table 2: EVALB results
Not surprisingly, reconstructing discontinuities is
hard. Therefore, when parsing without crossing
branches, the results are slightly better. In order to
see the influence of discontinuous structures during
parsing on the underlying phrase structure, we re-
solve the crossing branches in the parser output of
NeGra and TIGER and compare it to the respective
gold test data of N-CF and T-CF. Tab. 3 shows the
results.
LP LR LF1 UP UR UF1
NeGra 72.75 71.04 71.89 76.38 74.58 75.47
TIGER 75.28 72.25 73.74 78.81 75.64 77.20
Table 3: EVALB results (resolved crossing branches)
The results deteriorate slightly in comparison
with N-CF and T-CF, however, they are slightly
higher than for than for NeGra and TIGER. This
is due to the fact that during the transformation,
some errors in the LCFRS parses get ?corrected?:
Wrongly attached phrasal nodes are re-attached to
unique higher positions in the trees.
In order to give a point of comparison with previ-
ous work on parsing TIGER and NeGra, in Tab. 4,
we report some of the results from the literature. All
of them were obtained using PCFG parsers: Ku?bler
(2005) (Tab. 1, plain PCFG for NeGra), Ku?bler et al
(2008) (Tab. 3, plain PCFG and Stanford parser with
markovization v = 2 and h = 1 for TIGER), and
Petrov and Klein (2007) (Tab. 1, Berkeley parser, la-
tent variables). We include the results for N-CF and
T-CF.
Our results are slightly better than for the plain
PCFG models. We would expect the result for T-
CF to be closer to the corresponding result for the
Stanford parser, since we are using a comparable
63
plain this work markov. latent
NeGra 69.94 74.04 ? 80.1
TIGER 74.00 75.57 77.30 ?
Table 4: PCFG parsing of NeGra, Labeled F1
model. This difference is mostly likely due to losses
induced by the LR estimate. All items to which the
estimate assigns an outside log probability estimate
of ?? get blocked and are not put on the agenda.
This blocking has an extremely beneficial effect on
parser speed. However, it is paid by a worse recall,
as experiments with smaller data sets have shown.
A complete discussion of the effects of estimates, as
well as a discussion of other possible optimizations,
is presented in Kallmeyer and Maier (2010).
Recall finally that LCFRS parses are more infor-
mative than PCFG parses ? a lower score for LCFRS
EVALB than for PCFG EVALB does not necessarily
mean that the PCFG parse is ?better?.
4.2 Evaluation Using Tree Distance
Tab. 5 shows the results of evaluating with TDIST,
excluding unparsed sentences. We report the dice
and jaccard normalizations, as well as a summary
of the distribution of the tree distances between gold
trees and trees from the parser output (see Sect. 3).
tree distance distrib.
dice jaccard 0 ? 3 ? 10
NeGra 88.86 79.79 31.65 53.77 15.08
TIGER 89.47 80.84 29.87 56.78 18.18
N-CF 92.50 85.99 33.43 61.92 6.71
T-CF 92.70 86.46 31.80 63.81 4.56
Table 5: Tree distance evaluation
Again, we can observe that parsing LCFRS is
harder than parsing PCFG. As for EVALB, the re-
sults for TIGER are slightly higher than the ones for
NeGra. The distribution of the tree distances shows
that about a third of all sentences receive a com-
pletely correct parse. More than a half, resp. a third
of all parser output trees require ? 3 operations to be
mapped to the corresponding gold tree, and a only a
small percentage requires ? 10 operations.
To our knowledge, TDIST has not been used to
evaluate parser output for NeGra and TIGER. How-
ever, Emms (2008) reports results for the PTB using
different parsers. Collins? Model 1 (Collins, 1999),
e.g., lies at 93.62 (Dice) and 87.87 (Jaccard). For
the Berkeley Parser (Petrov and Klein, 2007), 94.72
and 89.87 is reported. We see that our results lie in
them same range. However, Jaccard scores are lower
since this normalization punishes a higher number
of edit operations more severely than Dice. In or-
der to meaningfully interpret which treebank prop-
erties are responsible for the fact that between the
gold trees and the trees from the parser, the German
data requires more tree edit operations than the En-
glish data, a TDIST evaluation of the output of an
off-the-shelf PCFG parser would be necessary. This
is left for future work.
4.3 Dependency Evaluation
For the dependency evaluation, we extract depen-
dency graphs from both the gold data and the test
data and compare the unlabeled accuracy. Tab. 6
shows the results. We report unlabeled attachment
score (UAS).
UAS
NeGra 76.50
TIGER 77.84
N-CF 77.52
T-CF 78.67
Table 6: Dependency evaluation
The dependency results are consistent with the
previous results in as much as the scores for PCFG
parsing are again higher. The dependency re-
sults reported in Ku?bler et al (2008) however are
much higher (85.6 UAS for the markovized Stan-
ford parser). While a part of the losses can again
be attributed to the LR estimate, another reason lies
undoubtedly in the different dependency conversion
method which we employ, and in further treebank
transformations which Ku?bler et al perform. In or-
der to get a more fine grained result, in future work,
we will consider graph modifications as proposed by
Lin (1995) as well as including annotation-specific
information from NeGra/TIGER in our conversion
procedure.
4.4 TePaCoC
The TePaCoC data set (Ku?bler et al, 2008) provides
100 hand-picked sentences from TIGER which con-
tain constructions that are especially difficult to
64
parse. Out of these 100 sentences, we only consider
69. The remaining 31 sentences are either longer
than 30 words or not included in the TIGER 2003
release (Ku?bler et al use the 2005 release). The
data is partitioned in groups of sentences with extra-
posed relative clauses (ERC), forward conjunction
reduction (FCR), noun PP attachment (PPN), verb
PP attachment (PPV), subject gap with finite/fronted
verbs (SGF) and coordination of unlike constituents
(CUC). Tab. 7 shows the EVALB results for the (dis-
continuous) TePaCoC. We parse these sentences us-
ing the same training set as before with all TePaCoC
sentences removed.
LP LR LF1 UP UR UF1
ERC 59.34 61.36 60.34 64.84 67.05 65.92
FCR 78.03 76.70 77.36 82.66 81.25 81.95
PPN 72.15 72.15 72.15 75.95 75.95 75.95
PPV 73.33 73.33 73.33 76.66 76.66 76.66
CUC 58.76 57.58 58.16 69.07 67.68 68.37
SGF 82.67 81.05 81.85 85.33 83.66 84.49
all 72.27 71.83 72.05 77.26 76.78 77.02
Table 7: EVALB scores for TePaCoC
While we cannot compare our results directly
with the PCFG results (using grammatical function
labels) of Ku?bler et al, their results nevertheless give
an orientation.
We take a closer look at all sentence groups. Our
result for ERC is more than 15 points worse than
the result of Ku?bler et al The relative clause itself
is mostly recognized as a sentence (though not ex-
plicitly marked as a relative clause, since we do not
consider grammatical functions). However, it is al-
most consistently attached too high (on the VP or
on clause level). While this is correct for Ku?bler et
al., with crossing branches, it treated as an error and
punished especially hard by EVALB. FCR is parsed
mostly well and with comparable results to Ku?bler
et al There are too few sentences to make a strong
claim about PP attachment. However, in both PPN
and PPV flat phrases seem to be preferred, which
has as a consequence that in PPN, PPs are attached
too high and in PPV too low. Our output confirms
the claim of Ku?bler et al?s that unlike coordinations
is the most difficult of all TePaCoC phenomena. The
conjuncts themselves are correctly identified in most
cases, however then coordinated at the wrong level.
SGF is parsed best. Ku?bler et al report for this group
only 78.6 labeled F1 for the Stanford Parser. Our
overall results are slightly worse than the results of
Ku?bler et al, but show less variance.
To sum up, not surprisingly, getting the right at-
tachment positions seems to be hard for LCFRS,
too. Additionally, with crossing branches, the out-
put is rated worse, since some attachments are not
present anymore without crossing branches. Since
especially for the relative clauses, attachment posi-
tions are in fact a matter of discussion from a syntac-
tic point of view, we will consider in future studies
to selectively resolve some of the crossing branches,
e.g., by attaching relative clauses to higher positions.
5 Related Work
The use of formalisms with a high expressivity has
been explored before (Plaehn, 2004; Levy, 2005).
To our knowledge, Plaehn is the only one to re-
port evaluation results. He uses the formalism of
Discontinuous Phrase Structure Grammar (DPSG).
Limiting the sentence length to 15, he obtains 73.16
labeled F1 on NeGra. Evaluating all sentences of
our NeGra data with a length of up to 15 words re-
sults, however, in 81.27 labeled F1. For a compari-
son between DPSG and LCFRS, refer to Maier and
S?gaard (2008).
6 Conclusion and Future Work
We have investigated the possibility of using Prob-
abilistic Linear Context-Free Rewriting Systems for
direct parsing of discontinuous constituents. Conse-
quently, we have applied a PLCFRS parser on the
German NeGra and TIGER treebanks. Our evalu-
ation, which used different metrics, showed that a
PLCFRS parser can achieve competitive results.
In future work, all of the presented evaluation
methods will be investigated to greater detail. In
order to do this, we will parse our data sets with
current state-of-the-art systems. Especially a more
elaborate dependency conversion should enable a
more informative comparison between the output of
PCFG parsers and the output of the PLCFRS parser.
Last, since an algorithm is available which extracts
LCFRSs from dependency structures (Kuhlmann
and Satta, 2009), the parser is instantly ready for
parsing them. We are currently performing the cor-
responding experiments.
65
References
Philip Bille. 2005. A survey on tree edit distance and
related problems. Theoretical Computer Science, 337.
Pierre Boullier. 1998. A Proposal for a Natural Lan-
guage Processing Syntactic Backbone. Technical Re-
port 3342, INRIA.
Adriane Boyd. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations. In
The Linguistic Annotation Workshop at ACL 2007.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER Tree-
bank. In Proceedings of Treebanks and Linguistic
Theories.
David Chiang. 2003. Statistical parsing with an automat-
ically extracted Tree Adjoining Grammar. In Data-
Oriented Parsing. CSLI Publications.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
Martin Emms. 2008. Tree Distance and some other vari-
ants of Evalb. In Proceedings of LREC 08.
Carlos Go?mez-Rodr??guez, Marco Kuhlmann, Giorgio
Satta, and David Weir. 2009. Optimal reduction of
rule length in linear context-free rewriting systems. In
Proceedings of NAACL-HLT.
Laura Kallmeyer and Wolfgang Maier. 2009. An in-
cremental earley parser for simple range concatenation
grammar. In Proceedings of IWPT 09.
Laura Kallmeyer and Wolfgang Maier. 2010. Data-
driven parsing with probabilistic linear context-free
rewriting systems. Unpublished Manuscript.
Yuki Kato, Hiroyuki Seki, and Tadao Kasami. 2006.
RNA pseudoknotted structure prediction using
stochastic multiple context-free grammar. IPSJ
Digital Courier, 2.
Dan Klein and Christopher D. Manning. 2003a. A* Pars-
ing: Fast Exact Viterbi Parse Selection. In Proceed-
ings of NAACL-HLT.
Dan Klein and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for natural lan-
guage parsing. In In Advances in Neural Information
Processing Systems 15 (NIPS). MIT Press.
Sandra Ku?bler, Wolfgang Maier, Ines Rehbein, and Yan-
nick Versley. 2008. How to compare treebanks. In
Proceedings of LREC 08.
Sandra Ku?bler. 2005. How do treebank annotation
schemes influence parsing results? Or how not to com-
pare apples and oranges. In Proceedings of RANLP
2005.
Marco Kuhlmann and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Proceedings of EACL.
Roger Levy. 2005. Probabilistic Models of Word Or-
der and Syntactic Discontinuity. Ph.D. thesis, Stan-
ford University.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings of
IJCAI 95.
Wolfgang Maier and Timm Lichte. 2009. Characterizing
discontinuity in constituent treebanks. In Proceedings
of Formal Grammar 2009.
Wolfgang Maier and Anders S?gaard. 2008. Treebanks
and mild context-sensitivity. In Proceedings of Formal
Grammar 2008.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
Proceedings of HLT.
Mark-Jan Nederhof. 2003. Weighted Deductive Parsing
and Knuth?s Algorithm. Computational Linguistics,
29(1).
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL 2007.
Oliver Plaehn. 2004. Computing the most probable parse
for a discontinuous phrase-structure grammar. In New
developments in parsing technology. Kluwer.
Anna Rafferty and Christopher D. Manning. 2008. Pars-
ing three German treebanks: Lexicalized and unlexi-
calized baselines. In Proceedings of the Workshop on
Parsing German at ACL 2008.
Ines Rehbein and Josef van Genabith. 2007. Evaluating
evaluation measures. In Proceedings of NODALIDA
2007.
Hiroyuki Seki, Takahashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context-free gram-
mars. Theoretical Computer Science, 88(2).
Wojciech Skut, Brigitte Krenn, Thorten Brants, and Hans
Uszkoreit. 1997. An annotation scheme for free word
order languages. In Proceedings of ANLP.
Heike Telljohann, Erhard Hinrichs, Sandra Ku?bler, and
Heike Zinsmeister. 2006. Stylebook for the Tu?bingen
Treebank of Written German (Tu?Ba-D/Z). Technis-
cher Bericht, Universita?t Tu?bingen.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions produced
by various grammatical formalisms. In Proceedings of
ACL.
Kaizhong Zhang and Dennis Shasha. 1989. Simple fast
algorithms for the editing distance between trees and
related problems. SIAM Journal of Computing, 18.
66
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146?182,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Overview of the SPMRL 2013 Shared Task:
Cross-Framework Evaluation of Parsing Morphologically Rich Languages?
Djam? Seddaha, Reut Tsarfatyb, Sandra K?blerc,
Marie Canditod, Jinho D. Choie, Rich?rd Farkasf , Jennifer Fosterg, Iakes Goenagah,
Koldo Gojenolai, Yoav Goldbergj , Spence Greenk, Nizar Habashl, Marco Kuhlmannm,
Wolfgang Maiern, Joakim Nivreo, Adam Przepi?rkowskip, Ryan Rothq, Wolfgang Seekerr,
Yannick Versleys, Veronika Vinczet, Marcin Wolin?skiu,
Alina Wr?blewskav, Eric Villemonte de la Cl?rgeriew
aU. Paris-Sorbonne/INRIA, bWeizman Institute, cIndiana U., dU. Paris-Diderot/INRIA, eIPsoft Inc., f,tU. of Szeged,
gDublin City U., h,iU. of the Basque Country, jBar Ilan U., kStanford U., l,qColumbia U., m,oUppsala U., nD?sseldorf U.,
p,u,vPolish Academy of Sciences, rStuttgart U., sHeidelberg U., wINRIA
Abstract
This paper reports on the first shared task on
statistical parsing of morphologically rich lan-
guages (MRLs). The task features data sets
from nine languages, each available both in
constituency and dependency annotation. We
report on the preparation of the data sets, on
the proposed parsing scenarios, and on the eval-
uation metrics for parsing MRLs given dif-
ferent representation types. We present and
analyze parsing results obtained by the task
participants, and then provide an analysis and
comparison of the parsers across languages and
frameworks, reported for gold input as well as
more realistic parsing scenarios.
1 Introduction
Syntactic parsing consists of automatically assigning
to a natural language sentence a representation of
its grammatical structure. Data-driven approaches
to this problem, both for constituency-based and
dependency-based parsing, have seen a surge of inter-
est in the last two decades. These data-driven parsing
approaches obtain state-of-the-art results on the de
facto standard Wall Street Journal data set (Marcus et
al., 1993) of English (Charniak, 2000; Collins, 2003;
Charniak and Johnson, 2005; McDonald et al, 2005;
McClosky et al, 2006; Petrov et al, 2006; Nivre et
al., 2007b; Carreras et al, 2008; Finkel et al, 2008;
?Contact authors: djame.seddah@paris-sorbonne.fr,
reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu
Huang, 2008; Huang et al, 2010; Zhang and Nivre,
2011; Bohnet and Nivre, 2012; Shindo et al, 2012),
and provide a foundation on which many tasks oper-
ating on semantic structure (e.g., recognizing textual
entailments) or even discourse structure (coreference,
summarization) crucially depend.
While progress on parsing English ? the main
language of focus for the ACL community ? has in-
spired some advances on other languages, it has not,
by itself, yielded high-quality parsing for other lan-
guages and domains. This holds in particular for mor-
phologically rich languages (MRLs), where impor-
tant information concerning the predicate-argument
structure of sentences is expressed through word for-
mation, rather than constituent-order patterns as is the
case in English and other configurational languages.
MRLs express information concerning the grammati-
cal function of a word and its grammatical relation to
other words at the word level, via phenomena such
as inflectional affixes, pronominal clitics, and so on
(Tsarfaty et al, 2012c).
The non-rigid tree structures and morphological
ambiguity of input words contribute to the challenges
of parsing MRLs. In addition, insufficient language
resources were shown to also contribute to parsing
difficulty (Tsarfaty et al, 2010; Tsarfaty et al, 2012c,
and references therein). These challenges have ini-
tially been addressed by native-speaking experts us-
ing strong in-domain knowledge of the linguistic
phenomena and annotation idiosyncrasies to improve
the accuracy and efficiency of parsing models. More
146
recently, advances in PCFG-LA parsing (Petrov et al,
2006) and language-agnostic data-driven dependency
parsing (McDonald et al, 2005; Nivre et al, 2007b)
have made it possible to reach high accuracy with
classical feature engineering techniques in addition
to, or instead of, language-specific knowledge. With
these recent advances, the time has come for estab-
lishing the state of the art, and assessing strengths
and weaknesses of parsers across different MRLs.
This paper reports on the first shared task on sta-
tistical parsing of morphologically rich languages
(the SPMRL Shared Task), organized in collabora-
tion with the 4th SPMRL meeting and co-located
with the conference on Empirical Methods in Natural
Language Processing (EMNLP). In defining and exe-
cuting this shared task, we pursue several goals. First,
we wish to provide standard training and test sets for
MRLs in different representation types and parsing
scenarios, so that researchers can exploit them for
testing existing parsers across different MRLs. Sec-
ond, we wish to standardize the evaluation protocol
and metrics on morphologically ambiguous input,
an under-studied challenge, which is also present in
English when parsing speech data or web-based non-
standard texts. Finally, we aim to raise the awareness
of the community to the challenges of parsing MRLs
and to provide a set of strong baseline results for
further improvement.
The task features data from nine, typologically di-
verse, languages. Unlike previous shared tasks on
parsing, we include data in both dependency-based
and constituency-based formats, and in addition to
the full data setup (complete training data), we pro-
vide a small setup (a training subset of 5,000 sen-
tences). We provide three parsing scenarios: one in
which gold segmentation, POS tags, and morphologi-
cal features are provided, one in which segmentation,
POS tags, and features are automatically predicted
by an external resource, and one in which we provide
a lattice of multiple possible morphological analyses
and allow for joint disambiguation of the morpholog-
ical analysis and syntactic structure. These scenarios
allow us to obtain the performance upper bound of
the systems in lab settings using gold input, as well
as the expected level of performance in realistic pars-
ing scenarios ? where the parser follows a morpho-
logical analyzer and is a part of a full-fledged NLP
pipeline.
The remainder of this paper is organized as follows.
We first survey previous work on parsing MRLs (?2)
and provide a detailed description of the present task,
parsing scenarios, and evaluation metrics (?3). We
then describe the data sets for the nine languages
(?4), present the different systems (?5), and empiri-
cal results (?6). Then, we compare the systems along
different axes (?7) in order to analyze their strengths
and weaknesses. Finally, we summarize and con-
clude with challenges to address in future shared
tasks (?8).
2 Background
2.1 A Brief History of the SPMRL Field
Statistical parsing saw initial success upon the avail-
ability of the Penn Treebank (PTB, Marcus et al,
1994). With that large set of syntactically annotated
sentences at their disposal, researchers could apply
advanced statistical modeling and machine learning
techniques in order to obtain high quality structure
prediction. The first statistical parsing models were
generative and based on treebank grammars (Char-
niak, 1997; Johnson, 1998; Klein and Manning, 2003;
Collins, 2003; Petrov et al, 2006; McClosky et al,
2006), leading to high phrase-structure accuracy.
Encouraged by the success of phrase-structure
parsers for English, treebank grammars for additional
languages have been developed, starting with Czech
(Hajic? et al, 2000) then with treebanks of Chinese
(Levy and Manning, 2003), Arabic (Maamouri et
al., 2004b), German (K?bler et al, 2006), French
(Abeill? et al, 2003), Hebrew (Sima?an et al, 2001),
Italian (Corazza et al, 2004), Spanish (Moreno et al,
2000), and more. It quickly became apparent that
applying the phrase-based treebank grammar tech-
niques is sensitive to language and annotation prop-
erties, and that these models are not easily portable
across languages and schemes. An exception to that
is the approach by Petrov (2009), who trained latent-
annotation treebank grammars and reported good
accuracy on a range of languages.
The CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007a) high-
lighted the usefulness of an alternative linguistic for-
malism for the development of competitive parsing
models. Dependency relations are marked between
input tokens directly, and allow the annotation of
147
non-projective dependencies that are parseable effi-
ciently. Dependency syntax was applied to the de-
scription of different types of languages (Tesni?re,
1959; Mel?c?uk, 2001), which raised the hope that in
these settings, parsing MRLs will further improve.
However, the 2007 shared task organizers (Nivre
et al, 2007a) concluded that: "[Performance] classes
are more easily definable via language characteris-
tics than via characteristics of the data sets. The
split goes across training set size, original data for-
mat [...], sentence length, percentage of unknown
words, number of dependency labels, and ratio of
(C)POSTAGS and dependency labels. The class
with the highest top scores contains languages with
a rather impoverished morphology." The problems
with parsing MRLs have thus not been solved by de-
pendency parsing, but rather, the challenge has been
magnified.
The first event to focus on the particular challenges
of parsing MRLs was a dedicated panel discussion
co-located with IWPT 2009.1 Work presented on
Hebrew, Arabic, French, and German made it clear
that researchers working on non-English parsing face
the same overarching challenges: poor lexical cover-
age (due to high level of inflection), poor syntactic
coverage (due to more flexible word ordering), and,
more generally, issues of data sparseness (due to
the lack of large-scale resources). Additionally, new
questions emerged as to the evaluation of parsers in
such languages ? are the word-based metrics used
for English well-equipped to capture performance
across frameworks, or performance in the face of
morphological complexity? This event provoked ac-
tive discussions and led to the establishment of a
series of SPMRL events for the discussion of shared
challenges and cross-fertilization among researchers
working on parsing MRLs.
The body of work on MRLs that was accumulated
through the SPMRL workshops2 and hosting ACL
venues contains new results for Arabic (Attia et al,
2010; Marton et al, 2013a), Basque (Bengoetxea
and Gojenola, 2010), Croatian (Agic et al, 2013),
French (Seddah et al, 2010; Candito and Seddah,
2010; Sigogne et al, 2011), German (Rehbein, 2011),
Hebrew (Tsarfaty and Sima?an, 2010; Goldberg and
1http://alpage.inria.fr/iwpt09/panel.en.
html
2See http://www.spmrl.org/ and related workshops.
Elhadad, 2010a), Hindi (Ambati et al, 2010), Ko-
rean (Chung et al, 2010; Choi and Palmer, 2011) and
Spanish (Le Roux et al, 2012), Tamil (Green et al,
2012), amongst others. The awareness of the model-
ing challenges gave rise to new lines of work on top-
ics such as joint morpho-syntactic processing (Gold-
berg and Tsarfaty, 2008), Relational-Realizational
Parsing (Tsarfaty, 2010), EasyFirst Parsing (Gold-
berg, 2011), PLCFRS parsing (Kallmeyer and Maier,
2013), the use of factored lexica (Green et al, 2013),
the use of bilingual data (Fraser et al, 2013), and
more developments that are currently under way.
With new models and data, and with lingering in-
terest in parsing non-standard English data, questions
begin to emerge, such as: What is the realistic per-
formance of parsing MRLs using today?s methods?
How do the different models compare with one an-
other? How do different representation types deal
with parsing one particular language? Does the suc-
cess of a parsing model on a language correlate with
its representation type and learning method? How to
parse effectively in the face of resource scarcity? The
first step to answering all of these questions is pro-
viding standard sets of comparable size, streamlined
parsing scenarios, and evaluation metrics, which are
our main goals in this SPMRL shared task.
2.2 Where We Are At: The Need for
Cross-Framework, Realistic, Evaluation
Procedures
The present task serves as the first attempt to stan-
dardize the data sets, parsing scenarios, and evalu-
ation metrics for MRL parsing, for the purpose of
gaining insights into parsers? performance across lan-
guages. Ours is not the first cross-linguistic task on
statistical parsing. As mentioned earlier, two previ-
ous CoNLL shared tasks focused on cross-linguistic
dependency parsing and covered thirteen different
languages (Buchholz and Marsi, 2006; Nivre et al,
2007a). However, the settings of these tasks, e.g.,
in terms of data set sizes or parsing scenarios, made
it difficult to draw conclusions about strengths and
weaknesses of different systems on parsing MRLs.
A key aspect to consider is the relation between
input tokens and tree terminals. In the standard sta-
tistical parsing setup, every input token is assumed
to be a terminal node in the syntactic parse tree (after
deterministic tokenization of punctuation). In MRLs,
148
morphological processes may have conjoined several
words into a single token. Such tokens need to be seg-
mented and their analyses need to be disambiguated
in order to identify the nodes in the parse tree. In
previous shared tasks on statistical parsing, morpho-
logical information was assumed to be known in ad-
vance in order to make the setup comparable to that
of parsing English. In realistic scenarios, however,
morphological analyses are initially unknown and are
potentially highly ambiguous, so external resources
are used to predict them. Incorrect morphological
disambiguation sets a strict ceiling on the expected
performance of parsers in real-world scenarios. Re-
sults reported for MRLs using gold morphological
information are then, at best, optimistic.
One reason for adopting this less-than-realistic
evaluation scenario in previous tasks has been the
lack of sound metrics for the more realistic scenario.
Standard evaluation metrics assume that the number
of terminals in the parse hypothesis equals the num-
ber of terminals in the gold tree. When the predicted
morphological segmentation leads to a different num-
ber of terminals in the gold and parse trees, standard
metrics such as ParsEval (Black et al, 1991) or At-
tachment Scores (Buchholz and Marsi, 2006) fail
to produce a score. In this task, we use TedEval
(Tsarfaty et al, 2012b), a metric recently suggested
for joint morpho-syntactic evaluation, in which nor-
malized tree-edit distance (Bille, 2005) on morpho-
syntactic trees allows us to quantify the success on
the joint task in realistic parsing scenarios.
Finally, the previous tasks focused on dependency
parsing. When providing both constituency-based
and dependency-based tracks, it is interesting to com-
pare results across these frameworks so as to better
understand the differences in performance between
parsers of different types. We are now faced with
an additional question: how can we compare pars-
ing results across different frameworks? Adopting
standard metrics will not suffice as we would be com-
paring apples and oranges. In contrast, TedEval is
defined for both phrase structures and dependency
structures through the use of an intermediate repre-
sentation called function trees (Tsarfaty et al, 2011;
Tsarfaty et al, 2012a). Using TedEval thus allows us
to explore both dependency and constituency parsing
frameworks and meaningfully compare the perfor-
mance of parsers of different types.
3 Defining the Shared-Task
3.1 Input and Output
We define a parser as a structure prediction function
that maps sequences of space-delimited input tokens
(henceforth, tokens) in a language to a set of parse
trees that capture valid morpho-syntactic structures
in that language. In the case of constituency parsing,
the output structures are phrase-structure trees. In de-
pendency parsing, the output consists of dependency
trees. We use the term tree terminals to refer to the
leaves of a phrase-structure tree in the former case
and to the nodes of a dependency tree in the latter.
We assume that input sentences are represented
as sequences of tokens. In general, there may be a
many-to-many relation between input tokens and tree
terminals. Tokens may be identical to the terminals,
as is often the case in English. A token may be
mapped to multiple terminals assigned their own POS
tags (consider, e.g., the token ?isn?t?), as is the case
in some MRLs. Several tokens may be grouped into
a single (virtual) node, as is the case with multiword
expressions (MWEs) (consider ?pomme de terre? for
?potatoe?). This task covers all these cases.
In the standard setup, all tokens are tree terminals.
Here, the task of a parser is to predict a syntactic
analysis in which the tree terminals coincide with the
tokens. Disambiguating the morphological analyses
that are required for parsing corresponds to selecting
the correct POS tag and possibly a set of morpho-
logical features for each terminal. For the languages
Basque, French, German, Hungarian, Korean, Polish,
and Swedish, we assume this standard setup.
In the morphologically complex setup, every token
may be composed of multiple terminals. In this case,
the task of the parser is to predict the sequence of tree
terminals, their POS tags, and a correct tree associ-
ated with this sequence of terminals. Disambiguating
the morphological analysis therefore requires split-
ting the tokens into segments that define the terminals.
For the Semitic languages Arabic and Hebrew, we
assume this morphologically complex setup.
In the multiword expression (MWEs) setup, pro-
vided here for French only, groupings of terminals
are identified as MWEs (non-terminal nodes in con-
stituency trees, marked heads in dependency trees).
Here, the parser is required to predict how terminals
are grouped into MWEs on top of predicting the tree.
149
3.2 Data Sets
The task features nine languages from six language
families, from Germanic languages (Swedish and
German) and Romance (French) to Slavic (Polish),
Koreanic (Korean), Semitic (Arabic, Hebrew), Uralic
(Hungarian), and the language isolate Basque.
These languages cover a wide range of morpho-
logical richness, with Arabic, Basque, and Hebrew
exhibiting a high degree of inflectional and deriva-
tional morphology. The Germanic languages, Ger-
man and Swedish, have greater degrees of phrasal
ordering freedom than English. While French is not
standardly classified as an MRL, it shares MRLs char-
acteristics which pose challenges for parsing, such as
a richer inflectional system than English.
For each contributing language, we provide two
sets of annotated sentences: one annotated with la-
beled phrase-structure trees, and one annotated with
labeled dependency trees. The sentences in the two
representations are aligned at token and POS levels.
Both representations reflect the predicate-argument
structure of the same sentence, but this information
is expressed using different formal terms and thus
results in different tree structures.
Since some of our native data sets are larger than
others, we provide the training set in two sizes: Full
containing all sentences in the standard training set
of the language, and 5k containing the number of
sentences that is equivalent in size to our smallest
training set (5k sentences). For all languages, the data
has been split into sentences, and the sentences are
parsed and evaluated independently of one another.
3.3 Parsing Scenarios
In the shared task, we consider three parsing scenar-
ios, depending on how much of the morphological
information is provided. The scenarios are listed
below, in increasing order of difficulty.
? Gold: In this scenario, the parser is provided
with unambiguous gold morphological segmen-
tation, POS tags, and morphological features for
each input token.
? Predicted: In this scenario, the parser is pro-
vided with disambiguated morphological seg-
mentation. However, the POS tags and mor-
phological features for each input segment are
unknown.
Scenario Segmentation PoS+Feat. Tree
Gold X X ?
Predicted X 1-best ?
Raw (1-best) 1-best 1-best ?
Raw (all) ? ? ?
Table 1: A summary of the parsing and evaluation sce-
narios. X depicts gold information, ? depicts unknown
information, to be predicted by the system.
? Raw: In this scenario, the parser is provided
with morphologically ambiguous input. The
morphological segmentation, POS tags, and
morphological features for each input token are
unknown.
The Predicted and Raw scenarios require predict-
ing morphological analyses. This may be done using
a language-specific morphological analyzer, or it may
be done jointly with parsing. We provide inputs that
support these different scenarios:
? Predicted: Gold treebank segmentation is given
to the parser. The POS tags assignment and mor-
phological features are automatically predicted
by the parser or by an external resource.
? Raw (1-best): The 1st-best segmentation and
POS tags assignment is predicted by an external
resource and given to the parser.
? Raw (all): All possible segmentations and POS
tags are specified by an external resource. The
parser selects jointly a segmentation and a tree.
An overview of all shown in table 1. For languages
in which terminals equal tokens, only Gold and Pre-
dicted scenarios are considered. For Semitic lan-
guages we further provide input for both Raw (1-
best) and Raw (all) scenarios. 3
3.4 Evaluation Metrics
This task features nine languages, two different repre-
sentation types and three different evaluation scenar-
ios. In order to evaluate the quality of the predicted
structures in the different tracks, we use a combina-
tion of evaluation metrics that allow us to compare
the systems along different axes.
3The raw Arabic lattices were made available later than the
other data. They are now included in the shared task release.
150
In this section, we formally define the different
evaluation metrics and discuss how they support sys-
tem comparison. Throughout this paper, we will be
referring to different evaluation dimensions:
? Cross-Parser Evaluation in Gold/Predicted
Scenarios. Here, we evaluate the results of dif-
ferent parsers on a single data set in the Gold
or Predicted setting. We use standard evalu-
ation metrics for the different types of anal-
yses, that is, ParsEval (Black et al, 1991)
on phrase-structure trees, and Labeled At-
tachment Scores (LAS) (Buchholz and Marsi,
2006) for dependency trees. Since ParsEval is
known to be sensitive to the size and depth of
trees (Rehbein and van Genabith, 2007b), we
also provide the Leaf-Ancestor metric (Samp-
son and Babarczy, 2003), which is less sensitive
to the depth of the phrase-structure hierarchy. In
both scenarios we also provide metrics to evalu-
ate the prediction of MultiWord Expressions.
? Cross-Parser Evaluation in Raw Scenarios.
Here, we evaluate the results of different parsers
on a single data set in scenarios where morpho-
logical segmentation is not known in advance.
When a hypothesized segmentation is not iden-
tical to the gold segmentation, standard evalua-
tion metrics such as ParsEval and Attachment
Scores break down. Therefore, we use TedEval
(Tsarfaty et al, 2012b), which jointly assesses
the quality of the morphological and syntactic
analysis in morphologically-complex scenarios.
? Cross-Framework Evaluation. Here, we com-
pare the results obtained by a dependency parser
and a constituency parser on the same set of sen-
tences. In order to avoid comparing apples and
oranges, we use the unlabeled TedEval metric,
which converts all representation types inter-
nally into the same kind of structures, called
function trees. Here we use TedEval?s cross-
framework protocol (Tsarfaty et al, 2012a),
which accomodates annotation idiosyncrasies.
? Cross-Language Evaluation. Here, we com-
pare parsers for the same representation type
across different languages. Conducting a com-
plete and faithful evaluation across languages
would require a harmonized universal annota-
tion scheme (possibly along the lines of (de
Marneffe and Manning, 2008; McDonald et al,
2013; Tsarfaty, 2013)) or task based evaluation.
As an approximation we use unlabeled TedEval.
Since it is unlabeled, it is not sensitive to label
set size. Since it internally uses function-trees,
it is less sensitive to annotation idiosyncrasies
(e.g., head choice) (Tsarfaty et al, 2011).
The former two dimensions are evaluated on the full
sets. The latter two are evaluated on smaller, compa-
rable, test sets. For completeness, we provide below
the formal definitions and essential modifications of
the evaluation software that we used.
3.4.1 Evaluation Metrics for Phrase Structures
ParsEval The ParsEval metrics (Black et al, 1991)
are evaluation metrics for phrase-structure trees. De-
spite various shortcomings, they are the de-facto stan-
dard for system comparison on phrase-structure pars-
ing, used in many campaigns and shared tasks (e.g.,
(K?bler, 2008; Petrov and McDonald, 2012)). As-
sume that G and H are phrase-structure gold and
hypothesized trees respectively, each of which is rep-
resented by a set of tuples (i, A, j) where A is a
labeled constituent spanning from i to j. Assume
that g is the same as G except that it discards the
root, preterminal, and terminal nodes, likewise for h
and H . The ParsEval scores define the accuracy of
the hypothesis in terms of the normalized size of the
intersection of the constituent sets.
Precision(g, h) = |g?h||h|
Recall(g, h) = |g?h||g|
F1(g, h) = 2?P?RP+R
We evaluate accuracy on phrase-labels ignoring any
further decoration, as it is in standard practices.
Evalb, the standard software that implements Par-
sEval,4 takes a parameter file and ignores the labels
specified therein. As usual, we ignore root and POS
labels. Contrary to the standard practice, we do take
punctuation into account. Note that, as opposed to the
official version, we used the SANCL?2012 version5
modified to actually penalize non-parsed trees.
4http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Evalb
5Modified by Petrov and McDonald (2012) to be less sensi-
tive to punctuation errors.
151
Leaf-Ancestor The Leaf-Ancestor metric (Samp-
son and Babarczy, 2003) measures the similarity be-
tween the path from each terminal node to the root
node in the output tree and the corresponding path
in the gold tree. The path consists of a sequence of
node labels between the terminal node and the root
node, and the similarity of two paths is calculated
by using the Levenshtein distance. This distance is
normalized by path length, and the score of the tree
is an aggregated score of the values for all terminals
in the tree (xt is the leaf-ancestor path of t in tree x).
LA(h, g) =
?
t?yield(g) Lv(ht,gt)/(len(ht)+len(gt))
|yield(g)|
This metric was shown to be less sensitive to dif-
ferences between annotation schemes in (K?bler et
al., 2008), and was shown by Rehbein and van Gen-
abith (2007a) to evaluate trees more faithfully than
ParsEval in the face of certain annotation decisions.
We used the implementation of Wagner (2012).6
3.4.2 Evaluation Metrics for Dependency
Structures
Attachment Scores Labeled and Unlabeled At-
tachment scores have been proposed as evaluation
metrics for dependency parsing in the CoNLL shared
tasks (Buchholz and Marsi, 2006; Nivre et al, 2007a)
and have since assumed the role of standard metrics
in multiple shared tasks and independent studies. As-
sume that g, h are gold and hypothesized dependency
trees respectively, each of which is represented by
a set of arcs (i, A, j) where A is a labeled arc from
terminal i to terminal j. Recall that in the gold and
predicted settings, |g| = |h| (because the number of
terminals determines the number of arcs and hence it
is fixed). So Labeled Attachment Score equals preci-
sion and recall, and it is calculated as a normalized
size of the intersection between the sets of gold and
parsed arcs.7
Precision(g, h) = |g?h||g|
Recall(g, h) = |g?h||h|
LAS(g, h) = |g?h||g| =
|g?h|
|h|
6The original version is available at
http://www.grsampson.net/Resources.
html, ours at http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Leaf.
7http://ilk.uvt.nl/conll/software.html.
3.4.3 Evaluation Metrics for Morpho-Syntactic
Structures
TedEval The TedEval metrics and protocols have
been developed by Tsarfaty et al (2011), Tsarfaty
et al (2012a) and Tsarfaty et al (2012b) for coping
with non-trivial evaluation scenarios, e.g., comparing
parsing results across different frameworks, across
representation theories, and across different morpho-
logical segmentation hypotheses.8 Contrary to the
previous metrics, which view accuracy as a normal-
ized intersection over sets, TedEval computes the ac-
curacy of a parse tree based on the tree-edit distance
between complete trees. Assume a finite set of (pos-
sibly parameterized) edit operations A = {a1....an},
and a cost function c : A ? 1. An edit script is the
cost of a sequence of edit operations, and the edit dis-
tance of g, h is the minimal cost edit script that turns
g into h (and vice versa). The normalized distance
subtracted from 1 provides the level of accuracy on
the task. Formally, the TedEval score on g, h is de-
fined as follows, where ted is the tree-edit distance,
and the |x| (size in nodes) discards terminals and root
nodes.
TedEval(g, h) = 1?
ted(g, h)
|g|+ |h|
In the gold scenario, we are not allowed to manipu-
late terminal nodes, only non-terminals. In the raw
scenarios, we can add and delete both terminals and
non-terminals so as to match both the morphological
and syntactic hypotheses.
3.4.4 Evaluation Metrics for
Multiword-Expression Identification
As pointed out in section 3.1, the French data set is
provided with tree structures encoding both syntactic
information and groupings of terminals into MWEs.
A given MWE is defined as a continuous sequence of
terminals, plus a POS tag. In the constituency trees,
the POS tag of the MWE is an internal node of the
tree, dominating the sequence of pre-terminals, each
dominating a terminal. In the dependency trees, there
is no specific node for the MWE as such (the nodes
are the terminals). So, the first token of a MWE is
taken as the head of the other tokens of the same
MWE, with the same label (see section 4.4).
8http://www.tsarfaty.com/unipar/
download.html.
152
To evaluate performance on MWEs, we use the
following metrics.
? R_MWE, P_MWE, and F_MWE are recall, pre-
cision, and F-score over full MWEs, in which
a predicted MWE counts as correct if it has the
correct span (same group as in the gold data).
? R_MWE +POS, R_MWE +POS, and F_MWE
+POS are defined in the same fashion, except
that a predicted MWE counts as correct if it has
both correct span and correct POS tag.
? R_COMP, R_COMP, and F_COMP are recall,
precision and F-score over non-head compo-
nents of MWEs: a non-head component of MWE
counts as correct if it is attached to the head of
the MWE, with the specific label that indicates
that it is part of an MWE.
4 The SPMRL 2013 Data Sets
4.1 The Treebanks
We provide data from nine different languages anno-
tated with two representation types: phrase-structure
trees and dependency trees.9 Statistics about size,
average length, label set size, and other character-
istics of the treebanks and schemes are provided in
Table 2. Phrase structures are provided in an ex-
tended bracketed style, that is, Penn Treebank brack-
eted style where every labeled node may be extended
with morphological features expressed. Dependency
structures are provided in the CoNLL-X format.10
For any given language, the dependency and con-
stituency treebanks are aligned at the token and ter-
minal levels and share the same POS tagset and mor-
phological features. That is, any form in the CoNLL
format is a terminal of the respective bracketed tree.
Any CPOS label in the CoNLL format is the pre-
terminal dominating the terminal in the bracketed
tree. The FEATS in the CoNLL format are repre-
sented as dash-features decorated on the respective
pre-terminal node in the bracketed tree. See Fig-
ure 1(a)?1(b) for an illustration of this alignment.
9Additionally, we provided the data in TigerXML format
(Brants et al, 2002) for phrase structure trees containing cross-
ing branches. This allows the use of more powerful parsing
formalisms. Unfortunately, we received no submissions for this
data, hence we discard them in the rest of this overview.
10See http://ilk.uvt.nl/conll/.
For ambiguous morphological analyses, we pro-
vide the mapping of tokens to different segmentation
possibilities through lattice files. See Figure 1(c) for
an illustration, where lattice indices mark the start
and end positions of terminals.
For each of the treebanks, we provide a three-way
dev/train/set split and another train set containing the
first 5k sentences of train (5k). This section provides
the details of the original treebanks and their anno-
tations, our data-set preparation, including prepro-
cessing and data splits, cross-framework alignment,
and the prediction of morphological information in
non-gold scenarios.
4.2 The Arabic Treebanks
Arabic is a morphologically complex language which
has rich inflectional and derivational morphology. It
exhibits a high degree of morphological ambiguity
due to the absence of the diacritics and inconsistent
spelling of letters, such as Alif and Ya. As a conse-
quence, the Buckwalter Standard Arabic Morpholog-
ical Analyzer (Buckwalter, 2004; Graff et al, 2009)
produces an average of 12 analyses per word.
Data Sets The Arabic data set contains two tree-
banks derived from the LDC Penn Arabic Treebanks
(PATB) (Maamouri et al, 2004b):11 the Columbia
Arabic Treebank (CATiB) (Habash and Roth, 2009),
a dependency treebank, and the Stanford version
of the PATB (Green and Manning, 2010), a phrase-
structure treebank. We preprocessed the treebanks
to obtain strict token matching between the treebanks
and the morphological analyses. This required non-
trivial synchronization at the tree token level between
the PATB treebank, the CATiB treebank and the mor-
phologically predicted data, using the PATB source
tokens and CATiB feature word form as a dual syn-
chronized pivot.
The Columbia Arabic Treebank The Columbia
Arabic Treebank (CATiB) uses a dependency repre-
sentation that is based on traditional Arabic grammar
and that emphasizes syntactic case relations (Habash
and Roth, 2009; Habash et al, 2007). The CATiB
treebank uses the word tokenization of the PATB
11The LDC kindly provided their latest version of the Arabic
Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al,
2005), PATB 2 v3.1 (Maamouri et al, 2004a) and PATB 3 v3.3.
(Maamouri et al, 2009)
153
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
train:
#Sents 15,762 7,577 14,759 40,472 8,146 23,010 6,578
#Tokens 589,220 96,368 443,113 719,532 170,141 351,184 68,424
Lex. Size 36,906 25,136 27,470 77,222 40,782 11,1540 22,911
Avg. Length 37.38 12.71 30.02 17.77 20.88 15.26 10.40
Ratio #NT/#Tokens 0.19 0.82 0.34 0.60 0.59 0.60 0.94
Ratio #NT/#Sents 7.40 10.50 10.33 10.70 12.38 9.27 9.84
#Non Terminals 22 12 32 25 16 8 34
#POS tags 35 25 29 54 16 1,975 29
#total NTs 116,769 79,588 152,463 433,215 100,885 213,370 64,792
Dep. Label Set Size 9 31 25 43 417 22 27
train5k:
#Sents 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000
#Tokens 224,907 61,905 150,984 87,841 128,046 109,987 68,336 52,123 76,357
Lex. Size 19,433 18,405 15,480 17,421 15,975 29,009 29,715 18,632 14,110
Avg. Length 44.98 12.38 30.19 17.56 25.60 21.99 13.66 10.42 15.27
Ratio #NT/#Tokens 0.15 0.83 0.34 0.60 0.42 0.57 0.68 0.94 0.58
Ratio #NT/#Sents 7.18 10.33 10.32 10.58 10.97 12.57 9.29 9.87 8.96
#Non Terminals 22 12 29 23 60 16 8 34 8
#POS Tags 35 25 29 51 50 16 972 29 25
#total NTs 35,909 5,1691 51,627 52,945 54,856 62,889 46,484 49,381 44,845
Dep. Label Set Size 9 31 25 42 43 349 20 27 61
dev:
#Sents 1,985 948 1,235 5,000 500 1,051 2,066 821 494
#Tokens 73,932 13,851 38,820 76,704 11,301 29,989 30,480 8,600 9,341
Lex. Size 12,342 5,551 6,695 15,852 3,175 10,673 15,826 4,467 2,690
Avg. Length 37.24 14.61 31.43 15.34 22.60 28.53 14.75 10.47 18.90
Ratio #NT/#Tokens 0.19 0.74 0.33 0.63 0.47 047 0.63 0.94 0.48
Ratio #NT/#Sents 7.28 10.92 10.48 9.71 10.67 13.66 9.33 9.90 9.10
#Non Terminals 21 11 27 24 55 16 8 31 8
#POS Tags 32 23 29 50 47 16 760 29 24
#total NTs 14,452 10,356 12,951 48,560 5,338 14,366 19,283 8,132 4,496
Dep. Label Set Size 9 31 25 41 42 210 22 26 59
test:
#Sents 1959 946 2541 5000 716 1009 2287 822 666
#Tokens 73878 11457 75216 92004 16998 19908 33766 8545 10690
Lex. Size 12254 4685 10048 20149 4305 7856 16475 4336 3112
Avg. Length 37.71 12.11 29.60 18.40 23.74 19.73 14.76 10.39 16.05
Ratio #NT/#Tokens 0.19 0.83 0.34 0.60 0.47 0.62 0.61 0.95 0.57
Ratio #NT/#Sents 7.45 10.08 10.09 11.07 11.17 12.26 9.02 9.94 9.18
#Non Terminals 22 12 30 23 54 15 8 31 8
#POS Tags 33 22 30 52 46 16 809 27 25
#total NTs 14,610 9,537 25,657 55,398 8,001 12,377 20,640 8,175 6,118
Dep. Label Set Size 9 31 26 42 41 183 22 27 56
Table 2: Overview of participating languages and treebank properties. ?Sents? = number of sentences, ?Tokens? =
number of raw surface forms. ?Lex. size? and ?Avg. Length? are computed in terms of tagged terminals. ?NT? = non-
terminals in constituency treebanks, ?Dep Labels? = dependency labels on the arcs of dependency treebanks. ? A more
comprehensive table is available at http://www.spmrl.org/spmrl2013-sharedtask.html/#Prop.
154
(a) Constituency Tree
% % every line is a single tree in a bracketed Penn Treebank format
(ROOT (S (NP ( NNP-#pers=3|num=sing# John))(VP ( VB-#pers=3|num=sing# likes)(NP ( NNP-#pers=3|num=sing# Mary)))))
(b) Dependency Tree
%% every line describes a terminal: terminal-id form lemma CPOS FPOS FEATS Head Rel PHead PRel
1 John John NNP NNP pers=3|num=sing 2 sbj _ _
2 likes like VB VB pers=3|num=sing 0 root _ _
3 Mary Mary NNP NNP pers=3|num=sing 2 obj _ _
Input Lattice
0 1 2 3 4 5 6
1:AIF/NN
1:AIF/VB
1:AIF/NNT
2:LA/RB
3:NISH/VB
3:NISH/NN
4:L/PREP
4:LHSTIR/VB
4:HSTIR/VB
5:ZAT/PRP
%% every line describes a terminal: start-id end-id form lemma CPOS FPOS FEATS token-id
0 1 AIF AIF NN NN _ 1
0 1 AIF AIF NNT NNT _ 1
0 1 AIF AIF VB VB _ 1
1 2 LA LA RB RB _ 2
2 3 NISH NISH VB VB _ 3
2 3 NISH NISH NN NN _ 3
3 5 LHSTIR HSTIR VB VB _ 4
3 4 L L PREP PREP _ 4
4 5 HSTIR HSTIR VB VB _ 4
5 6 ZAT ZAT PRP PRP _ 5
Figure 1: File formats. Trees (a) and (b) are aligned constituency and dependency trees for a mockup English example.
Boxed labels are shared across the treebanks. Figure (c) shows an ambiguous lattice. The red part represents the yield
of the gold tree. For brevity, we use empty feature columns, but of course lattice arcs may carry any morphological
features, in the FEATS CoNLL format.
and employs a reduced POS tagset consisting of six
tags only: NOM (non-proper nominals including
nouns, pronouns, adjectives and adverbs), PROP
(proper nouns), VRB (active-voice verbs), VRB-
PASS (passive-voice verbs), PRT (particles such as
prepositions or conjunctions) and PNX (punctuation).
(This stands in extreme contrast with the Buckwalter
Arabic tagset (PATB official tagset) which is almost
500 tags.) To obtain these dependency trees, we used
the constituent-to-dependency tool (Habash and Roth,
2009). Additional CATiB trees were annotated di-
rectly, but we only use the portions that are converted
from phrase-structure representation, to ensure that
the constituent and dependency yields can be aligned.
The Stanford Arabic Phrase Structure Treebank
In order to stay compatible with the state of the art,
we provide the constituency data set with most of the
pre-processing steps of Green and Manning (2010),
as they were shown to improve baseline performance
on the PATB parsing considerably.12
To convert the original PATB to preprocessed
phrase-structure trees ? la Stanford, we first discard
all trees dominated by X, which indicates errors and
non-linguistic text. At the phrasal level, we collapse
unary chains with identical categories like NP? NP.
We finally remove all traces, but, unlike Green and
Manning (2010), we keep all function tags.
In the original Stanford instance, the pre-terminal
morphological analyses were mapped to the short-
ened Bies tag set provided with the treebank (where
Determiner markers, ?DT?, were added to definite
noun and adjectives, resulting in 32 POS tags). Here
we use the Kulick tagset (Kulick et al, 2006) for
12Both the corpus split and pre-processing code are available
with the Stanford parser at http://nlp.stanford.edu/
projects/arabic.shtml.
155
pre-terminal categories in the phrase-structure trees,
where the Bies tag set is included as a morphological
feature (stanpos) in our PATB instance.
Adapting the Data to the Shared Task We con-
verted the CATiB representation to the CoNLL rep-
resentation and added a ?split-from-previous? and
?split-from-next? markers as in LDC?s tree-terminal
fields.
A major difference between the CATiB treebank
and the Stanford treebank lies in the way they han-
dle paragraph annotations. The original PATB con-
tains sequences of annotated trees that belong to a
same discourse unit (e.g., paragraph). While the
CATiB conversion tool considers each sequence a
single parsing unit, the Stanford pre-processor treats
each such tree structure rooted at S, NP or Frag as
a tree spanning a single sentence. To be compati-
ble with the predicted morphology data which was
bootstrapped and trained on the CATiB interpretation,
we deterministically modified the original PATB by
adding pseudo XP root nodes, so that the Stanford
pre-proprecessor will generate the same tree yields
as the CATiB treebank.
Another important aspect of preprocessing (often-
delegated as a technicality in the Arabic parsing lit-
erature) is the normalization of token forms. Most
Arabic parsing work used transliterated text based on
the schemes proposed by Buckwalter (2002). The
transliteration schemes exhibit some small differ-
ences, but enough to increase the out-of-vocabulary
rate by a significant margin (on top of strictly un-
known morphemes). This phenomenon is evident in
the morphological analysis lattices (in the predicted
dev set there is a 6% OOV rate without normalization,
and half a point reduction after normalization is ap-
plied, see (Habash et al, 2009b; Green and Manning,
2010)). This rate is much lower for gold tokenized
predicted data (with an OOV rate of only 3.66%,
similar to French for example). In our data set, all
tokens are minimally normalized: no diacritics, no
normalization.13
Data Splits For the Arabic treebanks, we use the
data split recommended by the Columbia Arabic and
Dialect Modeling (CADiM) group (Diab et al, 2013).
13Except for the minimal normalization present in MADA?s
back-end tools. This script was provided to the participants.
The data of the LDC first three annotated Arabic Tree-
banks (ATB1, ATB2 and ATB3) were divided into
roughly a 10/80/10% dev/train/test split by word vol-
ume. When dividing the corpora, document bound-
aries were maintained. The train5k files are simply
the first 5,000 sentences of the training files.
POS Tagsets Given the richness of Arabic mor-
phology, there are multiple POS tag sets and tokeniza-
tion schemes that have been used by researchers, (see,
e.g., Marton et al (2013a)). In the shared task, we fol-
low the standard PATB tokenization which splits off
several categories of orthographic clitics, but not the
definite article Al+. On top of that, we consider three
different POS tag sets with different degrees of gran-
ularity: the Buckwalter tag set (Buckwalter, 2004),
the Kulick Reduced Tag set (Kulick et al, 2006), and
the CATiB tag set (Habash et al, 2009a), considering
that granularity of the morphological analyses may
affect syntactic processing. For more information see
Habash (2010).
Predicted Morphology To prepare input for the
Raw scenarios (?3.3), we used the MADA+TOKAN
system (Habash et al, 2009b). MADA is a system
for morphological analysis and disambiguation of
Arabic. It can predict the 1-best tokenization, POS
tags, lemmas and diacritization in one fell swoop.
The MADA output was also used to generate the
lattice files for the Raw-all scenario.
To generate input for the gold token / predicted
tag input scenario, we used Morfette (Chrupa?a et al,
2008), a joint lemmatization and POS tagging model
based on an averaged perceptron. We generated two
tagging models, one trained with the Buckwalter tag
set, and the other with the Kulick tag set. Both were
mapped back to the CATiB POS tag set such that all
predicted tags are contained in the feature field.14
4.3 The Basque Treebank
Basque is an agglutinative language with a high ca-
pacity to generate inflected wordforms, with free
constituent order of sentence elements with respect
to the main verb. Contrary to many other treebanks,
the Basque treebank was originally annotated with
dependency trees, which were later on converted to
constituency trees.
14A conversion script from the rich Buckwalter tagset to
CoNLL-like features was provided to the participants.
156
The Basque Dependency Treebank (BDT) is a
dependency treebank in its original design, due to
syntactic characteristics of Basque such as its free
word order. Before the syntactic annotation, mor-
phological analysis was performed, using the Basque
morphological analyzer of Aduriz et al (2000). In
Basque each lemma can generate thousands of word-
forms ? differing in morphological properties such
as case, number, tense, or different types of subordi-
nation for verbs. If only POS category ambiguity is
resolved, the analyses remain highly ambiguous.
For the main POS category, there is an average of
1.55 interpretations per wordform, which rises to 2.65
for the full morpho-syntactic information, resulting
in an overall 64% of ambiguous wordforms. The
correct analysis was then manually chosen.
The syntactic trees were manually assigned. Each
word contains its lemma, main POS category, POS
subcategory, morphological features, and the la-
beled dependency relation. Each form indicates mor-
phosyntactic features such as case, number and type
of subordination, which are relevant for parsing.
The first version of the Basque Dependency Tree-
bank, consisting of 3,700 sentences (Aduriz et al,
2003), was used in the CoNLL 2007 Shared Task on
Dependency Parsing (Nivre et al, 2007a). The cur-
rent shared task uses the second version of the BDT,
which is the result of an extension and redesign of the
original requirements, containing 11,225 sentences
(150,000 tokens).
The Basque Constituency Treebank (BCT) was
created as part of the CESS-ECE project, where the
main aim was to obtain syntactically annotated con-
stituency treebanks for Catalan, Spanish and Basque
using a common set of syntactic categories. BCT
was semi-automatically derived from the dependency
version (Aldezabal et al, 2008). The conversion pro-
duced complete constituency trees for 80% of the
sentences. The main bottlenecks have been sentence
connectors and non-projective dependencies which
could not be straightforwardly converted into projec-
tive tree structures, requiring a mechanism similar to
traces in the Penn English Treebank.
Adapting the Data to the Shared Task As the
BCT did not contain all of the original non-projective
dependency trees, we selected the set of 8,000 match-
ing sentences in both treebanks for the shared task.15
This implies that around 2k trees could not be gen-
erated and therefore were discarded. Furthermore,
the BCT annotation scheme does not contain attach-
ment for most of the punctuation marks, so those
were inserted into the BCT using a simple lower-left
attachment heuristic. The same goes for some con-
nectors that could not be aligned in the first phase.
Predicted Morphology In order to obtain pre-
dicted tags for the non-gold scenarios, we used the
following pipeline. First, morphological analysis as
described above was performed, followed by a dis-
ambiguation step. At that point, it is hard to obtain a
single interpretation for each wordform, as determin-
ing the correct interpretation for each wordform may
require knowledge of long-distance elements on top
of the free constituency order of the main phrasal el-
ements in Basque. The disambiguation is performed
by the module by Ezeiza et al (1998), which uses
a combination of knowledge-based disambiguation,
by means of Constraint Grammar (Karlsson et al,
1995; Aduriz et al, 1997), and a posterior statistical
disambiguation module, using an HMM.16
For the shared task data, we chose a setting that
disambiguates most word forms, and retains ? 97%
of the correct interpretations, leaving an ambiguity
level of 1.3 interpretations. For the remaining cases
of ambiguity, we chose the first interpretation, which
corresponds to the most frequent option. This leaves
open the investigation of more complex approaches
for selecting the most appropriate reading.17
4.4 The French Treebank
French is not a morphologically rich language per se,
though its inflectional system is richer than that of
English, and it also exhibits a limited amount of word
order variation occurring at different syntactic levels
including the word level (e.g. pre- or post-nominal
15We generated a 80/10/10 split, ? train/dev/test ? The first 5k
sentences of the train set were used as a basis for the train5k.
16Note that the statistical module can be parametrized accord-
ing to the level of disambiguation to trade off precision and
recall. For example, disambiguation based on the main cate-
gories (abstracting over morpho-syntactic features) maintains
most of the correct interpretations but still gives an output with
several interpretations per wordform.
17This is not an easy task. The ambiguity left is the hardest to
solve given that the knowledge-based and statistical disambigua-
tion processes have not been able to pick out a single reading.
157
adjective, pre- or post-verbal adverbs) and the phrase
level (e.g. possible alternations between post verbal
NPs and PPs). It also has a high degree of multi-
word expressions, that are often ambiguous with a
literal reading as a sequence of simple words. The
syntactic and MWE analysis shows the same kind of
interaction (though to a lesser extent) as morphologi-
cal and syntactic interaction in Semitic languages ?
MWEs help parsing, and syntactic information may
be required to disambiguate MWE identification.
The Data Set The French data sets were gener-
ated from the French Treebank (Abeill? et al, 2003),
which consists of sentences from the newspaper Le
Monde, manually annotated with phrase structures
and morphological information. Part of the treebank
trees are also annotated with grammatical function
tags for dependents of verbs. In the SPMRL shared
task release, we used only this part, consisting of
18,535 sentences,18 split into 14,759 sentences for
training, 1,235 sentences for development, and 2,541
sentences for the final evaluation.19
Adapting the Data to the Shared Task The con-
stituency trees are provided in an extended PTB
bracketed format, with morphological features at the
pre-terminal level only. They contain slight, auto-
matically performed, modifications with respect to
the original trees of the French treebank. The syntag-
matic projection of prepositions and complementiz-
ers was normalized, in order to have prepositions and
complementizers as heads in the dependency trees
(Candito et al, 2010).
The dependency representations are projective de-
pendency trees, obtained through automatic conver-
sion from the constituency trees. The conversion pro-
cedure is an enhanced version of the one described
by Candito et al (2010).
Both the constituency and the dependency repre-
sentations make use of coarse- and fine-grained POS
tags (CPOS and FPOS respectively). The CPOS are
the categories from the original treebank. The FPOS
18The process of functional annotation is still ongoing, the
objective of the FTB providers being to have all the 20000 sen-
tences annotated with functional tags.
19The first 9,981 training sentences correspond to the canoni-
cal 2007 training set. The development set is the same and the
last 1235 sentences of the test set are those of the canonical test
set.
are merged using the CPOS and specific morphologi-
cal information such as verbal mood, proper/common
noun distinction (Crabb? and Candito, 2008).
Multi-Word Expressions The main difference
with respect to previous releases of the bracketed
or dependency versions of the French treebank
lies in the representation of multi-word expressions
(MWEs). The MWEs appear in an extended format:
each MWE bears an FPOS20 and consists of a se-
quence of terminals (hereafter the ?components? of
the MWE), each having their proper CPOS, FPOS,
lemma and morphological features. Note though that
in the original treebank the only gold information
provided for a MWE component is its CPOS. Since
leaving this information blank for MWE components
would have provided a strong cue for MWE recog-
nition, we made sure to provide the same kind of
information for every terminal, whether MWE com-
ponent or not, by providing predicted morphological
features, lemma, and FPOS for MWE components
(even in the ?gold? section of the data set). This infor-
mation was predicted by the Morfette tool (Chrupa?a
et al, 2008), adapted to French (Seddah et al, 2010).
In the constituency trees, each MWE corresponds
to an internal node whose label is the MWE?s FPOS
suffixed by a +, and which dominates the component
pre-terminal nodes.
In the dependency trees, there is no ?node? for a
MWE as a whole, but one node (a terminal in the
CoNLL format) per MWE component. The first com-
ponent of a MWE is taken as the head of the MWE.
All subsequent components of the MWE depend on
the first one, with the special label dep_cpd. Further-
more, the first MWE component bears a feature mwe-
head equal to the FPOS of the MWE. For instance,
the MWE la veille (the day before) is an adverb, con-
taining a determiner component and a common noun
component. Its bracketed representation is (ADV+
(DET la) (NC veille)), and in the dependency repre-
sentation, the noun veille depends on the determiner
la, which bears the feature mwehead=ADV+.
Predicted Morphology For the predicted mor-
phology scenario, we provide data in which the
mwehead has been removed and with predicted
20In the current data, we did not carry along the lemma and
morphological features pertaining to the MWE itself, though this
information is present in the original trees.
158
FPOS, CPOS, lemma, and morphological features,
obtained by training Morfette on the whole train set.
4.5 The German Treebank
German is a fusional language with moderately free
word order, in which verbal elements are fixed in
place and non-verbal elements can be ordered freely
as long as they fulfill the ordering requirements of
the clause (H?hle, 1986).
The Data Set The German constituency data set
is based on the TiGer treebank release 2.2.21 The
original annotation scheme represents discontinuous
constituents such that all arguments of a predicate
are always grouped under a single node regardless of
whether there is intervening material between them
or not (Brants et al, 2002). Furthermore, punctua-
tion and several other elements, such as parentheses,
are not attached to the tree. In order to make the
constituency treebank usable for PCFG parsing, we
adapted this treebank as described shortly.
The conversion of TiGer into dependencies is a
variant of the one by Seeker and Kuhn (2012), which
does not contain empty nodes. It is based on the same
TiGer release as the one used for the constituency
data. Punctuation was attached as high as possible,
without creating any new non-projective edges.
Adapting the Data to the Shared Task For
the constituency version, punctuation and other
unattached elements were first attached to the tree.
As attachment target, we used roughly the respec-
tive least common ancestor node of the right and
left terminal neighbor of the unattached element (see
Maier et al (2012) for details), and subsequently, the
crossing branches were resolved.
This was done in three steps. In the first step, the
head daughters of all nodes were marked using a
simple heuristic. In case there was a daughter with
the edge label HD, this daughter was marked, i.e.,
existing head markings were honored. Otherwise, if
existing, the rightmost daughter with edge label NK
(noun kernel) was marked. Otherwise, as default, the
leftmost daughter was marked. In a second step, for
each continuous part of a discontinuous constituent,
a separate node was introduced. This corresponds
21This version is available from http://www.ims.
uni-stuttgart.de/forschung/ressourcen/
korpora/tiger.html
to the "raising" algorithm described by Boyd (2007).
In a third steps, all those newly introduced nodes
that did not cover the head daughter of the original
discontinuous node were deleted. For the second
and the third step, we used the same script as for the
Swedish constituency data.
Predicted Morphology For the predicted scenario,
a single sequence of POS tags and morphologi-
cal features has been assigned using the MATE
toolchain via a model trained on the train set via cross-
validation on the training set. The MATE toolchain
was used to provide predicted annotation for lem-
mas, POS tags, morphology, and syntax. In order to
achieve the best results for each annotation level, a
10-fold jackknifing was performed to provide realis-
tic features for the higher annotation levels. The pre-
dicted annotation of the 5k training set were copied
from the full data set.22
4.6 The Hebrew Treebank
Modern Hebrew is a Semitic language, characterized
by inflectional and derivational (templatic) morphol-
ogy and relatively free word order. The function
words for from/to/like/and/when/that/the are prefixed
to the next token, causing severe segmentation ambi-
guity for many tokens. In addition, Hebrew orthogra-
phy does not indicate vowels in modern texts, leading
to a very high level of word-form ambiguity.
The Data Set Both the constituency and the de-
pendency data sets are derived from the Hebrew
Treebank V2 (Sima?an et al, 2001; Guthmann et
al., 2009). The treebank is based on just over 6000
sentences from the daily newspaper ?Ha?aretz?, man-
ually annotated with morphological information and
phrase-structure trees and extended with head infor-
mation as described in Tsarfaty (2010, ch. 5). The
unlabeled dependency version was produced by con-
version from the constituency treebank as described
in Goldberg (2011). Both the constituency and depen-
dency trees were annotated with a set grammatical
function labels conforming to Unified Stanford De-
pendencies by Tsarfaty (2013).
22We also provided a predicted-all scenario, in which we
provided morphological analysis lattices with POS and mor-
phological information derived from the analyses of the SMOR
derivational morphology (Schmid et al, 2004). These lattices
were not used by any of the participants.
159
Adapting the Data to the Shared Task While
based on the same trees, the dependency and con-
stituency treebanks differ in their POS tag sets, as
well as in some of the morphological segmentation
decisions. The main effort towards the shared task
was unifying the two resources such that the two tree-
banks share the same lexical yields, and the same
pre-terminal labels. To this end, we took the layering
approach of Goldberg et al (2009), and included two
levels of POS tags in the constituency trees. The
lower level is lexical, conforming to the lexical re-
source used to build the lattices, and is shared by
the two treebanks. The higher level is syntactic, and
follows the tag set and annotation decisions of the
original constituency treebank.23 In addition, we uni-
fied the representation of morphological features, and
fixed inconsistencies and mistakes in the treebanks.
Data Split The Hebrew treebank is one of the
smallest in our language set, and hence it is provided
in only the small (5k) setting. For the sake of com-
parability with the 5k set of the other treebanks, we
created a comparable size of dev/test sets containing
the first and last 500 sentences respectively, where
the rest serve as the 5k training.24
Predicted Morphology The lattices encoding the
morphological ambiguity for the Raw (all) scenario
were produced by looking up the possible analyses
of each input token in the wide-coverage morpholog-
ical analyzer (lexicon) of the Knowledge Center for
Processing Hebrew (Itai and Wintner, 2008; MILA,
2008), with a simple heuristic for dealing with un-
known tokens. A small lattice encoding the possible
analyses of each token was produced separately, and
these token-lattices were concatenated to produce the
sentence lattice. The lattice for a given sentence may
not include the gold analysis in cases of incomplete
lexicon coverage.
The morphologically disambiguated input files for
the Raw (1-best) scenario were produced by run-
ning the raw text through the morphological disam-
23Note that this additional layer in the constituency treebank
adds a relatively easy set of nodes to the trees, thus ?inflating?
the evaluation scores compared to previously reported results.
To compensate, a stricter protocol than is used in this task would
strip one of the two POS layers prior to evaluation.
24This split is slightly different than the split in previous stud-
ies.
biguator (tagger) described in Adler and Elhadad
(2006; Goldberg et al (2008),Adler (2007). The
disambiguator is based on the same lexicon that is
used to produce the lattice files, but utilizes an extra
module for dealing with unknown tokens Adler et al
(2008). The core of the disambiguator is an HMM
tagger trained on about 70M unannotated tokens us-
ing EM, and being supervised by the lexicon.
As in the case of Arabic, we also provided data
for the Predicted (gold token / predicted morphol-
ogy) scenario. We used the same sequence labeler,
Morfette (Chrupa?a et al, 2008), trained on the con-
catenation of POS and morphological gold features,
leading to a model with respectable accuracy.25
4.7 The Hungarian Treebank
Hungarian is an agglutinative language, thus a lemma
can have hundreds of word forms due to derivational
or inflectional affixation (nominal declination and
verbal conjugation). Grammatical information is typ-
ically indicated by suffixes: case suffixes mark the
syntactic relationship between the head and its argu-
ments (subject, object, dative, etc.) whereas verbs
are inflected for tense, mood, person, number, and
the definiteness of the object. Hungarian is also char-
acterized by vowel harmony.26 In addition, there are
several other linguistic phenomena such as causa-
tion and modality that are syntactically expressed in
English but encoded morphologically in Hungarian.
The Data Set The Hungarian data set used in
the shared task is based on the Szeged Treebank,
the largest morpho-syntactic and syntactic corpus
manually annotated for Hungarian. This treebank
is based on newspaper texts and is available in
both constituent-based (Csendes et al, 2005) and
dependency-based (Vincze et al, 2010) versions.
Around 10k sentences of news domain texts were
made available to the shared task.27 Each word is
manually assigned all its possible morpho-syntactic
25POS+morphology prediction accuracy is 91.95% overall
(59.54% for unseen tokens). POS only prediction accuracy is
93.20% overall (71.38% for unseen tokens).
26When vowel harmony applies, most suffixes exist in two
versions ? one with a front vowel and another one with a back
vowel ? and it is the vowels within the stem that determine which
form of the suffix is selected.
27The original treebank contains 82,000 sentences, 1.2 million
words and 250,000 punctuation marks from six domains.
160
tags and lemmas and the appropriate one is selected
according to the context. Sentences were manu-
ally assigned a constituency-based syntactic struc-
ture, which includes information on phrase structure,
grammatical functions (such as subject, object, etc.),
and subcategorization information (i.e., a given NP
is subcategorized by a verb or an infinitive). The
constituency trees were later automatically converted
into dependency structures, and all sentences were
then manually corrected. Note that there exist some
differences in the grammatical functions applied to
the constituency and dependency versions of the tree-
bank, since some morpho-syntactic information was
coded both as a morphological feature and as dec-
oration on top of the grammatical function in the
constituency trees.
Adapting the Data to the Shared Task Origi-
nally, the Szeged Dependency Treebank contained
virtual nodes for elided material (ELL) and phonolog-
ically covert copulas (VAN). In the current version,
they have been deleted, their daughters have been
attached to the parent of the virtual node, and have
been given complex labels, e.g. COORD-VAN-SUBJ,
where VAN is the type of the virtual node deleted,
COORD is the label of the virtual node and SUBJ is
the label of the daughter itself. When the virtual node
was originally the root of the sentence, its daughter
with a predicative (PRED) label has been selected as
the new root of the sentence (with the label ROOT-
VAN-PRED) and all the other daughters of the deleted
virtual node have been attached to it.
Predicted Morphology In order to provide the
same POS tag set for the constituent and dependency
treebanks, we used the dependency POS tagset for
both treebank instances. Both versions of the tree-
bank are available with gold standard and automatic
morphological annotation. The automatic POS tag-
ging was carried out by a 10-fold cross-validation
on the shared task data set by magyarlanc, a natu-
ral language toolkit for processing Hungarian texts
(segmentation, morphological analysis, POS tagging,
and dependency parsing). The annotation provides
POS tags and deep morphological features for each
input token (Zsibrita et al, 2013).28
28The full data sets of both the constituency and de-
pendency versions of the Szeged Treebank are available at
4.8 The Korean Treebank
The Treebank The Korean corpus is generated by
collecting constituent trees from the KAIST Tree-
bank (Choi et al, 1994), then converting the con-
stituent trees to dependency trees using head-finding
rules and heuristics. The KAIST Treebank consists
of about 31K manually annotated constituent trees
from 97 different sources (e.g., newspapers, novels,
textbooks). After filtering out trees containing an-
notation errors, a total of 27,363 trees with 350,090
tokens are collected.
The constituent trees in the KAIST Treebank29 also
come with manually inspected morphological analy-
sis based on ?eojeol?. An eojeol contains root-forms
of word tokens agglutinated with grammatical affixes
(e.g., case particles, ending markers). An eojeol can
consist of more than one word token; for instance, a
compound noun ?bus stop? is often represented as
one eojeol in Korean, ????????????, which can be
broken into two word tokens,???? (bus) and????????
(stop). Each eojeol in the KAIST Treebank is sepa-
rated by white spaces regardless of punctuation. Fol-
lowing the Penn Korean Treebank guidelines (Han
et al, 2002), punctuation is separated as individual
tokens, and parenthetical notations surrounded by
round brackets are grouped into individual phrases
with a function tag (PRN in our corpus).
All dependency trees are automatically converted
from the constituent trees. Unlike English, which
requires complicated head-finding rules to find the
head of each phrase (Choi and Palmer, 2012), Ko-
rean is a head final language such that the rightmost
constituent in each phrase becomes the head of that
phrase. Moreover, the rightmost conjunct becomes
the head of all other conjuncts and conjunctions in
a coordination phrase, which aligns well with our
head-final strategy.
The constituent trees in the KAIST Treebank do
not consist of function tags indicating syntactic or
semantic roles, which makes it difficult to generate
dependency labels. However, it is possible to gener-
ate meaningful labels by using the rich morphology
in Korean. For instance, case particles give good
the following website: www.inf.u-szeged.hu/rgai/
SzegedTreebank, and magyarlanc is downloadable from:
www.inf.u-szeged.hu/rgai/magyarlanc.
29See Lee et al (1997) for more details about the bracketing
guidelines of the KAIST Treebank.
161
indications of what syntactic roles eojeols with such
particles should take. Given this information, 21
dependency labels were generated according to the
annotation scheme proposed by Choi (2013).
Adapting the Data to the Shared Task All details
concerning the adaptation of the KAIST treebank
to the shared task specifications are found in Choi
(2013). Importantly, the rich KAIST treebank tag set
of 1975 POS tag types has been converted to a list of
CoNLL-like feature-attribute values refining coarse
grained POS categories.
Predicted Morphology Two sets of automatic
morphological analyses are provided for this task.
One is generated by the HanNanum morphological
analyzer.30 The HanNanum morphological ana-
lyzer gives the same morphemes and POS tags as the
KAIST Treebank. The other is generated by the Se-
jong morphological analyzer.31 The Sejong morpho-
logical analyzer gives a different set of morphemes
and POS tags as described in Choi and Palmer (2011).
4.9 The Polish Treebank
The Data Set Sk?adnica is a constituency treebank
of Polish (Wolin?ski et al, 2011; S?widzin?ski and
Wolin?ski, 2010). The trees were generated with
a non-probabilistic DCG parser S?wigra and then
disambiguated and validated manually. The ana-
lyzed texts come from the one-million-token sub-
corpus of the National Corpus of Polish (NKJP,
(Przepi?rkowski et al, 2012)) manually annotated
with morpho-syntactic tags.
The dependency version of Sk?adnica is a re-
sult of an automatic conversion of manually disam-
biguated constituent trees into dependency structures
(Wr?blewska, 2012). The conversion was an entirely
automatic process. Conversion rules were based
on morpho-syntactic information, phrasal categories,
and types of phrase-structure rules encoded within
constituent trees. It was possible to extract dependen-
cies because the constituent trees contain information
about the head of the majority of constituents. For
other constituents, heuristics were defined in order to
select their heads.
30http://kldp.net/projects/hannanum
31http://www.sejong.or.kr
The version of Sk?adnica used in the shared task
comprises parse trees for 8,227 sentences.32
Predicted Morphology For the shared task Pre-
dicted scenario, an automatic morphological an-
notation was generated by the PANTERA tagger
(Acedan?ski, 2010).
4.10 The Swedish Treebank
Swedish is moderately rich in inflections, including
a case system. Word order obeys the verb second
constraint in main clauses but is SVO in subordinate
clauses. Main clause order is freer than in English
but not as free as in some other Germanic languages,
such as German. Also, subject agreement with re-
spect to person and number has been dropped in
modern Swedish.
The Data Set The Swedish data sets are taken
from the Talbanken section of the Swedish Treebank
(Nivre and Megyesi, 2007). Talbanken is a syntacti-
cally annotated corpus developed in the 1970s, orig-
inally annotated according to the MAMBA scheme
(Teleman, 1974) with a syntactic layer consisting
of flat phrase structure and grammatical functions.
The syntactic annotation was later automatically con-
verted to full phrase structure with grammatical func-
tions and from that to dependency structure, as de-
scribed by Nivre et al (2006).
Both the phrase structure and the dependency
version use the functional labels from the original
MAMBA scheme, which provides a fine-grained clas-
sification of syntactic functions with 65 different la-
bels, while the phrase structure annotation (which
had to be inferred automatically) uses a coarse set
of only 8 labels. For the release of the Swedish tree-
bank, the POS level was re-annotated to conform to
the current de facto standard for Swedish, which is
the Stockholm-Ume? tagset (Ejerhed et al, 1992)
with 25 base tags and 25 morpho-syntactic features,
which together produce over 150 complex tags.
For the shared task, we used version 1.2 of the
treebank, where a number of conversion errors in
the dependency version have been corrected. The
phrase structure version was enriched by propagating
morpho-syntactic features from preterminals (POS
32Sk?adnica is available from http://zil.ipipan.waw.
pl/Sklicense.
162
tags) to higher non-terminal nodes using a standard
head percolation table, and a version without crossing
branches was derived using the lifting strategy (Boyd,
2007).
Adapting the Data to the Shared Task Explicit
attribute names were added to the feature field and the
split was changed to match the shared task minimal
training set size.
Predicted Morphology POS tags and morpho-
syntactic features were produced using the Hun-
PoS tagger (Hal?csy et al, 2007) trained on the
Stockholm-Ume? Corpus (Ejerhed and K?llgren,
1997).
5 Overview of the Participating Systems
With 7 teams participating, more than 14 systems for
French and 10 for Arabic and German, this shared
task is on par with the latest large-scale parsing evalu-
ation campaign SANCL 2012 (Petrov and McDonald,
2012). The present shared task was extremely de-
manding on our participants. From 30 individuals or
teams who registered and obtained the data sets, we
present results for the seven teams that accomplished
successful executions on these data in the relevant
scenarios in the given the time frame.
5.1 Dependency Track
Seven teams participated in the dependency track.
Two participating systems are based on MaltParser:
MALTOPTIMIZER (Ballesteros, 2013) and AI:KU
(Cirik and S?ensoy, 2013). MALTOPTIMIZER uses
a variant of MaltOptimizer (Ballesteros and Nivre,
2012) to explore features relevant for the processing
of morphological information. AI:KU uses a combi-
nation of MaltParser and the original MaltOptimizer.
Their system development has focused on the inte-
gration of an unsupervised word clustering method
using contextual and morphological properties of the
words, to help combat sparseness.
Similarly to MaltParser ALPAGE:DYALOG
(De La Clergerie, 2013) also uses a shift-reduce
transition-based parser but its training and decoding
algorithms are based on beam search. This parser is
implemented on top of the tabular logic programming
system DyALog. To the best of our knowledge, this
is the first dependency parser capable of handling
word lattice input.
Three participating teams use the MATE parser
(Bohnet, 2010) in their systems: the BASQUETEAM
(Goenaga et al, 2013), IGM:ALPAGE (Constant et
al., 2013) and IMS:SZEGED:CIS (Bj?rkelund et al,
2013). The BASQUETEAM uses the MATE parser in
combination with MaltParser (Nivre et al, 2007b).
The system combines the parser outputs via Malt-
Blender (Hall et al, 2007). IGM:ALPAGE also uses
MATE and MaltParser, once in a pipeline architec-
ture and once in a joint model. The models are com-
bined via a re-parsing strategy based on (Sagae and
Lavie, 2006). This system mainly focuses on MWEs
in French and uses a CRF tagger in combination
with several large-scale dictionaries to handle MWEs,
which then serve as input for the two parsers.
The IMS:SZEGED:CIS team participated in both
tracks, with an ensemble system. For the depen-
dency track, the ensemble includes the MATE parser
(Bohnet, 2010), a best-first variant of the easy-first
parser by Goldberg and Elhadad (2010b), and turbo
parser (Martins et al, 2010), in combination with
a ranker that has the particularity of using features
from the constituent parsed trees. CADIM (Marton et
al., 2013b) uses their variant of the easy-first parser
combined with a feature-rich ensemble of lexical and
syntactic resources.
Four of the participating teams use exter-
nal resources in addition to the parser. The
IMS:SZEGED:CIS team uses external morpholog-
ical analyzers. CADIM uses SAMA (Graff et al,
2009) for Arabic morphology. ALPAGE:DYALOG
and IGM:ALPAGE use external lexicons for French.
IGM:ALPAGE additionally uses Morfette (Chrupa?a
et al, 2008) for morphological analysis and POS
tagging. Finally, as already mentioned, AI:KU clus-
ters words and POS tags in an unsupervised fashion
exploiting additional, un-annotated data.
5.2 Constituency Track
A single team participated in the constituency parsing
task, the IMS:SZEGED:CIS team (Bj?rkelund et al,
2013). Their phrase-structure parsing system uses a
combination of 8 PCFG-LA parsers, trained using a
product-of-grammars procedure (Petrov, 2010). The
50-best parses of this combination are then reranked
by a model based on the reranker by Charniak and
163
Johnson (2005).33
5.3 Baselines
We additionally provide the results of two baseline
systems for the nine languages, one for constituency
parsing and one for dependency parsing.
For the dependency track, our baseline system is
MaltParser in its default configuration (the arc-eager
algorithm and liblinear for training). Results marked
as BASE:MALT in the next two sections report the
results of this baseline system in different scenarios.
The constituency parsing baseline is based on the
most recent version of the PCFG-LA model of Petrov
et al (2006), used with its default settings and five
split/merge cycles, for all languages.34 We use this
parser in two configurations: a ?1-best? configura-
tion where all POS tags are provided to the parser
(predicted or gold, depending on the scenario), and
another configuration in which the parser performs
its own POS tagging. These baselines are referred to
as BASE:BKY+POS and BASE:BKY+RAW respec-
tively in the following results sections. Note that
even when BASE:BKY+POS is given gold POS tags,
the Berkeley parser sometimes fails to reach a perfect
POS accuracy. In cases when the parser cannot find a
parse with the provided POS, it falls back on its own
POS tagging for all tokens.
6 Results
The high number of submitted system variants and
evaluation scenarios in the task resulted in a large
number of evaluation scores. In the following evalu-
ation, we focus on the best run for each participant,
and we aim to provide key points on the different
dimensions of analysis resulting from our evaluation
protocol. We invite our interested readers to browse
the comprehensive representation of our results on
the official shared-task results webpages.35
33Note that a slight but necessary change in the configuration
of one of our metrics, which occurred after the system submis-
sion deadline, resulted in the IMS:SZEGED:CIS team to submit
suboptimal systems for 4 languages. Their final scores are ac-
tually slightly higher and can be found in (Bj?rkelund et al,
2013).
34For Semitic languages, we used the lattice based PCFG-LA
extension by Goldberg (2011).
35http://www.spmrl.org/
spmrl2013-sharedtask-results.html.
6.1 Gold Scenarios
This section presents the parsing results in gold sce-
narios, where the systems are evaluated on gold seg-
mented and tagged input. This means that the se-
quence of terminals, POS tags, and morphological
features are provided based on the treebank anno-
tations. This scenario was used in most previous
shared tasks on data-driven parsing (Buchholz and
Marsi, 2006; Nivre et al, 2007a; K?bler, 2008). Note
that this scenario was not mandatory. We thank our
participants for providing their results nonetheless.
We start by reviewing dependency-based parsing
results, both on the trees and on multi-word expres-
sion, and continue with the different metrics for
constituency-based parsing.
6.1.1 Dependency Parsing
Full Training Set The results for the gold parsing
scenario of dependency parsing are shown in the top
block of table 3.
Among the six systems, IMS:SZEGED:CIS
reaches the highest LAS scores, not only on aver-
age, but for every single language. This shows that
their approach of combining parsers with (re)ranking
provides robust parsing results across languages with
different morphological characteristics. The second
best system is ALPAGE:DYALOG, the third best sys-
tem is MALTOPTIMIZER. The fact that AI:KU is
ranked below the Malt baseline is due to their sub-
mission of results for 6 out of the 9 languages. Simi-
larly, CADIM only submitted results for Arabic and
ranked in the third place for this language, after the
two IMS:SZEGED:CIS runs. IGM:ALPAGE and
BASQUETEAM did not submit results for this setting.
Comparing LAS results across languages is prob-
lematic due to the differences between languages,
treebank size and annotation schemes (see section 3),
so the following discussion is necessarily tentative. If
we consider results across languages, we see that the
lowest results (around 83% for the best performing
system) are reached for Hebrew and Swedish, the
languages with the smallest data sets. The next low-
est result, around 86%, is reached for Basque. Other
languages reach similar LAS scores, around 88-92%.
German, with the largest training set, reaches the
highest LAS, 91.83%.
Interstingly, all systems have high LAS scores
on the Korean Treebank given a training set size
164
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 89.83 86.68 90.29 91.83 83.87 88.06 89.59 89.58 83.97 88.19
ALPAGE:DYALOG 85.87 80.39 87.69 88.25 80.70 79.60 88.23 86.00 79.80 84.06
MALTOPTIMIZER 87.03 82.07 85.71 86.96 80.03 83.14 89.39 80.49 77.67 83.61
BASE:MALT 82.28 69.19 79.86 79.98 76.61 72.34 88.43 77.70 75.73 78.01
AI:KU 86.39 86.98 79.42 83.67 85.16 78.87 55.61
CADIM 85.56 9.51
2) gold setting / 5k training set
IMS:SZEGED:CIS 87.35 85.69 88.73 87.70 83.87 87.21 83.38 89.16 83.97 86.34
ALPAGE:DYALOG 83.25 79.11 85.66 83.88 80.70 78.42 81.91 85.67 79.80 82.04
MALTOPTIMIZER 85.30 81.40 84.93 83.59 80.03 82.37 83.74 79.79 77.67 82.09
BASE:MALT 80.36 67.13 78.16 76.64 76.61 71.27 81.93 76.64 75.73 76.05
AI:KU 84.98 83.47 79.42 82.84 84.37 78.87 54.88
CADIM 82.67 9.19
3) predicted setting / full training set
IMS:SZEGED:CIS 86.21 85.14 85.24 89.65 80.89 86.13 86.62 87.07 82.13 85.45
ALPAGE:DYALOG 81.20 77.55 82.06 84.80 73.63 75.58 81.02 82.56 77.54 79.55
MALTOPTIMIZER 81.90 78.58 79.00 82.75 73.01 79.63 82.65 79.89 75.82 79.25
BASE:MALT 80.36 70.11 77.98 77.81 69.97 70.15 82.06 75.63 73.21 75.25
AI:KU 72.57 82.32 69.01 78.92 81.86 76.35 51.23
BASQUETEAM 84.25 84.51 88.66 84.97 80.88 47.03
IGM:ALPAGE 85.86 9.54
CADIM 83.20 9.24
4) predicted setting / 5k training set
IMS:SZEGED:CIS 83.66 83.84 83.45 85.08 80.89 85.24 80.80 86.69 82.13 83.53
MALTOPTIMIZER 79.64 77.59 77.56 79.22 73.01 79.00 75.90 79.50 75.82 77.47
ALPAGE:DYALOG 78.65 76.06 80.11 73.07 73.63 74.48 73.79 82.04 77.54 76.60
BASE:MALT 78.48 68.12 76.54 74.81 69.97 69.08 74.87 75.29 73.21 73.37
AI:KU 71.23 79.16 69.01 78.04 81.30 76.35 50.57
BASQUETEAM 83.19 82.65 84.70 84.01 80.88 46.16
IGM:ALPAGE 83.60 9.29
CADIM 80.51 8.95
Table 3: Dependency parsing: LAS scores for full and 5k training sets and for gold and predicted input. Results in bold
show the best results per language and setting.
of approximately 23,000 sentences, which is a little
over half of the German treebank. For German, on
the other hand, only the IMS:SZEGED:CIS system
reaches higher LAS scores than for Korean. This
final observation indicates that more than treebank
size is important for comparing system performance
across treebanks. This is the reason for introducing
the reduced set scenario, in which we can see how the
participating system perform on a common ground,
albeit small.
5k Training Set The results for the gold setting
on the 5k train set are shown in the second block
of Table 3. Compared with the full training, we
see that there is a drop of around 2 points in this
setting. Some parser/language pairs are more sensi-
tive to data sparseness than others. CADIM, for in-
stance, exhibit a larger drop than MALTOPTIMIZER
on Arabic, and MALTOPTIMIZER shows a smaller
drop than IMS:SZEGED:CIS on French. On average,
among all systems that covered all languages, MALT-
OPTIMIZER has the smallest drop when moving to
5k training, possibly since the automatic feature opti-
mization may differ for different data set sizes.
Since all languages have the same number of sen-
tences in the train set, these results can give us limited
insight into the parsing complexity of the different
treebanks. Here, French, Arabic, Polish, and Korean
reach the highest LAS scores while Swedish reaches
165
Team F_MWE F_COMP F_MWE+POS
1) gold setting / full training set
AI:KU 99.39 99.53 99.34
IMS:SZEGED:CIS 99.26 99.39 99.21
MALTOPTIMIZER 98.95 98.99 0
ALPAGE:DYALOG 98.32 98.81 0
BASE:MALT 68.7 72.55 68.7
2) predicted setting / full training set
IGM:ALPAGE 80.81 81.18 77.37
IMS:SZEGED:CIS 79.45 80.79 70.48
ALPAGE:DYALOG 77.91 79.25 0
BASQUE-TEAM 77.19 79.81 0
MALTOPTIMIZER 70.29 74.25 0
BASE:MALT 67.49 71.01 0
AI:KU 0 0 0
3) predicted setting / 5k training set
IGM:ALPAGE 77.66 78.68 74.04
IMS:SZEGED:CIS 77.28 78.92 70.42
ALPAGE:DYALOG 75.17 76.82 0
BASQUETEAM 73.07 76.58 0
MALTOPTIMIZER 65.76 70.42 0
BASE:MALT 62.05 66.8 0
AI:KU 0 0 0
Table 4: Dependency Parsing: MWE results
the lowest one. Treebank variance depends not only
on the language but also on annotation decisions,
such as label set (Swedish, interestingly, has a rela-
tively rich one). A more careful comparison would
then take into account the correlation of data size,
label set size and parsing accuracy. We investigate
these correlations further in section 7.1.
6.1.2 Multiword Expressions
MWE results on the gold setting are found at
the top of Table 4. All systems, with the excep-
tion of BASE:MALT, perform exceedingly well in
identifying the spans and non-head components of
MWEs given gold morphology.36 These almost per-
fect scores are the consequence of the presence of
two gold MWE features, namely MWEHEAD and
PRED=Y, which respectively indicate the node span
of the whole MWE and its dependents, which do not
have a gold feature field. The interesting scenario is,
of course, the predicted one, where these features are
not provided to the parser, as in any realistic applica-
tion.
36Note that for the labeled measure F_MWE+POS, both
MALTOPTIMIZER and ALPAGE:DYALOG have an F-score of
zero, since they do not attempt to predict the MWE label at all.
6.1.3 Constituency Parsing
In this part, we provide accuracy results for phrase-
structure trees in terms of ParsEval F-scores. Since
ParsEval is sensitive to the non-terminals-per-word
ratio in the data set (Rehbein and van Genabith,
2007a; Rehbein and van Genabith, 2007b), and given
the fact that this ratio varies greatly within our data
set (as shown in Table 2), it must be kept in mind that
ParsEval should only be used for comparing parsing
performance over treebank instances sharing the ex-
act same properties in term of annotation schemes,
sentence length and so on. When comparing F-Scores
across different treebanks and languages, it can only
provide a rough estimate of the relative difficulty or
ease of parsing these kinds of data.
Full Training Set The F-score results for the gold
scenario are provided in the first block of Table 5.
Among the two baselines, BASE:BKY+POS fares
better than BASE:BKY+RAW since the latter selects
its own POS tags and thus cannot benefit from the
gold information. The IMS:SZEGED:CIS system
clearly outperforms both baselines, with Hebrew as
an outlier.37
As in the dependency case, the results are not
strictly comparable across languages, yet we can
draw some insights from them. We see consider-
able differences between the languages, with Basque,
Hebrew, and Hungarian reaching F-scores in the low
90s for the IMS:SZEGED:CIS system, Korean and
Polish reaching above-average F-scores, and Ara-
bic, French, German, and Swedish reaching F-scores
below the average, but still in the low 80s. The per-
formance is, again, not correlated with data set sizes.
Parsing Hebrew, with one of the smallest training
sets, obtains higher accuracy many other languages,
including Swedish, which has the same training set
size as Hebrew. It may well be that gold morphologi-
cal information is more useful for combatting sparse-
ness in languages with richer morphology (though
Arabic here would be an outlier for this conjecture),
or it may be that certain treebanks and schemes are
inherently harder to parser than others, as we investi-
gate in section 7.
For German, the language with the largest training
37It might be that the easy layer of syntactic tags benefits from
the gold POS tags provided. See section 4 for further discussion
of this layer.
166
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 82.20 90.04 83.98 82.07 91.64 92.60 86.50 88.57 85.09 86.97
BASE:BKY+POS 80.76 76.24 81.76 80.34 92.20 87.64 82.95 88.13 82.89 83.66
BASE:BKY+RAW 79.14 69.78 80.38 78.99 87.32 81.44 73.28 79.51 78.94 78.75
2) gold setting / 5k training set
IMS:SZEGED:CIS 79.47 88.45 82.25 74.78 91.64 91.87 80.10 88.18 85.09 84.65
BASE:BKY+POS 77.54 74.06 78.07 71.37 92.20 86.74 72.85 87.91 82.89 80.40
BASE:BKY+RAW 75.22 67.16 75.91 68.94 87.32 79.34 60.40 78.30 78.94 74.61
3) predicted setting / full training set
IMS:SZEGED:CIS 81.32 87.86 81.83 81.27 89.46 91.85 84.27 87.55 83.99 85.49
BASE:BKY+POS 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89
BASE:BKY+RAW 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
4) predicted setting / 5k training set
IMS:SZEGED:CIS 78.85 86.65 79.83 73.61 89.46 90.53 78.47 87.46 83.99 83.21
BASE:BKY+POS 74.84 72.35 76.19 69.40 85.42 83.82 67.97 87.17 80.64 77.53
BASE:BKY+RAW 74.57 66.75 75.76 68.68 86.96 79.35 58.49 78.38 79.18 74.24
Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in
bold show the best results per language and setting.
set and the highest scores in dependency parsing,
the F-scores are at the lower end. These low scores,
which are obtained despite the larger treebank and
only moderately free word-order, are surprising. This
may be due to case syncretism; gold morphological
information exhibits its own ambiguity and thus may
not be fully utilized.
5k Training Set Parsing results on smaller com-
parable test sets are presented in the second block
of Table 5. On average, IMS:SZEGED:CIS is less
sensitive than BASE:BKY+POS to the reduced size.
Systems are not equally sensitive to reduced training
sets, and the gaps range from 0.4% to 3%, with Ger-
man and Korean as outliers (Korean suffering a 6.4%
drop in F-score and German 7.3%). These languages
have the largest treebanks in the full setting, so it is
not surprising that they suffer the most. But this in
itself does not fully explain the cross-treebank trends.
Since ParsEval scores are known to be sensitive to
the label set sizes and the depth of trees, we provide
LeafAncestor scores in the following section.
6.1.4 Leaf-Ancestor Results
The variation across results in the previous subsec-
tion may have been due to differences across annota-
tion schemes. One way to neutralize this difference
(to some extent) is to use a different metric. We
evaluated the constituency parsing results using the
Leaf-Ancestor (LA) metric, which is less sensitive
to the number of nodes in a tree (Rehbein and van
Genabith, 2007b; K?bler et al, 2008). As shown in
Table 6, these results are on a different (higher) scale
than ParsEval, and the average gap between the full
and 5k setting is lower.
Full Training Set The LA results in gold setting
for full training sets are shown in the first block of Ta-
ble 6. The trends are similar to the ParsEval F-scores.
German and Arabic present the lowest LA scores
(in contrast to the corresponding F-scores, Arabic is
a full point below German for IMS:SZEGED:CIS).
Basque and Hungarian have the highest LA scores.
Hebrew, which had a higher F-score than Basque,
has a lower LA than Basque and is closer to French.
Korean also ranks worse in the LA analysis. The
choice of evaluation metrics thus clearly impacts sys-
tem rankings ? F-scores rank some languages suspi-
ciously high (e.g., Hebrew) due to deeper trees, and
another metric may alleviate that.
5k Training Set The results for the leaf-ancestor
(LA) scores in the gold setting for the 5k training set
are shown in the second block of Table 6. Across
167
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 88.61 94.90 92.51 89.63 92.84 95.01 91.30 94.52 91.46 92.31
BASE:BKY+POS 87.85 91.55 91.74 88.47 92.69 92.52 90.82 92.81 90.76 91.02
BASE:BKY+RAW 87.05 89.71 91.22 87.77 91.29 90.62 87.11 90.58 88.97 89.37
2) gold setting / 5k training set
IMS:SZEGED:CIS 86.68 94.21 91.56 85.74 92.84 94.79 88.87 94.17 91.46 91.15
BASE:BKY+POS 86.26 90.72 89.71 84.11 92.69 92.11 86.75 92.91 90.76 89.56
BASE:BKY+RAW 84.97 88.68 88.74 83.08 91.29 89.94 81.82 90.31 88.97 87.53
3) predicted setting / full training set
IMS:SZEGED:CIS 88.45 94.50 91.79 89.32 91.95 94.90 90.13 94.11 91.05 91.80
BASE:BKY+POS 86.60 90.90 90.96 87.46 89.66 91.72 89.10 92.56 89.51 89.83
BASE:BKY+RAW 86.97 89.91 91.11 87.46 90.77 90.50 86.68 90.48 89.16 89.23
4) predicted setting / 5k training set
IMS:SZEGED:CIS 86.69 93.85 90.76 85.20 91.95 94.05 87.99 93.99 91.05 90.61
BASE:BKY+POS 84.76 89.83 89.18 83.05 89.66 91.24 84.87 92.74 89.51 88.32
BASE:BKY+RAW 84.63 88.50 89.00 82.69 90.77 89.93 81.50 90.08 89.16 87.36
Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.
parsers, IMS:SZEGED:CIS again has a smaller drop
than BASE:BKY+POS on the reduced size. German
suffers the most from the reduction of the training
set, with a loss of approximately 4 points. Korean,
however, which was also severely affected in terms
of F-scores, only loses 1.17 points in the LA score.
On average, the LA seem to reflect a smaller drop
when reducing the training set ? this underscores
again the impact of the choice of metrics on system
evaluation.
6.2 Predicted Scenarios
Gold scenarios are relatively easy since syntactically
relevant morphological information is disambiguated
in advance and is provided as input. Predicted scenar-
ios are more difficult: POS tags and morphological
features have to be automatically predicted, by the
parser or by external resources.
6.2.1 Dependency Parsing
Eight participating teams submitted dependency
results for this scenario. Two teams submitted for a
single language. Four teams covered all languages.
Full Training Set The results for the predicted
scenario in full settings are shown in the third
block of Table 3. Across the board, the re-
sults are considerably lower than the gold sce-
nario. Again, IMS:SZEGED:CIS is the best per-
forming system, followed by ALPAGE:DYALOG and
MALTOPTIMIZER. The only language for which
IMS:SZEGED:CIS is outperformed is French, for
which IGM:ALPAGE reaches higher results (85.86%
vs. 85.24%). This is due to the specialized treatment
of French MWEs in the IGM:ALPAGE system, which
is thereby shown to be beneficial for parsing in the
predicted setting.
If we compare the results for the predicted set-
ting and the gold one, given the full training set,
the IMS:SZEGED:CIS system shows small differ-
ences between 1.5 and 2 percent. The only ex-
ception is French, for which the LAS drops from
90.29% to 85.24% in the predicted setting. The
other systems show somewhat larger differences than
IMS:SZEGED:CIS, with the highest drops for Ara-
bic and Korean. The AI:KU system shows a similar
problem as IMS:SZEGED:CIS for French.
5k Training Set When we consider the predicted
setting for the 5k training set, in the last block of
Table 3, we see the same trends as comparing with
the full training set or when comparing to the gold
setting. Systems suffer from not having gold stan-
dard data, and they suffer from the small training set.
Interestingly, the loss between the different training
set sizes in the predicted setting is larger than in the
168
gold setting, but only marginally so, with a differ-
ence < 0.5. In other words, the predicted setting
adds a challenge to parsing, but it only minimally
compounds data sparsity.
6.2.2 Multiword Expressions Evaluation
In the predicted setting, shown in the second
block of table 4 for the full training set and in the
third block of the same table for the 5k training set,
we see that only two systems, IGM:ALPAGE and
IMS:SZEGED:CIS can predict the MWE label when
it is not present in the training set. IGM:ALPAGE?s
approach of using a separate classifier in combination
with external dictionaries is very successful, reach-
ing an F_MWE+POS score of 77.37. This is com-
pared to the score of 70.48 by IMS:SZEGED:CIS,
which predicts this node label as a side effect of
their constituent feature enriched dependency model
(Bj?rkelund et al, 2013). AI:KU has a zero score
for all predicted settings, which results from an erro-
neous training on the gold data rather than the pre-
dicted data.38
6.2.3 Constituency Parsing
Full Training Set The results for the predicted set-
ting with the full training set are shown in the third
block of table 5. A comparison with the gold setting
shows that all systems have a lower performance in
the predicted scenario, and the differences are in the
range of 0.88 for Arabic and 2.54 for Basque. It is
interesting to see that the losses are generally smaller
than in the dependency framework: on average, the
loss across languages is 2.74 for dependencies and
1.48 for constituents. A possible explanation can be
found in the two-dimensional structure of the con-
stituent trees, where only a subset of all nodes is
affected by the quality of morphology and POS tags.
The exception to this trend is Basque, for which the
loss in constituents is a full point higher than for de-
pendencies. Another possible explanation is that all
of our constituent parsers select their own POS tags
in one way or another. Most dependency parsers ac-
cept predicted tags from an external resource, which
puts an upper-bound on their potential performance.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the bottom
38Unofficial updated results are to to be found in (Cirik and
S?ensoy, 2013)
block of table 5. They show the same trends as the
dependency ones: The results are slightly lower than
the results obtained in gold setting and the ones uti-
lizing the full training set.
6.2.4 Leaf Ancestor Metrics
Full Training Set The results for the predicted sce-
nario with a full training set are shown in the third
block of table 6. In the LA evaluation, the loss
in moving from gold morphology are considerably
smaller than in F-scores. For most languages, the
loss is less than 0.5 points. Exceptions are French
with a loss of 0.72, Hebrew with 0.89, and Korean
with 1.17. Basque, which had the highest loss in
F-scores, only shows a minor loss of 0.4 points. Also,
the average loss of 0.41 points is much smaller than
the one in the ParsEval score, 1.48.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the last
block of table 6. These results, though considerably
lower (around 3 points), exhibit the exact same trends
as observed in the gold setting.
6.3 Realistic Raw Scenarios
The previous scenarios assume that input surface to-
kens are identical to tree terminals. For languages
such as Arabic and Hebrew, this is not always the
case. In this scenario, we evaluate the capacity of a
system to predict both morphological segmentation
and syntactic parse trees given raw, unsegmented
input tokens. This may be done via a pipeline as-
suming a 1-st best morphological analysis, or jointly
with parsing, assuming an ambiguous morpholog-
ical analysis lattice as input. In this task, both of
these scenarios are possible (see section 3). Thus,
this section presents a realistic evaluation of the par-
ticipating systems, using TedEval, which takes into
account complete morpho-syntactic parses.
Tables 7 and 8 present labeled and unlabeled
TedEval results for both constituency and depen-
dency parsers, calculated only for sentence of length
<= 70.39 We firstly observe that labeled TedEval
scores are considerably lower than unlabeled Ted-
Eval scores, as expected, since unlabeled scores eval-
uate only structural differences. In the labeled setup,
39TedEval builds on algorithms for calculating edit distance
on complete trees (Bille, 2005). In these algorithms, longer
sentences take considerably longer to evaluate.
169
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 83.34 1.63 82.54 0.67 56.47 0.67 69.51 69.51
IMS:SZEGED:CIS 89.12 8.37 87.82 5.56 86.08 8.27 86.95 86.95
CADIM 87.81 6.63 86.43 4.21 - - 43.22 86.43
MALTOPTIMIZER 86.74 5.39 85.63 3.03 83.05 5.33 84.34 84.34
ALPAGE:DYALOG 86.60 5.34 85.71 3.54 82.96 6.17 41.48 82.96
ALPAGE:DYALOG (RAW) - - - - 82.82 4.35 41.41 82.82
AI:KU - - - - 78.57 3.37 39.29 78.57
Table 7: Realistic Scenario: Tedeval Labeled Accuracy and Exact Match for the Raw scenario.
The upper part refers to constituency results, the lower part refers to dependency results
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 92.06 9.49 91.29 7.13 89.30 13.60 90.30 90.30
IMS:SZEGED:CIS 91.74 9.83 90.85 7.30 89.47 16.97 90.16 90.16
ALPAGE:DYALOG 89.99 7.98 89.46 5.67 88.33 12.20 88.90 88.90
MALTOPTIMIZER 90.09 7.08 89.47 5.56 87.99 11.64 88.73 88.73
CADIM 90.75 8.48 89.89 5.67 - - 44.95 89.89
ALPAGE:DYALOG (RAW) - - - - 87.61 10.24 43.81 87.61
AI:KU - - - - 86.70 8.98 43.35 86.70
Table 8: Realistic Scenario: Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario.
Top upper part refers to constituency results, the lower part refers to dependency results.
the IMS:SZEGED:CIS dependency parser are the
best for both languages and data set sizes. Table 8
shows that their unlabeled constituency results reach
a higher accuracy than the next best system, their
own dependency results. However, a quick look at
the exact match metric reveals lower scores than for
its dependency counterparts.
For the dependency-based joint scenarios, there
is obviously an upper bound on parser performance
given inaccurate segmentation. The transition-based
systems, ALPAGE:DYALOG & MALTOPTIMIZER,
perform comparably on Arabic and Hebrew, with
ALPAGE:DYALOG being slightly better on both lan-
guages. Note that ALPAGE:DYALOG reaches close
results on the 1-best and the lattice-based input set-
tings, with a slight advantage for the former. This is
partly due to the insufficient coverage of the lexical
resource we use: many lattices do not contain the
gold path, so the joint prediction can only as be high
as the lattice predicted path allows.
7 Towards In-Depth Cross-Treebank
Evaluation
Section 6 reported evaluation scores across systems
for different scenarios. However, as noted, these re-
sults are not comparable across languages, represen-
tation types and parsing scenarios due to differences
in the data size, label set size, length of sentences and
also differences in evaluation metrics.
Our following discussion in the first part of this
section highlights the kind of impact that data set
properties have on the standard metrics (label set size
on LAS, non-terminal nodes per sentence on F-score).
Then, in the second part of this section we use the
TedEval cross-experiment protocols for comparative
evaluation that is less sensitive to representation types
and annotation idiosyncrasies.
7.1 Parsing Across Languages and Treebanks
To quantify the impact of treebank characteristics on
parsing parsing accuracy we looked at correlations
of treebank properties with parsing results. The most
highly correlated combinations we have found are
shown in Figures 2, 3, and 4 for the dependency track
and the constituency track (F-score and LeafAnces-
170
21/09/13 03:00SPMRL charts
Page 3 sur 3http://pauillac.inria.fr/~seddah/updated_official.spmrl_results.html
Correlation between label set size, treebank size, and mean LAS
FrP
FrP
GeP
GeP
HuP
HuP
SwP
ArP
ArP
ArG
ArG
BaP
BaP
FrG
FrG
GeG
GeG
HeP
HeG
HuG
HuG
PoP
PoP
PoG
PoG
SwG
BaG
BaG
KoP
KoP
KoG
KoG
10 50 100 500 1 000
72
74
76
78
80
82
84
86
88
90
treebank size / #labels
L
A
S
 
(
%
)
Figure 2: The correlation between treebank size, label set size, and LAS scores. x: treebank size / #labels ; y: LAS (%)
01/10/13 00:43SPMRL charts: all sent.
Page 1 sur 5file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-S?/SPMRL_FINAL/RESULTS/OFFICIAL/official_ptb-all.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, all sent.)
(13/10/01 00:34:34
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean F1
Arabic
Basque
French
German
Hebrew
Hungarian
Korean
Polish
Swedish
ArP
ArG
BaP
BaG
FrP
FrG
GeP
GeG
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
8 9 10
72
74
76
78
80
82
84
86
88
90
92
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 3: The correlation between the non terminals per sentence ratio and F-scores. x: #non terminal/ #sentence ; y:
F1 (%)
171
tor) respectively.
Figure 2 presents the LAS against the average num-
ber of tokens relative to the number of labels. The
numbers are averaged per language over all partici-
pating systems, and the size of the ?bubbles? is pro-
portional to the number of participants for a given
language setting. We provide ?bubbles? for all lan-
guages in the predicted (-P) and gold (-G) setting,
for both training set sizes. The lower dot in terms
of parsing scores always corresponds to the reduced
training set size.
Figure 2 shows a clear correlation between data-
set complexity and parsing accuracy. The simpler
the data set is (where ?simple" here translates into
large data size with a small set of labels), the higher
the results of the participating systems. The bubbles
reflects a diagonal that indicates correlation between
these dimensions. Beyond that, we see two interest-
ing points off of the diagonal. The Korean treebank
(pink) in the gold setting and full training set can be
parsed with a high LAS relative to its size and label
set. It is also clear that the Hebrew treebank (purple)
in the predicted version is the most difficult one to
parse, relative to our expectation about its complexity.
Since the Hebrew gold scenario is a lot closer to the
diagonal again, it may be that this outlier is due to the
coverage and quality of the predicted morphology.
Figure 340 shows the correlation of data complex-
ity in terms of the average number of non-terminals
per sentence, and parsing accuracy (ParsEval F-
score). Parsing accuracy is again averaged over all
participating systems for a given language. In this
figure, we see a diagonal similar to the one in figure 2,
where Arabic (dark blue) has high complexity of the
data (here interpreted as flat trees, low number of
non terminals per sentence) and low F-scores accord-
ingly. Korean (pink), Swedish (burgundy), Polish
(light green), and Hungarian (light blue) follow, and
then Hebrew (purple) is a positive outlier, possibly
due to an additional layer of ?easy" syntactic POS
nodes which increases tree size and inflates F-scores.
French (orange), Basque (red), and German (dark
green) are negative outliers, falling off the diago-
nal. German has the lowest F-score with respect to
40This figure was created from the IMS:SZEGED:CIS
(Const.) and our own PCFG-LA baseline in POS Tagged mode
(BASE:BKY+POS) so as to avoid the noise introduced by the
parser?s own tagging step (BASE:BKY+RAW).
what would be expected for the non-terminals per
sentence ratio, which is in contrast to the LAS fig-
ure where German occurs among the less complex
data set to parse. A possible explanation may be
the crossing branches in the original treebank which
were re-attached. This creates flat and variable edges
which might be hard predict accurately.
Figure 441 presents the correlation between parsing
accuracy in terms the LeafAncestor metrics (macro
averaged) and treebank complexity in terms of the
average number of non-terminals per sentence. As
in the correlation figures, the parsing accuracy is
averaged over the participanting systems for any lan-
guage. The LeafAncestor accuracy is calculated over
phrase structure trees, and we see a similar diago-
nal to the one in Figure 3 showing that flatter tree-
banks are harder (that is, are correlated with lower
averaged scores) But, its slope is less steep than for
the F-score, which confirms the observation that the
LeafAncestor metric is less sensitive than F-score to
the non-terminals-per-sentence ratio.
Similarly to Figure 3, German is a negative outlier,
which means that this treebank is harder to parse ? it
obtains lower scores on average than we would ex-
pect. As for Hebrew, it is much closer to the diagonal.
As it turns out, the "easy" POS layer that inflates the
scores does not affect the LA ratings as much.
7.2 Evaluation Across Scenarios, Languages
and Treebanks
In this section we analyze the results in cross-
scenario, cross-annotation, and cross-framework set-
tings using the evaluation protocols discussed in
(Tsarfaty et al, 2012b; Tsarfaty et al, 2011; Tsarfaty
et al, 2012a).
As a starting point, we select comparable sections
of the parsed data, based on system runs trained on
the small train set (train5k). For those, we selected
subsets containing the first 5,000 tree terminals (re-
specting sentence boundaries) of the test set. We only
used TedEval on sentences up to 70 terminals long,
and projectivized non-projective sentences in all sets.
We use the TedEval metrics to calculate scores on
both constituency and dependency structures in all
languages and all scenarios. Since the metric de-
fines one scale for all of these different cases, we can
41This figure was created under the same condition as the
F-score correlation in figure (Figure 3).
172
04/10/13 23:05SPMRL charts:
Page 1 sur 6file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-SHAREDTASK/SPMRL_FINAL/RESULTS/TESTLEAF.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, )
(13/10/04 23:05:31
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean Leaf Accuracy
ArP
ArG
BaP
BaG
FrP
FrG
GeP
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
GeG
8 9 10
74
76
78
80
82
84
86
88
90
92
94
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 4: The correlation between the non terminals per sentence ratio and Leaf Accuracy (macro) scores. x: #non
terminal/ #sentence ; y: Acc.(%)
compare the performance across annotation schemes,
assuming that those subsets are representative of their
original source.42
Ideally, we would be using labeled TedEval scores,
as the labeled parsing task is more difficult, and la-
beled parses are far more informative than unlabeled
ones. However, most constituency-based parsers do
not provide function labels as part of the output, to
be compared with the dependency arcs. Furthermore,
as mentioned earlier, we observed a huge difference
between label set sizes for the dependency runs. Con-
sequently, labeled scores will not be as informative
across treebanks and representation types. We will
therefore only use labels across scenarios for the
same language and representation type.
42We choose this sample scheme for replicability. We first
tried sampling sentences, aiming at the same average sentence
length (20), but that seemed to create artificially difficult test sets
for languages as Polish and overly simplistic ones for French or
Arabic.
7.2.1 Cross-Scenario Evaluation: raw vs. gold
One novel aspect of this shared task is the evalu-
ation on non-gold segmentation in addition to gold
morphology. One drawback is that the scenarios are
currently not using the same metrics ? the metrics
generally applied for gold and predicted scenrios can-
not apply for raw. To assess how well state of the art
parsers perform in raw scenarios compared to gold
scenarios, we present here TedEval results comparing
raw and gold systems using the evaluation protocol
of Tsarfaty et al (2012b).
Table 9 presents the labeled and unlabeled results
for Arabic and Hebrew (in Full and 5k training set-
tings), and Table 10 presents unlabeled TedEval re-
sults (for all languages) in the gold settings. The
unlabeled TedEval results for the raw settings are
substantially lower then TedEval results on the gold
settings for both languages.
When comparing the unlabeled TedEval results for
Arabic and Hebrew on the participating systems, we
see a loss of 3-4 points between Table 9 (raw) and Ta-
ble 10 (gold). In particular we see that for the best per-
173
forming systems on Arabic (IMS:SZEGED:CIS for
both constituency and dependency), the gap between
gold and realistic scenarios is 3.4 and 4.3 points,
for the constituency and the dependency parser re-
spectively. These results are on a par with results
by Tsarfaty et al (2012b), who showed for different
settings, constituency and dependency based, that
raw scenarios are considerably more difficult to parse
than gold ones on the standard split of the Modern
Hebrew treebank.
For Hebrew, the performance gap between unla-
beled TedEval in raw (Table 9) and gold (Table 10)
is even more salient, with around 7 and 8 points of
difference between the scenarios. We can only specu-
late that such a difference may be due to the difficulty
of resolving Hebrew morpho-syntactic ambiguities
without sufficient syntactic information. Since He-
brew and Arabic now have standardized morpholog-
ically and syntactically analyzed data sets available
through this task, it will be possible to investigate
further how cross-linguistic differences in morpho-
logical ambiguity affect full-parsing accuracy in raw
scenarios.
This section compared the raw and gold parsing
results only on unlabeled TedEval metrics. Accord-
ing to what we have seen so far is expected that
for labeled TedEval metrics using the same protocol,
the gap between gold and raw scenario will be even
greater.
7.2.2 Cross-Framework Evaluation:
Dependency vs. Constituency
In this section, our focus is on comparing parsing
results across constituency and dependency parsers
based on the protocol of Tsarfaty et al (2012a) We
have only one submission from IMS:SZEGED:CIS
in the constituency track, and. from the same group,
a submission on the dependency track. We only com-
pare the IMS:SZEGED:CIS results on constituency
and dependency parsing with the two baselines we
provided. The results of the cross-framework evalua-
tion protocol are shown in Table 11.
The results comparing the two variants of the
IMS:SZEGED:CIS systems show that they are very
close for all languages, with differences ranging from
0.03 for German to 0.8 for Polish in the gold setting.
It has often been argued that dependency parsers
perform better than a constituency parser, but we
notice that when using a cross framework protocol,
such as TedEval, and assuming that our test set sam-
ple is representative, the difference between the in-
terpretation of both representation?s performance is
alleviated. Of course, here the metric is unlabeled, so
it simply tells us that both kind of parsing models are
equally able to provide similar tree structures. Said
differently, the gaps in the quality of predicting the
same underlying structure across representations for
MRLs is not as large as is sometimes assumed.
For most languages, the baseline constituency
parser performs better than the dependency base-
line one, with Basque and Korean as an exception,
and at the same time, the dependency version of
IMS:SZEGED:CIS performs slightly better than their
constituent parser for most languages, with the excep-
tion of Hebrew and Hungarian. It goes to show that,
as far as these present MRL results go, there is no
clear preference for a dependency over a constituency
parsing representation, just preferences among par-
ticular models.
More generally, we can say that even if the linguis-
tic coverage of one theory is shown to be better than
another one, it does not necessarily mean that the
statistical version of the formal theory will perform
better for structure prediction. System performance
is more tightly related to the efficacy of the learning
and search algorithms, and feature engineering on
top of the selected formalism.
7.2.3 Cross-Language Evaluation: All
Languages
We conclude with an overall outlook of the Ted-
Eval scores across all languages. The results on the
gold scenario, for the small training set and the 5k
test set are presented in Table 10. We concentrate
on gold scenarios (to avoid the variation in cover-
age of external morphological analyzers) and choose
unlabeled metrics as they are not sensitive to label
set sizes. We emphasize in bold, for each parsing
system (row in the table), the top two languages that
most accurately parsed by it (boldface) and the two
languages it performed the worse on (italics).
We see that the European languages German
and Hungarian are parsed most accurately in the
constituency-based setup, with Polish and Swedish
having an advantage in dependency parsing. Across
all systems, Korean is the hardest to parse, with Ara-
174
Arabic Hebrew AVG1 SOFT AVG Arabic Hebrew AVG2 SOFT AVG2
1) Constituency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS (Bky) 83.59 56.43 70.01 70.01 92.18 88.02 90.1 90.1
2) Dependency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS 88.61 84.74 86.68 86.68 91.41 88.58 90 90
ALPAGE:DYALOG 87.20 81.65 40.83 81.65 90.74 87.44 89.09 89.09
CADIM 87.99 - 44 87.99 91.22 - 45.61 91.22
MALTOPTIMIZER 86.62 81.74 43.31 86.62 90.26 87.00 45.13 90.26
ALPAGE:DYALOG (RAW) - 82.82 41.41 82.82 - 87.43 43.72 87.43
AI:KU - 77.8 38.9 77.8 - 85.87 42.94 85.87
Table 9: Labeled and Unlabeled TedEval Results for raw Scenarios, Trained on 5k sentences and tested on 5k terminals.
The upper part refers to constituency parsing and the lower part refers to dependency parsing.
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) Constituency Evaluation
IMS:SZEGED:CIS (Bky) 95.35 96.91 95.98 97.12 96.22 97.92 92.91 97.19 96.65
BASE:BKY+POS 95.11 94.69 95.08 97.01 95.85 97.08 90.55 96.99 96.38
BASE:BKY+RAW 94.58 94.32 94.72 96.74 95.64 96.15 87.08 95.93 95.90
2) Dependency Evaluation
IMS:SZEGED:CIS 95.76 97.63 96.59 96.88 96.29 97.56 94.62 98.01 97.22
ALPAGE:DYALOG 93.76 95.72 95.75 96.4 95.34 95.63 94.56 96.80 96.55
BASE:MALT 94.16 95.08 94.21 94.55 94.98 95.25 94.27 95.83 95.33
AI:KU - - 95.46 96.34 95.07 96.53 - 96.88 95.87
MALTOPTIMIZER 94.91 96.82 95.23 96.32 95.46 96.30 94.69 96.06 95.90
CADIM 94.66 - - - - - - - -
Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and
a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing.
For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics.
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) gold setting
IMS:SZEGED:CIS (Bky) 95.82 97.30 96.15 97.43 96.37 98.25 94.07 97.22 96.89
IMS:SZEGED:CIS 95.87 98.06 96.61 97.46 96.31 97.93 94.62 98.04 97.24
BASE:BKY+POS 95.61 95.25 95.48 97.31 96.03 97.53 92.15 96.97 96.66
BASE:MALT 94.26 95.76 94.23 95.53 95.00 96.09 94.27 95.90 95.35
2) predicted setting
IMS:SZEGED:CIS (Bky) 95.74 97.07 96.21 97.31 96.10 98.03 94.05 96.92 96.90
IMS:SZEGED:CIS 95.18 97.67 96.15 97.09 96.22 97.63 94.43 97.50 97.02
BASE:BKY+POS 95.03 95.35 97.12 95.36 97.20 91.34 96.92 96.25
BASE:MALT 95.49 93.84 95.39 94.41 95.72 93.74 96.04 95.09
Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k
sentences and tested on 5k terminals.
bic, Hebrew and to some extent French following. It
appears that on a typological scale, Semitic and Asian
languages are still harder to parse than a range of Eu-
ropean languages in terms of structural difficulty and
complex morpho-syntactic interaction. That said,
note that we cannot tell why certain treebanks appear
more challenging to parse then others, and it is still
unclear whether the difficulty is inherent on the lan-
guage, in the currently available models, or because
of the annotation scheme and treebank consistency.43
43The latter was shown to be an important factor orthogonal
to the morphologically-rich nature of the treebank?s language
175
8 Conclusion
This paper presents an overview of the first shared
task on parsing morphologically rich languages. The
task features nine languages, exhibiting different lin-
guistic phenomena and varied morphological com-
plexity. The shared task saw submissions from seven
teams, and results produced by more than 14 different
systems. The parsing results were obtained in dif-
ferent input scenarios (gold, predicted, and raw) and
evaluated using different protocols (cross-framework,
cross-scenario, and cross-language). In particular,
this is the first time an evaluation campaign reports
on the execution of parsers in realistic, morphologi-
cally ambiguous, setting.
The best performing systems were mostly ensem-
ble systems combining multiple parser outputs from
different frameworks or training runs, or integrat-
ing a state-of-the-art morphological analyzer on top
of a carefully designed feature set. This is con-
sistent with previous shared tasks such as ConLL
2007 or SANCL?2012. However, dealing with am-
biguous morphology is still difficult for all systems,
and a promising approach, as demonstrated by AL-
PAGE:DYALOG, is to deal with parsing and morphol-
ogy jointly by allowing lattice input to the parser. A
promising generalization of this approach would be
the full integration of all levels of analysis that are
mutually informative into a joint model.
The information to be gathered from the results of
this shared task is vast, and we only scratched the
surface with our preliminary analyses. We uncov-
ered and documented insights of strategies that make
parsing systems successful: parser combination is
empirically proven to reach a robust performance
across languages, though language-specific strategies
are still a sound avenue for obtaining high quality
parsers for that individual language. The integration
of morphological analysis into the parsing needs to
be investigated thoroughly, and new approaches that
are morphologically aware need to be developed.
Our cross-parser, cross-scenario, and cross-
framework evaluation protocols have shown that, as
expected, more data is better, and that performance
on gold morphological input is significantly higher
than that in more realistic scenarios. We have shown
that gold morphological information is more help-
(Schluter and van Genabith, 2007)
ful to some languages and parsers than others, and
that it may also interact with successful identification
of multiword expressions. We have shown that dif-
ferences between dependency and constituency are
smaller than previously assumed and that properties
of the learning model and granularity of the output
labels are more influential. Finally, we observed
that languages which are typologically farthest from
English, such as Semitic and Asian languages, are
still amongst the hardest to parse, regardless of the
parsing method used.
Our cross-treebank, in-depth analysis is still pre-
liminary, owing to the limited time between the end
of the shared task and the deadline for publication
of this overview. but we nonetheless feel that our
findings may benefit researchers who aim to develop
parsers for diverse treebanks.44
A shared task is an inspection of the state of the
art, but it may also accelerate research in an area
by providing a stable data basis as well as a set of
strong baselines. The results produced in this task
give a rich picture of the issues associated with pars-
ing MRLs and initial cues towards their resolution.
This set of results needs to be further analyzed to be
fully understood, which will in turn contribute to new
insights. We hope that this shared task will provide
inspiration for the design and evaluation of future
parsing systems for these languages.
Acknowledgments
We heartily thank Miguel Ballesteros and Corentin
Ribeire for running the dependency and constituency
baselines. We warmly thank the Linguistic Data Con-
sortium: Ilya Ahtaridis, Ann Bies, Denise DiPersio,
Seth Kulick and Mohamed Maamouri for releasing
the Arabic Penn Treebank for this shared task and
for their support all along the process. We thank
Alon Itai and MILA, the knowledge center for pro-
cessing Hebrew, for kindly making the Hebrew tree-
bank and morphological analyzer available for us,
Anne Abeill? for allowing us to use the French tree-
bank, and Key-Sun Choi for the Kaist Korean Tree-
bank. We thank Grzegorz Chrupa?a for providing
the morphological analyzer Morfette, and Joachim
44The data set will be made available as soon as possible under
the license distribution of the shared-task, with the exception
of the Arabic data, which will continue to be distributed by the
LDC.
176
Wagner for his LeafAncestor implementation. We
finally thank ?zlem ?etinog?lu, Yuval Marton, Benoit
Crabb? and Benoit Sagot who have been nothing but
supportive during all that time.
At the end of this shared task (though watch out
for further updates and analyses), what remains to be
mentioned is our deep gratitude to all people involved,
either data providers or participants. Without all of
you, this shared task would not have been possible.
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Anne Abeill?,
editor, Treebanks. Kluwer, Dordrecht.
Szymon Acedan?ski. 2010. A Morphosyntactic Brill Tag-
ger for Inflectional Languages. In Advances in Natural
Language Processing, volume 6233 of Lecture Notes
in Computer Science, pages 3?14. Springer-Verlag.
Meni Adler and Michael Elhadad. 2006. An unsupervised
morpheme-based HMM for Hebrew morphological dis-
ambiguation. In Proceedings COLING-ACL, pages
665?672, Sydney, Australia.
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised lexicon-based resolution
of unknown words for full morphological analysis. In
Proceedings of ACL-08: HLT, pages 728?736, Colum-
bus, OH.
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev.
Itziar Aduriz, Jos? Mar?a Arriola, Xabier Artola, A D?az
de Ilarraza, et al 1997. Morphosyntactic disambigua-
tion for Basque based on the constraint grammar for-
malism. In Proceedings of RANLP, Tzigov Chark, Bul-
garia.
Itziar Aduriz, Eneko Agirre, Izaskun Aldezabal, I?aki
Alegria, Xabier Arregi, Jose Maria Arriola, Xabier Ar-
tola, Koldo Gojenola, Aitor Maritxalar, Kepa Sarasola,
et al 2000. A word-grammar based morphological
analyzer for agglutinative languages. In Proceedings
of COLING, pages 1?7, Saarbr?cken, Germany.
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arriola,
Aitziber Atutxa, A Diaz de Ilarraza, Aitzpea Garmen-
dia, and Maite Oronoz. 2003. Construction of a
Basque dependency treebank. In Proceedings of the
2nd Workshop on Treebanks and Linguistic Theories
(TLT), pages 201?204, V?xj?, Sweden.
Zeljko Agic, Danijela Merkler, and Dasa Berovic. 2013.
Parsing Croatian and Serbian by using Croatian depen-
dency treebanks. In Proceedings of the Fourth Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Seattle, WA.
I. Aldezabal, M.J. Aranzabe, A. Diaz de Ilarraza, and
K. Fern?ndez. 2008. From dependencies to con-
stituents in the reference corpus for the processing of
Basque. In Procesamiento del Lenguaje Natural, no
41 (2008), pages 147?154. XXIV edici?n del Congreso
Anual de la Sociedad Espa?ola para el Procesamiento
del Lenguaje Natural (SEPLN).
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL), Los Angeles, CA.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOpti-
mizer: An optimization tool for MaltParser. In Pro-
ceedings of EACL, pages 58?62, Avignon, France.
Miguel Ballesteros. 2013. Effective morphological fea-
ture selection with MaltOptimizer at the SPMRL 2013
shared task. In Proceedings of the Fourth Workshop on
Statistical Parsing of Morphologically-Rich Languages,
pages 53?60, Seattle, WA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Appli-
cation of different techniques to dependency parsing
of Basque. In Proceedings of the NAACL/HLT Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010), Los Angeles, CA.
Philip Bille. 2005. A survey on tree edit distance and re-
lated problems. Theoretical Computer Science, 337(1?
3):217?239, 6.
Anders Bj?rkelund, Ozlem Cetinoglu, Rich?rd Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking meets morphosyntax: State-of-the-art re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing
of Morphologically-Rich Languages, pages 134?144,
Seattle, WA.
Ezra Black, Steven Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Philip Harrison, Donald
Hindle, Robert Ingria, Frederick Jelinek, Judith Kla-
vans, Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991. A
procedure for quantitatively comparing the syntactic
coverage of English grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop 1991,
pages 306?311, Pacific Grove, CA.
177
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the EMNLP-CoNLL, pages 1455?1465, Jeju,
Korea.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of COL-
ING, pages 89?97, Beijing, China.
Adriane Boyd. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations. In
Proceedings of the Linguistic Annotation Workshop,
Prague, Czech Republic.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT), pages 24?41, Sozopol,
Bulgaria.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164, New York, NY.
Tim Buckwalter. 2002. Arabic morphological analyzer
version 1.0. Linguistic Data Consortium.
Tim Buckwalter. 2004. Arabic morphological analyzer
version 2.0. Linguistic Data Consortium.
Marie Candito and Djam? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Marie Candito, Benoit Crabb?, and Pascal Denis. 2010.
Statistical French dependency parsing: Treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of CoNLL,
pages 9?16, Manchester, UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Barcelona,
Spain.
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In AAAI/IAAI, pages
598?603.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of NAACL, pages 132?139, Seat-
tle, WA.
Jinho D. Choi and Martha Palmer. 2011. Statistical de-
pendency parsing in Korean: From corpus generation
to automatic parsing. In Proceedings of Second Work-
shop on Statistical Parsing of Morphologically Rich
Languages, pages 1?11, Dublin, Ireland.
Jinho D. Choi and Martha Palmer. 2012. Guidelines
for the Clear Style Constituent to Dependency Conver-
sion. Technical Report 01-12, University of Colorado
at Boulder.
Key-sun Choi, Young S. Han, Young G. Han, and Oh W.
Kwon. 1994. KAIST Tree Bank Project for Korean:
Present and Future Development. In In Proceedings
of the International Workshop on Sharable Natural
Language Resources, pages 7?14, Nara, Japan.
Jinho D. Choi. 2013. Preparing Korean data for the
shared task on parsing morphologically rich languages.
arXiv:1309.1649.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of LREC, Marrakech, Morocco.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Volkan Cirik and H?sn? S?ensoy. 2013. The AI-KU
system at the SPMRL 2013 shared task: Unsuper-
vised features for dependency parsing. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 68?75, Seat-
tle, WA.
Michael Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
Matthieu Constant, Marie Candito, and Djam? Seddah.
2013. The LIGM-Alpage architecture for the SPMRL
2013 shared task: Multiword expression analysis and
dependency parsing. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 46?52, Seattle, WA.
Anna Corazza, Alberto Lavelli, Giogio Satta, and Roberto
Zanoli. 2004. Analyzing an Italian treebank with
state-of-the-art statistical parsers. In Proceedings of
the Third Workshop on Treebanks and Linguistic Theo-
ries (TLT), T?bingen, Germany.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
D?ra Csendes, J?nos Csirik, Tibor Gyim?thy, and Andr?s
Kocsor. 2005. The Szeged treebank. In Proceedings of
the 8th International Conference on Text, Speech and
Dialogue (TSD), Lecture Notes in Computer Science,
pages 123?132, Berlin / Heidelberg. Springer.
Eric De La Clergerie. 2013. Exploring beam-based
shift-reduce dependency parsing with DyALog: Re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
178
Morphologically-Rich Languages, pages 81?89, Seat-
tle, WA.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mona Diab, Nizar Habash, Owen Rambow, and Ryan
Roth. 2013. LDC Arabic treebanks and associated cor-
pora: Data divisions manual. Technical Report CCLS-
13-02, Center for Computational Learning Systems,
Columbia University.
Eva Ejerhed and Gunnel K?llgren. 1997. Stockholm
Ume? Corpus. Version 1.0. Department of Linguis-
tics, Ume? University and Department of Linguistics,
Stockholm University.
Eva Ejerhed, Gunnel K?llgren, Ola Wennstedt, and Mag-
nus ?str?m. 1992. The linguistic annotation system
of the Stockholm?Ume? Corpus project. Technical
Report 33, University of Ume?: Department of Linguis-
tics.
Nerea Ezeiza, I?aki Alegria, Jos? Mar?a Arriola, Rub?n
Urizar, and Itziar Aduriz. 1998. Combining stochastic
and rule-based methods for disambiguation in aggluti-
native languages. In Proceedings of COLING, pages
380?384, Montr?al, Canada.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL, pages
959?967, Columbus, OH.
Alexander Fraser, Helmut Schmid, Rich?rd Farkas, Ren-
jing Wang, and Hinrich Sch?tze. 2013. Knowledge
sources for constituent parsing of German, a morpho-
logically rich and less-configurational language. Com-
putational Linguistics, 39(1):57?85.
Iakes Goenaga, Koldo Gojenola, and Nerea Ezeiza. 2013.
Exploiting the contribution of morphological informa-
tion to parsing: the BASQUE TEAM system in the
SPRML?2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 61?67, Seattle, WA.
Yoav Goldberg and Michael Elhadad. 2010a. Easy-first
dependency parsing of Modern Hebrew. In Proceed-
ings of the NAACL/HLT Workshop on Statistical Pars-
ing of Morphologically Rich Languages (SPMRL 2010),
Los Angeles, CA.
Yoav Goldberg and Michael Elhadad. 2010b. An ef-
ficient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of HLT: NAACL, pages
742?750, Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of ACL, Columbus, OH.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proc. of ACL, Columbus, OH.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities. In
Proceedings of EALC, pages 327?335, Athens, Greece.
Yoav Goldberg. 2011. Automatic syntactic processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University
of the Negev.
David Graff, Mohamed Maamouri, Basma Bouziri, Son-
dos Krouna, Seth Kulick, and Tim Buckwalter. 2009.
Standard Arabic Morphological Analyzer (SAMA) ver-
sion 3.1. Linguistic Data Consortium LDC2009E73.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proceedings of COLING, pages 394?402, Beijing,
China.
Nathan Green, Loganathan Ramasamy, and Zden?k
?abokrtsk?. 2012. Using an SVM ensemble system for
improved Tamil dependency parsing. In Proceedings
of the ACL 2012 Joint Workshop on Statistical Pars-
ing and Semantic Processing of Morphologically Rich
Languages, pages 72?77, Jeju, Korea.
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2013. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195?227.
Noemie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew Treebank.
In Proceedings of the Eighth International Workshop on
Treebanks and Linguistic Theories (TLT), Groningen,
The Netherlands.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of ACL-
IJCNLP, pages 221?224, Suntec, Singapore.
Nizar Habash, Ryan Gabbard, Owen Rambow, Seth
Kulick, and Mitch Marcus. 2007. Determining case in
Arabic: Learning complex linguistic behavior requires
complex linguistic features. In Proceedings of EMNLP-
CoNLL, pages 1084?1092, Prague, Czech Republic.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009a. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009b.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS tag-
ging, stemming and lemmatization. In Proceedings of
the Second International Conference on Arabic Lan-
guage Resources and Tools. Cairo, Egypt.
179
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publishers.
Jan Hajic?, Alena B?hmov?, Eva Hajic?ov?, and Barbora
Vidov?-Hladk?. 2000. The Prague Dependency Tree-
bank: A three-level annotation scenario. In Anne
Abeill?, editor, Treebanks: Building and Using Parsed
Corpora. Kluwer Academic Publishers.
P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz. 2007.
HunPos ? an open source trigram tagger. In Proceed-
ings of ACL, pages 209?212, Prague, Czech Republic.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ls?en Eryig?it,
Be?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? A study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933?939, Prague, Czech Republic.
Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Martha
Palmer, and Heejong Yi. 2002. Penn Korean Treebank:
Development and evaluation. In Proceedings of the
16th Pacific Asia Conference on Language, Information
and Computation, Jeju, Korea.
Tilman H?hle. 1986. Der Begriff "Mittelfeld", Anmerkun-
gen ?ber die Theorie der topologischen Felder. In Ak-
ten des Siebten Internationalen Germanistenkongresses
1985, pages 329?340, G?ttingen, Germany.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable grammars.
In Proceedings of EMNLP, pages 12?22, Cambridge,
MA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of ACL,
pages 586?594, Columbus, OH.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98, March.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4):613?
632.
Laura Kallmeyer and Wolfgang Maier. 2013. Data-driven
parsing using probabilistic linear context-free rewriting
systems. Computational Linguistics, 39(1).
Fred Karlsson, Atro Voutilainen, Juha Heikkilae, and Arto
Anttila. 1995. Constraint Grammar: a language-
independent system for parsing unrestricted text. Wal-
ter de Gruyter.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430, Sapporo, Japan.
Sandra K?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German? In Pro-
ceedings of EMNLP, pages 111?119, Sydney, Australia,
July.
Sandra K?bler, Wolfgang Maier, Ines Rehbein, and Yan-
nick Versley. 2008. How to compare treebanks. In
Proceedings of LREC, pages 2322?2329, Marrakech,
Morocco.
Sandra K?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63, Columbus, OH.
Seth Kulick, Ryan Gabbard, and Mitch Marcus. 2006.
Parsing the Arabic Treebank: Analysis and Improve-
ments. In Proceedings of the Treebanks and Linguistic
Theories Conference, pages 31?42, Prague, Czech Re-
public.
Joseph Le Roux, Benoit Sagot, and Djam? Seddah. 2012.
Statistical parsing of Spanish and data driven lemmati-
zation. In Proceedings of the Joint Workshop on Statis-
tical Parsing and Semantic Processing of Morphologi-
cally Rich Languages, pages 55?61, Jeju, Korea.
Kong Joo Lee, Byung-Gyu Chang, and Gil Chang Kim.
1997. Bracketing Guidelines for Korean Syntactic Tree
Tagged Corpus. Technical Report CS/TR-97-112, De-
partment of Computer Science, KAIST.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank? In
Proceedings of ACL, Sapporo, Japan.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2004a. Arabic Treebank: Part 2 v 2.0.
LDC catalog number LDC2004T02.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004b. The Penn Arabic Treebank:
Building a large-scale annotated Arabic corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2005. Arabic Treebank: Part 1 v 3.0. LDC
catalog number LDC2005T02.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma Gad-
deche, Wigdan Mekki, Sondos Krouna, and Basma
Bouziri. 2009. The Penn Arabic Treebank part 3 ver-
sion 3.1. Linguistic Data Consortium LDC2008E22.
Wolfgang Maier, Miriam Kaeshammer, and Laura
Kallmeyer. 2012. Data-driven PLCFRS parsing re-
visited: Restricting the fan-out to two. In Proceedings
of the Eleventh International Conference on Tree Ad-
joining Grammars and Related Formalisms (TAG+11),
Paris, France.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference. In
Proceedings of EMNLP, pages 34?44, Cambridge, MA.
180
Yuval Marton, Nizar Habash, and Owen Rambow. 2013a.
Dependency parsing of Modern Standard Arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161?194.
Yuval Marton, Nizar Habash, Owen Rambow, and Sarah
Alkhulani. 2013b. SPMRL?13 shared task system:
The CADIM Arabic dependency parser. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?80, Seat-
tle, WA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of HLT:NAACL, pages 152?159, New York, NY.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proceedings of ACL, pages 91?98,
Ann Arbor, MI.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
Tackstrom, Claudia Bedini, Nuria Bertomeu Castello,
and Jungmee Lee. 2013. Universal dependency anno-
tation for multilingual parsing. In Proceedings of ACL,
Sofia, Bulgaria.
Igor Mel?c?uk. 2001. Communicative Organization in Nat-
ural Language: The Semantic-Communicative Struc-
ture of Sentences. J. Benjamins.
Knowledge Center for Processing Hebrew
MILA. 2008. Hebrew morphological analyzer.
http://mila.cs.technion.ac.il.
Antonio Moreno, Ralph Grishman, Susana Lopez, Fer-
nando Sanchez, and Satoshi Sekine. 2000. A treebank
of Spanish and its application to parsing. In Proceed-
ings of LREC, Athens, Greece.
Joakim Nivre and Be?ta Megyesi. 2007. Bootstrapping a
Swedish treeebank using cross-corpus harmonization
and annotation projection. In Proceedings of the 6th
International Workshop on Treebanks and Linguistic
Theories, pages 97?102, Bergen, Norway.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 915?932, Prague,
Czech Republic.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G?ls?en Eryig?it, Sandra K?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. In Proceedings
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL), a NAACL-HLT 2012
workshop, Montreal, Canada.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, Sydney, Australia.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA.
Slav Petrov. 2010. Products of random latent variable
grammars. In Proceedings of HLT: NAACL, pages 19?
27, Los Angeles, CA.
Adam Przepi?rkowski, Miros?aw Ban?ko, Rafa? L. G?rski,
and Barbara Lewandowska-Tomaszczyk, editors. 2012.
Narodowy Korpus Jkezyka Polskiego. Wydawnictwo
Naukowe PWN, Warsaw.
Ines Rehbein and Josef van Genabith. 2007a. Eval-
uating Evaluation Measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
NODALIDA-2007, Tartu, Estonia.
Ines Rehbein and Josef van Genabith. 2007b. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of EMNLP-CoNLL, Prague, Czech Re-
public.
Ines Rehbein. 2011. Data point selection for self-training.
In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, pages 62?
67, Dublin, Ireland.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of HLT-NAACL, pages
129?132, New York, NY.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology covering
derivation, composition and inflection. In Proceedings
of LREC, Lisbon, Portugal.
Djam? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
First Workshop on Statistical Parsing of Morphologi-
cally Rich Languages (SPMRL), Los Angeles, CA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a German
181
treebank. In Proceedings of LREC, pages 3132?3139,
Istanbul, Turkey.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined tree
substitution grammars for syntactic parsing. In Pro-
ceedings of ACL, pages 440?448, Jeju, Korea.
Anthony Sigogne, Matthieu Constant, and Eric Laporte.
2011. French parsing enhanced with a word clustering
method based on a syntactic lexicon. In Proceedings
of the Second Workshop on Statistical Parsing of Mor-
phologically Rich Languages, pages 22?27, Dublin,
Ireland.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altmann,
and Noa Nativ. 2001. Building a tree-bank of Modern
Hebrew text. Traitement Automatique des Langues,
42:347?380.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Pro-
ceedings of Text, Speech and Dialogue, pages 197?204,
Brno, Czech Republic.
Ulf Teleman. 1974. Manual f?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
Lucien Tesni?re. 1959. ?l?ments De Syntaxe Structurale.
Klincksieck, Paris.
Reut Tsarfaty and Khalil Sima?an. 2010. Modeling mor-
phosyntactic agreement in constituency-based parsing
of Modern Hebrew. In Proceedings of the First Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra
K?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing for morphologically rich language (SPMRL):
What, how and whither. In Proceedings of the First
workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP, Edinburgh, UK.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012a. Cross-framework evaluation for statistical pars-
ing. In Proceeding of EACL, Avignon, France.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012b. Joint evaluation for segmentation and parsing.
In Proceedings of ACL, Jeju, Korea.
Reut Tsarfaty, Djam? Seddah, Sandra K?bler, and Joakim
Nivre. 2012c. Parsing morphologically rich languages:
Introduction to the special issue. Computational Lin-
guistics, 39(1):15?22.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A unified morpho-syntactic scheme
of Stanford dependencies. In Proceedings of ACL,
Sofia, Bulgaria.
Veronika Vincze, D?ra Szauter, Attila Alm?si, Gy?rgy
M?ra, Zolt?n Alexin, and J?nos Csirik. 2010. Hungar-
ian Dependency Treebank. In Proceedings of LREC,
Valletta, Malta.
Joachim Wagner. 2012. Detecting Grammatical Errors
with Treebank-Induced Probabilistic Parsers. Ph.D.
thesis, Dublin City University.
Marcin Wolin?ski, Katarzyna G?owin?ska, and Marek
S?widzin?ski. 2011. A preliminary version of
Sk?adnica?a treebank of Polish. In Proceedings of
the 5th Language & Technology Conference, pages
299?303, Poznan?, Poland.
Alina Wr?blewska. 2012. Polish Dependency Bank. Lin-
guistic Issues in Language Technology, 7(1):1?15.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL:HLT, pages 188?193, Portland,
OR.
J?nos Zsibrita, Veronika Vincze, and Rich?rd Farkas.
2013. magyarlanc: A toolkit for morphological and
dependency parsing of Hungarian. In Proceedings of
RANLP, pages 763?771, Hissar, Bulgaria.
182
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 102?106,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
The IUCL+ System: Word-Level Language Identification via Extended
Markov Models
Levi King, Eric Baucom, Timur Gilmanov, Sandra K
?
ubler, Daniel Whyatt
Indiana University
{leviking,eabaucom,timugilm,skuebler,dwhyatt}@indiana.edu
Wolfgang Maier
Universita?t Du?sseldorf
maierw@hhu.de
Paul Rodrigues
University of Maryland
prr@umd.edu
Abstract
We describe the IUCL+ system for the shared
task of the First Workshop on Computational
Approaches to Code Switching (Solorio et al.,
2014), in which participants were challenged
to label each word in Twitter texts as a named
entity or one of two candidate languages. Our
system combines character n-gram probabili-
ties, lexical probabilities, word label transition
probabilities and existing named entity recog-
nition tools within a Markovmodel framework
that weights these components and assigns a
label. Our approach is language-independent,
and we submitted results for all data sets
(five test sets and three ?surprise? sets, cov-
ering four language pairs), earning the high-
est accuracy score on the tweet level on two
language pairs (Mandarin-English, Arabic-
dialects 1 & 2) and one of the surprise sets
(Arabic-dialects).
1 Introduction
This shared task challenged participants to perform
word level analysis on short, potentially bilingual Twit-
ter and blog texts covering four language pairs: Nepali-
English, Spanish-English,Mandarin-English andMod-
ern Standard Arabic-Arabic dialects. Training sets
ranging from 1,000 to roughly 11,000 tweets were pro-
vided for the language pairs, where the content of the
tweets was tokenized and labeled with one of six la-
bels. The goal of the task is to accurately replicate
this annotation automatically on pre-tokenized texts.
With an inventory of six labels, however, the task is
more than a simple binary classification task. In gen-
eral, the most common labels observed in the train-
ing data are lang1 and lang2, with other (mainly
covering punctuation and emoticons) also common.
Named entities (ne) are also frequent, and accounting
for them adds a significant complication to the task.
Less common are mixed (to account for words that
may e.g., apply L1 morphology to an L2 word), and
ambiguous (to cover a word that could exist in either
language, e.g., no in the Spanish-English data).
Traditionally, language identification is performed
on the document level, i.e., on longer segments of
text than what is available in tweets. These methods
are based on variants of character n-grams. Seminal
work in this area is by Beesley (1988) and Grefenstette
(1995). Lui and Baldwin (2014) showed that character
n-grams also perform on Twitter messages. One of a
few recent approaches working on individual words is
by King et al. (2014), who worked on historical data;
see also work by Nguyen and Dogruz (2013) and King
and Abney (2013).
Our system is an adaptation of a Markov model,
which integrates lexical, character n-gram, and la-
bel transition probabilities (all trained on the provided
data) in addition to the output of pre-existing NER
tools. All the information sources are weighted in the
Markov model.
One advantage of our approach is that it is language-
independent. We use the exact same architecture for
all language pairs, and the only difference for the indi-
vidual language pairs lies in a manual, non-exhaustive
search for the best weights. Our results show that the
approachworks well for the one language pair with dif-
ferent writing systems (Mandarin-English) as well as
for the most complex language pair, the Arabic set. In
the latter data set, the major difficulty consists in the
extreme skewing with an overwhelming dominance of
words in Modern Standard Arabic.
2 Method
Our system uses an extension of a Markov model to
perform the task of word level language identification.
The system consists of three main components, which
produce named entity probabilities, emission probabil-
ities and label transition probabilities. The outputs of
these three components are weighted and combined in-
side the extended Markov model (eMM), where the
best tag sequence for a given tweet (or sentence) is de-
termined via the Viterbi algorithm.
In the following sections, we will describe these
components in more detail.
2.1 Named Entity Recognition
We regard named entity recognition (NER) as a stand-
alone task, independent of language identification. For
this reason, NER is performed first in our system.
In order to classify named entities in the tweets, we
employ two external tools, Stanford-NER and Twit-
terNLP. Both systems are used in a black box approach,
102
without any attempt at optimization. I.e., we use the
default parameters where applicable.
Stanford NER (Finkel et al., 2005) is a state-of-the-
art named entity recognizer based on conditional ran-
dom fields (CRF), which can easily be trained on cus-
tom data.
1
For all of the four language pairs, we train a
NER model on a modified version of the training data
in which we have kept the label ?ne? as our target la-
bel, but replaced all others with the label ?O?. Thus, we
create a binary classification problem of distinguishing
named entities from all other words. This method is
applicable for all data sets.
For the Arabic data, we additionally employ a
gazetteer, namely ANERgazet (Benajiba and Rosso,
2008).
2
However, we do not use the three classes (per-
son, location, organization) available in this resource.
The second NER tool used in our system is the Twit-
terNLP package.
3
This system was designed specifi-
cally for Twitter data. It deals with the particular dif-
ficulties that Twitter-specific language (due to spelling,
etc.) poses to named entity recognition. The system has
been shown to be very successful: Ritter et al. (2011,
table 6) achieve an improvement of 52% on segmen-
tation F-score in comparison with Stanford NER on
hand-annotated Twitter data, which is mainly due to a
considerably increased recall.
The drawback of using TwitterNLP for our task is
that it was developed for English, and adapting it to
other languages would involve a major redesign and
adaptation of the system. For this reason, we decided
to use it exclusively on the language pairs that include
English. An inspection of the training data showed that
for all language pairs involving English, a majority of
the NEs are written in English and should thus be rec-
ognizable by the system.
TwitterNLP is an IOB tagger. Since we do not dis-
tinguish between the beginning and the rest of a named
entity, we change all corresponding labels to ?ne? in
the output of the NER system.
In testing mode, the NER tools both label each word
in a tweet as either ?O? or ?ne?. We combine the output
such that ?ne? overrides ?O? in case of any disagree-
ments, and pass this information to the eMM. This out-
put is weighted with optimized weights unique to each
language pair that were obtained through 10-fold cross
validation, as discussed below. Thus, the decisions of
the NER systems is not final, but they rather provide
evidence that can be overruled by other system compo-
nents.
2.2 Label Transition Models
The label transition probability component models lan-
guage switches on the sequence of words. It is also
1
See http://nlp.stanford.edu/software/
CRF-NER.shtml.
2
As available from http://users.dsic.upv.es/
grupos/nle/.
3
See https://github.com/aritter/
twitter_nlp.
trained on the provided training data. In effect, this
component consists of unigram, bigram, and trigram
probability models of the sequences of labels found
in the training data. Our MM is second order, thus
the transition probabilities are linear interpolations of
the uni-, bi-, and trigram label transition probabili-
ties that were observed in the training data. We add
two beginning-of-sentence buffer labels and one end-
of-sentence buffer label to assist in deriving the start-
ing and ending probabilities of each label during the
training.
2.3 Emission Probabilities
The emission probability component is comprised of
two subcomponents: a lexical probability component
and a character n-gram probability component. Both
are trained on the provided training data.
Lexical probabilities: The lexical probability com-
ponent consists of a dictionary for each label contain-
ing the words found under that label and their rel-
ative frequencies. Each word type and its count of
tokens are added to the total for each respective la-
bel. After training, the probability of a given label
emitting a word (i.e., P (word|label)) is derived from
these counts. To handle out-of-vocabulary words, we
use Chen-Goodman ?one-count? smoothing, which ap-
proximates the probabilities of unknownwords as com-
pared to the occurrence of singletons (Chen and Good-
man, 1996).
Character n-gram probabilities: The character-
based n-grammodel serves mostly as a back-off in case
a word is out-of-vocabulary, in which case the lexi-
cal probability may not be reliable. However, it also
provides important information in the case of mixed
words, which may use morphology from one language
added to a stem from the other one. In this setting, un-
igrams are not informative. For this reason, we select
longer n-grams, with n ranging between 2 and 5.
Character n-gram probabilities are calculated as fol-
lows: For each training set, the words in that training
set are sorted into lists according to their labels. In
training models for each value of n, n ? 1 buffer char-
acters are added to the beginning and end of each word.
For example, in creating a trigram character model
for the lang1 (English) words in the Nepali-English
training set, we encounter the word star. We first gen-
erate the form $$star##, then derive the trigrams. The
trigrams from all training words are counted and sorted
into types, and the counts are converted to relative fre-
quencies.Thus, using four values of n for a data set
containing six labels, we obtain 24 character n-gram
models for that language pair. Note that because this
component operates on individual words, character n-
grams never cross a word boundary.
In testing mode, for each word and for each value of
n, the component generates a probability that the word
occurred under each of the six labels. These values
103
are passed to the eMM, which uses manually optimized
weights for each value of n to combine the four n-gram
scores for each label into a single n-gram score for each
label. In cases where an n-gram from the test word
was not present in the training data, we use a primitive
variant of LaPlace smoothing, which returns a fixed,
extremely low non-zero probability for that n-gram.
2.4 The Extended Markov Model
Our approach is basically a trigram Markov model
(MM), in which the observations are the words in
the tweet (or blog sentence) and the underlying states
correspond to the sequence of codeswitching labels
(lang1, lang2, ne, mixed, ambiguous,
other). The MM, as usual, also uses starting
and ending probabilities (in our case, derived from
standard training of the label transition model, due
to our beginning- and end-of-sentence buffer labels),
label/state transition probabilities, and probabilities
that the state labels will emit particular observations.
The only difference is that we modify the standard
HMM emission probabilities. We call this resulting
Markov model extended (eMM).
First, for every possible state/label in the sequence,
we linearly interpolate ?lexical (emission) probabil-
ities? P
lex
(the standard emission probabilities for
HMMs) with character n-gram probabilities P
char
.
That is, we choose 0 ? ?
lex
? 1 and 0 ? ?
char
? 1
such that ?
lex
+ ?
char
= 1. We use them to derive
a new emission probability P
combined
= ?
lex
? P
lex
+
?
char
?P
char
. This probability represents the likelihood
that the given label in the hidden layer will emit the lex-
ical observation, along with its corresponding character
n-gram sequence.
Second, only for ne labels in the hidden layer, we
modify the probabilities that they will emit the ob-
served word if that word has been judged by our NER
module to be a named entity. Since the NER compo-
nent exhibits high precision but comparatively low re-
call, we boost the P
combined
(label = ne|word) if the
observedword is judged to be a named entity, but we do
not penalize the regular P
combined
if not. This boosting
is accomplished via linear interpolation and another set
of parameters, 0 ? ?
ne
? 1 and 0 ? ?
combined
? 1
such that ?
ne
+ ?
combined
= 1. Given a positive de-
cision from the NER module, the new probability for
the ne label emitting the observed word is derived as
P
ne+combined
= ?
ne
? 0.80 + ?
combined
? P
combined
,
i.e., we simply interpolate the original probability with
a high probability. All lambda values, as well as the
weights for the character n-gram probabilities, were set
via 10-fold cross-validation, discussed below.
2.5 Cross Validation & Optimization
In total, the system uses 11 weights, each of which is
optimized for each language pair. In labeling named
entities, the output of the NER component is given one
weight and the named entity probabilities of the other
sources (emission and label transition components) is
given another weight, with these weights summing to
one. For the label transition component, the uni-, bi-
and trigram scores receive weights that sum to one.
Likewise, the emission probability component is com-
prised of the lexical probability and the character n-
gram probability, with weights that sum to one. The
character n-gram component is itself comprised of the
bi-, tri-, four- and five-gram scores, again with weights
that sum to one.
For each language pair, these weights were opti-
mized using a 10-fold cross validation script that splits
the original training data into a training file and a test
file, runs the split files through the system and averages
the output. As time did not allow an exhaustive search
for optimal weights in this multi-dimensional space, we
narrowed the space by first manually optimizing each
subset of weights independently, then exploring com-
binations of weights in the resulting neighborhood.
3 Results
3.1 Main Results
The results presented in this section are the official re-
sults provided by the organizers. The evaluation is split
into two parts: a tweet level evaluation and a token level
evaluation. On the tweet level, the evaluation concen-
trates on the capability of systems to distinguish mono-
lingual from multilingual tweets. The token level eval-
uation is concerned with the classification of individ-
ual words into the different classes: lang1, lang2,
ambiguous, mixed, ne, and other.
Our results for the tweet level evaluation, in com-
parison to the best or next-best performing system are
shown in table 1. They show that our system is ca-
pable of discriminating monolingual from multilingual
tweets with very high precision. This resulted in the
best results in the evaluation with regard to accuracy
for Mandarin-English and for both Arabic-dialects set-
tings. We note that for the latter setting, reaching good
results is exceedingly difficult without any Arabic re-
sources. This task is traditionally approached by us-
ing a morphological analyzer, but we decided to use
a knowledge poor approach. This resulted in a rather
high accuracy but in low precision and recall, espe-
cially for the first Arabic test set, which was extremely
skewed, with only 32 out of 2332 tweets displaying
codeswitching.
Our results for the token level evaluation, in com-
parison to the best performing system per language,
are shown in table 2. They show that our system sur-
passed the baseline for both language pairs for which
the organizers provided baselines. In terms of accu-
racy, our system is very close to the best performing
system for the pairs Spanish-English andMandarin En-
glish. For the other language pairs, we partially suffer
from a weak NER component. This is especially obvi-
ous for the Arabic dialect sets. However, this is also a
problem that can be easily fixed by using a more com-
104
lang. pair system Acc. Recall Precision F-score
Nep.-Eng. IUCL+ 91.2 95.6 94.9 95.2
dcu-uvt 95.8 99.4 96.1 97.7
Span.-Eng. IUCL+ 83.8 51.4 87.7 64.8
TAU 86.8 72.0 80.3 75.9
Man.-Eng. IUCL+ 82.4 94.3 85.0 89.4
MSR-India 81.8 95.5 83.7 89.2
Arab. dia. IUCL+ 97.4 12.5 11.1 11.8
MSR-India 94.7 34.4 9.7 15.2
Arab. dia. 2 IUCL+ 76.6 24.9 27.1 26.0
MSR-India 71.4 21.2 18.3 19.6
Table 1: Tweet level results in comparison to the system with (next-)highest accuracy.
lang1 lang2 mixed ne
lang. pair system Acc. R P F R P F R P F R P F
Nep.-Eng. IUCL+ 75.2 85.1 89.1 87.1 68.9 97.6 80.8 1.7 100 3.3 55.1 48.7 51.7
dcu-uvt 96.3 97.9 95.2 96.5 98.8 96.1 97.4 3.3 50.0 6.3 45.6 80.4 58.2
base 70.0 57.1 76.5 65.4 92.3 62.8 74.7 0.0 100 0.0 0.0 100 0.0
Span.-Eng. IUCL+ 84.4 88.9 82.3 85.5 85.1 89.9 87.4 0.0 100 0.0 30.4 48.5 37.4
TAU 85.8 90.0 83.0 86.4 86.9 91.4 89.1 0.0 100 0.0 31.3 54.1 39.6
base 70.3 85.1 67.6 75.4 78.1 72.8 75.4 0.0 100 0.0 0.0 100 0.0
Man.-Eng. IUCL+ 89.5 98.3 97.8 98.1 83.9 66.6 74.2 0.0 100 0.0 70.1 50.3 58.6
MSR-India 90.4 98.4 97.6 98.0 89.1 66.6 76.2 0.0 100 0.0 67.7 65.2 66.4
Arab. dia. IUCL+ 78.8 96.1 81.6 88.2 34.8 8.9 14.2 ? ? ? 3.3 23.4 5.8
CMU 91.0 92.2 97.0 94.6 57.4 4.9 9.0 ? ? ? 77.8 70.6 74.0
Arab. dia. 2 IUCL+ 51.9 90.7 43.8 59.0 47.7 78.3 59.3 0.0 0.0 0.0 8.5 28.6 13.1
CMU 79.8 85.4 69.0 76.3 76.1 87.3 81.3 0.0 100 0.0 68.7 78.8 73.4
Table 2: Token level results in comparison to the system with highest accuracy (results for ambiguous and
other are not reported).
lang1 lang2 ne
lang. pair system Acc. R P F R P F R P F
Nep.-Eng. IUCL+ 80.5 86.1 78.8 82.3 97.6 80.9 88.5 29.9 80.9 43.7
JustAnEagerStudent 86.5 91.3 80.2 85.4 93.6 91.1 92.3 39.4 83.3 53.5
Span.-Eng. IUCL+ 91.8 87.4 81.9 84.5 84.5 87.4 85.9 28.5 47.4 35.6
dcu-uvt 94.4 87.9 80.5 84.0 84.1 86.7 85.4 22.4 55.2 31.9
Arab. dia. IUCL+ 48.9 91.7 33.3 48.8 48.4 81.9 60.9 3.3 17.6 5.5
CMU 77.5 87.6 55.5 68.0 75.6 89.8 82.1 52.3 73.8 61.2
Table 3: Token level results for the out-of-domain data.
petitive, language dependent system. Another problem
constitutes the mixed cases, which cannot be reliably
annotated.
3.2 Out-Of-Domain Results
The shared task organizers provided ?surprise? data,
from domains different from the training data. Our re-
sults on those data sets are shown in table 3. For space
reasons, we concentrate on the token level results only.
The results show that our system is very robust with
regard to out-of-domain settings. For Nepali-English
and Spanish-English, we reach higher results than on
the original test sets, and for the Arabic dialects, the re-
sults are only slightly lower. These results need further
analysis for us to understand how our system performs
in such situations.
4 Conclusions
We have presented the IUCL+ system for word level
language identification. Our system is based on a
Markov model, which integrates different types of in-
formation, including the named entity analyses, lexical
and character n-gram probabilities as well as transition
probabilities. One strength of the system is that it is
completely language independent. The results of the
shared task have shown that the system generally pro-
vides reliable results, and it is fairly robust in an out-
of-domain setting.
105
References
Kenneth R. Beesley. 1988. Language identifier: A
computer program for automatic natural-language
identification of on-line text. In Proceedings of the
29th Annual Conference of the American Translators
Association, volume 47, page 54.
Yassine Benajiba and Paolo Rosso. 2008. Arabic
named entity recognition using conditional random
fields. In Proceedings of Workshop on HLT & NLP
within the Arabic World, LREC 2008, Marakech,
Morroco.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meet-
ing on Association for Computational Linguistics,
pages 310?318.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 363?370.
Gregory Grefenstette. 1995. Comparing two language
identification schemes. In Proceedings of the Third
International Conference on Statistical Analysis of
Textual Data (JADT), volume 2.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-languagedocuments using
weakly supervised methods. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1110?1119. As-
sociation for Computational Linguistics.
Levi King, Sandra Ku?bler, and Wallace Hooper. 2014.
Word-level language identification in The Chymistry
of Isaac Newton. Literary and Linguistic Comput-
ing.
Marco Lui and Timothy Baldwin. 2014. Accurate lan-
guage identification of Twitter messages. In Pro-
ceedings of the 5th Workshop on Language Analysis
for Social Media (LASM), pages 17?25, Gothenburg,
Sweden.
Dong Nguyen and A. Seza Dogruz. 2013. Word level
language identification in online multilingual com-
munication. In Proceedings of the 2013 Conference
on Empirical Methods in Natural LanguageProcess-
ing, pages 857?862.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1524?1534, Edinburgh, Scotland,
UK., July. Association for Computational Linguis-
tics.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Julia Hirshberg, Alison Chang, and Pas-
cale Fung. 2014. Overview for the first shared
task on language identification in code-switched
data. In Proceedings of the First Workshop on Com-
putational Approaches to Code-Switching. EMNLP
2014, Conference on Empirical Methods in Natural
Language Processing, Doha, Qatar.
106
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 25?35,
October 29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Language variety identification in Spanish tweets
Wolfgang Maier
Institute for Language and Information
University of D?usseldorf
D?usseldorf, Germany
maierw@hhu.de
Carlos G
?
omez-Rodr??guez
Depto. de Computaci?on
Universidade da Coru?na
A Coru?na, Spain
cgomezr@udc.es
Abstract
We study the problem of language vari-
ant identification, approximated by the
problem of labeling tweets from Spanish
speaking countries by the country from
which they were posted. While this task
is closely related to ?pure? language iden-
tification, it comes with additional com-
plications. We build a balanced collec-
tion of tweets and apply techniques from
language modeling. A simplified version
of the task is also solved by human test
subjects, who are outperformed by the
automatic classification. Our best auto-
matic system achieves an overall F-score
of 67.7% on 5-class classification.
1 Introduction
Spanish (or castellano), a descendant of Latin,
is currently the language with the second largest
number of native speakers after Mandarin Chi-
nese, namely around 414 million people (Lewis
et al., 2014). Spanish has a large number of re-
gional varieties across Spain and the Americas
(Lipski, 1994).
1
They diverge in spoken language
and vocabulary and also, albeit to a lesser extent,
in syntax. Between different American varieties
of Spanish, there are important differences; how-
ever, the largest differences can be found between
American and European (?Peninsular?) Spanish.
Language identification, the task of automati-
cally identifying the natural language used in a
given text segment, is a relatively well understood
problem (see Section 2). To our knowledge, how-
ever, there is little previous work on the identifica-
tion of the varieties of a single language, such as
the regional varieties of Spanish. This task is espe-
cially challenging because the differences between
1
We are aware that there are natively Spanish-speaking
communities elsewhere, such as on the Philippines, but we
do not consider them in this study.
variants are subtle, making it difficult to discern
between them. This is evidenced by the fact that
humans that are native speakers of the varieties
are often unable to solve the problem, particularly
when given short, noisy text segments (which are
the focus of this work) where the amount of avail-
able information is limited.
In this paper, we approximate the problem of
language variety identification by the problem
of classifying status messages from the micro-
blogging service Twitter (?tweets?) from Span-
ish speaking countries by the country from which
they were sent. With the tweet, the location of
the device from which the tweet was sent can be
recorded (depending on the Twitter users? permis-
sion) and can then be retrieved from the metadata
of the tweet. The tweet location information does
not always correlate with the actual language va-
riety used in the tweet: it is conceivable, e.g., that
migrants do not use the prevalent language vari-
ety of the country in which they live, but rather
their native variety. Nevertheless, Twitter can give
a realistic picture of actual language use in a cer-
tain region, which, additionally, is closer to spoken
than to standard written language. Eventually and
more importantly, Twitter data is available from
almost all Spanish speaking countries.
We proceed as follows. We build a balanced
collection of tweets sent by Twitter users from
five countries, namely Argentina, Chile, Colom-
bia, Mexico, and Spain. Applying different meth-
ods, we perform an automatic classification be-
tween all countries. In order to obtain a more de-
tailed view of the difficulty of our task, we also
investigate human performance. For this purpose,
we build a smaller sample of tweets from Ar-
gentina, Chile and Spain and have them classified
by both our system and three native human evalua-
tors. The results show that automatic classification
outperforms human annotators. The best variant
of our system, using a meta-classifier with voting,
25
reaches an overall F-score of 67.72 on the five-
class problem. On the two-class problem, human
classification is outperformed by a large margin.
The remainder of this paper is structured as fol-
lows. In the following section, we present related
work. Section 3 presents our data collection. Sec-
tions 4 and 5 present our classification methodol-
ogy and the experiments. Section 7 discusses the
results, and Section 8 concludes the article.
2 Related Work
Research on language identification has seen a va-
riety of methods. A well established technique is
the use of character n-gram models. Cavnar and
Trenkle (1994) build n-gram frequency ?profiles?
for several languages and classify text by match-
ing it to the profiles. Dunning (1994) uses lan-
guage modeling. This technique is general and
not limited to language identification; it has also
been successfully employed in other areas, e.g., in
authorship attribution (Ke?selj et al., 2003) and au-
thor native language identification (Gyawali et al.,
2013). Other language identification systems use
non-textual methods, exploiting optical properties
of text such as stroke geometry (Muir and Thomas,
2000), or using compression methods which rely
on the assumption that natural languages differ
by their entropy, and consequently by the rate
to which they can be compressed (Teahan, 2000;
Benedetto et al., 2002). Two newer approaches
are Brown (2013), who uses character n-grams,
and
?
Reh?u?rek and Kolkus (2009), who treat ?noisy?
web text and therefore consider the particular in-
fluence of single words in discriminating between
languages.
Language identification is harder the shorter the
text segments whose language is to be identified
(Baldwin and Lui, 2010). Especially due to the
rise of Twitter, this particular problem has recently
received attention. Several solutions have been
proposed. Vatanen et al. (2010) compare character
n-gram language models with elaborate smooth-
ing techniques to the approach of Cavnar and
Trenkle and the Google Language ID API, on the
basis of different versions of the Universal Decla-
ration of Human Rights. Other researchers work
on Twitter. Bergsma et al. (2012) use language
identification to create language specific tweet col-
lections, thereby facilitating more high-quality re-
sults with supervised techniques. Lui and Baldwin
(2014) review a wide range of off-the-shelf tools
for Twitter language identification, and achieve
their best results with a voting over three individ-
ual systems, one of them being langid.py (Lui
and Baldwin, 2012). Carter et al. (2013) exploit
particular characteristics of Twitter (such as user
profile data and relations between Twitter users)
to improve language identification on this genre.
Bush (2014) successfully uses LZW compression
for Twitter language identification.
Within the field of natural language processing,
the problem of language variant identification has
only begun to be studied very recently. Zampieri
et al. (2013) have addressed the task for Spanish
newspaper texts, using character and word n-gram
models as well as POS and morphological infor-
mation. Very recently, the Discriminating between
Similar Languages (DSL) Shared Task (Zampieri
et al., 2014) proposed the problem of identify-
ing between pairs of similar languages and lan-
guage variants on sentences from newspaper cor-
pora, one of the pairs being Peninsular vs. Argen-
tine Spanish. However, all these approaches are
tailored to the standard language found in news
sources, very different from the colloquial, noisy
language of tweets, which presents distinct chal-
lenges for NLP (Derczynski et al., 2013; Vilares et
al., 2013). Lui and Cook (2013) evaluate various
approaches to classify documents into Australian,
British and Canadian English, including a corpus
of tweets, but we are not aware of any previous
work on variant identification in Spanish tweets.
A review of research on Spanish varieties from
a linguistics point of view is beyond the scope of
this article. Recommended further literature in this
area is Lipski (1994), Quesada Pacheco (2002)
and Alvar (1996b; 1996a).
3 Data Collection
We first built a collection of tweets using the
Twitter streaming API,
2
requesting all tweets sent
within the geographic areas given by the coordi-
nates -120
?
, -55
?
and -29
?
, 30
?
(roughly delimit-
ing Latin America), as well as -10
?
, 35
?
and 3
?
,
46
?
(roughly delimiting Spain). The download ran
from July 2 to July 4, 2014. In a second step, we
sorted the tweets according to the respective coun-
tries.
Twitter is not used to the same extent in all
countries where Spanish is spoken. In the time
2
https://dev.twitter.com/docs/api/
streaming
26
it took to collect 2,400 tweets from Bolivia,
we could collect over 700,000 tweets from Ar-
gentina.
3
To ensure homogeneous conditions for
our experiments, our final tweet collection com-
prises exactly 100,000 tweets from each of the five
countries from which most tweets were collected,
that is, Argentina, Chile, Colombia, Mexico, and
Spain.
At this stage, we do not perform any cleanup
or normalization operations such as, e.g., deleting
forwarded tweets (?re-tweets?), deleting tweets
which are sent by robots, or tweets not written in
Spanish (some tweets use code switching, or are
entirely written in a different language, mostly in
English or in regional and minority languages that
coexist with Spanish in the focus countries). Our
reasoning behind this is that the tweet production
in a certain country captures the variant of Spanish
that is spoken.
We mark the start and end of single tweets by
<s> and </s>, respectively. We use 80% of the
tweets of each language for training, and 10% for
development and testing, respectively. The data
is split in a round-robin fashion, i.e., every ninth
tweet is put into the development set and every
tenth tweet is put in the test set, all other tweets
are put in the training set.
In order to help with the interpretation of clas-
sification results, we investigate the distribution of
tweet lengths on the development set, as shown in
Figure 1. We see that in all countries, tweets tend
to be either short, or take advantage of all available
characters. Lengths around 100 to 110 characters
are the rarest. The clearest further trend is that the
tweets from Colombia and, especially, Argentina
tend to be shorter than the tweets from the other
countries.
4 Automatic Tweet Classification
The classification task we envisage is similar to
the task of language identification in short text
segments. We explore three methods that have
been used before for that task, namely character
n-gram frequency profiles (Cavnar and Trenkle,
1994; Vatanen et al., 2010), character n-gram lan-
guage models (Vatanen et al., 2010), as well as
LZW compression (Bush, 2014). Furthermore, we
explore the usability of syllable-based language
3
We are aware that the Twitter API does not make all sent
tweets available. However, we still assume that this huge dif-
ference reflects a variance in the number of Twitter users.
 0
 50
 100
 150
 200
 0  20  40  60  80  100  120  140
#
Tweet length
ESCLCOARMX
Figure 1: Tweet length distribution
50 100 500 1k 10k
AR 31.68 29.72 43.93 31.77 18.42
CO 24.29 21.36 26.14 19.68 19.03
MX 31.86 28.97 32.58 30.28 22.27
ES 20.19 25.22 22.08 21.25 16.15
CL 22.95 29.74 35.67 26.01 16.69
Table 1: Results (F
1
): n-gram frequency profiles
(classes/profile sizes)
models. For all four approaches, we train mod-
els for binary classification for each class, i.e., five
models that decide for each tweet if it belongs to a
single class. As final label, we take the output of
the one of the five classifiers that has the highest
score.
We finally use a meta-classifier on the basis of
voting. All methods are tested on the development
set. For evaluation, we compute precision, recall
and F
1
overall as well as for single classes.
Note that we decided to rely on the tweet text
only. An exploration of the benefit of, e.g., directly
exploiting Twitter-specific information (such as
user mentions or hash tags) is out of the scope of
this paper.
4.1 Character n-gram frequency profiles
We first investigate the n-gram frequency ap-
proach of Cavnar and Trenkle (1994). We use the
well-known implementation TextCat.
4
The re-
sults for all classes with different profile sizes are
shown in Table 1. Table 2 shows precision and re-
call for the best setting, a profile with a maximal
size of 500 entries.
The results obtained with a profile size of 500
4
As available from http://odur.let.rug.nl/
?
vannoord/TextCat/.
27
class precision recall F
1
AR 32.60 67.33 43.93
CO 31.66 22.26 26.14
MX 51.52 23.82 32.58
ES 32.83 16.63 22.08
CL 31.96 40.36 35.67
overall 34.08 34.08 34.08
Table 2: Results: n-gram frequency profile with
500 n-grams
AR CO MX ES CL
AR 6,733 949 384 610 1,324
CO 4,207 2,226 720 803 2,044
MX 2,547 1,342 2,382 1,051 2,678
ES 3,781 1,361 649 1,663 2,546
CL 3,384 1,153 488 939 4,036
Table 3: Confusion matrix (n-gram freq. profiles,
500 n-grams)
entries for Colombia align with the results for
Spain and Mexico in that the precision is higher
than the recall. The results for Chile align with
those for Argentina with the recall being higher
than the precision. For Mexico and Argentina the
differences between recall and precision are par-
ticularly large (28 and 35 points, respectively).
The confusion matrix in Table 3 reveals that tweets
from all classes are likely to be mislabeled as
coming from Argentina, while, on the other hand,
Mexican tweets are mislabeled most frequently as
coming from other countries.
Overall, the n-gram frequency profiles are not
very good at our task, achieving an maximal over-
all F-score of only 34.08 with a profile size of 500
entries. However, this performance is still well
above the 20.00 F-score we would obtain with
a random baseline. Larger profile sizes deterio-
rate results: with 10,000 entries, we only have
an overall F-score of 18.23. As observed before
(Vatanen et al., 2010), the weak performance can
most likely be attributed to the shortness of the
tweets and the resulting lack of frequent n-grams
that hinders a successful profile matching. While
Vatanen et al. alleviate this problem to some ex-
tent, they have more success with character-level
n-gram language models, the approach which we
explore next.
 40
 45
 50
 55
 60
 65
 70
 2  3  4  5  6
F-sc
ore
n-gram order
no pruning0.01 pruning0.1 pruning1 pruning
Figure 2: Character n-gram lm: Pruning vs. n-
gram order
4.2 Character n-gram language models
We recur to n-gram language models as avail-
able in variKN (Siivola et al., 2007).
5
We run
variKN with absolute discounting and the cross-
product of four different pruning settings (no prun-
ing, and thresholds 0.01, 0.1 and 1) and five differ-
ent n-gram lengths (2 to 6).
Figure 2 contrasts the effect of different pruning
settings with different n-gram lengths. While ex-
cessive pruning is detrimental to the result, slight
pruning has barely any effect on the results, while
reducing look-up time immensely. The order of
the n-grams, however, does have an important in-
fluence. We confirm that also for this problem, we
do not benefit from increasing it beyond n = 6,
like Vatanen et al. (2010).
We now check if some countries are more dif-
ficult to identify than others and how they bene-
fit from different n-gram orders. Figure 3 visual-
izes the corresponding results. Not all countries
profit equally from longer n-grams. When com-
paring the 3- and 6-gram models without pruning,
we see that the F
1
for Argentina is just 8 points
higher, while the difference is more than 14 points
for Mexico.
Table 4 shows all results including precision and
recall for all classes, in the setting with 6-grams
and no pruning. We can see that this approach
works noticeably better than the frequency pro-
files, achieving an overall F-score of 66.96. The
behavior of the classes is not uniform: Argentina
shows the largest difference between precision and
recall, and is furthermore the only class in which
precision is higher than recall. Note also that in
5
https://github.com/vsiivola/variKN
28
 35
 40
 45
 50
 55
 60
 65
 70
 2  3  4  5  6
F-sc
ore
n-gram order
ESCLCOARMX
Figure 3: Character n-gram lm: Classes vs. n-
gram order (no pruning)
class precision recall F
1
AR 70.67 66.22 68.37
CO 62.56 62.77 62.66
MX 65.23 65.74 65.48
ES 68.75 69.36 69.06
CL 67.81 70.73 69.24
overall 66.96 66.96 66.96
Table 4: Results: 6-grams without pruning
general, the differences between precision and re-
call are lower than for the n-gram frequency pro-
file approach. The confusion matrix shown in Ta-
ble 5 reveals that the Colombia class is the one
with the highest confusion, particularly in com-
bination with the Mexican class. This could in-
dicate that those classes are more heterogeneous
than the others, possibly showing more Twitter-
specific noise, such as tweets consisting only of
URLs, etc.
We finally investigate how tweet length influ-
ences classification performance in the 6-gram
model. Figure 4 shows the F-scores for intervals
of length 20 for all classes. The graph confirms
that longer tweets are easier to classify. This cor-
relates with findings from previous work. Over
82 points F
1
are achieved for tweets from Chile
AR CO MX ES CL
AR 6,622 1,036 702 740 900
CO 800 6,277 1,151 875 897
MX 509 1,237 6,574 847 833
ES 630 850 857 6,936 727
CL 809 634 794 690 7,073
Table 5: Confusion matrix (6-grams, no pruning)
 40
 50
 60
 70
 80
 90
 20  40  60  80  100  120  140
F-sc
ore
length
ESCLCOARMX
Figure 4: Character n-grams: Results (F
1
) for
tweet length intervals
 40
 50
 60
 70
 80
 90
 20  40  60  80  100  120  140
prec
isio
n/re
call
length
CO precisionCO recallCL precisionCL recall
Figure 5: Character n-grams: Precision/recall for
AR and CL
longer than 120 characters, while for those con-
taining up to 20 characters, F
1
is almost 30 points
lower. We investigate precision and recall sepa-
rately. Figure 5 shows the corresponding curves
for the best and worst performing classes, namely,
CL and CO. For Chile, both precision and recall
develop in parallel to the F-score (i.e., the longer
the tweets, the higher the scores). For Colombia,
the curves confirm that the low F
1
is rather due to
a low precision than a low recall, particularly for
tweets longer than 40 characters. This correlates
with the counts in the confusion table (Tab. 5).
4.3 Syllable n-gram language models
Since varieties of Spanish exhibit differences in
vocabulary, we may think that models based on
word n-grams can be more useful than character
n-grams to discriminate between varieties. How-
ever, the larger diversity of word n-grams means
that such models run into sparsity problems. An
intermediate family of models can be built by us-
29
 45
 50
 55
 60
 65
 2  3  4
F-sc
ore
n-gram order
ESCLCOARMX
Figure 6: Syllable n-gram lm: pruning vs. n-gram
order
ing syllable n-grams, taking advantage of the fact
that Spanish variants do not differ in the criteria
for syllabification of written words. Note that this
property does not hold in general for the language
identification problem, as different languages typ-
ically have different syllabification rules, which is
a likely reason why syllable n-gram models have
not been used for this problem.
To perform the splitting of Spanish words into
syllables, we use the TIP syllabifier (Hern?andez-
Figeroa et al., 2012), which applies an algorithm
implementing the general syllabification rules de-
scribed by the Royal Spanish Academy of Lan-
guage and outlined in standard Spanish dictionar-
ies and grammars. These rules are enough to cor-
rectly split the vast majority of Spanish words, ex-
cluding only a few corner cases related with word
prefixes (Hern?andez-Figueroa et al., 2013). While
accurate syllabification requires texts to be written
correctly with accented characters, and this is of-
ten not the case in informal online environments
(Vilares et al., 2014); we assume that this need not
cause problems because the errors originated by
unaccented words will follow a uniform pattern,
producing a viable model for the purposes of clas-
sification.
We train n-gram language models with
variKN as described in the last section, using
absolute discounting. Due to the larger vocabulary
size, we limit ourselves to 0.01 pruning, and to
n-gram orders 2 to 4. Figure 6 shows the results
(F
1
) of all classes for the different n-gram orders,
and Table 6 shows the results for all classes for
the 4-gram language model.
As expected, shorter n-grams are more effective
for syllable than for character language models.
class precision recall F
1
AR 55.94 61.11 58.41
CO 53.23 53.03 53.13
MX 59.10 56.17 57.60
ES 62.35 56.96 59.53
CL 59.31 62.12 60.68
overall 57.88 57.88 57.88
Table 6: Results (F
1
): Syllable 4-gram lm
For the Chilean tweets, e.g., the F-score for the 2-
gram language model is around 11 points higher
than for the character 2-gram language model.
Furthermore, the performance seems to converge
earlier, given that the results change only slightly
when raising the n-gram order from 3 to 4. The
overall F-score for the 4-gram language model is
around 6 points lower than for character 4-grams.
However, the behavior of the classes is similar:
again, Mexico and Colombia have slightly lower
results than the other classes.
4.4 Compression
We eventually test the applicability of
compression-based classification using the
approach of Bush (2014). As mentioned ear-
lier, the assumption behind compression-based
strategies for text categorization is that different
text categories have a different entropy. Clas-
sification is possible because the effectivity of
compression algorithms depends on the entropy
of the data to be compressed (less entropy ? more
compression).
A simple classification algorithm is Lempel-
Ziv-Welch (LZW) (Welch, 1984). It is based on
a dictionary which maps sequences of symbols to
unique indices. Compression is achieved by re-
placing sequences of input symbols with the re-
spective dictionary indices. More precisely, com-
pression works as follows. First, the dictionary
is initialized with the inventory of symbols (i.e.,
with all possible 1-grams). Then, until the input is
fully consumed, we repeat the following steps. We
search the dictionary for the longest sequence of
symbols s that matches the current input, we out-
put the dictionary entry for s, remove s from the
input and add s followed by the next input symbol
to the dictionary.
For our experiments, we use our own imple-
mentation of LZW. We first build LZW dictionar-
ies by compressing our training sets as described
30
1k 8k 25k 50k
AR 28.42 38.78 46.92 51.89
CO 19.81 28.27 32.81 36.05
MX 22.07 33.90 43.10 45.06
ES 22.08 29.48 35.15 38.61
CL 27.08 28.22 33.59 36.68
Table 7: Results (F
1
): LZW without ties
above, using different limits on dictionary lengths.
As symbol inventory, we use bytes, not unicode
symbols. Then we use these dictionaries to com-
press all tweets from all test sets, skipping the ini-
tialization stage. The country assigned to each
tweet is the one whose dictionary yields the high-
est compression. We run LZW with different max-
imal dictionary sizes.
The problem with the evaluation of the results
is that the compression produced many ties, i.e.,
the compression of a single tweet with dictionaries
from different languages resulted in identical com-
pression rates. On the concatenated dev sets (50k
tweets, i.e., 10k per country) with a maximal dic-
tionary size of 1k, 8k, 25k and 50k entries, we got
14.867, 20,166, 22,031, and 23,652 ties, respec-
tively. In 3,515 (7%), 4,839 (10%), 5,455 (11%)
and 6,102 (12%) cases, respectively, the correct re-
sult was hidden in a tie. If we replace the labels
of all tied instances with a new label TIE, we ob-
tain the F-scores shown in Table 7. While they are
higher than the scores for n-gram frequency pro-
files, they still lie well below the results for both
syllable and character language models.
While previous literature mentions an ideal size
limit on the dictionary of 8k entries (Bush, 2014),
we obtain better results the larger the dictionaries.
Note that already with a dictionary of size 1000,
even without including the ties, we are above the
20.00 F-score of a random baseline. The high
rate of ties constitutes a major problem of this ap-
proach, and remains even if we would find im-
provements to the approach (one possibility could
be to use unicode characters instead of bytes for
dictionary initialization). It cannot easily be alle-
viated, because if the compression rate is taken as
the score, particularly the scores for short tweets
are likely to coincide.
4.5 Voting
Voting is a simple meta-classifying technique
which takes the output of different classifiers and
class precision recall F
1
AR 70.96 68.36 69.64
CO 62.44 64.22 63.32
MX 66.37 65.67 66.02
ES 70.10 69.64 69.87
CL 68.97 70.72 69.83
overall 67.72 67.72 67.72
Table 8: Results: Voting
decides based on a predefined method on one of
them, thereby combining their strengths and level-
ing out their weaknesses. It has been successfully
used to improve language identification on Twitter
data by Lui and Baldwin (2014).
We utilize the character 5-gram and 6-gram lan-
guage models without pruning, as well as the syl-
lable 3-gram and 4-gram models. We decide as
follows. All instances for which the output of the
5-gram model coincides with the output of at least
one of the syllable models are labeled with the out-
put of the 5-gram model. For all other instances,
the output of the 6-gram model is used. The corre-
sponding results for all classes are shown in Table
8.
We obtain a slightly higher F-score than for
the 6-gram character language model (0.8 points).
In other words, even though the 6-gram language
model leads to the highest overall results among
individual models, in some instances it is out-
performed by the lower-order character language
model and by the syllable language models, which
have a lower overall score.
5 Human Tweet Classification
In order to get a better idea of the difficulty of the
task of classifying tweets by the country of their
authors, we have tweets classified by humans.
Generally, speakers of Spanish have limited
contact with speakers of other varieties, simply
due to geographical separation of varieties. We
therefore recur to a simplified version of our task,
in which the test subjects only have to distinguish
their own variety from one other variety, i.e., per-
form a binary classification. We randomly draw
two times 150 tweets from the Argentinian test and
150 tweets from the Chilean and Spanish test sets,
respectively. We then build shuffled concatena-
tions of the first 150 Argentinian and the Chilean
tweets, as well as of the remaining 150 Argen-
tinian and the Spanish tweets. Then we let three
31
data subject class prec. rec. F
1
AR-ES AR AR 68.5 76.7 72.3
ES 73.5 64.7 68.8
ES AR 71.5 62.0 66.4
ES 66.5 75.3 70.6
n-gram AR 92.3 87.3 89.7
ES 88.0 92.7 90.3
AR-CL AR AR 61.0 77.3 68.2
CL 69.1 50.7 58.5
CL AR 70.0 70.0 70.0
CL 70.0 70.0 70.0
n-gram AR 93.4 84.7 88.8
CL 86.0 94.0 89.8
Table 9: Results: Human vs. automatic classifica-
tion
natives classify them. The test subjects are not
given any other training data samples or similar re-
sources before the task, and they are instructed not
to look up on the Internet any information within
the tweet that might reveal the country of its author
(such as hyperlinks, user mentions or hash tags).
Table 9 shows the results, together with the re-
sults on the same task of the character 6-gram
model without pruning. Note that with 300 test
instances out of 20,000, there is a sampling er-
ror of ? 4.7% (confidence interval 95%). The re-
sults confirm our intuition in the light of the good
performance achieved by the n-gram approach in
the 5-class case: when reducing the classification
problem from five classes to two, human classi-
fication performance is much below the perfor-
mance of automatic classification, by between 17
and 31 F-score points. In terms of error rate, the
human annotators made between 3 and 4 times
more classification errors than the automatic sys-
tem. One can observe a tendency among the hu-
man test subjects that more errors come from la-
beling too many tweets as coming from their na-
tive country than vice versa (cf. the recall values).
In order to better understand the large result dif-
ference, we ask the test subjects for the strategies
they used to label tweets. They stated that the eas-
iest tweets where those specifying a location (?Es-
toy en Madrid?), or referencing local named en-
tities (TV programs, public figures, etc.). In case
of absence of such information, other clues were
used that tend to occur in only one variety. They
include the use of different words (such as en-
fadado (Spain) vs. enojado (America) (?angry?)),
data subject class prec. rec. F
1
AR-ES AR AR 71.8 80.0 75.7
ES 74.8 65.4 69.7
ES AR 74.6 62.9 68.2
ES 65.1 76.3 70.2
n-gram AR 93.2 88.6 90.8
ES 88.1 92.9 90.4
AR-CL AR AR 61.1 78.6 68.8
CL 68.8 48.5 56.9
CL AR 73.0 71.4 72.2
CL 71.2 72.8 72.0
n-gram AR 95.3 87.1 91.0
CL 87.8 95.6 91.5
Table 10: Results: Human vs. automatic classifi-
cation (filtered)
a different distribution of the same word (such as
the filler pues), and different inflection, such as the
second person plural verb forms, which in Amer-
ican Spanish, albeit sometimes not in Chile, is re-
placed by the identical third person plural forms
(for the verb hacer (?do?), the peninsular form
would be hac?eis instead of hacen), and the per-
sonal pronoun vos (?you?), which is rarely used
in Chile, and not used in Spain. To sum up, the
test subjects generally relied on lexical cues on
the surface, and were therefore bound to miss non-
obvious information captured by the character n-
gram model.
Since the test subjects also stated that some
tweets were impossible to assign to a country be-
cause they contained only URLs, emoticons, or
similar, in Table 10 we show a reevaluation of a
second version of the two shuffled concatenated
samples in which we remove all tweets which con-
tain only emoticons, URLs, or numbers; tweets
which are entirely written in a language other than
Spanish; and tweets which are only two or one
words long (i.e., tweets with zero or one spaces).
For the AR-ES data, we remove 23 Spanish and
10 Argentinian tweets, while for the AR-CL data,
we remove 10 Argentinian and 14 Chilean tweets.
As for the human classification on the AR/ES
data, the results for Spain do not change much. For
Argentina, there is an increase in performance (2
to 3 points). On the AR/CL data, there is a slight
improvement on all sets except for the Chilean
data classified.
As for the automatic classification, the filter-
ing gives better result on all data sets. However,
32
training dev test
AR 57,546 (71.9%) 7,174 7,196
CO 58,068 (72.6%) 7,249 7,289
MX 48,527 (60.7%) 6,117 6,061
ES 53,199 (66.5%) 6,699 6,657
CL 56,865 (71.1%) 6,998 7,071
Table 11: Data sizes (filtered by langid.py)
the difference between the F
1
of the filtered and
unfiltered data is larger on the AR/CL data set.
This can be explained with the fact that among
the tweets removed from the AR/ES data set, there
were more longer tweets (not written in Spanish)
than among the tweets removed from the CL/AR
data set, the longer tweets being easier to iden-
tify. Note that the filtering of tweets does not cause
much change in the difference between human and
automatic classification.
6 Language Filtering
As mentioned before, our data has not been
cleaned up or normalized. In particular, the data
set contains tweets written in languages other than
Spanish. We have reasoned that those can be seen
as belonging to the ?natural? language production
of a country. However, in order to see what im-
pact they have on our classification results, we
perform an additional experiment on a version of
the data were we only include the tweets that the
state-of-the-art language identifier langid.py
labels Spanish (Lui and Baldwin, 2012).
6
Table
11 shows the sizes of all data sets after filtering.
Note that many of the excluded tweets are in fact
written in Spanish, but are very noisy, due to or-
thography, Twitter hash tags, etc. The next most
frequent labels across all tweets is English (9%).
Note that in the data from Spain, 2% of the tweets
are labeled as Catalan, 1.2% as Galician, and only
0.3% as Basque.
Table 12 finally shows the classification re-
sults for character 6-gram language models with-
out pruning.
The changes in F
1
are minor, i.e., below one
point, except for the Mexican tweets, which lose
around 4 points. The previous experiments have
already indicated that the Mexican data set is the
most heterogeneous one which also resulted in the
largest number of tweets being filtered out. In
general, we see that the character n-gram method
6
https://github.com/saffsd/langid.py.
class precision recall F
1
AR 70.32 66.09 68.14
CO 63.76 62.22 62.98
MX 61.52 61.11 61.31
ES 69.13 69.20 69.17
CL 67.12 73.29 70.07
overall 66.45 66.45 66.45
Table 12: Results: Filtered by langid.py
seems to be relatively stable with respect to a dif-
ferent number of non-Spanish tweets in the data.
More insight could be obtained by performing ex-
periments with advanced methods of tweet nor-
malization, such as those of Han and Baldwin
(2011). We leave this for future work.
7 Discussion
Human classification of language varieties was
judged by our test subjects to be considerably
more difficult that differentiating between lan-
guages. Additionally, the test subjects were only
able to differentiate between two classes. While
the automatic classification results lie below the
results which one would expect for language iden-
tification, n-gram classification still achieves good
performance.
Our experiments touch on the more general
question of how a language variety is defined. In
order to take advantage of the metadata provided
by Twitter, we had to restrict the classification
problem to identifying varieties associated with
countries were tweets were sent. In reality, the
boundaries between variants are often blurred, and
there can also be variance within the same country
(e.g., the Spanish spoken in the southern Spanish
region of Andalusia is different from that of As-
turias, even if they both share features common
to Peninsular Spanish and larger differences with
American Spanish). However, it would be diffi-
cult to obtain a reliable corpus with this kind of
fine-grained distinctions.
It is also worth noting that not all the classifica-
tion criteria used by the human test subjects were
purely linguistic ? for example, a subject could
guess a tweet as being from Chile by recogniz-
ing a mention to a Chilean city, public figure or
TV show. Note that this factor intuitively seems to
benefit humans ? who have a wealth of knowledge
about entities, events and trending topics from
their country ? over the automatic system. In spite
33
of this, automatic classification still vastly outper-
formed human classification, suggesting that the
language models are capturing linguistic patterns
that are not obvious to humans.
8 Conclusion
We have studied different approaches to the task
of classifying tweets from Spanish-speaking coun-
tries according to the country from which they
were sent. To the best of our knowledge, these are
the first results for this problem. On the problem
of assigning one of five classes (Argentina, Mex-
ico, Chile, Colombia, Spain) to 10,000 tweets, the
best performance, an overall F-score of 67.72, was
obtained with a voting meta-classifier approach
that recombines the results for four single clas-
sifiers, the 6-gram (66.96 F
1
) and 5-gram (66.75
F
1
) character-based language models, and the 4-
gram (57.87 F
1
) and 3-gram (57.24 F
1
) syllable-
based language models. For a simplified version
of the problem that only required a decision be-
tween two classes (Argentina vs. Chile and Spain
vs. Argentina), given a sample of 150 tweets from
each class, human classification was outperformed
by automatic classification by up to 31 points.
In future work, we want to investigate the ef-
fect of tweet normalization on our problem, and
furthermore, how the techniques we have used can
be applied to classify text from other social media
sources, such as Facebook.
Acknowledgements
The first author has been funded by Deutsche
Forschungsgemeinschaft (DFG). The second au-
thor has been partially funded by Ministerio
de Econom??a y Competitividad/FEDER (Grant
TIN2010-18552-C03-02) and by Xunta de Galicia
(Grant CN2012/008).
References
Manuel Alvar, editor. 1996a. Manual de dialectolog??a
hisp?anica. El espa?nol de Am?erica. Ariel, Barcelona.
Manuel Alvar, editor. 1996b. Manual de dialectolog??a
hisp?anica. El espa?nol de Espa?na. Ariel, Barcelona.
Timothy Baldwin and Marco Lui. 2010. Language
identification: The long and the short of the mat-
ter. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 229?237, Los Angeles, CA.
Dario Benedetto, Emanuele Caglioti, and Vittorio
Loreto. 2002. Language trees and zipping. Phys-
ical Review Letters, 88(4).
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media, LSM ?12, pages 65?
74, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Ralph D. Brown. 2013. Selecting and weighting n-
grams to identify 1100 languages. In Springer, ed-
itor, Proceedings of the 16th International Confer-
ence on Text, Speech, and Dialogue, volume 8082 of
LNCS, pages 475?483, Pilsen, Czech Republic.
Brian O. Bush. 2014. Language identication of tweets
using LZW compression. In 3rd Pacific Northwest
Regional NLP Workshop: NW-NLP 2014, Redmond,
WA.
Simon Carter, Wouter Weerkamp, and Manos
Tsagkias. 2013. Microblog language identification:
overcoming the limitations of short, unedited and id-
iomatic text. Language Resources and Evaluation,
47(1):195?215.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In In Proceedings
of SDAIR-94, 3rd Annual Symposium on Document
Analysis and Information Retrieval, pages 161?175,
Las Vegas, NV.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: Overcoming sparse and noisy data. In Pro-
ceedings of the International Conference on Recent
Advances in Natural Language Processing, pages
198?206. Association for Computational Linguis-
tics.
Ted Dunning. 1994. Statistical identification of lan-
guage. Technical Report MCCS-94-273, Comput-
ing Research Lab, New Mexico State University.
Binod Gyawali, Gabriela Ramirez, and Thamar
Solorio. 2013. Native language identification: a
simple n-gram based approach. In Proceedings of
the Eighth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 224?231,
Atlanta, Georgia, June. Association for Computa-
tional Linguistics.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twitter.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 368?378, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Zen?on Hern?andez-Figeroa, Gustavo Rodr??guez-
Rodr??guez, and Francisco J. Carreras-
Riudavets. 2012. Separador de s??labas
34
del espa?nol - silabeador TIP. Available at
http://tip.dis.ulpgc.es.
Zen?on Hern?andez-Figueroa, Francisco J. Carreras-
Riudavets, and Gustavo Rodr??guez-Rodr??guez.
2013. Automatic syllabification for Spanish
using lemmatization and derivation to solve the
prefix?s prominence issue. Expert Syst. Appl.,
40(17):7122?7131.
Vlado Ke?selj, Fuchun Peng, Nick Cercone, and Calvin
Thomas. 2003. N-gram-based author profiles for
authorship attribution. In Proceedings of PACLING,
pages 255?264.
M. Paul Lewis, Gary F. Simons, and Charles D. Fen-
nig, editors. 2014. Ethnologue: Languages of
the World. SIL International, Dallas, Texas, sev-
enteenth edition edition. Online version: http:
//www.ethnologue.com.
John M. Lipski. 1994. Latin American Spanish. Long-
man, London.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the ACL 2012 System Demonstrations,
pages 25?30, Jeju Island, Korea, July. Association
for Computational Linguistics.
Marco Lui and Timothy Baldwin. 2014. Accurate
language identification of twitter messages. In Pro-
ceedings of the 5th Workshop on Language Analysis
for Social Media (LASM), pages 17?25, Gothenburg,
Sweden.
Marco Lui and Paul Cook. 2013. Classifying english
documents by national dialect. In Proceedings of
the Australasian Language Technology Association
Workshop 2013 (ALTA 2013), pages 5?15, Brisbane,
Australia, December.
Douglas W. Muir and Timothy R. Thomas. 2000. Au-
tomatic language identification by stroke geometry
analysis, May 16. US Patent 6,064,767.
Miguel
?
Angel Quesada Pacheco. 2002. El Espa?nol
de Am?erica. Editorial Tecnol
?ogica de Costa Rica,
Cartago, 2a edition.
Vesa Siivola, Teemu Hirsim?aki, and Sami Virpi-
oja. 2007. On growing and pruning kneser-
ney smoothed n-gram models. IEEE Transac-
tions on Speech, Audio and Language Processing,
15(5):1617?1624.
William J. Teahan. 2000. Text classification and seg-
mentation using minimum cross-entropy. In Pro-
ceedings of RIAO?00, pages 943?961.
Tommi Vatanen, Jaakko J. Vyrynen, and Sami Virpi-
oja. 2010. Language identification of short text
segments with n-gram models. In Proceedings
of the Seventh International Conference on Lan-
guage Resources and Evaluation (LREC?10), Val-
letta, Malta. European Language Resources Associ-
ation (ELRA).
David Vilares, Miguel A. Alonso, and Carlos G?omez-
Rodr??guez. 2013. Supervised polarity classification
of spanish tweets based on linguistic knowledge. In
Proceedings of 13th ACM Symposium on Document
Engineering (DocEng 2013), pages 169?172, Flo-
rence, Italy.
David Vilares, Miguel A. Alonso, and Carlos G?omez-
Rodr??guez. 2014. A syntactic approach for opinion
mining on Spanish reviews. Natural Language En-
gineering, FirstView:1?25, 6.
Radim
?
Reh?u?rek and Milan Kolkus. 2009. Language
identification on the web: Extending the dictionary
method. In Proceedings of CICLing, pages 357?
368.
Terry A. Welch. 1984. A technique for high-
performance data compression. Computer, 17(6):8?
19, June.
Marcos Zampieri, Binyam Gebrekidan Gebre, and
Sascha Diwersy. 2013. N-gram language models
and pos distribution for the identification of spanish
varieties. In Proceedings of TALN2013, pages 580?
587.
Marcos Zampieri, Liling Tan, Nikola Ljube?si?c, and
J?org Tiedemann. 2014. A report on the dsl shared
task 2014. In Proceedings of the First Workshop
on Applying NLP Tools to Similar Languages, Va-
rieties and Dialects, pages 58?67, Dublin, Ireland,
August. Association for Computational Linguistics
and Dublin City University.
35
First Joint Workshop on Statistical Parsing of Morphologically Rich Languages
and Syntactic Analysis of Non-Canonical Languages, pages 1?14 Dublin, Ireland, August 23-29 2014.
Parsing German: How Much Morphology Do We Need?
Wolfgang Maier
Heinrich-Heine-Universita?t Du?sseldorf
Du?sseldorf, Germany
maierw@hhu.de
Sandra Ku?bler
Indiana University
Bloomington, IN, USA
skuebler@indiana.edu
Daniel Dakota
Indiana University
Bloomington, IN, USA
ddakota@indiana.edu
Daniel Whyatt
Indiana University
Bloomington, IN, USA
dwhyatt@indiana.edu
Abstract
We investigate how the granularity of POS tags influences POS tagging, and furthermore, how
POS tagging performance relates to parsing results. For this, we use the standard ?pipeline?
approach, in which a parser builds its output on previously tagged input. The experiments are
performed on two German treebanks, using three POS tagsets of different granularity, and six
different POS taggers, together with the Berkeley parser. Our findings show that less granularity
of the POS tagset leads to better tagging results. However, both too coarse-grained and too
fine-grained distinctions on POS level decrease parsing performance.
1 Introduction
German is a non-configurational language with a moderately free word order in combination with a case
system. The case of a noun phrase complement generally is a direct indicator of the phrase?s grammatical
function. For this reason, a morphological analysis seems to be a prerequisite for a syntactic analysis.
However, in computational linguistics, parsing was developed for English without the use of morpho-
logical information, and this same architecture is used for other languages, including German (Ku?bler et
al., 2006; Petrov and Klein, 2008). An easy way of introducing morphological information into parsing,
without modifying the architecture, is to attach morphology to the part-of-speech (POS) tagset. However,
this makes POS tagging more complex and thus more difficult.
In this paper, we investigate the following questions: 1) How well do the different POS taggers work
with tagsets of a varying level of morphological granularity? 2) Do the differences in POS tagger per-
formance translate into similar differences in parsing quality? Complementary POS tagging results and
preliminary parsing results have been published in German in Ku?bler and Maier (2013).
Our experiments are based on two different treebanks for German, TiGer (Brants et al., 2002) and
Tu?Ba-D/Z (Telljohann et al., 2012). Both treebanks are based on the same POS tagset, the Stuttgart-
Tu?bingen Tagset (STTS) (Schiller et al., 1995). We perform experiments with three variants of the tagset:
The standard STTS, the Universal Tagset (UTS) (Petrov et al., 2012) (a language-independent tagset),
and an extended version of the STTS that also includes morphological information from the treebanks
(STTSmorph). STTS consists of 54 tags, UTS uses 12 basic tags, and the morphological variants of the
STTS comprise 783 and 524 POS tags respectively. We use a wide range of POS taggers, which are
based on different strategies: Morfette (Chrupala et al., 2008) and RF-Tagger (Schmid and Laws, 2008)
are designed for large morphological tagsets, the Stanford tagger (Toutanova et al., 2003) is based on a
maximum entropy model, SVMTool (Gime?nez and Ma`rquez, 2004) is based on support vector machines,
TnT (Brants, 2000) is a Markov model trigram tagger, and Wapiti (Lavergne et al., 2010) a conditional
random field tagger. For our parsing experiments, we use the Berkeley parser (Petrov and Klein, 2007b;
Petrov and Klein, 2007a).
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Our findings for POS tagging show that Morfette reaches the highest accuracy on UTS and overall on
unknown words while TnT reaches the best performance for STTS and the RF-Tagger for STTSmorph.
These trends are stable across both treebanks. As for the parsing results, using STTS results in the best
accuracies. For TiGer, POS tags assigned by the parser perform better in combination with UTS and
STTSmorph. For TiGer in combination with STTS and all variants in Tu?Ba-D/Z, there are only minor
differences between the parser assigned POS tags and those by TnT.
The remainder of the article is structured as follows. In section 2, we review previous work. Section
3 presents the different POS tagsets. Section 4 describes our experimental setup. The POS tagging and
parsing results are discussed in the sections 5 and 6, respectively. Section 7 concludes the article.
2 Previous Work
In this section, we present a review of the literature that has previously examined the correlation of
POS tagging and parsing under different aspects. While this overview is not exhaustive, it presents the
major findings related to our work. The issues examined can be regarded under two orthogonal aspects,
namely, the parsing model used (data-driven or grammar-based), and the question of how to disambiguate
between various tags for a single word.
Some work has been done on investigating different tagsets for individual languages. Collins et al.
(1999) adapt the parser of Collins (1999) for the Czech Prague Dependency Treebank. Using an external
lexicon to reduce data sparseness for word forms did not result in any improvement, but adding case to the
POS tagset had a positive effect. Seddah et al. (2009) investigate the use of different parsers on French.
They also investigate two tagsets with different granularity and come to the conclusion that the finer
grained tagset leads to higher parser performance. The work that is closest to ours is work by Marton et
al. (2013), who investigate the optimal POS tagset for parsing Arabic. They come to the conclusion that
adding definiteness, person, number, gender, and lemma information to the POS tagset improve parsing
accuracy. Both Dehdari et al. (2011) and Sza?nto? and Farkas (2014) investigate automatic methods for
selecting the best subset of morphological features, the former for Arabic, the latter for Basque, French,
German, Hebrew, and Hungarian. However, note that Sza?nto? and Farkas (2014) used the data from the
SPMRL shared task 2013, which does not contain grammatical functions in the syntactic annotations.
Both approaches found improvements for subsets of morphological features.
Other works examine, also within a ?pipeline? method, possibilities for ambiguity reduction through
modification of tagsets, or of the lexicon by tagset reduction, or through word-clustering. Lakeland
(2005) uses lexicalized parsing a` la Collins (1999). Similarly to the more recent work by Koo et al.
(2008) or Candito and Seddah (2010), he addresses the question of how to optimally disambiguate for
parsing on the lexical level by clustering. A word cluster is thereby seen as an equivalence class of
words and assumes to a certain extent the function of a POS tag, but can be adapted to the training
data. Le Roux et al. (2012) address the issue of data sparseness on the lexical level with PCFG parsing
with the morphologically rich language Spanish. The authors use a reimplementation of the Berkeley
parser. They show that parsing results can be improved by simplifying the POS tagset, as well as by
lemmatization, since both approaches reduce data sparseness.
As already mentioned, a POS tag can be seen as an equivalence class of words. Since in the ?pipeline?
approach, the parse tree is built on POS tags, it is possible that a POS tagset is optimal from a linguistic
point of view, but that its behavior is not optimal with respect to parsing results, because relevant lexical
information is hidden from the parse tree by the POS tagset. While Koo et al. (2008) overcome this
deficit by automatically searching for ?better? clusters, other works copy certain lexical information into
the actual tree, e.g., by using grammatical function annotation (Versley, 2005; Versley and Rehbein,
2009). Seeker and Kuhn (2013) complement the ?pipeline? model (using a dependency parser (Bohnet,
2010)) by an additional component that uses case information as a filter for the parser. They achieve
improvements for Hungarian, German and Czech.
A number of works develop models for simultaneous POS tagging or morphological segmentation
and parsing. Based on work by Ratnaparkhi (1996) and Toutanova and Manning (2000), Chen and Kit
(2011) investigate disambiguation on the lexical level. They assume that local, i.e., sequential but not
2
tag description tag description tag description
NOUN noun PRON pronoun CONJ conjunction
VERB verb DET determiner, article PRT particle
ADJ adjective ADP preposition, postposition . punctuation
ADV adverb NUM numeral X everything else
Table 1: The 12 tags of the Universal Tagset.
hierarchical, features are decisive for the quality of POS tagging and note that a ?pipeline? model does not
take this into account since the parser effectively performs the POS disambiguation. On these grounds,
they present a factorized model for PCFG parsing which separates parsing into a discriminative lexical
model (with local features) and the actual parsing model, to be combined with a product-of-experts
(Hinton, 1999).
Particularly in the dependency parsing literature, combined models for simultaneous POS tagging and
parsing can be found. Research has concentrated on languages that require additional segmentation on
the word level, such as Chinese (Hatori et al., 2011) or Hebrew (Goldberg and Tsarfaty, 2008). A new
approach by Bohnet and Nivre (2012) was also evaluated on German. Results for POS tagging and
parsing of German by means of a constraint grammar can be found in Daum et al. (2003) as well as
in Foth et al. (2005). However, since these approaches are only marginally related to our approach, we
forego a further overview.
3 The Three Tagset Variants
In our experiments, we use three POS tagset variants: The standard Stuttgart-Tu?bingen Tagset (STTS),
the Universal Tagset (UTS) (Petrov et al., 2012), and an extended version of the STTS that also includes
morphological information from the treebanks (STTSmorph). Since the two treebanks differ in their
morphological annotation, in this variant, the tags differ between the two treebanks: For TiGer, we have
783 possible complex POS tags, and for Tu?Ba-D/Z, there are 524. By complex tags, we mean a combi-
nation of an STTS tag with the morphological tag. Also, note that not all of the possible combinations
are attested in the treebanks.
The UTS consists of 12 basic POS tags, shown in table 11. It was developed for multilingual appli-
cations, in which a common tagset is of importance, such as for a multilingual POS tagger. The UTS
only represents the major word classes. Thus, this tagset should result in a high POS tagging accuracy
since only major distinctions are made. However, it is unclear whether these coarse distinctions provide
enough information for a syntactic analysis.
The STTS is based on distributional regularities of German. It contains 54 tags and thus models more
fine grained distinctions than the UTS. For a list of tags, see Schiller et al. (1995). The finer distinctions
in STTS mostly concern word classes, but there is also a distinction between finite and infinite verbs.
This distinction is important for the syntactic analysis, especially in Tu?Ba-D/Z, but it can be difficult to
make by a POS tagger with a limited context.
The STTS can be extended by a morphological component. Both treebanks provide a morphological
analysis, but the analyses model different decisions. In TiGer, a set of 585 different feature combinations
is used, which can be combined from the features listed in table 2. The sentence in (1) gives an example
of the combination of the STTS and morphology, which are separated by the % sign. The feature ?
means that there are no morphological features for the given POS tag.
(1) Konzernchefs
NN%Nom.Pl.Masc
lehnen
VVFIN%3.Pl.Pres.Ind
den
ART%Acc.Sg.Masc
Milliarda?r
NN%Acc.Sg.Masc
als
APPR%?
US-Pra?sidenten
NN%Acc.Sg.Masc
ab
PTKVZ%?
/
$(%?
?Corporate CEOs disapprove of the billionaire as US president /?
1For a mapping from STTS to UTS, cf. https://code.google.com/p/universal-pos-tags/.
3
feature description
ambiguous: *
gender masculine (Masc), feminine (Fem), neuter (Neut)
gradation positive (Pos), comparative (Comp), superlative (Sup)
case nominative (Nom), genitive (Gen), dative (Dat), accusative (Akk)
mode indicative (Ind), conjunctive (Subj), imperative (Imp)
number singular (Sg), plural (Pl)
person 1., 2., 3.
tense present (Pres), past (Past)
Table 2: The morphological categories in TiGer.
feature description
ambiguous *
gender masculine (m), feminine (f), neuter (n)
case nominative (n), genitive (g), dative (d), accusative (a)
number singular (s), plural (p)
person 1., 2., 3.
tense present (s), past (t)
mode indicative (i), conjunctive (k)
Table 3: The morphological categories in Tu?Ba-D/Z.
Out of the 585 possible combinations of morphological features, 271 are attested in TiGer. In combi-
nation with the STTS, this results in 783 combinations of STTS and morphological tags. Out of those,
761 occur in the training set. However, we expect data sparseness during testing because of the high
number of possible tags. For this reason, we calculated which percentage of the tags in the development
and test set are known combinations. We found that 25% and 30%, respectively, do not occur in the train-
ing set. However, note that the number of tags in the development and test sets is considerably smaller
than the number of tags in the training set.
In Tu?Ba-D/Z, there are 132 possible morphological feature combinations which can be combined from
the features listed in table 3. The sentence in (2) gives an example of the combination of the STTS and
morphology.
(2) Aber
KON%?
Bremerhavens
NE%gsn
AfB
NE%nsf
fordert
VVFIN%3sis
jetzt
ADV%?
Untersuchungsausschu?
NN%asm
?But the Bremerhaven AfB now demands a board of inquiry?
Out of the 132 possible feature combinations, 105 are attested in Tu?Ba-D/Z. In combination with the
STTS, this results in 524 combinations of STTS and morphological tags. Out of those, 513 occur in the
training set. For the development and test set, we found that 16% and 18% respectively do not occur in
the training set. These percentages are considerably lower than the ones for TiGer.
Since the tagsets that include morphology comprise several hundred different POS tags, we expect
tagging to be more difficult, resulting in lower accuracies. We also expect that the Tu?Ba-D/Z tagset
is better suited for POS tagging than the TiGer set because of its smaller tagset size and its higher
coverage on the development and test set. It is, however, unknown whether this information can be used
successfully in parsing.
4
dieses
PDAT
Acc.Sg.Neut
Buch
NN
Acc.Sg.Neut
finden
VVFIN
3.Pl.Pres.Ind
vor
APPR
allem
PIS
Dat.Sg.Neut
diejenigen
PDS
Nom.Pl.*
schwierig
ADJD
Pos
,
$,
die
PRELS
Nom.Pl.*
am
PTKA
meisten
PIS
*.*.*
Bildung
NN
Acc.Sg.Fem
haben
VAFIN
3.Pl.Pres.Ind
,
$,
vor
APPR
allem
PIS
Dat.Sg.Neut
psychoanalytische
ADJA
Pos.Acc.Sg.Fem
Bildung
NN
Acc.Sg.Fem
(
$(
...
$(
)
$(
NK NK
NP
AC NK
PP
PM HD
AA
AC NK
PP
MO NK NK
NP
NK NK APP
NP
SB OAHD
S
MO NK RC
NP
OA HD SBMO
S
VROOT
Figure 1: A sentence from TiGer.
4 Experimental Setup
4.1 Treebanks
We use the treebanks TiGer (Brants et al., 2002), version 2.2, and Tu?Ba-D/Z (Telljohann et al., 2012),
release 8. Both are built on newspaper text, Frankfurter Rundschau for TiGer and taz for Tu?Ba-D/Z.
Both treebanks use the same POS tagset with only one minor difference in the naming of one POS label.
However, the treebanks differ considerably in the syntactic annotation scheme. While TiGer uses a very
flat annotation involving crossing branches, the annotations in Tu?Ba-D/Z are more hierarchical, and long
distance relations are modeled via grammatical function labels rather than via attachment. Figures 1 and
2 show examples.
For preprocessing, we follow the standard practices from the parsing community. In both treebanks,
punctuation and other material, such as parentheses, are not included in the annotation, but attached to a
virtual root node. We attach the respective nodes to the tree using the algorithm described by Maier et
al. (2012) so that every sentence corresponds to exactly one tree. In a nutshell, this algorithm uses the
left and right terminal neighbors as attachment targets. In TiGer, we then remove the crossing branches
using a two-stage process. In a first step, we apply the transformation described by Boyd (2007). This
transformation introduces a new non-terminal for every continuous block of a discontinuous constituent.
We keep a flag on each of the newly introduced nodes that indicates if it dominates the head daughter of
the original discontinuous node. Subsequently, we delete all those nodes for which this flag is false.2
For both POS tagging and parsing, we use the same split for training, development, and test. We use
the first half of the last 10 000 sentences in TiGer for development and the second half for testing. The
remaining 40 472 sentences are used for training. Accordingly, in order to ensure equal conditions, we
use the first 40 472 sentences in Tu?Ba-D/Z for training, and the first and second half of the following
10 000 sentences for development and testing. The remaining sentences in Tu?Ba-D/Z are not used.
4.2 POS Taggers
We employ six different POS tagger, each of them using a different tagging technique. Morfette (Chru-
pala et al., 2008), in its current implementation based on averaged Perceptron, is a tool designed for the
annotation of large morphological tagsets. Since none of the other POS taggers have access to lemmas,
we only provide full word forms to Morfette as well, which may inhibit its generalization capability.
The RF-Tagger (Schmid and Laws, 2008) assumes a tagset in a factorized version. I.e., the POS tag
VVFIN%3sis in sentence (2) would be represented as VVFIN.3.s.i.s, where the dots indicate different
subcategories, which are then treated separately by the POS tagger. It is based on a Markov model, but
the context size is determined by a decision tree. The Stanford tagger (Toutanova et al., 2003) is based
on a maximum entropy model, and SVMTool (Gime?nez and Ma`rquez, 2004) is based on support vector
machines. TnT (Brants, 2000; Brants, 1998), short for trigrams and tags, is a Markov model POS tagger.
2An implementation of all transformations is available at http://github.com/wmaier/treetools.
5
Beides
PIS
nsn
sind
VAFIN
3pis
Liedformen
NN
npf
,
$,
die
PRELS
np*
am
APPRART
dsn
Ende
NN
dsn
des
ART
gsn
achtzehnten
ADJA
gsn
Jahrhunderts
NN
gsn
die
ART
apn
ersten
ADJA
apn
Anzeichen
NN
apn
eines
ART
gsm
Verschmelzungsprozesses
NN
gsm
zeigen
VVFIN
3pis
.
$.
HD
NX
HD
VXFIN
HD
NX
HD
NX
HD
NX
HD
ADJX
HD
ADJX
- HD
NX
HD
VXFIN
PRED
VF
HD
LK
ON
MF
ON
C
- - HD
NX
- - HD
NX
HD
VC
HD -
NX
HD -
NX
- HD
PX
V-MOD OA
MF
- - -
R-SIMPX
ON-MO|
NF
- - - -
SIMPX
VROOT
Figure 2: A sentence from Tu?Ba-D/Z.
It uses an interpolation between uni-, bi- and trigrams as probability model. TnT has a sophisticated
mechanism for tagging unknown words. We also use Wapiti (Lavergne et al., 2010) a conditional ran-
dom field tagger. Since conditional random fields were developed for sequence tagging, this POS tagger
is expected to perform well.
All POS taggers are used with default settings. For the Stanford tagger, we use the bi-directional model
based on a context of 5 words. For SVMTool, we use the processing from left to right in combination
with features based on word and POS trigrams and word length, prefix and suffix information. Wapiti is
trained on uni-, bi-, and trigrams. Features used in training consist of tests concerning the alphanumeric,
upper or lower case characteristics, prefixes and suffixes of length three, and all possible POS tags for a
word.
For POS tagging evaluation, we use the script provided by TnT since it also allows us to calculate
accuracy on known and unknown words.
4.3 Parser
We use the Berkeley parser (Petrov and Klein, 2007b; Petrov and Klein, 2007a). We chose the Berke-
ley parser because we are aware of the fact that there are considerable differences in the tagset sizes,
which a plain PCFG parser cannot process successfully. The Berkeley parser split/merge capabilities
provide a way of smoothing over these differences. For parser evaluation, we use our own implementa-
tion of the PARSEVAL metrics.3 We report labeled precision (LP), labeled recall (LR), and the labeled
F-score(LF1). Note that the labeled evaluation does not only look at constituent labels but also at gram-
matical functions attached to the constituents, e.g. NP-SBJ for a subject NP. This is a considerably more
difficult task for German because of the relatively free word order. We also provide POS tagging ac-
curacy in the parse trees since the Berkeley parser adapts POS tags from the input if they do not fit its
syntax model.
5 POS Tagging Results
5.1 The Three Tagset Variants
The results for the POS tagging evaluation are shown in table 4. We are aware of the fact that the results
are not directly comparable across the different POS tagsets and across different treebanks since the
3The implementation is available at http://github.com/wmaier/evalb-lcfrs. Note that we evaluate the trees
as they are, i.e., we do not collapse or ignore tags.
6
TiGer Tu?Ba-D/Z
Tagset Tagger dev test dev test
UTS Morfette 98.51 98.09 98.25 98.49
RF-Tagger 97.89 97.41 97.69 97.96
Stanford 97.88 96.83 97.11 97.26
SVMTool 98.54 98.01 98.09 98.28
TnT 97.94 97.48 97.72 97.92
Wapiti 97.54 96.67 97.47 97.80
STTS Morfette 94.12 93.23 92.95 93.41
RF-Tagger 97.04 96.24 96.68 96.84
RF-Tagger (fact.) 97.05 96.26 96.69 96.85
Stanford 96.26 95.15 95.63 95.79
SVMTool 97.06 96.22 96.46 96.69
TnT 97.15 96.29 96.92 97.00
Wapiti 92.93 91.62 90.99 91.81
STTSmorph Morfette 82.71 80.10 81.19 82.26
RF-Tagger 86.56 83.90 85.68 86.31
Stanford ? ? ? ?
SVMTool 82.47 79.53 80.33 81.31
TnT 85.77 82.77 84.67 85.45
Wapiti 79.83 75.92 77.27 78.29
STTSmorph? STTS TnT 97.08 96.15 96.78 96.82
Table 4: POS tagging results using three versions of the German POS tagset and two treebanks.
corresponding tagging tasks differ in the level of difficulty. Any interpretation must therefore be taken
with a grain of salt, but we think that it is important to evaluate POS tagging on its own, especially
since it is not always the case that a larger label set automatically results in a more difficult task. The
results show that UTS, i.e., the variant with the least information, results in the highest POS tagging
results, between 96.67% and 98.54%. In tagging with the STTS, we reach a lower accuracy between
90.99% and 97.15%. When we include the morphological information, we reach considerably lower
results, between 75.92% and 86.56%. In other words, this shows that the more information there is in
the POS tagset, the harder the POS tagging task is. POS tagging with morphological information is the
most difficult task. We also see that there are no results for the Stanford POS tagger in the morphological
setting. We were unable to run these experiments, even when we used a high-memory cluster with access
to 120g of memory. It seems that the Stanford tagger is incapable of handling the large tagset sizes in the
setting using morphological information. Additionally, our assumption that the morphological tagset of
Tu?Ba-D/Z is less difficult to annotate because of its smaller tagset size is not borne out. The variation of
results on Tu?Ba-D/Z is often less than between the treebanks, across POS taggers.
If we compare the result of the different POS taggers, we see that for the different tagset variants,
different POS taggers perform best: For UTS, surprisingly, Morfette reaches the highest results, with
the exception of the TiGer development set, for which SVMTool performs slightly better. In general,
SVMTool is very close in accuracy to Morfette for this tagset variant. For STTS, TnT outperforms
all other POS taggers, and SVMTool is a close second. For STTSmorph, the RF-Tagger reaches the
highest results. For the RF-Tagger in combination with the STTS, we performed 2 experiments, one
using the standard STTS and one in which the STTS tags are factored, such that VVFIN is factored
into V.V.FIN. The latter variant reaches minimally higher results. In all settings, Wapiti is the weakest
approach; the difference between Wapiti and the best performing POS tagger reaches 6-7 percent points
for STTSmorph. This is rather surprising given that POS tagging is a typical sequence tagging task, for
which CRFs were developed.
Another fact worth mentioning is that there are considerable differences in POS tagging accuracy
7
TiGer Tu?Ba-D/Z
dev test dev test
Tagset Tagger Known Unkn. Known Unkn. Known Unkn. Known Unkn.
UTS Morfette 98.66 96.74 98.32 96.04 98.54 95.46 98.69 96.39
RF-Tagger 98.15 94.64 97.82 93.65 98.28 92.02 98.35 93.85
Stanford 99.05 91.85 98.78 87.70 98.94 79.30 98.92 79.69
SVMTool 98.81 95.26 98.41 94.45 98.63 92.89 98.66 94.27
TnT 98.06 96.50 97.67 95.74 98.07 94.28 98.25 95.25
Wapiti 98.94 80.71 98.51 80.04 98.68 85.79 98.83 86.91
STTS Morfette 94.42 90.60 93.56 90.24 93.17 90.83 93.59 91.57
RF-Tagger 97.80 87.92 97.30 86.71 97.62 87.59 97.73 87.52
RF-T. (fact.) 97.78 88.21 97.28 87.09 97.63 87.65 97.73 87.51
Stanford 98.16 73.56 97.75 71.60 97.96 73.04 97.97 72.64
SVMTool 97.86 87.41 97.26 86.82 97.50 86.47 97.60 87.05
TnT 97.80 89.25 97.21 87.95 97.65 89.78 97.72 89.33
Wapiti 94.51 73.78 93.48 74.83 93.21 69.45 93.71 71.74
STTSmorph Morfette 84.30 63.50 82.43 58.98 82.91 64.53 83.95 64.42
RF-Tagger 88.34 65.09 86.38 61.47 87.70 66.20 88.25 65.80
SVMTool 84.67 55.89 82.40 53.58 82.87 55.81 83.61 57.01
TnT 87.62 63.41 85.55 57.65 86.91 62.95 87.61 62.55
Wapiti 83.91 30.51 81.43 26.08 82.05 31.05 82.83 30.29
Table 5: Results for the different POS taggers for known and unknown words.
between the development and test set in both treebanks. For both STTS variants, these differences are
often larger than the differences between individual POS taggers on the same data set. Thus, in the
STTSmorph setting, the difference for TnT between the development and test set in TiGer is 3 percent
points while the differences between TnT and SVMTool and Morfette respectively are less.
One last question that we investigated concerns the effect of the morphological information on POS
tagging accuracy. We know that when we use morphological information, the POS tagging task is more
difficult. However, it is possible that the mistakes that occur concern only the morphological information
while the POS tags minus morphology may be predicted with equal or even higher accuracy. In order to
investigate this problem, we used the STTSmorph output of TnT and deleted all the morphological infor-
mation, thus leaving only the STTS POS tags. We then evaluated these POS tags against the gold STTS
tags. The results are shown in the last row in table 4, marked as STTSmorph? STTS. A comparison of
these results with the TnT results for STTS shows that the POS tagger reaches a higher accuracy when
trained directly on STTS rather than on STTSmorph, with a subsequent deletion of the morphological
information. This means that the morphological information is not useful but rather harmful in POS
tagging.
5.2 Evaluating on Known and Unknown Words
In a next set of experiments, we investigate how the different POS taggers perform on known and un-
known words. We define all words from the development and test set as known if the appear in the
training set. If they do not, they are considered unknown words. Note, however, that even if a word is
known, we still may not have the full set of POS tags in its ambiguity set. This is especially relevant for
the larger tagsets where the ambiguity rate per word is higher.
In TiGer, 7.64% of the words in the development set are unknown, 9.96% in the test set. In Tu?Ba-D/Z,
9.36% of the words in the development set are unknown, 8.64% in the test set. Note that this corresponds
to the levels of accuracy in table 4.
The results of the evaluation on known and unknown words are shown in table 5. These results show
that the Stanford POS tagger produces the highest accuracies for known words for UTS and STTS (note
8
TiGer Tu?Ba-D/Z
Morphology dev test dev test
STTS 97.15 96.29 96.92 97.00
STTSmorph 85.77 82.77 84.67 85.45
agreement 86.04 83.08 84.96 85.77
case 88.10 86.47 87.48 87.91
number 95.60 94.19 95.24 95.41
number + person 95.55 94.11 95.18 95.24
verbal features 97.03 96.02 96.55 96.44
Table 6: The results for TnT with different morphological variants.
that it could not be used for STTSmorph). For unknown words, Morfette reaches the highest results for
UTS and STTS, with TnT reaching the second highest results. For STTSmorph, the RF-Tagger reaches
the highest accuracy on both known and unknown words. The results for the RF-Tagger for STTS show
that the factored version performs better on unknown words than the standard one. It is also noticeable
that Wapiti, the CRF POS tagger, has the lowest performance on unknown words: For UTS, the results
are 10-16 percent points lower that the ones by Morfette; for STTS, the difference reaches 16-23 percent
points, and for STTSmorph, about 35 percent points. This shows that in order to reach a reasonable
accuracy rate, Wapiti?s unknown word handling model via regular expressions must be extended further.
However, note that Wapiti?s results on known words are also lower than the best performing system?s,
thus showing that CRFs are less well suited for POS tagging than originally expected.
5.3 Evaluating Morphological Variants
In this set of experiments, we investigate whether there are subsets of STTSmorph that are relevant for
parsing and that would allow us to reach higher POS tagging and parsing accuracies than on the full set
of morphological features. The subsets were chosen manually to model our intuition on which features
may be relevant for parsing. We investigate the following subsets: all agreement features, case only,
number only, number + person, and only verbal features. In this set of experiments, we concentrate on
TnT because it has been shown to be the most robust across the different settings. The results of these
experiments are shown in table 6. For comparison, we also list the results for the original STTS and
STTSmorph settings from table 4.
The results show that there are morphological subsets that allow reliable POS accuracies: If we use
verbal features, we reach results that are only slightly below the STTS results. For the subset using
number + person features, the difference is around 2 percent points. However, all subsets perform worse
than the STTS. The subsets that include case or all agreement features, which are the subsets most
relevant for parsing, reach accuracies that are slightly above STTSmorph, but still more than 10 percent
points below the original STTS.
6 Parsing Results
In this section, we report parsing results for TiGer in table 7 and for Tu?Ba-D/Z in table 8. We again
use the three POS tag variants as input, and we report results for 1) gold POS tags, 2) for tags assigned
by TnT, which proved to be the most reliable POS tagger across different settings, and 3) for POS tags
assigned by the Berkeley parser. Since the parser is known to alter POS tags given as input if they do not
fit the syntax model, we also report POS tagging accuracy. Note that this behavior of the parser explains
why we do not necessarily have a 100% POS tagging accuracy in the gold POS tag setting.
A first glance at the POS tagging results in the gold POS setting in tables 7 and 8 shows that for UTS
and STTS, the decrease in accuracy is minimal. In other words, the parser only changes a few POS tags.
When we compared the differences in POS tags between the output of the parser and the gold standard,
we found that most changes constitute a retagging of common nouns (NN) as proper nouns (NE). In
the STTSmorph setting, POS tagging accuracy is considerably lower, showing that the parser changed
9
dev test
Tag source Tagset POS LP LR LF1 POS LP LR LF1
gold UTS 100.00 77.97 77.23 77.60 99.97 71.80 70.26 71.02
STTS 99.98 78.09 77.55 77.82 99.97 71.90 71.11 71.50
STTSmorph 91.67 74.72 75.21 74.97 88.70 67.68 67.99 67.83
parser UTS 98.55 77.75 76.84 77.29 97.83 71.13 69.50 70.30
STTS 97.25 78.03 77.19 77.60 96.18 71.16 69.84 70.49
STTSmorph 83.06 75.53 75.24 75.39 79.05 67.67 67.02 67.34
TnT UTS 96.56 74.16 73.28 73.72 96.01 68.37 66.78 67.57
STTS 97.26 78.03 77.19 77.60 96.19 71.16 69.84 70.49
STTSmorph 77.94 73.06 72.69 72.88 75.05 65.43 64.78 65.10
Table 7: Parsing results for TiGer.
dev test
Tag source Tagset POS LP LR LF1 POS LP LR LF1
gold UTS 99.98 81.39 81.12 81.26 99.98 82.24 81.94 82.09
STTS 100.00 83.60 83.58 83.59 99.99 84.54 84.46 84.50
STTSmorph 89.75 82.27 78.85 80.53 90.55 83.57 79.91 81.70
parser UTS 98.35 79.97 79.61 79.79 98.58 81.07 80.66 80.87
STTS 97.20 81.84 81.65 81.74 97.39 82.93 82.78 82.85
STTSmorph 81.03 80.85 77.22 78.99 81.68 81.89 78.20 80.00
TnT UTS 98.35 79.97 79.61 79.79 98.58 81.07 80.66 80.87
STTS 97.21 81.84 81.65 81.74 97.39 82.93 82.78 82.85
STTSmorph 81.03 80.85 77.22 78.99 81.68 81.89 78.20 80.00
Table 8: Parsing results for Tu?Ba-D/Z.
between 8% (UTS) and 25% (STTSmorph) of the POS tags. This is a clear indication that the parser
suffers from data sparseness and has to adapt the POS tags in order to be able to parse the sentences.
We need to compare the POS tagging results based on automatically assigned POS tags; they show
the following trends: For TiGer in the STTS setting, the results based on TnT and on the parser are
very similar. For UTS and STTSmorph, the POS tags assigned by the parser reach a higher accuracy.
For Tu?Ba-D/Z, all the results are extremely similar.4 If we compare the POS tagging accuracies of the
parsed sentences and the accuracies of the original POS tags assigned by the tagger, we see that for
TiGer, the accuracy decreases by approximately 1.5 percent points for UTS, 0.1 percent points for STTS
and 9 percent points for STTSmorph. For Tu?Ba-D/Z, the loss in the STTSmorph setting is smaller, at
around 4 percent points. For UTS and STTS, there is a small improvement in POS tagging accuracy.
When we look at the parsing results, we see that gold POS tags always lead to the highest parsing
results, across treebanks and POS tagsets. We also see that across all conditions, the parsing results
for STTS are the highest. For TiGer, the results for UTS are only marginally lower, which seems to
indicate that some of the distinctions made in STTS are important, but not all of them. For Tu?Ba-D/Z,
the loss for UTS is more pronounced, at around 2 percent points. This suggests that for the Tu?Ba-D/Z
annotation scheme, the more fined grained distinctions in STTS are more important than for UTS. One
example would be the distinction between finite and infinite verbs, which is directly projected to the verb
group in Tu?Ba-D/Z (see the verb groups VXFIN and VXINF in figure 2). Note also that for Tu?ba-D/Z,
the parsing based on automatic POS tagging outperforms parsing based on gold UTS tags, thus again
confirming how important the granularity of STTS is for this treebank.
When we look at the parsing results for STTSmorph, it is obvious that this POS tagset variant leads
to the lowest parsing results, even in the gold POS setting. This means that even though agreement
4Because of the (almost) identical results, we checked our results with extreme care but could not find any errors.
10
information should be helpful for assigning grammatical functions, the information seems to be presented
to the parser in a form that it cannot exploit properly. We also performed preliminary experiments using
the morphological variants discussed in section 5.3 in parsing, but the results did not improve over the
STTS baseline.
When we compare the two sets of automatically assigned POS tags for TiGer, we see that the difference
in POS accuracy for UTS is 1.8 percent points while the difference in F-scores is 2.5 percent points. This
means that TnT tagging errors have a more negative impact on parsing quality than those in the POS
tags assigned by the parser itself. For STTSmorph, the difference is more pronounced in POS accuracy
(4 points as opposed to 2.2 in F-scores), which means that for STTSmorph, TnT errors are less harmful
than for UTS. We assume that this is the case because in many instances, the POS tags themselves will
be correct, and the error occurs in the morphological features. For Tu?Ba-D/Z, the difference between
UTS and STTSmorph is marginal; this is due to the fact that UTS results are much lower than for TiGer.
Thus, the difference between STTS and STTSmorph is stable across both treebanks.
A more in-depth investigation of the results shows that the aggregate EVALB score tends to hide indi-
vidual large differences between single sentences in the results. For example, in the results for the TiGer
dev set with gold POS tags, there are 119 sentences in STTSmorph which have an STTS counterpart with
an F-score that is at least 50 points higher. However, there are also 28 sentences for which the opposite
holds, i.e., for which STTSmorph wins over STTS. In Tu?Ba-D/Z, there are fewer sentences with such
extreme differences. There are 28 / 11 sentences with a score difference of 50 points or more between
STTS and STTSmorph in the Tu?Ba-D/Z development set, and vice versa. A manual inspection of the
results indicates that in some cases, the morphology is passed up into the tree and thereby contributes
to a correct grammatical function of a phrase label (such as for case information) while in other cases,
it causes an over-differentiation of grammatical functions and thereby has a detrimental effect (such as
for PPs, which are attached incorrectly). In the case of Tu?Ba-D/Z, this leads to trees with substructures
that are too flat, while in the case of TiGer, it leads to more hierarchical substructures. This finding is
corroborated by a further comparison of the number of edges produced by the parser, which reveals that
for the case of TiGer, the number of edges grows with the size of the POS tagset, while for the case of
Tu?Ba-D/Z, the number of edges produced with STTS is higher than with UTS, but drops considerably
for STTSmorph. The large differences in results for single sentences look more pronounced in TiGer due
to the average number of edges per sentence (7.60/8.72 for dev/test gold), which is much lower than for
Tu?Ba-D/Z (20.93/21.16 for dev/test gold); in other words, because of its flat annotation. We suspect that
there is data sparsity involved, but this needs to be investigated further.
7 Conclusion and Future Work
We have investigated how the granularity of POS tags influences POS tagging, and furthermore, how POS
tagging performance relates to parsing results, on the basis of experiments on two German treebanks,
using three POS tagsets of different granularity (UTS, STTS, and STTSmorph), and six different POS
taggers, together with the Berkeley parser.
We have shown that the tagging task is easier the less granular the tagset is. Furthermore, we have
shown that both too coarse-grained and too fine-grained distinctions on POS level hurt parsing perfor-
mance. The results for the morphological tagset are thus in direct contrast to previous studies, such as
(Dehdari et al., 2011; Marton et al., 2013; Seddah et al., 2009; Sza?nto? and Farkas, 2014), which show
for different languages that adding morphological information increases parsing accuracy. Surprisingly,
given the STTS tagset, the Berkeley parser itself was able to deliver a POS tagging performance which
was almost identical to the performance of the best tagger, TnT. Additionally, we can conclude that the
choice of the tagset and of the best POS tagger for a given treebank does not only depend on the language
but also on the annotation scheme.
In future work, we will undertake a systematic investigation of tag clustering methods in order to find a
truly optimally granular POS tagset. We will also investigate the exact relation between annotation depth
and the granularity of the POS tagset with regard to parsing accuracy and data sparsity. The latter may
elucidate reasons behind the differences between our results and those of the studies mentioned above.
11
References
Bernd Bohnet and Joakim Nivre. 2012. A transition-based system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In Proceedings of the Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1455?1465,
Jeju Island, Korea.
Bernd Bohnet. 2010. Very high accuracy and fast dependency parsing is not a contradiction. In Proceedings of
the 23rd International Conference on Computational Linguistics (IJCNLP), pages 89?97, Beijing, China.
Adriane Boyd. 2007. Discontinuity revisited: An improved conversion to context-free representations. In Pro-
ceedings of The Linguistic Annotation Workshop (LAW) at ACL 2007, pages 41?44, Prague, Czech Republic.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks and Linguistic Theories (TLT), pages 24?41, Sozopol,
Bulgaria.
Thorsten Brants, 1998. TnT?A Statistical Part-of-Speech Tagger. Universita?t des Saarlandes, Computational
Linguistics, Saarbru?cken, Germany.
Thorsten Brants. 2000. TnT?a statistical part-of-speech tagger. In Proceedings of the 1st Conference of the North
American Chapter of the Association for Computational Linguistics and the 6th Conference on Applied Natural
Language Processing (ANLP/NAACL), pages 224?231, Seattle, WA.
Marie Candito and Djame? Seddah. 2010. Parsing word clusters. In Proceedings of the NAACL HLT 2010 First
Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 76?84, Los Angeles, CA.
Xiao Chen and Chunyu Kit. 2011. Improving part-of-speech tagging for context-free parsing. In Proceedings of
5th International Joint Conference on Natural Language Processing (IJCNLP), pages 1260?1268, Chiang Mai,
Thailand.
Grzegorz Chrupala, Georgiana Dinu, and Josef van Genabith. 2008. Learning morphology with Morfette. In
Proceedings the Fifth International Conference on Language Resources and Evaluation (LREC), Marrakech,
Morocco.
Michael Collins, Jan Hajic?, Lance Ramshaw, and Christoph Tillmann. 1999. A statistical parser for Czech. In
Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 505?512,
College Park, MD.
Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania, Philadelphia, PA.
Michael Daum, Kilian Foth, and Wolfgang Menzel. 2003. Constraint based integration of deep and shallow
parsing techniques. In Proceedings of the 10th Conference of the European Chapter of the Association for
Computational Linguistics (EACL), Budapest, Hungary.
Jon Dehdari, Lamia Tounsi, and Josef van Genabith. 2011. Morphological features for parsing morphologically-
rich languages: A case of Arabic. In Proceedings of the Second Workshop on Statistical Parsing of Morpholog-
ically Rich Languages, pages 12?21, Dublin, Ireland.
Kilian Foth, Michael Daum, and Wolfgang Menzel. 2005. Parsing unrestricted German text with defeasible
constraints. In H. Christiansen, P. R. Skadhauge, and J. Villadsen, editors, Constraint Solving and Language
Processing, pages 140?157. Springer.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool: A general POS tagger generator based on Support Vector
Machines. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC),
pages 43?46, Lisbon, Portugal.
Yoav Goldberg and Reut Tsarfaty. 2008. A single generative model for joint morphological segmentation and
syntactic parsing. In Proceedings of The 46th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies (ACL:HLT), pages 371?379, Columbus, OH.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2011. Incremental joint POS tagging and
dependency parsing in Chinese. In Proceedings of 5th International Joint Conference on Natural Language
Processing (IJCNLP), pages 1216?1224, Chiang Mai, Thailand.
Geoffrey Hinton. 1999. Products of experts. In Proceedings of the Ninth International Conference on Artificial
Neural Networks, pages 1?6, Stockholm, Sweden.
12
Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Pro-
ceedings of The 46th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies (ACL:HLT), pages 595?603, Columbus, OH.
Sandra Ku?bler and Wolfgang Maier. 2013. U?ber den Einfluss von Part-of-Speech-Tags auf Parsing-Ergebnisse.
Journal for Language Technology and Computational Linguistics. Special Issue on ?Das Stuttgart-Tu?bingen
Wortarten-Tagset ? Stand und Perspektiven?, 28(1):17?44.
Sandra Ku?bler, Erhard W. Hinrichs, and Wolfgang Maier. 2006. Is it really that difficult to parse German? In
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
111?119, Sydney, Australia.
Corrin Lakeland. 2005. Lexical Approaches to Backoff in Statistical Parsing. Ph.D. thesis, University of Otago,
New Zealand.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon. 2010. Practical very large scale CRFs. In Proceedings
the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 504?513, Uppsala,
Sweden.
Joseph Le Roux, Benoit Sagot, and Djame? Seddah. 2012. Statistical parsing of Spanish and data driven lemma-
tization. In Proceedings of the ACL 2012 Joint Workshop on Statistical Parsing and Semantic Processing of
Morphologically Rich Languages, pages 55?61, Jeju, Republic of Korea.
Wolfgang Maier, Miriam Kaeshammer, and Laura Kallmeyer. 2012. Data-driven PLCFRS parsing revisited:
Restricting the fan-out to two. In Proceedings of the Eleventh International Conference on Tree Adjoining
Grammars and Related Formalisms (TAG+11), Paris, France.
Yuval Marton, Nizar Habash, and Owen Rambow. 2013. Dependency parsing of Modern Standard Arabic with
lexical and inflectional features. Computational Linguistics, 39(1):161?194.
Slav Petrov and Dan Klein. 2007a. Improved inference for unlexicalized parsing. In Proceedings of Human Lan-
guage Technologies 2007: The Conference of the North American Chapter of the Association for Computational
Linguistics, pages 404?411, Rochester, NY.
Slav Petrov and Dan Klein. 2007b. Learning and inference for hierarchically split PCFGs. In Proceedings of
AAAI (Nectar Track), Vancouver, Canada.
Slav Petrov and Dan Klein. 2008. Parsing German with language agnostic latent variable grammars. In Proceed-
ings of the ACL Workshop on Parsing German, pages 33?39, Columbus, OH.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of the
Eight International Conference on Language Resources and Evaluation (LREC), Istanbul, Turkey.
Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of the Empiri-
cal Methods in Natural Language Processing Conference (EMNLP), pages 133?142, Philadelphia, PA.
Anne Schiller, Simone Teufel, and Christine Thielen. 1995. Guidelines fu?r das Tagging deutscher Textkorpora mit
STTS. Technical report, Universita?t Stuttgart and Universita?t Tu?bingen.
Helmut Schmid and Florian Laws. 2008. Estimation of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceedings of the 22nd International Conference on Computational
Linguistics (COLING), pages 777?784, Manchester, UK.
Djame? Seddah, Marie Candito, and Beno??t Crabbe?. 2009. Cross parser evaluation: A French Treebanks study.
In Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 150?161, Paris,
France.
Wolfgang Seeker and Jonas Kuhn. 2013. Morphological and syntactic case in statistical dependency parsing.
Computational Linguistics, 39(1):23?55.
Zsolt Sza?nto? and Richa?rd Farkas. 2014. Special techniques for constituent parsing of morphologically rich lan-
guages. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational
Linguistics (EACL), pages 135?144, Gothenburg, Sweden.
Heike Telljohann, Erhard W. Hinrichs, Sandra Ku?bler, Heike Zinsmeister, and Kathrin Beck, 2012. Stylebook for
the Tu?bingen Treebank of Written German (Tu?Ba-D/Z). Seminar fu?r Sprachwissenschaft, Universita?t Tu?bingen,
Germany.
13
Kristina Toutanova and Christopher D. Manning. 2000. Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora (EMNLP/VLC), Hong Kong.
Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proceedings of the Human Language Technology Conference of
the North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pages 252?259,
Edmonton, Canada.
Yannick Versley and Ines Rehbein. 2009. Scalable discriminative parsing for German. In Proceedings of the 11th
International Conference on Parsing Technologies (IWPT), pages 134?137, Paris, France.
Yannick Versley. 2005. Parser evaluation across text types. In Fourth Workshop on Treebanks and Linguistic
Theories (TLT 2005), Barcelona, Spain.
14
