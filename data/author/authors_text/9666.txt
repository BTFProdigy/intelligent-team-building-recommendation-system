Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 13?16,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Dimensions of Subjectivity in Natural Language
Wei Chen
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
weichen@cs.cmu.edu
Abstract
Current research in automatic subjectivity
analysis deals with various kinds of subjec-
tive statements involving human attitudes and
emotions. While all of them are related to
subjectivity, these statements usually touch on
multiple dimensions such as non-objectivity1,
uncertainty, vagueness, non-objective measur-
ability, imprecision, and ambiguity, which are
inherently different. This paper discusses the
differences and relations of six dimensions of
subjectivity. Conceptual and linguistic char-
acteristics of each dimension will be demon-
strated under different contexts.
1 Introduction
Natural language involves statements that do not
contain complete, exact, and unbiased information.
Many of these are subjective, which share the com-
mon property described in narrative theory (Ban-
field, 1982) as ?(subjective statements) must all be
referred to the speaking subject for interpretation?.
Wiebe (1990) further adapted this definition of sub-
jectivity to be ?the linguistic expression of private
states (Quirk et al, 1985)?. So far, linguistic cues
have played an important role in research of sub-
jectivity recognition (e.g. (Wilson et al, 2006)),
sentiment analysis (e.g. (Wilson et al, 2005; Pang
and Lee, 2004)), and emotion studies (e.g. (Pen-
nebaker et al, 2001)). While most linguistic cues
1We use the term ?non-objectivity? to refer to the property
of creating a bias from a speaker?s point of view that is not sup-
ported by sufficient objective evidence. It is not identical to the
subjectivity that involves all the dimensions we discuss in this
paper.
are grouped under the general rubric of subjectiv-
ity, they are usually originated from different dimen-
sions, including:
? non-objectivity
? uncertainty
? vagueness
? non-objective measurability
? imprecision
? ambiguity
These dimensions all mingle in various applications
that deal with subjective statements. For example,
opinion extraction processes statements involving
non-objectivity and uncertainty. Evaluation and sen-
timent analysis deal with vague words, which of-
ten covers the issue of non-objective measurability
and imprecision. Ambiguity sometimes involves im-
plicit subjectivity that is hard to recognize from lin-
guistic patterns, which leads to great challenge of
identifying and understanding subjective statements.
Since multiple dimensions are involved in subjec-
tivity, discriminating them may be helpful in under-
standing subjectivity and related concepts. The fol-
lowing sections discuss characteristics and relations
of the six dimensions of subjectivity.
2 Dimensions of Subjective Statements
2.1 Non-objectivity
In this paper, we define non-objectivity as the prop-
erty of creating a bias according to personal beliefs,
judgments and emotions. This does not include the
kind of subjectivity originated from particular prop-
erties of linguistic units that lead to personal in-
terpretations. Non-objectivity exists in subjective
13
statements such as opinions, evaluations, and per-
suasive statements. Non-objectivity can be recog-
nized from linguistic patterns including words ex-
plicitly expressing thoughts, beliefs, speculations,
and postulations such as ?think?, ?believe?, ?hope?
and ?guess?. Although linguistic cues are found to
be reliable, there are cases of non-objectivity that
cannot be identified merely from lexical, syntactical
or morphological cues. For example, sentence (1)
and sentence (2) are very similar in linguistic struc-
tures, but only sentence (2) is non-objective.
(1) Living things cannot survive without water.
(2) He cannot survive without music.
Apart from linguistic patterns and conceptual
characteristics of non-objectivity, there are two
main issues in non-objectivity recognition. First,
non-objectivity cannot be clearly identified without
knowledge about its source (Wiebe et al, 2005).
For example, ?Bob says the red team is about to
win? is objective with respect to the position of the
speaker of the sentence, who objectively stated a
speech event. But the fragment ?the red team is
about to win? is an opinion of Bob. Hence, whether
a statement is an opinion depends on both the scope
of the statement and the source of that statement.
Second, non-objectivity always lies in a context,
which cannot be ignored (Wiebe, 1990). For ex-
ample, ?Pinocchio?s nose? is likely to be objective
when used within the context of the famous fairy
tale. But the same phrase can be used subjectively as
a metaphor in other contexts, where it may indicate
non-objectivity.
2.2 Uncertainty
Uncertainty can indicate either subjectivity or ob-
jectivity. Flagged by words such as ?probably?
and ?maybe?, statements expressing uncertainty are
usually considered subjective because ?being uncer-
tain? itself can be a subjective mental activity. How-
ever, uncertainty is not a subtype of subjectivity.
Consider the following sentences:
(3) Bob has probably already finished his home-
work.
(4) A poll of recent public opinions shows that Bob
is likely to win the nomination.
Sentence (3) is a subjective statement, where the
speaker expresses his/her postulation of ?Bob fin-
ished his homework? through the uncertainty indi-
cated by ?probably?. On the contrary, sentence (4)
is an objective statement, although uncertainty about
a future event exists. This sentence reports a conclu-
sion drawn from sufficient evident that Bob takes the
majority vote based on the survey, which does not
rely on a particular speaking subject for interpreta-
tion. In this case, uncertainty does not necessarily
imply subjectivity.
On the other hand, people sometimes explicitly
indicate uncertainty to avoid being subjective.
(5) It is possible that the red team will win.
(6) It is likely that the red team will win.
(7) The red team will win.
We could easily imagine a scenario where sentence
(5) is more objective than sentence (6) and (7). For
example, the speaker may believe that the red team
will lose, but in order to avoid personal bias, he/she
may instead say: ?It is possible that the red team
will win (but the blue team has a better chance).?
In general, explicitly showing uncertainty can imply
postulation, but it can also convey the intention of
being objective by not excluding other possibilities.
Uncertainty sometimes exists in statements where
no linguistic cues are present. For example, the lin-
guistic pattern of sentence (7) is similar to that of
?I will have an exam tomorrow?, but the later one
is usually used to describe an objective future event
while sentence (7) can be semantically identical to
sentence (6)2, although the indicator of uncertainty
in sentence (7) is not shown explicitly.
2.3 Vagueness, Non-objective Measurability,
and Imprecision
Vagueness refers to a property of the concepts that
have no precise definitions. For example, gradable
words such as ?small? and ?popular? are sometimes
treated as linguistic cues of vagueness, and they are
found to be good indicators of subjectivity (Hatzi-
vassiloglou and Wiebe, 2000).
Especially, gradable words are vague if there is no
well-defined frame of reference. This in some cases
2These two are identical as long as the game is not fixed.
14
leads to two issues: comparison class and bound-
ary. In the sentence ?Elephants are big?, the compar-
ison class of ?elephants? is unclear: we could com-
pare the size of elephants with either land animals
or all the animals including both land and aquatic
creatures3. Also, there is no clear boundary between
?being small? and ?not being small?. Different indi-
viduals usually have their own fuzzy boundaries for
vague concepts. As such, vague words are usually
treated as important cues for subjectivity. However,
learning which words are vague is non-trivial, be-
cause vagueness cannot be hard-coded into lexicons.
For example, the gradable word ?cold? is vague in
sentence (8) but not in sentence (9). The difference
between these two is the one in sentence (9) has a
known boundary which is the temperature for liquid
water to exist, and the one in sentence (8) simply
reflects personal perception.
(8) It is cold outside.
(9) It is too cold during the night on the moon for
liquid water to exist.
Vagueness is often a strong indicator of subjectiv-
ity because it involves personal explanation of a con-
cept. But there are exceptions. For example, the def-
inition of ?traditional education? can be vague, but
talking about ?traditional education? may not neces-
sarily imply subjectivity.
When speaking of qualities, there are two ma-
jor dimensions related to vagueness: non-objective
measurability and imprecision. Attributes like
height, length, weight, temperature, and time are
objectively measurable, whereas things like beauty
and wisdom are usually not objectively measur-
able. Vagueness exists at different levels for non-
objectively and objectively measurable qualities.
For non-objectively measurable qualities, vagueness
exists at the conceptual level, where it intersects with
non-objectivity. In the sentence ?He is not as charm-
ing as his brother?, the word ?charming? refers to
a quality whose interpretation may vary among dif-
ferent cultures and different individuals. For ob-
jectively measurable qualities, vagueness exists at
the boundary-setting level, where either subjectiv-
ity or common sense comes into play. Sentence
3Other comparison classes are also possible.
(10) shows an example of the objectively measur-
able quality ?long time? indicating an opinion that
the speaker is unsatisfied with someone?s work. On
the contrary, an objective meaning of ?long time? in
sentence (11) can be resolved by common sense.
(10) You finally finished the work, but it took you a
long time.
(11) Intelligent life took a long time to develop on
Earth.4
Statements involving objectively measurable
quantities often have an imprecision problem, where
vagueness is usually resolved from common agree-
ments on small variations of values. For example,
?Bob is six feet tall? usually implies that the height
is ?around? six feet5, with a commonly acceptable
precision of about an inch. Generally, specific preci-
sions are determined by variations tied to measure-
ment technologies for specific quantities: the preci-
sion for the size of a cell may be around a micron,
and the error tolerance for the distance between stars
can be on the order of light years. Imprecision can
also indicate subjectivity when used for subjective
estimation. For instance, ?Bob needs two days to
finish his homework? is usually not telling an exact
period of time, but a personal estimation.
2.4 Ambiguity
While vagueness exists at the conceptual level, am-
biguity lies at the level of linguistic expressions. In
other words, an ambiguous statement contains lin-
guistic expressions that can refer to multiple expla-
nations, whereas a vague statement carries a concept
with unclear or soft definition.
Previous studies have explored the relationship
between ambiguity and subjectivity. They have
shown that subjectivity annotations can be helpful
for word sense disambiguation when a word has dis-
tinct subjective senses and objective senses (Wiebe
and Mihalcea, 2006).
Lexical and syntactical ambiguity usually can be
resolved from contextual information and/or com-
mon consensus. But when ambiguity is used in-
tentionality, identifying and understanding the am-
biguity become creative and interactive procedures,
4Sentence fragment adapted from Astrobiology Magazine
(Dec 02, 2002).
5It could also mean ?at least six feet tall? in some cases.
15
which usually indicate subjectivity. The sentence
?I?d like to see more of you? is an example of this
kind, which could be used to indicate multiple mean-
ings under the same context 6.
3 Mixtures of Multiple Dimensions
In many cases, subjective statements involve mul-
tiple of the dimensions discussed in previous sec-
tions. For example, the subjectivity of the sentence
?It?s a nice car? comes from three dimensions: non-
objectivity, vagueness and ambiguity. First, ?a car
being nice? is usually a personal opinion which may
not be commonly acceptable. Second, the gradable
word ?nice? indicates vagueness, since there is no
clear boundary for ?being nice?. Third, the sentence
is also ambiguous because ?nice? could refer to ap-
pearance, acceleration, angle rate, and many other
metrics that might affect personal evaluations.
For information retrieval systems, processing nat-
ural queries such as ?find me the popular movies of
2007? requires proper understanding of the vague
word ?popular?. Besides, non-objectivity and am-
biguity also take part in the query: on the non-
objectivity side, the definition of ?popular? may dif-
fer according to different individuals; on the ambi-
guity side, the word ?popular? may refer to different
metrics related to the popularity of a movie such as
movie ratings and box office performance.
In applications requiring certain level of
language-understanding, things can get even
more complicated while different dimensions
weave together. As in sentence (5), the speaker
may bias towards the blue team while he/she
shows uncertainty towards the red team. Correctly
understanding this kind of subjective statements
would probably need some investigation in different
dimensions of subjectivity.
4 Conclusion
In this paper, we demonstrated that subjectivity in
natural language is a complex phenomenon that con-
tains multiple dimensions including non-objectivity,
uncertainty, vagueness, non-objective measurability,
imprecision and ambiguity. These dimensions pat-
tern together in various kinds of subjective state-
6Kent Bach, Ambiguity. Routledge Encyclopedia of Philos-
ophy, http://online.sfsu.edu/ kbach/ambguity.html
ments such as opinions, evaluations and natural
queries. Since these dimensions have different
behaviors in subjective statements, discriminating
them in both linguistic and psychological aspects
would be necessary in subjectivity analysis.
Acknowledgments
The author would like to thank Scott Fahlman for
the original motivation of the idea and helpful dis-
cussions.
References
Ann Banfield. 1982. Unspeakable Sentences: Narration
and Representation in the Language of Fiction. Rout-
ledge and Kegan Paul, Boston.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of adjective orientation and gradability on sen-
tence subjectivity. In Proceedings of the 18th confer-
ence on Computational linguistics, pages 299?305.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the ACL,
pages 271?278.
James Pennebaker, Martha Francis, and Roger Booth.
2001. Linguistic Inquiry and Word Count: LIWC.
Lawrence Erlbaum Associates, Mahwah.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Grammar
of the English Language. Longman, New York.
JanyceWiebe and RadaMihalcea. 2006. Word sense and
subjectivity. In Proceedings of the ACL, pages 1065?
1072.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. In Language Resources and Evaluation,
volume 39, pages 165?210.
Janyce Wiebe. 1990. Recognizing Subjective Sen-
tences: A Computational Investigation of Narrative
Text. Ph.D. thesis, SUNY Buffalo Dept. of Computer
Science.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT ?05: Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 347?354.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2006.
Recognizing strong and weak opinion clauses. Com-
putational Intelligence, 22(2):73?99.
16
Proceedings of the 8th International Conference on Computational Semantics, pages 61?72,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Understanding Mental States in Natural Language
Wei Chen
Language Technologies Institute, Carnegie Mellon University
Pittsburgh, PA 15213, USA
weichen@cs.cmu.edu
Abstract
Understanding mental states in narratives is an important aspect
of human language comprehension. By ?mental states? we refer to
beliefs, states of knowledge, points of view, and suppositions, all of
which may change over time. In this paper, we propose an approach
for automatically extracting and understanding multiple mental states
in stories. Our model consists of two parts: (1) a parser that takes
an English sentence and translates it to some semantic operations; (2)
a mental-state inference engine that reads in the semantic operations
and produces a situation model that represents the meaning of the
sentence. We present the performance of the system on a corpus of
children stories containing both fictional and non-fictional texts.
1 Introduction
Natural language involves statements that carry distinct world-views. By
world-views we refer to states of belief, supposition, intention, advice, per-
ceived reality, as well as situations expressed by tenses in natural language
such as past, present and future. In this paper, we call these world-views
?mental states?. Mental states are common phenomena. They span various
domains of natural language. Sentence (1a) and sentence (1b) are examples
drawn from two different domains: online news articles and fairy tales.
(1) a. The police believe the thieves were trying to steal a solar panel
from Sarah?s tin roof.
1
1
Excerpt from BBC online news: http://news.bbc.co.uk/1/hi/world/africa/
7609872.stm
61
b. She (little red-cap) was surprised to find the cottage-door standing
open.
2
Both of these two sentences involve multiple mental states, which may be
nested in one another. Sentence (1a) involves the police?s belief and the
intention of the thieves in the police?s belief; sentence (1b) contains little
red-cap?s old belief and her updated belief, both of which may be different
from the reality. Since the information in mental states is rich and often
important, we need some processing technique to extract that information
and understand it.
There are two problems in extracting and understanding mental states.
First, extracting mental states from text requires recognizing linguistic pat-
terns of mental states. Related problems such as subjectivity recognition
have been studied intensively in the natural language processing commu-
nity. The problem covers various aspects of the ?private state frame? [10],
including recognizing private states, the sources of private states, the in-
tensity and types of attitudes, among many others. Second, mental states
extracted from natural language need to be encoded in some representation
form and can be retrieved for further inference. There exists many systems
that implements nested evolving beliefs (e.g. [1]), but they generally lacked
the ability to draw inference directly from natural language.
In this paper, we propose an approach to extract and represent mental
states based on different mental contexts such as one?s belief, intention and
supposition. Our approach utilizes the mental spaces theory [4] in cognitive
linguistics. Our goal for the mental state extraction step is to identify space-
builders that establish new mental contexts (or spaces) or the ones that refer
to an existing mental context. The main body of our space-builders consists
of agent and psych-term pairs such as ?the police believe? (refers to the
police?s belief context) and ?little red-cap was surprised? (refers to multiple
belief contexts of little red-cap)
3
. Different objects and propositions are then
bundled in these mental contexts. In the mental state understanding step,
the mental contexts are instantiated and maintained in a context network,
where inference rules are applied within and across those contexts.
The rest of this paper is organized as follows. Section 2 provides a
high-level overview of our implemented mental state understanding system.
2
Excerpt from ?The Little Red-Cap? in Margaret Hunt?s translation of the Grimms
Fairy Tales. In some other translations, the story is also called ?Little Red Riding Hood?.
3
Fauconnier (1985) covers a much broader set of space-builders including prepositional
phrases (?from her point of view?), connectives (?if ... then ...?), and subject-verb com-
binations (?she thinks?). Our current system only deals with the last category.
62
Figure 1: Overview of the mental state understanding system
Simple examples will be presented to demonstrate the input and output
of the system. Section 3 and section 4 introduces our parser for mental
state extraction and the inference engine for mental state representation,
respectively. Section 5 discusses evaluation results on fictional and non-
fictional children stories.
2 System Overview
2.1 System Components
Similar to many story comprehension systems (e.g. [8]), our system con-
sists of two parts. A parser combines several natural language processing
components to extract useful information from raw text. The information
is then integrated and filled into psych-term-argument templates to gener-
ate an intermediate semantic form called mental operation. The inference
engine translates mental operations into a situation model represented by a
semantic network. During this process, it fills in non-literal semantic infor-
mation and maintains the situation model by a set of inference rules. Figure
1 shows the general structure of the system.
2.2 Example Output
To provide a general idea of what the system does, we temporarily treat it
as a black box and use concrete examples to demonstrate its function. The
63
input to the system is a piece of raw English text. The system processes
one sentence at a time and outputs a situation model which represents the
mental states of the characters appeared in the story.
Figure 2(a) and 2(b) show the semantic networks generated by our sys-
tem given input sentences (1a) and (1b), respectively. As shown in the
figure, our semantic representation of mental states is a set of mental con-
texts attached to different characters. Any inference or retrieval is done with
respect to a specific context. Figure 2(a) shows a structured representation
of the nested mental states in sentence (1a). We read the representation as:
In the police?s belief, there is the thieves? intention, in which the thieves try
to steal a solar panel from Sarah?s tin roof. Figure 2(b) shows a situation
where little red-cap?s mental image of the reality changes.
(a) The semantic representation of (1a). (b) The semantic representation of (1b).
Figure 2: The output models. Arrows with big solid heads represent ?sub-
context? relation. Arrows with dotted lines represent ?in-context? relation.
Arrows with solid lines represent ?property? relation. Double-headed arrows
represent ?equals? relation.
According to these models, our system can generate and answer yes-no ques-
tions like
4
:
(2) a. Question: Do the police believe that the thieves were trying to steal
a solar panel from Sarah?s tin roof?
Answer: Yes.
b. Question: In reality, were the thieves trying to steal a solar panel
from Sarah?s tin roof?
4
The questions are generated in the form of Scone language (a language used in the
Scone knowledge-base), not English. We translate Scone language into English for the
purpose of illustration.
64
Answer: Not sure
5
.
c. Question: Before little red-cap was surprised, did she think that
the cottage door was open?
Answer: No.
d. Question: Does little red-cap think the cottage door is open now?
Answer: Yes.
3 Integrated Parser
The parsing procedure consists of three stages: pre-processing, psych-term-
argument parsing, and statement building.
3.1 Pre-processing
The pre-processing component consists of a sentence detector, a tokenizer,
a chunker, and an anaphora annotator. We use the OpenNLP
6
toolkit to
perform the first three tasks. Then we apply an in-house anaphora annota-
tor to annotate pronouns in the text. The anaphora annotator implements
a modified version of Strube?s S-List and his coreference algorithm [9]. The
S-List is a datastructure that maintains a list of sorted candidate entities for
anaphora resolution. Following Strube, a set of ranking constraints is used
for modeling readers? attentions. The constraints include the reader-old and
reader-new discourse entitie labels (?brand new?, ?unused?, and ?evoked?).
Our features used for anaphora resolution include number/gender agree-
ments, binding constraints, types of nouns (proper or common) and partial
string matching. Our implementation is mostly developed on children sto-
ries, in which there are many instances where the gender information for
a proper noun is missing. For example, the program does not know little
red-cap?s gender the first time it sees the name in the story. But we allow
that information to be filled in as the annotator reads the text. Also, some
common features such as animacy and named entity class are not included
in our feature set because they do not adapt well to this kind of narratives.
3.2 Psych-term-argument Parsing
The goal for the psych-term-argument parsing process is to produce flat
mental operations (as opposed to structured mental operations which will be
5
?Not sure? means ?neither ?yes? nor ?no? ?.
6
http://opennlp.sourceforge.net/
65
generated by the statement builder). After pre-processing, the ASSERT [7]
semantic role labeler is used to annotate each sentence with PropBank [6] ar-
gument labels. The most frequently used argument labels include TARGET
(for psych-terms), ARG0 (for agents), ARG1 (for patients or propositions),
ARG2 (for patients or propositions when ARG1 is absent), ARG-MOD (for
modal verbs) and ARG-NEG (for negations).
Although every sentence is processed, the system only looks for those
that contain the pre-defined psych-terms. The set of psych-terms are cho-
sen from a larger set of mental expressions drawn automatically from the
WordNet alng the synset (sets of synonyms) links. The seed words used
for collecting mental expressions from the WordNet contains 6 mental verbs:
?think?, ?want?, ?pretend?, ?confess?, ?surprise? and ?realize?. For each
mental expression returned by WordNet, we restrict the next search depth
to 3. Using this method, WordNet returns 238 different verbs and phrases,
among which we choose 42 psych-terms that are relatively less ambiguous
for our initial system development. The 42 psych-terms also include modal
verbs such as ?will? and ?must?. These psych-terms are matched to the
text annotated by ASSERT. Parses like (3a) and (3b) are then extracted
7
.
(3) a. 0: [ARG0 The police] [TARGET believe ] [ARG1 the thieves were
trying to steal a solar panel from Sarah ?s tin roof]
b. 0: The police believe [ARG0 the thieves] were [TARGET trying ]
[ARG1 to steal a solar panel from Sarah ?s tin roof]
A set of grammar rules are used to map the target verbs and their arguments
to mental operations like (4a) and (4b). At the implementation level, the
mental operations are Lisp functions to be evaluated by the mental state
inference engine.
(4) a. (new-single-modal {The police} ?( (new-statement {the thieves were
trying to steal a solar panel from Sarah ?s tin roof})) {belief})
b. (new-single-modal {the thieves} ?((new-statement {to steal a solar
panel from Sarah ?s tin roof})) {intention})
3.3 Building Statement Structures
The psych-term-argument parsing components generate a set of mental op-
erations. However, not all of these operations ought to be evaluated by the
7
We have 2 statement checking rules to correct possible ASSERT output errors on
propositions.
66
inference engine. In general, mental operations generated from complemen-
tizer phrases should not be evaluated. Instead, they should be passed to
their parent operations as proposition arguments. For example, mental op-
eration (4b) comes from the complemetizer phrase ?the thieves were trying
to steal a solar panel from Sarah?s tin roof?. If this operation is evaluated
in the inference engine, we will not get the same semantic representation as
in Figure 2. Instead, the system will erroneously judge the proposition ?the
thieves were trying to steal a solar panel from Sarah?s tin roof? to be true
in reality, while the correct representation is to make it true in the police?s
belief. To avoid such errors, (4b) is passed to its parent mental operation
(4a) and made an argument of (4a), which ensures that (4b) will be evalu-
ated under the police?s belief. The product of this process is a structured
mental operation (5). And finally, only (5) is sent to the inference engine.
In summary, the goal of the statement builder is to build and evaluate the
correct mental operations.
(5)
(new-single-modal {The police} ?( (new-single-modal {the thieves}
?((new-statement {to steal a solar panel from Sarah ?s tin roof}))
{intention})) {belief})
To build a structured mental operation, each candidate mental operation
is stored in a list. A topological sort is performed based on the complemen-
tizer phrase relationships among different mental operations. Such relations
are found through argument span check. That is, if some arguments of oper-
ation 1 are all found in operation 2?s proposition argument, we will assume
operation 1 is operation 2?s child operation. After the topological sort, the
head of the list stores the element mental operations which do not have
children operations, and the tail of the list stores the mental operation that
covers the whole sentence. Then the operations are fed into their parent
operations one by one along the list. Finally, those operations that have no
parents are sent to the inference engine.
4 Mental State Inference Engine
In this section, we briefly explain the structure and mechanisms of the mental
state inference engine.
67
4.1 Context Activation Mechanism
The mental state inference engine is built on top of the Scone knowledge-
base (KB) system. Scone is designed to be a practical KB with emphasis on
its efficiency of the most commonly used operations for search and inference.
Regarding these goals, Scone provides default reasoning with exceptions. As
we have shown, Scone can be viewed as a semantic network representation,
with nodes representing entities and links representing relations or state-
ments tied to these entities. At a higher level, the types in Scone may be
viewed as frames or descriptions. A multi-context and context activation
mechanism is designed into Scone using the marker-passing algorithm [3].
In this paper, a context is used to represent the state of mental attitudes. In
Scone, the context nodes are treated as the other nodes in that they are also
tied into their own inheritance hierarchy using ?sub-context? links. How-
ever, the ?sub-context? relation represented by inheritance is a mechanical
one. The relation between the two contexts is neither ?is-a? nor ?part-of?,
but something more like ?is a clone of, modulo explicit changes?. Contexts
can also be used to represent the state of a changing world at different times.
Each of the contexts represents a mental attitude at a specific time; it begins
as a clone of the state in the previous time-step, and then we can add or
remove a few items if necessary.
4.2 Mental Context Representation
In general, the mental context model tracks the changes of the mental state
[2]. At each time point, it builds a mental context network that represents
nested mental states. The input to the model is a list of mental context
operations extracted from text. Each of the operations corresponds to one
psych-term. The complex semantics of a psych-term is factored into a set
of atomic operations on single mental contexts through semantic decom-
position. These contexts are organized into a hierarchical structure, which
constitutes a simplified representation of human memory.
The semantics of psych-terms are projected onto the context network
through semantic decomposition
8
. For example, one sense of the word ?pre-
tend? can be represented as ?X is not present in reality and person P1?s
belief, but P1 wants person P2 to believe it? (these semantic definitions
are restricted to mental contexts). This example involves several contexts:
the reality, the belief of P1, the intention of P1, the belief of P2 under the
intention of P1, as well as the before context and the after context of ?pre-
8
Recent work on verb entailment can be found in [5].
68
tend?. Note that the mental operations are higher order, so there can be
other psych-terms (e.g. ?want?) embedded the definition of ?pretend?.
Mental operations update the mental context network in two aspects.
First, they build a context inheritance chain which represents the evolution
of a single mental context at different time steps. For example, in the ?Little
Red-Cap? story, little red-cap has different belief contexts at different time.
By default, each of the newly updated versions of one?s belief would inherit
from his/her most recent belief. Second, the mental operations are used
to build a hierarchical context structure which organizes multiple types of
mental context according to events and agents. Figure 2 illustrates three
basic aspects of the context structure:
1. By default, the mental contexts inherit from a general knowledge con-
text which represents the general background knowledge of a story.
2. Mental contexts can be organized by events. A typical mental event
has an agent whose mental contexts are changed as an effect of the
event. Each psych-term would be mapped to one of the mental events.
When we retrieve an event, a set of related contexts of that event would
also be retrieved.
3. The mental contexts are environment-sensitive. For example, the
thieves? real intention can be different from the police?s belief of their
intention.
In our representation, different instances of the mental contexts are
organized in a dynamic context structure. We could then constrain the
behaviors of different mental contexts under different mental events using
inter-contextual rules. Once a mental event (e.g. ?little red-cap was sur-
prised?) triggers, the related mental contexts would check and modify their
own structures and contents based on the new information. Usually this
self-adjustment can be achieved by a realization of a difference between the
external world and the belief, assumption or expectation. According to this,
newly updated mental contexts would be constructed.
5 Evaluation Results
We use 513 children stories from Project LISTEN?s
9
reading tutor story
database for system evaluation. This corpus contains 213 fictional articles
9
http://www.cs.cmu.edu/
?
listen
69
and 300 non-fictional (or informational) articles. From these, our system ex-
tracts 1181 mental state expressions in fictions and 413 in non-fictions. Af-
ter the parsing stage, 60.80% of the fictional and 62.71% of the non-fictional
mental state expressions are fully parsed (i.e. there is no empty arguments
for the mental operations) and sent to the inference engine. After processing
of the mental operations, the system automatically generates 1437 yes-no
questions and answers for fictional texts, and 518 for non-fictional texts.
The question-answer pairs are generated by traversing newly visited mental
contexts and statements/objects bundled in those contexts immediately af-
ter each mental operation is evaluated. These questions and answers are all
in similar forms as the examples demonstrated in section 2.2.
We do a careful evaluation on 431 questions for fictional texts and 155
for non-fictional text that are randomly selected from the question-answer
pool. Since both the questions and answers come from the situation model
stored in the system, an error occurred in either the question or the answer
counts for an incorrect example.
Table 1 shows the error rates of each error category for both fictional
and non-fictional stories. The second column (?argument error?) gives the
percentage of incorrect question-answer pairs caused by wrong arguments
in the mental operations. This type of error comes from a misinterpreta-
tion of the ASSERT output. For example, we assume that ARG1 indicates
a proposition for psych-terms if it is not a noun phrase. But ?she [TAR-
GET wanted ] [ARG1 so much]? is an exception of this assumption. The
third column (?statement error?) gives the percentage of incorrect examples
(usually questions) caused by incomplete or unnecessary statements in the
mental contexts. This type of errors results from inaccurate ASSERT out-
puts that are not captured by our statement checking rules. Space-builder
error (the fourth column) refers to the cases in which the mental contexts
are not correctly constructed (this results in incorrect answers). This usu-
ally happens when there is a mental space that has been neglected by our
system. For example, when given sentence ?Dig for it if you want the gold?,
our system would only look at ?you want the gold? and treat it as a valid
statement, without noticing that it is embedded in an if-clause which in-
dicates another mental space. Ambiguity error (the fifth column) refers to
errors cause by the lexical ambiguity of psych-terms (e.g. the word ?will? in-
dicates future tense in ?Pilly will go to school tomorrow?, but not in ?Some
sharks will eat just about anything?). Negation error (the sixth column)
occurs when there is a negation that has been neglected (this results in an
incorrect answer). For example ?It will do him no good, neither will it help
anybody else? means ?It will not help anybody else?. But this negation is
70
Table 1: Evaluation Results
Argument Statement Space- Ambiguity Negation Total
Errors Errors Builder Errors Errors Errors
Errors
Fictions 6.26% 8.12% 8.35% 4.64% 4.64% 26.68%
Non-Fics 1.29% 2.58% 7.10% 7.74% 1.29% 19.35%
not captured by our system. Note that since the five types of errors listed in
Table 1 are not mutually exclusive, the error rates in each row of the table
do not sum up to the total error rate of question-answer pairs generated by
the system. Anaphora error has been counted separately since this type of
error alone has a significant effect on system performance, and the anaphora
annotator is relatively independent with our task compared to the other in-
house components. During evaluation, we only apply anaphora resolution
to pronouns (the word ?it? is not counted). In the same question-answer
pool, we observe the anaphora error rate of 19.49% on fictions and 17.42%
on non-fictions.
6 Conclusion
This paper presents an implemented system that understands mental states
expressed in narratives. The system extracts and processes mental states
by mapping psych-terms and their arguments to mental contexts stored in
a situation model. The system is evaluated by automatically generated
question-answer pairs. Future directions include extending the system to a
broader set of psych-terms and patterns to cover more mental states from
natural language. Meanwhile, we will explore the two-way interaction be-
tween the parser and the inference engine to refine the overall processing
accuracy.
7 Acknowledgements
The author would like to thank Scott Fahlman and Jack Mostow for many
helpful discussions. The research reported here was supported in part by
the Institute of Education Sciences, U.S. Department of Education, through
Grant R305B070458 to Carnegie Mellon University. The opinions expressed
71
are those of the authors and do not necessarily represent the views of the
Institute.
References
[1] Afzal Ballim and Yorick Wilks. Beliefs, stereotypes and dynamic agent
modeling. User Modeling and User-Adapted Interaction, pages 33?65,
1991.
[2] Wei Chen and Scott E. Fahlman. Modelling mental context and their
interactions. In AAAI Fall Symposium on Biologically Inspired Cogni-
tive Architectures, 2008.
[3] Scott E. Fahlman. Marker-passing inference in the scone knowledge-
based system. In KSEM?06. Springer-Verlag, 2006.
[4] Gilles Fauconnier. Mental Spaces: Aspects of Meaning Construction.
MIT Press, Cambridge, MA, USA, 1985.
[5] Lauri Karttunen. Word play. Computational Linguistics, 33(4):443?
467, 2007.
[6] Martha Palmer, Daniel Gildea, and Paul Kingsbury. The proposition
bank: An annotated corpus of semantic roles. Computational Linguis-
tics, 31(1):71?106, 2005.
[7] Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne Ward,
James H. Martin, and Daniel Jurafsky. Support vector learning for se-
mantic argument classification. Machine Learning, 60(1-3):11?39, 2005.
[8] C. K. Riesbeck and R. C. Schank. Comprehension by computer:
Expectation-based analysis of sentences in context. Technical Re-
port 78, Yale Computer Science Department, 1976.
[9] Michael Strube. Never look back: an alternative to centering. In
Proceedings of the 17th international conference on Computational lin-
guistics, pages 1251?1257, Morristown, NJ, USA, 1998. Association for
Computational Linguistics.
[10] Janyce Wiebe, Theresa Wilson, and Claire Cardie. Annotating expres-
sions of opinions and emotions in language. In Language Resources and
Evaluation, volume 39, pages 165?210, 2005.
72
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1159?1168,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Exploiting Community Emotion for Microblog Event Detection
Gaoyan Ou
1,2
, Wei Chen
1,2,
, Tengjiao Wang
1,2
, Zhongyu Wei
1,3
,
Binyang Li
4
, Dongqing Yang
1,2
and Kam-Fai Wong
1,3
1
Key Laboratory of High Confidence Software Technologies, Ministry of Education, China
2
School of Electronics Engineering and Computer Science, Peking University, China
3
Shenzhen Research Institute, The Chinese University of Hong Kong, China
4
Dept. of Information Science & Technology, University of International Relations, China
pekingchenwei@pku.edu.cn
Abstract
Microblog has become a major plat-
form for information about real-world
events. Automatically discovering real-
world events from microblog has attracted
the attention of many researchers. Howev-
er, most of existing work ignore the impor-
tance of emotion information for event de-
tection. We argue that people?s emotion-
al reactions immediately reflect the occur-
ring of real-world events and should be im-
portant for event detection. In this study,
we focus on the problem of community-
related event detection by community e-
motions. To address the problem, we pro-
pose a novel framework which include
the following three key components: mi-
croblog emotion classification, community
emotion aggregation and community emo-
tion burst detection. We evaluate our ap-
proach on real microblog data sets. Exper-
imental results demonstrate the effective-
ness of the proposed framework.
1 Introduction
Microblog has become a popular and convenient
platform for people to share information about so-
cial events in real time. When an external even-
t occurs, it will be quickly propagated between
microblog users. During propagation process of
an event, sufficient amount of users will express
their emotions. Taking Sina Weibo
1
as an exam-
ple, more than 12 percent of users use emoticons
2
when reposting an event-related microblog mes-
sage.
The emotion information can not only help us
better understand a given event, but also be u-
tilized to discover new events. Figure 1 shows
1
http://weibo.com/
2
An icon to indicate user?s emotion, as shown in Table 1.
 Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 127?130,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
1 
 
 Coreference Resolution System using Maximum Entropy Classifier 
 
 
Weipeng Chen,Muyu Zhang,Bing Qin  
                                                               Center for Information Retrieval 
Harbin Institute of Technology 
                                     {wpchen,myzhang,bing.qin}@ir.hit.edu.cn 
  
  
 
 
Abstract 
In this paper, we present our supervised 
learning approach to coreference resolution 
in ConLL corpus. The system relies on a 
maximum entropy-based classifier for pairs 
of mentions, and adopts a rich linguisitical- 
ly motivated feature set, which mostly has 
been introduced by Soon et al(2001), and 
experiment with alternaive resolution proc- 
ess, preprocessing tools,and classifiers. We 
optimize the system?s performance for M- 
UC (Vilain et al 1995), BCUB (Bagga and 
Baldwin, 1998) and CEAF (Luo, 2005) .  
1. Introduction 
The coreference resolution is the task in which all  
expressions refer to the same entity in a discourse 
will be identified. As the core of natural language 
processing, coreference resolution is significant to 
message understanding, information extraction, 
text summarization, information retrieval, informa-
tion filtration, and machine translation. 
A considerable engineering efforts is needed for 
the full coreference resolution task, and a signifi-
cant part of this effort concerns feature engineering.  
The backbone of our system can be split into two 
subproblems: mention detection and creation of 
entitly. We train a mention detector on the training 
texts. Once the mentions are identified, coreference 
resolution involves partitioning them into subsets 
corresponding to the same entity. This problem is 
cast into the binary classification problem of decid-
ing whether two given mentions are coreferent. 
Our system relies on maximum entropy-based 
classifier for pairs of mentions. Our system relies 
on a rich linguistically motivated feature set. Our 
system architecture makes it possible to define 
other kinds of features: atmoic word and markable 
features. This approach to feature engineering is 
suitable not only for knowledge-rich but also for 
knowledge-poor datasets. Finally, we use the best-
first clustering to create the coreference chains. 
 
2. System Description  
This section briefly describes our system. First the 
mention detection is presented. Next, the features 
which we import are described. Finally, we de-
scribled the learning and encoding methods. 
2.1 Mention Detector  
The first stage of the coreference resolution 
process try to identify the occurrence of mentions 
in document. To detect system mention from a test 
text, we train a mention detector on the training 
data. We formulate the mention problem as a clas-
sification, by assigning to each token in the text a 
label, indicating whether it is a mention or not. 
Hence, to learn the detector, we create one training 
text and derive its class value (one of b, i, o) from 
the annotated data. Each instance represents the  , 
the token under consideration, and consists of 19 
linguistic features, many of which are modeled af-
ter the systems of Bikel et al (1999) and Florian et 
al. (2004) , as describled below. 
(1) Lexical: Tokens in the windows of  three 
words before and after the target word: 
{     ,?,    }. 
(2) Capitalization: Determine whether    is 
IsAllCaP (all the characters of word are ca-
pitalized, such as ?BBN?), IsInitCap (the 
word starts with a capitalized character, 
127
2 
 
such as ?Sally? ), IsCapPeriod (more than 
one characters of word are capitalized but 
not all, and the first character is not capita-
lized too, such ?M.? ), and IsAllLower (all 
the character of word aren?t capitalized, 
such as ?can? ) (see Bikel et al  (1999)). 
(3) Grammatical: The single POS tags of the 
tokens in the window of three words before 
and after the target word{    ,?,    }. 
(4) Semantic: The  named entity (NE) tag and  
the Noun Phrase tag of  .  
We employ maximum entropy-based classifier, for 
training the mention detector. These detected 
mentions are to be used as system mentions in our 
coreference experiment. 
2.2 Features 
To determine which mentions belong to same en-
titly, we need to devise a set of features that is use-
ful in determining whether two mentions corefer or 
not. All the feature value are computed automati-
cally, without any manual intervention. 
     
(1) Distance Feature: A non-negative integer 
feature capture the distance between anap- 
hor and antecedent. If anaphor and antece-
dent are in the same sentence, the value is 
0; If their sentence distance is 1, the value 
is 1, and so on. 
(2) Antecedent-pronoun Feature: A Boolean 
feature capture whether the antecedent is p- 
ronoun or not. True if the antecedent is a p- 
ronoun. Pronouns include reflexive prono-
uns, personal pronouns, and possessive pr- 
onouns.  
(3) Anaphor-pronoun Feature: A Boolean f- 
eature capture whether  the anaphor is pro-
noun or not. True if the anaphor is a pron- 
oun. 
(4) String Match Feature: A non-negative in-
teger feature. If one candidate is a substrin-
g of another, its value is 0, else the value is 
0 plus the edit distance. 
(5) Anaphor Definite Noun Phrase Feature: 
A Boolean feature capture whether the ana- 
phor is a definite noun phrase or not. True 
if the anaphor is a pronoun. In our definiti- 
on, a definite noun phrase is someone that 
start with the word ?the?. 
(6) Anaphor Demonstrative Noun Phrase F-
eature:  A Boolean feature capture wheth- 
er the anaphor is a demonstractive  noun or 
not. True if the anaphor is a demonstractive  
noun. In our definition, a demonstractive  n 
oun is someone that start with the word, su- 
ch as this, that, those, these. 
(7) ProperName Feature: A Boolean feature. 
True if  anphor and antecedent both are pr- 
oper name. 
(8) Gender Feature: Its value are true, false   
or  unknow. If gender of pair of  instance   
matches, its value is true,else if  the value  
is umatches, the value is false; If one of the 
pair instance?s gender is unknown, the val-
ue is uknown.  
(9) Number Feature: A Boolean feature. True 
if the  number of pair of instance is match-
es; 
(10) Alias Feature: A Boolean feature. True if 
two markables refer to the same entity usi- 
ng different notation(acronyms, shorthands, 
etc), its value is true. 
(11) Semantic Feature: Its value are true, fals- 
e, or unknown. If semantic class relateness 
of a pair instance is the same, or one is the 
parent of other, its value is true; Else if the- 
y are unmatch,the value is false; If one of t- 
he the pair instance?s semantic class is unk- 
nown, the value is unknown. 
 
2.3 Learning   
We did not make any effort to optimize the nu- 
mber of training instances for the pair-wise learne- 
r: a positive instance for each adjacent coreferent 
markable pair and negative training instances for a 
markable m and all markables disreferent with m 
that occur before m (Soon et al,2001). For decod-
ing it generates all the possible links inside a win-
dow of 100 markables. 
Our system integrate many machine learning m 
ethods, such as maximum entropy (Tsuruoka,  200- 
6) , Descision Tree,Support Vector Machine  (Joa- 
chims, 2002) . We compare the result using differ- 
ent method in our system, and decide to rely on m-
aximum entropy-based classifier, and it led to the 
best results. 
2.4 Decoding 
In the decoding step, the coreference chains are 
created by the best-first clustering. Each mention is 
128
3 
 
compared with all of its previous mentions with 
probability greater than a fixed threshold, and is 
clustered with the one hightest probability. If none 
has probability greater than the threshold, the men-
tion becomes a new cluster. 
      
3. Setting and data 
3.1 Setting 
Our system has participated in the closed settings 
for English. Which means all the knowledge re-
quired by the mention detector and feature detector   
is obtained from the annotation of the corpus(see 
Pradhan et al  (2007)), with the exception of Wor- 
dNet.  
 
3.2 Data  
We selecte all ConLL training data and develop-
ment data, contain ?gold? files and ?auto? file, to 
train our final system. The "gold" indicates that 
the annotation is that file is hand-annotated and 
adjudicated quality, whereas the second means it 
was produced using a combination of automatic 
tools. The training data distribution is shown in 
Table 1. 
 
Category bc bn mz nw wb 
Quantity 40 1708 142 1666 190 
  Table 1: Final system?s training data distribution 
 
 
In this paper, we report the results from our dev- 
elopment system, which were trained on the traini- 
ng data and tested on the development set. The de- 
tail is shown in Table 2,3. 
 
Category bc bn mz nw wb 
Quantity 32 1526 128 1490 166 
  Table 2: Experiment system?s training data distribution 
  
 
Category bc bn mz nw wb 
Quantity 8 182 14 176 24 
   Table 3: Experiment system?s test set distribution 
 
 
4. Evaluation  
First, we have evaluated our mention detector mo- 
dule, which is train by the ConLL training data. It 
regards all the token as the candidate, and cast it i- 
nto the mention detector, and the detector decides 
it is  mention or not. The mention detector?s result 
is shown in Table4. 
 
 
Metric R P F 
Value 63.6 55.26 59.14 
Table 4: Performance of  mention detector on the de-
velopment set 
 
Second, we have evaluated our system with the 
system mention, and we use the previous mention 
detector to determine the mention boundary. As fo- 
llow, we list the system perfomance  of using MUC, 
B-CUB,CEAF (E) , CEAF (M) , BLANC (Recasens a- 
nd Hovy, in prep)  in Table 5 . 
 
Metric R P F 
MUC 45.53 47.00 46.25 
BCUB 61.29 68.07 64.50 
CEAF(M) 47.47 47.47 47.47 
CEAF(E) 39.23 37.91 38.55 
BLANC 64.00 68.31 65.81 
Table 5 :Result using  system mentions 
 
 
Finally, we  have evaluated our system with the 
gold mentions, which mention?s boundary is corect. 
The system performance is shown in Table 6: 
 
Metric R P F 
MUC 50.15 80.49 61.78 
BCUB 48.87 85.75 62.62 
CEAF(M) 54.50 54.50 54.50 
CEAF(E) 67.38 32.72 44.05 
BLANC 66.03 78.41 70.02 
Table6:Result using  gold mentions 
 
 
Result of system shows a big difference  betwee- 
n using gold mentions and using system mentions. 
In comparison to the system using system mention- 
s, we see that the F-score rises significantly by 
4.21- 15.53 for the system using gold mentions. It  
is worth noting that the F-scorer when using the B- 
CUB metric, the system using system mention rise- 
129
4 
 
s 2.12 for system using gold mention. Although t- 
his is surprising, in my opinion this correlation is 
because the mention detection recall more candid- 
ate mention, and the BCUB metric is benefit for t- 
he mention which is merge into the erroneous 
chain.  
5. Conclusion 
In this paper, we have presented a new modular 
system for coreference in English. We train a men-
tion detector to find the mention?s boundary based 
on maximum entropy classifier to decide pairs of 
mention refer to or not.  
     Due to the flexible architecture, it allows us ex-
tend the system to multi-language. And if it is ne-
cessary, we can obtain other modules to support 
the system. The results obtained confirm the feasi-
bility of our system. 
 
 
References  
Wee Meng Soon,Hwee You Ng,and Daniel Chung 
Yong Lim.2001.A machine learing approach to core-
ference resolution of noun phrases.Computational 
Linguistic(special Issue on Computational Anaphora 
Resolution),27(4):521-544 
Marc Vilain,John Burger,John Aberdeen,Dennis Con-
nolly,and Lynette Hirschman.1995.A modeltheoretic 
coreference scoring scheme.In Proceedings of the 6th 
Message Understanding Conference,pages 45-52. 
Amit Bagga and Breck baldwin.1998.Algorithms for 
scoring coreference chains.In Proceedings of the lin-
guistic Coreference Workshoop at the International 
Conference on Language Resources and Evalua-
tion(LREC-1998),pages 563-566. 
Xiaoqiang Luo.2005.On coreference resoluton perfor-
mance metrics.In Proceeddings of the Annual Meet-
ing of the North American Chapter of the Association 
for Computational Linguistics-Human Language 
Technology Conference(NAACL/HLY-2005),pages 
25-32 
Josef Steinberger,Massimo Poesio,Mijail A.kabadjov- 
b,and Karel jezek.2007.Two uses of anaphora resolu-
tion in summarization.In Information Processing and 
management,Special issue on Summarization,pages 
1663-1680 
Bikel,R.Schwartz,and R.Weischedel.1999.An algorithm 
that learns what's in a name.Machine Learning,34(1-
3):pages211-231 
Florian,H.Hassan,A.Ittycheriah,H.Jing,N.Kambhatla, X. 
Luo,N.Nicolov,and I.Zitouni.2004.A statistical model 
for multilingual entity detection and tracking.In 
Proc.of HLA/NAACL. 
Sameer Pradhan and Lance Ramshaw and Ralph Wei-
schedel and Jessica MacBride and Linnea Micciulla. 
2007.Unrestricted Coreference: Identifying Entities 
and Events in OntoNotes. In Proceedings of the IEEE 
International Conference on Semantic Computing 
(ICSC), Irvine, CA 
Marta Recasens and Eduard Hovy.in prep.BLAN- 
C:Implementing the rand index for coreference eval-
uation. 
Yoshimasa Tsuruoka.2006.A simple c++ library for 
maxium entropy classifiction.Ysujii laboratory,Dep- 
artment of Computer Science,University of Tokyo. 
Throsten Joachims.1999.Making large-scale SVM 
learning practical.In B.Scholkopf,C.Burges,and A.S- 
mola,editors,Advances in Kernel Methods-Support 
Vector Learning.MIT-Press. 
 
 
 
 
 
 
 
 
 
 
 
 
 
130
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 8?12,
Baltimore, Maryland, USA, June 26, 2014. c?2014 Association for Computational Linguistics
Context-based Natural Language Processing for  
GIS-based Vague Region Visualization 
1,2Wei Chen 
1Department of Computer Science and Engineering, 2Department of Geography 
The Ohio State University, Columbus OH, USA 43210 
chen.1381@osu.edu 
 
 
Abstract 
Vernacular regions such as central Ohio 
are popularly used in everyday language; 
but their vague and indeterministic bound-
aries affect the clarity of communicating 
them over the geographic space. This pa-
per introduced a context-based natural 
language processing approach to retrieve 
geographic entities. Geographic entities 
extracted from news articles were used as 
location-based behavioral samples to map 
out the vague region of central Ohio. Par-
ticularly, part of speech tagging and parse 
tree generation were employed to filter out 
candidate entities from English sentences. 
Propositional logic of context (PLC) was 
introduced and adapted to build the con-
textual model for deciding the member-
ship of named entities. Results were auto-
matically generated and visualized in GIS 
using both symbol and density mapping. 
Final maps were consistent with our intu-
ition and common sense knowledge of the 
vague region.  
1 Introduction 
Central Ohio is commonly used vernacular term 
to refer to an approximate area around the city of 
Columbus in Ohio. Although it may be effortless 
for humans to tell the relative location of this re-
gion, it remains challenging for computers to au-
tomatically locate this region by harvesting and 
analyzing online data such as news articles. Com-
puters that are capable of automatically delineat-
ing such vague regions may be of potential use to 
social science researchers for understanding other 
concepts that may not be as obvious such as cul-
tural regions, the Muslim world.  
     In the study of vague regions, previous studies 
introduced a behavioral method to map out down-
town Santa Barbara based on human survey data 
(Montello, Goodchild, Gottsegen, & Fohl, 2003). 
Their approach collected hand-drawn point-based 
locations and plotted them on the map of the city. 
Such data collection process may be very costly 
compared to computer-based automated ap-
proach. By comparison, natural language pro-
cessing (NLP) techniques such as part of speech 
tagging and parse tree generation provide power-
ful linguistic analysis tools that can help quickly 
retrieve data from a large number of corpus data 
(Jurafsky, Martin, Kehler, Vander Linden, & 
Ward, 2000). However, these NLP techniques 
have yet been widely used to extract geographic 
entities for visualizing vague regions like central 
Ohio.    
     On the other hand, linguistic contexts of named 
entities are important for deciding its relevancy to 
the underlying vague regions. For instance, for a 
place to be part of central Ohio, it must be in the 
context of Ohio as a precondition. Propositional 
logic of context (PLC) is a logic model in the field 
of artificial intelligence for formalizing contexts 
into propositional calculus (BuvaE & Mason, 
1993). Based on PLC, an arbitrary predicate cal-
culus can be evaluated according to selected con-
texts. 
In this paper, central Ohio is chosen as the ex-
perimental area to experiment the context-based 
natural language approach for visualizing vague 
regions. News articles are used and analyzed on 
three contextual levels: document, paragraph and 
sentence. Results are visualized in GIS. 
1.1 News data 
News articles are extracted from LexisNexis, a 
comprehensive database of both national and lo-
cal news (Smith, Ellenberg, Bell, & Rubin, 2008). 
All articles are retrieved based on caseless key-
word match for relevancy. The only keyword used 
is central Ohio and only news articles that contain 
this exact phrase are retrieved. As a result, 3281 
different articles are collected which cover central 
Ohio news from the year 1990 to the year 2013. 
8
1.2 Geonames database 
Geonames database contains names and locations 
of geographic entities. We create our geonames 
database two sources: the United States Geologi-
cal Survey's Geographic Names 
Information Server (USGS, 2013) and Census 
gazetteers (Census, 2013). Only place and feature 
names in Ohio used for analysis. Table 1 summa-
rizes compositions of entities in our Ohio 
geonames database.  
Category Percentages 
Administrative places 
(1054 records) 
23.0% cities 
66.3% villages 
10.6% CDPs (census desig-
nated place) 
Geographic features 
(67804 records) 
14.9% church 
13.7% school 
12.6% populated place 
among 53 categories 
Table 1. Geographic named entities in Ohio 
2 Natural Language Processing 
Part of speech tagging and parse tree generation 
are used to automatically extract geographic 
named entities from news articles in this paper. 
Part of speech (POS) tagging is the process of de-
ciding the functions of words such as nouns or 
verbs. Parse tree generation is based on POS tag-
ging results. It aims to generate hierarchical rep-
resentations of sentences for semantic understand-
ing (Jurafsky et al., 2000). Noun phrases in the 
parse tree are often useful indicators to named en-
tities in geolinguistic analysis (Chen et al., 2013).  
2.1 Part of speech tagging 
Part-of-speech (POS) tagging assigns a POS tag 
to each token in a sentence. A token can be either 
a word or a punctuation. The single best POS tag 
assigned to a token depends on the function of the 
word, the tag set, and POS tagging algorithm 
(Jurafsky et al., 2000). Contemporary POS tag-
gers can reach an average accuracy of above 97% 
on tokens (Manning, 2011).  
The part of speech tagger we use is Stanford 
NLP tagger with english-caseless-left3words-
distsim tagger model. This tagger model is trained 
with WSJ sections 0-18 and extra parser training 
data using the left3words architecture. It includes 
word shape and distributional similarity features 
for training the tagger (Gimpel et al., 2010). The 
results are represented using Penn Treebank tags 
and the average parsing accuracy is above 97% on 
sentences in news. Box 1 is the tagged sentence 
from one article with POS tags appended after the 
slash in uppercase letters. For a complete list, one 
may refer to Penn Treebank tag sets.  
Her/PRP$ friends/NNS at/IN the/DT Central/NNP 
Ohio/NNP Nazarene/NNP Church/NNP Camp/NNP 
she/PRP attended/VBD every/DT summer/NN in/IN Co-
lumbus/NNP convinced/VBD her/PRP to/TO 
attend/VB Mount/NNP Vernon/NNP Nazarene/NNP Col-
lege/NNP in/IN Knox/JJ county/NN ,/, OH/NNP ./. 
Box 1. Tagged sentence 
2.2 Parsing 
Stanford parsers are used to produce the parse tree 
from which noun phrases, named entity candi-
dates, can be extracted (De Marneffe, 
MacCartney, & Manning, 2006) . Fig.1 shows the 
result of parsing the tagged sentence in Box 1. It 
is observed that only noun phrases (NP) at the 
lowest level of the tree are useful for extracting 
named entities. Noun phrases at other levels con-
tain auxiliary structures such as prepositions often 
do not suggest named entities.  
 In Fig.1, NPs in dashed rectangles are candi-
date entities that do not match any records in our 
Ohio database. When looking up the database for 
a match, determinants like the are skipped as well 
as entity type terms like city and county. To find 
the location of a matched entity, a SQL query is 
used to return the latitude and longitude pair.  
 
Figure 1. Parse tree of tagged sentence in Box 1 
3 Geographic Information Retrieval 
3.1 Propositional logic of context (PLC) 
As previously discussed, candidate named entities 
are primarily noun phrases extracted at the root 
level of a parse tree. However, not all such entities 
should be considered as part of central Ohio. To 
determine the membership, we may define fol-
lowing logic heuristics: if (1) the name of an entity 
is in the same text segment as the phrase central 
Ohio and (2) the entity is an Ohio place, then the 
entity is of greater likelihood of being a central 
Ohio place than otherwise. Here, Ohio and central 
9
Ohio are linguistic contexts for discriminating 
central Ohio entities. 
     To formalize the contexts of analysis, we intro-
duce propositional logic of context (PLC) (BuvaE 
& Mason, 1993). Here, we only adapt its basic 
form as it already suffice the needs of our analysis. 
For detailed information of PLC, one may read the 
original paper from BuvaE (BuvaE & Mason, 
1993). The minimum PLC definition is below: 
x:   subject 
p:   preposition about the subject 
c:   context 
c1?c2:  logic AND, intersection of two contexts 
c1?c2:  logic OR, union of two contexts 
ist(c, p): the proposition p is true in context c.  
3.2 PLC-based matching and counting 
Based on the PLC definition, we count the men-
tions of named entities in all news articles.      
Here, we define the following PLC notations for 
our analysis:  
p: the preposition that x is a central Ohio city 
c1: the context of Ohio 
c2: the context of central Ohio 
c3: the context of not-central Ohio 
     Ohio context is defined according to records in 
geonames database. If an entity name is in the da-
tabase, it is said to be in the Ohio context. Central 
Ohio context is defined as the text segment con-
taining both the entity name and the phrase central 
Ohio. Not-central Ohio context is defined as the 
text segment with the following terms in it: 
north(ern) Ohio, northeast(ern) Ohio, east(ern) 
Ohio, southeast(ern) Ohio, south(ern) Ohio, 
southwest(ern) Ohio, west(ern) Ohio, and north-
west(ern) Ohio. Based on our observation, these 
eight azimuth phrases are found to be good indi-
cators of places that are obviously not in central 
Ohio.  
Accordingly, three types of entity counts are 
also developed.  
(1) Positive count (E): the total number of occur-
rences of the name of an entity E in the context 
c1?c2.  
(2) Neutral count (E): the total number of occur-
rences of the name of an entity E in the context 
c1??c2??c3.  
(3) Negative count (E): the total number of occur-
rences of the name of an entity E in the context 
c1?c3.  
3.3 Count and normalization 
We calculate the membership of an entity to the 
concept central Ohio using following counting 
and normalization rules. We define three variables 
to count entity occurrences in different contexts:  
C??? : positive count of the entity E. 
C??? : negative count of the entity E.  
C??? : neutral count of the entity E. 
IF ist(c1?c2, p), C???++. 
IF ist(c1?c3, p), C???++. 
IF ist(c1??c2??c3, p), C???++. 
Based on observations, big cities like Colum-
bus are mentioned more frequently than other 
smaller places in term of both C??? and C???. As 
it is the difference between C??? and C???that de-
termines the sign of the membership, we decide to 
use C???  as the normalization denominator for 
calculating the membership.  
Membership r of a place is calculated using 
Equation 1. It is a real value between -1 and 1. All 
places are classified by the sign of the member-
ship as either central Ohio or not-central Ohio 
place with the magnitude of the value being the 
strength of the membership. 1 means definitely a 
central Ohio place and -1 means definitely not a 
central Ohio place. 
? = {
  (C??? ? C???) C????    , if C??? > 0
0                                        , ?????????
  ???????? 1 
    As C??? is in the denominator, it must not be 
zero. Given observations, entities with C??? being 
zero are primarily entities with less than 3 total 
mentions. These entities take up 3.9% of all ex-
tracted entities. Therefore, we decide to exclude 
them from analysis as they are of a small percent-
age and are not expected to affect the overall re-
sults.  
4 Results and discussions 
Geographic entities are extracted from all 3281 
news articles and their membership values are 
mapped using the geographic information system 
(GIS) software ArcGIS which are popular in so-
cial science geographic research. 
4.1 Graduated symbol maps 
Graduated symbol map is a type of map that uses 
symbols of different sizes to represent geographic 
entities (Thrall, 1999). The symbol we choose is 
circle. The radius of the circle is decided by the 
attribute value associated with each entity. The 
map is configured as follows: 
(1) The size of each point is proportioned to the 
membership of the underlying named entity 
with size 4 and 24 representing the minimum 
and maximum membership respectively.  
(2) Symbols are classified into 10 classes based on 
equal interval classification method.  
10
There is one exception of using the member-
ship for making the graduated symbol map. On 
the article level, all entity counts are added to 
C???, and therefore there are no negative or neu-
tral counts. To make a map on the article level, we 
only use the positive count as the surrogate to the 
membership value. 
     Graduated symbol maps on three analytical 
levels are shown in Fig. 2. Results on the sentence 
level and paragraph levels conforms better to our 
intuition and common sense knowledge than on 
the article level. This is because results on the ar-
ticle level do not consider the contexts of c1 and c2 
discussed in section 4.2. Results from the sentence 
and paragraph levels are very similar with the 
membership on the paragraph level being slightly 
more visually significant.  
 
Figure 2. Graduated symbol map of central Ohio 
4.2 Kernel density map  
Results produced by graduated symbol maps are 
not continuous.  Kernel density mapping is a GIS 
mapping technique that generates a continuous 
surface based on the locations of the entities and 
their attribute values (Elgammal, Duraiswami, 
Harwood, & Davis, 2002). To create kernel den-
sity maps, a search radius need be defined. All 
data points within this radius will be used to inter-
polate a density area using a quadratic kernel 
function described in Silverman (p. 76, equation 
4.5) (Silverman, 1986).  
The kernel density tool in ArcGIS is used to 
create the density map. In ArcGIS, the search ra-
dius is defined as a percentage of the area?s mini-
mum extent width. We experiment on choosing 
1/10, 1/5, 1/20 of the area?s minimum extent 
width as the radius to generate the surface and find 
1/10 of the width most appropriate to generate a 
balanced looking map.  
     A kernel density map of central Ohio visual-
izes its estimated central location and extending 
trend over the space of Ohio. Fig. 3 is a kernel 
density result based on the paragraph level. It 
shows that the concept of central Ohio generated 
through automated approach conforms to our 
common sense knowledge of the assumptive loca-
tion of the vague region. 
 
Figure 3. Kernel density map of central Ohio 
5 Conclusions 
Vague geographic regions are important part of 
the entire geographic space; however they are dif-
ficult to be located and delineated on a map. Geo-
graphic questions like Where is central Ohio? re-
mains a challenge to computers because comput-
ers are not automatically given the knowledge of 
either central or Ohio as humans do.  
This paper introduced a context-based ap-
proach to extract geographic entities from news 
articles. Propositional logic of context was 
adapted to contextualize the reasoning process. 
Three types of context have been defined: Ohio, 
central Ohio, not-central Ohio, which corre-
sponded to Ohio places, central Ohio places and 
not-central Ohio places, respectively.  
Analysis was conducted on three contextual 
levels: article, paragraph and sentence. Visuali-
zation results showed that context was of signifi-
cant importance to deciding the membership of a 
place to central Ohio. Without defining the con-
text (e.g. results on the article level in Fig. 2), vis-
ualization results were largely incorrect compared 
with common sense knowledge. 
     Natural language processing (NLP) techniques 
such as part of speech tagging and parse tree gen-
eration were shown to be effective for extracting 
geographic information. Noun phrases could 
serve as good candidates to place names. For fu-
ture research, we suggest studies on experiment-
ing with different regional concepts using pro-
posed approach. It may also be useful to experi-
ment with methods that can quickly generate sam-
ples other than the tree parsing method used in this 
paper. Despite the possibility of generating more 
coarse results, noisier method may be more scala-
ble for building practical applications with scaled 
live data.  
 
11
Acknowledgements 
The author would like to thank Dr. Xiang Chen, 
Dr. Zhe Xu, Dr. Lili Wang, Dr. Xueying Zhang, 
Dr. Bo Zhao, Dr. Ningchuan Xiao and two other 
anonymous reviewers for their valuable com-
ments and suggestions for improving the paper. 
Presentation of the work was supported by the re-
search data and computing center of the research 
institute at the Nationwide Children?s Hospital. 
Reference 
BuvaE, Saga, & Mason, Ian A. (1993). Propositional 
logic of context. Paper presented at the Proceedings 
of the eleventh national conference on artificial 
intelligence. 
Census. (2013). U.S. Gazetteer Files. from 
http://www.census.gov/geo/www/gazetteer/files/Ga
z_places_national.txt 
Chen, Wei, Fosler-Lussier, Eric, Xiao, Ningchuan, 
Raje, Satyajeet, Ramnath, Rajiv, & Sui, Daniel. 
(2013). A Synergistic Framework for Geographic 
Question Answering. Paper presented at the 
Semantic Computing (ICSC), 2013 IEEE Seventh 
International Conference on. 
De Marneffe, Marie-Catherine, MacCartney, Bill, & 
Manning, Christopher D. (2006). Generating typed 
dependency parses from phrase structure parses. 
Paper presented at the Proceedings of LREC. 
Elgammal, Ahmed, Duraiswami, Ramani, Harwood, 
David, & Davis, Larry S. (2002). Background and 
foreground modeling using nonparametric kernel 
density estimation for visual surveillance. 
Proceedings of the IEEE, 90(7), 1151-1163.  
Gimpel, Kevin, Schneider, Nathan, O'Connor, 
Brendan, Das, Dipanjan, Mills, Daniel, Eisenstein, 
Jacob, . . . Smith, Noah A. (2010). Part-of-speech 
tagging for twitter: Annotation, features, and 
experiments: DTIC Document. 
Jurafsky, Dan, Martin, James H, Kehler, Andrew, 
Vander Linden, Keith, & Ward, Nigel. (2000). 
Speech and language processing: An introduction to 
natural language processing, computational 
linguistics, and speech recognition (Vol. 2): MIT 
Press. 
Manning, Christopher D. (2011). Part-of-speech 
tagging from 97% to 100%: is it time for some 
linguistics? Computational Linguistics and 
Intelligent Text Processing (pp. 171-189): Springer. 
Montello, Daniel R, Goodchild, Michael F, Gottsegen, 
Jonathon, & Fohl, Peter. (2003). Where's 
downtown?: Behavioral methods for determining 
referents of vague spatial queries. Spatial Cognition 
& Computation, 3(2-3), 185-204.  
Silverman, Bernard W. (1986). Density estimation for 
statistics and data analysis (Vol. 26): CRC press. 
Smith, Michael J, Ellenberg, Susan S, Bell, Louis M, 
& Rubin, David M. (2008). Media coverage of the 
measles-mumps-rubella vaccine and autism 
controversy and its relationship to MMR 
immunization rates in the United States. Pediatrics, 
121(4), e836-e843.  
Thrall, Susan Elshaw. (1999). Geographic information 
system (GIS) hardware and software. Journal of 
Public Health Management and Practice, 5(2), 
82&hyhen.  
USGS. (2013). Geographic Names Information Server. 
from http://geonames.usgs.gov/index.html 
 
 
12
