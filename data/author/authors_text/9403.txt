SpokenDialogueforVirtualAdvisersinasemi-imme rsiveCommand
andControlenvironment
DominiqueEstival,MichaelBroughton,AndrewZschor n,ElizabethPronger
HumanSystemsIntegrationGroup,CommandandContro lDivision
DefenceScienceandTechnologyOrganisation
POBox1500,EdinburghSA5111
AUSTRALIA
{Dominique.Estival,Michael.Broughton,AndrewZscho rn}@dsto.defence.gov.au


Abstract
We present the spoken dialogue system
designed and implemented for Virtual
Advisers in the FOCAL environment. Its
architectureisbasedon:DialogueAgents
using propositional attitudes, a Natural
Language Understanding component
using typed unification grammar, and a
commercial speaker-independent speech
recognition system. The current
application aims to facilitate the multi-
media presentation of military planning
information in a semi-immersive
environment.
1 Introduction
In this paper, we present the spoken dialogue
system implemented for communicating with the
virtual advisers (VAs) in the Future Operations
Centre Analysis Laboratory (FOCAL) at the
Australian Defence Science and Technology
Organisation(DSTO).Weareexperimentingwith
the use of spoken dialogue with virtual
conversational characters to access multi-media
information during the conduct of military
operations and in particular to facilitate the
planningofsuchoperations.
Unlike telephone-based dialogue systems
(Estival, 2002),whicharemainlycreated fornew
commercial applications, dialogue systems for
Command and Control applications (Moore et al
1997) generally seek to simulate the military
domain and therefore require an understanding of
thatdomain.
2 UsingVirtualAdvisersinFOCAL
FOCAL was established to "pioneer a paradigm
shiftincommandenvironmentsthroughasuperior
useof capability andgreater situationawareness".
The facility was designed to experiment with
innovativetechnologiestosupportthisgoal,andi t
hasnowbeenrunningfortwoyears.
FOCAL contains a large-screen, semi-
immersive virtual reality environment as its
primary display, allowing vast quantities of
informationtobedisplayed. OurcurrentVAscan
be described as 3-dimensional "Talking Heads",
i.e. only the head and upper portions of the body
are represented. Theycandisplayexpression, lip-
synchronisation and head movement, along with
certain autonomous behaviours such as blinking
and gaze (Taplin et al, 2001). These factors all
combinetoaddlife-likenesstotheVAsandcreate
moreengaginginteractionwithusers.
Presenting information via aTalkingHeadhas
been commercially demonstrated by the virtual
newscaster ?Ananova? (Ananova, 2002).
Embodiedcharactersarealsobeingdevelopedand
include the PPP (Andre, Rist and Muller, 1998)
and Rea (Cassell, 2000).  PPP is a cartoon style
Personalized Plan-based Presenter that combines
pointing, head movements and facial expressions
to draw the viewer?s attention to the information
beingpresented. Rea isa virtual real-estateagen t
that takesanactive role inconversation, shenods
herheadtoindicateunderstandingofspokeninput,
orcanraiseherhandtoindicateadesiretospeak .
Several VAs have been implemented for
FOCAL, each having a particular role or
knowledge expertise.  For example, one adviser
may have specialist knowledge relating to legal
issues, another may have information relating to
the geography of a region.  Each VA has a
differentfacialappearance,voiceandmannerisms.
To demonstrate and evaluate the performance
of VAs (and of the other FOCAL projects), a
fictitious scenario has been developed that
incorporates key elements of military planning at
the operational level (see section 8).  The VAs
provide information rich briefs through the
combineduseofspokenoutputviaText-to-Speech
(TTS)andmultimedia.  Relevantquestionscanbe
asked at the end of the briefs through the use of
spokendialogue.
3 Previousimplementation:Franco
Asdescribedin(Taplinetal.,2001)thefirstVA in
FOCAL, named Franco, was also an animated 3-
dimensional "Talking Head" model, intended to
either deliver prepared information, such as a
briefing or slide show, or to interact
conversationally with users. To demonstrate the
conversational functionality (Broughton et al,
2002), it was implemented with a commercial
speaker-dependent automated speech recogniser
(ASR),DragonNaturallySpeaking?.TheNatural
Language understanding component was
implemented in NatLink (Gould, 2001) and a
simpleuser-drivendialoguemanagement,basedon
key-word recognition and nesting of dialogue
statestoprovidecontext,wasalsoimplementedin
Python.
Franco has been successful in demonstrating
the proof-of-concept of a VA in the FOCAL
environment.  Answering spoken questions about
specific military assets and platforms, it also
permits the display of other types of information
such as pictures, animated video clips, tabular
information from a database, and location details
ondigitalmaps.
4 Improvements
Although Francowas successful in demonstrating
the potential usefulness of a VA in a Command
andControlenvironment foroperationalplanning,
it suffers from certain limitations which we are
nowaddressinginafollow-upproject.
The first limitation, and the easiest to remedy,
was the unnaturalness of the synthetic voice we
had given Franco.  For greater effectiveness, we
had to provide ourVAwith amore natural voice
andwithanAustralianaccent. Wechosethenew
Australian TTS voice from Rhetorical, developed
byAppen (rVoice,2002).   This requiredmaking
somechanges, some of them relatively important,
to the interface with the talking head model to
achieve lip-synchronisation, but that aspect of the
workwillnotbeaddressedinthispaper.
The second limitation was the relative rigidity
of the dialogue management strategy we were
using.  The alternative approach we have
developed is to create Dialogue Agents
implemented in A TTITUDE.  This is described in
section6.
The third limitation was due to the speaker-
dependent nature of the ASR.  While a speaker-
dependent ASR allows greater flexibility in the
input towhich theVAcanrespond,wewantedto
develop a system which could not only be
demonstratedby the fewpeoplewhohave trained
the speech recogniser, but where visitors
themselvescouldbeparticipantsandcouldinteract
with theVA. Switching toa speaker-independent
ASR led us to radically modify our Spoken
Language Understanding component, and this is
describedinsection7.
The new implementationwe describe here has
allowed us not only to address those three
limitations, but also to alter fundamentally the
architectureofthesystem,openingupthedialogue
managementcomponentstocontrolandinteraction
by other tools and agents in the FOCAL
environment.  The resulting system is now fully
modular and provides scalability as well as
flexibility.
Thisnewimplementationallowsustofocusour
research into dialogue management issue, to
investigate the use of A TTITUDE  for dialogue
management and to experimentwithmore natural
languageinput.
5 Integration
Communication between the various components
ofthesystem(speechrecogniser,dialoguecontrol,
virtual adviser control andmultimedia display) is
nowachievedwith theCoABS (ControlofAgent
Based Systems) Grid infrastructure (Global
InfoTek,2002).TheCoABSGridwasdesignedto
allowalargenumberofheterogeneousprocedural,
object-oriented and agent-based systems to
communicate.  Using the CoABS Grid as our
infrastructure has allowed us to integrate all the
components of the dialogue system and it will
provideaneasywaytointegrateotheragentsanda
variety of input and output devices.
Communication between CoABS agents is
accomplishedviastringmessages.
6 DialogueManagementwithA TTITUDE
ATTITUDE  is a multi-agent architecture developed
at DSTO, capable of representing and reasoning
both with uncertainty and about multiple
alternative scenarios (Lambert, 1999).  It is a
multi-agent extension of the MetaCon reactive
planner developed for control of phased array
radars on the Swedish Airborne Early Warning
aircraft(LambertandRelbe,1998). A TTITUDE has
some similarities with Prolog and other logic
programming languages as well as with AI
research on blackboard and multi-agent
architectures.  Because A TTITUDE  was designed
specificallytosupporttheprogrammingofreactive
systems, it possesses powerful facilities for
handling interactions of the internal system
entities,bothwitheachotherandwiththeexterna l
world.
ATTITUDE isveryhigh-level,weakly-typed,and
thanks to the agent paradigm, it produces loosely
coupled and modularised systems. For these
reasons, and because A TTITUDE  implements
reasoningabout propositionalattitudes ,itprovides
a very attractive framework in which to develop
and express dialogue management control
strategies.  It is worth emphasizing here that
ATTITUDE is not merely a notation to represent
speech acts or  communicative acts between
agents, but that it is actually the programming
language and environment in which both the
agents themselves and the control structure for
interaction between the agents are implemented
andexecuted.
BecauseA TTITUDE hasneverbeenusedforthis
purpose before, this is an interesting area of
research in itself, and one of the goals of the
projecthasbeentoseehowA TTITUDE needstobe
extended to implement dialogue management.
Further, this allows us to investigate how far
attitude programming  (see section 6.2) can go
towardsexpressingspeechactsandcommunicative
acttype. However,wedonotclaimtoemploythe
full power of propositional attitudes in our
implementation yet. This is another area of
researchwhichwearenowexploring. Neitherare
we yet at the stage where we could perform
automatic detection of utterance type (Wright,
1998) or of dialogue act (Carberry and Lambert,
1999;PrasadandWalker,2002).
6.1 Propositionalattitudes
The A TTITUDE  programming environment is so
named because it utilises propositional attitude
instructions  as programming instructions (this has
beendubbed attitudeprogramming ).  Propositional
attitudesareallegedmentalstatescharacterisedb y
propositional attitude expressions, which are the
means by which individuals relate their own
mentalbehaviourtoothers'.
Propositional attitude instructions are of the form
shownin(1).

(1)[subject][attitude][propositionalexpression]

In(1):
-[subject]denotesthe individualwhosemental
stateisbeingcharacterised;
- [propositional expression] describes some
propositionalclaimabouttheworld;and
- [attitude]expresses the subject'sdispositional
attitudetowardthatclaimabouttheworld.
6.2 ATTITUDEprogramming
When software agent Mary  encounters the
propositionalattitudeinstruction" Fred  desire [the
door is closed]", Mary  will issue a message to
softwareagent Fred instructing Fred  todesirethat
the door be closed. Similarly, when encountering
thepropositionalattitudeinstruction" I believe [the
sky isblue]", Mary  herselfwillattempt to believe
thattheskyisblue.
An important characteristic of A TTITUDE
programming is that each propositional attitude
instruction either succeeds or fails, possibly with
sideeffects,dependinguponwhetherthe recipient
agentisabletosatisfytheinstructionalrequest. As
each propositional attitude instruction either
succeeds or fails, the execution path selected
through a network of propositional attitude
instructions (routine) is determined by the
successesandfailuresof thepropositionalattitud e
instructionsattemptedalong theway. Thecontrol
structure is therefore governed by a semantics of
success.
Computational routines for a software agent
arise by linking together particular choices of
propositional attitude instructions.Thesenetworks
ofpropositionalattitudeinstructionsthenprescri be
recipesdefiningthepossiblementalbehaviourofa
softwareagent.
6.3 The  ATTITUDEDialogueAgents
We have implemented a number of A TTITUDE
DialogueAgents. ThemainagentinourDialogue
Management architecture (shown in Figure 1) is
the Conductor.  It is the agent responsible for the
flowofinformationbetweentheotheragentsandit
manages multi-modal interactions. The other
agents, also described further in this section, are
the Speaker, the NLG (Natural Language
Generator), the MMP (Multimedia Presenter) and
several IS(InformationSource)agents.Inaddition
to theseagents,eachdialoguestate (seesection8 )
isalsoimplementedasanA TTITUDE agent,withits
ownsetofroutines.
As explained in section 6.2, each A TTITUDE
agent?s behaviour is programmed as a set of
routines

Figure1.DialoguewithA TTITUDE

The interaction between the A TTITUDE
DialogueagentsisshowninFigure1,inwhichthe
frame around the A TTITUDE  agents can be
interpretedasrepresentingtheCoABSgrid.
SpeakerAgent
Whenspeechfromtheuserhasbeendetectedand
recognised, the attribute-value pairs for that
utterance (see section 7) are sent to Speaker.
Speaker takes that information and produces a
correspondingA TTITUDE expression,whichisthen
forwardedto Conductor.
The linguistic coverage of the system is
determinedbythegrammarswhichareavailableat
each dialogue state.  For now, the coverage is
limited to a set of utterances appropriate for the
briefing scenario described in section 8.  These
were used to define the Regulus1 grammars from
whichtheNuancegrammarsarecompiled.Weare
nowplanningtomovefromRegulus1toRegulus2,
which will allow us to derive dialogue state
grammarsfromalargeEnglishgrammarusingthe
EBLstrategydescribedin(Rayneretal.,2002b)








Conductor
Thisagentisresponsiblefordialogueflowcontrol andallothe  rdialogue
agentsmustregisterwithit.

Conductor

receivescommunicativeactsfrom
 Speaker

.Forexample:

( whquestion  (property mig- 29flying -range?value?units))
Thisqueryisforwardedontoallregisteredagents .
 Conductor

choosesthe

mostappropriateresponsereceivedandsendsthist o
 MMP

topresentthe

answer.
Speaker
Speakerreceivesspeechrecognitionresultsinthe form

ofattribute

-

valuepairs,andtranslatestheseinto

Attitudeexpressionstosendto
 Conductor

.

InformationSource
(IS)Thiscategoryofagentseachregisterwith  Conductor  and interfacewithabackgrounddatasource,forexampl e,a
databaseofaircraftproperties.

Eachusestheirdatasourcetorespondtoqueriesf rom

Conductor

.

MultimediaPresenter
(MMP)Thisagentreceivesalistofexpressionsfrom  Conductor anddirectstheappropriateservicestopresent multimediadatatotheuser.Forexample:

((whanswer (property mig -29flying - range810nautical - miles))(image mig- 29))
Inthiscase,
 MMP

requestsanEnglishformofthe
 whanswer expressionandsendstheresulttotheTTS
application.Similarly,anappropriateapplication isdirectedt

odisplaytherequestedimage.

NaturalLanguageGenerator
(NLG)Receivesexpressionsfrom  MMP andusestemplatestoreturn correspondingEnglishsentences.

Englishquestionfromuser
Nuance/Regulus

Attitudeexpression

Query

Response

Presentationdirectives

NLGdirective
TTS
Englishstring

VirtualAdvisorspeaking

Englishstring

Attribute/Valuepairs
Multimediadisplayed
ConductorAgent
Conductor  takes an A TTITUDE  expression from
Speakerandforwardsitontoallthe IS agentsthat
have registered with it.  It then waits for all the
responses to come back from those agents, in the
formoflistsofexpressions.
Every response Conductor  receives is put into
its knowledge base, along with some extra
information:
-Sender:whichISagentsenttheresponse.
- In-Reply-To: which previous communicative
actthisisaresponseto.
- Strength: whether every expression of the
response is 'strong' (the sender believes it is
either absolute truthor absolutenegation)or if
oneormore is 'weak' (the senderbelieves it is
neitherabsolutetruthnorabsolutenegation).
-Bound-State: if thereareanyfreevariablesin
theresponse,orifitisfullyground.
- Unifiability: whether one or more of the
expressionsintheresponseisofthesameform
as Speaker?s initial expression.
The final expression in Conductor?s knowledge
baseisasshownin(2).

(2)(response?in-reply-to?sender?strength
   ?bound_state?unifiability?content)

Given the initial expression from Speaker and the
replies it receives from the IS agents, Conductor
chooses the 'best' response. For example, a
response that is strong, fully ground and unifies
with Speaker?s expression is deemed to be more
relevant and informative than a response that is
weak and contains free variables.  Conductor
forwardsthisresponseto MMP.
MultimediaPresenter(MMP)
MMP  iterates through the list of expressions sent
by Conductor  andpresents eachexpression to the
user.  MMP recognises classes of expressionsand
chooses topresent themusingcertainmedia.  For
example, some expressions are instructions to
changetheVAheadmodel,whileothersaretobe
translatedintoEnglishsentencesandspokenbythe
VA. For the latter function MMP  uses NLG (see
below).
Othermediathroughwhich MMP canchooseto
present the information contained in the
expressionsinclude:imageryfromadatabase(e.g.
pictures of military platforms, or of strategic
locations), video clips, images from weather or
radar information sources, virtual video, 3-
dimensional virtual battle space maps, textual
informationandaudio.
NaturalLanguageGenerator(NLG)
For now, NLG  uses templates to transform
ATTITUDE expressionsintoEnglish.  Forexample,
the instruction in (3) provides two possible
responsesfortheA TTITUDE expressionspecified: 1

(3)(property?assetoverview?valuetext)
  whanswerpriority10
 ((response1("The"?asset"isa"?value".") )
 ((response2("Iunderstandthatthe"?asset "isa
   "?value"."))))

When NLG  is first requested to generate the
Englishoutputfortheexpressionin(4.a),intende d
to be a communicative act of type whanswer, it
uses the template given in (4.b), corresponding to
"response1"in(3), toproducetheEnglishanswer
givenin(4.c).

(4.a)(propertymig-29overview"Russianmulti-role
   fighter"text)
b.("The"?asset"isa"?value".")
c.TheMig-29isaRussianmulti-rolefighter.

When NLG  isrequestedasecond time togenerate
the output for (3), it uses the template in (5.a),
corresponding to "response 2" in (3), to produce
theEnglishanswergivenin(5.b).

(5.a)("Iunderstandthatthe"?asset"isa"?valu e".")
b.IunderstandthattheMig-29isaRussianm ulti-
rolefighter.

Thus NLG  cycles through the list of templates for
appropriateresponses. Prioritiescanalsobegive n
to templates, enabling NLG  to use general
templates togetherwithmore specificand tailored
ones.
It is clear that template-based language
generation is too rigid for fully natural dialogues ,
andweintendtoexploremoreflexible techniques
after we implement a wider coverage English
grammar;however,ithassofarbeensufficientfor
                                                         
1Variablesaredenotedwith"?",whiletextstrings (tobesent
tospeechsynthesis,ordisplayedonaslide)areb etween
doublequotes,"".
our purposes, namely to demonstrate and
investigateagent-baseddialoguemanagement.
InformationSourceAgent(IS)
The ISagents,e.g.aWeatherAgentor aPlatform
Capabilities Agent, can answer users' questions,
eitherby using their own internalknowledgebase
orbyaccessingexternalInformationSources,such
as a weather information server, or a database of
military assets.  All IS agents register with
Conductor, and when an expression is sent by
Speaker,all IS agentstrytorespondtoit.

ByusingtheCoABSGridastheinfrastructureand
implementing the agentwithA TTITUDE, we leave
the architecture extremely flexible and scalable
(Kahn and Della Torre Cicalese, 2001). For
instance, it is possible to increase the amount of
information at the system?s disposal during run-
time by launching a new IS agent and by adding
sometemplatesto NLG.
6.4 Dialoguedesign
Fornow, thedialogue is specifiedasa finite stat e
machineandisstillverymuchsystemdirected. In
thebriefingapplication (seesection8.1), theVAs
first "push" the information that needs to be
presented, as briefing officers do in a normal
briefing.Someoftheinformationisalsopresente d
using visual aids, such as power point slides and
maps for specifying location  information.  The
information to be presented and the media to be
usedaredeterminedbytheagentforthatparticula r
dialoguestate.
The VA then allows users to ask questions to
repeat or clarify particular points, or to gain
additionalinformation.
7 SpokenLanguageProcessing
7.1 Speaker-independentspeechrecognition
Asstatedinsection4,oneofthemainmotivations
formovingfromaspeaker-dependenttoaspeaker-
independentASRwastoallowvisitorsinFOCAL
the possibility of using the system themselves,
rather than relying on a small set of trained
individuals to run demonstrations.  We chose to
usetheNuanceToolkit(Nuance,2002)forseveral
reasons:  besides its reliability as a speaker-
independent ASR for both telephone and
microphone speech, Nuance 8.0 provides
Australian-New Zealand English, as well as US
andUKEnglish,acousticlanguagemodels.  Even
more importantly for our purposes, Nuance
grammarscanbecompiledfromRegulus,ahigher-
level language processing component which has
already been used to develop several spoken
dialogue systems in different domains (Rayner et
al.,2001,RaynerandBouillon,2002).
7.2 SpokenLanguageUnderstanding
Following our decision to move from a speaker-
dependent to a speaker-independent ASR, we
decided to use Regulus to implement our Natural
Language Understanding component.  Regulus is
an Open Source environment which compiles
typed unification grammars into context-free
grammar language models compatible with the
Nuance Toolkit.  It is "written in a Prolog-based
feature-value notation and compiles into Nuance
GSLgrammars."(Rayneretal.,2002a).  Regulus
isalsodescribedindetailin(Rayneretal.,2001 ).
The main motivation for using Regulus is the
usual one of greater efficiency due to the more
compact nature of a unification grammar
representation compared with a context-free
grammar.  In addition, using Regulus to define a
higherlevelgrammar,weareabletoobtainasour
semantic representation a list of attribute-value
pairs, and this permits a more sophisticated
processingoftheinformationbytheotheragents.
Regulus also allows the development of bi-
directional grammars, and we intend tomake use
ofthisfunctionalityinlaterimplementationsoft he
NLG  agent. However, fornow, thegrammarswe
have developed have been limited to recognition
andunderstanding.
8 Currentapplicationimplementation
8.1 Dialoguescenario
The scenario for the current application was
developed by members of the Human Systems
Integration (HSI) group and is grounded on their
experience with, and observations of, military
operational planning.  It is based on a fictitious
scenario developed for training (the examples
givenherehaveallbeenmodified)andexemplifies
theJointMilitaryAppreciationProcess(JMAP)for
militaryplanningacross the three services (Army,
Navy and Air Force). A sub-scenario was chosen
for the development of the spoken dialogue with
theVAs. 2
8.2 Dialogueflow
The structured nature of a military planning task
such as this onemakes it very easy to partition it
intodifferentstages,whichcanthenbemappedto
different dialogue states. In our dialogue script,
each top-level dialogue state corresponds to a
sectionoftheplanningexercise,givenin(6).

(6)Commander'sInitialGuidance
-CDF(ChiefofDefenceForces)Intent
-PlanningGuidance
-Constraints
-Restrictions
-LegalIssues
-CommandandControl

These6topleveldialoguestatesarethenfollowed
byanOverallQuestionTime.
The mixed-initiative nature of the system can
bemodelledinafinitestatediagram,allowingfor
a) briefing-like system ?pushes?, b) confirmation
queriesfromthesystemandc)questionsfromthe
user.   However, because the system is primarily
agent-based, the dialogue can also evolve
dynamically. Forinstance,oncethesystemisina
?question? state,  the dialogue flow then allows
users to ask anumberofquestions,until they are
satisfied,and thedialoguecanmove toadifferent
state.
Each of the top level dialogue states also
corresponds to an IS agent with its own set of
ATTITUDE  routines.  These agents register with
Conductor  and act as experts in their particular
fields (e.g., the Legal Issues adviser).  Theagent s
contain knowledge which they use to answer
questionsposedtothemby Conductor. Allagents
have the ability to keep track of which state (or
topic)theyarein.Thisallowsnotonly Conductor,
but also the other dialogue agents, to distinguish
between providing the user with new information
orinformationthathasalreadybeenpresented.
                                                         
2
ThisistheCommander?sinitialguidancetotheTh eatre
PlanningGroup(TPG),whichispartoftheMission Analysis
sectionofJMAP.
8.3 KnowledgeRepresentation
Thecurrentontologydevelopedforthisapplication
is only a small part of the larger Knowledge
Representationontologytobeusedthroughout the
whole FOCAL system.  For now, we only
representtheconceptsneededinoursmalldomain,
and their relationships are translated into
ATTITUDE  statements, allowing agents to draw
inferences.    For example, if a user can ask the
question given  in (7.a), it will be translated int o
the listof  attributevaluepairsgiven in (7.b)a nd
sent to Speaker. Speaker then translates these
attributevaluepairsinto theA TTITUDE expression
in(7.c)andforwardsitonto Conductor.

(7.a) Whatdepartmentoverseesnegotiations
withunionsandindustry?
b. [questionwhatquestion,concept
negotiation,attributeoversee,obj1department]
c. conductordesire(comm_act(negotiation
oversee?department)fromspeakertype
whatquestionin-response-tonull)

As described in section 6, when Conductor  poses
thequestiontotheappropriateagents,theyrespon d
with the information in their knowledge base or
information they can extract from a database.
Agentsstoreknowledgeas believe statementssuch
astheoneshownin(8):

(8) Ibelieve(negotiationoversee?department
ofworkplacerelations?)

These believe  statementsarethenunifiedwith the
propositions translated by Speaker, and if
unification is successful, a reply is sent back to
Conductor.  Finally, Conductor  passes the answer
on to NLG  to match a template and produce an
Englishanswer,forinstance(9).

(9) TheDepartmentofWorkplaceRelations
overseesnegotiationswithunionsandindustry.

An agentwhich has access to a databasecan also
translate a user's question into the relevant
databasequerytoobtaintheanswer. Animportant
issue under research concerns the automatic
derivation of A TTITUDE  statements from a pre-
existingdatabase.
8.4 SeveraldifferentVAs
As explained above, each stage of the planning
processispresentedtotheuserbyaparticularVA
withitsassociated IS agentandtheVAthenallows
users to ask further questions. Besides their
specialised knowledge, theVAs are differentiated
through different head models, different TTS
voices (maleorfemale,differentregionalaccents)
anddifferentpersonalities.
Onceadialoguestateiscompletedandtheuser
has no further questions, the VA for that state
sendsamessage to Conductor  tomovetothenext
state.  Conductor can then initiate the change in
recognition grammar, voice for the next VA and
modelforthenextVAhead.
Having several VAs coming on at different
stagestopresentdifferent informationallowsfor a
VA tobe specialised in a particular domain,  just
as real briefing officers are during a realmilitar y
planningexercise.
Fornow,weonlydisplayoneVAatatime,but
weintendtoexperimentwithhavingmultipleVAs
at the same time. The final state of the dialogue
flowallowsuserstoaskquestionsaboutanyaspect
of the planning process, and questions can be
posedtoalltheVAs,soitwouldbenaturalforth e
userstoseealltheVAsatthatstage.
8.5 RapidPrototypingandEvaluation
The key word version developed previously (see
Broughton et al, 2002) has been maintained as a
rapid prototyping environment for evaluating new
scripts and dialogues.  It allows newdialogues to
be quickly tested by entering suitable key words,
sufficient to discriminate one question from
another. Thissystemprovesfasterfortestingtha n
the more precise method of grammar building.
Multiple response strings can be generated,
providing more naturalness for those interacting
with the VAs on a regular basis.  By rapidly
prototyping questions and responses, we can test
the intuitiveness of expected questions and the
smoothness and timeliness of responses,
particularly when presented combined with
multimedia.
The implemented system described here has
so faronlybeen testedwithothermembersof the
group,butdemonstrations tovisitorsandpotential
users will provide a more rigorous  form of
evaluation on an on-going basis.  An evaluation
phase for the project is scheduled for 2003-2004,
during which time we will have access to more
users andwill be able to conductmore structured
experiments.
9 NaturalInteractionwithVAs
In addition to the ASR and TTS systems
previously discussed, other technologies can be
combined into the overall system to increase
naturalnessofinteraction,andweareinvestigatin g
speaker recognitionaswellasa rangeofpointing
technologies.
Theneed for a speaker recognition systemhas
emerged with the move to a speaker independent
ASR.WithaspeakerdependentASR,userswould
load their individual profile before use, thus
enabling the system to know who was using it.
With a speaker-independent ASR, a speaker
recognition system would allow the VAs to
recognisewho is talking to themandenable them
to address known users by name.  We plan to
integrate within FOCAL the speaker recognition
system which has been developed at DSTO
(Roberts, 1998).  This system uses statistical
modelling techniques and is capable of both
speaker identification (recognising users from a
database of stored speech profiles) and speaker
verification (verifying the identity of a particula r
user).
We are also proposing to use pointing
techniques in combination with the speech and
language technologies to build a multimodal
system.  Multimodal systems were originally
demonstrated by Bolts (1980) and research is
continuing across varied applications (e.g., Oviatt
et al, 2000 and Gibbon et al, 2000).  However,
unlike systems such as MATCH (Johnston et al,
2002), where the issue is allowing multimodal
interaction on portable devices with very small
screens, in FOCAL we are concerned with
ensuring thatusersget the full benefit of the ver y
large screen and with allowing several users to
interact at a distance from the screen.  It is also
worth mentioning that, unlike the interactive
systemdescribedin (Rickeletal.,2002),whichi s
concernedwithtraining inamilitaryenvironment,
we are not trying to simulate a complete virtual
worldwithembodiedagents.
However, we propose to include traditional
pointingtechnologies,suchasthestandarddesktop
mouse, through to3-dimensional trackingsystems
for gaze, gesture and user tracking.  This will
involve integrating more complex language
understanding, as information will need to be
derived from both the user's utterance and from
whatisbeingpointedto.Forexample,tointerpre t
an utterance such as (10) uttered while the user
points toa locationonamap,weneed toperform
reference resolution on "this region", and match
thatreferenttotheitembeingpointedat.

(10)Whatdoweknowaboutthisregion?

10 Conclusion
We have now implemented in FOCAL the
infrastructure needed to perform spoken and
multimodaldialoguewithseveralVAs. This isof
interestinitself,asitwillallowustocontinue our
research on spoken language understanding and
spokendialoguesystemsandalsotoaddressissues
of language generation which have for now been
left aside.  Already we have been able to move
from a rigid dialogue control structure, with very
constrained input, to a more flexible and scalable
control structure allowing real connectivity
betweenagents.
Havingmoved to a speaker-independent ASR,
and takingadvantageof theopensourcenatureof
Regulus, we intend to pursue research issues
regarding robustprocessing of spoken input, such
asusinggrammarspecialisationfromacorpusand
devisingtechniquesforignoringpartsoftheinput .
We have implementeda dialoguemanagement
architecture based on A TTITUDE  agents which
communicate with each other using propositional
attitude expressions.  Other agents can now be
developed to  perform additional functions, in
particular to launch the display of other types of
informationandtointerpretothertypesofinput.
This will allow us to explore how spoken
dialogue with VAs can be combined with other
virtual interaction technologies (e.g., gesture,
pointing, gaze tracking). In this respect, the next
step in our project is the development of a full
fledge MMP  agent based on the framework
describedin(ColineauandParis,2003).

However,theworkwehavereportedheremust
also be seen as part of the larger research
programmeundertakenwithinFOCAL. Fromthis
perspective, this work is of interest because it
allows othermembers of theHSI group topursue
research in the usability of new technologies to
perform the paradigm shift in command
environments.  In particular, this project is
providing the support for further research into
whether this way of presenting information is
helpful in an operational command environment.
It allows us to devise experiments to explore the
crucial issue of trust in the information being
presented,andhowtheway theinformationbeing
presentedcanaffectthattrust.
Integratingspokendialoguewithplanningtools
willalsoallowustoexplorewhetherVAscanhelp
inmilitaryoperationplanning,andhowbesttouse
thesetools.

Acknowledgements
We wish to thank the Chief of C2D, and the
Director of Information Sciences Laboratory, for
sponsoring and funding this work. We wish to
acknowledge the work of Paul Taplin in
integrating speech synthesis and lip-
synchronisation, and the work of Benjamin Fry
from the University of South Australia in
developing the Regulus/Nuance grammars.
Finallywewishtothanktheothermembersofthe
HSIgroupinC2Dfortheirconstantandinvaluable
helpwiththeFOCALproject.

References
Ananova.2002. http://www.ananova.com.
E. Andre, T. Rist, and J. Muller. 1998. Integrating
Reactive and Scripted Behaviours in a Life-Like
Presentation Agent, Proceedings of the Second
International Conference on Autonomous Agents ,
261-268.
Appen.2002. http://www.appen.com.au.
R.A.Bolt.1980."Put-that-there":voiceandgestu reat
the graphics interface . Proceedings of the
SIGGRAPH, July,262-270.
Michael Broughton, Oliver Carr, Dominique Estival,
Paul Taplin, Steven Wark, Dale Lambert.  2002.
"Conversing with Franco, FOCAL?s Virtual
Adviser". Conversation Characters Workshop,
HumanFactors2002 ,Melbourne,Australia.
Sandra Carberry and Lynn Lambert. 1999. "A Process
Model for Recognizing Communicative Acts and
ModelingNegotiationSubdialogues". Computational
Linguistics.25,1,pp.1-53
Justine Cassell. 2000. Embodied Conversational
InterfaceAgents, Communicationsof theACM ,Vol.
43,No.4,70-78.
Nathalie Colineau and C?cile Paris. 2003. Framework
fortheDesignofIntelligentMultimediaPresentati on
Systems: An architecture proposal for FOCAL.
CMISTechnicalReport03/92,CSIRO,May2003.
Dominique Estival. 2002. "The Syrinx Spoken
Language System". International Journal of  Speech
Technology. vol.5.no.1.pp.85-96.
Michael Johnston, Srinivas Bangalore, Gunaranjan
Vasireddy, Amanda Stent, Patrick Ehlen, Marilyn
Walker, Steve Whittaker, Preetam Maloor. 2002.
"MATCH: anArchitecture forMultimodalDialogue
Systems". Proceedingsofthe40thAnnualMeetingof
the Association for Computational Linguistics
(ACL'02).pp.376-383.Philadelphia..
DafyddGibbon, IngeMertins,RogerK.Moore (Eds.).
2000.  Handbook of Multimodal and Spoken
Dialogue Systems: Resources, Terminology and
ProductEvaluation. KluwerAcademicPublishers.
Global InfoTek Inc. 2002. Control of Agent Based
Systems.  http://coabs.globalinfotek.com.
JoelGould. 2001. "Implementation and Acceptance of
NatLink, aPython-BasedMacroSystem forDragon
NaturallySpeaking", The Ninth International Python
Conference,March5-8,California
Martha L. Kahn and Cynthia Della Torre Cicalese.
2001. "CoABS Grid Scalability Experiments".
Proceedings of the Second International Workshop
on Infrastructure for Agents, MAS, and Scalable
MAS, AutonomousAgents2001Conference.
Dale A. Lambert and Mikael G. Relbe.  1998.
"Reasoning with Tolerance".  2nd  International
Conference on Knowledge-Based Intelligent
ElectronicSystems .IEEE.pp.418-427.
DaleA.Lambert.1999."AdvisersWithA TTITUDE for
Situation Awareness". Proceedings of the 1999
Workshop on Defence Applications of Signal
Processing. pp.113-118, Edited A. Lindsey, B.
Moran, J. Schroeder, M. Smith and L. White.
LaSalle,Illinois.
Dale A. Lambert. 2003. "Automating Cognitive
Routines", accepted for publication in the 6th
InternationalConferenceonInformationFusion.
R.Moore, J.Dowding,H.Bratt, J. Gawron,Y.Gorfu ,
A. Cheyer. 1997. "CommandTalk: A spoken-
language interface for battlefield simulations". In
Proceedings of the Fifth Conference on Applied
NaturalLanguageProcessing,pp1-7.
Nuance.2002.http://www.nuance.com/ .
Oviatt, S., Cohen, P., Wu, L., Vergo, J., Duncan, L .,
Suhm, B., Bers, J., Holzman, T., Winograd, T.,
Landay,J.,Larson,J.,Ferro,D.2000."Designing the
user interface formultimodal speech and pen-based
gesture applications: state-of-the-art systems and
future research directions". Human Computer
Interaction.
RashmiPrasad andMarilynWalker. 2002. "Training a
Dialogue Act Tagger for Human-Human and
Human-ComputerTravelDialogues". Proceedingsof
3rdSIGDIALWorkshop .Philadelphia.pp.162-173.
Manny Rayner, John Dowding, Beth Ann Hockey.
2001. "A Baseline method for compiling typed
unification grammars into context free language
models". In Proceedings of Eurospeech 2001, pp
729-732.Aalborg,Denmark.
Manny Rayner, John Dowding, Beth Ann Hockey.
2002a."RegulusDocumentation".
Manny Rayner, Beth Ann Hockey, John Dowding.
2002b.  "Grammar Specialisation meets Language
Modelling". ICSLP2002. Denver.
Manny Rayner and Pierrette Bouillon.  2002. "A
Flexible Speech to Speech Phrasebook Translator".
Proceedings of the ACL-02 Speech-Speech
TranslationWorkshop ,pp69-76.
Jeff Rickel, Stacy Marsella, Jonathan Gratch, Randa ll
Hill,DavidTraum,WilliamSwartout.2002.Toward
aNewGenerationofVirtualHumans for Interactive
Experiences. IEEE Intelligent Systems, 1094-7167,
pp.32-38.
William Roberts. 1998. "Automatic Speaker
Recognition Using Statistical Models". DSTO
ResearchReport,DSTO-RR-0131 ,DSTOElectronics
andSurveillanceResearchLaboratory.
rVoice. 2002. Rhetorical Systems,
http://www.rhetoricalsystems.com/rvoice.html.
Paul Taplin, Geoffrey Fox, Michael Coleman, Steven
Wark, Dale Lambert. 2001.  "Situation Awareness
Using a Virtual Adviser", Talking HeadWorkshop,
OzCHI2001 ,Fremantle,Australia.
Helen Wright. 1998. "Automatic utterance type
detection using suprasegmental features".
Proceedings of the 5th International Conference on
SpokenLanguageProcessing(ICSLP'98).Sydney.
Towards Ontology-Based Natural Language Processing 
Dominique Estival, Chris Nowak and Andrew Zschorn 
Human Systems Integration Group 
Defence Science and Technology Organisation 
PO Box 1500, Edinburgh SA 5111 
AUSTRALIA 
{Dominique.Estival,Chris.Nowak,Andrew.Zschorn}@dsto.defence.gov.au 
 
 
 
Abstract 
Conceptualising a domain has long been 
recognised as a prerequisite for 
understanding that  domain and processing 
information about it. Ontologies are 
explicit specifications of conceptualisations 
which are now recognised as important 
components of information systems and 
information processing. In this paper, we 
describe a project in which ontologies are 
part of the reasoning process used for 
information management and for the 
presentation of information.  Both 
accessing and  presenting information  are 
mediated via natural language and the 
ontologies are coupled with the lexicon 
used in the natural language component.  
1 Introduction 
Ontologies are now being recognised as important 
components of information systems and 
information processing.  It is commonly accepted 
that an ontology is an explicit specification of a 
conceptualisation (Gruber, 1995).  In the areas of 
knowledge representation and reasoning (KR) and 
of conceptual modelling, it has long been 
recognised that conceptualising a domain is a 
prerequisite for understanding the domain and 
processing information about the domain, 
especially in the case of large, non-trivial domains.  
Nowadays, there is no clear-cut border between 
large and small domains, simply because 
information systems are no longer isolated but are 
parts of the global information system and need to 
be interoperable. Hence, conceptualisations and 
ontologies are required for all kinds of information 
systems and information processing.  In some 
cases it is not clear yet what functions and 
advantages ontologies can offer, but there is no 
doubt that in every case ontologies do offer 
something: at the very least they offer a way to 
address meaning of terms (concepts, relations) 
required for information processing.  
This paper attempts to provide some suggestions 
on how natural language processing can benefit 
from using ontologies.  We present a large-scale 
research project in which ontologies are part of the 
reasoning process used for information 
management and for the presentation of 
information.  Users' access to information and the 
presentation of information to users are both 
mediated via natural language, and the ontologies 
used in the reasoning component are coupled with 
the lexicon used in the natural language 
component. 
In Section 2,  we describe the FOCAL (Future 
Operations Centre Analysis Laboratory) project: 
both the ontological processing and the natural 
language processing work presented here are based 
on the relevant aspects of FOCAL.  In Section 3, 
we present ontology-related work for FOCAL and 
in Section 4, the NLP-related aspects of FOCAL.  
In Section 5, we show how ontologies and NLP are 
combined. Section 6 summarises the current state 
of this work and indicates directions for future 
research. 
2 Future Operations Centre Analysis 
Laboratory (FOCAL) 
The Future Operations Centre Analysis Laboratory 
(FOCAL) is a research project whose goal is to 
"pioneer a paradigm shift in command 
environments through a superior use of capability 
and greater situation awareness" (FOCAL Task 
Plan).  In part, this involves building a high-level 
information fusion system for the military domain 
(Lambert, 2003; FOCAL, 2002).  
To support this goal, the FOCAL facility was 
designed to experiment with innovative 
technologies. FOCAL contains a large-screen 
(150?) semi-immersive virtual reality environment 
as its primary display, allowing vast quantities of 
information (real or virtual) to be displayed.   
Spoken dialogue with virtual characters known as 
VAs (Virtual Advisers) is one of the means of 
delivering information (Estival et al, 2003).  
Within the FOCAL project, the Natural 
Language Processing (NLP) and the Knowledge 
Representation and Reasoning (KR) work 
packages are tasked with providing appropriate 
NLP and KR functionalities, including processing 
natural language queries and providing a 
formalisation of the domain and reasoning 
capabilities.  These two work packages are closely 
related in that a natural language query is to be 
processed, mapped to its formal representation and 
answered by a reasoning subsystem, and then a 
natural language answer is returned to the user. 
Current FOCAL work is focused on 
implementing a scenario, which is located within a 
particular military situation and describes a 
military domain, a limited (in space and time) 
region of the world, and other relevant elements of 
that situation.  Among other things, the domain 
description requires dealing with geography, 
logistics and planning.   
The FOCAL architecture is agent-based and 
uses the CoABS (Control of Agent Based Systems) 
Grid as its infrastructure (Global InfoTek, 2002).  
The CoABS Grid was designed to allow a large 
number of heterogeneous procedural, object-
oriented and agent-based systems to communicate. 
FOCAL agents process information, communicate 
and collaborate.  Most agents are implemented in 
ATTITUDE and communication between agents is 
accomplished via string messages (Wark et al, 
2004). 
Humans are also involved in FOCAL, as the end 
users who interact with the system to perform their 
work and achieve their goal: successfully planning 
and conducting an operation.  The current scenario 
provides a testbed for the system.  Extensions of 
the scenario and new scenarios for different 
domains will ensure that FOCAL functions as 
expected outside of the limited domain of the 
current scenario. 
There are many aspects of FOCAL which are 
not directly related to NLP and KR activities, and 
which are therefore excluded from this discussion.  
In the rest of this paper, only aspects relevant to 
NLP and KR are considered.  
3 Ontological Reasoning for FOCAL 
The main task of the KR work package within the 
FOCAL project is to provide the FOCAL users 
with automated knowledge management and with 
automated reasoning capabilities about a complex 
domain.  Ontologies have been chosen as the type 
of representation most suited for this task, and the 
provision of  ontological reasoning capabilities has 
been one of the main thrusts.   An ontology for 
FOCAL has been built and a number of reasoning 
activities are now ontology-based. 
3.1 Conceptualisation  
Lambert (2001) advocated Dennett's Intentional 
Stance framework (Dennett, 1987).  Dennett 
identified three means by which people predict and 
explain outcomes.  
1. The first is the Physical Stance, where one 
engages principles of Physics to predict 
outcomes. People employ this when playing 
snooker or assessing the trajectories of 
projectile weapons.  
2. The second is the Design Stance, where one 
engages principles of design to predict and 
explain outcomes. People employ this when 
troubleshooting an automobile fault or coding 
and maintaining computer programs.  
3. The third is the Intentional Stance, where one 
engages principles of rationality to predict 
outcomes. People employ this when 
forecasting the actions of a fighter pilot or 
when competing with an advanced computer 
game.  
 
The Design Stance is used whenever the physics of 
the situation is too difficult or laborious. The 
Intentional Stance is used whenever the design 
underpinning the situation is too difficult or 
laborious.  
Lambert (2001, 2003) adopts Dennett's 
framework for representing knowledge about the 
world, but adds two other layers: a metaphysical 
layer below the physical layer, and a social layer 
above the intentional layer. Therefore, formal 
theories that allow one to represent and reason 
about the world, would be assigned to the 
following levels: 
1. Metaphysical theories, for what there is, where 
and when. 
2. Physical theories, for the operation of aspects 
of the environment. 
3. Functional theories, for the operation of 
designed artefacts. 
4. Intentional  theories, for the operation of 
individual minds. 
5. Social theories, for the operation of groups of 
individuals. 
 
This five level framework proposed by Lambert 
suggests a way to conceptualise the domain in 
terms of processes, namely metaphysical, physical, 
functional, intentional and social processes (M, P, 
F, I, S processes). The resulting conceptualisation 
is referred to as a Mephisto conceptualisation 
(Nowak, 2003) and is the basis for the ontologies 
we are constructing for FOCAL.    
3.2 Ontological languages 
Ontologies are concerned with the meaning of 
terms.  It is therefore appropriate when selecting an 
ontological   language to choose a language which 
is equipped with a formal semantics. This 
requirement excludes XML from the list of 
possible candidates, as XML does not offer 
semantics, but only syntax.  RDF provides some 
semantics, but proper, formal semantics requires 
languages based on logics.  Description logics 
(DL) provide some frameworks, and several 
languages used for building and processing 
ontologies are DL-based, e.g. DAML and 
DAML+OIL languages, including such languages 
as SHF and SHIQ, and the OWL language 
(Horrocks et al, 2003).   
A commonly  used view of an architecture for 
the Semantic Web is a layered architecture, with 
XML as the bottom layer, RDF as the middle 
layer, and logic (e.g. DL) as the top layer 
(sometimes the top layer distinguishes ontological 
vocabulary, logic, proof;  on top of the logic layer 
a trust layer is sometimes placed).  The logic layer 
is a necessary component if the Semantic Web is to 
be equipped with a formal semantics; this logic 
layer can be based on a description logic (such as 
SHIQ or OWL), on first-order logics, KIF or 
CycL, and whichever logic is used determines the 
expressibility and tractability of the framework, 
but in every case a formal semantics is added.  
Frameworks based on DL (description logics) are 
most successful, because they provide expressive 
languages with practical tractability.  SHIQ is one 
such language, another is the closely related 
language OWL  
The ontological language chosen for FOCAL is 
SHIQ, a DL language of the DAML+OIL project 
(http://www.daml.org/), a successor of   the OIL 
project (http://www.ontoknowledge,org/oil/).  
FaCT (http://www.cs.man.ac.uk/~horrocks/FaCT/) 
is a reasoner for the SHIQ logic employed in the 
OilEd ontology editor (http://oiled.man.ac.uk/). 
The logic SHIQ has also been implemented in the 
(www.cs.concordia.ca/~faculty/haarslev/racer/) 
RACER project.   
SHIQ is closely related to OWL (Horrocks et 
al., 2003).  In fact, there are a few variants of 
OWL, namely OWL Lite, OWL DL and OWL 
Full.  OWL Lite is similar to a description logic 
SHIF(D), while OWL DL is similar to a 
description logic SHOIN(D).  The language 
implemented in the RACER framework is a 
version of SHIQ, which provides some 
functionalities for dealing with individuals, and 
dealing with concrete domains; this makes the 
RACER?s version of SHIQ very close to OWL 
Lite.  A proper discussion on these languages is 
beyond the scope of the paper, but clearly the 
RACER language is an implemented  language and 
reasoner for a logic very close to OWL DL. 
References related to OWL, SHIQ and OIL include 
(Horrocks et al, 2003), (Bechhofer and Horrocks, 
2003) and  (Horrocks, Sattler and Tobies, 2000). 
3.3 Ontological frameworks 
Ontology frameworks provide formalisms for 
building ontologies, but do not provide the 
contents.  Therefore, they should do at least two 
things: 
? provide a formal language in which the 
ontologies can be   expressed or specified, and  
? provide some reasoning capabilities, so that an 
ontology can be  demonstrated to be consistent 
(i.e. free of contradictions, assuming that   
contradictions indicate modelling mistakes or 
errors). 
Given this standpoint, frameworks that do not 
provide reasoning capabilities are unsatisfactory.  
Note also that a formal language is usually a 
logical language, with clearly specified syntax and 
semantics, and the logic should be sound, 
complete, decidable, and hopefully tractable (or 
tractable in practice).  These properties of the 
logical framework are necessary to obtain 
reasoning facilities. The most attractive ontology 
frameworks seem to be the following (see Table 1 
for a more detailed comparison of the different 
frameworks): 
1. the OIL framework based on description 
logics, 
2. the OntoEdit/OntoBroker framework (F-logic), 
3. the Ontolingua framework based on the KIF 
logic. 
 
 
 
For FOCAL, we have chosen to employ the OIL 
and RACER frameworks.  Ontologies are built 
using the OilEd ontology editor and verified using 
FaCT.  At run-time, a RACER agent is initialised 
with the ontology (see section 3.4). 
Higher order relations and Description Logic 
Although description logics on which OIL and 
RACER are based allow only binary relations, we 
use OIL and Racer in a way that also allows us to 
employ arbitrary n-ary relations and higher-order 
relations. Given that a ternary relation can be 
represented as a binary relation that takes another 
binary relation as one of its argument, any n-ary 
relations can be represented via higher-order 
relations, i.e. relations which take other relations as 
arguments.  Suppose that we want to implement a 
second-order relation that takes as its first 
argument a binary relation- more precisely, the 
second order relation takes as its first argument 
instances of that binary relation- rather than 
instances of a concept.  The instances of the binary 
relation can be mapped to instances of a newly 
created concept, i.e. the concept of individuals 
which are single entities but correspond to (and are 
linked to) the instances of the binary relation.  
There is an exact correspondence between the 
second-order relation taking a binary relation 
instance as its first argument and its 
implementation in terms of a binary relation that 
takes as its first argument an instance of the 
concept which has instances of the other binary 
relation as its individuals. The approach we 
described here has now been used to implement in 
the FOCAL ontology information which extends 
beyond the binary relation based language. 
Multiple facts involving n-ary relation and higher-
order relation are present in the current version of 
the FOCAL ontology. ATTITUDE agents are 
currently being built to allow automated reasoning 
with this extended language. 
Implemented Ontology 
As mentioned in section 1 the FOCAL scenario, 
which is based on real material for training 
exercise, provides background information in a 
number of domains, including geography, political 
situation, logistics, weather.  For now, the scenario 
also specifies what kinds of questions can be asked 
by FOCAL users, to be answered by the FOCAL 
agents. The ontology serves as a formal, clearly 
specified knowledge base containing the 
background information and allowing the agents to 
query that knowledge base and to get replies 
helping them to answer the queries. 
An initial version of the FOCAL ontology has 
been created manually using OilEd and verified 
using FaCT.1 There are in fact several ontologies, 
for the different domains covered in the scenario, 
and an important research issue is that of the 
combining (or merging) of the ontologies in the 
larger FOCAL one.  Another issue is that the 
manual creation of the ontologies is a time 
consuming and tedious process, but the existence 
of tools such as FaCT ensures that the result is 
consistent and free of mistakes due to user input 
errors.   
3.4 Ontological reasoning 
                                                            
1
 The FOCAL ontology currently contains over 300 
concepts, about 80 relations and over 100 individuals 
(plus a large number of facts connecting all of these).  
Both the FaCT and RACER reasoning agents 
provide reasoning facilities, FaCT during the 
building of the ontologies to ensure coherence and 
consistency, and RACER at run-time. When 
integrated within the FOCAL system, the RACER 
server can be initialised with a given ontology and 
there is a RACER client wrapped as a CoABS 
agent on the grid, which  can connect to the server.  
Other FOCAL agents, e.g. the Dialogue Manager 
(see section 4.1), can then communicate with the 
RACER server (via the RACER client agent) and 
receive answers using the ontology. 
The ontology can be also be accessed and 
queried outside of the FOCAL system, still using a 
client-server connection. 
? Using OilEd, the ontology "focal.daml" can be 
saved in the DIG format as a file named 
"focal.dig".2 
? The RACER server can be started and 
initialised to the "focal.dig" ontology. 
? A java package called jracer includes a test 
client (http://www.lsi.upc.es/~jalvarez/) which 
can be used to connect to the RACER server. 
 
At the ">" prompt, queries can be entered.  The 
queries are received and replied to by the server.  
For instance, we show in (1) an example of a query 
as to whether (the individual) AUSTRALIA is an 
instance of (the concept) nation, and give the 
server's answer to that query, i.e. T (for true). 
 
(1)  >  (individual-instance? AUSTRALIA nation) 
      T 
3.5 Hierarchies of concepts and relations 
A DL-based ontology, such as our OilEd "Focal" 
ontology,  is a knowledge base (KB) expressed in a 
DL language.  Every DL language provides 
facilities for defining concepts, with the relation of 
subsumption between the concepts being the core 
relation and the basis for building the definitions.  
The set of concepts can be seen as an ordered set, 
the subsumption relation being the ordering 
relation; hence, we have a hierarchy of concepts.  
There is also a hierarchy of relations ordered by 
the subsumption relation.  These two  hierarchies, 
together with the concepts' definitions, can be 
taken to form a lexicon, i.e. a list of words (for 
                                                            
2
 OilEd can export to SHIQ, OWL and other formats. 
concepts and relations) with well-defined 
meanings for those words. 
These two hierarchies of concepts and relations 
thus provide a basis for a domain specific lexicon 
and one of the advantages which ontologies can 
offer NLP systems is that a properly built 
knowledge base (as on ontology) will allow the 
semi-automatic creation of a lexicon. 
4 NLP in FOCAL 
The underlying architecture for dialogue 
management has been developed using ATTITUDE 
agents (Estival et al, 2003).  Input from FOCAL 
users can be either spoken or typed and is 
processed by the same NLP component.  We use 
Nuance for speaker-independent speech 
recognition (Nuance, 2002) and the open source 
Regulus NLP package  (Rayner et al, 2001) for 
grammar development.3 We are in the process of 
integrating language input with input from other 
devices, e.g. pointing devices such as mouse or 
wand, gesture tracking device and, in the future, 
gaze tracking. 
4.1 Dialogue Agents 
The FOCAL Dialogue Agents can be divided into 
3 categories: Input Agents, Internal Reasoning 
Agents and Output Agents.  The Input Agents 
comprise: 
? Speech Input 
The Speech Input agent is a straightforward 
wrapper around a Nuance Client implementation. 
It forwards on to the Input Fuser the interpretations 
of speech recognition results (in the form of lists of 
Attribute-Value pairs), notifications of failed 
recognition events and the interpretations of typed 
input. It also passes on instructions to activate and 
de-activate the recogniser.   
? Input Fuser 
The Input Fuser (IF) is responsible for receiving 
and combining user input.  This input can be via 
speech (Nuance), keyboard (typed input), gesture, 
gaze etc. The IF turns streams of input events into 
a Bayesian network of discrete communicative acts 
                                                            
3
 The existing grammar was developed using Regulus 1, 
but we are currently developing a larger, more flexible 
grammar with Regulus 2 (Rayner et al, 2002) which 
will provide a broader coverage, allowing the more 
na?ve users to be recognised more easily.   
which are then interpreted by the Dialogue 
Manager.  
 
The Internal Reasoning Agents comprise: 
? Reference Resolver 
This is currently a stub, but the Reference Resolver 
is meant to assist other agents (particularly the 
Input Fuser and the Dialogue Manager) resolve 
anaphoric references found in user communicative 
acts by maintaining context and linking dialogue 
variables to referents. 
? Dialogue Manager 
The Dialogue Manager (DM) is activated by a 
message that includes an activation context 
symbol. The DM receives the Bayesian network of 
interpretations of user(s) communicative acts from 
the IF and it finds the interpretation with the 
highest probability that unifies with the current 
dialogue context. The DM then informs the IF of 
which interpretation of the communicative act was 
chosen, so the IF can forward the full information 
on to the Transcriber. At the same time, the DM 
requests that the Presentation Planner present the 
response to this communicative act; this request is 
termed a communicative goal.  
? Presentation Planner 
The Presentation Planner (PP) receives requests 
from the DM to achieve communicative goals.  For 
now a communicative goal will succeed if there is 
a presentation clip which is marked-up with the 
conjunction of the DM's activation context and the 
meaning representation for the query, but current 
work is extending the PP agent along the lines 
given in (Colineau and Paris, 2003). 
 
The Output Agents comprise: 
? Transcriber 
The Transcriber agent receives notification of 
user's communicative acts from IF and of the 
system's communicative acts from DM. It produces 
an HTML listing of these communicative acts, 
which includes speech recognition results and a 
link pointing to the audio recording. 
? Text-to-Speech 
If the output is to be presented verbally by the 
Virtual Advisers, it is sent to the Text-to-Speech 
(TTS) component.  We use the rVoice TTS system, 
which gives us a choice of voices for the different 
VAs (rVoice, 2002). 
4.2 Lexicon for NLP 
As described above, language processing is 
performed by the Nuance/Regulus grammar.  
Regulus is an Open Source environment which 
compiles typed unification grammars into context-
free grammar language models compatible with the 
Nuance Toolkit.4   
The lexicon for Regulus 2 is of the form shown 
in (2) and (3), where the macro in (2) defines the 
properties of a noun class, and the instances in (3) 
specify the lexical items belonging to that class, in 
this case result, results, outcome, outcomes. 
 
(2) macro defining noun class 
macro(noun_like_result(Words,Sem),    
       @noun(Words, [sem= @noun_sem(abstract, Sem), 
        sem_n_type=abstract, takes_det_type=def\/null,  
        n_of_mod_type=_])). 
 
(3) examples of nouns for that class: 
@noun_like_result([result, results], result). 
@noun_like_result([outcome, outcomes], result). 
4.3 Meaning representation  
The Meaning Representation produced by the NLP 
component, and passed on by the Speech Input 
agent, is translated into an ATTITUDE expression. 
For example, if a user can ask the question given  
in (4.a), it will first be translated into the 
(simplified)  list of  attribute value pairs given in 
(4.b) and sent to the Speech Input agent.  Speech 
Input then translates these attribute value pairs into 
the (simplified) ATTITUDE expression given in 
(4.c) and forwards it on to the Input Fuser agent. 
 
(4) a.  What is our relationship with PNG? 
     b. (question whquestion concept relationship obj1 
Australia obj2 Papua_New_Guinea) 
     c. (comm_act (?relationship Australia 
Papua_New_Guinea) from speaker type whquestion ) 
5 Natural Language & Ontological Processing 
for FOCAL 
There are at least two ways that ontologies can 
facilitate language processing.  Firstly, an ontology 
can be used directly when building the lexicon, 
defining the terms (concepts and relations) for 
content words.  Secondly, an ontology is a 
knowledge base  (KB), expressed in a formal 
language, and therefore it provides (formally 
                                                            
4
 Regulus is described in detail in (Rayner et al, 2001). 
expressed) knowledge for more complex language 
processing. 
5.1 Ontology and the lexicon 
We view an ontology as a knowledge base, 
consisting of a structured list of concepts, relations 
and individuals. The ontology provides partial 
definitions for these, through the taxonomy 
relation between the terms and the properties 
specified for them.  An example of how a fragment 
of a lexicon, for the content words in the domain, 
can be obtained from an ontology is presented 
below.   
We give in (6) an ontology fragment, where 
every concept is listed in the format shown in (5). 
 
(5)  (  concept_n 
        list-of-parents_of_concept_n 
        list-of-children_of_concept_n  ) 
(6)   ( ( |ship|  
        (|platform|)  
        (|frigate|) ) 
   ( |platform|  
        (|asset|)  
        (|aircraft| |ship|) ) 
   ( |frigate|  
(|ship|)  
   (|ffg|) ) ) 
 
For completeness, we give in Figure 1 the actual 
OWL format for this fragment.  
 
<?xml version="1.0" encoding="ISO-8859-1"?> 
<owls:Ontology xmlns:owls=http://www.w3.org/2002/OWL-
XMLSchema         
xmlns:xsd="http://www.w3.org/2001/XMLSchema" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"  
xsi:schemaLocation="http://www.w3.org/2002/OWL-
XMLSchema 
http://potato.cs.man.ac.uk/owl/schemas/owl1-dl.xsd"> 
<owls:Class owls:complete="false" 
owls:name="file:/D:/ontology/focal.daml#platform"> 
<owls:Class 
owls:name="file:/D:/ontology/focal.daml#asset"/> 
    </owls:Class> <owls:Class owls:complete="false" 
owls:name="file:/D:/ontology/focal.daml#ffg">
  
<owls:Class 
owls:name="file:/D:/ontology/focal.daml#frigate"/> 
    </owls:Class> 
<owls:Class owls:complete="false" 
owls:name="file:/D:/ontology/focal.daml#ship"> 
<owls:Class 
owls:name="file:/D:/ontology/focal.daml#platform"/> 
    </owls:Class> 
<owls:Class owls:complete="false" 
owls:name="file:/D:/ontology/focal.daml#frigate"> 
<owls:Class owls:name="file:/D:/ontology/focal.daml#ship"/> 
    </owls:Class> 
<owls:Class owls:complete="false" 
owls:name="file:/D:/ontology/focal.daml#aircraft"> 
<owls:Class 
owls:name="file:/D:/ontology/focal.daml#platform"/> 
    </owls:Class> 
<owls:Class owls:complete="false" 
owls:name="file:/D:/ontology/focal.daml#asset"/> 
</owls:Ontology> 
Fig. 1:  FOCAL ontology fragment in OWL format 
 
Simplified lexical entries for the words aircraft, 
airplane, airplanes, plane, planes, ship, ships, 
frigate, frigates and FFG are shown in (7) and (8). 
 
(7) macro for noun class "platform": 
macro(noun_like_platform(Words,Sem),    
       @noun(Words, [sem= @noun_sem(platform, Sem), 
        sem_n_type=platform, takes_det_type=def\/null,  
        n_of_mod_type=_])). 
 
(8) examples of nouns for class "platform": 
@noun_like_platform([frigate, frigates], ship). 
@noun_like_platform([ffg], ship). 
@noun_like_platform([ship, ships], ship). 
@noun_like_platform([airplane,airplanes,plane,planes], 
aircraft). 
@noun_like_platform([aircraft], aircraft). 
 
This example shows how synonyms are handled 
in our system, with the same semantic 
interpretation, and the same parent class, given to a 
number of lexical items. 
5.2 Ontology as knowledge 
Since an ontology is a knowledge base expressed 
in a formal language, it provides formally 
expressed  knowledge for language processing.  
Although at this point not all this knowledge can 
be used directly by the speech recognition system 
which processes the speech input, nor by the 
grammar which builds the meaning 
representations, some of this knowledge can 
already be used by the other Dialogue agents, in 
particular the Dialogue Manager, and later by the 
Reference Resolver.   
The best example is the resolution of ambiguity, 
such as the polysemy of some terms.  For instance 
the name Adelaide can refer to a city (Adelaide in 
South Australia), a ship ("HMAS Adelaide"), a 
river (the Adelaide River in the Northern Territory 
of Australia), or even a person, (e.g. "Queen 
Adelaide").  While, as shown in Section 5.1,  
synonymy is handled by the lexicon, polysemy is 
resolved by drawing on a variety of sources, 
including the ontology.   
When the Dialogue Manager receives from the 
Input Fuser a set of communicative acts, if one of 
these communicative acts correspond to distinct 
plausible interpretation results, e.g. 
"Adelaide:{city, ship}", it can try to resolve the 
ambiguity by using the context information and by 
sending a request to the KR agent. 
6 Conclusion 
This paper has described our current work within 
the FOCAL project to combine ontologies built 
with the OIL/RACER framework with our spoken 
dialogue system.  It provides some suggestions on 
how ontologies can help a natural language 
processing component build semantic 
representations which are directly used in a 
complex information management system. 
This is work in progress and a formal evaluation 
has not yet been put in place. However, the 
reviewers for this paper have rightly asked how 
this would be conducted.  In the agent-based 
architecture we use, each agent can be tested in 
isolation and we have already conducted tests to 
ensure that the answers returned by the KR agent  
for specific questions in our scenario are correct 
and consistent.  A more interesting evaluation will 
be possible when the scenario is expanded, to see 
whether unplanned answers returned when the 
system is asked new unscripted questions are in 
fact useful to the users.  This will take place in the 
next phase of the project. 
For now, we conclude that an ontology is a 
knowledge base which can serve as the basis for 
creating the part of the lexicon for domain content 
words.  This is achieved by producing a list of 
terms with their meanings, i.e. partial definitions 
given the two hierarchies in the ontology, and we 
are exploring methods to automate this process. 
References 
S. Bechhofer and Ian Horrocks. 2003. The Wonder Web 
Ontology Language.  Report and Tutorial. 
S. Blackburn. 1996. The Oxford Dictionary of 
Philosophy. Oxford University Press. 
Nathalie Colineau and C?cile Paris. 2003. Framework 
for the Design of Intelligent Multimedia Presentation 
Systems: An architecture proposal for FOCAL. 
CMIS Technical Report 03/92, CSIRO, May 2003. 
Daniel C. Dennet. 1987. The Intentional Stance. 
Cambridge: MIT Press. 
Dominique Estival, Michael Broughton, Andrew 
Zschorn, Elizabeth Pronger. 2003. "Spoken Dialogue 
for Virtual Advisers in a semi-immersive Command 
and Control environment". In Proceedings of the 4th 
SIGdial Workshop on Discourse and Dialogue, 
Sapporo, Japan. pp.125-134. 
FOCAL. 2002. DSTO and Virtual Reality. 
http://www.dsto.defence.gov.au/isl/focal.pdf.  
Global InfoTek Inc. 2002. Control of Agent Based 
Systems.  http://coabs.globalinfotek.com. 
T. R. Gruber. 1995.  "Toward Principles for the Design 
of Ontologies Used for Knowledge Sharing". Human 
and Computer Studies,  vol. 43, no. 5-6. 
Ian Horrocks, Peter F. Patel-Schneider and Frank van 
Harmelen. 2003. "From SHIQ and RDF to OWL: 
The Making of a Web Ontology Language". Journal 
of Web Semantics,vol.1, no,1, pp.7-26.  
Ian Horrocks, U. Sattler and S. Tobies. 2000.  "Practical 
reasoning for very expressive description logics". 
Logic Journal of the IGPL, 8(3):239-263. 
Dale A. Lambert. 2001. "An Exegesis of Data Fusion". 
In Soft Computing in Measurement and Information 
Acquisition, eds. L. Reznik and V. Kreinovich.  
Physica-Verlag. 
Dale A. Lambert. 2003. "Grand Challenges of 
Information Fusion". In Proceedings of the Sixth 
International Conference on Information Fusion. 
Cairns, Australia. 
Chris Nowak. 2003. "On ontologies for high-level 
information fusion". In Proceedings of the Sixth 
International Conference on Information Fusion. 
Cairns, Australia. 
Nuance.  2002.  http://www.nuance.com/.   
Manny Rayner, John Dowding, Beth Ann Hockey.  
2001. "A Baseline method for compiling typed 
unification grammars into context free language 
models". In Proceedings of Eurospeech 2001, pp 
729-732. Aalborg, Denmark.  
Manny Rayner, Beth Ann Hockey, John Dowding.  
2002.  "Grammar Specialisation meets Language 
Modelling".  ICSLP 2002. Denver, USA.  
rVoice. 2002. Rhetorical Systems, 
http://www.rhetoricalsystems.com/rvoice.html. 
Steven Wark, Andrew Zschorn, Michael Broughton, 
Dale Lambert. 2004. "FOCAL: A Collaborative 
Multimodal Multimedia Display Environment".  In 
Proceedings of  SimTecT.  Canberra, Australia. 
