Automatic Text Categorization by Unsupervised Learning 
Youngjoong Ko 
Department of Colnputer Science, 
Sogang University 
1 Sinsu-dong, Mapo-gu 
Seoul, 121-742, Korea 
kyj @nlpzodiac.sogang.ac.kr, 
Jungyun Seo 
Depmlment of Computer Science, 
Sogang University 
1 Sinsu-dong, Mapo-gu 
Seoul, 121-742, Korea 
seoiy @ccs.sogang.ac.kr 
Abstract 
The goal of text categorization is to classify 
documents into a certain number of pre- 
defined categories. The previous works in 
this area have used a large number of 
labeled training doculnents for supervised 
learning. One problem is that it is difficult to 
create the labeled training documents. While 
it is easy to collect the unlabeled ocuments, 
it is not so easy to manually categorize them 
for creating traiuing documents. In this 
paper, we propose an unsupervised learning 
method to overcome these difficulties. The 
proposed lnethod divides the documents into 
sentences, and categorizes each sentence 
using keyword lists of each category and 
sentence simihuity measure. And then, it 
uses the categorized sentences for refining. 
The proposed method shows a similar 
degree of performance, compared with the 
traditional supervised learning inethods. 
Therefore, this method can be used in areas 
where low-cost ext categorization is needed. 
It also can be used for creating training 
documents. 
Introduction 
With the rapid growth of the internet, the 
availability of on-line text information has been 
considerably increased. As a result, text 
categorization has become one of the key 
techniques fox handling and organizing text data. 
Automatic text categorization in the previous 
works is a supervised learning task, defined as 
assigning category labels (pro-defined) to text 
documents based on the likelihood suggested by 
a training set of labeled doculnents. However, 
the previous learning algorithms have some 
problems. One of them is that they require a 
large, often prohibitive, number of labeled 
training documents for the accurate learning. 
Since the application area of automatic text 
categorization has diversified froln newswire 
articles and web pages to electronic mails and 
newsgroup postings, it is a difficult task to 
create training data for each application area 
(Nigam K. et al, 1998). 
In this paper, we propose a new automatic text 
categorization lnethod based on unsupervised 
learning. Without creating training documents 
by hand, it automatically creates training 
sentence sets using keyword lists of each 
category. And then, it uses them for training and 
classifies text documents. The proposed method 
can provide basic data fox" creating training 
doculnents from collected documents, and can 
be used in an application area to classify text 
documents in low cost. We use the 2 / statistic 
(Yang Y. et al, 1998) as a feature selection 
method and the naive Bayes classifier 
(McCailum A. et al, 1998) as a statistical text 
classifier. The naive Bayes classifier is one of 
the statistical text classifiers that use word 
frequencies as features. Other examples include 
k-nearest-neighbor (Yang Y. et al, 1994), 
TFIDF/Roccio (Lewis D.D. et al, 1996), 
support vector machines (Joachilns T. et al, 
1998) and decision tree (Lewis D.D. et al, 
1994). 
1 Proposal: A text categorization scheme 
The proposed system consists of three modules 
as shown in Figure 1; a module to preprocess 
collected ocuments, a module to create training 
sentence sets, and a module to extract features 
and to classify text doculnents. 
453 
............. 1\] 
i 
J 
Text (~ 
/ 
/ " 
,L',.~g,,r} 
i, I ) 
.1 
.l&ll ego 13', 
Ax~J~llJll~ ', /, 
I( ",'11 ello I'1\] 
Figurel : Architecture for the proposed system 
1.1 Preprocessing 
First, the html tags and special characters in the 
collected ocuments are removed. And then, the 
contents of the documents are segmented into 
sentences. We extract content words for each 
sentence using only nouns. In Korean, there are 
active-predicative common nouns which become 
verbs when they am combined with verb- 
derivational suffixes (e.g., ha-ta 'do', toy-la 
'become', etc.). There are also stative- 
predicative common nouns which become 
adjectives when they are combined with 
adjective-derivational suffixes such as ha. These 
derived verbs and adjectives are productive in 
Korean, and they are classified as nouns 
according to the Korean POS tagger. Other 
verbs and adjectives are not informative in many 
cases. 
1.2 Creating training sentence sets 
Because the proposed system does not have 
training documents, training sentence sets for 
each category corresponding to the training 
documents have to be created. We define 
keywords for each category by hand, which 
contain special features of each category 
sufficiently. To choose these keywords, we first 
regard category names and their synonyms as 
keywords. And we include several words that 
have a definite meaning of each category. The 
average number of keywords for each category 
is 3. (Total 141 keywords for 47 categories) 
Table 1 lists the examples of keywords for 
each category. 
Table 1: Examples of keywords for each category 
Category Keywords 
ye-hayng (trip), 
kwan-kwang 
(sightseeing) 
Um-ak(music) 
Cong-kyo 
(religion) 
Pang-song 
(broadcasting) 
ye-hayng (trip), 
kwan-kwang (sightseeing) 
Um-ak (music) 
Cong-kyo (religion), 
chen-cwu-kyo(Catholicism) 
ki-tok-kyo(Christianity), 
pwul-kyo(Buddhism) 
Pang-song (broadcasting), TV thal- 
ley-pi-cyen(television), la-ti-o(radio) 
Next, the sentences which contain pre-defined 
keywords of each category in their content 
words are chosen as the initial representative 
sentences. The remaining sentences am called 
unclassified sentences. We scale up the 
representative sentence sets by assigning the 
unclassified sentences to their related category. 
This assignment has been done through 
measuring similarities of the unclassified 
sentences to the representative sentences. We 
will elaborate this process in the next two 
subsections. 
1.2.1 Extracting and verifying representative 
sentences 
We define the representative s ntence as what 
contains pre-defined keywords of the category in 
its content words. But there exist error sentences 
in the representative sentences. They do not 
have special features of a category even though 
they contain the keywords of the category. To 
relnove such error sentences, we can rank the 
representative sentences by computing the 
weight of each sentence as follows: 
1) Word weights are computed using Term 
Frequency (TF) and Inverse Category Frequency 
(ICF) (Cho K. et al, 1997). 
@ The within-category word frequency(TF~j), 
TFij = the number of times words ti occurs 
in the j th category (1) 
? In Information Retrival, Inverse Document 
Frequency (IDF) are used generally. But a 
sentence is a processing unit in the 
proposed method. Therefore, the document 
frequency cannot be counted. Also, since 
ICF was defined by Cho K. et al (1997) 
454 
and its efficiency was verified, we use it in 
tile proposed method. ICF is computed as 
follows: 
ICF i -- Iog(M ) - I og(CF  i ) (2) 
? 
where CF is tile number of categories that 
contain t;, and M is tile total number of 
categories. 
Tile Colnbination (TFICF) of the above (9 
and ?, i.e., weight w~ i of word t; in ./tit 
category is computed as follows: 
wij --- TFii x ,cl~,. 
: Tl ' i j  X ( log(M)  - l og(CF  i ) ) (3) 
2) Using word weights (%) computed in 1), a 
sentence weight (We) in jth category are 
computed as follows: 
W Ij q- W2j +...-F WNj 
W!/ = (4) 
N 
where N is the total number of words in a 
sentence. 
3) The representative s ntences of each category 
are sorted in the decreasing order of weight, 
which was computed in 2). And then, the lop 
70% of tile representative s ntences are selected 
and used in our experiment. It is decided 
empirically. 
1.2.2 Extending representative s ntence sets 
To extend lhe representative s ntence sets, the 
unclassified sentences are classified into their 
related category through measuring similarities 
of the unclassified sentences to the 
representative s ntences. 
(l) Measurement of word and sentence 
similarities 
As similar words tend to appear in similar 
contexts, we compute the similarity by using 
contextual information (Kim H. et al, 1999; 
Karov Y. et al, 1999). In this paper, words and 
sentences play COlnplementary roles. That is, a 
sentence is represented by the set of words it 
contains, and a word by the set of sentences in 
which it appears. Sentences are simihu" to the 
extent that they contain similar words, and 
words are similar to the extent that they appear 
in similar sentences. This definition is circular. 
Titus, it is applied iteratively using two matrices 
as shown in Figure 2. in this paper, we set the 
number of iterations as 3, as is recommended by 
Karov Y. et al (1999). 
S i rn i la r~~ Wod " Sente,,C; : 
i A i d 
Figure 2: llerative computation of word and 
sentence similarities 
In Figure 2, each category has a word 
silnilarity matrix WSM,, and a sentence similarity 
matrix SSM,,. In each iteration n, we update 
WSM,, whose rows and columns are labeled by 
all content words encountered in the 
rcpresentatwe sentences of each category and 
input unclassified sentences. In that lnatrix, the 
cell (i j) hokls a value between 0 and l, 
indicating the extent to which the ith word is 
contextually similar to the jth word. Also, we 
keep and update a SSM,,, which holds similarities 
among sentences. The rows of SSM,, correspond 
to the unclassified sentences and the cohmms to 
the representative s ntences. In this paper, the 
number of input sentences of row and column in 
SSM is limited to 200, considering execution 
time and memory allocation. 
To compute tile similarities, we initialize 
WSM, to the identity matrix. That is, each word 
is fully similar (1) to itself and completely 
dissimilar (0) to other words. The following 
steps are iterated until the changes in the 
similarity values are small enough. 
1. Update the sentence similarity lnatrix SSM,,, 
using the word similarity matrix WSM,. 
2. Update the word similarity matrix WSM,,, 
using the sentence similarity matrix SSM,. 
(2) Affinity formulae 
qb simplify tile symmetric iterative treatment of 
similarity between words and sentences, we 
del'ine an auxiliary relation between words and 
sentences as affinity. A woM W is assumed to 
have a certain affinity to every sentence, which 
455 
is a real number between 0 and 1. It reflects the 
contextual relationships between W and the 
words of the sentence. If W belongs to a 
sentence S, its affinity to S is 1. If W is totally 
unrelated to S, the affinity is close to 0. If W is 
contextually similar to the words of S, its affinity 
to S is between 0 and 1. In a similar manner, a 
sentence S has some affinity to every word, 
reflecting the similarity of S to the sentences 
involving that word. 
Affinity formulae are defined as follows 
(Karov Y. et al, 1999). In these formulae, W ~ S 
means that a word belongs to a sentence: 
aft,, (W, S) = max w, es sire,, (W , W i ) 
aff,, (S, W) = max w~s; sire,, (S, S~ ) 
(5) 
(6) 
In the above formulae, n denotes the iteration 
number, and the similarity values are defined by 
WSM,, and SSM,,. Every word has some affinity 
to the sentence, and the sentence can be 
represented by a vector indicating the affinity of 
each word to it. 
(3) Similarity formulae 
The similarity of Wj to W2 is the average affinity 
of the sentences that include W~ to 1+'2, and the 
similarity of a sentence S~ to $2 is a weighted 
average of the affinity of the words in S~ to Se. 
Similarity formulae are defined as follows 
(Karov Y. et al, 1999): 
sim,,+l (Sj, S 2 ) = Z weight(W, S1 ). qlJ',, (W, S 2 ) (7) 
WE ,~'~ 
if W I =W 2 
sim,,+l (W l , W 2 ) = 1 
C/,?e 
sim"+l (Wl' W2 ) = Z weight(S, W l ). aft,, (S, W 2 ) (8) 
W~eS 
The weights in Formula 7 are computed 
following the methodology in the next section. 
The sum of weights in Formula 8, which is a 
reciprocal number of sentences that contain W, 
is !. These values are used to update the 
corresponding entries of WSM and SSM,,. 
(4) Word weights 
In Formula 7, the weight of a word is a product 
of three factors. It excludes the words that are 
expected to be given unreliable similarity values. 
The weights are not changed in their process of 
iterations. 
l. Global frequency: Frequent words in total 
sentences are less informative of sense and of 
sentence similarity. For example, a word like 
'phil-yo(necessity)' frequently appears in any 
sentence. The formula is as follows (Karov Y. 
et al, 1999): 
max{0,1 freq(W) "1 
max 5, freq(x) J
(9) 
In (9), max52\[req(x) is the sum of the five 
highest frequencies in total sentences. 
2.Log-likelihood faclor: In general, the words 
that are indicative of the sense appear in 
representative s ntences more frequently than 
in total sentences. The log-likelihood factor 
captures this tendency. It is computed as 
follows (Karov Y. et al, 1999): 
log pr(w; l w) (lO) 
Pr(Wi ) 
In (10), Pr(Wi) is estimated from the 
frequency of Wi in the total sentences, and 
Pr(WilW) fi'om the frequency of Wi in 
representative sentences. To avoid poor 
estimation for words with a low count in 
representative s ntences, we nmltiply the log- 
likelihood by (11) where count(Wi) is the 
number of occurrences of Wi in representative 
sentences. For the words which do not appear 
in representative s ntences, we assign weight 
(1.0) to them. And the other words are 
assigned weight that adds 1.0 to computed 
value: 
c?unt(Wi) t (11) min. 1, 3 
3.Part of ,q~eech: Each part of speech is 
assigned a weight. We assign weight (1.0) to 
proper noun, non-predicative common noun, 
and foreign word, and assign weight (0.6) to 
active-predicative common noun and stative- 
predicative common noun. 
456 
The total weight of a word is the product of the 
above t'actors, each norlnalized by the sum of 
factors of the words in a sentence as follows 
(Karov Y. et al, 1999): 
,&ctor(Wi, S) 
weight 
? J 'actor(Wi, S)  
IVieS 
(12) 
In (12), factor(W, S) is the weight before 
normalization. 
(5) Assigning unclassified sentences to a 
category 
We first computed similarities of the 
unclassified sentences to the representative 
sentences. And then, we decided a Silnilarity 
value o1' each unclassified sentence for each 
category using two alternate ways. 
1 tl sint(X,ci)=-- ? Sil!l (X,Sj) (13) 
tiE(' I1 . ( ,'~'jcR,., 
j=  J 
sinl(X,ci)=nlnxl.siul(X Si)} (14) 
(:'~(" I SjcRc, 
In (13) and (14), i) X is au unclassified sentence, 
ii) C = {c l,c2 ..... c,,,} is a category set, and iii) 
R,,,={&,Sa ...... S',,} is a representative sentence 
set of category c.. 
Each unclassified sentence is assigned to a 
category which has a maxinmln similarity wflue. 
But there exist unclassified sentences which do 
not belong to any category. To remove these 
unclassified sentences, we set up a threshold 
value using normal distribution of similarity 
values as follows: 
max{sim(X,c i )  } >_ tt + 017 (15) 
ciEC 
In (15), i) X is an unclassified sentence, ii) It is 
an average of similarity wflues, iii) o is a 
standard eviation of similarity wdues, and iv) 0 
is a numerical wdue corresponding to 
threshold(%) in normal distribution table. 
1.3 Feature selection and text classifier 
1.3.1 Feature Selection 
The size of the vocabulary used in our 
experiment is selected by ranking words 
according to their Z 2 statislic with respect o the 
category. Using the two-way contingency table 
of a word t and a category c - i) A is the number 
of times t and c co-occur, ii) B is the number of 
times t occurs without c, iii) C is the number of 
times c occurs without t, iv) D is the number ot' 
times ueither c nor t occurs, and vi) N is the total 
number of sentences - the word-goodness 
measure is defined as follows (Yang Y. et al, 
1997): 
Z2(t,c) = N?(AD-CB)2 (16) 
(A + C)(B + D)(A + B)(C + D) 
To measure the goodness of a word in a 
global feature selection, we combine the 
category-specific s ores of a word as follows: 
I I I  9 2 ZF,,.,~, (t) = n~a,x{ Z .= (t, (:~)} (17) 
1.3.2 Text classifier 
The method that we use for classifying 
documents is uaivc Bayes, with minor 
modifications based on Kullback-Leibler 
Divergence (Craven M. et al, 1999). The basic 
idea in naive Bayes approaches i to use the joint 
probabilities of words and categories to estimate 
the probabilities of categories given a document. 
Given a document d for chtssit'ication, we 
calculate the probabilities of each category c as 
follows: 
Pr(cld) Pr(c) Pr(d lc) 7' _ _ P r (c )H Pr(t i Ic.) N(rM~ 
Pr(d) i< 
T !c)) 
,, ;=, Id) ) 
In tile above l'ormula, i) 11, is the number of 
words in d, ii) N(t~ld) is the frequency of woM t i 
in clocument d, iii) 7" is the size of tile 
vocabulary, and iv) t~ is tile ith word in the 
vocabulary. Pr(tAc ) thus represents the 
probability that a randomly drawn woM from a 
randolnly drawn docmnent in category c will be 
the word 6 Pr(tild) represents the proportion of 
woMs in docmnent d that are word t c Each 
probability is estimated by formulae (19) and 
(20), which are called the expected likelihood 
457 
estimator (Li H. et al, 1997). The category 
predicted by the method for a given document is 
simply the category with the greatest score. This 
method performs exactly the same 
classifications as naive Bayes does, but produces 
classification scores that are less extreme. 
N(ti, c) + 0.5 
Pr(ti \[ c) = T( (l 9) 
Z N(t i' c) + 0.5 x T c 
j=l 
Pr(t i id) = N( t . i ,d )+O.5xT ,  z (20) 
0 if N(t i ,d) :  0 
2 Evaluation of experiment 
2.1 Performance measures 
In this paper, a document is assigned to only one 
category. We use the standard definition of 
recall, precision, and F, measure as performance 
measures. For evaluating performance average 
across categories, we use the micro-averaging 
method. F~ measure is defined by the following 
formula (Yang Y. et al, 1997): 
2 q) 
F 1 ( r ,  p )  - (21 ) 
r+ p 
where r represents recall and p precision. It 
balances recall and precision in a way that gives 
them equal weight. 
2.2 Experiment settings 
We used total 47 categories in our experiment. 
They consist of 2,286 documents to be collected 
in web. We did not use tag information of web 
documents. And a so-called bag of words or 
unigraln representation was used. Table 2 shows 
the settings of experiment data in detail. 
Table 2: Setting experiment data 
........................... i i avg# avg # of I #of '  #of  I .... of doc, sen. . . . . .  d0c  sen: ...... ~ .......... ~ inacat, inadoc. 
Training 1 ,383 67,506 29.4 48.8 
Set (60%) 
903 Test Set 56,446 19.2 62.5 (40%) 
2.3 Prinmry results 
2.3.1 Results of the different combinations of 
similarity value decisions and thresholds 
We evaluated our method according to the 
different combinations of similarity value 
decisions and thresholds in section 1.2.2. We 
used thresholds of top 5%, top 10%, top 15%, 
top20% in formula (15), and tested the two 
options, average and maximum in formulae (13) 
and (14). We limited our vocabulary to 2,000 
words in this experiment. 
+Close  Test(max) -~N-~Close Test(avg) 
+Open Test(max) ---')(~Open Test(avo) 
0.73 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
0.72 
UL 0.71 
0.7 
0.69 
E 0.68 
0.67 
0.66 
5% 10% 15% 20% 
Threshold(%) 
Figure 3: Results of the different combinations of 
similarity wdue decisions and thresholds 
Figure 3 shows results according to the two 
options in each threshold. Here, the result using 
maxinmm was better than that using average 
with regrad to all thresholds. The results of top 
10% and top 15% were best. Therefore, we used 
the maximum in the decision of similarity value 
and top 15% in threshold in our experiments. 
2.3.2 The proposed system vs. the system by 
supervised learning 
For the fair evaluation, we embodied a 
traditional system by supervised learning using 
the same feature selection method (2/ statistic) 
and classifier (naive Bayes Classifier), as used in 
the proposed system. And we tested these 
systems and compared their performance: 
458 
+method by supervised learning ' -~t r -p roposed  method 
0.8 .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
0.770 
0.75 
0.725 
E. o7 
>~ 0.675 
0.05 
E 0.0 
0675 
0.!i5 
0,525 
6.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
Vooabulary Size 
lqgure 4: Comparison of the proposed system and the 
syslem by supervised learning 
Figure 4 displays the performance urves for the 
proposed system and the system by supervised 
learning. The best F~ score of the proposed 
system is 71.8% and that of the system by 
supervised learning is 75.6%. Therefore, the 
difference between them is only 3.8%. 
Conclusion 
This paper has described a new automatic text 
categorization method. This method 
automatically created training sets using 
keyword lists of each category and used them 
for training. And then, it classified text 
documents. This could be a significant method 
in text learning because of the high cost of hand- 
labeling training docmnents and the awfilability 
of huge volumes of unlabeled ocuments. The 
experiment results showed that with respect o 
performance, the difference between the 
proposed method and the method by supervised 
learning is insignificant. Therefore, this method 
can be used in areas where low-cost text 
categorization is required, and can be used for 
creating training data. 
This study awaits further research. First, a 
more scientific approach for defining keyword 
lists should be investigated. Next, if we use a 
word sense disambiguation systeln in the 
extraction step of representative s ntences, we 
would be able to achieve a better performance. 
Acknowledgments  
This work was supported by KOSEF under 
Grant No. 97-0102-03-01-3. We wish to thank 
Jeoung-seok Kiln for his valuable COlnments to 
the earlier version of this paper. 
Refel-elices 
Cho K. and Kim J. (1997) Automatic Text 
Categorization on Hierarchical Category Structure 
by using ICF(Inverted Category Frequency) 
Weighting. In Proceedings of KISS coqference, 
pp.507-510. 
Craven M., DiPasquo D., Freitag I)., McCallum A., 
Mitchell T., Nigam K. and Slauery S. (1999) 
l,earning to Conslruct Knowledge Bases from lhe 
World Wide Web. to appear in Artificial 
httelligence. 
Joachims T. (1998)Text Categorization with Supporl 
Vector Machines: Learning with Many Relevant 
Features. In European Conference on Machine 
Learning(ECML). 
Karov Y. and 17,dehnan S. (1998) Similarity-based 
Word Sense l)isambiguation. Computational 
Linguistics, Vol 24, No I, pp. 41-60. 
Kim H., KEY., Park S. and See J. (1999) hfformal 
Requirements Analysis St,1)porling System for 
Htlulall Engineer. 111 Proceedings of Conference m~ 
IEEE- SMC99. Vol 3, pp. 1013-1018. 
Lewis D.D. and Ringuette M. (t994) A comparison 
of '\['wo 1,earning Algorithms for Text categorizalion. 
In Proceeding of the 3 ''~ Ammal &,ml~osium o~ 
Document Attalysis and h{/brmation Retrieval. 
Lewis I).D., Schapire P,.E., Calhm J.P. and Papka 
P,.(1996) Training Algorilhms for IJnear Text 
Classifiers. In Proceedings of the 19" htter~tatiomtl 
Coqference on Research and Deveh)l)ment in 
h!/btwtation Retrieval (SIGIR'96), pp. 289-297. 
Li H. and Yamanishi K. (1997) Document 
Classification Using a Finite Mixture Model. The 
Association for Co,qmtatiomtl Littguistics, 
ACE '97. 
McCallum A. and Nigram K. (1998) A comparison 
of Event Models for Naive Bayes Text 
Classification. AAAI '98 workshop on Leanting for 
7kvt Categorization. 
Nigam K., McCallum A., Thrun S. and Mitchell T. 
(1998) Learning to Classify Text from Labeled and 
Unlabeled l)oeuments. In Proceedings of 15" 
National Conference on Artificial httelligence 
(AAAI-98). 
Yang Y. (1999) An ewduation of statistical 
approaches to text categol"ization, ht.formation 
Retrieval Journal, May. 
Yaug Y. (1994) Expert netword: Effective and 
efficient learning fi'om human decisions in text 
catego,izatin and retriewfl. In 17" Ammal 
lnternatiomd A CM SIG1R Conference on Research 
and Development in hi formation Retrieval 
(SIGIR'94), pp. 13-22. 
Yang Y. and Pederson J.O. (1997) A comparative 
study on feature selection in text categorization, ht
Proceedings of the 14" International Conference on 
Machine Learning. 
459 
The Grammatical Function Analysis between Korean Adnoun Clause 
and Noun Phrase by Using Support Vector Machines 
 
Songwook Lee 
Dept. of Computer Science, 
Sogang University 
1 Sinsu-dong, Mapo-gu 
Seoul, Korea 121-742 
gospelo@nlprep.sogang.ac.kr
Tae-Yeoub Jang 
Dept. of English,  
Hankuk University of Foreign 
Studies 
270, Imun-dong, 
Dongdaemun-gu, Seoul, 
Korea 130-791 
tae@hufs.ac.kr 
Jungyun Seo 
Dept. of Computer Science, 
Sogang University 
1 Sinsu-dong, Mapo-gu 
Seoul, Korea 121-742 
seojy@ccs.sogang.ac.kr 
 
 
Abstract 
This study aims to improve the 
performance of identifying 
grammatical functions between an 
adnoun clause and a noun phrase in 
Korean.  The key task is to determine 
the relation between the two 
constituents in terms of such 
functional categories as subject, object, 
adverbial, and appositive. The 
problem is mainly caused by the fact 
that functional morphemes, which are 
considered to be crucial for identifying 
the relation, are frequently omitted in 
the noun phrases. To tackle this 
problem, we propose to employ the 
Support Vector Machines(SVM) in 
determining the grammatical functions. 
Through an experiment with a tagged 
corpus for training SVMs, the 
proposed model is found to be useful. 
 
1 Introduction 
 
Many structural ambiguities in Korean sentences 
are one of the major problems in Korean 
syntactic analyses. Most of those ambiguities can 
be classified into either of two categories known 
as "noun phrase (NP) attachment problem" and 
"verb phrase (VP) attachment problem".  The NP 
attachment problem refers to finding the VP 
which is the head of an NP. On the other hand, 
the VP attachment problem refers to finding the 
VP which is the head of a VP. 
In resolving the NP attachment problem, 
functional morphemes play an important role as 
they are the crucial elements in characterizing the 
grammatical function between an NP and its 
related VP. However, the problem is that there 
are many NPs that do not have such functional 
morphemes explicitly attached to each of them.  
This omission makes it difficult to identify the 
relation between constituents and subsequently 
to solve the NP attachment problem. Moreover, 
most Korean sentences are complex sentences, 
which also makes the problem more complicated. 
In this research, we make an attempt to solve 
this problem. The focus is on the analysis of the 
grammatical function between an NP and an 
embedded adnoun clause with a functional 
morpheme omitted.  
We adopt Support Vector Machines(SVM) as 
the device by which a given adnoun clause is 
analyzed as one of three relative functions 
(subject, object, or adverbial) or an appositive.  
Later in this paper (section 3), a brief description 
of SVM will be given. 
 
2 Korean Adnoun Clauses and their 
analysis problems 
 
Adnoun clauses are very frequent in Korean 
sentences.  In a corpus, for example, they appear 
as often as 18,264 times in 11,932 sentences (see 
section 4, for details).  It means that effective 
analyses of adnoun clauses will directly lead to 
improved  performance of lexical, morphological 
and syntactic processing by machine. 
In order to indicate the difficulties of the 
adnoun clause analysis, we need to have some 
basic knowledge on the structure of Korean 
adnoun clause formation.  Thus, we will briefly 
illustrate the types of Korean adnoun clauses. 
Then, what makes the analysis tricky will be 
made clear. 
 
2.1 Two types of adnoun clauses 
 
There are two types of adnoun clauses in 
Korean : relative adnoun clause and appositive 
adnoun clause.  The former  is a more general 
form of adnoun clause and its formation can be 
exemplified as follows : 
 
1.a Igeos-eun(this) geu-ga(he) sseu-n(wrote) 
chaeg-ida(book-is).  
(This is the book which he wrote.) 
 
1.b Igeos-eun(this) chaeg-ida(book-is). 
(This is a book.) 
 
1.c Geu-ga(he) chaeg-eul(book) 
sseoss-da(wrote). 
(He worte the book.) 
 
1.a is a complex sentence composed of two 
simple sentences 1.b and 1.c in terms of adnoun 
clause formation.  The functional morpheme 
?eul?, which represents the object relation 
between ?chaeg? and ?sseoss-da? in 1.c, does not 
appear in 1.a  but ?chaeg? is the functional object 
of ?sseu-n? in 1.a.  This adnoun clause is called a 
relative adnoun clause whose complement moves 
to the NP modified by the adnoun clause and the 
NP modified by a relative adnoun clause is called 
a head NP.  In 1.a ?geu-ga sseun? is a relative 
adnoun clause and ?chaeg? is its head noun (or 
NP). 
Let us consider another example of an adnoun 
clause. 
 
2. Geu-ga(he) jeongjigha-n(be honest) 
sasil-eun(fact) modeun(every) saram-i(body) 
an-da(know). 
(Everybody knows the fact that he is honest.) 
 
The adnoun clause in 2 is a complete sentence 
which has all necessary syntactic constituents in 
itself.  This type of adnoun clause is called an 
appositive adnoun clause. And the head NP 
modified by the appositive adnoun clause is 
called a complement noun (Lee, 1986; Chang 
1995). In 2, ?geu-ga jeongjig-han? is an 
appositive adnoun clause and ?sasil? is a 
complement noun. Generally, such words as 
?iyu(reason), gyeong-u(case),  jangmyeon(scene), 
il(work), cheoji(condition), anghwang(situation), 
saggeon(happening), naemsae(smell), 
somun(rumor) and geos(thing)? are typical 
examples of the complement noun (Chang, 1995; 
Lee, 1986). 
 
2.2 The problems 
 
The first problem we are faced with when 
analyzing grammatical functions of Korean 
adnoun clauses is obviously the disappearance of 
the functional morphemes which carry important 
information, as shown in the previous subsection 
(2.1). 
Apart from the morpheme-ommission 
problem, there is another reason for the difficulty.  
As it is directly related to a language particular 
syntactic characteristic of Korean, we need first 
to understand a unique procedure of Korean 
relativization.  Unlike English, in which relative 
pronouns (e.g., who, whom, whose, which and 
that) are used for relativization and they 
themselves bear crucial information for 
identifying grammatical function of the head 
noun in relative clauses (see example 1.a, in 
section 1), there is no such relative pronouns in 
Korean.  Instead, an adnominal verb ending is 
attached to the verb stem and plays a 
grammatical role of modifying its head noun.  
However, the problem is that these verb ending 
morphemes do not provide any information 
about the grammatical function associated with 
the relevant head noun. 
Take 3.a-c for examples. 
 
3.a Sigdang-eseo(restaurant) bab-eul(rice) 
meog-eun(ate) geu(he). 
(He who ate a rice in a restaurant.) 
 
3.b Sigdang-eseo geu-ga meog-eun bab. 
(the rice which he ate in a restaurant.) 
 
3.c Geu-ga bab-eul meog-eun sigdang. 
(the restaurant in which he ate a rice.) 
 
Despite all three sentences above have the same 
adnominal ending ?eun?, the grammatical 
function of each relative noun is different. The 
grammatical function of the head noun in 3.a is 
subject, in 3.b, object and in 3.c, adverbial. 
The word order gives little information 
because Korean is a partly free word-order 
language and some complements of a verb may 
be frequently omitted.  For example, in sentence 
4, the verb of relative clause ?sigdang-eseo 
meog-eun(who ate in the restaurant or which one 
ate in the restaurant)? have two omitted 
complements which are subject and object. So 
?bab? can be identified as either of subject or 
object in the relative clause. 
 
4. Sigdang-eseo(restaurant) meog-eun(ate) 
bab-eul(rice) na-neun(I) boass-da(saw). 
(I saw the rice which (one) ate in a restaurant.) 
 
Korean appositive adnoun clauses have the same 
syntactic structure of relative adnoun clauses as 
in example 2 in section 2. 
Yoon et al (1997) classified adnoun clauses 
into relative adnoun clauses and appositive 
adnoun clauses based on a complement noun 
dictionary which was manually constructed, and 
then tries to find the grammatical function of a 
relative noun using lexical co-occurrence 
information. But as shown in example 5, a 
complement noun can be used as a relative noun, 
so Yoon et al (1997)?s method using the 
dictionary has some limits. 
 
5. Geu-ga(he) balgyeonha-n(discover) 
sasil-eul(truth) mal-haess-da(talk). 
(He talked about the truth which he discovered.) 
 
Li et al (1998) described a method using 
conceptual co-occurrence patterns and syntactic 
role distribution of relative nouns. Linguistic 
information is extracted from corpus and 
thesaurus.  However,  he did not take into account 
appositive adnoun clauses but only considered 
relative adnoun clauses.  
Lee et al (2001) classified adnoun clauses into 
appositive clauses and one of relative clauses. He 
proposed a stochastic method based on a 
maximum likelihood estimation and adopted the 
backed-off model in estimating the probability 
P(r|v,e,n) to handle sparse data problem (the 
symbols r, v, e and n represent the grammatical 
relation, the verb of the adnoun clause, the 
adnominal verb ending, and the head noun 
modified by an adnoun clause, respectively).  
The backed-off model handles unknown words 
effectively but it may not be used with all the 
backed-off stages in real field problems where 
higher accuracy is needed. 
 
3 Support Vector Machines 
 
The technique of Support Vector 
Machines(SVM) is a learning approach for 
solving two-class pattern recognition problems 
introduced by Vapnik (1995).  It is based on the 
Structural Risk Minimization principle for which 
error-bound analysis has been theoretically 
motivated (Vapnik, 1995).  The problem is to 
find a decision surface that separates the data 
points in two classes optimally. A decision 
surface by SVM for linearly separable space is a 
hyperplane H : y = w?x ? b = 0 and two 
hyperplanes parallel to it and with equal 
distances to it,  
 
H1 : y = w?x ? b = +1, 
H2 : y = w?x ? b = ?1, 
 
with the condition that there are no data points 
between H1 and H2, and the distance between H1 
and H2 is maximized.  
   We want to maximize the distance between H1 
and H2.  So there will be some positive examples 
on H1 and some negative examples on H2.  These 
examples are called support vectors because they 
only participate in the definition of the separating 
hyperplane, and other examples can be removed 
and/or moved around as long as they do not cross 
the planes H1 and H2.  In order to maximize the 
distance, we should minimize ||w|| with the 
condition that there are no data points between 
H1 and H2, 
 
w?x ? b ? +1 for yi = +1, 
w?x ? b ? ?1 for yi = ?1. 
 
The SVM problem is to find such w and b that 
satisfy the above constraints. It can be solved 
using quadratic programming techniques(Vapnik, 
1995). The algorithms for solving linearly 
separable cases can be extended so that they can 
solve linearly non-separable cases as well by 
either introducing soft margin hyperplanes, or by 
mapping the original data vectors to a higher 
dimensional space where the new features 
contain interaction terms of the original features, 
and the data points in the new space become 
linearly separable (Vapnik, 1995). We use 
SVMlight1 system for our experiment (Joachimes, 
1998). 
SVM performance is governed by the features. 
We use the verb of each adnoun clause, the 
adnominal verb ending and the head noun of the 
noun phrase. To reflect context of sentence, we 
use the previous noun phrase, which is located 
right before the verb, and its functional 
morpheme. The previous noun phrase is the 
surface level word list not the previous argument 
for the verb in adnoun clause. Part of 
speech(POS) tags of all lexical item are also used 
as feature. For example, in sentence ?Igeos-eun 
geu-ga sseu-n chaeg-ida.?, ?geu? is a previos 
noun pharse feature, ?ga? is its functional 
morpheme feature, ?sseu? is a verb feature, ?n? is 
a verb ending feature, ?chaeg? is a head noun 
feature and all POS tags of lexical items are 
features. 
Because we found that the kernel of SVM does 
not strongly affect the performance of our 
problem through many experiments, we 
concluded that our problem is linearly separable. 
Thus we will use the linear kernel only. 
As the SVMs is a binary class classifier, we 
construct four classifiers, one for each class. 
Each classifier constructs a hyperplane between 
one class and other classes. We select the 
classifier which has the maximal distance from 
the margin for each test data point. 
 
4 Experimental Results 
 
We use the tree tagged corpus of Korean 
Information Base which is annotated as a form of 
phrase structured tree (Lee, 1996). It consists of 
11,932 sentences, which corresponds to 145,630 
eojeols.  Eojeol is a syntactic unit composed of 
one lexical morpheme with multiple functional 
morphemes optionally attached to it.  We extract 
the verb of an adnoun clause and the noun phrase 
which is modified by the adnoun clause. We 
regard an eojeol consisting of a main verb and 
auxiliary-verbs as a single main-verb eojeol. In 
case of a complex verb, we only take into account 
the first part of it. Every verb which has 
adnominal morphemes and the head word of a 
noun phrase which is modified by adnoun clause, 
were extracted.  Because Korean is head-fiinal 
                                                     
1  The SVMlight system is available at 
http://ais.gmd.de/~thorsten/svm_light/. 
language, we regard the last noun of a noun 
phrase as the head word of the noun phrase. 
   The total number of extracted pairs of verb and 
noun is 18,264. The grammatical function of 
each pair is manually tagged.  To experiment, the 
data was subdivided into a learning data set from 
10,739 sentences and a test data set from 1,193 
sentences. We use 16,413 training data points 
and 1,851 test data points in all experiments. 
Table 1 shows an accuracy at each of the 
grammatical categories between an adnoun 
clause and a noun phrase with SVMs, compared 
with the backed-off method which is proposed by 
(Lee, 2001).  
 
Table 1. the acuracy of SVM and Backed-off 
model at each of the grammatical categories 
 subj obj adv app total 
SVM 84.4 62.9 92.0 97.5 88.7 
SVM with 
context 
feature 
88.8 75.6 89.6 96.1 90.8 
Backed-off 86.2 42.0 62.0 91.7 83.5 
proportion in 
the training 
data(%) 
52.8 4.5 6.7 36.0 100
 
It should be noted that SVM outperforms 
Backed-off model in Table 1.  By using context 
information we acquire an improvement of 
overall 2.1%. 
Table 2 represents the accuracies of the 
proposed model compared with the Li?s model. 
The category ?appositive? is not taken into 
account for fair comparison.  It should be noted 
that Li et al (1998)?s results are drawn from most 
frequent 100 verbs while ours, from 4,684 verbs 
all of which are in the training corpus.  
Table 2. the accuracy of SVM without 
considering appositive clauses 
 subj obj adv total 
SVM with 
context 
feature 
94.1 87.8 85.7 93.3 
Li et al 
(1998) 90 92 89.2 90.4 
 
It is shown that our proposed model shows the 
better overall result in determining the 
grammatical function between an adnoun clause 
and its modifying head noun. 
Most errors are caued by lack of lexical 
information. Actually, lexical information in 
19% of the test data has not occurred in the 
training data. The other errors are caused by the 
characteristics that some verbs in adnoun clauses 
can have dual subjects which we did not consider 
in the problem. Take 6 for an example. 
 
6. Nun-i(eyes) keu-n(be big) Cheolsu 
(Cheolsu who has big eyes) 
 
In example 6, the context NP is ?nun? and its 
functional word is ?i? which may represent  that it 
is subject of ?keu-da?, thus system may wrongly 
determine that ?Cheolsu? is not a subject of 
?keu-da? because the subject of ?keu-da? has been 
made with ?nun?. However, both ?Cheolsu? and 
?nun? are the subjects of ?keu-da?. 
 
5 Conclusion and Future works 
 
Adnoun clause is a typical complex sentence 
structure of Korean. There are various types of 
grammatical relations between an adnoun clause 
and its relevant noun phrase.  Unlike in between 
general content words and modifying clauses 
where their grammatical relations can be easily 
extrated in terms of various grammatical 
characteristics by the functional morphemes, the 
functional morphemes are omitted in a noun 
phrase when it is modified by an adnoun clause. 
This omission makes it difficult to characterize 
their grammatical relation. 
In this paper, we used SVM to take care of this 
problem and analyze the relation between noun 
phrase and adnoun clause.  We reflected context 
information by using the previous word of the 
verb in adnoun clauses as feature. Context 
information helped the grammatical function 
analysis between adnoun clause and the head 
noun.  The SVM can also handle the sparse data 
problem as the backed-off model does. We 
acquired overall accuracy of 90.8%, which is 
obviously an improvement from the previous 
works. 
   In the future, we plan to compare with other 
machine learning methods and to enhance our 
system by using a publicly available Korean 
thesaurus to increases general accuracy. More 
data needs to be collected for further 
performance improvement.  We will also work 
on utilizing the proposed model in some partial 
parsing problem. 
References 
Chang, Suk-Jin, 1995. Information-based 
Korean Grammar, Hanshin Publishing Co. 
Yoon, J., 1997. Syntactic Analysis for Korean 
Sentences Using Lexical Association Based on 
Co-occurrence Relation, Ph.D. Dissertation, 
Yonsei University. 
Katz, S., 1987. Estimation of Probabilities 
from Sparse Data for the Language Model 
Component of a Speech Recogniser. IEEE 
Transactions on Acoustics, Speech, and Signal 
processing, Vol. ASSP-35, No. 3. 
Lee, Ik-Sop, Hong-Pin Im, 1986, Korean 
Grammar Theory, Hagyeonsa. 
Lee, Kong Joo, Jae-Hoon Kim, Key-Sun Choi, 
and Gil Chang Kim. 1996, Korean syntactic 
tagset for building a tree annotated corpus. 
Korean Journal of Cognitive Science, 7(4):7-24. 
Lee, Songwook, Tae-Yeoub Jang, Jungyun 
Seo. 2001, The Grammatical Function Analysis 
between Adnoun Clause and Noun Phrase in 
Korean, In Proceedings of the Sixth Natural 
Language Processing Pacific Rim Symposium, 
pp709-713.  
Li, Hui-Feng, Jong-Hyeok Lee, Geunbae Lee, 
1998. Identifying Syntactic Role of Antecedent 
in Korean Relative Clause Using Corpus and 
Thesaurus Information. In Proceeding of 
COLING-ACL, pp.756-762. 
Vapnik, Vladimir N. 1995, The Nature of 
Statistical Learning Theory. Springer, New 
York. 
Joachims, Thorsten. 1998, Text 
Categorization with Support Vector Machines: 
Learning with Many Relevant Features. In 
European Conference on Machine Learning, pp. 
137-142. 
Text Categorization using Feature Projections
Youngjoong Ko
Department of Computer Science,
Sogang University
1 Sinsu-dong, Mapo-gu
Seoul, 121-742, Korea
kyj@nlpzodiac.sogang.ac.kr,
Jungyun Seo
Department of Computer Science,
Sogang University
1 Sinsu-dong, Mapo-gu
Seoul, 121-742, Korea
seojy@ccs.sogang.ac.kr
Abstract
This paper proposes a new approach for text
categorization, based on a feature projection
technique. In our approach, training data are
represented as the projections of training
documents on each feature. The voting for a
classification is processed on the basis of
individual feature projections. The final
classification of test documents is
determined by a majority voting from the
individual classifications of each feature.
Our empirical results show that the proposed
approach, Text Categorization using Feature
Projections (TCFP), outperforms k-NN,
Rocchio, and Na?ve Bayes. Most of all,
TCFP is about one hundred times faster than
k-NN. Since TCFP algorithm is very simple,
its implementation and training process can
be done very easily. For these reasons,
TCFP can be a useful classifier in the areas,
which need a fast and high-performance text
categorization task.
Introduction
An issue of text categorization is to classify
documents into a certain number of pre-defined
categories. Text categorization is an active
research area in information retrieval and
machine learning. A wide range of supervised
learning algorithms has been applied to this
issue, using a training data set of categorized
documents. The Na?ve Bayes (McCalum et al,
1998; Ko et al, 2000), Nearest Neighbor (Yang
et al, 2002), and Rocchio (Lewis et al, 1996)
are well-known algorithms.
Among these learning algorithms, we focus
on the Nearest Neighbor algorithm. In particular,
the k-Nearest Neighbor (k-NN) classifier in text
categorization is one of the state-of-the-art
methods including Support Vector Machine
(SVM) and Boosting algorithms. Since the
Nearest Neighbor algorithm is much simpler
than the other algorithms, the k-NN classifier is
intuitive and easy to understand, and it learns
quickly. But the weak point of k-NN is too slow
at running time. The main computation is the
on-line scoring of all training documents, in
order to find the k nearest neighbors of a test
document. In order to reduce the scaling
problem in on-line ranking, a number of
techniques have been studied in the literature.
Techniques such as instance pruning technique
(Wilson et al, 2000) and projection (Akkus et al,
1996) are well known.
The instance pruning technique is one of the
most straightforward ways to speed
classification in a nearest neighbor system. It
reduces time necessary and storage requirements
by removing instances from the training set. A
large number of such reduction techniques have
been proposed, including the Condensed Nearest
Neighbor Rule (Hart, 1968), IB2 and IB3 (Aha et
al., 1991), and the Typical Instance Based
Learning (Zhang, 1992). These and other
reduction techniques were surveyed in depth in
(Wilson et al, 1999), along with several new
reduction techniques called DROP1-DROP5. Of
these, DROP4 had the best performance.
Another trial to overcome this problem exists
on feature projections. Akkus and Guvenir
presented a new approach to classification based
on feature projections (Akkus et al, 1996). They
called their resulting algorithm k-Nearest
Neighbor on Feature Projections (k-NNFP). In
this approach, the classification knowledge is
represented as the sets of projections of training
data on each feature dimension. The
classification of an instance is based on a voting
by the k nearest neighbors of each feature in a
test instance. The resulting system allowed the
classification to be much faster than that of
k-NN and its performance were comparable with
k-NN.
In this paper, we present a particular
implementation of text categorization using
feature projections. When we applied the feature
projection technique to text categorization, we
found several problems caused by the special
properties of text categorization problem. We
describe these problems in detail and propose a
new approach to solve them. The proposed
system shows the better performance than k-NN
and it is much faster than k-NN.
The rest of this paper is organized as follows.
Section 1 simply presents k-NN and k-NNFP
algorithm. Section 2 explains a new approach
using feature projections. In section 3, we
discuss empirical results in our experiments.
Section 4 is devoted to an analysis of time
complexity and strong points of the new
proposed classifier. The final section presents
conclusions.
1. k-NN and k-NNFP Algorithm
In this section, we simply describe k-NN and
k-NNFP algorithm.
1.1 k-NN Algorithm
As an instance-based classification method,
k-NN has been known as an effective approach
to a broad range of pattern recognition and text
classification problems (Duda et al, 2001; Yang,
1994). In k-NN algorithm, a new input instance
should belong to the same class as their k nearest
neighbors in the training data set. After all the
training data is stored in memory, a new input
instance is classified with the class of k nearest
neighbors among all stored training instances.
For the distance measure and the document
representation, we use the conventional vector
space model in text categorization; each
document is represented as a vector of term
weights, and similarity between two documents
is measured by the cosine value of the angle
between the corresponding vectors (Yang et al,
2002).
Let a document d with n terms (t) be
represented as the feature vector:
>=< ),(),...,,(),,( 21 dtwdtwdtwd n
rrrr (1)
We compute the weight vectors for each
document using one of the conventional TF-IDF
schemes (Salton et al, 1988). The weight of
term t in document d is calculated as follows:
d
nNdttfdtw tr
rr )/log()),(log1(),( ?+= (2)
where
i) ),( dtw
r
is the weight of term t in document d
r
ii) ),( dttf
r
is the within-document Term Frequency (TF)
iii) )/log(
t
nN is the Inverted Document Frequency
(IDF)
iv) N is the number of documents in the training set
v) nt is the number of training documents in which t
occurs
vi) ? ?= dt dtwd r rr 2),( is the 2-norm of vector dr
Given an arbitrary test document d, the k-NN
classifier assigns a relevance score to each
candidate category cj using the following
formula:
?
??
?=
jk DdRd
j dddcs
I
rr
rrr
)(
),cos(),( (3)
where )(dRk
r
denotes a set of the k nearest
neighbors of document d and Dj is a set of
training documents in class cj.
1.2 k-Nearest Neighbor on Feature Projection
(k-NNFP) Algorithm
The k-NNFP is a variant of k-NN method. The
main difference is that instances are projected on
their features in the n-dimensional space (see
figure 1) and distance between two instances is
calculated according to a single feature. The
distance between two instances di and dj with
regard to m-th feature tm is distm(tm(i), tm(j)) as
follows:
),(),())(),(( jmimmm dtwdtwjtitdistm
rr
?= (4)
where )(itm denotes m-th feature t in a instance
id
r
.
The classification on a feature is done
according to votes of the k-nearest neighbors of
that feature in a test instance. The final
classification of the test instance is determined
by a majority voting from individual
classification of each feature. If there are n
features, this method returns n? k votes whereas
k-NN method returns k votes.
2. A New Approach of Text
Categorization on Feature Projections
First of all, we show an example of feature
projections in text categorization for more easy
understanding. We then enumerate the problems
to be duly considered when the feature
projection technique is applied to text
categorization. Finally, we propose a new
approach using feature projections to overcome
these problems.
2.1 An Example of Feature Projections in
Text Categorization
We give a simple example of the feature
projections in text categorization. To simplify
our description, we suppose that all documents
have just two features (f1 and f2) and two
categories (c1 and c2). The TF-IDF value by
formula (2) is used as the weight of a feautre.
Each document is normalized as a unit vector
and each category has three instances:
{ }3211 ,, dddc = and { }6542 ,, dddc = . Figure 1
shows how document vectors in conventional
vector space are transformed into feature
projections and stored on each feature dimension.
The result of feature projections on a term (or
feature) can be seen as a set of weights of
documents for the term. Since a term with 0.0
weight is useless, the size of the set equals to the
DF value of the term.







	














	



	

			
	

	

		
	

















	








	


 



 



 



 



 

	
	
	

	
	
	



	
	
Figure 1. Feature representation on feature
projections
2.2 Problems in Applying Feature Projections
to Text Categorization
There are three problems: (1) the diversity of the
Document Frequency (DF) values of terms, (2)
the property of using TF-IDF value of a term as
the weight of the feature, and (3) the lack of
contextual information.
2.2.1 The diversity of the Document Frequency
values of terms
Table 1 shows a distribution of the DF values of
the terms in Newsgroup data set. The numerical
values of Table 1 are calculated from training
data set with 16,000 documents and 10,000
features chosen by feature selection. The k in
fourth column means the number of nearest
neighbors selected in k-NNFP; the k in k-NNFP
was set to 20 in our experiments.
Table 1. A distribution of the DF values of the terms
in Newsgroup data set
Average
DF
maximum
DF
Minimum
DF
The # of
features
DF < k (20)
54.59 8,407 4 6,489
According to Table 1, more than a half of the
features have the DF values less than k (20).
This result is also explained by Zipf?s law. The
problem is that some features have the DF
values less than k while other features have the
DF values much greater than k. For a feature that
has a DF value less than k, all the elements of
the feature projections on the feature could and
should participate for voting. In this case, the
number of elements chosen for voting is less
than k. For other features, only maximum k
elements among the elements of the feature
projections should be chosen for voting.
Therefore, we need to normalize the voting ratio
for each feature. As shown in formula (5), we
use a proportional voting method to normalize
the voting ratio.
2.2.2 The property of using TF-IDF value of a
term as weight of a feature
The TF-IDF value of a term is their presumed
value for identifying the content of a document
(Salton et al, 1983). On feature projections,
elements with a high TF-IDF value for a feature
become more useful classification criterions for
the feature than any elements with low TF-IDF
values. Thus we use only elements with TF-IDF
values above the average TF-IDF value for
voting. The selected elements also participate for
proportional voting with the same importance as
TF-IDF value of each element. The voting ratio
of each category cj in a feature tm(i) of a test
document id
r
is calculated by the following
formula:
??
??
?=
mmmm
jj
Ilt
lm
Ilt
mlmm dtwltcydtwitcr
)()(
),())(,(),())(,(
rr (5)
In above formula, Im denotes a set of elements
selected for voting and { }1.0))(,( ?ltcy mj is a
function; if the category for a element )(ltm is
equal to jc , the output value is 1. Otherwise, the
output value is 0.
2.2.3 The lack of contextual information
Since each feature votes separately on feature
projections, contextual information is missed.
We use the idea of co-occurrence frequency for
applying contextual information to our
algorithm.
To calculate a co-occurrence frequency value
between two terms ti and tl, we count the number
of documents that include both terms. It is
separately calculated in each category of training
data. Finally, the co-occurrence frequency value
of two terms is obtained by a maximum value
among co-occurrence frequency values in each
category as follows:
{ }),,(max),( jli
c
li cttcottco
j
= (6)
where ),( li ttco denotes a co-occurrence
frequency value of ti and tl, and
),,( jli cttco denotes a co-occurrence frequency
value of ti and tl in a category cj.
TF-IDF values of two terms ti and tj, which
occur in a test document d, are modified by
reflecting the co-occurrence frequency value.
That is, the terms with a high co-occurrence
frequency value and a low category frequency
value could have higher term weights as follows:
where i) tw(ti,d) denotes a modified term weight
assigned to term ti, ii) cf denotes the category
frequency, the number of categories in which ti
and tj co-occur, and iii) ),(max jttco i is the
maximum value among all co-occurrence
frequency values.
Finally, in order to apply these improvements
(formulae (5) and (7)) to our algorithm, we
calculate the voting score of each category jc
in mt of a test document id
r
as the following
formula:
))(,(),())(,( itcrdttwitcs mimm jj ?=
r
(8)
Here, since the modified TF-IDF value of a
feature in a test document has to be also
considered as an important factor, it is used for
voting score instead of the simple voting value
(1).
(7))),(log(max1
)),(log(1
)log(1
11),(),( ???
?
???
?
???
?
???
?
+
+
????
?
???
?
+
+?=
ji
ji
ii ttco
ttco
cfdtwdttw
rr
2.3 A New Text Categorization Algorithm
using Feature Projections
A new text categorization algorithm using
feature projections, named TCFP, is described
in the following:
In training phase, our algorithm needs only a
very simple process; the training documents are
projected on their each feature and numerical
values for the proportional voting (formula (5))
are calculated.
3. Empirical Evaluation
3.1 Data Sets and Experimental Settings
To test our proposed approach, we used two
different data sets. For fair evaluation, we used
the five-fold cross-validation method. Therefore,
all results of our experiments are averages of
five runs.
The Newsgroups data set, collected by Ken
Lang, contains about 20,000 articles evenly
divided among 20 UseNet discussion groups
(McCalum et al, 1998). After removing words
that occur only once or on a stop word list, the
average vocabulary from five training data has
51,325 words (with no stemming). The second
data set comes from the WebKB project at
CMU (Yang et al, 2002). We use the four most
populous entity-representing categories: course,
faculty, project, and student. The resulting
data set consists of 4,198 pages with a
vocabulary of 18,742 words. It is an uneven data
set; the largest category has 1,641 pages and the
smallest one has 503 pages.
We applied statistical feature selection at a
preprocessing stage for each classifier, using a
2? statistics (Yang et al, 1997).
To compare TCFP to other algorithms for
speeding classification, we implemented
k-NNFP and k-NN with reduction. We used
DROP4 as reduction technique (Wilson et al,
1999). By DROP4, only 26% of the original
training documents in both data sets was
retained. The k in k-NNFP was set to 20 and
the k in k-NN with reduction was set to 30. In
addition, we implement other classifiers: Naive
Bayes, k-NN, and Rocchio classifier. The k in
k-NN was set to 30 and and ?=16 and ?=4 were
used in Rocchio classifier.
As performance measures, we followed the
standard definition of recall, precision, and F1
measure. For evaluating performance average
across categories, we used the micro-averaging
method.
3.2 Experimental Results
3.2.1 Comparison of TCFP and k-NN (and
other algorithms for speeding classification )
Figure 2 and Table 2 show results from TCFP,
k-NN, k-NN with reduction, and k-NNFP. In
addition, we added other type of TCFP to our
experiment. It was TCFP without contextual
information (not using formula (7)).








    	    
 









 	


 
	

Figure 2. Comparison of TCFP , k-NN, k-NNFP, and
k-NN with reduction
test document: d
r
=<t1,t2,?,tn>, category set:
C={c1,c2,?,cm}
begin
for each category cj
vote[cj] =0
for each feature ti
tw(ti,d) is calculated by formula (7)
/* majority voting*/
for each feature ti
for each category cj
vote[cj]=vote[cj]+tw(ti,d)?r(cj,ti)
by formula (8)
for each category cj
prediction = ][maxarg j
c
cvote
j
return prediction
end
Table 2. The top micro-average F1 of each classifier
TCFP
TCFP
without
context
k-NN k-NNFP
k-NN
with
reduction
85.41 85.14 85.15 81.93 81.34
As a result, TCFP achieved the highest
micro-average F1 score. Also, TCFP without
contextual information presented the nearly
same performance as k-NN. Although, over all
vocabulary sizes, TCFP without contextual
information achieved little lower performance
than TCFP, it also can be useful classifier for its
simplicity and the fast running time(see Table 5).
3.2.2 Comparison with other classifiers
The comparisons with other classifiers are
shown in Figure 3 and Table 3. In this
experiment, we used Na?ve Bayes, and Rocchio
classifier.









    	    
 









	




   	


Figure 3. Comparison with other classifiers
Table 3. The top micro-average F1 of each classifier
TCFP k-NN NB Rocchio
85.41 85.15 82.51 81.68
The result shows that TCFP produced the higher
performance than the other classifiers.
3.2.3 Comparison of performances in an
uneven data set, WebKB.
In the above experiments, the Newsgroup data
set, which is an evenly divided data set, was
used. If we use an uneven data set, we can face a
problem. The cause of the problem is that a
category of the larger size has more voting
candidates than a category of the smaller size.
We simply modified the majority voting score
calculated in TCFP algorithm by the following
formula:
{ } ??
???
?
?= ),(/),(max][][ ji
c
jj cdnumcdnumcvotecvote
i
(9)
where num(d,cj) denotes the number of training
document in category cj.
The results of the modified algorithm are
shown in Table 4. As we can see in this table, the
modified TCFP algorithm performed similarly
on the uneven data set, WebKB; the modified
TCFP algorithm achieved the highest score.
Table 4. The top micro-average F1 of each classifier
TCFP k-NN NB Rocchio k-NNFP
k-NN
with
reduction
86.6 84.83 85.22 85.98 82.78 81.34
3.2.4 Run-time observation
Table 5 shows the average running times in CPU
seconds for each classifier on the Newsgroup
data. Note that we included only testing phase
with 4,000 documents.
Table 5. Average running time of each classifier
TCFP
without
context
Rocchio NB TCFP
k-NN
with
reduction
k-NN
0.69 0.8 1.22 1.38 37.97 142.5
Since the computations depend on the
vocabulary sizes, we calculated the above
numerical value by averaging running times
from 1,000 to 10,000 terms. In Table 5, the
running time of TCFP is similar to other faster
classifiers: Rocchio and Na?ve Bayes. Also it is
about one hundred times faster than that of k-NN.
Note that TCFP without contextual information
is the fastest classifier.
4. Discussions
First of all, time complexities between k-NN and
TCFP are compared. Using the inverted-file
indexing of training documents, the time
complexity of k-NN is O(m2l/n) (Yang, 1994),
where m is the number of unique words in the
document, l is the number of training documents,
and n is the number of unique terms in the
training collection. TCFP has the time
complexity of O(m2). Even more, the time
complexity of TCFP without contextual
information is O(mc), where c is the number of
categories. That is, the classification of TCFP
requires a simple calculation in proportion to the
number of unique terms in the test document.
On the other hand, in k-NN, a search in the
whole training space must be done for each test
document.
The other strong points of TCFP are the
simplicity of algorithm and high-performance.
Since the algorithm of TCFP is very simple like
k-NN, TCFP can be implemented quite easily
and its training phase can also be a simple
process. In our experiments, we achieved the
better performance than k-NN. We analyze that
our algorithm is more robust from irrelevant
features than k-NN. When a document contains
irrelevant features, the angle of the document
vector is changed in k-NN. In TCFP, however,
the irrelevant features contribute to only voting
of the features. Hence TCFP decreases the bad
effect of the irrelevant features.
Conclusions
In this paper, a new type of text categorization,
TCFP, has been presented. This algorithm has
been compared with k-NN and other classifiers.
Since each feature in TCFP individually
contributes to the classification process, TCFP is
robust from irrelevant features. By the simplicity
of TCFP algorithm, its implementation and
training process can be done very easily. The
experimental results show that, on the
performance, TCFP is superior to Rocchio,
Na?ve Bayes, and k-NN. Moreover, it
outperforms other classifiers for speeding
classification such as k-NNFP and k-NN with
reduction. In running time observation, TCFP is
about one hundred times faster than k-NN.
Therefore, we can use TCFP in the areas, which
require a fast and high-performance text
classifier.
References
Aha, D. W., Dennis K., and Marc K. A. (1991)
Instance-Based Learning Algorithms. Machine
Learning, vol. 6, pp. 37-66.
Akkus A. and Guvenir H.A. (1996) K Nearest
Neighbor Classification on Feature Projections. In
Proceedings of ICML? 96, Itally, pp. 12-19.
Duda R.O., Hart P.E., and Stork D.G. (2001) Pattern
Classification. John Wiley & Sons, Second Edition.
Hart, P. E. (1968) The Condensed Nearest Neighbor
Rule. Institute of Electrical and Electronics
Engineers Transactions on Information Theory. Vol.
14, pp. 515-516.
Ko Y. and Seo J. (2000) Automatic Text
Categorization by Unsupervised Learning. In
Proceedings of the 18th International Conference on
Computational Linguistics (COLING), pp. 453-459.
Lewis D.D., Schapire R.E., Callan J.P., and Papka R.
(1996) Training Algorithms for Linear Text
Classifiers. In Proceedings of the 19th International
Conference on Research and Development in
Information Retrieval (SIGIR?96), pp.289-297.
McCallum A. and Nigam K. (1998) A Comparison of
Event Models for Na?ve Bayes Text Classification.
AAAI ?98 workshop on Learning for Text
Categorization. pp. 41-48.
Salton G. and McGill M.J. (1983) Introduction to
Modern Information Retrieval. McGraw-Hill, Inc.
Salton G. and Buckley C. (1988) Term weighting
approaches in automatic text retrieval. Information
Processing and Management, 24:513-523.
Wilson D. R. and Martinez T. R. (2000) An
Integrated Instance-based Learning Algorithm,
Computational Intelligence, Volume 16, Number 1,
pp. 1-28.
Wilson, D. R. and Martinez T. R. (2000) Reduction
Techniques for Exemplar-Based Learning
Algorithms. Machine Learning, vol. 38, no. 3, pp.
257-286.
Yang Y. (1994) Expert network: Effective and
efficient learning from human decisions in text
categorization and retrieval. In Proceedings of 17th
International ACM SIGIR Conference on Research
and Development in Information Retrieval
(SIGIR?94), pp 13-22.
Yang Y. and Pedersen J.P. (1997) Feature selection
in statistical learning of text categorization. In The
Fourteenth International Conference on Machine
Learning, pages 412-420.
Yang Y., Slattery S., and Ghani R. (2002) A study of
approaches to hypertext categorization, Journal of
Intelligent Information Systems, Volume 18,
Number 2.
Zhang, J. (1992) Selecting Typical Instances in
Instance-Based Learning. Proceedings of the Ninth
International Conference on Machine Learning.
Automatic Text Categorization using the Importance of Sentences
Youngjoong Ko, Jinwoo Park, and Jungyun Seo
Department of Computer Science,
Sogang University
1 Sinsu-dong, Mapo-gu
Seoul, 121-742, Korea
{kyj,jwpark}@nlpzodiac.sogang.ac.kr, seojy@ccs.sogang.ac.kr
Abstract
Automatic text categorization is a problem
of automatically assigning text documents to
predefined categories. In order to classify
text documents, we must extract good
features from them. In previous research, a
text document is commonly represented by
the term frequency and the inverted
document frequency of each feature. Since
there is a difference between important
sentences and unimportant sentences in a
document, the features from more important
sentences should be considered more than
other features. In this paper, we measure the
importance of sentences using text
summarization techniques. Then a document
is represented as a vector of features with
different weights according to the
importance of each sentence. To verify our
new method, we conducted experiments on
two language newsgroup data sets: one
written by English and the other written by
Korean. Four kinds of classifiers were used
in our experiments: Na?ve Bayes, Rocchio,
k-NN, and SVM. We observed that our new
method made a significant improvement in
all classifiers and both data sets.
Introduction
The goal of text categorization is to classify
documents into a certain number of pre-defined
categories. Text categorization is an active
research area in information retrieval and
machine learning. A wide range of supervised
learning algorithms has been applied to this
problem using a training data set of categorized
documents. For examples, there are the Na?ve
Bayes (McCallum et al, 1998; Ko et al, 2000),
Rocchio (Lewis et al, 1996), Nearest Neighbor
(Yang et al, 2002), and Support Vector
Machines (Joachims, 1998).
A text categorization task consists of a
training phase and a text classification phase.
The former includes the feature extraction
process and the indexing process. The vector
space model has been used as the conventional
method for text representation (Salton et al,
1983). This model represents a document as a
vector of features using Term Frequency (TF)
and Inverted Document Frequency (IDF). This
model simply counts TF without considering
where the term occurs. But each sentence in a
document has different importance for
identifying the content of the document. Thus,
by assigning a different weight according to the
importance of the sentence to each term, we can
achieve better results. For this problem, several
techniques have been studied. First, term
weights were differently weighted by the
location of a term, so that the structural
information of a document was applied to term
weights (Murata et al, 2000). But this method
supposes that only several sentences, which are
located at the front or the rear of a document,
have the important meaning. Hence it can be
applied to only documents with fixed form such
as articles. The next technique used the title of a
document in order to choose the important terms
(Mock et al, 1996). The terms in the title were
handled importantly. But a drawback of this
method is that some titles, which do not contain
well the meaning of the document, can rather
increase the ambiguity of the meaning. This case
often comes out in documents with a informal
style such as Newsgroup and Email. To
overcome these problems, we have studied text
summarization techniques with great interest.
Among text summarization techniques, there are
statistical methods and linguistic methods
(Radev et al, 2000; Marcu et al, 1999). Since
the former methods are simpler and faster than
the latter methods, we use the former methods to
be applied to text categorization. Therefore, we
employ two kinds of text summarization
techniques; one measures the importance of
sentences by the similarity between the title and
each sentence in a document, and the other by
the importance of terms in each sentence.
In this paper, we use two kinds of text
summarization techniques for classifying
important sentences and unimportant sentences.
The importance of each sentence is measured by
these techniques. Then term weights in each
sentence are modified in proportion to the
calculated sentence importance. To test our
proposed method, we used two different
newsgroup data sets; one is a well known data
set, the Newsgroup data set by Ken Lang, and
the other was gathered from Korean UseNet
discussion group. As a result, our proposed
method showed the better performance than
basis system in both data sets.
The rest of this paper is organized as follows.
Section 1 explains the proposed text
categorization system in detail. In section 2, we
discuss the empirical results in our experiments.
Section 3 is devoted to the analysis of our
method. The final section presents conclusions
and future works.
1. The Proposed Text Categorization
System
The proposed system consists of two modules as
shown in Figure 1: one module for training
phase and the other module for text
classification phase. The each process of Figure
1 is explained in the following sections.


	








? 
	

		




	




R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 731 ? 741, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Improving Korean Speech Acts Analysis by Using 
Shrinkage and Discourse Stack 
Kyungsun Kim1, Youngjoong Ko2, and Jungyun Seo3
1
 Information Retrieval Division, Diquest.Inc, Seocho-dong,  
Seocho-gu, Seoul, 137-070, Korea 
kksun@diquest.com
2
 Dept. of Computer Engineering, Dong-A University, 840,  
Hadan 2-dong, Saha-gu,  Busan, 604-714, Korea 
yjko@dau.ac.kr
3
 Dept. of Computer Science and Interdisciplinary Program of Integrated Biotechnology, 
Sogang University, Seoul, 121-742, Korea 
seojy@sogang.ac.kr
Abstract. A speech act is a linguistic action intended by a speaker. It is impor-
tant to analyze the speech act for the dialogue understanding system because the 
speech act of an utterance is closely tied with the user?s intention in the utter-
ance. This paper proposes to use a speech acts hierarchy and a discourse stack 
for improving the accuracy of classifiers in speech acts analysis. We first adopt 
a hierarchical statistical technique called shrinkage to solve the data sparseness 
problem. In addition, we use a discourse stack in order to easily apply discourse 
structure information to the speech acts analysis. From the results of experi-
ments, we observed that the proposed model made a significant improvement 
for Korean speech acts analysis. Moreover, we found that it can be more useful 
when training data is insufficient. 
1   Introduction 
To understand a natural language dialogue, a dialogue system must be able to make 
out the speaker?s intentions indicated by utterances. Since the speech act of an utter-
ance is very important in understanding a speaker?s intentions, it is an essential part of 
a dialogue system. However, it is difficult to infer the speech act from a surface utter-
ance because the utterance may represent more than one speech act according to the 
context [5][7]. 
Various machine learning models have been used to efficiently classify speech acts 
such as MEM (Maximum Entropy Model) [1], HMM (Hidden Markov Model) with 
Decision Tree [8][11], Neural Network Model [5]. And there are also studies on 
methods of automatically selecting efficient features with useful information for 
speech acts analysis [5][10]. Since the machine learning models can efficiently ana-
lyze a large quantity of data and consider many different feature interactions, they can 
provide a means of associating features of utterances with particular speech acts. 
Generally, it is hard to create enough the number of examples for each speech act 
in the training examples. Thus this situation has been one of the main causes for  
errors occurred in speech acts analysis. That is, the sparse data problem from low 
732 K. Kim, Y. Ko, and J. Seo 
frequency of some speech acts has commonly occurred in the previous research [8]. 
Due to the problem, the accuracy of each speech act in previous research tends to be 
proportional to the frequency of each speech act in the training data. Therefore, we 
first focus on how to scale up statistical learning methods to solve the sparseness 
problem of training data in speech acts analysis. Then we propose to construct the 
commonly-available hierarchies of speech acts and apply a well-understood technique 
from Statistics called shrinkage to our speech acts analysis system. It provides im-
proved estimates of parameters that would otherwise be uncertain due to limited 
amounts of training data [3]. The technique uses a hierarchy to shrink parameter esti-
mates in data sparse children toward the estimates of the data-rich ancestors in ways 
that are probably optimal under the appropriate conditions [9]. We employ a simple 
form of shrinkage that creates new parameter estimates for a child by a linear interpo-
lation of all hierarchy nodes from the child to the root.  
In addition, discourse structure information can be used to identify the speech acts 
of utterances [1]. But most previous research has used only speech acts of previous 
utterances without considering discourse structure information to determine the speech 
act of current utterance. Therefore, in order to use discourse structure information for 
analyzing speech acts, we design a simple discourse stack. By using the discourse 
stack, the discourse structure information is easily applied to speech acts analysis. 
In this paper, we propose a new speech acts analysis model to improve the per-
formance by using shrinkage and discourse structure information. From the results of 
experiments, the proposed system showed significant improvement in comparison 
with previous research. 
The rest of this paper is organized as follows. Section 2 explains the proposed 
speech acts analysis system in detail. In section 3, we discuss the empirical results in 
our experiments. The final section presents conclusions. 
2   The Proposed Speech Acts Analysis System 
The proposed system consists of two modules as shown in Fig. 1: one module to  
extract  features  from  training  data  and  the  other module to build up a hierarchy of  
 	

 	

 	

 	







	

  	

		
	
 	

	
 	
Fig. 1. The overview of the proposed system 
 Improving Korean Speech Acts Analysis by Using Shrinkage and Discourse Stack 733
speech acts and estimate weights of each feature on the hierarchy by shrinkage. Each 
process of Fig. 1 is explained in the following sections. 
2.1   Feature Extraction 
2.1.1   Sentence Features Extraction 
We assume that clue words and a sequence of POS tags in an utterance provide very 
effective information for analyzing the speech act of the current utterance. We extract 
informative features for speech acts analysis using a Morphological analyzer; they are 
called the sentence features. The sentence features consist of content words annotated 
with POS tags and POS bi-grams of all words in an utterance. Fig. 2 shows an exam-
ple of sentence feature extraction. 
Input:
 	.
(My name is HongKildong.)
Morphological analyzer
The result of morphological analysis:

/np /j   /ncn /j /nq /jcp 	/ef ./s.
(My/np name/ncn is/jcp HongKildong/nq ./s.)
Feature extractor
Content Words:

/np /ncn /nq /jcp
(My/np name/ncn HongKilgong/nq is/jcp)
POS bi-grams:
np-j j-ncn ncn-j j-nq, nq-jcp jcp-ef ef-s.
Fig. 2 An example of sentence feature extraction
2.1.2   Context Features Extraction 
Most previous research uses the speech act of previous utterance as context feature 
(CF1 in Table 1) [5][8]. Since discourse structure information represents the relation-
ship between two consecutive utterances, it is efficient to use discourse structure  
For each utterance  
Begin 
if(Move a sub-dialogue?)  
Use speech acts of previous utterance and Sub-dialogue Start (SS) 
Push speech acts of current utterance. 
else if(Return from a sub-dialogue?)  
Use speech acts that pop in discourse stack and Sub-dialogue End (SE) 
else  
Use speech acts of previous utterance and Dialogue Continue (DC) 
End
734 K. Kim, Y. Ko, and J. Seo 
information for speech acts analysis [1]. Especially, the speech act of seventh utter-
ance in Table 1 (UID: 7) is tied with that of second utterance (UID: 2). In our system, 
we first design a discourse stack to easily detect discourse structure information and 
extract the discourse structure information from the discourse stack for context fea-
tures. Context features of our system consist of speech acts of previous utterance and 
markers of discourse structure information (CF2 in Table 1). An algorithm for dis-
course stack is described as the following:  
Table 1. An example of Context Feature
* UID: ID of utterances, DS: Discourse Structure, CF1: Using speech acts of previous utterances as features 
(Context Feature Type1), CF2: Using Discourse Structure Information by Discourse Stack as features 
(Context Feature Type2), Speech acts and discourse structure information were annotated by human. 
2.2   The Feature Weight Calculation by Shrinkage in a Hierarchy of Speech Acts 
Data sparseness is a common problem in mechanical learning fields. For speech acts 
analysis, the problem becomes more serious because it is a time-consuming and diffi-
cult task to collect dialogue examples and construct dialogue training data tagged with 
a lot of information for various application areas. Therefore, we apply the shrinkage 
technique to solve this data sparseness problem in speech acts analysis. The shrinkage 
technique was verified in its efficiency for text classification tasks learned with insuf-
ficient training data. Therefore, we first build up a hierarchy of speech acts to estimate 
the weight of features for each speech act by the shrinkage technique.  
2.2.1   The Hierarchy Construction for Speech Acts 
To model a dialogue system, the dialogue grammar has commonly used and it has 
observed  that  dialogues  consist  of adjacency pairs of the types of utterances such as  
UID DS Utterance
Speech 
Acts
CF1 CF2 
1 1 
????????????
(I would like to reserve a room) 
Inform 
Dialog-
start
Dialog-start,
NULL
2 1.1 
?????????
(What kind of room do you want?) 
Ask-ref Inform
Inform,  
SS
3 1.1.1 
????????????
(What kind of room do you have?) 
Ask-ref Ask-ref
Ask-ref,  
SS
4 1.1.1 
????????????.
(We have single and double rooms) 
Response Ask-ref
Ask-ref,  
DC
5 1.1.2 
???????
(How much are those rooms?) 
Ask-ref 
Re-
sponse
Response,  
DC
6 1.1.2 
?????????????????.
(Singles cost 30,000 won and doubles cost 
40,000 won.) 
Response Ask-ref
Ask-ref,  
DC
7 1.1 
?????????.
(A single room, please) 
Response
Re-
sponse
Ask-ref,  
SE
 Improving Korean Speech Acts Analysis by Using Shrinkage and Discourse Stack 735
Table 2. The Hierarchy of Speech Acts 
 Parent Child 
Ask-if 
Ask-ref 
Ask-confirm 
Offer 
Suggest 
Type1: Utterances of 
request type 
Request 
Accept 
Response 
Reject 
Type2: Utterances of 
response type 
Acknowledge 
Expressive 
Promise Type3: Utterances with a 
speaker emotion 
Closing 
Opening 
Introducing-oneself 
Correct 
Root 
Type4: Utterances of 
usually life 
Inform 
request-type and response-type [2][8]. Therefore, our speech acts hierarchy is built up 
according to this grammar. Table 2 shows the structure of our speech acts hierarchy. 
2.2.2   Mixture Weighting Model by Shrinkage in a Hierarchy of Speech Acts 
The shrinkage technique estimates the probability of a word as the weighted sum of 
the maximum-likelihood estimates from leaf to root in a hierarchy [9]. This estimate 
process can give us a possibility to resolve the data sparseness problem in some 
speech acts with insufficient examples. Fig.3 shows that the shrinkage-based estimate 
of the probability of a feature (? /np?) given a speech act class (?Accept?) is calcu-
lated from a weighted sum of the maximum-likelihood estimates from leaf to root. 
ROOT
 
 
?? 		

TYPE1
 
 
?? 

TYPE2
 
 
?? 

ACCEPT
 
 
?? 

? ? ?
 
 
 
 

 
 
	
 
 
	

?? 

1
type1.accept  ?? 


2
type1.accept   


3
type1.accept   		

Fig. 3. An example of the shrinkage-based estimate of the probability of features 
736 K. Kim, Y. Ko, and J. Seo 
Let }?,...,?,?{ 21 kjjj ???  be k such estimates, where jkj ?? =?  is the estimate at the leaf, 
and k-1 is the depth of speech acts ts in a hierarchy of Speech Acts. The interpolation 
weights among the ancestors of speech acts ts are written },...,,{ 21 kjjj ??? , where 
11 =? = ijki ? . We write j?

 for the new estimate of the speech act-conditioned feature 
probabilities based on shrinkage. The new estimate for the probability of feature 
tf given speech act js is as follows: 
11211 ?
...
??);( jtkjjtjjtjjjtjt sfP ???????? +++==

. (1)
We derive empirically optimal weights using the following iterative procedure: 
2.3   The SVM Classifier 
Support Vector Machines (SVM) is one of the state-of-the-art classifiers for classifi-
cation tasks [6][12]. Since SVM has shown the high performance in various research 
areas, we also employ it in our method. In our method, we use the linear models of-
fered by SVMlight [4] and jt?

, which are calculated by formula (1), are used as the 
feature weights of speech acts for the SVM classifier.  
Initialize:  
Set the j? ?s to some initial values, say k
i
j
1
=?
Iterate: 
1. Calculate the degree to which each estimate predicts the features tf  in the held-out 
feature set, jH , from speech acts js  : 
? ?? ??
==
jtjt Hw m
m
jt
m
j
i
jt
i
j
Hw
t
i
j
i
j fP ??
??
??
?
?
)generate toused was?(                (2) 
2. Compensate the degree for loss that is caused by large variation of each degree : 
m
m
m
jj
i
j
i
?
+=
???                                                      (3) 
3. Derive new weights by normalizing the s'? :
?
=
m
m
j
i
ji
j ?
??                                                       (4) 
Terminate: Upon convergence of the likelihood function
 Improving Korean Speech Acts Analysis by Using Shrinkage and Discourse Stack 737
Table 3. The part of mixture weights learned by shrinkage-based estimation 
Speech Acts Mixture Weights # training 
documents Root Parent Child Root Parent Child 
Ask-ref 0.289 0.32 0.39 Type1
Suggest 0.257 0.275 0.467 
Type2 Expressive 0.263 0.335 0.4 
Type3 Reject 0.259 0.269 0.47 
250 Root 
Type4 Inform 0.297 0.336 0.366 
Ask-ref 0.282 0.295 0.422 Type1 Suggest 0.217 0.22 0.562 
Type2 Expressive 0.229 0.279 0.49 
Type3 Reject 0.212 0.215 0.571 
8349 Root 
Type4 Inform 0.26 0.332 0.406 
3   Empirical Evaluation 
3.1   Experimental Data 
We used the Korean dialogue corpus which has used in previous research [1][5][8]. 
This corpus was transcribed from recordings in real fields such as hotel reservation, 
airline reservation and tour reservation and consists of 528 dialogues, 10,285 utter-
ances (19.48 utterances per dialogue). Each utterance in dialogues is manually anno-
tated with a speaker (SP), a speech act (SA) and a discourse structure (DS). This an-
notated dialogue corpus has 17 types of speech acts. Table 4 shows a part of the anno-
tated dialog corpus and Table 5 shows the distribution of speech acts in the annotated 
dialogue corpus. 
Table 4. A part of the annotated dialogue corpus
Tag Values 
SP Customer 
KS ??????????????????????.
EN
I?m a student and registered for a language course at University of Geor-
gia in U.S. 
SA Introducing-oneself 
DS [2] 
SP Customer 
KS ????????????????.
EN I have some questions about lodgings. 
SA Request 
DS [2] 
738 K. Kim, Y. Ko, and J. Seo 
Table 5. The distribution of speech acts in corpus
Speech act type Ratio (%) Speech act type Ratio (%) 
Accept 2.49 Introducing-oneself 6.75 
Acknowledge 5.75 Offer 0.4 
Ask-confirm 3.16 Opening 6.58 
Ask-if 5.36 Promise 2.42 
Ask-ref 13.39 Reject 1.07 
Closing 3.39 Request 4.96 
Correct 0.03 Response 24.73 
Expressive 5.64 Suggest 1.98 
Inform 11.9 Total 100 
We divided the annotated dialogue corpus into the training data with 428 dia-
logues, 8,349 utterances (19.51 utterances per dialogue), and the testing data with 100 
dialogues, 1,936 utterances (19.36 utterances per dialogue). 
3.2   Primary Experimental Results 
3.2.1   The Performances of Speech Acts Analysis Model Using Shrinkage and 
Discourse Stack 
In order to verify the proposed method, we made four kinds of speech acts analysis 
systems which use different kind of features. The Baseline System used default fea-
tures such as sentence features and context features [5]. The Second system (Type 1) 
was built up to verify the shrinkage technique. Its features were the same as those of 
the first system but they were weighted by the shrinkage technique. The third System 
(Type 2) used the discourse structure information from the proposed discourse stack 
without shrinkage. Finally, the fourth system (Type 3) combined the discourse struc-
ture information and the shrinkage technique. 
Table 6 shows the results of four speech acts analysis systems. As shown in Table 6, 
the performances of the proposed systems (Type 1,2,3) are better than the baseline 
system. The proposed system of Type 3 reported the best performance. 
3.2.2   The Improvement of the Proposed System Using the Shrinkage Technique 
in Sparse Data 
Here, we verify the facts that the shrinkage technique can improve the speech acts 
analysis when training data is sparse. We first compare the system with shrinkage 
(Type 3) and the system without shrinkage (Type 2). Fig. 4 shows the changes of 
performance in each number of training data from 250 to 8439. The proposed system 
with shrinkage obtains the better performance over all intervals in Fig. 4. Especially, 
the shrinkage technique provides more improvement when the amount of training data 
is small. This is a proof that the shrinkage technique can become an effective solution 
for sparse data problem from insufficient training data. 
 Improving Korean Speech Acts Analysis by Using Shrinkage and Discourse Stack 739
Table 6. The results of four speech acts analysis systems (precision %)
Speech acts Baseline      
System 
Proposed        
System 
(Type1) 
Proposed        
System 
(Type2) 
Proposed     
System 
(Type3) 
Accept 36.00% 50.00% 38.00% 50.00% 
Acknowledge 91.30% 91.30% 92.75% 95.65% 
ask-confirm 92.68% 96.34% 93.90% 95.12% 
ask-if 84.16% 86.14% 86.14% 89.11% 
ask-ref 89.88% 91.05% 90.66% 91.44% 
Closing 60.00% 61.43% 67.14% 71.43% 
Correct 0.00% 0.00% 0.00% 0.00% 
Expressive 85.84% 83.19% 87.61% 83.19% 
Inform 70.00% 70.00% 76.00% 75.60% 
Introducing-oneself 98.58% 98.58% 97.87% 98.58% 
Offer 12.50% 12.50% 12.50% 12.50% 
Opening 97.60% 96.80% 96.80% 96.80% 
Promise 92.50% 92.50% 87.50% 90.00% 
Reject 68.18% 72.73% 68.18% 68.18% 
Request 71.43% 73.81% 70.24% 69.05% 
Response 96.49% 96.07% 96.07% 96.07% 
Suggest 56.76% 56.76% 56.76% 62.16% 
TOTAL 85.18% 85.85% 86.31% 87.04% 
 
 
 
 
 
 
 
 
 
             	    	

 	
 	 	
	
Fig. 4. The performance according to different number of training data 
We then compare performances between the system of Type 2 and the system of 
Type 3 according to distribution of each speech act. As shown in Fig. 5, the pro-
posed system (Type 3) with the shrinkage technique shows higher performance in 
speech acts with insufficient examples such as ?Accept?, ?Closing?, ?Promise? and 
?Suggest?.  
740 K. Kim, Y. Ko, and J. Seo 
 
 
 
 
 
 
 
 
 	
 


 











 

	



	

 





 















	











 













	
 













	


 



























 













 






	
	

 


 
 



 	
  	
  

 

Fig. 5. The comparison of the performances for the shrinkage technique according to the distri-
bution of speech acts 
3.2.3   The Comparison of Performance with  Speech Acts Analysis Models 
Table 7 shows results from the proposed model and previous speech acts analysis 
models: the maximum entropy model (MEM) [1], the decision tree model (DTM) [8], 
and the neural network model (NNM) [5]. We report the performance of each system 
when using the same test data set as that of this paper. As a result, the proposed model 
achieved the highest performance. 
Table 7. The experimental results of the proposed model and other previous models
Model Precision (%) 
MEM 83.4% 
DTM 81.7% 
NNM 85.2% 
The propose model 87.0% 
In the experiment, it is difficult to compare the proposed model directly with the 
other models because input features are different respectively. Even though direct 
comparisons are impossible, we think that the proposed model is more robust and 
efficient than MEM and DTM. In MEM and DTM, they used many kinds of high 
level linguistic knowledge than ours such as sentence type, tense, modality and so on. 
Nevertheless, the performances of them are lower than that of the proposed model. 
Moreover, the proposed model is more effective than NNM because the performance 
of the proposed model is better than that of NNM in spite of using same features. 
4   Conclusions 
In this paper, we proposed the new speech analysis model to improve speech acts 
analysis by using the shrinkage technique and the discourse stack. We first made a 
Other
 Improving Korean Speech Acts Analysis by Using Shrinkage and Discourse Stack 741
hierarchy of speech acts by dialogue grammar for shrinkage and then estimate the 
probability of each feature on the hierarchy by the shrinkage technique. In experimen-
tal results, the proposed model is more effective for classifying speech acts. Espe-
cially, the shrinkage technique achieved more improvement when training data is 
sparse. Therefore, the shrinkage technique can be applied to the real applications that 
suffer from the data sparseness problem. We also proposed to use the discourse stack 
for easily extracting discourse structure information. As a result, the proposed model 
with shrinkage and the discourse stack showed the better performance than other 
speech acts analysis models. 
Acknowledgement 
This research was supported as a Brain Neuroinformatics Research Program spon-
sored by the Ministry of Commerce, Industry and Energy of Korea. 
References 
1. Choi, W., Cho, J. and Seo, J.: Analysis System of speech acts and Discourse Structures 
Using Maximum Entropy Model, In Proceedings of COLING-ACL99, (1999), 230-237 
2. Grosz, B.: Discourse and Dialogue, In Survey of the State of the Art in Human Language 
Technology, Center for Spoken Language Understanding, (1995), 227-254 
3. James, W. and Stein, C.: Estimation with Quadratic Loss, In Proceedings of the Fourth 
Berkeley Symposium on Mathematical Statistics and Probability 1, University of Califor-
nia Press, 361-379 
4. Joachims, T.: Text Categorization with Support Vector Machines: Learning with Many 
Relevant Features. In European conference on machine learning (ECML), (1998), 137-142 
5. Kim, K., Kim, H. and Seo, J.: A Neural Network Model with Feature Selection for Korean 
Speech Act Classification, International Journal of Neural System, VOL. 14 NO. 6, 
(2004), 407-414 
6. Ko, Y., Park, J, Seo, J.: Improving Text Categorization Using the Importance of Sen-
tences, Information Processing & Management, Vol. 40, No. 1, (2004), 65-79 
7. Lee, J., Kim, G., and Seo, J.: A Dialogue Analysis Model with Statistical Speech Act Proc-
essing for Dialogue Machine Translation, In Proceedings of ACL Workshop on Spoken 
Language Translation, (1997), 10-15 
8. Lee, S. and Seo, J.: A Korean Speech Act Analysis System Using Hidden Markov Model 
with Decision Trees, International Journal of Computer Processing of Oriental Languages. 
VOL. 15, NO. 3, (2002), 231-243 
9. MacCallum, A., Rosenfeld, R., Mitchell, T. and Ng, A.Y.: Improving Text Classification 
by Shrinkge in a Hierarchy of Classes, In Proceedings of the International Conference on 
Machine Learning. (1998) 
10. Samuel, K., Caberry, S., and Vijay-Shanker, K.: Automatically Selecting Useful Phrases 
for Dialogue Act Tagging, In Proceedings of the Fourth Conference of the Pacific Associa-
tion for Computational Linguistics, (1999) 
11. Tanaka, H. and Yokoo, A.: An Efficient Statistical Speech Act Type Tagging System for 
Speech Translation Systems, In Proceedings of COLING-ACL99, (1999), 381-388 
12. Vapnik, V.: The Nature of Statistical Learning Theory, Springer Verlag, New York, 
(1995)  
Learning with Unlabeled Data for Text Categorization Using Bootstrapping  
and Feature Projection Techniques 
Youngjoong Ko 
Dept. of Computer Science, Sogang Univ. 
Sinsu-dong 1, Mapo-gu 
Seoul, 121-742, Korea 
kyj@nlpzodiac.sogang.ac.kr 
Jungyun Seo 
Dept. of Computer Science, Sogang Univ. 
Sinsu-dong 1, Mapo-gu 
Seoul, 121-742, Korea 
    seojy@ccs.sogang.ac.kr 
 
Abstract 
A wide range of supervised learning 
algorithms has been applied to Text 
Categorization. However, the supervised 
learning approaches have some problems. One 
of them is that they require a large, often 
prohibitive, number of labeled training 
documents for accurate learning. Generally, 
acquiring class labels for training data is costly, 
while gathering a large quantity of unlabeled 
data is cheap. We here propose a new 
automatic text categorization method for 
learning from only unlabeled data using a 
bootstrapping framework and a feature 
projection technique. From results of our 
experiments, our method showed reasonably 
comparable performance compared with a 
supervised method. If our method is used in a 
text categorization task, building text 
categorization systems will become 
significantly faster and less expensive. 
1 Introduction 
Text categorization is the task of classifying 
documents into a certain number of pre-defined 
categories. Many supervised learning algorithms 
have been applied to this area. These algorithms 
today are reasonably successful when provided 
with enough labeled or annotated training 
examples.  For example, there are Naive Bayes 
(McCallum and Nigam, 1998), Rocchio (Lewis et 
al., 1996), Nearest Neighbor (kNN) (Yang et al, 
2002), TCFP (Ko and Seo, 2002), and Support 
Vector Machine (SVM) (Joachims, 1998). 
However, the supervised learning approach has 
some difficulties. One key difficulty is that it 
requires a large, often prohibitive, number of 
labeled training data for accurate learning. Since a 
labeling task must be done manually, it is a 
painfully time-consuming process. Furthermore, 
since the application area of text categorization has 
diversified from newswire articles and web pages 
to E-mails and newsgroup postings, it is also a 
difficult task to create training data for each 
application area (Nigam et al, 1998). In this light, 
we consider learning algorithms that do not require 
such a large amount of labeled data. 
While labeled data are difficult to obtain, 
unlabeled data are readily available and plentiful. 
Therefore, this paper advocates using a 
bootstrapping framework and a feature projection 
technique with just unlabeled data for text 
categorization. The input to the bootstrapping 
process is a large amount of unlabeled data and a 
small amount of seed information to tell the learner 
about the specific task. In this paper, we consider 
seed information in the form of title words 
associated with categories. In general, since 
unlabeled data are much less expensive and easier 
to collect than labeled data, our method is useful 
for text categorization tasks including online data 
sources such as web pages, E-mails, and 
newsgroup postings.  
To automatically build up a text classifier with 
unlabeled data, we must solve two problems; how 
we can automatically generate labeled training 
documents (machine-labeled data) from only title 
words and how we can handle incorrectly labeled 
documents in the machine-labeled data. This paper 
provides solutions for these problems. For the first 
problem, we employ the bootstrapping framework. 
For the second, we use the TCFP classifier with 
robustness from noisy data (Ko and Seo, 2004). 
How can labeled training data be automatically 
created from unlabeled data and title words? 
Maybe unlabeled data don?t have any information 
for building a text classifier because they do not 
contain the most important information, their 
category. Thus we must assign the class to each 
document in order to use supervised learning 
approaches. Since text categorization is a task 
based on pre-defined categories, we know the 
categories for classifying documents. Knowing the 
categories means that we can choose at least a 
representative title word of each category. This is 
the starting point of our proposed method. As we 
carry out a bootstrapping task from these title 
words, we can finally get labeled training data. 
Suppose, for example, that we are interested in 
classifying newsgroup postings about specially 
?Autos? category. Above all, we can select 
?automobile? as a title word, and automatically 
extract keywords (?car?, ?gear?, ?transmission?, 
?sedan?, and so on) using co-occurrence 
information. In our method, we use context (a 
sequence of 60 words) as a unit of meaning for 
bootstrapping from title words; it is generally 
constructed as a middle size of a sentence and a 
document. We then extract core contexts that 
include at least one of the title words and the 
keywords. We call them centroid-contexts because 
they are regarded as contexts with the core 
meaning of each category. From the centroid-
contexts, we can gain many words contextually co-
occurred with the title words and keywords: 
?driver?, ?clutch?, ?trunk?, and so on. They are 
words in first-order co-occurrence with the title 
words and the keywords. To gather more 
vocabulary, we extract contexts that are similar to 
centroid-contexts by a similarity measure; they 
contain words in second-order co-occurrence with 
the title words and the keywords. We finally 
construct context-cluster of each category as the 
combination of centroid-contexts and contexts 
selected by the similarity measure. Using the 
context-clusters as labeled training data, a Naive 
Bayes classifier can be built. Since the Naive 
Bayes classifier can label all unlabeled documents 
for their category, we can finally obtain labeled 
training data (machine-labeled data).  
When the machine-labeled data is used to learn a 
text classifier, there is another difficult in that they 
have more incorrectly labeled documents than 
manually labeled data. Thus we develop and 
employ the TCFP classifiers with robustness from 
noisy data. 
The rest of this paper is organized as follows. 
Section 2 reviews previous works. In section 3 and 
4, we explain the proposed method in detail. 
Section 5 is devoted to the analysis of the 
empirical results. The final section describes 
conclusions and future works. 
 
2 Related Works 
In general, related approaches for using unlabeled 
data in text categorization have two directions; 
One builds classifiers from a combination of 
labeled and unlabeled data (Nigam, 2001; Bennett 
and Demiriz, 1999), and the other employs 
clustering algorithms for text categorization 
(Slonim et al, 2002). 
Nigam studied an Expected Maximization (EM) 
technique for combining labeled and unlabeled 
data for text categorization in his dissertation. He 
showed that the accuracy of learned text classifiers 
can be improved by augmenting a small number of 
labeled training data with a large pool of unlabeled 
data.  
Bennet and Demiriz achieved small 
improvements on some UCI data sets using SVM. 
It seems that SVMs assume that decision 
boundaries lie between classes in low-density 
regions of instance space, and the unlabeled 
examples help find these areas. 
Slonim suggested clustering techniques for 
unsupervised document classification. Given a 
collection of unlabeled data, he attempted to find 
clusters that are highly correlated with the true 
topics of documents by unsupervised clustering 
methods. In his paper, Slonim proposed a new 
clustering method, the sequential Information 
Bottleneck (sIB) algorithm. 
 
3 The Bootstrapping Algorithm for Creating 
Machine-labeled Data 
The bootstrapping framework described in this 
paper consists of the following steps. Each module 
is described in the following sections in detail. 
 
1. Preprocessing: Contexts are separated from 
unlabeled documents and content words are 
extracted from them. 
2. Constructing context-clusters for training: 
- Keywords of each category are created 
- Centroid-contexts are extracted and verified 
- Context-clusters are created by a similarity  
measure 
3. Learning Classifier: Naive Bayes classifier are 
learned by using the context-clusters 
 
3.1 Preprocessing 
The preprocessing module has two main roles: 
extracting content words and reconstructing the 
collected documents into contexts. We use the Brill 
POS tagger to extract content words (Brill, 1995).  
Generally, the supervised learning approach with 
labeled data regards a document as a unit of 
meaning. But since we can use only the title words 
and unlabeled data, we define context as a unit of 
meaning and we employ it as the meaning unit to 
bootstrap the meaning of each category. In our 
system, we regard a sequence of 60 content words 
within a document as a context. To extract contexts 
from a document, we use sliding window 
techniques (Maarek et al, 1991). The window is a 
slide from the first word of the document to the last 
in the size of the window (60 words) and the 
interval of each window (30 words). Therefore, the 
final output of preprocessing is a set of context 
vectors that are represented as content words of 
each context. 
 
3.2 Constructing Context-Clusters for 
Training 
At first, we automatically create keywords from a 
title word for each category using co-occurrence 
information. Then centroid-contexts are extracted 
using the title word and keywords. They contain at 
least one of the title and keywords. Finally, we can 
gain more information of each category by 
assigning remaining contexts to each context-
cluster using a similarity measure technique; the 
remaining contexts do not contain any keywords or 
title words. 
3.2.1 Creating Keyword Lists 
The starting point of our method is that we have 
title words and collected documents. A title word 
can present the main meaning of each category but 
it could be insufficient in representing any 
category for text categorization. Thus we need to 
find words that are semantically related to a title 
word, and we define them as keywords of each 
category. 
The score of semantic similarity between a title 
word, T, and a word, W, is calculated by the cosine 
metric as follows: 
 
??
?
==
=
?
?=
n
i i
n
i i
n
i ii
wt
wt
WTsim
1
2
1
2
1),(               (1) 
 
where ti and wi represent the occurrence (binary 
value: 0 or 1) of words T and W in i-th document 
respectively, and n is the total number of 
documents in the collected documents. This 
method calculates the similarity score between 
words based on the degree of their co-occurrence 
in the same document.  
Since the keywords for text categorization must 
have the power to discriminate categories as well 
as similarity with the title words, we assign a word 
to the keyword list of a category with the 
maximum similarity score and recalculate the score 
of the word in the category using the following 
formula: 
 
)),(),((),(),( maxsecmaxmaxmax WTsimWTsimWTsimcWScore ond?+=  (2) 
 
where Tmax is the title word with the maximum 
similarity score with a word W, cmax is the category 
of the title word Tmax, and Tsecondmax is other title 
word with the second high similarity score with the 
word W. 
This formula means that a word with high 
ranking in a category has a high similarity score 
with the title word of the category and a high 
similarity score difference with other title words. 
We sort out words assigned to each category 
according to the calculated score in descending 
order. We then choose top m words as keywords in 
the category. Table 1 shows the list of keywords 
(top 5) for each category in the WebKB data set. 
 
Table 1. The list of keywords in the WebKB data set 
Category Title Word Keywords 
course course assignments, hours, instructor, class, fall 
faculty professor associate, ph.d, fax, interests, publications 
project project system, systems, research, software, information 
student student graduate, computer, science, page, university 
 
3.2.2 Extracting and Verifying Centroid-Contexts 
We choose contexts with a keyword or a title word 
of a category as centroid-contexts. Among 
centroid-contexts, some contexts could not have 
good features of a category even though they 
include the keywords of the category. To rank the 
importance of centroid-contexts, we compute the 
importance score of each centroid-context. First of 
all, weights (Wij) of word wi in j-th category are 
calculated using Term Frequency (TF) within a 
category and Inverse Category Frequency (ICF) 
(Cho and Kim, 1997) as follows:  
 
))log()(log( iijiijij CFMTFICFTFW ??=?=     (3) 
 
where CFi is the number of categories that contain 
wi and M is the total number of categories. 
Using word weights (Wij) calculated by formula 
3, the score of a centroid-context (Sk) in j-th 
category (cj) is computed as follows: 
 
N
WWW
cSScore Njjjjk
+++= ...),( 21            (4) 
 
where N is the  number of words in the centroid-
context. 
As a result, we obtain a set of words in first-
order co-occurrence from centroid-contexts of each 
category. 
3.2.3 Creating Context-Clusters 
We gather the second-order co-occurrence 
information by assigning remaining contexts to the 
context-cluster of each category. For the assigning 
criterion, we calculate similarity between 
remaining contexts and centroid-contexts of each 
category. Thus we employ the similarity measure 
technique by Karov and Edelman (1998). In our 
method, a part of this technique is reformed for our 
purpose and remaining contexts are assigned to 
each context-cluster by that revised technique. 
 
1) Measurement of word and context similarities 
As similar words tend to appear in similar contexts, 
we can compute the similarity by using contextual 
information. Words and contexts play 
complementary roles. Contexts are similar to the 
extent that they contain similar words, and words 
are similar to the extent that they appear in similar 
contexts (Karov and Edelman, 1998). This 
definition is circular. Thus it is applied iteratively 
using two matrices, WSM and CSM. 
Each category has a word similarity matrix 
WSMn and a context similarity matrix CSMn. In 
each iteration n, we update WSMn, whose rows and 
columns are labeled by all content words 
encountered in the centroid-contexts of each 
category and input remaining contexts. In that 
matrix, the cell (i,j) holds a value between 0 and 1, 
indicating the extent to which the i-th word is 
contextually similar to the j-th word. Also, we keep 
and update a CSMn, which holds similarities 
among contexts. The rows of CSMn correspond to 
the remaining contexts and the columns to the 
centroid-contexts. In this paper, the number of 
input contexts of row and column in CSM is 
limited to 200, considering execution time and 
memory allocation, and the number of iterations is 
set as 3.  
To compute the similarities, we initialize WSMn 
to the identity matrix. The following steps are 
iterated until the changes in the similarity values 
are small enough. 
1. Update the context similarity matrix CSMn, 
using the word similarity matrix WSMn. 
2. Update the word similarity matrix WSMn, using the 
context similarity matrix CSMn. 
2) Affinity formulae 
To simplify the symmetric iterative treatment of 
similarity between words and contexts, we define 
an auxiliary relation between words and contexts 
as affinity.  
Affinity formulae are defined as follows (Karov 
and Edelman, 1998): 
 
                  ),(max),( inXWn WWsimXWaff i?=   (5) 
 (6)                    ),(max),( jnXWn XXsimWXaff j?=
In the above formulae, n denotes the iteration 
number, and the similarity values are defined by 
WSMn and CSMn. Every word has some affinity to 
the context, and the context can be represented by 
a vector indicating the affinity of each word to it. 
 
3) Similarity formulae 
The similarity of W1 to W2 is the average affinity of 
the contexts that include W1 to W2, and the 
similarity of a context X1 to X2 is a weighted 
average of the affinity of the words in X1 to X2. 
Similarity formulae are defined as follows: 
 
  ),(),(),( 21211
1
XWaffXWweightXXsim n
XW
n ?= ?
?
+ (7) 
   (8)  
 ),(),(),(  
1),(   
  
21211
211
21
1
WXaffWXweightWWsim
else
WWsim
WWif 
n
XW
n
n
?=
=
=
?
?
+
+
The weights in formula 7 are computed as 
reflecting global frequency, log-likelihood factors, 
and part of speech as used in (Karov and Edelman, 
1998). The sum of weights in formula 8, which is a 
reciprocal number of contexts that contain W1, is 1. 
 
4) Assigning remaining contexts to a category 
We decided a similarity value of each remaining 
context for each category using the following 
method: 
       ),(),(   ??
?
??
?=
??
j
CCSiCc
SXsimavercXsim
icji
     (9) 
 
In formula 9, i) X is a remaining context, ii) 
{ }mcccC ,...,, 21= is a category set, and iii) { }nc SSi ,...,1=CC is 
a controid-contexts set of category ci. 
Each remaining context is assigned to a category 
which has a maximum similarity value. But there 
may exist noisy remaining contexts which do not 
belong to any category. To remove these noisy 
remaining contexts, we set up a dropping threshold 
using normal distribution of similarity values as 
follows (Ko and Seo, 2000): 
 
                         } ),( max{
Cci
??? +?
? i
cXsim (10) 
 
where i) X is a remaining context, ii) ? is an 
average of similarity values , iii) ? is a 
standard deviation of similarity values, and iv) ? is 
a numerical value corresponding to the threshold 
(%) in normal distribution table.  
),( iCc cXsimi?
Finally, a remaining context is assigned to the 
context-cluster of any category when the category 
has a maximum similarity above the dropping 
threshold value. In this paper, we empirically use a 
15% threshold value from an experiment using a 
validation set. 
3.3 Learning the Naive Bayes Classifier Using 
Context-Clusters 
In above section, we obtained labeled training data: 
context-clusters. Since training data are labeled as 
the context unit, we employ a Naive Bayes 
classifier because it can be built by estimating the 
word probability in a category, but not in a 
document. That is, the Naive Bayes classifier does 
not require labeled data with the unit of documents 
unlike other classifiers.  
We use the Naive Bayes classifier with minor 
modifications based on Kullback-Leibler 
Divergence (Craven et al, 2000). We classify a 
document di according to the following formula: 
 
 
?
?
=
=
???
?
???
?
+?
?=
||
1
||
1
),(
)?;|(
)?;|(
log)?;|(
)?;(log
                   
)?;|()?|(
)?|(
)?;|()?|(
)?;|(
V
t it
jt
it
j
V
t
dwN
jtj
i
jij
ij
dwP
cwP
dwP
n
cP
cwPcP
dP
cdPcP
dcP i
?
???
???
???
  (11) 
 
 
where i) n is the number of words in document di, 
ii) wt is the t-th word in the vocabulary, iii) N(wt,di) 
is the frequency of word wt in document di. 
Here, the Laplace smoothing is used to estimate 
the probability of word wt in class cj and the 
probability of class cj as follows: 
 
?=+
+
=
||
1
),(||
),(1
)?;|(
V
t ct
ct
jt
j
j
GwNV
GwN
cwP ?           (12) 
?+
+
=
i
i
j
c c
c
j
GC
G
cP
||||
||1
)?|( ?                    (13) 
 
where  is the count of the number of times 
word w
),(
jct GwN
t occurs in the context-cluster ( ) of 
category c
jcG
j. 
 
4 Using a Feature Projection Technique for 
Handling Noisy Data of Machine-labeled 
Data 
We finally obtained labeled data of a documents 
unit, machine-labeled data. Now we can learn text 
classifiers using them. But since the machine-
labeled data are created by our method, they 
generally include far more incorrectly labeled 
documents than the human-labeled data. Thus we 
employ a feature projection technique for our 
method. By the property of the feature projection 
technique, a classifier (the TCFP classifier) can 
have robustness from noisy data (Ko and Seo, 
2004). As seen in our experiment results, TCFP 
showed the highest performance among 
conventional classifiers in using machine-labeled 
data. 
 
The TCFP classifier with robustness from noisy 
data 
Here, we simply describe the TCFP classifier using 
the feature projection technique (Ko and Seo, 
2002; 2004). In this approach, the classification 
knowledge is represented as sets of projections of 
training data on each feature dimension. The 
classification of a test document is based on the 
voting of each feature of that test document. That 
is, the final prediction score is calculated by 
accumulating the voting scores of all features.  
First of all, we must calculate the voting ratio of 
each category for all features. Since elements with 
a high TF-IDF value in projections of a feature 
must become more useful classification criteria for 
the feature, we use only elements with TF-IDF 
values above the average TF-IDF value for voting. 
And the selected elements participate in 
proportional voting with the same importance as 
the TF-IDF value of each element. The voting ratio 
of each category cj in a feature tm is calculated by 
the following formula: 
 
 
??
??
?=
mmmm
jj
Ilt
lm
Ilt
mlmm dtwltcydtwtcr
)()(
),())(,(),(),(
rr
   (14) 
 
In formula 14, w ),( dtm
r
is the weight of term tm in 
document d, Im denotes a set of elements selected 
for voting and  is a function; if the 
category for an element t  is equal to c , the 
output value is 1. Otherwise, the output value is 0.  
{ }1.0?
)(lm
))(,( ltcy mj
j
Next, since each feature separately votes on 
feature projections, contextual information is 
missing. Thus we calculate co-occurrence 
frequency of features in the training data and 
modify TF-IDF values of two terms ti and tj in a 
test document by co-occurrence frequency between 
them; terms with a high co-occurrence frequency 
value have higher term weights.  
Finally, the voting score of each category c in 
the m-th feature t
j
m of a test document d is 
calculated by the following formula: 
 
))(1log(),(),(),( 2 mmmm ttcrdttwtcvs jj ?+??=
r
   (15) 
 
where tw(tm,d) denotes a modified term weight by 
the co-occurrence frequency and denotes 
the calculated ?
)(2 mt?
m
2 statistics value of . t
 
 
 Table 2. The top micro-avg F1 scores and  precision-recall breakeven points of each method.  
 OurMethod 
(basis) 
OurMethod
(NB) 
OurMethod
(Rocchio) 
OurMethod
(kNN) 
OurMethod 
(SVM) 
OurMethod
(TCFP) 
Newsgroups 79.36 83.46 83 79.95 82.49 86.19 
WebKB 73.63 73.22 75.28 68.04 73.74 75.47 
Reuters 88.62 88.23 86.26 85.65 87.41 89.09 
 
The outline of the TCFP classifier is as follow: 
 
5 Empirical Evaluation 
5.1 Data Sets and Experimental Settings 
To test our method, we used three different kinds 
of data sets: UseNet newsgroups (20 Newsgroups), 
web pages (WebKB), and newswire articles 
(Reuters 21578). For fair evaluation in 
Newsgroups and WebKB, we employed the five-
fold cross-validation method. 
The Newsgroups data set, collected by Ken 
Lang, contains about 20,000 articles evenly 
divided among 20 UseNet discussion groups 
(McCallum and Nigam, 1998). In this paper, we 
used only 16 categories after removing 4 
categories: three miscellaneous categories 
(talk.politics.misc, talk.religion.misc, and 
comp.os.ms-windows.misc) and one duplicate 
me
T B 
pro et 
con ty 
com
T et 
con es 
fro in 
(N us 
cat
A
eac
app
sta
cla
A
sta
me
across categories, we used the micro-averaging 
method (Yang et al, 2002). Results on Reuters are 
reported as precision-recall breakeven points, 
which is a standard information retrieval measure 
for binary classification (Joachims, 1998). 
1. input : test document: d
r
 =<t1,t2,?,tn> 
2. main process 
For each feature ti 
           tw(ti,d) is calculated  
 
For each feature ti 
          For each category cj 
                vote[cj]=vote[cj]+vs(cj,ti) by Formula 15
 
prediction = ][maxarg j
c
cvote
j
 
Title words in our experiment are selected 
according to category names of each data set (see 
Table 1 as an example). 
5.2 Experimental Results 
5.2.1 Observing the Performance According to 
the Number of Keywords 
First of all, we determine the number of keywords 
in our method using the validation set.  The 
number of keywords is limited by the top m-th 
keyword from the ordered list of each category. 
Figure 1 displays the performance at different 
number of keywords (from 0 to 20) in each data set. 
 
40
45
50
55
60
65
70
75
80
85
0 1 2 3 4 5 8 10 13 15 18 20
The number of keywords
M
i
cro
-a
v
g
. 
F1
Newsgroups WebKB Reuters 
 
Figure 1. The comparison of performance according to 
the number of keywords 
 
We set the number of keywords to 2 in 
Newsgroups, 5 in WebKB, and 3 in Reuters 
empirically. Generally, we recommend that the 
number of keywords be between 2 and 5. 
5.2.2 Comparing our Method Using TCFP with 
those Using other Classifiers 
In this section, we prove the superiority of TCFP 
over the other classifiers (SVM, kNN, Naive Bayes aning category (comp.sys. ibm.pc.hardware).  
he second data set comes from the WebK
ject at CMU (Craven et al, 2000). This data s
tains web pages gathered from universi
puter science departments.  
he Reuters 21578 Distribution 1.0 data s
sists of 12,902 articles and 90 topic categori
m the Reuters newswire. Like other study 
igam, 2001), we used the ten most populo
egories to identify the news topic.  bout 25% documents from training data of 
h data set are selected for a validation set. We 
lied a statistical feature selection method (?2 
tistics) to a preprocessing stage for each 
ssifier (Yang and Pedersen, 1997). 
s performance measures, we followed the 
ndard definition of recall, precision, and F1 
asure. For evaluation performance average 
(NB), Roccio) in training data with much noisy 
data such as machine-labeled data. As shown in 
Table 2, we obtained the best performance in using 
TCFP at all three data sets.  
Let us define the notations. OurMethod(basis) 
denotes the Naive Bayes classifier using labeled 
contexts and OurMethod(NB) denotes the Naive 
Bayes classifier using machine-labeled data as 
training data. The same manner is applied for other 
classifiers. 
OurMethod(TCFP) achieved more advanced 
scores than OurMethod(basis): 6.83 in 
Newsgroups, 1.84 in WebKB, and  0.47 in Reuters. 
5.2.3 Comparing with the Supervised Naive 
Bayes Classifier 
For this experiment, we consider two possible 
cases for labeling task. The first task is to label a 
part of collected documents and the second is to 
label all of them. As the first task, we built up a 
new training data set; it consists of 500 different 
documents randomly chosen from appropriate 
categories like the experiment in (Slonim et al, 
2002). As a result, we report performances from 
two kinds of Naive Bayes classifiers which are 
learned from 500 training documents and the 
whole training documents respectively. 
 
Table 3. The comparison of our method and the 
supervised NB classifier 
 OurMethod 
(TCFP) 
NB 
(500) 
NB 
(All) 
Newsgroups 86.19 72.68 91.72 
WebKB 75.47 74.1 85.29 
Reuters 89.09 82.1 91.64 
 
In Table 3, the results of our method are higher 
than those of NB(500) and are comparable to those 
of NB(All) in all data sets. Especially, the result in 
Reuters reached 2.55 close to that of NB(All) 
though it used the whole labeled training data. 
5.2.4 Enhancing our Method from Choosing 
Keywords by Human 
The main problem of our method is that the 
performance depends on the quality of the 
keywords and title words. As we have seen in 
Table 3, we obtained the worst performance in the 
WebKB data set. In fact, title words and keywords 
of each category in the WebKB data set alo have 
high frequency in other categories. We think these 
factors contribute to a comparatively poor 
performance of our method. If keywords as well as 
title words are supplied by humans, our method 
may achieve higher performance. However, 
choosing the proper keywords for each category is 
a much difficult task. Moreover, keywords from 
developers, who have insufficient knowledge about 
an application domain, do not guarantee high 
performance. In order to overcome this problem, 
we propose a hybrid method for choosing 
keywords. That is, a developer obtains 10 
candidate keywords from our keyword extraction 
method and then they can choose proper keywords 
from them. Table 4 shows the results from three 
data sets. 
Table 4. The comparison of our method and enhancing 
method 
 OurMethod 
(TCFP) 
Enhancing 
(TCFP)) Improvement
Newsgroups 86.19 86.23 +0.04 
WebKB 75.47 77.59 +2.12 
Reuters 89.09 89.52 +0.43 
 
As shown in Table 4, especially we could achieve 
significant improvement in the WebKb data set. 
Thus we find that the new method for choosing 
keywords is more useful in a domain with 
confused keywords between categories such as the 
WebKB data set. 
5.2.5 Comparing with a Clustering Technique 
In related works, we presented two approaches 
using unlabeled data in text categorization; one 
approach combines unlabeled data and labeled data, 
and the other approach uses the clustering 
technique for text categorization. Since our method 
does not use any labeled data, it cannot be fairly 
compared with the former approaches. Therefore, 
we compare our method with a clustering 
technique. Slonim et al (2002) proposed a new 
clustering algorithm (sIB) for unsupervised 
document classification and verified the superiority 
of his algorithm. In his experiments, the sIB 
algorithm was superior to other clustering 
algorithms. As we set the same experimental 
settings as in Slonim?s experiments and conduct 
experiments, we verify that our method 
outperforms ths sIB algorithm. In our experiments, 
we used the micro-averaging precision as 
performance measure and two revised data sets: 
revised_NG, revised_Reuters. These data sets were 
revised in the same way according to Slonim?s 
paper as follows:  
In revised_NG, the categories of Newsgroups were 
united with respect to 10 meta-categories: five comp 
categories, three politics categories, two sports 
categories, three religions categories, and two 
transportation categories into five big meta-
categories.  
The revised_Reuters used the 10 most frequent 
categories in the Reuters 21578 corpus under the 
ModApte split.  
As shown in Table 5, our method shows 6.65 
advanced score in revised_NG and 3.2 advanced 
score in revised_Reuters. 
 
Table 5. The comparison of our method and sIB 
 sIB OurMethod 
(TCFP) Improvement
revised_NG 79.5 86.15 +6.65 
revised_Reuters 85.8 89 +3.2 
6 Conclusions and Future Works 
This paper has addressed a new unsupervised or 
semi-unsupervised text categorization method. 
Though our method uses only title words and 
unlabeled data, it shows reasonably comparable 
performance in comparison with that of the 
supervised Naive Bayes classifier. Moreover, it 
outperforms a clustering method, sIB. Labeled data 
are expensive while unlabeled data are inexpensive 
and plentiful. Therefore, our method is useful for 
low-cost text categorization. Furthermore, if some 
text categorization tasks require high accuracy, our 
method can be used as an assistant tool for easily 
creating labeled training data.  
Since our method depends on title words and 
keywords, we need additional studies about the 
characteristics of candidate words for title words 
and keywords according to each data set. 
 
Acknowledgement  
This work was supported by grant No. R01-2003-
000-11588-0 from the basic Research Program of 
the KOSEF 
 
References  
K. Bennett and A. Demiriz, 1999, Semi-supervised 
Support Vector Machines, Advances in Neural 
Information Processing Systems 11, pp. 368-374. 
E. Brill, 1995, Transformation-Based Error-driven 
Learning and Natural Language Processing: A Case 
Study in Part of Speech Tagging, Computational 
Linguistics, Vol.21, No. 4. 
K. Cho and J. Kim, 1997, Automatic Text 
Categorization on Hierarchical Category Structure by 
using ICF (Inverse Category Frequency) Weighting, 
In Proc. of KISS conference, pp. 507-510. 
M. Craven, D. DiPasquo, D. Freitag, A. McCallum, T. 
Mitchell, K. Nigam, and S. Slattery, 2000, Learning 
to construct knowledge bases from the World Wide 
Web, Artificial Intelligence, 118(1-2), pp. 69-113. 
T. Joachims, 1998, Text Categorization with Support 
Vector Machines: Learning with Many Relevant 
Features. In Proc. of ECML, pp. 137-142. 
Y. Karov and S. Edelman, 1998, Similarity-based Word 
Sense Disambiguation, Computational Linguistics, 
Vol. 24, No. 1, pp. 41-60. 
Y. Ko and J. Seo, 2000, Automatic Text Categorization 
by Unsupervised Learning, In Proc. of 
COLING?2000, pp. 453-459. 
Y. Ko and J. Seo, 2002, Text Categorization using 
Feature Projections, In Proc. of COLING?2002, pp. 
467-473. 
Y. Ko and J. Seo, 2004, Using the Feature Projection 
Technique based on the Normalized Voting Method 
for Text Classification, Information Processing and 
Management, Vol. 40, No. 2, pp. 191-208. 
D.D. Lewis, R.E. Schapire, J.P. Callan, and R. Papka, 
1996, Training Algorithms for Linear Text 
Classifiers. In Proc. of SIGIR?96, pp.289-297. 
Y. Maarek, D. Berry, and G. Kaiser, 1991, An 
Information Retrieval Approach for Automatically 
Construction Software Libraries, IEEE Transaction 
on Software Engineering, Vol. 17, No. 8, pp. 800-
813. 
A. McCallum and K. Nigam, 1998, A Comparison of 
Event Models for Naive Bayes Text Classification. 
AAAI ?98 workshop on Learning for Text 
Categorization, pp. 41-48. 
K. P. Nigam, A. McCallum, S. Thrun, and T. Mitchell, 
1998, Learning to Classify Text from Labeled and 
Unlabeled Documents, In Proc. of AAAI-98. 
K. P. Nigam, 2001, Using Unlabeled Data to Improve 
Text Classification, The dissertation for the degree of 
Doctor of Philosophy. 
N. Slonim, N. Friedman, and N. Tishby, 2002, 
Unsupervised Document Classification using 
Sequential Information Maximization, In Proc. of 
SIGIR?02, pp. 129-136. 
Y. Yang and J. P. Pedersen. 1997, Feature selection in 
statistical leaning of text categorization. In Proc. of 
ICML?97, pp. 412-420. 
Y. Yang, S. Slattery, and R. Ghani. 2002, A study of 
approaches to hypertext categorization, Journal of 
Intelligent Information Systems, Vol. 18, No. 2. 
 
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 229?232,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Speakers? Intention Prediction Using Statistics of Multi-level Features in 
a Schedule Management Domain 
Donghyun Kim Hyunjung Lee Choong-Nyoung Seon 
Diquest Research Center Computer Science & Engineering Computer Science & Engineering 
Diquest Inc. Sogang University Sogang University 
Seoul, Korea Seoul, Korea Seoul, Korea 
kdh2007@sogang.ac.kr juvenile@sogang.ac.kr wilowisp@gmail.com 
   
Harksoo Kim Jungyun Seo 
Computer & Communications Engineering Computer Science & Engineering 
Kangwon National University Sogang University 
Chuncheon, Korea Seoul, Korea 
nlpdrkim@kangwon.ac.kr seojy@sogang.ac.kr 
 
Abstract 
Speaker?s intention prediction modules can be 
widely used as a pre-processor for reducing 
the search space of an automatic speech re-
cognizer. They also can be used as a pre-
processor for generating a proper sentence in a 
dialogue system. We propose a statistical 
model to predict speakers? intentions by using 
multi-level features. Using the multi-level fea-
tures (morpheme-level features, discourse-
level features, and domain knowledge-level 
features), the proposed model predicts speak-
ers? intentions that may be implicated in next 
utterances. In the experiments, the proposed 
model showed better performances (about 
29% higher accuracies) than the previous 
model. Based on the experiments, we found 
that the proposed multi-level features are very 
effective in speaker?s intention prediction. 
1 Introduction 
A dialogue system is a program in which a user 
and system communicate in natural language. To 
understand user?s utterance, the dialogue system 
should identify his/her intention. To respond 
his/her question, the dialogue system should gen-
erate the counterpart of his/her intention by refer-
ring to dialogue history and domain knowledge. 
Most previous researches on speakers? intentions 
have been focused on intention identification tech-
niques. On the contrary, intention prediction tech-
niques have been not studied enough although 
there are many practical needs, as shown in Figure 
1. 
 
When is the changed date?
Response, Timetable-update-dateAsk-ref, Timetable-update-date
It is changed into 4 May.
It is changed into 14 May.
?
Prediction of
user?s intention
Identification of
system?s intention
Reducing the search space 
of an ASR
It is changed into 12:40.
The date is changed.
Is it changed into 4 May?
?
It is changed into 4 May.
The result of
speech recognition
Example 1: Prediction of user?s intention
Example 2: Prediction of system?s intention
It is 706-8954.
Ask-confirm, Timetable-insert-phonenumResponse, Timetable-insert-phonenum
Response generation
Is it 706-8954?
Identification of
user?s intention
Prediction of
system?s intention
 
Figure 1. Motivational example 
 
In Figure 1, the first example shows that an inten-
tion prediction module can be used as a pre-
processor for reducing the search space of an ASR 
(automatic speech recognizer). The second exam-
ple shows that an intention prediction module can 
be used as a pre-processor for generating a proper 
sentence based on dialogue history. 
There are some researches on user?s intention 
prediction (Ronnie, 1995; Reithinger, 1995). Rei-
thinger?s model used n-grams of speech acts as 
input features. Reithinger showed that his model 
can reduce the searching complexity of an ASR to 
19~60%. However, his model did not achieve good 
performances because the input features were not 
rich enough to predict next speech acts. The re-
searches on system?s intention prediction have 
been treated as a part of researches on dialogue 
models such as a finite-state model, a frame-based 
229
model (Goddeau, 1996), and a plan-based model 
(Litman, 1987). However, a finite-state model has 
a weak point that dialogue flows should be prede-
fined. Although a plan-based model can manage 
complex dialogue phenomena using plan inference, 
a plan-based model is not easy to be applied to the 
real world applications because it is difficult to 
maintain plan recipes. In this paper, we propose a 
statistical model to reliably predict both user?s in-
tention and system?s intention in a schedule man-
agement domain. The proposed model determines 
speakers? intentions by using various levels of lin-
guistic features such as clue words, previous inten-
tions, and a current state of a domain frame.  
2 Statistical prediction of speakers? inten-
tions 
2.1 Generalization of speakers? intentions 
In a goal-oriented dialogue, speaker?s intention can 
be represented by a semantic form that consists of 
a speech act and a concept sequence (Levin, 2003). 
In the semantic form, the speech act represents the 
general intention expressed in an utterance, and the 
concept sequence captures the semantic focus of 
the utterance.  
 
Table 1. Speech acts and their meanings 
Speech act Description 
Greeting The opening greeting of a dialogue 
Expressive The closing greeting of a dialogue 
Opening Sentences for opening a goal-oriented dialogue 
Ask-ref WH-questions 
Ask-if YN-questions 
Response Responses of questions or requesting actions 
Request Declarative sentences for requesting actions 
Ask-
confirm Questions for confirming the previous actions 
Confirm Reponses of ask-confirm 
Inform Declarative sentences for giving some information 
Accept Agreement 
 
Table 2. Basic concepts in a schedule management 
domain. 
Table name Operation name Field name 
Timetable Insert, Delete, Select, Update 
Agent, Date, Day-of-week, 
Time, Person, Place 
Alarm Insert, Delete, Select, Update Date, Time 
 
Based on these assumptions, we define 11 domain-
independent speech acts, as shown in Table 1, and 
53 domain-dependent concept sequences according 
to a three-layer annotation scheme (i.e. Fully con-
necting basic concepts with bar symbols) (Kim, 
2007) based on Table 2. Then, we generalize 
speaker?s intention into a pair of a speech act and a 
concept sequence. In the remains of this paper, we 
call a pair of a speech act and a concept sequence) 
an intention. 
2.2 Intention prediction model 
Given n utterances 
nU ,1  in a dialogue, let 1+nSI  de-
note speaker?s intention of the n+1th utterance. 
Then, the intention prediction model can be for-
mally defined as the following equation: 
 
)|,(maxarg)|(
,111
,
,11
11
nnn
CSSA
nn UCSSAPUSIP
nn
+++
++
?       (1) 
 
In Equation (1), 1+nSA  and 1+nCS  are the speech act 
and the concept sequence of the n+1th utterance, 
respectively. Based on the assumption that the 
concept sequences are independent of the speech 
acts, we can rewrite Equation (1) as Equation (2). 
 
)|()|(maxarg)|(
,11,11
,
,11
11
nnnn
CSSA
nn UCSPUSAPUSIP
nn
+++
++
?
   (2) 
 
In Equation (2), it is impossible to directly com-
pute )|(
,11 nn USAP +  and )|( ,11 nn UCSP +  because a speaker 
expresses identical contents with various surface 
forms of n sentences according to a personal lin-
guistic sense in a real dialogue. To overcome this 
problem, we assume that n utterances in a dialogue 
can be generalized by a set of linguistic features 
containing various observations from the first ut-
terance to the nth utterance. Therefore, we simplify 
Equation (2) by using a linguistic feature set 1+nFS  
(a set of features that are accumulated from the 
first utterance to nth utterance) for predicting the 
n+1th intention, as shown in Equation (3). 
 
)|()|(maxarg)|( 1111
,
,11
11
+++++
++
? nnnn
CSSA
nn FSCSPFSSAPUSIP
nn
     (3) 
 
All terms of the right hand side in Equation (3) are 
represented by conditional probabilities given a 
various feature values. These conditional probabili-
ties can be effectively evaluated by CRFs (condi-
tional random fields) (Lafferty, 2001) that globally 
consider transition probabilities from the first ut-
230
terance to the n+1th utterance, as shown in Equa-
tion (4). 
 
)),(exp()(
1)|(
)),(exp()(
1)|(
1
11,1
1,11,1
1
11,1
1,11,1


+
=+
++
+
=+
++
=
=
n
i j
iijj
n
nnCRF
n
i j
iijj
n
nnCRF
FSCSF
FSZ
FSCSP
FSSAF
FSZ
FSSAP
?
?
 (4) 
 
In Equation (4), ),( iij FSSAF and ),( iij FSCSF  are fea-
ture functions for predicting the speech act and the 
concept sequence of the ith utterance, respectively. 
)(FSZ  is a normalization factor. The feature func-
tions receive binary values (i.e. zero or one) ac-
cording to absence or existence of each feature.  
2.3 Multi-level features 
The proposed model uses multi-level features as 
input values of the feature functions in Equation 
(4). The followings give the details of the proposed 
multi-level features. 
? Morpheme-level feature: Sometimes a few 
words in a current utterance give important 
clues to predict an intention of a next utterance. 
We propose two types of morpheme-level fea-
tures that are extracted from a current utterance: 
One is lexical features (content words annotated 
with parts-of-speech) and the other is POS fea-
tures (part-of-speech bi-grams of all words in 
an utterance). To obtain the morpheme-level 
features, we use a conventional morphological 
analyzer. Then, we remove non-informative 
feature values by using a well-known 2?  statis-
tic because the previous works in document 
classification have shown that effective feature 
selection can increase precisions (Yang, 1997). 
? Discourse-level feature: An intention of a cur-
rent utterance affects that dialogue participants 
determine intentions of next utterances because 
a dialogue consists of utterances that are se-
quentially associated with each other. We pro-
pose discourse-level features (bigrams of 
speakers? intentions; a pair of a current inten-
tion and a next intention) that are extracted 
from a sequence of utterances in a current di-
alogue. 
? Domain knowledge-level feature: In a goal-
oriented dialogue, dialogue participants accom-
plish a given task by using shared domain 
knowledge. Since a frame-based model is more 
flexible than a finite-state model and is more 
easy-implementable than a plan-based model, 
we adopt the frame-based model in order to de-
scribe domain knowledge. We propose two 
types of domain knowledge-level features; slot-
modification features and slot-retrieval features. 
The slot-modification features represent which 
slots are filled with suitable items, and the slot-
retrieval features represent which slots are 
looked up. The slot-modification features and 
the slot-retrieval features are represented by bi-
nary notation. In the slot-modification features, 
?1? means that the slot is filled with a proper 
item, and ?0? means that the slot is empty. In 
the slot-retrieval features, ?1? means that the 
slot is looked up one or more times. To obtain 
domain knowledge-level features, we prede-
fined speakers? intentions associated with slot 
modification (e.g. ?response & timetable-
update-date?) and slot retrieval (e.g. ?request & 
timetable-select-date?), respectively. Then, we 
automatically generated domain knowledge-
level features by looking up the predefined in-
tentions at each dialogue step. 
3 Evaluation 
3.1 Data sets and experimental settings 
We collected a Korean dialogue corpus simulated 
in a schedule management domain such as ap-
pointment scheduling and alarm setting. The dialo-
gue corpus consists of 956 dialogues, 21,336 
utterances (22.3 utterances per dialogue). Each 
utterance in dialogues was manually annotated 
with speech acts and concept sequences. The ma-
nual tagging of speech acts and concept sequences 
was done by five graduate students with the know-
ledge of a dialogue analysis and post-processed by 
a student in a doctoral course for consistency. To 
experiment the proposed model, we divided the 
annotated messages into the training corpus and 
the testing corpus by a ratio of four (764 dialogues) 
to one (192 dialogues). Then, we performed 5-fold 
cross validation. We used training factors of CRFs 
as L-BGFS and Gaussian Prior. 
3.2 Experimental results 
Table 3 and Table 4 show the accuracies of the 
proposed model in speech act prediction and con-
cept sequence prediction, respectively. 
231
 Table 3. The accuracies of speech act prediction 
Features Accuracy-S (%) Accuracy-U (%) 
Morpheme-level 
features 76.51 72.01 
Discourse-level 
features 87.31 72.80 
Domain know-
ledge-level feature 63.44 49.03 
All features 88.11 76.25 
 
Table 4. The accuracies of concept sequence pre-
diction 
Features Accuracy-S (%) Accuracy-U (%) 
Morpheme-level 
features 66.35 59.40 
Discourse-level 
features 86.56 62.62 
Domain know-
ledge-level feature 37.68 49.03 
All features 87.19 64.21 
 
In Table 3 and Table 4, Accuracy-S means the ac-
curacy of system?s intention prediction, and Accu-
racy-U means the accuracy of user?s intention 
prediction. Based on these experimental results, we 
found that multi-level features include different 
types of information and cooperation of the multi-
level features brings synergy effect. We also found 
the degree of feature importance in intention pre-
diction (i.e. discourse level features > morpheme-
level features > domain knowledge-level features). 
To evaluate the proposed model, we compare 
the accuracies of the proposed model with those of 
Reithinger?s model (Reithinger, 1995) by using the 
same training and test corpus, as shown in Table 5. 
 
Table 5. The comparison of accuracies 
Speaker Type Reithinger?s 
model 
The proposed 
model 
System 
Speech act 43.37 88.11 
Concept sequence 68.06 87.19 
User Speech act 37.59 76.25 Concept sequence 49.48 64.21 
 
As shown in Table 5, the proposed model outper-
formed Reithinger?s model in all kinds of predic-
tions. We think that the differences between 
accuracies were mainly caused by input features: 
The proposed model showed similar accuracies to 
Reithinger?s model when it used only domain 
knowledge-level features. 
4 Conclusion 
We proposed a statistical prediction model of 
speakers? intentions using multi-level features. The 
model uses three levels (a morpheme level, a dis-
course level, and a domain knowledge level) of 
features as input features of the statistical model 
based on CRFs. In the experiments, the proposed 
model showed better performances than the pre-
vious model. Based on the experiments, we found 
that the proposed multi-level features are very ef-
fective in speaker?s intention prediction. 
Acknowledgments 
This research (paper) was performed for the Intel-
ligent Robotics Development Program, one of the 
21st Century Frontier R&D Programs funded by 
the Ministry of Commerce, Industry and Energy of 
Korea. 
References  
D. Goddeau, H. Meng, J. Polifroni, S. Seneff, and S. 
Busayapongchai. 1996. ?A Form-Based Dialogue 
Manager for Spoken Language Applications?, Pro-
ceedings of International Conference on Spoken 
Language Processing, 701-704. 
D. Litman and J. Allen. 1987. A Plan Recognition Mod-
el for Subdialogues in Conversations, Cognitive 
Science, 11:163-200. 
H. Kim. 2007. A Dialogue-based NLIDB System in a 
Schedule Management Domain: About the method to 
Find User?s Intentions, Lecture Notes in Computer 
Science, 4362:869-877. 
J. Lafferty, A. McCallum, and F. Pereira. 2001. ?Condi-
tional Random Fields: Probabilistic Models for Seg-
menting And Labeling Sequence Data?, Proceedings 
of ICML, 282-289. 
L. Levin, C. Langley, A. Lavie, D. Gates, D. Wallace, 
and K. Peterson. 2003. ?Domain Specific Speech 
Acts for Spoken Language Translation?, Proceedings 
of the 4th SIGdial Workshop on Discourse and Di-
alogue. 
N. Reithinger and E. Maier. 1995. ?Utilizing Statistical 
Dialog Act Processing in VerbMobil?, Proceedings 
of ACL, 116-121. 
R. W. Smith and D. R. Hipp, 1995. Spoken Natural 
Language Dialogue Systems: A Practical Approach, 
Oxford University Press. 
Y. Yang and J. Pedersen. 1997. ?A Comparative Study 
on Feature Selection in Text Categorization?, Pro-
ceedings of the 14th International Conference on 
Machine Learning. 
232
Corpus -Based  Learn ing  of  Compound Noun Index ing  * 
Byung-Kwan Kwak, 
Jee-Hyub Kim, 
and Geunbae Lee t 
NLP Lab., Dept. of CSE 
Pohang University of 
Science & Technology 
(POSTECH) 
{nerguri,gblee} @postech.ac.kr 
J ung  Yun  Seo 
NLP Lab., 
Dept. of Computer  Science 
Sogang University 
seojy@ccs.sogang.ac.kr 
Abst ract  
In this paper, we present a corpus- 
based learning method that can 
index diverse types of compound 
nouns using rules automatically ex- 
tracted from a large tagged corpus. 
We develop an efficient way of ex- 
tracting the compound noun index- 
ing rules automatically and perform 
extensive experiments to evaluate 
our indexing rules. The automatic 
learning method shows about the 
same performance compared with 
the manual inguistic approach but 
is more portable and requires no 
human efforts. We also evaluate 
the seven different filtering meth- 
ods based on both the effectiveness 
and the efficiency, and present a 
new method to solve the problems of 
compound noun over-generation a d 
data sparseness in statistical com- 
pound noun processing. 
1 In t roduct ion  
Compound nouns are more specific and ex- 
pressive than simple nouns, so they are more 
valuable as index terms and can increase 
the precision in search experiments. There 
are many definitions for the compound noun 
which cause ambiguities as to whether a given 
continuous noun sequence is a compound 
noun or not. We, therefore, need a clean 
" This research was supported by KOSEF special 
purpose basic research (1997.9 - 2000.8 #970-1020- 
301-3) 
t Corresponding author 
definition of compound nouns in terms of in- 
formation retrieval, so we define a compound 
noun as "any continuous noun sequence that 
appears frequently in documents." 1 
In Korean documents, compound nouns are 
represented in various forms (shown in Table 
1), so there is a difficulty in indexing all types 
of compound nouns. Until now, there have 
been much works on compound noun index- 
ing, but they still have limitations of cover- 
ing all types of compound nouns and require 
much linguistic knowledge to accomplish this 
goal. In this paper, we propose a corpus- 
based learning method for compound noun 
indexing which can extract he rules automat- 
ically with little linguistic knowledge. 
Table 1: Various types of Korean compound 
noun with regard to "jeong-bo geom-saeg (in- 
formation retrieval)" 
jeong-bo-geom-saeg (information-retrieval) 
jeong-bo-eui geom-saeg (retrieval of information) 
jeong-bo geom-saeg (information retrieval) 
jeong-bo-leul geom-saeg-ha-neun 
(retrieving information) 
jeong-bo-geom-saeg si-seu-tem 
(information-retrieval system) 
As the number of the documents i growing 
retrieval, efficiency also becomes as important 
as effectiveness. To increase the efficiency, we 
focus on reducing the number of indexed spu- 
rious compound nouns. We perform experi- 
ments on several filtering methods to find the 
algorithm that can reduce spurious compound 
nouns most efficiently. 
1 The frequency threshold can be adjusted accord- 
ing to application systems. 
57 
The remainder of this paper ? is organized 
as follows. Section 2 describes previous com- 
pound noun indexing methods for Korean and 
compound noun filtering methods. We show 
overall compound noun indexing system ar- 
chitecture in Section 3, and expl~.~n each mod- 
ule of the system in Section 4 and 5 in de- 
tail. We evaluate our method with standard 
Korean test collections in Section 6. Finally, 
concluding remarks are given in Section 7. 
2 P rev ious  Research  
2.1 Compound Noun Indexing 
There have been two different methods 
for compound noun indexing: statistical 
and linguistic. In one Statistical method, 
(Fagan, 1989) indexed phrases using six 
different parameters, including information 
on co-occurrence of phrase elements, rela- 
tive location of phrase elements, etc., and 
achieved reasonable performance. However, 
his method couldn't reveal consistent sub- 
stantial improvements on five experimental 
document collections in effectiveness. (Strza- 
lkowski et al, 1996; Evans and Zhai, 1996) 
indexed subcompounds from complex noun 
phrases using noun-phrase analysis. These 
methods need to find the head-modifier rela- 
tions from noun phrases and therefore require 
difficult syntactic parsing in Korean. 
For Korean, in one statistical method, (Lee 
and Ahn, 1996) indexed general Korean nouns 
using n-grams without linguistic knowledge 
and the experiment results showed that the 
proposed method might be Mmost as effec- 
tive as the linguistic noun indexing. How- 
ever, this method can generate many spuri- 
ous n-grarn~ which decrease the precision in 
search performance. In linguistic methods, 
(Kim, 1994) used five manually chosen com- 
pound noun indexing rule patterns based on 
linguistic knowledge. However, this method 
cannot index the diverse types of compound 
nouns. (Won et al, 2000) used a full parser 
and increased the precision in search experi- 
ments. However, this linguistic method can- 
not be applied to unrestricted texts robustly. 
In summary, the previous methods, 
whether they are statistical or linguistic, 
have their own shortcomings. Statistical 
methods require signiAcant amounts of 
co-occurrence information for reasonable 
performance and can not index the diverse 
types of compound nouns. Linguistic meth- 
ods need compound noun indexing rules 
described by human and sometimes result 
in meaningless compound nouns, which 
decreases the performance of information 
retrieval systems. They cannot also cover the 
various types of compound nouns because of 
the limitation of human linguistic knowledge. 
In this paper, we present a hybrid method 
that uses linguistic rules but these rules are 
automatically acquired from a large corpus 
through statistical learning. Our method gen- 
erates more diverse compound noun index- 
ing rule patterns than the previous tandard 
methods (Kim, 1994; Lee et ah, 1997), be- 
cause previous methods use only most gen- 
eral rule patterns (shown in Table 2) and are 
based solely on human linguistic knowledge. 
Table 2: Typical hand-written compound 
noun indexing rule patterns for Korean 
Noun without case makers / Noun 
Noun with a genitive case maker / Noun 
Noun with a nominal case maker or 
an accusative case maker \[ 
Verbal common oun or adjectival common noun 
Noun with an adnominal ending \] Noun 
Noun within predicate particle phrase / Noun 
(The two nouns before and after a slash 
in the pattern can form a single compound noun.) 
2.2 Compound Noun F i l ter ing 
Compound noun indexing methods, whether 
they are statistical or linguistic, tend to gen- 
erate spurious compound nouns when they 
are actually applied. Since an information re- 
trieval system can be evaluated by its effec- 
tiveness and also by its efficiency (van Rijs- 
bergen, 1979), the spurious compound nouns 
should be efficiently filtered. (Kando et al, 
1998) insisted that, for Japanese, the smaller 
the number of index terms is, the better the 
performance of the information retrieval sys- 
tem should be. 
58 
For Korean, (Won et al, 2000) showed 
that segmentation f compound nouns is more 
efficient than compound noun synthesis in 
search performance. There have been many 
works on compound noun filtering methods; 
(Kim, 1994) used mutual information only, 
and (Yun et al, 1997) used mutual informa- 
tion and relative frequency of POS (Part-Of- 
Speech) pairs together. (Lee et ai., 1997) used 
stop word dictionaries which were constructed 
manually. Most of the previous methods for 
compound noun filtering utilized only one 
consistent method for generated compound 
nouns irrespective of the different origin of 
compound noun indexing rules, and the meth- 
ods cause many problems due to data sparse- 
hess in dictionary and training data. Our 
approach solves the data sparseness problem 
by using co-occurrence information on auto- 
matically extracted compound noun elements 
together with a statistical precision measure 
which fits best to each rule. 
3 Overa l l  Sys tem Arch i tec ture  
The compound noun indexing system pro- 
posed in this paper Consists of two major 
modules: one for automatically extracting 
compound noun indexing rules (in Figure 1) 
and the other for indexing documents, fil- 
tering the automatically generated compound 
nouns, and weighting the indexed compound 
nouns (in Figure 2). 
Compound ~ Tagged Corpus 1 
Compound ~ R  
Noun Statistical 
Information 
~ Roles with 
Precision 
Extracted Rules I 
Filtered Rules 
Figure 1: Compound noun indexing-rule ex- 
traction module (control flow =~, data flow 
Compound Noun~-----~-~ Indexing I ~_Indexing~'~ / 
Rules wig f--.......?-.~ 
Compound 
//Compound Noun\[ s~i~ \[ Infonnadon \[ 
Find \[ Compound I Nouns , I 
Weighted 
. Compound 
Nouns 
Figure 2: Compound noun indexing, filtering, 
and weighting module (control flow =~, data 
flow ~) 
4 Automat ic  Ext rac t ion  of  
Compound Noun Index ing  Ru les  
There are three major steps in automatically 
extracting compound noun indexing rules. 
The first step is to collect compound noun 
statistical information, and the second step is 
to extract the rules from a large tagged cor- 
pus using the collected statistical information. 
The final step is to learn each rule's precision.. 
4.1 Col lect ing Compound Noun 
Statist ics 
We collect initial compound noun seeds which 
were gathered from various types of well- 
balanced ocuments uch as ETRI Kemong 
encyclopaedia 2 nd many dictionaries on the 
Internet, and we collected 10,368 seeds, as 
shown in Table 3. The small number of seeds 
are bootstrapped to extract the Compound 
noun indexing rules for various corpora. 
Table 3: Collected compound noun seeds 
No. of 2 3 Total 
component elements 
ETRI Kemong encyclomedia 5,100 2,088 7,188 
Internet dictionaries 2,071 1,109 3,180 
To collect more practical statistics on the 
compound nouns, we made a 1,000,000 eo- 
jeol(Korean spacing unit which corresponds 
2 Courteously provided by ETRI, Korea. 
59 
to an English word or phrase) tagged cor- 
pus for a compound noun indexing experi- 
ment from a large document set (Korean In- 
formation Base). We collected complete com- 
pound nouns (a continuous noun sequence 
composed of at least two nouns on the condi- 
tion that both the preceding and the following 
POS of the sequence axe not nouns (Yoon et 
al., 1998)) composed of 1 - 3 no, ms from the 
tagged training corpus (Table 4). 
Table 4: Statistics for complete compound 
nouns 
No. of 1 2 3 
component elements 
Vocabulary 264,359 200,455 63,790 
4.2  Ext rac t ing  Index ing  Ru les  
We define a template (in Table 5) to extract 
the compound noun indexing rules from a 
POS tagged corpus. 
The template means that if a front- 
condition-tag, a rear-condition-tag, and sub- 
string-tags are coincident with input sentence 
tags, the lexical item in the synthesis position 
of the sentence can be indexed as a compound 
noun as "x /  y (for 3-noun compounds, x / 
y / z)". The tags used in the template are 
POS (Part-Of-Speech) tags and we use the 
POSTAG set (Table 17). 
The following is an algorithm to extract 
compound noun indexing rules from a large 
tagged corpus using the two-noun compound 
seeds and the template defined above. The 
rule extraction scope is limited to the end 
of a sentence or, if there is a conjunctive 
ending (eCC) in the sentence, only to the 
conjunctive nding of the sentence. A rule 
extraction example is shown in Figure 3. 
Algorithm 1: Extracting compound noun 
indexing rules (for 2-noun compounds) 
Read Template 
Read Seed 
(Consist of Constituent 1 / Constituent 2) 
TokeD/ze Seed into Constituents 
Put Constituent 1 into Key1 and Constituent 2 
? into Key2 
While (Not(End of Documents)) 
{ 
Read Initial Tag of Sentence 
While (Not(End of Sentence or eCC)) 
{ 
Read NeIt Tag of Sentence 
If (Read Tag =ffi Key1) 
{ 
While (Not(End of Sentence or eCC)) 
( 
Read Next Tag of Sentence 
If (Current Tag == Key2) 
Write Rule according 
to the Template 
} 
} 
} 
The next step is to  refine the extracted 
rules to select he proper ones. We used a rule 
filtering algorithm (Algorithm 2) using the 
frequency together with the heuristics that 
the rules with negative lexical items (shown in 
Table 6) will make spurious compound nouns. 
Algorithm 2: Filtering extracted rules us- 
ing frequency and heuristics 
I. For each compound noun seed, select 
the rules whose frequency is greater than 2. 
2. Among rules selected by step 1, select 
only rules that are extracted 
at least by 2 seeds. 
3. Discard rules which contain 
negative lexical items. 
Table 5: The template to extract the com- 
pound noun indexing rules 
,o  
front-condition-tag I 
sub-string-tags (tag 1 tag 2 ... tag n-1 tag n) \[ 
rear-condition-tag I 
synthesis locations (x y) 
lexicon x / lexicon y 
(for 3-noun compounds, 
synthesis locations (x, y, z) 
lexicon x / lexicon y / lexicon z) 
Table 6: Negative 
negative items (tags) 
je-oe(MC) (exclude) 
eobs(E) (not exist) 
mos-ha(D) (can not) 
lexical item examples 
example phrases 
no-jo-leul je-oe-han hoe-eui 
(meeting excluding union) 
sa-gwa-ga eobs~neun na-mu 
(tree without apple) 
dog-lib-eul mos-han gug-ga 
(country that 
cannot be liberated) 
We automatically extracted and filtered out 
60 
Tagged ~t~ 
B,~baI-Ib, 
MC< .kong-bo > 
.iC<tmb. 
MC< geom-sacg > 
fron~com~illm1_~g I sub_s~ring_mgs (~ I lag2 ... tag n-I ~n)  
~rcar_cond~iol~. I~ syn~lcsls location (x y) ~> lexicon x I Icxlco~ y 
(i.formafioa~?uicvall 
Indcxlag Rule: 
B I MC.jC<leul> MC I y I l 3 
Figure 3: Rule Extraction Process Example 
2,036 rules from the large tagged corpus (Ko- 
rean Information Base, 1,000,000 eojeol) us- 
ing the above Algorithm 2. Among the ill- 
tered rules, there are 19 rules with negative 
lexical items and we finally selected 2,017 
rules. Table 7 shows a distribution of the final 
rules according to the number of elements in 
their sub-string-tags. 
Table 7: Distribution of extracted rules by 
number of elements in sub-string-tags 
No. Distribution Example 
2 tags 79.6 % MC MC 
3 tags 12.6 % MC jO(eui) MC 
4 tags 4.7 % MC y eCNMG MC 
5 tags 1.5 % MC MC jO(e) 
DI<sog-ha-neun) MC 
over 6 tags 1.6 % 
The automatically extracted rules have 
more rule patterns and lexical items than 
human-made rules so they can cover more 
diverse types of compound nouns (Table 8). 
When checking the overlap between the two 
rule collections, we found that the manual in- 
guistic rules are a subset of our automatically 
generated statistical rules. Table 9 shows 
some of the example rules newly generated 
from our extraction algorithm, which were 
originally missing in the manual rule patterns. 
4.3 Learning the Precision o f  
Ext racted  Rules 
In the proposed method, we use the precision 
of rules to solve the compound noun over- 
generation and the data sparseness problems. 
The precision of a rule can be defined by 
Table 8: Comparison between the automati- 
cally extracted rules and the manual rules 
Method 
Manual 
linguistic 
method 
Our method 
No. of No. of 
general lexical terms 
rule patterns used in rule patterns 
16 
23 78 
Table 9: Examples of newly added rule pat- 
terns 
Rule 
Noun + bound noun / Noun 
Noun + suffix / Noun 
Noun + suffix + assignment verb + 
adnominal ending / Noun 
counting how many indexed compound noun 
candidates generated by the rule are actual 
compound nouns: 
Yactuat 
Prec(rule) = Ncandidate 
where Prec(rule) is the precision of a rule, 
Ndctual is the number of actual compound 
nouns, and Ncandidat e is the number of com- 
pound noun candidates generated by the au- 
tomatic indexing rules. 
To  calculate the precision, we need a defin- 
ing measurement for compound noun identi- 
fication. (Su et al, 1994) showed that the 
average mutual information of a compound 
noun tends to be higher than that of a non- 
compound noun, so we try to use the mutual 
information as the measure for identifying the 
compound nouns. If the mutual information 
of the compound noun candidate is higher 
than the average mutual information of the 
compound noun seeds, we decide that it is 
a compound noun. For mutual information 
(MI), we use two different equations: one for 
two-element compound nouns (Church and 
Hanks, 1990) and the other for three-element 
compound nouns (Suet  al., 1994). The equa- 
tion for two-element compound nouns is as 
follow: 
P(x,y) 
I(x;y) = log 2 P(x) x P(y) 
61 
where x and y are two words in the corpus, 
and I(x; y) is the mutual information of these 
two words (in this order). Table 10 shows 
the average MI value of the two and three 
elements. 
Table 10: Average value of the mutual infor- 
mation (MI) of compound noun seeds 
.Number of elements \[ 2 I 3 
Average MI 3.56 3.62 
The MI was calculated from the statistics of 
the complete compound nouns collected from 
the tagged training corpus (see Section 4.1). 
However, complete compound nouns are 
continuous noun sequences and cause the 
data sparseness problem. Therefore, we need 
to expand the statistics. Figure 4 shows 
the architecture of the precision learning 
module by expanding the statistics of the 
complete compound nouns along with an 
algorithmic explanation (Algorithm 3) of the 
process. Table 11 shows the improvement in
the average precision during the repetitive 
execution of this learning process. 
Norm Statistical ) 
Compound Norm of Rules ~ '~ Rule incision (step 5) (s~ 2.7) l~v v\[ (step s) 
Figure 4: Learning the precision of the com- 
pound noun indexing rules (The steps are 
shown in Algorithm 3) 
Algorithm 3: 
i. Calculate all rules' initial precision 
using initial complete compound noun 
statistical information. 
2. Calculate the average precision 
of the rules. 
3. Multiply a rule's precision by 
the frequency of the compound noun made 
by the  ru le .  
We ca l l  th i s  va lue  the  mod i f ied  f requency  
(MF). 
4. Collect the same compound nouns, and 
sum all the modified frequencies 
for each compound noun. 
5. If the sunm~ed modified frequency is greater 
than a threshold, add this compound noun 
to the complete compound noun 
s ta t i s t i ca l  information. 
6. Calculate all rules' precision again 
using the changed complete compound noun 
s ta t i s t i ca l  in fo rmat ion .  
7. Calculate the average precision of the rules. 
8. If the average precision of the rules is 
equal to the previous average precision, 
stop. Othervise, go to step 2. 
Table 11: Improvement in the average preci- 
sion of rules 
Learning 1 2 3 4 5 6 
cycles 
Avg. prec. 0.19 0.23 0.39 0.44 0.45 0.45 
of rules 
5 Compound Noun Index ing ,  
F i l te r ing ,  and  Weight ing  
In this section, we explai n how to use the au- 
tomatically extracted rules to actually index 
the compound nouns, and describe how to fil- 
ter and weight the indexed compound nouns. 
5.1 Compound Noun I ndex ing  
To index compound nouns from documents, 
we use a natural anguage processing engine, 
SKOPE (Standard KOrean Processing En- 
gine) (Cha et al, 1998), which processes doc- 
uments by analysing words into morphemes 
and tagging part-of-speeches. The tagging 
results are compared with the automatically 
learned compound noun indexing rules and, if 
they are coincident with each other, we index 
them as compound nouns. Figure 5 shows a 
process of the compound noun indexing with 
an example. 
5.2 Compound Noun F i l ter ing 
Among the indexed compound nouns above, 
still there can be meaningless compound 
nouns, which increases the number of index 
terms and the search time. To solve com- 
pound noun over-generation problem, we ex- 
periment with seven different filtering meth- 
ods (shown in Table 12) by analyzing their 
62 
?. " Tagging Result: 
bbal-li jeong-bo-leul B<bbal-li> 
geom-saeg-ha-netm ~ Auaty~ ~.  ~ MC<jeong-bo >
(~evmg I \~'~?_"2:? ?"/ ~ jc<,e.,> 
information I \ tagging / /1..~ ? :-t.~..~ \] ~ --  - /  / I.MU< geom-saeg > 
/ I eCNMG<neun> 
"~d~"~g"l~ ~es-- \ ] 'X~mpoun"~ Indexed 
i 1,2 . 
Complete l /  ~ geom-saeg 
_ C?mp~No~ ~ - -  (mf , ,~o . /  
Statistical Information \] retrieval) 
Figure 5: Compound noun indexing process 
relative effectiveness and efficiency, as shown 
in Table 16. These methods can be divided 
into three categories: first one using MI, sec- 
ond one using the frequency of the compound 
nouns (FC), and the last one using the fre- 
quency of the compound noun elements (FE). 
MI (Mutual Information) is a measure of word 
association, and used under the assumption 
that a highly associated word n-gram is more 
likely to be a compound noun. FC is used 
under the assumption that a frequently en- 
countered word n-gram is more likely to be a 
compound than a rarely encountered n-gram. 
FE is ;used under the assumption that a word 
n-gram with a frequently encountered specific 
element is more likely to be a compound. In 
the method of C, D, E, and F, each threshold 
was decided by calculating the average num- 
ber of compound nouns of each method. 
Table 12: Seven different filtering methods 
(MI) A. Mutual information of compound 
noun elements (0) 
(MI) B. Mutual information of compound 
noun elements 
(average of MI of compound noun seeds) 
(FC) C. Frequency of compound nouns 
in the training corpus (4) 
(FC) D. Frequency of compound nouns 
in the test corpus (2) 
(FE) E. Frequency of compound noun heads 
in the training corpus (5) 
(FE) F. Frequency of compound noun modifiers 
in the training corpus (5) 
G. No filtering 
(The value in parantheses is a threshold.) 
Among these methods, method B gener- 
ated the smallest number of compound nouns 
best efficiency and showed the reasonable f- 
fectiveness (Table 16). On the basis of this 
filtering method, we develop a smoothing 
method by combining the precision of rules 
with the mutual information of the compound 
noun elements, and propose our final filtering 
method (H) as follows: 
P(x, y) + ~ ? Precision T(x, y) = log 2 P(x) x P(y) 
where a is a weighting coefficient and Preci- 
sion is the applied rules learned in Section 4.3. 
For the three-element compound nouns, the 
MI part is replaced with the three-element MI
equation 3 (Su et al, 1994). 
6 Exper iment  Resu l ts  
To calculate the similarity between a docu- 
ment and a query, we use the p-norm retrieval 
model (Fox, 1983) and use 2.0 as the p-value. 
We also use fhe  component nouns in a com- 
pound as the indexing terms. We follow the 
standard TREC evaluation schemes (Salton 
and Buckley, 1991). For single index terms, 
we use the weighting method atn.ntc (Lee, 
1995). 
6.1 Compound Noun Index ing  
Exper iments  
This experiment shows how well the proposed 
method can index diverse types of compound 
nouns than the previous popular methods 
which use human-generated compound noun 
indexing rules (Kim, 1994; Lee et al, 1997). 
For simplicity, we filtered the generated com- 
pound nouns using the mutual information of 
the compound noun elements with a thresh- 
old of zero (method A in Table 12). 
Table 13 shows that the terms indexed by 
previous linguistic approach are a subset of 
the ones made by our statistical approach. 
This means that the proposed method can 
cover more diverse compound nouns than the 
3 
PD (x, ~, z) 
I(x;y;z) = log 2 Px(x,y,z) 
63 
Table 13: Compound noun indexing coverage 
experiment (With a 200,000 eojeol Korean In- 
formation Base) 
Manual 
linguistic 
rule patterns 
Our 
automatic 
rule patterns 
No. of 
generated actual 22,276 30,168 
compound nouns. (+35.4 %) 
No. of 
generated actual 7,892 
compound nouns 
without overlap 
manual inguistic rule method. We perform a 
retrieval experiment to evaluate the automat- 
ically extracted rules. Table 144 and table 155 
show that our method has slightly better re- 
call and l l -point average precision than the 
manual inguistic rule method. 
Table 14: Compound noun indexing effective- 
ness experiment I 
Avg. recall 
Manual linguistic 
rule patterns 
82.66 
Our automatic 
rule patterns 
83.62 
(+1.16 %) 
ll-pt. 42.24 42.33 
avg. precision (+0.21%) 
No. of 504,040 515,801 
index terms (+2.33 %) 
Table 15: Compound noun indexing effective- 
ness experiment II
Avg. recall 
ll-pt, avg. 
precision 
No. of 
index terms 
Manual linguistic 
rule patterns 
86.32 
34.33 
1,242,458 
Our automatic 
rule patterns 
87.50 
(+1.35 %) 
34.54 
(+0.61%) 
1,282,818 
(+3.15 %) 
4 With KTSET2.0 test collections (Courteously 
provided by KT, Korea. (4,410 documents and 50 
queries)) 
s With KRIST2.0 test collection (Courteously pro- 
vided by KORDIC, Korea. (13,514 documents and 30 
queries)) 
6.2 Retrieval Experiments Using 
Various Filtering Methods 
In this experiment, we compare the seven fil- 
tering methods to find out which one is the 
best in terms of effectiveness and efficiency. 
For this experiment, we used our automatic 
rules for the compound noun indexing, and 
the test collection KTSET2.0. To check the 
effectiveness, we used recall and l l -point  av- 
erage precision. To check the efficiency, we 
used the number of index terms. Table 16 
shows the results of the various filtering ex- 
periments. 
From Table 16, the methods using mu- 
tual information reduce the number of in- 
dex terms, whereas they have lower precision. 
The reason of this lower precision is that MI 
has a bias, i.e., scoring in favor of rare terms 
over common terms, so MI seems to have a 
problem in its sensitivity to probabil ity es- 
t imation error (Yang and Pedersen, 1997). 
In this experiment 6, we see that method B 
generates the smallest number of compound 
nouns (best efficiency) and our final propos- 
ing method H has the best recall and precision 
? (effectiveness) with the  reasonable number ? of 
compound nouns (efficiency). We can con- 
clude that the filtering method H is the best, 
considering the effectiveness and the efficiency 
at the same time. 
7 Conc lus ion  
In this paper, we presented a method to ex- 
tract the compound noun indexing rules au- 
tomatically from a large tagged corpus, and 
showed that this method can index compound 
nouns appearing in diverse types of docu- 
ments. 
In the view of effectiveness, this method is 
slightly better than the previous linguistic ap- 
proaches but requires no human effort. 
The proposed method also uses no parser 
and no rules described by humans, there- 
fore, it can be applied to unrestricted texts 
very robustly and has high domain porta- 
6 Our Korean NLQ (Natural Lan- 
guage Querying) demo system (located in 
'http:/ /nlp.postech.ac.kz /Resarch/POSNLQ/') 
can be tested. 
64 
Table 16: Retrieval experiment 
A B C 
Average 83.62 83.62 83.62 
recall (+0.00) (+0.00) 
ll-pt, avg. 42.45 42.42 42.49 
precision (-0.07) (+0.09) 
Precision 52.11 52.44 52.07 
at 10 Docs. 
No. of 515,80 508,20 514,54 5~ 
index terms (-1.47) (-0.24) (-+ 
results 
D 
83.62 
(+0.00) 
42.55 
(+0.24) 
52.80 
47,27 
+6.10) 
of various filtering 
E F 
83.62 
(+0.00) 
42.72 
(+0.64) 
52.26 
572,36 
(+10.97) 
83.62 
(+0.00) 
42.48 
(+0.07) 
51.89 
574,04 
(+11.29) 
; methods 
G 
84.32 
(+0.84) 
42.48 
(+0.07) 
52.81 
705,98 
(+36.87) 
H 
84.32 
(.+0.84) 
42.75 
(+o.71) 
52.98 
509,90 
(-1.14) 
bility. We also presented a filtering method 
to solve the compound noun over-generation 
problem. Our proposed filtering method (H) 
shows good retrieval performance both in the 
view of the effectiveness and the efficiency. 
In the future, we need to perform some 
experiments on much larger commercial 
databases to test the practicality of our 
method. 
. Finally, our method doesn't  require lan- 
guage dependent knowledge, so it needs to be 
verified whether it can be easily applied to 
other languages. 
References 
Jeongwon Cha, Geunbae Lee, and Jong-Hyeok 
Lee. 1998. Generalized unknown morpheme 
guessing for hybrid pos tagging of korean. 
In Proceedings of SIXTH WORKSHOP ON 
VERY LARGE CORPORA in Coling-ACL 98. 
K. W. Church and P. Hanks. 1990. Word associ- 
ation norms, mutual information, and lexicog- 
raphy. Computational Linguistics, 16(1):22-29. 
David A. Evans and Chengxiang Zhai. 1996. 
Noun-phrase analysis in unrestricted text for 
information retrieval. In Proceedingof the 3~th 
Annual Meetinof the Association for Computa- 
tional Linguistics, Santa Cruz, CA, pages 17- 
24. 
Joel L. Fagan. 1989. The effectiveness of a non- 
syntactic approach to automatic phrase index- 
ing for document retrieval. JASIS, 40(2):115- 
132. 
E. A. Fox. 1983. Extending the Boolean and Vec- 
tor Space Models of Information Retrieval with 
P-norm Queries and Multiple Concept Types. 
Ph.D. thesis, Cornell Univ. 
Noriko Kando, Kyo Kageura, Masaharu Yoshoka, 
and Keizo Oyama. 1998. Phrase processing 
methods for japanase text retrieval. SIGIR fo- 
rum, 32(2):23-28. 
Pan Koo Kim. 1994. The automatic indexing 
of compound words from korean text based on 
mutual information. Journal of KISS (in Ko- 
rean), 21(7):1333-1340. 
Joon Ho Lee and Jeong Soo Ahn. 1996. Using 
n-grams for korean text retrieval. In SIGIR'96, 
pages 216-224. 
Hyun-A Lee, Jong-Hyeok Lee, and Geunbae Lee. 
1997. Noun phrase indexing using clausal 
segmentation. Journal of KISS (in Korean), 
24(3):302-311. 
Joon Ho Lee. 1995. Combining multiple vidence 
from different properties of weighting schemes. 
In SIGIR'95, pages 180-188. 
Gerard Salton and Chris Buckley. 1991. 
Text retrieval conferences evaluation pro- 
gram. In .ftp://)2p.cs.corneU.edu/pub/smart/, 
trec_eval.7.0beta.tar.gz. 
Tomek Strzalkowski, Louise Guthrie, Jussi Karl- 
gren, Jura Leistensnider, Fang Lin, Jose Perez- 
Carballo, Troy Straszheim, Jin Wang, and Jon 
Wilding. 1996. Natural anguage information 
retrieval: Trec-5 report. In The Fifth Text 
REtrieval conference (TREC-5), NIST Special 
publication, pages 500-238. 
Keh-Yih Su, Mind-Wen Wu, and Jing-Shin 
Chang. 1994. A corpus-based approach to au- 
tomatic ompound extraction. In Proceedings 
of ACL 94, pages 242-247. 
C. J. van Rijsbergen. 1979. Information Re- 
trieval. University of Computing Science, 
Lodon. 
Hyungsuk Won, Mihwa Park, and Geunbae Lee. 
2000. Integrated multi-level indexing method 
for compound noun processing. In Journal o.f 
KISS, 27(1) (in Korean), pages 84-95. 
65 
Tag 
MC 
T 
B 
DI 
I 
js 
eGS 
eCNMM 
eCC 
+ 
so  
s. 
sf 
Table 17: The POS (Part-Of-Speech) set of POSTAG 
common noun 
pronoun 
adverb 
irregular verb 
assignment verb 
auxiliary particle 
prefmal ending 
nominal ending 
conjunctive ending 
prefix 
other symbol 
sentence closer 
foreign word 
MP 
G 
K 
HI~ 
E 
jo 
eCNDI 
eCNMG 
Y 
S c 
s -  
sh 
Description 
proper noun 
adnoun 
interjection 
regular adjective 
existential predicate 
other particle 
attx conj ending 
adnomina l  end ing  
predicative particle 
suffix 
left parenthesis 
sentence connection 
Chinese character 
Tag 
MD 
S 
DR 
HI 
jc 
eGE 
eCNDC 
eCNB 
b 
su  
s '  
s ,  
Description 
bound noun 
numeral 
regular verb 
irregular adjective 
case particle 
final ending 
quote conj ending 
adverbial ending 
auxiliary verb 
unit symbol 
right parenthesis 
sentence comma 
Yiming Yang and Jan O. Pedersen. 1997. A com- 
parative study on feature selection in text cat- 
egorization. In Douglas H. Fisher, editor, Pro- 
ceedings of ICML-97, l~th International Con- 
ference on Machine Learning, pages 412--420, 
Nashville, US. Morgan Kaufmann Publishers, 
San Francisco, US. 
Jun-Tae Yoon, Eui-Seok Jong, and Mansuk Song. 
1998. Analysis of korean compound noun in- 
dexing using lexical information between ouns. 
Journal of KISS (in Korean), 25(11):1716- 
1725. 
Bo-Hyun Yun, Yong-Jae Kwak, and Hae-Chang 
Rim. 1997. A korean information retrieval 
model alleviating syntactic term mismatches. 
In Proceedings ofthe Natural Language Process- 
ing Pacific Rim Symposium, pages 107-112. 
66 
MAYA: A Fast Question-answering System Based On A Predictive 
Answer Indexer* 
Harksoo Kim, Kyungsun Kim 
Dept. of Computer Science, 
Sogang University 
1 Sinsu-Dong, Mapo-Gu, Seoul, 
121-742, Korea 
{ hskim, kksun } 
@nlpzodiac.sogang.ac.kr 
Gary Geunbae Lee 
Dept. of Computer Science 
and Engineering, 
Pohang University of 
Science and Technology 
San 31, Hyoja-Dong, 
Pohang, 790-784, Korea 
gblee@postech.ac.kr 
Jungyun Seo 
Dept. of Computer Science, 
Sogang University 
1 Sinsu-Dong, Mapo-Gu, 
Seoul, 121-742, Korea 
seojy@ccs.sogang.ac.kr 
(Currently Visiting CSLI Stanford University) 
 
 
 
 
Abstract 
We propose a Question-answering 
(QA) system in Korean that uses a 
predictive answer indexer. The 
predictive answer indexer, first, 
extracts all answer candidates in a 
document in indexing time. Then, it 
gives scores to the adjacent content 
words that are closely related with each 
answer candidate. Next, it stores the 
weighted content words with each 
candidate into a database. Using this 
technique, along with a complementary 
analysis of questions, the proposed QA 
system can save response time because 
it is not necessary for the QA system to 
extract answer candidates with scores 
on retrieval time. If the QA system is 
combined with a traditional 
Information Retrieval system, it can 
improve the document retrieval 
precision for closed-class questions 
after minimum loss of retrieval time. 
1 Introduction? 
Information Retrieval (IR) systems have been 
applied successfully to a large scale of search 
area in which indexing and searching speed is 
important. Unfortunately, they return a large 
                                                          
?
 This research was partly supported by BK21 program of 
Ministry of Education and Technology Excellency 
Program of Ministry of Information and 
Telecommunications. 
amount of documents that include indexing 
terms in a user?s query. Hence, the user should 
carefully look over the whole text in order to 
find a short phrase that precisely answers his/her 
question. 
Question-answering (QA), an area of IR, is 
attracting more attention, as shown in the 
proceedings of AAAI (AAAI, 1999) and TREC 
(TREC, http://trec.nist.gov/overview.html). A 
QA system searches a large collection of texts, 
and filters out inadequate phrases or sentences 
within the texts. By using the QA system, a user 
can promptly approach to his/her answer phrases 
without troublesome tasks. However, most of 
the current QA systems (Ferret et al, 1999; Hull, 
1999; Srihari and Li, 1999; Prager et al, 2000) 
have two problems as follows: 
  It cannot correctly respond to all of the users? 
questions. It can answer the questions that are 
included in the pre-defined categories such as 
person, date, time, and etc. 

It requires more indexing or searching time than 
traditional IR systems do because it needs a 
deep linguistic knowledge such as syntactic or 
semantic roles of words. 
 
To solve the problems, we propose a QA 
system using a predictive answer indexer - 
MAYA (MAke Your Answer). We can easily 
add new categories to MAYA by only 
supplementing domain dictionaries and rules. 
We do not have to revise the searching engine of 
MAYA because the indexer is designed as a 
separate component that extracts candidate 
answers. In addition, a user can promptly obtain 
answer phrases on retrieval time because 
MAYA indexes answer candidates in advance. 
Most of the previous approaches in IR have 
been focused on the method to efficiently 
represent terms in a document because they 
want to index and search a large amount of data 
in a short time (Salton et al, 1983; Salton and 
McGill, 1983; Salton 1989). These approaches 
have been applied successfully to the 
commercial search engines (e.g. 
http://www.altavista.com) in World Wide Web 
(WWW). However, in a real sense of 
information retrieval rather than document 
retrieval, a user still needs to find an answer 
phrase within the vast amount of the retrieved 
documents although he/she can promptly find 
the relevant documents by using these engines. 
Recently, several QA systems are proposed to 
avoid the unnecessary answer finding efforts 
(Ferret et al, 1999; Hull, 1999; Moldovan et al 
1999; Prager et al, 1999; Srihari and Li, 1999). 
Recent researches have combined the 
strengths between a traditional IR system and a 
QA system (Prager et al, 2000; Prager et al, 
1999; Srihari and Li, 1999). Most of the 
combined systems access a huge amount of 
electronic information by using IR techniques, 
and they improve precision rates by using QA 
techniques. In detail, they retrieve a large 
amount of documents that are relevant to a 
user?s query by using a well-known TF  IDF. 
Then, they extract answer candidates within the 
documents, and filter out the candidates by 
using an expected answer type and some rules 
on the retrieval time. Although they have been 
based on shallow NLP techniques (Sparck-Jones, 
1999), they consume much longer retrieval time 
than traditional IR systems do because of the 
addictive efforts mentioned above. To save 
retrieval time, MAYA extracts answer 
candidates, and computes the scores of the 
candidates on indexing time. On retrieval time, 
it just calculates the similarities between a user?s 
query and the candidates. As a result, it can 
minimize the retrieval time. 
This paper is organized as follows. In Section 
2, we review the previous works of the QA 
systems. In Section 3, we describe the applied 
NLP techniques, and present our system. In 
Section 4, we analyze the result of our 
experiments. Finally, we draw conclusions in 
Section 5. 
2 Previous Works 
The current QA approaches can be classified 
into two groups; text-snippet extraction systems 
and noun-phrase extraction systems (also called 
closed-class QA) (Vicedo and Ferr?ndex, 2000). 
The text-snippet extraction approaches are 
based on locating and extracting the most 
relevant sentences or paragraphs to the query by 
assuming that this text will probably contain the 
correct answer to the query. These approaches 
have been the most commonly used by 
participants in last TREC QA Track (Ferret et al, 
1999; Hull, 1999; Moldovan et al, 1999; Prager 
et al, 1999; Srihari and Li, 1999). ExtrAns 
(Berri et al, 1998) is a representative QA 
system in the text-snippet extraction approaches. 
The system locates the phrases in a document 
from which a user can infer an answer. However, 
it is difficult for the system to be converted into 
other domains because the system uses syntactic 
and semantic information that only covers a very 
limited domain (Vicedo and Ferr?ndex, 2000). 
The noun-phrase extraction approaches are 
based on finding concrete information, mainly 
noun phrases, requested by users? closed-class 
questions. A closed-class question is a question 
stated in natural language, which assumes some 
definite answer typified by a noun phrase rather 
than a procedural answer. MURAX (Kupiec, 
1993) is one of the noun-phrase extraction 
systems. MURAX uses modules for the shallow 
linguistic analysis: a Part-Of-Speech (POS) 
tagger and finite-state recognizer for matching 
lexico-syntactic pattern. The finite-state 
recognizer decides users? expectations and 
filters out various answer hypotheses. For 
example, the answers to questions beginning 
with the word Who are likely to be people?s 
name. Some QA systems participating in Text 
REtrieval Conference (TREC) use a shallow 
linguistic knowledge and start from similar 
approaches as used in MURAX (Hull, 1999; 
Vicedo and Ferr?ndex, 2000). These QA 
systems use specialized shallow parsers to 
identify the asking point (who, what, when, 
where, etc). However, these QA systems take a 
long response time because they apply some 
rules to each sentence including answer 
candidates and give each answer a score on 
retrieval time. 
MAYA uses shallow linguistic information 
such as a POS tagger, a lexico-syntactic parser 
similar to finite-state recognizer in MURAX and 
a Named Entity (NE) recognizer based on 
dictionaries. However, MAYA returns answer 
phrases in very short time compared with those 
previous systems because the system extracts 
answer candidates and gives each answer a score 
using pre-defined rules on indexing time. 
3 MAYA Q/A approach 
MAYA has been designed as a separate 
component that interfaces with a traditional IR 
system. In other words, it can be run without IR 
system. It consists of two engines; an indexing 
engine and a searching engine. 
The indexing engine first extracts all answer 
candidates from collected documents. For 
answer extraction, it uses the NE recognizer 
based on dictionaries and the finite-state 
automata. Then, it gives scores to the terms that 
surround each candidate. Next, it stores each 
candidate and the surrounding terms with scores 
in Index DataBase (DB). For example, if n 
surrounding terms affects a candidate, n pairs of 
the candidate and terms are stored into DB with 
n scores. As shown in Figure 1, the indexing 
engine keeps separate index DBs that are 
classified into pre-defined semantic categories 
(i.e. users? asking points or question types). 
The searching engine identifies a user?s 
asking point, and selects an index DB that 
includes answer candidates of his/her query. 
Then, it calculates similarities between terms of 
his/her query and the terms surrounding the 
candidates.  The similarities are based on p-
Norm model (Salton et al, 1983). Next, it ranks 
the candidates according to the similarities. 
 
      	 
 
        
      A Reliable Indexing Method for a Practical QA System 
Harksoo Kim 
Diquest Inc. 
Sindo B/D, 1604-22, Seocho-dong 
Seocho-gu, Seoul, Korea, 137-070 
hskim@diquest.com 
Jungyun Seo 
Department of Computer Science 
Sogang University, 1 Sinsu-dong, 
Mapo-gu, Seoul, Korea, 121-742 
seojy@ccs.sogang.ac.kr 
Abstract  
We propose a fast and reliable 
Question-answering (QA) system in Korean, 
which uses a predictive answer indexer based on 
2-pass scoring method. The indexing process is 
as follows. The predictive answer indexer first 
extracts all answer candidates in a document. 
Then, using 2-pass scoring method, it gives 
scores to the adjacent content words that are 
closely related with each answer candidate. Next, 
it stores the weighted content words with each 
candidate into a database. Using this technique, 
along with a complementary analysis of 
questions, the proposed QA system saves 
response time and enhances the precision. 
Introduction 
Traditional Information Retrieval (IR) focuses 
on searching and ranking a list of documents in 
response to a user?s question. However, in many 
cases, a user has a specific question and want for 
IR systems to return the answer itself rather than 
a list of documents (Voorhees and Tice (2000)). 
To satisfy this need, the concept of Question 
Answering (QA) comes up, and a lot of 
researches have been carried out, as shown in 
the proceedings of AAAI (AAAI (n.d.)) and 
TREC (Text REtrieval Conference) (TREC 
(n.d.)). A QA system searches a large collection 
of texts, and filters out inadequate phrases or 
sentences within the texts. Owing to the filtering 
process, a user can promptly approach to his/her 
answer phrases without troublesome tasks. 
Unfortunately, most of the previous researches 
have passed over the following problems that 
occurs in real fields like World Wide Web 
(WWW): 
  Users want to find answers as soon as 
possible. If a QA system does not respond 
to their questions within a few seconds, they 
will keep a suspicious eye on usefulness of 
the system. 
 Users express their intentions by using 
various syntactic forms. The fact makes it 
difficult that a QA system performs well at 
any domains. Ultimately, the QA system 
cannot be easily converted into any 
domains. 
 A QA system cannot correctly respond to 
all of the users? questions. It can answer the 
questions that are included in the predefined 
categories such as person, date, and time. 
To solve the problems, we propose a practical 
QA system using a predictive answer indexer in 
Korean - MAYA (MAke Your Answer). MAYA 
focuses on resolving the practical problems such 
as real-time response and domain portability. 
We can easily add new categories to MAYA by 
only supplementing domain dictionaries and 
rules. We do not have to revise the searching 
engine of MAYA because the indexer is 
designed as a separate component that extracts 
candidate answers. Users can promptly obtain 
answer phrases on retrieval time because 
MAYA indexes answer candidates in advance. 
This paper is organized as follows. First, we 
review the previous works of the QA systems. 
Second, we present our system, and describe the 
applied NLP techniques. Third, we analyze the 
result of our experiments. Finally, we draw 
conclusions. 
1 Previous works 
The current QA approaches can be classified 
into two groups; text-snippet extraction methods 
and noun-phrase extraction methods (also called 
closed-class QA) (Vicedo and Ferr?ndex (2000)). 
The text-snippet extraction methods are based 
on locating and extracting the most relevant 
sentences or paragraphs to the query by 
assuming that this text will probably contain the 
correct answer to the query. These methods have 
been the most commonly used by participants in 
last TREC QA Track (Moldovan et al (1999); 
Prager, Radev, Brown and Coden (1999)). The 
noun-phrase extraction methods are based on 
finding concrete information, mainly noun 
phrases, requested by users? closed-class 
questions. A closed-class question is a question 
stated in natural language, which assumes a 
definite answer typified by a noun phrase rather 
than a procedural answer. 
ExtrAns (Berri, Molla and Hess (1998)) is a 
representative QA system using the text-snippet 
extraction method. The system locates the 
phrases in a document from which a user can 
infer an answer. However, it is difficult for the 
system to be converted into other domains 
because the system uses syntactic and semantic 
information that only covers a very limited 
domain (Vicedo and Ferr?ndex (2000)). 
FALCON (Harabagiu et al (2000)) is another 
text-snippet system. The system returns answer 
phrases with high precision because it integrates 
different forms of syntactic, semantic and 
pragmatic knowledge for the goal of archiving 
better performance. The answer engine of 
FALCON handles question reformulations of 
previously posed questions, finds the expected 
answer type from a large hierarchy that 
incorporates the WordNet (Miller (1990)), and 
extracts answers after performing unifications on 
the semantic forms of the question and its 
answer candidates. Although FALCON archives 
good performance, the system is not appropriate 
for a practical QA system because it is difficult 
to construct domain-specific knowledge like a 
semantic net. 
MURAX (Kupiec (1993)) is one of the 
noun-phrase extraction systems. MURAX uses 
modules for the shallow linguistic analysis: a 
Part-Of-Speech (POS) tagger and finite-state 
recognizer for matching lexico-syntactic pattern. 
The finite-state recognizer decides users? 
expectations and filters out various answer 
hypotheses. For example, the answers to 
questions beginning with the word Who are 
likely to be people?s name. Some QA systems 
participating in TREC use a shallow linguistic 
knowledge and start from similar approaches as 
used in MURAX (Vicedo and Ferr?ndex (2000)). 
These QA systems use specialized shallow 
parsers to identify the asking point (who, what, 
when, where, etc). However, these QA systems 
take a long response time because they apply 
some rules to each sentence including answer 
candidates and give each answer a score on 
retrieval time. To overcome the week point, 
GuruQA system (Prager, Brown and Coden 
(2000)), one of text-snippet systems, uses a 
method for indexing answer candidates in 
advance (so-called Predictive Annotation). 
Predictive Annotation identifies answer 
candidates in a text, annotates them accordingly, 
and indexes them. Although the GuruQA system 
quickly replies to users? queries and has good 
performance, the system passed over useful 
information out of a document boundary. In 
other words, the system restricts the size of a 
context window containing an answer candidate 
from a sentence to a whole document, and 
calculates a similarity between the keywords in a 
query and the keywords in the window. The 
system does not consider any information out of 
the window at all. 
2 Approach of MAYA 
MAYA has been designed as a separate 
component that interfaces with a traditional IR 
system. In other words, it can be run without IR 
system. As shown in Figure 1, it consists of two 
engines; an indexing engine and a searching 
engine. 
       	  
  
 	             
           
Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 13?18,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Information extraction using finite state automata and syllable n-grams 
in a mobile environment 
 
 
Choong-Nyoung Seon Harksoo Kim Jungyun Seo 
Computer Science and Engi-
neering 
Computer and Communications 
Engineering 
Computer Science and Engi-
neering 
Sogang University Kangwon National University Sogang University 
Seoul, Korea Chuncheon, Korea Seoul, Korea 
wilowisp@gmail.com nlpdrkim@kangwon.ac.kr seojy@sogang.ac.kr 
 
Abstract 
We propose an information extraction system 
that is designed for mobile devices with low 
hardware resources. The proposed system ex-
tracts temporal instances (dates and times) and 
named instances (locations and topics) from 
Korean short messages in an appointment 
management domain. To efficiently extract 
temporal instances with limited numbers of 
surface forms, the proposed system uses well-
refined finite state automata. To effectively 
extract various surface forms of named in-
stances with low hardware resources, the pro-
posed system uses a modified HMM based on 
syllable n-grams. In the experiment on in-
stance boundary labeling, the proposed system 
showed better performances than traditional 
classifiers. 
1 Introduction 
Recently, many people access various multi-media 
contents using mobile devices such as a cellular 
phone and a PDA (personal digital assistant). Ac-
cordingly, users? requests on NLP (natural lan-
guage processing) are increasing because they 
want to easily and simply look up the multi-media 
contents. Information extraction is one of useful 
applications in NLP that helps users to easily 
access core information in a large amount of free 
texts. Unfortunately, it is not easy to implement an 
information extraction system in mobile devices 
because target texts include many morphological 
variations (e.g. blank omission, typos, word ab-
breviation) and mobile devices have many hard-
ware limitations (e.g. a small volume of a main 
memory and the absence of an arithmetic logic unit 
for floating-point calculation) 
There are some researches on information ex-
traction from short messages in a mobile device, 
and Cooper?s research (Cooper, 2005) is represent-
ative. Cooper predefined various syntactic patterns 
with placeholders and matched an input message 
against the syntactic patterns. Then, he extracted 
texts in the placeholders and assigned them the 
attribute name of the placeholders. This method 
has some advantages like easy implementation and 
fast response time. However, it is inadequate to 
apply Cooper?s method to languages with partially-
free word-order like Korean and Japanese because 
a huge amount of syntactic patterns should be pre-
defined according as the degree of freedom on 
word order increases. Kang (2004) proposed a 
NLIDB (natural language interface to database) 
system using lightweight shallow NLP techniques. 
Kang raised problems of deep NLP techniques 
such as low portability and error-proneness. Kang 
proposed a lightweight approach to natural lan-
guage interfaces, where translation knowledge is 
semi-automatically acquired and user questions are 
only syntactically analyzed. Although Kang?s me-
thod showed good performances in spite of using 
shallow NLP techniques, it is difficult to apply his 
method to mobile devices because his method still 
needs a morphological analyzer with a large size of 
dictionary. In this paper, we propose an informa-
tion extraction system that is designed for mobile 
devices with low hardware resources.  The pro-
posed system extracts appointment-related infor-
mation (i.e. dates, times, locations, and topics) 
from Korean short messages. 
This paper is organized as follows. In section 2, 
we proposed an information extraction system for 
a mobile device in an appointment domain. In sec-
13
tion 3, we explain experimental setup and report 
some experimental results. Finally, we draw some 
conclusions in section 4. 
2 Lightweight information extraction sys-
tem 
Figure 1 shows an overall architecture of the pro-
posed system. 
 
 
Output
Normalization 
Part
Extraction PartInput
Short Message
Temporal instance 
extraction
Converting machine-
manageable forms
Date/Time extraction 
results
Named instance 
extraction
Ranking instances
Location/Topic 
extraction results
Date/Time
FSAs
n-gram statistics
 
Figure 1. The system architecture  
 
As shown in Figure 1, the proposed system con-
sists of an extraction part and a normalization part.  
In the extraction part, the proposed system first 
extracts temporal instance candidates (i.e. dates 
and times) using FSA (finite-state automata). Then, 
the proposed system extracts named instance can-
didates (i.e. locations and topics) using syllable n-
grams. Finally, the proposed system ranks the ex-
tracted instances and selects the highest one per 
target category. In the normalization part, the pro-
posed system converts the temporal instances into 
suitable forms. 
2.1 Information extraction using finite state 
automata 
Although short messages in an appointment do-
main often include many incorrect words, temporal 
instances like dates and times are expressed as cor-
rect as possible because they are very important to 
appointment management. In addition, temporal 
instances are expressed in tractable numbers of 
surface forms in order to make message receivers 
easily be understood. In MUC-7, these kinds of 
temporal instances are called TIMEX (Chinchor, 
1998), and it has known that TIMEX can be easily 
extracted by using FSA (Srihari, 2001). Based on 
these previous works, the proposed system extracts 
temporal instances from short messages by using 
FSA, as shown in Figure 2. 
 
 
Figure 2. An example of FSA for date extraction 
2.2 Information extraction using statistical 
syllable n-grams 
Unlike dates and times, locations and topics not 
only have various surface forms, but also their 
constituent words are not included in a closed set. 
In MUC-7, these kinds of named entities are called 
NAMEX (Chinchor, 1998), and many researches 
on NAMEX have been performed by using rules 
and statistics. Generally, rule-based methods show 
high precisions but they have a weak point that it is 
hard to maintain a system when new words are 
continuously added to the system (Goh, 2003). Sta-
tistical methods guarantee reasonable perfor-
mances but they need large-scale language 
resource and complex floating point operations. 
Therefore, it is not suitable to apply previous tradi-
tional approaches to mobile devices with many 
hardware limitations. To resolve this problem, we 
propose a statistical model based on syllable n-
grams, as shown in Figure 3. 
 
 
Figure 3. Statistical information extraction 
14
 The extraction of named instances has two kinds of 
problems; a instance boundary detection problem 
and a category assigning problem. If we can use a 
conventional morphological analyzer, the instance 
boundary detection problem is not big. However, it 
is not easy to use a morphological analyzer in a 
mobile device because of hardware limitations and 
users? writing habitations. Users often ignore word 
spacing and this habitation lowers the performance 
of the morphological analyzer. To resolve this 
problem, we adopt a syllable n-gram model that 
performs well in word boundary detection for lan-
guages like Chinese with no spacing between 
words (Goh, 2003; Ha, 2004). We first define 9 
labels that represent boundaries of named instance 
candidates by adopting BIO (begin, inner, and out-
er) annotation scheme, as shown in Table 1 (Hong, 
2005; Uchimoto, 2000).  
 
Tag Description Tag Description 
LB Begin of a location TB Begin of a topic 
LI Inner of  a location TI Inner of a topic 
LE End of a location TE End of a topic 
LS A single-syllable loca-
tion 
TS A single-syllable 
topic 
OT Other syllable   
Table 1. The definition of instance boundary labels 
 
Then, based on a modified HMM (hidden Markov 
model), the proposed system assigns boundary la-
bels to each syllable in an input message, as fol-
lows. 
Let 
nS ,1  denote a message which consists of a 
sequence of n syllable, 
nsss ,...,, 21 , and let nL ,1  de-
note the boundary label sequence, 
nlll ,...,, 21 , of  nS ,1 . 
Then, the label annotation problem can be formally 
defined as finding 
nL ,1  which is the result of Equa-
tion (1). 
 
),(maxarg            
)(
),(
maxarg            
)|(maxarg)(
,1,1
,1
,1,1
,1,1,1
,1
,1
,1
nn
L
n
nn
L
nn
L
def
n
SLP
SP
SLP
SLPSL
n
n
n
=
=
=
  (1) 
 
In Equation (1), we dropped )(
,1 nSP  as it is constant 
for all 
nL ,1 . Next, we break Equation (1) into bite-
size pieces about which we can collect statistics, as 
shown in Equation (2). 
 
?
=
???
=
n
i
iiiiiinn sllPslsPSLP
1
1,11,11,1,1,1,1 ),|(),|(),(  (2) 
 
We simplify Equation (2) by making the following 
two assumptions: one is that the current boundary 
label is only dependent on the previous boundary 
label, and the other is that current boundary label is 
affected by its contextual features.  
 
?
=
?
=
n
i
iiiinn llPlsPSLP
1
1
*
,1,1 )|()|(),(   (3) 
 
In Equation (3), )|(* ii lsP  is a modified observation 
probability that is adopted from a class probability 
in na?ve Bayesian classification (Zheng, 1998) as 
shown in Equation (4). The reason why we modify 
an original observation probability )|( ii lsP  in 
HMM is its sparseness that is caused by a size li-
mitation of training corpus in a mobile environ-
ment. 
 
?
=
=
f
j
iijiii lsPlPZ
lsP
1
* )|()(1)|(   (4) 
 
In Equation (4), f  is the number of contextual fea-
tures, and ijs  is the jth feature of the ith syllable. Z  
is a normalizing factor. Table 2 shows the contex-
tual features that the proposed system uses. 
 
Feature Composition Meaning 
1is  is  The current syllable 
2is  ii ss 1?  
The previous syllable and the 
current syllable 
3is  1+ii ss  
The current syllable and the 
next syllable 
Table 2. The composition of contextual features 
 
In Equation (1), the max scores are calculated by 
using the well-known Viterbi algorithm (Forney, 
1973). 
After performing instance boundary labeling, 
the proposed system extracts syllable sequences 
labeled with the same named categories. For ex-
ample, if a syllable sequence is labeled with ?TS 
OT LB LI LI?, the proposed system extracts the 
sub-sequence of syllables labeled with ?LB LI LI?, 
15
as a location candidate. Then, the proposed system 
ranks the extracted instance candidates by using 
some information such as position, length, and a 
degree of completion, as shown in Equation (5). 
 
iiii CompletionLengthPosition)Rank(NI ?+?+?= ???  (5) 
 
In Equation (5), iPosition  means the distance from 
the beginning of input message to the ith named 
instance candidate iNI . In Korean, important words 
tend to appear in the latter part of a message. 
Therefore, we assume that the latter part an in-
stance candidate appears in, the more important the 
instance candidate is. iLength  means the length of an 
instance candidate. We assume that the longer an 
instance candidate include is, the more informative 
the instance candidate is. iCompletion  means whether 
a sequence of instance boundary labels is complete. 
We assume that instance candidates with complete 
label sequences are more informative. To check the 
degree of completion, the proposed system uses 
FSA, as shown in Figure 4. In the training corpus, 
every transition is legal. Therefore most of candi-
dates were satisfied the completion condition. 
However, sometimes the completion condition is 
not satisfied, when the candidate was extracted 
from the boundary of a sentence. Accordingly the 
condition gave an effect to the rank.  
 
 
Figure 4. The FSA for checking label completion 
 
In the experiments, we set ? , ? ,  and ?  to 1, 2, 
and 10, respectively. 
2.3 Normalization of temporal instances 
It is inadequate for the proposed system to use the 
extracted temporal instances as database instances 
without any processing because the temporal in-
stances consist of various forms of human-readable 
strings like ?January 24, 2008?. Therefore, the pro-
posed system should normalize the temporal in-
stances into machine-manageable forms like 
?20080124?. However, the normalization is not 
easy because temporal instances often include the 
relative information like ?this Sunday? and ?after 
two days?. To resolve this problem, the proposed 
system converts relative temporal instances into 
absolute temporal instances by using a message 
arrival time. For example, if a message includes 
the temporal instance ?after two days?, the pro-
posed system checks arrival time information of 
the message. Then, the proposed system adds a 
date in the arrival time information to two days. 
Figure 5 shows an example of date normalization. 
 
 
Figure 5. An example of date normalization 
3 Evaluation 
3.1 Data sets and experimental settings 
We collected 6,190 short messages simulated in an 
appointment scheduling domain. These messages 
contain 4,686 locations and 4,836 topics.  Each 
message is manually annotated with the boundary 
labels in Table 1. The manual annotation was done 
by 2 graduate students majoring in natural lan-
guage processing and post-processed by a student 
in a doctoral course for consistency. In order to 
experiment the proposed system, we divided the 
annotated messages into the training corpus and 
the testing corpus by a ratio of four (4,952 messag-
es) to one (1,238 messages). Then, we performed 
5-fold cross validation and used a precision, a re-
call rate, and a F1-measure as performance meas-
ures. In this paper, we did not evaluate the 
performances on the temporal instance extraction 
because performances of the proposed method are 
fully dependent on the coverage of pre-constructed 
FSA. 
3.2 Experimental results 
16
To choose the proper size of language models in a 
mobile environment, we evaluated performance 
variations of the proposed system, as shown in 
Figure 6. 
 
 
Figure 6. The performance variations according to the 
size of language models  
 
As shown in Figure 6, the system using syllable 
unigrams showed much lower performances than 
the systems using syllable bigrams or syllable tri-
grams.  
 
 Bigram Trigram 
# of features 54,711 158,525 
Size of DB 1.33M 2.83M 
Table 3. Space requirements of language models. 
 
However, as shown in the Table 3, although the 
number of syllable trigrams was three times larger 
than the number of syllable bigrams, the difference 
of performances between the system using syllable 
bigrams and the system using syllable trigrams was 
not big (about 1%). Based on this experimental 
result, we conclude that the combination of sylla-
ble unigrams and syllable bigrams, as shown in 
Table 2, is the most suitable language model for 
mobile devices with low hardware resources. 
To evaluate the proposed system, we calculated 
two types of performances. One is boundary labe-
ling performances that measure whether the pro-
posed system can correctly annotate a test corpus 
with boundary labels in Table 1. The other is ex-
traction performances that measure whether the 
proposed system can correctly extract named in-
stances from a test corpus by using Equation (5). 
Table 4 shows the boundary labeling performances 
of the proposed system in comparisons with those 
of representative classifiers. 
 
Model Precision Recall 
rate 
F1-
measure 
NB 62.74% 75.17% 68.34% 
SVM 67.29% 67.58% 67.37% 
CRF 70.98% 66.27% 68.45% 
Proposed 
system 74.81% 77.20% 75.91% 
Table 4. The comparison of boundary labeling perfor-
mances 
 
In Table 4, NB is a classifier using na?ve Bayesian 
statistics, and SVM is a classifier using a support 
vector machine. CRF is a classifier using condi-
tional random fields. As shown in Table 4, the 
proposed system outperformed the comparative 
models in all measures. Based on this fact, we 
think that the modified HMM may be more effec-
tive in a labeling sequence problem.  
Table 5 shows the extraction performances of the 
proposed system. In Table 5, the reason why the 
performances on the topic extraction are lower is 
that topic instances can consist of more various 
syllables (e.g. the topic instance, ?a meeting in 
Samsung Research Center?, includes the location, 
?Samsung Research Center?).  
 
Category Precision Recall rate F1-
measure 
Location 79.37% 76.33% 77.78% 
Topic 58.54% 55.20% 56.72% 
Table 5. The extraction performances 
 
Table 6 shows performance variations according as 
the parameters in Equation (5) are changed. As 
shown in Table 6, the differences between perfor-
mances are not big, and the proposed model 
showed the best performance at (?=1, ?=2, ?=5) or 
(?=1, ?=2, ?=10). On the basis of this experiments, 
we set ?, ?, and ? to 1, 2, and 5, respectively. 
 
(?,?,?) Precision of Location 
Recall rate 
of Location 
F1-measure 
of Location 
(1,1,1) 79.23% 76.20% 77.65% 
(1,1,5) 79.28% 76.24% 77.69% 
17
(1,1,10) 79.30% 76.26% 77.71% 
(1,2,5) 79.37% 76.33% 77.78% 
(1,2,10) 79.37% 76.33% 77.78% 
(?,?,?) Precision of Topic 
Recall rate 
of Topic  
F1-measure 
of Topic 
(1,1,1) 58.09% 54.76% 56.28% 
(1,1,5) 58.09% 54.76% 56.28% 
(1,1,10) 58.11% 54.78% 56.30% 
(1,2,5) 58.54% 55.20% 56.72% 
(1,2,10) 58.54% 55.20% 56.72% 
Table 6. The performance variations according to para-
meter changes 
 
To evaluate usefulness of the proposed model in a 
real mobile phone environment, we measured an 
average response time of 100 short messages in a 
mobile phone with XSCALE PXA270 CPU, 
51.26MB memory, and Windows mobile 5.0. We 
obtained an average response time of 0.0532 
seconds.  
4 Conclusion 
We proposed an information extraction system for 
a mobile device in an appointment management 
domain. The proposed system efficiently extracts 
temporal instances with limited numbers of surface 
forms by using FSA. To effectively extract various 
surface forms of named instances with low hard-
ware resources, the proposed system uses a mod-
ified HMM based on syllable n-grams. In the 
experiment on instance boundary labeling, the pro-
posed system outperformed traditional classifiers 
that showed good performances in a labeling se-
quence problem. On the experimental basis, we 
think that the proposed method is very suitable for 
information extraction applications with many 
hardware limitations. 
Acknowledgments 
This research (paper) was funded by Samsung 
Electronics.  
5 Reference  
Chooi Ling Goh, Masayuki Asahara, Yuji Matsumoto. 
2003. Chinese unknown word identification using 
character-based tagging and chunking. Proceedings 
of ACL-2003 Interactive Posters and Demonstrations, 
197-200. 
G. David Forney, JR. 1973. The Viterbi Algorithm Pro-
ceedings of the IEEE, 61(3):268-278.  
Hong Shen, Anoop Sarkar. 2005. Voting Between Mul-
tiple Data Representations for Text Chunking. Cana-
dian Conference on AI 2005. 389-400. 
In-Su Kang, Seung-Hoon Na, Jong-Hyeok Lee, Gijoo 
Yang. 2004. Lightweight Natural Language Database 
Interfaces. Proceedings of the 9th International Con-
ference on Application of Natural Language to In-
formation Systems. 76-88. 
Juhong Ha, Yu Zheng, Byeongchang Kim, Gary Geun-
bae Lee, Yoon-Suk Seong. 2004. High Speed Un-
known Word Prediction Using Support Vector 
Machine for Chinese Text-to-Speech Systems. 
IJCNLP:509-517 
Kiyotaka Uchimoto, Qing Ma, Masaki Murata, Hiromi 
Ozaku, and Hitoshi Isahara. Named Entity Extraction 
Based on A Maximum Entropy Model and Trans-
formation Rules. In Proceedings of the 38th Annual 
Meeting of Association for Computational Linguis-
tics 
Nancy A. Chinchor. 1998. MUC-7 named entity task 
definition, Proceedings of the Seventh Message Un-
derstanding Conference. 
Richard Cooper, Sajjad Ali, Chenlan Bi, 2005. Extract-
ing Information from Short Messages, NLDB 2005, 
LNCS 3513, pp. 388-391.  
Rohini Srihari, Cheng Niu, Wei Li. 2001. A hybrid ap-
proach for named entity and sub-type tagging. In 
Proc. 6th Applied Natural Language Processing Con-
ference. 
Zijian Zheng. Naive Bayesian classifier committees. 
Proceedings of the 10th European Conference on 
Machine Learning. Berlin: Springet-Verlag (1998) 
196-207. 
SVM_light: http://svmlight.joachims.org/ 
CRF++: http://crfpp.sourceforge.net/ 
18
