Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 73?78,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Exploring Multilingual Semantic Role Labeling 
 
Baoli Li, Martin Emms, Saturnino Luz, Carl Vogel 
Department of Computer Science 
Trinity College Dublin 
Dublin 2, Ireland 
{baoli.li,mtemms,luzs,vogel}@cs.tcd.ie 
 
 
 
Abstract 
This paper describes the multilingual semantic 
role labeling system of Computational Lin-
guistics Group, Trinity College Dublin, for the 
CoNLL-2009 SRLonly closed shared task. 
The system consists of two cascaded compo-
nents: one for disambiguating predicate word 
sense, and the other for identifying and classi-
fying arguments. Supervised learning tech-
niques are utilized in these two components. 
As each language has its unique characteris-
tics, different parameters and strategies have 
to be taken for different languages, either for 
providing functions required by a language or 
for meeting the tight deadline. The system ob-
tained labeled F1 69.26 averaging over seven 
languages (Catalan, Chinese, Czech, English, 
German, Japanese, and Spanish), which ranks 
the system fourth among the seven systems 
participating the SRLonly closed track. 
1 Introduction 
Semantic role labeling, which aims at computa-
tionally identifying and labeling arguments of 
predicate words, has become a leading research 
problem in computational linguistics with the ad-
vent of various supporting resources (e.g. corpora 
and lexicons) (M?rquez et al, 2008). Word seman-
tic dependencies derived by semantic role labeling 
are assumed to facilitate automated interpretation 
of natural language texts. Moreover, techniques for 
automatic annotation of semantic dependencies can 
also play an important role in adding metadata to 
corpora for the purposes of machine translation 
and speech processing. We are currently investi-
gating such techniques as part of our research into 
integrated language technology in the Center for 
Next Generation Localization (CNGL, 
http://www.cngl.ie). The multilingual nature of the 
CoNLL-2009 shared task on syntactic and seman-
tic dependency analysis, which includes Catalan, 
Chinese, Czech, English, German, Japanese, and 
Spanish (Haji? et al, 2009), makes it a good test-
bed for our research. 
We decided to participate in the CoNLL-2009 
shared task at the beginning of March, signed the 
agreement for getting the training data on March 
2nd, 2009, and obtained all the training data (espe-
cially the part from LDC) on March 4th, 2009. Due 
to the tight time constraints of the task, we chose to 
use existing packages to implement our system. 
These time constraints also meant that we had to 
resort to less computationally intensive methods to 
meet the deadline, especially for some large data-
sets (such as the Czech data). In spite of these dif-
ficulties and resource limitations, we are proud to 
be among the 21 teams who successfully submitted 
the results1. 
As a new participant, our goals in attending the 
CoNLL-2009 SRLonly shared task were to gain 
more thorough knowledge of this line of research 
and its state-of-the-art, and to explore how well a 
system quickly assembled with existing packages 
can fare at this hard semantic analysis problem.  
Following the successful approaches taken by 
the participants of the CoNLL-2008 shared task 
(Surdeanu et al, 2008) on monolingual syntactic 
and semantic dependency analysis, we designed 
and implemented our CoNLL-2009 SRLonly sys-
tem with pipeline architecture. Two main compo-
nents are cascaded in this system: one is for 
disambiguating predicate word sense 2 , and the 
other for identifying and classifying arguments for 
                                                          
1
 According to our correspondence with Dr. Jan Haji?, totally 
31 teams among 60 registered ones signed and got the evalua-
tion data. 
2
 As predicate words are marked in the CoNLL-2009 datasets, 
we don?t need to identify predicate words. 
73
predicate words. Different supervised learning 
techniques are utilized in these two components. 
For predicate word sense disambiguation (WSD), 
we have experimented with three algorithms: SVM, 
kNN, and Na?ve Bayes. Based on experimental 
results on the development datasets, we chose 
SVM and kNN to produce our submitted official 
results. For argument identification and classifica-
tion, we used a maximum entropy classifier for all 
the seven datasets. As each language has its unique 
characteristics and peculiarities within the dataset, 
different parameters and strategies have to be taken 
for different languages (as detailed below), either 
for providing functions required by a language or 
for meeting the tight deadline. Our official submis-
sion obtained 69.26 labeled F1 averaging over the 
seven languages, which ranks our system fourth 
among the seven systems in the SRLonly closed 
track. 
The rest of this paper is organized as follows. 
Section 2 discusses the first component of our sys-
tem for predicate word sense disambiguation. Sec-
tion 3 explains how our system detects and 
classifies arguments with respect to a predicate 
word. We present experiments in Section 4, and 
conclude in Section 5. 
2 Predicate Word Sense Disambiguation 
This component tries to determine the sense of a 
predicate word in a specific context. As a sense of 
a predicate word is often associated with a unique 
set of possible semantic roles, this task is also 
called role set determination. Based on the charac-
teristics of different languages, we take different 
strategies in this step, but the same feature set is 
used for different languages. 
2.1 Methods 
Intuitively, each predicate word should be treated 
individually according to the list of its possible 
senses. We therefore designed an initial solution 
based on the traditional methods in WSD: repre-
sent each sense as a vector from its definition or 
examples; describe the predicate word for disam-
biguation as a vector derived from its context; and 
finally output the sense which has the highest simi-
larity with the current context. We also considered 
using singular value decomposition (SVD) to over-
come the data sparseness problem. Unfortunately, 
we found this solution didn?t work well in our pre-
liminary experiments. The main problem is that the 
definition of each sense of a predicate word is not 
available. What we have is just a few example con-
texts for one sense of a predicate word, and these 
contexts are often not informative enough for 
WSD. On the other hand, our limited computing 
resources could not afford SVD operation on a 
huge matrix. 
We finally decided to take each sense tag as a 
class tag across different words and transform the 
disambiguation problem into a normal multi-class 
categorization problem. For example, in the Eng-
lish datasets, all predicates with ?01? as a sense 
identifier were counted as examples for the class 
?01?. With this setting, a predicate word may be 
assigned an invalid sense tag. It is an indirect solu-
tion, but works well. We think there are at least 
two possible reasons: firstly, most predicate words 
take their popular sense in running text. For exam-
ple, in the English dataset (training and develop-
ment), 160,477 of 185,406 predicate occurrences 
(about 86.55%) take their default sense ?01?. Sec-
ondly, predicates may share some common role 
sets, even though their senses may not be exactly 
the same, e.g. ?tell? and ?inform?. 
Unlike the datasets in other languages, the Japa-
nese dataset doesn?t have specialized sense tags 
annotated for each predicate word, so we simply 
copy the predicted lemma of a predicate word to its 
PRED field. For other datasets, we derived a train-
ing sample for each predicate word, whose class 
tag is its sense tag. Then we trained a model from 
the generated training data with a supervised learn-
ing algorithm, and applied the learned model for 
predicting the sense of a predicate word. This is 
our base solution. 
When transforming the datasets, the Czech data 
needs some special processing because of its 
unique annotation format. The sense annotation for 
a predicate word in the Czech data does not take 
the form ?LEMMA.SENSE?. In most cases, no 
specialized sense tags are annotated. The PRED 
field of these words only contains ?LEMMA?. In 
other cases, the disambiguated senses are anno-
tated with an internal representation, which is 
given in a predicate word lexicon. We decomposed 
the internal representation of each predicate word 
into two parts: word index id and sense tag. For 
example, from ?zv??en? v-w10004f2? we know ?v-
w10004? is the index id of word ?zv??en??, and 
?f2? is its sense tag. We then use these derived 
74
sense tags as class tags and add a class tag ?=? for 
samples without specialized sense tag. 
For each predicate word, we derive a vector de-
scribing its context and attributes, each dimension 
of which corresponds to a feature. We list the fea-
ture types in the next subsection. Features appear-
ing only once are removed. The TF*IDF weighting 
schema is used to calculate the weight of a feature. 
Three different algorithms were tried during the 
development period: support vector machines 
(SVM), distance-weighted k-Nearest Neighbor 
(kNN) (Li et al, 2004), and Na?ve Bayes with mul-
tinomial model (Mccallum and Nigam, 1998). As 
to the SVM algorithm, we used the robust 
LIBSVM package (Chang and Lin, 2001), with a 
linear kernel and default values for other parame-
ters. The algorithms achieving the best results in 
our preliminary experiments are chosen for differ-
ent languages: SVM for Catalan, Chinese, and 
Spanish; kNN for German (k=20). 
We used kNN for English (k=20) and Czech 
(k=10) because we could not finish training with 
SVM on these two datasets in limited time. Even 
with kNN algorithm, we still had trouble with the 
English and Czech datasets, because thousands of 
training samples make the prediction for the 
evaluation data unacceptably slow. We therefore 
had to further constrain the search space for a new 
predicate word to those samples containing the 
same predicate word. If there are not samples con-
taining the same predicate word in the training data, 
we will assign it the most popular sense tag (e.g. 
?01? for English). 
How to use the provided predicate lexicons is a 
challenging issue. Lexicons for different languages 
take different formats and the information included 
in different lexicons is quite different. We derived 
a sense list lexicon from the original predicate 
lexicon for Chinese, Czech, English, and German. 
Each entry in a sense list lexicon contains a predi-
cate word, its internal representation (especially for 
Czech), and a list of sense tags that the predicate 
can have. Then we obtained a variant of our base 
solution, which uses the sense list of a predicate 
word to filter impossible senses. It works as fol-
lows: 
- Disambiguate a new predicate with the base 
solution; 
- Choose the most possible sense from all the 
candidate senses obtained in step 1: if the 
base classifier doesn?t output a vector of 
probabilities for classes, only check 
whether the predicted one is a valid sense 
for the predicate; 
- If there is not a valid sense for a new predi-
cate (including the cases where the predi-
cate does not have an entry in the sense list 
lexicon), output the most popular sense tag; 
Unfortunately, preliminary experiments on the 
German and Chinese datasets didn?t support to in-
clude such a post-processing stage. The perform-
ance with this filtering became a little worse. 
Therefore, we decided not to use it generally, but 
one exception is for the Czech data. 
With kNN algorithm, we can greatly reduce the 
time for training the Czech data, but we do have 
problem with prediction, as there are totally 
469,754 samples in the training dataset. It?s a time-
consuming task to calculate the similarities be-
tween a new sample and all the samples in the 
training dataset to find its k nearest neighbors, thus 
we have to limit the search space to those samples 
that contain the predicate word for disambiguation. 
To process unseen predicate words, we used the 
derived sense list lexicon: if a predicate word for 
disambiguation is out of the sense list lexicon, we 
simply copy its predicted lemma to the PRED field; 
if no sample in the training dataset has the same 
predicate word, we take its first possible sense in 
the sense list lexicon. With this strategy, our sys-
tem can process the huge Czech dataset in short 
time. 
2.2 Features 
The features we used in this step include3: 
 
a. [Lemma | (Lemma with POS)] of all words in the sen-
tence; 
b. Attributes of predicate word, which is obtained from 
PFEAT field by splitting the field at symbol ?|? and 
removing the invalid attribute of ?*?; 
c. [Lemma | POS] bi-grams of predicate word and its 
[previous | following] one word; 
d. [Lemma | POS] tri-grams of predicate word and its 
[previous | following] two words; 
e. [Lemma | (Lemma with POS)] of its most [left | right] 
child; 
f. [(Lemma+Dependency_Relation+Lemma) | (POS 
+Dependency_Relation+POS)] of predicate word and 
its most [left | right] child; 
                                                          
3
 We referred to those CoNLL-2008 participants? reports, e.g. 
(Ciaramita et al, 2008), when we designed the feature sets for 
the two components. 
75
g. [Lemma | (Lemma with POS)] of the head of the pre-
dicate word; 
h. [(Lemma+Dependency_Relation+Lemma) | (POS+D-
ependency_Relation+POS)] of predicate word and its 
head; 
i. [Lemma | (Lemma with POS)] of its [previous | fol-
lowing] two brothers; 
j. [Lemma | POS | (Dependency relation)] bi-gram of 
predicate word and its [previous | following] one 
brother; 
k. [Lemma | POS | (Dependency relation)] tri-gram of 
predicate word and its [previous | following] two 
brothers. 
3 Argument Identification and Classifica-
tion  
The second component of our system is used to 
detect and classify arguments with respect to a 
predicate word. We take a joint solution rather than 
solve the problem in two consecutive steps: argu-
ment identification and argument classification. 
3.1 Methods  
By introducing an additional argument type tag ?_? 
for non-arguments, we transformed the two tasks 
(i.e. argument identification and argument classifi-
cation) into one multi-class classification problem. 
As a word can play different roles with respect to 
different predicate words and a predicate word can 
be an argument of itself, we generate a training set 
by deriving a training example from each word-
predicate pair. For example, if a sentence with two 
predicates has 7 words, we will derive 7*2=14 
training examples. Therefore, the number of train-
ing examples generated in this step will be around 
L times larger than that obtained in the previous 
step, where L is the average length of sentences. 
We chose to use maximum entropy algorithm in 
this step because of its success in the CoNLL-2008 
shared task (Surdeanu et al, 2008). Le Zhang?s 
maximum entropy package (Zhang, 2006) is inte-
grated in our system. 
The Czech data cause much trouble again for us, 
as the training data derived by the above strategy 
became even larger. We had to use a special strat-
egy for the Czech data: we selectively chose word-
predicate pairs for generating the training dataset. 
In other words, not all possible combinations are 
used. We chose the following words with respect 
to each predicate: the first and the last two words 
of a sentence; the words between the predicate and 
any argument of it; two words before the predicate 
or any argument; and two words after the predicate 
or any argument. 
In the Czech and Japanese data, some words 
may play multiple roles with respect to a predicate 
word. We thus have to consider multi-label classi-
fication problem (Tsoumakas and Katakis, 2007) 
for these two languages? data. We tried the follow-
ing two solutions: 
? Take each role type combination as a class 
and transform the multi-label problem to a 
single-label classification problem; 
? Classify a word with a set of binary classi-
fiers: consider each role type individually 
with a binary classifier; any possible role 
type will be output; if no role type is ob-
tained after considering all the role types, 
the role type with the highest confidence 
value will be output; and, if ?_? is output 
with any other role type, remove it. 
We used the second solution in our official 
submission, but we finally found these two solu-
tions perform almost the same. The performance 
difference is very small. We found the cases with 
multi-labels (actually at most two) in the training 
data are very limited: 690 of 414,326 in the Czech 
data and 113 of 46,663 in the Japanese data. 
3.2 Features 
The features we used in this step include: 
 
a. Whether the current word is a predicate; 
b. [Lemma | POS] of current word and its [previous | fol-
lowing] one word; 
c. [Lemma | POS] bi-grams of current word and its [pre-
vious | following] one word; 
d. POS tri-grams of current word, its previous word and 
its following word; 
e. Dependency relation of current word to its head; 
f. [Lemma | POS] of the head of current word; 
g. [Lemma | POS] bi-grams of current word and its head; 
h. [(Lemma+Dependency_Relation+Lemma) | (POS+De 
pendency_Relation+POS)] of current word and its 
head; 
i. [Lemma | POS] of its most [left | right] child; 
j. [Lemma | POS] bi-grams of current word and its most 
[left | right] child; 
k. [(Lemma+Dependency_Relation+Lemma) | (POS+De 
pendency_Relation+POS) of current word and its 
most [left | right] child; 
l. The number of children of the current word and the 
predicate word; 
m. Attributes of the current word, which is obtained from 
PFEAT field by splitting the field at symbol ?|? and 
removing the invalid attribute of ?*?; 
n. The sense tag of the predicate word; 
76
o. [Lemma | POS] of the predicate word and its head; 
p. Dependency relation of the predicate word to its head; 
q. [Lemma | POS] bi-grams of the predicate word and its 
head; 
r. [(Lemma+Dependency_Relation+Lemma) | (POS+De 
pendency_Relation+POS)] of the predicate word and 
its head; 
s. [Lemma | POS] of the most [left | right] child of the 
predicate word; 
t. [(Lemma+Dependency_Relation+Lemma) | (POS+De 
pendency_Relation+POS)] of predicate word and its 
head; 
u. [Lemma | POS] bi-gram of the predicate word and its 
most [left | right] child; 
v. [(Lemma+Dependency_Relation+Lemma) | (POS+De 
pendency_Relation+POS)] of the predicate word and 
its most [left | right] child; 
w. The relative position of the current word to the predi-
cate one: before, after, or on; 
x. The distance of the current word to the predicate one; 
y. The relative level (up, down, or same) and level dif-
ference on the syntactic dependency tree of the current 
word to the predicate one; 
z. The length of the shortest path between the current 
word and the predicate word. 
4 Experiments 
4.1 Datasets  
The datasets of the CoNLL-2009 shared task con-
tain seven languages: Catalan (CA), Chinese (CN), 
Czech (CZ), English (EG), German (GE), Japanese 
(JP), and Spanish (SP). The training and evaluation 
data of each language (Taul? et al, 2008; Xue et 
al., 2008; Haji? et al, 2006; Palmer et al, 2002; 
Burchardt et al, 2006; Kawahara et al, 2002) have 
been converted to a uniform CoNLL Shared Task 
format. Each participating team is required to 
process all seven language datasets.  
 
Lanuage CA CN CZ EN GE JP SP 
Size (KB) 48974 41340 94284 58155 41091 8948 52430 
# of Sen-
tences 14924 24039 43955 40613 38020 4643 15984 
# of Predi-
cate words 42536 110916 469754 185404 17988 27251 48900 
Avg. # of 
Predicates 
per sentence 
2.85 4.61 10.69 4.57 0.47 5.87 3.06 
popular 
sense tag 
a2 
(37%) 
01 
(90%) 
= 
(81%) 
01 
(87%) 
1 
(75%) 
= 
(100%) 
a2 
(39%) 
Table 1. Statistical information of the seven language 
datasets (training and development). 
 
Table 1 shows some statistical information of 
both training and development data for each lan-
guage. The total size of the uncompressed original 
data without lexicons is about 345MB. The Czech 
dataset is the largest one containing 43,955 sen-
tences and 469,754 predicate words, while the 
Japanese dataset the smallest one. On average, 
10.69 predicate words appear in a Czech sentence, 
while only 0.47 predicate words exist in a German 
sentence. The most popular sense tag in the Czech 
datasets is ?=?, which means the PRED field has 
the same value as the PLEMMA field or the 
FORM field. About 81% of Czech predicate words 
take this value. 
4.2 Experimental Results  
F1 is used as the main evaluation metric in the 
CoNLL-2009 shared task. As to the SRLonly track, 
a joint semantic labeled F1, which considers predi-
cate word sense disambiguation and argument la-
beling equally, is used to rank systems. 
 
Avg. CA CN CZ EG GE JP SP 
69.26 74.06 70.37 57.46 69.63 67.76 72.03 73.54 
Table 2. Official results of our system. 
 
Table 2 gives the official results of our system 
on the evaluation data. The system obtained the 
best result (74.06) on the Catalan data, but per-
formed very poor (57.46) on the Czech data. Ex-
cept the Czech data, our system performs quite 
stable on the other six language data with mean of 
71.23 and standard deviation of 2.42. 
 
 Avg. CA CN CZ EG GE JP SP 
Over-
all F1 69.47 74.12 70.52 57.57 70.24 67.97 72.17 73.68 
Pred. 
WSD 
F1 
86.9 84.42 94.54 72.23 92.98 81.09 99.07 83.96 
Arg 
I&C 
F1 
57.24 69.29 57.71 33.19 58.25 60.64 52.72 68.86 
Arg 
I&C 
PR 
69.77 73.43 72.48 62.14 70.14 66.63 69.37 74.23 
Arg 
I&C 
RE 
49.77 65.6 47.94 22.64 49.81 55.64 42.52 64.21 
Table 3. Results of our system after fixing a minor bug. 
 
After submitting the official results, we found 
and fixed a minor bug in the implementation of the 
second component. Table 3 presents the results of 
our system after fixing this bug. The overall per-
formance doesn?t change much. We further ana-
lyzed the bottlenecks by checking the performance 
of different components. 
At the predicate WSD part, our system works 
reasonable with labeled F1 86.9, but the perform-
ance on the Czech data is lower than that of a base-
line system that constantly chooses the most 
popular sense tag. If we use this baseline solution, 
77
we can get predicate WSD F1 78.66, which further 
increases the overall labeled F1 on the Czech data 
to 61.68 from 57.57 and the overall labeled F1 
over the seven languages to 70.05 from 69.47. 
From table 3, we can see our system performs 
relatively poorly for argument identification and 
classification (57.24 vs. 86.9). The system seems 
too conservative for argument identification, which 
makes the recall very lower. We explored some 
strategies for improving the performance of the 
second component, e.g. separating argument iden-
tification and argument classification, and using 
feature selection (with DF threshold) techniques, 
but none of them helps much. We are thinking the 
features currently used may not be effective 
enough, which deserves further study. 
5 Conclusion and Future Work  
In this paper, we describe our system for the 
CoNLL-2009 shared task -- SRLonly closed track. 
Our system was built on existing packages with a 
pipeline architecture, which integrated two cas-
caded components: predicate word sense disam-
biguation and argument identification and 
classification. Our system performs well at disam-
biguating the sense of predicate words, but poorly 
at identifying and classifying arguments. In the 
future, we plan to explore much effective features 
for argument identification and classification. 
Acknowledgments 
This research was funded by Science Foundation 
Ireland under the CNGL grant. We used the IITAC 
Cluster in our initial experiments. We thank IITAC, 
the HEA, the National Development Plan and the 
Trinity Centre for High Performance Computing 
for their support. We are also obliged to John 
Keeney for helping us running our system on the 
CNGL servers. 
References  
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea 
Kowalski, Sebastian Pad? and Manfred Pinkal. 2006. 
The SALSA Corpus: a German Corpus Resource for 
Lexical Semantics. Proceedings of the 5th Interna-
tional Conference on Language Resources and Eval-
uation (LREC-2006). Genoa, Italy. 
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM: 
a library for support vector machines. Software 
available at http://www.csie.ntu.edu.tw/~cjlin/libsvm. 
Massimiliano Ciaramita, Giuseppe Attardi, Felice 
Dell?Orletta, and Mihai Surdeanu. 2008. DeSRL: A 
Linear-Time Semantic Role Labeling System. Pro-
ceedings of the CoNLL-2008. 
Jan Haji?, Massimiliano Ciaramita, Richard Johansson, 
Daisuke Kawahara, Maria Antonia Mart?, Llu?s 
M?rquez, Adam Meyers, Joakim Nivre, Sebastian 
Pad?, Jan ?t?p?nek, Pavel Stra??k, Mihai Surdeanu, 
Nianwen Xue and Yi Zhang. 2009. The CoNLL-2009 
Shared Task: Syntactic and Semantic Dependencies 
in Multiple Languages. Proceedings of the 13th 
Conference on Computational Natural Language 
Learning (CoNLL-2009). Boulder, Colorado, USA. 
Jan Haji?, Jarmila Panevov?, Eva Haji?ov?, Petr Sgall, 
Petr Pajas, Jan ?t?p?nek, Ji?? Havelka, Marie 
Mikulov? and Zden?k ?abokrtsk?. 2006. The Prague 
Dependency Treebank 2.0. Linguistic Data 
Consortium, USA. ISBN 1-58563-370-4. 
Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida. 
2002. Construction of a Japanese Relevance-tagged 
Corpus. Proceedings of the 3rd International Confer-
ence on Language Resources and Evaluation (LREC-
2002). Las Palmas, Spain. 
Baoli Li, Qin Lu and Shiwen Yu. 2004. An Adaptive k-
Nearest Neighbor Text Categorization Strategy. ACM 
Transactions on Asian Language Information 
Processing, 3(4): 215-226. 
Llu?s M?rquez, Xavier Carreras, Kenneth C. Litkowski 
and Suzanne Stevenson. 2008. Semantic Role Label-
ing: An Introduction to the Special Issue. Computa-
tional Linguistics, 34(2):145-159. 
Andrew Mccallum and Kamal Nigam. 1998. A Com-
parison of Event Models for Naive Bayes Text Clas-
sification. Proceedings of AAAI/ICML-98 Workshop 
on Learning for Text Categorization. 
Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Lluis Marquez and Joakim Nivre. 2008. The CoNLL-
2008 Shared Task on Joint Parsing of Syntactic and 
Semantic Dependencies. Proceedings of the 12th 
Conference on Computational Natural Language 
Learning (CoNLL-2008).  
Mariona Taul?, Maria Ant?nia Mart? and Marta 
Recasens. 2008. AnCora: Multilevel Annotated 
Corpora for Catalan and Spanish. Proceedings of the 
6th International Conference on Language Resources 
and Evaluation (LREC-2008). Marrakech, Morocco. 
Grigorios Tsoumakas and Ioannis Katakis. 2007. Multi-
Label Classification: An Overview. International 
Journal of Data Warehousing and Mining, 3(3):1-13. 
Nianwen Xue and Martha Palmer. 2009. Adding 
semantic roles to the Chinese Treebank.  Natural 
Language Engineering, 15(1):143-172.  
Le Zhang. 2006. Maximum Entropy Modeling Toolkit 
for Python and C++. Software available at 
http://homepages.inf.ed.ac.uk/s0450736/maxent_tool
kit.html. 
78
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2205?2216, Dublin, Ireland, August 23-29 2014.
Limitations of MT Quality Estimation Supervised Systems:
The Tails Prediction Problem
Erwan Moreau
CNGL and Computational Linguistics Group
Centre for Computing and Language Studies
School of Computer Science and Statistics
Trinity College Dublin
Dublin 2, Ireland
moreaue@cs.tcd.ie
Carl Vogel
Computational Linguistics Group
Centre for Computing and Language Studies
School of Computer Science and Statistics
Trinity College Dublin
Dublin 2, Ireland
vogel@cs.tcd.ie
Abstract
In this paper we address the question of the reliability of the predictions made by MT Quality
Estimation (QE) systems. In particular, we show that standard supervised QE systems, usually
trained to minimize MAE, make serious mistakes at predicting the quality of the sentences in the
tails of the quality range. We describe the problem and propose several experiments to clarify
their causes and effects. We use the WMT12 and WMT13 QE Shared Task datasets to prove that
our claims hold in general and are not specific to a dataset or a system.
1 Introduction
Machine Translation (MT) Quality Estimation (QE) has become an important subject of study in the past
few years (Callison-Burch et al., 2012; Bojar et al., 2013). This follows directly from the erratic quality
of MT output in general: although MT is now widely used in professional contexts, it is still prone to
many errors; therefore a careful post-editing stage, performed by human experts, is usually needed. In
this context, QE can help carrying out this process more efficiently, and more specifically to help in
the decision process between the automatic and the manual stages: if a reliable indication of quality is
provided for every machine-translated sentence, the human effort can be reduced. For example, a very
bad translation is worthless because the translator usually has to spend more time fixing it than she or he
would have spent translating the sentence from scratch; thus it makes more sense in such cases to either
send the sentence back to an alternative MT system (e.g. trained on a different corpus), or simply leave
it untranslated for the translator. Clearly the advantage of using a QE system depends on the reliability
of its predictions. If it makes too many errors, then it only confuses the translation workflow; in this case
the translators would perform better without it.
The quality of an (automatic) QE system cannot be perfect, but it should be at least controllable. That
is, it should be possible to assess the reliability of the predictions made by a system, for instance by
estimating the level of confidence of the predictions. Hopefully, QE systems will progress towards this
kind of behaviour, but currently the evaluation methods are not entirely satisfactory from this perspective.
In particular, after describing our experimental setting in ?2, we will observe in ?3 that the use of the
Mean Absolute Error
1
(MAE) as a global evaluation measure hides huge discrepancies in the distribution
of errors among the range of scores. More precisely, supervised systems optimized to minimize the MAE
have intrinsic flaws in the way they assess the tails of the quality range, i.e. the ?very good? and the ?very
bad? sentences. In ?4 we propose different ways to evaluate the impact of this problem, and also clarify
what might be an important misunderstanding in what a QE system actually does (?4.2). Finally we
propose in ?5 several experiments: in ?5.1 we show that the problem is not system-specific, and we test
two ways to circumvent it in ?5.2 and ?5.3, but the price to pay in global performance is high.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
The MAE is defined as the mean over all instances of the absolute error, where the absolute error is the absolute value of
the difference between the predicted and the actual value of the instance. Thus, the MAE score depends on the range of possible
values (i.e., two datasets using different ranges cannot be compared).
2205
2 Experimental Setup
2.1 Data
In this paper we use the three datasets from the WMT12 and WMT13 QE Shared Task (Callison-Burch
et al., 2012; Bojar et al., 2013) which are intended to predict the quality of individual machine-translated
sentences: the WMT12 task and the WMT13 task 1.1 and 1.3. The last two datasets are renamed wmt13a
and wmt13b in the rest of this paper. These three datasets differ by the way quality is measured:
? wmt12: effort scores, which have been assigned by three professional post-editors according to
predefined guidelines; scores range from 1: ?the MT output is incomprehensible [..]? to 5: ?the
MT output is perfectly clear [..]?. The dataset was cleaned to avoid the cases with a high level of
disagreement, and the scores were post-processed to harmonize the scale between the judges.
? wmt13a: HTER scores, which measure the distance between the MT output and the post-edited
sentence (Snover et al., 2006).
? wmt13b: post-editing time, that is, the time that the post-editor has spent correcting the MT output.
As a consequence, the set of scores have different characteristics: in wmt12, the distribution is highly
discrete due to the integer values assigned by the judges. In wmt13a the distribution is more dense,
whereas in wmt13b some values are spread extremely far from the mean.
2
General statistics for the
datasets are given in table 1. In all datasets the input and MT output sentences are available to the system;
the post-edited version of the sentences is also available, but it cannot be used by the QE systems (the
test set post-edited sentences were provided only after the end of the task). We focus on predicting
an absolute indication of quality rather than only ranking the sentences by quality; this is why we use
the Mean Absolute Error (MAE) as the main evaluation measure rather than Spearman?s correlation or
DeltaAvg (Callison-Burch et al., 2012).
2.2 Supervised QE System
In the observations and experiments described in this paper we use a QE system which follows a standard
supervised learning approach: it was trained on the full training set for every task considered; when the
performance on the training set is observed, it was assessed using 10-fold cross-validation (thus obtaining
a prediction for every sentence in the train set based on a 90% subset). We have used Quest
3
(Shah et
al., 2013), an open-source tool for QE, to compute the 17 ?black box features? which are also used in the
WMT QE ?baseline? system (see below). We have used Weka (Hall et al., 2009) (version 3.6.10), and
after testing several options
4
we found that using the SMOreg algorithm (Smola and Sch?olkopf, 2004;
Shevade et al., 2000) with an RBF kernel
5
was optimal with respect to the performance on the three
datasets.
We did not perform any feature selection or parameter tuning, because our main goal was to build a
generic system. Additionally we favor the ease of reproducibility over optimal performance, which is out
of the scope of this paper. We want our system to be as generic as possible (but still performing decently,
of course), because we need it to be fairly representative of standard, state-of-the-art, supervised learning
QE systems. This is very important, since our observations and experiments are supposed to generalize
to the current most common approaches in QE.
Our task of making the system representative of state-of-the-art QE systems has been greatly facilitated
by the fact that the organizers of the WMT12 and WMT13 QE Shared Task provide for every task the
performance of a so-called ?baseline system?. We can use exactly the same set of features and compare
the results of our system against these obtained by this baseline system, which in turn does not deserve
2
This is why we exclude the most striking outlier from the training set: 1115.906, line 294. The test set is left unchanged.
3
http://staffwww.dcs.shef.ac.uk/people/L.Specia/projects/quest.html ? last verified 05/14.
4
In particular, M5P regression trees generally achieve nearly as good performance as SVM regression. We have also
observed that at least the most important characteristics reported in this paper for an SVM system hold for M5P regression as
well.
5
With the default value C=1 and standardization of the features values.
2206
its name since it has actually always performed well in every task: it ranked 8th out of 20 in the WMT12
official ranking, 12th out of 17 in WMT13a, and 6th out of 14 in WMT13b (MAE ranking). Thus, we
can simply check that our system performs as well as this baseline system to ensure that it is equivalent,
and therefore probably reasonably similar to the other supervised systems submitted to the Shared Tasks
which perform similarly.
6
Table 1 shows that our system performs roughly the same as the baseline
system on the three datasets.
Range Statistics Performance (test set)
Dataset of Quality Train set Test set Our system Baseline system
values direction instances mean std. dev. instances mean std. dev. cor. MAE cor. MAE
wmt12 [1, 5] ? 1832 3.44 0.88 422 3.29 0.98 0.56 0.69 0.58 0.69
wmt13a [0, 1] ? 2254 0.32 0.17 500 0.26 0.19 0.44 0.15 0.46 0.15
wmt13b [0,+ inf[ ? 802 95.6 84.2 284 116.9 108.3 0.70 50.9 0.70 51.9
Table 1: Datasets: statistics and performance. Quality direction: ? means that the quality is better
when the score is higher,? means the opposite; ?cor.? is the Spearman?s correlation.
3 The Tails Prediction Problem
In this section we mostly observe the training set (using cross-validation), in order to dismiss the possi-
bility that the observed phenomenon is caused by the differences in the distributions of scores between
the training set and the test set. Since it is easier for a supervised learning algorithm to annotate some
data from the set it was trained on than from a different dataset, problems which appear with the former
are very likely to appear as well (possibly accentuated) with the latter.
2
3
4
5
1 2 3 4 5gold
predict
(a) wmt12. Spearman cor.: 0.53
0.2
0.3
0.4
0.5
0.00 0.25 0.50 0.75 1.00gold
predict
(b) wmt13a. Spearman cor.: 0.37
0
100
200
0 100 200 300 400 500gold
predict
(c) wmt13b. Spearman cor.: 0.62
Figure 1: Scatter plots showing how predicted scores differ from gold scores (test set). Every point
(X,Y) corresponds to one sentence for which X is the gold score and Y the predicted score. Darker
areas correspond to more dense areas; the vertical and horizontal lines indicate the frontiers of 20%-
quantiles for both variables (for instance, the points which are on the right side of the rightmost vertical
line account for the 20% highest gold scores). Remark: a few outliers are not visible on the wmt13b plot
(their gold scores are higher than 500, and their predicted scores are lower than 250).
Figure 1 shows that the points are very scattered and do not follow the diagonal very closely, but
also that the range of predicted scores is significantly different from the range of gold scores: no sen-
tence is predicted below 2 for wmt12, above 0.55 for wmt13a and above 260 for wmt13b, whereas the
corresponding range of gold scores is much wider. Figure 2, which shows the distribution of gold vs.
predicted scores for the training sets, gives a more precise picture of this difference: in all three datasets,
the predicted scores tend to belong to a smaller set of values centered approximately around the mean.
There are clearly more predicted values than gold values in this area, and this is confirmed by the much
smaller standard deviation for the predicted scores.
It is possible to obtain a clearer picture by ?flattening? the distribution, that is, instead of drawing
histograms in which points with the same value (or a close value) are accumulated, we represent every
6
In section 5.1 we also check more specifically that our observations hold for most of the systems submitted to WMT12.
2207
0
100
200
300
1 2 3 4 5score
count groupgoldpredict
(a) wmt12. ?
G
= 0.88, ?
P
= 0.50
0100
200300
400
0.00 0.25 0.50 0.75 1.00score
count groupgoldpredict
(b) wmt13a. ?
G
= 0.17, ?
P
= 0.07
0
50
100
150
0 200 400 600score
count groupgoldpredict
(c) wmt13b. ?
G
= 84.2, ?
P
= 46.5
Figure 2: Combined distributions of the gold scores and predicted scores on the training set for the
three datasets. ?
G
(resp. ?
P
) is the standard deviation for gold (resp. predicted) scores.
point on the X axis and sort the values on this axis, so that their actual value can be observed on the Y axis,
as shown on figure 3. This figure shows that, in all three cases, the predicted scores are tightly clustered
around the median, which is the point where the two curves cross each other. If the system predicted
scores according to the distribution it observed on the training set, the two curves would be close; instead,
they clearly diverge from each other as the distance to the median increases. This means that the model
tends globally to overestimate the points below the median and, symmetrically, underestimate the points
above the median (though the symmetry is degraded in 3c, since the range is unbound to the right).
In figure 3 the two sets of points are sorted independently: the sentence (x, y) on the curve of gold
scores is different from the one with the same x on the curve of predicted scores. Yet this observation
of ?tightened? predicted scores cannot be fully understood without taking into account the risk of error
in the prediction process, as it was visible on the scatter plots in figure 1. Thus it is also useful to look
at the sorted scores, but with their corresponding predicted score (for the same sentence) plotted on the
same x coordinate; this what is shown on figure 4, for the wmt13a dataset only (because the phenomenon
is the most accentuated in this dataset, and scores conveniently belong to [0, 1]). On figure 4a one can
see that the set of predicted scores are mostly contained in a slightly inclined rectangle; clearly they do
not follow the curve of gold scores, but here one can see why: the fact that there are many points at the
same level on the Y axis along the whole X axis shows that the algorithm cannot make a clear distinction
between the different levels of quality. For example, there are approximately as many scores predicted
around 0.3 which correspond to actually very good (rank near 0) and very bad sentences (rank near 1).
From a different perspective, figure 4b shows very clearly that the farther the gold score of a sentence is
from the mean (0.32), the more likely it is to be predicted with a large error.
2
3
4
5
0.00 0.25 0.50 0.75 1.00rank
score seriesgoldpredict
(a) wmt12
0.00
0.25
0.50
0.75
1.00
0.00 0.25 0.50 0.75 1.00rank
score seriesgoldpredict
(b) wmt13a
0
100
200
300
400
0.00 0.25 0.50 0.75 1.00rank
score seriesgoldpredict
(c) wmt13b
Figure 3: Sorted gold and predicted scores for the three datasets. The two sets of scores are sorted
independently. The X axis is the normalized rank (0 to 1 instead of 1 to the total number of sentences),
so that it is easier to observe the quantiles. Example: for wmt13a, the lowest fourth of gold scores ranges
from 0 to around 0.20, whereas the lowest fourth of predicted scores ranges from 0.125 to around 0.27.
Remark: on the wmt13b plot the scores higher than 400 are not visible (all are gold scores).
2208
0.000.25
0.500.75
1.00
0.00 0.25 0.50 0.75 1.00rank
score groupgoldpredict
(a) Gold scores as reference. Sentences are sorted by their
gold score; the X axis gives their corresponding rank; the
predicted score of a sentence is plotted on the same abscissa,
thus showing both the gold (in red) and predicted score (in
blue) of the sentence on the Y axis. The predicted scores
which appear on the same vertical line correspond to different
sentences which have the same (or very close) gold scores.
0.000.25
0.500.75
1.00
0.00 0.25 0.50 0.75 1.00rank
score groupabsErrgold
(b) Absolute error as reference. Sentences are sorted by
their absolute error; the X axis gives their corresponding
rank; the gold score of a given sentence is plotted on the same
abscissa, thus showing both the error (in red) and gold score
(in blue) of the sentence on the Y axis. The gold scores which
appear on the same vertical line correspond to different sen-
tences which have the same (or a very close) absolute error.
Figure 4: wmt13a, training set: sentences sorted by gold score (left) or absolute error (right).
The issue is constant among the datasets, but with a variable impact. To some extent, it could be
summarized in the following way: it appears that the system does not try to predict the actual quality
of the sentences, but instead applies a simple optimization strategy; since a large majority of sentences
belong to a relatively small range of values in the middle of the full possible range of scores, predicting
any score outside this range is taking a big risk. Consequently it is safer, in order to minimize the error
rate, to ignore (or barely take into account) the rare cases which belong to the tails. Hence the system
ends doing the opposite of what is usually expected from a quality estimation system: the most common
cases are rather accurately recognized, but the most striking anomalies are left undetected or poorly
labelled as such. This behaviour can be explained by the following reasons:
7
? The supervised learning optimization criterion is very often the minimization of the MAE,
8
as in our system. This leads the algorithm to favor the interval of scores where there are many
instances, since their weight is more important in the average.
? The datasets are unbalanced, which is certainly realistic in terms of application, but it also en-
courages the algorithm to assign scores in the interval which would be the ?default class? in a
classification problem; that is, without any clear indication in the features, it is strategically wiser
to bet on the most probable answer.
? The risk is lower with respect to MAE to assign a score in the middle of the range of possible
values rather than at the extremes. For instance if the range is [1, 5] the maximum absolute error at
3 is 2, whereas it is 4 at 1 or 5. However, at least for wmt13a, the data shows that, if this hypothesis
had a real impact, the predicted scores would be closer to 0.5 than to the mean 0.32.
4 Detecting and Evaluating the Tails Quality
4.1 Possible Measures
We propose below different measures intended to evaluate the impact of the tails prediction problem.
Since it can be defined as an increased level of error for sentences which are far from the mean, a simple
first measure is the correlation between the distance from the gold score to the mean and the absolute
error: this value reflects whether the errors are higher in the tails than close to the mean and to what
extent (in other words, it measures how strong the divergence observed on the right part of figure 4b is).
Table 2 shows how high Pearson?s correlation is in our data.
A simple way to measure the performance locally in the tails is to consider the task as a binary classi-
fication problem, as if we were only interested in recognizing whether a sentence belongs to a particular
7
The first two reasons are actually closely related, they only show different aspects of the same problem.
8
Especially in the WMT QE tasks, since this is the main evaluation measure for the scoring task.
2209
wmt12 wmt13a wmt13b
train test train test train test
all > 0 < 0 all > 0 < 0 all > 0 < 0 all > 0 < 0 all > 0 < 0 all > 0 < 0
0.58 0.54 0.64 0.62 0.64 0.65 0.82 0.84 0.78 0.76 0.86 0.76 0.80 0.89 -0.18 0.83 0.91 -0.01
Table 2: Correlation between the absolute error and the distance to the mean of the gold score.
?> 0? (resp. ?< 0?) is the correlation when taking only into account the scores above (resp. below) the
mean; this gives a more precise picture for the top/bottom quality scores. For example, in wmt13b the
top quality (lowest) scores are very well predicted, as opposed to the bottom quality (highest) scores.
subset of scores. For example, the frontier between the classes can be fixed between the 90% lowest
scores (negative) and the 10% highest (positive): it is then possible to observe the last 10% using the
standard evaluation measures: precision (proportion of true positive among the sentences labeled as
positive), recall (proportion of sentences labeled as positive among all positive sentences) and F1-score
(harmonic mean of the precision and recall).
9
The values of these measures are given for three thresh-
olds in table 3. As expected, the recall is extremely low in the tails; it is even 0 in most cases for the
5% threshold, which means that the system does not assign any score in the 5% top/bottom of the range
observed on the training data.
Data+tail 5% 10% 20%
limit P R F1 limit P R F1 limit P R F1
wmt12 B ? 2.0 ? 0.0 ? ? 2.3 0.33 0.01 0.02 ? 2.7 0.73 0.16 0.26
T ? 5.0 ? 0.0 ? ? 4.7 0.50 0.02 0.04 ? 4.2 0.65 0.18 0.28
wmt13a B ? 0.62 ? 0.0 ? ? 0.54 ? 0.0 ? ? 0.47 0.62 0.11 0.19
T ? 0.06 ? 0.0 ? ? 0.11 ? 0.0 ? ? 0.17 0.50 0.01 0.01
wmt13b B ? 272 ? 0.0 ? ? 186 0.76 0.26 0.39 ? 134 0.71 0.43 0.54
T ? 18.2 0.5 0.05 0.09 ? 24.8 0.18 0.06 0.10 ? 35.7 0.52 0.30 0.38
Table 3: Local classification measures (test set). ?T? (resp. ?B?) refers to the top (resp. bottom)
quality tail; P/R/F1 are the standard Precision/Recall/F1-score.
10
Example: 10% of the scores for the
wmt12 training data are higher than 4.7 (top quality tail); among the gold scores in the test set which are
higher than this 4.7 threshold, only 2% are predicted as higher than 4.7 (recall); and among the scores
predicted as higher than 4.7, exactly 50% are actually higher than 4.7 (precision).
Additionally, we have separately proposed a measure which aims to evaluate the ranking error locally
(Moreau and Vogel, 2013). The same idea can be applied to scoring errors: the Local MAE (LMAE)
can be computed on a particular range of scores. The difference with global MAE is that, for a given
sentence, the gold score or the predicted score can belong to the range while the other does not. This is
why there are two versions of this measure: gold-based LMAE and prediction-based LMAE, which, as
their names suggest, take into account only the gold scores (resp. predicted scores) which belong to the
range in the absolute difference | gold? predicted |, as defined in definition 4.1.
Definition 1 (Local MAE (LMAE)). Let S be a set of sentences, and D the interval of possible scores:
9
In the observations which follow we choose to set the limits (5%, etc.) based on the training set even though the test set
is observed. In other words, the absolute score corresponding to the percentage is calculated using the training set gold scores,
which might differ from the value calculated from the test set. The disadvantage is that the number of values in the test set in
the corresponding range does not necessarily correspond to the percentage, but this way the limits do not depend on the test set,
so that values obtained on different test sets would be comparable.
10
We consider theN% limits computed from the range of gold scores, and not from the range of predicted scores: this makes
more sense because otherwise the system is not evaluated against the actual scores in the tails, but since the range of predicted
scores is actually smaller than the range of gold scores, sometimes there are no predicted scores at all in this range of values
(especially for the lowest values of N , e.g. 5%). For example in the wmt12 dataset 5% of the gold scores are below 2, but the
system does not predict any value below 2. In such a case we consider that this is equivalent to a classifier which decides not to
label any instance in a given category. Since there are no instances labelled as positive at all, the precision is undefined, which
makes the F1-score undefined as well. The corresponding cells are marked as ??? in tables 3 and 6.
2210
for every sentence s ? S, predicted(s) ? D, gold(s) ? D. For any subinterval I ? D:
11
LMAE
gold
= mean
( {
?
?
gold(s)? predicted(s)
?
?
?
?
?
s such that gold(s) ? I
} )
LMAE
pred
= mean
( {
?
?
gold(s)? predicted(s)
?
?
?
?
?
s such that predicted(s) ? I
} )
To some extent, the gold-based LMAE (resp. prediction-based) is similar to a recall measure (resp.
precision) because it takes into account the true positive and the false negative (resp. the true positive and
the false positive) with respect to the range. This can be observed in table 4, which gives the values of
these two measures for three thresholds on the three datasets: LMAE
gold
is almost always much higher
than the global MAE, whereas there LMAE
pred
is often close to or lower than the global MAE. This is
because, compared to the gold scores, the top or bottom predicted scores are closer to the centre of the
range. Therefore the sentences taken into account include some actual ?tails sentences? (for which the
absolute error is high), but they can also contain many sentences which actually belong to the area (for
which the absolute error is low).
5% 10% 20%
Data+tail (Global) MAE LMAE
gold
LMAE
pred
LMAE
gold
LMAE
pred
LMAE
gold
LMAE
pred
wmt12 B
0.69
1.37 0.47 1.02 0.57 1.02 0.62
T 1.08 0.68 1.08 0.67 0.89 0.68
wmt13a B
0.15
0.35 0.18 0.28 0.18 0.19 0.17
T 0.28 0.12 0.28 0.13 0.24 0.13
wmt13b B
50.9
264 154 192 129 135 90.3
T 26.1 22.9 24.5 27.4 27.4 25.2
Table 4: Local MAE evaluation (test set). ?T? (resp. ?B?) refers to the top (resp. bottom) quality tail.
Example: for the wmt13b data, among the 10% actual top quality sentences (i.e. the 10% lowest gold
scores), the mean absolute error is 26.1. This is lower than the global MAE (50.9), as opposed to all
the other cases; this confirms that the top quality tail in wmt13b is particularly well predicted (this is
certainly a consequence of the strongly skewed distribution in this dataset).
4.2 The Post-edited Sentences Test
A good way to evaluate the discrepancies in the reliability of the quality scores in the tails is to apply
the QE system to a set of very good or very bad sentences. Thankfully the post-edited versions of the
sentences were provided with the WMT datasets; since by definition their quality is perfect, they make
a perfect case for such a test.
12
In theory, all these sentences should be assigned a score close to top
quality.
13
For every dataset we run the same QE system, i.e., we compute the features for the post-edited
sentences using Quest, then apply the model built with the regular training data to these features. We
tried with both the post-edited version of the training set and test set, when provided.
14
Our original goal was to observe how high the error rate was globally, but it turned out that the pre-
dicted scores follow a distribution which is very similar to the one followed by the MT output (the means
are very close as well, which implies that the MAE is very high). This led us to observe how the MT out-
put scores and the post-edited version scores are correlated. In most cases the two scores are very close,
as shown on figure 5. This is obviously a very serious issue, since it means that, in general, the system is
not able to distinguish between a sentence which needs correction and the same sentence after correction.
11
Remark: if I = D, LMAE
gold
= LMAE
pred
= MAE.
12
Independent assessment of the post-edited sentences is, of course, not guaranteed to yield the judgement that they would
not benefit from further editing, though.
13
That is, 5 for the wmt12 dataset and 0 for the wmt13a dataset (since HTER scores measure the distance against the post-
edited version, and here we compare the post-edited sentence against itself); the wmt13b dataset is based on post-edited time,
so there is no exact value corresponding to perfect sentences but the scores should very low.
14
The post-edited version was not available for the wmt13b test set.
2211
23
45
2 3 4 5MT.output.predictpos
ted.predict
(a) wmt12, train set;
cor.=0.977;
mean diff.=-0.03=-0.03?
34
5
2 3 4 5MT.output.predictpos
ted.predict
(b) wmt12, test set;
cor.=0.986;
mean diff.=-0.02=-0.02?
0.10.20.3
0.40.50.6
0.2 0.3 0.4 0.5 0.6MT.output.predictpos
ted.predict
(c) wmt13a, train set;
cor.=0.962;
mean diff.=0.004=0.02?
0.20.3
0.40.5
0.2 0.3 0.4 0.5MT.output.predictpos
ted.predict
(d) wmt13a, test set;
cor.=0.987;
mean diff.=0.002=0.01?
0100
200300
0 100 200 300MT.output.predictpos
ted.predict
(e) wmt13b, train set;
cor.=0.985;
mean diff.=3.14=0.04?
Figure 5: MT output predicted scores vs. post-edited predicted scores ?mean diff.? is the mean of the
difference between the post-edited score and the MT output score; it is also expressed as a multiple of ?,
where ? is the standard deviation of the MT output gold scores (specific to each particular dataset).
It is however able to see a slight difference at the document level: we have performed a paired Student?s
test for each dataset, which shows that the mean of the scores predicted for the post-edited sentences
is significantly lower in the wmt12 case and higher in the wmt13a and wmt13b cases (as expected by
the definition of scores) than the scores predicted for the MT output sentences. Nevertheless, the mean
difference is extremely low (see figure 5), never higher than 0.04 standard deviations.
Furthermore, there is no visible impact of the quality of the MT output, although one would expect the
correlation to be lower for low quality sentences: by definition, there are more differences between the
MT output and the post-edited version for these sentences, so it should be easier for the system to detect
the different level of quality between the two. In other words, it is quite understandable that the system
does not detect the difference for an MT output of relatively good quality, but the fact the post-edited
version of the really bad translations are also rated as really bad is a major issue. It must be remembered
that we are not refering to a flaw solely in our own system, but nearly across the board in the state of the
art systems.
These observations, which hold for every dataset, show that QE systems do not capture the actual
quality of the sentences: instead, it seems that what they measure is probably the difficulty of machine-
translating a sentence. Indeed, the set of Quest features that we use contains many features which depend
only on the source sentences. Moreover, this conclusion is consistent with the fact that Bic?ici et al. (2013)
obtain very good results on the WMT12 dataset using only the source sentences.
Explaining this observation with precision would require a more detailed analysis which is out of the
scope of this paper. Nevertheless, it is fairly clear that the features which are used fail to capture the
subtlety and/or the diversity of the difference between a faulty sentence and its corrected version; this
might be because a single sentence does not offer enough clues for the system to make such a fine-grained
distinction, in which case it would be necessary to rethink the definition of the QE problem.
In other work, we examine linguistic quality of items in relation to reference corpora (Moreau and
Vogel, 2013; ?). By comparison to the supervised learning studied here, such work is weakly supervised
since there is no use of absolute scores. This yields a version of the QE problem that may be deemed too
relativistic, but does represent an alternative approach. Unfortunately, because of the very difference in
the use of absolute scores, they cannot be directly compared on this. Thus, we focus here on empirical
exploration of the nature of the problem in estimating quality in the case of supervised learning.
5 Experiments
In this section we devise several experiments intended to explore different aspects of the problem in more
detail. In particular, we try to evaluate the impact of the possible causes described in ?3: first we show
in ?5.1 that it affects most QE systems, especially those optimized to minimize MAE. Then in ?5.2 and
?5.3 we confirm that the distribution of the training set is a major cause of the issue by showing that
alternative distributions have different effects.
2212
5.1 Tails Prediction for WMT12 Participating Systems
In order to test if the tails prediction problem is general to most supervised QE systems, we apply the local
performance measures to the scores predicted on the test set by the participating systems in WMT12.
15
Table 5 shows some detailed results for the four best systems at WMT12. It confirms that the predictions
made for the tails are generally significantly worse than they are globally, and especially that the systems
tend to predict very few values at the ends of the range of values: recall in the 10% bottom or top scores
is never higher than 12%.
16
It is also worth noticing that the first system, which performs significantly
better than the others, is the only one which was not optimized to minimize the MAE but to maximize
the DeltaAvg score (Soricut et al., 2012). In particular, this system obtains a recall higher than the others
in most cases (especially in the 5% and 10% tails), which is certainly due to the fact that it assigns
more scores far from the mean (in other words, this system takes more risk). This tends to confirm our
hypothesis that the minimization of the MAE as learning criterion is one of the causes of the problem.
Correlation Bottom Top
System ID Global dist.mean. 5% 10% 20% 5% 10% 20%
MAE vs abs.err. R G-LMAE R G-LMAE R G-LMAE R G-LMAE R G-LMAE R G-LMAE
SDLLW M5PbestDeltaAvg 0.61 0.49 0.05 1.02 0.07 0.76 0.32 0.76 0.02 0.96 0.12 0.99 0.26 0.84
UU best 0.64 0.53 0.0 1.21 0.07 0.91 0.26 0.91 0.0 1.02 0.04 1.01 0.22 0.81
SDLLW SVM 0.64 0.55 0.0 1.33 0.0 0.98 0.17 0.98 0.02 0.89 0.06 0.91 0.32 0.75
UU bltk 0.64 0.58 0.0 1.22 0.06 0.91 0.27 0.91 0.0 1.07 0.02 1.05 0.27 0.83
Table 5: Tails prediction quality for the 4 best systems at WMT12 (test set). The second column
contains the correlation between the distance to the mean and the absolute error; the columns R and
G-LMAE contain respectively the recall and the gold-based local MAE scores (see ?4.1).
5.2 Adding the Post-edited Sentences to the Training Set
In this experiment we use the post-edited sentences again (see ?4.2), but this time adding them to the
training set in order to observe the impact on the test set.
17
These instances are progressively added to
the official training set (in random order). We focus on the top quality tail, since it is the one which is
expected to benefit from adding sentences with top scores to the training set. Figure 6 shows how the
local MAE scores improve as post-edited instances are added. Only the gold-based LMAE scores are
represented, because these provide a recall-like information and the observations show that recall (in the
tails) is the main weakness of QE systems (see ?4.1).
As expected, in all cases adding top quality sentences to the training set makes the system decrease
the error rate in the top quality tail. Of course this local improvement comes at the price of degrading
the global performance, although for the wmt13a dataset (fig. 6b) the global error even improves until
almost half of the sentences have been added. In the case of the wmt13b dataset (fig. 6c), since the QE
system was already very good in predicting the top quality sentences (the LMAE is even better than the
global MAE), the improvement is smaller and proportionally more costly for the global performance.
5.3 Balancing the Training Set
In this final experiment, we resample the training set (with replacement), in order to balance the gold
scores over the full range of values. Since we can only use the discrete gold scores provided with the
original training set, we compute a (random) uniform distribution but select the closest available score
(randomly picking an instance among those with this score). The resulting distribution is not uniform,
and the training set contains many duplicate instances; therefore, the resulting training set is unlikely to
yield very good results in general, but it is no longer subject to the ?statistical attraction? towards the
mean that we have observed.
15
These values were kindly provided by the organizers of the WMT12 QE Shared Task.
16
This is true for all but 3 participating systems, and these exceptions correspond to systems which performed worse globally.
17
We assign perfect scores to all these sentences: 5 for wmt12, 0 for wmt13a; for wmt13b, we use the mean of the time spent
for the sentences in the training set which were left unmodified: there are 23 such sentences, and the mean is 16.19s.
2213
l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l
0.4
0.8
1.2
0% 25% 50% 75% 100%sentences
LMAE proportionl 0.050.10.2global
(a) wmt12.
l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l
0.1
0.2
0% 25% 50% 75% 100%sentences
LMAE proportionl 0.050.10.2global
(b) wmt13a.
l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l
2040
6080
0% 25% 50% 75% 100%sentences
LMAE proportionl 0.050.10.2global
(c) wmt13b.
Figure 6: Improvement of gold-based LMAE as post-edited sentences are added to the train set.
Example: in wmt12, the gold-based LMAE for the top 20% sentences is higher than 0.8 when the system
is trained only on the official train set (0% of the post-edited sentences added), but reaches 0.4 when
about half of the post-edited sentences are added to the training set. However the global MAE (which
takes all the sentences into account) increases from 0.7 (0%) to 0.9 (50% of the post-edited sentences
added): since the system assigns more scores in the top tail, it makes larger errors globally. Remark: the
MAE and LMAE values are measured on the same set of sentences for every percentage on the X axis.
The model obtained from the balanced training set has been applied to the original test set. Table 6
gives the local results observed in the tails: in most cases, the recall increases drastically compared to
using the regular training set, or is at least identical,
18
causing a great increase in the F1-scores as well.
The LMAE scores do not show such an improvement, in fact the mean error is often higher than with the
regular training set. This is due to the fact that the system is forced to assign scores far from the ?easy
cases? around the mean, therefore makes much bigger mistakes than in the previous case. As expected,
the global MAE scores for wmt13a and wmt13b are much higher than the original MAE values (0.27 and
110.7 respectively, i.e. about twice the original values). Interestingly, the MAE stays almost constant
(0.71 instead of 0.69) for wmt12. The correlation between the distance to the mean and the mean absolute
decreases to 0.42, 0.05 and 0.26 for wmt12, wmt13a and wmt13b, respectively.
Classification measures Local MAE measures
Data+tail 5% 10% 20% 5% 10% 20%
limit P R F1 limit P R F1 limit P R F1 gold pred. gold pred. gold pred.
wmt12 B ? 2.0 0.31 0.18 0.23 ? 2.3 0.49 0.20 0.28 ? 2.7 0.66 0.46 0.54 0.94 0.82 0.73 0.75 0.73 0.67
T ? 5.0 ? 0.0 ? ? 4.7 0.50 0.02 0.04 ? 4.2 0.69 0.19 0.29 1.28 0.72 1.26 0.65 1.07 0.71
wmt13a B ? 0.62 0.15 0.70 0.24 ? 0.54 0.17 0.75 0.28 ? 0.47 0.21 0.83 0.33 0.14 0.60 0.13 0.55 0.14 0.49
T ? 0.06 ? 0.0 ? ? 0.11 ? 0.0 ? ? 0.17 0.67 0.02 0.04 0.58 0.11 0.57 0.14 0.44 0.15
wmt13b B ? 272 0.15 0.55 0.23 ? 186 0.27 0.78 0.41 ? 134 0.38 0.91 0.54 156 243 118 235 101 199
T ? 18.2 0.20 0.20 0.20 ? 24.8 0.30 0.19 0.24 ? 35.7 0.55 0.26 0.35 128 61 114 45 110 41
Table 6: Local evaluation of the test set using a balanced training set. Cells in bold show an im-
provement over the corresponding value with the original training set, as given in tables 3 and 4. The
classification limits were computed on the original training set.
6 Conclusion and Future Work
To conclude, we have shown that there are very serious issues with the way supervised QE systems
are built: they tend to be unable to reliably evaluate both the worst and the best quality sentences.
Furthermore, they cannot distinguish between a faulty MT output sentence and its post-edited version.
We have also shown that it is possible to improve the detection of the best/worst sentences by altering
the distribution of the training set; however the question whether this can be achieved while maintaining
a decent level of global performance remains open. But even if the cost in global performance is high,
18
The only exception is the 20% top quality recall of the wmt13b dataset. This is certainly due to the very particular
distribution of scores in this dataset, and to the fact that the top quality tail was already predicted reliably in the regular version.
2214
the techniques that we have tested could be useful in some specific applications of QE (for example, if
the recall in the tails is more important than the precision).
We think that these observations raise questions about the definition of the QE problem. It might
actually be necessary to define different kinds of QE tasks: depending on the targeted application (e.g.
estimating post-editing time, retraining the MT model, discarding the worst sentences, etc.), there could
be a specific setting which is more appropriate in terms of supervised/unsupervised learning, evaluation
measure, precision/recall trade-off, etc. For instance, minimizing the MAE does not seem compatible
with detecting anomalies, but might be relevant for estimating the cost of post-editing. Similarly, under
the hypothesis that the sentence level is not sufficiently rich in information in order to obtain accurate
predictions, an intermediate level of granularity might be considered (e.g. at paragraph level).
Finally, another great challenge with respect to the reliability of QE systems is their consistency when
applied to different test sets, or more generally their dependency on the training set: in the perspective of
applications, it is very important to know what level of confidence can be expected when applying a QE
system or model to a new document.
Acknowledgements
We are grateful to Lucia Specia, Radu Soricut and Christian Buck, the organizers of the WMT 2012 and
2013 Shared Task on Quality Estimation, for releasing all the data related to the competition, including
post-edited sentences, features sets, etc.
This research is supported by Science Foundation Ireland (Grant 12/CE/I2267) as part of the Centre
for Next Generation Localisation (www.cngl.ie) funding at Trinity College, University of Dublin.
The graphics in this paper were created with R (R Core Team, 2012), using the ggplot2 library
(Wickham, 2009).
References
Ergun Bic?ici, Declan Groves, and Josef Genabith. 2013. Predicting sentence translation quality using extrinsic
and language independent features. Machine Translation, 27(3-4):171?192.
Ondrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine
Translation. In Eighth Workshop on Statistical Machine Translation, WMT-2013, pages 1?44, Sofia, Bulgaria.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings
of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Association for Computational Linguistics.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I.H. Witten. 2009. The weka data mining
software: an update. ACM SIGKDD Explorations Newsletter, 11(1):10?18.
Erwan Moreau and Carl Vogel. 2013. Weakly supervised approaches for quality estimation. Machine Translation,
27(3):pp 257?280, September.
R Core Team, 2012. R: A Language and Environment for Statistical Computing. R Foundation for Statistical
Computing, Vienna, Austria. ISBN 3-900051-07-0.
Kashif Shah, Eleftherios Avramidis, Ergun Bic?ici, and Lucia Specia. 2013. Quest - design, implementation and
extensions of a framework for machine translation quality estimation. Prague Bull. Math. Linguistics, 100:19?
30.
S.K. Shevade, SS Keerthi, C. Bhattacharyya, and K.R.K. Murthy. 2000. Improvements to the SMO algorithm for
SVM regression. Neural Networks, IEEE Transactions on, 11(5):1188?1193.
A.J. Smola and B. Sch?olkopf. 2004. A tutorial on support vector regression. Statistics and computing, 14(3):199?
222.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of transla-
tion edit rate with targeted human annotation. In In Proceedings of Association for Machine Translation in the
Americas, pages 223?231.
2215
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012. The SDL Language Weaver systems in the WMT12
Quality Estimation shared task. In Proceedings of the Seventh Workshop on Statistical Machine Translation,
pages 145?151, Montr?eal, Canada, June. Association for Computational Linguistics.
Hadley Wickham. 2009. ggplot2: elegant graphics for data analysis. Springer New York.
2216
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 126?131,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Exploiting CCG Structures with Tree Kernels for Speculation Detection
Liliana Mamani Sa?nchez, Baoli Li, Carl Vogel
Computational Linguistics Group
Trinity College Dublin
Dublin 2, Ireland
{mamanisl,baoli.li,vogel}@tcd.ie
Abstract
Our CoNLL-2010 speculative sentence
detector disambiguates putative keywords
based on the following considerations: a
speculative keyword may be composed of
one or more word tokens; a speculative
sentence may have one or more specula-
tive keywords; and if a sentence contains
at least one real speculative keyword, it is
deemed speculative. A tree kernel classi-
fier is used to assess whether a potential
speculative keyword conveys speculation.
We exploit information implicit in tree
structures. For prediction efficiency, only
a segment of the whole tree around a spec-
ulation keyword is considered, along with
morphological features inside the segment
and information about the containing doc-
ument. A maximum entropy classifier
is used for sentences not covered by the
tree kernel classifier. Experiments on the
Wikipedia data set show that our system
achieves 0.55 F-measure (in-domain).
1 Introduction
Speculation and its impact on argumentation has
been studied by linguists and logicians since at
least as far back as Aristotle (trans 1991, 1407a,
1407b), and under the category of linguistic
?hedges? since Lakoff (1973). Practical appli-
cation of this research has emerged due to the
efforts to create a biomedical database of sen-
tences tagged with speculation information: Bio-
Scope (Szarvas et al, 2008) and because of the
association of some kinds of Wikipedia data with
the speculation phenomenon (Ganter and Strube,
2009). It is clear that specific words can be con-
sidered as clues that can qualify a sentence as
speculative. However, the presence of a specu-
lative keyword not always conveys a speculation
assertion which makes the speculation detection a
tough problem. For instance, the sentences below
contain the speculative keyword ?may?, but only
the sentence (a) is speculative.
(a) These effects may be reversible.
(b) Members of an alliance may not attack each other.
The CoNLL-2010 Shared Task (Farkas et al,
2010), ?Learning to detect hedges and their scope
in natural language text? proposed two tasks re-
lated to speculation research. Task 1 aims to detect
sentences containing uncertainty and Task 2 aims
to resolve the intra-sentential scope of hedge cues.
We engaged in the first task in the biomedical and
Wikipedia domains as proposed by the organizers,
but eventually we got to submit only Wikipedia
domain results. However, in this paper we include
results in the biomedical domain as well.
The BioScope corpus is a linguistically hand an-
notated corpus of negation and speculation phe-
nomena for medical free texts, biomedical article
abstracts and full biomedical articles. The afore-
said phenomena have been annotated at sentence
level with keyword tags and linguistic scope tags.
Some previous research on speculation detection
and boundary determination over biomedical data
has been done by Medlock & Briscoe (2007) and
O?zgu?r & Radev (2009) from a computational view
using machine learning methods.
The Wikipedia speculation dataset was gener-
ated by exploiting a weasel word marking. As
weasel words convey vagueness and ambiguity by
providing an unsupported opinion, they are dis-
couraged by Wikipedia editors. Ganter & Strube
(2009) proposed a system to detect hedges based
on frequency measures and shallow information,
achieving a F-score of 0.691.
We formulate the speculation detection prob-
lem as a word disambiguation problem and de-
veloped a system as a pipelined set of natural
1They used different Wikipedia data.
126
language processing tools and procedures to pre-
process the datasets. A Combinatory Categorial
Grammar parsing (CCG) (Steedman, 2000) tool
and a Tree Kernel (TK) classifier constitute the
core of the system.
The Section 2 of this paper describes the over-
all architecture of our system. Section 3 depicts
the dataset pre-processing. Section 4 shows how
we built the speculation detection module, outlines
the procedure of examples generation and the use
of the Tree-kernel classifier. Section 5 presents
the experiments and results, we show that sentence
CCG derivation information helps to differentiate
between apparent and real speculative words for
speculation detection. Finally Section 6 gives our
conclusions.
2 Speculation detection system
Our system for speculation detection is a machine
learning (ML) based system (Figure 1). In the pre-
processing module a dataset of speculative/non-
speculative sentences goes through a process of
information extraction of three kinds: specula-
tive word or keyword extraction,2 sentence extrac-
tion and document feature extraction (i.e docu-
ment section). Later the extracted keywords are
used to tag potential speculative sentences in the
training/evaluation datasets and used as features
by the classifiers. The sentences are submitted to
the tokenization and parsing modules in order to
provide a richer set of features necessary for creat-
ing the training/evaluation datasets, including the
document features as well.
In the ML module two types of dataset are built:
one used by a TK classifier and other one by a bag-
of-features based maximum entropy classifier. As
the first one processes only those sentences that
contain speculative words, we use the second clas-
sifier, which is able to process samples of all the
sentences.
The models built by these classifiers are com-
bined in order to provide a better performance and
coverage for the speculation problem in the clas-
sification module which finally outputs sentences
labeled as speculative or non-speculative. Used
tools are the GeniaTagger (Tsuruoka et al, 2005)
for tokenization and lemmatization, and the C&C
Parser (Clark and Curran, 2004). The next sec-
tions explain in detail the main system compo-
nents.
2Extraction of keywords for the training stage.
3 Dataset pre-processing for rich feature
extraction
The pre-processing module extracts keywords,
sentences and document information.
All sentences are processed by the tok-
enizer/lemmatizer and at the same time specific in-
formation about the keywords is extracted.
Speculative keywords
Speculative sentences are evidenced by the pres-
ence of speculation keywords. We have the fol-
lowing observations:
? A hedge cue or speculative keyword 3 may be
composed of one or more word tokens.
? In terms of major linguistic categories, the
word tokens are heterogeneous: they may be
verbs, adjectives, nouns, determiners, etc. A
stop-word removing strategy was dismissed,
since no linguistic category can be elimi-
nated.
? A keyword may be covered by another longer
one. For instance, the keyword most can be
seen in keywords like most of all the heroes
or the most common.
Considering these characteristics for each sen-
tence, in the training stage, the keyword extraction
module retrieves the speculative/non-speculative
property of each sentence, the keyword occur-
rences, number of keywords in a sentence, the ini-
tial word token position and the number of word
tokens in the keyword. We build a keyword lex-
icon with all the extracted keywords and their
frequency in the training dataset, this speculative
keyword lexicon is used to tag keyword occur-
rences in non-speculative training sentences and
in all the evaluation dataset sentences.
The overlapping problem when tagging key-
words is solved by maximal matching strategy. It
is curious that speculation phrases come in de-
grees of specificity; the approach adopted here
favors ?specific? multi-word phrases over single-
word expressions.
Sentence processing
Often, speculation keywords convey certain in-
formation that can not be successfully expressed
by morphology or syntactic relations provided by
phrase structure grammar parsers. On the other
3Or just ?keyword? for sake of simplicity.
127
Figure 1: Block diagram for the speculation detection system.
hand, CCG derivations or dependencies provide
deeper information, in form of predicate-argument
relations. Previous works on semantic role label-
ing (Gildea and Hockenmaier, 2003; Boxwell et
al., 2009) have used features derived from CCG
parsings and obtained better results.
C&C parser provides CCG predicate-argument
dependencies and Briscoe and Carroll (2006) style
grammatical relations. We parsed the tokenized
sentences to obtain CCG derivations which are
binary trees as shown in the Figure 2. The
CCG derivation trees contain function category
and part-of-speech labels; this information is con-
tained in the tree structures to be used in building
a subtree dataset for the TK classifier.
4 Speculative sentence classifier
4.1 Tree Kernel classification
The subtree dataset is processed by a Tree Kernel
classifier (Moschitti, 2006) based on Support Vec-
tor Machines. TK uses a kernel function between
two trees, allowing a comparison between their
substructures, which can be subtrees (ST) or sub-
set trees (SST). We chose the comparison between
subset trees since it expands the kernel calculation
to those substructures with constituents that are
not in the leaves. Our intuition is that real specula-
tive sentences have deep semantic structures that
are particularly different from those ones in ap-
parent speculative sentences, and consequently the
comparison between the structures of well identi-
fied and potential speculative sentences may en-
hance the identification of real speculative key-
words.
4.2 Extracting tree structures
The depth of a CCG derivation tree is propor-
tional to the number of word tokens in the sen-
tence. Therefore, the processing of a whole deriva-
tion tree by the classifier is highly demanding and
many subtrees are not relevant for the classifica-
tion of speculative/non-speculative sentences, in
particular when the scope of the speculation is a
small proportion of a sentence.
In order to tackle this problem, a fragment of
the CCG derivation tree is extracted. This frag-
ment or subtree spans the keyword together with
neighbors terms in a fixed-size window of n word
tokens, (i.e. n word tokens to the left and n word
tokens to the right of the keyword) and has as root
the lower upper bound node of the first and last
tokens of this span. After applying the subtree ex-
traction, the subtree can contain more word tokens
in addition to those contained in the n-span, which
are replaced by a common symbol.
Potential speculative sentences are turned into
training examples. However, as described in Sec-
tion 3, a speculative sentence can contain one or
more speculative keywords. This can produce an
overlapping between their respective n-spans of
individual keywords during the subtree extraction,
producing subtrees with identical roots for both
keywords. For instance, in the following sen-
tence(c), the spans for the keywords suggests and
thought will overlap if n = 3.
(c) This suggests that diverse agents thought to ac-
tivate NF-kappa B ...
The overlapping interacts with the windows size
and potential extraction of dependency relations
128
It was reported to have burned for a day
PRP VBD VBN TO VB VBN IN DT NN
NP (S[dcl]\NP)/(S[pss]\NP) (S[pss]\NP)/(S[to]\NP) (S[to]\NP)/(S[b]\NP) (S[b]\NP)/(S[pt]\NP) S[pt]\NP ((S\NP)\(S\NP))/NP NP[nb]/N N
NP[nb]
(S[X]\NP)\(S[X]\NP)
S[pt]\NP
S[b]\NP
S[to]\NP
S[pss]\NP
S[dcl]\NP
S[dcl]
Figure 2: CCG derivations tree for It was reported to have burned for a day.
shared by terms belonging to the two different
spans. We deal with this issue by extracting one
training example if two spans have a common root
and two different examples otherwise.
4.3 Bag of features model
By default, our system classifies the sentences not
covered by the TK model using a baseline clas-
sifier that labels a sentence as speculative if this
has at least one keyword. Alternatively, a bag of
features classifier is used to complement the tree
kernel, aimed to provide a more precise method
that might detect even speculative sentences with
new keywords in the evaluation dataset. The set of
features used to build this model includes:
a) Word unigrams;
b) Lemma unigrams;
c) Word+POS unigrams;
d) Lemma+POS unigrams;
e) Word+Supertag unigrams;
f) Lemma+Supertag unigrams;
g) POS+Supertag unigrams;
h) Lemma bigrams;
i) POS bigrams;
j) Supertag bigrams;
k) Lemma+POS bigrams;
l) Lemma+Supertag bigrams;
m) POS+Supertag bigrams;
n) Lemma trigrams;
o) POS trigrams;
p) Supertag trigrams;
q) Lemma+POS trigrams;
r) Lemma+Supertag trigrams;
s) POS+Supertag trigrams;
t) Number of tokens;
u) Type of section in the document (Title, Text,
Section);
v) Name of section in the document;
w) Position of the sentence in a section starting
from beginning;
Dataset Dev. Train. Eval.
Biomedical 39 14541 5003
Wikipedia 124 11111 9634
Table 1: Datasets sizes.
x) Position of the sentence in a section starting
from end.
Position of the sentence information, composed by
the last four features, represents the information
about the sentence relative to a whole document.
The bag of features model is generated using a
Maximum Entropy algorithm (Zhang, 2004).
5 Experiments and results
5.1 Datasets
In the CoNLL-2010 Task 1, biomedical and
Wikipedia datasets were provided for develop-
ment, training and evaluation in the BioScope
XML format. Development and training datasets
are tagged with cue labels and a certainty feature.4
The number of sentences for each dataset 5 is de-
tailed in Table 1.
After manual revision of sentences not parsed
by C&C parser, we found that they contain equa-
tions, numbering elements (e.g. (i), (ii).. 1),
2) ), or long n-grams of named-entities, for in-
stance: ...mannose-capped lipoarabinomannan (
ManLAM ) of Mycobacterium tuberculosis ( M.
tuberculosis )... that out of a biomedical domain
appear to be ungrammatical. Similarly, in the
Wikipedia datasets, some sentences have many
named entities. This suggests the need of a spe-
cific pre-processor or a parser for this kind of sen-
tences like a named entity tagger.
In Table 2, we present the number of parsed sen-
tences, processed sentences by the TK model and
examples obtained in the tree structure extraction.
4certainty=?uncertain? and certainty=?certain?.
5The biomedical abstracts and biomedical articles training
datasets are processed as a single dataset.
129
Dataset Parsed Process. Samples
Biomedical train. 14442 10852 23511
Biomedical eval. 4903 3395 7826
Wikipedia train. 10972 7793 13461
Wikipedia eval. 9559 4666 8467
Table 2: Count of processed sentences.
5.2 Experimental results
The CoNLL-2010 organizers proposed in-domain
and cross-domain evaluations. In cross-domain
experiments, test datasets of one domain can be
used with classifiers trained on the other or on the
union of both domains. We report here our results
for the Wikipedia and biomedical datasets.
So far, we mentioned two settings for our clas-
sifier: a TK classifier complemented by a baseline
classifier (BL) and TK classifier complemented
by a bag of features classifier (TK+BF). Table
3 shows the scores of our submitted system (in-
domain Task 1) on the Wikipedia dataset, whereas
Table 4 gives the scores of the baseline system.
TP FP FN Precision Recall F
Our system 1033 480 1201 0.6828 0.4624 0.5514
Max. 1154 448 1080 0.7204 0.5166 0.6017
Min. 147 9 2087 0.9423 0.0658 0.123
Table 3: Comparative scores for our system with
CoNLL official maximum and minimum scores in
Task 1, Wikipedia dataset in-domain.
TP FP FN Precision Recall F
Biomedical 786 2690 4 0.2261 0.9949 0.3685
Wikipedia 1980 2747 254 0.4189 0.8863 0.5689
Table 4: Baseline results.
Additionally, we consider a bag of features clas-
sifier (BF) and a classifier that combines the base-
line applied to the sentences that have at least one
keyword plus the BF classifier for the remaining
sentences (BL+BF). In Tables 5 to 10, results for
the four classifiers (TK, TK+BF, BF, BL+BF) with
evaluations in-domain and cross-domain are pre-
sented6.The baseline scores confirm that relying on just
the keywords is not enough to identify speculative
sentences. In the biomedical domain, the classi-
fiers give high recall but too low precision result-
ing in low F-scores. Still, the TK, TK+BF and BF
(in-domain configurations) gives much better re-
sults than BL and BL+BF which indicates that the
information from CCG improves the performance
6It is worth to note that the keyword lexicons have been
not used in cross-domain way, so the TK and TK+BF models
have not been tested in regards to keywords.
TP FP FN Precision Recall F
BL 1980 2747 254 0.4189 0.8863 0.5689
TK 1033 480 1201 0.6828 0.4624 0.5514
TK+BF 1059 516 1175 0.6729 0.4740 0.5560
BF 772 264 1462 0.7452 0.3456 0.4722
BL+BF 2028 2810 206 0.4192 0.9078 0.5735
Table 5: Results for Wikipedia dataset in-domain.
TP FP FN Precision Recall F
BL 1980 2747 254 0.4189 0.8863 0.5689
TK 1776 2192 458 0.4476 0.7950 0.5727
TK+BF 1763 2194 471 0.4455 0.7892 0.5695
BF 403 323 1831 0.5551 0.1804 0.2723
BL+BF 1988 2772 246 0.4176 0.8899 0.5685
Table 6: Wikipedia data classified with biomedical
model scores (cross-domain).
TP FP FN Precision Recall F
BL 1980 2747 254 0.4189 0.8863 0.5689
TK 1081 624 1153 0.6340 0.4839 0.5489
TK+BF 1099 636 1135 0.6334 0.4919 0.5538
BF 770 271 1464 0.7397 0.3447 0.4702
BL+BF 2017 2786 217 0.4199 0.9029 0.5733
Table 7: Wikipedia data classified with biomedical
+ Wikipedia model scores (cross-domain).
TP FP FN Precision Recall F
BL 786 2690 4 0.2261 0.9949 0.3685
TK 759 777 31 0.4941 0.9606 0.6526
TK+BF 751 724 39 0.5092 0.9506 0.6631
BF 542 101 248 0.8429 0.6861 0.7565
BL+BF 786 2695 4 0.2258 0.9949 0.3681
Table 8: Biomedical data scores (in-domain).
TP FP FN Precision Recall F
BL 786 2690 4 0.2261 0.9949 0.3685
TK 786 2690 4 0.2261 0.9949 0.3685
TK+BF 771 2667 19 0.2243 0.9759 0.3647
BF 174 199 616 0.4665 0.2206 0.2992
BL+BF 787 2723 3 0.2242 0.9962 0.3660
Table 9: Biomedical data classified with
Wikipedia model scores (cross-domain).
TP FP FN Precision Recall F
BL 786 2690 4 0.2261 0.9949 0.3685
TK 697 357 93 0.6613 0.8823 0.7560
TK+BF 685 305 105 0.6919 0.8671 0.7697
BF 494 136 296 0.7841 0.6253 0.6958
BL+BF 786 2696 4 0.2257 0.9949 0.3679
Table 10: Biomedical data classified with biomed-
ical + Wikipedia model scores (cross-domain).
of the classifiers when compared to the baseline
classifier.
Even though in the Wikipedia domain the
TK+BF score is less than the baseline score, still
the performance of the classifiers do not fall much
in any of the in-domain and cross-domain exper-
iments. On the other hand, BF does not have a
good performance in 5 of 6 the experiments. To
make a more precise comparison between TK and
BF, the TK and BL+BF scores show that BL+BF
performs better than TK in only 2 of the 6 ex-
periments but the better performances achieved
by BL+BF are very small. This suggests that
130
the complex processing made by tree kernels is
more useful when disambiguating speculative key-
words than BF. Nonetheless, the bag-of-features
approach is also of importance for the task at hand
when combined with TK. We observe that the TK
classifer and BF classifier perform well making us
believe that the CCG derivations provide relevant
information for speculation detection. The use of
tree kernels needs further investigations in order to
evaluate the suitability of this approach.
6 Concluding remarks
Speculation detection is found to be a tough task
given the high ambiguity of speculative keywords.
We think these results can be improved by study-
ing the influences of context on speculation asser-
tions.
This paper presents a new approach for disam-
biguating apparent speculative keywords by us-
ing CCG information in the form of supertags and
CCG derivations. We introduce the use of the tree
kernel approach for CCG derivations trees. The
inclusion of other features like grammatical rela-
tions provided by the parser needs to be studied
before incorporating this information into the cur-
rent classifier and possibly to resolve the boundary
speculation detection problem.
Acknowledgments
This research is supported by the Trinity College
Research Scholarship Program and the Science
Foundation Ireland (Grant 07/CE/I1142) as part
of the Centre for Next Generation Localisation
(www.cngl.ie) at Trinity College of Dublin.
References
Aristotle. trans. 1991. The Art of Rhetoric. Penguin
Classics, London. Translated with an Introduction
and Notes by H.C. Lawson-Tancred.
Stephen Boxwell, Dennis Mehay, and Chris Brew.
2009. Brutus: A semantic role labeling system in-
corporating CCG, CFG, and dependency features.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 37?45, Suntec, Singapore.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on
the PARC DepBank. In Proceedings of the COL-
ING/ACL on Main conference poster sessions, pages
41?48, Morristown, NJ, USA.
Stephen Clark and James R. Curran. 2004. Parsing
the WSJ using CCG and log-linear models. In ACL
?04: Proceedings of the 42nd Annual Meeting on As-
sociation for Computational Linguistics, page 103,
Morristown, NJ, USA.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
Wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 173?176, Suntec, Singapore.
Daniel Gildea and Julia Hockenmaier. 2003. Identi-
fying semantic roles using combinatory categorial
grammar. In Proceedings of 2003 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Sapporo, Japan.
George Lakoff. 1973. Hedges: A study in meaning
criteria and the logic of fuzzy concepts. Journal of
Philosophical Logic, 2(4):458?508.
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 992?999, Prague, Czech Republic.
AlessandroMoschitti. 2006. Making tree kernels prac-
tical for natural language learning. In Proceedings
of the 11th Conference of the European Chapter of
the Association for Computational Linguistics.
Arzucan O?zgu?r and Dragomir R. Radev. 2009. Detect-
ing speculations and their scopes in scientific text.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1398?1407, Singapore.
Mark Steedman. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas, and
Ja?nos Csirik. 2008. The BioScope corpus: anno-
tation for negation, uncertainty and their scope in
biomedical texts. In Proceedings of the Workshop
on Current Trends in Biomedical Natural Language
Processing, pages 38?45, Columbus, Ohio.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Advances in
Informatics, pages 382?392.
Le Zhang. 2004. Maximum entropy modeling toolkit
for Python and C++ (version 20041229). In Natural
Language Processing Lab, Northeastern.
131
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 257?262,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
A Naive Bayes classifier for automatic correction of preposition
and determiner errors in ESL text
Gerard Lynch, Erwan Moreau and Carl Vogel
Centre for Next Generation Localisation
Integrated Language Technology Group
School of Computer Science and Statistics
Trinity College Dublin, Ireland
gplynch,moreaue,vogel@scss.tcd.ie
Abstract
This is the report for the CNGL ILT team en-
try to the HOO 2012 shared task. A Naive-
Bayes-based classifier was used in the task
which involved error detection and correction
in ESL exam scripts. The features we use in-
clude n-grams of words and POS tags together
with features based on the external Google N-
Grams corpus. Our system placed 11th out
of 14 teams for the detection and recognition
tasks and 11th out of 13 teams for the correc-
tion task based on F-score for both preposition
and determiner errors.
1 Introduction
The HOO 2012 shared task seeks to apply compu-
tational methods to the correction of certain types
of errors in non-native English texts. The previous
year?s task, (Dale and Kilgarriff, 2011), focused on
a larger scale of errors and a corpus of academic ar-
ticles. This year?s task focuses on six error types in a
corpus of non-native speaker text. The scope of the
errors is as follows:1
Error Code Description Example
RT Replace Preposition When I arrived at London
MT Missing preposition I gave it John
UT Unnecessary preposition I told to John that
RD Replace determiner Have the nice day
MD Missing determiner I have car
UD Unnecessary determiner There was a lot of the traffic
Table 1: Error types for HOO 2012 Shared Task
In Section 2, we give a brief summary of the data
for the shared task and in Section 3 we explain the
1http://correcttext.org/hoo2012/
errortypes.html last verified, May 10, 2012
individual steps in the system. Section 4 details the
different configurations for each of the runs submit-
ted and finally, Section 5 presents the results.
2 Training data
The training data for this shared task has been pro-
vided by Cambridge University Press and consists of
scripts from students sitting the Cambridge ESOL
First Certificate in English (FCE) exams. The top-
ics of the texts are comparable as they have been
drawn from two consecutive exam years. The data is
provided in XML format and contains 1000 original
exam scripts, together with a standoff file containing
edits of the type described in Section 1 above, also
in XML format. These edits consist of offset infor-
mation, edit type information and before and after
text for correction. The results for the shared task
were presented in this format.
The test data consists of 100 exam scripts drawn
from a new corpus of exam scripts.
Some extra metadata is present in the source files,
including information about the student?s mother
tongue and the age-range of the student, however the
mother tongue data is not present in the test set.
3 Approach
The approach we have chosen for this task involves
the use of supervised machine-learning algorithms
in a four-part classification task.
3.1 Overview of the system
The first part of the task involves identification of
edits in the training data, perhaps the most challeng-
257
ing given the large imbalance of edits vs non-edits
in the data.
The next step concerns classification of edits into
the six types described above, and the final task
involves correction of edits, replacing or adding
prepositions and determiners, and possibly in some
cases removal of same.
There is a fourth step involved which reassesses
the classification and correction based on some sim-
ple heuristics, using POS tags of the head word of
each instance. If the headword is not a preposition
and the system has marked a replace preposition er-
ror at that position, this error will be removed from
the system. Likewise when the headword is not a
determiner and a replace determiner error has been
marked. If the replacement suggested is the same
as the original text (in some cases this occurs), the
edit is also removed. Another case for removal in
this fashion includes an error type involving a miss-
ing determiner error where the head word is neither
a noun or an adjective. In some cases the system
reported and corrected an error suggesting the same
text as was originally there, i.e no change. These
cases are also removed from the end result.
3.2 Classification
We utilise the freely available Weka machine learn-
ing toolkit (Hall et al, 2009), and the algorithm used
for classification in each step is Naive Bayes.
3.2.1 Representing the data
We represent each word in the training data as a
vector of features. There are 39 basic features used
in the detection process, and 42 in the classification
and training step. The first 7 features contain in-
formation which is not used for classification but is
used to create the edit structures, such as start offset,
end offset, native language, age group and source
filename and part information. These features in-
clude the current word plus the four preceding and
following words, POS and spell-checked versions of
each, together with bigrams of the two following and
two preceding words with spell-checked and POS
versions for these. Information on speaker age and
native language is also included although native lan-
guage information is not present in the test set.
3.2.2 Additional processing
All tokens have been lower-cased and punctuation
has been removed. POS information for each token
has been added. The open-source POS tagger from
the OpenNLP tools package (OpenNLP, 2012) has
been used to this end. Spell correction facility has
been provided using the basic spellchecker in the
Lucene information retrieval API(Gospodnetic and
Hatcher, 2005) and the top match string as provided
by this spell correcting software is used in addition
to each feature. The basic maximum entropy model
for English is used for the POS tagger.
We had also planned to include features based
on the Google Books n-gram corpus, (Michel et al,
2011) which is freely available on the web, but un-
fortunately did not get to include them in the ver-
sion submitted due to errors which were found in the
scripts for generating the features late in the process.
Nevertheless, we describe these features in Section
3.3 and present some cross-validation results from
the training data for the detection step in Section 5.1.
3.3 Google N-grams Features
3.3.1 Motivation
The Google Books N-Grams2 is a collection of
datasets which consist of all the sequences of words
(n-grams) extracted from millions of books (Michel
et al, 2011). The ?English Million? dataset contains
more more than 500 millions distinct n-grams3, from
size 1 to 5. for every n-gram, its frequency, page
frequency (number of pages containing it) and book
frequency (number of books containing it) are pro-
vided.
In this Shared Task, we aim to use the Google N-
grams as a reference corpus to help detecting the
errors in the input. The intuition is the following:
if an error occurs, comparing the frequency of the
input n-grams against the frequency of other possi-
bilities in the Google N-grams data might provide
useful indication on the location/type of the error.
For example, given the input ?I had to go in a li-
brary?, The Google N-grams contain only 36,716
occurrences of the trigram ?go in a?, but 244,098
occurrences of ?go to a?, which indicates that the
latter is more likely.
2http://books.google.com/ngrams/datasets
3The least frequent n-grams were discarded.
258
However there are several difficulties in using
such a dataset:
? Technical limitations. Extracting information
from the dataset can take a lot of time because
of the size of the data, thus the range of ap-
proaches is restricted by efficiency constraints.
? Quality of the data. The Google N-grams were
extracted automatically using OCR, which
means that the dataset can contain errors or un-
expected data (for example, the English dataset
contains a significant number of non-English
words).
This is why the Google N-grams must be used
cautiously, and only as an indication among others.
3.3.2 Method
Our goal is to add features extracted from the
Google N-grams dataset to the features described
above, and feed the supervised classification process
with these. Before computing the features, a list L
of ?target expressions? is extracted from the train-
ing data, which contains all the words or sequences
of words (determiners and prepositions) which oc-
cur in a correction. Then, given an input sentence
A1 . . . Am and a position n in this sentence, two
types of information are extracted from the Google
data:
? Specific indications of whether an error exists
at this position:
1. No change: the frequency of the input se-
quence An?1An and An?1AnAn+1 ;
2. Unnecessary word(s): the frequency of the
sequence An?1An+1 if A ? L;
3. Missing word(s): the frequency of the se-
quence XAn (resp. An?1XAn for tri-
grams) for any target expression X ? L;
4. Replacement: if A ? L, the frequency of
XAn+1 (resp. An?1XAn+1 for trigrams)
for any target expression X ? L;
? Generic indications taking the context into ac-
count: for length N from 1 to 5 in a window
An?4 . . . An+4, 16 combinations are computed
based only on the fact the n-grams appear in the
Google data; for example, one of these combi-
nations is the normalized sum for the 4 5-grams
in this window of 0 or 1 (the n-gram occurs or
does not).
Additionally, several variants are considered:
? bigrams or trigrams for ?specific? features;
? binary values for ?specific? features: 1 if the
n-gram appears, 0 otherwise;
? keep only the ?generic? features and the first
three features.
4 Run configurations
Ten runs were submitted to the organisers based on
different configurations. Modification of the data
was carried out using both instance reduction and
feature selection techniques. The system facilitated
the use of different training data for each of the three
main classification steps.
4.1 Least frequent words filter
Before classification, the data is preprocessed by re-
placing all the least frequent words with a default
value (actually treated as missing values by the clas-
sifier). This is intended to help the classifier focus
on the most relevant indications and to prevent over-
specification of the classification model.
4.2 Instance reduction filters
4.2.1 POSTrigrams filter
The POS trigrams filter works as follows: during
the training stage, the sequences of POS tags for the
words current-1.current.current+1 are extracted for
each instance, together with its corresponding class.
Every POS trigram is then associated with the fol-
lowing ratio:
Frequency of true instances
Frequency of false instances
Then, when predicting the class, the filter is applied
before running the classifier: the sequences of tri-
grams are extracted for each instance, and are com-
pared against the corresponding ratio observed dur-
ing the training stage; the instance is filtered out if
the ratio is lower than some threshold N%. In Table
259
Run Detection Classification Correction
0 R1 Normal Normal
1 R20 Normal Normal
2 Full F12 Normal
3 R10 Normal Normal
4 R30 Normal Normal
5 F12 F12 Normal
6 R4new Normal Normal
7 R4 + F12 F12 Normal
8 R4 Normal Normal
9 R2 Normal Normal
Table 2: Run configurations
2, the label RN refers to the percentage (N) used as
cut-off in the experiments.
This filter is intended to reduce the impact of the
fact that the classes are strongly unbalanced. It per-
mits discarding a high number of false instances,
while removing only a small number of true in-
stances. However, as a side effect, it can cause the
classifier to miss some clues which were in the dis-
carded instances.
4.2.2 CurrentPlusOrMinusOne filter
The current plusorminus one filter works as fol-
lows: A list of all current.current+1 word bigrams
is made from the error instances in the training data,
along with all current-1.current bigrams. The non-
error instances in the training data are then filtered
based on whether an instance contains an occur-
rence of any current.current+1 or current-1.current
bigram in the list.
4.3 Feature selection filters
4.3.1 F12
During preliminary experiments, selecting a sub-
set of 12 features produced classification accuracy
gains in the detection and classification steps of the
process using ten-fold cross validation on the train-
ing set. These twelve features were: current, cur-
rent+1.current+2, current-1.current-2, currentSC,
currentPOS, current-1, current-2, current+1, cur-
rent+2, current+1SC, and current-1SC. The SC
postfix refers to the spell-corrected token, with POS
referring to the part-of-speech tag. The F12 config-
uration filter removes all other features except these.
5 Results
Table 3 displays the results for both preposition and
determiner errors which were obtained by the sys-
tem on the preliminary test set before teams sub-
mitted their revisions. Table 4 refers to the results
obtained by the system after the revised errors were
removed/edited.
Task Rank Run Precision Recall F-Score
Detection 11 9 5.33 25.61 8.82
Recognition 11 9 4.18 20.09 6.92
Correction 11 9 2.66 12.8 4.41
Table 3: Overall results on original data: TC
Task Rank Run Precision Recall F-Score
Detection 11 8 6.56 26.0 10.48
Recognition 11 8 4.91 19.45 7.84
Correction 11 8 3.09 12.26 4.94
Table 4: Overall results on revised data: TC
5.1 Some detailed results (detection)
The results reported here were obtained on the train-
ing data only, using 5-fold cross-validation, and only
for the detection task. We have studied various set-
tings for the parameters; figure 1 shows a global
overview of the performance depending on several
parameters (we show only a few different values in
order to keep the graph readable).
The results show that the Google features con-
tribute positively to the performance, but only
slightly: the F1 score is 0.6% better on average. This
overview also hides the fact that some combinations
of values work better together; for instance, contrary
to the fact that not filtering the POS trigrams per-
Run3 Recall Precision F
Detection 9.05 7.42 8.15
Correction 4.19 3.44 3.78
Recognition 9.05 7.42 8.15
Run8 Recall Precision F
Detection 22.51 5.44 8.76
Correction 11.25 2.72 4.38
Recognition 22.51 5.44 8.76
Run9 Recall Precision F
Detection 25.61 5.33 8.82
Correction 12.80 2.66 4.41
Recognition 20.09 4.18 6.92
Table 5: Top results on original test data
260
Figure 1: Average F-score depending on several parameters.
10
11
12
13
14
15
16
mea
n of 
f1
20
50
100
500
1000
POS?trigrams.0POS?trigrams.1
POS?trigrams.10
POS?trigrams.3 2?3?binary2?binary3 i r
none
window0
window2
window4
factor(minFreq) filter googleFeatures attributes
forms better on average, the best performances are
obtained when filtering, as shown in figure 2.
Figure 2: F-score (%) w.r.t POS trigrams filter threshold.
Parameters: window 2, Google features with bigrams and
trigrams.
0 2 4 6 8 10
0
5
10
15
20
filter threshold
f1 sc
ore
min. frequency 20min. frequency 50min. frequency 100min. frequency 500min. frequency 1000
? Minimum frequency4 (preprocessing, see 4.1).
4Remark: the values used as ?minimum frequencies? re-
ported in this paper can seem unusually high. This is due to
the fact that, for technical reasons, the thresholds were applied
globally to the data after it had been formatted as individual in-
stances, each instance containing a context window of 9 words.
As a consequence a threshold of N means that a given word
must occur at least N/9 times in the original input data.
As shown in Figure 2, using a high threshold
helps the classifier build a better model.
? POS trigrams filter (see 4.2.1.) Even if not fil-
tering at all performs better on average, the best
cases are obtained with a low threshold. Addi-
tionally, this parameter can be used to balance
between recall and precision (when one wants
to favor one or the other).
? Size of the context window. Results can show
important differences depending on the size
of the window, but no best configuration was
found in general for this parameter.
? Google features (see 3.3.2.) The Google fea-
tures help slightly in general, and are used in
the best cases that we have obtained. How-
ever there is no significantly better approach
between using the original frequencies, simpli-
fying these to binary values, or even not using
the list of target expressions.
6 Conclusions
The task of automated error correction is a difficult
one, with the best-performing systems managing ap-
prox. 40 % F-score for the detection, recognition
and correction (Dale et al, 2012). There are several
areas where our system?s performance might be im-
proved. The spellcheck dictionary which was used
261
was a general one and this resulted in many spelling
corrections which were out of context. A more tai-
lored dictionary employing contextual awareness in-
formation could be beneficial for the preprocessing
step.
Multi-word corrections were not supported by the
system due to how the instances were constructed
and these cases were simply ignored, to the detri-
ment of the results.
In the basic feature set, the majority of features
were based on word unigrams, however more n-
gram features could improve results as these were
found to perform well during classification.
There were many different ways to exploit the
Google N-Grams features and it may be the case
that better combinations of features can be found for
each of the classification steps.
Finally, very little time was spent tuning the
datasets for the classification and correction step as
opposed to the detection phase, this is another part of
the system where fine-tuning parameters could im-
prove performance.
Acknowledgments
This material is based upon works supported by
the Science Foundation Ireland under Grant No.[SFI
07/CE/I 1142.].
References
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 pilot shared task. In Proceed-
ings of the 13th European Workshop on Natural Lan-
guage Generation, Dublin, Ireland.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A Report on the Preposition and
Determiner Error Correction Shared Task. In Pro-
ceedings of the Seventh Workshop on Innovative Use
of NLP for Building Educational Applications, Mon-
treal, Canada.
O. Gospodnetic and E. Hatcher. 2005. Lucene. Man-
ning.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The WEKA data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
J.B. Michel, Y.K. Shen, A.P. Aiden, A. Veres, M.K.
Gray, J.P. Pickett, D. Hoiberg, D. Clancy, P. Norvig,
J. Orwant, et al 2011. Quantitative analysis of
culture using millions of digitized books. Science,
331(6014):176.
OpenNLP. 2012. Website: http://opennlp. apache. org.
262
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 120?126,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Quality Estimation:
an experimental study using unsupervised similarity measures
Erwan Moreau
CNGL and Computational Linguistics Group
Centre for Computing and Language Studies
School of Computer Science and Statistics
Trinity College Dublin
Dublin 2, Ireland
moreaue@cs.tcd.ie
Carl Vogel
Computational Linguistics Group
Centre for Computing and Language Studies
School of Computer Science and Statistics
Trinity College Dublin
Dublin 2, Ireland
vogel@cs.tcd.ie
Abstract
We present the approach we took for our par-
ticipation to the WMT12 Quality Estimation
Shared Task: our main goal is to achieve rea-
sonably good results without appeal to super-
vised learning. We have used various simi-
larity measures and also an external resource
(Google N -grams). Details of results clarify
the interest of such an approach.
1 Introduction
Quality Estimation (or Confidence Estimation)
refers here to the task of evaluating the quality of
the output produced by a Machine Translation (MT)
system. More precisely it consists in evaluating the
quality of every individual sentence, in order (for in-
stance) to decide whether a given sentence can be
published as it is, should be post-edited, or is so bad
that it should be manually re-translated.
To our knowledge, most approaches so far (Spe-
cia et al, 2009; Soricut and Echihabi, 2010; He et
al., 2010; Specia et al, 2011) use several features
combined together using supervised learning in or-
der to predict quality scores. These features be-
long to two categories: black box features which
can be extracted given only the input sentence and
its translated version, and glass box features which
rely on various intermediate steps of the internal MT
engine (thus require access to this internal data).
For the features they studied, Specia et al (2009)
have shown that black box features are informative
enough and glass box features do not significantly
contribute to the accuracy of the predicted scores.
In this study, we use only black box features, and
further, eschew supervised learning except in the
broadest sense. Our method requires some refer-
ence data, all taken to be equally good exemplars
of a positive reference category, against which the
experimental sentences are compared automatically.
This is the extent of broader-sense supervision. The
method does not require a training set of items each
annotated by human experts with quality scores (ex-
cept for the purpose of evaluation of course).
Successful unsupervised learning averts risks of
the alternative: supervised learning necessarily
makes the predicting system dependent on the an-
notated training data, i.e. less generic, and requires
a costly human evalution stage to obtain a reliable
model. Of course, our approach is likely not to per-
form as well as supervised approaches: here the goal
is to find a rather generic robust way to measure
quality, not to achieve the best accuracy. Neverthe-
less, in the context of this Quality Evaluation Shared
task (see (Callison-Burch et al, 2012) for a detailed
description) we have also used supervised learning
as a final stage, in order to submit results which can
be compared to other methods (see ?4).
We investigate the use of various similarity mea-
sures for evaluating the quality of machine translated
sentences. These measures compare the sentence
to be evaluated against a reference text, providing
a similarity score result. The reference data is sup-
posed to represent standard (well-formed) language,
so that the score is expected to reflect how complex
(source side) or how fluent (target side) the given
sentence is.
After presenting the similarity measures in sec-
120
tion 2, we will show in section 3 how they perform
individually on the ranking task; finally we will ex-
plain in section 4 how the results that we submitted
were obtained using supervised learning.
2 Approach
Our method consists in trying to find the best mea-
sure(s) to estimate the quality of machine translated
sentences, i.e. the ones which show the highest cor-
relation with the human annotators scores. The mea-
sures we have tested work always as follows.
Given a sentence to evaluate (source or target),
a score is computed by comparing the sentence
against a reference dataset (usually a big set of sen-
tences). This dataset is assumed to represent stan-
dard and/or well-formed language.1 This score rep-
resents either the quality (similarity measure) or the
faultiness (distance measure) of the sentence. It is
not necessarily normalized, and in general cannot be
interpreted straightforwardly (for example like the 1
to 5 scale used for this Shared Task, in which every
value 1, 2, 3, 4, 5 has a precise meaning). In the con-
text of the Shared task, this means that we focus on
the ?ranking? evaluation measures provided rather
than the ?scoring? measures. These scores are rather
intended to compare sentences relatively to one an-
other: for instance, they can be used to discard the
N% lowest quality sentences from post-editing.
The main interest in such an approach is in
avoiding dependence on costly-to-annotate training
data?correspondingly costly to obtain and which
risk over-tuning the predicting system to the articu-
lated features of the training items. Our method still
depends on the dataset used as reference, but this
kind of dependency is much less constraining, be-
cause the reference dataset can be any text data. To
obtain the best possible results, the reference data
has to be representative enough of what the eval-
uated sentences should be (if they were of perfect
quality), which implies that:
? a high coverage (common words or n-grams) is
preferable; this also means that the size of this
dataset is important;
1We use this definition of ?reference? in this article. Please
notice that this differs from the sense ?human translation of a
source sentence?, which is more common in the MT literature.
? the quality (grammaticality, language register,
etc.) must be very good: errors in the reference
data will infect the predicted scores.
It is rather easy to use different reference datasets
with our approach (as opposed to obtain new human
scores and training a new model on this data), since
nowadays numerous textual resources are available
(at least for the most common languages).
2.1 Similarity measures
All the measures we have used compare (in different
ways) the n-grams of the tested sentence against the
reference data (represented as a big bag of n-grams).
There is a variety of parameters for each measure;
here are the parameters which are common to all:
Length of n-grams: from unigrams to 6-grams;
Punctuation: with or without punctuation marks;
Case sensitivity: binary;
Sentence boundaries: binary signal of whether
special tokens should be added to mark the start
and the end of sentences.2 This permits:
? that there is the same number of n-grams
containing a token w, for every w in the
sentence;
? to match n-grams starting/ending a
sentence only against n-grams which
start/end a sentence.
Most configurations of parameters presented in this
paper are empirical (i.e. only the parameter set-
tings which performed better during our tests were
retained). Below are the main measures explored.3
2.1.1 Okapi BM25 similarity (TF-IDF)
Term Frequency-Inverse Document Frequency
(TF-IDF) is a widely used similarity measure in
Information Retrieval(IR). It has also been shown
to perform significantly better than only term fre-
quency in tasks like matching coreferent named
entities (see e.g. Cohen et al (2003)), which is
2With trigrams, ?Hello World !? (1 trigram) becomes
?# # Hello World ! # #? (5 trigrams).
3One of the measures is not addressed in this paper for IP
reasons (this measure obtained good results but was not best).
121
technically not very different from comparing sen-
tences. The general idea is to compare two docu-
ments4 using their bags of n-grams representations,
but weighting the frequency of every n-gram with
the IDF weight, which represents ?how meaning-
ful? the n-gram is over all documents based on its
inverse frequency (because the n-grams which are
very common are not very meaningful in general).
There are several variants of TF-IDF compari-
son measures. The most recent ?Okapi BM25? ver-
sion was shown to perform better in general than the
original (more basic) definition (Jones et al, 2000).
Moreover, there are different ways to actually com-
bine the vectors together (e.g. L1 or L2 distance). In
these experiments we have only used the Cosine dis-
tance, with Okapi BM25 weights. The weights are
computed as usual (using the number of sentences
containing X for any n-gram X), but are based only
on the reference data.
2.1.2 Multi-level matching
For a given length N, ?simple matching? is de-
fined as follows: for every N -gram in the sentence,
the score is incremented if this N -gram appears at
least once in the reference data. The score is then
relativized to the sentence N -gram length.
?Multi-level matching? (MLM) is similar but with
different lengths of n-grams. For (maximum) length
N , the algorithm is as follows (for every n-gram):
if the n-gram appears in the reference data the score
is incremented; otherwise, for all n-grams of length
N ? 1 in this n-gram, apply recursively the same
method, but apply a penalty factor p (p < 1) to
the result.5 This is intended to overcome the bi-
nary behaviour of the ?simple matching?. This way
short sentences can always be assigned a score, and
more importantly the score is smoothed according
to the similarity of shorter n-grams (which is the be-
haviour one wants to obtain intuitively).
4In this case every sentence is compared against the refer-
ence data; from an IR viewpoint, one can see the reference data
as the request and each sentence as one of the possible docu-
ments.
5This method is equivalent to computing the ?simple match-
ing? for different lengths N of N -grams, and then combine the
scores sN in the following way: if sN < sN?1, then add
p ? (sN?1 ? sN ) to the score, and so on. However this ?ex-
ternal? combination of scores can not take into account some of
the extensions (e.g. weights).
Two main variants have been tested. The first one
consists in using skip-grams.6 Different sizes and
configurations were tested (combining skip-grams
and standard sequential n-grams), but none gave
better results than using only sequential n-grams.
The second variant consists in assigning a more fine-
grained value, based on different parameters, instead
of always assigning 1 to the score when n-gram oc-
curs in the reference data. An optimal solution is not
obvious, so we tried different strategies, as follows.
Firstly, using the global frequency of the ngram
in the reference data: intuitively, this could be in-
terpreted as ?the more an n-gram appears (in the
reference data), the more likely it is well-formed?.
However there are obviously n-grams which appear
a lot more than others (especially for short n-grams).
This is why we also tried using the logarithm of the
frequency, in order to smooth discrepancies.
Secondly, using the inverse frequency: this is
the opposite idea, thinking that the common n-
grams are easy to translate, whereas the rare n-
grams are harder. Consequently, the critical parts
of the sentence are the rare n-grams: assigning them
more weight focuses on these. This works in both
cases (if the n-gram is actually translated correctly
or not), because the weight assigned to the n-gram
is taken into account in the normalization factor.
Finally, using the Inverse Document Frequency
(IDF): this is a similar idea as the previous one, ex-
cept that instead of considering the global frequency
the number of sentences containing the n-gram is
taken into account. In most cases (and in all cases
for long n-grams), this is very similar to the previ-
ous option because the cases where an n-gram (at
least with n > 1) appears several times in the same
sentence are not common.
2.2 Resources used as reference data
The reference data against which the sentences
are compared is crucial to the success of our ap-
proach. As the simplest option, we have used the
Europarl data on which the MT model was trained
(source/target side for source/target sentences). Sep-
arately we tested a very different kind of data,
namely the Google Books N -grams (Michel et al,
6The true-false-true skip-grams in ?There is
no such thing?: There no, is such and no thing.
122
2011): it is no obstacle that the reference sentences
themselves are unavailable, since our measures only
need the set of n-grams and possibly their frequency
(Google Books N -gram data contains both).
3 Individual measures only
In this section we study how our similarity measures
and the baseline features (when used individually)
perform on the ranking task. This evaluation can
only be done by means of DeltaAvg and Spearman
correlation, since the values assigned to sentences
are not comparable to quality scores. We have tested
numerous combinations of parameters, but show be-
low only the best ones (for every case).
3.1 General observations
Method Ref. data DeltaAvg Spearman
MLM,1-4 Google, eng 0.26 0.22
Baseline feature 1 0.29 0.29
Baseline feature 2 0.29 0.29
MLM,1-3,lf Google, spa 0.32 0.28
Okapi,3,b EP, spa 0.33 0.27
Baseline feature 8 0.33 0.32
Okapi,2,b EP, eng 0.34 0.30
Baseline feature 12 0.34 0.32
Baseline feature 5 0.39 0.39
MLM,1-5,b EP, spa 0.39 0.39
MLM,1-5,b EP, eng 0.39 0.40
Baseline feature 4 0.40 0.40
Table 1: Best results by method and by resource on train-
ing data. b = sentence boundaries ; lf = log frequency
(Google) ; EP = Europarl.
Table 1 shows the best results that every method
achieved on the whole training data with different
resources, as well as the results of the best base-
line features.7 Firstly, one can observe that the lan-
guage model probability (baseline features 4 and 5)
performs as good or slightly better than our best
measure. Then the best measure is the one which
combines different lengths of n-grams (multi-level
matching, combining unigrams to 5-grams), fol-
lowed by baseline feature 12 (percentage of bigrams
7 Baseline 1,2: length of the source/target sentence;
Baseline features 4,5: LM probability of source/target sentence;
Baseline feature 8: average number of translations per source
word with threshold 0.01, weighted by inverse frequency;
Baseline feature 12: percentage of bigrams in quartile 4 of fre-
quency of source words in a corpus of the source language.
in quartile 4 of frequency), and then Okapi BM25
applied to bigrams. It is worth noticing that compar-
ing either the source sentence or the target sentence
(against the source/target training data) gives very
similar results. However, using Google Ngrams as
reference data shows a significantly lower correla-
tion. Also using skip-grams or any of our ?fined-
grained? scoring techniques (see ?2.1.2) did not im-
prove the correlation, even if in most cases these
were as good as the standard version.
3.2 Detailed analysis: how measures differ
Even when methods yield strongly correlated re-
sults, differences can be significant. For example,
the correlation between the rankings obtained with
the two best methods (baseline 4 and MLM Eng.) is
0.53. The methods do not make the same errors.8 A
method may tend to make a lot of small errors, or on
the contrary, very few but big errors.
0 20 40 60 80 100
0
20
40
60
80
% sentences within error range
relat
ive r
ank 
erro
r (%)
Baseline feature 4MLM EP SpaMLM Google EngBaseline ranking
Figure 1: Percentage of best segments within an error
range. For every measure, the X axis represents the sen-
tences sorted by the difference between the predicted rank
and the actual rank (?rank error?), in such a way that for
any (relative) number of sentences x, the y value repre-
sents the maximum (relative) rank error for all prior sen-
tences: for instance, 80% of the ranks predicted by these
three measures are at most 40% from the actual rank.
Let R and R? be the actual and predicted ranks9
of sentence, respectively. Compute the difference
8This motivates use of supervised learning (but see ?1).
9It is worth noticing that ties are taken into account here: two
123
D = |R?R?|; then relativize to the total number of
sentences (the upper bound for D): D? = D/N .
D? is the relative rank error. On ascending sort
by D?, the predicted ranks for the first sentences
are closest to their actual rank. Taking the relative
rank error D?j for the sentence at position Mj , one
knows that all ?lower? sentences (?Mi, Mi ? Mj)
are more accurately assigned (D?i ? D
?
j). Thus, if
the position is also relativized to the total number
sentences: M ?k = Mk/N , M
?
k is the proportion of
sentences for which the predicted rank is at worst
D?k% from the real rank. Figure 1 shows the percent-
age of sentences withing a rank error range for three
good methods:10 the error distributions are surpris-
ingly similar. A baseline ranking is also represented,
which shows the same if all sentences are assigned
the same rank (i.e. all sentences are considered of
equal quality)11.
We have also studied effects of some parameters:
? Taking punctuation into account helps a little;
? Ignoring case gives slightly better results;
? Sentences boundaries significantly improve the
performance;
? Most of the refinements of the local score (fre-
quency, IDF, etc.) do not perform better than
the basic binary approach.
4 Individual measures as features
In this section we explain how we obtained the sub-
mitted results using supervised learning.
4.1 Approach
We have tested a wide range of regression algo-
rithms in order to predict the scores, using the
Weka12 toolkit (Hall et al, 2009). All tests were
sentences which are assigned the same score are given the same
rank. The ranking sum is preserved by assigning the average
rank; for instance if s1 > s2 = s3 > s4 the corresponding
ranks are 1, 2.5, 2.5, 4).
10Some are not shown, because the curves were too close.
11Remark: the plateaus are due to the ties in the actual ranks:
there is one plateau for each score level. This is not visible on
the predicted rankings because it is less likely that an impor-
tant number of sentences have both the same actual rank and
the same predicted rank (whereas they all have the same ?pre-
dicted? rank in the baseline ranking, by definition).
12www.cs.waikato.ac.nz/ml/weka ? l.v., 04/2012.
done using the whole training data in a 10 folds
cross-validation setting. The main methods were:
? Linear regression
? Pace regression (Wang and Witten, 2002)
? SVM for regression (Shevade et al, 2000)
(SMOreg in Weka)
? Decision Trees for regression (Quinlan, 1992)
(M5P in Weka)
We have tested several combinations of features
among the features provided as baseline and our
measures. The measures were primarily selected
on their individual performance (worst measures
were discarded). However we also had to take the
time constraint into account, because some measures
require a fair amount of computing power and/or
memory and some were not finished early enough.
Finally we have also tested several attributes selec-
tion methods before applying the learning method,
but they did not achieve a better performance.
4.2 Results
Table 2 shows the best results among the config-
urations we have tested (expressed using the offi-
cial evaluation measures, see (Callison-Burch et al,
2012) for details). These results were obtained using
the default Weka parameters.In this table, the differ-
ent features sets are abbreviated as follows:
? B: Baseline (17 features);
? M1: All measures scores (45 features);
? M2: Only scores obtained using the provided
resources (33 features);
? L: Lengths (of source and target sentence, 2
features).
For every method, the best results were obtained
using all possible features (baseline and our mea-
sures). The following results can also be observed:
? our measures increase the performance over
use of baseline features only (B+M1 vs. B);
? using an external resource (here Google n-
grams) with some of our measures increases the
performance (B+M1 vs. B+M2);
124
Features Method DeltaAvg Spearman MAE RMSE
B SVM 0.398 0.445 0.616 0.761
B Pace Reg. 0.399 0.458 0.615 0.757
L + M1 SVM 0.401 0.439 0.615 0.764
L + M1 Lin. Reg 0.408 0.441 0.610 0.757
B Lin. Reg. 0.408 0.461 0.614 0.754
L + M1 M5P 0.409 0.441 0.610 0.757
B + M2 SVM 0.409 0.447 0.605 0.753
B + M2 Pace Reg. 0.417 0.466 0.603 0.744
B + M2 M5P 0.419 0.472 0.601 0.746
L + M1 Pace Reg. 0.426 0.454 0.603 0.751
B + M2 Lin. Reg. 0.428 0.481 0.598 0.740
B M5P 0.434 0.487 0.586 0.729
B + M1 SVM 0.444 0.489 0.585 0.734
B + M1 Pace Reg. 0.453 0.505 0.584 0.724
B + M1 Lin. Reg. 0.456 0.507 0.583 0.724
B + M1 M5P 0.457 0.508 0.583 0.724
Table 2: Best results on 10-folds cross-validation on the
training data (sorted by DeltaAvg score).
? the baseline features contribute positively to the
performance (B+M1 vs. L+M1);
? The M5P (Decision trees) method works best
in almost all cases (3 out of 4).
Based on these training results, the two systems
that we used to submit the test data scores were:
? TCD-M5P-resources-only, where scores were
predicted from a model trained using M5P on
the whole training data, taking only the base-
line features (B) into account;
? TCD-M5P-all, where scores were predicted
from a model trained using M5P on the whole
training data, using all features (B+M1).
The TCD-M5P-resources-only submission
ranked 5th (among 17) in the ranking task, and
5th among 19 (tied with two other systems) in
the scoring task (Callison-Burch et al, 2012).
Unfortunately the TCD-M5P-all submission con-
tained an error.13 Below are the official results
for TCD-M5P-resources-only and the corrected
results for TCD-M5P-all :
13In four cases in which Google n-grams formed the refer-
ence data, the scores were computed using the wrong language
(Spanish instead of English) as the reference. Since this error
occured only for the test data (not the training data used to com-
pute the model), it made the predictions totally meaningless.
Submission DeltaAvg Spearman MAE RMSE
resources-only 0.56 0.58 0.68 0.82
all 0.54 0.54 0.70 0.84
Contrary to previous observations using the train-
ing data, these results show a better performance
without our measures. We think that this is mainly
due to the high variability of the results depending
on the data, and that the first experiments are more
significant because cross-validation was used.
5 Conclusion
In conclusion, we have shown that the robust ap-
proach that we have presented can achieve good re-
sults: the best DeltaAvg score reaches 0.40 on the
training data, when the best supervised approach is
at 0.45. We think that this robust approach com-
plements the more fine-grained approach with su-
pervised learning: the former is useful in the cases
where the cost to use the latter is prohibitive.
Additionally, it is interesting to see that using ex-
ternal data (here the Google N -grams) improves the
performance (when using supervised learning). As
future work, we plan to investigate this question
more precisely: when does the external data help?
What are the differences between using the training
data (used to produce the MT engine) and another
dataset? How to select such an external data in order
to maximize the performance? In our unsupervised
framework, is it possible to combine the score ob-
tained with the external data with the score obtained
from the training data? Similarly, can we combine
scores obtained by comparing the source side and
the target side?
Acknowledgments
This research is supported by Science Foundation
Ireland (Grant 07/CE/I1142) as part of the Centre for
Next Generation Localisation (www.cngl.ie) fund-
ing at Trinity College, University of Dublin.
We thank the organizers who have accepted to apply
a bug-fix (wrong numbering of the sentences) in the
official results, and for organizing the Shared task.
References
[Callison-Burch et al2012] Chris Callison-Burch,
Philipp Koehn, Christof Monz, Matt Post, Radu
125
Soricut, and Lucia Specia. 2012. Findings of the
2012 workshop on statistical machine translation.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics.
[Cohen et al2003] W.W. Cohen, P. Ravikumar, and S.E.
Fienberg. 2003. A comparison of string distance met-
rics for name-matching tasks. In Proceedings of the
IJCAI-2003 Workshop on Information Integration on
the Web (IIWeb-03), pages 73?78.
[Hall et al2009] M. Hall, E. Frank, G. Holmes,
B. Pfahringer, P. Reutemann, and I.H. Witten.
2009. The weka data mining software: an update.
ACM SIGKDD Explorations Newsletter, 11(1):10?18.
[He et al2010] Y. He, Y. Ma, J. van Genabith, and
A. Way. 2010. Bridging smt and tm with translation
recommendation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 622?630. Association for Computational
Linguistics.
[Jones et al2000] Karen Sparck Jones, Steve Walker, and
Stephen E. Robertson. 2000. A probabilistic model
of information retrieval: development and comparative
experiments - parts 1 and 2. Inf. Process. Manage.,
36(6):779?840.
[Michel et al2011] J.B. Michel, Y.K. Shen, A.P. Aiden,
A. Veres, M.K. Gray, J.P. Pickett, D. Hoiberg,
D. Clancy, P. Norvig, J. Orwant, et al 2011. Quan-
titative analysis of culture using millions of digitized
books. science, 331(6014):176.
[Quinlan1992] J.R. Quinlan. 1992. Learning with con-
tinuous classes. In Proceedings of the 5th Australian
joint Conference on Artificial Intelligence, pages 343?
348. Singapore.
[Shevade et al2000] S.K. Shevade, SS Keerthi, C. Bhat-
tacharyya, and K.R.K. Murthy. 2000. Improvements
to the smo algorithm for svm regression. Neural Net-
works, IEEE Transactions on, 11(5):1188?1193.
[Soricut and Echihabi2010] R. Soricut and A. Echihabi.
2010. Trustrank: Inducing trust in automatic trans-
lations via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 612?621. Association for Computational
Linguistics.
[Specia et al2009] Lucia Specia, Marco Turchi, Nicola
Cancedda, Marc Dymetman, and Nello Cristianini.
2009. Estimating the sentence-level quality of ma-
chine translation systems. In Proceedings of the 13th
Conference of the European Association for Machine
Translation, pages 28?35.
[Specia et al2011] L. Specia, N. Hajlaoui, C. Hallett, and
W. Aziz. 2011. Predicting machine translation ade-
quacy. In Machine Translation Summit XIII, Xiamen,
China.
[Wang and Witten2002] Y. Wang and I.H. Witten. 2002.
Modeling for optimal probability prediction. In Pro-
ceedings of the Nineteenth International Conference
on Machine Learning, pages 650?657. Morgan Kauf-
mann Publishers Inc.
126
Proceedings of the SIGDIAL 2013 Conference, pages 304?308,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Laughter and Topic Transition in Multiparty Conversation
Emer Gilmartin, Francesca Bonin, Carl Vogel, Nick Campbell
Trinity College Dublin
{gilmare, boninf, vogel, nick}@tcd.ie
Abstract
This study explores laughter distribution
around topic changes in multiparty conver-
sations. The distribution of shared and solo
laughter around topic changes was examined
in corpora containing two types of spoken in-
teraction; meetings and informal conversation.
Shared laughter was significantly more fre-
quent in the 15 seconds leading up to topic
change in the informal conversations. A sam-
ple of informal conversations was then anal-
ysed by hand to gain further insight into links
between laughter and topic change.
1 Introduction
Human spoken interaction comprises a bundle of
signals and cues, together and separately providing
information relevant to the topic or task at hand, and
serving to build or maintain social bonds. Dialogue
is multifunctional, serving social as well as informa-
tion transfer goals. Laughter is predominantly social
rather than a solo activity, is universally present in
humans, part of the ?universal human vocabulary?,
innate, instinctual, and inherited from primate an-
cestors (Provine, 2004; Glenn, 2003). In conversa-
tion, it predominantly punctuates rather than inter-
rupts speech. Accounts of laughter?s role range from
response to humour to a social cohesion or bonding
mechanism used since our primate days. It has been
suggested that laughter is often a co-operative mech-
anism which can provide clues to dialogue structure
(Holt, 2011). Herein, we investigate the relevance of
laughter to topic change by analysing two corpora of
conversational speech in terms of temporal distribu-
tion of laughter, first through statistical analysis of
laughter and topic change distribution, then by man-
ual study of an hour of spontaneous conversation.
2 Laughter and Topic Change
Conversation analysis has highlighted connections
between laughter and topic change; many conver-
sations in the Holt corpus of mostly two person tele-
phone dialogues include laughter at topic closings
(Holt, 2010). Laughter has been linked to topic
closure in situations where one participant produces
jokes or laughs, thus inviting others to join in, with
this invitation open to refusal if interlocutors con-
tinue speaking on the topic at hand (Jefferson, 1979).
Holt (2010) suggests that laughter may arise at topic
changes because turns consisting only of laughter
are backwards looking, not adding to the last topic,
and thus constituting a signal that the current topic
has been exhausted and that the conversation is at
a topic change relevant point. We hypothesise that
these laughter turns form a ?buffer? allowing partic-
ipants a reassuring moment of social bonding. In
a meeting, there is a set agenda, a chairperson, and
protocols for moving from topic to topic. In social
dialogue, the goal is to pass time together, and top-
ics are not lined up ready for use. Aversion to poten-
tially embarrassing silence may be more pertinent in
informal conversation; thus laughter preceding topic
change may be more likely in informal dialogue.
Although there is much mention of laughter in
conversation analysis, it is difficult to find quanti-
tative data on its distribution in spoken interaction.
Previous work (Bonin et al, 2012b) established that
laughter, particularly shared laughter, is less likely
to occur in the first quarter of a topic than in the fi-
nal quarter, and that this distinction is greater in so-
304
cial conversation. In this work we test the hypothe-
sis that laughter should be frequently found before
rather than simply around topic changes. We ex-
amine the frequency of laughter within a range of
distances from either side of a topic change, to in-
vestigate if there is a period of higher laughter fre-
quency independent of topic length. We are also
interested in exploring whether the turns leading
to topic change follow the observations on topic
change sequences and laughter distribution in two
party conversations in the literature. If there are
identifiable sequences involving laughter leading to
topic change, knowledge of their architecture will
aid in creating algorithms for discourse recognition
and segmentation in multiparty conversation.
The notion of topic in discourse has been stud-
ied extensively but a concise definition is diffi-
cult to find. Topic has been described at sen-
tence level (Lambrecht, 1996), at discourse level
(Van Dijk, 1981); as a manifestation of speakers in-
tentions (Passonneau and Litman, 1997), and as co-
herent segments of discourse about the same thing
(Van Dijk, 1996). Here, we consider topic at dis-
course level as a chunk of coherent content.
3 Corpora
We analysed two datasets to cover free natural inter-
action and more structured meetings.
3.1 Topic annotation in TableTalk and AMI
Both TableTalk and AMI have topic annotations
freely available. TableTalk topics were annotated
manually by two labellers at a single level; AMI
annotations include top-level or core topics whose
content reflects the main meeting structure, and
subtopics for small digressions inside the core top-
ics. Here we use the core topic segmentation which
is more in line with the TableTalk annotation.
3.2 TableTalk
The TableTalk corpus contains multimodal record-
ings of free flowing natural conversations among
five participants, recorded at the Advanced Telecom-
munication Research Labs in Japan (Campbell,
2009). In order to collect as natural data as possi-
ble, neither topics of discussion nor activities were
restricted in advance. Three sessions were recorded
over three consecutive days in an informal setting
over coffee, by three female (Australian, Finnish,
and Japanese) and two male (Belgian and British)
participants (Jokinen, 2009). The conversations are
fully transcribed and segmented for topic, and also
annotated for affective state of participants and for
gesture and postural communicative functions us-
ing MUMIN (Allwood et al, 2007). Table-talk has
been analyzed in terms of engagement and laugh-
ter (Bonin et al, 2012a) and lexical accommodation
(Vogel and Behan, 2012). Our analyses used tran-
scripts of the entire corpus: about 3h 30, 31523 to-
kens and 5980 turns. Laughter was transcribed in
intervals on the speech transcription tier as @w, (un-
less inserted as part of a longer utterance). The total
number of laughs is 713. Shared laughter was auto-
matically annotated as described in ?4.
3.3 AMI
The AMI (Augmented Multi-party Interaction)
Meeting Corpus is a multimodal data set of 100
hours of meeting recordings (McCowan et al,
2005). The corpus contains real and scenario-driven
meetings. We base our analysis on the scenario
based meetings, with a total of 717,239 tokens. Each
meeting has four participants, and the same subjects
meet over four different sessions to discuss a design
project. The sessions correspond to four different
project steps (Project kick-off meeting, Functional
Design, Conceptual Design and Detailed Design).
Each participant is given a role to play (project
manager, marketing expert, industrial designer and
user interface designer) and keeps this role until the
end of the scenario. Conversations are all in En-
glish, with 91 native speakers and 96 non-native
speakers participating. There are 11,277 instances
of laughter, annotated in the transcripts as vocal-
sounds/laugh. About 25% of these laughs are anno-
tated with start time only.
4 Analytical methodologies
4.1 Automated and manual analyses
Both corpora were also analysed automatically, and
a one-hour sample of the TableTalk corpus was anal-
ysed on a case-by-case basis to investigate if laugh-
ter around topic change did indeed follow the pat-
terns proposed in the literature.
For the initial stages of ongoing manual analysis
305
to gain more insight into the mechanisms underly-
ing laughter and topic change, a one-hour stretch of
conversation from the second day of the TableTalk
was selected for study. The mechanism outlined
by Holt, based on Jefferson?s work on laughter and
Schegloff?s topic final sequences (Schegloff, 2007),
hinges on whether a laughter invitation is taken up
an interlocutor in two party dialogue. If it is, then
one or more laughter turns ensue and the likelihood
of topic change is high. The opposite occurs when
the interlocutor does not take up the invitation but
rather continues with further talk on the topic, avert-
ing topic change. We were interested in observing if
this phenomenon occurred in multiparty conversa-
tion, and if subsequent topic change was dependent
on how many of the group took up the invitation to
laugh. As analysis of the two corpora showed higher
likelihood of laughter before topic change in more
informal conversation, we chose to examine a sam-
ple of TableTalk for preliminary study.
This sample contained 1834 utterances, 36 T-
event or topic change instants, and 329 laughs
among the five participants, of which 76 were solo
while the remainder contributed to a total of 68
shared laugh events, all of which were manually an-
notated on separate laughter tiers. For each instance
of laughter, we also annotated the number of partic-
ipants who laughed and the distance from the laugh-
ter to the next topic commencement.
4.2 Temporal definitions and measurement
We use an algorithm resulting from earlier work to
annotate shared and solo laughter. The algorithm
was motivated by the observation that in both cor-
pora laughter was sometimes annotated with start
time only, and also that laughter in response to the
same stimulus should be considered shared laugh-
ter. These two factors taken together allow us to
recover shared laughter that may be missed if we
simply count overlapping laughs of distinct speak-
ers. The algorithm defines shared laughter as: (a)
overlapping laughs of distinct speakers; or (b) con-
secutive laughs of distinct speakers within distance
?. We calculate ? using the probability distribution
that successive laughs with observation of start time
only are part of a shared laugh event, trained on a
subset of overlapping laughs from the corpora.
Topic changes (T-events) are the annotated time
points where topic shifts in conversation. We
counted the frequency of laughter, shared laughter,
and solo laughter into 5-second bins at T-event mi-
nus multiples of 5 seconds (T-5, T-10, T-15, T-20) in
order to look at the laughter trend near topic termi-
nation. A meaningful threshold emerges (T-15 sec-
onds) where a change in the laughter trend is vis-
ible. Hence we counted the frequency of laughter
between T-15 and T, and T and T+15.
5 Results
5.1 Automated processing
We counted the frequency of laughter, shared laugh-
ter, and solo laughter in 5-second bins at T- event
time T minus multiples of 5 seconds (T-5, T-10,
T-15, T-20). Fig. 1 shows the mean frequency of
laughs per bin in TableTalk. While in AMI the distri-
bution over the bins does not show significant trends,
in TableTalk, we noticed a significant change at T-
15.1 Hence we take T-15 as a rational threshold
marking some change in the laughter distribution be-
fore a topic boundary in informal chat.
Then we analyzed the frequency of laughter be-
tween T-15 and T (we call this segment wt) and
T+15 (wb). As shown in Fig. 2, we notice a signifi-
cant difference in the amount of both shared and solo
laughter between topic terminations (wt) and topic
beginnings (wb). In particular topic terminations
show a higher frequency of laughter than topic be-
ginnings. The result holds in AMI and in TableTalk.
5.2 Manual processing
The first observation from the manual analysis is
that the shared/solo laugh ratio is heavily skewed to-
wards shared laughter (253 laughs were shared vs 79
solo). Laughs were combined into laugh events ac-
cording to the number of participants involved. The
length of laugh events was significantly shorter for
one-person laugh events than for shared laughter, see
Fig. 3. Distance to next topic change and number of
1The laughter counts in the bins for each of T-5, T-10 and T-
15 are significantly greater than random samples of 5 sec. con-
versation slices (Wilcox directed test, p < 0.002); the counts
for T-20 are not significantly greater than random slices. Fur-
ther, the counts for T-20 are significantly less than those in each
of T-15 (p < 0.02), T-10 (p < 0.02) and T-5 (p < 0.005), while
the pairwise differences among T-15, T-10 and T-5 are not sign-
ficant. We conclude that T-15 contains an inflection point.
306
T?20 T?15 T?10 T?5
Bin of 5 seconds from T?20 to T
Mea
n # o
f Lau
gh
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Figure 1: Frequency of laughter in TableTalk between T-
20 and T in 5-second bins. Bars represent the mean laugh
count per bin
SH in wb SH in wt SO in wb SO in wt
Mea
n # o
f Lau
gh
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Figure 2: Shared (sh) and Solo (so) laughs in topic
termination (wt) and topic beginning segments (wb)-
TableTalk
laughers in a laugh event, seen in Fig. 4, showed sig-
nificant negative correlation (p < 0.05).
6 Discussion and Conclusion
Our results indicate a likelihood of shared laugher
appearing in the final 15 seconds before a new topic
commences. This is in line with the literature which
reports laughter at topic transition relevant places,
and thus before a topic change. We have also seen
that the number of people sharing laughter is re-
lated to reducing distance from the laughter to the
next topic change, and that laugh events are longer
1 2 3 4 5
Mea
n len
gth o
f lau
gh e
vent
 - se
c
0
2
4
6
8
Figure 3: Laughter event length by number of laughers.
1 2 3 4 5
Mea
n dis
tanc
e to 
next
 topi
c - s
ec
0
2
4
6
8
Figure 4: Distance to next topic by number of laughers.
as more participants join in. Models of a complex-
ity adequate to predict human behaviour require ex-
haustively detailed analysis of stretches of conver-
sation in addition to broad statistical analysis. Our
combination of approaches has proven fruitful. Sev-
eral observations from the preliminary close exami-
nation of the TableTalk data provide fruit for further
research. Many of the short solo laughs may be seen
as responses to one?s own or another participant?s
content, while stronger solo laughs may tend to in-
vite longer and stronger laughter from others, lead-
ing to topic change possibilities. An acoustic anal-
ysis of the laughter will investigate this. We also
observed that shared laughter among several partic-
ipants which did not result in topic change were fre-
quently interpretable as attempts to draw an ongo-
ing topic to a close. This merits investigation to
see whether these laugh events can be considered
topic transition relevant places. Analysis of speaker
changes and turn retrieval in and around these laugh-
ter events is underway to model these events.
307
Acknowledgments
This work is supported by the Innovation Bursary
of Trinity College Dublin, the Speech Communica-
tion Lab at TCD, and by the SFI FastNet project
09/IN.1/1263. We are grateful to the anonymous re-
viewers for helpful feedback.
References
Jens Allwood, Loredana Cerrato, Kristiina Jokinen,
Costanza Navarretta, and Patrizia Paggio. 2007. The
mumin coding scheme for the annotation of feedback,
turn management and sequencing phenomena. Lan-
guage Resources and Evaluation, 41(3-4):273?287.
Francesca Bonin, Ronald Bo?ck, and Nick Campbell.
2012a. How do we react to context? annotation of
individual and group engagement in a video corpus.
In SocialCom/PASSAT, pages 899?903.
Francesca Bonin, Nick Campbell, and Carl Vogel. 2012b.
Laughter and topic changes: Temporal distribution and
information flow. In Cognitive Infocommunications
(CogInfoCom), 2012 IEEE 3rd International Confer-
ence on, pages 53?58.
Nick Campbell. 2009. An audio-visual approach to mea-
suring discourse synchrony in multimodal conversa-
tion data. In Proceedings of Interspeech 2009.
P. Glenn. 2003. Laughter in Interaction. Studies in Inter-
actional Sociolinguistics. Cambridge University Press.
Elizabeth Holt. 2010. The last laugh: Shared laugh-
ter and topic termination. Journal of Pragmatics,
42:1513?1525.
Elizabeth Holt. 2011. On the nature of ?laughables? :
laughter as a response to overdone figurative phrases.
Pragmatics, 21(3):393?410, September.
Gail Jefferson. 1979. A technique for inviting laugh-
ter and its subsequent acceptance/declination. In
G Psathas, editor, Everyday language: Studies in eth-
nomethodology., pages 79?96. Irvington Publishers:
New York,NY.
Kristiina Jokinen. 2009. Gaze and gesture activity
in communication. In Constantine Stephanidis, ed-
itor, Universal Access in Human-Computer Interac-
tion. Intelligent and Ubiquitous Interaction Environ-
ments, volume 5615 of Lecture Notes in Computer Sci-
ence, pages 537?546. Springer Berlin / Heidelberg.
K. Lambrecht. 1996. Information Structure and Sen-
tence Form: Topic, Focus, and the Mental Represen-
tations of Discourse Referents. Cambridge Studies in
Linguistics. Cambridge University Press.
I. McCowan, G. Lathoud, M. Lincoln, A. Lisowska,
W. Post, D. Reidsma, and P. Wellner. 2005. The ami
meeting corpus. In In: Proceedings Measuring Be-
havior 2005, 5th International Conference on Meth-
ods and Techniques in Behavioral Research. L.P.J.J.
Noldus, F. Grieco, L.W.S. Loijens and P.H. Zimmer-
man (Eds.), Wageningen: Noldus Information Tech-
nology.
Rebecca J. Passonneau and Diane J. Litman. 1997. Dis-
course segmentation by human and automated means.
Computational Linguistics, 23(1):103?139.
Robert R. Provine. 2004. Laughing, tickling, and the
evolution of speech and self. Current Directions in
Psychological Science, 13(6):215?218.
E.A. Schegloff. 2007. Sequence Organization in Inter-
action: Volume 1: A Primer in Conversation Analysis.
Cambridge University Press.
Teun A. Van Dijk, 1981. Sentence Topic versus Dis-
course Topic, pages 177?194. Mouton.
Teun A. Van Dijk. 1996. Discourse, power and ac-
cess. In Carmen Rosa Caldas-Coulthard and Malcolm
Coulthard, editors, Texts and Practices, Readings in
Critical Discourse Analysis, pages 84?104. Routledge.
Carl Vogel and Lydia Behan. 2012. Measuring
synchrony in dialog transcripts. In Anna Espos-
ito, Antonietta M. Esposito, Alessandro Vinciarelli,
Ru?diger Hoffmann, and Vincent C. Mu?ller, edi-
tors, Behavioural Cognitive Systems, pages 73?88.
Springer, LNCS 7403.
308
Proceedings of the SIGDIAL 2013 Conference, pages 309?313,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
IMHO: An Exploratory Study of Hedging in Web Forums
Liliana Mamani Sanchez
Trinity College Dublin
mamanisl@scss.tcd.ie
Carl Vogel
Trinity College Dublin
vogel@tcd.ie
Abstract
We explore hedging in web forum con-
versations, which is interestingly different
to hedging in academic articles, the main
focus of recent automatic approaches to
hedge detection. One of our main results
is that forum posts using hedges are more
likely to get high ratings of their useful-
ness. We also make a case for focusing
annotation efforts on hedges that take the
form of first-person epistemic phrases.
1 Introduction
Computational linguistics research in hedging, use
of linguistic expressions whose contribution to
sentence meaning is a modulation of the accuracy
of the content they embed, and speculation detec-
tion has been done intensively in the domain of
scholarly texts. The interest created in this do-
main has expanded to some extent to other do-
mains such as news and reviews. Automatic pro-
cessing of speculation requires at some stage the
annotation of words or phrases conveying uncer-
tainty (Vincze et al, 2008). More complex en-
deavours imply the annotation of various elements
of context involved in the expression of hedging
(Rubin et al, 2005; Wiebe et al, 2005).
In web forums where users? contributions play
a vital role in the forum dynamics, such as mutual
support forums that are part of the ecosystem of
technology company supports for users, exploring
the features that make a contributor outstanding
is relevant.1 A user shows a distinctive behavior
by writing useful posts that help other users in the
problem that first motivated their participation in
1Throughout, we use ?web forum? to refer to such ecosys-
tems: we speculate that their informal nature makes our ob-
servations generalize to other sorts of web forum in which
solutions to problems are not the focal point; even general
discussion forums can be witnessed to trigger community
weighting of contributions.
the forum. This paper emerges from our interest
in finding features that predict which contributors
will be most appreciated.
Many lexical and grammatical devices aid
hedging (expressions such as epistemics verbs,
modals, adjectives, etc. name but a few) as do non-
lexical devices such as conditionals. We deem sin-
gular first person epistemic phrases as hedges that
can help to identify the subject of a hedging event.
We analyze the correlation between the use of
epistemic phrases (vs. other types of hedges) and
the probability of posts containing these hedges of
being considered useful by the forum community.
We also explore whether epistemic phrases consti-
tute a distinctive feature that support user classifi-
cations. In ?2, we described the function of hedges
according to a hedging classification framework
and in relation to the domain of web forums. Then
?3 describes the profiling work done and discusses
the main findings. We conclude in ?4.
2 Functions of hedging
The research by Hyland (1998) is one of the broad-
est studies about hedging functions in scientific
articles, and which makes use of categories that
have strong relationship, at face value, to the like-
lihood that the reader of hedged material will find
the material sufficiently useful or sufficiently well
expressed to prompt the reader to rate highly the
message containing the material, whether with an
explicit facility to record kudos or otherwise. Hy-
land proposed a poly-pragmatic classification of
hedges based on their indicating function: reader-
oriented, writer-oriented, attribute and reliability.
Briefly, attribute and reliability hedges both re-
late to the accuracy of the message conveyed. At-
tribute hedges relate to the conformity of the de-
scribed situation with encyclopedic expectations
(1), while reliability hedges relate to the level of
certainty of the speaker about the propositional
content (2). In a different dimension, reader ori-
309
ented hedges are composed with the concern that
the ?reader? accept the truth of the embedded
content (3), thereby presupposing the ?writer?s?
commitment to the content, while writer oriented
hedges disclaim commitment to the content (4).
(1) Protypical mammals are land-dwellers.
(2) Probably, respected ancient Greeks
thought whales to be fish.
(3) I think that if you reboot, the changes will
take effect.
(4) Based on what you?ve said, you seem
right.
Applying this classification scheme not to schol-
arly prose but to web forums, it seems likely that
readers in technical forums would prefer the accu-
racy of attribute hedges (1) over the relative uncer-
tainty of reliability hedges (2), and that the reader
oriented hedges (3) supply comfort in the implica-
tion of both the quality of the embedded claims
and the absence of arrogance. This research is
attempting to test these hypotheses by assessing
the relationship between the likelihood of posts re-
ceiving kudos and the quantity of hedges in these
categories that the posts contain.
Unfortunately, answering the question is com-
plex, because it is not in all cases obvious whether
a linguistic expression contains a hedge or what
function the hedges serve when they do exist.
Therefore, we attempt a partial answer to the ques-
tion by examining those hedge expressions which
can be processed with some reliability using au-
tomated means. Consider the taxonomy of lin-
guistic expressions in Fig. 1. The boxed regions
of this taxonomy are amenable to automatic pro-
cessing. Further, epistemic hedges with first-
person singular subjects relate strongly to reader
oriented hedges (3) in Hyland?s taxonomy. The
non-phrasal hedges are heterogeneous in function.
Figure 1: A taxonomy of linguistic expressions.
Linguistic Expressions
epistemic hedges other expressions
non?phrasal
lexical
phrasal
1st person singular other
conditionals ...
...
...
We do not claim this separation of hedging
markers can fully account for pragmatic and se-
mantic analysis of hedging in web forums, but we
are confident this classification supports reliable
annotation for quantificational assessment of cer-
tainty and hedging in this informal domain. We
base our profiling experiments (?3) on this func-
tional separation of hedging markers.
3 Profiling posts by hedging
3.1 Description of the forum dataset
The dataset we used created out of a forum that
is part of customer support services provided by
a software vendor company. Although we were
not able to confirm the forum demographics, we
can infer they are mostly American English speak-
ers as the forum was set up first for USA cus-
tomers. Some other features are best described by
Vogel and Mamani Sanchez (2013). Our dataset
is composed of 172,253 posts that yield a total
of 1,044,263 sentences. This dataset has been
intensively ?cleaned?, as originally it presented
a great variety of non-linguistic items such as
HTML codes for URLS, emoticons, IP addresses,
etc. These elements were replaced by wild-cards
and also user names have been anonymised, al-
though some non-language content may remain.
A forum user can give a post ?kudos? if he/she
finds it useful or relevant to the topic being ad-
dressed in a forum conversation.2 We counted
the number of kudos given to each post. There
are four user categories in the forum: {employee,
guru, notranked, ranked}.3 A poster?s rank de-
pends, among other factors, on the number of
posts they make and their aggregate kudos.
3.2 Epistemic phrases versus other hedges
We created two lexicons, one composed by first
person singular epistemic phrases and one by non-
phrasal hedges. Initially, a set of epistemic phrases
where taken from Ka?rkka?inen (2010): {I think, I
don?t know, I know, etc.} and from Wierzbicka
(2006). The non-phrasal hedge lexicon was cre-
ated from words conveying at least some degree of
uncertainty: {appear, seem, sometimes, suggest,
unclear, think, etc.}, taken from Rubin (2006).
Additional hedges were included after the pilot
2A user may accord kudos for any reason at all, in fact.
3In the forum we studied, there are actually many ranks,
with guru as the pinnacle for a non-employee; we grouped
the non-guru ranked posters together.
310
annotation. The lexicons are composed by 76
and 109 items, respectively. There are many
other hedge instances that are not included in
these lexicons but our experiment restricts to these
items. Epistemic phrases include acronyms such
as ?IMHO?, ?IMO? and ?AFAIK? that we deem
meet functions described in ?2.
A pilot manual annotation of hedges was con-
ducted on in order to verify the viability of auto-
matic annotation. Our automatic annotation pro-
cedure performs a sentence by sentence matching
and tagging of both kinds of hedging. The pro-
cedure uses a maximal matching strategy to tag
hedges, e.g. if ?I would suggest? is found, this
is tagged and not ?suggest?. This automatic tag-
ging procedure does not account for distinctions
between epistemic and deontic readings of hedges,
nor between speculative or non-speculative uses of
non-phrasal hedges. 107,134 posts contain at least
one hedge: 34,301 posts contain at least one epis-
temic phrase; 101,086, at least one non-phrasal
hedge; 28,253, at least one of each.
3.3 Methods of analysis
In ?3.1 we showed there are two ways to charac-
terize a post: 1) By its writer category and 2) by
the number of times it gets accorded kudos. We
devise a third characterisation by exploring epis-
temic phrases and non-phrasal hedge usage in in-
dividual posts as a whole, tracking use of both
types of hedge in each post. We devised three
discretization functions (DF) for assigning a la-
bel to each post depending on the type of hedges
contained within. The DFs take two parameters,
each one representing either the relative or bina-
rized frequency non-phrasal hedges and epistemic
phrases (nphr or epphr). DF1 relies on the oc-
currence of either type of hedge; a post is of a
mixed nature if it has at least one of each hedge
type. DF2 is based on a majority decision depend-
ing on the hedge type that governs the post and
only assigns the label hedgmixed when both types
of hedges appear in the same magnitude. DF3
expands DF1 and DF2 by evaluating whether ei-
ther majority or only one type of hedge is found,
e.g. we wanted to explore the fact that even when
non-phrasal hedges domain one post, an epistemic
phrase is contained as well, in contrast to when
only non-phrasal hedges occur in a post.
DF
1 epphr==0 epphr>0
nphr ==0 nohedges epphrasal
nphr >0 nonphrasal hedgmixed
DF
2 nphr=0 & epphr=0 nohedges
nphr > epphr nonphrasal
nphr < epphr epphrasal
nphr =epphr hedgmixed
DF
3
epphr=0 epphr>0
nphr=0 nohedges epphronly
nphr>0 nonphronly
nphr > epphr nonphrmostly
nphr < epphr epphrmostly
nphr =epphr hedgmixed
We computed four measures for each post based
on these functions, m1 is calculated by using DF1
having raw frequencies of hedges as parameters,
m2 and m3 result from applying DF3 and DF2
respectively to frequencies of hedge type averaged
by the corresponding lexicon size, and m4 is cal-
culated from DF3 over hedge frequencies aver-
aged by post word count. Other measures are also
possible, but these seemed most intuitive.
We were interested in the extent that hedge-
based post categories correlate with a post?s kudos
and with a post?s user category as tests of hypoth-
esis outlined in ?2. We want to know which cor-
relations hold regardless of the choice of intuitive
measure and which are measure dependent.
3.4 Results and discussion
1.40
1.45
1.50
1.55
1.60
Mea
n of
 kud
os
epphrasal
hedgmixed
nohedges
nonphrasal
epphrmostly
epphronly
hedgmixed
nohedges
nonphrmostly
nonphronly
epphrasal
hedgmixed
nohedges
nonphrasal
epphrmostlyepphronly
hedgmixed
nohedges
nonphrmostly
nonphronly
m1 m2 m3 m4
Figure 2: Design plot with the mean of kudos of
each kind of post per each measure.
In Fig. 2, we show how the different hedge-
based classifications of posts (m1, m2, m3, m4)
relate to the average kudo counts for posts. Each
measure is shown in an individual scale.4 The hor-
izontal line represents the average of kudos for
all posts so we can observe which categories are
above/below the mean. Comparison and contrast
4For this comparison, we dropped extreme outliers in the
number of kudos and hedges, and we calculated these mea-
sures only in posts that had at least one kudo attribution.
311
of the relationship between categorisation of posts
with each mi and mean kudos is interesting. For
example, when epistemic phrases dominate a post
(epphrmostly), there is the greatest mean of ku-
dos visible with the measure m2. The second
highest positive effect is of non-phrasal hedges
dominating a post (nonphrmostly) in m2 and m4.
The next strongest effect occurs when both of
hedges types appear in a post (hedgmixed in m1
and m3) and when they have about the same av-
erage density (m4), followed by when non-phrasal
hedges appear exclusively in a post. While there is
no consensus across the different scales that epis-
temic phrase-dominated posts are the most likely
to obtain kudos, still their occurrence has a posi-
tive effect in the average of kudos obtained. There
is low probability of kudos when only epistemic
phrases appear and the lowest probability when no
hedge occurs.5 Thus, we argue that the four mea-
sures are jointly and individually useful.
employee guru notranked ranked
0.0
0.1
0.2
0.3
0.4
0.5
epp
hras
al
hed
gmi
xed
noh
edg
es
non
phra
sal
epp
hras
al
hed
gmi
xed
noh
edg
es
non
phra
sal
epp
hras
al
hed
gmi
xed
noh
edg
es
non
phra
sal
epp
hras
al
hed
gmi
xed
noh
edg
es
non
phra
sal
m1
Per
cen
tage
 of p
osts
 per
 hed
ge t
ype
Figure 3: Percentages of m1-hedge types in each
user category.
The relationship between hedge use and user
category is depicted (for m1) in Fig. 3. While
for all four user roles, epistemic phrases are exclu-
sively present in the lowest percentage of posts,
their contribution is shown in posts with mixed
hedge types. Posts with only non-phrasal hedges
are the most frequent across all user categories.
We had predicted no significance in this respect
5The contribution of epistemic phrases to the likelihood
of kudos could be due to other factors such as the use of first
person in general. We profiled the use of pronouns ?I? and
?my? and we found a negative correlation between frequency
of these pronouns and the number of kudos per post. There
is a small but not significant correlation restricting to those
posts with non-zero kudos.
since non-phrasal hedges could map into any of
Hyland?s functions, however our intuition was
wrong as there is a significant difference (p<0.05)
in the proportions of posts per hedge type category
when making comparisons across user categories
one to one. Only when comparing proportions of
hedge type posts by gurus and notranked users
is there no significant difference in hedgmixed,
nonphrasal and nohedges posts.6 Employees and
ranked users have the highest rates of use of mixed
hedges. Ranked and guru posts have the high-
est ratios of exclusively epistemic phrase hedges,
meeting expectations. Employees have the low-
est ratio of user of epistemic phrases on their own,
this presumably since they frequently write posts
on behalf of the company so they are least likely
to make subjective comments: their posts have the
lowest percentage of use of ?I? and ?my?.
These two approaches to assessing associations
between different classifications of forum posts re-
veal that posts using hedges are the most likely to
be accorded kudos and that guru and ranked users
are the most frequent users of epistemic phrases
in general. This lends support to the view that
first person singular epistemic phrases, the epit-
ome of reader-oriented hedges, are predictive of
coarse grained rank in the forum.
4 Conclusions and future work
We have found that the hedges used contribute to
the probability of a post getting high ratings. Posts
with no hedges are the ones awarded least kudos.
We have still to test the correlation between epis-
temic phrases and other types of hedges when they
both are found in a single post. We think that au-
tomatic methods should focus in first person epis-
temic phrases as they show writer?s stance at the
same time as softening their commitment or antic-
ipating reader?s response. Following the annota-
tion described here, manual annotation work is un-
der way, where epistemic phrases and non-phrasal
hedges constitute two distinct categories. Our on-
going work seeks other ways to measure the con-
tribution of these categories to reader expression
of appreciation of posts and whether hedge us-
age creates natural user categorizations. We also
study other types of web forum dialogue to explore
whether hedging follows similar trends.
6A two-sample test of proportions was used to test the
significance of differences between amounts of hedge type
posts for each category.
312
Acknowledgements
This research is supported by the Trinity College
Research Scholarship Program and the Science
Foundation Ireland (Grant 07/CE/I1142) as part
of the Centre for Next Generation Localisation
(www.cngl.ie) at Trinity College Dublin. This pa-
per has benefited from input from the anonymous
reviewers and from Ron Artstein.
References
Ken Hyland. 1998. Hedging in Scientific Research Ar-
ticles. Pragmatics & beyond. John Benjamins Pub-
lishing Company.
Elise Ka?rkka?inen. 2010. Position and scope of epis-
temic phrases in planned and unplanned american
english. In New approaches to hedging, pages 207?
241. Elsevier, Amsterdam.
Victoria Rubin, Elizabeth Liddy, and N. Kando. 2005.
Certainty identification in texts: Categorization
model and manual tagging results. In James G.
Shanahan, Yan Qu, and Janyce Wiebe, editors, Com-
puting Attitude and Affect in Text. Springer.
Victoria L. Rubin. 2006. Identifying Certainty in Texts.
Ph.D. thesis, Syracuse University, Syracuse, NY.
Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The BioScope
corpus: biomedical texts annotated for uncertainty,
negation and their scopes. BMC Bioinformatics,
9(Suppl 11).
Carl Vogel and Liliana Mamani Sanchez. 2013. Epis-
temic signals and emoticons affect kudos. Cognitive
Infocommunications (CogInfoCom), 2012 IEEE 3rd
International Conference on, pages 517?522.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language ANN. Language Resources and
Evaluation, 39(2/3):164?210.
A. Wierzbicka. 2006. English: meaning and culture.
Oxford University Press, USA.
313
