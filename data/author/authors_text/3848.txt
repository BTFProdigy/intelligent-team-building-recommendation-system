What Prompts Translators to Modify Draft Translations?
An Analysis of Basic Modification Patterns for Use in
the Automatic Notification of Awkwardly Translated Text
Takeshi Abekawa and Kyo Kageura
Library and Information Science Course
Graduate School of Education,
University of Tokyo
{abekawa,kyo}@p.u-tokyo.ac.jp
Abstract
In human translation, translators first make
draft translations and then modify them.
This paper analyses these modifications, in
order to identify the features that trigger
modification. Our goal is to construct a sys-
tem that notifies (English-to-Japanese) vol-
unteer translators of awkward translations.
After manually classifying the basic modifi-
cation patterns, we analysed the factors that
trigger a change in verb voice from passive
to active using SVM. An experimental re-
sult shows good prospects for the automatic
identification of candidates for modification.
1 Introduction
We are currently developing an English-to-Japanese
translation aid system aimed at volunteer transla-
tors mainly working online (Abekawa and Kageura,
2007), As part of this project, we are developing a
module that notifies (inexperienced) translators of
awkwardly translated expressions that may need re-
finement or editing.
In most cases, translators first make draft trans-
lations, and then examine and edit them later, often
repeatedly. Thus there are normally at least two ver-
sions of a given translation, i.e. a draft and the final
translation. In commercial translation environments,
it is sometimes the case that texts are first translated
by inexperienced translators and then edited by ex-
perienced translators. However, this does not ap-
ply to voluntary translation. In addition, volunteer
translators tend to be less experienced than commer-
cial translators, and devote less time to editing. It
would therefore be of great help to these translators
if the CAT system automatically pointed out awk-
ward translations for possible modification. In order
to realise such a system, it is necessary to first clarify
(i) the basic types of modification made by transla-
tors to draft translations, and (ii) what triggers these
modifications.
In section 2 we introduce the data used in this
study. In section 3, we clarify the nature of modifica-
tion in the translation process. In section 4, we iden-
tify the actual modification patterns in the data. In
section 5, focusing on ?the change from the passive
to the active voice? pattern, we analyse and clarify
the triggers that may lead to modification. Section 6
is devoted to an experiment in which machine learn-
ing methods are used to detect modification candi-
dates. The importance of the various triggers is ex-
amined, and the performance of the system is evalu-
ated.
2 The data
The data used in the present study is the Japanese
translation of an English book about the problem of
peak oil (Leggett, 2005). The book is aimed at a
popular audience and is relevant to the sort of texts
we have in mind, because the majority of texts vol-
unteer translators translate deal with current affairs,
social issues, politics, culture and sports, and/or eco-
nomic issues for a popular audience1. The data con-
sists of the English original (henceforth ?English?),
the draft Japanese translation (?Draft?) and the fi-
nal Japanese translation (?Final?). The ?Draft? was
made by two translators (one with two years? experi-
ence and the other with five years? experience), and
1Software localisation is another area of translation in which
volunteers are heavily involved. We do not include it in our
target because it has different characteristics.
241
?????? ?? ???? ? ? ?? ? ?? ? ??? ?? ? ??? ? ? ? ??? ? ???? ? ?? ? ??? ?? ? ?? ?? ? ?
????????? ? ?? ? ? ?? ?? ? ?? ? ?? ? ?? ? ? ? ??? ? ?? ?? ???? ? ??? ?? ?? ?? ?? ? ?
Figure 1: An example of word alignment using GIZA++
the ?Final? was made by a translator with 12 years?
experience. Table 1 gives the quantities of the data.
?English? ?Draft? ?Final?
Number of sentences 4,587 4,629 4,648
Number of words 92,300 127,838 132,989
(Average per sentence) 20.1 27.6 28.6
Table 1: Basic quantities of the data
3 Nature of the modification process
State Cause
1. Mistranslation English is complex
2. Text is confusing English is complex / Trans-
lation is too literal
3. Text is unnatural Translation is too literal /
Japanese is underexamined
4. Against modi-
fiers? taste
Different Japanese ?model?
is assumed
5. Against editorial
policy
Lack of surface editing
Table 2: States in the draft and their causes
As little research has been carried out into the pro-
cess by which translators modify draft translations,
we manually analysed a part of the data in which
modifications were made, in consultation with a
translator. In the modification process, the translator
first recognises (though often not consciously) one
of a number of states in a draft translation and the
underlying cause of the state. S/he then modifies the
draft translation if necessary. Table 2 shows the ba-
sic classification of states and possible causes. Al-
though the states are conceptually clear, it is not nec-
essarily the case that translators can judge the state
of a given translation consistently, because judging
a sentence as being ?natural? or ?confusing? is not
a binary process but a graded one, and the distinc-
tion between different states is often not immedi-
ately clear. Many concrete modification patterns
found in the data are covered in translation textbooks
(Anzai, 1995; Nakamura, 2003). However, although
it is obvious in some cases that a section of trans-
lated text needs to be modified, in other cases it is
less clear, and judgments will vary according to the
translator. The task that automatic notification ad-
dresses, therefore, is essentially an ambiguous one,
even though the actual system output may be binary.
We also identified the distinction between two
types of modification: (i) ?generative? modification,
in which the modified translation is generated on the
spot, with reference to the English original; and (ii)
?considered? modification, in which alternate ex-
pressions (phrases, collocations, etc.) are retrieved
from the depository of useful, elegant, or conven-
tional expressions in the translator?s mind. These
two types of modification can be activated in the face
of one token of modification at once.
4 Modification patterns
The most natural way to classify modification pat-
terns is by means of basic linguistic labels such as
?change of voice? or ?change from nominal modifi-
cation to adverbial modification? (cf. Anzai, 1995).
These modification patterns consist of one or more
primitive operations. For instance, a ?change of
voice? may consist of such primitive operations as
?changing the case-marker of the subject,? ?swap-
ping the position of subject and object,? etc.
As preparation, we extracted modification pat-
terns from the data2. In order to do so, we first
aligned the ?Draft? and the ?Final? at the sentence
level using DP matching, and then at the morpheme
level using GIZA++ (Och and Ney, 2003). Figure
1 illustrates an example of word/morpheme level
2This task is similar to the acquisition of paraphrase knowl-
edge (Barzilay and McKeown, 2001; Shinyama et al, 2002;
Quirk et al 2004; Barzilay and Lee, 2003; Dolan et al, 2004).
However, our aim here is to clarify basic modification patterns
and not automatic identification.
242
English: If it was perceived to be true by the majority of Thinkers, ...
?Draft?: ??? ?????? ??? ?????? ???????
JINRUI-NO TASUU-NIYOTTE SORE-GA SINJITU-DE-ARU-TO NINSIKI-SA-RERE-BA
(thinkersgenitive) (majorityablative) (itsubject) (to be true) (be perceived)
?Final?: ??? ??? ??? ??? ??????
JINRUI-NO TASUU-GA SORE-WO SINJITU-TO NINSIKI-SURE-BA
(thinkersgenitive) (majoritysubject) (itobject) (to be true) (perceive)
Primitive replace(?NIYOTTE?, replace(?GA?, delete(?DE?) delete(?RARERU?)
operations: ?GA?) ?WO?) delete(?ARU?)
Table 3: An example of a primitive modification operation
alignment. Changes in word order occur frequently,
as is shown in Figure 1, and the ?Final? and the
?Draft? are not completely parallel at the word or
morpheme level. As a result, GIZA++ sometimes
misaligns the units.
From the aligned ?Draft? and ?Final? data, we
identified the primitive operations. We limited these
operations to syntactic operations and semantic op-
erations such as the changing of content words, be-
cause the latter is hard to generalise with a small
amount of data. Primitive operations were extracted
by calculating the difference between correspond-
ing bunsetsu, which basically consist of a content
word and postpositions/suffixes, in the ?Draft? and
in the ?Final?. An example is given in Table 3. Ta-
ble 4 shows the five most frequent changes in verb
inflections and case markers, which are two domi-
nant classes of primitive operation. In addition, we
observed deletions and insertions of Sahen verbs.
Modification patterns were identified by observ-
ing the degree of co-occurrence among these prim-
itive operations. We used Cabocha3 to identify the
syntactic dependencies and used the log-likelihood
ratio (LLR) to calculate the degree of co-occurrence
of primitive operations that occupy syntactically de-
pendent positions. Table 5 shows the top five pair-
wise co-occurrence patterns.
inflection del. ins. case marker del. ins.
DA 379 291 NI 476 384
TE 269 358 GA 387 502
TA 247 306 NO 366 204
RARERU 224 122 WO 293 421
IRU 197 267 DE 203 193
Table 4: Frequent primitive operations
3http://chasen.org/?taku/software/cabocha/
Three main modification patterns were identified:
(i) a change from the passive to the active voice (226
cases); (ii) a change from a Sahen verb to a Sa-
hen noun (208 cases); and (iii) a change from nom-
inal modification to clausal structure. These pat-
terns have been discussed in studies of paraphrases
(Inui and Fujita, 2004) and in translation textbooks
(Anzai, 1995; Nakamura, 2003). We focus on ?the
change from the passive to the active voice?. It is
one of the most important and interesting modifica-
tion patterns because (i) it is mostly concerned with
the main clausal structure in which other modifica-
tions are embedded; and (ii) the use of active and
passive voices differs greatly between English and
Japanese and thus there will be much to reveal.
5 Triggers that lead to modification
Given a draft translation, an experienced translator
will be able to recognise any problematic states in it
(see Table 2), identify the causes of these states and
deal with them. As computers (and inexperienced
translators) cannot do the same (cf. Sun et al, 2007),
it is necessary to break these causes down into com-
putationally tractable triggers. Keeping in mind the
nature of the modification process discussed in sec-
tion 3, we analysed the actual data, this time with
the help of a translator and a linguist.
At the topmost level, two types of triggers were
identified: (i) ?pushing? triggers that are identified
as negative characteristics of the draft translation ex-
pressions themselves; and (ii) ?pulling? triggers that
come from outside (from the depository of expres-
sions in the translator?s mind) and work as concrete
?model translations?. The distinction is not entirely
clear, because a model is needed in order to iden-
tify negative characteristics, and some sort of neg-
ative impression is needed for the ?model transla-
tion? to be called up. The distinction is nevertheless
243
LLR f(a,b) f(a) f(b) operation a operation b plain expression
146.2 28 35 224 replace(NIYOTTE,GA) delete(RARERU) A NIYOTTE B SARERU?A GA B SURU
105.2 34 90 224 replace(GA,WO) delete(RARERU) A GA B SARERU?A WO B SURU
91.7 34 115 208 replace(NO,GA) delete(SAHEN) A NO B?A GA B SURU
90.9 26 61 208 replace(NO,WO) delete(SAHEN) A NO B?A WO B SURU
36.3 15 68 168 replace(NI,WO) intransitive?transitive A NI B SURU?A WO C SURU
Table 5: Five of the most frequent co-occurrence patterns between two primitive operations
important, both theoretically and practically. Theo-
retically, it corresponds to the types of modification
observed in section 3. From the practical point of
view, the first type is related to the general structural
modelling (in its broad sense) of language, while the
second is closely related to the status of individual
lexicalised expressions. Correspondingly, an NLP
system that addresses the first type needs to assume
a language model, while a system that addresses the
second type needs to call on the relevant external
data on the spot. We address the first type of trig-
ger, because we can hypothesise that the modifica-
tion by change of voice is mainly related to the struc-
tural nature of expressions. It should also be noted
that, from the machine learning point of view, there
are positive and negative features which respectively
promote and restrict the modification.
We classified the features that may represent po-
tential triggers into five groups:
(A) Features related to the readability of the En-
glish, because the complexity of English sentences
(cf. Fry, 1968; Gunning, 1959) can affect the qual-
ity of draft translations. Thus the number of words
in a sentence, length of words, number of verbs in
a sentence, number of commas, etc. can be used as
tractable features for automatic treatment.
(B) Features reflecting the correspondence be-
tween the English and the draft Japanese trans-
lation. Translations that are very literal, either lex-
ically or structurally, are often also awkward. On
the other hand, a high degree of word order corre-
spondence can be a positive sign (cf. Anzai, 1995),
because it indicates that the information flow in En-
glish is maintained and the Japanese translation is
well examined.
(C) Features related to the Japanese target verbs.
The characteristics of the target verbs should affect
the environments in which they occur.
(D) Features related to the ?naturalness? of the
Japanese. Repetitions or redundancies of elements
or sound patterns may lead to unnatural Japanese
sentences.
(E) Features related to the complexity of the
Japanese. If a draft translation is too complex, it
may be confusing or hard to read. Structural com-
plexity, the length of a sentence, the number of com-
mas, etc. can be used as triggers that reflect the com-
plexity of the Japanese translation.
Table 6 shows the computationally tractable fea-
tures we defined within this framework. Features
with ?#? in their name are numeric features and the
others are binary features (taking either 0 or 1).
6 Detecting modification candidates
Using these features, we carried out an experiment
of automatic identification of modification candi-
dates. As a machine learning method, we used
SVM (Vapnik, 1995). The aim of the experiment
was twofold: (i) to observe the feasibility of auto-
matic notification of modification candidates, and
(ii) to examine the factors that trigger modifications
in more detail.
6.1 Experimental setup
In the application of SVM, we reduced the number
of binary features by using those that have higher
correlations with positive and negative examples, us-
ing mutual information (MI). Table 7 shows features
that have high correlations with positive and nega-
tive features (eight for each).
SVM settings: The liner kernel was used. For a
numeric feature X , the value x is normalized by z-
score, norm(x) = x?avg(X)?
var(X)
, where avg(x) is the
empirical mean of X and var(X) is the variance of
X.
Data: The numbers of positive and negative cases
in the data are 226 and 894, respectively (1120 in
total). In order to balance the positive and negative
examples, we used an equal number of examples for
training.
244
(A)
EN#word: the number of words in the English sentence
EN#pause: the number of delimiters in the English sen-
tence
EN#verb: the number of verbs in the English sentence
EN#VVN: the number of VNN verbs in the English sen-
tence
EN#word len: the average number of characters in a word
(B)
EPOS: POS of the English word corresponding
to the target Japanese verb
EPOS before: POS of a word before the English word
corresponding to the target Japanese verb
EPOS after: POS of a word after the English word cor-
responding to the target Japanese verb
EPOS before:POS : a bigram of EPOS before and EPOS
EPOS:POS after: a bigram of EPOS and EPOS after
EJ#translation: translation probability between the
source and target language sentences
(C)
Fsuffix: a suffix following the target verb
Fparticle: a particle following the target verb
Fpause park: a pause mark following the target verb
Dmodifying case: case marker of the element that modifies the
target verb
Dmodifying agent: case marker of the element that modifies the
target verb, if its case element has an AGENT
attribute
Dfunctional: functional noun which is modified by the tar-
get verb
Dmodified case: case marker of the element that is modified by
the target verb
Sfirst agent: first case element in the sentence has an
AGENT attribute
Sbefore passive: Is there a passive verb before the target
verb in the sentence?
Safter passive: Is there a passive verb after the target verb
in the sentence?
(D)
Nmodifying voice: the voice of the verb that modifies the
target verb
Nmodifying voice: the voice of the verb that is modified
by the target verb
Ngrandparent voice: the voice of the grandparent verb of
the target verb
Ngrandchild voice: the voice of the grandchild verb of the
target verb
Ncase adjacency; bigram consists of a particle of the tar-
get verb and a particle of the adja-
cency bunsetsu chunk
(E)
J#morpheme: the number of morphemes in the target
Japanese sentence
J#pause: the number of pause marks in the target
Japanese sentence
J#verb: the number of verbs in the target Japanese
sentence
J#passive: the number of verbs with passive voice in the
target Japanese sentence
J#depth: depth of the modifier which modifies the tar-
get verb
Table 6: Features
Methods of evaluation: We used (i) 10-fold cross
validation to check the power of classifiers for un-
known data and (ii) a partially closed test in which
the 226 positive and negative examples were used
for training and 1120 data were evaluated, in order
to observe the realistic prospects for actual use.
6.2 Result of experiment and feature analysis
Table 8 shows the results. Though they are reason-
able, the overall accuracy, especially for the partially
closed test, shows that the method is in need of im-
provement.
In order to evaluate the effectiveness of the fea-
ture sets, we carried out experiments only using and
without using each feature set. Table 9 shows that
how efficient is each feature set defined in Table 6.
The left-hand column in Table 9 shows the result
with all feature sets except focal feature set, and the
right-hand column shows the result when only the
focal feature set was used.
The experiment showed that the feature set that
contributed most was C (features related to the
Japanese target verbs). We also carried out an exper-
iment to check which features are effective among
this set, in the same manner as the experiments for
checking the effectiveness of the feature sets. The
result showed that the feature Dmodifying case is the
feature that contributed the most by far. In Japanese,
case markers are strongly correlated with the voice
of verbs, and the coverage of this feature for tokens
related to voice is high because it is common for a
verb to be modified by the case element with the case
marker.
It became clear that the numeric features A and
E contribute little to the overall accuracy. Table 10
shows the correlation coefficient between the nu-
meric features and correct answers. The table shows
that there is no noticeable relation between the nu-
245
accuracy (+)precision (+)recall (-)precision (-)recall
Cross validation 0.646 (291/452) 0.656 (138/214) 0.614 (138/226) 0.643 (153/238) 0.677 (153/226)
Partially closed 0.521 (583/1120) 0.277 (193/697) 0.854 (193/226) 0.922 (390/423) 0.436 (390/894)
Table 8: The accuracy of classification
without this feature set using only this feature set
feature set accuracy (+)precision (+)recall accuracy (+)precision (+)recall
(A) 0.638 0.638 (144/226) 0.639 (144/226) 0.521 0.541 (62/115) 0.277 (62/226)
(B) 0.634 0.649 (132/203) 0.584 (132/226) 0.563 0.549 (159/290) 0.705 (159/226)
(C) 0.579 0.576 (136/237) 0.604 (136/226) 0.610 0.620 (128/207) 0.570 (128/226)
(D) 0.645 0.654 (138/212) 0.615 (138/226) 0.523 0.679 (19/29) 0.087 (19/226)
(E) 0.629 0.666 (117/175) 0.518 (117/226) 0.492 0.491 (101/205) 0.447 (101/226)
Table 9: The evaluation result for each feature set
feature MI f(+) f(-)
Dmodifying agent=NIYOTTE 0.843 15 17
EPOS:POS after=VVN:NN 0.656 14 22
EPOS before=IN 0.536 10 19
EPOS before=JJ 0.530 12 23
Dmodified case=GA 0.428 13 29
Ngrandparent voice=passive 0.408 17 39
Ngrandchild voice=passive 0.368 14 34
EPOS=VVZ 0.368 14 34
Fsuffix=NARU 0.225 0 23
Ncase adjacency=GA:TO 0.225 0 12
Fsuffix=SHIMAU 0.225 0 16
EPOS=RB 0.225 0 10
EPOS:POS after=VVG:DT 0.225 0 10
EPOS:POS after=VVN:TO 0.179 2 42
EPOS:POS after=VVN:SENT 0.159 3 44
Dmodifying agent=NI 0.154 4 54
Table 7: Features which have high correlation with
positive and negative examples
meric features and the correct results. We introduced
most numeric features based on the study of read-
ability. In readability studies, however, these fea-
tures are defined in terms of the overall document,
and not in terms of individual sentences or of verb
phrases. It would be preferable to develop numer-
ical features that can properly reflect the nature of
individual sentences or smaller constructions.
Table 9 shows that the result when only using the
feature set D has a very low recall, but the highest
feature set (A)
EN#word 0.038
EN#pause -0.069
EN#verb -0.003
EN#VVN -0.061
EN#word len 0.033
feature set (E)
J#morpheme 0.083
J#pause 0.011
J#verb 0.056
J#passive 0.035
J#depth 0.098
Table 10: The correlation coefficient between each
feature and correct answer
precision of all the feature sets. This mean that there
are not many occasions on which the feature set D
can be applied, but when it is applied, the result is re-
liable. The feature set D thus is efficient as a trigger
once it is applied, and the different treatment of the
tokens that contain this feature set may contribute to
the performance improvement.
6.3 Diagnosis
The critical cases from the point of view of improv-
ing the performance are the false positives and false
negatives. We thus manually analysed the false pos-
itives and false negatives obtained in the partially
closed experiment (in the actual application envi-
ronment, as much training data as available should
be used; we thus used the results of the partially
246
closed experiment here). For the false positive, we
extracted 100 sample sentences from 504 sentences.
For the false negative we used all 33 sentences. We
asked two translators to judge whether (i) it would be
better to modify the draft translations or (ii) it would
not be necessary to modify the draft translations.
6.3.1 False positives
From the 100 sample sentences, we excluded 23
cases, 18 of which were judged as in need of mod-
ification by one of the translators and 5 of which
were judged as in need of modification by both of
the translators. We manually analysed the remaining
77 cases. Rather than the problems with the features
that we used, we identified the potential factors that
would contribute to the restriction of modification.
Three types of restricting factor were recognised:
1. The nature of individual verbs allows or re-
quires the passive voice. Within the data, three
subtypes were identified, i.e. (i) the use of the
passive is natural irrespective of context, as in ?
???? (consumed)? (48 cases); (ii) the use of
the passive is natural within certain fixed syn-
tactic patterns, as in ?X ????? Y (Y called
X)? (10 cases); and (iii) the passive is used as
part of a common collocation, as in ??????
?? (attacked by anxiety)? (2 cases);
2. The use of the active voice is blocked by selec-
tional restrictions, as in ???????? (a sedi-
ment made by ...)? (1 case); and
3. The structure of the sentence requires the pas-
sive, as in ??????????????????
????????????????????? (The
biggest companies were all companies making
cars, in which most of the oil was consumed)?
(16 cases).
Together they cover 73 cases (in 4 out of 77 cases
we could not identify the factor, and in 4 of the 73
cases two of the above factors were identified). It is
anticipated that the first type (60 cases; about 85%)
could be dealt with by introducing ?pulling? trig-
gers, i.e. using large corpora to identify the char-
acteristics of the use of voice for individual verbs,
in order to enable the system to judge the desirabil-
ity of given expressions vis-a`-vis the conventional
alternatives. To deal with the second type requires
a detailed semantic description of nouns, which is
difficult to achieve, though in some cases it could
be approximated by collocational tendencies. In
regards to the third type of false positive, we ex-
pected that the type of features used in the experi-
ment would have been sufficient to eliminate them,
but this was not the case. In fact, many of the fea-
tures require discourse level information, such as the
choice of subject within the flow of discourse, in or-
der to function properly, which we did not take into
account. Although high-performance discourse pro-
cessing is still in an embryonic stage, in the setting
of the present study the correspondence between key
information in English and that in Japanese could be
used to deal with this type of false positive.
6.3.2 False negatives
Here, it is necessary to find factors that would pro-
mote modification. Among the 33 false negatives, 4
were judged as not in need of modification by both
the translators. We thus examined the remaining 29
cases. In 13 cases, the verb was replaced by another
verb. Including these cases, we identified four basic
factors that are related to triggering modification:
1. The nature of the individual verbs strongly re-
quires the active voice, either independently or
within the particular context, as in ??????
???? (was asked by)? (9 cases);
2. The structure of the sentence is rendered rather
awkward by the use of passives, as in ?????
??????????????????? (a report
published in ...... by analysts)? (4 cases);
3. A given lexical collocation is unnatural or awk-
ward, as in ???????????????????
??????? (that all investments be screened
is collectively insisted)? (2 cases); and
4. A lexicalised collocation in the draft was sub-
tly awkward and there is a better collocation or
expression that fits the situation (14 cases).
Together they cover 26 cases. We could not iden-
tify features in 3 cases. As in false positives, the first,
second and fourth types (22 cases or about 85% are
fully covered by these three types) could be dealt
with by introducing ?pulling? triggers, using large
external corpora.
For the overall data, we would expect that around
85% of 388 (77% of 504 cases) false positives (330
247
cases) could be dealt with by introducing ?pulling?
triggers. If these false positives could be removed
completely, the precision would become well over
0.5 (193/(697-330)) and the ratio of notified cases
would become about one third ((697-330)/1120) of
the total relevant cases. Though it is unreasonable to
assume this ideal case, this indicates that the fea-
tures we defined and introduced in this study ?
though limited to those related to ?pushing? triggers
? were effective, and that what we have achieved
by using these features is very promising in terms
of realising a system that notifies users of awkward
translations.
7 Conclusions
In this paper, we examined the factors that trig-
ger modifications when translators are revising
draft translations, and identified computationally
tractable features relevant to the modification. We
carried out an experiment for automatic detection
of modification candidates. The result was highly
promising, though it revealed several issues that
need to be addressed further.
Following the results reported in this paper, we
are currently working on.
(i) extending the experiment by introducing out-
side data to carry out open experiments (we
have obtained draft and final translations of
three more books);
(ii) introducing the degree of necessity for modifi-
cations by asking translators to judge the data;
and
(iii) further examining the features used in the ex-
periment for the improvement of performance.
In addition, we are experimenting with a method for
making use of large-scale external corpora in order
to deal with ?pulling?-type triggers, with additional
features taken from large external corpora.
Acknowledgement
This research is partly supported by grant-in-aid (A)
17200018 ?Construction of online multilingual ref-
erence tools for aiding translators? by the Japan
Society for the Promotion of Sciences (JSPS), and
also by grant-in-aid from The HAKUHO FOUNDA-
TION, Tokyo.
References
Abekawa, T. and Kageura, K. 2007. A translation aid
system with a stratified lookup interface. In Proc. of
ACL 2007 Demos and Poster Sessions, p. 5?8.
Anzai, T. 1995. Eibun Hon?yaku Jutu (in Japanese).
Tokyo: Chikuma.
Barzilay, R. and McKeown, K. R. 2001. Extracting para-
phrases from a parallel corpus. In Proc. of ACL 2001,
p. 50-57.
Barzilay, R. and Lee, L. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence
alignment. In Proc. of HLT-NAACL 2003, p. 16-23.
Dolan, B. et. al. 2004. Unsupervised construction of
large paraphrase corpora: Exploiting massively paral-
lel news sources alignment. In Proc. of COLING 2004,
p. 350-356.
Fry, E. 1968. A readability formula that saves time.
Journal of Reading, 11, p. 513-516, 575-578.
Gunning, R. 1959. The Technique of Clear Writing.
New York: McGraw-Hill.
Haruno, M. and Yamazaki, T. 1996. High-performance
bilingual text alignment using statistical and dictionary
information. In Proc. of ACL 1996, p. 131-138.
Inui, K. and Fujita, A. 2004. A survey on paraphrase
generation and recognition. Journal of Natural Lan-
guage Processing, 11(5), p. 131-138.
Leggett, J. 2005. Half Gone. London: Portobello. [Ma-
suoka, K. et. al. trans. 2006. Peak Oil Panic. Tokyo:
Sakuhinsha.]
Nakamura, Y. 2003. Eiwa Hon?yaku no Genri Gihou (in
Japanese). Tokyo: Nichigai Associates.
Och, F. J. and Ney, H. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1), p. 19-51.
Quirk, C., Brocktt, C. and Dolan, W. B. 2004 Monolin-
gual machine translation for paraphrase generation. In
Proc. of EMNLP 2004, p. 142-149.
Schmid, H. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proc. of NeMLAP, p. 44-49.
Shinyama, Y. et. al. 2002. Automatic paraphrase acqui-
sition from news articles. In Proc. of HLT 2002, p.
40-46.
Sun, et. al. 2007. Detecting erroneous sentences using
automatically mined sequential patterns. In Proc. of
ACL 2007, p. 81-88.
Vapnik, V. N. 1995. The Nature of Statistical Learning
Theory. New York: Springer.
248
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 664?671,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Bilingual Terminology Mining ? Using Brain, not brawn comparable
corpora
E. Morin, B. Daille
Universit? de Nantes
LINA FRE CNRS 2729
2, rue de la Houssini?re
BP 92208
F-44322 Nantes Cedex 03
{morin-e,daille-b}@
univ-nantes.fr
K. Takeuchi
Okayama University
3-1-1, Tsushimanaka
Okayama-shi, Okayama,
700-8530, Japan
koichi@
cl.it.okayama-u.ac.jp
K. Kageura
Graduate School of Education
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku,
Tokyo, 113-0033, Japan
kyo@p.u-tokyo.ac.jp
Abstract
Current research in text mining favours the
quantity of texts over their quality. But for
bilingual terminology mining, and for many
language pairs, large comparable corpora
are not available. More importantly, as terms
are defined vis-?-vis a specific domain with
a restricted register, it is expected that the
quality rather than the quantity of the corpus
matters more in terminology mining. Our
hypothesis, therefore, is that the quality of
the corpus is more important than the quan-
tity and ensures the quality of the acquired
terminological resources. We show how im-
portant the type of discourse is as a charac-
teristic of the comparable corpus.
1 Introduction
Two main approaches exist for compiling corpora:
?Big is beautiful? or ?Insecurity in large collec-
tions?. Text mining research commonly adopts the
first approach and favors data quantity over qual-
ity. This is normally justified on the one hand by
the need for large amounts of data in order to make
use of statistic or stochastic methods (Manning and
Sch?tze, 1999), and on the other by the lack of oper-
ational methods to automatize the building of a cor-
pus answering to selected criteria, such as domain,
register, media, style or discourse.
For lexical alignment from comparable corpora,
good results on single words can be obtained from
large corpora ? several millions words ? the accu-
racy of proposed translation is about 80% for the top
10-20 candidates (Fung, 1998; Rapp, 1999; Chiao
and Zweigenbaum, 2002). (Cao and Li, 2002) have
achieved 91% accuracy for the top three candidates
using the Web as a comparable corpus. But for spe-
cific domains, and many pairs of languages, such
huge corpora are not available. More importantly,
as terms are defined vis-?-vis a specific domain with
a restricted register, it is expected that the quality
rather than the quantity of the corpus matters more in
terminology mining. For terminology mining, there-
fore, our hypothesis is that the quality of the corpora
is more important than the quantity and that this en-
sures the quality of the acquired terminological re-
sources.
Comparable corpora are ?sets of texts in different
languages, that are not translations of each other?
(Bowker and Pearson, 2002, p. 93). The term com-
parable is used to indicate that these texts share
some characteristics or features: topic, period, me-
dia, author, register (Biber, 1994), discourse... This
corpus comparability is discussed by lexical align-
ment researchers but never demonstrated: it is of-
ten reduced to a specific domain, such as the med-
ical (Chiao and Zweigenbaum, 2002) or financial
domains (Fung, 1998), or to a register, such as
newspaper articles (Fung, 1998). For terminology
664
mining, the comparability of the corpus should be
based on the domain or the sub-domaine, but also
on the type of discourse. Indeed, discourse acts
semantically upon the lexical units. For a defined
topic, some terms are specific to one discourse or
another. For example, for French, within the sub-
domain of obesity in the domain of medicine, we
find the term exc?s de poids (overweight) only in-
side texts sharing a popular science discourse, and
the synonym exc?s pond?ral (overweight) only in
scientific discourse. In order to evaluate how impor-
tant the discourse criterion is for building bilingual
terminological lists, we carried out experiments on
French-Japanese comparable corpora in the domain
of medicine, more precisely on the topic of diabetes
and nutrition, using texts collected from the Web and
manually selected and classified into two discourse
categories: one contains only scientific documents
and the other contains both scientific and popular
science documents.
We used a state-of-the-art multilingual terminol-
ogy mining chain composed of two term extraction
programs, one in each language, and an alignment
program. The term extraction programs are pub-
licly available and both extract multi-word terms
that are more precise and specific to a particular sci-
entific domain than single word terms. The align-
ment program makes use of the direct context-vector
approach (Fung, 1998; Peters and Picchi, 1998;
Rapp, 1999) slightly modified to handle both single-
and multi-word terms. We evaluated the candidate
translations of multi-word terms using a reference
list compiled from publicly available resources. We
found that taking discourse type into account re-
sulted in candidate translations of a better quality
even when the corpus size is reduced by half. Thus,
even using a state-of-the-art alignment method well-
known as data greedy, we reached the conclusion
that the quantity of data is not sufficient to obtain
a terminological list of high quality and that a real
comparability of corpora is required.
2 Multilingual terminology mining chain
Taking as input a comparable corpora, the multilin-
gual terminology chain outputs a list of single- and
multi-word candidate terms along with their candi-
date translations. Its architecture is summarized in
Figure 1 and comprises term extraction and align-
ment programs.
2.1 Term extraction programs
The terminology extraction programs are avail-
able for both French1 (Daille, 2003) and Japanese2
(Takeuchi et al, 2004). The terminological units
that are extracted are multi-word terms whose syn-
tactic patterns correspond either to a canonical or a
variation structure. The patterns are expressed us-
ing part-of-speech tags: for French, Brill?s POS tag-
ger3 and the FLEM lemmatiser4 are utilised, and for
Japanese, CHASEN5. For French, the main patterns
are N N, N Prep N et N Adj and for Japanese, N N,
N Suff, Adj N and Pref N. The variants handled are
morphological for both languages, syntactical only
for French, and compounding only for Japanese. We
consider as a morphological variant a morphological
modification of one of the components of the base
form, as a syntactical variant the insertion of another
word into the components of the base form, and as
a compounding variant the agglutination of another
word to one of the components of the base form. For
example, in French, the candidate MWT s?cr?tion
d?insuline (insulin secretion) appears in the follow-
ing forms:
  base form of N Prep N pattern: s?cr?tion
d?insuline (insulin secretion);
  inflexional variant: s?cr?tions d?insuline (in-
sulin secretions);
  syntactic variant (insertion inside the base
form of a modifier): s?cr?tion pancr?atique
d?insuline (pancreatic insulin secretion);
  syntactic variant (expansion coordination of
base form): secr?tion de peptide et d?insuline
(insulin and peptide secretion).
The MWT candidates secr?tion insulinique (insulin
secretion) and hypers?cr?tion insulinique (insulin
1http://www.sciences.univ-nantes.fr/
info/perso/permanents/daille/ and release
LINUX.
2http://research.nii.ac.jp/~koichi/
study/hotal/
3http://www.atilf.fr/winbrill/
4http://www.univ-nancy2.fr/pers/namer/
5http://chasen.org/$\sim$taku/software/
mecab/
665
WEB
dictionary
bilingual
Japanese documents French documents
terminology
extraction
terminology
extraction
lexical context
extraction
lexical context
extraction
process
translated
terms to be
translations
candidate
haversting
documents
lexical alignment
Figure 1: Architecture of the multilingual terminology mining chain
hypersecretion) have also been identified and lead
together with s?cr?tion d?insuline (insulin secretion)
to a cluster of semantically linked MWTs.
In Japanese, the MWT
 
. 
	
6 (in-
sulin secretion) appears in the following forms:
  base form of NN pattern:   /N  . 
	 /N  (insulin secretion);
  compounding variant (agglutination of a
word at the end of the base form):  

/N  . 	 /N  .  /N  (insulin secretion
ability)
At present, the Japanese term extraction program
does not cluster terms.
2.2 Term alignment
The lexical alignment program adapts the direct
context-vector approach proposed by (Fung, 1998)
for single-word terms (SWTs) to multi-word terms
(MWTs). It aligns source MWTs with target single
6For all Japanese examples, we explicitly segment the com-
pound into its component parts through the use of the ?.? sym-
bol.
words, SWTs or MWTs. From now on, we will refer
to lexical units as words, SWTs or MWTs.
2.2.1 Implementation of the direct
context-vector method
Our implementation of the direct context-vector
method consists of the following 4 steps:
1. We collect all the lexical units in the context of
each lexical unit  and count their occurrence
frequency in a window of  words around  .
For each lexical unit  of the source and the
target language, we obtain a context vector Proceedings of the ACL 2007 Demo and Poster Sessions, pages 5?8,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Translation Aid System with a Stratified Lookup Interface
Takeshi Abekawa and Kyo Kageura
Library and Information Science Course
Graduate School of Education,
University of Tokyo, Japan
{abekawa,kyo}@p.u-tokyo.ac.jp
Abstract
We are currently developing a translation
aid system specially designed for English-
to-Japanese volunteer translators working
mainly online. In this paper we introduce
the stratified reference lookup interface that
has been incorporated into the source text
area of the system, which distinguishes three
user awareness levels depending on the type
and nature of the reference unit. The dif-
ferent awareness levels are assigned to ref-
erence units from a variety of reference
sources, according to the criteria of ?com-
position?, ?difficulty?, ?speciality? and ?re-
source type?.
1 Introduction
A number of translation aid systems have been de-
veloped so far (Bowker, 2002; Gow, 2003). Some
systems such as TRADOS have proved useful for
some translators and translation companies1. How-
ever, volunteer (and in some case freelance) trans-
lators do not tend to use these systems (Fulford and
Zafra, 2004; Fulford, 2001; Kageura et al, 2006),
for a variety of reasons: most of them are too expen-
sive for volunteer translators2; the available func-
tions do not match the translators? needs and work
style; volunteer translators are under no pressure
from clients to use the system, etc. This does not
mean, however, that volunteer translators are satis-
fied with their working environment.
Against this backdrop, we are developing a trans-
lation aid system specially designed for English-to-
Japanese volunteer translators working mainly on-
line. This paper introduces the stratified reference
1http://www.trados.com/
2Omega-T, http://www.omegat.org/
lookup/notification interface that has been incorpo-
rated into the source text area of the system, which
distinguishes three user awareness levels depending
on the type and nature of the reference unit. We
show how awareness scores are given to the refer-
ence units and how these scores are reflected in the
way the reference units are displayed.
2 Background
2.1 Characteristics of target translators
Volunteer translators involved in translating English
online documents into Japanese have a variety of
backgrounds. Some are professional translators,
some are interested in the topic, some translate as a
part of their NGO activities, etc3. They nevertheless
share a few basic characteristics: (i) they are native
speakers of Japanese (the target language: TL); (ii)
most of them do not have a native-level command in
English (the source language: SL); (iii) they do not
use a translation aid system or MT; (iv) they want to
reduce the burden involved in the process of transla-
tion; (v) they spend a huge amount of time looking
up reference sources; (vi) the smallest basic unit of
translation is the paragraph and ?at a glance? read-
ability of the SL text is very important. A translation
aid system for these translators should provide en-
hanced and easy-to-use reference lookup functions
with quality reference sources. An important point
expressed by some translators is that they do not
want a system that makes decisions on their behalf;
they want the system to help them make decisions
by making it easier for them to access references.
Decision-making by translations in fact constitutes
an essential part of the translation process (Munday,
2001; Venuti, 2004).
3We carried out a questionnaire survey of 15 volunteer trans-
lators and interviewed 5 translators.
5
Some of these characteristics contrast with those
of professional translators, for instance, in Canada
or in the EU. They have native command in both
the source and target languages; they went through
university-level training in translation; many of them
have a speciality domain; they work on the principle
that ?time is money? 4. For this type of translator,
facilitating target text input can be important, as is
shown in the TransType system (Foster et al, 2002;
Macklovitch, 2006).
2.2 Reference units and lookup patterns
The major types of reference unit can be sum-
marised as follows (Kageura et al, 2006).
Ordinary words: Translators are mostly satisfied
with the information provided in existing dictionar-
ies. Looking up these references is not a huge bur-
den, though reducing it would be preferable.
Idioms and phrases: Translators are mostly sat-
isfied with the information provided in dictionaries.
However, the lookup process is onerous and many
translators worry about failing to recognise idioms
in SL texts (as they can often be interpreted liter-
ally), which may lead to mistranslations.
Technical terms: Translators are not satisfied
with the available reference resources 5; they tend
to search the Internet directly. Translators tend to be
concerned with failing to recognise technical terms.
Proper names: Translators are not satisfied with
the available reference resources. They worry more
about misidentifying the referent. For the identifica-
tion of the referent, they rely on the Internet.
3 The translation aid system: QRedit
3.1 System overview
The system we are developing, QRedit, has been de-
signed with the following policies: making it less
onerous for translators to do what they are currently
doing; providing information efficiently to facilitate
decision-making by translators; providing functions
in a manner that matches translators? behaviour.
QRedit operates on the client server model. It is
implemented by Java and run on Tomcat. Users ac-
4Personal communication with Professor Elliott
Macklovitch at the University of Montreal, Canada.
5With the advent of Wikipedia, this problem is gradually
becoming less important.
cess the system through Web browsers. The inte-
grated editor interface is divided into two main ar-
eas: the SL text area and the TL editing area. These
scroll synchronically. To enable translators to main-
tain their work rhythm, the keyboard cursor is al-
ways bound to the TL editing area (Abekawa and
Kageura, 2007).
3.2 Reference lookup functions
Reference lookup functions are activated when an
SL text is loaded. Relevant information (translation
candidates and related information) is displayed in
response to the user?s mouse action. In addition to
simple dictionary lookup, the system also provides
flexible multi-word unit lookup mechanisms. For
instance, it can automatically look up the dictionary
entry ?with one?s tongue in one?s cheek? for the ex-
pression ?He said that with his big fat tongue in his
big fat cheek? or ?head screwed on right? for ?head
screwed on wrong? (Kanehira et al, 2006).
The reference information can be displayed in two
ways: a simplified display in a small popup window
that shows only the translation candidates, and a full
display in a large window that shows the full refer-
ence information. The former is for quick reference
and the latter for in-depth examination.
Currently, Sanseido?s Grand Concise English-
Japanese Dictionary, Eijiro6, List of technical terms
in 23 domains, and Wikipedia are provided as refer-
ence sources.
4 Stratified reference lookup interface
In relation to reference lookup functions, the follow-
ing points are of utmost importance:
1. In the process of translation, translators often
check multiple reference resources and exam-
ine several meanings in SL and expressions in
TL. We define the provision of ?good informa-
tion? for the translator by the system as infor-
mation that the translator can use to make his
or her own decisions.
2. The system should show the range of avail-
able information in a manner that corresponds
to the translator?s reference lookup needs and
behaviour.
6http://www.eijiro.jp/
6
The reference lookup functions can be divided
into two kinds: (i) those that notify the user of the
existence of the reference unit, and (ii) those that
provide reference information. Even if a linguistic
unit is registered in reference sources, if the transla-
tor is unaware of its existence, (s)he will not look
up the reference, which may result in mistransla-
tion. It is therefore preferable for the system to no-
tify the user of the possible reference units. On the
other hand, the richer the reference sources become,
the greater the number of candidates for notification,
which would reduce the readability of SL texts dra-
matically. It was necessary to resolve this conflict
by striking an appropriate balance between the no-
tification function and user needs in both reference
lookup and the readability of the SL text.
4.1 Awareness levels
To resolve this conflict, we introduced three transla-
tor ?awareness levels?:
? Awareness level -2: Linguistic units that the
translator may not notice, which will lead to
mistranslation. The system always actively no-
tifies translators of the existence of this type of
unit, by underlining it. Idioms and complex
technical terms are natural candidates for this
awareness level.
? Awareness level -1: Linguistic units that trans-
lators may be vaguely aware of or may suspect
exist and would like to check. To enable the
user to check their existence easily, the rele-
vant units are displayed in bold when the user
moves the cursor over the relevant unit or its
constituent parts with the mouse. Compounds,
easy idioms and fixed expressions are candi-
dates for this level.
? Awareness level 0: Linguistic units that the
user can always identify. Single words and easy
compounds are candidates for this level.
In all these cases, the system displays reference in-
formation when the user clicks on the relevant unit
with the mouse.
4.2 Assignment of awareness levels
The awareness levels defined above are assigned to
the reference units on the basis of the following four
characteristics:
C(unit): The compositional nature of the unit.
Single words can always be identified in texts, so
the score 0 is assigned to them. The score -1 is as-
signed to compound units. The score -2 is assigned
to idioms and compound units with gaps.
D(unit): The difficulty of the linguistic unit for a
standard volunteer translator. For units in the list of
elementary expressions7, the score 1 is given. The
score 0 is assigned to words, phrases and idioms
listed in general dictionaries. The score -1 is as-
signed to units registered only in technical term lists.
S(unit): The degree of domain dependency of the
unit. The score -1 is assigned to units that belong to
the domain which is specified by the user. The score
0 is assigned to all the other units. The domain infor-
mation is extracted from the domain tags in ordinary
dictionaries and technical term lists. For Wikipedia
entries the category information is used.
R(unit): The type of reference source to which the
unit belongs. We distinguish between dictionaries
and encyclopaedia, corresponding to the user?s in-
formation search behaviour. The score -1 is assigned
to units which are registered in the encyclopaedia
(currently Wikipedia8 ), because the fact that fac-
tual information is registered in existing reference
sources implies that there is additional information
relating to these units which the translator might
benefit from knowing. The score 0 is assigned to
units in dictionaries and technical term lists.
The overall score A(unit) for the awareness level
of a linguistic unit is calculated by:
A(unit) = C(unit)+D(unit)+S(unit)+R(unit).
Table 1 shows the summary of awareness levels
and the scores of each characteristic. For instance, in
an the SL sentence ?The airplane took right off.?, the
C(take off) = ?2, D(take off) = 1, S(take off) =
0 and R(take off) = 0; hence A(take off) = ?1.
A score lower than -2 is normalised to -2, and a
score higher than 0 is normalised to 0, because we
assume three awareness levels are convenient for re-
alising the corresponding notification interface and
7This list consists of 1,654 idioms and phrases taken from
multiple sources for junior high school and high school level
English reference sources published in Japan.
8As the English Wikipedia has entries for a majority of or-
dinary words, we only assign the score -1 to proper names.
7
A(unit) : awareness level <= -2 -1 >= 0
Mode of alert always emphasis by mouse-over none
Score -2 -1 0 1
C(unit) : composition compound unit with gap compound unit single word
D(unit) : difficulty technical term general term elementary term
S(unit) : speciality specified domain general domain
R(unit) : resource type encyclopaedia dictionary
Table 1: Awareness levels and the scores of each characteristic
are optimal from the point of view of the user?s
search behaviour. We are currently examining user
customisation functions.
5 Conclusion
In this paper, we introduced a stratified reference
lookup interface within a translation aid environ-
ment specially designed for English-to-Japanese on-
line volunteer translators. We described the incorpo-
ration into the system of different ?awareness levels?
for linguistic units registered in multiple reference
sources in order to optimise the reference lookup in-
terface. The incorporation of these levels stemmed
from the basic understanding we arrived at after con-
sulting with actual translators that functions should
fit translators? actual behaviour. Although the effec-
tiveness of this interface is yet to be fully examined
in real-world situations, the basic concept should be
useful as the idea of awareness level comes from
feedback by monitors who used the first version of
the system.
Although in this paper we focused on the use
of established reference resources, we are currently
developing (i) a mechanism for recycling relevant
existing documents, (ii) dynamic lookup of proper
name transliteration on the Internet, and (iii) dy-
namic detection of translation candidates for com-
plex technical terms. How to fully integrate these
functions into the system is our next challenge.
References
Takeshi Abekawa and Kyo Kageura. 2007. Qredit:
An integrated editor system to support online volun-
teer translators. In Proceedings of Digital Humanities
2007 Poster/Demos.
Lynne Bowker. 2002. Computer-aided Translation Tech-
nology: A Practical Introduction. Ottawa: University
of Ottawa Press.
George Foster, Philippe Langlais, and Guy Lapalme.
2002. User-friendly text prediction for translators.
In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, pages 148?
155.
Heather Fulford and Joaqu??n Granell Zafra. 2004. The
uptake of online tools and web-based language re-
sources by freelance translators. In Proceedings of
the Second International Workshop on Language Re-
sources for Translation Work, Research and Training,
pages 37?44.
Heather Fulford. 2001. Translation tools: An ex-
ploratory study of their adoption by UK freelance
translators. Machine Translation, 16(3):219?232.
Francie Gow. 2003. Metrics for Evaluating Translation
Memory Software. PhD thesis, Ottawa: University of
Ottawa.
Kyo Kageura, Satoshi Sato, Koichi Takeuchi, Takehito
Utsuro, Keita Tsuji, and Teruo Koyama. 2006. Im-
proving the usability of language reference tools for
translators. In Proceedings of the 10th of Annual
Meeting of Japanese Natural Language Processing,
pages 707?710.
Kou Kanehira, Kazuki Hirao, Koichi Takeuchi, and Kyo
Kageura. 2006. Development of a flexible idiom
lookup system with variation rules. In Proceedings
of the 10th Annual Meeting of Japanese Natural Lan-
guage Processing, pages 711?714.
Elliott Macklovitch. 2006. Transtype2: the last word.
In Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC2006),
pages 167?172.
Jeremy Munday. 2001. Introducing Translation Studies:
Theories and Applications. London: Routledge.
Lawrence Venuti. 2004. The Translation Studies Reader.
London: Routledge, second edition.
8
Automatic Thesaurus Generation through Multiple Filtering 
Kyo KAGEURA t, Keita TSUJI*, and Akiko, N. AIZAWA * 
? National Inst itute of hffonnatics 
2-1.-2 Hitotsubashi, Chiyoda-ku, Tokyo, 101-8430 Japan 
E-Mail: {kyo,akiko} @nii.ac.jp 
tGraduate School of Education, University of Tokyo, 
7-3-1 Hongo, Bunkyo-ku, Tokyo, 113 Japan 
E-Mail: i34188{hn-unix.cc.u-tokyo.ac.jp 
Abstract; 
11, this paper, we propose a method of gen- 
(',rating bilingual keyword eh.lsters or thesauri 
from parallel or comi.m, able bilingual corpora. 
The method combines nmrphological nd lex- 
ical processing, bilingual word aligmnent, and 
graph-theoretic cluster generation. An experi- 
ment shows that the method is promising. 
1 In t roduct ion  
In this paper, we propose a method of auto- 
matte bilingual thesaurus generation by a com- 
bination of methods or multiple tiltering. The 
procedure consists of three modules: (i) a mor- 
phological and lexical processing module, (it) a 
translation pair extraction module, and (iii) a 
cluster generation module. The method takes 
parallel or comparable corpora as input, and 
produces as outlmt bilingual keyword clusters 
with a reasonable computational cost. 
Our aim is to construct domain-oriented 
bilingual thesauri, which are much in need both 
for cross-language IR and tbr technical tr~msla- 
tors. We assume domain-dependent parallel or 
comparM)le corpora as a source of inibrmation, 
which are. abundant in case of Japanese and En- 
glish. 
The techniques used in each module are 
reasonably well developed, including statistical 
word alignment methods. Itowever, there re- 
main at le.ast three problems: (i) ambiguity of 
multiple hNmx combinations ill an aligmnent, 
which cannot be resolved by purely statistical 
methods, (it) syntagmatie unit mismatches, es- 
pecially in such cases as English and Jal)anese, 
and (iii) difficulty ill final cleaning-up 1 .
In this paper, we show that the proper com- 
bination of the above modules can be useful es- 
pecially for resolving the cleaning-up roblem 
and can produce good results in bilingual ellis- 
ter or thesaurus generation. 
2 Method  
The procedure for thesaurus generation con- 
sists of the following three main nlodules. 
(1) Morphological and lexical processing mod- 
ule: keyword milts 2 for English and Japanese 
are extracted separately. 
(2) Translation pair extraction module: statis- 
tical weighting is applie.d to a corpus which has 
been through the morl}hological nd lexical pro- 
cessing module. The ailn of this stage is not to 
determine mdque translation pairs, but to re- 
strict translation candidates to a reasonable ex- 
tent. 
(3) Cleaning and cluster generation module: 
a bilingual keyword graph is constructed on 
the basis of" the pairs extracted at translation 
pair extraction module, and a graph-theoretic 
method is applied to the keyword graph, to gen- 
erate proper keyword clusters by removing er- 
roneous links. 
If we want to obtain a clean lexicon, minor trans- 
lation variations tend to be omitted, while many errors 
would be included if we want to retain minor variations. 
2 The word 'keyword' implies words that are impof  
tant with respect to documents or domains. In this pa- 
per, we use the word for convenience, roughly in the 
81une se~lse as "content-bearing words". If necessary, a
module of keyword or terin weighting (e.g. Fl'antzi $? 
Ananiadou 1995; Nakagawa & Mort 1998) can be incor- 
porated easily. 
397 
2.1 Morphological gz lexical processing 
At this stage, basic lexical units or keyword 
candidates are extracted. We separately extract 
minimum or shortest units and maxinnlm or 
longest complex units as syntagmatic units for 
keyword candidates. So two outputs are pro- 
duccd from this module, i.e. a bilingual key- 
word corpora of minimum units and another of 
maximum units. 
The processing proceeds as follows: 
(a) Morphological analysis 
First, the cortms is morphologically anal- 
ysed and POS-tagged. Currently, JUMAN3.5 
(Kurohashi ~z Nagao 1998) is used for Japanese 
and LT_POS/LT_CHUNK (Mikheev 1996) is 
used for English. 
(bl) Ext raet lon  of min imum units 
Minimum units in English are simply de- 
fined as non-flmctional simple words extracted 
from the output of LT_POS. Minimum mean- 
ingful units in Japanese are defined as: 
C_Pref ix* (C_AdvlC_AdjlN) C Suf f ix*  
where C_ indicates that the unit should consist 
of either Chinese characters or Katakana 3 . 
(b2) Extraction of maximum units 
Maximum complex units for English are the 
units extracted by LT_CHUNK, with some ad- 
hoc modifications. 
Maximum complex units fin' Japanese are 
defined by the following basic pattern, 
^C_Adj * (C_Affix l C_tdv l C_Adj \[ N) + 
where ^ C means that the unit should begin with 
either Chinese character or Katakana. The pat- 
tern remains deliberately coarse, to absorb er- 
rors by JUMAN. Coarse patterns with simple 
character type restrictions produce better re- 
sults than grammatically well-defined syntag- 
matic patterns. A separate stop word list for 
affixes is also prepared together with an excep- 
tional treatment routine, to make the Japanese 
units better corresl)ond to English units 4 . 
After these processes, two corpora, one con- 
sisting of minimum units and the other of max- 
3 In addition, we have made a few ad-hoc rules to 
screen out some consistent errors produced by the mor- 
phological analysers. 
4 For instance, the Japanese suffix 'Ill' is eliminated 
because it corresponds inmost cases to the English word 
'for', which tends to be excluded fi'om chunks made by 
LT_CHUNK. 
imum units, are created. 
Intermediate constituent units are not ex- 
tracted, because their inter-lingnal unit corre- 
spondence is less reliable. Also, many impor- 
tant intermediate units of longer complex units 
appear themselves as an independent complex 
unit in a large domain-specific corpus, and, even 
if they do not, intermediate units can be ex- 
tracted on the basis of minimum and maximum 
translation pairs if necessary. 
2.2 Extraction of translation candidates 
The module for extracting translation can- 
didate pairs consists of statistical weighting and 
postprocessing. These are applied to the data of 
nfinimum units and maximum units separately. 
After that, the two data are merged to make 
input for the cluster generation module. 
(a) Statistical weighting 
Many methods of extracting lexical transla- 
tion pairs have been proposed (Daille, Gaussier 
& Langd 1994; Eijk t993; Fung 1995; Gale ?~ 
Church 1991; Hiemstra 1996; ltull 1998; Ku- 
piec 1993; Melamed 1996; Smadja, McKeown & 
Hatzivmssiloglou 1996). Though it, is ditficult to 
evaluate the performance of existing methods as 
they use ditferent corpora for evaluation 5 , the 
performance does not seem to be radically dif- 
ferent. We adopted log-likelihood ratio (Dan- 
ning 1993), which gave the best pertbrmance 
among crude non-iterative methods in our test 
experiments 6 .
(b) Postproeessing f i lter 
As the output of statistical weighting is sim- 
ply a weighted list of all English and Japanese 
co-occurring pairs, it; is necessary to restrict 
translation candidates o that they can be ef- 
t~ctively used in the graph-theoretic cluster gen- 
eration module. In addition to restricting pos- 
sible translation pairs, it is necessary to deter- 
mine unique translation pairs for hapax legom- 
ena. We use both macro- and micro-filtering 
heuristics to restrict translation candidates. 
'~ A common testbed exists for French-English align- 
ment (Veronis 1996-99) but not for Japanese-English. 
6 At the time of writing this paper, we have finished a 
preliminary comparative xperiments of various meth- 
ods, among which the method proposed by Melamed 
(1996) gave by far the best result. We are thus plan- 
ning to replace this module with the method proposed 
by Melamed (1996). 
398 
Two macro heuristics, applied to the over- 
all list of pairs, are defined, i.e. (i) a proper 
translation should have a statistical score higher 
than the threshold Xs,  and (ii) a keyword 
should have maximal ly Xc translations or Xp  x 
token frequertcy when the frequency is less 
than Xc.  
Micro heuristics uses the information within 
each alignment; we assume that a keyword in 
one language only has one translation within 
an aligninent r . Selecting unique pairs in each 
al ignment is achieved by recursively taking a 
pair with tile highest score within an alignment, 
ead~ time deleting other pairs which have the 
same English or Japanese elements 8 . 
After this process, the data  of nl ininmm 
units and maximum units are merged, which 
constitutes input for the, next stage. 
2.3  Graph- theoret i c  c lus ter  generat ion  
Up to this stage, the cooccurrence inforlna- 
tion used to extract pairs has the depth of only 
one. In order to el iminate erroneous transla- 
tions, we re-organise the extracted pairs into 
graph and use multi-level topological informa- 
tion by applying tile graph-theoret ic method. 
For exi)lanation , let us assume that we obtained 
the data  in Table 1 fi'om the previous module 
~us an input to this module. 
Firstly, the initial ke!jword graph is con- 
structed, where each node represents an English 
or JaI)anese keyword, and a link or edge repre- 
se.nts the pairwise relation of corresponding key- 
words. W(' define the capacit~j or strength of a 
link by the frequency of occurrence of the pair 
in the corpus, i.e.. the nmnber of al ignments 
in which the pair occurs 9 . Figure 1 shows the 
This is not true for longer alignment units such as 
full texts. However, this will apply to parallel titles and 
abstracts which are readily available. Many lexical align- 
ment methods tarting fi'om sentence-levd aligmnents 
assume this or some variations of this. 
Many maximum unit pairs in fact have the same 
score. We used the arithmetic mean of the constituent 
minimum units to resolve aligmnent ambiguity. 
9 The score of likelihood ratio is a possible alternative 
for link eai)acity, but the result of a preliminary experi- 
ment was no better. In addition, after selecting pairs by 
threshold, whether a pair constitutes a proper transla- 
tion or not is not a matter of weight, because threshold 
setting implies that all pairs above that are regarded as 
correct. So we adopt simple frequency as the link ca- 
pacity. Itowever, we notice a lack of atfinity/)etween the 
Japanese keywords English keywords frequency 
- U -- b ~ information retrieval 1 
g- -- q -- b ? keyword 39 
5- --~ .7. b }~..~ information retrieval 1 
5- 4 ~ X b ~,.'~'~ text retrieval 6 
5- 4- X I- ~'~.~ text search 3 
J}~ >1 t~. ~rll/-~ iiii" keyword 1 
~ J."~.l'~ {~.~. "~"~ information retrieval 1 
'\]~i{fi}'~ information gathering 4 
'l'~ N ~  information retreival t 
i'~ ~ '~ information retrieval 320 
'1~ N~'5.'~ information search 5 
t~ ~f{l\[?{ ~JS inibrmation gathering 6 
' \] '~11~ information retrieval 1 
~i~t;~'~ bibliographic search 1 
~i}J~tt~ document retrieval 11 
~ ~: ~,~ "-.4~ document retrieval 19 
~ '~ text retrieval 1 
Table 1. Input exanlple for cluster generation 
iifitial keyword graph made from t, he data in 
Table 1. The task now is to detect and re- 
move erroneous links to generate independent 
graphs or clusters consisting of I)roper transla- 
tion pairs 1? . 
infornlalion galhcrillg O,ihlie,t, llqphic r,'uie~td) 
Figure I. Example of initial keyword graph 
The detect ion algorithm is based on the sim- 
ple principle that sets of links, which decompose 
a connected keyword cluster into disjoint sub- 
clusters when they are removed fronl tile origi- 
nal cluster, are candidates for improt)er transla- 
tions. In graph theory, such a link set is called 
an edge cut and the edge cut with the min- 
imal total  capacity is called a m, inimum edge 
cut. A min imum edge cut does not necessarily 
imply a single translat ion error. An efficient al- 
statistical alignment method we used here and the deft- 
nition of link cat)acity, which is currently under exami- 
nation and will be iml)roved by renewing the alignment 
module. 
m This approach is radically different fl'om statisti- 
cally oriented word clustering (Deerwester t. al 1990; 
Finch 1993; Grefenstette 1994; Schiitze & Pedersen 1997; 
Strzalkowski 1994); this is why we use the word '(:luster 
generation' instead of 'clustering'. 
399 
I~eworU t2~,f%1: kyewo~rd .~)/;/#o;'.','} 
i I ~, , /~  0 toxt search =F - ' / - -  4"~J " "~" z / 
, t , , , , , , ,~{D/  /:r~z, k t~? ~ . . . .  core  cl I~te 
v~.~ I (le'tl relR~i(ID ~Y'v"t 
g '~ l ,~t#t~' . .~ ; J * \ / \  \ x .  . ~',.J.Y~??;t,' 
(Ivld.'-,r?,kJ I ~lnfoll~latlon rotr~lva/ ? ) Id~.~u.lent 
inJorln(ttion . I~.  ~20~. \ .A~ 1 ~;'etrt~:val 
retrievtdl /V xl \~f~l ~\ 19 
r. / i , , ,~.~" , ~u~nl  rotfiova, 
x \  6 ~ifo a h . / , I 
I \ I thihho r.pldcTl~iz't~fl* :':-... "6../. \ ." ,  f :. infon'hzdiongalh'el~ng ~ \ ~/  ~ '\[~ 
~ / bibl~graphic sea r-ch 
'u!  
(a) (b) 
Figure 2. Steps of graph-theoretic cluster generation 
(c) 
gorithm exists for minimum edge cut detection 
(Nagamochi 1993). 
Our procedure first checks links that should 
not be eliminated, using the conditions: (i) the 
frequency is no less than Na, (ii) the Japanese 
and English notations are identical, or (iii) ei- 
ther of the Japanese or English expressions have 
only one corresponding translation (Figure 2 
(a); it is assumed that N~ = N/~ = Ne = 3). 
Secondly, core keywords whose fi'equency is no 
less than NZ are checked (Figure 2 (b)). This is 
used for the restriction that each cluster should 
include at le~t one core keyword. Lastly, edge 
cuts with a total capacity of less than Ne are 
detected and removed (Figure 2 (c)). This pro- 
cedure is repeated recursivety until no fllrther 
application is possible. Figure 3 shows the state 
after these steps are applied. 
. /  
/ 
! 
I .  
" ' "  . . . . . . . . . .  :L . . . . .  ~ . . . . . . . . . . .  2 . . . . . . . . .  ~'- 
Figure 3. Generated clusters 
3 Exper iment  
3.1 Settings and procedures 
We applied the method to Japanese and 
English bilingual parallel corpus consisting of 
25534 title pairs in the field of computer sci- 
ence. Table 2 shows the basic quantitative infor- 
mation after morphological nd lexical filtering 
was applied. 
Mininmm units 
Japanese Token: 178091 Type: 14938 
English Token: 154554 Type: 12634 
Maximum units 
Japanese Token: 89742 Type: 38813 
English Token: 80018 Type: 41693 
Table 2. Basic quantity of the data 
In the pair extraction module, the threshold 
Xs' was set to 1011 . The parameter X c w~s 
set to 10 and Xp to 0.5. As a result, 28905 
translation candidate pairs were obtained, with 
24855 Japanese and 23430 English keywords. 
Of these, 20071 pairs occurred only once and 
3581 only twice. The most frequent pair oc- 
curred 3196 times in the corpus. 8242 (28.5%) 
were minimum unit pairs, and 20663 (71.5%) 
were maximum unit pairs. 
Table 3 shows the number of keywords which 
had N translations. On average, a Japanese 
keyword had 1.16 English translations, while 
an English keyword had 1.23 Japanese trans- 
lations. 
N Jap. Eng. N ,lap. En. 
1 21796 19778 5 62 157 
2 2409 2693 6 10 59 
3 412 437 7 7 17 
4 159 285 8 0 4 
Table 3. Number of translations 
ix This is purely heuristic. Minimum units and maxi- 
mum units are given ditferent scores. But only 3 pairs 
below this threshold were proper translation pairs in 100 
random samples of minimum unit pairs, and 5 in 100 
samples of maxinmn~ units. 
400 
Evaluating recall and precision on the ba- 
sis of 100 randonfly selected title pairs, which 
consisted of 778 keyword token pairs, the pre- 
cision tokenwise was 84.06% (654 correct trans- 
lations) and the recall was 87.08% (654 of 751 
correct pairs). Typewise precision was 81.65% 
(543 correct of 665 pairs). 
The initial keyword graph generated fi'om 
these 28905 translation candidates consisted of 
19527 independent subgraphs, with the largest 
cluster containing 2701 pairs (i.e. 9.3% of all 
the pairs). The cluster generation method was 
applied with parameters Na =: 4, Ne = 10 
and N/~ = 1) 2 . As a result, 893 translation 
pairs were removed, and 20357 bilingual clus- 
ters were generated. The maximum cluster now 
contained only 64 pairs. Table 4 :shows the num- 
ber of clusters by size given by number of pairs. 
size no. of clusters size no. of (:lusters 
1 16693 5-9 322 
2 2354 10-19 52 
3 504 20-64 22 
4 410 
Table 4. Number of chlsters by size 
3.2 Overal l  evaluation 
The result was manually evaluated fi'om two 
points of view, i.e. consistency of clusters and 
cocrectness of link removal ~3 . 
(1) rib check the internal consistency, clusters 
were classified into three groul)S by size, and 
were separately evaluated. 2000 'small' clusters, 
consisting of only one pair, were randomly sam- 
pled and evaluated as 'correct' (c), 'more or less 
correct' (m) or ~wrong' (w). 4t}0 medimn size 
clusters consisting of 2-9 pairs and all the 74 
large clusters consisting of 10 or more pairs were 
evaluated as 'consistent' (c: consisting only of 
closely related keywords), 'mostly consistent' 
(Ill: consisting mostly of related keywords), 'hy- 
brid' (t1: consisting of two or more different key- 
word groups: 11) or q)ad' (w). Table 5 shows 
the result of the evaluation. The general per- 
formance is very good, with more or less 80% of 
the clusters being meaningflfl. 
12 This is again determined heuristically. For an exami- 
nation of the effect of parameters, ee Aizawa & Kageura 
(to apl)ear). 
~3 The evaluation was done by the first author. Cur- 
rently no cross-checking has been carried out. 
For small clusters, the performance was sep- 
arately evaluated for minimuln and maximuln 
refit pairs. Note that the ratio of maximum 
unit pairs is comparatively higher in the small 
cluster than the overall average. Most pairs 
ewfluated as partially correct, as well as some 
wrong pairs, suffered from mismatch of the syn- 
tagmatic units. 
c m w total 
Small 1389 370 241 2000 
(69.5%) (18.5%) (12.1%) (100%) 
milfimum 288 26 69 383 
(75.2%) (6.8%) (18.0%) 19.2% 
maximum 1101 344 172 1617 
(68.1%) (21.3%) (10.6%) 80.9% 
c m h w 
Medium 116 148 32 104 
(29.0%) (37.0%) (8.0%) (26.0%) 
Large 8 18 43 5 
(lo.8%) (24.3%) (58.1%) (&8%) 
Table 5. Evaluation of internal consistency 
73% of tile medium sized clusters were 'cor- 
reel), 'mostly correct' or 'hybrid'. Among the 
'lnostly con'ect' and 'hybrid' clusters, 97 (91 
and 6 respectively) were mainly caused by the 
mismatch of the units. For instance, in the 
case: { Kid, i~iN'fL, ~i~@, optimization, opti- 
mal, optimisation, optimum, network optimiza- 
tion }, the last English keyword has the excess 
unit 'network'. Other 'mostly correct' and 'hy- 
brid' chtsters were due to the l)roblem of corpus 
frequencies. 
Among the large clusters, more than half 
were qlybrid '14 . Among the hnostly correct' 
and qlybrid' large (;lusters, only 8 (3--t-5) were 
due to unit mismatch, while 53 (15+38) were 
due to quantitative factors. This shows a strik- 
ing contrast o the medium sized clusters. Large 
hybrid clusters tended to include lnany common 
word pairs which occur fi'equently. For instance, 
in the largest chlster, ' )  x ? .z, system' (3196), 
'lJ~l~} development' (1097), '~tki~\] design' (1073), 
and 'NiL enviromnent' (890) are included due 
to indirect associations. The tbllowing are two 
examples of hybrid clusters, whose hybridness 
comes fi:om quantitative factors and unit mis- 
matches respectively: 
Example  1: ~fg2:/~.{C6/~tJ/4) -x" ')/overview/outline/ 
summary/smmnarization/overall 
14 And most of the sub-clusters in these hybrid clusters 
are 'mostly correct'. 
401 
/pattern/patterns/patten/patterm matching 
In the first case, the 'overall' group and the 
'summary' group are mixed up. In the sec- 
ond case, the mismatch of syntagmatic units is 
caused by borrowed words. In fact, many errors 
caused by the mismatch of syntagmatic units in- 
volve borrowed words written in Katakana. 
(2) To look at the perfbrmance of graph- 
theoretic cluster generation, we exanfined the 
removed pairs fl'om two points of view, i.e. the 
correctness of link removal and the internal con- 
sistency of clusters generated by link remowfl. 
For the former, we introduced three categories 
for evaluation: mismatched pairs correctly re- 
moved (c), proper translation pairs wrongly re- 
moved (w), and pairs of related meaning re- 
moved (p). The consistency of newly generated 
clusters were evaluated in the same manner as 
above. 
c p w total 
cc 90 (10.1) 53 (5.9) 39 (4.4) 182 (20.4) 
cm 148 (16.6) 56 (6.6) 32 (3.6) 236 (26.4) 
ch 96 (10.8) 20 (2.2) 6 (0.7) 122 (13.7) 
mm 44 (4.9) 29 (3.3) 30 (3.4) 103 (tl.5) 
mh 52 (5.8) lS (1.5) 5 (0.6) 70 (7.8) 
hh 30 (3.4) 3 (0.3) 3 (0.3) 36 (4.0) 
xc 42 (4.7) 9 (1.0) 9 (1.0) 60 (6.7) 
xm 28 (3.t) 8 (0.9) 20 (2.2) 56 (6.3) 
xh 8 (0.9) 2 (0.3) 5 (0.6) 15 (1.7) 
xx 4 (0.5) 1 (0.1) 8 (0.9) 13 (1.5) 
all 542 (60.7) 194 (21.7) 157 (17.6) 893 (100) 
Table 6. Evaluation of removed links 
Table 6 shows the result of evaluation of all 
the 893 removed pairs. 'c' 'p' and 'w' in the top 
row indicate types of removed links, and 'cc', 
'cm' etc. in the leftmost column indicate inter- 
nal consistencies of two clusters generated by 
link removal. A total of 157 (17.6%) of the re- 
moved links were correct links wrongly removed, 
but among them, 115 links did not produce 
'bad' clusters. If we consider them to be toler- 
able, only 42 removals (4.7%) were fatal errors. 
By exanfining the renloved links, wc found 
that the links removed at the higher edge capac- 
ity included more wrongly removed pairs. For 
instance, among 142 edges removed at capacity 
4 (which is the maximum deletable value set by 
N,~), 41 or 28.9% were wrongly removed correct 
translations, while among 288 links removed at 
capacity l, only 15 or 5.2 % were correct trans- 
lations. 
4 Discuss ion 
From the experiment, we have found some 
factors that affect performance. 
(1) Many errors were produced at the stage of 
extracting keyword milts, by syntagmatic mis- 
match. A substantial nmnber of them involved 
Japanese Katakana keywords. Thereibre, in ad- 
dition to the general refinement of the morpho- 
logical processing module, the perfbrmance will 
be improved if we use string proxinfity informa- 
tion to determine syntagmatic units 15 . 
(2) We expect that some errors produced by 
statistical weighting and filtering could be re- 
moved by applying stemming and orthographic 
normalisations, which are not flflly exploited in 
the current implementation. Looking back from 
the cluster generation stage, frequently occur- 
ring keywords tend to cause problems due to 
indirect associations. At the time of writing, we 
are radically changing the statistical alignment 
module based on Melamed (1996) and incorpo- 
rating iterative alignment anchoring routine so 
that the method can be applied not only to titles 
but also to abstracts, etc. Used in conjunction 
with string proximity and stemming inforina- 
tion, we might be able to retain nfinor va.riations 
properly. 
(3) At the cluster generation stage, we observed 
that correct links tend to be wrongly removed 
for higher capacities of edge cut. In the cur- 
rent implementation, the parameter values re- 
main the same for all the clusters. Performance 
will be improved by introducing a method of 
dynamically changing the parameter w-dues ac- 
cording to the cluster size and the frequencies 
of their constituent pairs. 
5 Conclus ion 
We have proposed a method of constructing 
bilingual thesauri automatically, fl'om parallel 
or comparable corpora. The experiment showed 
that the performance is fairly good. We are cur- 
rently improving the method further, along the 
lines discussed in the previous ection. Further 
experiments are currently being carried out, us- 
ing the data of narrower domains (e.g. artificial 
ls This can also be used for resolving hapax ambiguity. 
402 
intelligence) as well as abstracts instead of ti- 
tles. 
At the next stag(.', we are 1)lanning to eval- 
uate the method fi'om the point of view of per- 
formance of generated clusters in practical ap- 
plications. We are currently planning to apply 
the generated clusters to query expansion and 
user navigation in cross-lingual Il ., as well as to 
on-line dictionary lookup systems used as trans- 
lation aids. 
Acknowledgement  
This research is a part of the research 
project "A Study on Ubiquitous Information 
Systems tbr Utilization of Highly Distributed 
Information FLesources", fimded by the Japan 
Society for the Promotion of Science. 
Re ferences  
\[1\] Aizawa, A. N. and Kageura, K. (to appear) "A 
grai)h-/)ased al)proach to the autoinatic gen- 
eration of multilingual keyword clusters." In: 
Bouligmflt, D., Jacquemin, C. and l'tIomme, 
M-C. (eds.) Recent Advances in Computational 
7~rminology. Amsterdam: John Benjanfins. 
\[2\] Dagan, I. and Church, K. (1994) "Termight: 
Identifying and translating technical terminol- 
ogy. " Prec. of the Fourth ANLP. p.34 40. 
\[3\] Daille, B., Gaussier, E. and Langd, J. M. (t994) 
"Towards automatic extraction of monolingual 
and bilingual terminology." COLING'9~. p. 
515-.521. 
\[4\] Deerwester, S., Dumais, S. T., Furnas, G. W., 
Landauer, T. K. and Harshman, R. (1990) "In- 
dexing by latent semantic analysis." JASIS. 
41(6), p. 391 407. 
\[5\] Dunning, T. (1993) "Accurate reel,hods for the 
statistics of surprise and coincidence." Compu- 
tational Lin.quistics. 19(1), p. 61 74. 
\[6\] Eijk, van der P. (1993) "Automating the acqui- 
sition of bilingual terminology." Prec. of the 6th 
EACL. p. 11.3-119. 
\[7\] Finch, S. P. (1993) Finding Structure in Lan- 
9ua.qe. PhD Thesis. Edinbourgh: University of 
Edinbourgh. 
\[8\] Frantzi, K. T. and Ananiadou, S. (1995) "Sta- 
tistical measures for terminological extraction." 
Proc. of 3rd Int'l Conf. on Statistical Analysis 
of Textual Data. p. 297-308. 
\[9\] Fung, P. (t995) "A t)attcrn matching method 
for finding noun and proper noun translations 
fi'om noisy parallel cort)ora.." Proe. of 33rd 
A CL. p. 233 236. 
\[10\] Gale, W. A. and Church, K. W. (1991.) "Idem 
tifying word correspondences in parallel texts." 
Proc. of DARPA &~eech. and Natural Lan.quwe 
Workshop. p. 152-157. 
\[11\] Grefenstette, G. (1994) Explorations in Auto- 
matic Thesaurus Discovery. Boston: Kluwer 
Academic. 
\[12\] tfiemstra, D. (1996) Using Statistical Methods 
to Creat a Bilingual Dictionary. MSc Thesis, 
Twcnte University. 
\[13\] Ifull, D. A. (1998) "A practical approach to ter- 
minology aligmnent." Computerm'98. p. 1---7. 
\[14\] Kitamura, M. and Matsumoto, Y. (1997) "Au- 
tomatic Extraction of Translation Patterns in 
Parallel Corpora." Transactions of IPSJ. 38(4), 
p. 727- 735. 
\[15\] Kupiec, J. (1993) "An algorithm for finding 
noun phrase correspondences in bilingual col  
pora." 15"oc. of 31st ACL. p.17--22. 
\[16\] Kurohashi, S. and Nagao, M. (1998) Japanese 
Morphological Analysis System .luman versioT~ 
3.5 User's Mawaal. Kyoto: Kyoto University." 
\[17\] Melamed, I. D. (1996) "Automatic onstruction 
of clean broad-coverage translation lexicons." 
2nd Conference of the Association for Mach, ine 
Translation in the Americas. p. 125-134. 
\[18\] Mikheev, A. (1996) '%earning pro:t-of-speech 
guessing rules from lexicon." COLING'96, p. 
770-775. 
\[19\] Nagamochi, H. (1993) "Minimum cut, in a 
graph." In: Fujisige, S. (ed.) Discrete Struc- 
ture and Algorithms H (Chapter 4). Tokyo: 
Kindaikagakusha. 
\[20\] Nal~Gawa , H. and Mori, T. (1998) "Nested col 
location and COml)ound noun for term extrac- 
tion." Computerm'98. p 64 70. 
\[21\] Sch{itze, It. and Pedersen, J.O. (1997) "A 
cooccurrence-based thesaurus and two appli- 
cations to information retrieval." Information 
Processing and Management. 33(3), I).307-318. 
\[22\] Smadja, F., MeKeown, K. R. and Hatzivas- 
siloglou, V. (1996) "Translating collocations 
for bilingual exicons: A statistical apt)roach." 
Computational Linguistics. 22(1), p. \]-38. 
\[23\] Strzalkowski, T. (1994) "Building a lexicM do- 
main map from text corpora." COLING'94, 
t).604-610. 
\[24\] Veronis, J. (1996-) "ARCADE: Evaluation of 
parallel text alignment systems." 
ht tl)://www.lpl.univ-aix.fi'/projects/arcade/ 
\[25\] Yonezawa, K. and Matsumoto, Y. (1998) 
"Zoshinteki taiouzuke ni yoru taiyaku tekisuto 
lmra no hol?yaku hyougen o cyusyutu." Proe 
of the \]#h, Annual Meeting of th.e Association 
for NLP. p. 576-579. 
403 
Implicit Ambiguity Resolution Using Incremental Clustering in 
Korean-to-English Cross-Language Information Retrieval 
 
Kyung-Soon Lee1, Kyo Kageura1, Key-Sun Choi2 
1 NII (National Institute of Informatics) 
2-1-2 Hitotsubashi, Chiyoda-ku, 
Tokyo, 101-8430, Japan 
{kslee, kyo}@nii.ac.jp 
2 Division of Computer Science, KAIST 
373-1 Kusung Yusong 
Daejeon, 305-701, Korea 
kschoi@cs.kaist.ac.kr 
 
Abstract  
This paper presents a method to implicitly 
resolve ambiguities using dynamic 
incremental clustering in Korean-to-English 
cross-language information retrieval. In the 
framework we propose, a query in Korean is 
first translated into English by looking up 
Korean-English dictionary, then documents 
are retrieved based on the vector space 
retrieval for the translated query terms. For 
the top-ranked retrieved documents, 
query-oriented document clusters are 
incrementally created and the weight of each 
retrieved document is re-calculated by using 
clusters. In experiment on TREC-6 CLIR 
test collection, our method achieved 28.29% 
performance improvement for translated 
queries without ambiguity resolution for 
queries. This corresponds to 97.27% of the 
monolingual performance for original 
queries. When we combine our method with 
query ambiguity resolution, our method 
even outperforms the monolingual retrieval. 
1 Introduction 
This paper describes a method of applying 
dynamic incremental clustering to the implicit 
resolution of query ambiguities in 
Korean-to-English cross-language information 
retrieval. The method uses the clusters of 
retrieved documents as a context for 
re-weighting each retrieved document and for 
re-ranking the retrieved documents. 
Cross-language information retrieval (CLIR) 
enables users to retrieve documents written in a 
language different from a query language. The 
methods used in CLIR fall into two categories:  
statistical approaches and translation approaches. 
Statistical methods establish cross-lingual 
associations without language translation 
(Dumais et al 1997; Rehder et al 1997; Yang et 
al, 1998). They require large-scale bilingual 
corpora. In translation approach, either queries 
or documents are translated. Though document 
translation is possible when high quality 
machine translation systems are available (Kwon 
et al 1997; Oard and Hackett, 1997), it is not 
very practical. Query translation methods (Hull 
and Grefenstette, 1996; Davis, 1996; Eichmann 
et al 1998; Yang et al 1998; Jang et al 1999; 
Chun, 2000) based on bilingual dictionaries, 
multilingual ontology or thesaurus are much 
more practical. Many researches adopt 
dictionary-based query translation because it is 
simpler and practical, given the wide availability 
of bilingual or multilingual dictionaries. In order 
to achieve a high performance CLIR using 
dictionary-based query translation, however, it is 
necessary to solve the problem of increased 
ambiguities of query terms. One way of 
resolving query ambiguities is to use the 
statistics, such as mutual information (Church 
and Hanks, 1990), to measure associations of 
query terms, on the basis of existing corpora 
(Jang et al 1999). 
Document clusters, widely adopted in various 
applications such as browsing and viewing of 
document results (Hearst and Pedersen, 1996) or 
topic detection (Allan et al 1998), also reflect 
the association of terms and documents. Lee et 
al (2001) showed that incorporating a document 
re-ranking method based on document clusters 
into the vector space retrieval achieved the 
significant improvement in monolingual IR, as it 
contributed to resolving ambiguities caused by 
polysemous query terms. 
The noise or ambiguity produced by 
dictionary-based query translation in CLIR is 
much larger than the polysemous ambiguities in 
monolingual IR. For example, a Korean term 
???[eun-haeng]? is a polysemous term with 
two meanings: ?bank? and ?ginkgo?. The English 
term ?bank? itself is polysemous, so the 
translated query ends up having magnified 
ambiguities. We will show that the method we 
propose, i.e. implicit ambiguity resolution using 
incremental clustering, is highly effective in 
dealing with the increased query ambiguities in 
CLIR. 
2 Implicit ambiguity resolution using 
incremental clustering 
Figure 1 shows the overall architecture of our 
system which incorporates implicit ambiguity 
resolution method based on query-oriented 
document clusters. In the system, a query in 
Korean is first translated into English by looking 
up dictionaries, and documents are retrieved 
based on the vector space retrieval for the 
translated query. For the top-ranked retrieved 
documents, document clusters are incrementally 
created and the weight of each retrieved 
document is re-calculated by using clusters with 
preference. This phase is the core of our implicit 
ambiguity resolution method. Below, we will 
describe each module in the system. 
2.1 Dictionary-based query translation and 
ambiguities 
Queries are written in natural language in 
Korean. We first apply morphological analysis 
and part-of-speech (POS) tagging to a query, 
and select keywords based on the POS 
information. For each keyword, we look up 
Korean-English dictionaries, and all the English 
translations in the dictionaries are chosen as 
query terms. We used a general-purpose 
bilingual dictionary and technical bilingual 
dictionaries (Chun, 2000). All in all, they have 
282,511 Korean entries and 505,003 English 
translations. 
Since a term can have multiple translations, 
the list of translated query terms can contain 
terms of different meanings as well as synonyms. 
While synonyms can improve retrieval 
effectiveness, terms with different meanings 
produced from the same original term can 
degrade retrieval performance tremendously. 
At this stage, we can apply statistical 
ambiguity resolution method based on mutual 
information. In the experiment below, we will 
examine two cases, i.e. with and without 
ambiguity resolution at this stage. 
2.2 Document retrieval based on vector space 
retrieval model 
For the query, documents are retrieved based on 
the vector space retrieval method. This method 
simply checks the existence of query terms, and 
calculates similarities between the query and 
documents. The query-document similarity of 
each document is calculated by vector inner 
product of the query and document vectors: 
di
t
i
qi wwdqsimD ?= ?
=1
),(              (1) 
where query and document weight, qiw and diw , 
are calculated by ntc-ltn weighting scheme  
which yields the best retrieval result in Lee et al
(2001) among several weighting schemes used 
in SMART system (Salton, 1989). 
As the translated query can contain noises, 
non-relevant documents may have higher ranks 
than relevant documents. 
Figure 1. System architecture of implicit 
ambiguity resolution by incremental clustering. 
English Query with ambiguities
TREC AP-news collection
Korean-English 
Dictionaries
Korean Query
retrieved top N docs
Dictionary-based
Query Translation
Vector Space Retrieval
Each  document view
re-ranked results
?
Incremental Clusters
Document context view
Reflecting context
to each document 
2.3 Query-oriented incremental clustering for 
implicit ambiguity resolution 
In order to exclude non-relevant documents 
from higher ranks, we take top N documents to 
create clusters incrementally and dynamically, 
and use similarities between the clusters and the 
query to re-rank the documents. Basic idea is: 
Each cluster created by clustering of retrieved 
documents can be seen as giving a context of the 
documents belonging to the cluster; by 
calculating the similarity between each cluster 
and the query, therefore, we can spot the 
relevant context of the query; documents that 
belong to more relevant context or cluster are 
likely to be relevant to the query. 
It should be noted here that the static global 
clustering is not practical in the current setup, 
because it takes much computational time and 
the document space is too sparse (see Anick and 
Vaithyanathan (1997) for the comparison of 
static and dynamic clustering). 
2.3.1 Dynamic incremental centroid clustering 
We make clusters based on incremental centroid 
method. There are a few variations in the 
agglomerative clustering method. The 
agglomerative centroid method joins the pair of 
clusters with the most similar centroid at each 
stage (Frakes and Baeza-Yates, 1992).  
Incremental centroid clustering method is 
straightforward. The input document of 
incremental clustering proceeds according to the 
ranks of the top-ranked N documents resulted 
from vector space retrieval for a query. 
Document and cluster centroid are represented 
in vectors. For the first input document (rank 1), 
create one cluster whose member is itself. For 
each consecutive document (rank 2, ..., N), 
compute cosine similarity between the document 
and each cluster centroid in the already created 
clusters. If the similarity between the document 
and a cluster is above a threshold, then add the 
document to the cluster as a member and update 
cluster centroid. Otherwise, create a new cluster 
with this document. Note that one document can 
be a member of several clusters as shown in 
Figure 2 (sold lines show that the document 
belongs to the cluster). 
2.3.2 Cluster preference 
Similarities between the clusters and the query, 
or query-cluster similarities, are calculated by 
the combination of the query inclusion ratio and 
vector inner product between the query vector 
and the centroid vectors of the clusters. 
ci
t
i
qi
q wwq
ccqsimC ??= ?
=1
),(          (2) 
where |q| is the number of terms in the query, 
|cq| is the number of query terms included in a 
cluster centroid, |cq|/|q| is the query inclusion 
ratio for the cluster. The documents included in 
the same cluster have the same query-cluster 
similarity. 
Cluster preferences are influenced by the 
query inclusion ratio, which prefers the cluster 
whose centroid includes more various query 
terms. Thus incorporating this information into 
the weighting of each document means adding 
information which is related to the behavior of 
terms in documents as well as the association of 
terms and documents into the evaluation of the 
relevance of each document; it therefore has the 
effect of ambiguity resolution. 
2.4 Reflecting cluster information to the 
documents 
Using the query-cluster similarity, we 
re-calculate the relevance of each document 
according to the following equation: 
 
),(),(),( cqsimCMAXdqsimDdqsim cd??= (3) 
 
where simD(q,d) is a query-document similarity 
by vector space retrieval as defined in equation 
(1) and simC(q,c) is a query-cluster similarity of 
a document d defined in equation (2). Since each 
document can be a member of several clusters, 
we assign the highest query-cluster similarity 
value to the document. The new document 
similarity, sim(q,d), is calculated by 
multiplication of a query-cluster similarity and a 
query-document similarity. Based on this new 
Figure 2. Incremental centroid clustering in order 
of the top-ranked N documents 
 
similarity sim(q,d), we re-rank the retrieved 
documents. In the equation, we tried to use 
weighted sum of a query-document similarity 
and a query-cluster similarity. The combination 
by multiplication showed better performances 
than that of weighted sum. 
Through this procedure, we can effectively 
take into account the contexts of all the terms in 
a document as well as of the query terms. Thus, 
even if a document which has a low 
query-document similarity can have a high 
query-cluster similarity thanks to the effect of 
neighboring documents in the same cluster. The 
reverse can be true as well. 
3 Experiments  
3.1 Experimental environment 
We evaluated our method on TREC-6 CLIR test 
collection which contains 242,918 English 
documents (AP news from 1988 to 1990) and 24 
English queries. English queries are translated to 
Korean queries manually. We use title field of 
queries which consist of three fields such as title, 
description and narrative. 
In dictionary-based query translation, one 
query term has multiple translations. Table 3 
shows the degree of ambiguities. 
The number of Korean query terms 47 
The number of translated terms 149 
The average number of translations 3.2 
Table 1. The degree of ambiguities for 24 queries. 
In our experiment, we only use 14 queries 
which consist of more than one term to observe 
real effects of our method. This is because, if a 
query consists of more than one term, human 
can select the correct meaning of the term by its 
neighbours. But if a query consists of one term 
such as ?bank? and it is polysemous, no one can 
resolve ambiguities without considering 
additional external information. The rest 10 
queries which consist of one term are used to 
decide a threshold in incremental clustering. 
We use SMART system (Salton, 1989) 
developed at Cornell as a vector space retrieval. 
3.2 Results 
The retrieval effectiveness was evaluated using 
the 11-point average precision metric. 
We compared our method with original 
English queries, with translated queries with 
ambiguities, and with translated queries with the 
best translation after disambiguation. The 
followings are the brief descriptions for 
comparison methods: 
1) monolingual: the performance of vector 
space retrieval system for original English 
queries as the monolingual baseline. 
2) tall_base: the performance of vector space 
retrieval system for translated English 
queries which have all possible translations 
in bilingual dictionaries without ambiguity 
resolution. 
3) tall_rerank: the performance of proposed 
method using dynamic incremental clusters 
for the retrieved documents of tall_base. 
4) tone_base: the performance of vector space 
retrieval system for translated queries with 
the best translations for each query term 
after ambiguity resolution based on mutual 
information. 
5) tone_rerank: the performance of proposed 
method using dynamic incremental clusters 
for the retrieved documents of tone_base. 
 
?tall_rerank? and ?tone_rerank? use our 
implicit disambiguation method. The number of 
top N documents used in dynamic incremental 
clustering is 300 and thresholds for incremental 
centroid clustering are set as 0.41 which are 
learned from training 10 queries with one term 
in both tall_rerank and tone_rerank. 
The main objective of this paper is to observe 
the performance change by incremental clusters 
for translated queries with ambiguities (tall_base 
and tall_rerank). 
 
Comparison 11-pt avg. C/M Change 
 precision (%) (%) 
1) monolingual 0.2858 100 - 
2) tall_base 0.2167 75.82 - 
3) tall_rerank 0.2780 97.27 +28.29 
4) tone_base 0.2559 89.54 - 
5) tone_rerank 0.3026 105.87 +18.25 
Table 2. The retrieval effectiveness for comparison 
methods. 
To observe the effect of clusters, we 
compared the results after disambiguation based 
on mutual information (tone_base and 
tone_rerank). We selected the best translation 
based on mutual information among all 
translation terms. Mutual information MI(x,y) is 
defined as following (Church and Hanks, 1990): 
)()(
),(log)()(
),(log),( 22 yfxf
yxfN
ypxp
yxpyxMI ?==  (4) 
where f(x) and f(y) are frequency of term x and  
term y, respectively. Co-occurrence frequency of 
term x and term y, f(x,y), is taken in window size 
6 for AP 1988 news documents. 
 
The 11-point average precision value, 
corresponding result to monolingual (C/M), and 
performance change are summarized in Table 2. 
The retrieval effectiveness of tall_rerank is 
0.2780, corresponding to 97.27% of 
monolingual performance. The performance of 
tone_rerank yields 0.3026 (105.87%). This is 
even better than the monolingual performance. 
The performance of our implicit ambiguity 
resolution method for all translations 
(tall_rerank) shows 8.63% improvement 
compared with that of ambiguity resolution 
based on mutual information (tone_base). The 
proposed method achieved 28% improvement 
for all translation queries and 18% for best 
translation queries compared with the vector 
space retrieval.  Our method after 
disambiguation (tone_rerank) using mutual 
information improved about 39.6% over vector 
space retrieval for all translations queries 
(tall_base). 
The cluster-based implicit disambiguation 
method, therefore, is more effective for 
performance improvement than the simple query 
disambiguation method based on mutual 
information; if used together, it shows yet 
further improvement. 
3.3 Result analysis 
We examined the effects of our method for a 
query with ambiguities increased after bilingual 
dictionary-based term translation. 
The Korean query is ????[ja-dong-cha] 
??[gong-gi] ??[o-yeom]? whose original 
English query is ?automobile air pollution?. The 
translated query with all the possible translations 
in Korean-English dictionaries for this query is 
as follows: 
In this query, the term ???? is polysemous 
which has several meanings such as <air>, 
<atmosphere>, <jackstone>, <co-occurrence>, 
and <bowl>. This is the cause of degrading 
system performance. 
 
146 clusters were created for the retrieved 300 
documents of this query. The token number of 
documents in the clusters was 435. The 
distribution of cluster members is shown in 
Figure 3. Most non-relevant documents had a 
tendency to make singleton cluster, and most 
relevant documents made large group clusters. 
 
We examined inside the clusters how to see 
cluster give effects to resolve ambiguity and 
reflect context. Cluster C4 in Figure 3 has 60 
members, which contains 56 relevant documents 
and 4 non-relevant documents, among 209 
relevant documents for this query. This cluster 
centroid includes following terms related to the 
query: 
car 0.069 
automobile 0.127 
air 0.082 
atmosphere 0.018 
pollution 0.196 
contamination 0.064 
 
??? 
[ja-dong-cha] 
car, automobile, autocar, 
motorcar 
?? 
[gong-gi] 
air, atmosphere, empty vessel, 
bowl, jackstone, pebble, marbles 
??[o-yeom]  contamination, pollution 
C2
C4 C17
0
10
20
30
40
50
60
70
80
90
100
0 20 40 60 80 100 120 140
cluster ID
# o
f m
em
ber
# o
f m
em
ber
Figure 3. The distribution of cluster members 
for the query with translation ambiguities. 
Although this centroid includes a noise term 
?atmosphere?, its weight is low. The other terms 
are appropriate to the query; they are synonyms. 
Since all of the query terms are included in the 
centroid, query inclusion ratio is 1 and all 
synonyms affect positively to the vector inner 
product value. Therefore, since this cluster 
preference is high, the ranks of all documents in 
this cluster changed higher. The cluster 
performed as a context of the documents 
relevant to the query. Cluster C85 is a singleton 
whose centroid includes one of three query 
terms: 
bowl 0.101 
marble 0.191 
Since query inclusion ratio is low, the cluster 
preference is low. Therefore this cluster?s effect 
is weak to the document. 
 
Figure 4 presents the rank changes, calculated 
by subtracting ranks by our method (tall_rerank) 
from those by vector space retrieval (tall_base) 
for each relevant document of the ambiguous 
query. The ranks of most documents are 
changed higher through cluster analysis, 
although the ranks of some documents are 
changed lower. Figure 5 shows recall/precision 
curves for the performances of original English 
query (monolingual; 0.6783 in 11-pt avg. 
precision), translated query without 
disambiguation (tall_base; 0.5635), and our 
method (tall_rerank; 0.6622). For increased 
query ambiguity, we could achieve 97.62% 
performance compared to the monolingual 
retrieval.  
These results indicate that cluster analysis 
help to resolve ambiguity. Thus, we could 
effectively take into account the context of all 
the terms in a document as well as the query 
terms. 
4 Conclusion 
We have proposed the method of applying 
dynamic incremental clustering to the implicit 
resolution of query ambiguities in 
Korean-to-English cross-language information 
retrieval. The method used the clusters of 
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.00.10.20.30.40.50.60.70.80.91.0
recall
pr
ec
isi
on
monolingual
tall_base
tall_rerank
 Figure 5. The performance comparison for the ambiguous query. 
Ra
nk
 ch
an
ge
s  
(ra
nk
 of
 ta
ll_
ba
se
  ?
ran
k o
f t
all
_re
ran
k )
-120
-80
-40
0
40
80
120
160
1 1
7 7
1 9
1 8
8
2 9
9 9
9
5 4
7 1
1
6 0
4 0
7
7 2
6 8
3
9 1
4 2
5
1 0
0 6
2 9
1 1
0 8
2 2
1 1
8 1
0 2
1 2
4 9
9 5
1 2
8 7
4 9
1 3
3 2
5 4
1 3
9 7
7 0
1 4
2 5
8 4
1 5
2 7
6 2
1 6
9 3
2 3
1 7
4 0
9 4
1 7
7 5
7 2
1 7
8 6
9 2
1 7
9 7
1 6
1 8
2 6
5 9
1 8
6 1
1 5
1 9
6 0
7 7
1 9
6 9
8 5
2 0
7 8
1 2
2 2
4 2
5 6
2 2
7 4
6 4
2 2
9 4
7 5
2 3
3 7
0 8
Relevant document ID for the query
Ra
nk
 ch
an
ge
s  
(ra
nk
 of
 ta
ll_
ba
se
  ?
ran
k o
f t
all
_re
ran
k )
 
Figure 4. The rank changes of tall_rerank from rank of tall_base for each relevant document of the query.  
retrieved documents as a context for 
re-weighting each retrieved document and for 
re-ranking the retrieved documents. 
Our method was evaluated on TREC-6 CLIR 
test collection. This method achieved 28.29% 
performance improvement for translated queries 
without ambiguity resolution. This corresponds 
to 97.27% of the monolingual performance. 
When our method was used with the query 
ambiguity resolution method based on mutual 
information, it showed 105.87% performance 
improvement of the monolingual retrieval. 
These results indicate that cluster analysis help 
to resolve ambiguity greatly, and each cluster 
itself provide a context for a query. 
Our method is a language independent model 
which can be applied to any language retrieval. 
We expect that our method will further 
improve the results, although further research is 
needed on combining a method to improve recall 
such as query expansion and relevance feedback. 
 
References 
Allan, J. Carbonell, J., Doddington, G. Yamron. J. 
and Yang, Y. (1998) Topic Detection and Tracking 
Pilot Study: Final Report. In Proc. of the DARPA 
Broadcast News Transcription and Understanding 
Workshop, pp.194-218. 
Anick, P.G. and Vaithyanathan, S. (1997) Exploiting 
Clustering and Phrases for Context-Based 
Information Retrieval. In Proc. of 20th ACM 
SIGIR Conference (SIGIR?97). 
Chun, J.H. (2000) Resolving Ambiguity and English 
Query Supplement using Parallel Corpora on 
Korean-English CLIR system. MS thesis, Dept. of 
Computer Science, KAIST (in Korean). 
Church, K.W. and Hanks P. (1990) Word Association 
Norms Mutual Information and Lexicography. 
Computational Linguistics, 16(1), pp.23-29.` 
Davis, M. (1996) New experiments in cross-language 
text retrieval at NMSU's computing research lab. In 
Proc. of the fifth Text Retrieval Conference 
(TREC-5). 
Dumais, S.T., Letsche, T.A., Littman, M.L. and 
Landauer, T.K. (1997) Automatic cross-language 
retrieval using latent semantic indexing. In Proc. of 
AAAI Symposium on Cross-Language Text and 
Speech Retrieval. 
Eichmann, D., Ruiz, M.E. and Srinivasan, P. (1998) 
Cross-Language Information Retrieval with the 
UMLS Metathesaurus. . In Proc. of the 21th ACM 
SIGIR Conference (SIGIR?98). 
Frakes, W.B., and Baeza-Yates, R. (1992) 
Information Retrieval: data structures & algorithms. 
New Jersey: Prentice Hall, pp.435-436. 
Gilarranz, J., Gonzalo, J. and Verdejo, F. (1997) An 
Approach to Conceptual Text Retrieval Using the 
EuroWordNet Multilingual Semantic Database. In 
Proc. of AAAI Spring Symposium on 
Cross-Language Text and Speech Retrieval. 
Hearst, M.A. and Pedersen, J.O. (1996) Reexamining 
the Cluster Hypothesis: Scatter/Gather on Retrieval 
Results. In Proc. of 19th ACM SIGIR Conference 
(SIGIR?96). 
Hull, D.A. and Grefenstette, G. (1996) Querying 
across languages: a dictionary-based approach to 
multilingual information retrieval. In Proc. of the 
19th ACM SIGIR Conference (SIGIR?96). 
Jang, M.G., Myaeng, S.H. and Park, S.H. (1999) 
Using Mutual Information to Resolve Query 
Translation Ambiguities and Query Term 
Weighting. In Proc. of the 37th Annual Meeting of 
the Association for Computational Linguistics. 
Kwon, O-W., Kang, I.S., Lee, J-H and Lee, G.B.  
(1997) Cross-Language Text Retrieval Based on 
Document Translation Using Japanese-to-Korean 
MT system. In Proc. of NLPRS'97, pp. 101-106. 
Lee, K.S., Park, Y.C., Choi, K.S. (2001) Re-ranking 
model based on document clusters. Information 
Processing and Management, 37(1), pp. 1-14. 
Oard, D.,W. and Hackett, P. (1997) Document 
Translation for the Cross-Language Text Retrieval 
at the University of Maryland. In  Proc. of the 
Sixth Text REtrieval Conference (TREC-6). 
Rehder, B., Littman, M.L., Dumais, S. and Landauer, 
T.K. (1997) Automatic 3-language cross-language 
information retrieval with latent semantic indexing. 
In Proc. of the Sixth Text REtrieval Conference 
(TREC-6). 
Salton, G. (1989) Automatic Text Processing: The 
Transformation, Analysis, and Retrieval of 
Information by Computer. Addison-Wesley, 
Reading, Pennsylvania. 
Voorhees, E.M. (1986) Implementing agglomerative 
hierarchic clustering algorithms for use in 
document retrieval. Information Processing & 
Management, 22(6), pp. 465-476. 
Yang, Y., Carbonell, J.G., Brown, R.D. and 
Frederking, R.E. (1998) Translingual Information 
Retrieval: Learning from Bilingual Corpora. AI 
Journal special issue, pp. 323-345. 
Deverbal Compound Noun Analysis Based on Lexical Conceptual Structure
Koichi Takeuchi Kyo Kageura
Human and Social Information Research Division
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyodaku, Tokyo 101-8430, Japan
 koichi,kyo,t koyama@nii.ac.jp
Teruo Koyama
Abstract
This paper proposes a principled approach
for analysis of semantic relations between
constituents in compound nouns based on
lexical semantic structure. One of the
difficulties of compound noun analysis is
that the mechanisms governing the deci-
sion system of semantic relations and the
representation method of semantic rela-
tions associated with lexical and contex-
tual meaning are not obvious. The aim of
our research is to clarify how lexical se-
mantics contribute to the relations in com-
pound nouns since such nouns are very
productive and are supposed to be gov-
erned by systematic mechanisms. The
results of applying our approach to the
analysis of noun-deverbal compounds in
Japanese and English show that lexical
conceptual structure contributes to the re-
strictional rules in compounds.
1 Introduction
The difficulty of compound noun analysis is that the
effective way of describing the semantic relations
in compounds has not been identified. The descrip-
tion should not remain just a kind of categorization.
Rather, it should take into account the construction
of the analysis model.
The previous work proposed semantic approaches
based on semantic categories (Levi, 1978; Isabelle,
1984; Iida et al, 1984) had proposed detailed analy-
sis of relations between constituents in compound
nouns. Some of approaches (Fabre, 1996; John-
ston and Busa, 1998) take the framework of Gen-
erative Lexicon (GL) (Pustejovsky, 1995). Se-
mantic approaches are especially well designed but
they should still clarify the complete lexical factors
needed for analysis model.
Probabilistic approaches (Lauer, 1995; Lapata,
2002) have been proposed to disambiguate semantic
relations between constituents in compounds. Their
experimental results show a high performance, but
only for shallow analysis of compounds using se-
mantically tagged corpora. To be fully effective,
they also need to incorporate factors that are effec-
tive in disambiguating semantic relations. It is there-
fore necessary to clarify what kinds of factors are re-
lated to the mechanisms that govern the relations in
compounds.
Against this background, we have carried out a re-
search which aims at clarifying how lexical seman-
tics contribute to, independently of languages, the
relations in compound nouns. This paper proposes
a principled approach for the analysis of semantic
relations between constituents in compound nouns
based on the theoretical framework of lexical con-
ceptual structure (LCS), and shows that the frame-
work originally developed on the basis of Japanese
compound noun data works well for both Japanese
and English compound nouns.
2 The Basic Framework
2.1 The Relation between Modifier and
Deverbal Head
The relation between constituents in deverbal com-
pounds1 can first be divided into two: (i) the modi-
fier becomes an internal argument (Grimshaw, 1990)
and (ii) the modifier functions as an adjunct. We as-
1In the case of English the equivalent is nominalizations, but
for simplicity we use deverbal compounds.
sume these two kinds of relations are the target of
our analysis model because argument/adjunct rela-
tions are basic but extensible to more detailed se-
mantic relations by assuming more complex seman-
tic system. Besides these relations related to argu-
ment structure of verbs are the boundary between
syntax and semantics, then our approach must be ex-
tendable to be incorporated into sytactic analysis.
2.2 LCS-based Disambiguation Model
We assume that the discrimination between argu-
ment and adjunct relations can be done by the com-
bination of the LCS (we call TLCS) on the side of
deverbal heads and the consistent categorization of
modifier nouns on the basis of their behavior vis-a`-
vis a few canonical TLCS types of deverbal heads.
Figure 1 shows examples of disambiguating re-
lations using TLCS for the deverbal heads ?sousa?
(operate) and ?hon?yaku? (translate). In TLCSes, the
words written in capital letters are semantics predi-
cates, ?x? denotes the external argument, and ?y? and
?z? denote the internal arguments (see Section 3).
Figure 1: Disambiguation of relations between noun
and deverbal head
The approach we propose consists of three ele-
ments: categorization of deverbals and nominaliza-
tions, categorization of modifier noun and restriction
rules for identifying relations.
3 TLCS
The framework of LCS (Hale and Keyser, 1990;
Rappaport and Levin, 1988; Jackendoff, 1990;
Kageyama, 1996) has shown that semantic decom-
position based on the LCS framework can system-
atically explain the word formation as well as the
syntax structure. However existing LCS frameworks
cannot be applied to the analysis of compounds
straightforwardly because they do not give extensive
semantic predicates for LCS. Therefore we construct
an original LCS, called TLCS, based on the LCS
framework with a clear set of LCS types and basic
predicates. We use the acronym ?TLCS? to avoid
the confusion with other LCS-based schemes.
Table 1 shows the current complete set of TLC-
Ses types we elaborated.2 The following list is for
Japanese deverbals, but the same LCS types are ap-
plied for nominalizations in English.3
Table 1: List of TLCS types
1 [x ACT ON y]
enzan (calculate), sousa (operate)
2 [x CONTROL[BECOME [y BE AT z]]]
kioku (memorize), hon?yaku (translate)
3 [x CONTROL[BECOME [y NOT BE AT z]]]
shahei (shield), yokushi (deter)
4 [x CONTROL [y MOVE TO z]]
densou (transmit), dempan (propagate)
5 [x=y CONTROL[BECOME [y BE AT z]]]
kaifuku (recover), shuuryou (close)
6 [BECOME[y BE AT z]]
houwa (become saturated)
bumpu (be distributed)
7 [y MOVE TO z]
idou (move), sen?i (transmit)
8 [x CONTROL[y BE AT z]]
iji (maintain), hogo (protect)
9 [x CONTROL[BECOME[x BE WITH y]]]
ninshiki (recognize), yosoku (predict)
10 [y BE AT z]
sonzai (exist), ichi (locate)
11 [x ACT]
kaigi (hold a meeting), gyouretsu (queue)
12 [x CONTROL[BECOME [ [FILLED]y BE AT z]]]
shomei (sign-name)
The number attached to each TLCS type in Table
1 will be used throughout the paper refer to specific
TLCS types. In Table 1, the capital letters (such as
?ACT? and ?BE?) are semantic predicates, which are
11 types. ?x? denotes an external argument and ?y?
and ?z? denote an internal argument (see (Grimshaw,
1990)). 4
2Basicaly these 12 types are set by the combination of argu-
ment structure and aspect analysis that is telic or atelic. After
applying all the combination, we arrange the TLCS patterns by
deleting patterns that does not appear and subcategorizing cer-
tain patterns.
3At the moment, there are about 500 deverbals in Japanese
and 40 nominalizations in English.
4In this paper, we limit the types of arguments are three, i.e.
x (Agent), y (Theme) and z (Goal).
4 Categorization of Modifier Noun
4.1 Categorization by the Accusativity of
Modifiers
In Japanese compounds, some of modifiers can not
take an accusative case. This is an adjectival stem
and it does not appear with inflections. Therefore,
the modifier is always the adjunct in the compounds.
So we introduce the distinction of ?-ACC? (unac-
cusative) and ?+ACC? (accusative).
ACC ?kimitsu? (secrecy) and ?kioku? (memory) are
?+ACC?, and ?sougo? (mutual-ity) and ?kinou?
(inductiv-e/ity) are ?-ACC?. In English, they
correspond to adjective modifier such as ?-ent?
of ?recurrent? or ?-al? of ?serial?.
4.2 Categorization by the Basic Components of
TLCS
If, as argued by some theoretical linguists, the LCS
representation can contribute to explaining these
phenomena related to the arguments and aspect
structure consistently, and if the combination of LCS
and noun categorization can explain properly these
phenomena related to argumet/adjunct, then there
should be a level of consistent noun categorization
which matches the LCS on the side of deverbals. We
used the predicates of some TLCS types to explore
the noun categorizations.
In the preliminary examination, we have found
that some TLCS types can be formed into the groups
that correspond to modifier categories in Table 2.
Below are examples of modifier nouns catego-
rized as negative or positive in terms of each of these
TLCS groups.
ON ?koshou? (fault) and ?seinou? (performance)
are ?+ON?, and ?heikou? (parallel) and ?rensa?
(chain) are ?-ON?. (?ON? stands for the predi-
cate in ?ACT ON?.)
EC ?imi? (semantic) and ?kairo? (circuit) are ?+EC?,
and ?kikai? (machine) and ?densou? (transmis-
sion) are ?-EC?. (?EC? stands for an External
argument Controls an internal argument?.)
AL ?fuka? (load) and ?jisoku? (flux) are ?+AL?, and
?kakusan? (diffusion) and ?senkei? (linearly) are
?-AL?. (?AL? stands for alternation verbs.)
UA ?jiki? (magnetic) and ?joutai? (state) are ?+UA?,
and ?junjo? (order) and ?heikou? (parallel) are
?-UA?. (?UA? stands for UnAccusative verbs.)
5 Procedure of Compound Noun Analysis
The noun categories introduced in Section 4 can
be used for disambiguating the intra-term relations
in deverbal compounds with various deverbal heads
that take different TLCS types. The range of ap-
plication of the noun categorizations with respect to
TLCS groups is summarized in Table 2. The num-
ber in the TLCS column corresponds to the number
given in Table 1.
Step 1 If the modifier has the category ?-ACC?, then
declare the relation as adjunct and terminate. If
not, go to next.
Step 2 If the TLCS of the deverbal head is 10, 11,
or 12 in Table 1, then declare the relation as
adjunct and terminate. If not, go to next.
Step 3 The analyzer determines the relation from
the interaction of lexical meanings between a
deverbal head and a modifier noun. In the case
of ?-ON?, ?-EC?,?-AL? or ?-UA?, declare the re-
lation as adjunct and terminate. If not, go to
next.
Step 4 Declare the relation as internal argument and
terminate.
With these rules and categories of nouns, we
can analyze the relations between words in com-
pounds with deverbal heads. For example, when
the modifier ?kikai? (machine) is categorized as
?-EC? but ?+ON?, the modifier in kikai-hon?yaku
(machine-translation) is analyzed as adjunct (that
means ?translation by a machine?), and the modi-
fier in kikai-sousa (machine-operation) is analyzed
as internal argument (that means ?operation of a ma-
chine?), both correctly.
6 Experiments and Evaluations
We applied the method to 1223 two-constituent
compound nouns with deverbal heads in Japanese.
809 of them are taken from a dictionary of techni-
cal terms (Aiso, 1993), and 414 from news articles
in a newspaper. We also applied the method to 200
compound nouns of technical terms (Aiso, 1993) in
English. They are extracted randomly.
According to the manual evaluation of the exper-
iment, 99.3% (1215/ 1223) of the results were cor-
rect in Japanese, and 97% (194/200) in English. The
performance is very high. Table 2 shows the details
of how the rules are applied to disambiguating the
relations between constituents in the deverbal com-
pounds. These results indicate that our set of LCS
and categorization of modifiers has the enough to
disambiguate the relationships we assumed.
Table 2: Combination of modifiers and TLCS of de-
verbal heads,and statistics of the correct analysis
role mod. cat. TLCS Jap.(%) Eng. (%)
adjunct -ACC any 263 (36.7) 84 (75.0)
any 10,11,12 88 (12.3) 4 (3.6)
-ON 1 95 (13.3) 10 (8.9)
-EC 2,3,4 186 (25.9) 14 (12.5)
-AL 5 26 (3.6) 0 (0.0)
-UA 6,7 59 (8.2) 0 (0.0)
total 717 112
role mod. cat. TLCS Jap.(%) Eng.(%)
int. argu. +ACC 8, 9 74 (14.9) 15 (18.3)
+ON 1 89 (17.9) 19 (23.2)
+EC 2,3,4 249 (50.0) 43 (52.4)
+AL 5 57 (11.4) 3 (3.7)
+UA 6,7 29 (5.8) 2 (3.4)
total 498 82
7 Discussion
Roughly speaking, our LCS-based approach can be
available both Japanese and English deverbal nouns.
Comparing with the results between Japanese com-
pounds and English compounds, the factor ?-ACC?
looks effective to disambiguate relations. The rea-
son is that the most of modifiers indicate adjec-
tive function by adding suffixes in English. While
in Japanese, adjectival nouns of modifiers have no
inflecitons, then the semantic-based approach is
needed for Japanese compound noun analysis.
We found that a small number of modifier nouns
deviate from our assumptions. The most typical case
is that our analysis model fails in a word with mul-
tiple semantics. For example, ?right justify? is mis-
understood as internal argument relation because of
ambiguity of the word ?right? which has both mean-
ings of an adjective and a noun. We consider dealing
with them as each different words like ?right adj?,
?right noun? in future work.
8 Conclusion
This paper proposes a principled approach for anal-
ysis of semantic relations between constituents in
compound nouns based on lexical conceptual struc-
ture we call it TLCS. The results of experiment for
Japanese compounds and English compounds show
our approach is highly promising, also the contribu-
tion of the lexical factor to disambiguation rule.
References
Hideo Aiso. 1993. Dictionary of Technical Terms of In-
formation Processing (Compact edition). Ohmusha.
(in Japanese).
Cecile Fabre. 1996. Interpretation of Nominal
Compounds: Combining Domain-Independent and
Domain-Specific Information. In Proceedings of
COLING-96, pages 364?369.
Jane Grimshaw. 1990. Argument Structure. MIT Press.
Ken Hale and Samuel J. Keyser. 1990. A View from the
Middle Lexicon (Lexicon Project Working Papers 10).
MIT.
Jin Iida, Kentaro Ogura, and Hirosato Nomura. 1984.
Analysis of Semantic Relations and Processing for
Compound Nouns in English. In Proceedings of Infor-
mation Processing Society of Japan, SIG Notes,NL,46-
4 (in Japanese), pages 1?8.
Pierre Isabelle. 1984. Another Look at Nominal Com-
pounds. In Proceedings of COLING-84, pages 509?
516.
Ray Jackendoff. 1990. Semantic Structures. MIT Press.
Michael Johnston and Federica Busa. 1998. The Com-
positional Interpretation of Nominal Compounds. In
E. Viegas, editor, Breadth and Depth of Semantics Lex-
icons. Kluwer.
Taro Kageyama. 1996. Verb Semantics. Kurosio Pub-
lishers. (In Japanese).
Maria Lapata. 2002. The Disambiguation of Nomi-
nalization. Association for Computational Liguistics,
28(3):357?388.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Department of Computing, Macquarie Univer-
sity.
Judith N. Levi. 1978. The Syntax and Semantics of Com-
plex Nominals. Academic Press.
James Pustejovsky. 1995. The Generative Lexicon. MIT
Press.
Malka Rappaport and Beth Levin. 1988. What to do
with  -roles. In W. Wilkins, editor, Thematic Rela-
tions (Syntax and Semantics 21), pages 7?36. Aca-
demic Press.
 
	  
	Quantitative Portraits of Lexical Elements
Kyo Kageura
Human and Social Information Research Division
National Institute of Informatics,
2-1-2 Hitotsubashi, Chiyoda-ku,
Tokyo, 101-8430, Japan
kyo@nii.ac.jp
Abstract
This paper clarifies the basic concepts and theoret-
ical perspectives by and from which quantitative
?weighting? of lexical elements are defined, and
then draws, quantitative portraits of a few lexical el-
ements in order to exemplify the relevance of the
concepts and perspectives examined.
1 Introduction
Since Luhn?s pioneering work (Luhn, 1958) in au-
tomatic term weighting, many methods have been
proposed in the fields of IR (e.g. Spark-Jones, 1973;
Harter, 1975) and NLP (e.g. Church et al, 1990).
Some ?standard? methods of term weighting such
as
  have been established (Aizawa, 2003; 
	
, 1999) and the application range has widened;
term weighting has become a mature technology.
Despite this, what has been technically proposed
has not been examined from a theoretical point
of view, i.e. what kind of weighting scheme re-
flects what kind of lexical nature within what kind
of framework of interpretations in language. We
will clarify this and then illustrate the relevance of
this clarification by drawing quantitative portraits of
some lexical items using the quantitative measures.
2 Texts and lexica
Automatic term weighting starts from
texts/documents. To what spheres the weights
are attributed can differ. Figure 1 shows the lin-
guistic spheres of lexica and texts (Kageura, 2002);
there are both concrete data spheres and abstract
spheres on both the lexical and textual sides.
Within this scheme, three types of relations be-
tween lexica and texts can be identified: concrete
terms attributed to concrete texts, concrete terms
corresponding to discourse, and abstract lexica cor-
responding to abstract discourse. We will show be-
low that three major types of automatic term weight-
ing methods correspond to these three types of rela-
tions between lexica and texts.
text text
text
text
texttext
text
A set of actual texts (targets of IR)
Textual sphere / theoretical sphere of discourse
term termterm termtermterm termterm
Terms as attributes of concrete set of documents
Lexicological sphere / theoretical sphere of lexica
Figure 1: Textual sphere and lexicological sphere.
3 Methods of term weighting
3.1 Tfidf


 is defined as:
 
ff  	

 
	
	Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 19?27,
Beijing, August 2010
Multilingual Lexical Network from the Archives of the Digital 
Silk Road 
Mohammad Daoud  
LIG, GETALP 
Universit? Joseph Fourier 
Mohammad.Daoud@imag.fr 
Kyo Kageura 
Graduate School of Education 
The University of Tokyo  
kyo@p.u-tokyo.ac.jp 
Christian Boitet 
LIG, GETALP 
Universit? Joseph Fourier 
Christian.Boitet@imag.fr 
Asanobu Kitamoto 
The National Institute of Informat-
ics (Tokyo) 
Kitamoto@nii.ac.jp 
Mathieu Mangeot 
LIG, GETALP 
Universit? Joseph Fourier 
Mathieu.Mangeot@imag.fr 
 
 
Abstract 
We are describing the construction 
process of a specialized multilingual 
lexical resource dedicated for the ar-
chive of the Digital Silk Road DSR. The 
DSR project creates digital archives of 
cultural heritage along the historical Silk 
Road; more than 116 of basic references 
on Silk Road have been digitized and 
made available online. These books are 
written in various languages and attract 
people from different linguistic back-
ground, therefore, we are trying to build 
a multilingual repository for the termi-
nology of the DSR to help its users, and 
increase the accessibility of these books. 
The construction of a terminological da-
tabase using a classical approach is dif-
ficult and expensive. Instead, we are in-
troducing specialized lexical resources 
that can be constructed by the commu-
nity and its resources; we call it Multi-
lingual Preterminological Graphs 
MPGs.  We build such graphs by ana-
lyzing the access log files of the website 
of the Digital Silk Road. We aim at 
making this graph as a seed repository 
so multilingual volunteers can contrib-
ute.  We have used the access log files 
of the DSR since its beginning in 2003, 
and obtained an initial graph of around 
116,000 terms. As an application, We 
have used this graph to obtain a preter-
minological multilingual database that 
has a number of applications. 
1 Introduction 
This paper describes the design and develop-
ment of a specialized multilingual lexical re-
source for the archive constructed and main-
tained by the Digital Silk Road project. The 
Digital Silk Road project (NII 2003) is an initia-
tive started by the National Institute of Infor-
matics (Tokyo/Japan) in 2002, to archive cul-
tural historical resources along the Silk Road, 
by digitizing them and making them available 
and accessible online.  
One of the most important sub-projects is the 
Digital Archive of Toyo Bunko Rare Books 
(NII 2008) where 116 (30,091 pages) of old rare 
books available at Toyo Bunko library have 
been digitized using OCR (Optical Character 
Recognition) technology. The digitized collec-
tion contains books from nine languages includ-
ing English. The website of the project attracts 
visitors from the domain of history, archeology, 
and people who are interested in cultural heri-
tage. It provides services of reading and search-
ing the books of Toyo Bunko, along with vari-
ety of services. Table 1 shows the countries 
from which DSR is being accessed. The table 
19
shows that around 60% of visitors are coming 
from countries other than Japan. The diversity 
of the visitors? linguistic backgrounds suggests 
two things: 1) Monolingual translation service is 
not enough. 2) It shows that we can benefit from 
allowing them to contribute to a multilingual 
repository. So we design and build a collabora-
tive multilingual terminological database and 
seed using the DSR project and its resources 
(Daoud, Kitamoto et al 2008). However, Dis-
covering and translating domain specific termi-
nology is a very complicated and expensive 
task, because (1) traditionally, it depends on 
human terminologists (Cabre and Sager 1999) 
which increases the cost, (2) terminology is dy-
namic (Kageura 2002), thousands of terms are 
coined each year, and (3) it is difficult to in-
volve domain experts in the construction proc-
ess. That will not only increase the cost, but it 
will reduce the quality, and the coverage (num-
ber of languages and size). Databases like (UN-
Geo 2002; IATE 2008; UN 2008) are built by 
huge organizations, and it is difficult for a 
smaller community to produce its own multilin-
gual terminological database. 
Country Visitors language Books in the same language  
Japan 117782 JA 2 books 
China 30379 CH 5 books 
USA 15626 EN 44 books 
Germany 8595 GE 14 books 
Spain 7076 SP - 
Australia 5239 EN See USA  
  Italy  4136 IT 1 book 
  France  3875 FR 14 books 
  Poland  2236  PO - 
  Russia  1895  RU 7 books 
other  87573 Other There are many books in 
different language 
Total 284412 
Table 1. Countries of the DSR visitors (from 
jan/2007 to dec/2008) 
In the next section we will give definitions 
for the basic concepts presented in this article, 
in particular, the preterminology and its lexical 
network (graph). Then, in the third section we 
will show the automatic approach to seed the 
multilingual preterminological graph based on 
the resources of the DSR. And then, we will 
discuss the human involvement in the develop-
ment of such a resource by providing a study of 
the possible contributors through analyzing the 
multilinguality and loyalty of the DSR visitors. 
In the fifth section we will show the experimen-
tal results. And finally, we will draw some con-
clusions.   
2 Multilingual Preterminological 
Graphs 
2.1 Preterminology 
Terminological sphere of a domain is the set of 
terms related to that domain. A smaller set of 
that sphere is well documented and available in 
dictionaries and terminological databases such 
as (FAO 2008; IEC 2008; IDRC 2009)... How-
ever, the majority of terms are not multilingual-
ized, nor stored into a database, even though, 
they may be used and translated by the commu-
nity and domain experts. This situation is shown 
in Figure 1, where the majority of terms are in 
area B. Preterminological sphere (area B) of a 
domain is a set of terms (preterms) related to 
the domain and used by the community but it 
might not be documented and included in tradi-
tional lexical databases. 
Multilingual Terminological Sphere
Preterminology
MTDB
B
A
C
 
Figure  1. Preterminological sphere 
Every year thousands of terms are coined and 
introduced in correspondence to new concepts, 
scientific discoveries or social needs. Most of 
these terms are produced in the top dominant 
languages, i.e. English. Interested people from 
different linguistic backgrounds would find 
suitable translations to new terms and use it 
amongst them. For example, the term ?status 
update? is used by people who visit social net-
working websites like facebook.com. Transla-
tion of this term to Arabic might not be avail-
able in area A of Figure 1. However the Arabic 
community found a translation that is acceptable 
which is  ????? ??????. So this term is in the area B. 
We are trying to use what is in area A, and what 
can be contributed from B to build preterminol-
ogy (Daoud, Boitet et al 2009).  
20
2.2 Structure of MPG 
We are building preterminological resource as a 
lexical network (graph) to handle the diversity 
of the resources that we use. A multilingual pre-
terminological graph MPG(N,E) is a finite non-
empty set N={n1,n2, ?} of objects called 
Nodes together with a set E={e1,e2, ?} of un-
ordered pairs of distinct nodes of MPG called 
edges. This definition is based on the general 
definition of a graph at the following references 
(Even 1979; Loerch 2000).  MPG of domain X, 
contains possible multilingual terms related to 
that domain connected to each other with rela-
tions. A multilingual lexical unit and its transla-
tions in different languages are represented as 
connected nodes with labels.  
In an MPG the set of nodes N consists of p,l, 
s, occ, where p is the string of the preterm, l is 
the language, s is the code of the first source of 
the preterm, and occ is the number of occur-
rences. Note that l could be undefined. For ex-
ample: N={[silk road, en, log],[Great Wall of China, en, 
,wikipedia, 5], [?????, ar, contributorx,6]}, here we have 
three nodes, 2 of them are English and one in 
Arabic, each term came from a different source. 
Note that English and Arabic terms belong to 
the same N thus, the same MPG. 
An Edge e={n, v} is a pair of nodes adjacent in 
an MPG. An edge represents a relation between 
two preterms represented by their nodes. The 
nature of the relation varies. However, edges are 
weighted with several weights (described be-
low) to indicate the possible nature of this rela-
tion. 
The following are the weights that label the 
edges on an MPG: Relation Weights rw: For an 
edge e={[p1,l1,s1], [p2,l2,s2]}, rw indicates 
that there is a relation between the preterm p1 
and p2. The nature of the relation could not be 
assumed by rw. Translation Weights tw: For an 
edge e={[p1,l1,s1], [p2,l2,s2]}, tw suggests that 
p1 in language l1 is a translation of p2 in lan-
guage l2. Synonym Weights sw: For an edge 
e={[p1,l1,s1], [p2,l1,s2]}, sw suggests that p1 
and p2 are synonyms. 
3 Automatic Initialization of DSR-
MPG  
Basically we seeded DSR-MPG, through two 
steps, the firs one is the automatic seeding, 
which consists of the following: 1) Initialization 
by finding interesting terms used to search the 
website of the DSR. 2) Multilingualization, us-
ing online resources. 3) Graph Expansion using 
the structure of the graph it self. The second 
step is the progressive enhancement, by receiv-
ing contributions from users, through set of use-
ful applications. In this section we will discuss 
the first three steps. In section 4, we will discuss 
the human factor in the development of DSR-
MPG. 
3.1 Analyzing Access Log Files 
We analyze two kinds of access requests that 
can provide us with information to enrich the 
MPG: (1) requests made to the local search en-
gine of DSR (2) requests from web-based 
search engine (like Google, Yahoo!?). These 
requests provide the search terms that visitors 
used to access the website. Moreover, we can 
understand the way users interpret a concept 
into lexical units. For example, if we find that 
five different users send two search requests t1 
and t2, then there is a possibility that t1 and t2 
have a relation. The graph constructor analyzes 
the requests to make the initial graph by creat-
ing edges between terms in the same session. 
rw(x,y), is set to the number of sessions contain-
ing x and y within the log file. 
For example, rw(x,y) = 10 means that 10 
people thought about x and y within the same 
search session. Figure 2 shows an example of a 
produced graph. The method did not discover 
the kind of relation between the terms. But it 
discovered that there is a relation, for example, 
three users requested results for ?yang? fol-
lowed by ?yin? within the same session. Hence, 
edge with weight of 2 was constructed based on 
this. 
21
 Figure  2. Example of constructing an MPG 
from an access log file 
3.2 Multilingualization Using Online Re-
sources 
Many researchers focused on the usage of dic-
tionaries in digital format to translate lexical 
resources automatically (Gopestake, Briscoe et 
al. 1994) (Etzioni, Reiter et al 2007). We are 
concerned with the automatic utilization of 
these resources to acquire multilingual preter-
minological resources through the following: 1) 
Wikipedia 2) online MT systems 3) online dic-
tionaries. 
Wikipedia (Wikipedia-A 2008) is a rich 
source of preterminology, it has good linguistic 
and lexical coverage. As of December, 2009, 
there are 279 Wikipedias in different languages, 
and 14,675,872 articles. There are 29 Wikipe-
dias with more that 100000 articles and 91 lan-
guages have more than 10,000 articles. Beside, 
Wikipedia is built by domain experts. We ex-
ploit the structure of Wikipedia to seed an 
MPG, by selecting a root set of terms, for each 
one of them we fetch its wikipedia article, and 
then we use the language roll of the article. For 
example, we fetch the article (Cuneiform script) 
En: http://en.wikipedia.org/wiki/Cuneiform_script, to reach its 
translation in Arabic from this url:  
http://ar.wikipedia.org/wiki/ ???????_?????  
We use also online machine translation sys-
tems as general purpose MRDs. One of the 
main advantages of MT systems is the good 
coverage even for multiword terms. The agree-
ment of some MT systems with other resources 
on the translation of one term enhanced the con-
fidence of the translation. Another positive 
point is that the results of MT provide a first 
draft to be post edited later. We used 3 MT sys-
tems: 
? Google Translate (Google 2008) (50 
languages) 
? Systran (Systran 2009) (14 languages) 
? Babylon (Babylon 2009) (26 languages) 
Here is an example of translating the term 
?great wall of China? into Arabic. 
 
Figure  3. MPG sample nodes 
In a similar way, we used several online re-
positories; to make good use of what is avail-
able and standardized, to initializing the MPG 
with various resources, and to construct a meta-
system to call online dictionaries automatically. 
We used IATE (IATE 2008)  as an example of a 
terminological db, and Google dictionary 
(Google 2008). The concept is similar to the 
concept of using online translations, where we 
construct an http request, to receive the result as 
html page. 
3.3 Graph Expansion 
 And then, the Graph is expanded by finding the 
synonyms according to formula (1) described at 
(Daoud, Boitet et al 2009). After finding syno-
nyms we assume that synonyms share the same 
translations. As Figure 4 shows, X1 and X2 have 
translations overlaps, and relatively high rw, so 
that suggest they are synonyms. Therefore we 
constructed heuristic edges between the transla-
tions of X1 and X2. 
Systran 
wight=1 
Wikipedia 
Google 
 Babylon 
wight=3 
great wall 
of China 
 ??? ?????
?????? 
 ?????? ????
????? 
22
 Figure  4. Graph expansion 
4 Human Involvement in the Develop-
ment of DSR-MPG 
After initializing the graph, we target contribu-
tions from the visitors to the DSR website. In 
this section we will start by analyzing the possi-
bility of receiving contributions from the visi-
tors, and then we will introduce some useful 
applications on the DSR-MPG that can help the 
visitors and attract them to get involved. 
4.1 Analyzing Possible Contributors of the 
DSR 
We are trying to analyze access log files to find 
out the possible contributors to a pretermi-
nological multilingual graph dedicated to an 
online community. This kind of information is 
necessary for the following reasons: 1) it pro-
vide feasibility analysis predicting the possibil-
ity of receiving contribution to a multilingual 
preterminological repository. 2) it gives infor-
mation that can be used by the collaborative 
environment to personalize the contribution 
process for those who prove to be able to con-
tribute. 
In the analysis process we are using the fol-
lowing information that can be easily extracted 
the access records: 
? Key terms to access the historical resources of 
the Digital Silk Road, whether it is the local 
search engine, or any external search engine. 
? Access frequency: number of access requests 
by a visitor over a period of time. 
? Language preferences 
? Period of visits 
Knowing these points helps determining the 
possible users who might be willing to contrib-
ute. A contributor should satisfy the following 
characteristics: 1) Loyalty 2) Multilinguality.  A 
multilingual user is a visitor who uses multilin-
gual search terms to access the online resources. 
We rank users based on their linguistic compe-
tence, we measure that by tracking users? search 
requests, and matching them with the multilin-
gual preterminological graph, users with higher 
matches in certain pair of languages are ranked 
higher. A loyal user is a user who visits the web 
site frequently and stays longer than other users. 
Users based on how many months they accessed 
the website more that k times. 
4.2 DSR-MPG Applications 
For a historical archive like the DSR, we find 
that reading and searching where the most im-
portant for users. Log files since 2003 shows 
that 80% of the project visitors were interested 
in reading the historical records. Moreover, 
around 140000 search requests have been sent 
to the internal search engine. So we imple-
mented two applications (1) ?contribute-while-
reading? and (2) ?contribute-while-searching?. 
4.2.1 Contribute While Searching 
Physical books have been digitized and indexed 
into a search engine. We expect users to send 
monolingual search requests in any language 
supported by our system to get multilingual an-
swers. Having a term base of multilingual 
equivalences could achieve this (Chen 2002). A 
bilingual user who could send a bilingual search 
request could be a valid candidate to contribute. 
We plan that users who use our search engine 
will use the DSR-pTMDB to translate their re-
quests and will contribute to the graph sponta-
neously. As Figure 5 shows, a user would trans-
late the search request, during the searching 
process; the user can ask to add new translation 
if s/he was not happy with the suggested transla-
tion, by clicking on ?Add Suggestions? to view 
a contribution page. 
 
Figure  5. A Japanese user translating his re-
quest 
23
4.2.2 Contribute While Reading 
The other application is trying to help users 
from different linguistic backgrounds to trans-
late some of the difficult terms into their lan-
guages while they are reading, simply by select-
ing a term from the screen. As shown in Figure 
6, readers will see a page from a book as an im-
age, with its OCR text. Important terms will be 
presented with yellow background. Once a term 
is clicked, a small child contribution/lookup 
window will be open, similar. Also user can 
lookup/translate any term from the screen by 
selecting it. This application helps covering all 
the important terms of each book. 
 
Figure 6. Translate while reading 
5 Experimental Results 
In this section present we will present the ex-
periment of seeding DSR-MPG, and the results 
of discovering possible contributors from the 
visitors of the DSR. 
5.1 DSR-MPG Initialization 
To build the initial DSR-MPG, we used the ac-
cess log files of the DSR website (dsr.nii.ac.jp) 
from December 2003 to January 2009. The ini-
tial graph after normalization contained 89,076 
nodes.  Also we extracted 81,204 terms using 
Yahoo terms. 27,500 of them were not discov-
ered from the access files. So, the total number 
of nodes in the initial graph was 116,576 nodes, 
see Figure 7 for sample nodes. 
After multilingualization, the graph has 210,781 
nodes containing terms from the most important 
languages. The graph has now 779,765 edges 
with tw > 0.  The important languages are the 
languages of the majority of the visitors, the 
languages of the archived books, and represen-
tative languages a long the Silk Road. DSR-
MPG achieved high linguistic coverage as 20 
languages have more than 1000 nodes on the 
graph. To evaluate the produced graph, we ex-
tracted 350 English terms manually from the 
index pages of the following books: 
Ancient Khotan, vol.1: 
http://dsr.nii.ac.jp/toyobunko/VIII-5-B2-7/V-1/ 
On Ancient Central-Asian Tracks, 
vol.1:http://dsr.nii.ac.jp/toyobunko/VIII-5-B2-
19/V-1 
Memoir on Maps of Chinese Turkistan and 
Kansu, vol.1: 
http://dsr.nii.ac.jp/toyobunko/VIII-5-B2-11/V-1 
0
5 0
10 0
15 0
2 0 0
2 5 0
3 0 0
DS R- M P G 2 D S R - M P G 1 P a n Ima g e s W i ki t io n a ry B i- d i c t io n a ry DS R1
En-Ar (only correct tranlstions) En-Fr (only correct translations)
 
Figure  7. A comparison between DSR-MPG, 
and other dictionaries. The En-Ar bi-dictionary 
is Babylon (Babylon 2009), and the En-Fr bi-
dictionary was IATE. 
We assume that the terms available in these 
books are strongly related to the DSR. Hence, 
we tried to translate them into Arabic and 
French. Figure 7 compares between DSR-MPG, 
and various general purpose dictionaries. Out of 
the 350 terms, we found 189 correct direct 
translations into Arabic. However, the number 
reached 214 using indirect translations.  On the 
other hand, the closest to our result was PanI-
mages, which uses Wikitionaries and various 
dictionaries, with only 83 correct translations. 
DSR-MPG1 is the translations obtained from 
formula 1, DSR-MPG2 represents the transla-
tions obtained from indirect translations, which 
increased the amount of correct translation by 
24
25 terms in the case of En-Ar. The result can be 
progressively enhanced by accepting contribu-
tions from volunteers through the applications 
we described in the section three and the generic 
nature of MPG makes it easy to accept contribu-
tions from any dictionary or terminological da-
tabase. 
Around 55200 root English terms were used 
as a seed set of terms; these terms were selected 
from the initial DSR-MPG. Around 35000 
terms have been translated from Wikipedia into 
at least 1 language, mostly in French, German. 
Wikipedia increased the density of the graph by 
introducing around 113,000 edges (with tw). 
Translations
0
2000
4000
6000
8000
10000
12000
fr de ja it zh es ru ar
 
Figure 8. Number of translated terms in sam-
ple languages using Wikipedia 
Naturally MT would achieve better coverage; 
we checked the results for Arabic, we selected 
60 terms randomly from the root set, around 25 
terms were translated correctly. 13 terms needed 
slight modification to be correct. 
0
2000
4000
6000
8000
10000
12000
fr de ja it zh es ru ar
Wikipedia
Google Translate confirmations
 
Figure 9. Terms translated by Google MT 
and matched the translation of Wikipedia 
5.2 DSR Possible Contributors 
With K=2, meaning that a multilinguality com-
petence is counted only if the two terms sent by 
a user has to have more than 2 points of transla-
tion weight on the MPG. 
The highest score was 33, achieved by this 
IP: p27250-adsao05douji-acca.osaka.ocn.ne.jp. 
That means that this user sent 33 multilingual 
search requests. We have another 115 users with 
score higher than 5.  
For example, the following two request, sent by 
one user: 
p27250-adsao05douji-acca.osaka.ocn.ne.jp
 &input=peshawar 
p27250-adsao05douji-acca.osaka.ocn.ne.jp
  &input=?????? 
On the DSR-MPG the translation weight be-
tween peshawer and ?????? = 5, thus 
this IP earned a point. With k=10, means that a 
user should send 10 requests to earn a loyalty 
point, only 309 users earned 12 point (for 12 
months), 43 of them has more than 3 points. 
6 Conclusions 
We presented our work in constructing a new 
lexical resource that can handle multilingual 
terms based on the historical archive of the 
Digital Silk Road. Multilingual Preterminologi-
cal Graphs (MPGs) are constructed based on 
domain dedicated resources, and based on vol-
unteer contributions.  
DSR Terminology
DSR-MPG (200,000 nodes)
previous DSR
dictionary (500
entries)
 
Figure  10. DSR preterminology 
It compiles terms available in the pretermi-
nological sphere of a domain. In this article we 
defined the framework of the construction of 
preterminology, and we described the approach 
for using access log files to initialize such pre-
terminological resource by finding the trends in 
the search requests used to access the resources 
of an online community. Aiming at a standard-
ized multilingual repository is very expensive 
25
and difficult.  Instead of that, MPGs tries to use 
all available contributions.  This way will en-
hance the linguistic and informational coverage, 
and tuning the weights (tw, rw, and sw) will 
give indications for the confidence of the trans-
lation equivalences, as the tedges accumulate 
the agreements of the contributors and MDRs 
(online resources). 
We used the resources of the Digital Silk 
Road Project to construct a DSR-MPG and 
some applications that attract further contribu-
tion to the MPG.  DSR-MPG achieved high lin-
guistic and informational coverage compared to 
other general purpose dictionaries, Figure 10. 
Furthermore, the generic structure of the MPG 
makes it possible to accept volunteer contribu-
tions, and it facilitates further study of comput-
ing more lexical functions and ontological rela-
tions between the terms. We made a study on 
the possibility of receiving contributions from 
users, by analyzing the access log file to find 
multilinguality and loyalty of the DSR visitors; 
we found 115 users with the needed linguistic 
capacity 43 of them scored high loyalty points. 
This gives an indication of the future of the con-
tributions. These measures are just estimations 
and expected to go high with the help of the 
MPG-DSR applications. 
References 
Babylon. (2009). "Babylon Dictionary."   Retrieved 
5/5/2009, 2009, from 
http://www.babylon.com/define/98/English-
Arabic-Dictionary.html. 
Cabre, M. T. and J. C. Sager (1999). Terminology: 
Theory, methods, and applications, J. Benjamins 
Pub. Co. 
Chen, A. (2002). "Cross-Language Retrieval Ex-
periments at CLEF 2002." in CLEF-2002 working 
notes,. 
Daoud, M., C. Boitet, et al (2009). Constructing 
multilingual preterminological graphs using vari-
ous online-community resources. the Eighth In-
ternational Symposium on Natural Language 
Processing (SNLP2009), Thailand. 
Daoud, M., C. Boitet, et al (2009). Building a 
Community-Dedicated Preterminological Multi-
lingual Graphs from Implicit and Explicit User In-
teractions. Second International Workshop on 
REsource Discovery (RED 2009), co-located with 
VLDB 2009, Lyon, France. 
Daoud, M., A. Kitamoto, et al (2008). A CLIR-
Based Collaborative Construction of Multilingual 
Terminological Dictionary for Cultural Resources. 
Translating and the Computer 30, London-UK. 
Etzioni, O., K. Reiter, et al (2007). Lexical transla-
tion with application to image searching on the 
web. MT Summit XI, Copenhagen, Denmark. 
Even, S. (1979). Graph Algorithms, Computer Sci-
ence Press. 
FAO. (2008). "FAO TERMINOLOGY."   Retrieved 
1/9/2008, 2008, from http://www.fao.org/faoterm. 
Google. (2008). "Google Dictionary."   Retrieved 
1/9/2008, 2008, from 
http://www.google.com/dictionary. 
Google. (2008). "Google Translate."   Retrieved 1 
June 2008, 2008, from http://translate.google.com. 
Gopestake, A., T. Briscoe, et al (1994). "Acquisition 
of lexical translation relations from MRDS." Ma-
chine Translation Volume 9, Numbers 3-4 / Sep-
tember, 1994: 183-219. 
IATE. (2008). "Inter-Active Terminology for 
Europe."   Retrieved 10/10/2008, 2008, from 
http://iate.europa.eu. 
IDRC. (2009, 10 January 2009). "The Water De-
mand Management Glossary (Second Edition)." 
from 
http://www.idrc.ca/WaterDemand/IDRC_Glossar
y_Second_Edition/index.html. 
IEC. (2008). "Electropedia."   Retrieved 10/10/2008, 
2008, from 
http://dom2.iec.ch/iev/iev.nsf/welcome?openform. 
Kageura, K. (2002). The Dynamics of Terminology: 
A descriptive theory of term formation and termi-
nological growth. 
Loerch, U. (2000). An Introduction to Graph Algo-
rithms Auckland, New Zealand, University of 
Auckland. 
NII. (2003). "Digital Silk Road."   Retrieved 
1/9/2008, 2008, from 
http://dsr.nii.ac.jp/index.html.en. 
NII. (2008). "Digital Archive of Toyo Bunko Rare 
Books."   Retrieved 1 June 2008, 2008, from 
http://dsr.nii.ac.jp/toyobunko/. 
Systran. (2009). "Systran Web Tranlstor."   Re-
trieved 20/12/2009, 2009, from 
www.systransoft.com/. 
UN-Geo (2002). Glossary of Terms for the Stan-
dardization of Geographical Names, UN, New 
York. 
26
UN. (2008). "United Nations Multilingual Terminol-
ogy Database."   Retrieved 10/10/2008, 2008, 
from http://unterm.un.org/. 
Wikipedia-A. (2008). "Wikipedia."   Retrieved 1 
June 2008, 2008, from http://www.wikipedia.org/. 
 
 
 
27
Proceedings of the 2nd Workshop on ?Collaboratively Constructed Semantic Resources?, Coling 2010, pages 63?66,
Beijing, August 2010
Helping Volunteer Translators, Fostering Language Resources
Masao Utiyama
MASTAR Project
NICT
mutiyama@nict.go.jp
Takeshi Abekawa
National Institute
of Informatics
abekawa@nii.ac.jp
Eiichiro Sumita
MASTAR Project
NICT
eiichiro.sumita@nict.go.jp
Kyo Kageura
Tokyo University
kyo@p.u-tokyo.ac.jp
Abstract
This paper introduces a website called
Minna no Hon?yaku (MNH, ?Translation
for All?), which hosts online volunteer
translators. Its core features are (1) a
set of translation aid tools, (2) high qual-
ity, comprehensive language resources,
and (3) the legal sharing of translations.
As of May 2010, there are about 1200
users and 4 groups registered to MNH.
The groups using it include such major
Figure 1: Screenshot of ?Minna no Hon?yaku?NGOs as Amnesty International Japan
site (http://trans- )and Democracy Now! Japan. aid.jp
1 Introduction
This paper introduces a website called Minna Second, MNH provides comprehensive lan-
no Hon?yaku (MNH, ?Translation for All?, Fig- guage resources, which are easily looked up in
ure 1), which hosts online volunteer translators QRedit. MNH, in cooperation with Sanseido,
(Utiyama et al, 2009).1 Its core features are (1) a provides ?Grand Concise English Japanese Dic-
set of translation aid tools, (2) high quality, com- tionary? (Sanseido, 2001) and plans to provide
prehensive language resources, and (3) the legal ?Grand Concise Japanese English Dictionary?
sharing of translations. (Sanseido, 2002) in fiscal year 2010. These dic-
First, the translation aid tools in MNH con- tionaries have about 360,000 and 320,000 en-
sist of the translation aid editor, QRedit, a bilin- tries, respectively, and are widely accepted as
gual concordancer, and a bilingual term extrac- standard and comprehensive dictionaries among
tion tool. These tools help volunteer translators translators. MNH also provides seamless access
to translate their documents easily as described to the web. For example, MNH provides a dictio-
in Section 3. These tools also produce language nary that was made from the English Wikipedia.
resources that are useful for natural language This enable translators to reference Wikipedia
processing as the byproduct of their use as de- articles during the translation process as if they
scribed in Section 4. are looking up dictionaries.
1Currently, MNH hosts volunteer translators who trans- Third, MNH uses Creative Commons Li-
late Japanese (English) documents into English (Japanese). censes (CCLs) to help translators share their
The English and Japanese interfaces are available at http: translations. CCLs are essential for sharing and//trans-aid.jp/en and http://trans-aid.
jp/ja, respectively. opening translations.
63
Figure 2: Screenshot of QRedit
2 Related work
There are many translation support tools, such
as Google Translator Toolkit, WikiBABEL (Ku-
maran et al, 2009), BEYtrans (Bey et al, 2008),
Caitra (Koehn, 2009) and Idiom WorldServer
system,2 an online multilingual document man-
agement system with translation memory func-
tions.
The functions that MNH provides are closer
to those provided by Idiom WorldServer, but
MNH provides a high-quality bilingual dictio-
naries and functions for seamless Wikipedia and
web searches within the integrated translation
aid editor QRedit. It also enables translators to
share their translations, which are also used as
language resources.
3 Helping Volunteer translators
This section describes a set of translation aid
tools installed in MNH.
3.1 QRedit
QRedit is a translation aid system which is de-
signed for volunteer translators working mainly
online (Abekawa and Kageura, 2007). When a
URL of a source language (SL) text is given to
QRedit, it loads the corresponding text into the
left panel, as shown in Figure 2. Then, QRedit
automatically looks up all words in the SL text.
When a user clicks an SL word, its translation
candidates are displayed in a pop-up window.
2http://www.idiominc.com/en/
Figure 3: Screenshot of bilingual concordancer
3.2 Bilingual concordancer
The translations published on MNH are used
to make a parallel corpus by using a sentence
alignment method (Utiyama and Isahara, 2003).
MNH also has parallel texts from the Amnesty
International Japan, Democracy Now! Japan,
and open source software manuals (Ishisaka et
al., 2009). These parallel texts are searched by
using a simple bilingual concordancer as shown
in Figure 3.
3.3 Bilingual term extraction tool
MNH has a bilingual term extraction tool that
is composed of a translation estimation tool
(Tonoike et al, 2006) and a term extraction tool
(Nakagawa and Mori, 2003).
First, we apply the translation estimation tool
to extract Japanese term candidates and their En-
glish translation candidates. Next, we apply the
term extraction tool to extract English term can-
didates. If these English term candidates are
found in the English translation candidates, then,
we accept these term candidates as the transla-
tions of those Japanese term candidates.
4 Fostering language resources
Being a ?one stop? translation aid tool for on-
line translators, MNH incorporates mechanisms
which enable users to naturally foster impor-
tant translation resources, i.e. terminological re-
sources and translation logs.
64
4.1 Terminological resources
As with most translation-aid systems, MNH pro-
vides functions that enable users to register their
own terminologies. Users can assign the status
of availability to the registered terms. They can
keep the registered terms for private use, make
them available for a specified group of people,
or make them publicly available. Several NGO
groups are using MNH for their translation activ-
ities. For instance, Amnesty International, which
uses MNH, maintains a list of term translations
in the field of human rights by which translators
should abide. Thus groups such as Amnesty up-
load a pre-compiled list of terms and make them
available among volunteers. It is our assumption
and aim that these groups make their termino-
logical resources not only available among the
group but also publicly available, which will cre-
ate win-win situation: NGOs and other groups
which make their lists of terms available will
have more chance of recruiting volunteer trans-
lators, while MNH has more chance of attracting
further users.
At the time of writing this paper (May 2010),
56,319 terms are registered, of which 45,843 are
made publicly available. More than 80 per cent
of the registered terms are made public. Cur-
rently, MNH does not identify duplicated terms
registered by different users, but when the num-
ber of registered terms become larger, this and
other aspects of quality control of registered
terms will become an important issue.
4.2 Translation corpus
Another important language resources accumu-
lated on MNH is the translation corpus. As
mentioned in the introduction, being a hosting
site, MNH naturally accumulates source and tar-
get documents with a clear copyright status. Of
particular importance in MNH, however, is that
it can accumulate a corpus that contains draft
and final translations made by human together
with their source texts (henceforth SDF corpus
for succinctness). This type of corpus is im-
portant and useful, because it can be used for
the training of inexperienced translators (for in-
stance, the MeLLANGE corpus, which contains
different versions of translation, is well known
for its usefulness in translator training (MeL-
LANGE, 2009)) and also because it provides
a useful information for improving the perfor-
mance of machine translation and translation-aid
systems. While the importance of such corpora
has been widely recognized, the construction of
such a corpus is not easy because the data are
not readily available due to the reluctance on the
side of translators of releasing the draft transla-
tion data.
The basic mechanisms of accumulating SDF
corpus is simple. Translators using MNH save
their translations to keep the data when they fin-
ish the translation. MNH keeps the log of up
to 10 versions of translation for each document.
MNH introduced two saving modes, i.e. snap-
shot mode and normal mode. The translation
version saved in the normal mode is overwrit-
ten when the next version is saved. Translation
versions saved in snapshot mode are retained, up
to 10 versions. Translators can thus consciously
keep the versions of their translations.
MNH can collect not only draft and final trans-
lations made by a single translator, but also those
made by different translators. MNH has a func-
tion that enables users to give permission for
other translators registered with MNH to edit
their original translations, thus facilitating the
collaborative translations. Such permission can
be open-ended, or restricted to a particular group
of users.
This function is of particular importance
for NGOs, NPOs, university classes and other
groups involved in group-based translation. In
these groups, it is a common process in transla-
tion that a draft translation is first made by inex-
perienced translators, which is then revised and
finalized by experienced translators. If an inex-
perienced translator gives permission of editing
his/her draft translations to experienced transla-
tors, the logs of revisions, including the draft and
final versions, will be kept on MNH database.
This is particularly important and useful for
the self-training of inexperienced translators and
thus potentially extremely effective for NGOs
and other groups that rely heavily on volunteer
65
Figure 4: Comparative view of different transla-
tion versions
translators. Many NGOs face chronically the
problem of a paucity of good volunteer transla-
tors. The retention rate of volunteer translators is
low, which increase the burden of a small num-
ber of experienced translators, leaving them no
time to give advice to inexperienced translators,
which further reduce the retention rate of volun-
teers. To overcome this vicious cycle, mecha-
nisms to enable inexperienced volunteer trans-
lators to train themselves in the cycle of actual
translation activities is urgently needed and ex-
pected to be highly effective. MNH provides a
comparative view function of any pairwise trans-
lation versions of the same document, as shown
in Figure 4. Translators can check which parts
are modified very easily through the compara-
tive view screen, which can effectively works as
a transfer of translation knowledge from experi-
enced translators to inexperienced translators.
At the time of writing this paper, MNH con-
tains 1850 documents that have more than one
translation versions, of which 764 are published.
The number of documents translated by a group
(more than one translator) is 110, of which 48 are
published. Although the number of translations
made by more than one translators is relatively
small, they are steadily increasing both in num-
ber and in ratio.
5 Conclusion
We have developed a website called Minna no
Hon?yaku (MNH, ?Translation for All?), which
hosts online volunteer translators. We plan to ex-
tend MNH to other language pairs in our future
work.
References
Abekawa, Takeshi and Kyo Kageura. 2007. QRedit:
An integrated editor system to support online vol-
unteer translators. In Digital humanities, pages 3?
5.
Bey, Y., K. Kageura, and C. Boitet. 2008. BEY-
Trans: A Wiki-based environment for helping on-
line volunteer translators. Yuste, E. ed. Topics in
Language Resources for Translation and Localisa-
tion. Amsterdam: John Benjamins. p. 139?154.
Ishisaka, Tatsuya, Masao Utiyama, Eiichiro Sumita,
and Kazuhide Yamamoto. 2009. Development of
a Japanese-English software manual parallel cor-
pus. In MT summit.
Koehn, Philipp. 2009. A web-based interactive com-
puter aided translation tool. In ACL-IJCNLP Soft-
ware Demonstrations.
Kumaran, A, K Saravanan, Naren Datha, B Ashok,
and Vikram Dendi. 2009. Wikibabel: A wiki-style
platform for creation of parallel data. In ACL-
IJCNLP Software Demonstrations.
MeLLANGE. 2009. Mellange. ttp://corpus.
leeds.ac.uk/mellange/ltc.tml.
Nakagawa, Hiroshi and Tatsunori Mori. 2003. Au-
tomaic term recognition based on statistics of com-
pound nouns and their components. Terminology,
9(2):201?209.
Sanseido. 2001. Grand Concise English Japanese
Dictionary. Tokyo, Sanseido.
Sanseido. 2002. Grand Concise Japanese English
Dictionary. Tokyo, Sanseido.
Tonoike, Masatsugu, Mitsuhiro Kida, Toshihiro Tak-
agi, Yasuhiro Sasaki, Takehito Utsuro, and Satoshi
Sato. 2006. A comparative study on composi-
tional translation estimation usign a domain/topic-
specific corpus collected from the web. In Proc. of
the 2nd International Workshop on Web as Corpus,
pages 11?18.
Utiyama, Masao and Hitoshi Isahara. 2003. Reli-
able measures for aligning Japanese-English news
articles and sentences. In ACL, pages 72?79.
Utiyama, Masao, Takeshi Abekawa, Eiichiro Sumita,
and Kyo Kageura. 2009. Hosting volunteer trans-
lators. In MT summit.
66
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), page 1,
Beijing, August 2010
Being Theoretical is Being Practical:
Multiword Units and Terminological Structure Revitalised
Kyo Kageura
University of Tokyo
kyo@p.u-tokyo.ac.jp
1 Invited Talk Abstract
Multiword units (MWUs) are critical in processing and understanding texts and have been extensively
studied in relation to their occurrences in texts. MWUs also play an essential role in organising vocab-
ulary, which is most prominently visible in domain-specific terminologies. There has been, however,
a limited and mostly theoretical concern with the latter aspect of MWUs; researchers interested in
NLP-related applications of terminologies have not paid sufficient attention to this aspect.
In this talk I will start by giving the basic framework within which the study of MWUs from the point
of view of vocabulary can be carried out, in the process clarifying the relationships between studies
of MWUs in texts and those in relation to vocabulary. I will then introduce some of the theoretical
studies in terminological structure which I have carried out in recent years. Referring to some of the
problems that practically-oriented research in terminology processing is currently facing, I will argue
why, how and in what possible ways the understanding of the roles MWUs take in terminological
structure constitute a sin qua non condition for making a breakthrough in current text-oriented studies
of terminological MWUs.
2 Speaker Biography
Kyo Kageura, PhD, is a Professor at the Library and Information Science Course, Graduate School of
Education, University of Tokyo. He works in the field of terminology and is interested in applying NLP
methods in constructing practicaly useful reference resources. His publications include Quantitative
Informatices (Maruzen, 2000, in Japanese) and The Dynamics of Terminology (John Benjamins, 2002).
He is currently the editor of the journal Terminology and a book series Terminology and Lexicography:
Research and Practice, both published by John Benjamins, with Professor Marie-Claude L?Homme of
the University of Montreal. He is also a member of the development and management team of an online
hosting site Minna no Hon?yaku (Translation of/by/for all: http://trans-aid.jp/).
1
