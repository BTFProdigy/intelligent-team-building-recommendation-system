Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 57?60,
Sydney, July 2006. c?2006 Association for Computational Linguistics
The SAMMIE System: Multimodal In-Car Dialogue
Tilman Becker, Peter Poller,
Jan Schehl
DFKI
First.Last@dfki.de
Nate Blaylock, Ciprian Gerstenberger,
Ivana Kruijff-Korbayova?
Saarland University
talk-mit@coli.uni-sb.de
Abstract
The SAMMIE1 system is an in-car multi-
modal dialogue system for an MP3 ap-
plication. It is used as a testing environ-
ment for our research in natural, intuitive
mixed-initiative interaction, with particu-
lar emphasis on multimodal output plan-
ning and realization aimed to produce out-
put adapted to the context, including the
driver?s attention state w.r.t. the primary
driving task.
1 Introduction
The SAMMIE system, developed in the TALK
project in cooperation between several academic
and industrial partners, employs the Information
State Update paradigm, extended to model collab-
orative problem solving, multimodal context and
the driver?s attention state. We performed exten-
sive user studies in a WOZ setup to guide the sys-
tem design. A formal usability evaluation of the
system?s baseline version in a laboratory environ-
ment has been carried out with overall positive re-
sults. An enhanced version of the system will be
integrated and evaluated in a research car.
In the following sections, we describe the func-
tionality and architecture of the system, point out
its special features in comparison to existing work,
and give more details on the modules that are in
the focus of our research interests. Finally, we
summarize our experiments and evaluation results.
2 Functionality
The SAMMIE system provides a multi-modal inter-
face to an in-car MP3 player (see Fig. 1) through
speech and haptic input with a BMW iDrive input
device, a button which can be turned, pushed down
and sideways in four directions (see Fig. 2 left).
System output is provided by speech and a graphi-
cal display integrated into the car?s dashboard. An
example of the system display is shown in Fig. 2.
1SAMMIE stands for Saarbru?cken Multimodal MP3 Player
Interaction Experiment.
Figure 1: User environment in laboratory setup.
The MP3 player application offers a wide range
of functions: The user can control the currently
playing song, search and browse an MP3 database
by looking for any of the fields (song, artist, al-
bum, year, etc.), search and select playlists and
even construct and edit playlists.
The user of SAMMIE has complete freedom in
interacting with the system. Input can be through
any modality and is not restricted to answers to
system queries. On the contrary, the user can give
new tasks as well as any information relevant to
the current task at any time. This is achieved by
modeling the interaction as a collaborative prob-
lem solving process, and multi-modal interpreta-
tion that fits user input into the context of the
current task. The user is also free in their use
of multimodality: SAMMIE handles deictic refer-
ences (e.g., Play this title while pushing the iDrive
button) and also cross-modal references, e.g., Play
the third song (on the list). Table 1 shows a typ-
ical interaction with the SAMMIE system; the dis-
played song list is in Fig. 2. SAMMIE supports in-
teraction in German and English.
3 Architecture
Our system architecture follows the classical ap-
proach (Bunt et al, 2005) of a pipelined architec-
ture with multimodal interpretation (fusion) and
57
U: Show me the Beatles albums.
S: I have these four Beatles albums.
[shows a list of album names]
U: Which songs are on this one?
[selects the Red Album]
S: The Red Album contains these songs
[shows a list of the songs]
U: Play the third one.
S: [music plays]
Table 1: A typical interaction with SAMMIE.
fission modules encapsulating the dialogue man-
ager. Fig. 2 shows the modules and their inter-
action: Modality-specific recognizers and analyz-
ers provide semantically interpreted input to the
multimodal fusion module that interprets them in
the context of the other modalities and the cur-
rent dialogue context. The dialogue manager de-
cides on the next system move, based on its model
of the tasks as collaborative problem solving, the
current context and also the results from calls to
the MP3 database. The turn planning module then
determines an appropriate message to the user by
planning the content, distributing it over the avail-
able output modalities and finally co-ordinating
and synchronizing the output. Modality-specific
output modules generate spoken output and graph-
ical display update. All modules interact with the
extended information state which stores all context
information.
Figure 2: SAMMIE system architecture.
Many tasks in the SAMMIE system are mod-
eled by a plan-based approach. Discourse mod-
eling, interpretation management, dialogue man-
agement and linguistic planning, and turn plan-
ning are all based on the production rule system
PATE2 (Pfleger, 2004). It is based on some con-
cepts of the ACT-R 4.0 system, in particular the
goal-oriented application of production rules, the
2Short for (P)roduction rule system based on (A)ctivation
and (T)yped feature structure (E)lements.
activation of working memory elements, and the
weighting of production rules. In processing typed
feature structures, PATE provides two operations
that both integrate data and also are suitable for
condition matching in production rule systems,
namely a slightly extended version of the general
unification, but also the discourse-oriented opera-
tion overlay (Alexandersson and Becker, 2001).
4 Related Work and Novel Aspects
Many dialogue systems deployed today follow a
state-based approach that explicitly models the
full (finite) set of dialogue states and all possible
transitions between them. The VoiceXML3 stan-
dard is a prominent example of this approach. This
has two drawbacks: on the one hand, this approach
is not very flexible and typically allows only so-
called system controlled dialogues where the user
is restricted to choosing their input from provided
menu-like lists and answering specific questions.
The user never is in control of the dialogue. For
restricted tasks with a clear structure, such an ap-
proach is often sufficient and has been applied suc-
cessfully. On the other hand, building such appli-
cations requires a fully specified model of all pos-
sible states and transitions, making larger applica-
tions expensive to build and difficult to test.
In SAMMIE we adopt an approach that mod-
els the interaction on an abstract level as collab-
orative problem solving and adds application spe-
cific knowledge on the possible tasks, available re-
sources and known recipes for achieving the goals.
In addition, all relevant context information is
administered in an Extended Information State.
This is an extension of the Information State Up-
date approach (Traum and Larsson, 2003) to the
multi-modal setting.
Novel aspects in turn planning and realization
include the comprehensive modeling in a sin-
gle, OWL-based ontology and an extended range
of context-sensitive variation, including system
alignment to the user on multiple levels.
5 Flexible Multi-modal Interaction
5.1 Extended Information State
The information state of a multimodal system
needs to contain a representation of contextual in-
formation about discourse, but also a represen-
tation of modality-specific information and user-
specific information which can be used to plan
system output suited to a given context. The over-
3http://www.w3.org/TR/voicexml20
58
all information state (IS) of the SAMMIE system is
shown in Fig. 3.
The contextual information partition of the IS
represents the multimodal discourse context. It
contains a record of the latest user utterance and
preceding discourse history representing in a uni-
form way the salient discourse entities introduced
in the different modalities. We adopt the three-
tiered multimodal context representation used in
the SmartKom system (Pfleger et al, 2003). The
contents of the task partition are explained in the
next section.
5.2 Collaborative Problem Solving
Our dialogue manager is based on an
agent-based model which views dialogue
as collaborative problem-solving (CPS)
(Blaylock and Allen, 2005). The basic building
blocks of the formal CPS model are problem-
solving (PS) objects, which we represent as
typed feature structures. PS object types form a
single-inheritance hierarchy. In our CPS model,
we define types for the upper level of an ontology
of PS objects, which we term abstract PS objects.
There are six abstract PS objects in our model
from which all other domain-specific PS objects
inherit: objective, recipe, constraint, evaluation,
situation, and resource. These are used to model
problem-solving at a domain-independent level
and are taken as arguments by all update opera-
tors of the dialogue manager which implement
conversation acts (Blaylock and Allen, 2005).
The model is then specialized to a domain by
inheriting and instantiating domain-specific types
and instances of the PS objects.
5.3 Adaptive Turn Planning
The fission component comprises detailed con-
tent planning, media allocation and coordination
and synchronization. Turn planning takes a set
of CPS-specific conversational acts generated by
the dialogue manager and maps them to modality-
specific communicative acts.
Information on how content should be dis-
tributed over the available modalities (speech or
graphics) is obtained from Pastis, a module which
stores discourse-specific information. Pastis pro-
vides information about (i) the modality on which
the user is currently focused, derived by the cur-
rent discourse context; (ii) the user?s current cog-
nitive load when system interaction becomes a
secondary task (e.g., system interaction while
driving); (iii) the user?s expertise, which is rep-
resented as a state variable. Pastis also contains
information about factors that influence the prepa-
ration of output rendering for a modality, like the
currently used language (German or English) or
the display capabilities (e.g., maximum number of
displayable objects within a table). Together with
the dialogue manager?s embedded part of the in-
formation state, the information stored by Pastis
forms the Extended Information State of the SAM-
MIE system (Fig. 3).
Planning is then executed through a set of pro-
duction rules that determine which kind of infor-
mation should be presented through which of the
available modalities. The rule set is divided in two
subsets, domain-specific and domain-independent
rules which together form the system?s multi-
modal plan library.
contextual-info:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
last-user-utterance:
:
[
interp : set(grounding-acts)
modality-requested : modality
modalities-used : set(msInput)
]
discourse-history:
: list(discourse-objects)
modality-info:
:
[
speech : speechInfo
graphic : graphicInfo
]
user-info:
:
[
cognitive-load : cogLoadInfo
user-expertise : expertiseInfo
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
task-info:
[
cps-state : c-situation (see below for details)
pending-sys-utt : list(grounding-acts)
]
Figure 3: SAMMIE Information State structure.
5.4 Spoken Natural Language Output
Generation
Our goal is to produce output that varies in the sur-
face realization form and is adapted to the con-
text. A template-based module has been devel-
oped and is sufficient for classes of system output
that do not need fine-tuned context-driven varia-
tion. Our template-based generator can also de-
liver alternative realizations, e.g., alternative syn-
tactic constructions, referring expressions, or lexi-
cal items. It is implemented by a set of straightfor-
ward sentence planning rules in the PATE system
to build the templates, and a set of XSLT trans-
formations to yield the output strings. Output in
German and English is produced by accessing dif-
ferent dictionaries in a uniform way.
In order to facilitate incremental development
of the whole system, our template-based mod-
ule has a full coverage wrt. the classes of sys-
59
tem output that are needed. In parallel, we are
experimenting with a linguistically more power-
ful grammar-based generator using OpenCCG4,
an open-source natural language processing en-
vironment (Baldridge and Kruijff, 2003). This al-
lows for more fine-grained and controlled choices
between linguistic expressions in order to achieve
contextually appropriate output.
5.5 Modeling with an Ontology
We use a full model in OWL as the knowledge rep-
resentation format in the dialogue manager, turn
planner and sentence planner. This model in-
cludes the entities, properties and relations of the
MP3 domain?including the player, data base and
playlists. Also, all possible tasks that the user may
perform are modeled explicitly. This task model
is user centered and not simply a model of the
application?s API.The OWL-based model is trans-
formed automatically to the internal format used
in the PATE rule-interpreter.
We use multiple inheritance to model different
views of concepts and the corresponding presen-
tation possibilities; e.g., a song is a browsable-
object as well as a media-object and thus allows
for very different presentations, depending on con-
text. Thereby PATE provides an efficient and ele-
gant way to create more generic presentation plan-
ning rules.
6 Experiments and Evaluation
So far we conducted two WOZ data collection
experiments and one evaluation experiment with
a baseline version of the SAMMIE system. The
SAMMIE-1 WOZ experiment involved only spo-
ken interaction, SAMMIE-2 was multimodal, with
speech and haptic input, and the subjects had
to perform a primary driving task using a Lane
Change simulator (Mattes, 2003) in a half of their
experiment session. The wizard was simulating
an MP3 player application with access to a large
database of information (but not actual music) of
more than 150,000 music albums (almost 1 mil-
lion songs). In order to collect data with a variety
of interaction strategies, we used multiple wizards
and gave them freedom to decide about their re-
sponse and its realization. In the multimodal setup
in SAMMIE-2, the wizards could also freely de-
cide between mono-modal and multimodal output.
(See (Kruijff-Korbayova? et al, 2005) for details.)
We have just completed a user evaluation to
explore the user-acceptance, usability, and per-
formance of the baseline implementation of the
4http://openccg.sourceforge.net
SAMMIE multimodal dialogue system. The users
were asked to perform tasks which tested the sys-
tem functionality. The evaluation analyzed the
user?s interaction with the baseline system and
combined objective measurements like task com-
pletion (89%) and subjective ratings from the test
subjects (80% positive).
Acknowledgments This work has been carried
out in the TALK project, funded by the EU 6th
Framework Program, project No. IST-507802.
References
[Alexandersson and Becker2001] J. Alexandersson and
T. Becker. 2001. Overlay as the basic operation for
discourse processing in a multimodal dialogue system. In
Proceedings of the 2nd IJCAI Workshop on Knowledge
and Reasoning in Practical Dialogue Systems, Seattle,
Washington, August.
[Baldridge and Kruijff2003] J.M. Baldridge and G.J.M. Krui-
jff. 2003. Multi-Modal Combinatory Categorial Gram-
mar. In Proceedings of the 10th Annual Meeting of the
European Chapter of the Association for Computational
Linguistics (EACL?03), Budapest, Hungary, April.
[Blaylock and Allen2005] N. Blaylock and J. Allen. 2005. A
collaborative problem-solving model of dialogue. In Laila
Dybkj?r and Wolfgang Minker, editors, Proceedings of
the 6th SIGdial Workshop on Discourse and Dialogue,
pages 200?211, Lisbon, September 2?3.
[Bunt et al2005] H. Bunt, M. Kipp, M. Maybury, and
W. Wahlster. 2005. Fusion and coordination for multi-
modal interactive information presentation: Roadmap, ar-
chitecture, tools, semantics. In O. Stock and M. Zanca-
naro, editors, Multimodal Intelligent Information Presen-
tation, volume 27 of Text, Speech and Language Technol-
ogy, pages 325?340. Kluwer Academic.
[Kruijff-Korbayova? et al2005] I. Kruijff-Korbayova?,
T. Becker, N. Blaylock, C. Gerstenberger, M. Kai?er,
P. Poller, J. Schehl, and V. Rieser. 2005. An experiment
setup for collecting data for adaptive output planning in
a multimodal dialogue system. In Proc. of ENLG, pages
191?196.
[Mattes2003] S. Mattes. 2003. The lane-change-task as a tool
for driver distraction evaluation. In Proc. of IGfA.
[Pfleger et al2003] N. Pfleger, J. Alexandersson, and
T. Becker. 2003. A robust and generic discourse model
for multimodal dialogue. In Proceedings of the 3rd
Workshop on Knowledge and Reasoning in Practical
Dialogue Systems, Acapulco.
[Pfleger2004] N. Pfleger. 2004. Context based multimodal
fusion. In ICMI ?04: Proceedings of the 6th interna-
tional conference on Multimodal interfaces, pages 265?
272, New York, NY, USA. ACM Press.
[Traum and Larsson2003] David R. Traum and Staffan Lars-
son. 2003. The information state approach to dialog man-
agement. In Current and New Directions in Discourse and
Dialog. Kluwer.
60
An Extended Architecture for Robust  Generation* 
T i lman Becker ,  Anne  K i lger ,  Pat r i ce  Lopez ,  Peter  Po l le r  
DFK I  GmbH 
Stuh lsatzenhausweg 3 
D-66123 Saarbr i i cken ,  Germany 
{becker, kilger, lopez, poller}@dfki, de 
Abst rac t  
Based on our experiences in VERBMOBIL, a large 
scale speech-to-speech translation system, we iden- 
tify two types of problems that a generation com- 
ponent must address in a realistic implementation 
and present relevant examples. As an extension to 
the architecture ofa translation system, we present a
module for robustness preprocessing on the interface 
between translation and generation. 
1 In t roduct ion  
Based on our experiences with VERBMOBIL, a large 
scale speech-to-speech translation system, we iden- 
tify two types of problems that a generation com- 
ponent must address in a comprehensive implemen- 
tation. Besides general task-inherent problems like, 
e.g., the processing of spontaneous speech input, the 
translation step itself, and real-time processing, we 
found that an implementation of such a large scale 
system additionally exhibits technical problems that 
are caused by various faults in the steps prior to 
generation. 
The task of VERBMOBIL is the multi-lingual (Ger- 
man, English, Japanese) speaker-independent trans- 
lation of spontaneous peech input that enables 
users to converse about the scheduling of a busi- 
ness appointment including travel, accommodation, 
and leisure time planning in a multi-lingual dia- 
logue. The system covers 10,000 words in each 
language with the corresponding knowledge bases 
(grammars, translation rules, etc.). In contrast to 
a text translation system, the processing of spon- 
taneous speech requires extended functionalities in 
almost ever:,- module because the system has to be 
able do deal with, e.g., ill-formed and disfluent (hes- 
Due to the high complexity of this task, the sys- 
tem is subdivided into 24 separate subtasks (imple- 
mented modules). 
For the translation step the system contains 
four different parallel translation "tracks" consist- 
ing Of three "shallow" (case based, statistical, and 
dialogue-act based (Reithinger, 1999)) and one 
"deep" translation track (see figure 1) for each lan- 
guage. The individual translation results are partly 
associated with confidence values reflecting their 
quality and then sent to a special selection compo- 
nent to choose the most appropriate one. Our prac- 
tical experience shows that there are cases in which 
the input to the generation component is impossible 
to process. Here the shallow translation paths serve 
as a fall-back in order to fulfill the strong necessity 
of a translation result as far as possible. 
Although some parts of the analysis task (e.g., re- 
solving scopal ambiguities) can actually be left un- 
solved when they are not necessary for the transla- 
tion task, in general, problems in some module result 
in an accumulated inadequacy of the final transla- 
tion. 
Since the translation task is distributed to a set 
of cooperating modules, there is a choice of solving 
the task inherent and technical problems either lo- 
cally inside the individual modules or handing them 
to problem specific correction modules. We found 
that robustness must be included in every module. 
For the architecture of the generation module, we 
have devised a submodule for robustness that pre- 
processes the input data. This proved an elegant 
and promising extension to achieve the required local 
module robustness without touching tile core gener- 
ation module directly. A similar module also ex- 
ists for analysis (see 'Robust Semantics' in figure 1), 
itations, repetitions, repairs) speech input. In a dia- (Worm, 1998). 
logue system, there is also.anapparently simp! ebut :. ? In.this paper~ we,foeus..on the generation eompo- 
very strong constraint on the system to achieve its 
task: For each user input the system has to produce 
a translation result. 
" The research within VERBMOBIL presented here is funded 
by the German Ministry of Research and q~mhnology under 
grant 011\.'101K/1. 
nent of our system. Besides the general robustness 
requirements, the mentioned inadequacy accunmla- 
tion reaches its maxinmm since generation is posi- 
tioned at the end of the translation process. In the 
following sections, we show how the strong robust- 
ness requirement influenced the architecture of our 
63 
User 1 
Language A 
User 2 
Language B 
Speech 
777 W ..... 
Figure 1: Simplified system architecture of the speech-to-speech translation system VERBMOBIL. 
generation module. We classify the above mentioned 
problems from the point of view of generation and 
present our solutions to these problems, mainly un- 
der the aspect of robust generation with problematic 
input. 
2 Task - inherent  and  Techn ica l  
Prob lems 
The problems for generation that arise in a speech- 
to-speech translation system fall into two main 
classes: as in any large-scale system, there will 
be software-engineering problems which we will call 
technical problems and there are task-inherent prob- 
lems that are particular to the translation task and 
the highly variable input in spontaneous peech. 
Since it is impossible to draw a clear line be- 
tween technical and task-inherent problems, we will 
present a short classification and then go into more 
detail without much discussion whether a certain 
problem should be viewed as technical or task- 
inherent. 
One would hope to be able to eliminate technical 
problems completely. However, in a large system, 
where development is distributed over many mod- 
ules (implemented at different sites), some robust- 
ness against certain technical problems can become 
a necessity, as our experiences have shown. This is 
even more important during the development phase- 
which a research system never leaves. Most technical 
problems have to do with violations of the interface 
definitions. Thisranges. from simple ~things uch as 
using unknown predicates in the semantic represen- 
tation to complex constructions that cannot be gen- 
erated (the generation gap). We actually regard the 
latter as a task-inherent problem. 
Secondly, tile task-inherent problems can be di- 
vided into problems that are caused by (i) spon- 
taneous speech input and (ii) insufficiencies in the 
analysis and translation steps. 
2.1 Robustness  in Ana lys i s  
The problems in (i) are quite varied and many cases 
are dealt with in analysis (and translation), some 
cases are dealt with in our robustness preprocess- 
ing submodule, a few in the classical submodules of 
generation. For example, there is a separate mod- 
ule on the level of speech recognition which deals 
with hesitations and self-corrections. Phenomena 
like ellipsis, phrasal and other incomplete utterances 
are handled by analysis, so generation must be able 
to deal with the corresponding semantic representa- 
tions too. Agreement errors are handled (i.e., cor- 
rected) in analysis. But some serious syntactic errors 
cannot be corrected. However, at least the maxi- 
mal analyzable segments are determined so that un- 
grammatical utterances are translated as sequences 
of several meaningful segments. 
2.2 Robustness  in Generat ion  
The problems in (ii) are caused by an accunmla- 
tion of problems which result in (semantic) input to 
the generator that cannot be processed. Robustness 
in our system concentrates on this type of problenl 
which is and should be handled as a separate step 
between analysis/transfer and generation. (See the 
discussion of the architecture in section 3.) 
The list below contains some examples that are 
picked up again in section 4. 
* Problems with the structure of the semantic rep- 
resentation: 
- unconnected subgraphs 
- multiple predicates referring to the same 
object 
64 
- omission of obligatory arguments 
? Problems with the content of the semantic rep- 
resentation: 
- contradicting information 
- missing information (e.g. agreement infor- 
mation) 
3 Arch i tecture  
As described in section t, the  deep processing in 
VERBMOBIL is based on a pipeline of modules which 
use a unique interface language (VIT 1) that incorpo- 
rates a semantic representation. Since this seman- 
tic representation is obviously grammar-independent 
and could reflect the effects of spoken, spontaneous 
language, we have no guarantee that the gram- 
mar covers the semantic representation given by the 
transfer module. Consequently we have chosen to 
extend the classical generation architecture with a 
new module dedicated to robust preprocessing. We 
first present our classical generator architecture (see 
also (Becker et al, 1998; Becker et al, 2000)) in 
terms of the RAGS architecture and then discuss its 
extension to the task-inherent problems. 
The RAGS architecture (Cahill et al, 1999) is a 
reference architecture for natural language genera- 
tion systems. Reflecting the common parts of natu- 
ral language generation systems, this proposal aims 
to provide a standard architecture allowing the iden- 
tification of some important generation subtasks and 
resources. By presenting our system in the light of 
the RAGS specifications, our goal is to propose gen- 
eral solutions that could be used by other researchers 
who need to extend their own generation architec- 
ture to similar tasks. 
While the macro-planning task is important and 
mandatory in text generation, it is limited in dia- 
logue translation. Most of the related problems, for 
instance the sentence segmentation a d the pronoun 
choices, have been solved by the user in the source 
language. Considering the RAGS architecture, con- 
ceptual and rhetorical evels of representation are 
also outside the scope of our system. Our architec- 
ture consists of four main modules (see figure 2). 
For an easy adaptation to other domains and lan- 
guages, we have emphasized an organization based 
on a general kernel system and the declarativity of 
knowledge sources (Becker et al, 1998). All but the 
first modules are captured by the RAGS architec- 
ture. However, the first module is dedicated solely 
to robustness in the specific speech-to-speech trans- 
lation task and will be presented and discussed last 
in this section. It can easily be added to a RAGS- 
like system whose whiteboard is perfectly suited for 
lVerbmobil Interface Term, (Bos et al, 1996; Dorna, 
1996) 
the transformations that the robustness preprocess- 
ing module performs. 
Robustness 
Preprocessing 
Module 
Standard 
Generation 
Module 
Repairing Strutural 
kx~.~ss ing  GHae~risfics for Generation J 
(%e:::::: 
Selecting Planning Ru les~ 
Checking Lexical Choice J 
C0nstraints . - ~ . .  
e Selecting LTAG Trees 
e Tree Combination 
? Inflection 
e Synthesis Annotation 
Figure 2: An extended generation system architec- 
ture 
M ic rop lann ing  Modu le  At the level of sentence 
generation, the quality of the planning process de- 
pends on the interdependencies between conceptual 
semantics, predicative semantics and syntax. A par- 
ticular lexical choice can imply constraints on other 
lexical items. The role of the microplanner is to re- 
alize lexical choices in a way that a syntactic realiza- 
tion is possible and costly backtracking is prevented. 
The microplanning task can be viewed as a con- 
straint solving problem and implemented using an 
adapted constraint solving mechanism in order to 
achieve efficiency, flexibility, and declarativity of 
knowledge. The microplanner produces a depen- 
dency tree representation i dicating for each node 
a set of syntactical constraints to be fulfilled by 
the corresponding lexical syntactic units (predicate, 
tense, aspect, mood, etc.). 
Syntact ic  Rea l izat ion Modu le  This module is 
in charge of the concrete syntax generation. The 
processing is .based ,on a fully lexicatized Tree- 
Adjoining Grammar derived from the HPSG gram- 
mar used in the deep-processing parser module 
(Kasper~et aL, 1995; Becker, 1998). 
S u r f a c e  Real izat ion  Modu le  The syntactic re- 
alization module produces a derived tree from which 
tile output string is extracted. The morphological 
features in preterminal nodes are used for inflection. 
The surface string is also annotated by syntactic in- 
formation (phrase boundary, aspect, sentence mood) 
65 
that are exploited by the speech synthesis module. 
Robustness Preprocess ing  Modu le  We have 
described three modules corresponding to classical 
tasks of generation systems and pointed out at the 
beginning of this section the necessity for robustness. 
Where can we integrate the required robustness in 
such a generation architecture? One approach could 
be the relaxation of constraints during the syntac- 
tic realization (relaxing word order or/and depen- 
dency relations). One can argue against this ap- 
proach that: 
clearly separated from the microplanning rules, jus- 
tifying our presentation of robustness as a separate 
module. 
4.2 Conforming  to the Interface Language 
Definition 
The definition of the interface language 2 comprises 
only its syntax and some semantic constraints. 
There is an implementation of expressions in the in- 
terface language as an abstract data type which can 
at least check syntactic conformity (Dorna, 1996). 
But we also have to deal with semantic faults. 
-*. There is no .straightf~r~ard~Way~t~Aimi.t~he.J~e~.:.~.~`-~,;~T~f~rs~e~amp~e~i~''minating>r`0bust~pre~r~ess- 
laxation of syntactic onstraints only to the ro- 
bustness problematic structures. 
? We must be sure that the microplanning module 
can deal with problematic semantic input. 
These points suggest to check and repair the 
inconsistencies of the semantic representation as 
early as possible, i.e., before sentence microplanning. 
Moreover we show in the next section that most of 
the problems presented in section 2 can be identified 
based on the microplanner input. 
We now present more concretely the robust pre- 
processing module. 
4 Robustness  
In this section we describe the types of problems 
defined in section 2 using examples from our system 
and discuss how our module is made robust enough 
to handle a lot of these problems. 
Before the semantic representation is handed to 
microplanning, the robustness preproeessing module 
of the generator checks the input, inspecting its parts 
for known problems. For each problem found, the 
preprocessor lowers a confidence value for the gen- 
eration output which measures the reliability of our 
result. In a number of cases, we use heuristics to fix 
problems, aiming at improved output. 
As discussed in section 2, problems in the input 
to the generator can be technical or task-inherent. 
Technical problems manifest themselves as faults 
wrt. the interface language definition, whereas the 
task-inherent problems concern mismatches between 
a specific semantic expression and the coverage of 
the natural language grammar used in the genera- 
tor. These mismatches are known as the generation 
gap (Meteer, 1990). 
4.1 Dec la ra t iv i ty  
In..our implementation; the  :robustness module is 
partly integrated into the constraint solving ap- 
proach of the microplanning module. Using the con- 
straint solving approach allows for a strict separa- 
tion of algorithms (i.e., some constraint solving al- 
gorithln) and declarative knowledge sources. On this 
level, the rules (constraints) for robustness can be 
ing is on the connectedness of the semantic input 
graph. Our interface language describes an interface 
term to contain a connected semantic graph plus an 
index pointing to the root of the graph. Two types 
of problems can occur according to this definition: 
Disconnectedness of the Graph: The robust- 
ness preprocessor checks whether the input 
graph is in fact connected. If there are several 
disconnected parts, a distinct generation call 
is made for each subgraph. In the end, all 
sub-results are connected to produce a global 
result. We are currently working on a better 
heuristic to order the sub-results, taking into 
account information about the word order in 
the source language. 
Wrong Index:  The robustness preprocessor tests 
whether the index points to the root of the 
graph or one of the subgraphs. For each sub- 
graph without an adequate index, we compute 
a local root pointer which is used for further 
processing. This turned out to be an easy and 
reliable heuristic, leading to good results. 
There are several types of technical problems 
which cannot be repaired well. Minimally, these 
cases are detected, warning messages are produced, 
and the confidence value is lowered. We apply 
heuristics where possible. Examples are unique- 
ness of labels (every semantic predicate must have 
a unique identifier), the use of undefined predicate 
names, and contradicting information (e.g., the use 
of  a DEFINITE and an INDEFINITE quantifier for the 
same object). In the case of incorrect predicate 
classes, i.e., where a predicate is used with an unde- 
fined-argument frame, only those parts of the input 
are handled which are analyzed as correct. 
4.3 Fal l ing into the Generat ion  Gap 
The robustness preprocessor even does more than 
checking for structural contradictions between in- 
put and interface language. Based on analyses of 
2A further complication in a research system like ours 
s tems from the fact that the interface language itself is de- 
veloped, i.e., changed over time. 
66 
a large amount of test-suites it is fed with some 
heuristics which help to bridge the generation gap 
that reflects the unpredictability, whether_a specific 
semantic structure can be mapped to an acceptable 
utterance in the target language. Some examples of 
heuristics used in our system are as follows: 
Conf l ic t ing In fo rmat ion :  Often it is inconsistent 
to allow several predicates to include the same 
depending structure in their argument frames, 
e.g., two predicates describing different prepo- 
sitions should not point to the same entity. We 
have to pick one-,possibitity~heuristically: ........ 
Gaps  in Generat ion  Knowledge:  There are in- 
put configurations that have no reflection 
within the generator's knowledge bases, e.g., 
the DISCOURSE predicate defining a sequence 
of otherwise unrelated parts of the input. The 
robustness preprocessor removes this predicate, 
thereby subdividing the connected graph into 
several unconnected ones and continuing as for 
disconnected graphs described above. 
Other examples for generation constraints that 
can conflict with the input are the occurrence 
of some specific cyclic subparts of graphs, self- 
referring predicates, and chains of predicates 
which are not realizable in generation. 
Robustness  inside the  Microp lanner  and the  
Syntact i c  Generator  additionally helps to get rid 
of some generation gap problems: 
Cont rad ic t ions  to Generat ion  Constra ints :  
The knowledge bases of the generator (mi- 
eroplanning rules, grammar and lexicon) 
describe constraints on the structure of the 
output utterance that might conflict with the 
input. A common problem occuring in our 
system is the occurrence of subordinating 
predicates with empty obligatory arguments. 
Here the microplanner relaxes the constraint 
for argument completeness and hands over a 
structure to the syntactic generator that does 
not fulfill all syntactic constraints or contains 
elliptical arguments. In these cases, the gram- 
mar constraints for obligatory arguments are 
relaxed in the syntactic generator and elliptical 
arguments are allowed.beyond the constraints 
of the grammar. The result is often output that 
reflects the spontaneous speech input which we 
accept for the sake of robustness. 
M iss ing  a t t r ibutes :  Often there are obligatory at- 
tributes for the semantic predicates missing in 
the input, e.g., statements about the direction- 
ality of prepositions, agreement information, 
etc. The generator uses heuristics to choose a 
value for its own. 
Cont rad ic t ions  on  the  Semant ic  Level: Some 
attributes may lead to conflicts during genera- 
tion,.e.g:, i f~ pronoun is given:as SORT~HUMAN 
and TYPE-~SPEAKER. The generator uses a 
heuristics to set the value of SORT in this case. 
So lv ing Part  o f  the  Ana lys i s  Task: Sometimes 
the input to the generator is underspecified in
a way that it can be improved easily by using 
simple heuristics to "continue analysis." A 
common example in. our system is an input 
expression like "on the third" which often is 
.... .. ~analyzed..as.. (ABSTR.,-NOM .A  OPal)(.3.).), e~-e,,..an 
elliptical noun with ordinal number 3. We 
add the sort TIME_DOFM 3 to the attributes of 
ABSTR_NOM SO that, e.g., a semantic relation 
TEMPORAL_OR_LOCAL is correctly mapped to 
the German preposition "an." 
4.4 How much robustness?  
There is a limit to the power of heuristics that we 
have determined using a large corpus of test data. 
Some examples for possible pitfalls: 
? When realizing "empty nominals" ABSTR_NOM 
as elliptical nouns, guessing the gender can 
cause problems: "Thursday is my free day ti " 
as FREE A DAY A ABSTR_NOM (with a reading 
as in "day job") might result in "*Donnerstag 
ist mein freies Tag ti." 
o Conflicts between sort and gender of a pronoun 
might be resolved incorrectly: "Es (English: 
'it') trifft sich ganz hervorragend" with PRON 
(GENDER:NTR, SORT~HUMAN) should not be 
translated as "#He is really great." 
Although the boundary beyond which deep trans- 
lation cannot be achieved even with heuristics is 
somewhat arbitrary, the big advantage of deep pro- 
cessing lies in the fact that the system 'knows' its 
boundaries and actually fails when a certain level of 
quality cannot be guaranteed. As discussed in sec- 
tion 1, in a dialogue system, a bad translation fight 
still be better than none at all, so one of the shallow 
modules can be selected when deep processing fails. 
5 Re la ted  Work  and  Conc lus ions  
VERB~;IOBIL also contains a component hat au- 
tomatically generates dialogue scripts and result 
summaries of the dialogues in all target languages 
(Alexandersson and Poller, 1998; Alexandersson and 
::Poller~ 2000 )~:: ~This component , uses the'generation 
modules of VERB1V\[OBIL for sentence generation as 
well as the translation system itself to achieve multi- 
linguality. To some extend, this task also benefits 
3DOFM: day of the month. Note that in this paper, the 
presentation of the semantic representation language is highly 
abstracted from tile actual interface language. 
67 
from the task inherent robustness features of the 
overall system and its modules which we described 
in this paper. 
Our problem classification also shows up in other 
generation systems. There is a multi-lingual gen- 
eration project (Uchida et al, 1999) that utilizes 
an interlingua-based semantic representation to gen- 
erate web-documents in different output languages 
from one common representation. Although techni- 
cal problems are less prominent, the task-inherent 
problems are almost he same. Again, the genera- 
R. Kasper, B. Kiefer, K. Netter, and K. Vijay- 
Shanker. 1995. Compilation of hpsg to tag. In 
. . . . .  Proceedings o f  .the:..33rd ~Aunual: Meeting :of the 
Association for Computational Linguistics, pages 
92-99, Cambridge, Mass. 
M.W. Meteer. t990. The "Generation Gap" - The 
Problem of Expressibility in Text Planning. Ph.D. 
thesis, Amherst, MA. BBN Report No. 7347. 
Norbert Reithinger. 1999. Robust information ex- 
traction in a speech translation system. In Pro- 
ceedings of EuroSpeech-99, pages 2427-2430. 
tot has to able to deal with, e.g., disconnected or Hiroshi Uchida, Meiying Zhu, and Tarcisio Della 
? contradicting inpu't graphs: . . . . . . . . . . . . . . . . . . . .  ~" ~ Sieiiit~2 "1999.-UNL::~I~S;" U/iitei:l ~Na~i6hg"Ufii-~~ ::: - 
sity, Tokyo, Japan, November. 
Re ferences  
Jan Alexandersson and Peter Poller. 1998. Toward 
multilingual protocol generation for spontaneous 
speech dialogues. In Proceedings of the Ninth In- 
ternational Workshop on Natural Language Gen- 
eration, Niagara-on-the-Lake, Ontario, Canada, 
August. 
Jan Alexandersson and Peter Poller. 2000. Multi- 
lingual summary generation i a speech-to-speech 
translation system for multilingual negotiation di- 
alogues. In Proceedings of INLG 2000, Mitzpe Ra- 
mon, Israel, June. 
T. Becker, W. Finkler, A. Kilger, and P. Poller. 
1998. An efficient kernel for multilingual genera- 
tion in speech-to-speech dialogue translation. In 
Proceedings of COLING/A CL-98, Montreal, Que- 
bec, Canada. 
Tilman Becker, Anne Kilger, Patrice Lopez, and 
Peter Poller. 2000. Multilingual generation for 
translation in speech-to-speech dialogues and its 
realization in verbmobil. In Proceedings of ECAI 
2000, Berlin, Germany, August. 
Tihnan Becker. 1998. Fully lexicalized head- 
driven syntactic generation. In Proceedings of the 
Ninth International Workshop on Natural Lan- 
guage Generation, Niagara-on-the-Lake, Ontario, 
Canada, August. 
Johan Bos, Bj6rn Gambfi.ck, Christian Lieske, 
Yoshiki Mori, Manfred Pinkal, and Karsten 
Worm. 1996. Compositional semantics in verb- 
mobil. In Proceedings of Coling '96, Copenhagen, 
Denmark. 
Lynne Cahill, Christy Doran, Roger Evans, Chris 
Mellish, Daniel Paiva, Mike Reape, Donia Scott, 
and Neil Tipper. 1999. Towards a Reference Ar- 
chitecture for Natural Language: Generation Sys- 
tems. Technical Report ITRI-99-14, Information 
Technology Research Institute (ITRI), University 
of Brighton. 
Michael Dorna. 1996. The adt package for the verb- 
mobil interface term. Verbmobil-lrleport 104, Uni- 
versity Stuttgart, April. 
Karsten Worm. 1998. A model for robust processing 
of spontaneous speech by integrating viable frag- 
ments. In Proceedings of COLING-ACL '98, Mon- 
treal, Canada. 
68 
Multilingual Summary Generation in a Speech-To-Speech 
Translation System for Multilingual Dialogues* 
J an  A lexandersson ,  Peter  Po l le r ,  M ichae l  K ipp ,  Ra l f  Enge l  
DFK I  GmbH 
Stuh lsatzenhausweg 3 
66123 Saarbr f i cken  
{alexanders son, poller, engel, kipp}@dfki, de 
Abst rac t  
This paper describes a novel functionality of the 
VERBMOBIL system, a large scale translation sys- 
tem designed for spontaneously spoken multilingual 
negotiation dialogues. The task is the on-demand 
generation of dialogue scripts and result summaries 
of dialogues. We focus on summary generation and 
show how the relevant data are selected from the 
dialogue memory and how they are packed into 
an appropriate abstract representation. Finally, we 
demonstrate how the existing generation module of 
VERBMOBIL was extended to produce multilingual 
and result summaries from these representations. 
1 I n t roduct ion  
In the last couple of years different methods for 
summarization have been developed. In this pa- 
per we report on a new system functionality within 
the scope of VERBMOBIL (Bub et al, 1997), a fully 
implemented speech-to-speech translation system, 
that generates German or English dialogue scripts 
(Alexandersson and Poller, 1998) as well as Ger- 
man or English summaries of a multilingual nego- 
tiation dialogue held with assistance of the system. 
By a script we mean a document hat reflects the 
domain-specific propositional contents of the indi- 
vidual turns of a dialogue as a whole, while a sum- 
mary gives a compact summarization of all negotia- 
tions the dialogue participants agreed on. 
The key idea behind our approach is to utilize 
as many existing resources as possible. Conceptu- 
ally we have added one module (although techni- 
cally realized in different already existing modules 
of the overall VERBMOBIL system) - the summary 
generator. Besides formatting, our new module gen- 
erates sequences of language specific (i.e., German) 
semantic representations for thegeneration of Sam: 
maries/seripts based on the content of the dialogue 
memory (Kipp et al, 1999). These descriptions are 
? The research within VERBMOBIL presented here is funded 
by the German Ministry of Research and Technology under 
grant 011V101K/1. The authors would like to thank Tilman 
Becker for comments on earlier drafts on this paper, and 
Stephan Lesch for invaluable help with programming. 
realized into text by the existing VERBMOBIL gen- 
erator (Becker et al, 1998). To produce multilingual 
summaries we utilize the transfer module of VERS- 
MOBIL (Dorna and Emele, 1996). 
The next section gives an overview of the VERB- 
MOBIL system focusing on the modules central for 
the production of summaries/scripts. It is followed 
by a section describing the extraction and mainte- 
nance of summary relevant data. We then describe 
the functionality of the summary generator in detail. 
An excerpt of the sample dialogue we refer to in the 
paper is given at the end of the paper. 
2 P rerequ is i tes  
VERBMOBIL is a speech-to-speech translation 
project, which at present is approaching its end and 
in which over 100 researchers 1 at academic and in- 
dustrial sites are developing a translation system 
for multilingual negotiation dialogues (held face to 
face or via telephone) using English, German, and 
Japanese. The main difference between VERBMO- 
BIL and, c.f., man-machine dialogue systems is that 
VERBMOBIL mediates the dialogue instead of con- 
trolling it. Consequently, the complete dialogue 
structure as well as almost the complete macro- 
planning is out of the system's control. 
The running system of today is complex, consist- 
ing of more than 75 separate modules. About one 
third of them concerns linguistic processing and the 
rest serves technical purposes. (For more informa- 
tion see for instance (Bub et al, 1997)). For the sake 
of this paper we concentrate on a small part of the 
system as shown in figure 1. 
A user contribution is called a turn which is di -  
vided into segments. A segment ideally resembles 
a complete sentence as we know it from traditional 
grammars,  However; because :of -the. spontaneity of 
the user input and because the turn is chunked by 
a statistical process, the input segments for the lin- 
guistic components are sometimes merely pieces of 
linguistic material. For the dialogue memory and 
one of the shallow translation components the dia- 
lSee http://verbmobil.dfki.de for the list of project 
partners. 
148 
Data  - ~ 
Figure 1: Part of the VERBMOBIL system 
logue act (Alexandersson et al, 1998) plays an im- 
portant role. The dialogue act represents the com- 
municative function of an utterance, which is an im- 
portant information for the translation as well as the 
modeling of the dialogue as a whole. Examples of il- 
locutionary acts are REQUEST and GREET. Other 
acts can carry propositional content, like SUGGEST 
and INFORM_FEATURE. 
To obtain a good translation and enhance the 
robustness of the overall system the translation is 
based on several competing translation tracks, each 
based on different paradigms. The deep translation 
track consists of an HPSG based analysis, semantic 
transfer and finally a TAG-based generator (VM- 
GECO). The linguistic information within this track 
is encoded in a so-called VIT 2 (Bos et al, 1996; 
Dorna, 1996) which is a formalism following DRT. 
It consists of a set of semantic onditions (i.e. predi- 
cates, roles, operators and quantifiers) and allows for 
underspecification with respect o scope and subor- 
dination or inherent underspecification. A graphical 
representation of the VIT for the English sentence 
"They will meet at the station" is shown in figure 2. 
Besides the deep translation track several shallow 
tracks have been developed. The main source of 
input for the generation of summaries comes from 
one of these shallow analysis components (described 
in section 3) which produces dialogue acts, topic 
suggestions and expressions in a new knowledge 
representation language called DIREX 3. These ex- 
pressions represent domain related information like 
source and destination-o!ties~ dates;-important hotel 
related data, and meeting points. This input is pro- 
cessed by the dialogue module which computes the 
relevant (accepted) objects of the negotiation (each 
consisting of dialogue act, topic, and a DIREX) 
Figure 3 shows the conceptual architecture, where 
2Verbmobil Interface Term 
aDomaln Represematioa EXpression 
. J.d.=C,.i;,hi3, h2) 
B Z I  ... II, ' " 
Figure 2: Graphical representation f VIT for "They 
will meet at the station" 
the summary generation process as a whole is indi- 
cated with thicker lines. It consists of the following 
steps: 
o Content  Select ion:  The relevant structures are 
selected from the dialogue memory. 
. ..o .Summary~ Generat ion :  These- Structures are 
converted into sequences of semantic descriptions 
(VITs) of full sentences for German (see section 4). 
o Transfer :  Depending on the target language, the 
German sentence VITs are sent through the transfer 
module. 
* Sentence Generat ion :  The VITs are generated 
by the existing VERBMOBIL generator (Becker et al, 
149 
Figure 3: Conceptual Architecture of the Summary Generation Process 
2000). . . . .  
? Presentat ion :  The sentences are incorporated 
into the final, e.g., HTML document. 
Throughout he paper we will refer to a German- 
English dialogue (see appendix for an excerpt). 
The information presented there is the spoken sen- 
tence(s) together with the information extracted as 
described in section 3. To save space we only present 
parts of it, namely those which give rise to the struc- 
tures in figure 4. 
3 Ext rac t ion  and  Ma intenance  o f  
Protocol Relevant Data 
The dialogue memory gets its input from one of 
the shallow translation components, which bases 
its translation on the dialogue act and Dll:tEX- 
expression extracted from the segment. The input 
is a triple consisting of: 
? D ia logue  Act  representing the intention of the 
segment. 
? Topic is one of the four topics scheduling, travel- 
ing, accommodation and entertainment. 
? D i rex  representing the propositional content of 
the segment. 
For the extraction of propositional content and in- 
tention we use a combination of knowledge based 
and statistical methods. To compute the propo- 
sitional content finite state transducers (FSTs) 
(Appelt et al, 1993) with built-in functions are 
used (Kipp et al, 1999). The intention (represented 
by a dialogue act) is computed statistically us- 
ing language models (Reithinger and Klesen, 1997). 
Both methods were chosen because of their robust- 
ness - since the speech recognizers have a word error 
rate of about 20%, we cannot expect sound input 
for the analysis. Also the segmentation of turns in 
utterances i stochastic and therefore sometimes de- 
livers suboptimal segments. Consider the input to 
be processed: 
I would  so we were  to leave Hamburg  on the 
f i rs t  
where the speech recognizer eplaced "good so we 
will" with "I would so we were to". The result of 
the extraction module looks like: 
..... """ "\[ITNFORMTtravel ing, he~s_move : \ [move, 
has_source_ locat  ion : \ [c ity,  has_name = 
' hamburg  ' \] , has_depar ture_ t ime : 
\ [date,  t ime= \ [day : i\] \] \] \] 
The result consists of the dialogue act INFORM, 
the topic suggestion t rave l ing ,  and and a DIREX. 
The top object is a move with two roles: A source 
location (which is a city - Hanover), and a departure 
time (which is a date - day 1). 
Dialog processing 
For each utterance, and hence each DIREX the di- 
alogue manager (1) estimates its relevance, and (2) 
enriches it with context. For summary generation, 
we are solely interested in the most specific, accepted 
objects. Therefore, we also (3) compute more spe- 
cific~general relations between objects: 
Relevance detection. Depending on the dialogue act 
of the current utterance different courses of action 
are taken. SUGGEST dialogue acts trigger the stor- 
age, completion, focusing and inter-object relation 
(see below) computation for the current structure. 
ACCEPT and REJECT acts let the system mark the 
focused object accepted/rejected. 
Object Completion. Suggestions in negotiation dia- 
logues are incomplete most of the time. E.g., the 
utterance "I would prefer to leave at five" is a sug- 
gestion referring to the departure time for a trip 
from Munich to Hanover on the 19. Jan. 2000 (see 
turn 1005 in the appendix). Most of the complete 
data has been mentioned in the preceding dialogue. 
Our completion algorithm uses the focused object 
(itself a completed suggestion) to complete the cur- 
rent structure. All non-conflicting information of tile 
focused object is copied onto the new object.  In our 
example the current temporal information "I would 
prefer to leave at five" would be completed with date 
(i.e., "19. Jan. 2000'" ) and other travel data ( " t r ip  
from-Munich to Hanover" ) .  Afterwards, it Will be  
put to focus. 
Object Relations. The processing results in a number 
of accepted and rejected objects. Normally, a nego- 
tiation produces a series of suggestions that become 
more specific over time. For each new object we cal- 
culate the relation to all other suggestions it\] terms 
of more specific/general or equal. A final inference 
150 
procedure then filters redundant objects and pro- representation to a semantic description (VIT) for 
duces a list of accepted objects with highest speci . . . . . .  each sentence (suitable foz.further processing by the 
ficity. Figure 4 shows two such objects extracted 
from the sample dialogue. Both structures have been 
completed from context data including situational 
data, i.e., current time and place of the negotiation. 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
Topic SCHEDULING 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
relations: 
( (MDRE_SPECIF IC_THAN.#~APPOINTMENT P2*>)) 
APPOINTMENT (Ph*+0) 
HAS_LOCATION --> CITY (P4*) 
HAS_NAME="hannover" 
HAS_MEETING --> MEETING (P3**) 
HAS_NAME="ges chae ft st re f fen" 
HAS_DATE --> DATE (Ph*) 
TEMPEX= \[year : 2000, 
month: j an, 
day : 20, 
part :am, 
time: ii :0\] 
relations : 
((MOKE_SPECIFIC_THAN . #<APPOINTMENT P26.>) 
(MORE_SPECIFIC_THAN . #<APPOINTMENT P30**+0>)) 
APPOINTMENT (P29.+0) 
HAS_LOCATION --> NONGEO_LOCATION (P30***) 
HAS_NAME="b~hnhof" 
HAS_DATE --> DATE (P29") 
TEMPEX=\[year:2000, 
month:jan, 
day:lg, 
time:9:30\] 
Figure 4: The scheduling part of the thematic struc- 
ture 
4 Generat ing  Summar ies  
Our system uses many of tim existing components 
of VERB~'IOBIL. However, we had to develop a new 
component, the summary generator, which is de- 
scribed below. It solves the task of mapping the 
DIREX structures elected in the dialogue nmmory 
into sequences of full fledged semant.ic sentence de- 
scriptions (VITs), thereby performing the following 
steps: 
* Document  P lann ing :  Extracting, preparing 
and dividing the content of the dialogue memory into 
a predefined format. -This includes, c.f., time/place 
of negotiation, participants, result of the negotia- 
tion. 
o Sentence  P lann ing :  Splitting the input into 
chunks suitable for a sentence. This process in- 
voh'es choosing an appropriate verb and arranging 
the parts of the chunk as arguments and/or a(l- 
.iuncts. The final step is the mapping of this internal 
existing VERBMOBIL components). 
? Generat ion :  Verbalizing the VITs by the exist- 
ing multilingual generator of VERBMOBIL. 
? Presentat ion :  Formatting of the complete doc- 
ument content o an, e.g., HTML-page. Finally, the 
document is displayed by an appropriate browser. 
Our approach as been mostly guided by robust- 
ness: our representation language (DIREX) was co- 
developed uring the course of the project. More- 
over, as the extraction component increased its vo: 
cabulary, we wanted to be able to generate new in- 
formation which had not been seen before. Hence 
we needed an approach which is fault tolerant. In- 
stead of failing when the representation changes or 
new type of objects were introduced we degrade in 
precision. Our two step approach as proven its use- 
fulness for this. 
4.1 Document  P lann ing  
The document itself contains two main parts. The 
top of the document includes general informa- 
tion about the dialogue (place, date, participants, 
theme). The body of the document contains the 
summary part which is divided into four paragraphs, 
each of them verbalizing the agreements for one ne- 
gotiation topic: scheduling, accommodation, travel- 
ing and entertainment. Therefore, our document 
planning is very straightforward. The four elements 
of the top document are processed in the following 
manner: 
o Place and Date: For place and date the informa- 
tion is simply retrieved from the dialogue memory. 
? Participants: The participants information are 
transformed into a VIT by the plan processor de- 
scribed below. In the absence of name/title infor- 
mation, a character, e.g., h, B, .. ? is used. 
? Theme: By a shallow examination of the result of 
the content extraction, a semantic description corre- 
sponding to a noun phrase mirroring the content of 
the document as a whole is construed. An example 
is Bus iness  tr ip  w i th  accommodat ion.  
? The summary." Finally, the summary relevant D1- 
REX objects are retrieved from the dialogue men> 
ory: First we compute the most specific suggestions 
by using the most specific/general nd equal rela- 
tions. The remaining suggestions are partitioned 
into equivalence classes which are filtered by com- 
puting the degree of acceptance. In case of conflict 
the most recent one is taken. The resulting set is par- 
titioned into the above mentioned topics the)' belong 
to. Finally these are processed by the plan processor 
as described below. 
4.2 Sentence P lann ing  
We now turn into the process of mapping the inter- 
esting part of the dialogue memory onto sequences 
151 
of VITs. An example of the content of one topic - 
scheduling - was shown in figure 4. O.ur two step 
approach consists of: 
* A p lan processor  whose task it is to split the 
objects selected into chunks suitable for a sentence. 
Possibly it contributes to the selection of verbs. 
o A semant ic  onst ructor  whose task it is to con- 
vert the output of the plan processor into full fledged 
semantic descriptions (VITs) for the sentences of the 
document. This second step can be viewed as a ro- 
bust fall-back: If the plan processor does not succeed 
in obtaining full Specifications of all sentence parts, 
this step secures a valid and complete specification. 
4.2.1 The  plan processor  
Input to the plan processor (Alexandersson and Rei- 
thinger, 1997) is the thematic structure partly shown 
in figure 4. The plan processor interprets (currently 
about 150) plan operators which are expanded in a 
top-down left to right fashion. 
For the overall structure of the text, the imposed 
topic structure of the thematic structure is kept. 
Within a topic we use a set of operators which are ca- 
pable of realizing (parts of) the structures to NPs, 
PPs and possibly verb information forming a high 
level specification of a sentence. 
P lan  operators  
A plan operator consists of a goal which is option- 
ally divided into subgoal(s). Its syntax contains the 
keywords :const ra in ts  and  :ac t ions  which can 
be any Lisp expression. Variables are indicated with 
question/exclamation marks (see figures 5 and 6). 
The goal of the operators uses an interface based 
on a triple with the following usage: 
o <descr ip t ion> This is the input position of the 
operator. It describes and binds the object which 
will be processed by this operator. 
o <context> This is the context - input/output. 
The context contains a stack for objects in focus, 
handled as described in (Grosz and Sidner, 1986). 
Additionally we put the generated information on a 
history list (Dale, 1995). The context supports the 
generation of, e.g., pronouns (see below). At present 
the context is only used local to each topic. 
o <output> The result of the operator. Tile possible 
output types are NP, PP and sentence(s). 
We the distinguish two types of operators; complex 
operators, responsible for complex objects, which 
can contain several roles, and simple operators, 
which can process imple objects (carrying only one 
role). The general design of a complex operator --see 
figure 5 for an operator esponsible for appointment 
objects - consists of three subgoals: 
o ( f ind - ro les  . . . )  Retrieve tile content of the 
object. "ghe operators responsible for soh'ing the 
f ind - ro les  goal optionally allow for an enumera- 
tion of the roles we want to use. 
e (sp l i t - ro les  . . . )  These  ro les (and values) will 
be partit ioned,into chunks, (which we, call a split) 
suitable for generating one sentence. 
? (generate -sp l i t s  . . . )  Finally the output - a 
sentence description - will be constructed. 
(defplan appointment 
:goal ((class (Vapp scheduling)) 
(?in-context ?out-context) 
?sentence) 
:constraints (appointment-p !app) 
:subgoals (:sequence 
(find-roles ?appZrels) 
(split-roles ?rels 
appointment ?l-of-splits) 
(generate-splits ?l-of-splits 
(Via-context ?out-context) 
appointment ?sentence))) 
Figure 5: An example of an operator for a "complex" 
object 
Behind the functionality of the sp l i t - ro les  goal 
we use pairs of operators (figure 6), where the first is 
a fact describing the roles of the split, and the second 
is a description for how to realize the sentence. In 
this example the selection of an appropriate verb is 
not performed by this operator but by the semantic 
constructor. 
The second type of operators are simple operators 
like the one for the generation of time expressions 
(tempex) or cities (see figure 4). 
Figure 7 shows a simplified plan processor output 
(building block) for one sentence. 
4.2.2 The Semant ic  Const ructor  
The task of the semantic onstructor is to map the 
information about sentences computed by the plan 
processor to full semantic representations (VITs). 
The knowledge source for this computational step 
is a declarative set of about 160 different semanti- 
cally oriented sentence patterns which are encoded 
in an easily extendable semantic/syntactic descrip- 
tion language. 
To obtain a complete semantic representation for 
a sentence we first select a sentence pattern. This 
pattern is then, together with tile output of the plan 
processor, interpreted to produce the VIT. The se- 
lection criteria for a sentence pattern are: 
All patterns are ordered topic-wise because 
the appropriateness of sentence patterns is topic- 
dependent (e.g., the insertion of topic-specific NPs 
or PPs into a sentence). 
-+ The int.entional state of the inforination to 
be verbalized highly restricts the set of appropriate 
verbs. 
Depending on the propositional content de- 
scribed within a DIat-:x-VIT - i.e., a VIT repre- 
senting one sentence part in a building block of the 
152 
; ;  - Das <Treffen> finder i n  <City> 
;;  am <tempex> statt 
? ;; - The <Meeting>takes place 
;; in <City> on the <tempex> 
(deffact sentence-split 
:goal (sentence-split 
((has_meeting ?has_name) 
(has_location ?has_location) 
(has_date ?has_date)) 
?_topic)) 
(defplan generate-split 
:goal (generate-split 
((has_meeting ?nmme) ......... ;;:meeting 
(has_location ?location) ;; city 
(has_date ?date)) ;; tempex 
(?in-context ?out-context) 
?topic 
?s) 
:subgoals 
(:seq ((class (?location ?scheduling pp)) 
7topic ?loc-pp) 
((class (?name ?scheduling)) 
?topic ?s-topic) 
(generate-full-tempex ?date ?tempex) 
(((generate-sentence decl) 
(subj ?topic has_topic) 
(obj ?l-pp has_location) 
(obj-add ?tempex has_date)) 
?in-context ?out-context ?s))) 
Figure 6: Example of sentence definition and gener- 
ation 
(ACCOMMODATION 
(ACCEPTED 
(HAS_SIZE VIT: <Einzelzimmer>) 
(HAS_PRICE VIT: <80-Euro-pro-Nacht>) 
)) 
Figure 7: Exmnple of a plan processor output 
plan processor output - it has to play different se- 
mantic roles in the sentence (e.g., verb-argument vs. 
verb-complement) 
Additionally, the number of DtREx-VITs given 
within a building block for a sentence, influences the 
distribution of them to appropriate semantic roles. 
Figure 8 shows a simplified sentence pattern that 
is selected for the building block in figure 7 to con- 
struct a VIT for, e.g., the German sentence Das 
Einzelzimmer kostet 80 Euro pro Nacht. ("The sin- 
gle room costs 80 euro per night."). According 
(( : verb kosten_v) 
( :subj  HAS_SIZE) 
(: obj HAS_PRICE) 
( : res t  DIREX_PPS)) 
Figure 8: Example of a sentence pattern 
to the above mentioned selection criteria, this pat- 
tern is selected only for building blocks within 
. ...the.~ accommodation:topi.c~ that-contain, a t  least ,val- 
ues for the roles HAS.SIZE and HAS.PRIZE, respec- 
tively. The sentence pattern contains the following 
"building instructions": The semantic verb predi- 
cate ( :verb) is kosten_v (to cost), its subject ar- 
gument ( :subj)  is to be filled by the DIREX-VIT 
associated to the DmEx-role HAS.SIZE while :obj 
means a similar instruction for the direct object. 
The robustness fallback ( : res t  DIREX._PPS) means 
.that.all_other DmEx=VITs are attached to the verb  
as PP  complement?. It i spah  ~/f a\]l 'Sen~df/6+ p i t -  
terns to ensure that even erroneous building blocks 
or erroneously selected sentence patterns produce a 
sentence VIT. 
Finally, the VIT is constructed by interpreting the 
sentence pattern. The interpreter walks through the 
sentence pattern and performs different actions de- 
pending on the keywords, e.g., :verb,  :subj and 
their values. 
4.2.3 Util izing Context 
During'the course of the generation, the plan proces- 
sor incrementally constructs a context (Dale, 1995), 
which allows for the generation of, c.f., anaphora or 
demonstratives for making the text fluent or con- 
trasting purposes. 
? Anaphora  If, e.g., a meeting is split into 
more than one sentence, the plan processor uses an 
anaphora to the meeting in the second sentence. 
? D iscourse Markers  In case of multiple, e.g., 
meetings we introduce the second with a discourse 
marker, e.g., "also". 
o Demonst ra t ives  In case of multiple meetings, we 
use a demonstrative to refer to the second meeting. 
In addition to the plan processor, the seman- 
tic constructor also takes care of coherence within 
the paragraphs produced for the individual topics 
hereby focusing on the generation of anaphora nd 
adverbial discourse markers. While the local con- 
text of the plan processor is based on the proposi- 
tional content at hand, the semantic onstructor uses 
a postprocessing module that is based oil the output  
\qTs  of the plan processor (DIREx-VITs) using its 
own semantically oriented local context memory. 
Anaphorization and insertion of discourse mark- 
ers within the semantic onstructor are based on a 
comparison of plan processor output VITs occur- 
ring within consecutive sentences of a paragraph. 
Identical verb arguments (NPs) in consecutive sen- 
., tences are replaced by .appropriate anaphoric pro- 
nouns while identical verbs themselves lead to the in- 
sertion of an appropriate adverbial discourse marker. 
5 Mu l t i l i ngua l i ty  
The generation of dialogue scripts and result sum- 
maries is fully implemented in VERB~VIoBIL for Ger- 
man and English. For the English smnmaries we 
153 
extracted, then the transfer module produces equiv- 
alent English VITs which are finally sent to the En- 
glish generation component for producing the En- 
glish text. 
Figure 9 shows the English result summary of the 
dialogue shown in the appendix. 
make use of the transfer component as follows. All o TN A feature was not part of the dialogue, and 
VITs from the German-document representation are . not included in. the..summary. 
The evaluation result is shown in figure 10. It uses 
the standard precision, recall and fallout as defined 
in (Mani et.al., 1998). 
Dialogue 1 2 3 4 aver 
Turns 33 33 31 32 32.25 
Corr 6 13 9 11 9.75 
Miss 6 3 5 4 4.5 
False 3 3 3 0 2.25 I 
TN 32 28 30 32 30.5 I 
Recall 0.5---0- 0.8-'--1- 0.6----4-- 0 .7 - ' - -3 - -~ 
10 I 
Fallout i 0.0__9 0.1___0_ 0.0____9_ _0"00 
Figure 10: Evaluation Results 
Figure 9: Example of an English result summary 
6 Eva luat ion  
We have performed a small evaluation of the overall 
system as described in this paper. Basis for the eval- 
uation were the transcripts of four German-English 
negotiation dialogues. For each dialogue the result- 
ing features of the negotiation (maximally 47, e.g., 
location, date for a meeting, speakers name and title, 
book agent) were annotated by a lmman, and then 
compared with the result of running the dialogues 
through the system and generating the summaries. 
The features in the summary were compared using 
the following classifications: 
? Cor r  The feature approximately corresponds to 
the human annotation. This means that the feature 
is either (1) a 100% match; (2) it was not sufficiently 
specified or (2) too specific. An example of (2) is 
when the correct date included a time, which was 
not captured. An example of (3) is when a date 
with time was annotated but the feature contained 
just a (late. 
o Miss A feature is not included in the summary. 
o False A feature was erroneously iimluded in the 
sumlnary, meaning that the feature was not part of 
the dialogue or it received a wrong value. 
Obviously, our approach tries to be on the safe 
side; the summary contains only those features that 
the system thinks both partners agreed on. The 
main reasons for not getting higher numbers is 
twofold. The recognition of dialogue acts, and thus 
the recognition of the intension behind the utter- 
ances reaches a 70% recall (Reithinger and Klesen, 
1997). We also still make errors during the content 
extraction. 
7 Conc lus ion  
We have presented an extension to existing modules 
allowing for the generation of summaries within the 
VERBMOBIL system. To our knowledge our system 
is the only one that uses semantic representation as
basis for summarizing. Other approaches use, e.g., 
statistical techniques or rhetorical parsing (Waibel 
et al, 1998; Hovy and Marcu, 1998) to obtain the 
summaries. Moreover, although our module is re- 
stricted to language specific processing, the use of 
semantics and the transfer module allow for the gen- 
eration of multilingual documents in a very straight- 
forward fashion. 
In the near future we will extend the system with 
respect o: 
o Sentence  Spl it  At present the first found sen- 
tence split is chosen. This is not necessarily the op- 
timal one. We are currently in the process of devel- 
oping criteria for ranking competing results. 
o Japanese  The VERBMOBIL system currently in- 
cludes German, English and Japanese. We intend 
to apply the same technique as for the English sum- 
maries to generate Japanese ones. 
References 
J. Alexandersson and P. Poller. 1998. Towards multilin- 
~oual protocol generation for spontaneous speech dia- 
gues. In Probeedings of INLG-98, Niagara-On-The- 
Lake. Ontario. Canada. 
154 
J. Alexandersson and N. Reithinger. 1997. Learning di- 
alogue structures from a corpus. In Proceedings of 
? EufoSpeech-97; pages' 2231-2235," Rhodes. 
Jan Alexandersson, Bianka Buschbeck-Wolf, Tsutomu 
Fujinami, Michael Kipp, 'S tephan Koch, Elisa- 
beth Maier, Norbert P~eithinger, Birte Schmitz, 
and Melanie Siegel. 1998. Dialogue Acts in 
VERBMOBIL-2 - Second Edition. Vergmobil-Report 
226, DFKI  Saarbrficken, Universitgt Stuttgart, Tech- 
nische Universit/it Berlin, Universit/it des Saarlandes. 
D. Appelt, J. Hobbs, J. Bear, and M. Tyson. 1993. FAS- 
TUS: A finite-state processor for information extrac- 
tion from real-world text. In IJCAL93. 
T. Becker, W. Finkler, A. Kilger, and P. Poller. 1998. An 
efficient kernel for multilingual generation in speech- 
to--speech dialogue -translation-.- In :Proceediiigs of 
COLING/ACL-98, Montreal, Quebec, Canada. 
T. Becket, A. Kilger, P. Lopez, and P. Poller. 2000. Mul- 
tilingual generation for translation in speech-to-speech 
dialoga.les and its realization in verbmobil. In Proceed- 
ings of ECAI-2000, Berlin, Germany. 
J. Bos, B. Gamb/ick, C. Lieske, Y. Mori, M. Pinkal, and 
K. Worm. 1996. Compositional semantics in verbmo- 
bil. In Proceedings of Coling '96, Copenhagen, Den- 
mark. 
T. Bub, W. Wahlster, and A. Waibel. 1997. Verbmo- 
bih The combination of deep and shallow processing 
for spontaneous speech translation. In Proceedings dr/ 
ICASSP-97, pages 71-74, Munich. 
R. Dale. 1995. An introduction to natural lan- 
guage generation. Technical report, Microsoft 
Research Institute (MRI), Macquarie Univer- 
sity. Presented at the 1995 European Summer 
School on Logic, Language and Information, Avail- 
able from http://www.mri.mq.edu.au/-rdale/nlg- 
textbook/ESSLLI95/. 
M. Dorna and M. Emele. 1996. Efficient Implementation 
of a Semantic-Based Transfer Approach. In Proceed- 
ings of ECAI-96, pages 567-571, Budapest, Hungary, 
August. 
M. Dorna. 1996. The ADT-Package for the VERBMOBIL 
Interface Term. Verbmobil Report 104, IMS, Univer- 
sit/it Stuttgart, Germany. 
B. Grosz and C. Sidner. 1986. Attention. Intentions and 
the Structure of Discourse. Journal o~ Computational 
Linguistics, 12(3). 
E. Hovy and D. Marcu. 1998. Coling/acl-98 tu- 
torial on automated text summarization. Avail- 
able from http://w~v.isi.edu/-marcu/coling-ac198- 
tutorial.html. 
M. Kipp, J. Alexandersson, and N. Reithinger. 1999. 
Understanding Spontaneous Negotiation Dialogue. In 
Workshop Proceedings 'Knowledge And Reasoning in 
Practica\[Dialogue Systems' of TJCAI '99, pages 57- 
64. 
I. Mani, D. House, G. Klein, L. Hirschman, L. 
Obrist, T. Firmin. M. Chrzanowski, and B. 
Sundheim. 1998. The tipster summac text sum- 
marization evaluation - final report. Technical 
reports The Mitre Corp. Available from http://www- 
24.nist.gov/related_projects/tipster_summac/finalxpt- 
.html. 
N. Reithinger and M. Klesen. 1997. Dialogue Act Clas- 
sification Using Language Models. In Proceedings of 
EuroSpeech-97, pages 2235-2238, Rhodes. 
A. Waibel, M. Bett, M. Finke, and R Stiefelhagen. 1998. 
Meeting Browser: Tracking and Summarizing Meet- 
ings. In Proceedings of the DARPA Broadcast News 
Workshop. 
Appendix 
Excerpt from our sample dialogue. 
\[...\] 
1002 
- j a  es  geht um das Geschftstreffen in 
Hannover ~lit.: Yes i t  is about the business 
meeting in Hanover) 
\[INIT,scheduling,has_appointment: 
\[appointment,has_meeting:\[meeting, 
has_name='geschaeftstreffen'\], 
has_location:\[city,has_name='hannover ' , 
has_loc_spec=in,has_det=nnknown\]\]\] 
- das  i s t  j a  am zwanzigsten Januar um elf 
Uhr vormittags 
\[SUGGEST,uncertain_scheduling,has_date: 
..\[date,tempex='.(ge_2920_O,\[from: 
\[dom:20,month:jan,tod:11:0, 
pod:morning_ger2\]\])'\]\] 
1003 
- so we have to leave Munich at six o'clock 
\[SUGGEST,traveling,has_move:\[move, 
has_source_location:\[city,has_name 
='muenchen'\],has_departure_time:\[date, 
tempex='(en_2920_O,\[from:tod:6:0\])'\]\]\] 
1004 
- vielleicht fahren wir lieber den Tag davor 
(lit.: maybe we better leave the day before) 
\[SUGGEST,traveling,has_move:\[move, 
has_departure_time:\[date,tempex = 
'(ge_2920_l,\[from: 
neg_shift(dur(l,days),ana_point)\])'\]\]\] 
- da gibt es einen Zug um zwei Uhr 
(lit.: there is a train at two o'clock) 
\[SUGGEST,traveling,has_move:\[move,has- 
_transportation:\[rail\],has_departure_time: 
\[date,tempex='(ge_2920_2,\[from:tod:2:0\])'\]\]\] 
1005 
I would prefer to leave at five 
\[SUGGEST,traveling,has_move:\[move, 
has_agent:\[speaker\],has_departure_time: 
\[date,tempex='(en_2920_l,\[from:tod:5:0\])'\]\]\] 
\[...\] 
I011 
- let us meet at the station on Wednesday 
\[SUGGEST,scheduling,has_appointment: 
\[appointment,has_location:\[nongeo_location, 
has_name='bahnhof',has_loc_spec=at, 
has_det=def\],has_date:\[date,tempex = 
'(en_2920_2,\[from:dow:wed\])'\]\]\] 
1012 
-um halb zehn am Bahnhof 
(lit.: at half past nine at the station) 
\[ACCEPT, uncert ain_s cheduling, has_date : \[date, 
tempex= ' (ge_2S20_3, \[fzom: rod : 9 : 30\] ) ' \] , 
has location: \[nongeo_location,has_name = 
' bahnhof ' \] \] 
\[...\] 
155 
MULTIPLATFORM Testbed: An Integration Platform for Multimodal
Dialog Systems
Gerd Herzog, Heinz Kirchmann, Stefan Merten, Alassane Ndiaye, Peter Poller
German Research Center for Artificial Intelligence
Erwin-Schro?dinger-Stra?e, D?67608 Kaiserslautern, Germany
 
herzog,kirchman,merten,ndiaye,poller  @dfki.de
Abstract
Modern dialog and information systems are in-
creasingly based on distributed component ar-
chitectures to cope with all kinds of hetero-
geneity and to enable flexible re-use of ex-
isting software components. This contribu-
tion presents the MULTIPLATFORM testbed as a
powerful framework for the development of in-
tegrated multimodal dialog systems. The paper
provides a general overview of our approach
and explicates its foundations. It describes ad-
vanced sample applications that have been re-
alized using the integration platform and com-
pares our approach to related works.
1 Motivation
One central element of research in the field of intelli-
gent user interfaces is the construction of advanced natu-
ral language and multimodal dialog systems that demon-
strate the high potential for more natural and much more
powerful human-computer interaction. Although lan-
guage technology already found its way into fundamen-
tal software products?as exemplified by the Microsoft
Speech SDK (software development kit) for Windows
and the Java Speech API (application programming in-
terface)?the development of novel research prototypes
still constitutes a demanding challenge. State-of-the-art
dialog systems combine practical results from various re-
search areas and tend to be rather complex software sys-
tems which can not simply be realized as a monolithic
desktop computer application. More elaborate software
designs are required in order to assemble heterogeneous
components into an integrated and fully operational sys-
tem (Klu?ter et al, 2000).
A typical research project involves several or even
many work groups from different partners, leading to a
broad spectrum of practices and preferences that govern
the development of software components. In particular, a
common software platform for the construction of an in-
tegrated dialog system often needs to support different
programming languages and operating systems so that
already existing software can be re-used and extended.
Taking into account the potential costs it is usually not
feasible to start an implementation from scratch. Another
important aspect is the use of rapid prototyping for accel-
erated progress which leads to frequent changes in design
and implementation as the project unfolds.
Over the last ten years we have been concerned with
the realization of various complex distributed dialog
systems. A practical result of our long-term work as
a dedicated system integration group is the so-called
MULTIPLATFORM testbed1 (Multiple Language Target
Integration Platform for Modules) which provides a
powerful and complete integration platform. In this con-
tribution, we will report on the experience gained in
the construction of integrated large-scale research pro-
totypes. The results obtained so far will be presented
and the underlying principles of our approach will be dis-
cussed.
2 Anatomy of the Testbed
The MULTIPLATFORM testbed in its diverse instantia-
tions comprises the software infrastructure that is needed
to integrate heterogeneous dialog components into a com-
plete system. Built on top of open source software (Wu
and Lin, 2001), the testbed SDK offers APIs as well as a
large set of tools and utilities to support the whole devel-
opment process, including installation and distribution.
The following sections provide an overview of the testbed
and describe the design principles that govern its realiza-
tion.
1Our current work in the context of the SmartKom project
is being funded by the German Federal Ministry for Education
and Research (BMBF) under grant 01 IL 905 K7.
2.1 Architecture Framework
A distributed system constitutes the natural choice to re-
alize an open, flexible and scalable software architecture,
able to integrate heterogeneous software modules imple-
mented in diverse programming languages and running
on different operating systems. In our project work, for
example, we encountered modules for Sun Solaris, GNU
Linux, and Microsoft Windows written in Prolog and
Lisp, as classical AI languages, as well as in common
conventional programming languages like C, C++, and
Java.
The testbed framework is based on a component ar-
chitecture (Hopkins, 2000) and our approach assumes a
modularization of the dialog system into distinct and in-
dependent software modules to allow maximum decou-
pling. These large-grained components?ranging from
more basic modules that encapsulate access to specific
hardware devices to complex components which may in-
clude entire application-specific subsystems?constitute
self-contained applications which are executed as sepa-
rate processes, or even process groups. The principle be-
hind this view is to consider software architecture on a
higher-level of abstraction as modularization is not con-
cerned with decomposition on the level of component li-
braries in a specific programming language. Continuous
evolution is one of the driving forces behind the devel-
opment of novel dialog systems. The creation of a com-
ponentized system makes the integrated system easier to
maintain. In a well-designed system, the changes will be
localized, and such changes can be made with little or no
effect on the remaining components. Component integra-
tion and deployment are independent of the component
development life cycle, and there is no need to recompile
or relink the entire application when updating with a new
implementation of a component.
The term middleware (Emmerich, 2000) denotes the
specific software infrastructure that facilitates the in-
teraction among distributed software modules, i.e. the
software layer between the operating system?including
the basic communication protocols?and the distributed
components that interact via the network. The testbed as
a component platform enables inter-process communica-
tion and provides means for configuring and deploying
the individual parts of the complete dialog system.
Our middleware solution does not exclude to connect
additional components during system execution. So far,
however, the testbed does not offer specific support for
dynamic system re-configuration at runtime. In our ex-
perience, it is acceptable and even beneficial to assume a
stable, i.e. a static but configurable, architecture of the
user interface components within a specific system in-
stantiation. It is obvious that ad hoc activation and invo-
cation of services constitutes an important issue in many
application scenarios, in particular Internet-based appli-
cations. We propose to hide such dynamic aspects within
the application-specific parts of the complete system so
that they do not affect the basic configuration of the dia-
log system itself.
The details of the specific component architecture of
different dialog systems vary significantly and an agreed-
upon standard architecture which defines a definite mod-
ularization simply does not exist. Nevertheless, we found
it helpful to use a well-defined naming scheme and dis-
tinguish the following categories of dialog system com-
ponents when designing a concrete system architecture:
Recognizer: Modality-specific components that pro-
cess input data on the signal level. Examples include
speech recognition, determination of prosodic informa-
tion, or gesture recognition.
Analyzer: Modules that further process recognized
user input or intermediary results on a semantic level.
Such components include in particular modality-specific
analyzers and media fusion.
Modeller: Active knowledge sources that provide ex-
plicit models of relevant aspects of the dialog system,
like for example discourse memory, lexicon, or a suitable
model of the underlying application functionality.
Generator: Knowledge-based components which de-
termine and control the reactions of the dialog system
through the transformation of representation structures.
This includes the planning of dialog contributions and
application-centric activities as well as fission of mul-
tiple modalities and media-specific generators, e.g., for
text and graphics.
Synthesizer: Media-specific realization components
that transform generated structures into perceivable out-
put. A typical example is a speech synthesis component.
Device: Connector modules that encapsulate access to
a hardware component like, for example, microphone and
sound card for audio input or a camera system that ob-
serves the user in order to identify facial expressions.
Service: Connector components that provide a well-
defined link to some application-specific functionality.
Service modules depend on the specific application
scenario and often encapsulate complete and complex
application-specific subsystems.
2.2 Inter-Process Communication
Nowadays, a very broad spectrum of practical technolo-
gies exists to realize communication between distributed
software modules. Techniques like remote procedure call
and remote method invocation, which follow the client-
server paradigm, have long been the predominant ab-
straction for distributed processing. In this programming
model, each component has to specify and implement a
specific API to make its encapsulated functionality trans-
parently available for other system modules. Only re-
cently, the need for scalability, flexibility, and decoupling
in large-enterprise and Internet applications has resulted
in a strong general trend toward asynchronous, message-
based communication in middleware systems.
In accordance with the long-standing distinction be-
ing made in AI between procedural vs. declarative rep-
resentations, we favor message-oriented middleware as
it enables more declarative interfaces between the com-
ponents of a dialog system. As illustrated by a hybrid
technology like SOAP, the simple object access proto-
col, where remote calls of object methods are encoded
in XML messages, the borderline between a procedural
and a declarative approach is rather difficult to draw in
general. Our own data-oriented interface specifications
will be discussed in more detail in section 3.
For message-based communication, two main schemes
can be distinguished:
  Basic point-to-point messaging employs unicast
routing and realizes the notion of a direct connection
between message sender and a known receiver. This
is the typical interaction style used within multi-
agent systems (Weiss, 2000).
  The more general publish/subscribe approach is
based on multicast addressing. Instead of address-
ing one or several receivers directly, the sender pub-
lishes a notification on a named message queue, so
that the message can be forwarded to a list of sub-
scribers. This kind of distributed event notification
makes the communication framework very flexible
as it focuses on the data to be exchanged and it de-
couples data producers and data consumers. The
well-known concept of a blackboard architecture,
which has been developed in the field of AI (Erman
et al, 1980), follows similar ideas.
Compared with point-to-point messaging, pub-
lish/subscribe can help to reduce the number and
complexity of interfaces significantly (Klu?ter et al,
2000).
The MULTIPLATFORM testbed includes a message-
oriented middleware. The implementation is based on
PVM, which stands for parallel virtual machine (Geist
et al, 1994). In order to provide publish/subscribe mes-
saging on top of PVM, we have added another software
layer called PCA (pool communication architecture). In
the testbed context, the term data pool is used to refer to
named message queues. Every single pool can be linked
with a pool data format specification in order to define
admissible message contents.
In the different dialog systems we designed so far,
typical architecture patterns can be identified since the
pool structure reflects our classification into different cat-
egories of dialog components. The pool names together
with the module names define the backbone for the over-
all architecture of the dialog system.
The messaging system is able to transfer arbitrary data
contents and provides excellent performance characteris-
tics. To give a practical example, it is possible to perform
a telephone conversation within a multimodal dialog sys-
tem. Message throughput on standard PCs with Intel Pen-
tium III 500 MHz CPU is off-hand sufficient to establish
a reliable bi-directional audio connection, where uncom-
pressed audio data are being transferred as XML mes-
sages in real-time. A typical multimodal user interaction
of about 10 minutes duration can easily result in a mes-
sage log that contains far more than 100 Megabytes of
data.
The so-called module manager provides a thin API
layer for module developers with language bindings for
the programming languages that are used to implement
specific dialog components. It includes the operations re-
quired to access the communication system and to realize
an elementary component protocol needed for basic co-
ordination of all participating distributed components.
2.3 Testbed Modules and Offline Tools
In addition to the functional components of the dialog
system, the runtime environment includes also special
testbed modules in support of system operation.
The testbed manager component, or TBM for short,
is responsible for system initialization and activates all
distributed components pertaining to a given dialog sys-
tem configuration. It forms the counterpart for functional
modules to carry out the elementary component protocol,
which is needed for proper system start-up, controlled ter-
mination of processes and restart of single components,
or a complete soft reset of the entire dialog system.
The freely configurable testbed GUI constitutes a sepa-
rate component which provides a graphical user interface
for the administration of a running system. In Figure 1 the
specific testbed GUI of the SMARTKOM system (cf. Sec-
tion 4.2) is shown as an example. The GUI basically pro-
vides means to monitor system activity, to interact with
the testbed manager, and to manually modify configura-
tion settings of individual components while testing the
integrated system.
A further logging component is being employed to
save a complete protocol of all exchanged messages for
later inspection. Flexible replay of selected pool data pro-
vides a simple, yet elegant and powerful mechanism for
the simulation of small or complex parts of the dialog
system in order to test and debug components during the
development process.
Another important development tool is a generic data
viewer for the online and offline inspection of pool data.
Figure 1: Testbed administration GUI and data viewer.
Currently active components are highlighted using a dif-
ferent background color. The data viewer windows pro-
vide a compact display of selected pool data.
Further offline tools include a standardized build and
installation procedure for components and utilities for
the preparation of software distributions and incremen-
tal updates during system integration. Additional project-
specific APIs and specifically adapted utilities are being
developed and made avaliable as needed.
3 High-level Interfaces for Dialog System
Components
Instead of using programming interfaces, the interac-
tion between distributed components within the testbed
framework is based on the exchange of structured data
through messages. The communication platform is open
to transfer arbitrary contents but careful design of infor-
mation flow and accurate specification of content formats
constitute essential elements of our approach.
Agent communication languages like KQML (Finin et
al., 1994) and FIPA ACL (Pitt and Mamdani, 1999) are
not a natural choice in our context. In general, large-
scale dialog systems are a mixture of knowledge-based
and conventional data-processing components. A further
aspect relates to the pool architecture, which does not
rely on unspecific point-to-point communication but on
a clear modularization of data links. The specification of
the content format for each pool defines the common lan-
guage that dialog system components use to interoperate.
3.1 XML-based Data Interfaces
Over the last few years, the so-called extensible markup
language has become the premier choice for the flexible
definition of application-specific data formats for infor-
mation exchange. XML technology, which is based on
standardized specifications, progresses rapidly and offers
an enormous spectrum of useful techniques and tools.
XML-based languages define an external notation for
the representation of structured data and simplify the
interchange of complex data between separate applica-
tions. All such languages share the basic XML syn-
tax, which defines whether an arbitrary XML structure
is well-formed, and they are build upon fundamental con-
cepts like elements and attributes. A specific markup lan-
guage needs to define the structure of the data by impos-
ing constraints on the valid use of selected elements and
attributes. This means that the language serves to encode
semantic aspects of the data into syntactic restrictions.
Various approaches have been developed for the for-
mal specification of XML-based languages. The most
prominent formalism is called document type definition.
A DTD basically defines for each allowed element all al-
lowed attributes and possibly the acceptable attribute val-
ues as well as the nesting and occurrences of each ele-
ment. The DTD approach, however, is more and more
superseded by XML Schema. Compared with the older
DTD mechanism, a schema definition (XSD) offers two
main advantages: The schema itself is also specified in
XML notation and the formalism is far more expressive
as it enables more detailed restrictions on valid data struc-
tures. This includes in particular the description of el-
ement contents and not only the element structure. As
a schema specification can provide a well-organized type
structure it also helps to better document the details of the
data format definition. A human friendly presentation of
the communication interfaces is an important aid during
system development.
It should be noted that the design of an XML language
for the external representation of complex data consti-
tutes a non-trivial task. Our experience is that design
decisions have to be made carefully. For example, it is
better to minimize the use of attributes. They are limited
to unstructured data and may occur at most once within a
single element. Preferring elements over attributes better
supports the evolution of a specification since the con-
tent model of an element can easily be redefined to be
structured and the maximum number of occurrences can
simply be increased to more than one. A further princi-
ple for a well-designed XML language requires that the
element structure reflects all details of the inherent struc-
ture of the represented data, i.e. textual content for an
element should be restricted to well-defined elementary
types. Another important guideline is to apply strict nam-
ing rules so that it becomes easier to grasp the intended
meaning of specific XML structures.
From the point of view of component development,
XML offers various techniques for the processing of
<intentionLattice>
[?]
<hypothesisSequences>
<hypothesisSequence>
<score>
<source>     acoustic   </source>
<value>       0.96448     </value>
</score>
<score>
<source>    understanding  </source>
<value>       0.91667 </value>
</score>
<hypothesis>
<discourseStatus>
<discourseAction> set </discourseAction>
<discourseTopic><goal> epg_info </goal></discourseTopic>
[?]
<event id="dim868">
<informationSearch id="dim869">
<pieceOfInformation>
<broadcast id="dim863">
<avMedium>
<avMedium id="dim866">
<avType> featureFilm </avType>
<title>    Die innere Sicherheit </title>
[?]
</hypothesisSequence>
[?]
</hypothesisSequences>
</intentionLattice>
i t ti tti
[ ]
t i
t i
r
r      ti    / r
l        .      / l
/ r
r
r     r t i   / r
l        . / l
/ r
t i
i r t t
i r ti  t / i r ti
i r i l  i f  / l / i r i
[ ]
t i i
i f r ti r i i
i fI f r ti
r t i " i "
i
i i " i "
f t r il /
titl     i  i r  i r it /titl
[ ]
/ t i
[ ]
/ t i
/i t ti tti
Confidence in the Speech 
Recognition Result
Confidence in the Speech
Understanding Result
Planning Act
Object Reference
Figure 2: Partial M3L structure. The shown intention lat-
tice represents the interpretation result for a multimodal
user input that can be stated as: ?I would like to know
more about this [   ].?
transferred content structures. The DOM API makes the
data available as a generic tree structure?the document
object model?in terms of elements and attributes. An-
other interesting option is to employ XSLT stylesheets
to flexibly transform between the external XML format
used for communication and a given internal markup lan-
guage of the specific component. The use of XSLT makes
it easier to adapt a component to interface modifications
and simplifies its re-use in another dialog system. In-
stead of working on basic XML structures like elements
and attributes, XML data binding can be used for a di-
rect mapping between program internal data structures
and application-specific XML markup. In this approach,
the language specification in form of a DTD or an XML
Schema is exploited to automatically generate a corre-
sponding object model in a given programming language.
3.2 Multimodal Markup Language
In the context of the SMARTKOM project (see sec-
tion 4.2) we have developed M3L (Multimodal Markup
Language) as a complete XML language that covers all
data interfaces within this complex multimodal dialog
system. Instead of using several quite different XML
languages for the various data pools, we aimed at an in-
tegrated and coherent language specification, which in-
cludes all sub-structures that may occur on the different
pools. In order to make the specification process man-
ageable and to provide a thematic organization, the M3L
language definition has been decomposed into about 40
schema specifications.
Figure 2 shows an excerpt from a typical M3L expres-
sion. The basic data flow from user input to system output
continuously adds further processing results so that the
representational structure will be refined step-by-step. In-
tentionally, M3L has not been devised as a generic knowl-
edge representation language, which would require an
inference engine in every single component so that the
exchanged structures can be interpreted adequately. In-
stead, very specific element structures are used to convey
meaning on the syntactic level. Obviously, not all rele-
vant semantic aspects can be covered on the syntax level
using a formalism like DTD or XSD. This means, that
it is impossible to exclude all kinds of meaningless data
from the language definition and the design of an inter-
face specification will always be a sort of compromise.
Conceptual taxonomies provide the foundation for the
representation of domain knowledge as it is required
within a dialog system to enable a natural conversation
in the given application scenario. In order to exchange
instantiated knowledge structures between different sys-
tem components they need to be encoded in M3L. In-
stead of relying on a manual reproduction of the under-
lying terminological knowledge within the M3L defini-
tion we decided to automate that task. Our tool OIL2XSD
(Gurevych et al, 2003) transforms an ontology written in
OIL (Fensel et al, 2001) into an M3L compatible XML
Schema definition. The resulting schema specification
captures the hierarchical structure and a significant part
of the semantics of the ontology. For example in Figure
2, the representation of the event structure inside the in-
tention lattice originates from the ontology. The main ad-
vantage of this approach is that the structural knowledge
available on the semantic level is consistently mapped to
the communication interfaces and M3L can easily be up-
dated as the ontology evolves.
In addition to the language specification itself, a spe-
cific M3L API has been developed, which offers a light-
weight programming interface to simplify the process-
ing of such XML structures within the implementation
of a component. Customized testbed utilities like tailored
XSLT stylesheets for the generic data viewer as well as
several other tools are provided for easier evaluation of
M3L-based processing results.
4 Sample Applications
Our framework and the MULTIPLATFORM testbed have
been employed to realize various natural language and
multimodal dialog systems. In addition to the re-
search prototypes mentioned here, MULTIPLATFORM has
also been used as an integration platform for inhouse
projects of industrial partners and for our own commer-
cial projects.
The first incarnation of MULTIPLATFORM arose from
the VERBMOBIL project where the initial system archi-
tecture, which relied on a multi-agent approach with
point-to-point communication, did not prove to be scal-
able (Klu?ter et al, 2000). The testbed has been enhanced
in the context of the SMARTKOM project and was re-
cently adapted for the COMIC system. As described in
the previous sections, the decisive improvement of the
current MULTIPLATFORM testbed is, besides a more ro-
bust implementation, a generalized architecture frame-
work for multimodal dialog systems and the use of XML-
based data interfaces as examplified by the Multimodal
Markup Language M3L.
4.1 VERBMOBIL
VERBMOBIL (Wahlster, 2000) is a speaker-independent
and bidirectional speech-to-speech translation system
that aims to provide users in mobile situations with si-
multaneous dialog interpretation services for restricted
topics. The system handles dialogs in three business-
oriented domains?including appointment scheduling,
travel planning, and remote PC maintenance?and pro-
vides context-sensitive translations between three lan-
guages (German, English, Japanese).
VERBMOBIL follows a hybrid approach that incorpo-
rates both deep and shallow processing schemes. A pe-
culiarity of the architecture is its multi-engine approach.
Five concurrent translations engines, based on statistical
translation, case-based translation, substring-based trans-
lation, dialog-act based translation, and semantic transfer,
compete to provide complete or partial translation results.
The final choice of the translation result is done by a sta-
tistical selection module on the basis of the confidence
measures provided by the translation paths.
In addition to a stationary prototype for face-to-face di-
alogs, a another instance has been realized to offer trans-
lation services via telephone (Kirchmann et al, 2000).
The final VERBMOBIL demonstrator consists of about
70 distributed software components that work together to
recognize spoken input, analyze and translate it, and fi-
nally utter the translation. These modules are embedded
into an earlier version of the MULTIPLATFORM testbed
using almost 200 data pools?replacing several thousand
point-to-point connections?to interconnect the compo-
nents.
4.2 SMARTKOM
SMARTKOM is a multimodal dialog system that com-
bines speech, gesture, and facial expressions for both,
user input and system output (Wahlster et al, 2001). The
system aims to provide an anthropomorphic and affective
user interface through its personification of an interface
agent. The interaction metaphor is based on the so-called
situated, delegation-oriented dialog paradigm. The basic
idea is, that the user delegates a task to a virtual commu-
nication assistant which is visualized as a life-like char-
acter. The interface agent recognizes the user?s inten-
tions and goals, asks the user for feedback if necessary,
Multimodal
Dialog
Backbone
Home:
Consumer Electronics
EPG
Public:
Cinema,
Phone, 
Fax,
Mail, 
Biometrics
Mobile:
Car and
Pedestrian
Navigation
Application
Layer
SmartKom-Mobile
SmartKom-Public
SmartKom-Home
Figure 3: SMARTKOM kernel and application scenarios.
Smartakus, the SMARTKOM life-like character is shown
in the lower left corner.
accesses the various services on behalf of the user, and
presents the results in an adequate manner.
The current version of the MULTIPLATFORM testbed,
including M3L, is used as the integration platform for
SMARTKOM. The overall system architecture includes
about 40 different components. As shown in Figure 3, the
SMARTKOM project addresses three different application
scenarios.
SMARTKOM PUBLIC realizes an advanced multi-
modal information and communication kiosk for airports,
train stations, or other public places. It supports users
seeking for information concerning movie programs, of-
fers reservation facilities, and provides personalized com-
munication services using telephone, fax, or electronic
mail.
SMARTKOM HOME serves as a multimodal portal to
information services. Using a portable webpad, the user
is able to utilize the system as an electronic program
guide or to easily control consumer electronics devices
like a TV set or a VCR. Similar to the kiosk application,
the user may also use communication services at home.
In the context of SMARTKOM HOME two different inter-
action modes are supported and the user is able to easily
switch between them. In lean-forward mode coordinated
speech and gesture input can be used for multimodal in-
teraction with the system. Lean-backward mode instead
is constrained to verbal communication.
SMARTKOM MOBILE uses a PDA as a front end,
which can be added to a car navigation system or is
carried by a pedestrian. This application scenario com-
prises services like integrated trip planning and incre-
mental route guidance through a city via GPS and GSM,
GPRS, or UMTS connectivity.
Figure 4: Adapted testbed GUI for the COMIC system.
4.3 COMIC
COMIC2 (Conversational Multimodal Interaction with
Computers) is a recent research project that focuses on
computer-based mechanisms of interaction in coopera-
tive work. One specific sample application for COMIC
is a design tool for bathrooms with an enhanced multi-
modal interface. The main goal of the experimental work
is to show that advanced multimodal interaction can make
such a tool usable for non-experts as well.
The realization of the integrated COMIC demonstrator
is based on the MULTIPLATFORM testbed. Figure 4 dis-
plays the control interface of the multimodal dialog sys-
tem. On the input side, speech and handwriting in com-
bination with 3-dimensional pen-based gestures can be
employed by the user. On the output side, a dynamic
avatar with synthesized facial, head and eye movements
is combined with task-related graphical and textual infor-
mation. In addition to multiple input and output chan-
nels, there are components that combine the inputs?
taking into account paralinguistic information like into-
nation and hesitations?and interpret them in the context
of the dialog, plan the application-specific actions to be
taken and finally split the output information over the
available channels.
5 Related Work
GCSI, the Galaxy Communicator software infrastructure
(Seneff et al, 1999), is an open source architecture for
the realization of natural language dialog systems. It
can be described as a distributed, message-based, client-
server architecture, which has been optimized for con-
structing spoken dialog systems. The key component in
this framework is a central hub, which mediates the inter-
action among various servers that realize different dialog
system components. The central hub does not only han-
dle all communications among the server modules but is
2see http://www.hcrc.ed.ac.uk/comic/
also responsible to maintain the flow of control that de-
termines the processing within the integrated dialog sys-
tem. To achieve this, the hub is able to interpret scripts
encoded in a special purpose, run-time executable pro-
gramming language.
The GCSI architecture is fundamentally different from
our approach. Within the MULTIPLATFORM testbed there
exists no centralized controller component which could
become a potential bottleneck for more complex dialog
systems.
OAA, the Open Agent Architecture (Martin et al,
1999), is a framework for integrating a community of
heterogeneous software agents in a distributed environ-
ment. All communication and cooperation between the
different is achieved via messages expressed in ICL, a
logic-based declarative language capable of representing
natural language expressions. Similar to the GCSI archi-
tecture, a sort of centralized processing unit is required
to control the behavior of the integrated system. So-
called facilitator agents reason about the agent interac-
tions necessary for handling a given complex ICL ex-
pression, i.e. the facilitator coordinates the activities of
agents for the purpose of achieving higher-level, complex
problem-solving objectives. Sample applications built
with the OAA framework also incorporated techniques to
use multiple input modalities. The user can point, speak,
draw, handwrite, or even use a standard graphical user
interface in order to communicate with a collection of
agents.
RAGS (Cahill et al, 2000) does not address the entire
architecture of dialog systems and multimodal interac-
tion. The RAGS approach, which stands for Reference
Architecture for Generation Systems, focuses instead on
natural language generation systems and aims to produce
an architectural specification and model for the develop-
ment of new applications in this area. RAGS is based
on the well-known three-stage pipeline model for natural
language generation which distinguishes between content
determination, sentence planning, and linguistic realiza-
tion. The main component of the RAGS architecture is a
data model, in the form of a set of declarative linguistic
representations which cover the various levels of repre-
sentation that have to be taken into account within the
generation process. XML-based notations for the data
model can be used in order to exchange RAGS represen-
tations between distributed components. The reference
architecture is open regarding the technical interconnec-
tion of the different components of a generation system.
One specifically supported solution is the use of a single
centralized data repository.
6 Conclusion
MULTIPLATFORM provides a practical framework for
large-scale software integration that results from the re-
alization of various natural language and multimodal di-
alog systems. The MULTIPLATFORM testbed is based on
an open component architecture which employs message-
passing to interconnect distributed software modules. We
propose to operationalize interface specifications in the
form of an XML language as a viable approach to assem-
ble knowledge-based as well as conventional components
into an integrated dialog system. The testbed software is
currently being refactored and we are planning to make it
publicly available as open source software.
More than one hundred modules have already been
used within the MULTIPLATFORM testbed. So far, how-
ever, these dialog system components are not freely avail-
able for public distribution. The availability of pre-
fabricated modules as part of the testbed software would
also enable third parties to develop complete dialog sys-
tem applications through the reuse of provided standard
components.
In addition to the software infrastructure, the practical
organization of the project constitutes a key factor for the
successful realization of an integrated multimodal dialog
system. Stepwise improvement and implementation of
the design of architecture details and interfaces necessi-
tates an intensive discussion process that has to include
all participants who are involved in the realization of sys-
tem components in order to reach a common understand-
ing of the intended system behavior. Independent integra-
tion experts that focus on the overall dialog system have
proven to be helpful for the coordination of this kind of
activities.
References
Lynne Cahill, Christy Doran, Roger Evans, Rodger Kib-
ble, Chris Mellish, Daniel Paiva, Mike Reape, Donia
Scott, and Neil Tipper. 2000. Enabling Resource Shar-
ing in Language Generation: An Abstract Reference
Architecture. In Proc. of the 2nd Int. Conf. on Lan-
guage Resources and Evaluation, Athens, Greece.
Wolfgang Emmerich. 2000. Software Engineering and
Middleware: A Roadmap. In Proc. of the Conf. on the
Future of Software Engineering, pages 117?129. ACM
Press.
Lee D. Erman, Frederick Hayes-Roth, Victor R. Lesser,
and D. Raj Reddy. 1980. The Hearsay-II Speech-
Understanding System: Integrating Knowledge to
Resolve Uncertainty. ACM Computing Surveys,
12(2):213?253.
Dieter Fensel, Frank van Harmelen, Ian Horrocks, Deb-
orah L. McGuinness, and Peter F. Patel-Schneider.
2001. OIL: An Ontology Infrastructure for the Seman-
tic Web. IEEE Intelligent Systems, 16(2):38?45.
Tim Finin, Richard Fritzson, Don McKay, and Robin
McEntire. 1994. KQML as an Agent Communica-
tion Language. In Proc. of the 3rd Int. Conf. on Infor-
mation and Knowledge Management, pages 456?463.
ACM Press.
Al Geist, Adam Beguelin, Jack Dongorra, Weicheng
Jiang, Robert Manchek, and Vaidy Sunderman. 1994.
PVM: Parallel Virtual Machine. A User?s Guide and
Tutorial for Networked Parallel Computing. MIT
Press.
Iryna Gurevych, Stefan Merten, and Robert Porzel. 2003.
Automatic creation of interface specifications from
ontologies. In Proc. of the HLT-NAACL?03 Work-
shop on the Software Engineering and Architecture of
Language Technology Systems (SEALTS), Edmonton,
Canada.
Jon Hopkins. 2000. Component Primer. Communica-
tions of the ACM, 43(10):27?30.
Heinz Kirchmann, Alassane Ndiaye, and Andreas Klu?ter.
2000. From a Stationary Prototype to Telephone
Translation Services. In Wahlster (Wahlster, 2000),
pages 659?669.
Andreas Klu?ter, Alassane Ndiaye, and Heinz Kirchmann.
2000. Verbmobil From a Software Engineering Point
of View: System Design and Software Integration. In
Wahlster (Wahlster, 2000), pages 635?658.
David L. Martin, Adam J. Cheyer, and Douglas B.
Moran. 1999. The Open Agent Architecture: A
Framework for Building Distributed Software Sys-
tems. Applied Artificial Intelligence, 13(1?2):91?128.
Jeremy Pitt and Abe Mamdani. 1999. Some Remarks on
the Semantics of FIPA?s Agent Communication Lan-
guage. Autonomous Agents and Multi-Agent Systems,
2(4):333?356.
Stephanie Seneff, Raymond Lau, and Joseph Polifroni.
1999. Organization, Communication, and Control in
the Galaxy-II Conversational System. In Proc. of Eu-
rospeech?99, pages 1271?1274, Budapest, Hungary.
Wolfgang Wahlster, Norbert Reithinger, and Anselm
Blocher. 2001. SmartKom: Multimodal Communi-
cation with a Life-Like Character. In Proc. of Eu-
rospeech?01, pages 1547?1550, Aalborg, Denmark.
Wolfgang Wahlster, editor. 2000. Verbmobil: Foun-
dations of Speech-to-Speech Translation. Springer,
Berlin.
Gerhard Weiss, editor. 2000. Multiagent Systems:
A Modern Approach to Distributed Artificial Intelli-
gence. MIT Press.
Ming-Wei Wu and Ying-Dar Lin. 2001. Open Source
Software Development: An Overview. Computer,
34(6):33?38.
Ends-based Dialogue Processing  
Jan Alexandersson, Tilman Becker, Ralf Engel, Markus Lo?ckelt,
Elsa Pecourt, Peter Poller, Norbert Pfleger and Norbert Reithinger
DFKI GmbH
Stuhlsatzenhausweg 3
66123 Saarbru?cken
 janal,becker,engel,loeckelt,pecourt,poller,pfleger,bert  @dfki.de
Abstract
We describe a reusable and scalable dialogue
toolbox and its application in multiple systems.
Our main claim is that ends-based representa-
tion and processing throughout the complete
dialogue backbone it essential to our approach.
1 Introduction
In the last couple of years our group at DFKI in
Saarbru?cken has been involved in a number of projects
aiming at interfacing different devices in an intelligent
way. The main goal of these projects has been to build
functioning robust systems with which it is natural to
communicate (not only for some few examples phrases).
During the projects we have developed a dialogue tool-
box consisting of a number of modules. By combining
these modules in different ways we are able to realize a
number of different types of dialogues, e. g., information
seeking/browsing, device control, multi/cross-application
and agent-mediated interactions for a number of (diverse)
applications and systems. The full-blown combination of
all modules form our dialogue backbone capable of en-
gaging in multimodal man?machine communication.
In this paper, we discuss some of the design decisions
taken along the road as well as lessons learned during the
projects. Based on our experiences, we argue that ends-
based processing is vital to the success of our approach.
We strive for a balance between complex theories and
pragmatic decisions. Of secondary interest is the imple-
mentation of theories capable of processing linguistically
exotic phenomena in favor of ends-based processing in
all modules of the toolbox. Hence it is more important to
reach the representation rather than how we get there.
An ontology is often ? as we understand it ? a good
ends-based representation but we can do without it. In
the MIAMM project (see section 2) we use no ontology

The research presented here is funded by the German Min-
istry of Research and Technology under grant 01 IL 905, the Eu-
ropean Union under the grants IST-2000-29487 and IST-2001-
32311 and IDS-Scheer AG.
but instead an event based representation. Whatever rep-
resentation we do choose, we would like to stress the im-
portance of a consequent principle-based design of the
representation and the fact that the complete backbone
uses it. Exactly this guarantees, e. g., the scalability of
our approach.
The paper is organized as follows: the next section pro-
vides an overview of projects and systems central to the
development of our toolbox. Section 3 describes most of
its modules. Before we conclude the paper, we provide a
list of claims and lessons learned in section 4.
2 A Number of Projects
In this paper, we describe a toolbox which we can cus-
tomize according to the projects needs. Using this tool-
box we have implemented a number of systems, all hav-
ing different requirements, needs and ends. They range
from (monomodal) typed input/output as in the NaRATo
project to multimodal agent-mediated communication as
in SmartKom. Below we describe the different projects
and systems showing that we are able to cover several
kinds of communication paradigm.
SmartKom
SMARTKOM is a mixed-initiative dialogue system that
provides full symmetric multimodality by combining
speech, gesture, and facial expressions for both user in-
put and system output (Wahlster, 2003). It provides
an anthropomorphic and affective user interface through
its personification of an embodied conversational agent,
called Smartakus. The interaction metaphor is based
on the so-called situated, delegation-oriented dialogue
paradigm: the user delegates a task to a virtual commu-
nication assistant which is visualized as a life-like char-
acter. The interface agent recognizes the user?s inten-
tions and goals, asks the user for feedback if necessary,
accesses the various services on behalf of the user, and
presents the results in an adequate manner. Non-verbal
reactions of the users are extracted from their facial ex-
pression or the prosodic features and affect subsequent
system presentations.
As it is depicted in Figure 1, SMARTKOM realizes a
flexible and adaptive shell for multimodal dialogues and
addresses three different application scenarios:
Figure 1: SMARTKOM?s dialogue backbone and applica-
tion scenarios
SMARTKOM PUBLIC realizes an advanced multimodal
information and communication kiosk for, e. g., shop-
ping malls. The user can get information about movies,
reserve seats in a theater, and communicate using
telephone, fax, or electronic mail. Before the sys-
tem grants access to personal data, e. g., an address
book, the user has to authenticate himself using either
hand contour recognition, signature or voice verification.
SMARTKOM HOME serves as a multimodal infotainment
companion for the home theater. A portable web-pad acts
as an advanced remote control where the user gets pro-
gramming information from an electronic program guide
service and easily controls consumer electronics devices
like a TV set or a VCR. Similar to the kiosk application,
the user may also use communication services at home.
SMARTKOM MOBILE realizes a mobile travel compan-
ion for navigation and location-based services. It uses a
PDA as a front end which can be added to a car navigation
system. This system offers services like integrated trip
planning and incremental route guidance. In the mobile
scenario speech input can be combined with pen-based
pointing.
All functionalities, modality combinations and techni-
cal realizations including a wide variety of hardware op-
tions for the periphery are addressed by the same core di-
alogue system with common shared knowledge sources.
The processing relies on a knowledge based, configurable
approach: we provide general solutions based on declar-
ative knowledge sources in favour for special solutions
and/or shortcuts or application specific procedural pro-
cessing steps within the dialogue core of the system.
The interaction processing is based on M3L (Multimodal
Markup Language), a complete XML language designed
in the context of SMARTKOM that covers all data in-
terfaces within the complex multimodal dialogue sys-
tem (Gurevych et al, 2003a). The technical realization
is based on the MULTIPLATFORM testbed (Herzog et
al., 2004), an integration platform that provides a dis-
tributed component architecture. MULTIPLATFORM is
implemented on the basis of the scalable and efficient
publish/subscribe approach that decouples data produc-
ers and data consumers. Software modules communicate
via so-called data pools that correspond to named mes-
sage queues. Every data pool can be linked to an individ-
ual data type specification in order to define admissible
message contents.
MIAMM
The main objective of the MIAMM project is to de-
velop new concepts and techniques in the field of multi-
modal interaction to allow fast and natural access to large
multimedia databases (Reithinger et al, 2003b). This
implies both the integration of available technologies in
the domain of speech interaction (Natural Language Un-
derstanding ? SPIN ? see section 3.1) and interaction
management (Action Planner ? AP ? see section 3.3)
and the design of novel technology for haptic designation
and manipulation coupled with an adequate visualization.
The envisioned end-user device is a hand-held PDA that
provides an interface to a music database. The device in-
cludes three force-feedback buttons on the left side and
one wheel on the upper right side (see figure 2). The
buttons allow navigation through the visualized data, and
performing of various actions on the presented objects
(e. g., select, play).
The MIAMM architecture follows the ?standard? ar-
chitecture of interactive systems, with the consecutive
steps mode analysis, mode coordination, interaction man-
agement, presentation planning, and mode design. To
cope with artefacts arising from processing time require-
ments and coordination of different processes, this archi-
tecture was modified, so that only events that are rele-
vant to other modules are sent, whereas the others remain
internal. Thus, haptic interaction is decoupled from the
more time-consuming speech processes, and only sends
feedback when it is needed for the resolution of under-
specified structures or when the interaction involves ex-
ternal actions, e. g., playing a selected track. The sys-
tem consists of two modules for natural language input
processing, namely recognition and interpretation. On
the output side an MP3 player is used to play the songs
and the pre-recorded speech prompts to provide acoustic
feedback. The visual-haptic-tactile module is responsi-
ble for the selection of the visualization, and for the as-
signment of haptic features to the force-feedback buttons.
The visualization module renders the graphic output and
interprets the force imposed by the user to the haptic but-
tons. The dialogue manager consists of two main blocks,
Figure 2: The force-feedback device developed in the MI-
AMM project. The display shows a view of a database
using a timeline.
the multimodal FUSION (see section 3.2) which is re-
sponsible for the resolution of multimodal references us-
ing the contextual information hold in the dialogue his-
tory, and the AP, that interprets the user intention and trig-
gers a suitable system response. The AP is connected via
a domain model to the multimedia database. The domain
model uses an inference engine that facilitates access to
the database.
The integration environment is based on the Simple
Object Access Protocol (SOAP) (see www.w3.org/
TR/SOAP). The communication between the modules
is based on the multimodal interface language (MMIL).
This specification accounts for the incremental integra-
tion of multimodal data to achieve a full understanding
of the multimodal acts within the system. It is flexible
enough to handle the various types of information pro-
cessed and generated by the different modules.
COMIC
COMIC is an European IST 5th framework project fo-
cusing on new methods of work and e-commerce (den
Os and Boves, 2003). Goal of this project is to develop
a user centric, multimodal interface for a bathroom de-
sign tool which was developed by the COMIC partner Vi-
Soft (see www.visoft.de). The implementation work
is accompanied by research in the cognitive aspects of
human-human and human-computer interaction.
Figure 3 shows a user interacting with the initial pro-
totype of the system. The system enables the users to
enter by speech and pen the blueprint of their bathroom
including handwriting and drawing dimensions of walls,
windows, and doors respectively. In a second step the
user can browse and choose decoration and sanitary ware
for the bathroom. Finally, the underlying application al-
lows real-time, three-dimensional exploring of the mod-
eled bathroom. System output includes the application
itself and a realistically animated, speaking head.
Figure 3: Interaction with the COMIC system.
The architecture of the COMIC system again resem-
bles the architecture of our core dialogue backbone.
However, only SPIN, FUSION and Generation1 are used
for this project all other modules are provided by other
partners. COMIC is also based on MULTIPLATFORM as
the integration middleware, allowing a reuse of the mod-
ule wrappers and engines. The representation of infor-
mation is similar to that of SMARTKOM although the ac-
tual ontology differs in significant parts (e. g., no upper
model). Hence the integration of SPIN and Generation
was limited to the revision and adaption of the language
and ontology dependent knowledge sources. FUSION,
however, needed a deeper adaption as outlined in section
3.2.
Yet another (kind of) system
For the system NaRATo we have used parts of our tool-
box ? language understanding, discourse modeling , ac-
tion planning, and generation ? for a dialogue system in-
terfacing the ARIS tool-set, a business process manage-
ment system (see www.ids-scheer.com). The sys-
tem uses typed input and output to provide access to a
given process model stored in a database.
3 A Number of Modules
Our toolbox deploys a number of modules which are con-
nected in a (nowadays) standard fashion (see figure 4).
The input channels are fused by the modality fusion. This
module is also responsible for resolving not just deictic
expressions using gesture and speech but also referen-
tial expressions involving the dialogue context. The dis-
course module is the central repository for modality de-
pendent and modality independent information. Here, the
1Generation in COMIC is actually only realization as the
Fission module takes care of content selection and (most of)
sentence planning.
user contribution is interpreted in context which involves
resolving, e. g., a wide range of elliptical contributions.
The action planner is the actual engine: using a regres-
sion planning approach the next system action is planned
possibly preceeded by access of some external device. Fi-
nally, the presentation manager renders the system action.
Here, the availability of different output modalities and
the situation are influencing the realization of the action.
Our architecture differs from that of (Blaylock et al,
2003) in that the responsibility of the next system ac-
tion is in our case purely decided by the action planner;
the approach has some similarities with the one taken in
(Larsson, 2002) in that most communicative actions rep-
resent request-response interactions along goals (akin to
QUDs), and there is a notion of information state, which
is however kept separated between the discourse mod-
eler (for information specific to dialogue content, roughly
equivalent to the SHARED information in IBiS) and the
action planner (for other information, such as the agenda
of the dialogue engine).
3.1 Natural Language Understanding
The task of the natural language understanding module is
to transform the output of the speech recognizer into a list
of possible user intentions which are already represented
in the system-wide high-level ontology (see section 4).
For this task a new template-based semantic parsing ap-
proach called SPIN (Engel, 2002) was developed at DFKI
and is used in all aforementioned projects.
As typical for a semantic parser, the approach does not
need a syntactic analysis, but the high level output struc-
ture is built up directly from word level. This is feasi-
ble since the input consists of spoken utterances intended
to interact with a computer system and therefore, they
are usually syntactically less complicated and limited in
length. Furthermore, the lack of a syntactical analysis in-
creases the robustness against speech recognition errors
(speaker independent recognizers still have a word error
rate of 10%-30%) and syntactically incorrect input by the
user.
SPIN differs from other existing semantic parsing ap-
proaches by providing a more powerful rule language
and a powerful built-in ontology formalism. The main
motivation for the powerful rule language is to simplify
the creation and maintenance of rules. As the amount
of required rules is quite large (e.g., in the SmartKom
project 435 templates are used), easy creation and main-
tenance of the rules is one of the most important issues for
parsers in dialogue systems. Additionally, high-level out-
put structures have to be generated and these output struc-
tures may be structurally quite different from the implied
structure of the input utterance. A powerful rule language
simplifies this task significantly.
Several off-line optimizations still provide fast pro-
cessing despite the increased rule power. The most im-
portant off-line optimization is the computation of a fixed
rule application order with the objective to avoid wasting
time by the generation of sub-optimal results.
The powerful built-in ontology formalism helps to in-
tegrate the module in dialogue systems by only creating
the knowledge bases and an interface layer but without
any changes in the code base. Due to the lack of a stan-
dard ontology formalism for dialogue systems, each di-
alogue system uses a slightly different formalism. The
powerful internal ontology formalism simplifies the task
of mapping the system-wide ontology formalism to the
internal one.
Current research will improve the approach in two ar-
eas. First, the time-consuming creation of the knowl-
edge bases which has to be done completely manually
up to now will be supported by machine learning tech-
niques. Second, the external linguistic preprocessing of
the speech recognizer output, like a syntactic analysis,
will be possible without incorporating linguistic informa-
tion into the knowledge bases. This would allow to pro-
cess syntactically more complicated user utterances and
still provides easy creation of the knowledge bases.
3.2 Modality Fusion
Multimodal dialogue systems like SmartKom or Comic
give users the opportunity to express their needs not only
by speech but also by different modalities, e. g., by ges-
turing or by using a pen. Furthermore, users can also
combine several modalities to express one multimodal ut-
terance (e. g., ?I want to start here? accompanied by a
pointing gesture towards a location on a map). As the rec-
ognizers and analyzers of the different modalities gener-
ate modality specific hypotheses, a component is needed
to synchronize and integrate those monomodal hypothe-
ses into multimodal ones. This module is called FUSION.
Based on human-human communication research,
e. g., (Oviatt, 1999), we can identify four basic interac-
tion patterns of how to use different modalities within a
single multimodal utterance:
redundant the information provided by two modalities
is basically the same,
concurrent two modalities are used one after another to
provide information,
complementary the information provided by two
modalities can be intertwined,
contradicting the information provided by one modality
is contradictory to the information provided by the other
modality.
All these interaction patterns can be resolved by ob-
taining access to information about the internal structure
of objects. Especially when having to integrate informa-
tion from one source into another, we need to know what
specific objects look like, e. g., which sub-objects they
Discourse
Modelling
Management
Application
Fusion
Modality Action
Planning
Presentation
Manager
Analyzer Generator
Context Information
Main Data Flow
Figure 4: The architecture of the full blown version of our dialogue toolbox. Modality Fusion combines the different
results from the analyzers; Discourse Modeling interprets in context; Action Planning determines the next system
action; Presentation Management splits and coordinates the output on the different output modalities.
comprise. This information is typically provided by an
ontology, e. g., via the type hierarchy and the slot defini-
tions of each object. So, what FUSION must accomplish
is to utilize processing strategies based on a type hierar-
chy and a given set of object definitions.
In SmartKom we applied a so called slot-filling ap-
proach for the integration of the two modalities speech
and gesture. Multimodal hypotheses are compiled by in-
serting the hypotheses of the gestural modality into the
hypotheses of the speech modality. The advantage of this
approach is that apart from an ontology no further knowl-
edge sources are required. This approach proved to be
very fast and robust. However, the drawback is that an
adaption to a different dialogue system or to new modal-
ities is quite expensive.
With respect to our overall goal of building a scal-
able and reusable core dialogue system, we uncoupled
the core FUSION system from the needs of the dialogue
system, the available modalities, and processing strate-
gies. Thus, we implemented a special purpose production
rule system. Key to this approach is that all processing
strategies are defined by production rules which can be
easily created and adapted to the new surroundings and
there are two powerful operations for accumulating in-
formation ? unification and overlay (Alexandersson and
Becker, 2003).
3.3 Action Planning
Task oriented cooperative dialogues, where participants
collaborate to achieve a common goal, can be viewed
as coherent sequences of utterances asking for actions to
be performed or introducing new information to the di-
alogue context. The task of the action planner is to rec-
ognize the user?s goal, to trigger required actions for its
achievement, and to devise appropriate sub-dialogues and
feedback. The actions can be internal, such as updating
the internal state of the system, or external, like database
queries, device operation or communication with the user.
Thus, the action planner controls both the task and the
interaction structure. Task and dialogue interactions are
viewed as joint communicative games played with dif-
ferent agents, including the user and all modules that di-
rectly communicate with the action planner.2 Participants
are abstractly represented by communication channels
transforming between the uniform internal representation
of communicative moves to the data structures used by
external participants. Each game is composed of a num-
ber of moves, defined by dialogue knowledge sources.
The game definitions are similar to STRIPS plan opera-
tors. They specify sets of preconditions and effects, and
additionally, for each move the channel through which
the data flows, and data structures containing the seman-
tic content of the move intention. The adoption of a dia-
logue goal triggers a planning process (non-linear regres-
sion planning, with hierarchical decomposition of sub-
goals) resulting in a series of communicative games to
be played to achieve the goal. Move execution is then
interleaved with checking their outcome, and possibly re-
planning if preconditions are violated. This strategy al-
lows the system to deal with unexpected user inputs like
misunderstandings or changing of goals.
The approach of planning with communicative games
has two benefits with respect to the scalability of the sys-
tem, one regarding communication channels, the other
stemming from the use of small dialogue game units.
It is possible to integrate support for any number of ad-
ditional devices to an already existing system by adding
new communication channels (one Java class each); di-
alogue moves that do not use these channels will not be
affected. Still, dialogue specifications for newly added
devices can make use of the already defined ones.
As described above, the dialogue behavior is coded
in terms of communicative games consisting of dialogue
moves. For predetermined sequences of moves (e. g., a
2We use the term ?communicative games? in addition to ?di-
alogue games,? since our dialogue model also includes com-
munication interaction with applications and devices, such as
database requests and answers, in terms of game moves.
fixed protocol for sending fax messages: (1) scan docu-
ment, (2) convert to fax format, (3) send it via fax ap-
plication), the dialogue game can resemble a fixed script,
like the pre-made plans used, e. g., by (Larsson, 2002)),
but in general, games specify atomic steps like single
request-response subdialogues. To devise the course of
action, a plan is then constructed dynamically as a game
sequence. This has the advantage that (1) the plan can be
flexibly adapted to changed circumstances, e. g., if a step
becomes obsolete or is addressed early, and (2) games
can be shared and reused as building blocks for other
applications. So, when new functionality is integrated,
the plan knowledge source will stay reasonably small?
growing linearly in the number of games, not exponen-
tially with the possible recipes.3
3.4 Discourse Modeling
The main objective of the discourse modeler (henceforth
DIM) is to incorporate information stemming from the
previous discourse context into the current intention hy-
potheses produced by the analysis modules. This objec-
tive decomposes into two main tasks which are on the
one hand enhancing a hypothesis with compatible back-
ground information and estimating how well it fits the
previous discourse context ? what we call enrichment and
validation ? and on the other hand the resolution of refer-
ring expressions.
Discourse processing in the framework of a multi-
modal dialogue system has to deal with an extended set
of input and output devices. Gestures, for example, ac-
companying speech not only support the resolution of re-
ferring expressions, in addition they change the discourse
context. In general, the resolution of referring expres-
sions within a multimodal approach requires access to a
visual context representation. One key aspect of DIM is
a unified context representation taking both the discourse
and the visual context into account.
Our approach consists of a three-tiered discourse rep-
resentation combined with a two layered focus handling,
see (Pfleger et al, 2003). The actual processing is
done by utilizing two operations: unification and over-
lay (Alexandersson and Becker, 2003). In combination
with a scoring function (Pfleger et al, 2002), the latter
is our main tool for enrichment and validation. Key to
this approach is that DIM can be easily adapted to other
dialogue systems with different tasks and demands. In
that sense, the actual context representation is indepen-
dent from the type of objects to be stored. Additionally,
DIM can be used not only within a multimodal dialogue
system but also within monomodal ones, as we showed
in the NaRATo project.
3The usual downside is, the planning space is of course ex-
ponential. But as we use goal-directed search, only a small frac-
tion of the possible plans is ever examined in practice.
3.5 Modality Fission
The modalities used in the SmartKom system are ges-
ture, mimics, speech and also graphical presentations on
devices of different sizes. The main task of multimodal
fission is partitioning, i. e., dividing the presentation tasks
into subtasks and generating an execution plan. A follow-
up task is then the coordination and synchronization of re-
lated tasks, e. g., presentation of a graphical element with
a pointing gesture and synchronization with speech.
The fission module is embedded in a presentation plan-
ner that also subsumes the graphical realization task. The
module generates a full plan for graphics, gesture and
mimics while the plan for speech is generated only on an
abstract subtask level that is handed as input to the Text
Generator (see next section).
The planning of a multimodal presentation consists of
two parts: static gesture-sensitive graphical elements and
a corresponding multimodal animation of the agent in-
cluding gestures referring to objects with aligned audio-
visual speech output. The first step performed on the in-
put is a transformation into the internal input format of
the core planning component PrePlan by applying an ap-
propriate XSLT-stylesheet.
Then, the presentation planner starts the planning pro-
cess by applying a set of presentation strategies which
define how the facts are presented in the given scenario.
Based on constraints, the strategies decompose the com-
plex presentation goal into primitive tasks and at the same
time they execute the media fission step depending on
available modalities, which means they decide which part
of the presentation should be instantiated as spoken out-
put, graphics, or gestures of our presentation agent.
After planning the graphical presentation, appropriate
speech and gesture presentations are generated. The ges-
ture and speech form is chosen depending on the graph-
ically shown information. I.e., if the graphically pre-
sented information is in the focus of a presentation, only
a comment is generated in speech output. The goal of the
gesture presentation is then to focus on the appropriate
graphical element. If there is no graphically presentable
information or it is insufficient, more speech is generated.
3.6 Natural Language Generator
The design of the Natural Language Generation (NLG)
module is guided by the need to (i) adapt only knowledge
sources when adding a new application and (ii) general-
izing the knowledge sources from the applications.
Thus the NLG module is divided into an engine and
declarative knowledge sources which are designed with
the goal of capturing generalizations. The input to the
NLG module are abstract presentation goals that are
based on the ends-based presentation; the output is (an-
notated) text that typically is sent to a speech synthesizer.
E.g., the NLG module in SmartKom uses syntactic struc-
ture and discourse information to supply richly annotated
text for the Concept-To-Speech (CTS) approach.
On the one hand, the NLG module is templated?based
(see also SPIN), skipping multiple layers of represen-
tation when mapping from the presentation goals. On
the other hand, the templates are ?fully specified? in
the sense that they include intermediate layers of rep-
resentation where possible to permit a later separation
of rules into a multi-stage generation module. E.g., in-
cluding syntax was also necessary for CTS, including se-
mantics allows for the extraction of a realization module
for COMIC. The template rules are based on the same
PrePlan planning component used in fission. At least
since (Reiter, 1995) the use of templates and ?deep rep-
resentations? is not seen as a contradiction. Picking up
on this idea, the generation component in SmartKom is
based on fully lexicalized generation (Becker, 1998), us-
ing partial derivation trees of a Tree-Adjoining Grammar
(TAG). Right from the beginning of development, deriva-
tion trees which are seen as reflecting syntactic depen-
dencies have been an explicitly represented layer in the
template rules. Thus the higher level planning rules de-
cide content selection, sentence plans and lexicalization,
leaving syntactic realization to a TAG-based second step.
During development, we have enriched the syntactic
trees with nested feature structures and have just finished
a transformation of the phrasal templates to a fully lexi-
calized TAG, where every lexical item has its unique tree.
4 Ends-Based Processing
One of the most important constraints when building a
functioning system has been the domain of the appli-
cation. Based on the domain we developed ends-based
representations which have so far mostly been ontolo-
gies or ontology-like structures, e. g., (Gurevych et al,
2003b) but which in fact could be event-based represen-
tations as well. How interpretation and presentation are
connected to the abstract representation is of secondary
interest; Our backbone uses this task-oriented represen-
tation for communication and processing and the way
there and back may exclude, for instance, traditional se-
mantics. We make two important observations: on the
one hand, that the complete backbone should use a sin-
gle representation, so that translations between different
representations are avoided. Important here is that each
module (ideally) separates its engine from its knowledge
base. On the other hand, the common representation has
to be ends-based and fulfil the needs of the application.
The latter point leads us to another lesson learned: The
application has to be examined and its needs have to be
mirrored in the representation. We also have to deter-
mine what interactions we are aiming for. Since, e. g., in
SmartKom, we pursue a situated delegation-oriented dia-
log paradigm ? meaning that the system is in itself not a
dialogue partner as in (Blaylock et al, 2003) but instead
the dialogue is mediated by an animated agent ? we en-
capsulate the details of the application APIs in an applica-
tion manager and hence provide a user-oriented view of
the application(s). Additionally, the dialogue plans are
represented separately from the ends-based representa-
tion in a different knowledge base, i. e., the plan speci-
fications for the action planner. However, the plans refer
to the application using the ends-based representation.
We have acquired our knowledge, e. g., ends-based
represenations or interpretation rules completely by hand.
While we avoid the potentially costly resources for the
collection and annotation of corpora for automated lear-
ning4, the question remains whether expanding knowl-
edge sources by hand is feasible. Our approach has in-
deed allowed for scaling up ? in SmartKom we have ex-
tended the system to more than 50 functionalities overall
(Reithinger et al, 2003a).
In the following, we list the most important lessons we
learned, which is by no means exhaustive:
Encapsulation Encapsulate the backbone from the ap-
plication(s). This was one of the main lessons from the
NaRATo and the SmartKom projects. We did not do it in
the NaRATo project and spent lots of time interfacing the
database. In SmartKom, such a module exists, and the
backbone developers could concentrate on more relevant
tasks in dialogue processing proper.
Representation Use one representation throughout the
backbone. It is a secondary question how exactly it is
done, but it is essential that you get there and avoid spend-
ing time on converting between different formalisms.
Representation (revisited) There is to be no presentation
(system output) without representation on the ends-based
representation level. This representation is part of the
global dialogue history residing in the discourse module
and can be accessed by any module, e. g., for reference
resolution at any time during the course of the dialogue.
Interface In the case of a multi-module approach, use
one well-defined representation for module communica-
tion. In most cases we have used XML and XML Schema
which is convenient because a wide variety of infrastruc-
ture and tools is available. For instance, most XML pro-
cessing tools allow for syntactic validation. However,
XML is not mandatory. A final remark here: using XML
in combination with stylesheets, we can in fact ? contrary
to the advice in Representation (above) ? translate or con-
vert messages to some internal representation easily.
Interface (revisited) Interfaces should be clean and well-
defined. One reason for the success of the SmartKom
project was the requirement to define every interface for-
mally by XML Schema. These XML Schemata were kept
in a project-wide repository and changed at this one place
4Supervised as well as unsupervised
after mutual agreement only. Due to the multi-blackboard
approach, there are not point-to-point connections, but   -
to-  connections, and an interface definition comprises
of a precise description of what is supposed to be an al-
lowed message for a specific blackboard.
Integration Our large projects have profited enormously
of a dedicated integration group providing infrastructure,
integration cycles and ? for, e. g., the SmartKom and
COMIC systems ? a testbed (Herzog et al, 2004).
Multimodality More modalities allow for more natural
communication, which normally employs multiple chan-
nels of expression, suited to the content to be communi-
cated. For natural language processing per se this raises
new and interesting challenges, e. g., cross-modal refer-
ential expressions. It is also the case that more modal-
ities constrain interpretation and hence enhance robust-
ness. The ends-based representation allow for modality-
independent processing in the backbone.
Standards Standards ease scalability. For, e. g., ends-
based representations and tools, we have previously de-
veloped custom-built software providing short-lived so-
lutions. In other situations we have chosen standards and
standard tools. We claim that the latter is beneficial in at
least two ways: It opens up the door for scalability since
we can re-use our as well as other?s resources. Secondly
it is easier to maintain our solution over time and projects.
5 Conclusion
DFKI?s dialogue toolbox was used in a number of fully
functional, differently sized systems with a variety of
interaction paradigms. Vital to its success in terms of
reusability and scalability was the choice of a modular de-
sign and ends-based representations throughout the com-
plete backbone. Starting from basic functionalities, it is
possible to extend the system coverage while incorporat-
ing new features. Future work includes reusing (parts of)
the backbone in EU and nationally funded large projects
like AMI, TALK, Inscape, VirtualHuman and SmartWeb.
References
Jan Alexandersson and Tilman Becker. 2003. The Formal
Foundations Underlying Overlay. In Proceedings of the
Fifth International Workshop on Computational Semantics
(IWCS-5), Tilburg, The Netherlands, February.
Tilman Becker. 1998. Fully lexicalized head-driven syntac-
tic generation. In Proceedings of the Ninth International
Workshop on Natural Language Generation, Niagara-on-the-
Lake, Ontario, Canada, August.
Nate Blaylock, James Allen, and George Ferguson. 2003.
Managing communicative intentions with collaborative
problem solving. In Ronnie W. Smith and Jan van Kup-
pevelt, editors, Current and New Directions in Discourse and
Dialogue. Kluwer.
Els den Os and Lou Boves. 2003. Towards ambient intelli-
gence: Multimodal computers that understand our intentions.
In eChallenges e-2003, pages 22?24.
Ralf Engel. 2002. SPIN: Language understanding for spoken
dialogue systems using a production system approach. In
Proceedings of 7th International Conference on Spoken Lan-
guage Processing (ICSLP-2002), pages 2717?2720, Denver,
Colorado, USA.
Iryna Gurevych, Robert Porzel, Elena Slinko, Norbert Pfleger,
Jan Alexandersson, and Stefan Merten. 2003a. Less is more:
Using a single knowledge representation in dialogue sys-
tems. In Proceedings of the HLT-NAACL Workshop on Text
Meaning, pages 14?21, Edmonton, Canada, May.
Iryna Gurevych, Robert Porzel, Hans-Peter Zorn, and Rainer
Malaka. 2003b. Semantic coherence scoring using an on-
tology. In Proceedings of the Human Language Technology
Conference - HLT-NAACL 2003, Edmonton, CA, May, 27?
June, 1.
Gerd Herzog, Heinz Kirchmann, Stefan Merten, Alassane Ndi-
aye, Peter Poller, and Tilman Becker. 2004. Large-scale
software integration for spoken language and multimodal di-
alog systems. Journal of Natural Language Engineering.
To appear in the special issue on ?Software Architecture for
Language Engineering?.
Staffan Larsson. 2002. Issue-based Dialogue Management.
Ph.D. thesis, Go?teborg University.
Sharon Oviatt. 1999. Ten myths of multimodal interaction.
Communications of the ACM, 42(11):74?81.
Norbert Pfleger, Jan Alexandersson, and Tilman Becker. 2002.
Scoring functions for overlay and their application in dis-
course processing. In KONVENS-02, Saarbru?cken, Septem-
ber ? October.
Norbert Pfleger, Ralf Engel, and Jan Alexandersson. 2003. Ro-
bust multimodal discourse processing. In Kruijff-Korbayova
and Kosny, editors, Proceedings of Diabruck: 7th Workshop
on the Semantics and Pragmatics of Dialogue, Wallerfangen,
Germany, September.
Ehud Reiter. 1995. NLG vs. templates. In 5th European Work-
shop in Natural Language Generation, pages 95?105, Lei-
den, May.
Norbert Reithinger, Jan Alexandersson, Tilman Becker, Anselm
Blocher, Ralf Engel, Markus Lo?eckelt, Jochen Mu?eller, Nor-
bert Pfleger, Peter Poller, Michael Streit, and Valentin Tsch-
ernomas. 2003a. Smartkom - adaptive and flexible mul-
timodal access to multiple applications. In Proceedings of
ICMI 2003, Vancouver, B.C.
Norbert Reithinger, Dirk Fedeler, Ashwani Kumar, Christoph
Lauer, Elsa Pecourt, and Laurent Romary. 2003b. MIAMM
- A Multimodal Dialogue System Using Haptics. In Jan van
Kuppevelt, Laila Dybkjaer, and Niels Ole Bersen, editors,
Natural, Intelligent and Effective Interaction in Multimodal
Dialogue Systems. Kluwer Academic Publishers.
Wolfgang Wahlster. 2003. Smartkom: Symmetric multimodal-
ity in an adaptive and reusable dialogue shell. In R. Krahl
and D. Gnther, editors, Proceedings of the Human Computer
Interaction Status Conference 2003, pages 47?62, Berlin:
DLR, June.
An Experiment Setup for Collecting Data for Adaptive Output Planning
in a Multimodal Dialogue System
Ivana Kruijff-Korbayova?, Nate Blaylock,
Ciprian Gerstenberger, Verena Rieser
Saarland University, Saarbru?cken, Germany
korbay@coli.uni-sb.de
Tilman Becker, Michael Kai?er,
Peter Poller, Jan Schehl
DFKI, Saarbru?cken, Germany
tilman.becker@dfki.de
Abstract
We describe a Wizard-of-Oz experiment setup for
the collection of multimodal interaction data for a
Music Player application. This setup was devel-
oped and used to collect experimental data as part
of a project aimed at building a flexible multimodal
dialogue system which provides an interface to an
MP3 player, combining speech and screen input
and output. Besides the usual goal of WOZ data
collection to get realistic examples of the behav-
ior and expectations of the users, an equally im-
portant goal for us was to observe natural behavior
of multiple wizards in order to guide our system
development. The wizards? responses were there-
fore not constrained by a script. One of the chal-
lenges we had to address was to allow the wizards
to produce varied screen output a in real time. Our
setup includes a preliminary screen output planning
module, which prepares several versions of possi-
ble screen output. The wizards were free to speak,
and/or to select a screen output.
1 Introduction
In the larger context of the TALK project1 we are develop-
ing a multimodal dialogue system for a Music Player appli-
cation for in-car and in-home use, which should support nat-
ural, flexible interaction and collaborative behavior. The sys-
tem functionalities include playback control, manipulation of
playlists, and searching a large MP3 database. We believe
that in order to achieve this goal, the system needs to provide
advanced adaptive multimodal output.
We are conducting Wizard-of-Oz experiments
[Bernsen et al, 1998] in order to guide the development
of our system. On the one hand, the experiments should
give us data on how the potential users interact with such
an application. But we also need data on the multimodal
interaction strategies that the system should employ to
achieve the desired naturalness, flexibility and collaboration.
We therefore need a setup where the wizard has freedom of
1TALK (Talk and Look: Tools for Ambient Linguistic Knowl-
edge; www.talk-project.org) is funded by the EU as project
No. IST-507802 within the 6th Framework program.
choice w.r.t. their response and its realization through single
or multiple modalities. This makes it different from previous
multimodal experiments, e.g., in the SmartKom project
[Tu?rk, 2001], where the wizard(s) followed a strict script.
But what we need is also different in several aspects from
taking recordings of straight human-human interactions: the
wizard does not hear the user?s input directly, but only gets a
transcription, parts of which are sometimes randomly deleted
(in order to approximate imperfect speech recognition);
the user does not hear the wizard?s spoken output directly
either, as the latter is transcribed and re-synthesized (to
produce system-like sounding output). The interactions
should thus more realistically approximate an interaction
with a system, and thereby contain similar phenomena (cf.
[Duran et al, 2001]).
The wizard should be able to present different screen out-
puts in different context, depending on the search results and
other aspects. However, the wizard cannot design screens on
the fly, because that would take too long. Therefore, we de-
veloped a setup which includes modules that support the wiz-
ard by providing automatically calculated screen output op-
tions the wizard can select from if s/he want to present some
screen output.
Outline In this paper we describe our experiment setup and
the first experiences with it. In Section 2 we overview the
research goals that our setup was designed to address. The
actual setup is presented in detail in Section 3. In Section 4
we describe the collected data, and we summarize the lessons
we learnt on the basis of interviewing the experiment partici-
pants. We briefly discuss possible improvements of the setup
and our future plans with the data in Section 5.
2 Goals of the Multimodal Experiment
Our aim was to gather interactions where the wizard can com-
bine spoken and visual feedback, namely, displaying (com-
plete or partial) results of a database search, and the user can
speak or select on the screen.
Multimodal Presentation Strategies The main aim was to
identify strategies for the screen output, and for the multi-
modal output presentation. In particular, we want to learn
Figure 1: Multimodal Wizard-of-Oz data collection setup for
an in-car music player application, using the Lane Change
driving simulator. Top right: User, Top left: Wizard, Bottom:
transcribers.
when and what content is presented (i) verbally, (ii) graphi-
cally or (iii) by some combination of both modes. We expect
that when both modalities are used, they do not convey the
same content or use the same level of granularity. These are
important questions for multimodal fission and for turn plan-
ning in each modality.
We also plan to investigate how the presentation strategies
influence the responses of the user, in particular w.r.t. what
further criteria the user specifies, and how she conveys them.
Multimodal Clarification Strategies The experiments
should also serve to identify potential strategies for multi-
modal clarification behavior and investigate individual strat-
egy performance. The wizards? behavior will give us an ini-
tial model how to react when faced with several sources of
interpretation uncertainty. In particular we are interested in
what medium the wizard chooses for the clarification request,
what kind of grounding level he addresses, and what ?sever-
ity? he indicates. 2 In order to invoke clarification behavior
we introduced uncertainties on several levels, for example,
multiple matches in the database, lexical ambiguities (e.g., ti-
tles that can be interpreted denoting a song or an album), and
errors on the acoustic level. To simulate non-understanding
on the acoustic level we corrupted some of the user utterances
by randomly deleting parts of them.
3 Experiment Setup
We describe here some of the details of the experiment. The
experimental setup is shown schematically in Figure 1. There
are five people involved in each session of the experiment: an
experiment leader, two transcribers, a user and a wizard.
The wizards play the role of an MP3 player application
and are given access to a database of information (but not
actual music) of more than 150,000 music albums (almost 1
2Severity describes the number of hypotheses indicated by the
wizard: having no interpretation, an uncertain interpretation, or sev-
eral ambiguous interpretations.
Figure 2: Screenshot from the FreeDB-based database appli-
cation, as seen by the wizard. First-level of choice what to
display.
million songs), extracted from the FreeDB database.3 Fig-
ure 2 shows an example screen shot of the music database
as it is presented to the wizard. Subjects are given a set of
predefined tasks and are told to accomplish them by using
an MP3 player with a multimodal interface. Tasks include
playing songs/albums and building playlists, where the sub-
ject is given varying amounts of information to help them
find/decide on which song to play or add to the playlist. In
a part of the session the users also get a primary driving task,
using a Lane Change driving simulator [Mattes, 2003]. This
enabled us to test the viability of combining primary and sec-
ondary task in our experiment setup. We also aimed to gain
initial insight regarding the difference in interaction flow un-
der such conditions, particularly with regard to multimodal-
ity.
The wizards can speak freely and display the search result
or the playlist on the screen. The users can also speak as well
as make selections on the screen.
The user?s utterances are immediately transcribed by a typ-
ist and also recorded. The transcription is then presented to
the wizard.4 We did this for two reasons: (1) To deprive
the wizards of information encoded in the intonation of utter-
ances, because our system will not have access to it either. (2)
To be able to corrupt the user input in a controlled way, sim-
ulating understanding problems at the acoustic level. Unlike
[Stuttle et al, 2004], who simulate automatic speech recogni-
tion errors using phone-confusion models, we used a tool that
?deletes? parts of the transcribed utterances, replacing them
by three dots. Word deletion was triggered by the experiment
leader. The word deletion rate varied: 20% of the utterances
got weakly and 20% strongly corrupted. In 60% of the cases
the wizard saw the transcribed speech uncorrupted.
The wizard?s utterances are also transcribed (and recorded)
3Freely available at http://www.freedb.org
4We were not able to use a real speech recognition system, be-
cause we do not have one trained for this domain. This is one of the
purposes the collected data will be used for.
Figure 3: Screenshot from the display presentation tool offer-
ing options for screen output to the wizard for second-level
of choice what to display an how.
and presented to the user via a speech synthesizer. There are
two reasons for doing this: One is to maintain the illusion for
the subjects that they are actually interacting with a system,
since it is known that there are differences between human-
human and human-computer dialogue [Duran et al, 2001],
and we want to elicit behavior in the latter condition; the
other has to do with the fact that synthesized speech is imper-
fect and sometimes difficult to understand, and we wanted to
reproduce this condition.
The transcription is also supported by a typing and spelling
correction module to minimize speech synthesis errors and
thus help maintain the illusion of a working system.
Since it would be impossible for the wizard to construct
layouts for screen output on the fly, he gets support for his
task from the WOZ system: When the wizard performs a
database query, a graphical interface presents him a first level
of output alternatives, as shown in Figure 2. The choices are
found (i) albums, (ii) songs, or (iii) artists. For a second level
of choice, the system automatically computes four possible
screens, as shown in Figure 3. The wizard can chose one of
the offered options to display to the user, or decide to clear
the user?s screen. Otherwise, the user?s screen remains un-
changed. It is therefore up to the wizard to decide whether
to use speech only, display only, or to combine speech and
display.
The types of screen output are (i) a simple text-message
conveying how many results were found, (ii) output of a list
of just the names (of albums, songs or artists) with the cor-
responding number of matches (for songs) or length (for al-
bums), (iii) a table of the complete search results, and (iv) a
table of the complete search results, but only displaying a sub-
set of columns. For each screen output type, the system uses
heuristics based on the search to decide, e.g., which columns
should be displayed. These four screens are presented to the
wizard in different quadrants on a monitor (cf. Figure 3),
allowing for selection with a simple mouse click. The heuris-
tics for the decision what to display implement preliminary
strategies we designed for our system. We are aware that due
to the use of these heuristics, the wizard?s output realization
may not be always ideal. We have collected feedback from
both the wizards and the users in order to evaluate whether
the output options were satisfactory (cf. Section 4 for more
details).
Technical Setup To keep our experimental system modu-
lar and flexible we implemented it on the basis of the Open
Agent Architecture (OAA) [Martin et al, 1999], which is a
framework for integrating a community of software agents in
a distributed environment. Each system module is encapsu-
lated by an OAA wrapper to form an OAA agent, which is
able to communicate with the OAA community. The exper-
imental system consists of 12 agents, all of them written in
Java. We made use of an OAA monitor agent which comes
with the current OAA distribution to trace all communication
events within the system for logging purposes.
The setup ran distributed over six PCs running different
versions of Windows and Linux.5
4 Collected Data and Experience
The SAMMIE-26 corpus collected in this experiment contains
data from 24 different subjects, who each participated in one
session with one of our six wizards. Each subject worked on
four tasks, first two without driving and then two with driving.
The duration was restricted to twice 15 minutes. Tasks were
of two types: searching for a title either in the database or in
an existing playlist, building a playlist satisfying a number of
constraints. Each of the two sets for each subject contained
one task of each type. The tasks again differed in how specific
information was provided. We aimed to keep the difficulty
level constant across users. The interactions were carried out
in German.7
The data for each session consists of a video and audio
recording and a logfile. Besides the transcriptions of the spo-
ken utterances, a number of other features have been anno-
tated automatically in the log files of the experiment, e.g.,
the wizard?s database query and the number of found results,
the type and form of the presentation screen chosen by the
wizard, etc. The gathered logging information for a single
experiment session consists of the communication events in
chronological order, each marked by a timestamp. Based on
this information, we can recapitulate the number of turns and
the specific times that were necessary to accomplish a user
task. We expect to use this data to analyze correlations be-
5We would like to thank our colleagues from CLT Sprachtech-
nologie http://www.clt-st.de/ for helping us to set up the
laboratory.
6SAMMIE stands for Saarbru?cken Multimodal MP3 Player In-
teraction Experiment. We have so far conducted two series of data-
collection experiments: SAMMIE-1 involved only spoken interaction
(cf. [Kruijff-Korbayova? et al, 2005] for more details), SAMMIE-2 is
the multimodal experiment described in this paper.
7However, most of the titles and artist names in the music
database are in English.
tween queries, numbers of results, and spoken and graphical
presentation strategies.
Whenever the wizard made a clarification request, the
experiment leader invoked a questionnaire window on the
screen, where the wizard had to classify his clarification re-
quest according to the primary source of the understanding
problem. At the end of each task, users were asked to what
extent they believed they accomplished their tasks and how
satisfied they were with the results. Similar to methods used
by [Skantze, 2003] and [Williams and Young, 2004], we plan
to include subjective measures of task completion and cor-
rectness of results in our evaluation matrix, as task descrip-
tions can be interpreted differently by different users.
Each subject was interviewed immediately after the ses-
sion. The wizards were interviewed once the whole experi-
ment was over. The interviews were carried out verbally, fol-
lowing a prepared list of questions. We present below some
of the points gathered through these interviews.
Wizard Interviews All 6 wizards rated the overall under-
standing as good, i.e., that communication completed suc-
cessfully. However, they reported difficulties due to delays in
utterance transmission in both directions, which caused un-
necessary repetitions due to unintended turn overlap.
There were differences in how different wizards rated and
used the different screen output options: The table containing
most of the information about the queried song(s) or album(s)
was rated best and shown most often by some wizards, while
others thought it contained too much information and would
not be clear at first glance for the users and hence they used
it less or never. The screen option containing the least infor-
mation in tabular form, namely only a list of songs/albums
with their length, received complementary judgments: some
of the wizards found it useless because it contained too little
information, and they thus did not use it, and others found it
very useful because it would not confuse the user by present-
ing too much information, and they thus used it frequently.
Finally, the screen containing a text message conveying only
the number of matches, if any, has been hardly used by the
wizards. The differences in the wizards? opinions about what
the users would find useful or not clearly indicate the need
for evaluation of the usefulness of the different screen output
options in particular contexts from the users? view point.
When showing screen output, the most common pattern
used by the wizards was to tell the user what was shown (e.g.,
I?ll show you the songs by Prince), and to display the screen.
Some wizards adapted to the user?s requests: if asked to show
something (e.g., Show me the songs by Prince), they would
show it without verbal comments; but if asked a question
(e.g., What songs by Prince are there? or What did you find?),
they would show the screen output and answer in speech.
Concerning the adaptation of multimodal presentation
strategies w.r.t. whether the user was driving or not, four
of the six wizards reported that they consciously used speech
instead of screen output if possible when the user was driving.
The remaining two wizards did not adapt their strategy.
On the whole, interviewing the wizards brought valuable
information on presentation strategies and the use of modal-
ities, but we expect to gain even more insight after the an-
notation and evaluation of the collected data. Besides ob-
servations about the interaction with the users, the wizards
also gave us various suggestions concerning the software used
in the experiment, e.g., the database interface (e.g., the pos-
sibility to decide between strict search and search for par-
tial matches, and fuzzy search looking for items with similar
spelling when no hits are found), the screen options presenter
(e.g., ordering of columns w.r.t. their order in the database in-
terface, the possibility to highlight some of the listed items),
and the speech synthesis system.
Subject Interviews In order to use the wizards? behavior as
a model for interaction design, we need to evaluate the wiz-
ards? strategies. We used user satisfaction, task experience,
and multi-modal feedback behavior as evaluation metrics.
The 24 experimental subjects were all native speakers of
German with good English skills. They were all students
(equally spread across subject areas), half of them male and
half female, and most of them were between 20 to 30 years
old.
In order to calculate user satisfaction, users were inter-
viewed to evaluate the system?s performance with a user sat-
isfaction survey. The survey probed different aspects of the
users? perception of their interaction with the system. We
asked the users to evaluate a set of five core metrics on a
5-point Likert scale. We followed [Walker et al, 2002] def-
inition of the overall user satisfaction as the sum of text-to-
speech synthesis performance, task ease, user expertise, over-
all difficulty and future use. The mean for user satisfaction
across all dialogues was 15.0 (with a standard derivation of
2.9). 8 A one-way ANOVA for user satisfaction between wiz-
ards (df=5, F=1.52 p=0.05) shows no significant difference
across wizards, meaning that the system performance was
judged to be about equally good for all wizards.
To measure task experience we elicited data on perceived
task success and satisfaction on a 5-point Likert scale after
each task was completed. For all the subjects the final per-
ceived task success was 4.4 and task satisfaction 3.9 across
the 4 tasks each subject had to complete. For task success
as well as for task satisfaction no significant variance across
wizards was detected.
Furthermore the subjects were asked about the employed
multi-modal presentation and clarification strategies.
The clarification strategies employed by the wizards
seemed to be successful: From the subjects? point of view,
mutual understanding was very good and the few misunder-
standings could be easily resolved. Nevertheless, in the case
of disambiguation requests and when grounding an utterance,
subjects ask for more display feedback. It is interesting to
note that subjects judged understanding difficulties on higher
levels of interpretation (especially reference resolution prob-
lems and problems with interpreting the intention) to be more
costly than problems on lower levels of understanding (like
the acoustic understanding). For the clarification strategy this
8[Walker et al, 2002] reported an average user satisfaction of
16.2 for 9 Communicator systems.
implies that the system should engage in clarification at the
lowest level a error was detected.9
Multi-modal presentation strategies were perceived to be
helpful in general, having a mean of 3.1 on a 5-point Lik-
ert scale. However, the subjects reported that too much in-
formation was being displayed especially for the tasks with
driving. 85.7% of the subjects reported that the screen out-
put was sometimes distracting them. 76.2% of the sub-
jects would prefer to more verbal feedback, especially while
driving. On a 3-point Likert scale subjects evaluated the
amount of the information presented verbally to be about
right (mean of 1.8), whereas they found the information pre-
sented on the screen to be too much (mean of 2.3). Stud-
ies by [Bernsen and Dybkjaer, 2001] on the appropriateness
of using verbal vs. graphical feedback for in-car dialogues
indicate that the need for text output is very limited. Some
subjects in that study, as well subjects in our study report that
they would prefer to not have to use the display at all while
driving. On the other hand subjects in our study perceived the
screen output to be very helpful in less stressful driving situa-
tions and when not driving (e.g. for memory assistance, clari-
fications etc.). Especially when they want to verify whether a
complex task was finally completed (e.g. building a playlist),
they ask for a displayed proof. For modality selection in in-
car dialogues the driver?s mental workload on primary and
secondary task has to be carefully evaluated with respect to a
situation model.
With respect to multi-modality subjects also asked for
more personalized data presentation. We therefore need to
develop intelligent ways to reduce the amount of data being
displayed. This could build on prior work on the generation
of ?tailored? responses in spoken dialogue according to a user
model [Moore et al, 2004].
The results for multi-modal feedback behavior showed no
significant variations across wizards except for the general
helpfulness of multi-modal strategies. An ANOVA Planned
Comparison of the wizard with the lowest mean against the
other wizards showed that his behavior was significantly
worse. It is interesting to note, that this wizard was using
the display less than the others. We might consider not to in-
clude the 4 sessions with this wizard in our output generation
model.
We also tried to analyze in more detail how the wizards?
presentation strategies influenced the results. The option
which was chosen most of the time was to present a table
with the search results (78.6%); to present a list was only cho-
sen in 17.5% of the cases and text only 0.04%. The wizards?
choices varied significantly only for presenting the table op-
tion. The wizard who was rated lowest for multimodality was
using the table option less, indicating that this option should
be used more often. This is also supported by the fact that the
show table option is the only presentation strategy which is
positively correlated to how the user evaluated multimodality
(Spearman?s r = 0.436*). We also could find a 2-tailed corre-
9Note that engaging at the lowest level just helps to save dialogue
?costs?. Other studies have shown that user satisfaction is higher
for strategies that would ?hide? the understanding error by asking
questions on higher levels [Skantze, 2003], [Raux et al, 2005]
lation between user satisfaction and multimodality judgment
(Spearman?s r = 0.658**). This indicates the importance of
good multimodal presentation strategies for user satisfaction.
Finally, the subjects were asked for own comments. They
liked to be able to provide vague information, e.g., ask for ?an
oldie?, and were expecting collaborative suggestions. They
also appreciated collaborative proposals based on inferences
made from previous conversations.
In sum, as the measures for user satisfaction, task experi-
ence, and multi-modal feedback strategies, the subjects? judg-
ments show a positive trend. The dialogue strategies em-
ployed by most of the wizards seem to be a good starting
point for building a baseline system. Furthermore, the results
indicate that intelligent multi-modal generation needs to be
adaptive to user and situation models.
5 Conclusions and Future Steps
We have presented an experiment setup that enables us to
gather multimodal interaction data aimed at studying not only
the behavior of the users of the simulated system, but also
that of the wizards. In order to simulate a dialogue system in-
teraction, the wizards were only shown transcriptions of the
user utterances, sometimes corrupted, to simulate automatic
speech recognition problems. The wizard?s utterances were
also transcribed and presented to the user through a speech
synthesizer. In order to make it possible for the wizards to
produce contextually varied screen output in real time, we
have included a screen output planning module which auto-
matically calculated several screen output versions every time
the wizard ran a database query. The wizards were free to
speak and/or display screen output. The users were free to
speak or select on the screen. In a part of each session, the
user was occupied by a primary driving task.
The main challenge for an experiment setup as described
here is the considerable delay between user input and wizard
response. This is due partly to the transcription and spelling
correction step and partly due to the time it takes the wizard to
decide on and enter a query to the database, then select a pre-
sentation and in parallel speak to the user. We have yet to ana-
lyze the exact distribution of time needed for these tasks. Sev-
eral ways can be chosen to speed up the process. Transcrip-
tion can be eliminated either by using speech recognition and
dealing with its errors, or instead applying signal processing
software, e.g., to filter out prosodic information from the user
utterance and/or to transform the wizard?s utterance into syn-
thetically sounding speech (e.g., using a vocoder). Database
search can be sped up in a number of ways too, ranging from
allowing selection directly from the transcribed text to auto-
matically preparing default searches by analyzing the user?s
utterance. Note, however, that the latter will most likely prej-
udice the wizard to stick to the proposed search.
We plan to annotate the corpus, most importantly w.r.t.
wizard presentation strategies and context features relevant
for the choice between them. We also plan to compare the
presentation strategies to the strategies in speech-only mode,
for which we collected data in an earlier experiment (cf.
[Kruijff-Korbayova? et al, 2005]).
For clarification strategies previous studies already showed
that the decision process needs to be highly dynamic by tak-
ing into account various features such as interpretation uncer-
tainties and local utility [Paek and Horvitz, 2000]. We plan
to use the wizard data to learn an initial multi-modal clarifi-
cation policy and later on apply reinforcement learning meth-
ods to the problem in order to account for long-term dialogue
goals, such as task success and user satisfaction.
The screen output options used in the experiment will also
be employed in the baseline system we are currently imple-
menting. The challenges involved there are to decide (i) when
to produce screen output, (ii) what (and how) to display and
(iii) what the corresponding speech output should be. We will
analyze the corpus in order to determine what the suitable
strategies are.
References
[Bernsen and Dybkjaer, 2001] Niels Ole Bernsen and Laila
Dybkjaer. Exploring natural interaction in the car. In
CLASS Workshop on Natural Interactivity and Intelligent
Interactive Information Representation, 2001.
[Bernsen et al, 1998] N. O. Bernsen, H. Dybkj?r, and
L. Dybkj?r. Designing Interactive Speech Systems ?
From First Ideas to User Testing. Springer, 1998.
[Duran et al, 2001] Christine Duran, John Aberdeen, Laurie
Damianos, and Lynette Hirschman. Comparing several as-
pects of human-computer and human-human dialogues. In
Proceedings of the 2nd SIGDIAL Workshop on Discourse
and Dialogue, Aalborg, 1-2 September 2001, pages 48?57,
2001.
[Kruijff-Korbayova? et al, 2005] Ivana Kruijff-Korbayova?,
Tilman Becker, Nate Blaylock, Ciprian Gerstenberger,
Michael Kai?er, Peter Poler, Jan Schehl, and Verena
Rieser. Presentation strategies for flexible multimodal
interaction with a music player. In Proceedings of
DIALOR?05 (The 9th workshop on the semantics and
pragmatics of dialogue (SEMDIAL), 2005.
[Martin et al, 1999] D. L. Martin, A. J. Cheyer, and D. B.
Moran. The open agent architecture: A framework for
building distributed software systems. Applied Artificial
Intelligence: An International Journal, 13(1?2):91?128,
Jan?Mar 1999.
[Mattes, 2003] Stefan Mattes. The lane-change-task as a tool
for driver distraction evaluation. In Proceedings of IGfA,
2003.
[Moore et al, 2004] Johanna D. Moore, Mary Ellen Foster,
Oliver Lemon, and Michael White. Generating tailored,
comparative descriptions in spoken dialogue. In Proceed-
ings of the Seventeenth International Florida Artificial In-
telligence Research Sociey Conference, AAAI Press, 2004.
[Paek and Horvitz, 2000] Tim Paek and Eric Horvitz. Con-
versation as action under uncertainty. In Proceedings of
the Sixteenth Conference on Uncertainty in Artificial In-
telligence, 2000.
[Raux et al, 2005] Antoine Raux, Brian Langner, Dan Bo-
hus, Allan W. Black, and Maxine Eskenazi. Let?s go pub-
lic! taking a spoken dialog system to the real world. 2005.
[Skantze, 2003] Gabriel Skantze. Exploring human error
handling strategies: Implications for spoken dialogue sys-
tems. In Proceedings of the ISCA Tutorial and Research
Workshop on Error Handling in Spoken Dialogue Systems,
2003.
[Stuttle et al, 2004] Matthew Stuttle, Jason Williams, and
Steve Young. A framework for dialogue data collection
with a simulated asr channel. In Proceedings of the IC-
SLP, 2004.
[Tu?rk, 2001] Ulrich Tu?rk. The technical processing in
smartkom data collection: a case study. In Proceedings
of Eurospeech2001, Aalborg, Denmark, 2001.
[Walker et al, 2002] Marylin Walker, R. Passonneau, J. Ab-
erdeen, J. Boland, E. Bratt, J. Garofolo, L. Hirschman,
A. Le, S. Lee, S. Narayanan, K. Papineni, B. Pellom,
J. Polifroni, A. Potamianos, P. Prabhu, A. Rudnicky,
G. Sandersa, S. Seneff, D. Stallard, and S. Whittaker.
Cross-site evaluation in darpa communicator: The june
2000 data collection. 2002.
[Williams and Young, 2004] Jason D. Williams and Steve
Young. Characterizing task-oriented dialog using a sim-
ulated asr channel. In Proceedings of the ICSLP, 2004.
