Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1732?1740,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Orthonormal Explicit Topic Analysis for Cross-lingual Document Matching
John Philip McCrae
University Bielefeld
Inspiration 1
Bielefeld, Germany
Philipp Cimiano
University Bielefeld
Inspiration 1
Bielefeld, Germany
{jmccrae,cimiano,rklinger}@cit-ec.uni-bielefeld.de
Roman Klinger
University Bielefeld
Inspiration 1
Bielefeld, Germany
Abstract
Cross-lingual topic modelling has applications
in machine translation, word sense disam-
biguation and terminology alignment. Multi-
lingual extensions of approaches based on la-
tent (LSI), generative (LDA, PLSI) as well as
explicit (ESA) topic modelling can induce an
interlingual topic space allowing documents
in different languages to be mapped into the
same space and thus to be compared across
languages. In this paper, we present a novel
approach that combines latent and explicit
topic modelling approaches in the sense that
it builds on a set of explicitly defined top-
ics, but then computes latent relations between
these. Thus, the method combines the ben-
efits of both explicit and latent topic mod-
elling approaches. We show that on a cross-
lingual mate retrieval task, our model signif-
icantly outperforms LDA, LSI, and ESA, as
well as a baseline that translates every word in
a document into the target language.
1 Introduction
Cross-lingual document matching is the task of,
given a query document in some source language,
estimating the similarity to a document in some tar-
get language. This task has important applications in
machine translation (Palmer et al, 1998; Tam et al,
2007), word sense disambiguation (Li et al, 2010)
and ontology alignment (Spiliopoulos et al, 2007).
An approach that has become quite popular in re-
cent years for cross-lingual document matching is
Explicit Semantics Analysis (ESA, Gabrilovich and
Markovitch (2007)) and its cross-lingual extension
CL-ESA (Sorg and Cimiano, 2008). ESA indexes
documents by mapping them into a topic space de-
fined by their similarity to predefined explicit top-
ics ? generally articles from an encyclopaedia ? in
such a way that there is a one-to-one correspondence
between topics and encyclopedic entries. CL-ESA
extends this to the multilingual case by exploiting
a background document collection that is aligned
across languages, such as Wikipedia. A feature of
ESA and its extension CL-ESA is that, in contrast to
latent (e.g. LSI, Deerwester et al (1990)) or genera-
tive topic models (such as LDA, Blei et al (2003)),
it requires no training and, nevertheless, has been
demonstrated to outperform LSI and LDA on cross-
lingual retrieval tasks (Cimiano et al, 2009).
A key choice in Explicit Semantic Analysis is the
document space that will act as the topic space. The
standard choice is to regard all articles from a back-
ground document collection ? Wikipedia articles are
a typical choice ? as the topic space. However, it
is crucial to ensure that these topics cover the se-
mantic space evenly and completely. In this pa-
per, we present an alternative approach where we
remap the semantic space defined by the topics in
such a manner that it is orthonormal. In this way,
each document is mapped to a topic that is distinct
from all other topics. Such a mapping can be con-
sidered as equivalent to a variant of Latent Seman-
tic Indexing (LSI) with the main difference that our
model exploits the matrix that maps topic vectors
back into document space, which is normally dis-
carded in LSI-based approaches. We dub our model
ONETA (OrthoNormal Explicit Topic Analysis) and
empirically show that on a cross-lingual retrieval
1732
task it outperforms ESA, LSI, and Latent Dirichlet
Allocation (LDA) as well as a baseline consisting of
translating each word into the target language, thus
reducing the task to a standard monolingual match-
ing task. In particular, we quantify the effect of dif-
ferent approximation techniques for computing the
orthonormal basis and investigate the effect of vari-
ous methods for the normalization of frequency vec-
tors.
The structure of the paper is as follows: we situate
our work in the general context of related work on
topic models for cross-lingual document matching
in Section 2. We present our model in Section 3 and
present our experimental results and discuss these
results in Section 4.
2 Related Work
The idea of applying topic models that map docu-
ments into an interlingual topic space seems a quite
natural and principled approach to tackle several
tasks including the cross-lingual document retrieval
problem.
Topic modelling is the process of finding a rep-
resentation of a document d in a lower dimensional
space RK where each dimension corresponds to one
topic that abstracts from specific words and thus al-
lows us to detect deeper semantic similarities be-
tween documents beyond the computation of the
pure overlap in terms of words.
Three main variants of document models have
been mainly considered for cross-lingual document
matching:
Latent methods such as Latent Semantic Indexing
(LSI, Deerwester et al (1990)) induce a de-
composition of the term-document matrix in
a way that reduces the dimensionality of the
documents, while minimizing the error in re-
constructing the training data. For example,
in Latent Semantic Indexing, a term-document
matrix is approximated by a partial singu-
lar value decomposition, or in Non-Negative
Matrix Factorization (NMF, Lee and Seung
(1999)) by two smaller non-negative matrices.
If we append comparable or equivalent doc-
uments in multiple languages together before
computing the decomposition as proposed by
Dumais et al (1997) then the topic model is
essentially cross-lingual allowing to compare
documents in different languages once they
have been mapped into the topic space.
Probabilistic or generative methods instead at-
tempt to induce a (topic) model that has the
highest likelihood of generating the documents
actually observed during training. As with la-
tent methods, these topics are thus interlin-
gual and can generate words/terms in differ-
ent languages. Prominent representatives of
this type of method are Probabilistic Latent Se-
mantic Indexing (PLSI, Hofmann (1999)) or
Latent Dirichlet Allocation (LDA, Blei et al
(2003)), both of which can be straightforwardly
extended to the cross-lingual case (Mimno et
al., 2009).
Explicit topic models make the assumption that
topics are explicitly given instead of being in-
duced from training data. Typically, a back-
ground document collection is assumed to be
given whereby each document in this corpus
corresponds to one topic. A mapping from doc-
ument to topic space is calculated by comput-
ing the similarity of the document to every doc-
ument in the topic space. A prominent exam-
ple for this kind of topic modelling approach is
Explicit Semantic Analysis (ESA, Gabrilovich
and Markovitch (2007)).
Both latent and generative topic models attempt to
find topics from the data and it has been found that
in some cases they are equivalent (Ding et al, 2006).
However, this approach suffers from the problem
that the topics might be artifacts of the training data
rather than coherent semantic topics. In contrast, ex-
plicit topic methods can use a set of topics that are
chosen to be well-suited to the domain. The princi-
ple drawback of this is that the method for choosing
such explicit topics by selecting documents is com-
paratively crude. In general, these topics may be
overlapping and poorly distributed over the seman-
tic topic space. By comparison, our method takes the
advantage of the pre-specified topics of explicit topic
models, but incorporates a training step to learn la-
tent relations between these topics.
1733
3 Orthonormal explicit topic analysis
Our approach follows Explicit Semantic Analysis in
the sense that it assumes the availability of a back-
ground document collection B = {b1, b2, ..., bN}
consisting of textual representations. The map-
ping into the explicit topic space is defined by a
language-specific function ? that maps documents
into RN such that the jth value in the vector is given
by some association measure ?j(d) for each back-
ground document bj . Typical choices for this associ-
ation measure ? are the sum of the TF-IDF scores or
an information retrieval relevance scoring function
such as BM-25 (Sorg and Cimiano, 2010).
For the case of TF-IDF, the value of the j-th ele-
ment of the topic vector is given by:
?j(d) =
????
tf-idf(bj)
T ????tf-idf(d)
Thus, the mapping function can be represented as
the product of a TF-IDF vector of document dmulti-
plied by anW?N matrix, X, each element of which
contains the TF-IDF value of word i in document bj :
?(d) =
?
?
?
?
????
tf-idf(b1)T
...
????
tf-idf(bN )T
?
?
?
?
????
tf-idf(d) = XT ?
????
tf-idf(d)
For simplicity, we shall assume from this point on
that all vectors are already converted to a TF-IDF
or similar numeric vector form. In order to com-
pute the similarity between two documents di and
dj , typically the cosine-function (or the normalized
dot product) between the vectors ?(di) and ?(dj) is
computed as follows:
sim(di, dj) = cos(?(di),?(dj)) =
?(di)T?(dj)
||?(di)||||?(dj)||
If we represent the above using our above defined
W ?N matrix X then we get:
sim(di, dj) = cos(X
Tdi,X
Tdj) =
dTi XX
Tdj
||XTdi||||XTdj ||
The key challenge with ESA is choosing a good
background document collection B = {b1, ..., bN}.
A simple minimal criterion for a good background
document collection is that each document in this
collection should be maximally similar to itself and
less similar to any other document:
?i 6= j 1 = sim(bj , bj) > sim(bi, bj) ? 0
While this criterion is trivially satisfied if we have
no duplicate documents in our collection, our intu-
ition is that we should choose a background collec-
tion that maximizes the slack margin of this inequal-
ity, i.e. |sim(bj , bj) ? sim(bi, bj)|. We can see that
maximizing this margin for all i,j is the same as
minimizing the semantic overlap of the background
documents, which is given as follows:
overlap(B) =
?
i = 1, . . . , N
j = 1, . . . , N
i 6= j
sim(bi, bj)
We first note that we can, without loss of general-
ity, normalize our background documents such that
||Xbj || = 1 for all j, and in this case we can re-
define the semantic overlap as the following matrix
expression1
overlap(X) = ||XTXXTX? I||1
It is trivial to verify that this equation has a mini-
mum when XTXXTX = I. This is the case when
the topics are orthonormal:
(XTbi)T(XTbj) = 0 if i 6= j
(XTbi)T(XTbi) = 1
Unfortunately, this is not typically the case as the
documents have significant word overlap as well as
semantic overlap. Our goal is thus to apply a suitable
transformation to X with the goal of ensuring that
the orthogonality property holds.
Assuming that this transformation of X is done
by multiplication with some other matrix A, we can
define the learning problem as finding that matrix A
such that:
(AXTX)T(AXTX) = I
1||A||p =
?
i,j |aij |
p is the p-norm. ||A||F =
?
||A||2 is
the Frobenius norm.
1734
If we have the case that W ? N and that the rank
of X is N , then XTX is invertible and thus A =
(XTX)?1 is the solution to this problem.2
We define the projection function of a document
d, represented as a normalized term frequency vec-
tor, as follows:
?ONETA(d) = (X
TX)?1XTd
For the cross-lingual case we assume that we have
two sets of background documents of equal size,
B1 = {b11, . . . , b
1
N}, B
2 = {b21, . . . , b
2
N} in lan-
guages l1 and l2, respectively and that these doc-
uments are aligned such that for every index i, b1i
and b2i are documents on the same topic in each
language. Using this we can construct a projec-
tion function for each language which maps into the
same topic space. Thus, as in CL-ESA, we obtain
the cross-lingual similarity between a document di
in language l1 and a document dj in language l2 as
follows:
sim(di, dj) = cos(?
l1
ONETA(di),?
l2
ONETA(dj))
We note here that we assume that ? could be rep-
resented as a symmetric inner product of two vec-
tors. However, for many common choices of asso-
ciation measures, including BM25, this is not the
case. In this case the expression XTX can be re-
placed with a kernel matrix specifying the associ-
ation of each background document to each other
background document.
3.1 Relationship to Latent Semantic Indexing
In this section we briefly clarify the relationship be-
tween our method ONETA and Latent Semantic In-
dexing. Latent Semantic Indexing defines a map-
ping from a document represented as a term fre-
quency vector to a vector in RK . This transforma-
tion is defined by means of calculating the singu-
lar value decomposition (SVD) of the matrix X as
above, namely
2In the case that the matrix is not invertible we can in-
stead solve ||XTXA ? I||F , which has a minimum at A =
V??1UT where XTX = U?VT is the singular value de-
composition of XTX.
As usual we do not in fact compute the inverse for our exper-
iments, but instead the LU Decomposition and solve by Gaus-
sian elimination at test time.
X = U?VT
Where ? is diagonal and U V are the eigenvec-
tors of XXT and XTX., respectively. Let ?K de-
note the K ? K submatrix containing the largest
eigenvalues, and UK ,VK denote the corresponding
eigenvectors. Thus LSI can be defined as:
?LSI(d) = ?
?1
K UKd
With regards to orthonormalized topics, we see
that using the SVD, we can simply derive the fol-
lowing:
(XTX)?1XT = V??1UT
When we set K = N and thus choose the maxi-
mum number of topics, ONETA is equivalent to LSI
modulo the fact that it multiplies the resulting topic
vector by V, thus projecting back into document
space, i.e. into explicit topics.
In practice, both methods differ significantly in
that the approximations they make are quite differ-
ent. Furthermore, in the case that W  N and X
has n non-zeroes, the calculation of the SVD is of
complexity O(nN + WN2) and requires O(WN)
bytes of memory. In contrast, ONETA requires com-
putation time ofO(Na) for a > 2, which is the com-
plexity of the matrix inversion algorithm3, and only
O(n+N2) bytes of memory.
3.2 Approximations
The computation of the inverse has a complexity
that, using current practical algorithms, is approxi-
mately cubic and as such the time spent calculating
the inverse can grow very quickly. There are sev-
eral methods for obtaining an approximate inverse.
The most commonly used are based on the SVD or
eigendecomposition of the matrix. As XTX is sym-
metric positive definite, it holds that:
XTX = U?UT
Where U are the eigenvectors of XTX and ? is
a diagonal matrix of the eigenvalues. With UK ,?K
3Algorithms with a = 2.3727 are known but practical algo-
rithms have a = 2.807 or a = 3 (Coppersmith and Winograd,
1990)
1735
as the first K eigenvalues and eigenvectors, respec-
tively, we have:
(XTX)?1 ' UK?
?1
K U
T
K (1)
We call this the orthonormal eigenapproxima-
tion or ON-Eigen. The complexity of calculating
(XTX)?1XT from this is O(N2K + Nn), where
n is the number of non-zeros in X.
Similarly, using the formula derived in the previ-
ous section we can derive an approximation of the
full model as follows:
(XTX)?1XT ' UK?
?1
K V
T
K (2)
We call this approximation Explicit LSI as it first
maps into the latent topic space and then into the
explicit topic space.
We can consider another approximation by notic-
ing that X is typically very sparse and moreover
some rows of X have significantly fewer non-zeroes
than others (these rows are for terms with low fre-
quency). Thus, if we take the first N1 columns (doc-
uments) in X, it is possible to rearrange the rows
of X with the result that there is some W1 such
that rows with index greater than W1 have only ze-
roes in the columns up to N1. In other words, we
take a subset of N1 documents and enumerate the
words in such a way that the terms occurring in the
first N1 documents are enumerated 1, . . . ,W1. Let
N2 = N ? N1, W2 = W ?W1. The result of this
row permutation does not affect the value of XTX
and we can write the matrix X as:
X =
(
A B
0 C
)
where A is a W1 ? N1 matrix representing term
frequencies in the first N1 documents, B is a W1 ?
N2 matrix containing term frequencies in the re-
maining documents for terms that are also found in
the firstN1 documents, and C is aW2?N2 contain-
ing the frequency of all terms not found in the first
N1 documents.
Application of the well-known divide-and-
conquer formula (Bernstein, 2005, p. 159) for
matrix inversion yields the following easily verifi-
able matrix identity, given that we can find C? such
that C?C = I.
(
(ATA)?1AT ?(ATA)?1ATBC?
0 C?
)(
A B
0 C
)
= I
(3)
We denote the above equation using a matrix L
as LTX = I. We note that L 6= (XTX)?1X,
but for any document vector d that is representable
as a linear combination of the background doc-
ument set (i.e., columns of X) we have that
Ld = (XTX)?1XTd and in this sense L '
(XTX)?1XT.
We further relax the assumption so that we only
need to find a C? such that C?C ' I. For this,
we first observe that C is very sparse as it contains
only terms not contained in the first N1 documents
and we notice that very sparse matrices tend to be
approximately orthogonal, hence suggesting that it
should be very easy to find a left-inverse of C. The
following lemma formalizes this intuition:
Lemma: If C is a W ? N matrix with M non-
zeros, distributed randomly and uniformly across the
matrix, and all the non-zeros are 1, then DCTC has
an expected value on each non-diagonal value of MN2
and a diagonal value of 1 if D is the diagonal matrix
whose values are given by ||ci||?2, the square of the
norm of the corresponding column of C.
Proof: We simply observe that if D? = DCTC,
then the (i, j)th element of D? is given by
dij =
cTi cj
||ci||2
If i 6= j then the cTi cj is the number of non-zeroes
overlapping in the ith and jth column of C and under
a uniform distribution we expect this to be M
2
N3 . Sim-
ilarly, we expect the column norm to be MN such that
the overall expectation is MN2 . The diagonal value is
clearly equal to 1.
As long as C is very sparse, we can use the fol-
lowing approximation, which can be calculated in
O(M) operations, where M is the number of non-
zeroes.
C? '
?
?
?
||c1||?2 0
. . .
0 ||cN2 ||
?2
?
?
?CT
We call this method L-Solve. The complexity
of calculating a left-inverse by this method is of
1736
Document
Normalization
Frequency Normalization No Yes
TF 0.31 0.78
Relative 0.23 0.42
TFIDF 0.21 0.63
SQRT 0.28 0.66
Table 1: Effect of Term Frequency and Document Nor-
malization on Top-1 Precision
order O(Na1 ), being much more efficient than the
eigenvalue methods. However, it is potentially more
error-prone as it requires that a left-inverse of C ex-
ists. On real data this might be violated if we do not
have linear independence of the rows of C, for ex-
ample if W2 < N2 or if we have even one document
which has only words that are also contained in the
first N1 documents and hence there is a row in C
that consists of zeros only. This can be solved by
removing documents from the collection until C is
row-wise linear independent.4
3.3 Normalization
A key factor in the effectiveness of topic-based
methods is the appropriate normalization of the el-
ements of the document matrix X. This is even
more relevant for orthonormal topics as the matrix
inversion procedure can be very sensitive to small
changes in the matrix. In this context, we con-
sider two forms of normalization, term and docu-
ment normalization, which can also be considered
as row/column normalizations of X.
A straightforward approach to normalization is to
normalize each column of X to obtain a matrix as
follows:
X? =
(
x1
||x1||
. . .
xN
||xN ||
)
If we calculate X?TX? = Y then we get that the
(i, j)-th element of Y is:
yij =
xTi xj
||xi||||xj ||
4In the experiments in the next section we discarded 4.2% of
documents at N1 = 1000 and 47% of documents at N1 = 5000
l
l
l
l
l
l
l
l l l
l l
l l l l
l
l
l l
100 200 300 400 500
0.0
0.2
0.4
0.6
0.8
Approximation rate
Pre
cisio
n
l l
l
l
l
l l
l
l l l
l l
l l
l
l
l
?
ON?EigenL?SolveExplicit LSILSIESA
Figure 1: Effect on Top-1 Precision by various approxi-
mation method
Thus, the diagonal of Y consists of ones only and
due to the Cauchy-Schwarz inequality we have that
|yij | ? 1, with the result that the matrix Y is al-
ready close to I. Formally, we can use this to state
a bound on ||X?TX? ? I||F , but in practice it means
that the orthonormalizing matrix has more small or
zero values.
A further option for normalization is to consider
some form of term frequency normalization. For
term frequency normalization, we use TF (tfwn),
Relative ( tfwnFw ), TFIDF (tfwn log(
N
dfw
)), and SQRT
( tfwn?
Fw
). Here, tfwn is the term frequency of word w
in document n, Fw is the total frequency of word
w in the corpus, and dfw is the number of docu-
ments containing the words w. The first three of
these normalizations have been chosen as they are
widely used in the literature. The SQRT normaliza-
tion has been shown to be effective for explicit topic
methods in previous experiments not reported here.
4 Experiments and Results
For evaluation, we consider a cross-lingual mate re-
trieval task from English/Spanish on the basis of
Wikipedia as aligned corpus. The goal is to, for each
document of a test set, retrieve the aligned document
or mate. For each test document, on the basis of
1737
Method Top-1 Prec. Top-5 Prec. Top-10 Prec. MRR Time Memory
ONETA L-Solve (N1 = 1000) 0.290 0.501 0.596 0.390 73s 354MB
ONETA L-Solve (N1 = 2000) 0.328 0.531 0.600 0.423 2m18s 508MB
ONETA L-Solve (N1 = 3000) 0.462 0.662 0.716 0.551 4m12s 718MB
ONETA L-Solve (N1 = 4000) 0.599 0.755 0.781 0.667 7m44s 996MB
ONETA L-Solve (N1 = 5000) 0.695 0.817 0.843 0.750 12m28s 1.30GB
ONETA L-Solve (N1 = 6000) 0.773 0.883 0.905 0.824 18m40s 1.69GB
ONETA L-Solve (N1 = 7000) 0.841 0.928 0.937 0.881 26m31s 2.14GB
ONETA L-Solve (N1 = 8000) 0.896 0.961 0.968 0.927 37m39s 2.65GB
ONETA L-Solve (N1 = 9000) 0.924 0.981 0.987 0.950 52m52s 3.22GB
ONETA (No Approximation) 0.929 0.987 0.990 0.956 57m10s 3.42GB
Word Translation 0.751 0.884 0.916 0.812 n/a n/a
ESA (SQRT Normalization) 0.498 0.769 0.835 0.621 72s 284MB
LDA (K=1000) 0.287 0.568 0.659 0.417 4h12m 8.4GB
LSI (K=4000) 0.615 0.756 0.783 0.676 13h51m 19.7GB
ONETA + Word Translation 0.932 0.987 0.993 0.958 n/a n/a
Table 2: Result on large-scale mate-finding studies for English to Spanish matching
the similarity of the query document to all indexed
documents, we compute the value ranki indicating
at which position the mate of the ith document oc-
curs. We use two metrics: Top-k Precision, defined
as the percentage of documents for which the mate is
retrieved among the first k elements, and Minimum
Reciprocal Rank, defined as
MRR =
?
i?test
1
ranki
For our experiments, we first extracted a subset
of documents (every 20th) from Wikipedia, filtering
this set down to only those that have aligned pages
in both English and Spanish with a minimum length
of 100 words. This gives us 10,369 aligned doc-
uments in total, which form the background docu-
ment collection B. We split this data into a training
and test set of 9,332 and 1,037 documents, respec-
tively. We then removed all words whose total fre-
quencies were below 50. This resulted in corpus of
6.7 millions words in English and 4.2 million words
in Spanish.
Normalization Methods: In order to investigate
the impact of different normalization methods, we
ran small-scale experiments using the first 500 doc-
uments from our dataset to train ONETA and then
evaluate the resulting models on the mate-finding
task on 100 unseen documents. The results are pre-
sented in Table 1, which shows the Top-1 Precision
for the different normalization methods. We see that
the effect of applying document normalization in
all cases improves the quality of the overall result.
Surprisingly, we do not see the same result for fre-
quency normalization yielding the best result for the
case where we do no normalization at all5 . In the re-
maining experiments we thus employ document nor-
malization and no term frequency normalization.
ApproximationMethods: In order to evaluate the
different approximation methods, we experimen-
tally compare 4 different approximation methods:
standard LSI, ON-Eigen (Equation 1), Explicit LSI
(Equation 2), L-Solve (Equation 3) on the same
small-scale corpus. For convenience we plot an ap-
proximation rate which is either K or N1 depending
on method; at K = 500 and N1 = 500, these ap-
proximations become exact. This is shown in Figure
1. We also observe the effects of approximation and
see that the performance increases steadily as we
increase the computational factor. We see that the
orthonormal eigenvector (Equation 1) method and
the L-solve (Equation 3) method are clearly simi-
lar in approximation quality. We see that the explicit
LSI method (Equation 2) and the LSI method both
perform significantly worse for most of the approxi-
5A likely explanation for this is that low frequency terms are
less evenly distributed and the effect of calculating the matrix
inverse magnifies the noise from the low frequency terms
1738
mation amounts. Explicit LSI is worse than the other
approximations as it first maps the test documents
into a K-dimensional LSI topic space, before map-
ping back into theN -dimensional explicit space. As
expected this performs worse than standard LSI for
all but high values of K as there is significant error
in both mappings. We also see that the (CL-)ESA
baseline, which is very low due to the small number
of documents, is improved upon by even the least ap-
proximation of orthonormalization. In the remain-
ing of this section, we report results using the L-
Solve method as it has a very good performance and
is computationally less expensive than ON-Eigen.
Evaluation and Comparison: We compare
ONETA using the L-Solve method with N1 values
from 1000 to 9000 topics with (CL-)ESA (using
SQRT normalization), LDA (using 1000 topics)
and LSI (using 4000 topics). We choose the largest
topic count for LSI and LDA we could to provide
the best possible comparison. For LSI, the choice of
K was determined on the basis of operating system
memory limits, while for LDA we experimented
with higher values for K without any performance
improvement, likely due to overfitting. We also
stress that for L-Solve ONETA, N1 is not the topic
count but an approximation rate of the mapping. In
all settings we use N topics as with standard ESA,
and so should not be considered directly comparable
to the K values of these methods.
We also compare to a baseline system that re-
lies on word-by-word translation, where we use the
most likely single translation of a word as given by a
phrase table generated by the Moses system (Koehn
et al, 2007) on the EuroParl corpus (Koehn, 2005).
Top 1, Top 5 and Top 10 Precision as well as Mean
Reciprocal Rank are reported in Table 2.
Interestingly, even for a small number of docu-
ments (e.g., N1 = 6000) our results improve both
the word-translation baseline as well as all other
topic models, ESA, LDA and LSI in particular. We
note that at this level the method is still efficiently
computable and calculating the inverse in practice
takes less time than training the Moses system. The
significance for results (N1 ? 7000) have been
tested by means of a bootstrap resampling signifi-
cance test, finding out that our results significantly
improve on the translation base line at a 99% level.
Further, we consider a straightforward combina-
tion of our method with the translation system con-
sisting of appending the topic vectors and the trans-
lation frequency vectors, weighted by the relative
average norms of the vectors. We see that in this
case the translations continue to improve the perfor-
mance of the system (albeit not significantly), sug-
gesting a clear potential for this system to help in im-
proving machine translation results. While we have
presented results for English and Spanish here, simi-
lar results were obtained for the German and French
case but are not presented here due to space limita-
tions.
In Table 2 we also include the user time and peak
resident memory of each of these processes, mea-
sured on an 8 Core Intel Xeon 2.50 GHz server.
We do not include the results for Word Translation
as many hours were spent learning a phrase table,
which includes translations for many phrases not in
the test set. We see that the ONETA method signif-
icantly outperforms LSI and LDA in terms of speed
and memory consumption. This is in line with the
theoretical calculations presented earlier where we
argued that inverting the N ?N dense matrix XTX
when W  N is computationally lighter than find-
ing an eigendecomposition of the W ? W sparse
matrix XXT. In addition, as we do not multiply
(XTX)?1 and XT, we do not need to allocate a
large W ? K matrix in memory as with LSI and
LDA.
The implementations of ESA, ONETA,
LSI and LDA used as well as the data
for the experiments are available at
http://github.com/jmccrae/oneta.
5 Conclusion
We have presented a novel method for cross-lingual
topic modelling, which combines the strengths of
explicit and latent topic models and have demon-
strated its application to cross-lingual document
matching. We have in particular shown that the
method outperforms widely used topic models such
as Explicit Semantic Analysis (ESA), Latent Seman-
tic Indexing (LSI) and Latent Dirichlet Allocation
(LDA). Further, we have shown that it outperforms
a simple baseline relying on word-by-word transla-
tion of the query document into the target language,
1739
while the induction of the model takes less time
than training the machine translation system from a
parallel corpus. We have also presented an effec-
tive approximation method, i.e. L-Solve, which sig-
nificantly reduces the computational cost associated
with computing the topic models.
Acknowledgements
This work was funded by the Monnet Project
and the Portdial Project under the EC Sev-
enth Framework Programme, Grants No.
248458 and 296170. Roman Klinger has been
funded by the ?Its OWL? project (?Intelli-
gent Technical Systems Ostwestfalen-Lippe?,
http://www.its-owl.de/), a leading-edge
cluster of the German Ministry of Education and
Research.
References
Dennis S Bernstein. 2005. Matrix mathematics, 2nd Edi-
tion. Princeton University Press Princeton.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, and Steffen Staab. 2009. Explicit versus la-
tent concept models for cross-language information re-
trieval. In IJCAI, volume 9, pages 1513?1518.
Don Coppersmith and Shmuel Winograd. 1990. Matrix
multiplication via arithmetic progressions. Journal of
symbolic computation, 9(3):251?280.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391?407.
Chris Ding, Tao Li, and Wei Peng. 2006. NMF and
PLSI: equivalence and a hybrid algorithm. In Pro-
ceedings of the 29th annual international ACM SIGIR,
pages 641?642. ACM.
Susan T Dumais, Todd A Letsche, Michael L Littman,
and Thomas K Landauer. 1997. Automatic cross-
language retrieval using latent semantic indexing. In
AAAI spring symposium on cross-language text and
speech retrieval, volume 15, page 21.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using Wikipedia-based ex-
plicit semantic analysis. In Proceedings of the 20th In-
ternational Joint Conference on Artificial Intelligence,
volume 6, page 12.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual interna-
tional ACM SIGIR conference, pages 50?57. ACM.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, et al 2007. Moses: Open source toolkit for sta-
tistical machine translation. In Proceedings of the 45th
Annual Meeting of the ACL, pages 177?180. Associa-
tion for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In MT summit, volume 5.
Daniel D Lee and H Sebastian Seung. 1999. Learning
the parts of objects by non-negative matrix factoriza-
tion. Nature, 401(6755):788?791.
Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010.
Topic models for word sense disambiguation and
token-based idiom detection. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, pages 1138?1147. Association for
Computational Linguistics.
David Mimno, Hanna M Wallach, Jason Naradowsky,
David A Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 880?889. Association for
Computational Linguistics.
Martha Palmer, Owen Rambow, and Alexis Nasr. 1998.
Rapid prototyping of domain-specific machine trans-
lation systems. In Machine Translation and the Infor-
mation Soup, pages 95?102. Springer.
Philipp Sorg and Philipp Cimiano. 2008. Cross-lingual
information retrieval with explicit semantic analysis.
In Proceedings of the Cross-language Evaluation Fo-
rum 2008.
Philipp Sorg and Philipp Cimiano. 2010. An experi-
mental comparison of explicit semantic analysis im-
plementations for cross-language retrieval. In Natural
Language Processing and Information Systems, pages
36?48. Springer.
Vassilis Spiliopoulos, George A Vouros, and Vangelis
Karkaletsis. 2007. Mapping ontologies elements us-
ing features in a latent space. In IEEE/WIC/ACM
International Conference on Web Intelligence, pages
457?460. IEEE.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical machine
translation. Machine Translation, 21(4):187?207.
1740
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 848?854,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Bidirectional Inter-dependencies of Subjective Expressions and
Targets and their Value for a Joint Model
Roman Klinger and Philipp Cimiano
Semantic Computing Group
Cognitive Interaction Technology ? Center of Excellence (CIT-EC)
Bielefeld University
33615 Bielefeld, Germany
{rklinger,cimiano}@cit-ec.uni-bielefeld.de
Abstract
Opinion mining is often regarded as a clas-
sification or segmentation task, involving
the prediction of i) subjective expressions,
ii) their target and iii) their polarity. In-
tuitively, these three variables are bidirec-
tionally interdependent, but most work has
either attempted to predict them in isolation
or proposing pipeline-based approaches
that cannot model the bidirectional interac-
tion between these variables. Towards bet-
ter understanding the interaction between
these variables, we propose a model that
allows for analyzing the relation of target
and subjective phrases in both directions,
thus providing an upper bound for the im-
pact of a joint model in comparison to a
pipeline model. We report results on two
public datasets (cameras and cars), show-
ing that our model outperforms state-of-
the-art models, as well as on a new dataset
consisting of Twitter posts.
1 Introduction
Sentiment analysis or opinion mining is the task of
identifying subjective statements about products,
their polarity (e. g. positive, negative or neutral)
in addition to the particular aspect or feature of
the entity that is under discussion, i. e., the so-
called target. Opinion analysis is thus typically
approached as a classification (Ta?ckstro?m and Mc-
Donald, 2011; Sayeed et al, 2012; Pang and Lee,
2004) or segmentation (Choi et al, 2010; Johans-
son and Moschitti, 2011; Yang and Cardie, 2012)
task by which fragments of the input are classi-
fied or labelled as representing a subjective phrase
(Yang and Cardie, 2012), a polarity or a target (Hu
and Liu, 2004; Li et al, 2010; Popescu and Etzioni,
2005; Jakob and Gurevych, 2010). As an example,
the sentence ?I like the low weight of the camera.?
contains a subjective term ?like?, and the target
?low weight?, which can be classified as a positive
statement.
While the three key variables (subjective phrase,
polarity and target) intuitively influence each other
bidirectionally, most work in the area of opinion
mining has concentrated on either predicting one
of these variables in isolation (e. g. subjective ex-
pressions by Yang and Cardie (2012)) or modeling
the dependencies uni-directionally in a pipeline ar-
chitecture, e. g. predicting targets on the basis of
perfect and complete knowledge about subjective
terms (Jakob and Gurevych, 2010). However, such
pipeline models do not allow for inclusion of bidi-
rectional interactions between the key variables. In
this paper, we propose a model that can include
bidirectional dependencies, attempting to answer
the following questions which so far have not been
addressed but provide the basis for a joint model:
? What is the impact of the performance loss
of a non-perfect subjective term extraction in
comparison to perfect knowledge?
? Further, how does perfect knowledge about
targets influence the prediction of subjective
terms?
? How is the latter affected if the knowledge
about targets is imperfect, i. e. predicted by a
learned model?
We study these questions using imperatively de-
fined factor graphs (IDFs, McCallum et al (2008),
McCallum et al (2009)) to show how these bi-
directional dependencies can be modeled in an ar-
chitecture which allows for further steps towards
joint inference. IDFs are a convenient way to define
probabilistic graphical models that make structured
predictions based on complex dependencies.
848
2 A Model for the Extraction of Target
Phrases and Subjective Expressions
This section gives a brief introduction to impera-
tively defined factor graphs and then introduces our
model.
2.1 Imperatively Defined Factor Graphs
A factor graph (Kschischang et al, 2001) is a bi-
partite graph over factors and variables. Let factor
graph G define a probability distribution over a
set of output variables y conditioned on input vari-
ables x. A factor ?i computes a scalar value over
the subset of variables xi and yi that are neighbors
of ?i in the graph. Often this real-valued function
is defined as the exponential of an inner product
over sufficient statistics {fik(xi,yi)} and parame-
ters {?ik}, where k ? [1,Ki] and Ki is the number
of parameters for factor ?i.
A factor template Tj consists of parameters
{?jk}, sufficient statistic functions {fjk}, and a
description of an arbitrary relationship between
variables, yielding a set of tuples {(xj ,yj)}. For
each of these tuples, the factor template instan-
tiates a factor that shares {?jk} and {fjk} with
all other instantiations of Tj . Let T be the set of
factor templates and Z(x) be the partition func-
tion for normalization. The probability distri-
bution can then be written as p(y|x) = 1Z(x)?
Tj?T
?
(xi,yi)?Tj exp
(?Kj
k=1 ?jkfjk(xi,yi)
)
.
FACTORIE1 (McCallum et al, 2008; McCallum
et al, 2009) is an implementation of imperatively
defined factor graphs in the context of Markov
1http://factorie.cs.umass.edu
better than CCD shift systems
POS=JJR
W=better
POS-W=better JJR
ONE-EDGE-POS=JJR
ONE-EDGE-W=better
ONE-EDGE-POS-W=better JJR
ONE-EDGE-POS-SEQ=JJR
BOTH-POS=JJR
BOTH-W=better
BOTH-POS-W=better JJR
BOTH-POS-POS-SEQ=JJR
POS=NN
W=shift
W=systems
POS-W=shift NN
POS-W=systems NNS
POS-SEQ=NN-NNS
NO-CLOSE-NOUN
ONE-EDGE-POS=NN
ONE-EDGE-POS=NNS
ONE-EDGE-W=shift
ONE-EDGE-W=sensors
BOTH-POS=NN
BOTH-POS=NNS
. . .
subjective target
sin
gle
sp
an
in
te
rs
pa
n
Figure 1: Example for features extracted for target
and subjective expressions (text snippet taken from
the camera data set (Kessler et al, 2010)). IOB-like
features are merged for simplicity in this depiction.
chain Monte Carlo (MCMC) inference, a common
approach for inference in very large graph struc-
tures (Culotta and McCallum, 2006; Richardson
and Domingos, 2006; Milch et al, 2006). The
term imperative is used to denote that actual code
in an imperative programming language is writ-
ten to describe templates and the relationship of
tuples they yield. This flexibility is beneficial for
modeling inter-dependencies as well as designing
information flow in joint models.
2.2 Model
Our model is similar to a semi-Markov conditional
random field (Sarawagi and Cohen, 2004). It pre-
dicts the offsets for target mentions and subjective
phrases and can use the information of each other
during inference. In contrast to a linear chain con-
ditional random field (Lafferty et al, 2001), this al-
lows for taking distant dependencies of unobserved
variables into account and simplifies the design of
features measuring characteristics of multi-token
phrases. The relevant variables, i. e. target and sub-
jective phrase, are modelled via complex span vari-
ables of the form s = (l, r, c) with a left and right
offset l and r, and a class c ? {target, subjective}.
These offsets denote the span on a token sequence
t = (t1, . . . , tn).
We use two different templates to define factors
between variables: a single span template and an
inter-span template. The single span template de-
fines factors with scores based on features of the
tokens in the span and its vicinity. In our model,
all features are boolean. As token-based features
we use the POS tag, the lower-case representation
of the token as well as both in combination. The
actual span representation consists of these features
prefixed with ?I? for all tokens in the span, with ?B?
for the token at the beginning of the span, and with
?E? for the token at the end of the span. In addition,
the sequence of POS tags of all tokens in the span
is included as a feature.
The inter-span template takes three characteris-
tics of spans into account: Firstly, we measure if
a potential target span contains a noun which is
the closest noun to a subjective expression. Sec-
ondly, we measure for each span if a span of the
other class is in the same sentence. A third fea-
ture indicates whether there is only one edge in the
dependency graph between the tokens contained
in spans of a different class. These features are
to a great extent inspired by Jakob and Gurevych
849
(2010). For parsing, we use the Stanford parser
(Klein and Manning, 2003).
The features described so far, however, cannot
differentiate between a possible aspect mention
which is a target of a subjective expression and
one which is not. Therefore, the features of the
inter-span template are actually built by taking the
cross-product of the three described characteristics
with all single-span features. Spans which are not
in the context of a span of a different class are rep-
resented by a ?negated? feature (namely No-Close-
Noun, No-Single-Edge, and Not-Both-In-Sentence).
The example in Figure 1 shows features for two
spans which are in context of each other. All of
these features representing the text are taken into
account for each class, i. e., target and subjective
expression.
Inference is performed via Markov Chain Monte
Carlo (MCMC) sampling. In each sampling step,
only the variables which actually change need to
be evaluated, and therefore the sampler directs the
process of unrolling the templates to factors. These
world changes are necessary to find the maximum
a posteriori (MAP) configuration as well as learn-
ing the parameters of the model. For each token
in the sequence, a span of length one of each class
is proposed if no span containing the token exists.
For each existing span, it is proposed to change
its label, shorten or extend it by one token if pos-
sible (all at the beginning and at the end of the
span, respectively). Finally, a span can be removed
completely.
In order to learn the parameters of our model, we
apply SampleRank (Wick et al, 2011). A crucial
component in the framework is the objective func-
tion which gives feedback about the quality of a
sample proposal during training. We use the follow-
ing objective function f(t) to evaluate a proposed
span t:
f(t) = max
g?s
o(t,g)
|g| ? ? ? p(t,g) ,
where s is the set of all spans in the gold standard.
Further, the function o calculates the overlap in
terms of tokens of two spans and the function p
returns the number of tokens in t that are not con-
tained in g, i. e., those which are outside the overlap
(both functions taking into account the class of the
span). Thus, the first part of the objective function
represents the fraction of correctly proposed con-
tiguous tokens, while the second part penalizes a
span for containing too many tokens that are out-
side the best span. Here, ? is a parameter which
controls the penalty.
3 Results and Discussion
3.1 Experimental Setting
We report results on the J.D. Power and Associates
Sentiment Corpora2, an annotated data set of blog
posts in the car and in the camera domain (Kessler
et al, 2010). From the rich annotation set, we
use subjective terms and entity mentions which
are in relation to them as targets. We do not con-
sider comitter, negator, neutralizer,
comparison, opo, or descriptor annota-
tions to be subjective expressions. Results on these
data sets are compared to Jakob and Gurevych
(2010).
In addition, we report results on a Twitter data
set3 for the first time (Spina et al, 2012). Here,
we use a Twitter-specific tokenizer and POS tag-
ger4 (Owoputi et al, 2013) instead of the Stanford
parser. Hence, the single-edge-based feature de-
scribed in Section 2.2 is not used for this dataset. A
short summary of the datasets is given in Table 1.
As evaluation metric we use the F1 measure, the
harmonic mean between precision and recall. True
positive spans are evaluated in a perfect match and
approximate match mode, where the latter regards
a span as positive if one token within it is included
in a corresponding span in the gold standard. In this
case, other predicted spans matching the same gold
span do not count as false positives. In the objective
function, ? is set to 0.01 to prefer spans which are
longer than the gold phrase over predicting no span.
Four different experiments are performed (all
via 10-fold cross validation): First, predicting sub-
jectivity expressions followed by predicting targets
while making use of the previous prediction. Sec-
2http://verbs.colorado.edu/jdpacorpus/
3http://nlp.uned.es/?damiano/datasets/
entityProfiling_ORM_Twitter.html
4In version 0.3, http://www.ark.cs.cmu.edu/
TweetNLP/
Car Camera Twitter
Texts 457 178 9238
Targets 11966 4516 1418
Subjectives 15056 5128 1519
Table 1: Statistics of the data sets.
850
 0
 0.2
 0.4
 0.6
 0.8
 1
pred. S. ? T. pred. T. ? S. Gold S. ? T. Gold T. ? S. Jakob 2010
F 1
Target-F1 PartialSubjective-F1 Partial
Target-F1Subjective-F1
 0.53
 0.44
 0.65 0.61  0.65
 0.71
 0.48
 0.32
 0.58
 1.00
 0.50 0.54
 0.60
 1.00
 0.65
 1.00
Figure 2: Results for the workflow of first predicting subjective phrases, then targets (pred. S.? T.), and
vice versa (pred. T.? S.), as well as in comparison to having perfect information for the first step for the
camera data set.
 0
 0.2
 0.4
 0.6
 0.8
 1
pred. S. ? T. pred. T. ? S. Gold S. ? T. Gold T. ? S. Jakob 2010
F 1
Target-F1 PartialSubjective-F1 Partial
Target-F1Subjective-F1
 0.51
 0.43
 0.62 0.64  0.69
 0.74
 0.43
 0.33
 0.55
 1.00
 0.50 0.56
 0.66
 1.00
 0.70
 1.00
Figure 3: Results for the car data set.
ond, predicting targets followed by predicting sub-
jective expressions. Third, assuming perfect knowl-
edge of subjective expressions when predicting tar-
gets, and fourth, assuming perfect knowledge of
targets in predicting subjective expressions. This
provides us with the information how good a pre-
diction can be with perfect knowledge of the other
variable as well as an estimate of how good the
prediction can be without any previous knowledge.
3.2 Results
Figures 2, 3 and 4 show the results for the four
different settings compared to the results by Jakob
and Gurevych (2010) for cars and cameras. The
darker bars correspond to perfect match, the lighter
ones to the increase when taking partial matches
into account. In the following we only discuss the
perfect match.
Comparing the results (for the car and camera
data sets, Figure 2 and 3) for subjectivity predic-
tion, one can observe a limited performance when
targets are not known (0.54F1 for the camera set,
0.56F1 for the car set), an upper bound with per-
fect target information is much higher (0.65F1,
0.7F1). When first predicting targets followed by
subjective term prediction, we obtain results of
0.6F1 and 0.66F1. The results for target predic-
tion are much lower when not knowing subjec-
tive expressions in advance (0.32F1, 0.33F1), and
clearly increase with predicted subjective expres-
sions (0.48F1, 0.43F1) and outperform previous
results when compared to Jakob and Gurevych
(2010) (0.58F1, 0.55F1 in comparison to their
0.5F1 on both sets).
The results for the Twitter data set show the same
characteristics (in Figure 4). However, they are
generally much lower. In addition, the difference
between exact and partial match evaluation modes
851
 0
 0.2
 0.4
 0.6
 0.8
 1
pred. S. ? T. pred. T. ? S. Gold S. ? T. Gold T. ? S.
F 1
Target-F1 PartialSubjective-F1 Partial
Target-F1Subjective-F1
 0.42
 0.32
 0.67
 0.40  0.41
 0.60
 0.26
 0.13
 0.40
 1.00
 0.22  0.28
 1.00
 0.35
Figure 4: Results for the Twitter data set.
 0
 0.2
 0.4
 0.6
 0.8
 1
Sentence Edge Noun All
F 1  0.48
 0.57  0.52
 0.65
 0.41
 0.48  0.42
 0.58
(a) Camera Data Set, given subjective terms.
 0
 0.2
 0.4
 0.6
 0.8
 1
Sentence Edge Noun All
F 1
 0.68
 0.55
 0.17
 0.71
 0.62
 0.51
 0.17
 0.65
(b) Camera Data Set, given target terms.
Figure 5: Evaluation of the impact of different features.
is higher. This is due to the existence of many more
phrases spanning multiple tokens.
Exemplarily, the impact of the three features in
the inter-span templates for the camera data set is
depicted in Figure 5 for (a) given subjective terms
(b) given targets, respectively. Detecting the clos-
est noun is mainly of importance for target iden-
tification and only to a minor extent for detecting
subjective phrases. A short path in the dependency
graph and detecting if both phrases are in the same
sentence have a high positive impact for both sub-
jective and target phrases.
3.3 Conclusion and Discussion
The experiments in this paper show that target
phrases and subjective terms are clearly interde-
pendent. However, the impact of knowledge about
one type of entity for the prediction of the other
type of entity has been shown to be asymmetric.
The results clearly suggest that the impact of sub-
jective terms on target terms is higher than the other
way round. Therefore, if a pipeline architecture is
chosen, this order is to be preferred. However, the
results with perfect knowledge of the counterpart
entity show (in both directions) that the entities
influence each other positively. Therefore, the chal-
lenge of extracting subjective expressions and their
targets is a great candidate for applying supervised,
joint inference.
Acknowledgments
Roman Klinger has been funded by the ?It?s
OWL? project (?Intelligent Technical Systems
Ostwestfalen-Lippe?, http://www.its-owl.
de/), a leading-edge cluster of the German Min-
istry of Education and Research. We thank the
information extraction and synthesis laboratory
(IESL) at the University of Massachusetts Amherst
for their support.
852
References
Yoonjung Choi, Seongchan Kim, and Sung-Hyon
Myaeng. 2010. Detecting Opinions and their Opin-
ion Targets in NTCIR-8. Proceedings of NTCIR8
Workshop Meeting, pages 249?254.
A. Culotta and A. McCallum. 2006. Tractable Learn-
ing and Inference with High-Order Representations.
In ICML Workshop on Open Problems in Statistical
Relational Learning.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168?177,
New York, NY, USA. ACM.
Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single- and cross-domain set-
ting with conditional random fields. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1035?1045,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Richard Johansson and Alessandro Moschitti. 2011.
Extracting opinion expressions and their polarities:
exploration of pipelines and joint models. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: short papers ? Volume 2, pages
101?106, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Jason S. Kessler, Miriam Eckert, Lyndsie Clark, and
Nicolas Nicolov. 2010. The 2010 ICWSM JDPA
Sentment Corpus for the Automotive Domain. In
4th International AAAI Conference on Weblogs and
Social Media Data Workshop Challenge (ICWSM-
DWC 2010).
D. Klein and Ch. D. Manning. 2003. Fast exact in-
ference with a factored model for natural language
parsing. In Advances in Neural Information Process-
ing Systems 16 [Neural Information Processing Sys-
tems.
F.R. Kschischang, B.J. Frey, and H.-A. Loeliger. 2001.
Factor graphs and the sum-product algorithm. Infor-
mation Theory, IEEE Trans on Information Theory,
47(2):498?519.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In International Conference on Ma-
chine Learning, pages 282?289.
Fangtao Li, Minlie Huang, and Xiaoyan Zhu. 2010.
Sentiment analysis with global topics and local de-
pendency. In Proceedings of the Twenty-Fourth
AAAI Conference on Artificial Intelligence, pages
1371?1376, Atlanta, Georgia, USA.
A. McCallum, K. Rohanimanesh, M. Wick, K. Schultz,
and Sameer Singh. 2008. FACTORIE: Efficient
Probabilistic Programming via Imperative Declara-
tions of Structure, Inference and Learning. In NIPS
Workshop on Probabilistic Programming.
Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. FACTORIE: Probabilistic programming via
imperatively defined factor graphs. In Neural Infor-
mation Processing Systems (NIPS).
B. Milch, B. Marthi, and S. Russell. 2006. BLOG:
Relational Modeling with Unknown Objects. Ph.D.
thesis, University of California, Berkeley.
O. Owoputi, B. OConnor, Ch. Dyer, K. Gimpely,
N. Schneider, and N. A. Smith. 2013. Improved
part-of-speech tagging for online conversational text
with word clusters. In The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: Sentiment analysis using subjectivity sum-
marization based on minimum cuts. In Proceedings
of the 42nd Meeting of the Association for Compu-
tational Linguistics, Main Volume, pages 271?278,
Barcelona, Spain, July.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of Human Language Technology Con-
ference and Conference on Empirical Methods in
Natural Language Processing, pages 339?346, Van-
couver, British Columbia, Canada, October. Associ-
ation for Computational Linguistics.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning, 62(1-2):107?136.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Advances in Neural Information Pro-
cessing Systems 17 [Neural Information Processing
Systems.
Asad Sayeed, Jordan Boyd-Graber, Bryan Rusk, and
Amy Weinberg. 2012. Grammatical structures for
word-level sentiment detection. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 667?
676, Montre?al, Canada, June. Association for Com-
putational Linguistics.
D. Spina, E. Meij, A. Oghina, M. T. Bui, M. Breuss,
and M. de Rijke. 2012. A Corpus for Entity Pro-
filing in Microblog Posts. In LREC Workshop on
Information Access Technologies for Online Reputa-
tion Management.
Oscar Ta?ckstro?m and Ryan McDonald. 2011. Semi-
supervised latent variable models for sentence-level
sentiment analysis. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
853
569?574, Portland, Oregon, USA, June. Association
for Computational Linguistics.
M. Wick, K. Rohanimanesh, K. Bellare, A. Culotta,
and A. McCallum. 2011. SampleRank: Training
factor graphs with atomic gradients. In Interna-
tional Conference on Machine Learning.
Bishan Yang and Claire Cardie. 2012. Extracting opin-
ion expressions with semi-markov conditional ran-
dom fields. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1335?1345, Stroudsburg, PA, USA.
Association for Computational Linguistics.
854
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 35?43,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Improving Distantly Supervised Extraction of Drug-Drug and
Protein-Protein Interactions
Tamara Bobic?,1,2? Roman Klinger,1? Philippe Thomas,3 and Martin Hofmann-Apitius1,2
1Fraunhofer Institute for
Algorithms and Scientific
Computing (SCAI)
Schloss Birlinghoven
53754 Sankt Augustin
Germany
2Bonn-Aachen Center for
Information Technology
Dahlmannstra?e 2
53113 Bonn
Germany
3Computer Science Institut
Humboldt-Universita?t
Unter den Linden 6
10099 Berlin
Germany
{tbobic,klinger,hofmann-apitius}@scai.fraunhofer.de
thomas@informatik.hu-berlin.de
Abstract
Relation extraction is frequently and suc-
cessfully addressed by machine learning
methods. The downside of this approach
is the need for annotated training data, typi-
cally generated in tedious manual, cost inten-
sive work. Distantly supervised approaches
make use of weakly annotated data, like au-
tomatically annotated corpora.
Recent work in the biomedical domain
has applied distant supervision for protein-
protein interaction (PPI) with reasonable
results making use of the IntAct database.
Such data is typically noisy and heuristics
to filter the data are commonly applied. We
propose a constraint to increase the qual-
ity of data used for training based on the
assumption that no self-interaction of real-
world objects are described in sentences.
In addition, we make use of the Univer-
sity of Kansas Proteomics Service (KUPS)
database. These two steps show an increase
of 7 percentage points (pp) for the PPI cor-
pus AIMed. We demonstrate the broad appli-
cability of our approach by using the same
workflow for the analysis of drug-drug in-
teractions, utilizing relationships available
from the drug database DrugBank. We
achieve 37.31 % in F1 measure without man-
ually annotated training data on an indepen-
dent test set.
1 Introduction
Assuming co-mentioned entities to be related is
an approach of extracting relations of real-world
objects with limited precision. Extracting high
quality interaction pairs from free text allows for
?These authors contributed equally.
building networks, e. g. of proteins, which need
less manual curation to serve as a model for further
knowledge processing steps. Nevertheless, just as-
suming co-occurrence to model an interaction or
relation is common, as the development of inter-
action extraction systems can be time-consuming
and complex.
Currently, a lot of relation extraction (RE) sys-
tems rely on machine learning, namely classifying
pairs of entities to be related or not (Airola et al,
2008; Miwa et al, 2009; Kim et al, 2010). De-
spite the fact that machine learning has been most
successful in identifying relevant relations in text,
a drawback is the need for manually annotated
training data. Domain experts have to dedicate
time and effort to this tedious and labor-intensive
process.
Specific biomedical domains have been ex-
plored more extensively than others, thus creating
an imbalance in the number of existing corpora
for a specific RE task. Protein-protein interactions
(PPI) have been investigated the most, which gave
rise to a number of available corpora. Pyysalo et al
(2008) standardized five PPI corpora to a unified
XML format. Recently, a drug-drug-interaction
(DDI) corpus is made available in the same for-
mat, originally for the DDI Extraction Workshop1
(Segura-Bedmar et al, 2011b).
As a consequence of the overall scarcity of an-
notated corpora for RE in the biomedical domain,
the approach of distant supervision, e. g. to auto-
matically label a training set is emerging. Many
approaches make use of the distant supervision as-
sumption (Mintz et al, 2009; Riedel et al, 2010):
1Associated with the conference of the spanish society
for natural language processing (SEPLN) in 2011, http:
//labda.inf.uc3m.es/DDIExtraction2011/
35
If two entities participate in a relation,
all sentences that mention these two en-
tities express that relation.
Obviously, this assumption does not hold in gen-
eral, and therefore exceptions need to be detected
which are not used for training a model. Thomas et
al. (2011b) successfully used simple filtering tech-
niques in a distantly supervised setting to extract
PPI. In contrast to their work, we introduce a more
generic filter to detect frequent exceptions from
the distant supervision assumption and make use
of more data sources, by merging the interaction
information from IntAct and KUPS databases (dis-
cussed in Section 2.1). In addition, we present the
first system (to our knowledge), evaluating distant
supervision for drug-drug interaction with promis-
ing results.
1.1 Related work
Distant supervision approaches have received con-
siderable attention in the past few years. However,
most of the work is focusing on domains other
than biomedical texts.
Mintz et al (2009) use distant supervision to
learn to extract relations that are represented in
Freebase (Bollacker et al, 2008). Yao et al (2010)
use Freebase as a source of supervision, dealing
with entity identification and relation extraction
in a joint fashion. Entity types are restricted to
those compatible with selected relations. Riedel et
al. (2010) argue that distant supervision leads to
noisy training data that hurts precision and suggest
a two step approach to reduce this problem. They
identify the sentences which express the known re-
lations (?expressed-at-least-once? assumption) and
thus frame the problem of distant supervision as
an instance of constraint-driven semi-supervision,
achieving 31 % of error reduction.
Vlachos et al (2009) tackle the problem of
biomedical event extraction. The scope of their
interest is to identify different event types without
using a knowledge base as a source of supervision,
but explore the possibility of inferring relations
from the text based on the trigger words and de-
pendency parsing, without previously annotated
data.
Thomas et al (2011b) develop a distantly la-
beled corpus for protein-protein interaction extrac-
tion. Different strategies are evaluated to select
valuable training instances. Competitive results
are obtained, compared to purely supervised meth-
ods.
Very recent work examines the usability of
knowledge from PharmGKB (Gong et al, 2008)
to generate training sets that capture gene-drug,
gene-disease and drug-disease relations (Buyko et
al., 2012). They evaluate the RE for the three inter-
action classes in intrisic and extrinsic experimental
settings, reaching F1 measure of around 80 % and
up to 77.5 % respectively.
2 Resources
2.1 Interaction Databases
The IntAct database (Kerrien et al, 2012) con-
tains protein-protein interaction information. It is
freely available, manually curated and frequently
updated. It consists of 290,947 binary interaction
evidences, including 39,235 unique pairs of inter-
acting proteins for human species.2
In general, PPI databases are underanno-
tated and the overlap between them is marginal
(De Las Rivas and Fontanillo, 2010). Combining
several databases allows to cover a larger fraction
of known interactions resulting in a more complete
knowledge base. KUPS (Chen et al, 2010) is a
database that combines entries from three manu-
ally curated PPI databases (IntAct, MINT (Chatr-
aryamontri et al, 2007) and HPRD50 (Prasad et al,
2009)) and contains 185,446 positive pairs from
various model organisms, out of which 69,600
belong to human species.3 Enriching IntAct inter-
action information with the KUPS database leads
to 57,589 unique pairs.4
The database DrugBank (Knox et al, 2011)
combines detailed drug data with comprehensive
drug target information. It consists of 6,707 drug
entries. Apart from information about its targets,
for certain drugs known interactions with other
drugs are given. Altogether, we obtain 11,335
unique DDI pairs.
2.2 Corpora
For evaluation of protein-protein interaction, the
five corpora made available by Pyysalo et al
(2008) are used. Their properties, like size and ra-
tio of positive and negative examples, differ greatly,
2As of January 27th, 2012.
3As of August 16th, 2010.
4Only 45,684 out of 69,600 human PPI pairs are available
from the KUPS web service due to computational and storage
limitations (personal communication).
36
Corpus Positive pairs Negative pairs Total
AIMed 1000 (0.17) 4,834 (0.82) 5,834
BioInfer 2,534 (0.26) 7,132 (0.73) 9,666
HPRD50 163 (0.38) 270 (0.62) 433
IEPA 335 (0.41) 482 (0.59) 817
LLL 164 (0.49) 166 (0.50) 330
DDI train 2,400 (0.10) 21,411 (0.90) 23,811
DDI test 755 (0.11) 6,275 (0.89) 7,030
Table 1: Basic statistics of the five PPI and two DDI
corpora. Ratios are given in brackets.
the latter being the main cause of performance dif-
ferences when evaluating on these corpora. More-
over, annotation guidelines and contexts differ:
AIMed (Bunescu et al, 2005) and HPRD50 (Fun-
del et al, 2007) are human-focused, LLL (Nedel-
lec, 2005) on Bacillus subtilis, BioInfer (Pyysalo
et al, 2007) contains information from various or-
ganisms and IEPA (Ding et al, 2002) is made of
sentences that describe 10 selected chemicals, the
majority of which are proteins, and their interac-
tions.
For the purposes of DDI extraction, the corpus
published by Segura-Bedmar et al (2011b) is used.
This corpus is generated from web-documents de-
scribing drug effects. It is divided into a training
and testing set. An overview of the corpora is
given in Table 1.
3 Methods
In this section, the relation extraction system used
for classification of interacting pairs is presented.
Furthermore, the process of generating an automat-
ically labeled corpus is explained in more detail,
along with specific characteristics of the PPI and
DDI task.
3.1 Interaction Classification
We formulate the task of relation extraction as
feature-based classification of co-occurring enti-
ties in a sentence. Those are assigned to be either
related or not, without identifying the type of re-
lation. Our RE system is based on rich feature
vectors and the linear support vector machine clas-
sifier LibLINEAR, which has shown high perfor-
mance (in runtime as well as model accuracy) on
large and sparse data sets (Fan et al, 2008).
The approach is based on lexical features, op-
tionally with dependency parsing features created
using the Stanford parser (Marneffe et al, 2006).
Lexical features are bag-of-words (BOW) and n-
Methods P R F1
Thomas et al (2011a) 60.54 71.92 65.74
Chowdhury et al (2011) 58.59 70.46 63.98
Chowdhury and Lavelli (2011) 58.39 70.07 63.70
Bjo?rne et al (2011) 58.04 68.87 62.99
Minard et al (2011) 55.18 64.90 59.65
Our system (lex) 63.30 52.32 57.28
Our system (lex+dep) 66.46 56.69 61.19
Table 2: Comparison of fully supervised relations ex-
traction systems for DDI. (lex denotes the use of lexi-
cal features, lex+dep the additional use of dependency
parsing-based features.)
grams based, with n ? {1, 2, 3, 4}. They encom-
pass the local (window size 3) and global (window
size 13) context left and right of the entity pair,
along with the area between the entities (Li et al,
2010). Additionally, dictionary based domain spe-
cific trigger words are taken into account.
The respective dependency parse tree is in-
cluded through following the shortest dependency
path hypothesis (Bunescu and Mooney, 2005), by
using the syntactical and dependency information
of edges (e) and vertices (v). So-called v-walks
and e-walks of length 3 are created as well as n
grams along the shortest path (Miwa et al, 2010).
3.2 Automatically Labeling a Corpus in
General
One of the most important source of publications
in the biomedical domain is MEDLINE5, currently
containing more than 21 million citations.6 The
initial step is annotation of named entities ? in
our case performed by ProMiner (Hanisch et al,
2005), a tool proving state-of-the-art results in e. g.
the BioCreative competition (Fluck et al, 2007).
Based on the named entity recognition, only sen-
tences containing co-occurrences are further pro-
cessed. Based on the distant supervision assump-
tion, each pair of entities is labeled as related if
mentioned so in a structured interaction databases.
Note that this requires the step of entity normaliza-
tion.
3.3 Filtering Noise
A sentence may contain two entities of an inter-
acting pair (as known from a database), but does
not describe their interaction. Likewise, a sentence
5http://www.ncbi.nlm.nih.gov/pubmed/
6As of January, 2012.
37
may talk about a novel interaction which has not
been stored in the database. Therefore, filtering
strategies need to be employed to help in decid-
ing which pairs are annotated as being related and
which not.
Thomas et al (2011b) propose the use of trigger
words, i. e., an entity pair of a certain sentence is
marked as positive (related) if the database has in-
formation about their interaction and the sentence
contains at least one trigger word. Similarly, a
negative (non-related) example is a pair of entities
that does not interact according to the database
and their sentence does not contain any trigger
word. Pairs which do not fulfil both constraints are
discarded.
Towards improvement of the heuristics for re-
ducing noise, we introduce the constraint of ?auto-
interaction filtering? (AIF): If entities from an en-
tity pair both refer to the same real-world object,
the pair is labeled as not interacting. Even though
self-interactions are known for proteins and drugs,
such pairs can rarely be observed to describe an
interaction but rather are repeated occurences or
abbreviations. Moreover, the fundamental advan-
tage of AIF is that it requires no additional manual
effort.
3.4 Application on Protein-Protein
Interaction and Drug-Drug Interaction
In biomedical texts there are often mentions of
multiple proteins in the same sentence. However,
this co-occurrence does not necessarily signal that
the sentence is talking about their relation. Hence,
to reduce noise, a list of trigger words specific to
the problem is required. The rationale behind this
filter is that the interaction between two entities is
usually expressed by a specific (trigger) word. For
protein-protein-interactions, we use the trigger list
compiled by Thomas et al (2011b)7. In addition to
using IntAct alone, we introduce the use of KUPS
database (as described in Section 2.2).
For drug-drug-interaction, to our knowledge,
no DDI-specific trigger word list developed by
domain experts is available. Therefore, filtering
via such term occurrences is not applied in this
case.
7http://www2.informatik.hu-berlin.de/
?thomas/pub/2011/iwords.txt
4 Results
In this section, we start with an overview of state-
of-the-art results for fully supervised relation ex-
traction on PPI and DDI corpora (see Table 1).
Furthermore, experimental settings for distant su-
pervision are explained. Finally, we present spe-
cific results for models trained on distantly labeled
data, when evaluated on manually annotated PPI
and DDI corpora.
4.1 Performance overview of supervised RE
systems
Protein-protein interactions has been extensively
investigated in the past decade because of their bio-
logical significance. Machine learning approaches
have shown the best performance in this domain
(e. g. BioNLP (Cohen et al, 2011) and DDIExtrac-
tion Shared Task (Segura-Bedmar et al, 2011a)).
Table 3 gives a comparison of RE systems? per-
formances on 5 PPI corpora, determined by doc-
ument level 10-fold cross-validation.8 The use of
dependency parsing-based features increases the
F1 measure by almost 4 pp.
Table 2 shows results of the five best perform-
ing systems on the held out test data set of the
DDI extraction workshop (Segura-Bedmar et al,
2011b). In addition, the result of our system is
shown. Note that the first three systems use ensem-
ble based methods combining the output of several
different systems.
The results presented in Table 2 and 3 give a
performance overview of the RE system used in
distant learning strategies.
4.2 Experimental Setting
To avoid information leakage and biased classifi-
cation, all documents which are contained in the
test corpus are removed. For each experiment we
sample random subsets to reduce processing time.
This allows us to evaluate the impact of different
combinations of subset size and the ratio of related
and non-related (pos/neg) entity pairs, having in
mind the problem of imbalanced datasets (Chawla
et al, 2004). All experiments are performed five
times to reduce the influence of sampling differ-
ent subsets. This leads to more reliable precision,
recall, and F1 values.
8Separating into training and validation sets is performed
on document level, not on instance (entity pair) level. The
latter could lead to an unrealisticallly optimistic estimate
(Van Landeghem et al, 2008)
38
AIMed BioInfer HPRD50 IEPA LLL
P R F1 P R F1 P R F1 P R F1 P R F1
(Airola et al, 2008) 52.9 61.8 56.4 56.7 67.2 61.3 64.3 65.8 63.4 69.6 82.7 75.1 72.5 87.2 76.8
(Kim et al, 2010) 61.4 53.2 56.6 61.8 54.2 57.6 66.7 69.2 67.8 73.7 71.8 72.9 76.9 91.1 82.4
(Fayruzov et al, 2009) 39.0 34.0 56.0 72.0 76.0
(Liu et al, 2010) 54.7 59.8 64.9 62.1 78.1
(Miwa et al, 2009) 55.0 68.8 60.8 65.7 71.1 68.1 68.5 76.1 70.9 67.5 78.6 71.7 77.6 86.0 80.1
(Tikk et al, 2010) 47.5 65.5 54.5 55.1 66.5 60.0 64.4 67 64.2 71.2 69.3 69.3 74.5 85.3 74.5
Our s. (lex) 62.3 46.3 53.1 59.1 54.3 56.6 69.7 69.4 69.6 67.5 73.2 70.2 66.9 84.6 74.7
Our s. (lex+dep) 65.1 48.6 55.7 64.7 57.6 61.0 69.3 69.8 69.5 67.0 72.5 69.7 71.2 86.3 78.0
Table 3: Comparison of fully supervised relations extraction systems for PPI.
Strategy Pairs Positive pairs Sentences
1 3,304,033 511,665 (0.155) 842,339
2 5,560,975 1,389,036 (0.250) 1,172,920
3 2,764,626 359,437 (0.130) 780,658
4 3,454,805 650,455 (0.188) 896,344
Table 4: Statistics of the fours strategies used in distant
supervision for PPI task: 1) IntAct, 2) IntAct + KUPS,
3) IntAct + AIF, 4) IntAct + KUPS + AIF. Ratios are
given in brackets.
4.3 Protein-protein interaction
We explore four strategies to determine the impact
of using additional database knowledge (IntAct
and KUPS) and to test the utility of our novel
condition (AIF).
Table 4 shows the difference in retrieved num-
ber of sentences and protein pairs, including the
percentage of positive examples in the whole data
set. As expected, by using more background know-
ledge, the number of sentences and instances re-
trieved from MEDLINE rises. An increase of both
negative and positive pairs is observed, since a
relevant sentence can have negative pairs along
with the positive ones. After applying additional
interaction knowledge, the fraction of positive ex-
amples (see 3rd column in Table 4) increases from
15.5 % (IntAct) to 25 % (IntAct+KUPS). However,
employment of the AIF condition to both IntAct
and IntAct+KUPS strategies leads to a reduction
of these values (e. g. fraction of positive examples
reduces from 15.5 % to 13 % and from 25 % to
18.8 %).
For simplicity reasons all runs are performed
using only lexical features.
Table 5 shows the average values of distant super-
vision experiments carried out for the PPI task. A
significant correlation between pos/neg ratio and
precision/recall holds. This clearly indicates the
tendency of classifiers to assign more test instances
to the class more often observed during training.
In accordance with their class distribution, AIMed
reaches highest performance in case of lower frac-
tion of positive instances (i. e. 30 % or 40 %), while
for IEPA and LLL the optimal ratio is in favor of
the positive class (i. e. 70 % or 80 %).
Comparative results of the distant learning
strategies IntAct and IntAct+KUPS tested on five
PPI corpora indicate that additional knowledge
bases do not help per se. Supplementary employ-
ment of the KUPS database leads to a drop in
performances seen in four out of five test cases (a
decrease of 1.7 pp in F1 measure is most notably
observed in case of HPRD50). However, introduc-
tion of the novel filtering condition, in both strate-
gies IntAct+AIF and IntAct+KUPS+AIF, shows
a favorable effect on the precision and leads to an
increase of up to 6 pp in F1 measure, compared to
IntAct and IntAct+KUPS.
Applying AIF to the baseline IntAct increases
F1 measure of AIMed and HPRD50 from 34.4 %
to 37.8 % and from 56.1 % to 59.1 %, respectively.
An even larger impact is observed when compar-
ing IntAct+KUPS and IntAct+KUPS+AIF. For
AIMed, HPRD50 and IEPA an increase of around
6 pp is achieved, while F1 measure of BioInfer
and LLL is improved around 3 pp. Table 5 clearly
shows that IntAct+KUPS+AIF is outperforming
other strategies in all five test cases by achiev-
ing F1 measures of 39.0 % for AIMed, 52.0 % for
BioInfer, 60.2 % for HPRD50, 63.4 % for IEPA
and 69.3 % for LLL.
Analysis of the database (IntAct+KUPS) pairs
reveals that in total there are 5,550 (around 10 %)
proteins that interact with themselves, with 4,918
(89 %) originating from the KUPS database. This
indicates a number of instances that represent auto-
interacting proteins which contribute to increase of
false positives. Such proportion where a majority
of them come from KUPS explains the decrease
39
AIMed BioInfer HPRD50 IEPA LLL
Strategy pos/neg P R F1 P R F1 P R F1 P R F1 P R F1
IntAct
30-70 22.3 75.8 34.4 41.7 54.1 46.9 42.6 73.8 53.9 44.6 70.3 54.5 58.9 63.5 61.0
40-60 21.5 83.5 34.2 40.0 61.9 48.5 42.0 81.7 55.5 44.4 78.0 56.6 55.7 73.3 63.2
50-50 20.8 87.0 33.5 38.7 67.1 49.0 41.4 86.9 56.1 43.7 82.2 57.1 54.6 80.7 65.1
60-40 20.0 90.8 32.8 37.3 72.6 49.2 40.5 91.2 56.1 43.2 85.6 57.4 52.4 86.7 65.3
70-30 19.0 94.5 32.1 35.4 79.5 48.9 39.6 93.4 55.6 42.6 89.3 57.7 50.7 92.1 65.4
80-20 18.6 96.8 31.2 33.5 86.5 48.3 38.6 96.2 55.1 42.1 93.3 58.1 49.4 96.7 65.0
IntAct
+
KUPS
30-70 20.6 48.9 29.0 37.5 30.0 33.3 38.6 45.8 41.8 33.1 25.3 28.6 55.3 25.4 34.6
40-60 21.6 70.3 33.0 39.3 47.4 42.9 40.7 70.2 51.5 41.0 49.6 44.9 58.6 49.3 53.2
50-50 20.8 81.6 33.2 38.2 59.4 46.5 39.6 80.4 53.0 42.9 65.3 51.8 58.5 61.1 59.5
60-40 20.0 89.0 32.7 37.0 68.8 48.2 38.9 87.4 53.8 43.4 76.8 55.4 55.2 74.4 63.2
70-30 19.2 94.3 31.9 35.2 79.1 48.7 38.6 92.3 54.4 42.9 86.2 57.2 52.8 88.5 66.1
80-20 18.3 97.5 30.9 32.2 88.6 47.3 37.8 96.1 54.2 41.9 92.7 57.8 50.8 97.0 66.6
IntAct
+
AIF
30-70 25.1 76.7 37.8 42.8 54.1 47.7 45.7 75.7 57.0 49.9 77.2 60.6 58.4 69.5 63.4
40-60 24.5 78.9 37.4 42.3 56.5 48.3 46.1 79.2 58.3 49.2 79.0 60.7 58.2 72.8 64.6
50-50 23.9 81.1 36.9 42.3 59.2 49.2 45.9 83.1 59.1 49 81.6 61.2 57.8 75.5 65.3
60-40 23.1 83.8 36.1 41.8 63.3 50.3 44.9 85.3 58.8 48.4 84.7 61.6 56.8 79.2 66.1
70-30 22.1 85.8 35.2 40.8 66.4 50.5 43.9 86.5 58.2 47.6 87.9 61.8 56.3 82.1 66.7
80-20 21.3 88.3 34.3 39.6 69.9 50.5 42.9 89.8 58.1 46.0 91.6 61.3 54.0 84.9 66.0
IntAct
+
KUPS
+
AIF
30-70 26.6 72.1 38.8 43.8 50.8 47.0 48.1 78.6 59.7 51.1 75.3 60.9 60.2 63.7 61.8
40-60 26.0 77.8 39.0 43.2 55.4 48.5 47.6 82.5 60.4 50.7 80.6 62.2 58.8 68.7 63.3
50-50 25.5 81.6 38.8 44.8 56.2 49.8 46.0 83.9 59.4 51.4 78.7 62.2 60.3 72.2 65.6
60-40 24.6 84.1 38.0 44.5 60.0 51.1 45.6 88.6 60.2 50.6 83.8 63.1 59.4 77.8 67.3
70-30 23.6 86.7 37.1 43.3 64.4 51.8 44.3 90.5 59.5 49.3 88.8 63.4 59.4 83.3 69.3
80-20 22.1 90.4 35.5 41.0 71.3 52.0 42.5 93.4 58.4 46.8 91.8 62.0 56.2 88.2 68.6
Thomas et al (2011b) 22.3 81.3 35.0 38.7 76.0 51.2 45.6 92.9 61.2 42.6 88.3 57.3 53.7 93.3 68.1
Tikk et al (2010) 28.3 86.6 42.6 62.8 36.5 46.2 56.9 68.7 62.2 71.0 52.5 60.4 79.0 57.3 66.4
Our system 34.3 74.0 46.9 70.8 22.5 34.2 63.3 61.3 62.3 70.0 46.0 55.5 82.4 45.7 58.8
Co-occurrence 17.1 100 29.3 26.2 100 41.5 37.6 100 54.7 41.0 100 58.2 49.7 100 66.4
Table 5: Results achieved with lexical features, trained on 10,000 distantly labeled instances and tested on 5 PPI
corpora.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
AIMed BioInfer HPRD50 IEPA LLL DDI
F 1
Co-occurrence
IntAct/DrugBank
IntAct+KUPS
IntAct+AIF
IntAct+KUPS+AIF
Figure 1: Comparison of four distant learning strategies with co-occurrence baseline. ?IntAct/DrugBank? denotes
the database used as source of supervision for PPI corpora and DDI corpus, respectively.
40
of performance in strategy IntAct+KUPS and the
recovery after applying the AIF condition.
The strategy IntAct+KUPS+AIF results in a
higher quality of data used for training and
achieves the best performance in all five test cases
thus proving the effectiveness of the novel condi-
tion. More knowledge is beneficial, but only when
appropriate filtering of the data is applied.
Distantly supervised systems outperform
co-occurrence results for all five PPI corpora.
Considering the best performing strategy
(IntAct+KUPS+AIF), F1 measure of AIMed and
BioInfer, for which we assume to have the most
realistic pos/neg ratio, increased around 10 pp.
HPRD50, IEPA and LLL have an improvement of
5.5 pp, 5.2 pp and 2.9 pp respectively, due to high
fractions of positive instances (leading to a strong
co-occurrence baseline).
Cross-learning9 evaluation may be more realis-
tic to be compared to distant-learning than cross
validation (Airola et al, 2008). For AIMed and
HPRD50 our approach performs on a par with Tikk
et al (2010) or better (up to 6 pp for BioInfer).
4.4 Drug-drug interaction
The problem of drug-drug interactions has not
been previously explored in terms of distant super-
vision. It is noteworthy that DDI corpora are gener-
ated from web documents discussing drug effects
which are in general not contained in MEDLINE.
Hence, this evaluation corpus can be considered as
out-domain and provides additional insights on the
robustness of distant-supervision. The AIF setting
is not evaluated for the DDI task, because only 1
of all 11,335 unique pairs describes a self interac-
tion. In MEDLINE, only 7 sentences with multiple
mentions of this drug (Sulfathiazole, DrugBank
identifier DB06147) are found.
Table 6 gives an overview of the results for dis-
tant supervision on DDI, with the parameter of
size of the training corpus and the pos/neg ratio. A
slight increase in F1 measure can be observed with
additional training instances, both in case of using
just lexical features and when dependency based
features are additionally utilized (e. g. (lex+dep)
from 36.2 % (5k) to 37.3 % (25k) in F1 measure).
Accounting for dependency parsing features
leads to an increase of 0.5 pp in F1 measure, i. e.
from 36.5 % to 37.0 % (10k) and 36.7%? to 37.3 %
9For five PPI corpora: train on four, test on the remaining.
size pos/neg P R F1
5k
30-70 35.4 32.4 33.7
40-60 33.3 37.0 34.9
50-50 31.9 41.7 36.0
50-50 (lex+dep) 32.7 40.7 36.2
60-40 30.1 46.6 36.5
70-30 27.4 51.8 35.7
10k
30-70 36.0 34.4 34.9
40-60 34.2 38.9 36.3
50-50 32.9 41.0 36.5
50-50 (lex+dep) 33.8 41.1 37.0
60-40 30.8 44.8 36.4
70-30 28.2 48.7 35.6
25k
30-70 35.8 35.0 35.3
40-60 34.3 38.6 36.2
50-50 33.2 41.1 36.7
50-50 (lex+dep) 32.5 43.7 37.3
60-40 31.7 42.6 36.3
70-30 28.9 47.2 35.7
Co-occurrence 10.7 100 19.4
Table 6: Results for distant supervision with only lexi-
cal features on the DDI test corpus.
(25k)), the latter being our best result obtained for
weakly supervised DDI.
Compared to co-occurence, a gain of around
18 pp is achieved. Taking into account the high
class imbalance of the DDI test set (see Table 1),
which is most similar to AIMed corpus, the F1
measure of 37.3 % is encouraging.
Figure 1 shows the results of PPI and DDI experi-
ments in addition. The error bars denote the stan-
dard deviation over 5 differently sampled training
corpora.
5 Discussion
This paper presents the application of distant su-
pervision on the task to find protein-protein inter-
actions and drug-drug interactions. The first is
addressed using the databases IntAct and KUPS,
the second using DrugBank.
More database knowledge does not necessar-
ily have a positive impact on a trained model, ap-
propriate instance selection methods need to be
applied. This is demonstrated with the KUPS
database and the automatic curation via auto-
interaction filtering leading to state-of-the-art re-
sults for weakly supervised protein-protein inter-
action detection.
We present the first results of applying the dis-
tant supervision paradigm to drug-drug-interaction.
41
The results may seem comparatively limited in
comparison to protein-protein interaction, but are
encouraging when taking into account the imbal-
ance of the test corpus and its differing source
domain.
Future development of noise reduction ap-
proaches is important to make use of the full poten-
tial of available database knowledge. The results
shown are encouraging that manual annotation of
corpora can be avoided in other application areas
as well. Another future direction is the investiga-
tion of specifically difficult structures, e. g. listings
and enumerations of entities in a sentence.
Acknowledgments
We would like to thank the reviewers for their
valuable feedback. Thanks to Sumit Madan and
Theo Mevissen for fruitful discussions. T. Bobic?
was partially funded by the Bonn-Aachen Inter-
national Center for Information Technology (B-
IT) Research School. P. Thomas was funded by
the German Federal Ministry of Education and
Research (grant No 0315417B). R. Klinger was
partially funded by the European Community?s
Seventh Framework Programme [FP7/2007-2011]
under grant agreement no. 248726. We acknowl-
edge financial support provided by the IMI-JU,
grant agreement no. 115191 (Open PHACTS).
References
A. Airola, S. Pyysalo, J. Bjo?rne, T. Pahikkala, F. Ginter,
and T. Salakoski. 2008. All-paths Graph Kernel for
Protein-protein Interaction Extraction with Evalua-
tion of Cross-corpus Learning. BMC Bioinformatics,
9(Suppl 11):S2.
J. Bjo?rne, A. Airola, T. Pahikkala, and T. Salakoski.
2011. Drug-drug interaction extraction with RLS
and SVM classiffers. In Challenge Task on Drug-
Drug Interaction Extraction, pages 35?42.
K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and
J. Taylor. 2008. Freebase: a collaboratively created
graph database for structuring human knowledge. In
SIGMOD.
R. C. Bunescu and R. J. Mooney. 2005. A shortest
path dependency kernel for relation extraction. In
HLT and EMNLP.
R. C. Bunescu, R. Ge, R. J. Kate, E. M. Marcotte, R. J.
Mooney, A. K. Ramani, and Y. Wah Wong. 2005.
Comparative experiments on learning information
extractors for proteins and their interactions. Artif
Intell Med, 33(2):139?155, Feb.
E. Buyko, E. Beisswanger, and U. Hahn. 2012. The ex-
traction of pharmacogenetic and pharmacogenomic
relations?a case study using pharmgkb. PSB, pages
376?387.
A. Chatr-aryamontri, A. Ceol, L. M. Palazzi,
G. Nardelli, M.V. Schneider, L. Castagnoli, and
G. Cesareni. 2007. MINT: the Molecular INTer-
action database. Nucleic Acids Res, 35(Database
issue):D572?D574.
N. V. Chawla, N Japkowicz, and A. Kotcz. 2004. Ed-
itorial: special issue on learning from imbalanced
data sets. SIGKDD Explor. Newsl., 6:1?6.
X. Chen, J. C. Jeong, and P. Dermyer. 2010.
KUPS: constructing datasets of interacting and non-
interacting protein pairs with associated attributions.
Nucleic Acids Res, 39(Database issue):D750?D754.
F. M. Chowdhury and A. Lavelli. 2011. Drug-drug
interaction extraction using composite kernels. In
Challenge Task on Drug-Drug Interaction Extrac-
tion, pages 27?33.
F. M. Chowdhury, A. B. Abacha, A. Lavelli, and
P. Zweigenbaum. 2011. Two different machine
learning techniques for drug-drug interaction extrac-
tion. In Challenge Task on Drug-Drug Interaction
Extraction, pages 19?26.
K. B. Cohen, D. Demner-Fushman, S. Ananiadou,
J. Pestian, J. Tsujii, and B. Webber, editors. 2011.
Proceedings of the BioNLP.
J. De Las Rivas and C. Fontanillo. 2010. Protein-
protein interactions essentials: key concepts to build-
ing and analyzing interactome networks. PLoS Com-
put Biol, 6:e1000807+.
J. Ding, D. Berleant, D. Nettleton, and E. Wurtele.
2002. Mining MEDLINE: abstracts, sentences, or
phrases? Pac Symp Biocomput, pages 326?337.
E. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin.
2008. LIBLINEAR: A Library for Large Linear
Classification. Machine Learning Research, 9:1871?
1874.
T. Fayruzov, M. De Cock, C. Cornelis, and V. Hoste.
2009. Linguistic feature analysis for protein interac-
tion extraction. BMC Bioinformatics, 10(1):374.
J. Fluck, H. T. Mevissen, H. Dach, M. Oster, and
M. Hofmann-Apitius. 2007. ProMiner: Recognition
of Human Gene and Protein Names using regularly
updated Dictionaries. In BioCreative 2, pages 149?
151.
K. Fundel, R. Kuffner, and R. Zimmer. 2007. Relex-
relation extraction using dependency parse trees.
Bioinformatics, 23(3):365?371.
L. Gong, R. P. Owen, W. Gor, R. B. Altman, and T. E.
Klein. 2008. PharmGKB: an integrated resource of
pharmacogenomic data and knowledge. Curr Protoc
Bioinformatics, Chapter 14:Unit14.7.
D. Hanisch, K. Fundel, H. T. Mevissen, R. Zimmer,
and J. Fluck. 2005. ProMiner: rule-based protein
and gene entity recognition. BMC Bioinformatics,
6(Suppl 1):S14.
42
S. Kerrien, B. Aranda, L. Breuza, A. Bridge,
F. Broackes-Carter, C. Chen, M. Duesbury, M. Du-
mousseau, M. Feuermann, U. Hinz, C. Jandrasits,
R.C. Jimenez, J. Khadake, U. Mahadevan, P. Masson,
I. Pedruzzi, E. Pfeiffenberger, P. Porras, A. Raghu-
nath, B. Roechert, S. Orchard, and H. Hermjakob.
2012. The IntAct molecular interaction database in
2012. Nucleic Acids Res, 40:D841?D846.
S. Kim, J. Yoon, J. Yang, and S. Park. 2010. Walk-
weighted subsequence kernels for protein-protein
interaction extraction. BMC Bioinformatics, 11:107.
C. Knox, V. Law, T. Jewison, P. Liu, S. Ly, A. Frolkis,
A. Pon, K. Banco, C. Mak, V. Neveu, Y. Djoum-
bou, R. Eisner, A. Chi Guo, and D.S Wishart. 2011.
Drugbank 3.0: a comprehensive resource for ?omics?
research on drugs. Nucleic Acids Res, 39(Database
issue):D1035?D1041.
Y. Li, X. Hu, H. Lin, and Z. Yang. 2010. Learning
an enriched representation from unlabeled data for
protein-protein interaction extraction. BMC Bioin-
formatics, 11(Suppl 2):S7.
B. Liu, L. Qian, H. Wang, and G. Zhou. 2010.
Dependency-driven feature-based learning for ex-
tracting protein-protein interactions from biomedical
text. In COLING, pages 757?765.
M. C. De Marneffe, B. Maccartney, and C. D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC.
A. L. Minard, L. Makour, A. L. Ligozat, and B. Grau.
2011. Feature Selection for Drug-Drug Interac-
tion Detection Using Machine-Learning Based Ap-
proaches. In Challenge Task on Drug-Drug Interac-
tion Extraction, pages 43?50.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009.
Distant supervision for relation extraction without
labeled data. In ACL-IJCNLP, pages 1003?1011.
M. Miwa, R. Saetre, Y. Miyao, and J. Tsujii. 2009.
A Rich Feature Vector for Protein-Protein Interac-
tion Extraction from Multiple Corpora. EMNLP,
1(1):121?130.
M. Miwa, R. Saetre, J. D. Kim, and J. Tsujii. 2010.
Event extraction with complex event classification
using rich features. J Bioinform Comput Biol,
8(1):131?146.
C. Nedellec. 2005. Learning language in logic-genic
interaction extraction challenge. In Proc. of the
ICML05 workshop: Learning Language in Logic
(LLL?05), volume 18, pages 97?99.
T. S. Prasad, R. Goel, K. Kandasamy, S. Keerthiku-
mar, S. Kumar, S. Mathivanan, D. Telikicherla,
R. Raju, B. Shafreen, A. Venugopal, L. Balakrish-
nan, A. Marimuthu, S. Banerjee, D. S. Somanathan,
A. Sebastian, S. Rani, S. Ray, C. J. Kishore, S. Kanth,
M. Ahmed, M. K. Kashyap, R. Mohmood, Y. L.
Ramachandra, V. Krishna, B. A.Rahiman, S. Mo-
han, P. Ranganathan, S. Ramabadran, R. Chaerkady,
and A. Pandey. 2009. Human Protein Refer-
ence Database?2009 update. Nucleic Acids Res,
37(Database issue):D767?D772.
S. Pyysalo, F. Ginter, J. Heimonen, J. Bjo?rne, J. Boberg,
J. Ja?rvinen, and T. Salakoski. 2007. Bioinfer: A
corpus for information extraction in the biomedical
domain. BMC Bioinformatics, 8(50).
S. Pyysalo, A. Airola, J. Heimonen, J. Bjo?rne, F. Gin-
ter, and T. Salakoski. 2008. Comparative analysis
of five protein?protein interaction corpora. BMC
Bioinformatics, 9 Suppl 3:S6.
S. Riedel, L. Yao, and A. McCallum. 2010. Modeling
Relations and Their Mentions without Labeled Text.
In ECML PKDD.
I. Segura-Bedmar, P. Mart??nez, and D. Sanchez-
Cisneros, editors. 2011a. Proceedings of the 1st
Challenge Task on Drug-Drug Interaction Extrac-
tion.
I. Segura-Bedmar, P. Mart??nez, and D. Sanchez-
Cisneros. 2011b. The 1st DDIExtraction-2011 chal-
lenge task: Extraction of Drug-Drug Interactions
from biomedical texts. In Challenge Task on Drug-
Drug Interaction Extraction 2011, pages 1?9.
P. Thomas, M. Neves, I. Solt, D. Tikk, and U. Leser.
2011a. Relation Extraction for Drug-Drug Interac-
tions using Ensemble Learning. In Challenge Task
on Drug-Drug Interaction Extraction, pages 11?18.
P. Thomas, I. Solt, R. Klinger, and U. Leser. 2011b.
Learning Protein Protein Interaction Extraction us-
ing Distant Supervision. In Robust Unsupervised
and Semi-Supervised Methods in Natural Language
Processing, pages 34?41.
D. Tikk, P. Thomas, P. Palaga, J. Hakenberg, and
U. Leser. 2010. A comprehensive benchmark of ker-
nel methods to extract protein-protein interactions
from literature. PLoS Comput Biol, 6:e1000837.
S. Van Landeghem, Y. Saeys, B. De Baets, and
Y. Van de Peer. 2008. Extracting protein-protein
interactions from text using rich feature vectors and
feature selection. SMBM, pages 77?84.
A. Vlachos, P. Buttery, D. O? Se?aghdha, and T. Briscoe.
2009. Biomedical Event Extraction without Training
Data. In BioNLP, pages 37?40.
L. Yao, S. Riedel, and A. McCallum. 2010. Collec-
tive Cross-Document Relation Extraction Without
Labeled Data. In EMNLP.
43
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 42?49,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
An Impact Analysis of Features in a Classification Approach to
Irony Detection in Product Reviews
Konstantin Buschmeier, Philipp Cimiano and Roman Klinger
Semantic Computing Group
Cognitive Interaction Technology ? Center of Excellence (CIT-EC)
Bielefeld University
33615 Bielefeld, Germany
kbuschme@techfak.uni-bielefeld.de
{rklinger,cimiano}@cit-ec.uni-bielefeld.de
Abstract
Irony is an important device in human com-
munication, both in everyday spoken con-
versations as well as in written texts includ-
ing books, websites, chats, reviews, and
Twitter messages among others. Specific
cases of irony and sarcasm have been stud-
ied in different contexts but, to the best of
our knowledge, only recently the first pub-
licly available corpus including annotations
about whether a text is ironic or not has
been published by Filatova (2012). How-
ever, no baseline for classification of ironic
or sarcastic reviews has been provided.
With this paper, we aim at closing this gap.
We formulate the problem as a supervised
classification task and evaluate different
classifiers, reaching an F
1
-measure of up to
74 % using logistic regression. We analyze
the impact of a number of features which
have been proposed in previous research as
well as combinations of them.
1 Introduction
Irony is often understood as ?the use of words that
mean the opposite of what you really think espe-
cially in order to be funny? or ?a situation that
is strange or funny because things happen in a
way that seems to be the opposite? of what is ex-
pected.
1
Many dictionaries make this difference
between verbal irony and situational irony (British
Dictionary, 2014; New Oxford American Dictio-
nary, 2014; Merriam Webster Dictionary, 2014).
1
as defined in the Merriam Webster Dictionary
(2014), http://www.merriam-webster.com/
dictionary/irony
The German Duden (2014) mentions sarcasm as
synonym to irony, while the comprehension of sar-
casm as a special case of irony might be more
common. For instance, the Merriam Webster Dic-
tionary (2014) defines sarcasm as ?a sharp and
often satirical or ironic utterance designed to cut or
give pain?.
2
Irony is a frequent phenomenon within human
communication, occurring both in spoken and writ-
ten discourse including books, websites, fora, chats,
Twitter messages, Facebook posts, news articles
and product reviews. Even for humans it is some-
times difficult to recognize irony. Irony markers
are thus often used in human communication, sup-
porting the correct interpretation (Attardo, 2000).
The automatic identification of ironic formulations
in written text is a very challenging as well as im-
portant task as shown by the comment
3
?Read the book!?
which in the context of a movie review could be
regarded as ironic and as conveying the fact that the
film was far worse compared to the book. Another
example is taken from a review for the book ?Great
Expectations? by Charles Dickens:
4
?i would recomend this book to friends
who have insomnia or those who i abso-
lutely despise.?
The standard approach of recommending X implies
that X is worthwhile is clearly not valid in the given
context as the author is stating that she disliked the
book.
2
http://www.merriam-webster.com/
dictionary/sarcasm, accessed April 28, 2014
3
Example from Lee (2009).
4
http://www.amazon.com/review/
R86RAMEBZSB11, access date March 10, 2014
42
In real world applications of sentiment analysis,
large data sets are automatically classified into pos-
itive statements or negative statements and such
output is used to generate summaries of the sen-
timent about a product. In order to increase the
accurateness of such systems, ironic or sarcastic
statements need to be identified in order to infer
the actual communicative intention of the author.
In this paper, we are concerned with approaches
for the automatic detection of irony in texts, which
is an important task in a variety of applications,
including the automatic interpretation of text-based
chats, computer interaction or sentiment analysis
and opinion mining. In the latter case, the detec-
tion is of outmost importance in order to correctly
assign a polarity score to an aspect of a reviewed
product or a person mentioned in a Twitter mes-
sage. In addition, the automatic detection of irony
or sarcasm in text requires an operational definition
and has therefore the potential to contribute to a
deeper understanding of the linguistic properties
of irony and sarcasm as linguistic phenomena and
their corpus based evaluation and verification.
The rest of this paper is structured as follows:
We introduce the background and theories on irony
in Section 1.1 and discuss previous work in the area
of automatically recognizing irony in Section 1.2.
In the methods part in Section 2, we present our
set of features (Section 2.1) and the classifiers we
take into account (Section 2.2). In Section 3, we
discuss the data set used in this work in more detail
(Section 3.1), present our experimental setting (Sec-
tion 3.2) and show the evaluation of our approach
(Section 3.3). We conclude with a discussion and
summary (Section 4) and with an outlook on possi-
ble future work (Section 5).
1.1 Background
Irony is an important and frequent device in human
communication that is used to convey an attitude
or evaluation towards the propositional content of a
message, typically in a humorous fashion (Abrams,
1957, p. 165?168). Between the age of six (Nakas-
sis and Snedeker, 2002) and eight years (Creusere,
2007), children are able to recognize ironic utter-
ances or at least notice that something in the sit-
uation is not common (Glenwright and Pexman,
2007). The principle of inferability (Kreuz, 1996)
states that figurative language is used if the speaker
is confident that the addressee will interpret the
utterance and infer the communicative intention
of the speaker/author correctly. It has been shown
that irony is ubiquitous, with 8 % of the utterances
exchanged between interlocutors that are familiar
with each other being ironic (Gibbs, 2007).
Utsumi (1996) claim that an ironic utterance can
only occur in an ironic environment, whose pres-
ence the utterance implicitly communicates. Given
the formal definition it is possible to computation-
ally resolve if an utterance is ironic using first-order
predicate logic and situation calculus. Different the-
ories such as the echoic account (Wilson and Sper-
ber, 1992), the Pretense Theory (Clark and Gerrig,
1984) or the Allusional Pretense Theory (Kumon-
Nakamura et al., 1995) have challenged the un-
derstanding that an ironic utterance typically con-
veys the opposite of its literal propositional content.
However, in spite of the fact that the attributive
nature of irony is widely accepted (see Wilson and
Sperber (2012)), no formal or operational definition
of irony is available as of today.
1.2 Previous Work
Corpora providing annotations as to whether ex-
pressions are ironic or not are scarce. Kreuz and
Caucci (2007) have automatically generated such
a corpus exploiting Google Book search
5
. They
collected excerpts containing the phrase ?said sar-
castically?, removed that phrase and performed a
regression analysis on the remaining text, exploit-
ing the number of words as well as the occurrence
of adjectives, adverbs, interjections, exclamation
and question marks as features.
Tsur et al. (2010) present a system to identify
sarcasm in Amazon product reviews exploiting fea-
tures such as sentence length, punctuation marks,
the total number of completely capitalized words
and automatically generated patterns which are
based on the occurrence frequency of different
terms (following the approach by Davidov and
Rappoport (2006)). Unfortunately, their corpus
is not publicly available. Carvalho et al. (2009) use
eight patterns to identify ironic utterances in com-
ments on articles from a Portuguese online newspa-
per. These patterns contain positive predicates and
utilize punctuation, interjections, positive words,
emoticons, or onomatopoeia and acronyms for
laughing as well as some Portuguese-specific pat-
terns considering the verb-morphology. Gonz?alez-
Ib?a?nez et al. (2011) differentiate between sarcastic
and positive or negative Twitter messages. They
5
http://books.google.de/
43
exploit lexical features like unigrams, punctuation,
interjections and dictionary-based as well as prag-
matic features including references to other users
in addition to emoticons. Reyes et al. (2012) distin-
guish ironic and non-ironic Twitter messages based
on features at different levels of linguistic analysis
including quantifiers of sentence complexity, struc-
tural, morphosyntactic and semantic ambiguity, po-
larity, unexpectedness, and emotional activation,
imagery, and pleasantness of words. Tepperman
et al. (2006) performed experiments to recognize
sarcasm in spoken language, specifically in the ex-
pression ?yeah right?, using spectral, contextual
and prosodic cues. On the one hand, their results
show that it is possible to identify sarcasm based on
spectral and contextual features and, on the other
hand, they confirm that prosody is insufficient to
reliably detect sarcasm (Rockwell, 2005, p. 118).
Very recently, Filatova (2012) published a prod-
uct review corpus from Amazon, being annotated
with Amazon Mechanical Turk. It contains 437
ironic and 817 non-ironic reviews. A more de-
tailed description of this resource can be found in
Section 3.1. To our knowledge, no automatic classi-
fication approach has been evaluated on this corpus.
We therefore contribute a text classification system
including the previously mentioned features. Our
results serve as a strong baseline on this corpus as
well as an ?executable review? of previous work.
6
2 Methods
We model the task of irony detection as a super-
vised classification problem in which a review is
categorized as being ironic or non-ironic. We inves-
tigate different classifiers and focus on the impact
analysis of different features by investigating what
effect their elimination has on the performance of
the approach. In the following, we describe the
features used and the set of classifiers compared.
2.1 Features
To estimate if a review is ironic or not, we measure
a set of features. Following the idea that irony is
expressing the opposite of its literal content, we
take into account the imbalance between the over-
all (prior) polarity of words in the review and the
star-rating (as proposed by Davidov et al. (2010)).
We assume the imbalance to hold if the star-rating
6
The system as implemented to perform the described
experiments is made available at https://github.com/
kbuschme/irony-detection/
is positive (i. e., 4 or 5 stars) but the majority of
words is negative, and, vice versa, if the star-rating
is negative (i. e., 1 or 2 stars) but occurs with a
majority of positive words. We refer to this feature
as Imbalance. The polarity of words is determined
based on a dictionary consisting of about 6,800
words with their polarity (Hu and Liu, 2004).
7
The feature Hyperbole (Gibbs, 2007) indicates
the occurrence of a sequence of three positive or
negative words in a row. Similarly, the feature
Quotes indicates that up to two consecutive adjec-
tives or nouns in quotation marks have a positive
or negative polarity.
The feature Pos/Neg&Punctuation indicates that
a span of up to four words contains at least one
positive (negative) but no negative (positive) word
and ends with at least two exclamation marks or a
sequence of a question mark and an exclamation
mark (Carvalho et al., 2009). Analogously, the fea-
ture Pos/Neg&Ellipsis indicates that such a positive
or negative span ends with an ellipsis (?. . . ?). El-
lipsis and Punctuation indicates that an ellipsis is
followed by multiple exclamation marks or a com-
bination of an exclamation and a question mark.
The Punctuation feature conveys the presence of
an ellipses as well as multiple question or excla-
mation marks or a combination of the latter two.
The Interjection feature indicates the occurrence of
terms like ?wow? and ?huh?, and Laughter mea-
sures onomatopoeia (?haha?) as well as acronyms
for grin or laughter (?*g*?, ?lol?). In addition, the
feature Emoticon indicates the occurrence of an
emoticon. In order to capture a range of emotions,
it combines a variety of emoticons such as happy,
laughing, winking, surprised, dissatisfied, sad, cry-
ing, and sticking tongue out. In addition, we use
each occurring word as a feature (bag-of-words).
All together, we have 21,773 features. The num-
ber of specific features (i. e., without bag-of-words)
alone is 29.
2.2 Classifiers
In order to perform the classification based on the
features mentioned above, we explore a set of stan-
dard classifiers typically used in text classification
research. We employ the open source machine
learning library scikit-learn (Pedregosa et al., 2011)
for Python.
7
Note that examples can show that this is not always the
case. Funny or odd products ironically receive a positive star-
rating. However, this feature may be a strong indicator for
irony.
44
We use a support vector machine (SVM, Cortes
and Vapnik (1995)) with a linear kernel in the im-
plementation provided by libSVM (Fan et al., 2005;
Chang and Lin, 2011). The na??ve Bayes classifier is
employed with a multinomial prior (Zhang, 2004;
Manning et al., 2008). This classifier might suffer
from the issue of over-counting correlated features,
such that we compare it to the logistic regression
classifier as well (Yu et al., 2011).
Finally, we use a decision tree (Breiman et al.,
1984; Hastie et al., 2009) and a random forest clas-
sifier (Breiman, 2001).
3 Experiments and Results
3.1 Data Set
The data set by Filatova (2012) consists of 1,254
Amazon reviews, of which 437 are ironic, i. e.,
contain situational irony or verbal irony, and
817 are non-ironic. It has been acquired using
the crowd sourcing platform Amazon Mechanical
Turk
8
. Note that Filatova (2012) interprets sarcasm
as being verbal irony.
In a first step, the workers were asked to find
pairs of reviews on the same product so that one
of the reviews is ironic while the other one is not.
They were then asked to submit the ID of both
reviews, and, in the case of an ironic review, to
provide the fragment conveying the irony.
In a second step, each collected review was an-
notated by five additional workers and remained
in the corpus if three of the five new annotators
concurred with the initial category, i. e., ironic or
non-ironic. The corpus contains 21,744 distinct
tokens
9
, of which 5,336 occur exclusively in ironic
reviews, 9,468 exclusively in non-ironic reviews,
and the remaining 6,940 tokens occur in both ironic
and non-ironic reviews. Thus, all ironic reviews
comprise a total of 12,276 distinct tokens, whereas
a total of 16,408 distinct tokens constitute all non-
ironic reviews. On average, a single review consists
of 271.9 tokens, a single ironic review of an aver-
age of 261.4 and a single non-ironic review of an
average of 277.5 tokens. The distribution of ironic
and non-ironic reviews for the different star-ratings
is shown in Table 2. Note that this might be a result
of the specific annotation procedure applied by the
8
https://www.mturk.com/mturk/, accessed on
March 10, 2014
9
Using the TreeBankWordTokenizer as implemented in the
Natural Language Toolkit (NLTK) (http://www.nltk.
org/)
annotators to search for ironic reviews. Neverthe-
less, this motivates a simple baseline system which
just takes one feature into account: the numbers of
stars assigned to the respective review (?Star-rating
only?).
3.2 Experimental Settings
We run experiments for three baselines: The star-
rating baseline relies only on the number of stars
assigned in the review as a feature. The bag-of-
words baseline exploits only the unigrams in the
text as features. The sentiment word count only
uses the information whether the number of posi-
tive words in the text is larger than the number of
negative words.
We emphasize that the first baseline is only of
limited applicability as it requires the explicit avail-
ability of a star-rating. The second baseline relies
on standard text classification features that are not
specific for the task. The third baseline relies on a
classical feature used in sentiment analysis, but is
not specific for irony detection.
We refer to the feature set ?All? encompassing
all features described in Section 2.1, including bag-
of-words and the set ?Specific Features?.
In order to understand the impact of a specific
feature A, we run three sets of experiments:
? Using all features with the exception of A.
? Using all specific features with the exception
of A.
? Using A as the only feature.
In addition to evaluating each single feature as
described above, we evaluate the set of positive and
negative instantiations of features when using the
sentiment dictionary. The ?Positive set? and ?Neg-
ative set? take into account the respective subsets
of all specific features.
Each experiment is performed in a 10-fold cross-
validation setting on document level. We report
recall, precision and F
1
-measure for each of the
classifiers.
3.3 Evaluation
Table 1 shows the results for the three baselines and
different feature set combinations, all for the differ-
ent classifiers. The star-rating as a feature alone is a
very strong indicator for irony. However, this result
is of limited usefulness as it only regards reviews
of a specific rating as ironic, namely results with
45
Linear SVM Logistic Regression Decision Tree Random Forest Naive Bayes
Feature set R. P. F
1
R. P. F
1
R. P. F
1
R. P. F
1
R. P. F
1
Star-rating only 66.7 78.4 71.7 66.7 78.4 71.7 66.7 78.4 71.7 66.7 78.4 71.7 66.7 78.4 71.7
BOW only 61.8 67.2 64.1 63.3 76.0 68.8 53.8 53.4 53.4 21.7 70.4 32.9 48.1 77.4 59.1
Sentiment Word Count 57.3 59.4 58.1 57.3 59.4 58.1 57.3 59.4 58.1 57.3 59.4 58.1 0.0 100.0 0.0
All + Star-rating 69.0 74.4 71.3 68.9 81.7 74.4 71.7 73.2 72.2 34.0 85.0 48.2 55.3 79.7 65.0
All (= Sp. Features + BOW) 61.3 68.0 64.3 62.2 75.2 67.8 55.0 59.8 56.9 24.1 73.2 35.3 50.9 77.3 61.2
All ? Imbalance 62.4 67.1 64.4 62.5 75.0 67.9 53.0 54.3 53.3 22.3 75.9 33.8 47.8 75.8 58.4
All ? Hyperbole 61.3 68.0 64.3 62.2 75.2 67.8 57.1 61.5 58.9 22.3 79.6 34.4 50.9 77.3 61.2
All ? Quotes 61.3 68.0 64.3 62.8 75.1 68.2 57.2 61.7 59.1 25.9 76.8 38.5 50.6 77.0 60.9
All ? Pos/Neg&Punctuation 61.5 67.9 64.4 62.4 75.2 68.0 56.7 60.1 58.0 21.8 77.8 33.5 50.9 77.3 61.2
All ? Pos/Neg&Ellipsis 61.0 67.4 63.8 63.0 75.1 68.3 57.6 60.5 58.8 29.0 79.2 42.2 50.4 76.6 60.7
All ? Ellipsis and Punctuation 61.3 68.0 64.3 62.4 75.2 68.0 55.1 59.7 56.9 24.6 73.6 36.2 50.9 77.3 61.2
All ? Punctuation 61.8 67.9 64.5 62.5 74.9 67.8 56.1 61.2 58.3 28.6 78.1 41.5 50.2 76.7 60.6
All ? Injections 61.3 68.0 64.3 62.2 75.0 67.8 56.1 61.8 58.5 24.1 75.2 35.6 50.9 77.3 61.2
All ? Laughter 61.3 68.2 64.4 62.4 75.3 68.0 56.6 60.9 58.2 24.0 79.3 36.5 50.9 77.3 61.2
All ? Emoticons 61.3 68.2 64.4 62.6 75.3 68.1 57.7 60.2 58.6 24.3 76.5 36.7 50.9 77.3 61.2
All ? Negative set 61.0 68.0 64.1 62.3 74.7 67.7 59.0 61.1 59.7 25.4 76.8 37.6 50.2 76.6 60.5
All ? Positive set 62.6 67.3 64.6 62.5 75.7 68.2 53.7 55.1 54.2 20.5 67.7 31.1 47.8 75.8 58.4
Sp. Features 37.5 77.2 50.2 38.2 77.5 50.8 38.3 76.0 50.6 38.3 74.8 50.2 34.3 80.5 47.7
Sp. Features ? Imbalance 9.3 50.4 15.4 11.0 54.1 18.1 11.3 48.5 18.1 12.9 47.4 20.0 5.9 55.8 10.3
Sp. Features ? Hyperbole 37.5 77.4 50.3 38.2 77.5 50.8 38.3 76.7 50.7 38.8 76.4 51.2 34.3 80.9 47.8
Sp. Features ? Quotes 37.7 76.9 50.3 38.0 78.1 50.7 37.8 75.6 50.1 38.3 73.6 50.0 34.3 80.5 47.7
Sp. Features ? Pos/Neg&Punctuation 37.7 77.9 50.5 37.8 77.6 50.5 37.1 74.5 49.2 38.2 73.8 49.9 33.3 80.2 46.7
Sp. Features ? Pos/Neg&Ellipsis 37.7 77.3 50.4 38.1 78.2 50.9 37.9 76.2 50.4 39.1 72.3 50.3 34.5 79.7 47.8
Sp. Features ? Ellipsis and Punctuation 37.8 76.9 50.3 37.8 76.9 50.3 38.3 75.8 50.6 39.0 72.5 50.5 34.5 80.2 47.9
Sp. Features ? Punctuation 37.1 79.7 50.3 37.6 78.7 50.6 37.0 76.7 49.6 38.4 75.4 50.5 32.6 78.9 45.6
Sp. Features ? Interjections 37.7 76.9 50.3 37.9 77.5 50.6 38.1 76.1 50.4 38.7 75.2 50.7 34.3 80.5 47.7
Sp. Features ? Laughter 37.8 77.3 50.5 38.0 77.7 50.7 37.3 75.5 49.6 37.5 73.4 49.4 34.5 81.2 48.0
Sp. Features ? Emoticons 37.3 78.2 50.2 38.2 77.5 50.8 38.0 75.4 50.2 38.7 75.0 50.7 33.4 80.7 46.8
Sp. Features ? Positive set 10.5 48.7 17.1 11.0 56.3 18.1 9.9 49.3 16.3 12.3 50.8 19.5 6.3 64.8 11.0
Sp. Features ? Negative set 37.7 78.2 50.6 38.0 78.7 50.9 38.2 75.1 50.3 37.6 72.0 48.9 34.9 79.8 48.3
Imbalance only 36.9 81.4 50.4 36.9 81.4 50.4 36.9 81.4 50.4 36.9 81.4 50.4 0.0 100.0 0.0
Hyperbole only 0.0 80.0 0.0 0.0 90.0 0.0 0.0 80.0 0.0 0.2 55.0 0.4 0.0 100.0 0.0
Quotes only 3.9 45.5 7.0 0.9 67.0 1.7 4.0 43.8 7.0 2.5 52.2 4.5 0.0 100.0 0.0
Pos/Neg&Punctuation only 0.9 90.0 1.8 0.5 90.0 0.9 0.0 90.0 0.0 0.4 90.0 0.8 0.9 90.0 1.8
Pos/Neg&Ellipsis only 6.8 59.0 12.1 6.8 59.0 12.1 6.8 59.0 12.1 6.8 59.0 12.1 0.0 100.0 0.0
Ellipsis and Punctuation only 0.9 90.0 1.7 0.4 90.0 0.8 0.9 90.0 1.7 0.9 90.0 1.7 0.0 100.0 0.0
Punctuation only 5.4 64.6 9.8 5.4 64.6 9.8 3.3 60.8 6.2 4.0 60.8 7.5 4.7 64.6 8.6
Interjections only 0.5 75.8 0.9 0.3 82.5 0.5 0.5 75.8 0.9 1.4 74.2 2.7 0.0 100.0 0.0
Laughter only 0.0 100.0 0.0 0.0 100.0 0.0 0.0 100.0 0.0 0.0 80.0 0.0 0.0 100.0 0.0
Emoticons only 0.0 100.0 0.0 0.0 100.0 0.0 0.0 100.0 0.0 0.0 100.0 0.0 0.0 80.0 0.0
Positive set only 36.9 81.4 50.4 36.9 81.1 50.4 37.1 80.5 50.5 37.3 79.3 50.5 32.4 80.7 45.6
Negative set only 8.2 54.5 14.1 7.3 48.8 12.5 8.8 49.4 14.8 9.0 49.9 15.2 0.0 80.0 0.0
Table 1: Comparison of different classification methods using different feature sets. ?All? refers to the
features described in Section 2 including bag-of-words (?BOW?). ?Sp. Features? are ?All? without
?BOW?.
a positive rating by the author, as explained by Ta-
ble 2, which shows the more real-world compatible
result of a rich feature set in addition. Obviously,
the depicted distribution is very similar to the dis-
tribution of the manually annotated data set, which
can obviously not be achieved by the star-rating
feature alone.
The best result is achieved by using the star-
rating together with bag-of-words and specific fea-
tures with a logistic regression approach (leading
to an F
1
-measure of 74 %). The SVM and decision
tree have a comparable performance on the task,
which is albeit lower compared to the performance
of the logistic regression approach.
Using the task-agnostic pure bag-of-words ap-
proach leads to a performance of 68.8 % for logistic
regression; this classifier has the property of deal-
ing well with correlated features and the additional
specific features cannot contribute positively to the
result. Similarly, the F
1
-measure of 64.1 % pro-
duced by the SVM cannot be increased by includ-
ing additional features. In contrast, a positive im-
pact of additional features can be observed for the
decision tree in the case that specific features are
combined with bag-of-word-based features, reach-
ing close to 59 % F
1
in comparison to 53.4 % F
1
for bag-of-words alone.
It would be desirable to have a model only or
mainly based on the problem-specific features, as
this leads to a much more compact and therefore ef-
46
ficient representation than taking all words into ac-
count. In addition, the model would be easier to un-
derstand. By exploiting task-specific features alone,
the performance reaches at most an F
1
-measure of
50.9 %, which shows that task-agnostic features
such as unigram features are needed. A significant
drop in performance when leaving out a feature
or feature set can be observed for the Imbalance
feature and the Positive set. Both these feature sets
take into account the star-rating.
The task-specific features alone yield high preci-
sion results at the expense of a very low recall. This
clearly shows that task-specific features should
be used with standard, task-independent features
(the bag-of-words). The most helpful task-specific
features are: Imbalance, Positive set, Quotes and
Pos/Neg&Ellipses.
4 Discussion and Summary
The best performance is achieved with very corpus-
specific features taking into account meta-data
from Amazon, namely the product rating of the
reviewer. This leads to an F
1
-measure of 74 %.
However, we could not show a competitive perfor-
mance with more problem-specific features (lead-
ing to 51 % F
1
) or in combination with bag-of-
word-based features (leading to 68 % F
1
).
The baseline only predicting based on the star-
rating itself is highly competitive, however, not
applicable to texts without meta-data and of lim-
ited use due to its naturally highly biased outcome
towards positive reviews being non-ironic and neg-
ative reviews being ironic. Our results show that
the best results are achieved via meta-data and it re-
mains an open research task to develop comparably
good approaches only based on text features.
It should be noted that the corpus used in this
Distribution
Corpus Predicted
Rating ironic non-ironic ironic non-ironic
5 114 605 126 593
4 14 96 17 93
3 20 35 14 41
2 27 17 17 27
1 262 64 192 134
1?5 437 817 366 888
Table 2: Frequencies for the different star-ratings
of a review, as annotated, and according to the
logistic regression classifier with the feature set
?All ? Imbalance?.
work is not a random sample from all reviews avail-
able in a specific group of products. We actually
assume ironic reviews to be much more sparse
when sampling equally distributed. The evaluation
should be seen from the angle of the application
scenario: For instance, in a discovery setting in
which the task is to retrieve examples for ironic
reviews, a highly precise system would be desir-
able. In a setting in which only a small number
of reviews should be used for opinion mining, the
polarity of a text would be discovered taking the
classifier?s result into account ? therefore a sys-
tem with high precision and high recall would be
needed.
5 Future Work
As discussed at the end of the last section, a study
on the distribution of irony in the entirety of avail-
able reviews is needed to better shape the structure
and characteristics of an irony or sarcasm detection
system. This could be approached by perform-
ing a random sample from reviews and annotation,
though this would lead to a substantial amount of
annotation work in comparison to the directed se-
lection procedure used in the corpus by Filatova
(2012).
Future research should focus on the development
of approaches analyzing the vocabulary used in the
review in a deeper fashion. Our impression is that
many sarcastic and ironic reviews use words and
phrases which are non-typical for the specific do-
main or product class. Such out-of-domain vocabu-
lary can be detected with text similarity approaches.
Preliminary experiments taking into account the av-
erage cosine similarity of a review to be classified
to a large set of reviews from the same product class
have been of limited success. We propose that fu-
ture research should focus on analyzing the specific
vocabulary and develop semantic similarity mea-
sures which we assume to be more promising than
approaches taking into account lexical approaches
only.
Most work has been performed on text sets from
one source like Twitter, books, reviews, etc. Some
of the proposed features mentioned in this paper
or previous publications are probably transferable
between text sources. However, this still needs
to be proven and further development might be
necessary to actually provide automated domain
adaption for the area of irony and sarcasm detection.
We assume that not only the vocabulary changes
47
(as known in other domain adaptation tasks) but
actually the linguistic structure might change.
Finally, it should be noted that the corpus is actu-
ally a mixture of ironic and sarcastic reviews. Irony
and sarcasm are not fully exchangeable and can be
assumed to have different properties. Further inves-
tigations and analyses regarding the characteristics
that can be transferred are necessary.
Acknowledgements
Roman Klinger has been funded by the ?It?s
OWL? project (?Intelligent Technical Systems
Ostwestfalen-Lippe?, http://www.its-owl.
de/), a leading-edge cluster of the German Min-
istry of Education and Research. We thank the
reviewers for their valuable comments. We thank
Christina Unger for proof-reading the manuscript
and helpful comments.
References
Meyer Howard Abrams. 1957. A Glossary of Literary
Terms. Cengage Learning Emea, 9th edition.
Salvatore Attardo. 2000. Irony markers and functions:
Towards a goal-oriented theory of irony and its pro-
cessing. Rask: Internationalt Tidsskrift for Sprog og
Kommunikation, 12:3?20.
Leo Breiman, Jerome H. Friedman, Richard A. Olshen,
and Charles J. Stone. 1984. Classification and Re-
gression Trees. Wadsworth, Belmont, California.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5?32.
British Dictionary. 2014. MacMillan Publishers. On-
line: http://www.macmillandictionary.
com/dictionary/british/irony. ac-
cessed April 28, 2014.
Paula Carvalho, Lu??s Sarmento, M?ario J. Silva, and
Eug?enio de Oliveira. 2009. Clues for detecting
irony in user-generated contents: oh. . . !! it?s ?so
easy? ;-). In Proceedings of the 1st international
CIKM workshop on Topic-sentiment analysis for
mass opinion, TSA ?09, pages 53?56, New York,
NY, USA. ACM.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology, 2(3).
Herbert H. Clark and Richard J. Gerrig. 1984. On the
pretense theory of irony. Journal of Experimental
Psychology: General, 113(1):121?126.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?
297.
Marlena A. Creusere. 2007. A developmental test
of theoretical perspective on the understanding of
verbal irony: Children?s recognition of allusion and
pragmatic insincerity. In Raymond W. Jr. Gibbs and
Herbert L. Colston, editors, Irony in Language and
Thought: A Cognitive Science Reader, chapter 18,
pages 409?424. Lawrence Erlbaum Associates, 1st
edition.
Dmitry Davidov and Ari Rappoport. 2006. Efficient
unsupervised discovery of word categories using
symmetric patterns and high frequency words. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 297?304, Sydney, Australia, July. Association
for Computational Linguistics.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the
Fourteenth Conference on Computational Natural
Language Learning, CoNLL ?10, pages 107?116,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Duden. 2014. Duden Verlag. Online: http://www.
duden.de/rechtschreibung/Ironie. ac-
cessed April 28, 2014.
Rong-En Fan, Pai-Hsuen Chen, and Chih-Jen Lin.
2005. Working set selection using second order
information for training support vector machines.
Jounral of Machine Learning Reasearch, 6:1889?
1918.
Elena Filatova. 2012. Irony and sarcasm: Corpus gen-
eration and analysis using crowdsourcing. In Nico-
letta Calzolari, Khalid Choukri, Thierry Declerck,
Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eighth International Conference on
Language Resources and Evaluation (LREC-2012),
pages 392?398, Istanbul, Turkey, May. European
Language Resources Association (ELRA).
Raymond W. Jr. Gibbs. 2007. Irony in talk among
friends. In Raymond W. Jr. Gibbs and Herbert L.
Colston, editors, Irony in Language and Thought:
A Cognitive Science Reader, chapter 15, pages
339?360. Lawrence Erlbaum Associates, 1st edition,
May.
Melanie Harris Glenwright and Penny M. Pexman.
2007. Children?s perceptions of the social func-
tions of verbal irony. In Raymond W. Jr. Gibbs and
Herbert L. Colston, editors, Irony in Language and
Thought: A Cognitive Science Reader, chapter 20,
pages 447?464. Lawrence Erlbaum Associates, 1st
edition.
Roberto Gonz?alez-Ib?a?nez, Smaranda Muresan, and
Nina Wacholder. 2011. Identifying sarcasm in twit-
ter: a closer look. In Proceedings of the 49th Annual
48
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: short pa-
pers - Volume 2, HLT ?11, pages 581?586, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Trevor Hastie, Robert Tibshirani, and Jerome H. Fried-
man. 2009. Elements of Statistical Learning.
Springer, 2nd edition.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Roger J. Kreuz and Gina M. Caucci. 2007. Lexical
influences on the perception of sarcasm. In Proceed-
ings of the Workshop on Computational Approaches
to Figurative Language, pages 1?4, Rochester, New
York, April. Association for Computational Linguis-
tics.
Roger J. Kreuz. 1996. The use of verbal irony:
Cues and constraints. In Jeffery S. Mio and Al-
bert N. Katz, editors, Metaphor: Implications and
Applications, pages 23?38, Mahwah, NJ, October.
Lawrence Erlbaum Associates.
Sachi Kumon-Nakamura, Sam Glucksberg, and Mary
Brown. 1995. How about another piece of pie: The
allusional pretense theory of discourse irony. Jour-
nal of Experimental Psychology: General, 124(1):3?
21, Mar 01. Last updated - 2013-02-23.
Lillian Lee. 2009. A tempest or, on the flood of inter-
est in: sentiment analysis, opinion mining, and the
computational treatment of subjective language. Tu-
torial at ICWSM, May.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to Information
Retrieval. Cambridge University Press.
Merriam Webster Dictionary. 2014. Merriam Webster
Inc. Online: www.merriam-webster.com/
dictionary/irony. accessed April 28, 2014.
Constantine Nakassis and Jesse Snedeker. 2002. Be-
yond sarcasm: Intonation and context as relational
cues in children?s recognition of irony. In A. Green-
hill, M. Hughs, H. Littlefield, and H. Walsh, editors,
Proceedings of the Twenty-sixth Boston University
Conference on Language Development, Somerville,
MA, July. Cascadilla Press.
New Oxford American Dictionary. 2014. Ox-
ford University Press. Online: http:
//www.oxforddictionaries.com/us/
definition/american_english/ironic.
accessed April 28, 2014.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Antonio Reyes, Paolo Rosso, and Davide Buscaldi.
2012. From humor recognition to irony detection:
The figurative language of social media. Data &
Knowledge Engineering, 74:1?12.
Patricia Rockwell. 2005. Sarcasm on television talk
shows: Determining speaker intent through verbal
and nonverbal cues. In Anita V. Clark, editor, Psy-
chology of Moods, chapter 6, pages 109?122. Nova
Science Pubishers Inc.
Joseph Tepperman, David Traum, and Shrikanth S.
Narayanan. 2006. ?yeah right?: Sarcasm recogni-
tion for spoken dialogue systems. In Proceedings
of InterSpeech, pages 1838?1841, Pittsburgh, PA,
September.
Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010.
ICWSM ? A Great Catchy Name: Semi-Supervised
Recognition of Sarcastic Sentences in Online Prod-
uct Reviews. In Proceedings of the Fourth Interna-
tional AAAI Conference on Weblogs and Social Me-
dia, pages 162?169. The AAAI Press.
Akira Utsumi. 1996. A unified theory of irony and its
computational formalization. In Proceedings of the
16th conference on Computational linguistics - Vol-
ume 2, COLING ?96, pages 962?967, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Deirdre Wilson and Dan Sperber. 1992. On verbal
irony. Lingua, 87:53?76.
Deirdre Wilson and Dan Sperber, 2012. Explaining
Irony, chapter 6, pages 123?145. Cambridge Uni-
versity Press, 1st edition, April.
Hsiang-Fu. Yu, Fang-Lan Huang, and Chih-Jen Lin.
2011. Dual coordinate descent methods for logistic
regression and maximum entropy. Machine Learn-
ing, 85(1?2):41?75, October.
Harry Zhang. 2004. The optimality of naive bayes. In
Valerie Barr and Zdravko Markov, editors, Proceed-
ings of the Seventeenth International Florida Artifi-
cial Intelligence Research Society (FLAIRS) Confer-
ence, pages 3?9. AAAI Press.
49
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 118?127,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Towards Gene Recognition from Rare and
Ambiguous Abbreviations using a Filtering Approach
Matthias Hartung
?
, Roman Klinger
?
, Matthias Zwick
?
and Philipp Cimiano
?
?
Semantic Computing Group
Cognitive Interaction Technology ?
Center of Excellence (CIT-EC)
Bielefeld University
33615 Bielefeld, Germany
{mhartung,rklinger,cimiano}
@cit-ec.uni-bielefeld.de
?
Research Networking
Boehringer Ingelheim Pharma GmbH
Birkendorfer Str. 65
88397 Biberach, Germany
matthias.zwick
@boehringer-ingelheim.com
Abstract
Retrieving information about highly am-
biguous gene/protein homonyms is a chal-
lenge, in particular where their non-protein
meanings are more frequent than their pro-
tein meaning (e. g., SAH or HF). Due to
their limited coverage in common bench-
marking data sets, the performance of exist-
ing gene/protein recognition tools on these
problematic cases is hard to assess.
We uniformly sample a corpus of eight am-
biguous gene/protein abbreviations from
MEDLINEr and provide manual annota-
tions for each mention of these abbrevia-
tions.
1
Based on this resource, we show
that available gene recognition tools such
as conditional random fields (CRF) trained
on BioCreative 2 NER data or GNAT tend
to underperform on this phenomenon.
We propose to extend existing gene recog-
nition approaches by combining a CRF
and a support vector machine. In a cross-
entity evaluation and without taking any
entity-specific information into account,
our model achieves a gain of 6 points
F
1
-Measure over our best baseline which
checks for the occurrence of a long form
of the abbreviation and more than 9 points
over all existing tools investigated.
1 Introduction
In pharmaceutical research, a common task is to
gather all relevant information about a gene, e. g.,
from published articles or abstracts. The task of rec-
ognizing the mentions of genes or proteins can be
understood as the classification problem to decide
1
The annotated corpus is available for future research at
http://dx.doi.org/10.4119/unibi/2673424.
whether the entity of interest denotes a gene/protein
or something else. For highly ambiguous short
names, this task can be particularly challenging.
Consider, for instance, the gene acyl-CoA syn-
thetase medium-chain family member 3 which has
synonyms protein SA homolog or SA hypertension-
associated homolog, among others, with abbrevia-
tions ACSM3, and SAH.
2
Standard thesaurus-based
search engines would retrieve results where SAH
denotes the gene/protein of interest, but also oc-
currences in which it denotes other proteins (e. g.,
ATX1 antioxidant protein 1 homolog
3
) or entities
from semantic classes other than genes/proteins
(e. g., the symptom sub-arachnoid hemorrhage).
For an abbreviation such as SAH, the use as de-
noting a symptom or another semantic class dif-
ferent from genes/proteins is more frequent by a
factor of 70 compared to protein-denoting men-
tions according to our corpus analysis, such that
the retrieval precision for acyl-CoA synthetase by
the occurrence of the synonym SAH is only about
0.01, which is totally unacceptable for practical
applications.
In this paper, we discuss the specific challenge
of recognizing such highly ambiguous abbrevia-
tions. We consider eight entities and show that
common corpora for gene/protein recognition are
of limited value for their investigation. The abbre-
viations we consider are SAH, MOX, PLS, CLU,
CLI, HF, AHR and COPD (cf. Table 1). Based
on a sample from MEDLINE
4
, we show that these
names do actually occur in biomedical text, but
are underrepresented in corpora typically used for
benchmarking and developing gene/protein recog-
nition approaches.
2
http://www.ncbi.nlm.nih.gov/gene/6296
3
http://www.ncbi.nlm.nih.gov/gene/
443451
4
http://www.nlm.nih.gov/pubs/
factsheets/medline.html
118
Synonym Other names Other meaning EntrezGene ID
SAH acyl-CoA synthetase medium-chain family
member 3; ACSM3
subarachnoid hemorrhage;
S-Adenosyl-L-homocysteine hydrolase
6296
MOX monooxygenase, DBH-like 1 moxifloxacin; methylparaoxon 26002
PLS POLARIS partial least squares; primary lateral sclerosis 3770598
CLU clusterin; CLI covalent linkage unit 1191
CLI clusterin; CLU clindamycin 1191
HF complement factor H; CFH high frequency; heart failure; Hartree-Fock 3075
AHR aryl hydrocarbon receptor; bHLHe76 airway hyperreactivity 196
COPD archain 1; ARCN1; coatomer protein
complex, subunit delta
Chronic Obstructive Pulmonary Disease 22819; 372
Table 1: The eight synonyms for genes/proteins which are subject of analysis in this paper and their long
names together with frequent other meanings.
We propose a machine learning-based filtering
approach to detect whether a mention in question
actually denotes a gene/protein or not and show
that for the eight highly ambiguous abbreviations
that we consider, the performance of our approach
in terms of F
1
measure is higher than for a state-of-
the-art tagger based on conditional random fields
(CRF), a freely available dictionary-based approach
and an abbreviation resolver. We evaluate differ-
ent parameters and their impact in our filtering
approach and discuss the results. Note that this
approach does not take any information about the
specific abbreviation into account and can therefore
be expected to generalize to names not considered
in our corpus.
The main contributions of this paper are:
(i) We consider the problem of recognizing
highly ambiguous abbreviations that fre-
quently do not denote proteins as a task that
has so far attracted only limited attention.
(ii) We show that the recognition of such ambigu-
ous mentions is important as their string rep-
resentation is frequent in collections such as
MEDLINE.
(iii) We show, however, that this set of ambiguous
names is underrepresented in corpora com-
monly used for system design and develop-
ment. Such corpora do not provide a suffi-
cient data basis for studying the phenomenon
or for training systems that appropriately han-
dle such ambiguous abbreviation. We con-
tribute a manually annotated corpus of 2174
occurrences of ambiguous abbreviations.
(iv) We propose a filtering method for classifying
ambiguous abbreviations as denoting a pro-
tein or not. We show that this method has a
positive impact on the overall performance of
named entity recognition systems.
2 Related Work
The task of gene/protein recognition consists in
the classification of terms as actually denoting a
gene/protein or not. The task is typically either
tackled by using machine learning or dictionary-
based approaches. Machine learning approaches
rely on appropriate features describing the local
context of the term to be classified and induce a
model to perform the classification from training
data. Conditional random fields have shown to
yield very good results on the task (Klinger et al.,
2007; Leaman and Gonzalez, 2008; Kuo et al.,
2007; Settles, 2005).
Dictionary-based approaches rely on an explicit
dictionary of gene/protein names that are matched
in text. Such systems are common in practice due
to the low overhead required to adapt and maintain
the system, essentially only requiring to extend the
dictionary. Examples of commercial systems are
ProMiner (Fluck et al., 2007) or I2E (Bandy et al.,
2009); a popular free system is made available by
Hakenberg et al. (2011).
Such dictionary-based systems typically incorpo-
rate rules for filtering false positives. For instance,
in ProMiner (Hanisch et al., 2003), ambiguous syn-
onyms are only accepted based on external dictio-
naries and matches in the context. Abbreviations
are only accepted if a long form matches all parts of
the abbreviation in the context (following Schwartz
and Hearst (2003)). Similarly, Hakenberg et al.
(2008) discuss global disambiguation on the doc-
ument level, such that all mentions of a string in
one abstract are uniformly accepted as denoting an
entity or not.
A slightly different approach is taken by the web-
service GeneE
5
(Schuemie et al., 2010): Entering a
query as a gene/protein in the search field generates
5
http://biosemantics.org/geneE
119
MEDLINE BioCreative2 GENIA
Protein # Tokens % tagged # Tokens % of genes # Tokens % of genes
SAH 30019 6.1 % 2 0 % 0
MOX 16007 13.1 % 0 0
PLS 11918 25.9 % 0 0
CLU 1077 29.1 % 0 0
CLI 1957 4.8 % 4 0 % 0
HF 42563 7.9 % 8 62.5 % 4 0 %
AHR 21525 75.7 % 12 91.7 % 0
COPD 44125 0.6 % 6 0 % 0
Table 2: Coverage of ambiguous abbreviations in MEDLINE, BioCreative2 and GENIA corpora. The
percentage of tokens tagged as a gene/protein in MEDLINE (% tagged) is determined with a conditional
random field in the configuration described by Klinger et al. (2007), but without dictionary-based features
to foster the usage of contextual features). The percentages of genes/proteins (% of genes) in BC2 and
GENIA are based on the annotations in these corpora.
a query to e. g. PubMedr
6
with the goal to limit
the number of false positives.
Previous to the common application of CRFs,
other machine learning methods have been popu-
lar as well for the task of entity recognition. For
instance, Mitsumori et al. (2005) and Bickel et al.
(2004) use a support vector machine (SVM) with
part-of-speech information and dictionary-based
features, amongst others. Zhou et al. (2005) use an
ensemble of different classifiers for recognition.
In contrast to this application of a classifier
to solve the recognition task entirely, other ap-
proaches (including the one in this paper) aim at
filtering specifically ambiguous entities from a pre-
viously defined set of challenging terms. For in-
stance, Al-mubaid (2006) utilize a word-based clas-
sifier and a mutual information-based feature selec-
tion to achieve a highly discriminating list of terms
which is applied for filtering candidates.
Similarly to our approach, Tsuruoka and Tsujii
(2003) use a classifier, in their case a na??ve Bayes
approach, to learn which entities to filter from
the candidates generated by a dictionary-based ap-
proach. They use word based features in the con-
text including the candidate itself. Therefore, the
approach is focused on specific entities.
Gaudan et al. (2005) use an SVM and a dictio-
nary of long forms of abbreviations to assign them
a specific meaning, taking contextual information
into account. However, their machine learning ap-
proach is trained on each possible sense of an ab-
breviation. In contrast, our approach consists in
deciding if a term is used as a protein or not. Fur-
ther, we do not train to detect specific, previously
given senses.
6
http://www.ncbi.nlm.nih.gov/pubmed/
Xu et al. (2007) apply text similarity measures to
decide about specific meanings of mentions. They
focus on the disambiguation between different en-
tities. A corpus for word sense disambiguation is
automatically built based on MeSH annotations by
Jimeno-Yepes et al. (2011). Okazaki et al. (2010)
build a sense inventory by automatically applying
patterns on MEDLINE and use this in a logistic
regression approach.
Approaches are typically evaluated on freely
available resources like the BioCreative Gene Men-
tion Task Corpus, to which we refer as BC2 (Smith
et al., 2008), or the GENIA Corpus (Kim et al.,
2003). When it comes to identifying particular pro-
teins by linking the protein in question to some
protein in an external database ? a task we do
not address in this paper ? the BioCreative Gene
Normalization Task Corpus is a common resource
(Morgan et al., 2008).
In contrast to these previous approaches, our
method is not tailored to a particular set of entities
or meanings, as the training methodology abstracts
from specific entities. The model, in fact, knows
nothing about the abbreviations to be classified and
does not use their surface form as a feature, such
that it can be applied to any unseen gene/protein
term. This leads to a simpler model that is applica-
ble to a wide range of gene/protein term candidates.
Our cross-entity evaluation regime clearly corrobo-
rates this.
3 Data
We focus on eight ambiguous abbreviations of
gene/protein names. As shown in Table 2, these
homonyms occur relatively frequently in MEDLINE
but are underrepresented in the BioCreative 2 entity
120
Protein Pos. Inst. Neg. Inst. Total
SAH 5 349 354
MOX 62 221 283
PLS 1 206 207
CLU 235 30 265
CLI 11 211 222
HF 2 353 355
AHR 53 80 133
COPD 0 250 250
Table 3: Number of instances per protein in the
annotated data set and their positive/negative distri-
bution
recognition data set and the GENIA corpus which
are both commonly used for developing and evalu-
ating gene recognition approaches. We compiled
a corpus from MEDLINE by randomly sampling
100 abstracts for each of the eight abbreviations (81
for MOX) such that each abstract contains at least
one mention of the respective abbreviation. One
of the authors manually annotated the mentions
of the eight abbreviations under consideration to
be a gene/protein entity or not. These annotations
were validated by another author. Both annotators
disagreed in only 2% of the cases. The numbers
of annotations, including their distribution over
positive and negative instances, are summarized
in Table 3. The corpus is made publicly available
at http://dx.doi.org/10.4119/unibi/
2673424 (Hartung and Zwick, 2014).
In order to alleviate the imbalance of positive
and negative examples in the data, additional pos-
itive examples have been gathered by manually
searching PubMed
7
. At this point, special attention
has been paid to extract only instances denoting the
correct gene/protein corresponding to the full long
name, as we are interested in assessing the impact
of examples of a particularly high quality. This
process yields 69 additional instances for AHR
(distributed over 11 abstracts), 7 instances (3 ab-
stracts) for HF, 14 instances (2 abstracts) for PLS
and 15 instances (7 abstracts) for SAH. For the
other gene/proteins in our dataset, no additional
positive instances of this kind could be retrieved
using PubMed. In the following, this process will
be referred to as manual instance generation. This
additional data is used for training only.
7
http://www.ncbi.nlm.nih.gov/pubmed
4 Gene Recognition by Filtering
We frame gene/protein recognition from ambigu-
ous abbreviations as a filtering task in which a set
of candidate tokens is classified into entities and
non-entities. In this paper, we assume the candi-
dates to be generated by a simple dictionary-based
approach taking into account all tokens that match
the abbreviation under consideration.
4.1 Filtering Strategies
We consider the following filtering approaches:
? SVM classifies the occurring terms based on a
binary support vector machine.
? CRF classifies the occurring terms based on
a conditional random field (configured as de-
scribed by Klinger et al. (2007)) trained on the
concatenation of BC2 data and our newly gen-
erated corpus. This setting thus corresponds
to state-of-the-art performance on the task.
? CRF?SVM considers the candidate an entity
if both the standard CRF and the SVM from
the previous steps yield a positive prediction.
? HRCRF?SVM is the same as the previous
step, but the output of the CRF is optimized
towards high recall by joining the recognition
of entities of the five most likely Viterbi paths.
? CRF?SVM is similar to the first setting, but
the output of the CRF is taken into account as
a feature in the SVM.
4.2 Features for Classification
Our classifier uses local contextual and global fea-
tures. Local features focus on the immediate con-
text of an instance, whereas global features encode
abstract-level information. Throughout the follow-
ing discussion, t
i
denotes a token at position i that
corresponds to a particular abbreviation to be classi-
fied in an abstract A. Note that we blind the actual
representation of the entity to be able to generalize
to all genes/proteins, not being limited to the ones
contained in our corpus.
4.2.1 Local Information
The feature templates context-left and context-right
collect the tokens immediately surrounding an ab-
breviation in a window of size 6 (left) and 4 (right)
in a bag-of-words-like feature generation. Addi-
tionally, the two tokens from the immediate context
on each side are combined into bigrams.
The template abbreviation generates features if
t
i
occurs in brackets. It takes into account the min-
imal Levenshtein distance (ld, Levenshtein (1966))
121
between all long forms L of the abbreviation (as
retrieved from EntrezGene) in comparison to each
string on the left of t
i
(up to a length of seven,
denoted by t
k:i
as the concatenation of tokens
t
k
, . . . , t
i
). Therefore, the similarity value sim(t
i
)
taken into account is given by
sim(t
i
) = max
l?L;k?[1:7]
1?
ld(t
k:i?1
, l)
max(|t
i
|, |l|)
,
where the denominator is a normalization term.
The features used are generated by cumulative bin-
ning of sim(t
i
).
The feature tagger
local
takes the prediction of the
CRF for t
i
into account. Note that this feature is
only used in the CRF?SVM setting.
4.2.2 Global Information
The feature template unigrams considers each word
in A as a feature. There is no normalization or
frequency weighting. Stopwords are ignored
8
. Oc-
currences of the same string as t
i
are blinded.
The feature tagger
global
collects all tokens in A
other than t
i
that are tagged as an entity by the CRF.
In addition, the cardinality of these entities in A is
taken into account by cumulative binning.
The feature long form holds if one of the long
forms previously defined to correspond with the ab-
breviation occurs in the text (in arbitrary position).
Besides using all features, we perform a greedy
search for the best feature set by wrapping the best
model configuration. A detailed discussion of the
feature selection process follows in Section 5.3.
4.2.3 Feature Propagation
Inspired by the ?one sense per discourse? heuristic
commonly adopted in word sense disambiguation
(Gale et al., 1992), we apply two feature combi-
nation strategies. In the following, n denotes the
number of occurrences of the abbreviation in an
abstract.
In the setting propagation
all
, n ? 1 identical
linked instances are added for each occurrence.
Each new instance consists of the disjunction of
the feature vectors of all occurrences. Based on
the intuition that the first mention of an abbrevia-
tion might carry particularly valuable information,
propagation
first
introduces one additional linked in-
stance for each occurrence, in which the feature
vector is joined with the first occurrence.
8
Using the stopword list at http://www.ncbi.nlm.
nih.gov/books/NBK3827/table/pubmedhelp.
T43/, last accessed on March 25, 2014
Setting P R F
1
SVM 0.81 0.45 0.58
CRF?SVM 0.99 0.26 0.41
HRCRF?SVM 0.95 0.27 0.42
CRF?SVM 0.83 0.49 0.62
CRF?SVM+FS 0.97 0.74 0.84
GNAT 0.73 0.45 0.56
CRF 0.55 0.43 0.48
AcroTagger 0.92 0.63 0.75
Long form 0.98 0.65 0.78
lex 0.18 1.00 0.32
Table 4: Overall micro-averaged results over eight
genes/proteins. For comparison, we show the re-
sults of a default run of GNAT (Hakenberg et al.,
2011), a CRF trained on BC2 data (Klinger et al.,
2007), AcroTagger (Gaudan et al., 2005), and a
simple approach of accepting every token of the
respective string as a gene/protein entity (lex). Fea-
ture selection is denoted with +FS.
In both settings, all original and linked instances
are used for training, while during testing, original
instances are classified by majority voting on their
linked instances. For propagation
all
, this results in
classifying each occurrence identically.
5 Experimental Evaluation
5.1 Experimental Setting
We perform a cross-entity evaluation, in which we
train the support vector machine (SVM) on the ab-
stracts of 7 genes/proteins from our corpus and test
on the abstracts for the remaining entities, i. e., the
model is evaluated only on tokens representing en-
tities which have never been seen labeled during
training. The CRFs are trained analogously with
the difference that the respective set used for train-
ing is augmented with the BioCreative 2 Training
data. The average numbers of precision, recall and
F
1
measure are reported.
As a baseline, we report the results of a simple
lexicon-based approach assuming that all tokens
denote an entity in all their occurrences (lex). In ad-
dition, the baseline of accepting an abbreviation as
gene/protein if the long form occurs in the same ab-
stract is reported (Long form). Moreover, we com-
pare our results with the publicly available toolkit
GNAT (Hakenberg et al., 2011)
9
and the CRF ap-
9
The gene normalization functionality of GNAT is not
taken into account here. We acknowledge that this comparison
122
proach as described in Section 4. In addition, we
take into account the AcroTagger
10
that resolves
abbreviations to their most likely long form which
we manually map to denoting a gene/protein or not.
5.2 Results
5.2.1 Overall results
In Table 4, we summarize the results of the recogni-
tion strategies introduced in Section 4. The lexical
baseline clearly proves that a simple approach with-
out any filtering is not practical. GNAT adapts well
to ambiguous short names and turns out as a com-
petitive baseline, achieving an average precision of
0.73. In contrast, the filtering capacity of a stan-
dard CRF is, at best, mediocre. The long form
baseline is very competitive with an F
1
measure of
0.78 and a close-to-perfect precision. The results of
AcroTagger are similar to this long form baseline.
We observe that the SVM outperforms the CRF
in terms of precision and recall (by 10 percentage
points in F
1
). Despite not being fully satisfactory
either, these results indicate that global features
which are not implemented in the CRF are of im-
portance. This is confirmed by the CRF?SVM
setting, where CRF and SVM are stacked: This fil-
tering procedure achieves the best precision across
all models and baselines, whereas the recall is still
limited. Despite being designed for exactly this
purpose, the HRCRF?SVM combination can only
marginally alleviate this problem, and only at the
expense of a drop in precision.
The best trade-off between precision and recall
is offered by the CRF?SVM combination. This
setting is not only superior to all other variants of
combining a CRF with an SVM, but outperforms
GNAT by 6 points in F
1
score, while being inferior
to the long form baseline. However, performing
feature selection on this best model using a wrapper
approach (CRF?SVM+FS) leads to the overall
best result of F
1
= 0.84, outperforming all other
approaches and all baselines.
5.2.2 Individual results
Table 5 summarizes the performance of all filter-
ing strategies broken down into individual entities.
Best results are achieved for AHR, MOX and CLU.
COPD forms a special case as no examples for the
might be seen as slightly inappropriate as the focus of GNAT
is different.
10
ftp://ftp.ebi.ac.uk/pub/software/
textmining/abbreviation_resolution/, ac-
cessed April 23, 2014
occurrence as a gene/protein are in the data; how-
ever the results show that the system can handle
such a special distribution.
SVM and CRF are mostly outperformed by a
combination of both strategies (except for CLI and
HF), which shows that local and global features
are highly complementary in general. Complemen-
tary cases generally favor the CRF?SVM strategy,
except for PLS, where stacking is more effective.
In SAH, the pure CRF model is superior to all
combinations of CRF and SVM. Apparently, the
global information as contributed by the SVM is
less effective than local contextual features as avail-
able to the CRF in these cases. In SAH and CLI,
moreover, the best performance is obtained by the
AcroTagger.
5.2.3 Impact of instance generation
All results reported in Tables 4 and 5 refer to con-
figurations in which additional training instances
have been created by manual instance generation.
The impact of this method is analyzed in Table 6.
The first column reports the performance of our
models on the randomly sampled training data. In
order to obtain the results in the second column,
manual instance generation has been applied.
The results show that all our recognition mod-
els generally benefit from additional information
that helps to overcome the skewed class distribu-
tion of the training data. Despite their relatively
small quantity and uneven distribution across the
gene/protein classes, including additional exter-
nal instances yields a strong boost in all mod-
els. The largest difference is observed in SVM
(?F
1
= +0.2) and CRF?SVM (?F
1
= +0.16).
Importantly, these improvements include both pre-
cision and recall.
5.3 Feature Selection
The best feature set (cf. CRF?SVM+FS in Ta-
ble 4) is determined by a greedy search using a
wrapper approach on the best model configuration
CRF?SVM. The results are depicted in Table 7.
In each iteration, the table shows the best feature
set detected in the previous iteration and the results
for each individual feature when being added to
this set. In each step, the best individual feature
is kept for the next iteration. The feature analysis
starts from the long form feature as strong base-
line. The added features are, in that order, context,
tagger
global
, and propagation
all
.
Overall, feature selection yields a considerable
123
AHR CLI CLU COPD
Setting P R F
1
P R F
1
P R F
1
P R F
1
SVM 1.00 0.72 0.84 0.30 0.27 0.29 1.00 0.41 0.58 0.00 1.00 0.00
CRF?SVM 1.00 0.70 0.82 0.00 0.00 0.00 1.00 0.15 0.26 1.00 1.00 1.00
HRCRF?SVM 1.00 0.70 0.82 1.00 0.00 0.00 1.00 0.16 0.28 1.00 1.00 1.00
CRF?SVM 0.96 0.83 0.89 0.30 0.27 0.29 1.00 0.40 0.57 0.00 1.00 0.00
CRF?SVM+FS 0.93 0.98 0.95 0.50 0.09 0.15 0.99 0.84 0.91 1.00 1.00 1.00
GNAT 0.74 0.66 0.70 1.00 0.18 0.31 0.97 0.52 0.68 1.00 1.00 1.00
CRF 0.52 0.98 0.68 0.00 0.00 0.00 1.00 0.20 0.33 0.00 1.00 0.00
AcroTagger 1.00 0.60 0.75 1.00 0.82 0.90 1.00 0.00 0.00 1.00 1.00 1.00
Long form 1.00 0.96 0.98 1.00 0.09 0.17 0.99 0.80 0.88 1.00 1.00 1.00
lex 0.40 1.00 0.57 0.05 1.00 0.09 0.89 1.00 0.94 0.00 1.00 0.00
HF MOX PLS SAH
Setting P R F
1
P R F
1
P R F
1
P R F
1
SVM 0.25 1.00 0.40 0.87 0.44 0.58 0.14 1.00 0.25 0.00 0.00 0.00
CRF?SVM 1.00 0.00 0.00 1.00 0.39 0.56 1.00 1.00 1.00 1.00 0.00 0.00
HRCRF?SVM 1.00 0.00 0.00 1.00 0.39 0.56 0.20 1.00 0.33 1.00 0.00 0.00
CRF?SVM 0.25 1.00 0.40 0.91 0.63 0.74 0.50 1.00 0.67 1.00 0.00 0.00
CRF?SVM+FS 1.00 0.00 0.00 1.00 0.37 0.54 0.00 0.00 0.00 1.00 0.00 0.00
GNAT 1.00 0.00 0.00 0.38 0.08 0.14 0.00 0.00 0.00 0.00 0.00 0.0
CRF 0.00 0.00 0.00 0.43 0.90 0.59 0.14 1.00 0.25 1.00 0.50 0.67
AcroTagger 0.33 1.00 0.50 1.00 0.00 0.00 1.00 0.00 0.00 1.00 0.60 0.75
Long form 1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00
lex 0.01 1.00 0.02 0.22 1.00 0.36 0.00 1.00 0.01 0.01 1.00 0.03
Table 5: Results for the eight genes/proteins and results for our different recognition schemes.
randomly sampled +instance generation
P R F
1
?P ?R ?F
1
SVM 0.73 0.25 0.38 +0.08 +0.20 +0.20
CRF?SVM 1.00 0.17 0.29 -0.01 +0.09 +0.13
HRCRF?SVM 0.97 0.18 0.30 -0.02 +0.09 +0.12
CRF?SVM 0.79 0.32 0.46 +0.05 +0.17 +0.16
CRF?SVM+FS 0.99 0.60 0.75 -0.02 +0.14 +0.09
Table 6: Impact of increasing the randomly sampled training set by adding manually curated additional
positive instances (+instance generation), measured in terms of the increase in precision, recall and F
1
(?P, ?R, ?F
1
).
boost in recall, while precision remains almost con-
stant. Surprisingly, the unigrams feature has a par-
ticularly strong negative impact on overall perfor-
mance.
While the global information contributed by the
CRF turns out very valuable, accounting for most
of the improvement in recall, local tagger informa-
tion is widely superseded by other features. Like-
wise, the abbreviation feature does not provide any
added value to the model beyond what is known
from the long form feature.
Comparing the different feature propagation
strategies, we observe that propagation
all
outper-
forms propagation
first
.
5.4 Discussion
Our experiments show that the phenomena inves-
tigated pose a challenge to all gene recognition
paradigms currently available in the literature, i. e.,
dictionary-based, machine-learning-based (e. g. us-
ing a CRF), and classification-based filtering.
Our results indicate that stacking different meth-
ods suffers from a low recall in early steps of the
workflow. Instead, a greedy approach that consid-
ers all occurrences of an abbreviation as input to
a filtering approach yields the best performance.
Incorporating information from a CRF as features
into a SVM outperforms all baselines at very high
levels of precision; however, the recall still leaves
room for improvement.
124
Iter. Feature Set P R F
1
?F
1
1 long form 0.98 0.65 0.78
+propagation
1
st
0.98 0.65 0.78 +0.00
+propagation
all
0.98 0.65 0.78 +0.00
+tagger
local
0.72 0.81 0.76 -0.02
+tagger
global
0.55 0.79 0.65 -0.13
+context 0.98 0.67 0.79 +0.01
+abbreviation 0.98 0.65 0.78 +0.00
+unigrams 0.71 0.43 0.53 -0.25
2 long form
+context 0.98 0.67 0.79
+propagation
1
st
0.98 0.67 0.79 +0.00
+propagation
all
0.96 0.70 0.81 +0.02
+tagger
local
0.98 0.70 0.82 +0.03
+tagger
global
0.97 0.72 0.83 +0.04
+abbreviation 0.98 0.67 0.80 +0.01
+unigrams 0.77 0.39 0.52 -0.27
3 long form
+context
+tagger
global
0.97 0.72 0.83
+propagation
1
st
0.97 0.71 0.82 -0.01
+propagation
all
0.97 0.74 0.84 +0.01
+tagger
local
0.97 0.72 0.82 -0.01
+abbreviation 0.97 0.72 0.82 -0.01
+unigrams 0.77 0.44 0.56 -0.27
4 long form
+context
+tagger
global
+propagation
all
0.97 0.74 0.84
+tagger
local
0.90 0.66 0.76 -0.08
+abbreviation 0.97 0.74 0.84 -0.00
+unigrams 0.80 0.49 0.61 -0.23
Table 7: Greedy search for best feature combina-
tion in CRF?SVM (incl. additional positives).
In a feature selection study, we were able to show
a largely positive overall impact of features that
extend local contextual information as commonly
applied by state-of-the-art CRF approaches. This
ranges from larger context windows for collecting
contextual information over abstract-level features
to feature propagation strategies. However, feature
selection is not equally effective in all individual
classes (cf. Table 5).
The benefits due to feature propagation indi-
cate that several instances of the same abbreviation
in one abstract should not be considered indepen-
dently of one another, although we could not verify
the intuition that the first mention of an abbrevia-
tion introduces particularly valuable information
for classification.
Overall, our results seem encouraging as the ma-
chinery and the features used are in general suc-
cessful in determining whether an abbreviation ac-
tually denotes a gene/protein or not. The best pre-
cision/recall balance is obtained by adding CRF
information as features into the classifier.
As we have shown in the cross-entity experi-
ment setting, the system is capable of generalizing
to other unseen entities. For a productive system,
we assume our workflow to be applied to specific
abbreviations such that the performance on other
entities (and therefore on other corpora) is not sub-
stantially influenced.
6 Conclusions and Outlook
The work reported in this paper was motivated from
the practical need for an effective filtering method
for recognizing genes/proteins from highly ambigu-
ous abbreviations. To the best of our knowledge,
this is the first approach to tackle gene/protein
recognition from ambiguous abbreviations in a
systematic manner without being specific for the
particular instances of ambiguous gene/protein
homonyms considered.
The proposed method has been proven to allow
for an improvement in recognition performance
when added to an existing NER workflow. Despite
being restricted to eight entities so far, our approach
has been evaluated in a strict cross-entity manner,
which suggests sufficient generalization power to
be extended to other genes as well.
In future work, we plan to extend the data set
to prove the generalizability on a larger scale and
on an independent test set. Furthermore, an inclu-
sion of the features presented in this paper into the
CRF will be evaluated. Moreover, assessing the
impact of the global features that turned out benefi-
cial in this paper on other gene/protein inventories
seems an interesting path to explore. Finally, we
will investigate the prospects of our approach in an
actual black-box evaluation setting for information
retrieval.
Acknowledgements
Roman Klinger has been funded by the ?It?s
OWL? project (?Intelligent Technical Systems
Ostwestfalen-Lippe?, http://www.its-owl.
de/), a leading-edge cluster of the German Min-
istry of Education and Research. We thank J?org
Hakenberg and Philippe Thomas for their support
in performing the baseline results with GNAT. Ad-
ditionally, we thank the reviewers of this paper for
their very helpful comments.
125
References
Hisham Al-mubaid. 2006. Biomedical term disam-
biguation: An application to gene-protein name dis-
ambiguation. In In IEEE Proceedings of ITNG06.
Judith Bandy, David Milward, and Sarah McQuay.
2009. Mining protein-protein interactions from pub-
lished literature using linguamatics i2e. Methods
Mol Biol, 563:3?13.
Steffen Bickel, Ulf Brefeld, Lukas Faulstich, J?org Hak-
enberg, Ulf Leser, Conrad Plake, and Tobias Schef-
fer. 2004. A support vector machine classifier for
gene name recognition. In In Proceedings of the
EMBO Workshop: A Critical Assessment of Text
Mining Methods in Molecular Biology.
Juliane Fluck, Heinz Theodor Mevissen, Marius Os-
ter, and Martin Hofmann-Apitius. 2007. ProMiner:
Recognition of Human Gene and Protein Names
using regularly updated Dictionaries. In Proceed-
ings of the Second BioCreative Challenge Evalua-
tion Workshop, pages 149?151, Madrid, Spain.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the Workshop on Speech and Natural
Language, pages 233?237, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Sylvain Gaudan, Harald Kirsch, and Dietrich Rebholz-
Schuhmann. 2005. Resolving abbreviations to their
senses in medline. Bioinformatics, 21(18):3658?
3664.
J?org Hakenberg, Conrad Plake, Robert Leaman,
Michael Schroeder, and Graciela Gonzalez. 2008.
Inter-species normalization of gene mentions with
GNAT. Bioinformatics, 24(16):i126?i132, Aug.
J?org Hakenberg, Martin Gerner, Maximilian Haeus-
sler, Ills Solt, Conrad Plake, Michael Schroeder,
Graciela Gonzalez, Goran Nenadic, and Casey M.
Bergman. 2011. The GNAT library for local and
remote gene mention normalization. Bioinformatics,
27(19):2769?2771, Oct.
Daniel Hanisch, Juliane Fluck, Heinz-Theodor Mevis-
sen, and Ralf Zimmer. 2003. Playing biology?s
name game: identifying protein names in scientific
text. Pac Symp Biocomput, pages 403?414.
Matthias Hartung and Matthias Zwick. 2014. A cor-
pus for the development of gene/protein recognition
from rare and ambiguous abbreviations. Bielefeld
University. doi:10.4119/unibi/2673424.
Antonio J Jimeno-Yepes, Bridget T McInnes, and
Alan R Aronson. 2011. Exploiting mesh indexing
in medline to generate a data set for word sense dis-
ambiguation. BMC bioinformatics, 12(1):223.
J-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. Ge-
nia corpus?semantically annotated corpus for bio-
textmining. Bioinformatics, 19 Suppl 1:i180?i182.
Roman Klinger, Christoph M. Friedrich, Juliane Fluck,
and Martin Hofmann-Apitius. 2007. Named
Entity Recognition with Combinations of Condi-
tional Random Fields. In Proceedings of the Sec-
ond BioCreative Challenge Evaluation Workshop,
Madrid, Spain, April.
Cheng-Ju Kuo, Yu-Ming Chang, Han-Shen Huang,
Kuan-Ting Lin, Bo-Hou Yang, Yu-Shi Lin, Chun-
Nan Hsu, and I-Fang Chung. 2007. Rich feature
set, unication of bidirectional parsing and dictionary
filtering for high f-score gene mention tagging. In
Proceedings of the Second BioCreative Challenge
Evaluation Workshop, Madrid, Spain, April.
Robert Leaman and Graciela Gonzalez. 2008. Ban-
ner: An executable survey of advances in biomed-
ical named entity recognition. In Russ B. Altman,
A. Keith Dunker, Lawrence Hunter, Tiffany Murray,
and Teri E. Klein, editors, Pacific Symposium on Bio-
computing, pages 652?663. World Scientific.
Vladimir I. Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions, and reversals. So-
viet Physics Doklady, 10:707?710.
Tomohiro Mitsumori, Sevrani Fation, Masaki Mu-
rata, Kouichi Doi, and Hirohumi Doi. 2005.
Gene/protein name recognition based on support
vector machine using dictionary as features. BMC
Bioinformatics, 6 Suppl 1:S8.
Alexander A. Morgan, Zhiyong Lu, Xinglong Wang,
Aaron M. Cohen, Juliane Fluck, Patrick Ruch, Anna
Divoli, Katrin Fundel, Robert Leaman, Jrg Haken-
berg, Chengjie Sun, Heng-hui Liu, Rafael Torres,
Michael Krauthammer, William W. Lau, Hongfang
Liu, Chun-Nan Hsu, Martijn Schuemie, K Bretonnel
Cohen, and Lynette Hirschman. 2008. Overview of
biocreative ii gene normalization. Genome Biol, 9
Suppl 2:S3.
Naoaki Okazaki, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2010. Building a high-quality sense inventory
for improved abbreviation disambiguation. Bioinfor-
matics, 26(9):1246?1253, May.
Martijn J. Schuemie, Ning Kang, Maarten L. Hekkel-
man, and Jan A. Kors. 2010. Genee: gene and pro-
tein query expansion with disambiguation. Bioinfor-
matics, 26(1):147?148, Jan.
Ariel S. Schwartz and Marti A. Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. Pac Symp Biocomput, pages 451?
462.
Burr Settles. 2005. Abner: an open source tool for au-
tomatically tagging genes, proteins and other entity
names in text. Bioinformatics, 21(14):3191?3192,
Jul.
Larry Smith, Lorraine K. Tanabe, Rie Johnson nee J.
Ando, Cheng-Ju J. Kuo, I-Fang F. Chung, Chun-
Nan N. Hsu, Yu-Shi S. Lin, Roman Klinger,
126
Christoph M. Friedrich, Kuzman Ganchev, Man-
abu Torii, Hongfang Liu, Barry Haddow, Craig A.
Struble, Richard J. Povinelli, Andreas Vlachos,
William A. Baumgartner, Lawrence Hunter, Bob
Carpenter, Richard Tzong-Han T. Tsai, Hong-Jie J.
Dai, Feng Liu, Yifei Chen, Chengjie Sun, Sophia Ka-
trenko, Pieter Adriaans, Christian Blaschke, Rafael
Torres, Mariana Neves, Preslav Nakov, Anna Divoli,
Manuel Ma?na L?opez, Jacinto Mata, and W. John
Wilbur. 2008. Overview of BioCreative II gene
mention recognition. Genome biology, 9 Suppl
2(Suppl 2):S2+.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2003. Boost-
ing precision and recall of dictionary-based pro-
tein name recognition. In Proceedings of the ACL
2003 Workshop on Natural Language Processing in
Biomedicine, pages 41?48, Sapporo, Japan, July. As-
sociation for Computational Linguistics.
Hua Xu, Jung-Wei Fan, George Hripcsak, Eneida A
Mendonc?a, Marianthi Markatou, and Carol Fried-
man. 2007. Gene symbol disambiguation us-
ing knowledge-based profiles. Bioinformatics,
23(8):1015?1022.
GuoDong Zhou, Dan Shen, Jie Zhang, Jian Su, and
SoonHeng Tan. 2005. Recognition of protein/gene
names from text using an ensemble of classifiers.
BMC Bioinformatics, 6 Suppl 1:S7.
127
Proceedings of Third Workshop on Semantic Web and Information Extraction, pages 25?32,
Dublin, Ireland, 24 August, 2014.
Ontology-based Extraction of Structured Information from Publications
on Preclinical Experiments for Spinal Cord Injury Treatments
Benjamin Paassen
?
, Andreas St?ckel
?
, Raphael Dickfelder
?
, Jan Philip G?pfert
?
,
Tarek Kirchhoffer
?
, Nicole Brazda
?,?
, Hans Werner M?ller
?,?
,
Roman Klinger
?
, Matthias Hartung
?
, Philipp Cimiano
?1
?
Semantic Computing Group, CIT-EC, Bielefeld University, 33615 Bielefeld, Germany
?
Molecular Neurobiology, Neurology, HHU D?sseldorf, 40225 D?sseldorf, Germany
?
Center for Neuronal Regeneration, Life Science Center, 40225 D?sseldorf, Germany
{bpaassen,astoecke,rdickfel,jgoepfert}@techfak.uni-bielefeld.de
tarek.kirchhoffer@cnr.de, {nicole.brazda,hanswerner.mueller}@uni-duesseldorf.de
{rklinger,mhartung,cimiano}@cit-ec.uni-bielefeld.de
Abstract
Preclinical research in the field of central nervous system trauma advances at a fast pace, currently
yielding over 8,000 new publications per year, at an exponentially growing rate. This amount of
published information by far exceeds the capacity of individual scientists to read and understand the
relevant literature. So far, no clinical trial has led to therapeutic approaches which achieve functional
recovery in human patients.
In this paper, we describe a first prototype of an ontology-based information extraction system that
automatically extracts relevant preclinical knowledge about spinal cord injury treatments from nat-
ural language text by recognizing participating entity classes and linking them to each other. The
evaluation on an independent test corpus of manually annotated full text articles shows a macro-
average F
1
measure of 0.74 with precision 0.68 and recall 0.81 on the task of identifying entities
participating in relations.
1 Introduction
Injury to the central nervous system of adult mammals typically results in lasting deficits, like permanent
motor and sensor impairments, due to a lack of profound neural regeneration. Specifically, patients who have
sustained spinal cord injuries (SCI) usually remain partially paralyzed for the rest of their lives. Preclinical
research in the field of central nervous system trauma advances at fast pace, currently yielding over 8,000
new publications per year, at an exponentially growing rate, with a total amount of approximately 160,000
PubMed-listed papers today.
2
However, translational neuroscience faces a strong disproportion between the immense preclinical re-
search effort and the lack of successful clinical trials in SCI therapy: So far, no therapeutic approach has
led to functional recovery in human patients (Filli and Schwab, 2012). As the vast amount of published in-
formation by far exceeds the capacity of individual scientists to read and understand the relevant knowledge
(Lok, 2010), the selection of promising therapeutic interventions for clinical trials is notoriously based on
incomplete information (Prinz et al., 2011; Steward et al., 2012).
Thus, automatic information extraction methods are needed to gather structured, actionable knowledge
from large amounts of unstructured text that describe outcomes of preclinical experiments in the SCI do-
main. Being stored in a database, such knowledge provides a highly valuable resource enabling curators
and researchers to objectively assess the prospective success of experimental therapies in humans, and sup-
ports the cost-effective execution of meta studies based on all previously published data. First steps towards
such a database have already been undertaken by manually extracting the desired information from a limited
number of papers (Brazda et al., 2013), which is not feasible on a large scale, though.
In this paper, we present a first prototype of an automated ontology-based information extraction system
for the acquisition of structured knowledge about experimental SCI therapies. As main contributions, we
point out the highly relational problem structure by describing the entity classes and relations relevant for
1
The first four authors contributed equally.
2
As in this query to the database PubMed (link to http://www.ncbi.nlm.nih.gov/pubmed), as of April 2014.
25
Named Entity Recognition Relation Extraction
Rule-based
Ontology-based
Regular
Expression
Rule-based
recombination
Token
lookup
Candidate
generation
Candidate
filtering
Inp
ut 
PD
F
Ontological
reduction
Relations
Ou
tpu
t
Animal
Injury
Treatment
Resulttex
t e
xtr
act
ion
Figure 1: Workflow of our implementation, from the input PDF document to the generation of the output
relations. Named entity recognition is described in Section 3.1, relation extraction in Section 3.2.
knowledge representation in the domain, and provide a cascaded workflow that is capable of extracting these
relational structures from unstructured text with an average F
1
measure of 0.74.
2 Related Work
Our workflow for acquiring structured information in the domain of spinal cord injury treatments is an
example of ontology-based information extraction systems (Wimalasuriya and Dou, 2010): Large amounts
of unstructured natural language text are processed through a mechanism guided by an ontology, in order to
extract predefined types of information. Our long-term goal is to represent all relevant information on SCI
treatments in structured form, similar to other automatically populated databases in the biomedical domain,
such as STRING-DB for protein-protein interactions (Franceschini et al., 2013), among others.
A strong focus in biomedical information extraction has long been on named entity recognition, for which
machine-learning solutions such as conditional random fields (Lafferty et al., 2001) or dictionary-based
systems (Schuemie et al., 2007; Hanisch et al., 2005; Hakenberg et al., 2011) are available which tackle
the respective problem with decent performance and for specific entity classes such as organisms (Pafilis
et al., 2013) or symptoms (Savova et al., 2010; Jimeno et al., 2008). A detailed overview on named entity
recognition, covering other domains as well, can be found in Nadeau and Sekine (2007).
The use case described in this paper, however, involves a highly relational problem structure in the sense
that individual facts or relations have to be aggregated in order to yield accurate, holistic domain knowledge,
which corresponds most closely to the problem structure encountered in event extraction, as triggered by
the ACE program (Doddington et al., 2004; Ji and Grishman, 2008; Strassel et al., 2008), and the BioNLP
shared task series (Nedellec et al., 2013; Tsujii et al., 2011; Tsujii, 2009). General semantic search engines
in the biomedical domain mainly focus on isolated entities. Relations are typically only taken into account
by co-occurrence on abstract or sentence level. Examples for such search engines include GoPubMed (Doms
and Schroeder, 2005), SCAIView (Hofmann-Apitius et al., 2008), and GeneView (Thomas et al., 2012).
With respect to the extraction methodology, our work is similar to Saggion et al. (2007) and Buitelaar et
al. (2008), in that a combination of gazetteers and extraction rules is derived from the underlying ontology,
in order to adapt the workflow to the domain of interest. A schema in terms of a reporting standard has
recently been proposed by the MIASCI-consortium (Lemmon et al., 2014, Minimum Information About a
Spinal Cord Injury Experiment). To the best of our knowledge, our work is the first attempt at automated
information extraction in the SCI domain.
3 Method and Architecture
An illustration of the proposed workflow is shown in Figure 1. Based on the unstructured information
management architecture (UIMA, Ferrucci and Lally (2004)), full text PDF documents serve as input to the
workflow. Plain text and structural information are extracted from these documents using Apache PDFBox
3
.
The proposed system extracts relations which we define as templates that contain slots, each of which is
to be filled by an instance of a particular entity class (cf. Table 1). At the same time, a particular instance
can be a filler for different slots (cf. Figure 2). We argue that a relational approach is essential to information
extraction in the SCI domain as (i) many instances of entity classes found in the text do not convey relevant
3
Apache PDFBox ? A Java PDF Library http://pdfbox.apache.org/
26
Relation Entity Class Example Method Resource Count
Integer ?42?, ?2k?, ?1,000? R Regular Expressions
Float ?4.23?, ?8.12 ? 10
-8
? R Regular Expressions
Roman Number ?XII?, ?MCLXII? R Regular Expressions
Word Number ?seventy-six" O Word Number List 99
Range ?2-4? R QTY + PARTICLE + QTY
Language Quantifier ?many?, ?all? O Quantifier List 11
Time ?2 h?, ?14 weeks? R QTY + TIME UNIT
Duration ?for 2h? R PARTICLE + TIME
Animal
Organism ?dog?, ?rat?, ?mice? O NCBI Taxonomy 67657
Laboratory Animal ?Long-Evans rats? O Special Laboratory Animals 5
Sex ?male?, ?female? O Gender List 2
Exact Age ?14 weeks old? R TIME + AGE PARTICLE
Age ?adult?, ?juvenile? O Age Expressions 2
Weight ?200 g? R QTY + WEIGHT UNIT
Number ?44?, ?seventy-six? R QTY
Injury
Injury Type ?compression? O Injury Type List 7
Injury Device ?NYU Impactor? O Injury Device List 21
Vertebral Position ?T4?, ?T8-9? R Regular Expressions
Injury Height ?cervical?, ?thoracic? O Injury Height Expressions 4
Treatment
Drug ?EPO?, ?inosine? O MeSH 14000
Delivery ?subcutaneous?, ?i.v.? O Delivery Dictionary 34
Dosage ?14 ml/kg? R QTY + UNIT
Result
Investigation Method ?walking analysis? O Method List 117
Significance ?significant? O Significance Quantifiers 2
Trend ?decreased?, ?improved? O Trend Dictionary 4
p Value ?p < 0.05? R P + QTY 4
Table 1: A detailed list of relations and the entity classes whose instances are valid slot fillers for them.
Examples for instances of each entity class are also shown, as well as the extraction method, and resources
used for extraction. Instances are either extracted from the text using regular expressions (R) or on a
lookup in our ontology database (O). Resources in italics were specifically created for this application,
resources in SMALL CAPITALS are regular expression-based recombinations of other entities. Entity
classes in bold face are required arguments for relation extraction (cf. Section 3.2). The count specifies
the number of elements in the respective resource.
information on their own, but only in combination with other instances (e. g., surgical devices mentioned in
the text are only relevant if used to inflict a spincal cord injury to the animals in an experimental group), and
(ii) a holistic picture of a preclinical experiment can only be captured by aggregating several relations (e. g.,
a certain p value being mentioned in the text implies a particular treatment of one group of animals to be
significantly different from another treatment of a control group).
We take four relations (Animal, Injury, Treatment and Result) into account which capture the semantic
essence of a preclinical experiment: Laboratory animals are injured, then treated and the effect of the treat-
ment is measured. Table 1 provides an overview of all entity classes and relations. The workflow consists
of two steps: Firstly, rule- and ontology-based named entity recognition (NER) is performed (cf. Section
3.1). Secondly, the pool of entities recognized during NER serves as a basis for relation extraction (cf.
Section 3.2).
3.1 Ontology-based Named Entity Recognition
We store ontological information in a relational database as a set of directed graphs, accompanied by a
dictionary for efficient token lookup. Each entity is stored with possible linguistic surface forms (e. g.,
?Wistar rats? as a surface form of the Wistar rat entity from the class Laboratory Animal). Each surface
form s is tokenized (on white space and non-alphanumeric symbols, including transformation to lowercase,
e. g., leading to tokens ?wistar? and ?rats?) and normalized (stemming, removal of special characters and
stop words) resulting in a set of dictionary keys (e. g., ?wistar? and ?rat?). The resources used as content
for the ontology are shown in Table 1. We use specifically crafted resources for our use case
4
as well as the
4
Resources built specifically are made publicly available at http://opensource.cit-ec.de/projects/scie
27
Five adult male guinea pigs weighing 200-250 g.
Animal Animal
Organism: guinea pigs
Weight: 200-250 g
Age: adult
Sex: male
Number: Five (5)
Organism: guinea pigs
Weight: -
Age: adult
Sex: male
Number: 200
Figure 2: Two example instances of the
Animal relation that can be generated
from the same text. Given its entity
class, the number 200 is a valid filler for
the ?number? slot as well as the ?weight?
slot. Both candidates are generated and
ranked according to their probability (cf.
Equation 4). The manually defined con-
straints of p
sem
ensure that 200 cannot
fill both slots at the same time.
NCBI taxonomy
5
and the Medical Subject Headings
6
(MeSH). The process of ontology-based NER consists
of (i) token lookup in the dictionary, (ii) candidate generation, (iii) probabilistic candidate filtering and (iv)
ontological reduction (cf. Figure 1).
Token lookup. For each token t in the document, the corresponding surface form tokens s
t
are retrieved
from the database. A confidence value p
conf
based on the Damerau-Levenshtein-Distance without swaps
(dld, Damerau (1964)) is calculated as
p
conf
(t, s
t
) := max
{
0, 1?min
t
?
?s
t
dld(t
?
, t)
|t
?
|
}
, (1)
where |t| denotes the number of characters in token t. Assuming to find t = ?rat? in the text with the
according surface form s
t
= (?wistar?, ?rats?), p
conf
(t, s
t
) = 1 ?
1
4
= 0.75. Tokens with p
conf
< 0.5 are
discarded.
Candidate generation. A candidate h for matching the surface form tokens s
h
is a list of tokens (t
h
1
, . . . , t
h
n
)
from the text. Candidates are constructed using all possible combinations of matching tokens for each surface
form token (as retrieved above). To keep this tractable, we restrict the search space to combinations with the
proximity d(t
h
k
, t
h
`
) ? 9 for all t
h
k
, t
h
`
? h, where d(u, v) := N
W
(u, v) + 3 ? N
S
(u, v) + 10 ? N
P
(u, v)
models the distance between two tokens u and v in the text withN
W
, N
S
, N
P
denoting the number of words,
sentences and paragraphs between u and v. In our example, a candidate would be h = (?rat?).
Candidate filtering. For a candidate h and the surface form tokens s
h
it refers to, we calculate a total
match probability, taking into account the distance d(u, v) of all tokens in the candidate, the confidence
p
conf
(t
?
, s
h
) that the token actually belongs to the surface form, and the ratio
?
t
?
?h
|t
?
|/
?
t?s
h
|t| of the
surface form tokens covered by the candidate:
p
match
(h, s
h
) =
1
?
t?s
h
|t|
max
t?h
?
t
?
?h
(
p
3
dist
(t, t
?
) ? p
conf
(t
?
, s
h
) ? |t
?
|
)
, (2)
where p
?
dist
(u, v) := exp
(
?
d(u, v)
2
2?
2
)
(3)
models the confidence that two tokens u and v belong together given their distance in the text. In our example
of the candidate h = (?rat?) with the surface form tokens s
h
= (?wistar?, ?rats?) is p
match
(h, s
h
) =
1 ? 0.75 ?
3
6+4
= 0.225. Candidates with p
match
< 0.7 are discarded. The resulting set of all recognized
candidates is denoted with H .
Ontological reduction. As the algorithm ignores the hierarchical information provided by the ontologies,
we may obtain overlapping matches for ontologically related entities. Therefore, in case of overlapping
entities that are related in an ?is a? relationship in the ontology, only the more specific one is kept. Assume
for instance the candidates ?Rattus norvegicus? and ?Rattus norvegicus albus?, where the latter is more
specific and therefore accepted.
3.2 Relation Extraction
We frame relation extraction as a template filling task such that each slot provided by a relation has to be
assigned a filler of the correct entity class. Entity classes for the four relations of interest are shown in
5
Sayers et al. (2012), database limited to vertebrates: http://www.ncbi.nlm.nih.gov/taxonomy/?term=
txid7742[ORGN
6
Lipscomb (2000), excerpt of drugs from Descriptor and Supplemental: https://www.nlm.nih.gov/mesh/
28
Table 1, where required slots are in bold face, whereas all other slots are optional.
The slot filling process is based on testing all combinations of appropriate entities taking into account
their proximity and additional constraints. In more detail, we define the set of all recognized relationsR
?
of
a type ? as
R
?
=
?
?
?
r
?
? P(H)
?
?
?
?
?
?
p
sem
(r
?
)
n
?
?
?
h?r
?
,h6=g(r
?
)
p
match
(h, s
h
) min
t?h,t
?
?g(r
?
)
p
?
?
dist
(t, t
?
) > 0.2
?
?
?
(4)
where P(H) denotes the power set over all candidates H recognized by NER. g(r
?
) returns the filler for
the required slot of r
?
, p
match
and p
dist
are defined as in Section 3.1 and p
sem
implements manually defined
constraints on r
?
: A wrongly typed filler h for one slot of r
?
leads to p
sem
(r
?
) = 0, as does a negative number
in the Number slot of the Animal relation. Animal Numbers larger than 100 or Animal Weights smaller than
1 g or larger than 1 t are punished. All other cases lead to p
sem
(r
?
) = 1. Note that p
match
(h, s
h
) = 1 for
candidates h retrieved by rule-based entity recognition. Further, we set ?
Animal
= ?
Treatment
= 6, ?
Injury
= 10
and ?
Result
= 15.
4 Experiments
4.1 Data Set
Overall 1186
Organism 58
Weight 32
Sex 33
Age 17
Injury Height 35
Injury Type 62
Injury Device 23
Drug 134
Dosage 106
Delivery 70
Investigation Method 129
Trend 219
Significance 137
p Value 131
Table 2: The number of anno-
tations in our evaluation set for
each entity class.
The workflow is evaluated against an independent, manually annotated
corpus of 32 complete papers which contain 1186 separate annotations
of entities, produced by domain experts
7
. Information about relations
is not provided in the corpus. Only entities which participate in the
description of the preclinical experiment are marked. The frequencies
of annotations among the different classes are shown in Table 2.
4.2 Experimental Settings
We evaluate the system with regard to two different tasks: extraction (?Is
the approach able to extract relevant information from the text, without
regard to the exact location of the information??) and annotation (?Is the
system able to annotate relevant information at the correct location as in-
dicated by medical experts??). Furthermore, we distinguish between an
all instances setting, where we consider all instances independently, and
a fillers only setting, where only those annotations in the system output
are considered, that are fillers in a relation (i.e. the fillers only-setting
evaluates a subset of the all instances-setting). The relation extraction
procedure is not evaluated separately. For each setting, we report preci-
sion, recall, and F
1
measure.
Taking the architecture into account, we have the following hypotheses: (i) For the all instances setting we
expect high recall, but low precision. (ii) For the fillers only setting, precision should increase notably. (iii)
Comparing the all entities and the fillers only setting, recall should remain at the same level. We therefore
expect the extraction task to be simpler than the annotation task: For any information to be annotated at
the correct position, it must have been extracted correctly. On the other hand, information that has been
extracted correctly, can still be found at a ?wrong? location in the text. Thus, we expect a drop of precision
and recall when moving from extraction to annotation.
4.3 Results
The results are presented in Table 3: For each relation mentioned in Section 3, and the entity classes partic-
ipating in it, we report precision, recall and F
1
-measure
8
. This is done for all four combinations of setting
and task. For each relation we also provide the macro-average of precision, recall and F
1
-measure over all
entity classes considered in that relation and the overall average.
7
Performed in Prot?g? http://protege.stanford.edu/ with the plug-in Knowtator http://knowtator.
sourceforge.net/ (Ogren, 2006)
8
Note that VertebralPosition and InjuryHeight are merged in the result table, as are Organism and Laboratory Animal and
Age and Exact Age. The Animal Number was excluded from the evaluation as it has not been annotated in our evaluation set.
29
Task Extraction Annotation
Setting All Instances Fillers Only All Instances Fillers Only
Entity Class Prec. Rec. F
1
Prec. Rec. F
1
Prec. Rec. F
1
Prec. Rec. F
1
Overall Average 0.58 0.95 0.72 0.68 0.81 0.74 0.13 0.77 0.22 0.21 0.51 0.30
Animal Average 0.62 0.99 0.76 0.82 0.94 0.87 0.12 0.91 0.21 0.31 0.81 0.44
Organism 0.41 1.00 0.58 0.88 0.90 0.89 0.02 1.00 0.04 0.24 0.66 0.35
Weight 0.20 1.00 0.33 0.52 0.94 0.67 0.08 0.97 0.15 0.49 0.91 0.64
Sex 0.85 0.99 0.91 0.87 0.98 0.92 0.18 0.94 0.30 0.26 0.94 0.41
Age 1.00 0.95 0.97 1.00 0.93 0.96 0.19 0.71 0.30 0.23 0.71 0.35
Injury Average 0.63 0.94 0.76 0.74 0.75 0.75 0.12 0.72 0.21 0.18 0.38 0.24
Injury Height 0.42 0.98 0.59 0.56 0.74 0.64 0.10 0.91 0.18 0.24 0.51 0.33
Injury Type 0.70 0.91 0.79 0.81 0.73 0.77 0.07 0.48 0.12 0.18 0.35 0.24
Injury Device 0.78 0.93 0.85 0.86 0.79 0.82 0.20 0.77 0.32 0.11 0.28 0.16
Treatment Average 0.45 0.91 0.61 0.53 0.78 0.63 0.14 0.72 0.23 0.19 0.54 0.28
Drug 0.10 0.98 0.18 0.24 0.69 0.36 0.01 0.74 0.02 0.10 0.42 0.16
Dosage 1.00 0.81 0.90 1.00 0.76 0.86 0.30 0.52 0.38 0.32 0.46 0.38
Delivery 0.26 0.95 0.41 0.34 0.89 0.49 0.11 0.89 0.20 0.15 0.74 0.25
Result Average 0.59 0.93 0.72 0.60 0.75 0.67 0.13 0.71 0.22 0.15 0.30 0.20
Investigation Method 0.29 0.96 0.45 0.27 0.79 0.40 0.03 0.66 0.06 0.02 0.16 0.04
Trend 0.37 0.91 0.53 0.44 0.78 0.56 0.06 0.63 0.11 0.07 0.27 0.11
Significance 0.70 0.90 0.79 0.70 0.71 0.70 0.17 0.69 0.27 0.22 0.39 0.28
p Value 1.00 0.96 0.98 1.00 0.71 0.83 0.27 0.86 0.41 0.30 0.36 0.33
Table 3: The macro-averaged evaluation results for each class given in precision, recall and F
1
measure.
For the extraction task with all instances setting, recall is close to 100% for all entity classes considered
in the Animal relation. It is 81% for Dosages. The rule-based recognition for Dosages (as for Ages and p
Values) is very precise: All recognized entities have been annotated by medical experts somewhere in the
document. This strong difference between entity classes can be observed in the annotation task and the fillers
only setting as well: The best average performance in F
1
-measure is achieved for entity classes that are part
of the Animal relation. Precision is best for Dosages, Ages and p Values.
The recall for the all instances setting is high in both the extraction and in the annotation task. However,
the number of annotated instances (29,628 annotations in total) is about 25 times higher than the number of
expert annotations, which leads to low precision especially in the annotation task. For the fillers only setting,
the number of annotations decreases dramatically (to 4069 annotations); at the same time, precision improves.
Regarding the comparison of both tasks, precision and recall are both notably lower in the annotation task,
for the all entities setting, as well as for the fillers only setting. The overall recall is lower by 14 percentage
points (pp) in the extraction task and by 26 pp in the annotation task when considering the fillers only setting.
The decrease is most pronounced for Investigation Methods in the annotation task with a drop of 50 pp.
4.4 Discussion
The results are promising for named entity recognition. Recall is close-to-perfect in the extraction task
and acceptable in the annotation task. The results for relation extraction leave space for improvement: An
increase in precision can be observed but the decrease in recall is too substantial. The Animal relation is an
exception, where an increase in F
1
measure is observed for the fillers only setting for nearly all entity classes,
leading to 0.87 F
1
for Animals in the extraction task.
An error analysis revealed that for the fillers only setting, most false positives (55%) are due to the fact
that the medical experts did not annotate all occurrences of the correct entity, but only one or a few. 18% are
due to ambiguities of surface forms (for instance the abbreviation ?it? for ?intrathecal? leads to many false
positives). Regarding false negatives, 41% are due to missing entries in our ontology database and further
26% are caused by wrong treatment of characters (mostly wrong transcriptions of characters from the PDF).
30
5 Conclusion and Outlook
We described the challenge of extracting relational descriptions about preclinical experiments on spinal cord
injury from scientific literature. To tackle that challenge, we introduced a cascaded approach of named
entity recognition, followed by relation extraction. Our results show that the first step can be achieved by
relying strongly on domain-specific ontologies. We show that modeling relations as aggregated entities,
and extracting them using a distance filtering principle combined with domain specific knowledge, yields
promising results, specifically for the Animal relation.
Future work will focus on improving the recognition at the correct position in the text. This is a pre-
requisite to actually tackle and evaluate the relation extraction not only on the basis of detected participating
entities. Therefore, improved relation detection approaches will be implemented which relax the assumption
that relevant entities are found close-by in the text. In addition, we will relax the assumption that different
slots of the annotation are all equally important. Finally, we will address aggregation beyond individual
relations in order to allow for a fully accurate holistic assessment of experimental therapies.
Our system offers a semantic analysis of scientific papers on spinal cord injuries. This lays groundwork
for populating a comprehensive semantic database on preclinical studies of SCI treatment approaches as de-
scribed by Brazda et al. (2013), laying ground and supporting transfer from preclinical to clinical knowledge
in the future.
References
N. Brazda, M. Kruse, F. Kruse, T. Kirchhoffer, R. Klinger, and H.-W. M?ller. 2013. The CNR preclinical database
for knowledge management in spinal cord injury research. Abstracts of the Society of Neurosciences, 148(22).
P. Buitelaar, P. Cimiano, A. Frank, M. Hartung, and S. Racioppa. 2008. Ontology-based information extraction
and integration from heterogeneous data sources. Int. J. Hum.-Comput. Stud., 66(11):759?788.
F. J. Damerau. 1964. A Technique for Computer Detection and Correction of Spelling Errors. Commun. ACM,
7(3):171?176, March.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw, S. Strassel, and R. Weischedel. 2004. The Automatic
Content Extraction (ACE) program: tasks, data, and evaluation. In Proceedings of LREC 2004, pages 837?840.
A. Doms and M. Schroeder. 2005. GoPubMed: exploring PubMed with the Gene Ontology. Nucleic Acids Res,
33(Web Server issue):W783?W786, Jul.
D. Ferrucci and A. Lally. 2004. Building an example application with the Unstructured Information Management
Architecture. IBM Systems Journal, 43(3):455?475.
L. Filli and M. E. Schwab. 2012. The rocky road to translation in spinal cord repair. Ann Neurol, 72(4):491?501.
A. Franceschini, D. Szklarczyk, S. Frankild, M. Kuhn, M. Simonovic, A. Roth, J. Lin, P. Minguez, P. Bork, C. von
Mering, and L. J. Jensen. 2013. STRING v9.1: protein-protein interaction networks, with increased coverage
and integration. Nucleic Acids Res, 41(Database issue):D808?D815, Jan.
J. Hakenberg, M. Gerner, M. Haeussler, I. Solt, C. Plake, M. Schroeder, G. Gonzalez, G. Nenadic, and C. M.
Bergman. 2011. The GNAT library for local and remote gene mention normalization. Bioinformatics,
27(19):2769?2771, Oct.
D. Hanisch, K. Fundel, H.-T. Mevissen, R. Zimmer, and J. Fluck. 2005. ProMiner: rule-based protein and gene
entity recognition. BMC Bioinformatics, 6 Suppl 1:S14.
M. Hofmann-Apitius, J. Fluck, L. Furlong, O. Fornes, C. Kolarik, S. Hanser, M. Boeker, S. Schulz, F. Sanz,
R. Klinger, T. Mevissen, T. Gattermayer, B. Oliva, and C. M. Friedrich. 2008. Knowledge environments
representing molecular entities for the virtual physiological human. Philos Trans A Math Phys Eng Sci,
366(1878):3091?3110, Sep.
H. Ji and R. Grishman. 2008. Refining Event Extraction through Cross-Document Inference. In Proceedings of
ACL-08: HLT, pages 254?262, Columbus, Ohio, June. Association for Computational Linguistics.
A. Jimeno, E. Jimenez-Ruiz, V. Lee, S. Gaudan, R. Berlanga, and D. Rebholz-Schuhmann. 2008. Assessment of
disease named entity recognition on a corpus of annotated sentences. BMC Bioinformatics, 9 Suppl 3:S3.
31
J. Lafferty, A. McCallum, and F. C. N. Pereira. 2001. Conditional Random Fields: Probabilistic Models for
Segmenting and Labeling Sequence Data. In Proceedings of ICML 2001, pages 282?289. Morgan Kaufmann.
V. P. Lemmon, A. R. Ferguson, P. G. Popovich, X.-M. Xu, D. M. Snow, M. Igarashi, C. E. Beattie, J. L. Bixby
et al. 2014. Minimum Information About a Spinal Cord Injury Experiment (MIASCI) ? a proposed reporting
standard for spinal cord injury experiments. Neurotrauma. in press.
C. E. Lipscomb. 2000. Medical Subject Headings (MeSH). Bull Med Libr Assoc, 88(3):265?266, Jul.
C. Lok. 2010. Literature mining: Speed reading. Nature, 463(7280):416?418, Jan.
D. Nadeau and S. Sekine. 2007. A survey of named entity recognition and classification. Lingvisticae Investiga-
tiones, 30(1):3?26.
C. Nedellec, R. Bossy, J.-D. Kim, J. jae Kim, T. Ohta, S. Pyysalo, and P. Zweigenbaum, editors. 2013. Proceedings
of the BioNLP Shared Task 2013 Workshop. Association for Computational Linguistics, Sofia, Bulgaria, August.
P. V. Ogren. 2006. Knowtator: a prot?g? plug-in for annotated corpus construction. In Proceedings NAACL/HLT
2006, pages 273?275, Morristown, NJ, USA. Association for Computational Linguistics.
E. Pafilis, S. P. Frankild, L. Fanini, S. Faulwetter, C. Pavloudi, A. Vasileiadou, C. Arvanitidis, and L. J. Jensen.
2013. The SPECIES and ORGANISMS Resources for Fast and Accurate Identification of Taxonomic Names
in Text. PLoS One, 8(6):e65390.
F. Prinz, T. Schlange, and K. Asadullah. 2011. Believe it or not: how much can we rely on published data on
potential drug targets? Nat Rev Drug Discov, 10(9):712, Sep.
H. Saggion, A. Funk, D. Maynard, and K. Bontcheva. 2007. Ontology-Based Information Extraction for Business
Intelligence. In K. A. et al., editor, The Semantic Web, volume 4825 of Lecture Notes in Computer Science,
pages 843?856. Springer.
G. K. Savova, J. J. Masanz, P. V. Ogren, J. Zheng, S. Sohn, K. C. Kipper-Schuler, and C. G. Chute. 2010. Mayo
clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and
applications. J Am Med Inform Assoc, 17(5):507?513.
E. W. Sayers, T. Barrett, D. A. Benson, E. Bolton, S. H. Bryant, K. Canese, V. Chetvernin, D. M. Church, M. Dicuc-
cio, S. Federhen, M. Feolo, I. M. Fingerman, L. Y. Geer, W. Helmberg, Y. Kapustin, S. Krasnov, D. Landsman,
D. J. Lipman, Z. Lu, T. L. Madden, T. Madej, D. R. Maglott, A. Marchler-Bauer, V. Miller, I. Karsch-Mizrachi,
J. Ostell, A. Panchenko, L. Phan, K. D. Pruitt, G. D. Schuler, E. Sequeira, S. T. Sherry, M. Shumway, K. Sirotkin,
D. Slotta, A. Souvorov, G. Starchenko, T. A. Tatusova, L. Wagner, Y. Wang, W. J. Wilbur, E. Yaschenko, and
J. Ye. 2012. Database resources of the National Center for Biotechnology Information. Nucleic Acids Res,
40(Database issue):D13?D25, Jan.
M. Schuemie, R. Jelier, and J. Kors. 2007. Peregrine: lightweight gene name normalization by dictionary lookup.
In Proceedings of the Biocreative 2 workshop 2007, page 131?140, Madrid, Spain, April.
O. Steward, P. G. Popovich, W. D. Dietrich, and N. Kleitman. 2012. Replication and reproducibility in spinal cord
injury research. Exp Neurol, 233(2):597?605, Feb.
S. Strassel, M. Przybocki, K. Peterson, Z. Song, and K. Maeda. 2008. Linguistic Resources and Evaluation
Techniques for Evaluation of Cross-Document Automatic Content Extraction. In Proceedings of the Language
Resources and Evaluation Conference, pages 2706?2709.
P. Thomas, J. Starlinger, A. Vowinkel, S. Arzt, and U. Leser. 2012. GeneView: a comprehensive semantic search
engine for PubMed. Nucleic Acids Res, 40:W585?W591, Jul.
J. Tsujii, J.-D. Kim, and S. Pyysalo, editors. 2011. Proceedings of BioNLP Shared Task 2011 Workshop. Associa-
tion for Computational Linguistics, Portland, Oregon, USA, June.
J. Tsujii, editor. 2009. Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task. Associa-
tion for Computational Linguistics, Boulder, Colorado, June.
D. C. Wimalasuriya and D. Dou. 2010. Ontology-based information extraction: An introduction and a survey of
current approaches. Journal of Information Science, 36(3):306?323.
32
