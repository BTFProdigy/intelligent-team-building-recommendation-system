Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 441?448
Manchester, August 2008
Normalizing SMS: are two metaphors better than one ?
Catherine Kobus
Orange Labs
2, avenue Pierre Marzin
F-22300 Lannion
Fran?ois Yvon
Univ Paris-Sud 11 & LIMSI/CNRS
BP 133
F-91403 Orsay Cedex
{catherine.kobus,geraldine.damnati}@orange-ftgroup.com, yvon@limsi.fr
G?raldine Damnati
Orange Labs
2, avenue Pierre Marzin
F-22300 Lannion
Abstract
Electronic written texts used in computer-
mediated interactions (e-mails, blogs,
chats, etc) present major deviations from
the norm of the language. This paper
presents an comparative study of systems
aiming at normalizing the orthography of
French SMS messages: after discussing
the linguistic peculiarities of these mes-
sages, and possible approaches to their au-
tomatic normalization, we present, evalu-
ate and contrast two systems, one draw-
ing inspiration from the Machine Transla-
tion task; the other using techniques that
are commonly used in automatic speech
recognition devices. Combining both ap-
proaches, our best normalization system
achieves about 11% Word Error Rate on a
test set of about 3000 unseen messages.
1 Introduction
The rapid dissemination of electronic communi-
cation devices (e-mails, Short Messaging Systems
(SMS), chatrooms, instant messaging programs,
blogs, etc) has triggered the emergence of new
forms of written texts (see eg. (Crystal, 2001;
V?ronis and Guimier de Neef, 2006)). Addressed
to relatives or peers, written on the spur of the
moment, using interfaces, each with its specific
constraints (computer keyboards, PDAs, mobile
phones keypads), these electronic messages are
characterised by massive and systematic devia-
tions from the orthographic norm, as well as by
a non conventional use of alphabetical symbols.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
In fact, letter and punctuation marks are not only
used to conventionally encode a phonetic content,
but also to introduce meta-discourse, and to sig-
nal emotions, verbal effects (eg. laughters), or at-
titudes (humor, derision, emphasis etc). If each
media enforces its own set of constraints and pro-
motes idiosyncratic forms of writings, these new
types of texts nonetheless share a lot of common-
alities. To effectively process these messages, it
is thus necessary to develop robust language pro-
cessing tools, capable of bearing with the extreme
form of ?noise? they contain. In this study, we
focus more specifically on SMS, which, due to
the paucity of their input interface (mobile phone
keypads) seem to constitute the most challenging
type of data. To a large extent, the techniques we
present in this paper are also applicable to other
types of electronic messages.
The ?SMS language? (or ?texting language?)
has been the subject of several linguistic studies
(notably, for French, (Anis, 2001; Fairon et al,
2006)), which have emphasized its main charac-
teristics, amongst which the extraordinary ortho-
graphic variability of lexical forms. In brief, this
variability results partly from the mixing of several
encoding systems: in SMS, the usual alphabetic
system competes with a more ?phonetic? type of
writing (e.g. rite for right
1
), as well as with traces
of a ?consonantic? spelling (vowels are deleted,
as in wrk for work or cn for can), and with
non-conventional use of letters or numbers, some-
times used to encode the phonetic value of their
1
We will illustrate this general presentation of the SMS
language using examples taken from English messages, even
though our systems deal with French messages. As far as we
can see, the same types of deviations from the orthographic
norm are observed in both languages, albeit in different pro-
portions. A more thorough comparison of both languages cer-
tainly remains to be carried out.
441
spelling, as in ani1 for anyone. These spelling sys-
tems can also be mixed as in Rtst (for artist) or
bcum (for become). This variability is also the re-
sult of an informal style of communication, which
licenses many deviations from the orthographic
(simplification of repeated consonants, use of non-
conventional abreviations) and grammatical (ab-
sence of case distinction, erratic use of punctuation
marks, non-respect of agreement or tense mark-
ers, etc) prescriptions, notwithstanding truly unin-
tentional typos. Finally, practitioners of the tex-
ting language excel in devising acronyms which
condense, sometimes in a radical way, multi-word
units: this is for instance the case with afair, which
stands for as far as I recall. As a result, from a
natural language processing (NLP) point of view,
these messages contain an abnormally high rate of
out-of-vocabulary forms, and the ambiguity of ex-
isting word forms is aggravated, two factors that
contribute to degrade the performance of natural
language processing tools. Recovering a normal-
ized orthography seems thus to be a necessary pre-
processing step for many real-world NLP applica-
tions, such as text-to-speech, translation, or text
mining applications (filtering, routing, information
retrieval, etc).
These short messages have so far received rel-
atively little attention from the NLP community
2
:
see, for English, (Aw et al, 2006; Choudhury et
al., 2007), which both address the problem with
statistical learning techniques, and, for French,
(Guimier de Neef et al, 2007), which details
a complete pipe-line of hand-crafted, symbolic,
modules. In fact, the problem of normalizing
SMS shares a lot of commonalities with other NLP
applications, and can be addressed from several
viewpoints. The first, maybe the most natural an-
gle, is to make an analogy with the spelling cor-
rection problem. This problem has been exten-
sively studied in the past and a variety of statisti-
cal approaches are readily available, most notably
the ?noisy channel? approach (see eg. (Church and
Gale, 1991; Brill and Moore, 2000; Toutanova and
Moore, 2002)). An alternative metaphor is the
translation metaphor: under this view, the normal-
ization task is accomplished by taking the SMS
2
A couple of on-line SMS-to-English translation systems
are accessible on the Internet, see notably http://www.
transl8it.com/ and http://www.lingo2word.
com/; ?Netspeak? dictionaries, again for English, also
abound. The situation is more or less comparable for French,
see eg http://www.traducteur-sms.com/.
language as a foreign language, and using standard
(statistical) translation techniques. Both views
have their own merit, and their limitations, which
we shall review shortly. In this paper, we propose
yet another metaphor, which stems from the simi-
larities between the SMS language and speech, and
notably from the fact that in SMS, word separators
are much less reliable than in conventional writ-
ings. As a result, it seems necessary to implement
techniques, which are, as in speech recognition,
capable of recovering the correct word segmenta-
tion.
The main contribution of this work is to present,
evaluate and contrast two approaches to the SMS
normalization problem. We demonstrate that both
approaches have different advantages and pitfalls,
and show their combination yields significant im-
provements wrt. to both single systems.
This paper is organized as follows: in section 2,
we discuss these three metaphors; then go on to
describe our own implementation of two different
systems for normalizing SMS in 3. A comparative
evaluation of these systems is conducted in sec-
tion 4, where we emphasize the complementarity
of both approaches, and assess the performance of
a combined systems. Section 5 presents a sum-
mary of the main findings and future prospects.
2 Three metaphors for SMS
normalization systems
In this section, we set the general problem of SMS
normalization, and discuss, based on the analysis
of SMS examples, the relevance of various NLP
approaches to this task.
2.1 The "spell checking" metaphor
A first approach to the problem considers each in-
put token as ?noisy? version of the correct word
form: normalization is thus viewed as a spell
checking task. The spell-checking problem has re-
ceived considerable attention in the past, and a va-
riety of correction techniques have been proposed:
in this context, noisy channel models (Church and
Gale, 1991; Brill and Moore, 2000; Toutanova and
Moore, 2002), to quote just a few, constitute one of
the predominant and most successfull approaches.
Under this paradigm, correction is performed on a
word-per-word basis, and concerns primarily out-
of-vocabulary tokens: the general assumptions are
that most words are correctly spelled, and that
in-vocabulary words should preferably be left un-
442
touched. In this context, the best correction(s) w
of an erroneous word v is retrieved from the dictio-
nary by combining the individual context indepen-
dant probability ofw with an error model probabil-
ity, which computes the probability of mistyping v
forw, based on the surface similarity between both
forms. The key component here is the error model,
which should not only capture orthographic simi-
larities (Brill and Moore, 2000), but also phonetic
similarities (Toutanova and Moore, 2002).
This framework easily extends to the case where
several words should simultaneously be corrected:
it is simply a matter of exploring the lattice of all
possible corrections, which can be re-ranked us-
ing conventional tools such as statistical language
models. This is basically the idea behind the reac
system (Michel Simard, 2001),which recovers the
correct accentuation of unaccented French texts
using an error model, complemented with a sta-
tistical language model.
As far as SMS are concerned, this approach
is essentially the one followed by (Choudhury
et al, 2007), which models the joint probabil-
ity of observing the word w represented by the
character sequence c
1
...c
l
by a Hidden Markov
Model (HMM) whose topology takes into account
both ?graphemic? variants (typos, omissions of re-
peated letters, etc) and ?phonemic? variants (i.e
spellings that resemble the word?s pronunciation).
This HMM is initialized by considering the word?s
received orthography and phonology, with addi-
tional transitions to account for the possibility of
inserting, substituting or deleting symbols. For the
most frequent words in the corpus, the various pa-
rameters associated with these transitions are esti-
mated on a training corpus; various heuristic are
then used to plug these values in the HMMs that
model the less frequent dictionary items.
2.2 The ?translation? metaphor
A second approach to the problem consists in
adopting the translation metaphor: using this anal-
ogy, the SMS language is just another foreign lan-
guage and the normalization can be viewed as a
?pure? machine translation (MT) task.
Using machine translation tools might be re-
garded as ?an overkill? (Choudhury et al, 2007),
considering the close relationships between source
and target languages. Furthermore, learning the
kinds of many-to-many correspondences between
source and target sentences that make up for the
high translation accuracy of phrase-based systems
might be seen as introducing an unnecessary com-
plexity, as SMS tend to be shorter, in terms of
words, than their normalized counterparts. This
suggests that looking for many (on the normal-
ized side) to one (on the SMS side) might be good
enough to capture most pairings. Finally, statistical
machine translation tools incorporate mechanisms
to model the possible mismatch in word order be-
tween source and target, which are virtually non-
existing when it comes to translating SMS.
This metaphor is, nonetheless, the one resorted
to in (Aw et al, 2006), which uses a statisti-
cal phrase-based machine translation tool to con-
vert English SMS texts into standardized English.
This system incorporates some of the peculiarities
of this translation task, which both simplifies the
construction of the phrase-table and the decoding
search algorithm. Using this system, (Aw et al,
2006) reports a 0.81 BLEU (Papineni et al, 2001)
score on a set of 5,000 English SMS.
Normalization as translation is certainly a nat-
ural, and simple to implement, idea. Using
phrase-based systems, it becomes possible to
model (context-dependant) one-to-many relation-
ships that are out-of-reach of the spell checking
approach. We feel that it still overlooks some as-
pects of the task, notably the fact that the lexi-
cal creativity attested in SMS messages can hardly
be captured in a static phrase table, where corre-
spondences between SMS phrases and normalized
phrases are learned by rote, rather than modeled.
2.3 The "speech recognition" metaphor
The SMS language has on occasions been de-
scribed, sometimes abusively, as being closer to
oral productions than to regular written texts. If
we do not subscribe to this view, we nonetheless
feel that a third metaphor is worth considering, that
we call the ?automatic speech recognition? (ASR)
metaphor. This analogy stems from the fact that,
for a significant fraction of tokens, the spelling of
SMS forms tends to be a closer approximation of
the phonemic representation of a word than of is
its normative spelling.
In the speech recognition metaphor, an SMS
message is thus primarily viewed as an alpha-
betic/syllabic approximation of a phonetic form.
Given a suitable mechanism for converting the
SMS stream into a phone lattice, the problem of
SMS normalization becomes very similar to that
443
of speech recognition, that is, the decoding of a
word sequence in a (weighted) phone lattice. It
actually becomes a much simpler problem, as (i)
the acoustic ambiguity of speech input is typically
much higher than the phonemic indeterminacy of
SMS messages, (ii) some segmentation informa-
tion is already available in the SMS text, which is
in sharp contrast with (continuous) speech recogni-
tion, where word boundaries have to be uncovered.
Based on this general principle, we devised an
ASR-like normalization system, which has three
additional benefits: using a phonemic approxima-
tion provides the system with the ability to correct
some (unintentional) typos; adopting an ASR-like
architecture provides us with a ?natural? frame-
work for resegmenting agglutinated word forms;
finally, in the larger context of SMS-to-speech ap-
plications, which is one of our targeted applica-
tions, the computation of a phonemic representa-
tion of the message can prove extremely valuable.
3 Two normalization systems
3.1 The MT-like system
Our first normalization system is entirely based on
open-source, public domain packages for statisti-
cal machine translation. Giza++ (Och and Ney,
2003) is used to induce, based on statistical princi-
ples (Brown et al, 1990), an automatic word align-
ment of SMS tokens with their normalized coun-
terparts; Moses (Koehn et al, 2007) is used to
learn the various parameters of the phrase-based
model, to optimize the weight combination and to
perform the translation using a multi-stack search
algorithm; the SRI language model toolkit (Stol-
cke, 2002) is finally used to estimate statistical lan-
guage models. For this system, the training set has
been split in a learning set
3
(approximately 25000
messages) and a development set (about 11700
messages), which is used to tune parameters.
As suggested in the previous sections, we have
constrained both systems to consider only ?mono-
tonic? alignments between the source and the tar-
get languages.
3.2 The ASR-like system
In a nutshell, our second normalization system
mimics the behavior of speech recognition sys-
tem and decodes SMS message through a non-
deterministic phonemic transduction; based on
preliminary experiments, this simple architecture
3
See section 4 for a description of the corpora.
was augmented by an additional mechanism which
specifically deals with out-of-vocabulary tokens.
In the following, we denote ? the set of alpha-
betic symbols, ? the set of phonemic symbols, and
? the set of lexical items. Using these notations,
our architecture can be described as a pipe-line of
the following components:
? the first processing step consists in a
dictionary-based grapheme-to-phoneme con-
version of some highly idiosyncratic forms,
which deals with tokens
4
whose spelling in
SMS does not reflect the phonemic content of
the corresponding lexical item(s). This is, for
instance, the case for common abbreviations
(eg. btw for by the way) and for instances
of ?consonantic? spellings. The dictionnary
used in the experiments reported above con-
tains about 4,200 entries.
This module is implemented as a finite-
state transducer E which transduces letter
sequences in ?
?
into mixed grapheme and
phoneme sequences (in (? ? ?)
?
).
? the second module converts the graphemic
portions of the input message into a phone-
mic string using a set of manually encoded
non-deterministic letter-to-phone rules; these
rules notably encode the possibility for each
symbol to encode its spelling (eg. u for /ju/ or
R for /@r/). Our system currently comprises
about 150 letter-to-phone rules. The output
of this module is a phone lattice, which rep-
resents all the possible pronunciations of the
complete input stream.
This module is also implemented as a finite-
state transducer P representing a rational
relation between (? ? ?)
?
and ?
?
: each
grapheme-to-phoneme rule is compiled into a
finite-state transducer; these individual rules
are then, once properly ordered, combined
through the composition operator. The result-
ing finite-state machine is denoted P .
? this phone lattice is then turned into a word
based lattice, using an inverted pronuncia-
tion dictionary, which registers the known
associations between phone sequences and
words. This inverted dictionary contains ap-
proximately 21K words, which are the most
4
At this stage, we take advantage of usual word separators
to identify tokens in the message.
444
frequent words in our reference training cor-
pus. This module is also implemented as
a finite-state transducer D, which maps se-
quences in ?
?
to sequences in ?
?
.
A key aspect of this module is its ability to
alter the original tokenization, by freely in-
serting word separators whenever a phonetic
word is recognized, no matter whether it cor-
responds to a complete input token or not.
This mechanism is illustrated on Figure 1. As
a result, this system can bear with agglutina-
tions (i.e. absence of one or several word sep-
arators) in the input sequence.
? the final processing step consists in searching
the word lattice for the most probable word
sequence, as computed by a statistical lan-
guage model (here, a smoothed n-gram lan-
guage model estimated on the training cor-
pus). Here again, the entire process is com-
puted through finite state operations: the out-
put language of the previous steps is inter-
sected with the stochastic language model
S (a weighted finite-state acceptor), and the
most likely path is computed through dy-
namic programming.
As mentioned earlier, each module is imple-
mented as a finite state acceptor or transducer;
these modules are built and combined using tools
from the FSM (Mohri et al, 1998) and the GRM
(Allauzen et al, 2005) toolkit. As a result, the
entire normalization process is computed by a
weighted transducer (E ?P ?D ?S), which can be
optimized off-line as is commonly done in finite-
state speech recognition systems (Mohri and Riley,
1998).
In addition to these four main modules, the pre-
processing module of the ASR-like system con-
tains a number of small enhancements that im-
prove the normalization of dates and hours. We
furthermore had to modify the processing of out-
of-vocabulary words: in the architecture sketched
above, any word that does not belong to the vocab-
ulary has to be decomposed into smaller, known,
words, causing systematic errors. Our final ASR-
like system allows these forms to be either decom-
posed phonetically or copied verbatim in the out-
put. A complete description of this system is given
in (Kobus et al, 2008).
1
2
3
0/0
4
5
6
<e
ps>
:<e
ps>
_#
:<e
ps>
p:p
au
l
O:<
ep
s>
l:<
ep
s>
_#
:<e
ps>
<e
ps>
:<e
ps>
l:lo
uis
w:<
ep
s>
i:<
ep
s>
Figure 1: Transducing phone sequences into word
sequences with a dictionary
This simplistic inverted dictionary recognizes two
phonemic sequences: /lwi/ (for Louis) and /pOl/
(for Paul). Upon recognition of any such sequence,
two transitions loop back to the initial state: one
carries the input symbol ?#?, which is used when-
ever a word separator is encountered; the other is
an ? transition, which allows to re-segment the in-
put stream.
4 Experiments
4.1 Experimental protocol
The experiments reported below use two corpora.
The first one has been collected at the University
of Aix-en-Provence (Hocq, 2006); it contains ap-
proximately 9700 messages. The second corpus
has been gathered in Belgium by the Catholic Uni-
versity of Louvain, and totals about 30000 mes-
sages (Fairon and Paumier, 2006). Both corpora
contain, for each message, a reference normaliza-
tion which has been produced and validated by
human annotators. Both corpora were merged,
lowercased, stripped from punctuation signs and
standardized (in particular with respect to the
anonymization conventions). This database was
split in a training set (about 36700 messages) and
a distinct test set of about 3000 messages. The
training set was used both to train and tune the
MT-like system and to estimate a 3-gram language
model required in both approaches, using standard
back-off procedures. Some relevant statistics re-
garding the sub-corpora that were used for training
are given in Table 4.1.
For the evaluation, contrarily to (Aw et al, 2006;
Guimier de Neef et al, 2007), who by analogy
445
Aix Louvain Total
# messages 8,700 28,000 36,700
Original messages
avg. length 14.3 21.6 19.9
# tokens 124 700 606 100 730 800
# types 13 600 37 900 43 600
% unknown 43.7 % 30.4 % 32.7 %
Normalized messages
avg. length 15.4 23.2 21.3
# tokens 133 800 650 100 783 900
# types 8 200 20 800 23 300
Table 1: Statistics of the training corpora
Statistics on the original messages are computed
after preprocessing (punctuation removal, etc.);
the length of a message is the number of tokens;
% unknown is the percentage of tokens that do not
occur in the normalized message.
with the machine translation task, assess their sys-
tem with the BLEU metric (Papineni et al, 2001),
we decided to measure the performance of our nor-
malization tool with the Word Error Rate (WER)
and Sentence Error Rate (SER) metrics. This
choice is motivated by the fact that the outcome
of the normalization process is _ notwithstanding
a couple of arbitrary normalization decisions _ al-
most deterministic, and does not warrant the use
of BLEU, which is more appropriate to evaluate
tasks with multiple references. Additionally, we
feel that error rates are easier to interprete than
BLEU values; for the sake of comparisons, some
BLEU scores will nonetheless be reported.
4.2 Baseline results
In a first series of experiments, we evaluate our two
systems and analyze their respective strengths and
weakness. Table 4.2 reports the results of these ex-
periments; the line ?initial? gives the corresponding
numbers for the original messages, which gives a
rough idea of the number of words that must be
modified. As these results demonstrate, the MT-
like system proves to be much more accurate than
the ASR-like system.
Looking at the errors, the main problem with
the latter system stems from the loss of the origi-
nal spelling and tokenization information incurred
during the grapheme-to-phoneme conversion step:
as a consequence, many words that were correctly
spelled in the input message are erroneously reseg-
mented and decoded. This is evidenced by the high
Sub Ins Del WER SER
initial 33.23 0.42 8.54 42.18 91.39
ASR-like 11.94 2.21 2.36 16.51 76.05
MT-like 7.34 0.71 4.22 12.26 63.41
Table 2: Evaluation of the baseline systems
Columns ?Sub?, ?Ins?, and ?Del? report respectively
the number of substitution, insertion and deletion
errors at the word level.
number of substitutions and insertions produced
by this system, a large number of which concern
function words. This phenomena is accentuated by
the excessive liberality of grapheme-to-phoneme
rules. For instance, to account for the erratic use of
accentuated letters in SMS, the most general pro-
nunciation rule for letter e (incidentally, e is the
most frequent letter in French) predicts five pro-
nunciations: /@/, /e/, /E/, /?/, /?/, plus the pos-
sibility of being deleted. As a result, first group
verbal forms such as aime (?I or he/he love(s)?)
yield (at least) six different pronunciations, which
result in many more combinations after resegmen-
tation, such as aime (?(I, you, he/she) love(s)?),
aim? (?loved?), aimer (?to love?), aime et (?I love
and), aime est (I love is) etc. The same occurs with
de (?of (the)?, ?some-SING?) and le (?the-SING?),
which, through the phonemic encoding/decoding,
become systematically ambiguous with their cor-
responding plural des and les. Sorting out the cor-
rect combination seems to be too hard a task for
the statistical language model, since all these hy-
potheses include very high frequency tokens.
The MT-like system is significantly less error-
prone: an error analysis reveals that the most com-
mon errors concern the insertion or deletion of
function words, which can be attributed to noisy
alignments in the phrase table. Another frequent
source of error stems from agglutinated forms, no-
tably combinations of clitic(s)+verb (e.g. jtombrai
for je tomberai (?I will fall?), jrentr for je rentre
(?I am coming back?)) or (preposition and/or ar-
ticle)+noun or verbs (e.g. cours 2droit for cours
de droit (?law class?), dsortir for de sortir (?to
go out?)...): whenever these forms are met in the
training corpus, they can be correctly decoded;
however, many novel forms only occur in the test
set, owing to the fact that these types of aggluti-
nations are ?productive? (in the appropriate con-
text). Contrarily to the findings of (Choudhury
et al, 2007), which considered English messages,
446
our corpus study reveals that this phenomena is far
from marginal, and is a systematic source of errors
for theMT system. It is noteworthy that about 17%
of the tokens in the test SMS corpus do not occur in
the training set, when the ?true? out-of-vocabulary
rate (computed on the reference messages) is only
about 2.1 %.
Both systems are finally at pain to correctly
recover the right number/gender/tense agreement,
which is a general problem with n-gram language
models in French, aggravated here by some irre-
ducible indeterminacy: should ?d?sol? ? (masc.) in
?je suis d?sol? ? be corrected as ?d?sol?e? (fem) ?
ultimately, this depends on the sex of the sender,
which may be deduced from some other, poten-
tially long distant, part of the message; the same
indeterminacy occurs with the normalization of
?1 ?, which can be mapped to ?un? (masc.) or ?une?
(fem); with ?aurai ? (?I will have?), which can be
mapped with ?aurai ? or ?aurais? (?I would have?),
etc.
4.3 System combination
The analysis of normalization errors reveals that
both systems have different strengths and weak-
nesses, suggesting that they could be used in com-
bination. Indeed, oracle selection of the best out-
put on a per message basis would yield an over-
all 9,63 WER, about 2.5 points absolute below the
performance of the MT-like system.
Various ways to combine both approaches have
been considered: we eventually decided to use the
MT-like system for producing a first normaliza-
tion; out-of-vocabulary tokens in the original SMS
appear untouched in this output. For each of these,
we use the ASR-like system to produce a series of
?local? hypothesis, which are combined in a word
lattice. This lattice is rescored with the statisti-
cal language model to yield the final output. This
simple combination proved to yield significant im-
provements, decreasing the word-error rate from
12.26 to 10.82. The corresponding BLEU score
is close to 0,8, in line with the findings of (Aw
et al, 2006) for English, and comparing favorably
with the 0.68 score reported in (Guimier de Neef et
al., 2007) (for French, using a different test bed).
Preliminary experiments suggest that using n-best
list outputs from Moses instead of just the one best
could buy us an small additional WER decrease.
The typical improvements brought by the com-
bined system are illustrated by the following exam-
ple, where two cases of agglutinated word forms
are corrected, resulting in a correct output:
SMS oubli?2tdir: tom a pom? c
foto dlui en string
MT-like oubli?2tdir tom a paum? ses
photos dlui en string
Combined oubli? de te dire tom a paum?
ses photos de lui en string
(I) forgot to tell you (that)
tom lost photos of himself in
a thong
5 Conclusion and Perspectives
In this paper, we studied various ways to address
the problem of normalization of SMS, by drawing
analogy with related NLP problems, and accord-
ingly reusing as much as possible existing tools or
modules. Following (Aw et al, 2006), we found
that using off-the-shell statistical MT systems al-
lows to achieve very satisfactory WER; combin-
ing this system with a system based on an anal-
ogy with the speech recognition problem yields
an additional 1.5 absolute improvement in WER.
As it stands, our statistical normalization system
seems to be sufficiently efficient to be used for text
mining purposes; it also provides a useful tool to
quantitatively analyze the various mechanisms in-
volved in SMS spelling. The problem nonetheless
remains far from being solved: our best system
still makes at least one error on about 60% of the
test messages.
There are a number of obvious improvements
we might consider, such as using more accu-
rate grapheme-to-phoneme rules, or plugging in a
larger statistical language model, but we feel these
would buy us only small increase in performance.
As a first step to improve our normalization sys-
tem, we would rather like to combine the existing
approaches with a spell-checking approach. The
most natural way to proceed would be to devise an
alternative letter-to-word finite-state transducer C,
aimed at converting space separated sequences of
alphabetic symbols to the corresponding sequence
of words, allowing for usual spelling errors (dele-
tion/insertion of a letter, substitution, etc). Using
the notations of section 3.2, the normalization sys-
tem would thus be computed by the following fi-
nite state machine: ([E ? P ?D] ? C) ? S.
Another natural extension would be to make this
finite-state transducer stochastic: again, this would
447
be a rather simple matter to train this transducer us-
ing the forward-backward algorithm (see (Jansche,
2003)) on the available training data.
6 acknowledgments
The authors wish to thank E. Guimier de Neef for
providing us with one of the databases and other
useful resources. Many thanks to our anonymous
reviewers for helpful comments.
References
Allauzen, Cyril, Mehryar Mohri, and Brian Roark.
2005. The design principles and algorithms of a
weighted grammar library. International Journal of
Foundations of Computer Science, 16(3):403?421.
Anis, Jacques. 2001. Parlez-vous texto ? Guide des
nouveaux langages du r?seau. ?ditions du Cherche
Midi.
Aw, Aiti, Min Zhang, Juan Xiao, and Jian Su. 2006.
A phrase-based statistical model for SMS text nor-
malization. In Proceedings of COLING/ACL, pages
33?40.
Brill, Eric and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting of the As-
sociation for Computational Linguistics, pages 286?
293, Hong Kong.
Brown, Peter F., John Cocke, Stephen Della Pietra, Vin-
cent J. Della Pietra, Frederick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990.
A statistical approach to machine translation. Jour-
nal of Natural Language Engineering, 16(2):79?85.
Choudhury, Monojit, Rahul Saraf, Vijit Jain, Sudeshna
Sarkar, and Anupam Basu. 2007. Investigation and
modeling of the structure of texting language. In
Proceedings of the IJCAIWorkshop on "Analytics for
Noisy Unstructured Text Data", pages 63?70, Hyder-
abad, India.
Church, Kenneth W. and William Gale. 1991. Proba-
bility scoring for spelling correction. Statistics and
Computing, 1:91?103.
Crystal, David. 2001. Language and the Internet.
Cambridge University Press.
Fairon, C?drick and S?bastien Paumier. 2006. A trans-
lated corpus of 30,000 French SMS. In Proceedings
of LREC 2006, Genoa, Italy.
Fairon, C?drick, Jean Ren? Klein, and S?bastien Pau-
mier. 2006. Le langage SMS. UCL Presses Univer-
sitaires de Louvain.
Guimier de Neef, ?milie, Arnaud Debeurme, and
Jungyeul Park. 2007. TILT correcteur de SMS :
?valuation et bilan quantitatif. In Actes de TALN,
pages 123?132, Toulouse, France.
Hocq, S. 2006. ?tude des sms en fran?ais : constitution
et exploitation d?un corpus align? sms-langue stan-
dard. Technical report, Universit? Aix-Marseille.
Jansche, Martin. 2003. Inference of string mappings
for language technology. Ph.D. thesis, Ohio State
University.
Kobus, Catherine, Fran?ois Yvon, and G?raldine
Damnati. 2008. Transcrire les SMS comme on re-
conna?t la parole. In Actes de la Conf?rence sur
le Traitement Automatique des Langues (TALN?08),
pages 128?138, Avignon, France.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. Annual Meeting of the Association for Com-
putational Linguistics (ACL), demonstration session,
Prague, Czech Republic.
Michel Simard, Alexandre Deslauriers. 2001. Real-
time automatic insertion of accents in French text.
Natural Language Engineering, 7(2):143?165.
Mohri, Mehryar andMichael Riley. 1998. Network op-
timisation for large vocabulary speech recognition.
Speech Communication, 25(3):1?12.
Mohri, Mehryar, Fernando Pereira, and Michael Riley.
1998. A rational design for a weighted finite-state
transducer library. Lecture Notes in Computer Sci-
ence, (1438).
Och, Franz Josef and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Papineni, Kishore, Salim Roukos, ToddWard, andWei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Technical Re-
port RC22176 (W0109-022), IBM Research Divi-
sion, Thomas J. Watson Research Center.
Stolcke, Andreas. 2002. Srilm ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Langage Processing
(ICSLP), volume 2, pages 901?904, Denver, CO.
Toutanova, Kristina and Robert Moore. 2002. Pronun-
ciation modeling for improved spelling correction.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 144?
151, Philadelphia, PA.
V?ronis, Jean and ?milie Guimier de Neef. 2006. Le
traitement des nouvelles formes de communication
?crite. In Sabah, G?rard, editor, Compr?hension
automatique des langues et interaction, pages 227?
248. Paris: Herm?s Science.
448
On the use of confidence for statistical decision in dialogue strategies
Christian Raymond1 Fre?de?ric Be?chet1 Renato de Mori1 Ge?raldine Damnati2
1 LIA/CNRS, University of Avignon France 2 France Telecom R&D, Lannion, France
christian.raymond,frederic.bechet,renato.demori@lia.univ-avignon.fr
geraldine.damnati@rd.francetelecom.com
Abstract
This paper describes an interpretation and deci-
sion strategy that minimizes interpretation er-
rors and perform dialogue actions which may
not depend on the hypothesized concepts only,
but also on confidence of what has been rec-
ognized. The concepts introduced here are ap-
plied in a system which integrates language
and interpretation models into Stochastic Finite
State Transducers (SFST). Furthermore, acous-
tic, linguistic and semantic confidence mea-
sures on the hypothesized word sequences are
made available to the dialogue strategy. By
evaluating predicates related to these confi-
dence measures, a decision tree automatically
learn a decision strategy for rescoring a n-best
list of candidates representing a user?s utter-
ance. The different actions that can be then per-
formed are chosen according to the confidence
scores given by the tree.
1 Introduction
There is a wide consensus in the scientific community
that human-computer dialogue systems based on spoken
natural language make mistakes because the Automatic
Speech Recognition (ASR) component may not hypothe-
size some of the pronounced words and the various levels
of knowledge used for recognizing and reasoning about
conceptual entities are imprecise and incomplete. In spite
of these problems, it is possible to make useful applica-
tions with dialogue systems using spoken input if suit-
able interpretation and decision strategies are conceived
that minimize interpretation errors and perform dialogue
actions which may not depend on the hypothesized con-
cepts only, but also on confidence of what has been rec-
ognized.
This paper introduces some concepts developed for
telephone applications in the framework of stochastic
models for interpretation and dialogue strategies, a good
overview of which can be found in (Young, 2002).
The concepts introduced here are applied in a system
which integrates language and interpretation models into
Stochastic Finite State Transducers (SFST). Furthermore,
acoustic, linguistic and semantic confidence measures on
the hypothesized word sequences are made available to
the dialogue strategy. A new way of using them in the
dialogue decision process is proposed in this paper.
Most of the Spoken language Understanding Systems
(SLU) use semantic grammars with semantic tags as non-
terminals (He and Young, 2003) with rules for rewriting
them into strings of words.
The SFSTs of the system used for the experiments de-
scribed here, represent knowledge for the basic building
blocks of a frame-based semantic grammar. Each block
represents a property/value relation. Different SFSTs
may share words in the same sentence. Property/value
hypotheses are generated with an approach described in
(Raymond et al, 2003) and are combined into a sentence
interpretation hypothesis in which the same word may
contribute to more than one property/value pair. The di-
alogue strategy has to evaluate the probability that each
component of each pair has been correctly hypothesized
in order to decide to perform an action that minimizes the
risk of user dissatisfaction.
2 Overview of the decoding process
The starting point for decoding is a lattice of word
hypotheses generated with an n-gram language model
(LM). Decoding is a search process which detects com-
binations of specialized SFSTs and the n-gram LM. The
output of the decoding process consists of a n-best list
of conceptual interpretations ?. An interpretation ? is
a set of property/value pairs sj = (cj, vj) called con-
cepts. cj is the concept tag and vj is the concept value
of sj . Each concept tag cj is represented by a SFST and
can be related either to the dialogue application (phone
number, date, location expression, etc.) or to the dia-
logue management (confirmation, contestation, etc.). To
each string of words recognized by a given SFST cj is
associated a value vj representing a normalized value
for the concept detected. For example, to the word
phrase: on July the fourteenth, detected by a
SFST dedicated to process dates, is associated the value:
????/07/14.
The n-best list of interpretations output by the decod-
ing process is structured according to the different con-
cept tag strings that can be found in the word lattice. To
each concept tag string is attached another n-best list on
the concept values. This whole n-best is called a struc-
tured n-best. After presenting the statistical model used
in this study, we will describe the implementation of this
decoding process.
3 Statistical model
The contribution of a sequence of words W to a concep-
tual structure ? is evaluated by the posterior probability
P (? | Y ), where Y is the description of acoustic features.
Such a probability is computed as follows:
P (? | Y ) =
?
W?SW P (Y | W )P (? | W )
?P (W )?
?
W?SW P (Y | W )P (W )?
(1)
where P (Y | W ) is provided by the acoustic models,
P (W ) is computed with the LM. Exponents ? and ? are
respectively a semantic and a syntactic fudge factor. SW
corresponds to the set of word strings that can be found in
the word lattice. P (? | W ) is computed by considering
that thus:
P (? | W ) = P (s1 | W ).
J
?
j=2
P (sj | sj?11 W ) (2)
P (sj | sj?11 W ) ? P (sj | W )
If the conceptual component sj is hypothesized with
a sentence pattern pij(W ) recognized in W and pik(W )
triggers a pair sk and there is a training set with which
the probabilities P (pik(W ) | sk) ?k, can be estimated,
then the posterior probability can be obtained as follows:
P (sj | W ) =
P (pij(W ) | sj)P (sj)
?K
k=1 P (pik(W ) | sk)P (sk)
(3)
where P (sk) is a unigram probability of conceptual
components.
4 Structured N-best list
N-best lists are generally produced by simply enumerat-
ing the n best paths in the word graphs produced by Au-
tomatic Speech Recognition (ASR) engines. The scores
used in such graphs are usually only a combination of
acoustic and language model scores, and no other linguis-
tic levels are involved. When an n-best word hypothesis
list is generated, the differences between the hypothesis
i and the hypothesis i+1 are often very small, made of
only one or a few words. This phenomenon is aggravated
when the ASR word graph contains a low confidence
area, due for example to an Out-Of-Vocabulary word, to
a noisy input or to a speech disfluency.
This is the main weakness of this approach in a Spoken
Dialogue context: not all words are important to the Di-
alogue Manager, and all the n-best word hypotheses that
differ only between each other because of some speech
disfluency effects can be considered as equals. That?s
why it is important to generate not only a n-best list of
word hypotheses but rather a n-best list of interpretations,
each of them corresponding to a different meaning from
the Dialogue Manager point of view.
We propose here a method for directly extracting such
a structured n-best from a word lattice output by an ASR
engine. This method relies on operations between Finite
State Machines and is implemented thanks to the AT&T
FSM toolkit (see (Mohri et al, 2002) for more details).
4.1 Word-to-Concept transducer
Each concept ck of the dialogue application is associated
with an FSM. These FMSs are called acceptors (Ak for
the concept ck). In order to process strings of words that
don?t belong to any concept, a filler model, called AF
is used. Because the same string of words can?t belong
to both a concept model and the background text, all the
paths contained in the acceptors Ak are removed from the
filler model AF in the following way:
AF = ? ? ?
m
?
k=1
Ak
where ? is the word lexicon of the application and m
is the number of concepts used.
All these acceptors are now turned into transducers
that take words as input symbols and start or end con-
cept tags as output symbols. Indeed, all acceptors Ak be-
come transducers Tk where the first transition emits the
symbol <Ck> and the last transition the symbol </Ck>.
Similarly the filler model becomes the transducer Tbk
which emits the symbols <BCK> and </BCK>. Except
these start and end tags, no other symbols are emitted: all
words in the concept or background transducers emit an
empty symbol.
Finally all these transducers are linked together in a
single model called Tconcept as presented in figure 1.
FILLER
out=<BCK>
in=<start>
out=<>
in=<> in=<>
out=</BCK>
in=<end>
out=<>
out=<>
in=<start>
out=<>
in=w1
in=w2
out=<>
in=<end>
out=<>
in=<>
out=</C1>out=<C1>
in=<>
out=<C2>
in=<>
out=</C2>
in=<>
in=<>
out=<Cn>
in=<>
out=</Cn>
in=<>
out=<>
in=<>
out=<>in=<>
out=<>
SFST C1
SFST Cn
SFST C2
Figure 1: Word-to-Concept Transducer
4.2 Processing the ASR word lattice
The ASR word lattice is coded by an FSM: an acceptor L
where each transition emits a word. The cost function for
a transition corresponds to the acoustic score of the word
emitted.
The first step in the word lattice processing consists
of rescoring each transition of L by means of a 3-gram
Language Model (LM) in order to obtain the probabili-
ties P (W ) of equation 1. This is done by composing the
word lattice with a 3-gram LM also coded as an FSM (see
(Allauzen et al, 2003) for more details about statistical
LMs and FSMs).
The resulting FSM is then composed with the trans-
ducer TConcept in order to obtain the word-to-concept
transducer L?. A path in L? corresponds to a word string
if only the input symbols of the transducer are considered
and its score is the one expressed by equation 1; simi-
larly by considering only the output symbols, a path in L?
corresponds to a concept tag string.
The structured n-best list is directly obtained from L?:
by extracting the n-best concept tag strings (output label
paths) we obtain an n-best list on the conceptual interpre-
tations. The score of each conceptual interpretation is the
sum of all the word strings (input label paths) in the word
lattice producing the same interpretation.
Finally, for every conceptual interpretations C kept at
the previous step, a local n-best list on the word strings is
calculated by selecting in L? the best paths outputting the
string C .
The resulting structured n-best is illustrated by the fol-
lowing example. If we keep the 2 best conceptual in-
terpretations C1, C2 of a transducer L? and, for each of
these, the 2 best word strings, we obtain:
1 : C1 = <c1_1,c1_2,..,c1_x>
1.1 : W1.1 = <v1.1_1,v1.1_2,..,v1.1_x>
1.2 : W1.2 = <v1.2_1,v1.2_2,..,v1.2_x>
2 : C2 = <c2_1,c2_2,..,c2_y>
2.1 : W2.1 = <v2.1_1,v2.1_2,..,v2.1_y>
2.2 : W2.2 = <v2.2_1,v2.2_2,..,v2.2_y>
where <ci_1,ci_2,..,ci_y> is the conceptual
interpretation at the rank i in the n-best list; Wi.j is the
word string ranked j of interpretation i; and vi.j_k
is the concept value of the kth concept ci_k of the jth
word string of interpretation i.
5 Use of correctness probabilities
In order to select a particular interpretation ? (concep-
tual interpretation + concept values) from the structured
n-best list, we are now interested in computing the proba-
bility that ? is correct, given a set of confidence measures
M : P (? | M ). The choice of the confidence measures
determines the quality of the decision strategy. Those
used in this study are briefly presented in the next sec-
tions.
5.1 Confidence measures
5.1.1 Acoustic confidence measure (AC)
This confidence measure relies on the comparison of
the acoustic likelihood provided by the speech recogni-
tion model for a given hypothesis to the one that would
be provided by a totally unconstrained phoneme loop
model. In order to be consistent with the general model,
the acoustic units are kept identical and the loop is over
context dependent phonemes. This confidence measure
is used at the utterance level and at the concept level (see
(Raymond et al, 2003) for more details).
5.1.2 Linguistic confidence measure (LC)
In order to assess the impact of the absence of ob-
served trigrams as a potential cause of recognition errors,
a Language Model consistency measure is introduced.
This measure is simply, for a given word string candi-
date, the ratio between the number of trigrams observed
in the training corpus of the Language Model vs. the total
number of trigrams in the same word string. Its computa-
tion is very fast and the confidence scores obtained from
it give interesting results as presented in (Este`ve et al,
2003).
5.1.3 Semantic confidence measure (SC)
Several studies have shown that text classification tools
(like Support Vector Machines or Boosting algorithms)
can be an efficient way of labeling an utterance transcrip-
tion with a semantic label such as a call-type (Haffner et
al., 2003) in a Spoken Dialogue context. In our case, the
semantic labels attached to an utterance are the different
concepts handled by the Dialogue Manager. One classi-
fier is trained for each concept tag in the following way:
Each utterance of a training corpus is labeled with a
tag, manually checked, indicating if a given concept oc-
curs or not in the utterance. In order to let the classi-
fier model the context of occurrence of a concept rather
than its value we removed most of the concept headwords
from the list of criterion used by the classifier.
During the decision process, if the interpretation eval-
uated contains 2 concepts c1 and c2, then the classifiers
corresponding to c1 and c2 are used to give to the utter-
ance a confidence score of containing these two concepts.
The text classifier used in the experimental section
is a decision-tree classifier based on the Semantic-
Classification-Trees introduced for the ATIS task
by (Kuhn and Mori, 1995) and used for semantic disam-
biguation in (Be?chet et al, 2000).
5.1.4 Rank confidence measure (R)
To the previous confidence measures we added the
rank of each candidate in its n-best. This rank contains
two numbers: the rank of the interpretation of the utter-
ance and the rank of the utterance among those having
the same interpretation.
5.2 Decision Tree based strategy
As the dependencies of these measures are difficult to es-
tablish, their values are transformed into symbols by vec-
tor quantization (VQ) and conjunctions of these symbols
expressing relevant statistical dependencies are obtained
by a decision tree which is trained with a development
set of examples. At the leaves probabilities P (M |?) are
obtained when ? represents any correct hypothesis, the
case in which only the properties have been correctly rec-
ognized or both properties and values have errors. With
these probabilities we are now able to estimate P (? | M )
in the following way:
P (? | M) = 1
1 + P (M |??)P (??)P (M |?)P (?)
(4)
where ?? indicates that the interpretation in question is
incorrect and P (M |??) = 1 ? P (M |?).
6 From hypotheses to actions
Once concepts have been hypothesized, a dialog system
has to decide what action to perform. Let A = aj be
the set of actions a system can perform. Some of them
can be requests for clarification or repetition. In partic-
ular, the system may request the repetition of the entire
utterance. Performing an action has a certain risk and the
decision about the action to perform has to be the one that
minimizes the risk of user dissatisfaction.
It is thus possible that some or all the hypothesized
components of a conceptual structure ? do not corre-
spond to the user intention because the word sequence
W based on which the conceptual hypothesis has been
generated contains some errors. In particular, there are
requests for clarification or repetition which should be
performed right after the interpretation of an utterance in
order to reduce the stress of the user. It is important to
notice that actions consisting in requests for clarification
or repetition mostly depend on the probability that the in-
terpretation of an utterance is correct, rather than on the
utterance interpretation.
The decoding process described in section 2 provides
a number of hypotheses containing a variable number of
pairs sj = (cj, vj) based on the score expressed by equa-
tion 1.
P (? | M ) is then computed for these hypotheses. The
results can be used to decide to accept an interpretation
or to formulate a clarification question which may imply
more hypotheses.
For simplification purpose, we are going to consider
here only two actions: accepting the hypothesis with the
higher P (? | M ) or rejecting it. The risk associated to the
acceptation decision is called ?fa and corresponds to the
cost of a false acceptation of an incorrect interpretation.
Similarly the risk associated to the rejection decision is
called ?fr and corresponds to the cost of a false rejection
of a correct interpretation. In a spoken dialogue context,
?fa is supposed to be higher than ?fr .
The choice of the action to perform is determined by
a threshold ? on P (? | M ). This threshold is tuned on
a development corpus by minimizing the total risk R ex-
pressed as follows:
R = ?fa ?
Nfa
Ntotal
+ ?fr ?
Nfr
Ntotal
(5)
Nfa and Nfr are the numbers of false acceptation and
false rejection decisions on the development corpus for a
given value of ?. Ntotal is the total number of examples
available for tuning the strategy.
The final goal of the strategy is to make negligible Nfa
and the best set of confidence measures is the one that
minimizes Nfr . In fact, the cost of these cases is lower
because the corresponding action has to be a request for
repetition.
Instead of simply discarding an utterance if P (? | M )
is below ?, another strategy we are investigating consists
of estimating the probability that the conceptual interpre-
tation alone (without the concept values) is correct. This
probability can be estimated the same way as P (? | M )
and can be used to choose a third kind of actions: accept-
ing the conceptual meaning of an utterance but asking for
clarifications about the values of the concepts.
A final decision about the strategy to be adopted should
be based on statistics on system performance to be col-
lected and updated after deploying the system on the tele-
phone network.
7 Experiments
7.1 Application domain
The application domain considered in this study is a
restaurant booking application developed at France Tele-
com R&D. At the moment, we only consider in our strat-
egy the most frequent concepts related to the application
domain: PLACE, PRICE and FOOD TYPE. They can be
described as follows:
? PLACE: an expression related to a restaurant loca-
tion (eg. a restaurant near Bastille);
? PRICE: the price range of a restaurant (eg. less than
a hundred euros);
? FOOD TYPE: the kind of food requested by the
caller (eg. an Indian restaurant).
These entities are expressed in the training corpus by
short sequences of words containing three kinds of to-
ken: head-words like Bastille, concept related words like
restaurant and modifier tokens like near.
A single value is associated to each concept entity
simply be adding together the head-words and some
modifier tokens. For example, the values associated to
the three contexts presented above are: Bastille ,
less+hundred+euros and indian.
In the results section a concept detected is considered a
success only if the tag exists in the reference corpus and if
both values are identical. It?s a binary decision process:
a concept can be considered as a false detection even if
the concept tag is correct and if the value is partially cor-
rect. The measure on the errors (insertion, substitution,
deletion) of these concept/value tokens is called in this
paper the Understanding Error Rate, by opposition to the
standard Word Error Rate measure where all words are
considered equals.
7.2 Experimental setup
Experiments were carried out on a dialogue corpus pro-
vided by France Telecom R&D. The task has a vocabu-
lary of 2200 words. The language model used is made
of 44K words. For this study we selected utterances cor-
responding to answers to a prompt asking for the kind
of restaurant the users were looking for. This corpus has
been cut in two: a development corpus containing 511
utterances and a test corpus containing 419 utterances.
This development corpus has been used to train the deci-
sion tree presented in section 5.2. The Word Error Rate
on the test corpus is 22.7%.
7.3 Evaluation of the rescoring strategy
Table 1 shows the results obtained with a rescoring strat-
egy that selects, from the structured n-best list, the hy-
pothesis with the highest P (? | M ). The baseline re-
sults are obtained with a standard maximum-likelihood
approach choosing the hypothesis maximizing the proba-
bility P (? | Y ) of equation 1. No rejection is performed
in this experiment.
The size of the n-best lists was set to 12 items: the first
4 candidates of the first 3 interpretations in the structured
n-best list. The gain obtained after rescoring is very sig-
nificant and justify our 2-step approach that first extract
an n-best list of interpretations thanks to P (? | Y ) and
then choose the one with the highest confidence accord-
ing to a large set of confidence measures M . This gain
can be compared to the one obtained on the Word Error
Rate measure: the WER drops from 21.6% to 20.7% af-
ter rescoring on the development corpus and from 22.7%
to 22.5% on the test corpus. It is clear here that the
WER measure is not an adequate measure in a Spoken
Dialogue context as a big reduction in the Understanding
Error Rate might have very little effect on the Word Error
Rate.
Corpus baseline rescoring UER reduction %
Devt. 15.0 12.4 17.3%
Test 17.7 14.5 18%
Table 1: Understanding Error Rate results with and with-
out rescoring on structured n-best lists (n=12) (no rejec-
tion)
7.4 Evaluation of the decision strategy
In this experiment we evaluate the decision strategy con-
sisting of accepting or rejecting an hypothesis ? thanks to
a threshold on the probability P (? | M ). Figure 2 shows
the curve UER vs. utterance rejection on the development
and test corpora. As we can see very significant improve-
ments can be achieved with very little utterance rejection.
For example, at a 5% utterance rejection operating point,
the UER on the development corpus drops from 15.0% to
8.6% (42.6% relative improvement) and from 17.7% to
11.4% (35.6% relative improvement).
By using equation 5 for finding the operating point
minimizing the risk fonction (with a cost ?fa = 1.5 ?
?fr) on the development corpus we obtain:
? on the development corpus: UER=6.5 utterance re-
jection=13.1
? on the test corpus: UER=9.6 utterance rejec-
tion=15.9
46
8
10
12
14
16
18
0 5 10 15 20
un
de
rst
an
din
g e
rro
r r
ate
utterance rejection (%)
devt
test
Figure 2: Understanding Error Rate vs. utterance rejec-
tion on the development and test corpora
8 Conclusion
This paper describes an interpretation and decision strat-
egy that minimizes interpretation errors and perform dia-
logue actions which may not depend on the hypothesized
concepts only, but also on confidence of what has been
recognized. The first step in the process consists of gen-
erating a structured n-best list of conceptual interpreta-
tions of an utterance. A set of confidence measures is
then used in order to rescore the n-best list thanks to a de-
cision tree approach. Significant gains in Understanding
Error Rate are achieved with this rescoring method (18%
relative improvement). The confidence score given by the
tree can also be used in a decision strategy about the ac-
tion to perform. By using this score, significant improve-
ments in UER can be achieved with very little utterance
rejection. For example, at a 5% utterance rejection op-
erating point, the UER on the development corpus drops
from 15.0% to 8.6% (42.6% relative improvement) and
from 17.7% to 11.4% (35.6% relative improvement). Fi-
nally the operating point for a deployed dialogue system
can be chosen by explicitly minimizing a risk function on
a development corpus.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In 41st Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?03), Sapporo,
Japan.
Fre?de?ric Be?chet, Alexis Nasr, and Franck Genet. 2000.
Tagging unknown proper names using decision trees.
In 38th Annual Meeting of the Association for Compu-
tational Linguistics, Hong-Kong, China, pages 77?84.
Yannick Este`ve, Christian Raymond, Renato De Mori,
and David Janiszek. 2003. On the use of linguistic
consistency in systems for human-computer dialogs.
IEEE Transactions on Speech and Audio Processing,
(Accepted for publication, in press).
Patrick Haffner, Gokhan Tur, and Jerry Wright. 2003.
Optimizing SVMs for complex call classification. In
IEEE International Conference on Acoustics, Speech
and Signal Processing, ICASSP?03, Hong-Kong.
Y. He and S. Young. 2003. A data-driven spoken lan-
guage understanding system. In Automatic Speech
Recognition and Understanding workshop - ASRU?03,
St. Thomas, US-Virgin Islands.
R. Kuhn and R. De Mori. 1995. The application of se-
mantic classification trees to natural language under-
standing. IEEE Trans. on Pattern Analysis and Ma-
chine Intelligence, 17(449-460).
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer, Speech and Language,
16(1):69?88.
Christian Raymond, Yannick Este`ve, Fre?de?ric Be?chet,
Renato De Mori, and Ge?raldine Damnati. 2003. Belief
confirmation in spoken dialogue systems using confi-
dence measures. In Automatic Speech Recognition and
Understanding workshop - ASRU?03, St. Thomas, US-
Virgin Islands.
Steve Young. 2002. Talking to machines (statisti-
cally speaking). In International Conference on Spo-
ken Language Processing, ICSLP?02, pages 113?120,
Denver, CO.
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 48?55,
NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Experiments on the France Telecom 3000 Voice Agency corpus: academic
research on an industrial spoken dialog system?
Ge?raldine Damnati
France Te?le?com R&D
TECH/SSTP/RVA
2 av. Pierre Marzin
22307 Lannion Cedex 07, France
geraldine.damnati@orange-ftgroup.com
Fre?de?ric Be?chet Renato De Mori
LIA
University of Avignon
AGROPARC, 339 ch. des Meinajaries
84911 Avignon Cedex 09, France
frederic.bechet,renato.demori
@univ-avignon.fr
Abstract
The recent advances in speech recognition
technologies, and the experience acquired
in the development of WEB or Interac-
tive Voice Response interfaces, have facil-
itated the integration of speech modules
in robust Spoken Dialog Systems (SDS),
leading to the deployment on a large scale
of speech-enabled services. With these
services it is possible to obtain very large
corpora of human-machine interactions by
collecting system logs. This new kinds of
systems and dialogue corpora offer new
opportunities for academic research while
raising two issues: How can academic re-
search take profit of the system logs of
deployed SDS in order to build the next
generation of SDS, although the dialogues
collected have a dialogue flow constrained
by the previous SDS generation? On the
other side, what immediate benefits can
academic research offer for the improve-
ment of deployed system? This paper ad-
dresses these aspects in the framework of
the deployed France Telecom 3000 Voice
Agency service.
?This work is supported by the 6th Framework Research
Programme of the European Union (EU), Project LUNA,
IST contract no 33549. The authors would like to thank
the EU for the financial support. For more information
about the LUNA project, please visit the project home-page,
www.ist-luna.eu .
1 Introduction
Since the deployment on a very large scale of the
AT&T How May I Help You? (HMIHY) (Gorin et
al., 1997) service in 2000, Spoken Dialogue Sys-
tems (SDS) handling a very large number of calls are
now developed from an industrial point of view. Al-
though a lot of the remaining problems (robustness,
coverage, etc.) are still spoken language process-
ing research problems, the conception and the de-
ployment of such state-of-the-art systems mainly re-
quires knowledge in user interfaces.
The recent advances in speech recognition tech-
nologies, and the experience acquired in the devel-
opment of WEB or Interactive Voice Response inter-
faces have facilitated the integration of speech mod-
ules in robust SDS.
These new SDS can be deployed on a very large
scale, like the France Telecom 3000 Voice Agency
service considered in this study. With these services
it is possible to obtain very large corpora of human-
machine interactions by collecting system logs. The
main differences between these corpora and those
collected in the framework of evaluation programs
like the DARPA ATIS (Hemphill et al, 1990) or the
French Technolangue MEDIA (Bonneau-Maynard
et al, 2005) programs can be expressed through the
following dimensions:
? Size. There are virtually no limits in the
amount of speakers available or the time
needed for collecting the dialogues as thou-
sands of dialogues are automatically processed
every day and the system logs are stored.
Therefore Dialog processing becomes similar
48
to Broadcast News processing: the limit is not
in the amount of data available, but rather in the
amount of data that can be manually annotated.
? Speakers. Data are from real users. The speak-
ers are not professional ones or have no reward
for calling the system. Therefore their behav-
iors are not biased by the acquisition protocols.
Spontaneous speech and speech affects can be
observed.
? Complexity. The complexity of the services
widely deployed is necessarily limited in order
to guarantee robustness with a high automation
rate. Therefore the dialogues collected are of-
ten short dialogues.
? Semantic model. The semantic model of such
deployed system is task-oriented. The inter-
pretation of an utterance mostly consists in the
detection of application-specific entities. In an
application like the France Telecom 3000 Voice
Agency service this detection is performed by
hand-crafted specific knowledge.
The AT&T HMIHY corpus was the first large dia-
logue corpus, obtained from a deployed system, that
has the above mentioned characteristics. A service
like the France Telecom 3000 Voice Agency service
has been developed by a user interface development
lab. This new kind of systems and dialogue corpora
offer new opportunities for academic research that
can be summarized as follows:
? How can academic research take profit of the
system logs of deployed SDS in order to build
the next generation of SDS, although the di-
alogues collected have a dialogue flow con-
strained by the previous SDS generation?
? On the other side, what immediate benefits can
academic research offer for the improvement
of deployed system, while waiting for the next
SDS generation?
This paper addresses these aspects in the frame-
work of the deployed FT 3000 Voice Agency ser-
vice. Section 3 presents how the ASR process can
be modified in order to detect and reject Out-Of-
Domain utterances, leading to an improvement in
the understanding performance without modifying
the system. Section 4 shows how the FT 3000 cor-
pus can be used in order to build stochastic models
that are the basis of a new Spoken Language Un-
derstanding strategy, even if the current SLU system
used in the FT 3000 service is not stochastic. Sec-
tion 5 presents experimental results obtained on this
corpus justifying the need of a tighter integration be-
tween the ASR and the SLU models.
2 Description of the France Telecom 3000
Voice Agency corpus
The France Telecom 3000 (FT3000) Voice Agency
service, the first deployed vocal service at France
Telecom exploiting natural language technologies,
has been made available to the general public in Oc-
tober 2005. FT3000 service enables customers to
obtain information and purchase almost 30 differ-
ent services and access the management of their ser-
vices. The continuous speech recognition system re-
lies on a bigram language model. The interpretation
is achieved through the Verbateam two-steps seman-
tic analyzer. Verbateam includes a set of rules to
convert the sequence of words hypothesized by the
speech recognition engine into a sequence of con-
cepts and an inference process that outputs an inter-
pretation label from a sequence of concepts.
2.1 Specificities of interactions
Given the main functionalities of the application,
two types of dialogues can be distinguished. Some
users call FT 3000 to activate some services they
have already purchased. For such demands, users
are rerouted toward specific vocal services that are
dedicated to those particular tasks. In that case, the
FT3000 service can be seen as a unique automatic
frontal desk that efficiently redirects users. For such
dialogues the collected corpora only contain the in-
teraction prior to rerouting. It can be observed in that
case that users are rather familiar to the system and
are most of the time regular users. Hence, they are
more likely to use short utterances, sometimes just
keywords and the interaction is fast (between one or
two dialogue turns in order to be redirected to the
demanded specific service).
Such dialogues will be referred as transit dia-
logues and represent 80% of the calls to the FT3000
49
service. As for the 20% other dialogues, referred to
as other, the whole interaction is proceeded within
the FT3000 application. They concern users that are
more generally asking for information about a given
service or users that are willing to purchase a new
service. For these dialogues, the average utterance
length is higher, as well as the average number of
dialogue turns.
other transit
# dialogues 350 467
# utterances 1288 717
# words 4141 1454
av. dialogue length 3.7 1.5
av. utterance length 3.2 2.0
OOV rate (%) 3.6 1.9
disfluency rate (%) 2.8 2.1
Table 1: Statistics on the transit and other dialogues
As can be observed in table 1 the fact that users
are less familiar with the application in the other dia-
logues implies higher OOV rate and disfluency rate1.
An important issue when designing ASR and SLU
models for such applications that are dedicated to
the general public is to be able to handle both naive
users and familiar users. Models have to be robust
enough for new users to accept the service and in
the meantime they have to be efficient enough for
familiar users to keep on using it. This is the reason
why experimental results will be detailed on the two
corpora described in this section.
2.2 User behavior and OOD utterances
When dealing with real users corpora, one has to
take into account the occurrence of Out-Of-Domain
(OOD) utterances. Users that are familiar with a ser-
vice are likely to be efficient and to strictly answer
the system?s prompts. New users can have more di-
verse reactions and typically make more comments
about the system. By comments we refer to such
cases when a user can either be surprised what am
I supposed to say now?, irritated I?ve already said
that or even insulting the system. A critical aspect
for other dialogues is the higher rate of comments
uttered by users. For the transit dialogues this phe-
nomenon is much less frequent because users are fa-
1by disfluency we consider here false starts and filled pauses
miliar to the system and they know how to be effi-
cient and how to reach their goal. As shown in ta-
ble 2, 14.3% of the other dialogues contain at least
one OOD comment, representing an overall 10.6%
of utterances in these dialogues.
other transit
# dialogues 350 467
# utterances 1288 717
# OOD comments 137 24
OOD rate (%) 10.6 3.3
dialogues with OOD (%) 14.3 3.6
Table 2: Occurrence of Out-Of-Domain comments
on the transit and other dialogues
Some utterances are just comments and some con-
tain both useful information and comments. In the
next section, we propose to detect these OOD se-
quences and to take this phenomenon into account
in the global SLU strategy.
3 Handling Out-Of-Domain utterances
The general purpose of the proposed strategy is to
detect OOD utterances in a first step, before entering
the Spoken Language Understanding (SLU) mod-
ule. Indeed standard Language Models (LMs) ap-
plied to OOD utterances are likely to generate erro-
neous speech recognition outputs and more gener-
ally highly noisy word lattices from which it might
not be relevant and probably harmful to apply SLU
modules.
Furthermore, when designing a general interac-
tion model which aims at predicting dialogue states
as proposed in this paper, OOD utterances are as
harmful for state prediction as can be an out-of-
vocabulary word for the prediction of the next word
with an n-gram LM.
This is why we propose a new composite LM that
integrates two sub-LMs: one LM for transcribing in-
domain phrases, and one LM for detecting and delet-
ing OOD phrases. Finally the different SLU strate-
gies proposed in this paper are applied only to the
portions of signal labeled as in-domain utterances.
50
3.1 Composite Language Model for decoding
spontaneous speech
As a starting point, the comments have been manu-
ally annotated in the training data in order to easily
separate OOD comment segments from in-domain
ones. A specific bigram language model is trained
for these comment segments. The comment LM was
designed from a 765 words lexicon and trained on
1712 comment sequences.
This comment LM, called LMOOD has been in-
tegrated in the general bigram LMG. Comment
sequences have been parsed in the training corpus
and replaced by a OOD tag. This tag is added to
the general LM vocabulary and bigram probabilities
P ( OOD |w) and P (w| OOD ) are trained along
with other bigram probabilities (following the prin-
ciple of a priori word classes). During the decoding
process, the general bigram LM probabilities and the
LMOOD bigram probabilities are combined.
3.2 Decision strategy
Given this composite LM, a decision strategy is ap-
plied to select those utterances for which the word
lattice will be processed by the SLU component.
This decision is made upon the one-best speech
recognition hypotheses and can be described as fol-
lows:
1. If the one-best ASR output is a single OOD
tag, the utterance is simply rejected.
2. Else, if the one-best ASR output contains an
OOD tag along with other words, those words
are processed directly by the SLU component,
following the argument that the word lattice for
this utterance is likely to contain noisy infor-
mation.
3. Else (i.e. no OOD tag in the one-best ASR
output), the word-lattice is transmitted to fur-
ther SLU components.
It will be shown in the experimental section that
this pre-filtering step, in order to decide whether a
word lattice is worth being processed by the higher-
level SLU components, is an efficient way of pre-
venting concepts and interpretation hypothesis to be
decoded from an uninformative utterance.
3.3 Experimental setup and evaluation
The models presented are trained on a corpus col-
lected thanks to the FT3000 service. It contains real
dialogues from the deployed service. The results
presented are obtained on the test corpus described
in section 2.
The results were evaluated according to 3 crite-
ria: the Word Error Rate (WER), the Concept Error
Rate (CER) and the Interpretation Error Rate (IER).
The CER is related to the correct translation of an
utterance into a string of basic concepts. The IER is
related to the global interpretation of an utterance
in the context of the dialogue service considered.
Therefore this last measure is the most significant
one as it is directly linked to the performance of the
dialogue system.
IER all other transit
size 2005 717 1288
LMG 16.5 22.3 13.0
LMG + OOD 15.0 18.6 12.8
Table 3: Interpretation error rate according to the
Language Model
Table 3 presents the IER results obtained with the
strategy strat1 with 2 different LMs for obtaining
W? : LMG which is the general word bigram model;
and LMG + OOD which is the LM with the OOD com-
ment model. As one can see, a very significant im-
provement, 3.7% absolute, is achieved on the other
dialogues, which are the ones containing most of
the comments. For the transit dialogues a small im-
provement (0.2%) is also obtained.
4 Building stochastic SLU strategies
4.1 The FT3000 SLU module
The SLU component of the FT3000 service consid-
ered in this study contains two stages:
1. the first one translates a string of words W =
w1, . . . , wn into a string of elementary con-
cepts C = c1, . . . , cl by means of hand-written
regular grammars;
2. the second stage is made of a set of about 1600
inference rules that take as input a string of con-
cepts C and output a global interpretation ? of
51
a message. These rules are ordered and the
first match obtained by processing the concept
string is kept as the output interpretation.
These message interpretations are expressed by an
attribute/value pair representing a function in the vo-
cal service.
The models used in these two stages are manually
defined by the service designers and are not stochas-
tic. We are going now to present how we can use a
corpus obtained with such models in order to define
an SLU strategy based on stochastic processes.
4.2 Semantic knowledge representation
The actual FT3000 system includes semantic knowl-
edge represented by hand-written rules. These rules
can also be expressed in a logic form. For this rea-
son, some basic concepts are now described with the
purpose of showing how logic knowledge has been
integrated in a first probabilistic model and how it
can be used in a future version in which optimal poli-
cies can be applied.
The semantic knowledge of an application is a
knowledge base (KB) containing a set of logic for-
mulas. Formulas return truth and are constructed
using constants which represent objects and may be
typed, variables, functions which are mappings from
tuples of objects to objects and predicates which
represent relations among objects. An interpretation
specifies which objects, functions and relations in
the domain are represented by which symbol. Basic
inference problem is to determine whether KB |= F
which means that KB entails a formula F .
In SLU, interpretations are carried on by binding
variables and instantiating objects based on ASR re-
sults and inferences performed in the KB. Hypothe-
ses about functions and instantiated objects are writ-
ten into a Short Term Memory (STM).
A user goal is represented by a conjunction of
predicates. As dialogue progresses, some predi-
cates are grounded by the detection of predicate tags,
property tags and values. Such a detection is made
by the interpretation component. Other predicates
are grounded as a result of inference. A user goal G
is asserted when all the atoms of its conjunction are
grounded and asserted true.
Grouping the predicates whose conjunction is the
premise for asserting a goal Gi is a process that goes
through a sequence of states: S1(Gi), S2(Gi), . . .
Let ?ik be the content of the STM used for as-
serting the predicates grounded at the k-th turn of a
dialogue. These predicates are part of the premise
for asserting the i-th goal.
Let Gi be an instance of the i-th goal asserted after
grounding all the predicates in the premise.
?ik can be represented by a composition from a
partial hypothesis ?ik? 1 available at turn k ? 1, the
machine action ak?1 performed at turn k ? 1 and
the semantic interpretation ?ik i.e.:
?ik = ?
(
?ik, ak?1,?ik?1
)
Sk(Gi) is an information state that can lead to a
user?s goal Gi and ?ik is part of the premise for as-
serting Gi at turn k.
State probability can be written as follows:
P (Sk(Gi)|Yk) = P
(
Gi|?ik
)
P
(
?ik|Yk
) (1)
where P
(
Gi|?ik
)
is the probability that Gi is the
type of goal that corresponds to the user interac-
tion given the grounding predicates in ?ik. Yk is the
acoustic features of the user?s utterance at turn k.
Probabilities of states can be used to define a be-
lief of the dialogue system.
A first model allowing multiple dialog state se-
quence hypothesis is proposed in (Damnati et al,
2007). In this model each dialog state correspond
to a system state in the dialog automaton. In order
to deal with flexible dialog strategies and following
previous work (Williams and Young, 2007), a new
model based on a Partially Observable Markov De-
cision Process (POMDP) is currently studied.
If no dialog history is taken into account,
P
(
?ik|Y
)
comes down to P
(
?ik|Y
)
, ?ik being a
semantic attribute/value pair produced by the Ver-
bateam interpretation rules.
The integration of this semantic decoding process
in the ASR process is presented in the next section.
5 Optimizing the ASR and SLU processes
With the stochastic models proposed in section 4,
different strategies can be built and optimized. We
are interested here in the integration of the ASR and
SLU processes. As already shown by previous stud-
ies (Wang et al, 2005), the traditional sequential ap-
proach that first looks for the best sequence of words
52
W? before looking for the best interpretation ?? of an
utterance is sub-optimal. Performing SLU on a word
lattice output by the ASR module is an efficient way
of integrating the search for the best sequence of
words and the best interpretation. However there are
real-time issues in processing word lattices in SDS,
and therefore they are mainly used in research sys-
tems rather than deployed systems.
In section 3 a strategy is proposed for selecting
the utterances for which a word lattice is going to be
produced. We are going now to evaluate the gain in
performance that can be obtained thanks to an inte-
grated approach on these selected utterances.
5.1 Sequential vs. integrated strategies
Two strategies are going to be evaluated. The first
one (strat1) is fully sequential: the best sequence of
word W? is first obtained with
W? = argmax
W
P (W |Y )
Then the best sequence of concepts C? is obtained
with
C? = argmax
C
P (C|W? )
Finally the interpretation rules are applied to C? in
order to obtain the best interpretation ??.
The second strategy (strat2) is fully integrated: ??
is obtained by searching at the same time for W? and
C? and ??. In this case we have:
?? = argmax
W,C,?
P (?|C)P (C|W )P (W |Y )
The stochastic models proposed are implemented
with a Finite State Machine (FSM) paradigm thanks
to the AT&T FSM toolkit (Mohri et al, 2002).
Following the approach described in (Raymond
et al, 2006), the SLU first stage is implemented by
means of a word-to-concept transducer that trans-
lates a word lattice into a concept lattice. This con-
cept lattice is rescored with a Language Model on
the concepts (also encoded as FSMs with the AT&T
GRM toolkit (Allauzen et al, 2003)).
The rule database of the SLU second stage is en-
coded as a transducer that takes as input concepts
and output semantic interpretations ?. By applying
this transducer to an FSM representing a concept lat-
tice, we directly obtain a lattice of interpretations.
The SLU process is therefore made of the com-
position of the ASR word lattice, two transducers
(word-to-concepts and concept-to-interpretations)
and an FSM representing a Language Model on the
concepts. The concept LM is trained on the FT3000
corpus.
This strategy push forward the approach devel-
opped at AT&T in the How May I Help You? (Gorin
et al, 1997) project by using richer semantic mod-
els than call-types and named-entities models. More
precisely, the 1600 Verbateam interpretation rules
used in this study constitute a rich knowledge base.
By integrating them into the search, thanks to the
FSM paradigm, we can jointly optimize the search
for the best sequence of words, basic concepts, and
full semantic interpretations.
For the strategy strat1 only the best path is kept in
the FSM corresponding to the word lattice, simulat-
ing a sequential approach. For strat2 the best inter-
pretation ?? is obtained on the whole concept lattice.
error WER CER IER
strat1 40.1 24.4 15.0
strat2 38.2 22.5 14.5
Table 4: Word Error Rate (WER), Concept Error
Rate (CER) and Interpretation Error Rate (IER) ac-
cording to the SLU strategy
The comparison among the two strategies is given
in table 4. As we can see a small improvement is ob-
tained for the interpretation error rate (IER) with the
integrated strategy (strat2). This gain is small; how-
ever it is interesting to look at the Oracle IER that
can be obtained on an n-best list of interpretations
produced by each strategy (the Oracle IER being the
lowest IER that can be obtained on an n-best list of
hypotheses with a perfect Oracle decision process).
This comparison is given in Figure 1. As one can
see a much lower Oracle IER can be achieved with
strat2. For example, with an n-best list of 5 interpre-
tations, the lowest IER is 7.4 for strat1 and only 4.8
for strat2. This is very interesting for dialogue sys-
tems as the Dialog Manager can use dialogue con-
text information in order to filter such n-best lists.
53
 4
 5
 6
 7
 8
 9
 10
 1  2  3  4  5  6  7  8  9  10
O
ra
cl
e 
IE
R
size of the n-best list of interpretations
sequential search (strat1)
integrated search (strat2)
Figure 1: Oracle IER according to an n-best list of interpretations for strategies strat1 and strat2
5.2 Optimizing WER, CER and IER
Table 4 also indicates that the improvements ob-
tained on the WER and CER dimensions don?t al-
ways lead to similar improvements in IER. This is
due to the fact that the improvements in WER and
CER are mostly due to a significant reduction in the
insertion rates of words and concepts. Because the
same weight is usually given to all kinds of errors
(insertions, substitutions and deletions), a decrease
in the overall error rate can be misleading as inter-
pretation strategies can deal more easily with inser-
tions than deletions or substitutions. Therefore the
reduction of the overall WER and CER measures is
not a reliable indicator of an increase of performance
of the whole SLU module.
level 1-best Oracle hyp.
WER 33.7 20.0
CER 21.2 9.7
IER 13.0 4.4
Table 5: Error rates on words, concepts and interpre-
tations for the 1-best hypothesis and for the Oracle
hypothesis of each level
These results have already been shown for WER
by previous studies like (Riccardi and Gorin, 1998)
IER
from word Oracle 9.8
from concept Oracle 7.5
interpretation Oracle 4.4
Table 6: IER obtained on Oracle hypotheses com-
puted at different levels.
or more recently (Wang et al, 2003). They are il-
lustrated by Table 5 and Table 6. The figures shown
in these tables were computed on the subset of utter-
ances that were passed to the SLU component. Ut-
terances for which an OOD has been detected are
discarded. In Table 5 are displayed the error rates
obtained on words, concepts and interpretations both
on the 1-best hypothesis and on the Oracle hypothe-
sis (the one with the lowest error rate in the lattice).
These Oracle error rates were obtained by looking
for the best hypothesis in the lattice obtained at the
corresponding level (e.g. looking for the best se-
quence of concepts in the concept lattice). As for Ta-
ble 6, the mentioned IER are the one obtained when
applying SLU to the Oracles hypotheses computed
for each level. As one can see the lowest IER (4.4)
is not obtained on the hypotheses with the lowest
WER (9.8) or CER (7.5).
54
6 Conclusion
This paper presents a study on the FT3000 corpus
collected from real users on a deployed general pub-
lic application. Two problematics are addressed:
How can such a corpus be helpful to carry on re-
search on advanced SLU methods eventhough it has
been collected from a more simple rule-based dia-
logue system? How can academic research trans-
late into short-term improvements for deployed ser-
vices? This paper proposes a strategy for integrating
advanced SLU components in deployed services.
This strategy consists in selecting the utterances for
which the advanced SLU components are going to
be applied. Section 3 presents such a strategy that
consists in filtering Out-Of-Domain utterances dur-
ing the ASR first pass, leading to significant im-
provement in the understanding performance.
For the SLU process applied to in-domain utter-
ances, an integrated approach is proposed that looks
simultaneously for the best sequence of words, con-
cepts and interpretations from the ASR word lat-
tices. Experiments presented in section 5 on real
data show the advantage of the integrated approach
towards the sequential approach. Finally, section 4
proposes a unified framework that enables to define
a dialogue state prediction model that can be applied
and trained on a corpus collected through an already
deployed service.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In 41st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?03), Sap-
poro, Japan.
Helene Bonneau-Maynard, Sophie Rosset, Christelle Ay-
ache, Anne Kuhn, and Djamel Mostefa. 2005. Se-
mantic annotation of the french media dialog corpus.
In Proceedings of the European Conference on Speech
Communication and Technology (Eurospeech), Lis-
boa, Portugal.
Geraldine Damnati, Frederic Bechet, and Renato
De Mori. 2007. Spoken Language Understanding
strategies on the France Telecom 3000 voice agency
corpus. In Proceedings of the International Con-
ference on Acoustics, Speech and Signal Processing
(ICASSP), Honolulu, USA.
A. L. Gorin, G. Riccardi, and J.H. Wright. 1997. How
May I Help You ? In Speech Communication, vol-
ume 23, pages 113?127.
Charles T. Hemphill, John J. Godfrey, and George R.
Doddington. 1990. The ATIS spoken language sys-
tems pilot corpus. In Proceedings of the workshop on
Speech and Natural Language, pages 96?101, Hidden
Valley, Pennsylvania.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer, Speech and Language,
16(1):69?88.
Christian Raymond, Frederic Bechet, Renato De Mori,
and Geraldine Damnati. 2006. On the use of finite
state transducers for semantic interpretation. Speech
Communication, 48,3-4:288?304.
Giuseppe Riccardi and Allen L. Gorin. 1998. Language
models for speech recognition and understanding. In
Proceedings of the International Conference on Spo-
ken Langage Processing (ICSLP), Sidney, Australia.
Ye-Yi Wang, A. Acero, and C. Chelba. 2003. Is word
error rate a good indicator for spoken language under-
standing accuracy? In Automatic Speech Recognition
and Understanding workshop - ASRU?03, St. Thomas,
US-Virgin Islands.
Ye-Yi Wang, Li Deng, and Alex Acero. 2005. Spoken
language understanding. In Signal Processing Maga-
zine, IEEE, volume 22, pages 16?31.
Jason D. Williams and Steve Young. 2007. Partially ob-
servable markov decision processes for spoken dialog
systems. Computer, Speech and Language, 21:393?
422.
55
