Learning Semantic-Level Information Extraction Rules by 
Type-Oriented ILP 
Yutaka Sasaki and Yoshihiro Matsuo  
NTT Communicat ion  Science Laboratories 
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan 
{sasaki, yosihiro} ~cslab.kecl .ntt .co. jp 
Abstract  
This paper describes an approach to using se- 
mantic rcprcsentations for learning information 
extraction (IE) rules by a type-oriented induc- 
tire logic programming (ILl)) system. NLP 
components of a lnachine translation system are 
used to automatically generate semantic repre- 
sentations of text corpus that can be given di- 
rectly to an ILP system. The latest experimen- 
tal results show high precision and recall of the 
learned rules. 
1 Int roduct ion 
Information extraction (IE) tasks in this paper 
involve the MUC-3 style IE. The input for the 
information extraction task is an empty tem- 
plate and a set of natural anguage texts that de- 
scribe a restricted target domain, such as corpo- 
rate mergers or terrorist atta.cks in South Amer- 
ica. Templates have a record-like data struc- 
ture with slots that have names, e.g., "company 
name" and "merger d~te", and v~lues. The out- 
put is a set of filled templates. IE tasks are 
highly domain-dependent, so rules and dictio- 
naries for filling values in the telnp\]ate slots de- 
pend on the domain. 
it is a heavy burden for IE system develop- 
ers that such systems depend on hand-made 
rules, which cannot be easily constructed and 
changed. For example, Umass/MUC-3 needed 
about 1,500 person-hours of highly skilled labor 
to build the IE rules and represent them as a 
dictionary (Lehnert, 1992). All the rules must 
be reconstructed i'rom scratch when the target 
domain is changed. 
To cope with this problem, some pioneers 
have studied methods for learning information 
extraction rules (Riloff,1996; Soderland ctal., 
1.995; Kim et el., 1995; Huffman, 1996; Califf 
and Mooney, 1997). Along these lines, our ap- 
preach is to a.pply an inductive logic program- 
ruing (ILP) (Muggleton, 1991)system to the 
learning of IE rules, where information is ex- 
tracted from semantic representations of news 
articles. The ILP system that we employed is 
a type-oriented ILP system I{\]\]B + (Sasaki and 
Haruno, 1997), which can efficiently and effec- 
tively h~mdle type (or sort) information in train- 
ing data. 
2 Our Approach to IE Tasks 
This section describes our approach to IE tasks. 
Figure 1. is an overview of our approach to learn- 
ing IE rules using an II, P system from seman- 
tic representations. First, training articles are 
analyzed and converted into semantic represen- 
tations, which are filled case fl'ames represented 
as atomic formulae. Training templates are pre- 
pared by hand as well. The ILP system learns 
\]!!; rules in the tbrm of logic l)rograms with type 
information. To extract key inlbrmation from a 
new ~rticle, semantic representation s au tomat- 
ically generated from the article is matched by 
the IE rules. Extracted intbrmation is filled into 
the template slots. 
3 NLP  Resources and Tools 
3.1 The Semantic Attribute System 
We used the semantic attribute system of "Ge l  
Taikei - -  A Japanese Lexicon" (lkehara el el., 
1997a; Kurohashi and Sakai, 1.999) compiled by 
the NTT Communication Science Laboratories 
for a Japanese-to-English machine translation 
system, ALT- J /E  (Ikehm:a et al, 1994). The se- 
mantic attribute system is a sort of hierarchical 
concept thesaurus represented as a tree struc- 
ture in which each node is called a semantic 
cateqory. An edge in the tree represents an is_a 
or has_a relation between two categories. The 
semantic attribute system is 11.2 levels deep and 
698 
semantic representation new article 
' ' '  \[\]\]\] s?~chy yze ~ Analyze I rolease(cl,pl) articles sentences announce(cl,dl) 
i'~kackgrou n d Anal 
.... nzwledge / \[E rules ~ F representatiOn sentences I semantic 
.ooitive ..... l 
I re,oa o x. , II 
answer 
templates filled Company: c2 ~7"A;p'-'"iyrule~='~" "~  
by hand Draotauotd..~2 to semantic I -  I ,opreseot t,on 
Figure l: l/lock diagram of IE using IM ) 
contains about 3,000 sema.ntic ategory nodes. 
More than 300,000 Japanese words a.re linked to 
the category nodes. 
3.2 Verb Case Frame Dict ionary 
The Japanese-to-li;nglish valency 1)a.ttern dic- 
t ionary of "(\]oi-Taikei" ( lkehara et al, 1997b; 
Kurohash.i and Saka.i, 1999) was also originally 
developed for ALT-,I/IB. The. wde:ncy dictionary 
conta.ins about 15,000 case frames with sema.n- 
tic restrictions on their arguments lbr 6,000 
a apanese verbs. Each ca.se frame consists of one 
predicate a.nd one or more case elements tha.t 
h ave a list; of sere an tic categories. 
3.3 Natural  Language Processing Tools 
We used the N I,P COml)onents of kl/ l ' - . I /F,  for 
text a, nalysis. These inclu<le the morphologica,l 
amdyzer, the syntactic analyzer, and the case 
aDalyzer for Japanese. The components a.re ro- 
bust a:nd generic tools, mainly ta:rgeted to news- 
paper articles. 
3.3.1 Generic Case Analyzer  
l,et us examine the case a.nalysis in more de- 
tail. The <'as(; analyzer eads a set of parse tree 
candidates produced by the J a.panese syntactic 
analyzer. The parse tree is :represented as a de- 
penden cy of ph rases (i. e., .\] al>anese bu'nsctmt). 
First, it divides the parse tree into unit sen- 
tences, where a unit sentence consists of one 
predicate and its noun and adverb dependent 
phrases. Second, it compares each unit sen- 
tence.with a verb case fl'alne dictionary, l!;ach 
frame consists a predicate condition and several 
cast elements conditions. The predicate con- 
dition specifies a verb that matches the frame 
a.:nd each case-role has a. case element condition 
whi ch sl>ecifie.s particles an d sere an tic categories 
of" noun phrases. The preference va.lue is de- 
lined as the summation of noun phrase \])refer- 
ences which are calculated from the distances 
between the categories of the input sentences 
m~d the categories written in the f i ' amcs .  The 
case a.na.lyzer then chooses the most preferable 
pa.rse tree and the most preferable combination 
of case frames. 
The valency dictionary also has case<roles 
(Table \] ) for :noun phrase conditions. The case- 
roles of adjuncts are determined by using the 
particles of adjuncts and the sema.ntic a.tegories 
of n ou n ph ra.ses. 
As a result, the OUtl)ut O\[' the case a.nalysis is 
a set; el" (;ase fl:ames for ca.oh unit se:ntence. The 
noun phra.ses in \['tames are la.beled by case-roh;s 
in Tal)le 1. 
l!'or siml)\]icity , we use case-role codes, such a.s 
N 1 and N2, a.s the labels (or slot ha.rues) to rep- 
resent case li:ames. The relation between sen- 
tences and case-roles is described in detail in 
( Ikehara el el., 1993). 
3.3.2 Logical Form Translator 
We developed a logical form translator li'E1 ~ 
that generates semantic representations ex- 
pressed a,s atomic Ibrmulae from the cast; fi:a.mes 
and parse trees. For later use, document II) 
and tense inlbrmation a.re also added to the case 
frames. 
For example, tile case fl:ame in 'l.'able 2 is ob- 
tained a:l'ter analyzing the following sentence of 
document l) 1: 
"Jalctcu(.lack) h,a suts,tkesu(suitca.se) we 
699 
Table 1: Case-Roles 
Name Code Description l~xampl.e 
Subject N1 the agent/experiencer of I throw a ball. 
an event/situation 
Objectl  N2 the object of an event 
Object2 N3 another object of an event 
Loc-Source N4 source location of a movement 
Loc-Goal N5 goal location of a movement 
Purpose N6 the purpose of an action 
Result N7 the result of an event 
Locative N8 the location of an event 
Comitative N9 co-experiencer 
Quotative N10 quoted expression 
Material N 11 material/ ingredient 
Cause N12 the reason for an event 
Instrument N13 a concrete instrument 
Means N14 an abstract instrument 
Time-Position TN1 the time of an event 
Time-Source TN2 the starting time of an event 
Time-Goal TN3 the end time of ~n event 
Amount QUANT quantity of something 
I throw a ball. 
I compare it with them. 
I start fl'om Japan. 
I go to Japan. 
I go shopping. 
It results in failure. 
it occurs at the station. 
I share a room with him. 
I say that .... 
I fill the glass with water. 
It collapsed fr'om the weight. 
I speak with a microphone. 
I speak in Japanese. 
I go to bed at i0:00. 
I work from Monday. 
It continues until Monday. 
I spend $10. 
 hok,,ba(the omce) kava(from)   o(the air 
port) ,),i(to)ha~obu(carry)" 
("Jack carries a suitcase from the office to the 
airport.") 
Table 2: Case Frame of the Sample Sentence 
predicate: hakobu (carry) 
article: 1) 1 
tense: present 
NI: Jakhu (Jack) 
N2: sutsukesu (suitcase) 
N4: sl, okuba (the office) 
N5: kuko (the airport) 
4 Induct ive Learning Tool 
Conventional ILP systems take a set of positive 
and negative xamples, and background knowl- 
edge. The output is a set of hypotheses in the 
form of logic programs that covers positives and 
do not cover negatives. We employed the type- 
oriented ILP system RHB +. 
4.1 Features of Type-orlented ILP 
System RHB + 
The type-oriented I\],P system has the tbllowing 
features that match the needs for learning l\]"~ 
rules. 
? A type-oriented ILP system can efficiently 
and effectively handle type (or seman- 
tic category) information in training data.. 
This feature is adwmtageous in controlling 
the generality and accuracy of learned IE 
rules. 
? It can directly use semantic representations 
of the text as background knowledge. 
, It can learn from only positive examples. 
? Predicates are allowed to have labels (or 
keywords) for readability and expressibil- 
ity. 
4.2 Summary of Type-oriented ILP 
System RHB + 
This section summarizes tile employed type- 
oriented ILP system RHB +. The input of 
RHB + is a set of positive examples and back- 
ground knowledge including type hierarchy (or 
700 
the semantic attribute system). The output is 
a set of I\[orn clauses (Lloyd, 11.987) having vari- 
;tl~les with tyl)e intbrmation. That is, the term 
is extended to the v-term. 
4.3 v-terms 
v-terms are the restricted form of 0-terms (Ai't- 
K~tci and Nasr, 1986; Ait-Kaci et al, 11994). In- 
l'ormttlly, v-terms are Prolog terms whose vari- 
ables a.re replaced with variable Var of type T, 
which is denoted as Var:T. Predicttte ~tnd tim(:- 
tion symbols ~tre allowed to h;we features (or 
labels). For examl)\]e, 
speak( agent~ X :human,objcct~ Y :language) 
is a clause based on r-terms which ha.s labels 
agent and object, and types human and 
language. 
4.,4 A lgor i thm 
The algorithm of lHllI + is basically ~t greedy 
covering algorithm. It constructs clauses one- 
by-one by calling inner_loop (Algorithm \]) 
which returns a hypothesis clause. A hypoth- 
esis clause is tel)resented in the form of head :-- 
body. Covered examples are removed from 1 ) in 
each cycle. 
The inner loop consists of two phases: the 
head construction phase and the body construc- 
tion I)hase. It constrncts heads in a bottom-up 
manner and constructs the body in a top-down 
lna.nner, following the result described in (Zelle 
el al., 1994). 
"\['he search heuristic PWI  is weighted infor- 
m~tivity eml)loying the l,a.place estimate. Let 
7' = {Head : -Body } U B K. 
rwz( r ,T )_  l I f ' l+ J 
- - I . f ' - - /?  1?g2 IQ-~\]'i\[ _12 2' 
where IPl denotes the number of positive ex- 
amples covered by T and Q(T) is the empirical 
content. The smaller the value of PWI, the can- 
didate clause is better. Q(T) is defined as the 
set of atoms (1) that are derivable from T ~md 
(2) whose predicate is the target I)redicate, i.e., 
the predicate name of the head. 
The dynamic type restriction, by positivc ex- 
amples uses positive examples currently covered 
in order to determine appropriate types to wtri- 
~bles for the current clause. 
A lgor i thm 1 inner_loop 
1. Given positives P, original positives 1~o, back- 
ground knowledge 1Hr. 
2. Decidc typcs of variables in a head by comput- 
ing the lyped least general generalizations (lgg) 
of N pairs of clcmcnts in P, and select he most 
general head as H cad. 
3. If the stopping condition is satisfied, return 
Head. 
It. Let Body bc empty. 
5, Create a set of all possible literals L using vari- 
ables in Head and Body. 
6. Let BEAM be top If litcrals l~, of L wilh 
respect to the positive weighted informalivily 
PWI.  
7. Do later steps, assuming that l~ is added to 
Body for each literal lk in BEAM.  
8. Dynamically restrict types in Body by callin, g 
the dynamic type restriction by positive exam- 
pies. 
9. If the slopping condition is satisfied, rct'aru 
(Head :- Body). 
lO. Goto 5. 
5 I l l us t ra t ion  o f  a Learn ing  Process  
Now, we examine tile two short notices of' new 
products release in Table 3. The following table 
shows a sample te:ml)late tbr articles reporting 
a new product relea.se. 
Tom pl ate 
1. article id: 
2. coml)any: 
3. product: 
4. release date: 
5.1 Preparat ion 
Suppose that the following semantic represen- 
tations is obtained from Article 1. 
(cl) announce( article => I, 
tense => past, 
tnl => "this week", 
nl => "ABC Corp.", 
nlO => (c2) ) .  
(c2) release( article => I, 
tense => future, 
tni => "Jan. 20", 
nl => "ABC Corp.", 
n2 => "a color printer" ). 
701 
Table 3: Sample Sentences 
Article id Sentence 
#1 "ABC Corp. this week zmnounced that it will release a color printer on Jan. 20." 
#2 "XYZ Corp. released a color scanner last month." 
The filled template for Article 1 is as follows. 
Template \] 
\]. article id: 1 
2. colnpany: ABC Corp. 
3. product: a color printer 
4. release date: Jan. 20 
Suppose that the following semantic represen- 
tation is obtained from Article 2. 
(c3) release( article => 2, 
tense => past, 
tnl => "last month", 
nl => "XYZ Corp.", 
n2 => "a color scanner" ). 
The filled template for Article 2 is as follows. 
Template 2 
1. article id: 2 
2. company: XYZ Corp. 
3. product: a color scanner 
4. release date: last month 
5.2 Head Const ruct ion  
Two positive examples are selected for the tem- 
plate slot "company". 
company(ar t i c le -number  => i 
name => "ABe Corp") .  
company(ar t i c le -number  => 2 
name => "XYZ Corp") .  
By computing a least general generalization 
(lgg)sasaki97, the following head is obtained: 
company( article-number => Art: number 
name => Co: organization). 
5.3 Body  Construction 
Generate possible literals 1 by combining predi- 
cate names and variables, then check the PWI  
1,1iterals,, here means atomic formulae or negated 
ones .  
values of clauses to which one of the literal 
added. In this case, suppose that adding the fol- 
lowing literal with predicate release is the best 
one. After the dynamic type restriction, the 
current clause satisfies the stopping condition. 
Finally, the rule for extracting "company name" 
is returned. Extraction rules for other slots 
"product" and "release date" can be learned in 
the sanle manner. Note that several literals may 
be needed in the body of the clause to satisfy 
the stopping condition. 
company(article-number => Art:number, 
name => Co: organization ) 
? - release( article => Art, 
tense => T: tense, 
tnl => D: time, 
nl => Co, 
n2 => P: product ). 
5.4 Ext rac t ion  
Now, we have tile following sen\]antic represen- 
tation extracted from the new article: 
Article 3: " JPN Corp. has released a new CI) 
player. ''2 
(c4) release( article => 3, 
tense => perfect_present, 
tnl => nil, 
n l  => "JPN Corp.", 
n2 => "a new CD player" ). 
Applying the learned IE rules and other rules, 
we can obtain the filled template for Article 3. 
Template 3 
1. article id: 3 
2. company: JPN Corp. 
3. product: C I )p layer  
4. release date: 
2\;Ve always assume nil for the case that is not in- 
cluded in the sentence. 
702 
Table d: Learning results of new product release 
(a) Without data correction 
company product release date 
Precision 89.6% 
Recall 82.1% 
Average time (set.) 15.8 
l)recision 911 .1% 
Recall 85.7% 
Average time (sec.) 22.9 
80.5% 
66.7% 
22.J 
90.6% 
66.7% 
ld.d 
announce date \[ price 
lOO.O% 58.4% 
82.4:% 60.8% 
2.2 I 1.0 
(b) With data. correction 
company product release date 
80.o% 
69.7% 
25.2 
92.3% 
82.8% 
33.55 
annotmce date \[ price 
100.0% 87.1% 
88.2% 82.4% 
5.1.5 11.9 
6 Experimental Results 
6.1. Setting of Experhnents 
We extracted articles related to the release of 
new products from a one-year newspaper cor- 
pus written in Japanese 3. One-hundred arti- 
cles were randomly selected fi'om 362 relevant 
articles. The template we used consisted of 
tive slots: company name, prod'uct name, re- 
lease date, a~tnomzcc date, and price. We also 
filled one template for each a.rticle. After an- 
a.lyzing sentences, case fi'ames were converted 
into atomic tbrmulae representing semantic rep- 
re,,~entationx a.  described in Section 2 and 3. All 
the semantic representations were given to the 
lea.rner as background \]?nowledge, ~md the tilled 
templates were given as positive examples. To 
speed-up the leCturing process, we selected pred- 
icate names that are relevant o the word s in the 
templates as the target predicates to be used by 
the ILl ~ system, and we also restricted the num- 
ber of literals in the body of hypotheses to one. 
Precision and recM1, the standard metrics \['or 
IF, tasks, are counted by using the remove-one- 
out cross validation on tile e, xamples for each 
item. We used a VArStation with tlie Pentium 
H Xeon (450 MHz):for this experiment. 
6.2 Results 
'l?M)le 4 shows the results of our experiment. In 
the experiment of learning from semantic repre- 
sentations, including errors in case-role selection 
and semantic ategory selection, precision was 
3We used ~rticles from the Mainichi Newspaimrs of 
1994 with permission. 
very high. 'l'he precision of the learned rules 
lot price was low beta.use the seman tic category 
name automatieaJly given to the price expres- 
sions in the dat~ were not quite a.ppropriate. 
For the tire items, 6?-82% recall was achieved. 
With the background knowledge having sere an- 
tic representations corrected by hand, precision 
was very high mid 70-88% recMl was achieved. 
The precision of price was markedly improved. 
It ix important that the extraction of live 
ditthrent pieces o1' information showed good re- 
sults. This indica.tex that the \]LI' system RIII~ + 
has a high potential in IE tasks. 
7 Related Work 
l)revious researches on generating lli; rules 
from texts with templates include AutoSlog- 
TS (Riloff,1996), (',I{YS'FAL (Soderland et al, 
1995), I'AIAKA (l(im et al, 1995), MlgP (Iluff- 
man, 11.996) and RAPII~;I~ (Califl' and Mooney, 
1997). In our approach, we use the type- 
oriented H,P system RItlJ +, which ix indepen- 
dent of natural language analysis. This point 
differentiates our ~pproach from the others. 
Learning semantic-level IE rules using an II,P 
system from semantic representations is also a 
new challenge in II'; studies. 
Sasald (Sasaki and Itaruno, 11997) applied 
RI{B + to the extraction of the number of deaths 
and injuries fi'om twenty five articles. That 
experiment was sufficient o assess the perfor- 
mance of the learner, but not to evaJuate its 
feasibility in IE tasks. 
703 
8 Conc lus ions  and Remarks  
This paper described a use of semantic repre- 
sentations for generating information extraction 
rules by applying a type-oriented ILP system. 
Experiments were conducted on the data gen- 
erated fi'om 100 news articles in the domain of 
new product release. The results showed very 
high precision, recall of 67-82% without data 
correction and 70-88% recall with correct se- 
mantic representations. The extraction of five 
different pieces of information showed good re- 
sults. This indicates that our learner RHB + has 
a high potential in IE tasks. 
References  
H. Ai't-Kaci and R. Nasr, LOGIN: A logic pro- 
gramming language with built-in inheritance, 
Journal oJ' Logic Programming, 3, pp.185- 
215, 1986. 
lt. Ai't-Kaci, B. Dumant, R. Meyer, A. Podel- 
ski, and P. Van Roy, The Wild Life Itandbook, 
1994. 
M. E. Califf and R. J. Mooney, Relational 
Learning of Pattern-Match Rules for Informa- 
tion Extraction, Proc. of ACL-97 Workshop 
in Natural Language Learning, 1997. 
S. B. Huffman, Learning Information Extrac- 
tion Patterns from Examples, Statistical and 
Symbolic Approaches to Learning for Natural 
Language Processing, pp.246 260, 1996. 
S. ikehara, M. Miyazaki, and A. Yokoo, Clas- 
si:fication of language knowledge for mean- 
ing analysis in machine translations, Trans- 
actions of Information Processing Society 
of Japan, Vol.34, pp.1692-1704, 1993. (in 
.Japanese) 
S. Ikehara, S. Shirai, K. Ogura, A. Yokoo, 
H. Nakaiwa and T. Kawaoka, ALT-J/E: A 
Japanese to English Machine Translation Sys- 
tem tbr Communication with Translation, 
Proc. of The 13th IFIP World Computer 
Congress, pp.80-85, 1994. 
S. Ikehara, M. Miyazaki, S. Shirai, A. Yokoo, 
H. Nakaiwa, K. Ogura, Y. Oyama and 
Y. Hayashi (eds.), The Semantic Attribute 
System, Goi-lktikci -- A Japanese Lexi- 
con, Vol.1, Iwanami Publishing, 1997. (in 
Japanese) 
S. Ikehara, M. Miyazaki, S. Shirai, A. Yokoo, 
H. Nakaiwa, K. Ogura, Y. Oyama and 
Y. Hayashi (eds.), The Valency Dictionary, 
Goi-Taikei -- A Japanese Lcxicon, Vol.5, 
Iwa.nami Publishing, 1997. (in Japanese) 
J.-T. Kim and D. I. Moldovan, Acquisition 
of Linguistic Patterns for Knowledge-Based 
Information Extraction, \[EEE Transaction 
on Knowledge and Data Engineering (IEEE 
TKDE), Vol.7, No.5, pp.713 724, 1995. 
S. Kurohashi and Y. Sakai, Semantic Analysis 
of Japanese Noun Phrases: A New Approach 
to Dictionary-Based Understanding Thc 37th 
Annual Meeting of the Association for Com- 
putational Linguistics (A CL-99), pp.481-488, 
1999. 
W. Lehnert, C. Cardie, D. Fisher, J. McCarthy, 
E. Riloff and S. Soderland, University of Mas- 
sachusetts: MUC-4 Test Results and Analy- 
sis, Proc. of The 1;burth Message Understand- 
ing Conference (MUC-4), pp.151-158, 1992. 
J. Lloyd, Foundations of Logic Prog'mmming, 
Springer, 1987. 
S. Muggleton, Inductive logic programming, 
New Generation Computing, Vol.8, No.4, 
pp.295-318, 1991. 
E. Riloff, Automatically Generating Extrac- 
tion Pattern from Untagged Text, Proc.of 
American Association for Artificial IntcIli- 
gcnce (AAAI-96), pp.1044-1049, 1996. 
Y. Sasaki and M. IIaruno, RHB+: A Type- 
Oriented 1LP System Learning from Positive 
Data, Proc. of The l/jth International Joint 
Conference on Artificial Intelligence (LJCA l- 
9"/), pp.894-899, 1997. 
S. Soderland, 1). Fisher, J. Aseltine, W. Lenert, 
CRYSTAL: Inducing a Conceptual Dictio- 
n~ry, Proc. of The 13th International Joint 
ConJ'crcnce on Artificial Intelligence (IJCAI- 
95), pp.1314 1319, 1995. 
J. M. Zelle and R. J. Mooney, J. B. Konvisser, 
Combining Top-down and Bottom-up Meth- 
ods in Inductive Logic Programming, Proc 
of The 11th Tntcrnational Conference on Ma- 
chine Learning (ML-94), pp.343-351, 1994. 
J 
704 
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 61?64, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Portable Translator Capable of Recognizing Characters on
Signboard and Menu Captured by Built-in Camera
Hideharu Nakajima, Yoshihiro Matsuo, Masaaki Nagata, Kuniko Saito
NTT Cyber Space Laboratories, NTT Corporation
Yokosuka, 239-0847, Japan
 
nakajima.hideharu, matsuo.yoshihiro, nagata.masaaki, saito.kuniko 
@lab.ntt.co.jp
Abstract
We present a portable translator that rec-
ognizes and translates phrases on sign-
boards and menus as captured by a built-
in camera. This system can be used on
PDAs or mobile phones and resolves the
difficulty of inputting some character sets
such as Japanese and Chinese if the user
doesn?t know their readings. Through the
high speed mobile network, small images
of signboards can be quickly sent to the
recognition and translation server. Since
the server runs state of the art recogni-
tion and translation technology and huge
dictionaries, the proposed system offers
more accurate character recognition and
machine translation.
1 Introduction
Our world contains many signboards whose phrases
provide useful information. These include destina-
tions and notices in transportation facilities, names
of buildings and shops, explanations at sightseeing
spots, and the names and prices of dishes in restau-
rants. They are often written in just the mother
tongue of the host country and are not always ac-
companied by pictures. Therefore, tourists must be
provided with translations.
Electronic dictionaries might be helpful in trans-
lating words written in European characters, because
key-input is easy. However, some character sets
such as Japanese and Chinese are hard to input if
the user doesn?t know the readings such as kana and
pinyin. This is a significant barrier to any translation
service. Therefore, it is essential to replace keyword
entry with some other input approach that supports
the user when character readings are not known.
One solution is the use of optical character recog-
nition (OCR) (Watanabe et al, 1998; Haritaoglu,
2001; Yang et al, 2002). The basic idea is the
connection of OCR and machine translation (MT)
(Watanabe et al, 1998) and implementation with
personal data assistant (PDA) has been proposed
(Haritaoglu, 2001; Yang et al, 2002). These are
based on the document OCR which first tries to ex-
tract character regions; performance is weak due to
the variation in lighting conditions. Although the
system we propose also uses OCR, it is character-
ized by the use of a more robust OCR technology
that doesn?t first extract character regions, by lan-
guage processing to offset the OCR shortcomings,
and by the use of the client-server architecture and
the high speed mobile network (the third generation
(3G) network).
2 System design
Figure 1 overviews the system architecture. After
the user takes a picture by the built-in camera of a
PDA, the picture is sent to a controller in a remote
server. At the server side, the picture is sent to the
OCR module which usually outputs many charac-
ter candidates. Next, the word recognizer identifies
word sequences in the candidates up to the number
specified by the user. Recognized words are sent to
the language translator.
The PDA is linked to the server via wireless com-
61
PDA with 
built-in camera and
mobile phone
Language
Translator
image
character candidates
Word
Recognizer
OCR
Controller
character candidates
word candidates
word candidates
translation
image translation
Figure 1: System architecture: http protocol is used
between PDAs and the controller.
munication. The current OCR software is Windows-
based while the other components are Linux pro-
grams. The PDA uses Windows.
We also implemented the system for mobile
phones using the i-mode and FOMA devices pro-
vided by NTT-DoCoMo.
3 Each component
3.1 Appearance-based full search OCR
Research into the recognition of characters in nat-
ural scenes has only just begun (Watanabe et al,
1998; Haritaoglu, 2001; Yang et al, 2002; Wu et
al., 2004). Many conventional approaches first ex-
tract character regions and then classify them into
each character category. However, these approaches
often fail at the extraction stage, because many pic-
tures are taken under less than desirable conditions
such as poor lighting, shading, strain, and distortion
in the natural scene. Unless the recognition target is
limited to some specific signboard (Wu et al, 2004),
it is hard for the conventional OCR techniques to
obtain sufficient accuracy to cover a broad range of
recognition targets.
To solve this difficulty, Kusachi et al proposed
a robust character classifier (Kusachi et al, 2004).
The classifier uses appearance-based character ref-
erence pattern for robust matching even under poor
capture conditions, and searches the most probable
Figure 2: Many character candidates raised by
appearance-based full search OCR: Rectangles de-
note regions of candidates. The picure shows that
candidates are identified in background regions too.
region to identify candidates. As full details are
given in their paper (Kusachi et al, 2004), we focus
here on just its characteristic performance.
As this classifier identifies character candidates
from anywhere in the picture, the precision rate is
quite low, i.e. it lists a lot of wrong candidates. Fig-
ure 2 shows a typical result of this OCR. Rectangles
indicate erroneous candidates, even in background
regions. On the other hand , as it identifies multiple
candidates from the same location, it achieves high
recall rates at each character position (over 80%)
(Kusachi et al, 2004). Hence, if character positions
are known, we can expect that true characters will be
ranked above wrong ones, and greater word recog-
nition accuracies would be achieved by connecting
highly ranked characters in each character position.
This means that location estimation becomes impor-
tant.
3.2 Word recognition
Modern PDAs are equipped with styluses. The di-
rect approach to obtaining character location is for
the user to indicate them using the stylus. However,
pointing at all the locations is tiresome, so automatic
estimation is needed. Completely automatic recog-
nition leads to extraction errors so we take the mid-
dle approach: the user specifies the beginning and
ending of the character string to be recognized and
translated. In Figure 3, circles on both ends of the
string denote the user specified points. All the lo-
cations of characters along the target string are esti-
mated from these two locations as shown in Figure
3 and all the candidates as shown in Figure 2.
62
Figure 3: Two circles at the ends of the string are
specified by the user with stylus. All the charac-
ter locations (four locations) are automatically esti-
mated.
3.2.1 Character locations
Once the user has input the end points, assumed
to lie close to the centers of the end characters, the
automatic location module determines the size and
position of the characters in the string. Since the
characters have their own regions delineated by rect-
angles and have x,y coordinates (as shown in Fig-
ure 2), the module considers all candidates and rates
the arrangement of rectangles according to the dif-
ferences in size and separation along the sequences
of rectangles between both ends of the string. The
sequences can be identified by any of the search al-
gorithms used in Natural Language Processing like
the forward Dynamic Programming and backward
A* search (adopted in this work). The sequence with
the highest score, least total difference, is selected as
the true rectangle (candidate) sequence. The centers
of the rectangles are taken as the locations of the
characters in the string.
3.2.2 Word search
The character locations output by the automatic
location module are not taken as specifying the cor-
rect characters, because multiple character candi-
dates are possible at the same location. Therefore,
we identify the words in the string by the probabil-
ities of character combinations. To increase the ac-
curacy, we consider all candidates around each es-
timated location and create a character matrix, an
example of which is shown in Figure 4. At each
location, we rank the candidates according to their
OCR scores, the highest scores occupy the top row.
Next, we apply an algorithm that consists of simi-
lar character matching, similar word retrieval, and
word sequence search using language model scores
    

 
  
 
 
 	
	 	
	 
 
 

   
   
   
  
Figure 4: A character matrix: Character candidates
are bound to each estimated location to make the
matrix. Bold characters are true.
(Nagata, 1998).
The algorithm is applied from the start to the end
of the string and examines all possible combinations
of the characters in the matrix. At each location, the
algorithm finds all words, listed in a word dictionary,
that are possible given the location; that is, the first
location restricts the word candidates to those that
start with this character. Moreover, to counter the
case in which the true character is not present in the
matrix, the algorithm identifies those words in the
dictionary that contain characters similar to the char-
acters in the matrix and outputs those words as word
candidates. The connectivity of neighboring words
is represented by the probability defined by the lan-
guage model. Finally, forward Dynamic Program-
ming and backward A* search are used to find the
word sequence with highest probability. The string
in the Figure 3 is recognized as ? Proceedings of the ACL 2007 Demo and Poster Sessions, pages 157?160,
Prague, June 2007. c?2007 Association for Computational Linguistics
Detecting Semantic Relations between Named Entities in Text
Using Contextual Features
Toru Hirano, Yoshihiro Matsuo, Genichiro Kikui
NTT Cyber Space Laboratories, NTT Corporation
1-1 Hikarinooka, Yokosuka-Shi, Kanagawa, 239-0847, Japan
{hirano.tohru, matsuo.yoshihiro, kikui.genichiro}@lab.ntt.co.jp
Abstract
This paper proposes a supervised learn-
ing method for detecting a semantic rela-
tion between a given pair of named enti-
ties, which may be located in different sen-
tences. The method employs newly intro-
duced contextual features based on center-
ing theory as well as conventional syntac-
tic and word-based features. These features
are organized as a tree structure and are
fed into a boosting-based classification al-
gorithm. Experimental results show the pro-
posed method outperformed prior methods,
and increased precision and recall by 4.4%
and 6.7%.
1 Introduction
Statistical and machine learning NLP techniques are
now so advanced that named entity (NE) taggers are
in practical use. Researchers are now focusing on
extracting semantic relations between NEs, such as
?George Bush (person)? is ?president (relation)? of
?the United States (location)?, because they provide
important information used in information retrieval,
question answering, and summarization.
We represent a semantic relation between two
NEs with a tuple [NE1, NE2, Relation Label]. Our
final goal is to extract tuples from a text. For exam-
ple, the tuple [George Bush (person), the U.S. (loca-
tion), president (Relation Label)] would be extracted
from the sentence ?George Bush is the president of
the U.S.?. There are two tasks in extracting tuples
from text. One is detecting whether or not a given
pair of NEs are semantically related (relation detec-
tion), and the other is determining the relation label
(relation characterization).
In this paper, we address the task of relation de-
tection. So far, various supervised learning ap-
proaches have been explored in this field (Culotta
and Sorensen, 2004; Zelenko et al, 2003). They
use two kinds of features: syntactic ones and word-
based ones, for example, the path of the given pair of
NEs in the parse tree and the word n-gram between
NEs (Kambhatla, 2004).
These methods have two problems which we con-
sider in this paper. One is that they target only intra-
sentential relation detection in which NE pairs are
located in the same sentence, in spite of the fact that
about 35% of NE pairs with semantic relations are
inter-sentential (See Section 3.1). The other is that
the methods can not detect semantic relations cor-
rectly when NE pairs located in a parallel sentence
arise from a predication ellipsis. In the following
Japanese example1, the syntactic feature, which is
the path of two NEs in the dependency structure,
of the pair with a semantic relation (?Ken11? and
?Tokyo12?) is the same as the feature of the pair with
no semantic relation (?Ken11? and ?New York14?).
(S-1) Ken11-wa Tokyo12-de, Tom13-wa
New York14-de umareta15.
(Ken11 was born15 in Tokyo12, Tom13 in
New York14.)
To solve the above problems, we propose a super-
vised learning method using contextual features.
The rest of this paper is organized as follows. Sec-
tion 2 describes the proposed method. We report the
results of our experiments in Section 3 and conclude
the paper in Section 4.
2 Relation Detection
The proposed method employs contextual features
based on centering theory (Grosz et al, 1983) as
well as conventional syntactic and word-based fea-
tures. These features are organized as a tree struc-
ture and are fed into a boosting-based classification
algorithm. The method consists of three parts: pre-
processing (POS tagging, NE tagging, and parsing),
1The numbers show correspondences of words between
Japanese and English.
157
feature extraction (contextual, syntactic, and word-
based features), and classification.
In this section, we describe the underlying idea of
contextual features and how contextual features are
used for detecting semantic relations.
2.1 Contextual Features
When a pair of NEs with a semantic relation appears
in different sentences, the antecedent NE must be
contextually easily referred to in the sentence with
the following NE. In the following Japanese exam-
ple, the pair ?Ken22? and ?amerika32 (the U.S.)?
have a semantic relation ?wataru33 (go)?, because
?Ken22? is contextually referred to in the sentence
with ?amerika32? (In fact, the zero pronoun ?i
refers to ?Ken22?). Meanwhile, the pair ?Naomi25?
and ?amerika32? has no semantic relation, because
the sentence with ?amerika32? does not refer to
?Naomi25?.
(S-2) asu21, Ken22-wa Osaka23-o otozure24
Naomi25-to au26.
(Ken22 is going to visit24 Osaka23 to see26
Naomi25, tomorrow21.)
(S-3) sonogo31, (?i-ga) amerika32-ni watari33
Tom34-to ryoko35 suru.
(Then31, (hei) will go33 to the U.S.32 to travel35
with Tom34.)
Furthermore, when a pair of NEs with a seman-
tic relation appears in a parallel sentence arise from
predication ellipsis, the antecedent NE is contextu-
ally easily referred to in the phrase with the follow-
ing NE. In the example of ?(S-1)?, the pair ?Ken11?
and ?Tokyo12? have a semantic relation ?umareta15
(was born)?. Meanwhile, the pair ?Ken11? and
?New York14? has no semantic relation.
Therefore, using whether the antecedent NE is re-
ferred to in the context with the following NE as fea-
tures of a given pair of NEs would improve relation
detection performance. In this paper, we use cen-
tering theory (Kameyama, 1986) to determine how
easily a noun phrase can be referred to in the follow-
ing context.
2.2 Centering Theory
Centering theory is an empirical sorting rule used to
identify the antecedents of (zero) pronouns. When
there is a (zero) pronoun in the text, noun phrases
that are in the previous context of the pronoun are
sorted in order of likelihood of being the antecedent.
The sorting algorithm has two steps. First, from the
beginning of the text until the pronoun appears, noun
Osaka
23
o asu
21
, Naomi
25
othersni
ga Ken22wa
Priority
Figure 1: Information Stacked According to Center-
ing Theory
phrases are stacked depending on case markers such
as particles. In the above example, noun phrases,
?asu21?, ?Ken22?, ?Osaka23? and ?Naomi25?, which
are in the previous context of the zero pronoun ?i,
are stacked and then the information shown in Fig-
ure 1 is acquired. Second, the stacked information is
sorted by the following rules.
1. The priority of case markers is as follows: ?wa
> ga > ni > o > others?
2. The priority of stack structure is as follows:
last-in first-out, in the same case marker
For example, Figure 1 is sorted by the above rules
and then the order, 1: ?Ken22?, 2: ?Osaka23?, 3:
?Naomi25?, 4: ?asu21?, is assigned. In this way, us-
ing centering theory would show that the antecedent
of the zero pronoun ?i is ?Ken22?.
2.3 Applying Centering Theory
When detecting a semantic relation between a given
pair of NEs, we use centering theory to determine
how easily the antecedent NE can be referred to in
the context with the following NE. Note that we do
not explicitly execute anaphora resolutions here.
Applied centering theory to relation detection is
as follows. First, from the beginning of the text until
the following NE appears, noun phrases are stacked
depending on case markers, and the stacked infor-
mation is sorted by the above rules (Section 2.2).
Then, if the top noun phrase in the sorted order is
identical to the antecedent NE, the antecedent NE is
?positive? when being referred to in the context with
the following NE.
When the pair of NEs, ?Ken22? and ?amerika32?,
is given in the above example, the noun phrases,
?asu21?, ?Ken22?, ?Osaka23? and ?Naomi25?, which
are in the previous context of the following NE
?amerika32?, are stacked (Figure 1). Then they are
sorted by the above sorting rules and the order, 1:
?Ken22?, 2: ?Osaka23?, 3: ?Naomi25?, 4: ?asu21?,
is acquired. Here, because the top noun phrase in
the sorted order is identical to the antecedent NE,
the antecedent NE ?Ken22? is ?positive? when be-
158
amerika
32wa: Ken
22
o: Osaka
23
others: Naomi
25others: asu
21
Figure 2: Centering Structure
ing referred to in the context with the following NE
?amerika32?. Whether or not the antecedent NE is
referred to in the context with the following NE is
used as a feature. We call this feature Centering Top
(CT).
2.4 Using Stack Structure
The sorting algorithm using centering theory tends
to rank highly thoes words that easily become sub-
jects. However, for relation detection, it is necessary
to consider both NEs that easily become subjects,
such as person and organization, and NEs that do not
easily become subjects, such as location and time.
We use the stack described in Section 2.3 as a
structural feature for relation detection. We call this
feature Centering Structure (CS). For example, the
stacked information shown in Figure 1 is assumed
to be structure information, as shown in Figure 2.
The method of converting from a stack (Figure 1)
into a structure (Figure 2) is described as follows.
First, the following NE, ?amerika32?, becomes the
root node because Figure 1 is stacked information
until the following NE appears. Then, the stacked
information is converted to Figure 2 depending on
the case markers. We use the path of the given pair
of NEs in the structure as a feature. For example,
?amerika32 ? wa:Ken22?2 is used as the feature of
the given pair ?Ken22? and ?amerika32?.
2.5 Classification Algorithm
There are several structure-based learning algo-
rithms proposed so far (Collins and Duffy, 2001;
Suzuki et al, 2003; Kudo and Matsumoto, 2004).
The experiments tested Kudo and Matsumoto?s
boosting-based algorithm using sub trees as features,
which is implemented as the BACT system.
In relation detection, given a set of training exam-
ples each of which represents contextual, syntactic,
and word-based features of a pair of NEs as a tree
labeled as either having semantic relations or not,
the BACT system learns that a set of rules are ef-
fective in classifying. Then, given a test instance,
which represents contextual, syntactic, and word-
2
?A? B? means A has a dependency relation to B.
Type % of pairs with semantic relations
(A) Intra-sentential 31.4% (3333 / 10626)
(B) Inter-sentential 0.8% (1777 / 225516)
(A)+(B) Total 2.2% (5110 / 236142)
Table 1: Percent of pairs with semantic relations in
annotated text
based features of a pair of NEs as a tree, the BACT
system classifies using a set of learned rules.
3 Experiments
We experimented with texts from Japanese newspa-
pers and weblogs to test the proposed method. The
following four models were compared:
1. WD : Pairs of NEs within n words are detected
as pairs with semantic relation.
2. STR : Supervised learning method using syn-
tactic3 and word-based features, the path of the
pairs of NEs in the parse tree and the word n-
gram between pairs of NEs (Kambhatla, 2004)
3. STR-CT : STR with the centering top feature
explained in Section 2.3.
4. STR-CS : STR with the centering structure fea-
ture explained in Section 2.4.
3.1 Setting
We used 1451 texts from Japanese newspapers and
weblogs, whose semantic relations between person
and location had been annotated by humans for the
experiments4. There were 5110 pairs with seman-
tic relations out of 236,142 pairs in the annotated
text. We conducted ten-fold cross-validation over
236,142 pairs of NEs so that sets of pairs from a
single text were not divided into the training and test
sets.
We also divided pairs of NEs into two types: (A)
intra-sentential and (B) inter-sentential. The reason
for dividing them is so that syntactic structure fea-
tures would be effective in type (A) and contextual
features would be effective in type (B). Another rea-
son is that the percentage of pairs with semantic rela-
tions out of the total pairs in the annotated text differ
significantly between types, as shown in Table 1.
In the experiments, all features were automati-
cally acquired using a Japanese morphological and
dependency structure analyzer.
3There is no syntactic feature in inter-sentential.
4We are planning to evaluate the other pairs of NEs.
159
(A)+(B) Total (A) Intra-sentential (B) Inter-sentential
Precision Recall Precision Recall Precsion Recall
WD10 43.0(2501/5819) 48.9(2501/5110) 48.1(2441/5075) 73.2(2441/3333) 8.0(60/744) 3.4(60/1777)
STR 69.3(2562/3696) 50.1(2562/5110) 75.6(2374/3141) 71.2(2374/3333) 33.9(188/555) 10.6(188/1777)
STR-CT 71.4(2764/3870) 54.1(2764/5110) 78.4(2519/3212) 75.6(2519/3333) 37.2(245/658) 13.8(245/1777)
STR-CS 73.7(2902/3935) 56.8(2902/5110) 80.1(2554/3187) 76.6(2554/3333) 46.5(348/748) 27.6(348/1777)
WD10: NE pairs that appear within 10 words are detected.
Table 2: Results for Relation Detection
0
0.2
0.4
0.6
0.8
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
P
r
e
c
i
s
i
o
n
WD
STR
STR-CT
STR-CS
STR-CS
STR
WD
STR-CT
Figure 3: Recall-precision Curves: (A)+(B) total
3.2 Results
To improve relation detection performance, we in-
vestigated the effect of the proposed method using
contextual features. Table 2 shows results for Type
(A), Type (B), and (A)+(B). We also plotted recall-
precision curves5, altering threshold parameters, as
shown in Figure 3.
The comparison between STR and STR-CT and
between STR and STR-CS in Figure 3 indicates that
the proposed method effectively contributed to rela-
tion detection. In addition, the results for Type (A):
intra-sentential, and (B): inter-sentential, in Table
2 indicate that the proposed method contributed to
both Type (A), improving precision by about 4.5%
and recall by about 5.4% and Type (B), improving
precision by about 12.6% and recall by about 17.0%.
3.3 Error Analysis
Over 70% of the errors are covered by two major
problems left in relation detection.
Parallel sentence: The proposed method solves
problems, which result from when a parallel
sentence arises from predication ellipsis. How-
ever, there are several types of parallel sentence
that differ from the one we explained. (For ex-
ample, Ken and Tom was born in Osaka and
New York, respectively.)
5Precision = # of correctly detected pairs / # of detected pairs
Recall = # of correctly detected pairs / # of pairs with semantic
relations
Definite anaphora: Definite noun phrase, such as
?Shusho (the Prime Minister)? and ?Shacho
(the President)?, can be anaphors. We should
consider them in centering theory, but it is dif-
ficult to find them in Japanese .
4 Conclusion
In this paper, we propose a supervised learning
method using words, syntactic structures, and con-
textual features based on centering theory, to im-
prove both inter-sentential and inter-sentential rela-
tion detection. The experiments demonstrated that
the proposed method increased precision by 4.4%,
up to 73.7%, and increased recall by 6.7%, up to
56.8%, and thus contributed to relation detection.
In future work, we plan to solve the problems re-
lating to parallel sentence and definite anaphora, and
address the task of relation characterization.
References
M. Collins and N. Duffy. 2001. Convolution Kernels for
Natural Language. Proceedings of the Neural Information
Processing Systems, pages 625?632.
A. Culotta and J. Sorensen. 2004. Dependency Tree Kernels
for Relation Extraction. Annual Meeting of Association of
Computational Linguistics, pages 423?429.
B. J. Grosz, A. K. Joshi, and S. Weistein. 1983. Providing a
unified account of definite nounphrases in discourse. Annual
Meeting of Association of Computational Linguistics, pages
44?50.
N. Kambhatla. 2004. Combining Lexical, Syntactic, and Se-
mantic Features with Maximum Entropy Models for Infor-
mation Extraction. Annual Meeting of Association of Com-
putational Linguistics, pages 178?181.
M. Kameyama. 1986. A property-sharing constraint in center-
ing. Annual Meeting of Association of Computational Lin-
guistics, pages 200?206.
T. Kudo and Y. Matsumoto. 2004. A boosting algorithm for
classification of semi-structured text. In Proceedings of the
2004 EMNLP, pages 301?308.
J. Suzuki, T. Hirao, Y. Sasaki, and E. Maeda. 2003. Hier-
archical directed acyclic graph kernel : Methods for struc-
tured natural language data. Annual Meeting of Association
of Computational Linguistics, pages 32?39.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel Meth-
ods for Relation Extraction. Journal of Machine Learning
Research, pages 3:1083?1106.
160
Coling 2010: Poster Volume, pages 409?417,
Beijing, August 2010
Recognizing Relation Expression between Named Entities based on
Inherent and Context-dependent Features of Relational words
Toru Hirano?, Hisako Asano?, Yoshihiro Matsuo?, Genichiro Kikui?
?NTT Cyber Space Laboratories, NTT Corporation
?Innovative IP Architecture Center, NTT Communications Corporation
hirano.tohru@lab.ntt.co.jp
hisako.asano@ntt.com
{matsuo.yoshihiro,kikui.genichiro}@lab.ntt.co.jp
Abstract
This paper proposes a supervised learn-
ing method to recognize expressions that
show a relation between two named en-
tities, e.g., person, location, or organiza-
tion. The method uses two novel fea-
tures, 1) whether the candidate words in-
herently express relations and 2) how the
candidate words are influenced by the past
relations of two entities. These features
together with conventional syntactic and
contextual features are organized as a tree
structure and are fed into a boosting-based
classification algorithm. Experimental re-
sults show that the proposed method out-
performs conventional methods.
1 Introduction
Much attention has recently been devoted to us-
ing enormous amount of web text covering an ex-
ceedingly wide range of domains as a huge knowl-
edge resource with computers. To use web texts as
knowledge resources, we need to extract informa-
tion from texts that are merely sequences of words
and convert them into a structured form. Although
extracting information from texts as a structured
form is difficult, relation extraction is a way that
makes it possible to use web texts as knowledge
resources.
The aim of relation extraction is to extract se-
mantically related named entity pairs, X and Y ,
and their relation, R, from a text as a struc-
tured form [X , Y , R]. For example, the triple
[Yukio Hatoyama, Japan, prime minister] would
be extracted from the text ?Yukio Hatoyama is the
prime minister of Japan?. This extracted triple
provides important information used in informa-
tion retrieval (Zhu et al, 2009) and building an
ontology (Wong et al, 2010).
It is possible to say that all named entity pairs
that co-occur within a text are semantically related
in some way. However, we define that named en-
tity pairs are semantically related if they satisfy
either of the following rules:
? One entity is an attribute value of the other.
? Both entities are arguments of a predicate.
Following the above definition, explicit and im-
plicit relations should be extracted. An explicit re-
lation means that there is an expression that shows
the relation between a named entity pair in a given
text, while an implicit relation means that there is
no such expression. For example, the triple [Yukio
Hatoyama, Kunio Hatoyama, brother] extracted
from the text ?Yukio Hatoyama, the Democratic
Party, is Kunio Hatoyama?s brother? is an explicit
relation. In contrast, the triple [Yukio Hatoyama,
the Democratic Party, member] extracted from the
same text is an implicit relation because there is
no expression showing the relation (e.g. member)
between ?Yukio Hatoyama? and ?the Democratic
Party? in the text.
Extracting triples [X , Y , R] from a text in-
volves two tasks. One is detecting semantically
related pairs from named entity pairs that co-occur
in a text and the other is determining the rela-
tion between a detected pair. For the former task,
various supervised learning methods (Culotta and
Sorensen, 2004; Zelenko et al, 2003; Hirano et
al., 2007) and bootstrapping methods (Brin, 1998;
Pantel and Pennacchiotti, 2006) have been ex-
plored to date. In contrast, for the latter task,
409
only a few methods have been proposed so far
(Hasegawa et al, 2004; Banko and Etzioni, 2008;
Zhu et al, 2009). We therefore addressed the
problem of how to determine relations between a
given pair.
We used a three-step approach to address this
problem. The first step is to recognize an expres-
sion that shows explicit relations between a given
named entity pair in a text. If no such expression
is recognized, the second step is to estimate the
relationship that exists between a given named en-
tity pair that has an implicit relation. The last step
is to identify synonyms of the relations that are
recognized or estimated in the above steps. In this
paper, we focus on the first step. The task is se-
lecting a phrase from the text that contains a re-
lation expression linking a given entity pair and
outputting the expression as one showing the rela-
tionship between the pair.
In our preliminary experiment, it was found
that using only structural features of a text, such
as syntactic or contextual features, is not good
enough for a number of examples. For instance,
the two Japanese sentences shown in Figure 1
have the same syntactic structure but (a) contains a
relation expression and (b) does not. We therefore
assume there are clues for recognizing relation
expressions other than conventional syntactic and
contextual information. In this paper, we propose
a supervised learning method that includes two
novel features of relational words as well as con-
ventional syntactic and contextual features. The
novel features of our method are:
Inherent Feature: Some words are able to ex-
press the relations between named entities
and some are not. Thus, it would be useful to
know the words that inherently express these
relations.
Context-dependent Feature: There are a num-
ber of typical relationships that change as
time passes, such as ?dating? ? ?engage-
ment? ? ?marriage? between persons. Fur-
thermore, present relations are influenced by
the past relations of a given named entity
pair. Thus, it would be useful to know the
past relations between a given pair and how
the relations change as time passes.
In the rest of this paper, Section 2 references re-
lated work, Section 3 outlines our method?s main
features and related topics, Section 4 describes our
experiments and experimental results, and Section
5 briefly summarizes key points and future work
to be done.
2 Related Work
The ?Message Understanding Conference? and
?Automatic Content Extraction? programs have
promoted relational extraction. The task was stud-
ied so as to extract predefined semantic relations
of entity pairs in a text. Examples include the
supervised learning method cited in (Kambhatla,
2004; Culotta and Sorensen, 2004; Zelenko et al,
2003) and the bootstrapping method cited in (Pan-
tel and Pennacchiotti, 2006; Agichtein and Gra-
vano, 2000). Recently, open information extrac-
tion (Open IE), a novel domain-independent ex-
traction paradigm, has been suggested (Banko and
Etzioni, 2008; Hasegawa et al, 2004). The task is
to detect semantically related named entity pairs
and to recognize the relation between them with-
out using predefined relations.
Our work is a kind of open IE, but our approach
differs from that of previous studies. Banko
(2008) proposed a supervised learning method us-
ing conditional random fields to recognize the re-
lation expressions from words located between a
given pair. Hasegawa (2004) also proposed a rule-
based method that selects all words located be-
tween a given pair as a relation expression if a
given named entities appear within ten words. The
point of these work is that they selected relation
expressions only from the words located between
Osaka Fucho
01
-nosaka ucho
01
-noKacho02-noacho02-no
Yumei
04
-desu.u ei
04
-desu.Suzuki
03
-san-wauzuki
03
-san- a
D
DD
Osaka Fucho
05
-nosaka ucho
05
-noSoumukyoku06-noou ukyoku06-no
Yumei
08
-desu.u ei
08
-desu.Suzuki
07
-san-wauzuki
07
-san- a
D
DD
(a)Mr.Suzuki
03
, a manager
02
of Osaka Prefectural Government
01
, is famous
04
.(b)Mr.Suzuki
07
, administration office
06
in Osaka PrefecturalGovernment
05
, is famous
08
.
(a) (b)
Figure 1: Same syntactic examples
410
given entities in the text, because as far as English
texts are concerned, 86% of the relation expres-
sions of named entity pairs appear between the
pair (Banko and Etzioni, 2008). However, our tar-
get is Japanese texts, in which only 26% of entity
pair relation expressions appear between the pair.
Thus, it is hard to incorporate previous approaches
into a Japanese text.
To solve the problem, our task was to select a
phrase from the entire text that would include a
relation expression for connecting a given pair.
3 Recognizing Relation Expressions
between Named Entities
To recognize the relation expression for a given
pair, we need to select a phrase that includes an
expression that shows the relation between a given
entity pair from among all noun and verb phrases
in a text. Actually, there are two types of candi-
date phrases in this case. One is from a sentence
that contains a given pair (intra-sentential), and
the other is from a sentence that does not (inter-
sentential). For example, the triple [Miyaji21,
Ishii22, taiketsu12] extracted from the following
text is inter-sentential.
(S-1) Chumokoku11-no taiketsu12-ga
mamonaku13 hajimaru14.
(The showcase11 match12 will start14 soon13.)
(S-2) Ano Miyaji21-to Ishii22-toiu
kanemochi23-niyoru yume24-no
kikaku25.
(The dream24 event25 between the rich mens23,
Miyaji21 and Ishii22.)
According to our annotated data shown in Ta-
ble 2, 53% of the semantically-related named en-
tity pairs are intra-sentential and 12% are inter-
sentential. Thus, we first select a phrase from
those in a sentence that contains a given pair, and
if no phrase is selected, select one from the rest of
the sentences in a text.
We propose a supervised learning method that
uses two novel features of relational words as
well as conventional syntactic and contextual fea-
tures. These features are organized as a tree struc-
ture and are fed into a boosting-based classifica-
tion algorithm (Kudo and Matsumoto, 2004). The
highest-scoring phrase is then selected if the score
exceeds a given threshold. Finally, the head of the
selected phrase is output as the relation expression
of a given entity pair.
The method consists of four parts: preprocess-
ing (POS tagging and parsing), feature extraction,
classification, and selection. In this section, we
describe the idea behind using our two novel fea-
tures and how they are implemented to recognize
the relation expressions of given pairs. Before
that, we will describe our proposed method?s con-
ventional features.
3.1 Conventional Features
Syntactic feature
To recognize the intra-sentential relation ex-
pressions for a given pair, we assume that there
is a discriminative syntactic structure that consists
of given entities and their relation expression. For
example, there is a structure for which the com-
mon parent phrase of the given pair, X = ?Ha-
toyama Yukio32? and Y = ?Hatoyama Kunio33?,
has the relation expression, R = ?ani34? in the
Japanese sentence S-3. Figure 2 shows the depen-
dency tree of sentence S-3.
(S-3) Minshuto31-no Hatoyama Yukio32-wa
Hatoyama Kunio33-no ani34-desu.
(Yukio Hatoyama32, the Democratic Party31,
is Kunio Hatoyama33?s brother34.)
To use a discriminative structure for each can-
didate, we make a minimum tree that consists of
given entities and the candidate where each phrase
is represented by a case marker ?CM?, a depen-
dency type ?DT?, an entity class, and the string
and POS of the candidate (See Figure 3).
Minshuto
31
-noinshuto
31
-no
Hatoyama Yukio
32
-waatoya a ukio
32
- a
Ani
34
-desu.ni
34
-desu.
Hatoyama Kunio
33
-noatoya a unio
33
-noD
D D
Figure 2: Dependency tree of sentence S-3
411
X:person:person
Phrasehrase
PhrasehraseCandidateandidatePhrasehrase
Y:person:person
CM:wa: a DT:D:
STR:Ani
34
: ni
34
POS:Noun: ounCM:?: DT:O:CM:no:no DT:D: Inh:1Inh:1C
rank
:1
rank
:1C
prob
:0.23
prob
:0.23
Figure 3: Intra-sentential feature tree
Contextual Feature
To recognize the inter-sentential relation ex-
pressions for a given pair, we assume that there
is a discriminative contextual structure that con-
sists of given entities and their relation expression.
Here, we use a Salient Referent List (SRL) to ob-
tain contextual structure. The SRL is an empirical
sorting rule proposed to identify the antecedent
of (zero) pronouns (Nariyama, 2002), and Hirano
(2007) proposed a way of applying SRL to rela-
tion detection. In this work, we use this way to
apply SRL to recognize inter-sentential relation
expressions.
We applied SRL to each candidate as follows.
First, from among given entities and the candi-
date, we choose the one appearing last in the text
as the root of the tree. We then append noun
phrases, from the chosen one to the beginning of
the text, to the tree depending on case markers,
?wa? (topicalised subject), ?ga? (subject), ?ni?
(indirect object),?wo? (object), and ?others?, with
the following rules. If there are nodes of the same
case marker already in the tree, the noun phrase
is appended as a child of the leaf node of them.
In other cases, the noun phrase is appended as a
child of the root node. For example, we get the
SRL tree shown in Figure 4 with the given entity
pair, X = ?Miyaji21? and Y = ?Ishii22?, and the
candidate, ?taiketsu12?, with the text (S-1, S-2).
To use a discriminative SRL structure, we make
a minimum tree that consists of given entities and
the candidate where each phrase is represented by
an entity class, and the string and POS of the can-
didate (See Figure 5). In this way, there is a prob-
lem when the candidate is a verb phrase, because
ga: Taiketsu
12
ga: aiketsu
12
Ishii
22
Ishii
22
others: Miyaji
21
others: iyaji
21
others: Chumoku
11
others: hu oku
11
Figure 4: Salient referent list tree
only noun phrases are appended to the SRL tree.
If the candidate is a verb phrase, we cannot make
a minimum tree that consists of given entities and
the candidate.
To solve this problem, a candidate verb phrase
is appended to the feature tree using a syntactic
structure. In a dependency tree, almost all verb
phrases have some parent or child noun phrases
that are in the SRL tree. Thus, candidate verb
phrases are appended as offspring of these noun
phrases represented syntactically as ?parent? or
?child?. For example, when given the entity pair,
X = ?Miyaji21? and Y = ?Ishii22?, and the can-
didate, ?hajimaru14? from the text (S-1, S-2), a
feature tree cannot be made because the candi-
date is not in an SRL tree. By extending the way
the syntactic structure is used, ?hajimaru14? has a
child node ?taiketsu12?, which is in an SRL tree,
and this makes it possible to make the feature tree
shown in Figure 6.
3.2 Proposed Features
To recognize intra-sentential or inter-sentential re-
lation expressions for given pairs, we assume
there are clues other than syntactic and contex-
tual information. Thus, we propose inherent and
SRL:gaL:ga Candidateandidate
Y:person:person
X:person:personSRL:othersL:othersSTR:Taiketsu
12
: aiketsu
12
POS:Noun: ounInh:1Inh:1 C
rank
:1
rank
:1 C
prob
:0.23
prob
:0.23
Figure 5: Inter-sentential feature tree
412
SRL:gaL:ga
Dep:Childep: hild Candidateandidate
Y:person:person X:person:personSRL:othersL:othersSTR:Hajimaru
14
: aji aru
14
POS:Verb: erbInh:0Inh:0 C
rank
:2
rank
:2 C
prob
:0.00
prob
:0.00
Figure 6: Extended inter-sentential feature tree
context-dependent features of relational words.
Inherent Feature of Relational words
Some words are able to express the relations be-
tween named entities and some are not. For exam-
ple, the word ?mother? can express a relation, but
the word ?car? cannot. If there were a list of words
that could express relations between named enti-
ties, it would be useful to recognize the relation
expression of a given pair. As far as we know,
however, no such list exists in Japanese. Thus,
we estimate which words are able to express rela-
tions between entities. Here, we assume that al-
most all verbs are able to express relations, and
accordingly we focus on nouns.
When the relation expression, R, of an entity
pair, X and Y , is a noun, it is possible to say ?Y is
R of X? or ?Y is X?s R?. Here, we can say noun
R takes an argument X . In linguistics, this kind
of noun is called a relational noun. Grammatically
speaking, a relational noun is a simple noun, but
because its meaning describes a ?relation? rather
than a ?thing?, it is used to describe relations just
as prepositions do. To estimate which nouns are
able to express the relations between named enti-
ties, we use the characteristics of relational nouns.
In linguistics, many researchers describe the rela-
tionship between possessives and relational nouns
(Chris, 2008). Thus, we use the knowledge that
in the patterns ?B of A? or ?A?s B?, if word B is
a relational noun, the corresponding word A be-
longs to a certain semantic category. In contrast,
if word B is not a relational noun, the correspond-
ing word A belongs to many semantic categories
(Tanaka et al, 1999). Figure 7 shows scattering
of the semantic categories of ?mother? and ?car?
Semantic categoriesRelative
 Frequency
Semantic categoriesRelative
 Frequency
Figure 7: Scattering of semantic category of
?mother? (left) and ?car? (right).
acquired by the following way.
First, we acquired A and B using the patterns
?A no B?1 from a large Japanese corpus, then
mapped words A into semantic categories C= {
c1, c2, ? ? ? , cm } using a Japanese lexicon (Ikehara
et al, 1999). Next, for each word B, we calcu-
lated a scattering score Hc(B) using the semantic
category of corresponding words A. Finally, we
estimated whether a word is a relational noun by
using k-NN estimation with positive and negative
examples. As estimated results, ?Inh:1? shows
that it is a relational noun and ?Inh:0? shows that
it is not. In both cases, the result is appended to
the feature tree as a child of the candidate node
(See Figure 3, 5, or 6).
Hc(B) = ?
?
c?C
P (c|B)logmP (c|B)
P (c|B) = freq(c,B)freq(B)
In our experiments, we acquired 55,412,811
pairs of A and B from 1,698,798 newspaper ar-
ticles and 10,499,468 weblog texts. As training
data, we used the words of relation expressions as
positive examples and other words as negative ex-
amples.
Context-dependent Feature of Relational
words
There are a number of typical relationships that
change as time passes, such as ?dating? ? ?en-
gagement? ? ?marriage? between persons. Fur-
thermore, present relations are affected by the past
relations of a given named entity pair. For in-
stance, if the past relations of a given pair are ?dat-
ing? and ?engagement? and one of the candidates
is ?marriage?, ?marriage? would be selected as the
relation expression of the given pair. Therefore, if
1
?B of A? or ?A?s B? in English.
413
Pair of entity class rm rn PT (rn|rm) Count(rm, rn)
dating 0.050 102
?person,person? dating marriage 0.050 101
engagement 0.040 82
marriage 0.157 786
?person,person? engagement engagement 0.065 325
wedding 0.055 276
president 0.337 17,081
?person,organization? vice president vice president 0.316 16,056
CEO 0.095 4,798
fellow 0.526 61
?person,organization? researcher manager 0.103 12
member 0.078 9
alliance 0.058 8,358
?organization,organization? alliance accommodated 0.027 3,958
acquisition 0.027 3,863
mutual consultation 0.022 2,670
?location,location? neighbour support 0.015 1,792
visit 0.012 1,492
war 0.077 78,170
?location,location? war mutual consultation 0.015 15,337
support 0.010 10,226
Table 1: Examples of calculated relation trigger model between entity classes defined by IREX
we know the past relations of the given pair and
the typical relational change that occurs as time
passes, it would be useful to recognize the rela-
tion expression of a given pair.
In this paper, we represent typical relational
changes that occur as time passes by a simple re-
lation trigger model PT (rn|rm). Note that rm
is a past relation and rn is a relation affected by
rm. This model disregards the span between rn
and rm. To make the trigger model, we automat-
ically extract triples [X , Y , R] from newspaper
articles and weblog texts, which have time stamps
of the document creation. Using these triples with
time stamps for each entity pair, we sort rela-
tions in order of time and count pairs of present
and previous relations. For example, if we ex-
tract ?dating? occurring for an entity pair on Jan-
uary 10, 1998, ?engagement? occurring on Febru-
ary 15, 2001, and ?marriage? occurring on De-
cember 24, 2001, the pairs ?dating, engagement?,
?dating, marriage?, and ?engagement, marriage?
are counted. The counted score is then summed
up by the pair of entity class and the trigger model
is calculated by the following formula.
PT (rn|rm) =
Count(rm, rn)?
rn Count(rm, rn)
For the evaluation, we extracted triples by
named entity recognition (Suzuki et al, 2006), re-
lation detection (Hirano et al, 2007), and the pro-
posed method using the inherent features of rela-
tional words described in Section 3.2. A total of
10,463,232 triples were extracted from 8,320,042
newspaper articles and weblog texts with time
stamps made between January 1, 1991 and June
30, 2006. As examples of the calculated relation
trigger model, Table 1 shows the top three proba-
bility relations rn of several relations rm between
Japanese standard named entity classes defined
in the IREX workshop2. For instance, the rela-
tion ?fellow? has the highest probability of being
changed from the relation ?researcher? between
person and organization as time passes.
2http://nlp.cs.nyu.edu/irex/
414
To obtain the past relations of a given pair in
the input text, we again used the triples with time
stamps extracted as above. The only relations we
use as past relations, Rm = {rm1 , rm2 , ? ? ? , rmk},
are those of a given pair whose time stamps are
older than the input text. Finally, we calcu-
lated probabilities with the following formula us-
ing the past relations Rm and the trigger model
PT (rn|rm).
PT (rn|Rm) = max{PT (rn|rm1),
PT (rn|rm2), ? ? ? , PT (rn|rmk)}
Using this calculated probability, we ranked
candidates and appended the rank ?Crank? and
the probability score ?Cprob? to the feature tree
as a child of the candidate node (See Figure 3,
5, or 6). For example, if the past relations Rm
were ?dating? and ?engagement? and candidates
were ?marriage?, ?meeting?, ?eating?, or ?drink-
ing?, the candidates probabilities were calculated
and ranked as ?marriage? (Cprob:0.15, Crank:1),
?meeting? (Cprob:0.08, Crank:2), etc.
3.3 Classification Algorithms
Several structure-based learning algorithms have
been proposed so far (Collins and Duffy, 2002;
Suzuki et al, 2003; Kudo and Matsumoto, 2004).
The experiments tested Kudo and Matsumoto?s
boosting-based algorithm using sub-trees as fea-
tures, which is implemented as a BACT system.
Given a set of training examples each of which
is represented as a tree labeling whether the can-
didate is the relation expression of a given pair or
not, the BACT system learns that a set of rules
is effective in classifying. Then, given a test in-
stance, the BACT system classifies using a set of
learned rules.
4 Experiments
We conducted experiments using texts from
Japanese newspaper articles and weblog texts to
test the proposed method for both intra- and inter-
sentential tasks. In the experiments, we compared
the following methods:
Conventional Features: trained by conventional
syntactic features for intra-sentential tasks as
Relation Types #
Explicit Intra-sentential 9,178Inter-sentential 2,058
Implicit 5,992
Total 17,228
Table 2: Details of the annotated data
described in Section 3.1, and contextual fea-
tures for inter-sentential tasks as described in
Section 3.1.
+Inherent Features: trained by conventional
features plus inherent features of relational
words described in Section 3.2.
++Context-dependent FeaturesTM: trained
by conventional and inherent features plus
context-dependent features of relational
words with the trigger model described in
Section 3.2.
++Context-dependent FeaturesCM: trained
by conventional and inherent features
plus context-dependent features of rela-
tional words with a cache model. We
evaluated this method to compare it with
Context-dependent FeaturesTM to show the
effectiveness of the proposed trigger model.
The cache model is a simple way to use past
relations in which the probability PC(rcand)
calculated by the following formula and the
rank based on the probability is appended to
every candidate feature tree.
PC(rcand) =
|rcand in past relations|
|past relations|
4.1 Settings
We used 6,200 texts from Japanese newspapers
and weblogs dated from January 1, 2004 to June
30, 2006, manually annotating the semantic rela-
tions between named entities for experiment pur-
poses. There were 17,228 semantically-related
entity pairs as shown in Table 2. In an intra-
sentential experiment, 17,228 entity pairs were
given, but only 9,178 of them had relation expres-
sions. In contrast, in an inter-sentential experi-
ment, 8,050 entity pairs excepted intra-sentential
415
Precision Recall F
Conventional Features 63.5? (3,436/5,411) 37.4? (3,436/9,178) 0.471
+Inherent Features 67.2? (4,036/6,001) 43.9? (4,036/9,178) 0.531
++Context-dependent FeaturesTM 70.7? (4,460/6,312) 48.6? (4,460/9,178) 0.576
++Context-dependent FeaturesCM 67.5? (4,042/5,987) 44.0? (4,042/9,178) 0.533
Table 3: Experimental results of intra-sentential
Precision Recall F
Conventional Features 70.1? (579/825) 28.1? (579/2,058) 0.401
+Inherent Features 77.1? (719/932) 34.9? (719/2,058) 0.480
++Context-dependent FeaturesTM 75.2? (794/1,055) 38.5? (794/2,058) 0.510
++Context-dependent FeaturesCM 74.3? (732/985) 35.5? (732/2,058) 0.481
Table 4: Experimental result of inter-sentential
were given, but only 2,058 of them had relation
expressions.
We conducted five-fold cross-validation over
17,228 entity pairs so that sets of pairs from a sin-
gle text were not divided into the training and test
sets. In the experiments, all features were auto-
matically acquired using a Japanese POS tagger
(Fuchi and Takagi, 1998) and dependency parser
(Imamura et al, 2007).
4.2 Results
Tables 3 and 4 show the performance of several
methods for intra-sentential and inter-sentential.
Precision is defined as the percentage of cor-
rect relation expressions out of recognized ones.
Recall is the percentage of correct relation ex-
pressions from among the manually annotated
ones. The F measure is the harmonic mean of
precision and recall.
A comparison with the Conventional Fea-
tures and Inherent Features method for intra-
/inter-sentential tasks indicates that the proposed
method using inherent features of relational words
improved intra-sentential tasks F by 0.06 points
and inter-sentential tasks F by 0.08 points. Us-
ing a statistical test (McNemar Test) demonstrably
showed the proposed method?s effectiveness.
A comparison with the Inherent Features and
Context-dependent FeaturesTM method showed
that the proposed method using context-dependent
features of relational words improved intra-/inter-
sentential task performance by 0.045 and 0.03
points, respectively. McNemar test results also
showed the method?s effectiveness.
To further compare the usage of context-
dependent features, trigger models, and cache
models, we also used Context-dependent
FeaturesCM method for comparison. Tables
3 and 4 show that our proposed trigger model
performed better than the cache model, and
McNemar test results showed that there was a
significant difference between the models. The
reason the trigger model performed better than
the cache model is that the trigger model correctly
recognized the relation expressions that did not
appear in the past relations of a given pair. Thus,
we can conclude that using typical relationships
that change as time passes helps to recognize
relation expressions between named entities.
5 Conclusion
We proposed a supervised learning method that
employs inherent and context-dependent features
of relational words and uses conventional syntac-
tic or contextual features to improve both intra-
and inter-sentential relation expression recogni-
tion. Our experiments demonstrated that the
method improves the F measure and thus helps
to recognize relation expressions between named
entities.
In future work, we plan to estimate implicit re-
lations between named entities and to identify re-
lational synonyms.
416
References
Agichtein, Eugene and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In Proceedings of the 5th ACM conference
on Digital libraries, pages 85?94.
Banko, Michele and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of the 46th Annual Meeting on Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 28?36.
Brin, Sergey. 1998. Extracting patterns and rela-
tions from the world wide web. In WebDB Work-
shop at 6th International Conference on Extending
Database Technology, pages 172?183.
Chris, Barker, 2008. Semantics: An international
handbook of natural language meaning, chap-
ter Possessives and relational nouns. Walter De
Gruyter Inc.
Collins, Michael and Nigel Duffy. 2002. Convolution
kernels for natural language. Advances in Neural
Information Processing Systems, 14:625?632.
Culotta, Aron and Jeffrey Sorensen. 2004. Depen-
dency tree kernels for relation extraction. In Pro-
ceedings of the 42nd Annual Meeting on Association
for Computational Linguistics, pages 423?429.
Fuchi, Takeshi and Shinichiro Takagi. 1998. Japanese
morphological analyzer using word co-occurrence
- jtag. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Compu-
tational Linguistics, volume 1, pages 409?413.
Hasegawa, Takaaki, Satoshi Sekine, and Ralph Grish-
man. 2004. Discovering relations among named
entities from large corpora. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, pages 415?422.
Hirano, Toru, Yoshihiro Matsuo, and Genichiro Kikui.
2007. Detecting semantic relations between named
entities in text using contextual features. In Pro-
ceedings of the 45th Annual Meeting on Association
for Computational Linguistics, pages 157?160.
Ikehara, Satoru, Masahiro Miyazaki, Satoru Shirai,
Akio Yoko, Hiromi Nakaiwa, Kentaro Ogura, Masa-
fumi Oyama, and Yoshihiko Hayashi. 1999. Ni-
hongo Goi Taikei (in Japanese). Iwanami Shoten.
Imamura, Kenji, Genichiro Kikui, and Norihito Ya-
suda. 2007. Japanese dependency parsing using se-
quential labeling for semi-spoken language. In Pro-
ceedings of the 45th Annual Meeting on Association
for Computational Linguistics, pages 225?228.
Kambhatla, Nanda. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for extracting relations. In Proceedings of
the 42nd Annual Meeting on Association for Com-
putational Linguistics, pages 178?181.
Kudo, Taku and Yuji Matsumoto. 2004. A boosting
algorithm for classification of semi-structured text.
In Proceedings of the 2004 Conference on Empiri-
cal Methods in Natural Language Processing, pages
301?308.
Nariyama, Shigeko. 2002. Grammar for ellipsis res-
olution in japanese. In Proceedings of the 9th In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation, pages 135?
145.
Pantel, Patrick and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automat-
ically harvesting semantic relations. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 113?120.
Suzuki, Jun, Tsutomu Hirao, Yutaka Sasaki, and
Eisaku Maeda. 2003. Hierarchical directed acyclic
graph kernel: Methods for structured natural lan-
guage data. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguis-
tics, pages 32?39.
Suzuki, Jun, Erik McDermott, and HIdeki Isozaki.
2006. Training conditional random fields with mul-
tivariate evaluation measures. In Proceedings of the
43th Annual Meeting on Association for Computa-
tional Linguistics.
Tanaka, Shosaku, Yoichi Tomiura, and Toru Hitaka.
1999. Classification of syntactic categories of
nouns by the scattering of semantic categories (in
japanese). Transactions of Information Processing
Society of Japan, 40(9):3387?3396.
Wong, Wilson, Wei Liu, and Mohammed Bennamoun.
2010. Acquiring semantic relations using the web
for constructing lightweight ontologies. In Proceed-
ings of the 13th Pacific-Asia Conference on Knowl-
edge Discovery and Data Mining.
Zelenko, Dmitry, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Journal of Machine Learning Research,
3:1083?1106.
Zhu, Jun, Zaiqing Nie, Xiaojing Liu, Bo Zhang, and
Ji-Rong Wen. 2009. Statsnowball: a statistical ap-
proach to extracting entity relationships. In Pro-
ceedings of the 18th international conference on
World Wide Web, pages 101?110.
417
Coling 2010: Poster Volume, pages 910?918,
Beijing, August 2010
Opinion Summarization with Integer Linear Programming Formulation
for Sentence Extraction and Ordering
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Matsuo and Genichiro Kikui
NTT Cyber Space Laboratories, NTT Corporation{ nishikawa.hitoshi, hasegawa.takaaki
matsuo.yoshihiro, kikui.genichiro
}
@lab.ntt.co.jp
Abstract
In this paper we propose a novel algorithm
for opinion summarization that takes ac-
count of content and coherence, simulta-
neously. We consider a summary as a se-
quence of sentences and directly acquire
the optimum sequence from multiple re-
view documents by extracting and order-
ing the sentences. We achieve this with a
novel Integer Linear Programming (ILP)
formulation. Our proposed formulation is
a powerful mixture of the Maximum Cov-
erage Problem and the Traveling Sales-
man Problem, and is widely applicable to
text generation and summarization tasks.
We score each candidate sequence accord-
ing to its content and coherence. Since
our research goal is to summarize reviews,
the content score is defined by opinions
and the coherence score is developed in
training against the review document cor-
pus. We evaluate our method using the
reviews of commodities and restaurants.
Our method outperforms existing opinion
summarizers as indicated by its ROUGE
score. We also report the results of human
readability experiments.
1 Introduction
The Web now holds a massive number of reviews
describing the opinions of customers about prod-
ucts and services. These reviews can help the cus-
tomer to reach purchasing decisions and guide the
business activities of companies such as product
improvement. It is, however, almost impossible to
read all reviews given their sheer number.
Automatic text summarization, particularly
opinion summarization, is expected to allow all
possible reviews to be efficiently utilized. Given
multiple review documents, our summarizer out-
puts text consisting of ordered sentences. A typ-
This restaurant offers customers a delicious menu and a
relaxing atmosphere. The staff are very friendly but the
price is a little high.
Table 1: A typical summary.
ical summary is shown in Table 1. This task is
considered as multidocument summarization.
Existing summarizers focus on organizing sen-
tences so as to include important information in
the given document into a summary under some
size limitation. A serious problem is that most of
these summarizers completely ignore coherence
of the summary, which improves reader?s compre-
hension as reported by Barzilay et al (2002).
To make summaries coherent, the extracted
sentences must be appropriately ordered. How-
ever, most summarization systems delink sentence
extraction from sentence ordering, so a sentence
can be extracted that can never be ordered natu-
rally with the other extracted sentences. More-
over, due to recent advances in decoding tech-
niques for text summarization, the summarizers
tend to select shorter sentences to optimize sum-
mary content. It aggravates this problem.
Although a preceding work tackles this prob-
lem by performing sentence extraction and order-
ing simultaneously (Nishikawa et al, 2010), they
adopt beam search and dynamic programming to
search for the optimal solution, so their proposed
method may fail to locate it.
To overcome this weakness, this paper proposes
a novel Integer Linear Programming (ILP) formu-
lation for searching for the optimal solution effi-
ciently. We formulate the multidocument sum-
marization task as an ILP problem that tries to
optimize the content and coherence of the sum-
mary by extracting and ordering sentences simul-
taneously. We apply our method to opinion sum-
marization and show that it outperforms state-of-
the-art opinion summarizers in terms of ROUGE
evaluations. Although in this paper we challenge
910
our method with opinion summarization, it can be
widely applied to other text generation and sum-
marization tasks.
This paper is organized as follows: Section 2
describes related work. Section 3 describes our
proposal. Section 4 reports our evaluation experi-
ments. We conclude this paper in Section 5.
2 Related Work
2.1 Sentence Extraction
Although a lot of summarization algorithms have
been proposed, most of them solely extract sen-
tences from a set of sentences in the source docu-
ment set. These methods perform extractive sum-
marization and can be formalized as follows:
S? = argmax
S?T
L(S) (1)
s.t. length(S) ? K
T stands for all sentences in the source docu-
ment set and S is an arbitrary subset of T . L(S)
is a function indicating the score of S as deter-
mined by one or more criteria. length(S) indi-
cates the length of S, K is the maximum size of
the summary. That is, most summarization algo-
rithms search for, or decode, the set of sentences S?
that maximizes function L under the given maxi-
mum size of the summary K. Thus most stud-
ies focus on the design of function L and efficient
search algorithms (i.e. argmax operation in Eq.1).
Objective Function
Many useful L functions have been proposed
including the cosine similarity of given sentences
(Carbonell and Goldstein, 1998) and centroid
(Radev et al, 2004); some approaches directly
learn function L from references (Kupiec et al,
1995; Hirao et al, 2002).
There are two approaches to defining the score
of the summary. One defines the weight on each
sentence forming the summary. The other defines
a weight for a sub-sentence, concept, that the sum-
mary contains.
McDonald (2007) and Martins and Smith
(2009) directly weight sentences and use MMR
to avoid redundancy (Carbonell and Goldstein,
1998). In contrast to their approaches, we set
weights on concepts, not sentences. Gillick
and Favre (2009) reported that the concept-based
model achieves better performance and scalability
than the sentence-based model when it is formu-
lated as ILP.
There is a wide range of choice with regard
to the unit of the concept. Concepts include
words and the relationship between named en-
tities (Filatova and Hatzivassiloglou, 2004), bi-
grams (Gillick and Favre, 2009), and word stems
(Takamura and Okumura, 2009).
Some summarization systems that target re-
views, opinion summarizers, extract particular
information, opinion, from the input sentences
and leverage them to select important sentences
(Carenini et al, 2006; Lerman et al, 2009). In
this paper, since we aim to summarize reviews,
the objective function is defined through opinion
as the concept that the reviews contain. We ex-
plain our detailed objective function in Section 3.
We describe features of above existing summariz-
ers in Section 4 and compare our method to them
as baselines.
Decoding Method
The algorithms proposed for argmax operation
include the greedy method (Filatova and Hatzivas-
siloglou, 2004), stack decoding (Yih et al, 2007;
Takamura and Okumura, 2009) and Integer Linear
Programming (Clarke and Lapata, 2007; McDon-
ald, 2007; Gillick and Favre, 2009; Martins and
Smith, 2009). Gillick and Favre (2009) and Taka-
mura and Okumura (2009) formulate summariza-
tion as a Maximum Coverage Problem. We also
use this formulation. While these methods focus
on extracting a set of sentences from the source
document set, our method performs extraction and
ordering simultaneously.
Some studies attempt to generate a single sen-
tence (i.e. headline) from the source document
(Banko et al, 2000; Deshpande et al, 2007).
While they extract and order words from the
source document as a unit, our model uses the unit
of sentences. This problem can be formulated as
the Traveling Salesman Problem and its variants.
Banko et al (2000) uses beam search to identify
approximate solutions. Deshpande et al (2007)
uses ILP and a randomized algorithm to find the
optimal solution.
2.2 Sentence Ordering
It is known that the readability of a collection of
sentences, a summary, can be greatly improved
by appropriately ordering them (Barzilay et al,
2002). Features proposed to create the appropri-
ate order include publication date of document
(Barzilay et al, 2002), content words (Lapata,
2003; Althaus et al, 2004), and syntactic role of
911
 
  
    
 
  
             	      	

     
     
Figure 1: Graph representation of summarization.
words (Barzilay and Lapata, 2005). Some ap-
proaches use machine learning to integrate these
features (Soricut and Marcu, 2006; Elsner et al,
2007). Generally speaking, these methods score
the discourse coherence of a fixed set of sentences.
These methods are separated from the extraction
step so they may fail if the set includes sentences
that are impossible to order naturally.
As mentioned above, there is a preceding work
that attempted to perform sentence extraction and
ordering simultaneously (Nishikawa et al, 2010).
Differences between this paper and that work are
as follows:
? This work adopts ILP solver as a decoder.
ILP solver allows the summarizer to search
for the optimal solution much more rapidly
than beam search (Deshpande et al, 2007),
which was adopted by the prior work. To
permit ILP solver incorporation, we propose
in this paper a totally new ILP formulation.
The formulation can be widely used for text
summarization and generation.
? Moreover, to learn better discourse coher-
ence, we adopt the Passive-Aggressive al-
gorithm (Crammer et al, 2006) and use
Kendall?s tau (Lapata, 2006) as the loss func-
tion. In contrast, the above work adopts Av-
eraged Perceptron (Collins, 2002) and has no
explicit loss function.
These advances make this work very different
from that work.
3 Our Method
3.1 The Model
We consider a summary as a sequence of sen-
tences. As an example, document set D =
{d1, d2, d3} is given to a summarizer. We de-
fine d as a single document. Document d1,
which consists of four sentences, is describe
by d1 = {s11, s12, s13, s14}. Documents d2
and d3 consist of five sentences and three sen-
tences (i.e. d2 = {s21, s22, s23, s24, s25}, d3 =
e1 e2 e3 . . . e6 e7 e8
s11 1 0 0 1 0 0
s12 0 1 0 0 0 0
s13 0 0 0 0 0 1
.
.
.
.
.
.
s31 0 0 0 0 0 0
s32 0 0 1 0 1 0
s33 0 0 0 0 0 1
Table 2: Sentence-Concept Matrix.
{s31, s32, s33}). If the summary consists of four
sentences s11, s23, s32, s33 and they are ordered as
s11 ? s23 ? s32 ? s33, we add symbols indicat-
ing the beginning of the summary s0 and the end
of the summary s4, and describe the summary as
S = ?s0, s11, s23, s32, s33, s4?. Summary S can
be represented as a directed path that starts at s0
and ends at s4 as shown in Fig. 1.
We describe a directed arc between si and sj as
ai,j ? A. The directed path shown in Fig. 1 is de-
composed into nodes, s0, s11, s23, s32, s33, s4, and
arcs, a0,11, a11,23, a23,32, a32,33, a33,4.
To represent the discourse coherence of two ad-
jacent sentences, we define weight ci,j ? C as
the coherence score on the directed arc ai,j . We
assume that better summaries have higher coher-
ence scores, i.e. if the sum of the scores of the arcs?
ai,j?S ci,jai,j is high, the summary is coherent.
We also assume that the source document set
D includes set of concepts e ? E. Each concept
e is covered by one or more of the sentences in
the document set. We show this schema in Ta-
ble 2. According to Table 2, document set D has
eight concepts e1, e2, . . . , e7, e8 and sentence s11
includes concepts e1 and e6 while sentence s12 in-
cludes e2.
We consider each concept ei has a weight wi.
We assume that concept ei will have high weight
wi if it is important. This paper improves sum-
mary quality by maximizing the sum of these
weights.
We define, based on the above assumption, the
following objective function:
L(S) = ?ei?S wiei +
?
ai,j?S ci,jai,j (2)
s.t. length(S) ? K
Summarization is, in this paper, realized by
maximizing the sum of weights of concepts in-
cluded in the summary and the coherence score of
all adjacent sentences in the summary under the
912
limit of maximum summary size. Note that while
S and T represents the set of sentences in Eq.1,
they represent the sequence of sentences in Eq.2.
Maximizing Eq.2 is NP-hard. If each sen-
tence in the source document set has one concept
(i.e. Table 2 is a diagonal matrix), Eq.2 becomes
the Prize Collecting Traveling Salesman Problem
(Balas, 1989). Therefore, a highly efficient decod-
ing method is essential.
3.2 Parameter Estimation
Our method requires two parameters: weights
w ? W of concepts and coherence c ? C of two
adjacent sentences. We describe them here.
Content Score
In this paper, as mentioned above, since we at-
tempt to summarize reviews, we adopt opinion
as a concept. We define opinion e = ?t, a, p?
as the tuple of target t, aspect a and its polarity
p ? {?1, 0, 1}. We define target t as the tar-
get of an opinion. For example, the target t of
the sentence ?This digital camera has good im-
age quality.? is digital camera. We define aspect
a as a word that represents a standpoint appro-
priate for evaluating products and services. With
regard to digital cameras, aspects include image
quality, design and battery life. In the above ex-
ample sentence, the aspect is image quality. Po-
larity p represents whether the opinion is positive
or negative. In this paper, we define p = ?1 as
negative, p = 0 as neutral and p = 1 as posi-
tive. Thus the example sentence contains opinion
e = ?digital camera, image quality, 1?.
Opinions are extracted using a sentiment ex-
pression dictionary and pattern matching from de-
pendency trees of sentences. This opinion extrac-
tor is the same as that used in Nishikawa et al
(2010).
As the weight wi of concept ei, we use only
the frequency of each opinion in the input docu-
ment set, i.e. we assume that an opinion that ap-
pears frequently in the input is important. While
this weighting is relatively naive compared to Ler-
man et al (2009)?s method, our ROUGE evalua-
tion shows that this approach is effective.
Coherence Score
In this section, we define coherence score c.
Since it is not easy to model the global coherence
of a set of sentences, we approximate the global
coherence by the sum of local coherence i.e. the
sum of coherence scores of sentence pairs. We
define local coherence score ci,j of two sentences
x = {si, sj} and their order y = ?si, sj? repre-
senting si ? sj as follows:
ci,j = w ? ?(x, y) (3)
w??(x, y) is the inner product ofw and ?(x, y),
w is a parameter vector and ?(x, y) is a feature
vector of the two sentences si and sj .
Since coherence consists of many different el-
ements and it is difficult to model all of them,
we approximate the features of coherence as the
Cartesian product of the following features: con-
tent words, POS tags of content words, named en-
tity tags (e.g. LOC, ORG) and conjunctions. Lap-
ata (2003) proposed most of these features.
We also define feature vector ?(x,y) of the bag
of sentences x = {s0, s1, . . . , sn, sn+1} and its
entire order y = ?s0, s1, . . . , sn, sn+1? as follows:
?(x,y) =
?
x,y
?(x, y) (4)
Therefore, the score of order y is w ? ?(x,y).
Given a training set, if trained parameter vector w
assigns score w ? ?(x,yt) to correct order yt that
is higher than score w ??(x, y?) assigned to incor-
rect order y?, it is expected that the trained parame-
ter vector will give a higher score to coherently or-
dered sentences than to incoherently ordered sen-
tences.
We use the Passive-Aggressive algorithm
(Crammer et al, 2006) to find w. The Passive-
Aggressive algorithm is an online learning algo-
rithm that updates the parameter vector by taking
up one example from the training examples and
outputting the solution that has the highest score
under the current parameter vector. If the output
differs from the training example, the parameter
vector is updated as follows;
min ||wi+1 ?wi|| (5)
s.t. s(x,yt;wi+1)? s(x, y?;wi+1) ? `(y?;yt)
s(x,y;w) = w ? ?(x,y)
wi is the current parameter vector and wi+1 is
the updated parameter vector. That is, Eq.5 means
that the score of the correct order must exceed the
score of an incorrect order by more than loss func-
tion `(y?;yt) while minimizing the change in pa-
rameters.
When updating the parameter vector, this al-
gorithm requires the solution that has the highest
score under the current parameter vector, so we
have to run an argmax operation. Since we are
913
attempting to order a set of sentences, the opera-
tion is regarded as solving the Traveling Salesman
Problem (Althaus et al, 2004); that is, we locate
the path that offers the maximum score through
all n sentences where s0 and sn+1 are starting and
ending points, respectively. This operation is NP-
hard and it is difficult to find the global optimal
solution. To overcome this, we find an approxi-
mate solution by beam search.1
We define loss function `(y?;yt) as follows:
`(y?;yt) = 1? ? (6)
? = 1 ? 4 S(y?,yt)N(N ? 1) (7)
? indicates Kendall?s tau. S(y?,yt) is the mini-
mum number of operations that swap adjacent ele-
ments (i.e. sentences) needed to bring y? to yt (La-
pata, 2006). N indicates the number of elements.
Since Lapata (2006) reported that Kendall?s tau
reliably reproduces human ratings with regard to
sentence ordering, using it to minimize the loss
function is expected to yield more reliable param-
eters.
We omit detailed derivations due to space limi-
tations. Parameters are updated as per the follow-
ing equation.
wi+1 = wi + ?i(?(x,yt)? ?(x, y?)) (8)
?i = `(y?;yt) ? s(x,yt;w
i) + s(x, y?;wi)
||?(x,yt)? ?(x, y?)||2 + 12C
(9)
C in Eq.9 is the aggressiveness parameter that
controls the degree of parameter change.
Note that our method learns w from documents
automatically annotated by a POS tagger and a
named entity tagger. That is, manual annotation
isn?t required.
3.3 Decoding with Integer Linear
Programming Formulation
This section describes an ILP formulation of the
above model. We use the same notation con-
vention as introduced in Section 3.1. We use
s ? S, a ? A, e ? E as the decision variable.
Variable si ? S indicates the inclusion of the i
th sentence. If the i th sentence is part of the
summary, then si is 1. If it is not part of the
1Obviously, ILP can be used to search for the path that
maximizes the score. While beam search tends to fail to find
out the optimal solution, it is tractable and the learning al-
gorithm can estimate the parameter from approximate solu-
tions. For these reasons we use beam search.
summary, then si is 0. Variable ai,j ? A indi-
cates the adjacency of the i th and j th sentences.
If these two sentences are ordered as si ? sj ,
then ai,j is 1. Variable ei ? E indicates the in-
clusion of the i th concept ei. Taking Fig.1 as
an example, variables s0, s11, s23, s32, s33, s4 and
a0,11, a11,23, a23,32, a32,33, a33,4 are 1. ei, which
correspond to the concepts in the above extracted
sentences, are also 1.
We represent the above objective function
(Eq.2) as follows:
max
?
?
??
?
ei?E
wiei + (1 ? ?)
?
ai,j?A
ci,jai,j
?
?
? (10)
Eq.10 attempts to cover as much of the concepts
included in input document set as possible accord-
ing to their weights w ? W and orders sentences
according to discourse coherence c ? C. ? is a
scaling factor to balance w and c.
We then impose some constraints on Eq.10 to
acquire the optimum solution.
First, we range the above three variables s ?
S, a ? A, e ? E.
si, ai,j , ei ? {0, 1} ?i, j
In our model, a summary can?t include the same
sentence, arc, or concept twice. Taking Table 2
for example, if s13 and s33 are included in a sum-
mary, the summary has two e8, but e8 is 1. This
constraint avoids summary redundancy.
The summary must meet the condition of maxi-
mum summary size. The following inequality rep-
resents the size constraint:
?
si?S
lisi ? K
li ? L indicates the length of sentence si. K is
the maximum size of the summary.
The following inequality represents the rela-
tionship between sentences and concepts in the
sentences.
?
i
mijsi ? ej ?j
The above constraint represents Table 2. mi,j is
an element of Table 2. If si is not included in the
summary, the concepts in si are not included.
Symbols indicating the beginning and end of
the summary must be part of the summary.
914
s0 = 1
sn+1 = 1
n is the number of sentences in the input docu-
ment set.
Next, we describe the constraints placed on
arcs.
The beginning symbol must be followed by a
sentence or a symbol and must not have any pre-
ceding sentences/symbols. The end symbol must
be preceded by a sentence or a symbol and must
not have any following sentences/symbols. The
following equations represent these constraints:
?
i
a0,i = 1
?
i
ai,0 = 0
?
i
an+1,i = 0
?
i
ai,n+1 = 1
Each sentence in the summary must be pre-
ceded and followed by a sentence/symbol.
?
i
ai,j +
?
i
aj,i = 2sj ?j
?
i
ai,j =
?
i
aj,i ?j
The above constraints fail to prevent cycles. To
rectify this, we set the following constraints.
?
i
f0,i = n
?
i
fi,0 ? 1
?
i
fi,j ?
?
i
fj,i = sj ?j
fi,j ? nai,j ?i, j
The above constraints indicate that flows f are
sent from s0 as a source to sn+1 as a sink. n unit
flows are sent from the source and each node ex-
pends one unit of flows. More than one flow has
to arrive at the sink. By setting these constraints,
the nodes consisting of a cycle have no flow. Thus
solutions that contain a cycle are prevented. These
constraints have also been used to avoid cycles in
headline generation (Deshpande et al, 2007).
4 Experiments
This section evaluates our method in terms of
ROUGE score and readability. We tested our
method and two baselines in two domains: re-
views of commodities and restaurants. We col-
lected 4,475 reviews of 100 commodities and
2,940 reviews of 100 restaurants from websites.
The commodities included items such as digital
cameras, printers, video games, and wines. The
average document size was 10,173 bytes in the
commodity domain and 5,343 bytes in the restau-
rant domain. We attempted to generate 300 byte
summaries, so the summarization rates were about
3% and 6%, respectively.
We prepared 4 references for each review, thus
there were 400 references in each domain. The au-
thors were not those who made up the references.
These references were used for ROUGE and read-
ability evaluation.
Since our method requires the parameter vec-
tor w for determining the coherence scores. We
trained the parameter vector for each domain.
Each parameter vector was trained using 10-fold
cross validation. We used 8 samples to train, 1
to develop, and 1 to test. In the restaurant do-
main, we added 4,390 reviews to each training set
to alleviate data sparseness. In the commodity do-
main, we add 47,570 reviews.2
As the solver, we used glpk.3 According to the
development set, ? in Eq.10 was set as 0.1.
4.1 Baselines
We compare our method to the references (which
also provide the upper bound) and the opinion
summarizers proposed by Carenini et al (2006)
and Lerman et al (2009) as the baselines.
In the ROUGE evaluations, Human indicates
ROUGE scores between references. To compare
our summarizer to human summarization, we cal-
culated ROUGE scores between each reference
and the other three references, and averaged them.
In the readability evaluations, we randomly se-
lected one reference for each commodity and each
restaurant and compared them to the results of the
three summarizers.
Carenini et al (2006)
Carenini et al (2006) proposed two opinion
2The commodities domain suffers from stronger review
variation than the restaurant domain so more training data
was needed.
3http://www.gnu.org/software/glpk/
915
summarizers. One uses a natural language genera-
tion module, and other is based on MEAD (Radev
et al, 2004). Since it is difficult to mimic the natu-
ral language generation module, we implemented
the latter one. The objective function Carenini et
al. (2006) proposed is as follows:
L1(S) =
?
a?S
?
s?D
|polaritys(a)| (11)
polaritys(a) indicates the polarity of aspect a
in sentence s present in source document set D.
That is, this function gives a high score to a sum-
mary that covers aspects frequently mentioned in
the input, and whose polarities tend to be either
positive or negative.
The solution is identified using the greedy
method. If there is more than one sentence that
has the same score, the sentence that has the
higher centroid score (Radev et al, 2004) is ex-
tracted.
Lerman et al (2009)
Lerman et al (2009) proposed three objective
functions for opinion summarization, and we im-
plemented one of them. The function is as fol-
lows:
L2(S) = ?(KL(pS(a), pD(a)) (12)
+
?
a?A
KL(N (x|?aS , ?2aS ),N (x|?aD , ?
2
aD)))
KL(p, q) means the Kullback-Leibler diver-
gence between probability distribution p and q.
pS(a) and pD(a) are probability distributions in-
dicating how often aspect a ? A occurs in sum-
mary S and source document set D respectively.
N (x|?, ?2) is a Gaussian distribution indicating
distribution of polarity of an aspect whose mean
is ? and variance is ?2. ?aS , ?aD and ?2aS , ?2aD
are the means and the variances of aspect a in
summary S and source document set D, respec-
tively. These parameters are determined using
maximum-likelihood estimation.
That is, the above objective function gives high
score to a summary whose distributions of aspects
and polarities mirror those of the source document
set.
To identify the optimal solution, Lerman et al
(2009) use a randomized algorithm. First, the
summarizer randomly extracts sentences from the
source document set, then iteratively performs in-
sert/delete/swap operations on the summary to in-
crease Eq.12 until summary improvement satu-
rates. While this method is prone to lock onto
Commodity R-2 R-SU4 R-SU9
(Carenini et al, 2006) 0.158 0.202 0.186
(Lerman et al, 2009) 0.205 0.247 0.227
Our Method 0.231 0.251 0.230
Human 0.384 0.392 0.358
Restaurant R-2 R-SU4 R-SU9
(Carenini et al, 2006) 0.251 0.281 0.258
(Lerman et al, 2009) 0.260 0.296 0.273
Our Method 0.285 0.303 0.273
Human 0.358 0.370 0.335
Table 3: Automatic ROUGE evaluation.
# of Sentences
(Carenini et al, 2006) 3.79
(Lerman et al, 2009) 6.28
Our Method 7.88
Human 5.83
Table 4: Average number of sentences in the sum-
mary.
local solutions, the summarizer can reach the op-
timal solution by changing the starting sentences
and repeating the process. In this experiment, we
used 100 randomly selected starting points.
4.2 ROUGE
We used ROUGE (Lin, 2004) for evaluating the
content of summaries. We chose ROUGE-2,
ROUGE-SU4 and ROUGE-SU9. We prepared
four reference summaries for each document set.
The results of these experiments are shown in
Table 3. ROUGE scores increase in the order of
(Carenini et al, 2006), (Lerman et al, 2009) and
our method, but no method could match the per-
formance of Human. Our method significantly
outperformed Lerman et al (2009)?s method over
ROUGE-2 according to the Wilcoxon signed-rank
test, while it shows no advantage over ROUGE-
SU4 and ROUGE-SU9.
Although our weighting of the set of sentences
is relatively naive compared to the weighting pro-
posed by Lerman et al (2009), our method out-
performs their method. There are two reasons
for this; one is that we adopt ILP for decoding,
so we can acquire preferable solutions efficiently.
While the score of Lerman et al (2009)?s method
may be improved by adopting ILP, it is difficult
to do so because their objective function is ex-
tremely complex. The other reason is the coher-
ence score. Since our coherence score is based on
916
Commodity (Carenini et al, 2006) (Lerman et al, 2009) Our Method Human
(Carenini et al, 2006) - 27/45 18/29 8/46
(Lerman et al, 2009) 18/45 - 29/48 11/47
Our Method 11/29 19/48 - 5/46
Human 38/46 36/47 41/46 -
Restaurant (Carenini et al, 2006) (Lerman et al, 2009) Our Method Human
(Carenini et al, 2006) - 31/45 17/31 8/48
(Lerman et al, 2009) 14/45 - 25/47 7/46
Our Method 14/31 22/47 - 8/50
Human 40/48 39/46 42/50 -
Table 5: Readability evaluation.
content words, it may impact the content of the
summary.
4.3 Readability
Readability was evaluated by human judges.
Since it is difficult to perform absolute evalua-
tion to judge the readability of summaries, we
performed a paired comparison test. The judges
were shown two summaries of the same input and
decided which was more readable. The judges
weren?t informed which method generated which
summary. We randomly chose 50 sets of reviews
from each domain, so there were 600 paired sum-
maries.4 However, as shown in Table 4, the aver-
age numbers of sentences in the summary differed
widely from the methods and this might affect the
readability evaluation. It was not fair to include
the pairs that were too different in terms of the
number of sentences. Therefore, we removed the
pairs that differed by more than five sentences.
In the experiment, 523 pairs were used, and 21
judges evaluated about 25 summaries each. We
drew on DUC 2007 quality questions5 for read-
ability assessment.
Table 5 shows the results of the experiment.
Each element in the table indicates the number
of times the corresponding method won against
other method. For example, in the commodity do-
main, the summaries that Lerman et al (2009)?s
method generated were compared with the sum-
maries that Carenini et al (2006)?s method gener-
ated 45 times, and Lerman et al (2009)?s method
won 18 times. The judges significantly preferred
the references in both domains. There were no
significant differences between our method and
the other two methods. In the restaurant do-
4
4C2 ? 100 = 600
5http://www-nlpir.nist.gov/projects/
duc/duc2007/quality-questions.txt
main, there was a significant difference between
(Carenini et al, 2006) and (Lerman et al, 2009).
Since we adopt ILP, our method tends to pack
shorter sentences into the summary. However,
our coherence score prevents this from degrading
summary readability.
5 Conclusion
This paper proposed a novel algorithm for opinion
summarization that takes account of content and
coherence, simultaneously. Our method directly
searches for the optimum sentence sequence by
extracting and ordering sentences present in the
input document set. We proposed a novel ILP
formulation against selection-and-ordering prob-
lems; it is a powerful mixture of the Maximum
Coverage Problem and the Traveling Salesman
Problem. Experiments revealed that the algo-
rithm creates summaries that have higher ROUGE
scores than existing opinion summarizers. We
also performed readability experiments. While
our summarizer tends to extract shorter sentences
to optimize summary content, our proposed co-
herence score prevented this from degrading the
readability of the summary.
One future work includes enriching the features
used to determine the coherence score. We expect
that features such as entity grid (Barzilay and La-
pata, 2005) will improve overall algorithm perfor-
mance. We also plan to apply our model to tasks
other than opinion summarization.
Acknowledgments
We would like to sincerely thank Tsutomu Hirao
for his comments and discussions. We would also
like to thank the anonymous reviewers for their
comments.
917
References
Althaus, Ernst, Nikiforos Karamanis and Alexander Koller.
2004. Computing Locally Coherent Discourses. In Proc.
of the 42nd Annual Meeting of the Association for Com-
putational Linguistics.
Balas, Egon. 1989. The prize collecting traveling salesman
problem. Networks, 19(6):621?636.
Banko, Michele, Vibhu O. Mittal and Michael J. Witbrock.
2000. Headline Generation Based on Statistical Transla-
tion. In Proc. of the 38th Annual Meeting of the Associa-
tion for Computational Linguistics.
Barzilay, Regina, Noemie Elhadad and Kathleen McKeown.
2002. Inferring Strategies for Sentence Ordering in Mul-
tidocument Summarization. Journal of Artificial Intelli-
gence Research, 17:35?55.
Barzilay, Regina and Mirella Lapata. 2005. Modeling Lo-
cal Coherence: An Entity-based Approach. In Proc. of
the 43rd Annual Meeting of the Association for Compu-
tational Linguistics.
Carbonell, Jaime and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering docu-
ments and producing summaries. In Proc. of the 21st An-
nual International ACM SIGIR Conference on Research
and Development in Information Retrieval.
Carenini, Giuseppe, Raymond Ng and Adam Pauls. 2006.
Multi-Document Summarization of Evaluative Text. In
Proc. of the 11th Conference of the European Chapter of
the Association for Computational Linguistics.
Clarke, James and Mirella Lapata. 2007. Modelling Com-
pression with Discourse Constraints. In Proc. of the 2007
Joint Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Language
Learning.
Collins, Michael. 2002. Discriminative Training Methods for
Hidden Markov Models: Theory and Experiments with
Perceptron Algorithms. In Proc. of the 2002 Conference
on Empirical Methods in Natural Language Processing.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning Re-
search, 7:551?585.
Deshpande, Pawan, Regina Barzilay and David R. Karger.
2007. Randomized Decoding for Selection-and-Ordering
Problems. In Proc. of Human Language Technologies
2007: The Conference of the North American Chapter of
the Association for Computational Linguistics.
Elsner, Micha, Joseph Austerweil and Eugene Charniak.
2007. A unified local and global model for discourse co-
herence. In Proc. of Human Language Technologies 2007:
The Conference of the North American Chapter of the As-
sociation for Computational Linguistics.
Filatova, Elena and Vasileios Hatzivassiloglou. 2004. A For-
mal Model for Information Selection in Multi-Sentence
Text Extraction. In Proc. of the 20th International Con-
ference on Computational Linguistics.
Gillick, Dan and Benoit Favre. 2009. A Scalable Global
Model for Summarization. In Proc. of Human Language
Technologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computational
Linguistics Workshop on Integer Linear Programming for
NLP.
Hirao, Tsutomu, Hideki Isozaki, Eisaku Maeda and Yuji
Matsumoto. 2002. Extracting important sentences with
support vector machines. In Proc. of the 19th Interna-
tional Conference on Computational Linguistics.
Kupiec, Julian, Jan Pedersen and Francine Chen. 1995. A
Trainable Document Summarizer. In Proc. of the 18th An-
nual International ACM SIGIR Conference on Research
and Development in Information Retrieval.
Lapata, Mirella. 2003. Probabilistic Text Structuring: Exper-
iments with Sentence Ordering. In Proc. of the 41st An-
nual Meeting of the Association for Computational Lin-
guistics.
Lapata, Mirella. 2006. Automatic Evaluation of Informa-
tion Ordering: Kendall?s Tau. Computational Linguistics,
32(4):471?484.
Lerman, Kevin, Sasha Blair-Goldensohn and Ryan McDon-
ald. 2009. Sentiment Summarization: Evaluating and
Learning User Preferences. In Proc. of the 12th Confer-
ence of the European Chapter of the Association for Com-
putational Linguistics.
Lin, Chin-Yew. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In Proc. of Text Summarization
Branches Out.
Martins, Andre F. T., and Noah A. Smith. 2009. Summariza-
tion with a Joint Model for Sentence Extraction and Com-
pression. In Proc. of Human Language Technologies: The
2009 Annual Conference of the North American Chapter
of the Association for Computational Linguistics Work-
shop on Integer Linear Programming for NLP.
McDonald, Ryan. 2007. A Study of Global Inference Algo-
rithms in Multi-document Summarization. In Proc. of the
29th European Conference on Information Retrieval.
Nishikawa, Hitoshi, Takaaki Hasegawa, Yoshihiro Matsuo
and Genichiro Kikui. 2010. Optimizing Informativeness
and Readability for Sentiment Summarization. In Proc. of
the 48th Annual Meeting of the Association for Computa-
tional Linguistics.
Radev, Dragomir R., Hongyan Jing, Magorzata Sty and
Daniel Tam. 2004. Centroid-based summarization of mul-
tiple documents. Information Processing and Manage-
ment, 40(6):919?938.
Soricut, Radu and Daniel Marcu. 2006. Discourse Genera-
tion Using Utility-Trained Coherence Models. In Proc. of
the 21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association for
Computational Linguistics.
Takamura, Hiroya and Manabu Okumura. 2009. Text Sum-
marization Model based on Maximum Coverage Problem
and its Variant. In Proc. of the 12th Conference of the Eu-
ropean Chapter of the Association for Computational Lin-
guistics.
Yih, Wen-tau, Joshua Goodman, Lucy Vanderwende and
Hisami Suzuki. 2007. Multi-Document Summarization by
Maximizing Informative Content-Words. In Proc. of the
20th International Joint Conference on Artificial Intelli-
gence.
918
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 928?939, Dublin, Ireland, August 23-29 2014.
Towards an open-domain conversational system fully based on natural
language processing
Ryuichiro Higashinaka
1
, Kenji Imamura
1
, Toyomi Meguro
2
, Chiaki Miyazaki
1
Nozomi Kobayashi
1
, Hiroaki Sugiyama
2
, Toru Hirano
1
Toshiro Makino
1
, Yoshihiro Matsuo
1
1
NTT Media Intelligence Laboratories
2
NTT Communication Science Laboratories
{higashinaka.ryuichiro, imamura.kenji, meguro.toyomi, miyazaki.chiaki,
kobayashi.nozomi, sugiyama.hiroaki, hirano.tohru,
makino.toshiro, matsuo.yoshihiro}@lab.ntt.co.jp
Abstract
This paper proposes an architecture for an open-domain conversational system and evaluates an
implemented system. The proposed architecture is fully composed of modules based on natu-
ral language processing techniques. Experimental results using human subjects show that our
architecture achieves significantly better naturalness than a retrieval-based baseline and that its
naturalness is close to that of a rule-based system using 149K hand-crafted rules.
1 Introduction
Although task-oriented dialogue systems have been extensively researched over the decades (Walker
et al., 2001; Williams et al., 2013), it is only recently that non-task-oriented dialogue, open-domain
conversation, or chat has been attracting attention for its social and entertainment aspects (Bickmore
and Picard, 2005; Ritter et al., 2011; Bessho et al., 2012). Creating an open-domain conversational
system is a challenging problem. In task-oriented dialogue systems, it is possible to prepare knowledge
for a domain and create understanding and generation modules for that domain (Nakano et al., 2000).
However, for open-domain conversation, such preparation cannot be performed. Since it is difficult to
handle users? open-domain utterances, to create workable systems, conventional approaches have used
hand-crafted rules (Wallace, 2004). Although elaborate rules may work well, the problem with the rule-
based approach is the high cost and the dependence on individual skills of developers, which hinders
systematic development. Another problem with the rule-based approach is its low coverage; that is, the
inability to handle unexpected utterances.
The recent increase of web data has propelled the development of approaches that use data retrieved
from the web for open-domain conversation (Shibata et al., 2009; Ritter et al., 2011). The merit of such
retrieval-based approaches is that, owing to the diversity of the web, systems can retrieve at least some
responses for user input, which solves the coverage problem. However, this comes at the cost of utterance
quality. Since the web, especially Twitter, is inherently noisy, it is, in many cases, difficult to sift out
appropriate sentences from retrieval results.
In this paper, we propose an architecture for an open-domain conversational system. The proposed
architecture is fully composed of modules based on natural language processing (NLP) techniques. Our
stance is not just to hand-craft or to search the web for utterances, but to create a system that can fully
understand and generate utterances. We want to show that it is possible to build an open-domain conver-
sational system by combining NLP modules, which will open the way to a systematic development and
improvement. We describe our open-domain conversational system based on our architecture and present
results of an evaluation of its performance by human subjects. We compare our system with rule-based
and retrieval-based systems, and show that our architecture is a promising direction. In this work, we
regard the term open-domain conversation to be interchangeable with non-task-oriented dialogue, casual
conversation (Eggins and Slade, 2005), chat, or social dialogue (Bickmore and Cassell, 2000). We use
the term to denote that user input is not restricted in any way as in open-domain question answering
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
928
	
			
	

	
		
	

Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1648?1659, Dublin, Ireland, August 23-29 2014.
Learning to Generate Coherent Summary
with Discriminative Hidden Semi-Markov Model
Hitoshi Nishikawa
1
, Kazuho Arita
1
, Katsumi Tanaka
1
,
Tsutomu Hirao
2
, Toshiro Makino
1
and Yoshihiro Matsuo
1
Nippon Telegraph and Telephone Corporation
1
1-1 Hikari-no-oka, Yokosuka-shi, Kanagawa, 239-0847 Japan
2
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
{
nishikawa.hitoshi, arita.kazuho, tanaka.katsumi
hirao.tsutomu, makino.toshiro, matsuo.yoshihiro
}
@lab.ntt.co.jp
Abstract
In this paper we introduce a novel single-document summarization method based on a hidden
semi-Markov model. This model can naturally model single-document summarization as the
optimization problem of selecting the best sequence from among the sentences in the input doc-
ument under the given objective function and knapsack constraint. This advantage makes it
possible for sentence selection to take the coherence of the summary into account. In addition
our model can also incorporate sentence compression into the summarization process. To demon-
strate the effectiveness of our method, we conduct an experimental evaluation with a large-scale
corpus consisting of 12,748 pairs of a document and its reference. The results show that our
method significantly outperforms the competitive baselines in terms of ROUGE evaluation, and
the linguistic quality of summaries is also improved. Our method successfully mimicked the
reference summaries, about 20 percent of the summaries generated by our method were com-
pletely identical to their references. Moreover, we show that large-scale training samples are
quite effective for training a summarizer.
1 Introduction
Single-document summarization is attracting much more attention as a key technology in providing
better information access in a commercial context. The Financial Times and CNN have been providing
summaries of articles in their websites to attract users, and Summly, which has been acquired by Yahoo!,
provided the service of automatically summarizing articles on the Internet. Given the cost of manual
summarization, we can greatly improve the information access of Internet users by creating an automatic
summarizer that can approach the summarization quality of humans.
To mimic manually-written summaries, one important aspect is coherence (Nenkova and McKeown,
2011). Although coherence has been studied widely in a field of multi-document summarization (Kara-
manis et al., 2004; Barzilay and Lapata, 2005; Nishikawa et al., 2010; Christensen et al., 2013), it has not
been studied enough in the context of single-document summarization. In this paper, we revisit the prob-
lem of coherence and employ it to produce both informative and linguistically high-quality summaries.
To obtain such summaries, we introduce a novel summarization method based on a hidden semi-
Markov model. The method has the properties of both the popular single-document summarization
model, the knapsack problem, which packs the sentences into the given length and the hidden Markov
model, which takes summary coherence into account by determining sentence context when selecting
sentences. By leveraging this, we can build a summarizer that naturally achieves coherence.
We state the novelty and contributions of this paper as follows:
? We regard single-document summarization as a combinatorial optimization problem modeled by a
hidden semi-Markov model and propose an efficient decoding algorithm for the problem.
? We introduce various features related to coherence in a combinatorial formulation. We extend a
hidden semi-Markov model to achieve discrimination, so our method can take advantage of many
features for predicting coherence.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1648
? We show that our large-scale corpus greatly improves the performance of summarization.
This paper is organized as follows. In Section 2, we describe related work. In Section 3, we detail
our proposed model. We also explain how the parameters in our model are optimized and how sentences
are compressed. In Section 4, we explain how variants of the original sentences are generated. In
Section 5, we explain the decoding algorithm for our method. In Section 6, we explain the settings of
our experiments, our corpus, and compared methods. In Section 7, we show results of the experiments
conducted to evaluate our method. In Section 8, we conclude this paper.
2 Related Work
2.1 Single-Document Summarization
Basically, single-document summarization can be done through sentence selection (Nenkova and McK-
eown, 2011) . The document to be summarized is decomposed into a set of sentences and then the
summarizer selects a subset of the sentences as a summary.
McDonald (2007) pointed out that single-document summarization can be formulated as a well-known
combinatorial optimization problem, the knapsack problem. Given a set of sentences together with their
lengths and values, the summarizer packs them into a summary so that the total value is as large as possi-
ble but the total length is less than or equal to a given maximum summary length. Interestingly, a hidden
semi-Markov model (Yu, 2010) can be regarded as a natural extension of the knapsack problem, we take
advantage of this property for single-document summarization. We elaborate the relation between the
knapsack problem and the hidden semi-Markov model in Section 3.
To generate coherent summaries in single-document summarization, there are two types of ap-
proaches
1
: tree-based approaches (Marcu, 1997; Daume and Marcu, 2002; Hirao et al., 2013) and
sequence-based approaches (Barzilay and Lee, 2004; Shen et al., 2007). The former rely on the tree
representation of a document based on the Rhetorical Structure Theory (RST) (Mann and Thompson,
1988). Basically, the former approaches (Marcu, 1997; Daume and Marcu, 2002; Hirao et al., 2013) trim
the tree representation of a document by making use of nucleus-satellite relations among sentences. The
advantage of RST-based approaches is that they can take advantage of global information about the doc-
uments. However, a drawback is that they depend heavily on the RST parser that is used. Performance
is remarkably sensitive to the accuracy of RST parsing, and hence we have to build a good RST parser.
Instead of making use of the global structure of the document, the sequence-based methods rely on and
take advantage of the local coherence of sentences. As one advantage over the tree-based approaches,
the sequence-based approaches do not require tools as RST parsers, and hence they are more robust. For
this reason, this paper focuses on sequence-based approaches.
The previous works most closely related to our method are those proposed by Barzilay and Lee (2004)
and Shen et al. (2007). Barzilay and Lee built a hidden Markov model to capture the content structure of
documents and used it to identify the important sentences. Shen et al. (2007) extended the HMM-based
approach to make it discriminative by making use of conditional random fields (Lafferty et al., 2001).
Conditional random fields can incorporate various features to identify the importance of a sentence and
they showed its effectiveness. A shortcoming of these approaches is that their model only classifies sen-
tences into two classes, it cannot take account of output length directly. This deficiency is problematic
because in practical usage the maximum length of a summary is specified by the user; hence, the sum-
marizer should be able to control output length. In contrast to their method, our approach naturally takes
the maximum summary length into account when summarizing a document.
2.2 Coherence
In the context of multi-document summarization, coherence has been studied widely. In multi-document
summarization, sentences are selected from different documents, and hence some way of ordering the
sentences is required. Sentence ordering (Barzilay et al., 2002; Althaus et al., 2004; Karamanis et al.,
1
As an interesting related work, Clarke and Lapata (2007) compresses documents by making use of Centering Theory
(Grosz et al., 1995). However, in their approach, the desired length of an output summary could not be specified and hence they
said their method was compression rather than summarization.
1649
Figure 1: An example of the hidden semi-Markov model. The system observes a sequence consisting
of 10 symbols o
1
...o
10
over time t
1
...t
10
and transitions between states s
1
...s
3
. Unlike the basic hidden
Markov model, states can persist for a non-unit length. In this figure, state s
2
and state s
3
persist for
non-unit lengths. Hence, the system traverses only 6 states despite observing 10 symbols.
2004; Okazaki et al., 2004) is a task to order extracted sentences and is closely related to coherence
(Lapata, 2003; Barzilay and Lapata, 2005; Nenkova et al., 2010; Pitler et al., 2010; Louis and Nenkova,
2012). Many effective features have been found out to capture coherence and we utilize these features.
Some work proposed a model that could jointly taking the content of the summary and its coherence
into account (Nishikawa et al., 2010; Christensen et al., 2013). Since extracted sentences in multi-
document summarization must be ordered, a task that is NP-hard, they relied on integer linear program-
ming (Nishikawa et al., 2010) or a local search strategy (Christensen et al., 2013). The former can locate
the optimal solution at a heavy computation cost, while the latter runs quickly but there is no guarantee
of locating the optimal solution. In contrast to their trade-off, our proposed algorithm, based on dynamic
programming, can locate the optimal solution quickly because the single-document summarization can
skip the ordering operation by reproducing the original order of the input sentences.
In this paper, we show that coherence also takes an important role in single-document summarization.
We model the coherence between adjacent sentences in the summary by leveraging the hidden semi-
Markov model, which can naturally capture the coherence between sentences.
3 Summarization with Hidden Semi-Markov Model
We first introduce the knapsack problem, which can naturally model single-document summarization.
Next, we explain the hidden semi-Markov model and show its relationship to the knapsack problem.
Then, we elaborate our summarization method.
3.1 Knapsack Problem
The knapsack problem is a type of combinatorial optimization problem (Korte and Vygen, 2008). Given
a set of elements, each of which has a score and size, the problem is formulated as the task of finding
the best subset in terms of maximizing the sum of their scores under the size limitation. As mentioned
above, single-document summarization can be regarded as an instance of the knapsack problem. The
best combination of input sentences can be found by calculating the value of each sentence and packing
them into a summary through the dynamic programming knapsack algorithm.
3.2 Hidden Semi-Markov Model
The hidden semi-Markov model (HSMM) is an extension of the hidden Markov model (HMM) (Yu,
2010). In the popular hidden Markov model, each state persists for only one unit length. For example,
if a system observes 10 discrete symbols, it outputs 10 hidden states. In the HSMM, each state can
persist for some unit lengths through the concept of duration. For example, if a system observes 10
discrete symbols and each state persists for two unit lengths, i.e., their duration is 2, the system outputs
5 hidden states. We show an example in Figure 1. The system observes a sequence consisting of 10
symbols o
1
...o
10
over time t
1
...t
10
and transitions between states s
1
...s
3
. Unlike the basic HMM, states
can persist for a non-unit length. In this figure, state s
2
and state s
3
persist for a non-unit length. Hence,
the system traverses 6 states even though it observes 10 symbols. This property has been utilized for
1650
sequential tagging, such as named entity recognition (Sarawagi and Cohen, 2004), scene text recognition
(Weinman et al., 2008) and phonetic recognition (Kim et al., 2011).
The hidden semi-Markov model is closely related to the knapsack problem. The length, K, of the
observed symbols can be regarded as a knapsack constraint. We can consider that the system tries to pack
the states of the model into the observed sequence of symbols by transitioning over the states under the
knapsack constraint so as to maximize the likelihood. Therefore, the hidden semi-Markov can naturally
be used for single-document summarization. Suppose that the document to be summarized consists of
10 sentences and the length of each of them is measured by the number of words. In this case, the system
transitions over 10 states corresponding to the 10 sentences until it cannot select any further sentence due
to the given length requirement. Since each state persists for the length of the corresponding sentence,
the remaining length decreases every time the system transitions to a new state.
While an HMM is basically a generative model, Collins (2002) extended it to create a discriminative
model. An HSMM can also be extended to become discriminative model (Sarawagi and Cohen, 2004).
Our discriminative HSMM learns through the application of max-margin training.
3.3 Formulation
We consider there are n input sentences s
1
, s
2
, ..., s
n
. These sentences have lengths ?
1
, ?
2
, ..., ?
n
and
weights w
1
, w
2
, ..., w
n
. We assume that a sentence that has a high weight should be present in the output
summary. We also consider each sentence, s
i
, has m
i
variants s
i,1
, s
i,2
, ..., s
i,m
, each produced by some
sort of sentence compression or paraphrase module. These variants also have lengths ?
i,1
, ?
i,2
, ..., ?
i,m
i
and weights w
i,1
, w
i,2
, ..., w
i,m
i
. For simplicity, we hereinafter note the original sentences s
1
, s
2
, ..., s
n
as s
1,0
, s
2,0
, ..., s
n,0
. Hence we have original sentence s
i,0
and variants s
i,1
, s
i,2
, ..., s
i,m
. Let s
0,0
and
s
n+1,0
be special symbols indicating the beginning of a document and the end of a document, respec-
tively. We define coherence c
g,h,i,j
as the coherence between sentence s
g,h
and sentence s
i,j
. An output
summary is described as a sequence of input sentences, g. LetG be the entire set of sequences that can be
constructed from the input sentences, i.e., g ? G. Finally, let K be the maximum length of the summary
desired. With these notations, our proposed method can be formulated as the following optimization
problem:
g
?
= argmax
g?G
?
s
i,j
?sent(g)
w
i,j
+
?
(s
g,h
,s
i,j
)?adj(g)
c
g,h,i,j
(1)
s.t.
?
s
i,j
?sent(g)
?
i,j
? K, (2)
where sent(g) and adj(g) indicate a set of sentences in g and a set of adjacent sentences in g, respec-
tively. That is, our model tries to find the best sequence of sentences under the knapsack constraint so as
to maximize the sum of weights and sentence coherence. In contrast to the common knapsack problem
which cannot take the variants and sentence coherence into account, our method, based on the hidden
semi-Markov model, does so naturally.
3.4 Parameter Optimization
Here we elaborate how parameters in the model are optimized to achieve the desired summaries. The
goal is to determine the value of w
i,j
for all i, j and c
g,h,i,j
for all g, h, i, j. We define w
i,j
and c
g,h,i,j
as
follows:
w
i,j
= w
w
? f
w
(s
i,j
) (3)
c
g,h,i,j
= w
c
? f
c
(s
g,h
, s
i,j
), (4)
where f
w
and f
c
are d
w
-dimensional and d
c
-dimensional feature vectors for sentences and sentence pairs,
respectively, andw
w
andw
c
are d
w
-dimensional and d
c
-dimensional parameter vectors for sentences and
sentence pairs, respectively. The goal of optimization is to determine the values of both vector w
w
and
1651
wc
, given feature function f
w
and f
c
. For simplicity, let s be a summary, let f = ?f
w
, f
c
? be a (d
w
+ d
c
)-
dimensional feature function for the whole summary and let w = ?w
w
,w
c
? be a (d
w
+ d
c
)-dimensional
weight vector. The value that the objective function outputs for summary s is w ? f(s).
To optimize the parameter, we employ the Passive-Aggressive algorithm (Crammer, 2006), a widely-
used structured learning method. Since the algorithm offers online learning, it can learn the parameter
quickly and is easy to implement. To learn the parameter so that the output summary is optimized to
the evaluation criteria popular in document summarization research, ROUGE (Lin, 2004), we introduce
ROUGE as the loss function. The parameter is estimated by solving the following formula iteratively
2
:
w
new
= argmin
w
1
2
||w ? w
old
||
2
(5)
s.t. w ? f(r) ? w ? f(s) ? loss(s; r),
where w
new
is the parameter vector after update, w
old
is the parameter vector before update, r is a
reference summary, and loss is the loss function. We define loss as 1 ? ROUGE(s; r). Among the
variants of ROUGE, we used ROUGE-1 for the loss function.
3.4.1 Sentence Feature
The features introduced in this section are used to calculate the weights of sentences, w
i,j
.
Term Frequency: Term frequency is a classic feature in document summarization (Luhn, 1958). We
calculate the total number of times each content word occurs in the document and then, for each sentence,
sum the totals of the content words that appear in the sentence as the value of this feature.
Word: We also use the words and parts-of-speech as features.
Named Entity: Named entities such as a name of person or organization are important. We use named
entities and classes as features.
Length: The length of a sentence may indicate the information value of its content. We use the length of
a sentence, measured by character number, as a feature.
Position: The position of a sentence is a classically important feature. We use the position of a sentence,
the relative position of a sentence, whether the sentence is the first in the document and whether the
sentence is the first in a paragraph, the position of the paragraph in which the sentence is, as features.
3.4.2 Coherence Feature
The features introduced in this section are used to calculate sentence coherence, c
g,h,i,h
.
Lexical Transition: Lapata (2003) showed that the structure of the document can be captured by word-
pairs consisting of words of two adjacent sentences. We use this feature for capturing the links between
two sentences
3
. We build a set of word pairs where one occurs in a precedent sentence and the other
occurs in a succeeding one, and use the elements of the set as a feature.
Lexical Cohesion: Pitler et al. (2010) showed that the similarity of two sentences is one of the strongest
features for predicting coherence. We reproduce this feature for generating coherent summaries. We
calculate cosine similarity between two sentences and use its value as a feature.
Entity Grid: Previous studies showed that Entity Grid (Barzilay and Lapata, 2005) is a strong feature
for predicting coherence (Pitler et al., 2010). We also employ this feature for summarization. While the
entity vector made from the entity grid was originally defined for whole documents, we build the entity
vector for each pair of two sentences because our model is based on the Markovian assumption, and
hence the coherence score is defined between two sentences.
2
As we explain later in Section 5, computation complexity of our algorithm is pseudo-polynomial, and hence the best
solution of our model can be located quickly. This is also advantageous in the learning phase because to learn parameters using
structured learning, the learner has to generate a summary to calculate the loss. Since our algorithm can quickly find the best
solution and generate a summary, it can also contribute to shortening the time required for learning.
3
It is expected that this feature will also contribute to sentence selection. Barzilay and Elhadad (1997) showed that a closely
related word-pair was a good indicator for sentence selection. This feature captures this property by learning.
1652
 0
 2000
 4000
 6000
 8000
 10000
 12000
 14000
 16000
 18000
 0  5  10  15  20  25  30
Th
e n
um
be
r o
f s
en
ten
ces
Levenshtein distance
Figure 2: Distribution of Levenshtein distance in the
aligned sentences. Among the 36,413 sentences in
the references, 16,643 were identical (Levenshtein
distance is 0) to the aligned sentences in the input
documents.
 0.6
 0.61
 0.62
 0.63
 0.64
 0.65
 0.66
 0.67
 0.68
 0.69
 0.7
 0  2000  4000  6000  8000  10000
RO
UG
E-
2
The number of training samples
Figure 3: Learning curve of HSMM.
4 Generating Sentence Variants
Since our model can take the variants of an original sentence in the input document as in the multi-
candidate reduction framework (Zajic et al., 2007), we incorporate sentence compression.
We generate a few variants of each original sentence by trimming the dependency tree of the sentence;
this simple operation is sufficient for reproducing reference summaries. By aligning sentences in a refer-
ence summary with those in the corresponding input document
4
, we found that human summaries were
quite conservative. Among the 36,413 sentences in the references, 16,643 were identical to the aligned
sentences in the input documents. Furthermore, most remaining sentences were virtually identical to the
original sentences; revisions were minor, and can be reproduced by simple operations. Few sentences
exhibited paraphrasing or more sophisticated operations. We plot the distribution of Levenshtein distance
in the aligned sentences in Figure 2. According to this observation, we produce the following types of
variants by sentence compression:
1. Removing information in parentheses. Some sentences contain parentheses containing additional
information for readers. The first type of variant deletes text in parentheses.
2. Shortening sentences by trimming their dependency trees. Basically this method follows the sen-
tence trimmer proposed by Nomoto (2008). While using his method, we keep the predicate and its
obligatory arguments in the sentences to keep the sentences grammatical. If a predicate is trimmed,
its obligatory arguments are also trimmed and vice versa. Since there are an exponential number
of subtrees in one tree, we use only n-best subtrees by ranking them according to n-gram language
likelihood and dependency-based language likelihood. We used the dependency parser proposed by
Imamura et al (Imamura et al., 2007) to acquire the dependency tree.
5 Decoding with Dynamic Programming
To solve Equation 1 under the constraints of Equation 2, we use dynamic programming. Algorithm
1 shows the pseudo code of the decoding algorithm. Line 1 to Line 7 initializes the variables used in
the algorithm. Vector x = ?x
0
, ..., x
n+1
? stores which sentence and which variants are included in the
output summary. If x
3
= 2, s
3,2
is included in the summary. V , P and S are two-dimensional arrays,
each of which is used as a dynamic programming table. They store the process of dynamic programming.
4
Alignment proceeds in two steps: first, we calculate the Levenshtein distance between sentences in the document and its
reference, and then we align sentences so as to minimize the distance between them.
1653
Algorithm 1 Decoding Algorithm: Filling Table
1: x = ?x
0
, ..., x
n+1
?
2: for i = 0 to n + 1 do
3: x
i
= ?1
4: V [0][i]? ?1
5: P [0][i]? ?1
6: S[0][i]? 0
7: V [0][0] = 0
8: for k = 1 to K do
9: for i = 1 to n do
10: V [k][i]? V [k ? 1][i]
11: P [k][i]? P [k ? 1][i]
12: S[k][i]? S[k ? 1][i]
13: for v = 0 to m
i
do
14: if ?
i,v
? k then
15: for h = 0 to i? 1 do
16: u = V [k ? ?
i,v
][h]
17: if u ?= ?1 ? S[k ? ?
i,v
][h] + w
i,v
+ c
h,u,i,v
? S[k][i] then
18: V [k][i]? v
19: P [k][i]? h
20: S[k][i]? S[k ? ?
i,v
][h] + w
i,v
+ c
h,u,i,v
21: V [K + 1][n + 1]? 0
22: P [K + 1][n + 1]? 0
23: S[K + 1][n + 1]? 0
24: for h = 1 to n do
25: u = V [K][h]
26: if S[K][h] + c
h,u,n+1,0
? S[K + 1][n + 1] then
27: P [K + 1][n + 1]? h
28: S[K + 1][n + 1]? S[K][h] + c
h,u,n+1,0
Document Reference
Avg. # of characters 476.2 142.0
Avg. # of words 298.6 88.3
Avg. # of sentences 9.7 2.9
Table 1: The statistics of our corpus.
V [k][i] stores which variants are used at time k, i. If V [k][i] = 0, original sentence s
i,0
is selected at
time k, i. If V [k][i] = ?1, no sentence is selected at time k, i. P [k][i] stores a pointer to the sentence
connected to the front of the current sentence. S[k][i] stores the value of the objective function at time
k, i. Line 8 to Line 36 locates the best sequence of sentences based on the following recurrence formula:
S[k][i] =
{
max
h=0...i?1,v=0...m
S[k ? ?
i,v
][h] + w
i,v
+ c
h,V [k??
i,v
][h],i,v
(A)
S[k ? 1][i] (B),
(6)
where case A is: ?
i,v
? k ? S[k ? 1][i] ? S[k ? ?
i,v
][h] + w
i,v
+ c
h,V [k??
i,v
][h],i,v
and case B is:
otherwise. This recurrence formula means that at time k, i the best variant to be selected as can be
determined at time k ? ?
i,v
, h. Hence, for all k ? 1...K and i ? 1...n, the algorithm finds the best
sequence of sentences at time k, i. After Algorithm 1 locates the best sequence of sentences by filling
the tables, the best sequence can be restored by backtracing along the pointers stored in P . Finally, the
algorithm outputs x, which stores which sentences and variants are used in the best sequence. Since
this algorithm is based on a dynamic programming knapsack algorithm (Korte and Vygen, 2008), it runs
in pseudo-polynomial time. This is a significant advantage over the methods that rely on integer linear
programming solvers due to their substantial computation cost.
6 Experiments
6.1 Data
We prepared 12,748 pairs of Japanese newspaper articles and their manually-written reference sum-
maries. This is one of the largest corpus available for single-document summarization research. The
length of all references is within 150 characters. All references in the corpus were written by a specialist
staff in a Japanese newspaper company and the company sold these summaries for commercial purposes.
1654
We list the statistics of our corpus in Table 1. As shown, the task is to summarize the document in about
a third of its original length in terms of the number of words.
6.2 Evaluation Criteria
ROUGE; ROUGE is an automatic evaluation method for automatic summarization proposed by Lin
(2004). We used ROUGE-1 and ROUGE-2 to evaluate the summaries. Since our document-reference
pairs are written in Japanese, we segmented the sentences into words using the Japanese morphological
analyzer developed by Fuchi and Takagi (1998). When calculating the ROUGE score, we used only
content words (i.e. nouns, verbs and adjectives) and so excluded function words as stop words.
Linguistic Quality: To evaluate the linguistic quality of the summaries generated by our method, we
performed a manual evaluation according to quality questions proposed by the National Institute of
Standards and Technology (NIST) (2007)
5
. We randomly sampled 100 summaries from the outputs of
each method described below and asked 7 subjects to evaluate the summaries according to the questions.
All subjects were Japanese native and none were among the authors. Since the quality questions by
NIST (2007) were designed for multi-document summarization, we used 3 of the 5 NIST questions for
single-document summarization: grammaticality, referential clarity, and structure/coherence. We also
asked the subjects to evaluate overall summary quality.
6.3 Compared Methods
We compared the following 8 methods.
Random: Random method selects sentences in the input document randomly.
Lead: Lead method is a classic baseline in single-document summarization. It only extracts the words
from the beginning of the document until the extracted words reach the given length. We simply extracted
150 characters from the beginning of each document.
Knapsack: The knapsack problem can be used as a single-document summarization model (McDonald,
2007). In this baseline, the weight of each sentence was calculated based on the average probabilities
of the words in the sentence (Nenkova and Vanderwende, 2005). Then, a summary was generated by
solving the knapsack problem.
Knapsack with Supervision: Instead of the average word probabilities used in the above baseline, we
used only sentence features f
w
to weigh a sentence.
Conditional Random Fields: Conditional random fields can be used to weigh sentences (Shen et al.,
2007). Since CRFs required binary labels in learning, we aligned sentences in an input document with
the sentences in its reference as explained in Section 4. We used the probabilities of sentences from
CRFs as the weights of the knapsack problem.
Hidden Semi-Markov Model: This is our proposed method without variants of the original sentences.
It selected sentences only from the set of original sentences.
Hidden Semi-Markov Model with Compression: This is our proposed method with variants of the
original sentences. It selected from among the variants and the original ones.
Human: In the linguistic quality evaluation, we added references to the summaries generated by the
above methods to show the upper bound.
When learning, we did 10-fold cross validation. In the experiments, statistical significance was
checked by Wilcoxon signed-rank test (Wilcoxon, 1945). To counteract the problem of multiple com-
parisons, we used the Holm-Bonferroni method (Holm, 1979) to adjust the significance level, ?.
7 Results and Discussion
We show the results of our experiment in Table 2 and Table 3. In this section, first we discuss the results
of the ROUGE evaluation, and then we discuss the results of the linguistic quality evaluation.
In the ROUGE evaluation, all the compared methods except for RANDOM showed good performance.
This is because, as shown in Section 4, many references consisted of sentences identical to the original
5
Some recent studies have tried to predict the readability of the text automatically (Pitler et al., 2010).
1655
Method R-1 R-2 Idt.
RANDOM 0.417 0.291 1.2%
LEAD 0.779
C,S,U,R
0.727
C,S,U,R
4.4%
KP 0.704
R
0.611
R
9.3%
KP(S) 0.729
U,R
0.647
U,R
10.4%
CRFs 0.741
U,R
0.675
S,U,R
11.3%
HSMM 0.769
C,S,U,R
0.703
C,S,U,R
15.2%
HSMM(C) 0.785
C,S,U,R
0.722
C,S,U,R
20.4%
Table 2: Results of the ROUGE evaluation.
?R-1? and ?R-2? correspond to ROUGE-1 and
ROUGE-2, respectively. The values in the col-
umn of ?Idt.? are the percentage of summaries
completely-identical to the corresponding refer-
ences. In the table,
C,S,U,L,R
indicate statisti-
cal significance against CRFs, KP(S), KP, LEAD,
RANDOM, respectively.
Method Gram. Ref. S./C. Overall
LEAD 1.9 3.9 2.5 2.1
KP 4.1
L
3.7 3.4 3.5
KP(S) 4.2
L
3.6 3.5 3.6
L
CRFs 4.1
L
3.9 3.7
L
3.6
L
HSMM 4.3
L
4.0 4.1
L
4.0
L
HSMM(C) 4.0
L
3.9 4.0
L
3.9
L
HUMAN 4.7
L
4.5 4.7
L
4.8
L
Table 3: Results of the linguistic quality evalua-
tion. The values ranged from 1 (very poor) to 5
(very good) (National Institute of Standards and
Technology, 2007). We show statistical signifi-
cance with the same notations as Table 2.
ones, and hence the references can be reproduced if important sentences are identified. Since the com-
pression rate in our corpus was relatively light, it made important information easy to identify. Among
the compared methods, both LEAD and our proposed method, HSMM(C), achieved the best result. There
was no significant difference between LEAD and HSMM(C). This surprising performance of LEAD was
due to the ROUGE evaluation. The words in the document leads were likely to be important, and LEAD
drew on this property. However, as we mentioned later, it sacrificed the linguistic quality to achieve the
high ROUGE score. Furthermore, it failed to yield summaries identical to the reference. In contrast to
LEAD, almost 20% of the summaries generated by HSMM(C) were identical to the references. This
shows that our method successfully mimicked human assessments. HSMM followed the best models.
There was a statistically significant difference between HSMM(C) and HSMM. Since some sentences,
especially the first sentence in the document, were long and the first sentence was particularly impor-
tant to summarize the document, sentence compression yielded a significant improvement. As shown
in Table 2, employing compression greatly improved the percentage of identical summaries. HSMM
significantly outperformed all of the baseline extractive methods except LEAD. While CRFs can take
advantage of all features used in HSMM, CRFs cannot take the evaluation measure such as ROUGE and
the knapsack constraint into account in learning. HSMM also significantly outperformed KP(S). This
difference is particularly important, and shows the usefulness of features related to coherence. While
KP(S) used only features about sentences, HSMM successfully mimicked the references as it drew on
the features related to coherence.
We show the learning curve of HSMM in Figure 3. We fixed 2,748 pairs for testing, and learned
parameters from 100, 250, 500, 1,000, 2,500, 5,000, 7,500 and 10,000 pairs. The curve in the figure
clearly shows the effectiveness of our large-scale corpus in learning. It seems that the curve does not
saturate and hence HSMM performance can be improved by more training samples. As in the results
recently shown by Filippova (2013), this result implies that large-scale data is important in the field
of document summarization as in other fields of computational linguistics. Past studies in document
summarization relied on relatively small datasets consisting of a few dozen or at most a few hundred
pairs of a document and its reference in learning. In contrast to the past studies, there are over 10,000
pairs in our dataset and the results show its effectiveness.
Second, we discuss the result of the linguistic quality evaluation. Unlike the ROUGE evaluation,
HSMM achieved the best result. As previous studies have pointed out (Nenkova and McKeown, 2011),
sentence compression commonly tends to degrade the linguistic quality of a summary while improving
its content. As shown in Table 3, the grammaticality of HSMM(C) is lower than that of HSMM, but the
1656
difference is not significant. Although we could not observe any significant difference between HSMM
and other extractive baselines, our proposals, HSMM and HSMM(C), yielded the best result in terms
of structure/coherence. By making use of the features related to coherence, we successfully improved
summary quality. In contrast to the surprising performance of LEAD in the ROUGE evaluation, in the
linguistic quality evaluation, LEAD yielded the worst performance. Since LEAD had to cut the sentences
when it reached the given length, it create ungrammatical fragments.
Finally, we touch on the balance between the quality of content and linguistic quality. Comparing
Table 2 to 3, we can see the correlation between the quality of content and linguistic quality. This re-
sult is reasonable because we can extract much more information from grammatical and well-organized
sentences. Although we optimized the parameter to maximize the ROUGE score, it also yielded im-
provements in linguistic quality. This is because the manually-generated reference summaries are ba-
sically grammatical and well-organized and the parameter is learnt to mimic them. However, there is
an inherent trade-off between the quality of content and linguistic quality. For example, under stricter
length limitations, instead of cohesive devices such as conjunctions, which can improve the coherence of
sentences, content words would be preferred for summary inclusion to augment information. Balancing
them to maximize reader satisfaction is an interesting problem.
8 Conclusions
In this paper we presented a novel single-document summarization method based on the hidden semi-
Markov model, which is a natural extension of the knapsack problem. Our model naturally takes account
of sentence context when identifying important sentences. This property is particularly important to
ensure the coherence of output summaries and to produce informative and linguistically high-quality
summaries. We also proposed an algorithm based on dynamic programming so the best solution can be
located quickly. Experiments on a very large-scale single-document summarization corpus showed that
our proposed method significantly outperforms competitive baselines.
As future work, we plan to tackle on the summarization task where higher compression is demanded.
To generate shorter summaries, we plan to employ more sophisticated approaches, such as paraphrasing.
Acknowledgement
The corpus used in this paper is owned by The Mainichi Newspapers Co., Ltd. and is leased to Nippon
Telegraph and Telephone Corporation. We sincerely appreciate their consideration. We also appreciate
the insightful comments from reviewers. Their comments greatly improved the quality of this paper.
References
Ernst Althaus, Nikiforos Karamanis, and Alexander Koller. 2004. Computing locally coherent discourses. In
Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL), pages 399?406.
Regina Barzilay and Michael Elhadad. 1997. Using lexical chains for text summarization. In Proceedings of the
Intelligent Scalable Text Summarization Workshop (ISTS), pages 10?17.
Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: an entity-based approach. In Proceedings
of the 43rd Annual Meeting on Association for Computational Linguistics (ACL), pages 141?148.
Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to
generation and summarization. In HLT-NAACL 2004: Main Proceedings, pages 113?120.
Regina Barzilay, Noemie Elhadad, and Kathleen R. McKeown. 2002. Inferring strategies for sentence ordering in
multidocument news summarization. Journal of Artificial Intelligence Research, 17:35?55.
Janara Christensen, Mausam, Stephen Soderland, and Oren Etzioni. 2013. Towards coherent multi-document
summarization. In Proceedings of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 1163?1173.
James Clarke and Mirella Lapata. 2007. Modelling compression with discourse constraints. In Proceedings of
the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 1?11.
1657
Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1?8.
Koby Crammer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research,
7(Mar):551?585.
Hal Daume, III and Daniel Marcu. 2002. A noisy-channel model for document compression. In Proceedings of
the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 449?456.
Katja Filippova. 2013. Overcoming the lack of parallel data in sentence compression. In Proceedings of the 2013
Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1481?1491.
Takeshi Fuchi and Shinichiro Takagi. 1998. Japanese morphological analyzer using word co-occurrence: Jtag.
In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th
International Conference on Computational Linguistics (ACL-COLING), pages 409?413.
Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi. 1995. Centering: a framework for modeling the local
coherence of discourse. Computational Linguistics, 21(2):203?225.
Tsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino, Norihito Yasuda, and Masaaki Nagata. 2013. Single-
document summarization as a tree knapsack problem. In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pages 1515?1520.
Sture Holm. 1979. A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics,
6(2):65?70.
Kenji Imamura, Genichiro Kikui, and Norihito Yasuda. 2007. Japanese dependency parsing using sequential label-
ing for semi-spoken language. In Proceedings of the 45th Annual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 225?228.
Nikiforos Karamanis, Massimo Poesio, Chris Mellish, and Jon Oberlander. 2004. Evaluating centering-based
metrics of coherence. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics
(ACL), pages 391?398.
Sungwoong Kim, Sungrack Yun, and Chang D. Yoo. 2011. Large margin discriminative semi-markov model
for phonetic recognition. IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING,
7(19):1999?2012.
Bernhard Korte and Jens Vygen. 2008. Combinatorial Optimization. Springer-Verlag, third edition.
John Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on
Machine Learning (ICML), pages 282?289.
Mirella Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings of the
41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 545?552.
Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Proceedings of ACL Workshop
Text Summarization Branches Out, pages 74?81.
Annie Louis and Ani Nenkova. 2012. A coherence model based on syntactic patterns. In Proceedings of the 2012
Conference on Empirical Methods on Natural Language Processing and Computational Natural Language
Learning (EMNLP-CoNLL).
Hans P. Luhn. 1958. The automatic creation of literature abstracts. IBM Journal of Research and Development,
22(2):159?165.
William C. Mann and Sandra A Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Daniel Marcu. 1997. From discourse structure to text summaries. In Proceedings of ACL/EACL 1997 Summariza-
tion Workshop, pages 82?88.
Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proceedings
of the 29th European Conference on Information Retrieval (ECIR), pages 557?564.
1658
National Institute of Standards and Technology. 2007. The linguistic quality questions. http://www-nlpir.
nist.gov/projects/duc/duc2007/quality-questions.txt.
Ani Nenkova and Kathleen McKeown. 2011. Automatic Summarization. Now Publishers.
Ani Nenkova and Lucy Vanderwende. 2005. The impact of frequency on summarization. Technical report,
MSR-TR-2005-101.
Ani Nenkova, Jieun Chae, Annie Louis, and Emily Pitler. 2010. Structural features for predicting the linguistic
quality of text: Applications to machine translation, automatic summarization and human-authored text. In
Emiel Krahmer and Theunem Mariet, editors, Empirical Methods in Natural Language Generation: Data-
oriented Methods and Empirical Evaluation, pages 222?241. Springer.
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Matsuo, and Genichiro Kikui. 2010. Opinion summarization
with integer linear programming formulation for sentence extraction and ordering. In Coling 2010: Posters,
pages 910?918.
Tadashi Nomoto. 2008. A generic sentence trimmer with crfs. In Proceedings of the 46th Annual Conference of
the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), pages 299?307.
Naoaki Okazaki, Yutaka Matsuo, and Mitsuru Ishizuka. 2004. Improving chronological sentence ordering by
precedence relation. In Proceedings of the 20th International Conference on Computational Linguistics (Col-
ing), pages 750?756.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010. Automatic evaluation of linguistic quality in multi-document
summarization. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics
(ACL), pages 544?554.
Sunita Sarawagi and William W. Cohen. 2004. Semi-markov conditional random fields for information extraction.
In Advances in Neural Information Processing Systems 17, pages 1185?1192.
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and Zheng Chen. 2007. Document summarization using conditional
random fields. In Proceedings of the 20th international joint conference on Artifical intelligence (IJCAI), pages
2862?2867.
Jerod J. Weinman, Erik Learned-Miller, and Allen Hanson. 2008. A discriminative semi-markov model for robust
scene text recognition. In Proceedings of the 19th International Conference on Pattern Recognition (ICPR),
pages 1?5.
Frank Wilcoxon. 1945. Individual comparisons by ranking methods. Biometrics Bulletin, 1(6):80?83.
Shun-Zheng Yu. 2010. Hidden semi-markov models. Artificial Intelligence, 174(2):215?243.
David M. Zajic, Bonnie J. Dorr, Jimmy Lin, and Schwartz Richard. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization tasks. Information Processing and Management, 43:1549?
1570.
1659
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1773?1782, Dublin, Ireland, August 23-29 2014.
Morphological Analysis for Japanese Noisy Text
Based on Character-level and Word-level Normalization
SAITO Itsumi, SADAMITSU Kugatsu, ASANO Hisako and MATSUO Yoshihiro
NTT Media Intelligence Laboratories
{saito.itsumi, sadamitsu.kugatsu,
asano.hisako, matsuo.yoshihiro}@lab.ntt.co.jp
Abstract
Social media texts are often written in a non-standard style and include many lexical variants
such as insertions, phonetic substitutions, abbreviations that mimic spoken language. The nor-
malization of such a variety of non-standard tokens is one promising solution for handling noisy
text. A normalization task is very difficult to conduct in Japanese morphological analysis because
there are no explicit boundaries between words. To address this issue, in this paper we propose a
novel method for normalizing and morphologically analyzing Japanese noisy text. We generate
both character-level and word-level normalization candidates and use discriminative methods to
formulate a cost function. Experimental results show that the proposed method achieves accept-
able levels in both accuracy and recall for word segmentation, POS tagging, and normalization.
These levels exceed those achieved with the conventional rule-based system.
1 Introduction
Social media texts attract a lot of attention in the fields of information extraction and text mining. Al-
though texts of this type contain a lot of information, such as one?s reputation or emotions, they often
contain non-standard tokens (lexical variants) that are considered out-of-Vocabulary (OOV) terms. We
define an OOV as a word that does not exist in the dictionary. Texts in micro-blogging services such
as Twitter are particularly apt to contain words written in a non-standard style, e.g., by lengthening
them (?goooood? for ?good?) or abbreviating them (?thinkin? ? for ?thinking?). This is also seen in the
Japanese language, which has standard word forms and variants of them that are often used in social
media texts. To take one word as an example, the standard form is???? (oishii, ?It is delicious?) and
its variants include ???????(oishiiiii), ??? (?oishii), and ????(oishii), where the un-
derlined characters are the differences from the standard form. Such non-standard tokens often degrade
the accuracy of existing language processing systems, which are trained using a clean corpus.
Almost all text normalization tasks for languages other than Japanese (e.g., English), aim to replace
the non-standard tokens that are explicitly segmented using the context-appropriate standard words (Han
et al. (2012), Han and Baldwin (2011), Hassan and Menezes (2013), Li and Liu (2012), Liu et al. (2012),
Liu et al. (2011), Pennell and Liu (2011), Cook and Stevenson (2009), Aw et al. (2006)). On the other
hand, the problem is more complicated in Japanese morphological analysis because Japanese words are
not segmented by explicit delimiters. In traditional Japanese morphological analysis, word segmentation
and part-of-speech (POS) tagging are simultaneously estimated. Therefore, we have to simultaneously
analyze normalization, word segmentation, and POS tagging to estimate the normalized form using the
context information. For example, the input ??????? ???(pan-keiki oishiiii, ?This pancake
tastes good?) written in the standard form is????????? (pan-keiki oishii). The result obtained
with the conventional Japanese morphological analyzer MeCab (Kudo (2005)) for this input is????
? (pancake, noun)/? ?? (unk)/? (unk)/? (unk)/, where slashes indicate the word segmentations and
?unk? means an unknown word. As this result shows, Japanese morphological analyzers often fail to
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1773
correctly estimate the word segmentation if there are unknown words, so the pipeline method (e.g., first
estimating the word segmentations and then estimating the normalization forms) is unsuitable.
Moreover, Japanese has several writing scripts, the main ones being Kanji, Hiragana, and Katakana.
Each word has its own formal written script (e.g., ??? (kyoukasyo, ?textbook?) as formally written
in Kanji), but in noisy text, there are many words that are intentionally written in a different script
(e.g., ?????? (kyoukasyo, ?textbook?) is the Hiragana form of???). These tokens written in
different script also degrade the performance of existing systems because dictionaries basically include
only the standard script. Unlike the character-level variation we described above, this type of variation
occurs on a word?level one. Therefore, there are both character-level and word-level non-standard
tokens in Japanese informal written text. Several normalization approaches have been applied to Japanese
text. Sasano et al. (2013) and Oka et al. (2011) introduced simple character level derivational rules for
Japanese morphological analysis that are used to normalize specific patterns of non-standard tokens, such
as for word lengthening and lower-case substitution. Although these approaches handle Japanese noisy
text fairly effectively, they can handle only limited kinds of non-standard tokens.
We propose a novel method of normalization in this study that can handle both character- and word-
level lexical variations in one model. Since it automatically extracts character-level transformation pat-
terns in character-level normalization, it can handle many types of character-level transformations. It
uses two steps (character- and word-level) to generate normalization candidates, and then formulates a
cost function of the word sequences as a discriminative model. The contributions this research makes
can be summarized by citing three points. First, the proposed system can analyze a wider variety of
non-standard token patterns than the conventional system by using our two-step normalization candidate
generation algorithms. Second, it can largely improve the accuracy of Japanese morphological analysis
for non-standard written text by simultaneously performing the normalization and morphological analy-
ses. Third, it can automatically extract character alignments and in so doing reduces the cost of manually
creating many types of transformation patterns. The rest of this paper is organized as follows. Section 2
describes the background to our research, including Japanese traditional morphological analysis, related
work, and data collection methods. Section 3 introduces the proposed approach, which includes lattice
generation and formulation, as a discriminative model. Section 4 discusses experiments we performed
and our analyses of the experimental results. Section 5 concludes the paper with a brief summary and a
mention of future work.
2 Background
2.1 Japanese Morphological Analysis
Many approaches to joint word segmentation and POS tagging including Japanese Morphological anal-
ysis can be interpreted as re-ranking while using a word lattice (Kaji and Kitsuregawa (2013)). There
are two points to consider in the analysis procedure: how to generate the word lattice and how to formu-
late the cost of each path. In Japanese morphological analysis, the dictionary-based approach has been
widely used to generate the word lattice (Kudo et al. (2004), Kurohashi et al. (1994)). In a traditional
approach, an optimal path is sought by using the sum of the two types of costs for the path: the cost
for a candidate word that reflects the word?s occurrence probability, and the cost for a pair of adjacent
POS that reflects the probability of an adjacent occurrence of the pair (Kudo et al. (2004), Kurohashi et
al. (1994)). A greater cost means less probability. The Viterbi algorithm is usually used for finding the
optimal path.
2.2 Related Work
Several studies have been conducted on Japanese morphological analysis in the normalized form. The
approach proposed by Sasano et al. (2013) aims to develop heuristics to flexibly search by using a simple,
manually created derivational rule. Their system generates normalized character sequence based on the
derivational rule, and adding new nodes that are generated from normalized character sequence when
generating the word lattice using dictionary lookup. Figure 1 presents an example of this approach.
If the non-standard written sentence ??????? (suugoku tanoshii, ?It is such fun?) is input, the
1774
Figure 1: Example of Japanese morphological analysis and normalization
type non-standard form standard form
(1) Insertion ??????? (arigatoou) ????? (arigatou, ?Thank you?)
(2) Deletion ?? (samu) ??? (samui, ?cold?)
(3) Substitution with phonetic variation ???? (kawaee) ???? (kawaii, ?cute?)
(4) Substitution with lowercases and uppercases ????? (arigatou) ????? (arigatou, ?Thank you?)
(5) Hiragana substitution ????? (aidei) ID (aidei, ?identification card?)
(6) Katakana substitution ????? (arigatou) ????? (arigatou, ?Thank you?)
(7) Any combination of (1) to (6) ????? (kaunta) ????? (kaunta, ?counter?)
???? (attsui) ??? (atsui, ?hot?)
Table 1: Types of non-standard tokens and examples of annotated data
traditional dictionary-based system generates Nodes that are described using solid lines, as shown in Fig.
1. Since ?????? (suugoku, ?such?) and ????? (tanoshii, ?fun?) are OOVs, the traditional system
cannot generate the correct word segments or POS tags. However, their system generates additional
nodes for the OOVs, shown as broken line rectangles in Fig. 1. In this case, derivational rules that
substitute ??? with ?null? and ??? (i) with ??? (i) are used and the system can generate the standard
forms ????? (sugoku, ?such?) and ????? (tanoshii, ?fun?) and their POS tags. If we can generate
sufficiently appropriate rules, these approaches seem to be effective. However, there are many types of
derivational patterns in SNS text and it is difficult to cover all of them by hand. Moreover, it becomes a
serious problem how to set the path cost for appropriately re-ranking the word lattice when the number
of candidates increases. Our approach is also based on the dictionary-based approach, however, our
approach is significantly dissimilar from their approach in two ways. First, we automatically generate
derivational patterns (we call them transformation tables) based on the character-level alignment between
non-standard tokens and their standard forms. Compared to generating the rules by hand, our approach
can generate broad coverage rules. Second, we use discriminative methods to formulate a cost function.
Jiang et al. (2008), Kaji and Kitsuregawa (2013) introduce several features to appropriately re-rank the
added nodes. This enables our system to perform well even when the number of candidates increases.
On the other hand, several studies have applied a statistical approach. For example, Sasaki et al.
(2013) proposed a character-level sequential labeling method for normalization. However, it handles
only one-to-one character transformations and does not take the word-level context into account. The
proposed method can handle many-to-many character transformations and takes word-level context into
account, so the scope for handling non-standard tokens is different. Many studies have been done on text
normalization for English; for example Han and Baldwin (2011) classifies whether or not OOVs are non-
standard tokens and estimates standard forms on the basis of contextual, string, and phonetic similarities.
In these studies it was assumed that clear word segmentations existed. However, since Japanese is an
unsegmented language the normalization problem needs to be treated as a joint normalization, word
segmentation, and POS tagging problem.
2.3 Data Collection and Analysis of Non-standard Tokens
In previous studies (Hassan and Menezes (2013), Ling et al. (2013), Liu et al. (2011)), the researchers
proposed unsupervised ways to extract non-standard tokens and their standard forms. For Japanese text,
however, it is very difficult to extract word pairs in an unsupervised way because there is no clear word
segmentation. To address this problem we first extracted non-standard tokens from Twitter text and blog
1775
Figure 2: Structure of proposed system
Figure 3: Example of candidate generation
text and manually annotated their standard (dictionary) forms. In total, we annotated 4808 tweets and
8023 blog text sentences. Table 1 lists the types of non-standard tokens that we targeted in this study
and examples of the annotated data. Types (1), (2), (3) and (4) are similar to English transform patterns.
Types (5) and (6) are distinctive patterns in Japanese. As previously mentioned Japanese has several
kinds of scripts, the main ones being Kanji, Hiragana, and Katakana. These scripts can be used to write
the same word in several ways. For example, the dictionary entry ?? (sensei, ?teacher?) can also
be written in Hiragana form ???? (sensei) or Katakana form ???? (sensei). Most words are
normally written in the standard form, but in informal written text (e.g., Twitter text), these same words
are often written in a non-standard form. In examining Twitter data for such non-standard tokens, we
found that 55.0% of them were types (1) to (3) in Table 1, 4.5% were type (4), 20.1% were types (5)
to (6), 2.7% were type (7), and the rest did not fall under any of these types since they were the result
of dialects, typos, and other factors. In other words, a large majority of the non-standard tokens fell
under types (1) to (7). We excluded those that did not as targets in this study because our proposed
method cannot easily handle them. Types (1) to (4) occur at character-level and so can be learned from
character-level alignment, but types (5) to (6) occur at word-level and it is inefficient to learn them on
a character?level basis. Accordingly, we considered generating candidates and features on two levels:
character-level and word-level.
3 Proposed Method
3.1 Overview of Proposed System
We showed the structure of the proposed system in Fig. 2. Our approach adds possible normalization
candidates to a word lattice and finds the best sequence using a Viterbi decoder based on a discriminative
model. We introduced several features that can be used to appropriately evaluate the confidence of the
added nodes as normalization candidates. We generate normalization candidates as indicated in Fig. 3.
1776
Figure 4: Example of character alignment
We describe the details in the following section.
3.2 Character-level Lattice
3.2.1 Character Alignment between Non-standard Tokens and Their Normalized Forms
We have to create a character-level transformation table to generate the character-level lattice. We used
the joint multigram model proposed by Sittichai et al. (2007) to create the transformation table because
this model can handle many-to-many character alignments between two character sequences. In ob-
serving non-standard tokens and their standard forms, we find there are not only one-to-one character
transformations but also many-to-many character transformations. Furthermore, unlike in translation,
there is no character reordering so the problems that arise are similar to those in transliteration. Accord-
ingly, we adopted a joint multigram model that is widely used for transliteration problems. The optimal
alignment can be formulated as q? = arg max
q?K
d
?
q?q
p(q) , where d is a pair of non-standard tokens
and its standard form (e.g., d is?????? (arigatoou), ????? (arigatou). Here, q is a partial
character alignment in d (e.g., q is ????, ???), q is the character alignment q set in d (e.g., q of
path 1 in Fig. 4 is {(??, ??), (??, ??), (??, ??), (????, ???)}. K
d
is the possible character
alignment sequence candidates generated from d. We generate n-best optimal path for K
d
in this study.
The maximum likelihood training can be performed using the EM algorithm derivated in Bisani and Ney
(2008) and Kubo et al. (2011) to estimate p(q). p(q) can be formulated as follow:
p(q) = ?
q
/
?
q?Q
?
q
(1)
?
q
=
?
d?D
?
q?K
d
p(q)n
q
(q) =
?
d?D
?
q?K
d
?
q?q
p?(q)
?
q?K
d
?
q?q
p?(q)
n
q
(q),
and where D is the number of the d pair, Q is the set of q, and n
q
(q) is the count of q that occurred in
q. In our system, we allow for standard form deletions (i.e., mapping of a non-standard character to a
null standard character) but not non-standard token deletions. Since we use this alignment as the trans-
formation table when generating a character-level lattice, the lattice size becomes unnecessarily large
if we allow for non-standard form deletions. In the calculation step of the EM algorithm, we calculate
the expectation (partial counts) ?
q
of each alignment in the E-step, calculate the joint probability p(q)
that maximizes the likelihood function in the M-step as described before, and repeat these steps until
convergence occurs. p?(q) indicates the result of p(q) calculated in the previous step over the iteration.
When generating the character-level lattice, we used alignments that were expected to exceed a prede-
fined threshold. We used ?
q
(q = (c
t
, c
v
)) and r(c
t
, c
v
) as thereshold, where c
t
and c
v
are the partial
character sequence of non-standard token and it?s standard form respectively. r(c
t
, c
v
) is calculated by
r(c
t
, c
v
) = ?
q
/n
c
v
., where n
c
v
is the number of occurrences of c
v
in the training data. We set the thresh-
old ?
q thres
= 0.5 , and r(c
t
, c
v
)
thres
= 0.0001 in this study. We also used r(c
t
, c
v
) as a feature of cost
1777
function in subsection. 3.4.2. When calculating initial value, we set p(c
t
, c
v
) high if the character c
t
and
c
v
are the same character and the length of each character is 1. We also give the limitation that a Kanji
character does not change to a different character and is aligned with same character in the calculation
step of the character alignment.
3.2.2 Generation of Character-level Lattice Based on Transformation Table
First, repetitions of more than one letter of ???, ???, ?-?, and ??? are reduced back to one letter (e.g.,
???????? (arigatooooou, ?Thank you?) is reduced to ?????? (arigatoou)) for the
input text. In addition, repetitions of more than three letters other than ???, ???, ?-?, and ??? are
reduced back to three letters (e.g.,???????? (uresiiiiiii, ?I?m happy?) is reduced back to??
???? (uresiiii)). These preprocessing rules are inspired by Han and Baldwin (2011) and determined
by taking the Japanese characteristics into consideration. We also used these rules when we estimated the
alignments of the non-standard tokens and their standard forms. Next, we generate the character-level
normalization candidates if they match the key transformation table in the input text. For example, if the
transformation table contains (q, logp(q))= (??? (yoo), ?? (you)?, -8.39), (?? (o), ? (o)?, -7.56),
and the input text includes the character sequence ???? ? (tyoo), we generate a new sequence ?????
(tyou) and ????? (tyoo). In other words, we add new nodes ???? (you) and ??? (o) in the position
of ??? ? (yoo) and ??? (o), respectively (see Fig. 3).
3.3 Generation of Word-level Lattice
We generate the word lattice based on the generated character-level lattice using dictionary lookup. We
exploit dictionary lookup by using the possible character sequence of the character-level lattice while
the traditional approach exploits it by using only the input character sequence. For example, we exploit
dictionary lookup for character sequences such as ???? ????? (tyoo kawaii) and ?????????
(tyou kawaii) and ????????? (chiyou kawaii) and ???? ????? (tyoo kawaii) (see Fig. 3)
Furthermore, we use the phonetic information of the dictionary to generate the normalization candi-
dates for Hiragana and Katakana substitution. For example, assume ??? (tyou, ?super?) and ??????
(kawaii, ?cute?) are the dictionary words. Then, if the input text contains the character sequences ???
?? (tyo) (which is written in Hiragana) and ?????? (kawaii) (which is written in Katakana), we add
??? (tyo, ?super?) and ?????? (kawaii, ?cute?) to the word lattice as the normalization candidates
since the two character sequences are pronounced identically. By using this two-step algorithm, we can
handle any combinational derivational patterns, such as Katakana substitutions or substitutions of lower-
cases like ?????? (kawaii)? ?????? (kawaii)? ?????? (kawaii, ?cute?) (see Fig. 3). Note
that we filtered candidates on the basis of a predefined threshold to prevent the generation of unneces-
sary candidates. The threshold was defined on the basis of the character sequence cost of normalization,
which is described in subsection 3.4.2. Furthermore, we limited the number of character transformations
to two per word.
3.4 Decoder
3.4.1 Objective Function
The decoder selects the optimal sequence y? from L(s) when given the candidate set L(s) for sentence
s. This is formulated as y? = arg min
y?L(s)
w ? f(y) (Jiang et al. (2008), Kaji and Kitsuregawa (2013)), where
y? is the optimal path, L(s) is the lattice created for sentence s, and w ? f(y) is the dot product between
weight vector w and feature vector f(y). The optimal path is selected according to the w ? f(y) value.
3.4.2 Features
The proposed lattice generation algorithm generates a lattice larger than that generated in traditional
dictionary-based lattice generation. Therefore, we need to introduce an appropriate normalization cost
into the objective function. We listed the features we used in Table 2. Let w
i
be the ith word candidate
and p
i
be the POS tag of w
i
. p
i?1
andw
i?1
are adjacent POS tag and word respectively. We also used the
word unigram cost f
w
i
p
i
, the cost for a pair of adjacent POS f
p
i?1
,p
i
that are quoted from MeCab (Kudo,
1778
Name Feature
Word unigram cost f
w
i
p
i
POS bi-gram cost f
p
i?1
,p
i
Word-POS bi-gram cost ?logp
w
i?1
p
i?1
,w
i
p
i
Character sequence cost log(p?
s
/p
?
t
i
)
where, p?
x
= p
1/length(x)
x
, p
x
=
?
n
j=1
p(c
j
|c
j?1
j?5
), x ? {s, t
i
}
Character transformation cost ?
trans
i
? (?logr(c
t
, c
v
))
Hiragana substitution cost ?
h
i
? f
w
i
p
i
Katakana substitution cost ?
k
i
? f
w
i
p
i
Table 2: Feature list of the decoder. ?
trans
i
is 1 if w
i
is generated by character transformation, otherwise
0. ?
h
i
is 1 ifw
i
is generated by Hiragana substitution, otherwise 0. ?
k
i
is 1 ifw
i
is generated by Katakana
substitution, otherwise 0.
2005), and five additional types of costs. These are the word-pos bi-gram cost ?logp
w
i?1
p
i?1
,w
i
p
i
of a
blog corpus; the character transformation cost ?
trans
i
?(?logr(c
t
, c
v
)), which is calculated in Section3.2,
for nodes generated by character transformation; the Hiragana substitution cost ?
h
i
? f
w
i
p
i
for nodes
generated by Hiragana substitution; the Katakana substitution cost ?
k
i
? f
w
i
p
i
for nodes generated by
Katakana substitution; and the character sequence cost log(p?
s
/p
?
t
i
) for all the normalized nodes. The
character sequence cost reflects the character sequence probability of the normalization candidates. Here,
s and t
i
are input string and transformed string respectively. (e.g., In Fig. 3, for the normalized node
?????? (cute, adjective), s is ???? ????? and t
i
is ???? ?????). Then p
s
and p
t
i
are
calculated by using the character 5-gram of a blog corpus, which is formulated by p
s
= p(c
1
? ? ? c
n
) =
?
n
j=1
p(c
j
|c
j?1
j?5
), where c
j
is the j th character of character sequence s. p?
t
i
and p?
s
are normalized by
using the length of each string s and t
i
as p?
t
i
= p
1/length(t
i
)
t
i
. We set the threshold (p?
s
/p
?
t
i
)
thres
= 1.5
for generating a Hiragana or Katakana normalization candidate in this study. Since all those features can
be factorized, the optimal path is searched for by using the Viterbi algorithm.
3.4.3 Training
We formulated the objective function for tuning weights w by using Eq. 2. The weights w are trained
by using the minimum error rate training (MERT) Machery et al. (2008). We defined the error function
as the differences between the reference word segmentations and the POS tags of the reference sequence
y
ref
and the system output arg min
y?L(s)
w ? f(y).
w? = arg min
w?W
N
?
i=1
error(y
ref
, arg min
y?L(s)
w ? f(y)) (2)
4 Experiments
4.1 Dataset and Estimated Transformation Table
We conducted experiments to confirm the effectiveness of the proposed method, in which we annotated
corpora of a Japanese blog and Twitter. The Twitter corpus was split into three parts: the training, devel-
opment, and test sets. The test data comprised 300 tweets, development data comprised 500 sentences
and the training data comprised 4208 tweets. We randomly selected the test data which contained at least
one non-standard token. The test data comprised 4635 words, 403 words of them are non-standard token
and are orthographically transformed into normalized form and POS tags. The blog corpus comprised
8023 sentences and all of them were used as training data. Training data was used for extracting char-
acter transformation table and development data was used for estimating parameters of discriminative
model. We used the IPA dictionary provided by MeCab to generate the word-level lattice and extracted
the dictionary-based features. We itemized the estimated character transformation patterns in Table 3.
There were 5228 transformation patterns that were learned from the training data and we used 3268 of
them, which meets the predefined condition. The learned patterns cover most of the previously pro-
1779
non-standard
character c
t
standard
character c
v
logp(q)
non-standard
character c
t
standard
character c
v
logp(q)
? null -4.233 ?? (ssu) ?? (desu) -5.999
??(maa) ?? (maa) -5.059 ?? (doo) ?? (dou) -6.210
??(syo) ??? (syou) -5.211 ?? (nee) ?? (nai) -6.232
?? (daro) ??? (darou) -5.570 ??(rya) ?? (reha) -6.492
?(ttsu) null -5.648 ?? (ten) ?? (teru) -6.633
?? (nto) ??? (ntou) -5.769 ?? (yuu) ?? (iu) -6.660
?(wa) ? (wa) -5.924 ?? (nan) ?? (nano) -6.706
Table 3: Example of character-level transformation table
posed rules. In addition, our method can learn more of the variational patterns that are difficult to create
manually.
4.2 Baseline and Evaluation Metrics
We compared the five methods listed in Table 4 in our experiments. Traditional means that which gen-
erates no normalization candidates and only uses the word cost and the cost for a pair of adjacent POS,
so we can consider it as a traditional Japanese morphological analysis. We compared three baselines,
Baseline1, Baseline2 and Baseline3. Baseline1 is the conventional rule-based method (considering in-
sertion of long sound symbols and lowercases, and substitution with long sound symbols and lower-
cases), which was proposed by Sasano et al. (2013). In Baseline2, 3, and Proposed, we basically use
the proposed discriminative model and features, but there are several differences. Baseline2 only gen-
erates character-level normalization candidates. Baseline3 uses our two-step normalization candidate
generation algorithms, but the character transformation cost of all the normalization candidates that are
generated by character normalization is the same. Proposed generates the character-level and Hiragana
and Katakana normalization candidates and use all features we proposed.
We evaluated each method on the basis of precision and recall and the F-value for the overall system
accuracy. Since Japanese morphological analysis simultaneously estimates the word segmentation and
POS tagging, we have to check whether or not our system is negatively affected by anything other than the
non-standard tokens. We also evaluated the recall with considering only normalized words. That value
directly reflects the performance of our normalization method. We registered emoticons that occurred in
the test data in the dictionary so that they would not negatively affect the systems? performance.
4.3 Results and Discussion
The results are classified in Table 4. As the table shows, the proposed methods performed statistically
significantly better than the baselines and the traditional method in both precision and recall (p < 0.01),
where the precision was greatly improved. This indicates that our method can not only correctly analyze
the non-standard tokens, but can also reduce the number of wrong words generated. Baseline1 also
improved the accuracy and recall compared to the traditional method, but the effect was limited. When
we compare Proposed with Baseline2, we find the F-value is improved when we take the Hiragana
and Katakana substitution into consideration. Baseline3 also improved the F-value but its performance is
inferior to proposed method.This proves that even if we can generate sufficient normalization candidates,
the results worsen if the weight parameter of each normalization candidate is not appropriately tuned. The
column of ?recall?? in Table 4 specifies the improvement rates of the non-standard tokens. The proposed
methods improve about seven times when using Baseline1 while preventing degradation. These results
prove that we have to generate appropriate and sufficient normalization candidates and appropriately tune
the cost of each candidate to improve both the precision and recall.
We show examples of the system output in Table 5. In the table, slashes indicate the position of the
estimated word segmentations and the words that were correctly analyzed are written in bold font. Exam-
ples (1) to (5) are examples improved by using the proposed method. Examples (6) to (7) are examples
that were not improved and example (8) is an example that was degraded. Examples (1) to (3) include
phonetic variations and example (4) is a Hiragana substitution. Example (5) is a combinational trans-
1780
word segmentation word segmentation and POS tag
method precision recall F-value precision recall F-value recall?
Traditional 0.716 0.826 0.767 0.683 0.788 0.732 -
Rule based (BL1??) 0.753 0.833 0.791 0.717 0.794 0.754 0.092
Proposed 0.856 0.883 0.869 0.822 0.849 0.835 0.667
- without Hiragana and Katakana normalization (BL2) 0.834 0.875 0.854 0.798 0.838 0.818 0.509
- character transformation cost is fixed (BL3) 0.838 0.865 0.851 0.807 0.834 0.821 0.533
? considering only normalized words, ?? BL:baseline
Table 4: Results of precision and recall of test data
input traditional proposed gold standard
(1)???(adii) ? (a)/? (di)/? ??? (atsui) ??? (atsui, ?hot?)
(2)???(sugee) ?? (suge)/? ??? (sugoi) ??? (sugoi, ?great?)
(3)????? (gommeen) ? (go)/?/? (me)/?/? (n)/ ??? (gomen) ??? (gomen, ?I?m sorry?)
(4)????(hitsuyou) ?? (hitsu)/?? (you) ?? (hitsuyou) ?? (hitsuyou, ?necessary?)
(5)?????(daichuki) ? (da)/?? (ichi)/?(yu)/? (ki)/ ??? (daisuki) ??? (daisuki, ?like very much?)
(6)?????(oseee) ?? (ose)/?? (ee)/? (e) ?? (ose) ??? (osoi, ?slow?)
(7)????? (kanwaii) ?? (kan)/? (wa)/?? (ii) ?? (kanwa)/?? (ii) ???? (kawaii, ?cute?)
(8)??? (inai) ? (i)/?? (nai) ?? (inai) ?/?? (i/nai, ?absent?)
Table 5: System output examples
formation pattern of a phonetic variation and Hiragana substitution. We can see our system can analyze
such variational non-standard tokens for all these examples. Two types of errors were identified. The first
occurred as the result of a lack of a character transformation pattern and the second was search errors.
Example (6) shows an example of a case in which our system couldn?t generate correct normalization
candidate because there was not corresponding character transformation pattern, even though there was
a similar phonetic transformation pattern. To ensure there will be no lack of transformation patterns,
we should either increase the parallel corpus size to enable the learning of more patterns or derive new
transformation patterns from the learned patterns. Example (7) shows an example of a case in which a
normalized candidate was generated but a search failed to locate it. Example (8) shows an example of a
case in which the result was degraded. Our system can control the degradation well, but there are several
degradation caused by normalization. We will need to develop a more complicated model or introduce
other features into the current model to reduce the number of search errors.
5 Conclusion and Future Work
We introduced a text normalization approach into joint Japanese morphological analysis and showed that
our two-step lattice generation algorithm and formulation using discriminative methods outperforms the
previous method. In future work, we plan to extend this approach by introducing an unsupervised or
semi-supervised parallel corpus extraction for learning character alignments to generate more patterns
at a reduced cost. We also plan to improve our model?s structure and features and implement it with a
decoding method to reduce the number of search errors. In addition, we should consider adding other
types of unknown words (such as named entities) to the morphological analysis system to improve its
overall performance.
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A phrase-based statistical model for sms text normalization.
Proceedings of the COLING/ACL on Main Conference Poster Sessions, pages 33?40.
Maximilian Bisani and Hermann Ney. 2008. Joint-sequence models for grapheme-to-phoneme conversion.
Speech Commun., 50(5):434?451, May.
Paul Cook and Suzanne Stevenson. 2009. An unsupervised model for text message normalization. Proceedings
of the Workshop on Computational Approaches to Linguistic Creativity, pages 71?78.
1781
Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a #twitter. Pro-
ceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Tech-
nologies - Volume 1, pages 368?378.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Automatically constructing a normalisation dictionary for
microblogs. Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing
and Computational Natural Language Learning, pages 421?432.
Hany Hassan and Arul Menezes. 2013. Social text normalization using contextual graph random walks. Proceed-
ings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages 1577?1586, August.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008. Word lattice reranking for chinese word segmentation and part-of-
speech tagging. Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1,
pages 385?392.
Nobuhiro Kaji and Masaru Kitsuregawa. 2013. Efficient word lattice generation for joint word segmentation
and pos tagging in japanese. Proceedings of the Sixth International Joint Conference on Natural Language
Processing, pages 153?161.
Keigo Kubo, Hiromichi Kawanami, Hiroshi Saruwatari, and Kiyohiro Shikano. 2011. Unconstrained many-to-
many alignment for automatic pronunciation annotation. In Proc. of APSIPA ASC.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to japanese
morphological analysis. In Proc. of EMNLP, pages 230?237.
T. Kudo. 2005. Mecab : Yet another part-of-speech and morphological analyzer. http://mecab.sourceforge.net/.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto, and Makoto Nagao. 1994. Improvements of japanese
morphological analyzer juman. In Proc. of The International Workshop on Sharable Natural Language Re-
sources, page 22?38.
Chen Li and Yang Liu. 2012. Improving text normalization using character-blocks based models and system
combination. Proceedings of COLING 2012, pages 1587?1602.
Wang Ling, Chris Dyer, Alan W Black, and Isabel Trancoso. 2013. Paraphrasing 4 microblog normalization.
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 73?84,
October.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu. 2011. Insertion, deletion, or substitution? normaliz-
ing text messages without pre-categorization nor supervision. Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Language Technologies, pages 71?76, June.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broad-coverage normalization system for social media language.
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 1035?1044.
W Machery, F J Och, and I Uszkoreit J Thayer. 2008. Lattice-based minimum error rate training for statistical
machine translation. In Proc. of EMNLP, 1:725?734.
Teruaki Oka, Mamoru Komachi, Toshinobu Ogiso, and Yuji Matsumoto. 2011. Handling orthographic variations
in morphological analysis for near-modern japanese (in japanese). In Proc. of The 27th Annual Conference of
the Japanese Society for Articial Intelligence.
Deana Pennell and Yang Liu. 2011. A character-level machine translation approach for normalization of sms
abbreviations. Proceedings of 5th International Joint Conference on Natural Language Processing, pages 974?
982, November.
Akira Sasaki, Junta Mizuno, Naoaki Okazaki, and Kentaro Inui. 2013. Normalization of text in microblogging
based on machine learning(in japanese) (in japanese). In Proc. of The 27th Annual Conference of the Japanese
Society for Articial Intelligence.
Ryohei Sasano, Sadao Kurohashi, and Manabu Okumura. 2013. A simple approach to unknown word process-
ing in japanese morphological analysis. Proceedings of the Sixth International Joint Conference on Natural
Language Processing, pages 162?170.
Jiampojamarn Sittichai, Kondrak Grzegorz, and Sherif Tarek. 2007. Applying many-to-many alignments and
hidden markov models to letter-to-phoneme conversion. In Proc. of The Conference of the North American
Chapter of the Association for Computational Linguistics, pages 372?379.
1782
Proceedings of the ACL 2010 Conference Short Papers, pages 325?330,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Optimizing Informativeness and Readability
for Sentiment Summarization
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Matsuo and Genichiro Kikui
NTT Cyber Space Laboratories, NTT Corporation
1-1 Hikari-no-oka, Yokosuka, Kanagawa, 239-0847 Japan
{
nishikawa.hitoshi, hasegawa.takaaki
matsuo.yoshihiro, kikui.genichiro
}
@lab.ntt.co.jp
Abstract
We propose a novel algorithm for senti-
ment summarization that takes account of
informativeness and readability, simulta-
neously. Our algorithm generates a sum-
mary by selecting and ordering sentences
taken from multiple review texts according
to two scores that represent the informa-
tiveness and readability of the sentence or-
der. The informativeness score is defined
by the number of sentiment expressions
and the readability score is learned from
the target corpus. We evaluate our method
by summarizing reviews on restaurants.
Our method outperforms an existing al-
gorithm as indicated by its ROUGE score
and human readability experiments.
1 Introduction
The Web holds a massive number of reviews de-
scribing the sentiments of customers about prod-
ucts and services. These reviews can help the user
reach purchasing decisions and guide companies?
business activities such as product improvements.
It is, however, almost impossible to read all re-
views given their sheer number.
These reviews are best utilized by the devel-
opment of automatic text summarization, partic-
ularly sentiment summarization. It enables us to
efficiently grasp the key bits of information. Senti-
ment summarizers are divided into two categories
in terms of output style. One outputs lists of
sentences (Hu and Liu, 2004; Blair-Goldensohn
et al, 2008; Titov and McDonald, 2008), the
other outputs texts consisting of ordered sentences
(Carenini et al, 2006; Carenini and Cheung, 2008;
Lerman et al, 2009; Lerman and McDonald,
2009). Our work lies in the latter category, and
a typical summary is shown in Figure 1. Although
visual representations such as bar or rader charts
This restaurant offers customers delicious foods and a
relaxing atmosphere. The staff are very friendly but the
price is a little high.
Figure 1: A typical summary.
are helpful, such representations necessitate some
simplifications of information to presentation. In
contrast, text can present complex information that
can?t readily be visualized, so in this paper we fo-
cus on producing textual summaries.
One crucial weakness of existing text-oriented
summarizers is the poor readability of their results.
Good readability is essential because readability
strongly affects text comprehension (Barzilay et
al., 2002).
To achieve readable summaries, the extracted
sentences must be appropriately ordered (Barzilay
et al, 2002; Lapata, 2003; Barzilay and Lee, 2004;
Barzilay and Lapata, 2005). Barzilay et al (2002)
proposed an algorithm for ordering sentences ac-
cording to the dates of the publications from which
the sentences were extracted. Lapata (2003) pro-
posed an algorithm that computes the probability
of two sentences being adjacent for ordering sen-
tences. Both methods delink sentence extraction
from sentence ordering, so a sentence can be ex-
tracted that cannot be ordered naturally with the
other extracted sentences.
To solve this problem, we propose an algorithm
that chooses sentences and orders them simulta-
neously in such a way that the ordered sentences
maximize the scores of informativeness and read-
ability. Our algorithm efficiently searches for the
best sequence of sentences by using dynamic pro-
gramming and beam search. We verify that our
method generates summaries that are significantly
better than the baseline results in terms of ROUGE
score (Lin, 2004) and subjective readability mea-
sures. As far as we know, this is the first work to
325
simultaneously achieve both informativeness and
readability in the area of multi-document summa-
rization.
This paper is organized as follows: Section 2
describes our summarization method. Section 3
reports our evaluation experiments. We conclude
this paper in Section 4.
2 Optimizing Sentence Sequence
Formally, we define a summary S? =
?s0, s1, . . . , sn, sn+1? as a sequence consist-
ing of n sentences where s0 and sn+1 are symbols
indicating the beginning and ending of the se-
quence, respectively. Summary S? is also defined
as follows:
S? = argmax
S?T
[Info(S) + ?Read(S)] (1)
s.t. length(S) ? K
where Info(S) indicates the informativeness
score of S, Read(S) indicates the readability
score of S, T indicates possible sequences com-
posed of sentences in the target documents, ?
is a weight parameter balancing informativeness
against readability, length(S) is the length of S,
and K is the maximum size of the summary.
We introduce the informativeness score and the
readability score, then describe how to optimize a
sequence.
2.1 Informativeness Score
Since we attempt to summarize reviews, we as-
sume that a good summary must involve as many
sentiments as possible. Therefore, we define the
informativeness score as follows:
Info(S) =
?
e?E(S)
f(e) (2)
where e indicates sentiment e = ?a, p? as the tu-
ple of aspect a and polarity p = {?1, 0, 1}, E(S)
is the set of sentiments contained S, and f(e) is the
score of sentiment e. Aspect a represents a stand-
point for evaluating products and services. With
regard to restaurants, aspects include food, atmo-
sphere and staff. Polarity represents whether the
sentiment is positive or negative. In this paper, we
define p = ?1 as negative, p = 0 as neutral and
p = 1 as positive sentiment.
Notice that Equation 2 defines the informative-
ness score of a summary as the sum of the score
of the sentiments contained in S. To avoid du-
plicative sentences, each sentiment is counted only
once for scoring. In addition, the aspects are clus-
tered and similar aspects (e.g. air, ambience) are
treated as the same aspect (e.g. atmosphere). In
this paper we define f(e) as the frequency of e in
the target documents.
Sentiments are extracted using a sentiment lex-
icon and pattern matched from dependency trees
of sentences. The sentiment lexicon1 consists of
pairs of sentiment expressions and their polarities,
for example, delicious, friendly and good are pos-
itive sentiment expressions, bad and expensive are
negative sentiment expressions.
To extract sentiments from given sentences,
first, we identify sentiment expressions among
words consisting of parsed sentences. For ex-
ample, in the case of the sentence ?This restau-
rant offers customers delicious foods and a relax-
ing atmosphere.? in Figure 1, delicious and re-
laxing are identified as sentiment expressions. If
the sentiment expressions are identified, the ex-
pressions and its aspects are extracted as aspect-
sentiment expression pairs from dependency tree
using some rules. In the case of the example sen-
tence, foods and delicious, atmosphere and relax-
ing are extracted as aspect-sentiment expression
pairs. Finally extracted sentiment expressions are
converted to polarities, we acquire the set of sen-
timents from sentences, for example, ? foods, 1?
and ? atmosphere, 1?.
Note that since our method relies on only senti-
ment lexicon, extractable aspects are unlimited.
2.2 Readability Score
Readability consists of various elements such as
conciseness, coherence, and grammar. Since it
is difficult to model all of them, we approximate
readability as the natural order of sentences.
To order sentences, Barzilay et al (2002)
used the publication dates of documents to catch
temporally-ordered events, but this approach is not
really suitable for our goal because reviews focus
on entities rather than events. Lapata (2003) em-
ployed the probability of two sentences being ad-
jacent as determined from a corpus. If the cor-
pus consists of reviews, it is expected that this ap-
proach would be effective for sentiment summa-
rization. Therefore, we adopt and improve Lap-
ata?s approach to order sentences. We define the
1Since we aim to summarize Japanese reviews, we utilize
Japanese sentiment lexicon (Asano et al, 2008). However,
our method is, except for sentiment extraction, language in-
dependent.
326
readability score as follows:
Read(S) =
n
?
i=0
w>?(si, si+1) (3)
where, given two adjacent sentences si and
si+1, w>?(si, si+1), which measures the connec-
tivity of the two sentences, is the inner product of
w and ?(si, si+1), w is a parameter vector and
?(si, si+1) is a feature vector of the two sentences.
That is, the readability score of sentence sequence
S is the sum of the connectivity of all adjacent sen-
tences in the sequence.
As the features, Lapata (2003) proposed the
Cartesian product of content words in adjacent
sentences. To this, we add named entity tags (e.g.
LOC, ORG) and connectives. We observe that the
first sentence of a review of a restaurant frequently
contains named entities indicating location. We
aim to reproduce this characteristic in the order-
ing.
We also define feature vector ?(S) of the entire
sequence S = ?s0, s1, . . . , sn, sn+1? as follows:
?(S) =
n
?
i=0
?(si, si+1) (4)
Therefore, the score of sequence S is w>?(S).
Given a training set, if a trained parameter w as-
signs a score w>?(S+) to an correct order S+
that is higher than a score w>?(S?) to an incor-
rect order S?, it is expected that the trained pa-
rameter will give higher score to naturally ordered
sentences than to unnaturally ordered sentences.
We use Averaged Perceptron (Collins, 2002) to
find w. Averaged Perceptron requires an argmax
operation for parameter estimation. Since we at-
tempt to order a set of sentences, the operation is
regarded as solving the Traveling Salesman Prob-
lem; that is, we locate the path that offers maxi-
mum score through all n sentences as s0 and sn+1
are starting and ending points, respectively. Thus
the operation is NP-hard and it is difficult to find
the global optimal solution. To alleviate this, we
find an approximate solution by adopting the dy-
namic programming technique of the Held and
Karp Algorithm (Held and Karp, 1962) and beam
search.
We show the search procedure in Figure 2. S
indicates intended sentences and M is a distance
matrix of the readability scores of adjacent sen-
tence pairs. Hi(C, j) indicates the score of the
hypothesis that has covered the set of i sentences
C and has the sentence j at the end of the path,
Sentences: S = {s1, . . . , sn}
Distance matrix: M = [ai,j ]i=0...n+1,j=0...n+1
1: H0({s0}, s0) = 0
2: for i : 0 . . . n ? 1
3: for j : 1 . . . n
4: foreach Hi(C\{j}, k) ? b
5: Hi+1(C, j) = maxHi(C\{j},k)?bHi(C\{j}, k)
6: +Mk,j
7: H? = maxHn(C,k) H
n(C, k) +Mk,n+1
Figure 2: Held and Karp Algorithm.
i.e. the last sentence of the summary being gener-
ated. For example, H2({s0, s2, s5}, s2) indicates
a hypothesis that covers s0, s2, s5 and the last sen-
tence is s2. Initially, H0({s0}, s0) is assigned the
score of 0, and new sentences are then added one
by one. In the search procedure, our dynamic pro-
gramming based algorithm retains just the hypoth-
esis with maximum score among the hypotheses
that have the same sentences and the same last sen-
tence. Since this procedure is still computationally
hard, only the top b hypotheses are expanded.
Note that our method learns w from texts auto-
matically annotated by a POS tagger and a named
entity tagger. Thus manual annotation isn?t re-
quired.
2.3 Optimization
The argmax operation in Equation 1 also involves
search, which is NP-hard as described in Section
2.2. Therefore, we adopt the Held and Karp Algo-
rithm and beam search to find approximate solu-
tions. The search algorithm is basically the same
as parameter estimation, except for its calculation
of the informativeness score and size limitation.
Therefore, when a new sentence is added to a hy-
pothesis, both the informativeness and the read-
ability scores are calculated. The size of the hy-
pothesis is also calculated and if the size exceeds
the limit, the sentence can?t be added. A hypoth-
esis that can?t accept any more sentences is re-
moved from the search procedure and preserved
in memory. After all hypotheses are removed,
the best hypothesis is chosen from among the pre-
served hypotheses as the solution.
3 Experiments
This section evaluates our method in terms of
ROUGE score and readability. We collected 2,940
reviews of 100 restaurants from a website. The
327
R-2 R-SU4 R-SU9
Baseline 0.089 0.068 0.062
Method1 0.157 0.096 0.089
Method2 0.172 0.107 0.098
Method3 0.180 0.110 0.101
Human 0.258 0.143 0.131
Table 1: Automatic ROUGE evaluation.
average size of each document set (corresponds to
one restaurant) was 5,343 bytes. We attempted
to generate 300 byte summaries, so the summa-
rization rate was about 6%. We used CRFs-
based Japanese dependency parser (Imamura et
al., 2007) and named entity recognizer (Suzuki et
al., 2006) for sentiment extraction and construct-
ing feature vectors for readability score, respec-
tively.
3.1 ROUGE
We used ROUGE (Lin, 2004) for evaluating the
content of summaries. We chose ROUGE-2,
ROUGE-SU4 and ROUGE-SU9. We prepared
four reference summaries for each document set.
To evaluate the effects of the informativeness
score, the readability score and the optimization,
we compared the following five methods.
Baseline: employs MMR (Carbonell and Gold-
stein, 1998). We designed the score of a sentence
as term frequencies of the content words in a doc-
ument set.
Method1: uses optimization without the infor-
mativeness score or readability score. It also used
term frequencies to score sentences.
Method2: uses the informativeness score and
optimization without the readability score.
Method3: the proposed method. Following
Equation 1, the summarizer searches for a se-
quence with high informativeness and readability
score. The parameter vector w was trained on the
same 2,940 reviews in 5-fold cross validation fash-
ion. ? was set to 6,000 using a development set.
Human is the reference summaries. To com-
pare our summarizer to human summarization, we
calculated ROUGE scores between each reference
and the other references, and averaged them.
The results of these experiments are shown in
Table 1. ROUGE scores increase in the order of
Method1, Method2 and Method3 but no method
could match the performance of Human. The
methods significantly outperformed Baseline ac-
Numbers
Baseline 1.76
Method1 4.32
Method2 10.41
Method3 10.18
Human 4.75
Table 2: Unique sentiment numbers.
cording to the Wilcoxon signed-rank test.
We discuss the contribution of readability to
ROUGE scores. Comparing Method2 to Method3,
ROUGE scores of the latter were higher for all cri-
teria. It is interesting that the readability criterion
also improved ROUGE scores.
We also evaluated our method in terms of sen-
timents. We extracted sentiments from the sum-
maries using the above sentiment extractor, and
averaged the unique sentiment numbers. Table 2
shows the results.
The references (Human) have fewer sentiments
than the summaries generated by our method. In
other words, the references included almost as
many other sentences (e.g. reasons for the senti-
ments) as those expressing sentiments. Carenini
et al (2006) pointed out that readers wanted ?de-
tailed information? in summaries, and the reasons
are one of such piece of information. Including
them in summaries would greatly improve sum-
marizer appeal.
3.2 Readability
Readability was evaluated by human judges.
Three different summarizers generated summaries
for each document set. Ten judges evaluated the
thirty summaries for each. Before the evalua-
tion the judges read evaluation criteria and gave
points to summaries using a five-point scale. The
judges weren?t informed of which method gener-
ated which summary.
We compared three methods; Ordering sen-
tences according to publication dates and posi-
tions in which sentences appear after sentence
extraction (Method2), Ordering sentences us-
ing the readability score after sentence extrac-
tion (Method2+) and searching a document set
to discover the sequence with the highest score
(Method3).
Table 3 shows the results of the experiment.
Readability increased in the order of Method2,
Method2+ and Method3. According to the
328
Readability point
Method2 3.45
Method2+ 3.54
Method3 3.74
Table 3: Readability evaluation.
Wilcoxon signed-rank test, there was no signifi-
cance difference between Method2 and Method2+
but the difference between Method2 and Method3
was significant, p < 0.10.
One important factor behind the higher read-
ability of Method3 is that it yields longer sen-
tences on average (6.52). Method2 and Method2+
yielded averages of 7.23 sentences. The difference
is significant as indicated by p < 0.01. That is,
Method2 and Method2+ tended to select short sen-
tences, which made their summaries less readable.
4 Conclusion
This paper proposed a novel algorithm for senti-
ment summarization that takes account of infor-
mativeness and readability, simultaneously. To
summarize reviews, the informativeness score is
based on sentiments and the readability score is
learned from a corpus of reviews. The preferred
sequence is determined by using dynamic pro-
gramming and beam search. Experiments showed
that our method generated better summaries than
the baseline in terms of ROUGE score and read-
ability.
One future work is to include important infor-
mation other than sentiments in the summaries.
We also plan to model the order of sentences glob-
ally. Although the ordering model in this paper is
local since it looks at only adjacent sentences, a
model that can evaluate global order is important
for better summaries.
Acknowledgments
We would like to sincerely thank Tsutomu Hirao
for his comments and discussions. We would also
like to thank the reviewers for their comments.
References
Hisako Asano, Toru Hirano, Nozomi Kobayashi and
Yoshihiro Matsuo. 2008. Subjective Information In-
dexing Technology Analyzing Word-of-mouth Con-
tent on the Web. NTT Technical Review, Vol.6, No.9.
Regina Barzilay, Noemie Elhadad and Kathleen McK-
eown. 2002. Inferring Strategies for Sentence Or-
dering in Multidocument Summarization. Journal of
Artificial Intelligence Research (JAIR), Vol.17, pp.
35?55.
Regina Barzilay and Lillian Lee. 2004. Catching the
Drift: Probabilistic Content Models, with Applica-
tions to Generation and Summarization. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL),
pp. 113?120.
Regina Barzilay and Mirella Lapata. 2005. Modeling
Local Coherence: An Entity-based Approach. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pp.
141?148.
Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDon-
ald, Tyler Neylon, George A. Reis and Jeff Rey-
nar. 2008. Building a Sentiment Summarizer for Lo-
cal Service Reviews. WWW Workshop NLP Chal-
lenges in the Information Explosion Era (NLPIX).
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR confer-
ence on Research and development in information
retrieval (SIGIR), pp. 335?356.
Giuseppe Carenini, Raymond Ng and Adam Pauls.
2006. Multi-Document Summarization of Evalua-
tive Text. In Proceedings of the 11th European
Chapter of the Association for Computational Lin-
guistics (EACL), pp. 305?312.
Giuseppe Carenini and Jackie Chi Kit Cheung. 2008.
Extractive vs. NLG-based Abstractive Summariza-
tion of Evaluative Text: The Effect of Corpus Con-
troversiality. In Proceedings of the 5th International
Natural Language Generation Conference (INLG),
pp. 33?41.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of the 2002 Conference on Empirical Methods on
Natural Language Processing (EMNLP), pp. 1?8.
Michael Held and Richard M. Karp. 1962. A dy-
namic programming approach to sequencing prob-
lems. Journal of the Society for Industrial and Ap-
plied Mathematics (SIAM), Vol.10, No.1, pp. 196?
210.
Minqing Hu and Bing Liu. 2004. Mining and Summa-
rizing Customer Reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining (KDD), pp. 168?
177.
329
Kenji Imamura, Genichiro Kikui and Norihito Yasuda.
2007. Japanese Dependency Parsing Using Sequen-
tial Labeling for Semi-spoken Language. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL) Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pp. 225?228.
Mirella Lapata. 2003. Probabilistic Text Structuring:
Experiments with Sentence Ordering. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL), pp. 545?552.
Kevin Lerman, Sasha Blair-Goldensohn and Ryan Mc-
Donald. 2009. Sentiment Summarization: Evalu-
ating and Learning User Preferences. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), pp. 514?522.
Kevin Lerman and Ryan McDonald. 2009. Contrastive
Summarization: An Experiment with Consumer Re-
views. In Proceedings of Human Language Tech-
nologies: the 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL-HLT), Companion Vol-
ume: Short Papers, pp. 113?116.
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Proceedings of
the Workshop on Text Summarization Branches Out,
pp. 74?81.
Jun Suzuki, Erik McDermott and Hideki Isozaki. 2006.
Training Conditional Random Fields with Multi-
variate Evaluation Measures. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the ACL
(COLING-ACL), pp. 217?224.
Ivan Titov and Ryan McDonald. 2008. A Joint Model
of Text and Aspect Ratings for Sentiment Summa-
rization. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT),
pp. 308?316.
330
Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 78?84,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Pilot Study on Readability Prediction with Reading Time
Hitoshi Nishikawa, Toshiro Makino and Yoshihiro Matsuo
NTT Media Intelligence Laboratories, NTT Corporation
1-1 Hikari-no-oka, Yokosuka-shi, Kanagawa, 239-0847 Japan
{
nishikawa.hitoshi
makino.toshiro, matsuo.yoshihiro
}
@lab.ntt.co.jp
Abstract
In this paper we report the results of a pi-
lot study of basing readability prediction
on training data annotated with reading
time. Although reading time is known to
be a good metric for predicting readabil-
ity, previous work has mainly focused on
annotating the training data with subjec-
tive readability scores usually on a 1 to
5 scale. Instead of the subjective assess-
ments of complexity, we use the more ob-
jective measure of reading time. We create
and evaluate a predictor using the binary
classification problem; the predictor iden-
tifies the better of two documents correctly
with 68.55% accuracy. We also report a
comparison of predictors based on reading
time and on readability scores.
1 Introduction
Several recent studies have attempted to predict
the readability of documents (Pitler and Nenkova,
2008; Burstein et al, 2010; Nenkova et al, 2010;
Pitler et al, 2010; Tanaka-Ishii et al, 2010). Pre-
dicting readability has a very important role in the
field of computational linguistics and natural lan-
guage processing:
? Readability prediction can help users retrieve
information from the Internet. If the read-
ability of documents can be predicted, search
engines can rank the documents according to
readability, allowing users to access the infor-
mation they need more easily (Tanaka-Ishii et
al., 2010).
? The predicted readability of a document can
be used as an objective function in natural
language applications such as machine trans-
lation, automatic summarization, and docu-
ment simplification. Machine translation can
use a readability predictor as a part of the ob-
jective function to make more fluent transla-
tions (Nenkova et al, 2010). The readabil-
ity predictor can also be used as a part of a
summarizer to generate readable summaries
(Pitler et al, 2010). Document simplification
can help readers understand documents more
easily by automatically rewriting documents
that are not easy to read (Zhu et al, 2010;
Woodsend and Lapata, 2011). This is pos-
sible by paraphrasing the sentences so as to
maximize document readability.
? Readability prediction can be used for educa-
tional purposes (Burstein et al, 2010). It can
assess human-generated documents automat-
ically.
Most studies build a predictor that outputs a
readability score (generally 1-5 scale) or a clas-
sifier or ranker that identifies which of two doc-
uments has the better readability. Using textual
complexity to rank documents may be adequate
for several applications in the fields of information
retrieval, machine translation, document simplifi-
cation, and the assessment of human-written doc-
uments. Approaches based on complexity, how-
ever, do not well support document summariza-
tion.
In the context of automatic summarization,
users want concise summaries to understand the
important information present in the documents as
rapidly as possible? to create summaries that can
be read as quickly as possible, we need a func-
tion that can evaluate the quality of the summary
in terms of reading time.
78
To achieve this goal, in this paper, we show the
results of our pilot study on predicting the reading
time of documents. Our predictor has two features
as follows:
1. Our predictor is trained by documents di-
rectly annotated with reading time. While
previous work employs subjective assess-
ments of complexity, we directly use the
reading time to build a predictor. As a pre-
dictor, we adopt Ranking SVM (Joachims,
2002).
2. The predictor predicts the reading time with-
out recourse to features related to document
length since our immediate goal is text sum-
marization. A preliminary experiment con-
firms that document length is effective for
readability prediction confirming the work
by (Pitler and Nenkova, 2008; Pitler et al,
2010). Summarization demands that the pre-
dictor work well regardless of text length.
This is the first report to show that the result of
training a predictor with data annotated by read-
ing time is to improve the quality of automatic
readability prediction. Furthermore, we report
the result of the comparison between our read-
ing time predictor and a conventional complexity-
based predictor.
This paper is organized as follows: Section 2
describes related work. Section 3 describes the
data used in the experiments. Section 4 describes
our model. Section 5 elaborates the features for
predicting document readability based on read-
ing time. Section 6 reports our evaluation exper-
iments. We conclude this paper and show future
directions in Section 7.
2 Related Work
Recent work formulates readability prediction as
an instance of a classification, regression, or rank-
ing problem. A document is regarded as a mix-
ture of complex features and its readability is pre-
dicted by the use of machine learning (Pitler and
Nenkova, 2008; Pitler et al, 2010; Tanaka-Ishii et
al., 2010). Pitler and Nenkova (2008) built a clas-
sifier that employs various features extracted from
a document and newswire documents annotated
with a readability score on a 1 to 5 scale. They in-
tegrated complex features by using SVM and iden-
tified the better document correctly with 88.88%
accuracy. They reported that the log likelihood of
a document based on its discourse relations, the
log likelihood of a document based on n-gram, the
average number of verb phrases in sentences, the
number of words in the document were good in-
dicators on which to base readability prediction.
Pitler et al, (2010) used the same framework to
predict the linguistic quality of a summary. In the
field of automatic summarization, linguistic qual-
ity has been assessed manually and hence to auto-
mate the assessment is an important research prob-
lem (Pitler et al, 2010). A ranker based on Rank-
ing SVM has been constructed (Joachims, 2002)
and identified the better of two summaries cor-
rectly with an accuracy of around 90%. Tanaka-
Ishii et al, (2010) also built a ranker to predict the
rank of documents according to readability. While
Tanaka-Ishii et al used word-level features for the
prediction, Pitler and Nenkova (2008) and Pitler
et al, (2010) also leveraged sentence-level fea-
tures and document-level features. In this paper,
we extend their findings to predict readability. We
elaborate our feature set in Section 5. While all
of them either classify or rank the documents by
assigning a readability score on a 1-5 scale, our
research goal is to build a predictor that can also
estimate the reading time.
In the context of multi-document summariza-
tion, the linguistic quality of a summary is pre-
dicted to order the sentences extracted from the
original documents (Barzilay and Lapata, 2005;
Lapata, 2006; Barzilay and Lapata, 2008). In
multi-document summarization, since sentences
are extracted from the original documents without
regard for context, they must be ordered in some
way to make the summary coherent. One of the
most important features for ordering sentences is
the entity grid suggested by Barzilay and Lapata
(2005; 2008). It captures transitions in the seman-
tic roles of the noun phrases in a document, and
can predict the quality of an order of the sentences
with high accuracy. It was also used as an im-
portant feature in the work by Pitler and Nenkova
(2008) and Piter et al, (2010) to predict the read-
ability of a document. Burstein et al, (2010) used
it for an educational purpose, and used it to predict
79
the readability of essays. Lapata (Lapata, 2006)
suggested the use of Kendall?s Tau as an indicator
of the quality of a set of sentences in particular or-
der; she also reported that self-paced reading time
is a good indicator of quality. While Lapata fo-
cuses on sentence ordering, our research goal is to
predict the overall quality of a document in terms
of reading time.
3 Data
To build a predictor that can estimate the read-
ing time of a document, we made a collection
of documents and annotated each with its read-
ing time and readability score. We randomly se-
lected 400 articles from Kyoto Text Corpus 4.0 1.
The corpus consists of newswire articles written
in Japanese and annotated with word boundaries,
part-of-speech tags and syntactic structures. We
developed an experimental system that showed ar-
ticles for each subject and gathered reading times.
Each article was read by 4 subjects. All subjects
are native speakers of Japanese.
Basically, we designed our experiment follow-
ing Pitler and Nenkova (2008). The subjects were
asked to use the system to read the articles. They
could read each document without a time limit, the
only requirement being that they were to under-
stand the content of the document. While the sub-
jects were reading the article, the reading time was
recorded by the system. We didn?t tell the subjects
that the time was being recorded.
To prevent the subjects from only partially read-
ing the document and raise the reliability of the re-
sults, we made a multiple-choice question for each
document; the answer was to be found in the doc-
ument. This was used to weed out unreliable re-
sults.
After the subjects read the document, they were
asked to answer the question.
Finally, the subjects were asked questions re-
lated to readability as follows:
1. How well-written is this article?
2. How easy was it to understand?
3. How interesting is this article?
Following the work by Pitler and Nenkova
(2008), the subjects answered by selecting a value
1http://nlp.ist.i.kyoto-u.ac.jp/EN/
between 1 and 5, with 5 being the best and 1 be-
ing the worst and we used only the answer to the
first question (How well-written is this article?) as
the readability score. We dropped the results in
which the subjects gave the wrong answer to the
multiple-choice question. Finally, we had 683 tu-
ples of documents, reading times, and readability
scores.
4 Model
To predict the readability of a document according
to reading time, we use Ranking SVM (Joachims,
2002). A target document is converted to a feature
vector as explained in Section 5, then the predictor
ranks two documents. The predictor assigns a real
number to a document as its score; ranking is done
according to score. In this paper, a higher score
means better readability, i.e., shorter reading time.
5 Features
In this section we elaborate the features used to
predict the reading time. While most of them were
introduced in previous work, see Section 3, the
word level features are introduced here.
5.1 Word-level Features
Character Type (CT)
Japanese sentences consist of several types of
characters: kanji, hiragana, katakana, and Roman
letters. We use the ratio of the number of kanji to
the number of hiragana as a feature of the docu-
ment.
Word Familiarity (WF)
Amano and Kondo (2007) developed a list of
words annotated with word familiarity; it indicates
how familiar a word is to Japanese native speakers.
The list is the result of a psycholinguistic experi-
ment and the familiarity ranges from 1 to 7, with
7 being the most familiar and 1 being the least fa-
miliar. We used the average familiarity of words
in the document as a feature.
5.2 Sentence-level Features
Language Likelihood (LL)
Language likelihood based on an n-gram language
model is widely used to generate natural sen-
tences. Intuitively, a sentence whose language
likelihood is high will have good readability. We
80
made a trigram language model from 17 years
(1991-2007) of Mainichi Shinbun Newspapers by
using SRILM Toolkit. Since the language model
assigns high probability to shorter documents, we
normalized the probability by the number of words
in a document.
Syntactic Complexity (TH/NB/NC/NP)
Schwarm and Ostendorf (2005) suggested that
syntactic complexity of a sentence can be used as
a feature for reading level assessment. We use the
following features as indicators of syntactic com-
plexity:
? The height of the syntax tree (TH): we use
the height of the syntax tree as an indicator of
the syntactic complexity of a sentence. Com-
plex syntactic structures demand that readers
make an effort to interpret them. We use the
average, maximum and minimum heights of
syntax trees in a document as a feature.
? The number of bunsetsu (NB): in Japanese
dependency parsing, syntactic relations are
defined between bunsetsu; they are almost
the same as Base-NP (Veenstra, 1998) with
postpositions. If a sentence has a lot of bun-
setsu, it can have a complex syntactic struc-
ture. We use the average, maximum and min-
imum number of them as a feature.
? The number of commas (NC): a comma sug-
gests a complex syntax structure such as sub-
ordinate and coordinate clauses. We use the
average, maximum and minimum number of
them as a feature.
? The number of predicates (NP): intuitively,
a sentence can be syntactically complex if it
has a lot of predicates. We use the average,
maximum and minimum number of them as
a feature.
5.3 Document-level Features
Discourse Relations (DR)
Pitler and Nenkova (2008) used discourse rela-
tions of the Penn Discourse Treebank (Prasad
et al, 2008) as a feature. Since our corpus
doesn?t have human-annotated discourse relations
between the sentences, we use the average num-
ber of connectives per sentence as a feature. In-
tuitively, the explicit discourse relations indicated
by the connectives will yield better readability.
Entity Grid (EG)
Along with the previous work (Pitler and
Nenkova, 2008; Pitler et al, 2010), we use entity
grid (Barzilay and Lapata, 2005; Barzilay and La-
pata, 2008) as a feature. We make a vector whose
element is the transition probability between syn-
tactic roles (i.e. subject, object and other) of the
noun phrases in a document. Since our corpus
consists of Japanese documents, we use postpo-
sitions to recognize the syntactic role of a noun
phrase. Noun phrases with postpositions ?Ha? and
?Ga? are recognized as subjects. Noun phrases
with postpositions ?Wo? and ?Ni? are recognized
as objects. Other noun phrases are marked as
other. We combine the entity grid vector to form a
final feature vector for predicting reading time.
Lexical Cohesion (LC)
Lexical cohesion is one of the strongest features
for predicting the linguistic quality of a summary
(Pitler et al, 2010). Following their work, we
leverage the cosine similarity of adjacent sen-
tences as a feature. To calculate it, we make
a word vector by extracting the content words
(nouns, verbs and adjectives) from a sentence. The
frequency of each word in the sentence is used as
the value of the sentence vector. We use the aver-
age, maximum and minimum cosine similarity of
the sentences as a feature.
6 Experiments
This section explains the setting of our experi-
ment. As mentioned above, we adopted Ranking
SVM as a predictor. Since we had 683 tuples (doc-
uments, reading time and readability scores), we
made 683C2 = 232, 903 pairs of documents for
Ranking SVM. Each pair consists of two docu-
ments where one has a shorter reading time than
the other. The predictor learned which parameters
were better at predicting which document would
have the shorter reading time, i.e. higher score.
We performed a 10-fold cross validation on the
pairs consisting of the reading time explained in
Section 3 and the features explained in Section 5.
In order to analyze the contribution of each feature
81
Features Accuracy
ALL 68.45
TH + EG + LC 68.55
Character Type (CT) 52.14
Word Familiarity (WF) 51.30
Language Likelihood (LL) 50.40
Height of Syntax Tree (TH) 61.86
Number of Bunsetsu (NB) 51.54
Number of Commas (NC) 47.07
Number of Predicates (NP) 52.82
Discourse Relations (DR) 48.04
Entity Grid (EG) 67.74
Lexical Cohesion (LC) 61.63
Document Length 69.40
Baseline 50.00
Table 1: Results of proposed reading time predic-
tor.
to prediction accuracy, we adopted a linear kernel.
The range of the value of each feature was normal-
ized to lie between -1 and 1.
6.1 Classification based on reading time
Table 1 shows the results yielded by the read-
ing time predictor. ALL indicates the accuracy
achieved by the classifier with all features ex-
plained in Section 5. At the bottom of Table 1,
Baseline shows the accuracy of random classifica-
tion. As shown in Table 1, since the height of syn-
tax tree, entity grid and lexical cohesion are good
indicators for the prediction, we combined these
features. TH + EG + LC indicates that this combi-
nation achieves the best performance.
As to individual features, most of them couldn?t
distinguish a better document from a worse one.
CT, WF and LL show similar performance to
Baseline. The reason why these features failed to
clearer identify the better of the pair could be be-
cause the documents are newswire articles. The
ratio between kanji and hiragana, CT, is similar in
most of the articles and hence it couldn?t identify
the better document. Similarly, there isn?t so much
of a difference among the documents in terms of
word familiarity, WF. The language model used,
LL, was not effective against the documents tested
but it is expected that it would useful if the target
documents came from different fields.
Among the syntactic complexity features, TH
offers the best performance. Since its learned
feature weight is negative, the result shows that
a higher syntax tree causes longer reading time.
While TH has shows good performance, NB, NC
and NP fail to offer any significant advantage. As
with the word-level features, there isn?t so much
of a difference among the documents in terms of
the values of these features. This is likely because
most of the newswire articles are written by ex-
perts for a restricted field.
Among the document-level features, EG and
LC show good performance. While Pitler and
Nenkova (2008) have shown that the discourse re-
lation feature is strongest at predicting the linguis-
tic quality of a document, DR shows poor perfor-
mance. Whereas they modeled the discourse rela-
tions by a multinomial distribution using human-
annotated labels, DR was simply the number of
connectives in the document. A more sophisti-
cated approach will be needed to model discourse.
EG and LC show the best prediction perfor-
mance of the single features, which agrees with
previous work (Pitler and Nenkova, 2008; Pitler
et al, 2010). While, as shown above, most of the
sentence-level features don?t have good discrimi-
native performance, EG and LC work well. Since
these features can work well in homogeneous doc-
uments like newswire articles, it is reasonable to
expect that they will also work well in heteroge-
neous documents from various domains.
We also show the classification result achieved
with document length. Piter and Nenkova (2008)
have shown that document length is a strong indi-
cator for readability prediction. We measure docu-
ment length by three criteria: the number of char-
acters, the number of words and the number of
sentences in the document. We used these values
as features and built a predictor. While the docu-
ment length has the strongest classification perfor-
mance, the predictor with TH + EG + LC shows
equivalent performance.
6.2 Classification based on readability score
We also report that the result of the classification
based on the readability score in Table 2. Along
with the result of the reading time, we tested
ALL and TH + EG + LC, and the single features.
While DR shows poor classification performance
in terms of reading time, it shows the best classi-
82
Features Accuracy
ALL 57.25
TH + DR + EG + LC 56.51
TH + EG + LC 56.50
Character Type (CT) 51.96
Word Familiarity (WF) 51.50
Language Likelihood (LL) 50.68
Height of Syntax Tree (TH) 55.77
Number of Bunsetsu (NB) 52.99
Number of Commas (NC) 51.50
Number of Predicates (NP) 52.56
Discourse Relations (DR) 58.14
Entity Grid (EG) 56.14
Lexical Cohesion (LC) 55.77
Document Length 56.83
Baseline 50.00
Table 2: A result of classification based on read-
ability score.
Cor. coef.
Reading Time 0.822
Readability Score 0.445
Table 3: Correlation coefficients of the reading
time and readability score between the subjects.
We calculated the coefficient for each pair of sub-
jects and then averaged them.
fication performance as regards readability score.
Hence we add the result of TH + DR + EG + LC.
It agrees with the findings showed by Pitler and
Nenkova (2008) in which they have shown dis-
course relation is the best feature for predicting the
readability score.
In general, the same features used for classifica-
tion based on the reading time work well for pre-
dicting the readability score. TH and EG, LC have
good prediction performance.
6.3 Variation in reading time vs. variation in
readability score
We show the correlation between the subjects in
terms of the variation in reading time and read-
ability score in Table 3. As shown, the reading
time shows much higher correlation (less varia-
tion) than the readability score. This agrees with
the findings shown by Lapata (2006) in which
the reading time is a better indicator for read-
ability prediction. Since the readability score
varies widely among the subjects, training be-
comes problematic with lowers predictor perfor-
mance.
The biggest difference between the prediction
of the reading time and readability score is the
effect of feature DR. One hypothesis that could
explain the difference is that the use of connec-
tives works as a strong sign that the document has
a good readability score?it doesn?t necessarily
imply that the document has good readability?
for the subjects. That is, the subjects perceived
the documents with more connectives as readable,
however, those connectives contribute to the read-
ing time. Of course, our feature about discourse
relations is just based on their usage frequency and
hence more precise modeling could improve per-
formance.
7 Conclusion and Future Work
This paper has described our pilot study of read-
ability prediction based on reading time. With au-
tomatic summarization in mind, we built a predic-
tor that can predict the reading time, and read-
ability, of a document. Our predictor identified
the better of two documents with 68.55% accuracy
without using features related to document length.
The following findings can be extracted from
the results described above:
? The time taken to read documents can be
predicted through existing machine learning
technique and the features extracted from
training data annotated with reading time
(Pitler and Nenkova, 2008; Pitler et al,
2010).
? As Lapata (2006) has shown, reading time is
a highly effective indicator of readability. In
our experiment, reading time showed good
agreement among the subjects and hence
more coherent prediction results can be ex-
pected.
Future work must proceed in many directions:
1. Measuring more precise reading time is one
important problem. One solution is to use an
eye tracker; it can measure the reading time
more accurately because it can capture when
83
the subject finishes reading a document. In
order to prepare the data used in this paper,
we set questions so as to identify and drop
unreliable data. The eye tracker could allevi-
ate this effort.
2. Testing the predictor in another domain is
necessary for creating practical applications.
We tested the predictor only in the domain
of newswire articles, as described earlier, and
different results might be recorded in do-
mains other than newswire articles.
3. Improving the accuracy of the predictor is
also important. There could be other fea-
tures associated with readability prediction.
We plan to explore other features.
4. Applying the predictor to natural language
generation tasks is particularly important. We
plan to integrate our predictor into a summa-
rizer and evaluate its performance.
References
Shigeaki Amano and Tadahisa Kondo. 2007. Reliabil-
ity of familiarity rating of ordinary japanese words
for different years and places. Behavior Research
Methods, 39(4):1008?1011.
Regina Barzilay and Mirella Lapata. 2005. Model-
ing local coherence: an entity-based approach. In
Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics (ACL), pages
141?148.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1?34.
Jill Burstein, Joel Tetreault, and Slava Andreyev. 2010.
Using entity-based features to model coherence in
student essays. In Proceedings of Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL-HLT), pages
681?684.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the ACM
Conference on Knowledge Discovery and Data Min-
ing (KDD), pages 133?142.
Mirella Lapata. 2006. Automatic evaluation of infor-
mation ordering: Kendall?s tau. Computational Lin-
guistics, 32(4):471?484.
Ani Nenkova, Jieun Chae, Annie Louis, and Emily
Pitler. 2010. Structural features for predicting the
linguistic quality of text: Applications to machine
translation, automatic summarization and human-
authored text. In Emiel Krahmer and Theunem
Mariet, editors, Empirical Methods in Natural Lan-
guage Generation: Data-oriented Methods and Em-
pirical Evaluation, pages 222?241. Springer.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 186?195.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 544?554.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC).
Sarah Schwarm and Mari Ostendorf. 2005. Reading
level assessment using support vector machines and
statistical language models. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 523?530.
Kumiko Tanaka-Ishii, Satoshi Tezuka, and Hiroshi Ter-
ada. 2010. Sorting by readability. Computational
Linguistics, 36(2):203?227.
Jorn Veenstra. 1998. Fast np chunking using memory-
based learning techniques. In Proceedings of the
8th Belgian-Dutch Conference on Machine Learn-
ing (Benelearn), pages 71?78.
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
409?420.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of the
23rd International Conference on Computational
Linguistics (Coling 2010), pages 1353?1361.
84
