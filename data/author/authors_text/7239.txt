Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 57?60,
Sydney, July 2006. c?2006 Association for Computational Linguistics
The SAMMIE System: Multimodal In-Car Dialogue
Tilman Becker, Peter Poller,
Jan Schehl
DFKI
First.Last@dfki.de
Nate Blaylock, Ciprian Gerstenberger,
Ivana Kruijff-Korbayova?
Saarland University
talk-mit@coli.uni-sb.de
Abstract
The SAMMIE1 system is an in-car multi-
modal dialogue system for an MP3 ap-
plication. It is used as a testing environ-
ment for our research in natural, intuitive
mixed-initiative interaction, with particu-
lar emphasis on multimodal output plan-
ning and realization aimed to produce out-
put adapted to the context, including the
driver?s attention state w.r.t. the primary
driving task.
1 Introduction
The SAMMIE system, developed in the TALK
project in cooperation between several academic
and industrial partners, employs the Information
State Update paradigm, extended to model collab-
orative problem solving, multimodal context and
the driver?s attention state. We performed exten-
sive user studies in a WOZ setup to guide the sys-
tem design. A formal usability evaluation of the
system?s baseline version in a laboratory environ-
ment has been carried out with overall positive re-
sults. An enhanced version of the system will be
integrated and evaluated in a research car.
In the following sections, we describe the func-
tionality and architecture of the system, point out
its special features in comparison to existing work,
and give more details on the modules that are in
the focus of our research interests. Finally, we
summarize our experiments and evaluation results.
2 Functionality
The SAMMIE system provides a multi-modal inter-
face to an in-car MP3 player (see Fig. 1) through
speech and haptic input with a BMW iDrive input
device, a button which can be turned, pushed down
and sideways in four directions (see Fig. 2 left).
System output is provided by speech and a graphi-
cal display integrated into the car?s dashboard. An
example of the system display is shown in Fig. 2.
1SAMMIE stands for Saarbru?cken Multimodal MP3 Player
Interaction Experiment.
Figure 1: User environment in laboratory setup.
The MP3 player application offers a wide range
of functions: The user can control the currently
playing song, search and browse an MP3 database
by looking for any of the fields (song, artist, al-
bum, year, etc.), search and select playlists and
even construct and edit playlists.
The user of SAMMIE has complete freedom in
interacting with the system. Input can be through
any modality and is not restricted to answers to
system queries. On the contrary, the user can give
new tasks as well as any information relevant to
the current task at any time. This is achieved by
modeling the interaction as a collaborative prob-
lem solving process, and multi-modal interpreta-
tion that fits user input into the context of the
current task. The user is also free in their use
of multimodality: SAMMIE handles deictic refer-
ences (e.g., Play this title while pushing the iDrive
button) and also cross-modal references, e.g., Play
the third song (on the list). Table 1 shows a typ-
ical interaction with the SAMMIE system; the dis-
played song list is in Fig. 2. SAMMIE supports in-
teraction in German and English.
3 Architecture
Our system architecture follows the classical ap-
proach (Bunt et al, 2005) of a pipelined architec-
ture with multimodal interpretation (fusion) and
57
U: Show me the Beatles albums.
S: I have these four Beatles albums.
[shows a list of album names]
U: Which songs are on this one?
[selects the Red Album]
S: The Red Album contains these songs
[shows a list of the songs]
U: Play the third one.
S: [music plays]
Table 1: A typical interaction with SAMMIE.
fission modules encapsulating the dialogue man-
ager. Fig. 2 shows the modules and their inter-
action: Modality-specific recognizers and analyz-
ers provide semantically interpreted input to the
multimodal fusion module that interprets them in
the context of the other modalities and the cur-
rent dialogue context. The dialogue manager de-
cides on the next system move, based on its model
of the tasks as collaborative problem solving, the
current context and also the results from calls to
the MP3 database. The turn planning module then
determines an appropriate message to the user by
planning the content, distributing it over the avail-
able output modalities and finally co-ordinating
and synchronizing the output. Modality-specific
output modules generate spoken output and graph-
ical display update. All modules interact with the
extended information state which stores all context
information.
Figure 2: SAMMIE system architecture.
Many tasks in the SAMMIE system are mod-
eled by a plan-based approach. Discourse mod-
eling, interpretation management, dialogue man-
agement and linguistic planning, and turn plan-
ning are all based on the production rule system
PATE2 (Pfleger, 2004). It is based on some con-
cepts of the ACT-R 4.0 system, in particular the
goal-oriented application of production rules, the
2Short for (P)roduction rule system based on (A)ctivation
and (T)yped feature structure (E)lements.
activation of working memory elements, and the
weighting of production rules. In processing typed
feature structures, PATE provides two operations
that both integrate data and also are suitable for
condition matching in production rule systems,
namely a slightly extended version of the general
unification, but also the discourse-oriented opera-
tion overlay (Alexandersson and Becker, 2001).
4 Related Work and Novel Aspects
Many dialogue systems deployed today follow a
state-based approach that explicitly models the
full (finite) set of dialogue states and all possible
transitions between them. The VoiceXML3 stan-
dard is a prominent example of this approach. This
has two drawbacks: on the one hand, this approach
is not very flexible and typically allows only so-
called system controlled dialogues where the user
is restricted to choosing their input from provided
menu-like lists and answering specific questions.
The user never is in control of the dialogue. For
restricted tasks with a clear structure, such an ap-
proach is often sufficient and has been applied suc-
cessfully. On the other hand, building such appli-
cations requires a fully specified model of all pos-
sible states and transitions, making larger applica-
tions expensive to build and difficult to test.
In SAMMIE we adopt an approach that mod-
els the interaction on an abstract level as collab-
orative problem solving and adds application spe-
cific knowledge on the possible tasks, available re-
sources and known recipes for achieving the goals.
In addition, all relevant context information is
administered in an Extended Information State.
This is an extension of the Information State Up-
date approach (Traum and Larsson, 2003) to the
multi-modal setting.
Novel aspects in turn planning and realization
include the comprehensive modeling in a sin-
gle, OWL-based ontology and an extended range
of context-sensitive variation, including system
alignment to the user on multiple levels.
5 Flexible Multi-modal Interaction
5.1 Extended Information State
The information state of a multimodal system
needs to contain a representation of contextual in-
formation about discourse, but also a represen-
tation of modality-specific information and user-
specific information which can be used to plan
system output suited to a given context. The over-
3http://www.w3.org/TR/voicexml20
58
all information state (IS) of the SAMMIE system is
shown in Fig. 3.
The contextual information partition of the IS
represents the multimodal discourse context. It
contains a record of the latest user utterance and
preceding discourse history representing in a uni-
form way the salient discourse entities introduced
in the different modalities. We adopt the three-
tiered multimodal context representation used in
the SmartKom system (Pfleger et al, 2003). The
contents of the task partition are explained in the
next section.
5.2 Collaborative Problem Solving
Our dialogue manager is based on an
agent-based model which views dialogue
as collaborative problem-solving (CPS)
(Blaylock and Allen, 2005). The basic building
blocks of the formal CPS model are problem-
solving (PS) objects, which we represent as
typed feature structures. PS object types form a
single-inheritance hierarchy. In our CPS model,
we define types for the upper level of an ontology
of PS objects, which we term abstract PS objects.
There are six abstract PS objects in our model
from which all other domain-specific PS objects
inherit: objective, recipe, constraint, evaluation,
situation, and resource. These are used to model
problem-solving at a domain-independent level
and are taken as arguments by all update opera-
tors of the dialogue manager which implement
conversation acts (Blaylock and Allen, 2005).
The model is then specialized to a domain by
inheriting and instantiating domain-specific types
and instances of the PS objects.
5.3 Adaptive Turn Planning
The fission component comprises detailed con-
tent planning, media allocation and coordination
and synchronization. Turn planning takes a set
of CPS-specific conversational acts generated by
the dialogue manager and maps them to modality-
specific communicative acts.
Information on how content should be dis-
tributed over the available modalities (speech or
graphics) is obtained from Pastis, a module which
stores discourse-specific information. Pastis pro-
vides information about (i) the modality on which
the user is currently focused, derived by the cur-
rent discourse context; (ii) the user?s current cog-
nitive load when system interaction becomes a
secondary task (e.g., system interaction while
driving); (iii) the user?s expertise, which is rep-
resented as a state variable. Pastis also contains
information about factors that influence the prepa-
ration of output rendering for a modality, like the
currently used language (German or English) or
the display capabilities (e.g., maximum number of
displayable objects within a table). Together with
the dialogue manager?s embedded part of the in-
formation state, the information stored by Pastis
forms the Extended Information State of the SAM-
MIE system (Fig. 3).
Planning is then executed through a set of pro-
duction rules that determine which kind of infor-
mation should be presented through which of the
available modalities. The rule set is divided in two
subsets, domain-specific and domain-independent
rules which together form the system?s multi-
modal plan library.
contextual-info:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
last-user-utterance:
:
[
interp : set(grounding-acts)
modality-requested : modality
modalities-used : set(msInput)
]
discourse-history:
: list(discourse-objects)
modality-info:
:
[
speech : speechInfo
graphic : graphicInfo
]
user-info:
:
[
cognitive-load : cogLoadInfo
user-expertise : expertiseInfo
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
task-info:
[
cps-state : c-situation (see below for details)
pending-sys-utt : list(grounding-acts)
]
Figure 3: SAMMIE Information State structure.
5.4 Spoken Natural Language Output
Generation
Our goal is to produce output that varies in the sur-
face realization form and is adapted to the con-
text. A template-based module has been devel-
oped and is sufficient for classes of system output
that do not need fine-tuned context-driven varia-
tion. Our template-based generator can also de-
liver alternative realizations, e.g., alternative syn-
tactic constructions, referring expressions, or lexi-
cal items. It is implemented by a set of straightfor-
ward sentence planning rules in the PATE system
to build the templates, and a set of XSLT trans-
formations to yield the output strings. Output in
German and English is produced by accessing dif-
ferent dictionaries in a uniform way.
In order to facilitate incremental development
of the whole system, our template-based mod-
ule has a full coverage wrt. the classes of sys-
59
tem output that are needed. In parallel, we are
experimenting with a linguistically more power-
ful grammar-based generator using OpenCCG4,
an open-source natural language processing en-
vironment (Baldridge and Kruijff, 2003). This al-
lows for more fine-grained and controlled choices
between linguistic expressions in order to achieve
contextually appropriate output.
5.5 Modeling with an Ontology
We use a full model in OWL as the knowledge rep-
resentation format in the dialogue manager, turn
planner and sentence planner. This model in-
cludes the entities, properties and relations of the
MP3 domain?including the player, data base and
playlists. Also, all possible tasks that the user may
perform are modeled explicitly. This task model
is user centered and not simply a model of the
application?s API.The OWL-based model is trans-
formed automatically to the internal format used
in the PATE rule-interpreter.
We use multiple inheritance to model different
views of concepts and the corresponding presen-
tation possibilities; e.g., a song is a browsable-
object as well as a media-object and thus allows
for very different presentations, depending on con-
text. Thereby PATE provides an efficient and ele-
gant way to create more generic presentation plan-
ning rules.
6 Experiments and Evaluation
So far we conducted two WOZ data collection
experiments and one evaluation experiment with
a baseline version of the SAMMIE system. The
SAMMIE-1 WOZ experiment involved only spo-
ken interaction, SAMMIE-2 was multimodal, with
speech and haptic input, and the subjects had
to perform a primary driving task using a Lane
Change simulator (Mattes, 2003) in a half of their
experiment session. The wizard was simulating
an MP3 player application with access to a large
database of information (but not actual music) of
more than 150,000 music albums (almost 1 mil-
lion songs). In order to collect data with a variety
of interaction strategies, we used multiple wizards
and gave them freedom to decide about their re-
sponse and its realization. In the multimodal setup
in SAMMIE-2, the wizards could also freely de-
cide between mono-modal and multimodal output.
(See (Kruijff-Korbayova? et al, 2005) for details.)
We have just completed a user evaluation to
explore the user-acceptance, usability, and per-
formance of the baseline implementation of the
4http://openccg.sourceforge.net
SAMMIE multimodal dialogue system. The users
were asked to perform tasks which tested the sys-
tem functionality. The evaluation analyzed the
user?s interaction with the baseline system and
combined objective measurements like task com-
pletion (89%) and subjective ratings from the test
subjects (80% positive).
Acknowledgments This work has been carried
out in the TALK project, funded by the EU 6th
Framework Program, project No. IST-507802.
References
[Alexandersson and Becker2001] J. Alexandersson and
T. Becker. 2001. Overlay as the basic operation for
discourse processing in a multimodal dialogue system. In
Proceedings of the 2nd IJCAI Workshop on Knowledge
and Reasoning in Practical Dialogue Systems, Seattle,
Washington, August.
[Baldridge and Kruijff2003] J.M. Baldridge and G.J.M. Krui-
jff. 2003. Multi-Modal Combinatory Categorial Gram-
mar. In Proceedings of the 10th Annual Meeting of the
European Chapter of the Association for Computational
Linguistics (EACL?03), Budapest, Hungary, April.
[Blaylock and Allen2005] N. Blaylock and J. Allen. 2005. A
collaborative problem-solving model of dialogue. In Laila
Dybkj?r and Wolfgang Minker, editors, Proceedings of
the 6th SIGdial Workshop on Discourse and Dialogue,
pages 200?211, Lisbon, September 2?3.
[Bunt et al2005] H. Bunt, M. Kipp, M. Maybury, and
W. Wahlster. 2005. Fusion and coordination for multi-
modal interactive information presentation: Roadmap, ar-
chitecture, tools, semantics. In O. Stock and M. Zanca-
naro, editors, Multimodal Intelligent Information Presen-
tation, volume 27 of Text, Speech and Language Technol-
ogy, pages 325?340. Kluwer Academic.
[Kruijff-Korbayova? et al2005] I. Kruijff-Korbayova?,
T. Becker, N. Blaylock, C. Gerstenberger, M. Kai?er,
P. Poller, J. Schehl, and V. Rieser. 2005. An experiment
setup for collecting data for adaptive output planning in
a multimodal dialogue system. In Proc. of ENLG, pages
191?196.
[Mattes2003] S. Mattes. 2003. The lane-change-task as a tool
for driver distraction evaluation. In Proc. of IGfA.
[Pfleger et al2003] N. Pfleger, J. Alexandersson, and
T. Becker. 2003. A robust and generic discourse model
for multimodal dialogue. In Proceedings of the 3rd
Workshop on Knowledge and Reasoning in Practical
Dialogue Systems, Acapulco.
[Pfleger2004] N. Pfleger. 2004. Context based multimodal
fusion. In ICMI ?04: Proceedings of the 6th interna-
tional conference on Multimodal interfaces, pages 265?
272, New York, NY, USA. ACM Press.
[Traum and Larsson2003] David R. Traum and Staffan Lars-
son. 2003. The information state approach to dialog man-
agement. In Current and New Directions in Discourse and
Dialog. Kluwer.
60
An Experiment Setup for Collecting Data for Adaptive Output Planning
in a Multimodal Dialogue System
Ivana Kruijff-Korbayova?, Nate Blaylock,
Ciprian Gerstenberger, Verena Rieser
Saarland University, Saarbru?cken, Germany
korbay@coli.uni-sb.de
Tilman Becker, Michael Kai?er,
Peter Poller, Jan Schehl
DFKI, Saarbru?cken, Germany
tilman.becker@dfki.de
Abstract
We describe a Wizard-of-Oz experiment setup for
the collection of multimodal interaction data for a
Music Player application. This setup was devel-
oped and used to collect experimental data as part
of a project aimed at building a flexible multimodal
dialogue system which provides an interface to an
MP3 player, combining speech and screen input
and output. Besides the usual goal of WOZ data
collection to get realistic examples of the behav-
ior and expectations of the users, an equally im-
portant goal for us was to observe natural behavior
of multiple wizards in order to guide our system
development. The wizards? responses were there-
fore not constrained by a script. One of the chal-
lenges we had to address was to allow the wizards
to produce varied screen output a in real time. Our
setup includes a preliminary screen output planning
module, which prepares several versions of possi-
ble screen output. The wizards were free to speak,
and/or to select a screen output.
1 Introduction
In the larger context of the TALK project1 we are develop-
ing a multimodal dialogue system for a Music Player appli-
cation for in-car and in-home use, which should support nat-
ural, flexible interaction and collaborative behavior. The sys-
tem functionalities include playback control, manipulation of
playlists, and searching a large MP3 database. We believe
that in order to achieve this goal, the system needs to provide
advanced adaptive multimodal output.
We are conducting Wizard-of-Oz experiments
[Bernsen et al, 1998] in order to guide the development
of our system. On the one hand, the experiments should
give us data on how the potential users interact with such
an application. But we also need data on the multimodal
interaction strategies that the system should employ to
achieve the desired naturalness, flexibility and collaboration.
We therefore need a setup where the wizard has freedom of
1TALK (Talk and Look: Tools for Ambient Linguistic Knowl-
edge; www.talk-project.org) is funded by the EU as project
No. IST-507802 within the 6th Framework program.
choice w.r.t. their response and its realization through single
or multiple modalities. This makes it different from previous
multimodal experiments, e.g., in the SmartKom project
[Tu?rk, 2001], where the wizard(s) followed a strict script.
But what we need is also different in several aspects from
taking recordings of straight human-human interactions: the
wizard does not hear the user?s input directly, but only gets a
transcription, parts of which are sometimes randomly deleted
(in order to approximate imperfect speech recognition);
the user does not hear the wizard?s spoken output directly
either, as the latter is transcribed and re-synthesized (to
produce system-like sounding output). The interactions
should thus more realistically approximate an interaction
with a system, and thereby contain similar phenomena (cf.
[Duran et al, 2001]).
The wizard should be able to present different screen out-
puts in different context, depending on the search results and
other aspects. However, the wizard cannot design screens on
the fly, because that would take too long. Therefore, we de-
veloped a setup which includes modules that support the wiz-
ard by providing automatically calculated screen output op-
tions the wizard can select from if s/he want to present some
screen output.
Outline In this paper we describe our experiment setup and
the first experiences with it. In Section 2 we overview the
research goals that our setup was designed to address. The
actual setup is presented in detail in Section 3. In Section 4
we describe the collected data, and we summarize the lessons
we learnt on the basis of interviewing the experiment partici-
pants. We briefly discuss possible improvements of the setup
and our future plans with the data in Section 5.
2 Goals of the Multimodal Experiment
Our aim was to gather interactions where the wizard can com-
bine spoken and visual feedback, namely, displaying (com-
plete or partial) results of a database search, and the user can
speak or select on the screen.
Multimodal Presentation Strategies The main aim was to
identify strategies for the screen output, and for the multi-
modal output presentation. In particular, we want to learn
Figure 1: Multimodal Wizard-of-Oz data collection setup for
an in-car music player application, using the Lane Change
driving simulator. Top right: User, Top left: Wizard, Bottom:
transcribers.
when and what content is presented (i) verbally, (ii) graphi-
cally or (iii) by some combination of both modes. We expect
that when both modalities are used, they do not convey the
same content or use the same level of granularity. These are
important questions for multimodal fission and for turn plan-
ning in each modality.
We also plan to investigate how the presentation strategies
influence the responses of the user, in particular w.r.t. what
further criteria the user specifies, and how she conveys them.
Multimodal Clarification Strategies The experiments
should also serve to identify potential strategies for multi-
modal clarification behavior and investigate individual strat-
egy performance. The wizards? behavior will give us an ini-
tial model how to react when faced with several sources of
interpretation uncertainty. In particular we are interested in
what medium the wizard chooses for the clarification request,
what kind of grounding level he addresses, and what ?sever-
ity? he indicates. 2 In order to invoke clarification behavior
we introduced uncertainties on several levels, for example,
multiple matches in the database, lexical ambiguities (e.g., ti-
tles that can be interpreted denoting a song or an album), and
errors on the acoustic level. To simulate non-understanding
on the acoustic level we corrupted some of the user utterances
by randomly deleting parts of them.
3 Experiment Setup
We describe here some of the details of the experiment. The
experimental setup is shown schematically in Figure 1. There
are five people involved in each session of the experiment: an
experiment leader, two transcribers, a user and a wizard.
The wizards play the role of an MP3 player application
and are given access to a database of information (but not
actual music) of more than 150,000 music albums (almost 1
2Severity describes the number of hypotheses indicated by the
wizard: having no interpretation, an uncertain interpretation, or sev-
eral ambiguous interpretations.
Figure 2: Screenshot from the FreeDB-based database appli-
cation, as seen by the wizard. First-level of choice what to
display.
million songs), extracted from the FreeDB database.3 Fig-
ure 2 shows an example screen shot of the music database
as it is presented to the wizard. Subjects are given a set of
predefined tasks and are told to accomplish them by using
an MP3 player with a multimodal interface. Tasks include
playing songs/albums and building playlists, where the sub-
ject is given varying amounts of information to help them
find/decide on which song to play or add to the playlist. In
a part of the session the users also get a primary driving task,
using a Lane Change driving simulator [Mattes, 2003]. This
enabled us to test the viability of combining primary and sec-
ondary task in our experiment setup. We also aimed to gain
initial insight regarding the difference in interaction flow un-
der such conditions, particularly with regard to multimodal-
ity.
The wizards can speak freely and display the search result
or the playlist on the screen. The users can also speak as well
as make selections on the screen.
The user?s utterances are immediately transcribed by a typ-
ist and also recorded. The transcription is then presented to
the wizard.4 We did this for two reasons: (1) To deprive
the wizards of information encoded in the intonation of utter-
ances, because our system will not have access to it either. (2)
To be able to corrupt the user input in a controlled way, sim-
ulating understanding problems at the acoustic level. Unlike
[Stuttle et al, 2004], who simulate automatic speech recogni-
tion errors using phone-confusion models, we used a tool that
?deletes? parts of the transcribed utterances, replacing them
by three dots. Word deletion was triggered by the experiment
leader. The word deletion rate varied: 20% of the utterances
got weakly and 20% strongly corrupted. In 60% of the cases
the wizard saw the transcribed speech uncorrupted.
The wizard?s utterances are also transcribed (and recorded)
3Freely available at http://www.freedb.org
4We were not able to use a real speech recognition system, be-
cause we do not have one trained for this domain. This is one of the
purposes the collected data will be used for.
Figure 3: Screenshot from the display presentation tool offer-
ing options for screen output to the wizard for second-level
of choice what to display an how.
and presented to the user via a speech synthesizer. There are
two reasons for doing this: One is to maintain the illusion for
the subjects that they are actually interacting with a system,
since it is known that there are differences between human-
human and human-computer dialogue [Duran et al, 2001],
and we want to elicit behavior in the latter condition; the
other has to do with the fact that synthesized speech is imper-
fect and sometimes difficult to understand, and we wanted to
reproduce this condition.
The transcription is also supported by a typing and spelling
correction module to minimize speech synthesis errors and
thus help maintain the illusion of a working system.
Since it would be impossible for the wizard to construct
layouts for screen output on the fly, he gets support for his
task from the WOZ system: When the wizard performs a
database query, a graphical interface presents him a first level
of output alternatives, as shown in Figure 2. The choices are
found (i) albums, (ii) songs, or (iii) artists. For a second level
of choice, the system automatically computes four possible
screens, as shown in Figure 3. The wizard can chose one of
the offered options to display to the user, or decide to clear
the user?s screen. Otherwise, the user?s screen remains un-
changed. It is therefore up to the wizard to decide whether
to use speech only, display only, or to combine speech and
display.
The types of screen output are (i) a simple text-message
conveying how many results were found, (ii) output of a list
of just the names (of albums, songs or artists) with the cor-
responding number of matches (for songs) or length (for al-
bums), (iii) a table of the complete search results, and (iv) a
table of the complete search results, but only displaying a sub-
set of columns. For each screen output type, the system uses
heuristics based on the search to decide, e.g., which columns
should be displayed. These four screens are presented to the
wizard in different quadrants on a monitor (cf. Figure 3),
allowing for selection with a simple mouse click. The heuris-
tics for the decision what to display implement preliminary
strategies we designed for our system. We are aware that due
to the use of these heuristics, the wizard?s output realization
may not be always ideal. We have collected feedback from
both the wizards and the users in order to evaluate whether
the output options were satisfactory (cf. Section 4 for more
details).
Technical Setup To keep our experimental system modu-
lar and flexible we implemented it on the basis of the Open
Agent Architecture (OAA) [Martin et al, 1999], which is a
framework for integrating a community of software agents in
a distributed environment. Each system module is encapsu-
lated by an OAA wrapper to form an OAA agent, which is
able to communicate with the OAA community. The exper-
imental system consists of 12 agents, all of them written in
Java. We made use of an OAA monitor agent which comes
with the current OAA distribution to trace all communication
events within the system for logging purposes.
The setup ran distributed over six PCs running different
versions of Windows and Linux.5
4 Collected Data and Experience
The SAMMIE-26 corpus collected in this experiment contains
data from 24 different subjects, who each participated in one
session with one of our six wizards. Each subject worked on
four tasks, first two without driving and then two with driving.
The duration was restricted to twice 15 minutes. Tasks were
of two types: searching for a title either in the database or in
an existing playlist, building a playlist satisfying a number of
constraints. Each of the two sets for each subject contained
one task of each type. The tasks again differed in how specific
information was provided. We aimed to keep the difficulty
level constant across users. The interactions were carried out
in German.7
The data for each session consists of a video and audio
recording and a logfile. Besides the transcriptions of the spo-
ken utterances, a number of other features have been anno-
tated automatically in the log files of the experiment, e.g.,
the wizard?s database query and the number of found results,
the type and form of the presentation screen chosen by the
wizard, etc. The gathered logging information for a single
experiment session consists of the communication events in
chronological order, each marked by a timestamp. Based on
this information, we can recapitulate the number of turns and
the specific times that were necessary to accomplish a user
task. We expect to use this data to analyze correlations be-
5We would like to thank our colleagues from CLT Sprachtech-
nologie http://www.clt-st.de/ for helping us to set up the
laboratory.
6SAMMIE stands for Saarbru?cken Multimodal MP3 Player In-
teraction Experiment. We have so far conducted two series of data-
collection experiments: SAMMIE-1 involved only spoken interaction
(cf. [Kruijff-Korbayova? et al, 2005] for more details), SAMMIE-2 is
the multimodal experiment described in this paper.
7However, most of the titles and artist names in the music
database are in English.
tween queries, numbers of results, and spoken and graphical
presentation strategies.
Whenever the wizard made a clarification request, the
experiment leader invoked a questionnaire window on the
screen, where the wizard had to classify his clarification re-
quest according to the primary source of the understanding
problem. At the end of each task, users were asked to what
extent they believed they accomplished their tasks and how
satisfied they were with the results. Similar to methods used
by [Skantze, 2003] and [Williams and Young, 2004], we plan
to include subjective measures of task completion and cor-
rectness of results in our evaluation matrix, as task descrip-
tions can be interpreted differently by different users.
Each subject was interviewed immediately after the ses-
sion. The wizards were interviewed once the whole experi-
ment was over. The interviews were carried out verbally, fol-
lowing a prepared list of questions. We present below some
of the points gathered through these interviews.
Wizard Interviews All 6 wizards rated the overall under-
standing as good, i.e., that communication completed suc-
cessfully. However, they reported difficulties due to delays in
utterance transmission in both directions, which caused un-
necessary repetitions due to unintended turn overlap.
There were differences in how different wizards rated and
used the different screen output options: The table containing
most of the information about the queried song(s) or album(s)
was rated best and shown most often by some wizards, while
others thought it contained too much information and would
not be clear at first glance for the users and hence they used
it less or never. The screen option containing the least infor-
mation in tabular form, namely only a list of songs/albums
with their length, received complementary judgments: some
of the wizards found it useless because it contained too little
information, and they thus did not use it, and others found it
very useful because it would not confuse the user by present-
ing too much information, and they thus used it frequently.
Finally, the screen containing a text message conveying only
the number of matches, if any, has been hardly used by the
wizards. The differences in the wizards? opinions about what
the users would find useful or not clearly indicate the need
for evaluation of the usefulness of the different screen output
options in particular contexts from the users? view point.
When showing screen output, the most common pattern
used by the wizards was to tell the user what was shown (e.g.,
I?ll show you the songs by Prince), and to display the screen.
Some wizards adapted to the user?s requests: if asked to show
something (e.g., Show me the songs by Prince), they would
show it without verbal comments; but if asked a question
(e.g., What songs by Prince are there? or What did you find?),
they would show the screen output and answer in speech.
Concerning the adaptation of multimodal presentation
strategies w.r.t. whether the user was driving or not, four
of the six wizards reported that they consciously used speech
instead of screen output if possible when the user was driving.
The remaining two wizards did not adapt their strategy.
On the whole, interviewing the wizards brought valuable
information on presentation strategies and the use of modal-
ities, but we expect to gain even more insight after the an-
notation and evaluation of the collected data. Besides ob-
servations about the interaction with the users, the wizards
also gave us various suggestions concerning the software used
in the experiment, e.g., the database interface (e.g., the pos-
sibility to decide between strict search and search for par-
tial matches, and fuzzy search looking for items with similar
spelling when no hits are found), the screen options presenter
(e.g., ordering of columns w.r.t. their order in the database in-
terface, the possibility to highlight some of the listed items),
and the speech synthesis system.
Subject Interviews In order to use the wizards? behavior as
a model for interaction design, we need to evaluate the wiz-
ards? strategies. We used user satisfaction, task experience,
and multi-modal feedback behavior as evaluation metrics.
The 24 experimental subjects were all native speakers of
German with good English skills. They were all students
(equally spread across subject areas), half of them male and
half female, and most of them were between 20 to 30 years
old.
In order to calculate user satisfaction, users were inter-
viewed to evaluate the system?s performance with a user sat-
isfaction survey. The survey probed different aspects of the
users? perception of their interaction with the system. We
asked the users to evaluate a set of five core metrics on a
5-point Likert scale. We followed [Walker et al, 2002] def-
inition of the overall user satisfaction as the sum of text-to-
speech synthesis performance, task ease, user expertise, over-
all difficulty and future use. The mean for user satisfaction
across all dialogues was 15.0 (with a standard derivation of
2.9). 8 A one-way ANOVA for user satisfaction between wiz-
ards (df=5, F=1.52 p=0.05) shows no significant difference
across wizards, meaning that the system performance was
judged to be about equally good for all wizards.
To measure task experience we elicited data on perceived
task success and satisfaction on a 5-point Likert scale after
each task was completed. For all the subjects the final per-
ceived task success was 4.4 and task satisfaction 3.9 across
the 4 tasks each subject had to complete. For task success
as well as for task satisfaction no significant variance across
wizards was detected.
Furthermore the subjects were asked about the employed
multi-modal presentation and clarification strategies.
The clarification strategies employed by the wizards
seemed to be successful: From the subjects? point of view,
mutual understanding was very good and the few misunder-
standings could be easily resolved. Nevertheless, in the case
of disambiguation requests and when grounding an utterance,
subjects ask for more display feedback. It is interesting to
note that subjects judged understanding difficulties on higher
levels of interpretation (especially reference resolution prob-
lems and problems with interpreting the intention) to be more
costly than problems on lower levels of understanding (like
the acoustic understanding). For the clarification strategy this
8[Walker et al, 2002] reported an average user satisfaction of
16.2 for 9 Communicator systems.
implies that the system should engage in clarification at the
lowest level a error was detected.9
Multi-modal presentation strategies were perceived to be
helpful in general, having a mean of 3.1 on a 5-point Lik-
ert scale. However, the subjects reported that too much in-
formation was being displayed especially for the tasks with
driving. 85.7% of the subjects reported that the screen out-
put was sometimes distracting them. 76.2% of the sub-
jects would prefer to more verbal feedback, especially while
driving. On a 3-point Likert scale subjects evaluated the
amount of the information presented verbally to be about
right (mean of 1.8), whereas they found the information pre-
sented on the screen to be too much (mean of 2.3). Stud-
ies by [Bernsen and Dybkjaer, 2001] on the appropriateness
of using verbal vs. graphical feedback for in-car dialogues
indicate that the need for text output is very limited. Some
subjects in that study, as well subjects in our study report that
they would prefer to not have to use the display at all while
driving. On the other hand subjects in our study perceived the
screen output to be very helpful in less stressful driving situa-
tions and when not driving (e.g. for memory assistance, clari-
fications etc.). Especially when they want to verify whether a
complex task was finally completed (e.g. building a playlist),
they ask for a displayed proof. For modality selection in in-
car dialogues the driver?s mental workload on primary and
secondary task has to be carefully evaluated with respect to a
situation model.
With respect to multi-modality subjects also asked for
more personalized data presentation. We therefore need to
develop intelligent ways to reduce the amount of data being
displayed. This could build on prior work on the generation
of ?tailored? responses in spoken dialogue according to a user
model [Moore et al, 2004].
The results for multi-modal feedback behavior showed no
significant variations across wizards except for the general
helpfulness of multi-modal strategies. An ANOVA Planned
Comparison of the wizard with the lowest mean against the
other wizards showed that his behavior was significantly
worse. It is interesting to note, that this wizard was using
the display less than the others. We might consider not to in-
clude the 4 sessions with this wizard in our output generation
model.
We also tried to analyze in more detail how the wizards?
presentation strategies influenced the results. The option
which was chosen most of the time was to present a table
with the search results (78.6%); to present a list was only cho-
sen in 17.5% of the cases and text only 0.04%. The wizards?
choices varied significantly only for presenting the table op-
tion. The wizard who was rated lowest for multimodality was
using the table option less, indicating that this option should
be used more often. This is also supported by the fact that the
show table option is the only presentation strategy which is
positively correlated to how the user evaluated multimodality
(Spearman?s r = 0.436*). We also could find a 2-tailed corre-
9Note that engaging at the lowest level just helps to save dialogue
?costs?. Other studies have shown that user satisfaction is higher
for strategies that would ?hide? the understanding error by asking
questions on higher levels [Skantze, 2003], [Raux et al, 2005]
lation between user satisfaction and multimodality judgment
(Spearman?s r = 0.658**). This indicates the importance of
good multimodal presentation strategies for user satisfaction.
Finally, the subjects were asked for own comments. They
liked to be able to provide vague information, e.g., ask for ?an
oldie?, and were expecting collaborative suggestions. They
also appreciated collaborative proposals based on inferences
made from previous conversations.
In sum, as the measures for user satisfaction, task experi-
ence, and multi-modal feedback strategies, the subjects? judg-
ments show a positive trend. The dialogue strategies em-
ployed by most of the wizards seem to be a good starting
point for building a baseline system. Furthermore, the results
indicate that intelligent multi-modal generation needs to be
adaptive to user and situation models.
5 Conclusions and Future Steps
We have presented an experiment setup that enables us to
gather multimodal interaction data aimed at studying not only
the behavior of the users of the simulated system, but also
that of the wizards. In order to simulate a dialogue system in-
teraction, the wizards were only shown transcriptions of the
user utterances, sometimes corrupted, to simulate automatic
speech recognition problems. The wizard?s utterances were
also transcribed and presented to the user through a speech
synthesizer. In order to make it possible for the wizards to
produce contextually varied screen output in real time, we
have included a screen output planning module which auto-
matically calculated several screen output versions every time
the wizard ran a database query. The wizards were free to
speak and/or display screen output. The users were free to
speak or select on the screen. In a part of each session, the
user was occupied by a primary driving task.
The main challenge for an experiment setup as described
here is the considerable delay between user input and wizard
response. This is due partly to the transcription and spelling
correction step and partly due to the time it takes the wizard to
decide on and enter a query to the database, then select a pre-
sentation and in parallel speak to the user. We have yet to ana-
lyze the exact distribution of time needed for these tasks. Sev-
eral ways can be chosen to speed up the process. Transcrip-
tion can be eliminated either by using speech recognition and
dealing with its errors, or instead applying signal processing
software, e.g., to filter out prosodic information from the user
utterance and/or to transform the wizard?s utterance into syn-
thetically sounding speech (e.g., using a vocoder). Database
search can be sped up in a number of ways too, ranging from
allowing selection directly from the transcribed text to auto-
matically preparing default searches by analyzing the user?s
utterance. Note, however, that the latter will most likely prej-
udice the wizard to stick to the proposed search.
We plan to annotate the corpus, most importantly w.r.t.
wizard presentation strategies and context features relevant
for the choice between them. We also plan to compare the
presentation strategies to the strategies in speech-only mode,
for which we collected data in an earlier experiment (cf.
[Kruijff-Korbayova? et al, 2005]).
For clarification strategies previous studies already showed
that the decision process needs to be highly dynamic by tak-
ing into account various features such as interpretation uncer-
tainties and local utility [Paek and Horvitz, 2000]. We plan
to use the wizard data to learn an initial multi-modal clarifi-
cation policy and later on apply reinforcement learning meth-
ods to the problem in order to account for long-term dialogue
goals, such as task success and user satisfaction.
The screen output options used in the experiment will also
be employed in the baseline system we are currently imple-
menting. The challenges involved there are to decide (i) when
to produce screen output, (ii) what (and how) to display and
(iii) what the corresponding speech output should be. We will
analyze the corpus in order to determine what the suitable
strategies are.
References
[Bernsen and Dybkjaer, 2001] Niels Ole Bernsen and Laila
Dybkjaer. Exploring natural interaction in the car. In
CLASS Workshop on Natural Interactivity and Intelligent
Interactive Information Representation, 2001.
[Bernsen et al, 1998] N. O. Bernsen, H. Dybkj?r, and
L. Dybkj?r. Designing Interactive Speech Systems ?
From First Ideas to User Testing. Springer, 1998.
[Duran et al, 2001] Christine Duran, John Aberdeen, Laurie
Damianos, and Lynette Hirschman. Comparing several as-
pects of human-computer and human-human dialogues. In
Proceedings of the 2nd SIGDIAL Workshop on Discourse
and Dialogue, Aalborg, 1-2 September 2001, pages 48?57,
2001.
[Kruijff-Korbayova? et al, 2005] Ivana Kruijff-Korbayova?,
Tilman Becker, Nate Blaylock, Ciprian Gerstenberger,
Michael Kai?er, Peter Poler, Jan Schehl, and Verena
Rieser. Presentation strategies for flexible multimodal
interaction with a music player. In Proceedings of
DIALOR?05 (The 9th workshop on the semantics and
pragmatics of dialogue (SEMDIAL), 2005.
[Martin et al, 1999] D. L. Martin, A. J. Cheyer, and D. B.
Moran. The open agent architecture: A framework for
building distributed software systems. Applied Artificial
Intelligence: An International Journal, 13(1?2):91?128,
Jan?Mar 1999.
[Mattes, 2003] Stefan Mattes. The lane-change-task as a tool
for driver distraction evaluation. In Proceedings of IGfA,
2003.
[Moore et al, 2004] Johanna D. Moore, Mary Ellen Foster,
Oliver Lemon, and Michael White. Generating tailored,
comparative descriptions in spoken dialogue. In Proceed-
ings of the Seventeenth International Florida Artificial In-
telligence Research Sociey Conference, AAAI Press, 2004.
[Paek and Horvitz, 2000] Tim Paek and Eric Horvitz. Con-
versation as action under uncertainty. In Proceedings of
the Sixteenth Conference on Uncertainty in Artificial In-
telligence, 2000.
[Raux et al, 2005] Antoine Raux, Brian Langner, Dan Bo-
hus, Allan W. Black, and Maxine Eskenazi. Let?s go pub-
lic! taking a spoken dialog system to the real world. 2005.
[Skantze, 2003] Gabriel Skantze. Exploring human error
handling strategies: Implications for spoken dialogue sys-
tems. In Proceedings of the ISCA Tutorial and Research
Workshop on Error Handling in Spoken Dialogue Systems,
2003.
[Stuttle et al, 2004] Matthew Stuttle, Jason Williams, and
Steve Young. A framework for dialogue data collection
with a simulated asr channel. In Proceedings of the IC-
SLP, 2004.
[Tu?rk, 2001] Ulrich Tu?rk. The technical processing in
smartkom data collection: a case study. In Proceedings
of Eurospeech2001, Aalborg, Denmark, 2001.
[Walker et al, 2002] Marylin Walker, R. Passonneau, J. Ab-
erdeen, J. Boland, E. Bratt, J. Garofolo, L. Hirschman,
A. Le, S. Lee, S. Narayanan, K. Papineni, B. Pellom,
J. Polifroni, A. Potamianos, P. Prabhu, A. Rudnicky,
G. Sandersa, S. Seneff, D. Stallard, and S. Whittaker.
Cross-site evaluation in darpa communicator: The june
2000 data collection. 2002.
[Williams and Young, 2004] Jason D. Williams and Steve
Young. Characterizing task-oriented dialog using a sim-
ulated asr channel. In Proceedings of the ICSLP, 2004.
The SAMMIE Multimodal Dialogue Corpus Meets the Nite XML Toolkit
Ivana Kruijff-Korbayova?, Verena Rieser,
Ciprian Gerstenberger
Saarland University, Saarbru?cken, Germany
vrieser@coli.uni-sb.de
Jan Schehl, Tilman Becker
DFKI, Saarbru?cken, Germany
jan.schehl@dfki.de
Abstract
We demonstrate work in progress1 us-
ing the Nite XML Toolkit on a cor-
pus of multimodal dialogues with an
MP3 player collected in a Wizard-of-Oz
(WOZ) experiments and annotated with
a rich feature set at several layers. We
designed an NXT data model, converted
experiment log file data and manual tran-
scriptions into NXT, and are building an-
notation tools using NXT libraries.
1 Introduction
In the TALK project2 we are developing a mul-
timodal dialogue system for an MP3 application
for in-car and in-home use. The system should
support natural, flexible interaction and collabo-
rative behavior. To achieve this, it needs to pro-
vide advanced adaptive multimodal output.
To determine the interaction strategies and
range of linguistic behavior naturally occurring
in this scenario, we conducted two WOZ exper-
iments: SAMMIE-1 involved only spoken inter-
action, SAMMIE-2 was multimodal, with speech
and screen input and output.3
We have been annotating the corpus on sev-
eral layers, representing linguistic, multimodal
and context information. The annotated corpus
will be used (i) to investigate various aspects of
1Our demonstration results from the efforts of a larger
team including also N. Blaylock, B. Fromkorth, M. Gra?c,
M. Kai?er, A. Moos, P. Poller and M. Wirth.
2TALK (Talk and Look: Tools for Ambient Linguis-
tic Knowledge; http://www.talk-project.org), funded by the
EU 6th Framework Program, project No. IST-507802.
3SAMMIE stands for Saarbru?cken Multimodal MP3
Player Interaction Experiment.
multimodal presentation and interaction strate-
gies both within and across the annotation lay-
ers; (ii) to design an initial policy for reinforce-
ment learning of multimodal clarifications.4 We
use the Nite XML Toolkit (NXT) (Carletta et al,
2003) to represent and browse the data and to de-
velop annotation tools.
Below we briefly describe our experiment
setup, the collected data and the annotation lay-
ers; we comment on methods and tools for data
representation and annotation, and then present
our NXT data model.
2 Experiment Setup
24 subjects in SAMMIE-1 and 35 in SAMMIE-2
performed several tasks with an MP3 player ap-
plication simulated by a wizard. For SAMMIE-
1 we had two, for SAMMIE-2 six wizards. The
tasks involved searching for titles and building
playlists satisfying various constraints. Each ses-
sion was 30 minutes long. Both users and wiz-
ards could speak freely. The interactions were
in German (although most of the titles and artist
names in the database were English).
SAMMIE-2 had a more complex setup. The
tasks the subjects had to fulfill were divided in
two classes: with vs. without operating a driv-
ing simulator. When presenting the search re-
sults, the wizards were free to produce mono-
or multimodal output as they saw fit; they could
speak freely and/or select one of four automati-
cally generated screen outputs, which contained
tables and lists of found songs/albums. The
users also had free choice between unconstrained
4See (Kruijff-Korbayova? et al, 2006) for more details
about the annotation goals and further usage of the corpus.
69
natural language and/or selecting items on the
screen. Both wizard and user utterances were im-
mediately transcribed. The wizard?s utterances
were presented to the user via a speech synthe-
sizer. To simulate acoustic understanding prob-
lems, the wizard sometimes received only part
of the transcribed user?s utterance, to elicit CRs.
(See (Kruijff-Korbayova? et al, 2005) for details.)
3 Collected Data
The SAMMIE-2 data for each session consists of
a video and audio recording and a log file.5 The
gathered logging information per session con-
sists of Open Agent Architecture (Martin et al,
1999) (OAA) messages in chronological order,
each marked by a timestamp. The log files con-
tain various information, e.g., the transcriptions
of the spoken utterances, the wizard?s database
query and the number of results, the screen op-
tion chosen by the wizard, classification of clari-
fication requests (CRs), etc.
4 Annotation Methods and Tools
The rich set of features we are interested in nat-
urally gives rise to a multi-layered view of the
corpus, where each layer is to be annotated inde-
pendently, but subsequent investigations involve
exploration and automatic processing of the inte-
grated data across layers.
There are two crucial technical requirements
that must be satisfied to make this possible: (i)
stand-off annotation at each layer and (ii) align-
ment of base data across layers. Without the for-
mer, we could not keep the layers separate, with-
out the latter we would not be able to align the
separate layers. An additional equally important
requirement is that elements at different layers
of annotation should be allowed to have overlap-
ping spans; this is crucial because, e.g., prosodic
units and syntactic phrases need not coincide.
Among the existing toolkits that support
multi-layer annotation, it was decided to use
NXT (Carletta et al, 2003)6 in the TALK
project. The NXT-based SAMMIE-2 corpus we
5For 19 sessions the full set of data files exists.
6http://www.ltg.ed.ac.uk/NITE/
are demonstrating has been created in several
steps: (1) The speech data was manually tran-
scribed using the Transcriber tool.7 (2) We auto-
matically extracted features at various annotation
layers by parsing the OAA messages in the log
files. (3) We automatically converted the tran-
scriptions and the information from the log files
into our NXT-based data representation format;
features annotated in the transcriptions and fea-
tures automatically extracted from the log files
were assigned to elements at the appropriate lay-
ers of representation in this step.
Manual annotation: We use tools specifi-
cally designed to support the particular annota-
tion tasks. We describe them below.
As already mentioned, we used Transcriber for
the manual transcriptions. We also performed
certain relatively simple annotations directly on
the transcriptions and coded them in-line by us-
ing special notation. This includes the identifica-
tion of self-speech, the identification of expres-
sions referring to domain objects (e.g., songs,
artists and albums) and the identification of utter-
ances that convey the results of database queries.
For other manual annotation tasks (the annota-
tion of CRs, task segmentation and completion,
referring expressions and the relations between
them) we have been building specialized tools
based on the NXT library of routines for build-
ing displays and interfaces based on Java Swing
(Carletta et al, 2003). Although NXT comes
with a number of example applications, these are
tightly coupled with the architecture of the cor-
pora they were built for. We therefore developed
a core basic tool for our own corpus; we mod-
ify this tool to suite each annotation task. To fa-
cilitate tool development, NXT provides GUI el-
ements linked directly to corpora elements and
support for handling complex multi-layer cor-
pora. This proved very helpful.
Figure 4 shows a screenshot of our CR anno-
tation tool. It allows one to select an utterance
in the left-hand side of the display by clicking
on it, and then choose the attribute values from
the pop-down lists on the right-hand side. Cre-
7http://trans.sourceforge.net/
70
ating relations between elements and creating el-
ements on top of other elements (e.g., words or
utterances) are extensions we are currently im-
plementing (and will complete by the time of the
workshop). First experiences using the tool to
identify CRs are promising.8 When demonstrat-
ing the system we will report the reliability of
other manual annotation tasks.
Automatic annotation using indexing: NXT
also provides a facility for automatic annotation
based on NiteQL query matches (Carletta et al,
2003). Some of our features, e.g., the dialogue
history ones, can be easily derived via queries.
5 The SAMMIE NXT Data Model
NXT uses a stand-off XML data format that con-
sist of several XML files that point to each other.
The NXT data model is a multi-rooted tree with
arbitrary graph structure. Each node has one set
of children, and can have multiple parents.
Our corpus consists of the following layers.
Two base layers: words and graphical output
events; both are time-aligned. On top of these,
structural layers correspond to one session per
subject, divided into task sections, which con-
sist of turns, and these consist of individual ut-
terances, containing words. Graphical output
events will be linked to turns at a featural layer.
Further structural layers are defined for CRs
and dialogue acts (units are utterances), domain
objects and discourse entities (units are expres-
sions consisting of words). We keep independent
layers of annotation separate, even when they can
in principle be merged into a single hierarchy.
Figure 2 shows a screenshot made with Ami-
gram (Lauer et al, 2005), a generic tool for
browsing and searching NXT data. On the left-
hand side one can see the dependencies between
the layers. The elements at the respective layers
are displayed on the right-hand side.
Below we indicate the features per layer:
? Words: Time-stamped words and other
sounds; we mark self-speech, pronuncia-
tion, deletion status, lemma and POS.
8Inter-annotator agreement of 0.788 (? corrected for
prevalence).
? Graphical output: The type and amount of
information displayed, the option selected
by the wizard, and the user?s choices.
? Utterances: Error rates due to word dele-
tion, and various features describing the
syntactic structure, e.g., mood, polarity,
diathesis, complexity and taxis, the pres-
ence of marked syntactic constructions such
as ellipsis, fronting, extraposition, cleft, etc.
? Turns: Time delay, dialogue duration so
far, and other dialogue history features, i.e.
values which accumulate over time.
? Domain objects and discourse entities:
Properties of referring expressions reflect-
ing the type and information status of dis-
course entities, and coreference/bridging
links between them.
? Dialogue acts: DAs based on an agent-
based approach to dialogue as collaborative
problem-solving (Blaylock et al, 2003),
e.g., determining joint objectives, find-
ing and instantiating recipes to accomplish
them, executing recipes and monitoring for
success. We also annotate propositional
content and the database queries.
? CRs: Additional features including the
source and degree of uncertainty, and char-
acteristics of the CRs strategy.
? Tasks: A set of features for estimating user
satisfaction online for reinforcement learn-
ing (Rieser et al, 2005).
? Session: Subject and wizard information,
user questionnaire aswers, and accumulat-
ing attribute values from other layers.
6 Summary
We described a multi-layered corpus of multi-
modal dialogues represented and annotated us-
ing NXT-based tools. Our data model relates lin-
guistic and graphical realization to a rich set of
context features and represents structural, hierar-
chical interactions between different annotation
layers. We combined different annotation meth-
ods to construct the corpus. Manual annotation
and annotation evaluation is on-going. The cor-
pus will be used (i) investigate multimodal pre-
sentation and interaction strategies with respect
71
Figure 1: NXT-based tool for annotating CRs
Figure 2: SAMMIE-2 corpus displayed in Amigram
to dialogue context and (ii) to design an initial
policy for reinforcement learning of multimodal
clarification strategies.
References
[Blaylock et al2003] N. Blaylock, J. Allen, and G. Fergu-
son. 2003. Managing communicative intentions with
collaborative problem solving. In Current and New
Directions in Discourse and Dialogue, pages 63?84.
Kluwer, Dordrecht.
[Carletta et al2003] J. Carletta, S. Evert, U. Heid, J. Kil-
gour, J. Robertson, and H. Voormann. 2003. The NITE
XML Toolkit: flexible annotation for multi-modal lan-
guage data. Behavior Research Methods, Instruments,
and Computers, special issue on Measuring Behavior.
Submitted.
[Kruijff-Korbayova? et al2005] I. Kruijff-Korbayova?,
T. Becker, N. Blaylock, C. Gerstenberger, M. Kai?er,
P. Poller, J. Schehl, and V. Rieser. 2005. An experiment
setup for collecting data for adaptive output planning in
a multimodal dialogue system. In Proc. of ENLG.
[Kruijff-Korbayova? et al2006] I. Kruijff-Korbayova?,
T. Becker, N. Blaylock, C. Gerstenberger, M. Kai?er,
P. Poller, V. Rieser, and J. Schehl. 2006. The SAMMIE
corpus of multimodal dialogues with an mp3 player. In
Proc. of LREC (to appear).
[Lauer et al2005] C. Lauer, J. Frey, B. Lang, T. Becker,
T. Kleinbauer, and J. Alexandersson. 2005. Amigram
- a general-purpose tool for multimodal corpus annota-
tion. In Proc. of MLMI.
[Martin et al1999] D. L. Martin, A. J. Cheyer, and D. B.
Moran. 1999. The open agent architecture: A frame-
work for building distributed software systems. Applied
Artificial Intelligence: An International Journal, 13(1?
2):91?128, Jan?Mar.
[Rieser et al2005] V. Rieser, I. Kruijff-Korbayova?, and
O. Lemon. 2005. A corpus collection and annotation
framework for learning multimodal clarification strate-
gies. In Proc. of SIGdial.
72
Generation of Output Style Variation in the SAMMIE Dialogue System
Ivana Kruijff-Korbayova?, Ciprian Gerstenberger
Olga Kukina
Saarland University, Germany
{korbay|gerstenb|olgak}@coli.uni-sb.de
Jan Schehl
DFKI, Germany
jan.schehl@dfki.de
Abstract
A dialogue system can present itself and/or
address the user as an active agent by means
of linguistic constructions in personal style, or
suppress agentivity by using impersonal style.
We describe how we generate and control per-
sonal and impersonal style variation in the out-
put of SAMMIE, a multimodal in-car dialogue
system for an MP3 player. We carried out an
experiment to compare subjective evaluation
judgments and input style alignment behavior
of users interacting with versions of the sys-
tem generating output in personal vs. imper-
sonal style. Although our results are consis-
tent with earlier findings obtained with simu-
lated systems, the effects are weaker.
1 Introduction
One of the goals in developing dialogue systems that
users find appealing and natural is to endow the sys-
tems with contextually appropriate output. This en-
compasses a broad range of research issues. Our
present contribution concerns the generation of per-
sonal and impersonal style.
We define the personal/impersonal style di-
chotomy as reflecting primarily a distinction with
respect to agentivity: personal style involves the ex-
plicit realization of an agent, whereas impersonal
style avoids it. In the simplest way this is mani-
fested by the presence of explicit reference to the di-
alogue participants (typically by means of personal
pronouns) vs. its absence, respectively. More gen-
erally, active voice and finite verb forms are typical
for personal style, whereas impersonal style often,
though not exclusively, employs passive construc-
tions or infinite verb forms:
(1) Typical personal style constructions:
a. I found 20 albums.
b. You have 20 albums.
c. Please search for albums by The Beatles.
(2) Typical impersonal style constructions:
a. 20 albums have been found.
b. There are 20 albums.
c. The database contains 20 albums.
d. 20 albums found.
The dialogue system SAMMIE developed in the
TALK project uses either personal or impersonal out-
put style, employing constructions such as (1a?1c)
and (2a?2d), respectively, to manifest its own and
the user?s agentivity linguistically. We ran an ex-
periment to assess the effects of the system output
style on users? judgments of the system?s usability
and performance and on their input formulation.
In Section 2 we review related work on system
output adaptation and previous experiments con-
cerning the effect of system output style on users?
judgments and style. We describe the SAMMIE sys-
tem and the generation of style variation in Sec-
tion 3. In Section 4 we describe our experiment and
in Section 5 present the results. In Section 6 we pro-
vide a discussion and conclusions.
2 Previous Work
Although recently developed dialogue systems
adapt their output to the users in various ways, this
129
usually concerns content selection rather than sur-
face realization. There is to our knowledge no sys-
tem that varies the style of its output in the in-
terpersonal dimension as we have done in SAM-
MIE. Work on animated conversational agents has
addressed various issues concerning agents display-
ing their personality, but this usually concerns emo-
tional states and personality traits, rather than the
personal/impersonal alteration. (Isard et al, 2006)
model personality and alignment in generated dia-
logues between pairs of agents using OpenCCG and
an over-generation and ranking approach, guided by
a set of language models. Their approach probably
could produce the personal/impersonal style varia-
tion as an effect of personality or a side-effect of
syntactic alignment.
The question whether a system should generate
output in personal or impersonal style has been ad-
dressed by (Nass and Brave, 2005): They observe
that agents that use ?I? are generally perceived more
like a person than those that do not. However, sys-
tems tend to be more positively rated when consis-
tent with respect to such parameters as personality,
gender, ontology (human vs. machine), etc. On
the basis of an investigation of a range of user atti-
tudes to their simulated system with a synthetic vs. a
recorded voice, they conclude that a recorded voice
system is perceived as more human-like and thus en-
titled to use ?I?, whereas a synthetic-voice system is
not perceived as human enough to use ?I? to refer to
itself (Nass et al, 2006).
Another question is whether system output style
influences users? input formulation, as would be ex-
pected due to the phenomenon of alignment, which
is generally considered a basic principle in natural
language dialogue (Garrod and Pickering, 2004).1
Experiments targeting human-human conversa-
tion show that speakers in spontaneous dialogues
tend to express themselves in similar ways at lexi-
cal and syntactic levels (e.g., (Hadelich et al, 2004;
Garrod and Pickering, 2004). Lexical and syntactic
alignment is present in human-computer interaction,
too. (Brennan, 1996) suggested that users adopt
system?s terms to avoid errors, expecting the sys-
1This dialogue phenomenon goes under a variety of terms in
the literature, besides alignment, e.g., accommodation, adapta-
tion, convergence, entrainment or shaping (used, e.g., by (Bren-
nan and Ohaeri, 1994)).
tem to be inflexible. However, recent experiments
show that alignment in human-computer interaction
is also automatic and its strength is comparable to
that in human-human communication (Branigan et
al., 2003; Pearson et al, 2006).
Early results concerning users? alignment to sys-
tem output style in the interpersonal dimension are
reported in (Brennan and Ohaeri, 1994): They dis-
tinguish three styles: anthropomorphic (the system
refers to itself using first person pronouns, like in
(1a) above, fluent (complete sentences, but no self-
reference) and telegraphic, like (2d). They found no
difference in users? perception of the system?s in-
telligence across the different conditions. However,
they observed that the anthropomorphic group was
more than twice as likely to refer to the computer
using the second person pronoun ?you? and it used
more indirect requests and conventional politeness
than the other groups. They conclude that the an-
thropomorphic style is undesirable for dialogue sys-
tems because it encourages more complex user input
which is harder to recognize and interpret.
The described experiments used either the
Wizard-of-Oz paradigm (Brennan and Ohaeri, 1994)
or preprogrammed system output (Branigan et al,
2003; Nass and Brave, 2005) and involved written
communication. Such methods allow one to test as-
sumptions about idealized human-computer interac-
tion. Experimenting with the SAMMIE system al-
lows us to test whether similar effects arise in an in-
teraction with an actual dialogue system, which is
plagued, among other factors, by speech recognition
problems.
3 The SAMMIE System
SAMMIE is a multimodal dialogue system developed
in the TALK project with particular emphasis on mul-
timodal turn-planning and natural language genera-
tion to support intuitive mixed-initiative interaction.
The SAMMIE system provides a multimodal in-
terface to an in-car MP3 player through speech and
haptic input with a BMW iDrive input device, a but-
ton which can be turned, pushed down and sideways
in four directions. System output is by speech and a
graphical display integrated into the car?s dashboard.
SAMMIE has a German and an English version with
the same functionality.
130
The MP3 player application offers a wide range
of tasks: The user can control the currently playing
song, search and browse by looking for fields in the
MP3 database (song, artist, album, etc.), search and
select playlists and construct and edit them. A sam-
ple interaction is shown below (Becker et al, 2006).
(3) U: Show me the Beatles albums.
S: I have these four Beatles albums. [shows a list
of album names]
U: Which songs are on this one? [selects the Red
Album]
S: The Red Album contains these songs [shows a
list of the songs]
U: Play the third one.
S: [song ?From Me To You? plays]
The system puts the user in control of the inter-
action. Input can be given through any modality
and is not restricted to answers to system queries.
On the contrary, the user can provide new tasks as
well as any information relevant to the current task
at any time. This is achieved through modeling the
interaction as a collaborative problem solving (CPS)
process, modeling the tasks and their progression as
recipes and a multimodal interpretation that fits any
user input into the context of the current task (Blay-
lock and Allen, 2005). To support dialogue flexibil-
ity, we model discourse context, the CPS state and
the driver?s attention state by an enriched informa-
tion state (Kruijff-Korbayova? et al, 2006a).
3.1 System Architecture
The SAMMIE system architecture follows the classi-
cal approach of a pipelined architecture with mul-
timodal fusion and fission modules encapsulating
the dialogue manager (Bunt et al, 2005). Figure 1
shows the modules and their interaction: Modality-
specific recognizers and analysers provide seman-
tically interpreted input to the multimodal fusion
module (interpretation manager in Figure 1), that in-
terprets them in the context of the other modalities
and the current dialog context. The dialogue man-
ager decides on the next system move, based on its
CPS encoded task model, on the current context and
also on the results from calls to the MP3 database.
The multimodal fission component then generates
the system reaction on a modality-dependent level
Figure 1: SAMMIE system architecture.
by selecting the content to present, distributing it ap-
propriately over the available output modalities and
finally co-ordinating and synchronizing the output.
Modality-specific output modules generate spoken
output and an update of the graphical display. All
modules interact with the extended information state
in which all context information is stored.
Many tasks in the SAMMIE system are modeled by
a rule-based approach. Discourse modeling, inter-
pretation management, dialogue management, turn
planning and linguistic planning are all based on
the production rule system PATE (Pfleger, 2004;
Kempe, 2004). For speech recognition, we use Nu-
ance. The spoken output is synthesized with the
Mary TTS (Schro?der and Trouvain, 2003).2
3.2 Generation of Natural Language Output
with Variation
To generate natural language output in SAMMIE, we
developed a template-based generator. It is imple-
mented by a set of sentence planning rules in PATE
to build the templates, and a set of XSLT transforma-
tions for sentence realization, which yield the out-
put strings. German and English output is produced
by accessing different dictionaries in a uniform way.
The output is either plain text, if it is to be displayed
in the graphical user interface (e.g., captions in ta-
bles, written messages to the user) or it is text with
mark-up for speech synthesis using the MaryXML
format (Schro?der and Trouvain, 2003), if it is to be
spoken by a speech synthesizer.
2http://mary.dfki.de/
131
The SAMMIE generator can produce alternative
realizations for a given content that it receives as in-
put from the turn planner. The implemented range
of system output variation involves the following as-
pects, which have been determined by an analysis
of a corpus of dialogues collected in a Wizard-of-
Oz experiment using several wizards who were free
to formulate their responses to the users (Kruijff-
Korbayova? et al, 2006b):
1. Personal vs. impersonal style: Ich habe 3 Lieder ge-
funden (I?ve found three songs) vs. 3 Lieder wurden
gefunden (Three songs have been found);
2. Telegraphic vs. non-telegraphic style: 23 Alben ge-
funden (23 albums found) vs. Ich habe 23 Alben
gefunden (I found 23 albums)
3. Reduced vs. non-reduced referring expressions: der
Song ?Kinder An Die Macht? (the song ?Kinder An
Die Macht?) vs. der Song (the song) vs. ?Kinder
An Die Macht? (?Kinder An Die Macht?);
4. Lexical choice for (quasi-)synonyms: Song vs. Lied
vs. Titel (song vs. track)
5. Presence vs. absence of adverbs/adverbials: Ich
spiele jetzt den Song (I?ll now play the song) vs. Ich
spiele den Song (I?ll play the song).
The generation of alternatives is achieved by con-
ditioning the sentence planning and realization de-
cisions. The system can be set either to use one
style consistently throughout a dialogue, or to align
to the user, i.e., mimic the user?s style on a turn-
by-turn basis. For the purpose of experimenting
with system output variation, the generator supports
three sources of control for the available choices:
(a) global (default) parameter settings (resulting in
no variation); (b) random selection (resulting in ran-
dom variation); (c) contextual information (resulting
in variation based on the dialogue context).
The contextual information used by the genera-
tor to control realization includes (i) the grounding
status of the content to be communicated (e.g., to
decide for vs. against reducing a referring expres-
sion); and (ii) linguistic features extracted from the
recognized user input (e.g., to make the correspond-
ing syntactic and lexical choices in the output).
3.3 Personal/Impersonal Style Variation
The style variation in SAMMIE amounts to varying
between active voice for personal style and passive
voice or the ?es-gibt? (?there is?) construction for
impersonal style whenever applicable, as illustrated
for several typical dialogue moves below (where (i)
always shows the impersonal, and (ii) the personal
version).
(4) Search result:3
i. Es gibt 20 Alben.
There are 20 albums.
ii. Ich habe 20 Alben gefunden.
I found 20 albums.
Sie haben 20 Alben. / Du hast 20 Alben.
You have 20 albums
Wir haben 20 Alben.
We have 20 albums.
(5) Song addition:
i. Der Titel Bittersweet Symphony wurde zu
der Playliste 2 hinzugefu?gt.
The track Bittersweet Symphony has been
added to Playlist 2.
ii. Ich habe den Titel Bittersweet Symphony zu
der Playliste 2 hinzugefu?gt.
I added the track Bittersweer Symphony to
Playlist 2.
(6) Song playback:
i. Der Titel Ma?nner von Herbert Gro?nemeyer
wird gespielt.
The track Ma?nner by Herbert Gro?nemeyer is
playing.
ii. Ich spiele den Titel Ma?nner von Herbert
Gro?nemeyer.
I am playing the track Ma?nner by Herbert
Gro?nemeyer.
(7) Non-understanding:
i. Das wurde leider nicht verstanden.
That has unfortunately not been understood.
ii. Das habe ich leider nicht verstanden.
I have unfortunately not understood that.
(8) Clarification request:
i. Welches von diesen acht Liedern?/Welches
von diesen acht Liedern wird gewu?nscht?
Which of these eight songs? / Which of these
eight songs is desired?
ii. Welches von diesen acht Liedern mo?chtest du
/ mo?chten Sie ho?ren?
Which of these eight songs would you like to
hear?
3When referring to the user, personal style has several vari-
ants which differ in formality (formal and informal address) and
first vs. second person reference.
132
Figure 2: Experiment setup
The personal/impersonal style variation is not ap-
plicable for some dialogue moves, e.g., (9), and for
output in telegraphic style.
(9) Song interpreter:
Der Titel Bongo Girl ist von Nena.
The track Bongo Girl is by Nena.
4 Experiment
In order to assess the effects of style manipulation in
the SAMMIE system, we ran an experiment in simu-
lated driving conditions, comparing two versions of
the system: one consistently using personal and the
other impersonal style output.4 The experiment em-
ployed the German version of SAMMIE. The setup
(see Figure 2), participants, procedure and collected
data are described in detail in (Kruijff-Korbayova?
and Kukina, 2008), and summarized below.
There were 28 participants, all native speakers
of German. We balanced gender and background
when assigning them to the style conditions. The
experiment followed a fixed script for each partici-
pant: welcome, instruction, warm-up driving, 2 trial
and 11 experimental tasks, evaluation questionnaire,
payment and farewell. The participants were in-
structed to use mainly spoken input, although they
could also use the iDrive button. It took them about
40 minutes to complete all the tasks. The tasks in-
volved exploring the contents of a database of about
25 music albums and were of four types: (1) find-
ing some specified title(s); (2) selecting some title(s)
4For the time being we have not evaluated the version of the
system aligning to the user?s style.
satisfying certain constraints; (3) manipulating the
playlists by adding or removing songs and (4) free-
use of the system.
The experimental tasks were presented to each
participant in randomized order apart from the free
use of the system, which was always the last task.
The experimenter (E) repeated each task assignment
twice to the participant, once in personal and once
in impersonal style, as shown in the example below.
(10) E: Bitte frage das System nach den Liedern von
?Pur?. Du willst also wissen welche Lieder von
?Pur? es gibt.
E: Please ask the the system about the songs by
?Pur?. You would like to know which songs by
?Pur? there are.
The questionnaire was based on (Nass and Brave,
2005) and (Mutschler et al, 2007). It contained
questions with a 6-point scale ranging from 1 (low
grade) to 6 (high grade), such as How do you assess
the system in general: technical (1) ? human-like
(6); Communication with the system seemed to you:
boring (1) ? exciting (6); In terms of usability, the
system is: inefficient (1) ?efficient(6).
The recorded dialogues have been transcribed, the
questionnaire responses tabulated. We manually an-
notated the participants? utterances (on average 95
per session) with the following features for further
analysis:
? Construction type:
Personal (+/-) Is the utterance a complete sen-
tence in active voice or imperative form
Impersonal (+/-) Is the utterance expressed
by passive voice, infinite verb form (e.g.,
?Lied abspielen? (lit. ?song play?)), or ex-
pletive ?es-gibt? (?there-is?) construction
Telegraphic (+/-) Is the utterance expressed
by a phrase, e.g., ?weiter? (?next?)
? Personal pronouns: (+/-) Does the utterance
contain a first or second person pronoun
? Politeness marking: (+/-) Does the utterance
contain a politeness marker, such as ?bitte?
(?please?), ?danke? (?thanks?) and verbs in
subjunctive mood (eg. ?ich ha?tte gerne?)
133
5 Results
The results concerning users? attitudes and align-
ment are presented in detail in (Kruijff-Korbayova?
and Kukina, 2008). Here we summarize the signif-
icant findings and provide an additional analysis of
the influence of speech recognition problems.
5.1 Style and Users? Attitudes
The first issue addressed in the experiment was
whether the users have different judgments of the
personal vs. impersonal version of the system. Since
the system used a synthetic voice, the judgments
were expected to be more positive in the impersonal
style condition (Nass and Brave, 2005). Based on
factor analysis performed on attitudinal data from
the user questionnaires we created the six indices
listed below. All indices were meaningful and reli-
able. (A detailed description of the indices including
the contributing factors from the questionnaires can
be found in (Kruijff-Korbayova? and Kukina, 2008).)
1. General satisfaction with the communication
with the system (Cronbach?s ?=0.86)
2. Easiness of communication with the system
(?=0.83)
3. Usability of the system (?=0.76)
4. Clarity of the system?s speech (?=0.88)
5. Perceived ?humanness? of the system (?=0.69)
6. System?s perceived flexibility and creativity
(?=0.78)
We did not find any significant influence of sys-
tem output style on users? attitudes. Only for per-
ceived humanness of the system we found a weak
tendency in the predicted direction (independent
samples test: t(25)=1.64, p=0.06 (one-tailed)), in
line with the earlier observation that an interface that
refers to itself by a personal pronoun is perceived to
be more human-like than one that does not (Nass and
Brave, 2005).
5.2 Style and Alignment
The next issue we investigated was whether the users
formulated their input differently in the personal vs.
impersonal system version. For each dialogue ses-
sion, we calculated the percentage of utterances con-
taining the feature of interest relative to the total
number of user utterances in the session.
In accordance with the expectation based on style
alignment in terms of agentivity, we observed a sig-
nificant difference in the number of personal con-
structions across style conditions (t(19)=1.8, p=0.05
(one-tailed)). But we did not find a significant dif-
ference in the distribution of impersonal construc-
tions. Not surprisingly, there was also no signifi-
cant difference in the distribution of telegraphic con-
structions. An unexpected finding was the higher
proportion of telegraphic constructions than verb-
containing ones within the impersonal style condi-
tion (t(13)=3.5, p<0.001 (one-tailed)). However, no
such difference was found in the personal style con-
dition. Contrary to expectations, we also did not find
any significant effect of style-manipulation on the
number of personal pronouns, nor on the number of
politeness markers.
Since alignment can also be seen as a process
of gradual adjustment among dialogue participants
over time we compared the proportion of personal,
impersonal and telegraphic constructions in the first
and second halves of the conversations for both style
conditions. The only significant effect we found was
a decrease in the number of personal constructions
in the second halves of the impersonal style interac-
tions (t(13)=2.5, p=0.02 (one-tailed)).
5.3 Influence of Speech Recognition Problems
Unlike an interaction in a Wizard-of-Oz simulation
or similar, an interaction with a real system is bound
to suffer from speech recognition problems. There-
fore, we made a post-hoc analysis with respect to
how much speech recognition difficulty the partici-
pants experienced, in terms of the proportion of par-
ticipant utterances not recognized by the system rel-
ative to the total number of participant utterances in
a session.
On average, around 33% of participant utterances
were not understood by the system.5 We classi-
fied the participants into three groups according to
the performance of speech recognition they expe-
rienced: the good group with less than 27% of in-
put not understood (7 participants); the poor group
5This is admittedly rather bad performance, nevertheless it
mostly does not prevent the participants from getting their tasks
successfully completed within a reasonable time, as was shown
in an rigorous usability evaluation of the system in normal driv-
ing conditions (Mutschler et al, 2007).
134
Figure 3: Judgments of the system by the ?good? and ?poor? speech recognition group
with more than 37% of input not uderstood (7 par-
ticipants); the average group (the remaining 14 par-
ticipants).
Speech Recognition and Attitudinal Data We
suspected that speech recognition problems might
be neutralizing a potential influence of style. There-
fore we contrasted the judgments on all six factors
between the good and the poor speech recognition
group (see Figure 3). The ?good? speech recognition
group showed higher satisfaction with the communi-
cation (t(16)=1.9, p=0.04 (one-tailed)) and evaluated
the clarity of the system?s speech better (t(16)= 2.0,
p=0.03 (one-tailed)). The good speech recognition
group also showed a tendency to assess the usabil-
ity and flexibility of the system higher than the poor
speech recognition group (t(16)=1.71, p=0.05 and
t(16)=1.61, p=0.06, respectively (marginally signif-
icant results)). The two groups did not differ with
respect to their judgments of the ease of commu-
nication and perceived humanness of the system
(t(16)=0.45, p=0.66 and t(16)=0.90, p=0.38). These
results are not surprising. They confirm that speech
recognition does have an effect on the user?s percep-
tion of the system.
Speech Recognition and Style Alignment We
also checked post-hoc whether differences in the ex-
perienced speech recognition performance had an
influence on the style employed by the participants,
again in terms of the proportion of utterances with
personal, impersonal and telegraphic constructions,
personal pronouns and politeness marking. How-
ever, we found no significant effect on the linguistic
structure of the participant input across the groups
(politeness marking: F(2)=1.5, p=0.24; all other
Fs<1 (ANOVA)).
6 Discussion and Conclusions
We presented the generation of personal/impersonal
style variation in the SAMMIE multimodal dialogue
system, and the results of an experiment evaluating
the influence of the system output style on the users?
subjective judgments and their formulation of input.
Although our results are not conclusive, they point
at a range of issues for further research.
Regarding users? attitudes to the system, we
found no significant difference among the styles.
This is similar to (Brennan and Ohaeri, 1994) who
found no difference in intelligence attributed to the
system by the users, but it is at odds with the earlier
finding that a synthetic voice interface was judged
to be more useful when avoiding self-reference by
personal pronouns (Nass and Brave, 2005).
Whereas (Brennan and Ohaeri, 1994) used a flight
reservation dialogue system, (Nass and Brave, 2005)
used a phone-based auction system which read out
an introduction and five object descriptions. There
are two points to note: First, the subjects heard
system output that was a read out continuous text
rather than turns in an interaction. This may have
reinforced the activation of particular style features.
Second, the auction task may have sensibilized the
subjects to the distinction between subjective (the
system?s) vs. objective information presentation,
and thus make them more sensitive to whether the
system presents itself as an active agent or not.
Regarding the question whether users align their
style to that of the system, where previous experi-
ments showed strong effects of alignment (Brennan
and Ohaeri, 1994), our experiment shows some ef-
fects, but some of the results are conflicting. On
the one hand, subjects interacting with the personal
style version of the system used more personal con-
structions than those interacting with the impersonal
style version. However, subjects in either condi-
135
tion did not show any significant difference with re-
spect to the use of impersonal constructions or tele-
graphic forms. We also found a higher proportion of
telegraphic constructions than verb-containing ones
within the impersonal style condition, but no such
difference in the personal style. Finally, when we
considered alignment over time, we found no change
in construction use in the personal style, whereas we
found a decrease in the use of personal constructions
in the impersonal style. It is possible that divid-
ing the interactions into three parts and comparing
alignment in the first and the last part might lead to
stronger results.
That there is no difference in the use of tele-
graphic constructions across conditions is not sur-
prising. Being just phrasal sentence fragments, these
constructions are neutral with respect to style. But
why does there seem to be an alignment effect for
personal constructions and not for others? One way
of explaining this is that (some of) the construc-
tions that we counted as impersonal are common in
both styles. Besides their deliberate use as means
to avoid explicit reference to oneself, they also have
their normal, neutral usage, and therefore, some of
the utterances that we classified as impersonal style
may just be neutral formulations, rather than cases
of distancing or ?de-agentivization?. However, we
could not test this hypothesis, because we have not
found a way to reliably distinguish between neutral
and marked, truly impersonal utterances. This is an
issue for future work.
The difference between our results concerning
alignment and those of (Brennan and Ohaeri, 1994)
is not likely to be due to a difference in the degree
of interactivity (as with (Nass and Brave, 2005)).
We now comment on other differences between our
systems, which might have contributed to the differ-
ences in results.
One aspect where we differ concerns our distinc-
tion between personal and impersonal style, both in
the implementation of the SAMMIE system and in
the experiment: We include the presence/absence
of agentivity not only in the system?s reference to
itself (akin to (Nass and Brave, 2005) and (Bren-
nan and Ohaeri, 1994)), but also in addressing the
user. This concept of the personal/impersonal dis-
tinction was inspired by such differences observed
in a study of instructional texts in several languages
(Kruijff et al, 1999), where the latter dimension is
predominant. The present experiment results make
it pertinent that more research into the motives be-
hind expressing or suppressing agentivity in both di-
mensions is needed.
Apart from the linguistic design of the system?s
output, other factors influence users? behavior and
perception of the system, and thus might confound
experiment results, e.g., functionality, design, er-
gonomics, speech synthesis and speech recognition.
A system with synthesized speech should be more
positively rated when it does not refer to itself as an
active agent by personal pronouns (Nass and Brave,
2005). (Brennan and Ohaeri, 1994) used a sys-
tem with written interaction, the SAMMIE system
employs the MARY text-to-speech synthesis system
(Schro?der and Trouvain, 2003) with an MBROLA
diphone synthesiser, which produces an acceptable
though not outstanding output quality. Our post-hoc
analysis showed a tendency towards better judge-
ments of the system by the participants experienc-
ing less speech recognition problems. This is as
expected. We did not find any statistically signif-
icant effect regarding the style-related features we
analyzed. A future experiment should address the
possibility of an interaction between system style
and speech recognition performance as both factors
might be influencing the user simultaneously.
One radical difference between our experiment
and the earlier ones is that the users of the SAMMIE
system are occupied by the driving task, and thus
only have a limited cognitive capacity left for the
interaction with the system. This may make them
less susceptible to the subtleties of style manipula-
tion than would be the case if they were free of other
tasks. A possible future experiment could address
this issue by including a non-driving condition.
Finally, the SAMMIE system has also the style-
alignment mode, where it mimics the user?s style on
turn-to-turn basis. We plan to present experimental
results comparing the alignment-mode with the fixed
personal/impersonal style in a future publication.
Acknowledgments
This work was carried out in the TALK project
(www.talk-project.org) funded by the EU as project
No. IST-507802 within the 6th Framework Program.
136
References
T. Becker, N. Blaylock, C. Gerstenberger, I. Kruijff-
Korbayova?, A. Korthauer, M. Pinkal, M. Pitz, P. Poller,
and J. Schehl. 2006. Natural and intuitive multimodal
dialogue for in-car applications: The SAMMIE system.
In Proceedings of ECAI, PAIS Special section.
N. Blaylock and J. Allen. 2005. A collaborative
problem-solving model of dialogue. In L. Dybkj?r
and W. Minker, editors, Proceedings of the 6th SIGdial
Workshop on Discourse and Dialogue, pages 200?211,
Lisbon, September 2?3.
H. Branigan, M. Pickering, J. Pearson, J. F. McLean, and
C. Nass. 2003. Syntactic alignment between com-
puter and people: the role of belief about mental states.
In Proceedings of the Annual Conference of the Cog-
nitive Science Society.
S. Brennan and J.O. Ohaeri. 1994. Effects of mes-
sage style on user?s attribution toward agents. In Pro-
ceedings of CHI?94 Conference Companion Human
Factors in Computing Systems, pages 281?282. ACM
Press.
S. Brennan. 1996. Lexical entrainment in spontaneous
dialogue. In Proceedings of the International Sympo-
sium on Spoken Dialogue (ISSD-96), pages 41?44.
H. Bunt, M. Kipp, M. Maybury, and W. Wahlster. 2005.
Fusion and coordination for multimodal interactive in-
formation presentation: Roadmap, architecture, tools,
semantics. In O. Stock and M. Zancanaro, editors,
Multimodal Intelligent Information Presentation, vol-
ume 27 of Text, Speech and Language Technology,
pages 325?340. Kluwer Academic.
S. Garrod and M. Pickering. 2004. Why is conversation
so easy? TRENDS in Cognitive Sciences, 8.
K. Hadelich, H. Branigan, M. Pickering, and M. Crocker.
2004. Alignment in dialogue: Effects of feedback
on lexical overlap within and between participants.
In Proceedings of the AMLaP Conference. Aix en
Provence, France.
Amy Isard, Carsten Brockmann, and Jon Oberlander.
2006. Individuality and alignment in generated di-
alogues. In Proceedings of the 4th International
Natural Language Generation Conference (INLG-06),
pages 22?29, Sydney, Australia.
Benjamin Kempe. 2004. PATE a production rule sys-
tem based on activation and typed feature structure ele-
ments. Bachelor Thesis, Saarland University, August.
G.J.M. Kruijff, I. Kruijff-Korbayova?, J. Bateman,
D. Dochev, N. Gromova, T. Hartley, E. Teich,
S. Sharoff, L. Sokolova, and K. Staykova. 1999.
Deliverable TEXS2: Specification of elaborated text
structures. Technical report, AGILE Project, EU
INCO COPERNICUS PL961104.
I. Kruijff-Korbayova? and O. Kukina. 2008. The effect of
dialogue system output style variation on users? eval-
uation judgements and input style. In Proceedings of
SigDial?08, Columbus, Ohio.
I. Kruijff-Korbayova?, G. Amores, N. Blaylock, S. Eric-
sson, G. Pe?rez, K. Georgila, M. Kaisser, S. Larsson,
O. Lemon, P. Mancho?n, and J. Schehl. 2006a. De-
liverable D3.1: Extended information state modeling.
Technical report, TALK Project, EU FP6, IST-507802.
Ivana Kruijff-Korbayova?, Tilman Becker, Nate Blaylock,
Ciprian Gerstenberger, Michael Kaisser, Peter Poller,
Verena Rieser, and Jan Schehl. 2006b. The SAMMIE
corpus of multimodal dialogues with an MP3 player.
In Proceedings of LREC, Genova, Italy.
H. Mutschler, F. Steffens, and A. Korthauer. 2007. De-
liverable D6.4: Final report on multimodal experi-
ments Part I: Evaluation of the SAMMIE system. Tech-
nical report, TALK Project, EU FP6, IST-507802.
C. Nass and S. Brave, 2005. Should voice interfaces say
?I?? Recorded and synthetic voice interfaces? claims
to humanity, chapter 10, pages 113?124. The MIT
Press, Cambridge.
C. Nass, S. Brave, and L. Takayama. 2006. Socializing
consistency: from technical homogeneity to human
epitome. In P. Zhang & D. Galletta (Eds.), Human-
computer interaction in management information sys-
tems: Foundations, pages 373?390. Armonk, NY: M.
E. Sharpe.
J. Pearson, J. Hu, H. Branigan, M. J. Pickering, and C. I.
Nass. 2006. Adaptive language behavior in HCI: how
expectations and beliefs about a system affect users?
word choice. In CHI ?06: Proceedings of the SIGCHI
conference on Human Factors in computing systems,
pages 1177?1180, New York, NY, USA. ACM.
N. Pfleger. 2004. Context based multimodal fusion. In
ICMI ?04: Proceedings of the 6th international confer-
ence on Multimodal interfaces, pages 265?272, New
York, NY, USA. ACM Press.
M. Schro?der and J. Trouvain. 2003. The German text-to-
speech synthesis system MARY: A tool for research,
development and teaching. International Journal of
Speech Technology, 6:365?377.
137
