Local context templates for Chinese constituent boundary 
prediction 
Qiang Zhou 
The State Key Laboratory of Intelligent Technology and Systems 
Dept. o1' Computer Science and technology, 
Tsinghua University, Beijing 100084 
zhouq @ s 1000e.cs.t singhu a.ed u.cn 
Abstract: 
in this paper, we proposed a shallow 
syntactic knowledge description: 
constituent boundary representation a d its 
simple and efficient prediction algorithm, 
based on different local context templates 
learned fiom the annotated corpus. An open 
test on 2780 Chinese real text sentences 
showed the satisfying results: 94%(92%) 
precision for the words with multiple 
(single) boundary tag output. 
llt simplified the complex constituent levels in 
parse trees and only kept the boundary 
information of every word in different 
constituents. Then, we developed a simple and 
efficient constituent boundary prediction 
algorithm, based on different local context 
templates learned flom the annotated corpus. An 
open test on 2780 Chinese real text sentences 
showed the satisfying results: 94%(92%) 
precision for lhe words with multiple (single) 
boundary lag output. 
I. Introduction 
Research on syntactic parsing has been a focus 
in natural anguage processing for a long lime. As 
the developlnent of corpus linguistics, many 
statistics-based parsers were proposed, such as 
Magerman(1995)'s statistical decision tree parser, 
Collins(1996)'s bigram dependency model parser, 
1;/atnaparkhi(1997)'s maximum entropy model 
parser. All of lhem fried to get the complete parse 
trees of the input sentences, based on the 
statistical data extracted l'rom an annotated corpus. 
The besl parsing accuracy of these parsers was 
about 87%. 
Realizing the difficulties o1' complete parsing, 
many researches turned to explore the partial 
parsing techniques. Church(1988) proposed a 
silnple stochastic technique for lecognizing the 
non-recursive base noun phrases in English. 
\;outilaimen(1993) designed an English noun 
phrase recognition tool --~ NPTbol. Abney(1997) 
applied both rule-based and statistics-based 
approaches for parsing chunks in English. Due to 
the advantages of simplicity and robustness, these 
systems can be acted as good preprocessors for 
the further colnplete parsing. 
In tiffs paper, we will introduce our partial 
parsing aPl)roach for the Chinese language. We 
first proposed a shallow syntactic knowledge 
description: constituent boundary representation. 
2. Constituent boundary description 
The constituent boundary representation 
comes fl'om the simplification of the complete 
parse Irees of the senlences. It omits the 
constituenfl levels in parse trees and only keeps 
the boundary information of every word in 
different constituents, i.e. it is at the left boundary, 
right boundary or middle position of a 
constituent. 
l~,vidently, if the input sentence has only one 
parse lree, i.e. without syntactic ambiguity, the 
constituent boundary position of every word in 
the sentence is clear and definite. In the sense, the 
constituent boundary tag indicates the basic 
syntactic structure information in the sentence. 
Separating them frolll the constituent smlcture 
tree and assigning them Io every word in the 
sentence, we can form a special syntactic unit: 
word botmdao, block (WBB). 
Definition: A word boumla O, block is lhe 
combination o1' the word(including part-of-speech 
information) and its constituent boundary tag, i.e. 
wbb~=<%, b~>, where % is the ith word in the 
sentence, b~ can value 0,1,2, which means % is at 
I llereafler, 'constiluent' represents all internal or root 
nodes in a parse tree, i.e. phrase oF sentence tags. In 
our syslem, each consliluen( must consist of two or 
more words{leaf node in parser tree). 
975 
the middle, left-most, or right--most position of a 
constituent respectively. 
In the view of syntactic description capability, 
the WBBs defined above, the chunks defined by 
Abney(1991) and the phrases(i.e, constituents) 
defined in a parse tree have the following 
tealtions: WBBs < chunks < phrases 
Here is an example: 
? The input sentence (10 words): 
(My brother gives him a book.) 
? Its parse tree representation (7 phrases): 
1,,, \[,,~ \[,,~ @~ t'('J ~ '}  \] \[,,4 l,,~ ~(t T \] 
41J~ \[1'6 \[,'~ - -  $ \] -I~ \]1\] O \] 
? l t s  chunk representation (5 chunks): 
? Its constituent boundary tepresentation 
(10 WBBs): <~f~,l> <l'l<J,0> <~'~'A\],2> < 
)(\],1> <T,2> <41g,0> <- ,1> </l<,2> < 
:1~,2> <o ,2> 
The goal of the constituent boundary 
prediction is to assign a suitable boundary tag for 
every word in the sentence. It can provide basic 
information for further syntactic parsing research. 
The following lists some application examples: 
? To develop a statistics-based Chinese 
parser(Zhou 1997) based on tile bracket 
matching principle(Zhou and Huang, 1997). 
? To develop a Chinese maxinmm noun 
phrase identifier(Zhou,Sun a d Huang, 1999). 
? The automatic inference of Chinese 
probabilistic context-five grammar(PCFG) 
(Zhou and throng 1998). 
3.  Loca l  context  templates  
The linguistic intuitions tell us that many local 
contexts may be useful for constituent boundary 
prediction. For example, many function words in 
Chinese have their certain constituent boundary 
position in the sentences, such as, most 
prepositions are at the left boundaries, and the 
aspectual particles ("le", "zhe", "guo") ate at the 
right boundaries. Moreover, seine content words 
also show their pteferential constituent boundary 
positions in a special ocal context, such as most 
adjectives arc at the right boundary in local 
context: "adverb + adjective". 
A tentative idea is how to use such simple 
local context information(including the part-of- 
speech(POS) tags and the number of Chinese 
characters(CN)) todevelop an elTicient automatic 
boundary prediction algorithm. Therefore, we 
defined the following local context templates 
(LCTs): 
1) Unigram POS template: t~, BPFL, 
2) Bigram POS templates: 
, Left restriction: t~_, t~, BI'I;L~ 
? Right restriction:L t~+,, BPFL~ 
3) Trigram POS template: t~_~t~t~+~, BPFL, 
4) Trigram POS+CN template: t~_~+cn~_~ 
t~+cn,~ t~+~+cn~+~, BPFL~ 
In tile above LCTs, t~ is tile POS tag of the ith 
word in the sentence, cn~ is its character number, 
and BPFL~ is the frequency distribution list of its 
diffetent BP(boundary prediction) value(0,1,2) 
under the local context restrictions(I.CR)(the left 
and right word). 
Table 1 Some examples of the local context 
templates 
I ~  Token 
Unigram p, 
39 849 476 
. . . . . . . . . . . . . . . . . . . . . . .  
bigram a n, 
(left) 5 164 2007 
bigram a n, 
(right) 4 2012 160 
Meaning 
A preposition is prior to 
at the constituent left 
boundary in Chinese. 
A noun is prior to at the 
right boundary if its 
previous word is an 
adjective. 
An adjective is prior to 
at llle left boundary if
its nexl word is a noun. 
Trigranl n n u.Jl)l'~, A noun is priorlo at tile 
POS 1 18 1496 right boundary if its 
previous word is a norm 
and its next one is a 
. . . . . . . . . .  /ra_r_tial(De). 
Table shows some examples of LCTs. All 
these templates can be easily acquired fiom the 
Chinese treebanks or (.hme:e ~ " s corpus annotated 
with constituent boundary tags. 
Among these templates, some special ones 
have the following properties: 
a) TIV~ = ~ BPI;L, \[bl),\] >(z, 
b) 3bl),e 10,21, P(bPALCR)=BPFL, \[1%11 7T, > f3 
where the total frequency threshokl o~ and the BP 
probability threshold f~ are set to 3 and 0.95, 
respectively. They are called the proiectcd 
templates (PTs) (i.e. the local context template 
with a projecting BP wflue). 
Based on the different PTs, we can design a 
thtee-stage training procedure to overcome tile 
problem of data sparseness: 
976 
Slage 1 ? I,carn the unigram and bigram 
tClllplales Oil the whole inslallCOS in annolated COl|mS. 
Slago 2 : I,carn lhc Irigraln P()S lonlphltos Oil the 
nonq~rojocled unigraili and bigraiil illSlallCCs (see i1o?l 
section for illOl'O dclailod). 
Slago 3: \[,carll lho lfigfaill PO,q-I-CN Ioinphiles 
oil Iho non-projcclt;d Irigrani P()~ iliSlanccs. 
Therefore, only the useful trigranl templates 
Call be lear|led. 
4. Automat ic  p red ic t ion  a lgor i t lnn  
After getting lhc LCTs, the auloillatic 
prediction algorithm becomes very simple: 1) to 
sot the protecting BPs based on the projected 
LCTs, 2) to select he best l:IPs based on tile lion- 
pro|coted LCTs. Sonic detailed inl~rmation will 
be discussed in the 12fllowing sections. 
4.1 Set tile project ing l ips 
In this stage, tile refercllce seqlielice lo the 
LCTs is : unigfanl ~ I)igralil ~ higranl  POS 
tiigranl I>OS+CN, i.e. l:ronl the rough roslriclion 
Lcrrs to tile tight restriction L(\]Ts. This sequence 
is same with the LCT training procedure. 
The detailed algoritlnn is as follows: 
Input: the position of the/ill word in the sentence. 
Background: the LCTs learned |'1o111 corpus. 
Output: the pro|coting BP of the word - if' fo/lnd; 
- 1 - otherwise. 
Procedure: 
? Gel lhc local context of the iih word. 
Ill I f  its unigran\] tonal)late is a PT, thor| return 
its projecting BP. 
? If its left and right bigram template satisfy 
the following conditions: 
> rE,+ TF,~- Z SmFL, Ijl +Z BPFL, Ijl > a 
> p0,/,,I Lc#O : (BPFL, Ijl + nS'FL. Ijl) / 
(77:, + 77=,< ) > I~ 
thor  returl l  this combined  pro jec t ing  l~P(l)l@ 
? If its trigram POS template is a PT, then 
retul'u its protecting BP. 
? If its trigram POS+CN template is a PT, 
then return its projecting BP. 
4.2 Select the best liPs 
In this stage, tile reference sequence to the 
LCTs is : trigram POS+CN --> trigram POS "-> 
bigram ---7 unigram. It's a backing-off model 
(K.atz, 1987), just like the approach of Collins and 
Brooks(1995) for the prepositional phrase 
allachineilt plot)loin in English. The detailed 
alger|finn is as follows: 
Input: the position of the ith word in lho sentence. 
Background: lho LCTs learned from corpus. 
Output: tile best BP of the word. 
Procedure: 
? Get the local context of tile ith word. 
o For tile kth nlatched lrigram POS+CN 
tonlplatos, i f  77,'x > CZ, lhen rolHrll SelectBestl l l  > 
UHJFL,). 
? |Sor the ruth niatchod loft bigranl and nth 
matched righl: bigrain, 
)~ Gel lho Combined BI 'FL = Blqq,  + IHq<L,, 
lJ" TFc<,,,,I,i,,,',l ,<',,,H ..... > 0, then rol!.lril 
SelectlJestBP(C<mibined Blqq O. 
I For lhe kth nlatched unigram templates, if 
7"/~ > 0, lhen relurn SelectlJestBl~(17PlrLk). 
I 17,olurn l(dol\]nilt is at the loft t)oundary). 
The internal function SelectBeslBP() tries to 
select the best BP based on the fi'equency 
distribution list of different BP wllue in LCTs. It 
has two output modes: I) single-output mode: 
only output the best t3t > with the highest 
fiequency in the LCT; 2) nmltiple-outlmt mode: 
outpul the BPs salisfying the conditions: 
\[/',,,,,-Pt,,.,,\[ < 7, where 7 = 0.2 
5. Exper imental  results 
5.1 Training and test data 
The training data were extracted fl'Olll two 
ditTerent parts of annotated Chinese corpus: 
1) The small Chinese treebank developed 
in Peking University(Zhou, 1996b), which 
consists of the sentences extracted fiom 
two parts of Chinese texts: (a) test set for 
Chinese-Englisla machine transhltion 
systems, (b) Singapore priiaary school 
textbooks. 
2) The test suite treebank being 
developed in Tsinghua University(Zhou 
and Sun, 1999), which consists of about 
10,000 representative Chinese sentences 
extracted from a large-scale Chinese 
bahmced corpus with about 2,000,000 
Chinese characters. 
The test data were extracted from the articles 
of People's Daily and manually annotated with 
977 
correct constituent boundary tags. It was also 
divided into two parts: 
1) The ordinary sentences. 
2) The sentences with keywords for 
conjunction structures (such as the 
conjunctions or special punctuation 
'DunHao'). They can be used to test the 
performance of our prediction algorithm 
on complex conjunction structures. 
Table 2 shows some basic statistics of these 
training and test data. Only the sentences with 
more than one word were used for training and 
testing. 
Table 2 The basic statistics of training and 
test data. (ASL = Average sentence length) 
Trainl 
Train2 
Testl 
Test2 
Sent. Word Char. ASL 
Num. Num. Num. (w/s) 
5573 64426 89492 11.56 
7774 108542 173334 13.96 
2780 68986 108218 24.82 
1071 32358 51169 30.21 
5.2 The learned templates 
After the three-stage l arning procedure, we 
got four kinds of local context emplates. Table 3 
shows their different distribution data, where the 
section 'Type' lists the distribution of different 
kinds of LCTs and the section 'Token' lists the 
distribution of total words(i.e, tokens) covered by 
the LCTs. In the colmnn 'PTs' and 'Ratio', the 
slash '/' was used to separate the PTs with total 
frequency threshold 0 and 3. 
More than 66% words in the training corpus 
can be covered by the unigram and bigram POS 
projected templates. Then only about 1/3 tokens 
will be used for training the trigram templates. 
Although the type distribution of the trigram 
templates hows the tendency of data sparseness 
(more than 70% trigram projected templates with 
total fi'equency less than 3), the useful trigram 
templates (TF>3) still covers about 70% tokens 
learned. Therefore, we can expect hat them can 
play an important role during constituent 
boundary prediction in open test set. 
5.3 Prediction results 
In order to evaluate the performance of the 
constituent boundary prediction algorithm, the 
followiug measures were used: 
1) The cost time(CT) of the kernal 
functions(CPU: Celeron TM 366, RAM: 64M). 
2) Prediction precision(PP) = 
number of words with correct BPs(CortBP) 
total word number (TWN) 
For the words with single BP output, the 
correct condition is: 
Annotated BP = Predicted BP 
For the words with nmltiple BP outputs, the 
correct conditiou is: 
Annotated BP ~ Predicted BP set 
Tile prediction results of the two test sets were 
shown in Table 4 and Table 5, whose first 
columns list the different emplate combinations 
using in the algorithm. In the columns 'CortBP' 
and 'PP', the slash '/' was used to list the 
different results of the single and multiple BP 
outputs. 
After analyzing the experimental results, we 
found: 
1) The POS information in local context is 
very important for constituent boundary 
prediction. After using the bigram and trigram 
POS templates, the prediction accuracy was 
increased by about 9% and 3% respectively. 
But the chmacter number information shows 
lower boundary restriction capability. Their 
application only results in a slight increase of 
precision in single-output mode but a slight 
decrease in lnultiple-output lnode. 
Table 3 Distribution data of different learned LCTs 
LCTs 
l -gram 
2-gram(Left) 
2-gram(Right) 
3-gram (POS) 
3-graln(P+CN) 
Total 
59 
1448 
Type 
PTs(c~=0/3) 
24 
1030/591 
Ratio(tz=0/3) 
40.68 
71.13/40.81 
Total 
171705 
171705 
Token 
PTs(o~=0/3) 
53932 
87027/86339 
Ratio0x=0/3) 
31.41 
50.68 / 50.28 
1440 1008/567 70.00/39.38 171705 99443/98754 57.92/57.51 
3105 2324 ! 713 74.85 / 22.96 50333 24280 / 21982 48.24 / 43.07 
2553 1677 / 287 65.69 / I1.24 19098 5978 / 4079 31.30 / 21.36 
978 
Table 4 Experimental results of the test set 1 
Set the Projecting BPs templates 
used 
I -gram 22408 
+2-gram 46167 
TWN \[ CortBP 
22143 
45285 
53969 
55866 
PP(%) 
98.82 
98.09 
97.56 
97.40 
TWN CortBP 
46555 32876/ 
24374 
22796 16188/ 
17678 
13642 9946/ 
10986 
11603 8168/ 
8955 
+3-gram 55321 
POS 
+3-gram 57360 
P+CN 
Select he best BPs 
PP(%) 
70.62/ 
73.84 
71.01/ 
77.55 
72.90/ 
80.53 
70.40/ 
77.18 
TWN 
Total 
CortBP PP(%) CT 
68963 
68963 
68963 
55019/ 
56517 
61473/ 
62963 
63915/ 
64955 
79.78/ 14/16 
81.95 
89.14/ 11/15 
91.30 
92.68/ 13/I 1 
94.19 
68963 64034/ 92.85/ 
64821 93.99 
11/14 
2) Most of the prediction errors can be 
attributed to the special structures in the 
sentences, uch as col!iunction structures (CSs) 
or collocation structures. I)ue to the long 
distance dependencies among them, it's very 
difficult to assign the conect botmdary lags to 
the words in these structures only according to 
the local context emplates. The lower overall 
precision of the test set 2 (about 2% lower 
than tesl set 1) also indicates the boundary 
prediction difficulties of the conjunction 
structures, because there are more CSs in test 
set 2 than in test set I. 
3) The accuracy of the multiple outlml 
results is about 2% better than the single 
OUtlmt results. But the words with multiple 
boundary tags constitute only about 10% of 
the tolal words predicted. Therefore, the 
multil)le-output mode shows a good trade-off 
between precision and redundancy. It can be 
used as the best preprocessing data for the 
further syntactic parser. 
4) The maximal ratio of the words set by 
prqjected templates can reach 80%. It 
guarantees the higher overall pl'ecisioiL 
Table 5 Experimental results of the test set 2 
5) Tile algoritlml shows high efficiency. It 
can process about 6,000 words per second 
(CPU: Celeron TM 366, RAM: 64M). 
5.4 Compare with other work 
Zhou(1996) proposed a constituent boundary 
prediction algorithm based on hidden Marcov 
model(HMM). The Viterbi algorithm was used to 
find the best boundary path B': 
B' = arg max P (W,  T \[ B )P (B)  
II 
= arg  max I'(CT, Ib,)P(t, iII,,-,) 
i - I  
where the local POS probability l ' (C~ \[ b) was 
computed by backing-off model and the bigram 
parameters:./(/~, t, , b) and.l(b~ , t,, \[i+l)" 
To compare its 19erformance with our 
algorithm, the trigram (POS and POS+CN) 
information was added up to its backing-off 
model. Table 6 and Table 7 show the prediction 
results of lhc HMM-based algorithm, based on the 
same parameters learned from training set 1 and 
2. 
Table 6. Prediction results of the HMM-based 
_ _ I  
Templates Set tile Projecti~ 
Used L TWN CoriBP 
I -gram 9873 
+2-gram 20454 
+3-D'am 24079 
POS 
+3-gram 24866 
P+CN 
PP(%) 
98.57 
97.00 
96.42 
Select ile best BPs 
fWN CortgP I )P(%)II 
22342 15737/1 70.44/ 
6593 74.27 
11273 7856/ 70.44/ 
8607 74.27 
7384 5225/ 70.76/ 
5777 78.24 
6519 4525/ 69.40/ 
4958 76.05 
96.23 
Total 
TWN 
32358 
32358 
32358 
32358 
CortBP I 
25610/ 
26466 
28310/ 
29061 
29304/ 
29856 
29390/ 
29824 
PP(%) 
79. t 5/ 
81.79 
87.49/ 
89.81 
90.56/ 
92.27 
90.83/ 
92.17 
\[\[ CT 
6/5 
4/4 
3/6 
8/6 
979 
algorithm(test set 1) 
TWN CortBP PP(%) Ctime 
2-gram 68963 60908 88.32 144 
+ 3g-POS 68963 63397 91.93 138 
+3g-P+CN 68963 63649 92.29 139 
Table 7. Prediction results of the HMM-based 
algorithm(test set 2) 
+2-gram 
+3g-POS 
+3g-P+CN 
TWN CortBP PP(%) 
32358 27792 85.89 
32358 28918 89.37 
32358 29030 89.72 
Ctime 
68 
7O 
68 
The performance of the LCT-based algorithm 
surpassed the HMM-based algorithm in 
accuracy(about 1%) and efficiency (about 10 
times). 
Another similar work is Sun(1999). The 
difference lies in the definition of the constituent 
boundary tags: he defined them between word 
pair: w; /_? w;~;, not for the word. By using the 
HMM and Viterbi model, his algorithm showed 
the similar performance with Zhou(1996) (using 
bigram POS parameters): 
? Training data : 3051 sentences extracted 
from People's Daily. 
? Test data: I000 sentences. 
? Best precision:86.3% 
6. Conclusions 
The paper proposed a constituent boundary 
prediction algorithm based on local context 
templates. Its characteristics can be summarized 
as follows: 
? The simple definition of the local context 
templates made the training procedure very 
easy. 
? The three-stage training procedure 
guarantees that only the useful trigram 
templates can be learned. Thus, the data 
sparseness problem was partially overcome. 
? The high coverage of different ypes of 
projected templates assures a higher overall 
prediction accuracy. 
? The multiple output mode provides the 
possibility to describe different boundary 
ambiguities. 
? The algorithm runs very fast, surpasses 
the HMM-based algorithm in accuracy and 
efficiency. 
There are a few possible improvement which 
may raise performance flwther. Firstly, some 
lexical-based templates, such as prepositions as 
left restriction, may improve performance further 
- this needs to be investigated. The introduction 
of the automatic identifiers for some special 
structures, such as conjunction structures or 
collocation structures, may reduce the prediction 
errors due to the long distance dependency 
problem. Finally, more training data is ahnost 
certain to improve results. 
Acknowledgements 
The research was supported by National 
Natural Science Foundation of China (NSFC) 
(Grant No. 69903007). 
References 
Abney S. (1991). "Parsing by Chunks", In Robert 
Berwick, Steven Abney and Carol Tenny (eds.) 
Principle-Based Parsing, Kluwer Academic 
Publishers. 
Abney S. (1997). "Part-of-speech Tagging and Partial 
Parsing", Ill Young S. Bloothooft G. (eds.) Corpus- 
based ntethods in language and speech processings, 
118-136. 
Collins M. and Brooks J. (1995) "Prepositional Phrase 
Attachment through a Backing-OIT Model", In 
David Yarowsky & Ken Church(eds.) Proceedings 
of the third workshop on very large corpora, MIT. 
27-38. 
Church K. (1988). "A Stochastic Parts Program and 
Noun Phrase Parser for Unrestricted Text." In: 
Proceedings of Second Conference on Applied 
Natural Language Processing, Austin, Texas, 136- 
143. 
Collins M. J. (1996). "A New Statistical Parser Based 
on Bigram Lexical Dependencies." In Proc. of ACL- 
34, 184-191. 
Katz S. (1987). "Estimation of Probabilities from 
sparse data for the language model component of a 
speech recogniser". IEEE Transactions on ASSP, 
Vol .35, No. 3. 
Magerman. D.M. (1995). "Statistical Decision-Tree 
Models for Parsing", In Proc. o1' ACL-95,276-303. 
Ratnaparkhi A.(1997). '% linear observed time 
statistical parser based on maximum entropy 
models". In Claire Cardie and Ralph 
Weischedel(eds.), Second Conference on EmpMcal 
Methods in Natural Language Proeessing(EMNLP- 
2), Somerset, New Jersey, ACL. 
Sun H. L., Lu Q. and Yu S. W.(1999). "Two-level 
shallow parser for t, nrestricted Chinese text", In 
980 
Changning Huang and Zhendong l)ong (eds.) 
Proceedings of Computalional linguistics, Beijing: 
Tsinghua University press, 280-286. 
Votllilanmn A. (1993). "NPTool, a delector of English 
Noun Phrases." In: Ken Church (ed.) t'roceedings of
lhe Workshop on Very La,ge Corpora: Academic 
and lnduslrial Perspeelives. Columbus, Ohio, USA, 
48-57. 
Zhou Q. (1996a). '% Model for Automalic Prediction 
of (;hinese l'hrase Boundary I,ocation", 
Zhou Q. (1996b). Phrase Bracketing and Annotating 
on Chinese l,anguage Corpus . l~h.l), l)issertalion, 
Peking University. 
Zhou Q. (1997) "A Statistics-Based Chinese Parser", 
In l;roc, of lhe Fiflh Wol'kshol~ on Very I,arge 
Corpora, 4-15. 
Zhou Q. and l\]uang C.N. (1997) "A Chincse syntaclic 
parser based on bracket malehing principle", 
Communication f COIdPS, 7(2), #97008. 
Zhotl Q. alld Htlang C.N. (1998). "An hll'crence 
Approach for Chinese Probabilistic Con/exl-Free 
Gramnmr", Chinese .Iournal of Computers, 21(5), 
385-392. 
Zhou Q. and Sun M.S. (1999). "P, uiM a Chinese 
Trecbank as the lest suite for Chinese parser", In 
Key-Sun Choi & Young-Soog Chae(cds.) 
Proceedings of the workshop MAI,'99, Beijing. 32- 
37. 
Zhou (,)., Sun M.S. and ltuallg C.N.(1999) 
"Attlonmlically Identify Chinese Maximal Noun 
Phrases", Technical Report 99001, Slate Key l~ab. o1' 
Intelligent Technology and Syslcm,% l)epl, of 
Comlmter Science and Technology, Tsinghua 
University. 
981 
Automatic rule acquisition for Chinese intra-chunk relations 
 
Qiang Zhou 
Center for Speech and Language Technologies, Division of Technical Innovation and Development 
Tsinghua National Laboratory for Information Science and Technology 
Tsinghua University, Beijing 100084, P. R. China 
zq-lxd@mail.tsinghua.edu.cn 
 
 
 
Abstract 
 
Multiword chunking is defined as a task to 
automatically analyze the external function 
and internal structure of the multiword 
chunk(MWC) in a sentence. To deal with 
this problem, we proposed a rule acquisition 
algorithm to automatically learn a chunk 
rule base, under the support of a large scale 
annotated corpus and a lexical knowledge 
base. We also proposed an expectation 
precision index to objectively evaluate the 
descriptive capabilities of the refined rule 
base. Some experimental results indicate 
that the algorithm can acquire about 9% 
useful expanded rules to cover 86% 
annotated positive examples, and improve 
the expectation precision from 51% to 83%. 
These rules can be used to build an efficient 
rule-based Chinese MWC parser. 
1 Introduction 
In recent years, the chunking problem has 
become a hot topic in the communities of natural 
language processing. From 2000 to 2005, several 
different chunking-related tasks, such as text 
chunking (Sang and Buchholz, 2000), clause 
identification (Sang and Dejean, 2001), semantic 
role labeling (Carreras and Marquez, 2005), were 
defined in the CoNLL conferences. Much research 
has been devoted to the problem through different 
points of view. 
Many computational linguists regard chunking 
as a shallow parsing technique. Due to its 
efficiency and robustness on non-restricted texts, 
it has become an interesting alternative to full 
parsing in many NLP applications. On the base of 
the chunk scheme proposed by Abney (1991) and 
the BIO tagging system proposed in Ramshaw and 
Marcus(1995), many machine learning techniques 
are used to deal with the problem. However, 
almost all the chunking systems focus on the 
recognition of non-overlapping cores of chunks till 
now, none of them care about the internal 
structure analysis of chunks. 
In our opinion, the internal structure of a chunk, 
including its head and the dependency relation 
between head and other components, plays an 
important role for semantic content understanding 
for the chunk. They are especially useful for the 
languages with few morphological inflections, 
such as the Chinese language. Therefore, we 
design a multiword chunking task to recognize 
different multiword chunks (MWCs) with the 
detailed descriptions of external function and 
internal structure in real texts. Its main difficulty 
lies in the preciously identification of different 
lexical relationships among the MWC components. 
Some detailed lexical semantic knowledge is 
required in the task.  
To deal with this problem, we proposed a rule 
acquisition algorithm to automatically learn a 
MWC rule base, under the support of a large scale 
annotated corpus and a lexical knowledge base. 
We also proposed an expectation precision index 
to evaluate the descriptive capabilities of the 
refined rule base. Some experimental results 
indicate that our current algorithm can acquire 
about 9% useful expanded rules to cover 86% 
annotated positive examples, and improve the 
expectation precision from 51% to 83%.  
2 Multiword chunking task 
Informally, a MWC is a chunk with two or 
more words, where each word links to a semantic 
head through different dependency relations. Four 
syntactic dependency relationships are used in the 
paper: (1) Modifier-Head relation, (2) Predicate- 
601
Object relation,(3) Predicate-Compliment relation, 
(4) Coordinate relation. They can determinate the 
following functional position tags for each word in 
a MWC: (1) M--Modifier; (2) H--Head; (3) 
P--Predicate; (4) O--Object; (5) C--Compliment; 
(6) J--Coordinate constituent. Based on them, we 
define three topological constructions as follows: 
(1) Left-Corner-Centre (LCC) construction  
All the words in a chunk link to the left-corner 
head and form a left-head dependency structure. 
Its basic pattern is: H C1 ? Cn. The typical 
dependencies among them are Predicate-Object or 
Predicate-Compliment relations: C1?H, ? , 
Cn?H. They form the following functional tag 
serial : P [C|O]. 
(2) Right-Corner-Centre (RCC) construction 
All the words in a chunk link to the 
right-corner head and form a right-head 
dependency structure. Its basic pattern is: A1 ? An 
H. The typical dependencies among them are 
Modifier-Head relations: A1?H, ? , An?H. 
They form the following functional tag serial : 
{M}+ H. 
(3) Chain Hooking (CH) construction 
Each word in a chunk links to its right- 
adjacent word. All of them form a multi-head 
hooking chain. Its basic pattern is: H0 H1 ? Hn, 
where Hi, i?[1,n-1] is the chain head in differnt 
levels, Hn is the semantic head of the overall chunk. 
The typical dependencies among them are 
Modifier-Head or Coordinate relations : H0? 
H1, ? , Hn-1?H n. They form the following 
functional tag serial : {J}* or [M|J] {K|J}* H, where 
K represents the internal chain head. 
We think the above three constructions can 
cover almost all important syntactic relations in 
real text sentences. Now, we can give a formal 
definition for a multiword chunk. 
Definition: two or more words can form a 
multiword chunk if and only if it has one of the 
above three internal topological constructions.  
The MWC definition builds the one-to-one 
corresponding between the word serials with 
different function tags and their dependency 
structure. So we can easily describe some MWCs 
with complex nested structures. In the paper, we 
add a further restriction that each MWC can only 
comprise the content words, such as nouns, verbs, 
adjectives, etc. This restriction can make us focus 
on the analysis of the basic content description 
units in a sentence. 
Each MWC is assigned two tags to describe its 
external function and internal structure. For 
example, a ?np-ZX? MWC represents a noun 
chunk with internal modifier-head relationship. 
Table 1 lists all the function and relation tags used 
in our MWC system. The np, mp, tp, sp form as 
the nominal chunk set. Their typical relation tags 
are ZX, LN and LH. The vp and ap form as the 
predicate chunk set. Their typical relation tags are 
ZX, PO, SB and LH. 
F-tags Descriptions R-tags Descriptions 
np noun chunk ZX modifier-head 
relationship 
vp verb chunk PO verb-object 
relationship 
ap adjective 
chunk 
SB verb-compliment 
relationship 
mp quantity 
chunk  
LH Coordinate 
relationship 
sp space chunk LN chain hooking 
relationship 
tp time chunk   
Table 1 Function and relation tags of MWCs 
The following is a MWC annotated sentence: 
[tp-ZX ??/t(long time) ??/f(since) ] ?/w 
?/r(he) ?/p(for) ??/v(safeguard) [np-ZX ?
?/n (world) ??/n(peace) ] ?/u [np-ZX ??
/a(lofty) ?? /n(undertaking)] [vp-PO ?? /v 
(devote) ??/n (painstaking)] ?/w ??/v(make) 
? /u ? ? /a(outstanding) ? /u ? ? /v 
(contribution)  ?/w1 (For a long time past, he has 
devoted all his energy into the lofty undertaking to 
safeguard world peace and made a outstanding 
contribution.)                           (1) 
There are four MWCs in the sentence. From 
which, we can easily extract the positive and 
negative examples for a MWC rule. For example, 
in the sentence, we can extract a positive example: 
?? /v (devote) ?? /n (painstaking), and a 
negative example: ?? /v(safeguard) ?? /n 
(world) for the verb chunk rule : v+n? vp-PO. 
3 Automatic rule acquisition 
The goal of the rule acquisition algorithm is to 
                                                     
1 POS tags used in the sentence: t-time noun, f-direction, 
r-pronoun, p-preposition, v-verb, n-noun, u-auxilary, 
a-adjective, d-adverb, w-puntuation. 
602
automatically acquire some syntactic structure 
rules to describe which words in which context in 
a sentence can be reduced to a reliable MWC, on 
the base of a large scale annotated corpus and a 
lexical knowledge base.  
Each rule will have the following format: 
<structure description string> ? <reduced tag> 
<confidence score> 
Two types of structural rules are used in our 
algorithm: (1) Basic rules, where only POS tags 
are used in the components of a structure rule; (2) 
Expanded rules, where some lexical and 
contextual constraint is added into the structure 
rule string to give more detailed descriptions. The 
reduced tag has two kinds of MWC tags that are 
same as ones defined in Table 1. 
Each rule consists of all the positive and 
negative examples covered by the rule in the 
annotated corpus. For the word serial matched 
with the structure description string of a rule, if it 
can be reduced as a MWC in the annotated 
sentence, it can be regarded as a positive example. 
Otherwise, it is a negative example. All of them 
form a special state space for each acquired rule. 
Therefore, the confidence score (?) for the rule can 
be easily computed to evaluate the accuracy 
expectation to apply it in an automatic parser. Its 
computation formula is: ? = fP / ( fP + fN), where fP 
is the frequency of the positive examples, and fN is 
the frequency of the negative examples. 
A two-step acquisition strategy is adopted in 
our algorithm. 
The first step is rule learning. We firstly extract 
all basic rules with positive examples from the 
annotated corpus. Then, we match the extracted 
structure string of each basic rule in all the corpus 
sentences to find all possible negative examples 
and build state space for it. Through rule 
reliability computation (see the following section), 
we can extract all high-reliability basic rules as the 
final result, and all other basic rules with higher 
frequency for further rule refinement.  
The second step is rule refining. We gradually 
expand each rule with suitable lexical and 
contextual constraint based on an outside lexical 
knowledge base, dynamically divide and 
automatically allocate its positive and negative 
examples into the expanded rules and form 
different state spaces for them. From them, we can 
extract all the high and middle reliability 
expanded rules as the final results. 
At last, by combining all the extracted basic and 
expanded rules, we build a hierarchical acquired 
rule base for parser application. 
Two key techniques are proposed in the 
algorithm: 
(1) Rule reliability evaluation 
The intuition assumption is that: if a rule has a 
higher confidence score and can cover more 
positive examples, then it can be regarded as a 
reliable rule.  
Types Decision conditions 
1 z (fP>=10) && (?>=0.85) 
z ((fP>=5) && (fP<10)) && (?>=0.9) 
z ((fP>=2) && (fP<5)) && (?>=0.95) 
2 z (fP>=10) && (?>=0.5) 
z ((fP>=5) && (fP <10)) && (?>=0.55)
z ((fP>=2) && (fP<5)) && (?>=0.6) 
z (fP >0) && (?>=0.6) 
3 z (fP >=10) && (?>=0.1) 
z ((fP>=5) && (fP<10)) && (?>=0.2) 
z ((fP>=2) && (fP<5)) && (?>=0.3) 
z (fP>0) && (?>=0.3) 
4 All others 
Table 2 Four reliability types of the acquired 
rules 
By setting different thresholds for ? and fP, we 
can classify all acquired rules into the following 
four types of rule sets: (1) high-reliability (HR) 
rules; (2) middle-reliability (MR) rules; (3) 
low-reliability rules; (4) Useless and noise rules. 
Table 2 shows different decision conditions for 
them in our current algorithm. Based on this 
uniform evaluation standard, we can easily extract 
effective rules from different acquired rule base 
and quickly exclude useless noise rules. 
 (2) Rule expansion and refinement 
When a rule is not reliable enough, the 
expansion step is set off: new knowledge is added 
to the rule in order to constrain it. The purpose is 
to dynamically divide the state space of the rule 
and reduce the proportion of negative examples 
covered by the current rule. For every annotated 
positive or negative example, our expansion 
strategy is as follows: 
Firstly, we expand a rule description through 
looking up different lexical knowledge base. For 
the verb chunks with LCC constructions, we use 
the following lexical constraint: (1) Lexical- 
syntactic relation pairs, (2) Subcategory frame of 
603
head verb. For the noun chunks with RCC and CH 
constructions, we use the following lexical 
constraint: (1) Lexical-syntactic relation pairs, (2) 
Semantic class of head noun.  
Secondly, we expand a rule description example 
with or without lexical constraint through looking 
up its left and right adjacent contexts. For each 
rule waiting for expansion, we add its left-adjacent 
POS tag, right-adjacent POS tag, left and right 
adjacent POS tag to form three expanded rule with 
contextual constraint. 
For example, for the positive example ???/v 
(devote) ??/n (painstaking) ? of ?v+n? rule in 
the above sentence (1), we can get the following 
expanded rules: 
z v(WC-L)+n(WC-R) // + v-n relationship pair 
z v(winl:VNPLIST)+n // + verb subcate frame 
z n__v+n // + left POS constraint 
z v+n__w  // +right POS constraint 
z n__v+n__w  // +l and +r POS constraint  
They can be put into the state space pool as the 
expanded rules with positive example information 
for frequency calculation. 
Unlike the information-gain measure used in 
FOIL system (Quinlan, 1990), we do not impose 
any criteria for selecting different knowledge. All 
the suitable expanded rules are selected through 
the final confidence score evaluation indexes. 
4 Experimental results 
All the news files with about 200,000 words in 
the Chinese treebank TCT (Zhou, 2004) were 
selected as the experimental data. They were 
separated into two data sets: (1) training set, which 
consists of about 80% data and is used for rule 
acquisition; (2) test set, which consists of about 
20% data and is used for parser evaluation.  
Then we automatically extracted all the MWCs 
from the annotated trees and built two MWC 
banks. Among them, 76% are noun chunks and 
verb chunks. They are the key points for rule 
acquisition and parsing application. In the training 
set, about 94% verb chunks are two-word chunks. 
But for noun chunks, the percentage of two-word 
chunks is only 76%. More than 24% noun chunks 
comprise three or more words. The complexities 
of noun chunks bring more difficulties for rule 
acquisition and automatic MWC parsing.  
We also used the following lexical knowledge 
base for rule expansion and refinement: (1) 
Lexical relationship base. It consists of 966953 
lexical pairs with different syntactic relationships. 
All the data are extracted from 4 different 
language resources. (2) Verb subcategory data. It 
consists of 5712 verbs with the ?v+np? subcat 
frames and 1065 verbs with the ?v+vp? subcat 
frames. All the data are extracted from a Chinese 
grammatical dictionary (Yu and al., 1998). (3) 
Noun thesaura data. It consists of 26906 nouns 
annotated with the different semantic types All the 
data are extracted from Hownet-20002.  
4.1 Rule base acquisition 
We ran our algorithm on the above language 
resources and obtained the following results. 
In the rule learning stage, we extracted 735 
basic rules from the training set. After reliability 
evaluation, we obtained 61 HR rules and 150 less 
reliable rules for further refinement. Although 
these 211 rules only make up 29% of all the 735 
acquired rules, they cover about 97% positive 
examples in the training set. Thus, almost all the 
useful information can be reserved for further rule 
expansion and refinement. 
In the rule refining stage, 47858 rules were 
expanded from the 150 basic rules. Among them, 
all 2036 HR and 2362 MR rules were selected as 
the final results. They make up about 9% of all the 
expanded rules, but cover 86% positive examples. 
It indicates the effectiveness of our current rule 
acquisition algorithm. 
In order to evaluate the descriptive capability of 
the acquired rules objectively, we proposed an 
expectation precision (EP) index to estimate the 
parsing accuracy when we apply the acquired 
rules to all the positive examples in the training set. 
Its computation formula is as follows: 
??
==
=
N
i
Pii
N
i
Pi ffEP
11
/)*( ?  
where N is the total number of the rules in a rule 
base, fPi and ?i are the positive example frequency 
and confidence score of the ith rule in the rule base. 
An intuition assumption behind the EP definition 
is that a rule base with higher EP index will imply 
its better descriptive capability for some special 
linguistic phenomena. Therefore, its better parsing 
performance in a rule-based parser can be 
expected. To prove this assumption, we designed a 
                                                     
2 The data is available in http://www.keenage.com 
604
simple comparison experiment to analyze the 
improvement effects of different lexical and 
contextual constraint used in our expanded rules. 
We divided all 150 basic rules into 4 subsets, 
according to their different internal structure 
characteristics: (1) Noun chunks with RCC and 
CH constructions; (2) Verb chunks with LCC 
constructions; (3) Verb chunks with RCC 
constructions; (4) All other MWCs. 
The rules in the subset 1 and 2 cover majority of 
the positive examples in the training set. They 
have complex internal structures and lexical 
relations. So we applied the lexical knowledge 
base and contextual constraint to expand them. 
Comparatively, the rules in subset 3 and 4 have 
simpler structures, so we only used the contextual 
constraint to expand them.  
Table 3 shows the EP indexes of these rule 
subsets before and after rule refining. For all 150 
basic rules, after rule expansion and refinement, 
the EP index was improved about 65%. For the 
simpler structure rules in subset 3 and 4, just the 
application of contextual constraint can bring 
dramatic improvement in the EP index. It 
indicates the importance of the local contextual 
information for multiword chunk recognition. 
Sub 
set 
Rule 
sum 
Covered 
positive 
examples 
EP before 
expansion 
(%) 
EP after 
expansion 
(%) 
1 51 13689 52.70 81.40 
2 20 8859 45.14 80.56 
3 24 2342 28.12 93.27 
4 55 3566 66.85 93.22 
Total 150 28456 50.56 83.36 
Table 3 Descriptive capability analysis of 
different kinds of expanded rule sets 
For the major subset 1 and 2, EP index also 
shows great improvement. It increased about 54% 
and 78% in the subset 1 and 2 respectively. As we 
can see, the applying effects of lexical and 
contextual constraint on the verb chunks were 
superior to that on the noun chunks. Two factors 
contribute to this phenomenon. First, the simpler 
internal structures of most verb chunks guarantee 
the availability of almost all corresponding lexical 
relationship pairs. Second, most lexical pairs used 
in verb chunks have stronger semantic relatedness 
than that in noun chunks. 
4.2 Parsing performance evaluation 
Based on the rule base automatically acquired 
through the above algorithm, we developed a 
rule-based MWC parser to automatically 
recognize different kinds of MWCs in the 
Chinese sentences after word segmentation and 
POS tagging. Through ?-based disambiguation 
technique, the parser can output most reliable 
MWCs in the disambiguated region of a sentence 
and keep some ambigous regions with less 
reliable MWC structures to provide multiple 
selection possibilities for a full syntactic parser. 
Some detailed information of the parser can be 
found in (Zhou, 2007). 
We used three commonly-used indexes : 
precision, recall and F-measure to evaluate the 
performance of the parser. Two different criteria 
were set to determinate the correctness of a 
recognized MWC. (1) ?B+F+R? criterion : It 
must have the same left and right boundaries, 
function tag and relation tag as that of the gold 
standard. (2) ?B+F? criterion : It must have the 
same left and right boundaries, function tag as 
that of the gold standard. 
Table 4 shows the experimental results under 
the disambigutated regions, which cover 95% of 
the test data.  
Type ?B+F+R? criterion ?B+F? criterion 
np 75.25/75.76/75.50 83.68/84.25/83.97
vp 83.23/81.46/82.34 87.35/85.49/86.41
mp 94.89/95.26/95.08 94.89/95.26/95.08
ap 93.99/97.33/95.63 93.99/97.33/95.63
tp 92.75/88.18/90.40 93.52/88.92/91.16
sp 78.76/86.41/82.41 79.65/87.38/83.33
Total 81.76/81.44/81.60 87.01/86.67/86.84
Table 4  Open test results (P/R/F-m, %) 
under the disambiguated regions 
The differences of F-measures among three 
MWC subsets, i.e. noun chunks, verb chunks and 
other chunks, show interesting positive 
association with the differences of their EP 
indexes listed in the previous sections. When we 
apply the acquired rule base with higher EP 
index in the rule-based parser, we can get better 
parsing performance. It indicates that EP value 
can be used as an important objective index to 
evaluate the descriptive capability of the rule 
base automatically acquired for large scale 
annotated corpus. 
The lower F-measure of noun and verb chunk 
605
under ?B+F+R? criterion shows the difficulty for 
lexical relation recognition, especially for the 
complex noun chunks. There are still much 
improvement room in future research. 
5 Related work 
In the area of chunking rule acquisition and 
refinement, several approaches have been 
proposed. Cardie and Pierce(1999) explored the 
role of lexicalization and pruning of grammars for 
base noun phrase identification. Their conclusion 
is that error-driven pruning is a remarkably robust 
method for improving the performance of 
grammar rules. Dejean(2002) proposed a 
top-down inductive system, ALLis, for learning 
and refining linguistic structures on the base of 
contextual and lexicalization knowledge extracted 
from an annotated corpus. Choi et al(2005) 
proposed a method for automatically extracting 
partial parsing rules from a tree-annotated corpus 
using decision tree induction. The acquired 
grammar is similar to a phrase structure grammar, 
with contextual and lexical information, but it 
allows building structures of depth one or more. 
All these researches prove the important role 
of lexical and contextual information for 
improving the rule descriptive capability. 
However, the lexical information used in these 
systems is still restricted in the lexical head of a 
constituent. None of the lexical relationship 
knowledge extracted from the annotated corpus or 
other outside language resources has been applied. 
Therefore, the room for improvement of the rule 
descriptive capability is restricted to a certain 
extent. 
6 Conclusions 
Three main contributions of the paper are 
summarized as follows. (1) We design a new 
multiword chunking task. Based on the 
topological structure definition, we establish the 
built-in relations between multiword chunk 
examples in annotated corpus and lexical 
relationship pairs in outside lexical knowledge 
base. (2) We propose an efficient algorithm to 
automatically acquire hierarchical structure rules 
from large-scale annotated corpus. By introducing 
different kinds of lexical knowledge coming from 
several different language resources, we set up an 
open learning environment for rule expansion and 
refinement. (3) We propose an expectation 
precision index to evaluate the descriptive 
capability of the refined rule base. Experimental 
results show that it has stronger positive 
association with the F-measure of parser 
performance evaluation. 
Acknowledgements. The research was 
supported by NSFC (Grant No. 60573185, 
60520130299). Thank the comments and advice of 
the anonymous reviewers. 
References 
Steven Abney. 1991. Parsing by Chunks. In R. Berwick, 
S. Abney and C. Tenny (eds.) Principle-Based 
Parsing, Kluwer Academic Publishers. 
Claire Cardie and D. Pierce. 1999. The Role of 
Lexicalization and Pruning for Base Noun Phrase 
Grammars. In Proceedings of the Sixteenth National 
Conference on Artificial Intelligence (AAAI-99).  
X. Carreras and L. M`arquez. 2005. Introduction to the 
conll-2004 shared tasks: Semantic role labeling. In Proc. of 
CoNLL-2005. 
Myung-Seok Choi, Chul Su Lim, and Key-Sun Choi. 
2005. Automatic Partial Parsing Rule Acquisition 
Using Decision Tree Induction. In R. Dale et al 
(Eds.). Proc. of IJCNLP 2005, Seoul, Korea . 
p143?154. 
Herve Dejean. 2002 Learning rules and their exceptions. 
Journal of Machine Learning Research, 2002: 
669?693. 
J R. Quinlan 1990. Learning logical definitions from 
relations. Machine Learning, 5:239?266. 
L Ramshaw and M Marcus. 1995.Text chunking using 
transformation-based learning. In Proc. of the Third 
Workshop on Very Large Corpora, p82-94. 
Erik F. Tjong Kim Sang and S. Buchholz 2000 
Introduction to CoNLL-200 Shared Task: Chunking. 
In Proc. of CoNLL-2000 and LLL-2000. Lisbon. 
p127-132. 
Erik F. Tjong Kim Sang and H. D?jean 2001. 
Introduction to the CoNLL-2001 Shared Task: 
Clause Identification. In Proc. of CoNLL-2001, 
Toulouse, France, p53-57. 
Shiwen Yu, Xuefeng Zhu, et al 1998 A Complete 
Specification of the Grammatical Knowledge-base of 
Contemporary Chinese. Tsinghua University Press. 
(in Chinese) 
Qiang Zhou 2004. Annotation scheme for Chinese 
Treebank. Journal of Chinese Information 
Processing, 18(4): 1-8. (in Chinese) 
Qiang Zhou. 2007. A rule-based Chinese chunk parser. 
In Proc. Of ICCC-2007, furthercoming. 
606
Using Co-occurrence Statistics as an Information Source 
for Partial Parsing of Chinese 
Elliott Franeo DRABEK 
The State Key Laboratory 
for Intelligent Technology and Systems 
Department ofComputer Science 
Tsinghua University, Beijing 100084 
elliott_drabek@ACM.org 
Qiang ZHOU 
The State Key Laboratory 
for Intelligent Technology and Systems 
Department ofComputer Science 
Tsinghua University, Beijing 100084 
zhouq@slOOOe.cs.tsinghua.edu.cn 
Abstract 
Our partial parser for Chinese uses a learned 
classifier to guide a bottom-up parsing 
process. We describe improvements in
performance obtained by expanding the 
information available to the classifier, from 
POS sequences only, to include measures of 
word association derived from 
co-occurrence statistics. We compare 
performance using different measures of 
association, and find that Yule's coefficient 
of colligation Y gives somewhat better 
results over other measures. 
Introduction 
In learning-based approaches to syntactic 
parsing, the earliest models developed generally 
ignored the individual identities of words, 
making decisions based only on their 
part-of-speech classes. On the othor hand, 
many later models see each word as a 
monolithic entity, with parameters estimated 
separately for each word type. In between have 
been models which auempt o generalize by 
considering similarity between words, where 
knowledge about similarity is deduced fi'om 
hand-written sources (e.g. thesauri), or induced 
from text. For example, The SPATTER parser 
(Magerman, 1995) makes use of the output of a 
clustering algorithm based on co-occurrence 
information. Because this co-occurrence 
information can be derived from inexpensive 
data with a minimum of pre-processing, it can be 
very inclusive and informative about even 
relatively rare words, thus increasing the 
generalization capability of the parser trained on 
a much smaller fully annotated corpus. 
The cunent work is in this spirit, making 
complementary use of a relatively small 
treebank for syntactic information and a 
relatively large collection of flat text for 
co-occurrence information. However, we do 
not use any kind of clustering, instead using the 
co-occurrence data directly. Our parser is a 
bottom-up arser whose actions are guided by a 
machine-learning-based decision-making 
module (we use the SNoW learner developed at 
the University of Illinois, Urbana..Champaign 
(Roth, 1998) for its strength with potentially 
very large feature sets and for its ease of use). 
The learner is able to directly use statistics 
derived from the co-occu~euce data to guide its 
decisions. 
We collect a variety of statistical measures of 
association based on bigram co-occurrence data 
(specifically, mutual information, t-score, X 2, 
likelihood ratio and Yule's coefficient of 
colligation Y), and make the statistics available 
to the decision-making module. We use 
labelled constituent precision and recall to 
compare performance of different versions of 
our parser on unseen test data. We observe a
marked improvement in some of the versions 
using the co-occurrence data, with strongest 
performance observed in the versions using 
Yule's coefficient of  colligation Y and mutual 
information, and more modest improvements in 
those using the other measures. 
1 Background 
1.I Our Task m Partial Parsing 
The current work has developed inthe context 
of developing a partial or "chunk" parser for 
Chinese, whose task is to identify certain kinds 
of local syntactic structure. The syntactic 
22 
analysis we use largely follows the outline of 
Steven Abney's work (Abney, 1994). We 
adopt he concept of a "e-head" and an "s-head" 
for each phrase, where the e-head corresponds 
roughly to the generally used concept of head 
(e.g., the main verb in a verb phrase, or the 
preposition in a prepositional phrase), and the 
s-bead is the "main content word" of a phrase 
(e.g., the main verb in a verb phrase, but the 
object of the preposition in a prepositional 
phrase). The core of our chunk definition is also 
in line with Abney's: A chunk is essentially the 
contiguous range of words s-headed by a given 
major content word. Within this basic 
framework, we make some aecorunaodations to 
the Chinese language and to practicality. For 
example, by our understanding of Abney's 
definition, a numeral-classifier phrase followed 
immediately by the noun it modifies should 
constitute two separate chunks. However such 
units seem likely to be useful in further 
processing, and easy to accurately identify, so 
we chose to include them in our definition of 
chunk. 
For simplicity and consistency, we adopt a 
very restricted phrase-structured syntactic 
formalism, somewhat similar to a 
phrase-structured formulation of a dependency 
grammar. In our formalism, all constituents are 
bina_ry branching, and the purpose of the 
non-terminal labels is restricted to indicating the 
direction of dependency between the two 
children. Figure 1 shows an example sentence 
with some indicative structures. 
Dependencies within individual chunks are 
shown with heavy arrows. A fight-pointing 
dependency, such as the three dependencies 
within the noun phrase " ) l~ .~t :~y~r~" ,  
corresponds to a constituent labelled 
"right-headed". A left-pointing dependency, 
such as that between the verb "~,.~\[~" and its 
aspect particle "T ' ,  corresponds toa constituent 
labelled "left-headed". These are cases where 
the s-head and the e-head of the phrase are 
identical. When they are not identical, we 
have a "two-headed" ependency, like those in 
the phrase "~_L \ [~ ' .  Here, the relation 
between "~"  and ".J~" (and between "~ 
_.L" and "~")  is that the left constituent 
provides the s-head of the phrase, while the right 
constituent provides the e-head. 
These four non-terminal categories can 
descn'be high- or low- level syntactic structures. 
However, for chunking we wish to leave the 
higher-level structures of a sentence unspecified, 
leaving only a list of local structares. We treat 
this in a consistent way by adding a fifth 
non-terminM category "unspecified", and 
replacing all higher str~tures with a backbone 
of strictly left-branching "unspecified" nodes, 
anchored to a special "wall" token to the left of 
the sentence. This backbone structure is shown 
by the light lines in the figure. 
1.2  Our  Data  Sources  ~ One Large and One 
Small 
During development, we made use of two 
corpora. The first is a relatively small-scale 
treebank of approximately 3500 sentences, 
39,000 words, and 55,000 characters (Zhou, 
1996). We transformed this corpus by annotating 
each phrase with c-heads and s-heads, using a 
large collection of hand-written rules, and then 
extracted chunks from this transformed version. 
The second corpus, which we use only as a 
source of co-occurrence statistics, ismuch larger, 
with approximately 67,000 sentences, 1.5 
million words, and 2.2 million characters, with 
sentences eparated and words separated and 
marked with parts-of-speech, but with no further 
syntactic annotation (Zhou and Sun, 1999). In 
the current work we make no use of the 
part-of-speech annotation, taking co-occurrence 
? counts of word-types alone. 
1.3 Our Framework - -  C lass i f ier -Guided 
Shi f t -Reduce Pars ing 
The parsing framework we use has been 
chosen for maximum simplicity, aided by the 
simplicity of the syntactic framework. In 
parsing, we model a left-to-right shift-reduce 
automaton which builds a parse-tree 
constituent-by-constituent in a deterministic 
left--to-right process. The parsing process is 
thus reduced to making a series of decisions of 
exactly what to build. 
For training, we extract the series of actions 
the shift-reduce parser would have had to make 
to produce the trees from the surface structure of 
the sentences. This gives a long series of 
state-action pairs: "when the parser was in 
state X, it took action Y'. The state description 
X is set of binary predicates describing the local 
surface structure of the sentence and the contents 
23 
cucurbit vegetable raising methods already occur lperf.\] foundation on \[rel.\] change 
Methods of raising cucurbit vegetables have changed fundamentally. 
Figure 1. An example sentence annotated according to our system. 
of the stack. We describe these predicates in 
detail below. This series of state-action pairs is 
presented to the SNoW learner, which tries to 
learn to predict the parser actions from the 
parser states, attempting to find a linear 
diserimin:mt over these binary predicates which 
best accounts for the corresponding actions in 
the training data. 
These parse actions can be either "shift a word 
from the right on to the stack", or "reduce the 
? , top elements of the stack" into a single 
constituent. Because our syntactic framework 
is strictly binary branching, each reduce action 
operates on exactly the top two items on the 
stack, so the automaton eed only choose a 
category for the new constituent. This decision 
turns out to be nearly trivial, and we were able to 
achieve 100% accuracy on our test set using 
only part-of-speech information, so in the 
remainder of this paper we discuss only issues 
relating to the more difficult decision of whether 
to shift or reduce. 
Within the shift-reduce decisions, over half 
are pre<letermined by the basic requirements of
the framework. For example, if there are no 
words left to shift, we can only reduce. If there 
is only one item on the stack, we can only shift. 
These decisions are handled by simple 
deterministic rules within the parser and are not 
shown to the classifier either in training or in 
parsing. 
In the first version of the parser, prior to the 
introduction of co-occurrence statistics, the 
information available to the classifier is limited 
to parts-of-speech of words in the surface 
structure of the sentence, nonterminal categories 
of constituents already built on the stack, and 
parts-of-speech of the s- and e-heads of 
constituents already built on the stack. These 
are collected into schemas representing sets of 
poss~le binary predicates. Table 1 shows a 
representative subset of this original set of 18 
predicate schemas (space does not allow us to 
present all of them). The total of all the 
instantiations of all these templates presents a
potentially huge feature set, so we rely on an 
important property of the SNoW architecture, 
that it can handle an indefinitely large set of 
24 
Pred icate  Schema 
POS (Surface-word \[k\] ) = t 
Range o f  
Parameters  
-I K k_< 2 
POS(Sur face-word \ [k \ ] )  = tl /~ POS(Sur face-word \ [k  + I\]) = t2 -2 ~ k ~ 1 
Category(Stack \ [k \ ]  ) = c 0 ~ k ~ 1 
Category(Stack \ [k \ ]  ) = ci /k Category  (Stack \[k + I\] ) = c2 0 ~ k -< 1 
POS(S-head(Stack \ [k \ ] ) )  = t 0 ~ k ~ 2 
POS(S-head(Stack \ [k \ ] ) )  =t l /k  POS(S-head(Stack \ [k+ I\])) =t2  0 ~ k -< 1 
POS(S-head(Stack \ [kx \ ]  )) = tx /k POS (Sur face-word \ [k2 \ ] )  = t2 0 --< kx ~ 1 
- I~  k2-<0 
Category(Stack\[kx\] )  = c /k POS(C-head(Stack\[kx\]) )  = t~ /~ 
POS (Surface-word \[k2\] ) = t2. 
0-< kz~ 1 
- i~  k2-<0 
Table 1. A Subset of the Feature Schemas in the Original Version of the Parser. The variables t, tt, 
and t2 range over the set of part-of-speech categories, while the variables c, et, and c2 range over the 
set of non-terminal categories. Surface words are indexed relative to the parsing position, such that 
Surface-word\[O\] is the next word to be shifted. 
features, actually using only those features 
which are active. The set of these actually 
active features is reasonable for our set of 
schemas. 
2 Enriching the Feature Set with 
Co-occurrence Statistics 
statist ic(wl,  w2) ~ Xl 
s ta t i s t i c  (wl, w2) ~- X2 
rather than mutually-exclusive predicates of 
the form: 
X0 < stat ist ic(wl,  w2) ~-Xl  
X0 < stat ist ic(wl,  w2) ~ X2 
2.1 Measures of Association 
Table 2 shows the definitions of the five 
measures we have chosen to compare in the 
current work, taken from (Manning and Shfitze, 
1999), (Kageura, 1999). 
These measures are based on empirical counts 
of word occurrences and co-occurrences. 
Because these events are very prone to 
zero-counts, both for unseen bigrams and for 
unseen words, we applied Simple Good-Turing 
smoothing (Gale and Sampson, 1995) to both 
bigram and word counts. 
2.2 Making Measures of Association Available to 
the Parser 
To make the measures of association available 
to the parser, we started by discretizing each 
measure, that, is substituting for each continuous 
measurement a set of binary predicates coarsely 
describing its approximate value. We used a 
very simple form of discretization, counting 
occurrences of each value, and then dividing the 
values into bins of approximately equal counts. 
Informal exploration showed consistently better 
performance when bin membership was made 
cumulative; that is, using 
non-mutually-exclusive pr dicates of the form: 
Using these cumulative predicates, parsing 
accuracy consistently improved with increases in 
the number of bins, though the rate of 
improvement slowed at the same time. The 
cost of increasing the number of bins came 
primarily in the algorithm's training time. We 
chose thirty-two to be a good number of bins. 
The predicates resulting ~om discretization 
are predicates over values of a statistic. To 
apply these predicates in parsing, we created 
features relating to particular slots within 
parse-state descriptions. Specifically, we made 
three new feature schemas available to the 
Winnow learner, as shown in Table 3. Each of 
these feature schemas is an extension to one 
available to the original parser. In each case, 
the original schema was of the form: 
POS(wl) = t~ A POS(w2) = t2 
And the extended schema was of the form: 
POS(wl) = tl A POS(w2) = t2 A 
stat ist ic(wl,  w2) ~ X 
In this way, the learner is able to condition 
separately depending on the parts-of-speech of 
the two words in question. This is based on the 
intuitions that different eases for part-of-speech 
combinations would behave veery differently, 
and that the training data was sufficient that 
25 
Measure  Definition 
Mx lo~- c(w~'w2) 
c(w. . )c( . ,  w2) ) 
T-score  4C(Wl, w2)f C (Wd~-2~ w2) ~ Cl, W,,W2) 1} 
x' c(.,.)(c(w. - c(w. ))2 
c( , . )c( , ,)c( . , )c( . ) 
Likeli- 
hood 
Ratio 
L I ,  c(*.-) ) ct.,-) ) j 
Note :  LogL(p; n; k) = k log(p) + (n - k) log(1 - p)  
Yule's Y .~- I  
4 +1 
Note: C(O. o)c(w,, 
C( WI, W2 )C( WI, W2) 
Table 2. Definitions of the Five Measures of Association. c(x~,wz) represents the count of the 
event that x and y occur adjacent and in this order in the training corpus. - 'w  represents ummation 
over all words other than w, and ? represents ummation over all words. 
performance would not be hurt by the resulting 
sub-division; however we have no specific 
empirical support for this. 
2.3 Experimental Results 
We trained a series of SNoW networks using 
features sets extended with each of thef ive 
measures, and tested five versions of our parser, 
One using each of the resulting networks. This 
was done on a held-out test set comprising 
approximately ten percent of our treebank. The 
resulting measurements for labeled constituent 
precision and recall are shown in Table 4, 
arranged according to the geometric mean of the 
two measurements. 
It is clear from the table that co-occurrence 
information can be made useful, and that the 
measure used to represent this information has a 
large influence on its usefulness. There is also 
a large disparity between the in~rovement in
precision, 1.7%, and the improvement in recall, 
4.1%. We con jec ture  that this is because the 
parser odg/nally tended to err in the direction of 
splitting words into separate chunks, the 
commoner case, while with the co-occurrence 
infommtion, it is able to pick out some cases 
where a strong association suggests that words 
be joined in the same chunk. 
3 Related Work 
Statistical measures of association appfied to 
bigram co-occurrence counts have been used 
most extensively in terminology and collocation 
extraction. (Manning and Shfitze, 1999) 
contains a good introduction to this topic. 
(Kageura, 1999) is an especially good empirical 
comparison of the performance of several 
measures of association on a set of tasks in both 
terminology extraction and in morpheme 
splitting of Chinese character sequences. This 
latter tasks which can be seen as a very restricted 
form of parsing, has been treated in a body of 
interesting work, including (Sun, Shen and Tsou, 
1998), (Lee, 1999) . This work has generally 
used vee/y simple heuristic ontrol policies, such 
as repeatedly splitting at the point of lowest 
mutual information. The use of similar 
26 
Pred icate  Schema 
P0S (Sur face -word \ [k \ ] )  = tz A POS (Sur face -word \ [k  + 1\] ) 
Stat ist ic(Surface-word\[k\ ]  , Sur face -word  \[k + I\] ) ~< X 
=t2  A 
Range of 
Parameters  
-2_< k~ 1 
POS(S-head(Staek \ [k \ ] ) )  = tx /k POS(S -head(Stack \ [k  + i\])) = t2 /~ 0 ~- k ~ 1 
S ta t i s t i c (S -head(Stack \ [k \ ] ) ,S -head(Stack \ [k  + i\])) ~- X 
POS (S -head(Stack  \[kz\] ) ) = tz /k POS (Sur face-word\ [k2\ ] )  
Statistic (S -head (Stack \[kl\] ) , Sur face-word  \[k2\] ) ~- X 
Table 3. Augmented Feature Schemas. 
approaches for general parsing received some 
early exploration (Brill, Magerman, Marcus and 
Santofini, 1990), (Magerman and Marcus, 1990), 
but this approach seems to have lost popularity. 
This may be because using co-occurrence 
statistics as a sole source of guidance may 
become insufficient as the object of parsing 
moves from the veery local structure of word 
splitting to the longer-distance dependencies of
general parsing. The current work attempts to 
remedy this by using a general eafing device 
to balance co-occurrence statistics with other 
information to be integrated into a larger control 
policy. 
Conclusions and Future Work 
Our experiments show that simple statistical 
information gathered ~om the unprocessed 
surface structure of large-scale text has value in 
guiding parsing decisions. However, we feel 
that there is still a great deal of further advantage 
to be gained from this approach. Our next step 
will be to include co-oecu~ence information 
from a much larger corpus, containing on the 
order of 108 characters. 
We would also like to experiment with other 
definitions of co-occurrence. (Yuret, 1998) 
describes some very interesting work, in a 
different framework from ours, in which a parser 
using only co-occurrence mutual information 
was able to achieve a high precision but low 
recall when co-occurrence was defined as 
adjacent co-occurrence, and low precision but 
high recall when co-occurrence was defined as 
occurrence within the same sentence. We 
would like to experiment with ways of balancing 
these two measures. 
We also suspect hat significant gal.~ are 
possible through a more sophisticated inclusion 
of the statistics in the decision making process. 
The current diseretization scheme is very simple, 
but there is ample empirical evidence that 
= t2 /k 0 ~ kx -< 1 
-i ~ k2 ~ 0 
discrefization which takes into account arget 
categories can significantly improve 
classification accuracy (Dougherty, Kohavi, and 
Sahami, 1995). 
The several articles we have cited which use 
exclusively co-occurrence information to predict 
constituent boundaries are very interesting for 
the simplicity of their control structures, but in 
one important way they are more complex than 
the current work: they make decisions by 
explicitly comparing the measures of association 
between different pairs of words. We predict 
that augmenting the feature set to allow our 
parser to be sensitive to this kind of information 
would be a very valuable xtension. 
A related issue is the choice of learning 
methodology. The Winnow learner has served us 
well with its ability to handle very large feature 
sets, but it is weak in its ability to take 
advantage of the interaction between features. 
We would like to experiment with learning 
methods which do not suffer from this weakness, 
and with methods for automatic feature 
extraction which could supplement Winnow. 
We experimented with a nondeterministic 
control policy for the parser, using cost-front 
search to fred the most probable series of 
parsing decisions, but we found this not to be 
very useful. Over a series of comparative 
experiments, the non.deterministic ontrol 
policy consistently raised precision by a small 
margin, lowered recall by a small margin, 
increased run times by an order of magnitude or 
more, and for about 10% of the test.set 
sentences exhausted system resources before 
finding any parse at all. We posit that these 
problems may in part be due to the fact that 
while the Winnow learner is otherwise quite 
well adapted for our purposes, its output is not 
intended to be interpreted probabilistically. In 
the future we intend to run parallel experiments 
with more probabilisticaUy oriented learners; we 
27 
Measure of Association Precision Recall 
Yule's Y 0.882 0.875 
Geometric Mean 
0.879 
Mutual Information 0.885 0.857 0.871 
Likelihood Ratio 0.879 0.845 0.862 
X z 0.870 0.848 0.859 
T-score 0.870 0.836 0.853 
None (Ori~na! Feature Set) 0.868 0.834 0.851 
Table 4. Accuracy Measurements of Parsing with Different Measures of Association 
are espeeiaUy interested in experimenting with a 
Maximum Entropy model. 
In the larger context, we plan to experiment 
with more sophisticated, model-based 
unsupervised learning methods, including 
clustering and beyond, and ways of providing 
their gathered knowledge to the parser, to make 
the fullest possible use of the vast wealth of 
un-annotated text available. 
Acknowledgements 
The research was supported by National 
Natural Science Foundation of China (NSFC) 
(Grant No. 69903007) and National 973 
Foundation (Grant No. G 1998030507-2). 
References 
Steven Abney (1994) Parsing by Chunks. 
http://www.sfs.phil.uni-tuebingen.de/~abney/ 
Erie Brill, David Magerrnan, Mitch Marcus and B. 
Santorini (1990) Deducing Linguistic Structure 
from the Statistics of Large Corpora. 
Proceedings of the DARPA Speech and Natural 
Language Workshop, pp. 275-281. 
Kenneth W. Church and P. Hanks (1990) Word 
Associaffon Norms, Mutual Information, and 
Lexicography. Computational Linguistics, 
Computational Linguistics, 16/1, pp. 22-29. 
Ted Dunning (1993) Accurate Methods for the 
Statistics of Surprise and Coincidence. 
Computational Linguistics, Computational 
Linguistics, 19/1, pp. 61-74. 
Kyo Kageura (1999) Bigram Statistics R~isited: A
Comparative Examination of Some Statistical 
Measures in Morphological Analysis of Japanese 
Kanfi Sequences. Journal of Quantitative 
Linguistics, 6/22, pp. 149-166. 
James Dougherty, Ron Kohavi and Mehran Sahami 
(1995) Supervised and Uusupen, ised 
Discretization of Continuous Features. In 
"Machine Learning: Proceedings of the Twelfth 
International Conference", Morgan Kaufmann 
Publishers. 
William A. Gale and Geoffrey Sampson (1995) 
GoOd-Turing Frequency Estimation without Tears. 
Journal of Quantitative Linguistics, 2, pp. 217-237. 
Christopher D. Manning and Hinrich Shfitze (1999) 
Foundaffons of Statistical Natural Language 
Processing. The MIT Press, Cambridge, 
Massachusetts. 
David M. Magerman (1995) Natural Language 
Parsing as Statistical Pattern Recognition. Ph.D. 
Dissertation, Stanford University. 
David Magerman and Mitch Marcus (1990) 
Parsing a Natural Language Using Mutual 
Information Statistics. In "Proceedings, Eighth 
National Conference on Artificial Intelligence 
(AAAI 9O)". 
Adwait Ratnaparkhi (1997) A Linear 
Observed- Time Statistical Parser Based on 
Maximum Entropy Models. In "Proceedings of the 
Second Conference on Empirical Methods in 
Natural Language Processing". 
Dan Roth (1998) Learning to Resolve Natural 
Language Ambiguities, a Unified Approach. In 
"AAAI'9$ ". 
K-Y. Su, M-W. Wu, and J-S. Chang (1994) A 
Corpus-Based Approach to Automatic Compound 
Extraction. In "Proceedings of the 32rid Annual 
Meeting of the ACL", pp. 27-30. 
Maosong Sun, Dayang Shen, and Benjamin tC Tsou 
(1998) Chinese Word Segmentation without 
Using Lexicon and Hand-Crafted Training Data. 
In "Proceedings of the 36th Annual Meeting of the 
ACL", pp. 1265-1271. 
Aboy Wong, Dekai Wu (1999) Are Phrase 
Structured Grammars Useful in Statistical 
Parsing?. In "Proceedings of the Fifth Natural 
Language Processing Pacific Rim Symposium", pp. 
120-125. 
Deniz Yuret (1998) Discovery of Lexical Re~tions 
Using Lexical Attraction. Ph.D. Thesis, 
Massachusetts In titute of Technology. 
Qiang Zhou (1996) Phrase Bracketing and 
Annotating on Chinese Language Corpus. (in 
Chinese), Ph.D. Thesis, Beijing University. 
28 
Chinese Base-Phrases Chunking  
Yuqi Zhang and Qiang Zhou 
State Key Laboratory of Intelligent Technology and Systems 
Department of Computer Science and Technology 
Tsinghua University, Beijing, 100084, P.R.China  
{zyq, zhouq}@s1000e.cs.Tsinghua.edu.cn 
   Abstract 
This paper introduces new definitions of Chinese 
base phrases and presents a hybrid model to 
combine Memory-Based Learning method and 
disambiguation proposal based on lexical 
information and grammar rules populated from a 
large corpus for 9 types of Chinese base phrases 
chunking. Our experiment achieves an accuracy 
(F-measure) of 93.4%. The significance of the 
research lies in the fact that it provides a solid 
foundation for the Chinese parser. 
1 Introduction 
Recognizing simple and non-recursive base phrases 
is an important subtask for many natural language 
processing applications, such as information 
retrieval. Gee and Grosjean (Gee and Grosjean, 
1983) showed psychological evidence that chunks 
like base phrases play an important role in human 
language understanding. CoNLL-2000?s shared 
task identified many kinds of English base phrases, 
which are syntactically related non-overlapping 
groups of words (Tjong and Buchholz, 2000). The 
shared task has significantly heightened the 
progress in the techniques of English partial 
parsing. For Chinese processing, Zhao (1998) put 
forward a definition of Chinese baseNP that is a 
combination of determinative modifier and head 
noun (Zhao, 1998). Based on that research, Zhao et 
al. (2000) extended the concept of baseNP to seven 
types of Chinese base phrases. These base phrases 
may consist of words or other base phrases, but its 
constituents, in turn, should not contain any base 
phrases.  
   In this paper, we put forward the new definition 
of Chinese base phrases, which are simple and 
non-recursive, similar to the CoNLL-2000?s shared 
task. The definition enables us to resolve most local 
ambiguities and is very useful for NLP tasks such as 
name entity recognition and information extraction. 
We construct a hybrid model to recognize nine 
types of Chinese base phrases. Many researches in 
Chinese partial parsing (Zhou, 1996; Zhao, 1998; 
Sun, 2001) have shown that statistical learning is of 
great use for Chinese chunking, especially for large 
corpus. However, the lack of morphological hints in 
Chinese makes it necessary to use semantic and 
syntactic information such as context free grammar 
rules in Chinese processing. In our approach, 
viewing chunking as a tagging problem by 
encoding the chunk structure in new tags attached 
to each word, we use Memory-Based Learning 
(MBL) method to set a tag indicating type and 
position in a base phrase on each word. After which 
grammar rules are used to disambiguate the tags.  
Our test with a corpus of about 2 MB showed that 
the experiment achieves 94.4% in precision and 
92.5% in recall. 
2 Definitions of Chinese Base 
Phrases 
The idea of parsing by chunks goes back to Abney 
(1991). In his definition of chunks in English, he 
assumed that a chunk has syntactic structure and he 
defined chunks in terms of major heads, which are 
all content words except those that appear between 
a function word and the content word which  
selects. A major head is the ?semantic? head (s-head) 
for the root of the chunk headed by it. However, 
s-heads can be defined in terms of syntactic heads. 
If the syntactic head h  of a phrase P is a content 
word,  is also the s-head of P. If h  is a function 
word, the s-head of P is the s-head of the phrase 
selected by . 
f f
h
h
The research enlightens us about the definition of 
Chinese base phrases. In this paper, a Chinese base 
phrase consists of a single content word surrounded 
by a cluster of function words. The single content 
word is the semantic head of the base phrase. The 
forms of base phrases can be expressed as follows. 
 
 
{Modifier} * + head + {complement}* or 
Coordinate structure 
The components of ?modifier? and ?complement? 
are optional. A head could be a simple word as well 
as the structure of ?modifier + head? or ?head + 
complement?, but not ?modifier + head + 
complement?. Coordinate structure could not 
consist of coordinate symbols such as comma and 
co-ordinating conjunction. The type of base phrases 
is congruent with its head?s semantic information. 
In most cases, the type accords with the head?s 
syntactical information, for example, when the head 
is a noun, the phrase is a noun phrase. However, 
when a head is a noun that denotes a place, the base 
phrase including that head is not a noun phrase, but 
a location phrase. 
   We consider 9 types of Chinese base phrases in 
our research: namely adjective phrase (ap), 
distinguisher phrase (bp), adverbial phrase (dp), 
noun phrase (np), temporal phrase (tp), location 
phrase (sp), verb phrase (vp), quantity phrase (mp), 
quasi quantity phrase (mbar). The inner grammar 
structures of every base phrase are very important 
too, but we will discuss that in another paper. 
3 Overview 
The frame of Chinese base phrase parsing is 
composed of two parts: one is the ?Type and 
bracket tagging model?, the other is the ?Base 
phrases acquisition model? which consists of two 
modules which are ?brackets matching ?and 
?correct the types of base phrases?. (See figure 1.) 
The input to the system is a sequence of POS. In the 
?Predict the phrase boundary? module, we predict 
the type, which each word belongs to, and the 
position of each word in a base phrase with 
Memory-Based Learning (MBL)(Using the 
software package provided by Tilburg University.). 
And the result is expressed as a pair formed by base 
phrase type and position information. Because our 
Chinese base phrases are non-recursive and 
non-overlapping, the left and right boundaries of 
base phrases must match with each other which 
means they should be a pair and alternative. 
However, the errors involving in the first part will 
lead to incorrect base phrases because the 
boundaries do not match, for example ?[?[?]?.  In 
the second part, grammar rules that indicate the 
inner structures of base phrases are used to resolve 
the boundary ambiguities. Furthermore, it also 
takes lexical information into account to correct the 
type mistakes. 
The corpus used in the experiment includes 7606 
sentences. It comes from the Chinese Balance 
Corpus including about 2000 thousand words with 
four types: literature (44%), news (30%), academic 
article (20%) and spoken Chinese (6%). These 7606 
sentences are split into 6846 training sentences and 
760 held out for testing.  
  
   
 
                                
 
 
 
 
 
  
Input Type and bracket tagging 
model 
 Obtain feature vectors 
 Predict the phrase boundary 
 Brackets matching Grammar rules
                       
  
Correct the types of base 
phrases  Lexical 
information
Output Base phrases acquisition model
Figure 1: system overview 
4 Predicting the phrase boundaries 
with MBL 
Memory-Based Learning (MBL) is a classification 
based, supervised learning approach: a 
memory-based learning algorithm constructs a 
classifier for a task by storing a set of examples. 
Each example associates a finite number of classes. 
Given a new feature vector, the classifier 
extrapolates its class from those of the most similar 
feature vectors in memory (Daelemans et, al., 1999). 
The input to the ?Predict the phrase boundary? 
module is some feature vectors, which compose of a 
sequence of POS. The solution of the module is to 
find >< ii cr ,
i
ic
,,,{
 (Wojciech and Thorsten, 1998), a 
duple formed by a type tag and a boundary tag for 
each word t . Here r  indicates the boundary tag, 
while  denotes the type tag. 
i
,, },,,, ?? mbarmpbpspdpc j
, LRri
tp
}
apvpnp
,,,{ OIRL
(?-? denotes 
the word is not in any type of base phrases.) 
? The  indicates the position 
of the word in a base phrase as shown below: 
ir
?L?: the left boundary,     ?R?: the right boundary,  
 
?I?: the middle position, ?O?: outside any base 
phrases, ?LR?: the left and right boundary. 
What information is used to represent data in 
feature vectors is an important aspect in MBL 
algorithms. We tried many feature vectors with 
various lengths. And it is interesting to note that the 
feature window is not the bigger the better. When 
the feature window is (-2, +2) in the context, the 
result is the best. So the feature vector in the 
experiment is: (POS-2, POS-1, POS0, POS+1, 
POS+2). The pattern describes the combination of 
feature vector and result duple >< mncr  
: 90,40 ???? mn
(POS-2, POS-1, POS0, POS+1, POS+2, ). >< mncr
For the experiment in the first step, we use 
1TiMBL , an MBL software package developed in 
the ILK-group (Daelemans et, al., 2001). The 
results of phrase boundary prediction with MBL 
shows in table 1.  
Table1?The result of word boundary prediction 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 1 shows that there is much difference 
between the results of various types of base phrases. 
The precisions and recalls of np, vp, mp, ap and dp 
are all almost over 90%. Comparatively, the results 
of sp, tp, bp and mbar are much lower, especially 
their recalls. This is due to some resemblances 
between sp, tp and np in Chinese syntactical 
grammars. Sp and tp may be considered as belong 
to NP, however, in the definition of Chinese base 
phrases, sp, tp and np are defined separately for the 
semantic difference. And the separation can also 
help in other tasks such as proper noun 
identification, information retrieval etc.  
                                                     
TiMBL1  is a software bag about many MBL 
algorithms. It can be download free from 
http://ilk.kub.nl/  
5 Obtaining Chinese base phrases  
5.1 The errors in phrase boundary 
prediction 
There are three types of errors in the results of first 
processing model.  
(1) Boundary ambiguity: the r  ?s mistakes 
will cause the multiple choices regarding the 
boundaries. For example:  ?
i
{np ?/rN  } ?/m  ??/n  } ?
/?  ?/p  {np ??/t  {np ??/n  } ?/u  {np ??/vN  ??
/vN  } {ap ?/dD  ?/a  } ?/??. (Please pay attention to the 
?__? part.) There are altogether three modalities: 
?{ ?{ ?}?, ?{ ?}?}? and 
?{ ?{ ?}?}?. These are caused by the 
redundancy and absence of boundaries. 
mc
mc
mc mc
mc
(2) The type mistake of base phrases: For 
example: in the sentence of ?{np ??/n  } {dp ???
/d  } {vp ?/vC  } {np ????/nS  } ?/f  {tp ??/nR  ??
/n  } ?/p??, the parser mistakes the type of ?{ ??
/nR  ??/n  }? ,which is np, for tp. This error type 
commonly appears between sp, tp and np, as well as 
mbar and mp. 
 
 
Precision  
for<  >mncr
Recall  
For >< mncr
np 
vp 
sp 
tp 
ap 
bp 
dp 
mp 
mbar 
92.27% 
90.40% 
75.15% 
82.87% 
93.52% 
92.60% 
97.56% 
93.90% 
74.15% 
93.61% 
89.65% 
48.41% 
71.62% 
91.89% 
76.38% 
97.63% 
92.38% 
72.26% 
Total1 91.90% 91.65% 
- 97.85% 98.41% 
Total2 93.83% 93.83% 
(3)   Boundaries absence: For example, in the 
sentence of ?{vp ??/v  } {np ??/n  } ?/?  {np ??/n  ?
?/n  } ??/c  {vp ??/v  }?,  ?{np ??/n  ??/n  }? 
should be ?{np ??/n  } {np ??/n  }?. It is very 
difficult to correct this type of errors because the 
boundary distribution accords with the definition of 
Chinese base phrases. The left and right boundaries 
alternate with each other. Therefore, it is very 
difficult to find the errors in the sequence from the 
modalities.  
5.2 Obtaining the whole base phrases 
with Grammar rules 
With the bracket (boundary) representation, 
incorrect bracket will be generated but these will be 
eliminated in the bracket combination process. In 
the experiment, we attempt to apply grammar rules 
that represent the inner structures of Chinese base 
phrases to get rid of the boundary ambiguities. 
These grammar rules are derived from the corpus. 
On the other hand, boundary predictions can find 
many base phrases that do not accord with the 
limited grammar rules.  
   Figure 2 shows the main strategy of how to use 
the grammar rules. When if ()>1, there are more 
 
than one pair of combined brackets in which the 
sequences accord with the grammar rules. We are 
apt to choose the longest possible because the 
shorter sequences appear more in the corpus. The 
longer the sequence, the more weight it should carry. 
When there is only the shorter sequence according 
with grammar rules, it is more possible to be the 
correct one. In this case, one or more boundaries 
will be left. They often need some other boundaries 
to match, so we try to retrieve some missing 
boundaries through the partitions in the sentences 
that should not belong to any base phrases. These 
partitions are the marks of base phrase boundaries. 
If we find these partitions between two ambiguous 
boundaries, we will know where to place the new 
boundary. 
5.3 Correct the type mistake with 
lexical information 
In the Chinese language, some POS sequences may 
belong to different types. For example, ?{vN n}? 
could be np, sp or tp. These sequences often appear 
in np, sp, tp, mp and mbar. It is difficult to know its 
right type even with the grammar rules, as we have 
done in section 5.2. In order to resolve this problem, 
we attempt to use lexical information because it 
implies semantic information to some extent. 
   The lexical information is distinctive between mp 
and mbar. mbar is often composed of numbers such 
as ?1200? and numbers in Chinese such as ???. 
The lexical information between tp and np is also 
obvious, such as ????, ???? and ???? etc. For 
sp and np, the words are ????, ???? etc. 
5.4 Experimental results  
Step 1:  Finding the sequence where the errors appear. The sequences are three types: 
?{?{?}?, ?{?}?}?, ?{?{?}?}?. 
Step 2:  if  (the number of sequences of POS in a pair of matched boundaries according with the grammar
rules) >1 
then {Select the boundaries that make the sequence longest} 
Step 3:  if (the number of sequences of POS in a pair of combined boundaries according with the grammar 
rules) =1 
if (Only the sequence with the shortest length accords with the grammar rules). 
then { Find partitions such as conjunctions, localizers, punctuations and some 
prepositions between the ambiguous boundaries in sequences; 
if (The partitions exist) 
then {Add boundaries to generate whole base phrases according to the
partitions} 
} 
 
Figure 2:  The Algorithm of Matching Boundaries     
The simplest bracket combination algorithm is very 
strict: it only uses adjacent brackets if they appear 
next to each other in the correct order (first open 
and then close) without any intervening brackets. 
The result of the algorithm is shown in table 2, as 
the baseline of the boundary combination 
experiment.  
Table 2: The base-line result 
   Precision Recall F_M 
Np 93.9% 86.1% 89.8% 
Vp 90.6% 86.2% 88.4% 
Sp 75.5% 47.7% 58.4% 
Tp 85.4% 70.2% 77.0% 
Ap 93.4% 83.4% 88.1% 
Bp 93.4% 71.3% 80.9% 
dp 97.7% 94.0% 95.8% 
mp 92.0% 85.3% 88.5% 
mbar ------- 0 ------- 
Total 92.9% 85.7% 89.2% 
 
Table 3: The result of disambiguation with 
grammar rules 
 Precision Recall F_M 
np 94.3% 91.9% 93.1% 
vp 95.0% 94.2% 94.6% 
sp 73.6% 50.9% 60.2% 
tp 84.9% 73.8% 79.0% 
ap 93.5% 89.7% 91.5% 
bp 91.6% 79.4% 85.0% 
dp 97.6% 98.1% 97.8% 
mp 86.7% 90.9% 88.7% 
mbar 63.6% 12.6% 21.1% 
Total 93.9% 92.0% 92.9% 
 
From the table 2, we could see the recalls are 
commonly low. We change another strategy to 
obtain the whole base phrases as described in 
section 5.2. The result of using the grammar rules is 
shown in table 3. 
With the help of grammar rules, all kinds of base 
phrases improved their f-measures though the 
precisions or recalls of some types decrease slightly. 
Comparing with the baseline results in table 2, all 
the recalls increase significantly. However, the 
recalls of sp, tp and mp still do not satisfy us. There 
are more than twenty structures of np which also 
belong to tp or sp. Except in the case where mp and 
mbar have the same structure {m}, they are easily 
distinguished in other structures. (Mbar is always 
composed of numerals and mp always ends with a 
quantifier.) In order to distinguish tp from np, sp 
from np and mbar from mp, we use lexical 
information for the type disambiguation. The 
results are shown in table 4. 
Table 4: The result after using lexical 
information 
 Precision Recall F_M 
np 95.0% 91.9% 93.5% 
vp 95.0% 94.3% 94.6% 
sp 69.2% 71.3% 70.2% 
tp 79.8% 84.1% 81.9% 
ap 93.1% 90.0% 91.5% 
bp 91.6% 79.4% 85.0% 
dp 97.6% 98.1% 97.8% 
mp 93.4% 90.9% 92.1% 
mbar 67.6% 54.1% 60.1% 
Total 94.4% 92.5% 93.4% 
  From the table 4, we could see improvement in 
all the results (precisions and recalls) of mp and 
mbar. It shows that the lexical information is 
effective for distinguishing between them. On the 
contrary, although the f-measures of np and sp 
increase, their precisions decline. Thus, those words 
marking tp and sp are not appropriate for 
disambiguation. We could see the effect of lexical 
information is limited because it is difficult to find 
the words that could distinguish different types of 
base phrases.  
6 Conclusions 
The experiment on identifying Chinese base 
phrases shows that the definition of Chinese base 
phrases is suitable for parsing. It shows good results 
and the efficiency of the proposed approach in 
simplifying sentence structures. Many tasks such as 
chunking on high level could benefit from this. 
With the system described here, we get 9 types of 
Chinese base phrases, and acquire high precisions 
and recalls on most types of base phrases. The 
results of the experiment also show that the use of 
grammar rules is necessary. Grammar rules have 
effects on boundary disambiguation particularly. 
The lexical information is effective in 
distinguishing between mbar and mp.  
Acknowledgements 
This work was supported by the National Science 
Foundation of China (Grant No. 69903007), 
National 973 Foundation (Grant No. 1998030507) 
and National 863 Plan (Grant No. 2001AA114040). 
References 
Abney, Steven. (1991) Parsing by chunks. In 
Berwick, Abney, and Tenny, editors, 
Principle-Based Parsing. Kluwer Academic 
Publishers. 
Erik F. Tjong Kim Sang and Sabine Buchholz. 
(2000). ?Introduction to CoNLL-200 Shared 
Task: Chunking?. Proceedings of CoNLL-2000 
and LLL-2000. Lisbon, Portugal. 127-132. 
J. P. Gee and F. Grosjean (1983) Performance 
structures: A psycholinguistic and linguistic 
appraisal. Cognitive Psychology, 15:411-458 
Sun Honglin (2001) A Content Chunk Parser for 
Unrestricted Chinese Text, Dissertation for the 
degree of Doctor of Science, Peking University.  
Walter Daelemans, Jakub Zavrel, Ko van der Sloot 
(2001) TiMBL:Tilburg Memory-Based Learner 
version 4.0 Reference Guide. 
http://ilk.kub.nl/downloads/pub/papers/ilk0104.p
s.pz. 
Wojciech Skut and Thorsten Brants (1998) Chunk 
Tagger, Statistical Recognition of Noun Phrase, 
In ESSLLI-98 Workshop on Automated 
Acquisition of Syntax and Parsing, Saarbrvcken. 
Zhao Jun (1998) The research on Chinese BaseNP 
Recognition and Structure Analysis, Dissertation 
for the degree of Doctor of Engineering, 
Tsinghua University. 
Zhao et al, (2000) Tie-jun ZHAO, et al ?Statistics 
Based Hybrid Approach to Chinese Base Phrase 
Identification?, In Proceedings of the Second 
Chinese Language Processing Workshop, ACL 
2000, 73-77. 
Zhou, Qiang (1996). Phrase Bracketing and 
Annotating on Chinese Language Corpus, Ph.D. 
dissertation, Peking University. 
 
 
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 94?101,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A SVM-based Model for Chinese Functional Chunk Parsing 
 
 
Yingze Zhao 
State Key Laboratory of Intelligent Technol-
ogy and Systems 
Dept. of Computer Science and Technology, 
Tsinghua University 
Beijing 100084, P. R. China  
zhaoyingze@gmail.com 
Qiang Zhou 
State Key Laboratory of Intelligent Technol-
ogy and Systems 
Dept. of Computer Science and Technology, 
Tsinghua University 
Beijing 100084, P. R. China 
zq-lxd@mail.tsinghua.edu.cn 
 
  
 
Abstract 
Functional chunks are defined as a series 
of non-overlapping, non-nested segments 
of text in a sentence, representing the im-
plicit grammatical relations between the 
sentence-level predicates and their argu-
ments. Its top-down scheme and com-
plexity of internal constitutions bring in a 
new challenge for automatic parser. In 
this paper, a new parsing model is pro-
posed to formulate the complete chunk-
ing problem as a series of boundary de-
tection sub tasks. Each of these sub tasks 
is only in charge of detecting one type of 
the chunk boundaries. As each sub task 
could be modeled as a binary classifica-
tion problem, a lot of machine learning 
techniques could be applied.  
In our experiments, we only focus on 
the subject-predicate (SP) and predicate-
object (PO) boundary detection sub tasks. 
By applying SVM algorithm to these sub 
tasks, we have achieved the best F-Score 
of 76.56% and 82.26% respectively. 
1 Introduction 
Parsing is a basic task in natural language proc-
essing; however, it has not been successful in 
achieving the accuracy and efficiency required 
by real world applications. As an alternative, 
shallow parsing or partial parsing has been pro-
posed to meet the current needs by obtaining 
only a limited amount of syntactic information 
needed by the application. In recent years, there 
has been an increasing interest in chunk parsing. 
From CoNLL-2000 to CoNLL-2005, a lot of ef-
forts have been made in the identification of ba-
sic chunks and the methods of combining them 
from bottom-up to form large, complex units. In 
this paper, we will apply functional chunks to 
Chinese shallow parsing. 
Functional chunks are defined as a series of 
non-overlapping, non-nested functional units in a 
sentence, such as subjects, predicates, objects, 
adverbs, complements and so on. These units 
represent the implicit grammatical relations be-
tween the sentence-level predicates and their ar-
guments. Different from the basic chunks de-
fined by Abney (1991), functional chunks are 
generated from a top-down scheme, and thus 
their constitutions may be very complex. In addi-
tion, the type of a functional chunk could not be 
simply determined by its constitution, but de-
pends heavily on the context. Therefore, we will 
have new challenges in the functional chunk 
parsing. 
Ramshaw and Marcus (1995) first introduced 
the machine learning techniques to chunking 
problem. By formulating the NP-chunking task 
as a tagging process, they marked each word 
with a tag from set {B, I, O}, and successfully 
applied TBL to it. Inspired by their work, we 
introduce SVM algorithm to our functional 
chunking problem. Instead of using the BIO tag-
ging system, we propose a new model for solv-
ing this problem. In this model, we do not tag the 
words with BIO tags, but directly discover the 
chunk boundaries between every two adjacent 
functional chunks. Each of these chunk bounda-
ries will be assigned a type to it, which contains 
the information of the functional chunk types 
before and after it. Then we further decompose 
this model into a series of sub modules, each of 
which is in charge of detecting only one type of 
94
the chunk boundaries. As each sub module can 
be modeled as a binary classifier, various ma-
chine learning techniques could be applied. 
In our experiments, we focus on the subject-
predicate (SP) and predicate-object (PO) bound-
ary detection tasks, which are the most difficult 
but important parts in our parsing model. By ap-
plying SVM algorithm to these tasks, we achieve 
the best F-Score of 76.56% and 82.26% respec-
tively.                                                                                                              
This paper is organized as follows. In section 
2, we give a brief introduction to the concept of 
our functional chunks. In section 3, we propose 
the parsing model for Chinese functional chunk 
parsing. In section 4, we compare SVM with sev-
eral other machine learning techniques, and illus-
trate how competitive SVM is in our chunking 
task. In section 5, we build 2 sub modules based 
on SVM algorithm for SP and PO boundary de-
tection tasks. In section 6, some related work on 
functional chunk parsing is introduced. Section 7 
is the conclusion. 
2 Functional Chunk Scheme 
Functional chunks are defined as a series of 
non-overlapping, non-nested segments of text at 
the sentence level without leaving any words 
outside. Each chunk is labeled with a functional 
tag, such as subject, predicate, object and so on. 
These functional chunks in the sentence form a 
linear structure within which the grammatical 
relations between sentence-level predicates and 
their arguments or adjuncts are kept implicitly. 
Table 1 lists all the tags used in our functional 
chunk scheme:  
Table 1. Functional Chunk Tag Set. 
Chunk Tag Basic Function Description
S Subject 
P Predicate 
O Object 
J Raised Object 
D Adverbial adjunct 
C Complement 
T Independent constituent 
Y Modal particle 
Here, we list some examples to illustrate how 
these functional tags are used in Chinese sen-
tences. 
1. ?[D ?? /t (afternoon)  ? /?   [D ? /p 
(when)  ?/rN (I) ??/v (come to)  ????
/nS (Xi Bai Po village) ??/s (eastern entrance)  
?/n  ?/?  [D ?/d (already) [P ?/v (there is) 
[J ?/m  ?/qN (a) ??/n (brainman) [D ?/p  
??/rS (there) [P ??/v (waiting) [Y ?/y ?
/?  ? 
2. ?[T ???/l (frankly speaking)  ?/?  [S 
?/rN (that) [P ?/vC (was) [O ?/rN (I) ??/d 
(lifetime) ?/dN  ?/vM (can?t) ??/v (forget) 
?/u  ?/?  ? 
3. ?[S ??/n (time) [P ??/v  ?/u (schedule) 
[C ?/dD (very) ?/a (tight) ?/?  ? 
Compared with the basic chunk scheme de-
fined by Abney (1991), our functional chunk 
scheme has the following two main differences: 
(1) Functional chunks are not constituted from 
bottom-up, but generated from top-down, thus 
some functional chunks are usually longer and 
more complex than the basic chunks.  
We have a collection of 185 news files as our 
functional chunk corpus. Each file is manually 
annotated with functional chunks. There are 
about 200,000 Chinese words in the corpus. To 
investigate the complex constitutions of func-
tional chunks, we list the average chunk lengths 
(ACL) of different types in Table 2: 
Table 2. Average Chunk Lengths of Different 
Types. 
Chunk Type Count Word Sum ACL
P 21988 27618 1.26 
D 19795 46919 2.37 
O 14289 61401 4.30 
S 11920 34479 2.89 
J 855 2083 2.44 
Y 594 604 1.02 
T 407 909 2.23 
C 244 444 1.82
From the table above, we can find that O 
chunk has the longest average length of 4.30 
words, and S chunk has the second longest aver-
age length of 2.89 words, and D chunk has an 
average length of 2.37 words. Although the aver-
age length doesn?t seem so long, the length of a 
specific chunk varies greatly.  
In Table 3, we list some detailed length distri-
butional data of three chunks. 
Table 3. Length Distribution of S, O and D 
Chunks. 
Chunk Length # of S # of O # of D
1 5322 3537 12147
2 2093 2228 2499 
3 1402 2117 1431 
4 917 1624 1010 
5 627 1108 696 
>5 1559 3675 2013 
Sum 11920 14289 19796
95
From the table above, we can find that there 
are totally 1559 S chunks with a length of more 
than 5 words which takes up 13.08% of the total 
number. And when we refer to the S chunks with 
more than 3 words, the percentage will increase 
to 26.03%. These long chunks are usually consti-
tuted with several complex phrases or clauses as 
the modifiers of a head word. Among the O 
chunks, 25.72% of them have a length of more 
than 5 words, and 44.84% of them are longer 
than 3 words. The reason why O chunks have a 
longer length may be that many of them contain 
the entire clauses. Although most of the D 
chunks are less than 5 words, some constituted 
with complex preposition phrases can still be 
very long. 
The complex constitutions of S, O, D chunks 
are the main parsing difficulties. 
(2) The type of functional chunks can?t be 
simply determined by their constitutions, but de-
pends heavily on their contexts. 
As the constitution of a basic chunk is very 
simple, its type can be largely determined by its 
head word, but in the case of functional chunks, 
the relationships between the functional chunks 
play an important role. For example, a NP phrase 
before a P chunk can be identified as a subject 
chunk, but in other sentences, when it follows 
another P chunk, it will be recognized as an ob-
ject chunk. Thus we can?t determine the type of a 
functional chunk simply by its constitution. 
The context dependencies of functional 
chunks bring a new challenge for our chunk 
parser. 
In the next section, we will propose a top-
down model for Chinese functional chunk pars-
ing. Since the functional chunk boundaries have 
the information of linking two adjacent chunks, 
they will be very helpful in the determination of 
chunk types. 
3 Parsing Model 
The Chinese functional chunk parser takes a 
stream of segmented and tagged words as its in-
put, and outputs all the functional chunk bounda-
ries in a sentence. In this section, we will present 
a parsing model which formulates the functional 
chunk parsing problem as a boundary detection 
task, and then decompose this model into a series 
of sub modules that are easy to build. 
3.1 Formulation 
Functional chunks have the property of exhaust-
ibility and no words will be left outside the 
chunks. Thus we don?t need to find the end posi-
tion for a functional chunk as it could be identi-
fied by the start of the next one. In this case, we 
can simply regard the chunking task as a process 
of cutting the input sentence into several seg-
ments of words, each of which is labeled with a 
functional tag. Based on this idea, we can model 
the functional chunk parsing problem as a 
boundary detection task. 
Let S=<W, T> denote the input sentence to be 
parsed by the functional chunk parser, where 
W=w1w2w3?wn is the sequence of words in S, 
and T=t1t2t3?tn is sequence of the POS tags as-
signed to each word in W. If wi is a punctuation 
mark, ti will be equal to wi. 
A chunk boundary is defined as a pair <C1, 
C2> where  C1 ,C2 ?{S, P, O, J, D, C, T, Y}, C1 
is the chunk type before this boundary and C2 is 
the chunk type following it. The output of the 
chunk parser is denoted as O=<B, P> where 
B=b1b2b3?bm is the sequence of chunk bounda-
ries generated by the parser, and P=p1p2p3?pm is 
the corresponding positions of b1b2b3?bm in the 
sentence. 
Chinese functional chunk parser can be con-
sidered as a function h(S) which maps the input 
sentence S to the chunk boundary sequence O.  
Take the following sentence for example: 
?14  ??/n(Nuclear electricity) 1 ?/vC(is) 2 
?/m(a)  3 ?/qN(kind) 4 ??/a(safe) 5 ?/? 6  
??/a(safe) 7 ?/? 8 ??/a(economical) 9 ?/u 
10  ??/n(energy) 11 ?/?? 
?Nuclear electricity is a kind of safe, clean and 
economical energy.? 
In this sentence, there are totally 12 Chinese 
words (punctuation marks are treated the same 
way as words) with 11 numbers falling between 
them indicating the positions where a functional 
chunk boundary may appear. If the input sen-
tence is parsed correctly by the functional chunk 
parser, a series of boundaries will arise at posi-
tion 1 and 2, which are illustrated as below: 
?14  ??/n <S, P> ?/vC <P, O> ?/m ?/qN ?
?/a ?/?  ??/a ?/?  ??/a ?/u  ??/n ?
/?? 
 From the information provided by these 
boundaries, we can easily identify the functional 
chunks in the sentence: 
?14  [S ??/n  [P ?/vC  [O ?/m  ?/qN  ?
?/a  ?/?  ??/a  ?/?  ??/a  ?/u  ??
/n  ?/?? 
96
3.2 Decomposition of Parsing Model 
The functional chunk parser presented above 
could be further divided into several sub modules, 
each of which is only in charge of detecting one 
type of the chunk boundaries in a sentence. The 
sub module in charge of detecting boundary b 
could be formulated as a Boolean function hb(S, i) 
where S is the input sentence and i is the position 
between word wi and wi+1. Function hb(S, i) will 
take true if there is a chunk boundary of type b at 
position i, and it will take false if there?s not. 
Since the Boolean function hb(S, i) can be treated 
as a binary classifier, many machine learning 
techniques could be applied. 
If we combine every two chunk types in the 
tag set, we can make a total number of 8*8=64 
boundary types in our chunking task. However, 
not all of them appear in the natural language 
text, for example, we don?t have any SO bounda-
ries in our corpus as S and O chunks can?t be-
come neighbors in a sentence without any P 
chunks between them. In our corpus, we could 
find 43 boundary types, but only a small number 
of them are used very frequently. In table 4, we 
list the 5 most frequently used boundaries in our 
corpus:  
Table 4. The 5 Most Frequently Used Bounda-
ries in the Corpus. 
Boundary Type Count 
PO 14209 
DP 11459 
SD 6156 
DD 5238 
SP 5233 
The top 5 boundaries take up 67.76% of all the 
62418 boundaries in our corpus. If we further 
investigate the chunk types associated with these 
boundaries, we can find that only four types are 
involved: P, D, O and S. Referred to Table 2, we 
can find that these chunks are also the 4 most 
frequently used chunks in our corpus. 
In most cases, S, P, and O chunks constitute 
the backbone of a Chinese sentence, and they 
usually contain the most useful information we 
need. Therefore, we are more concerned about S, 
P and O chunks. In the following sections, we 
will focus on the construction of sub modules for 
SP and PO boundary detection tasks. 
4 Statistical Model Selection 
After decomposing the parsing model into sev-
eral sub modules, a lot of machine learning tech-
niques could be applied to the constructions of 
these sub modules. 
SVM 1  is a machine learning technique for 
solving the binary classification problems. It is 
well known for its good generalization perform-
ance and high efficiency. In this section, we will 
make a performance comparison between SVM  
(Vapnik, 1995) and several other machine learn-
ing techniques including Na?ve Bayes, ID3 2 
(Quinlan, 1986) and C4.53 (Quinlan, 1993), and 
then illustrates how competitive SVM is in the 
boundary detection tasks. 
4.1 Experimental Data 
The corpus we use here is a collection of 185 
news files which are manually corrected after 
automatic sentence-split, word segmentation and 
part-of-speech tagging. After these processes, 
they have been manually annotated with func-
tional chunks. Among the 185 files, 167 of them 
are taken as the training data and the remaining 
18 are left as the test data, which takes up ap-
proximately 10% of all the data.  
In our experiments, we will use feature tem-
plates to describe which features are to be used 
in the generation of feature vectors. For example, 
if the current feature template we use is w-1t2, 
then the feature vector generated at position i 
will take the first word on the left and the second 
word tag on the right as its features. 
Before we perform any experiments, all the 
data have been converted to the vectors that are 
acceptable by different machine learning algo-
rithms. Thus we have a total number of 199268 
feature vectors generated from the 185 files. 
Among them, 172465 vectors are in the training 
data and 26803 vectors are in the test data. Two 
sets of training and test data are prepared respec-
tively for the SP and PO boundary detection 
tasks.  
The performance of each experiment is meas-
ured with 3 rates: precision, recall and F?=1, 
where precision is the percentage of detected 
boundaries that are correct, recall is the percent-
age of boundaries in the test data that are found 
by the parser, and F?=1 is defined as 
F?=(?2+1)*precision*recall/(?2*precision + recall) 
with ?=1.  
                                                 
1 The software package we use is SVMlight v6.00, it is avail-
able at http://svmlight.joachims.org/. We use linear kernel 
function and other default parameters in our experiments. 
2 We use the weka?s implementation of Na?ve Bayes and 
ID3 algorithms. Weak 3.4 is available at 
http://www.cs.waikato.ac.nz/ml/weka/. 
3 We use Quinlan?s C4.5 software package with its default 
parameters in our experiments. 
97
4.2 Algorithm Comparison 
We first use t-3t-2t-1t1t2 as the feature tem-
plate, and list all the experimental results in Ta-
ble 5 and Table 6. From these results, we can 
find that SVM has achieved the best precision, 
recall and F-Score in SP boundary detection task, 
while C4.5 has an overwhelming advantage in 
PO boundary detection task. In both tasks, Na?ve 
Bayes algorithm performs the worst, which 
makes us very disappointed. 
Table 5. Results of Different Algorithms in SP 
Boundary Detection Task. 
Algorithm Precision Recall F?=1 
SVM 82.21% 57.10% 67.39%
ID3 67.60% 50.70% 57.94%
C4.5 81.10% 44.60% 57.55%
Na?ve Bayes 47.90% 51.00% 49.40%
Table 6. Results of Different Algorithms in 
PO Boundary Detection Task. 
Algorithm Precision Recall F?=1 
C4.5 72.00% 74.70% 73.33%
SVM 67.27% 64.96% 66.09%
ID3 70.70% 59.90% 64.85%
Na?ve Bayes 48.10% 60.10% 53.43%
As the feature template we use here is too sim-
ple, the results we have got may not seem so per-
suasive. Therefore we decide to conduct another 
experiment using a more complex feature tem-
plate.  
In the following experiments, we will use w-
2w-1w1w2t-2t-1t1t2 as the feature template. The 
experimental results are listed in Table 7 and Ta-
ble 8. 
After adding the word information to the fea-
ture template, the dimensions of feature vectors 
used by some algorithms increase dramatically. 
We remove Na?ve Bayes algorithm from the fol-
lowing experiments, as it fails to deal with such 
high dimensional data.  
Table 7. Results of Different Algorithms in SP 
Boundary Detection Task. 
Algorithm Precision Recall F?=1 
SVM 82.25% 61.22% 70.19%
ID3 64.70% 51.70% 57.47%
C4.5 79.70% 37.40% 50.91%
Table 8. Results of Different Algorithms in 
PO Boundary Detection Task. 
Algorithm Precision Recall F?=1 
SVM 74.83% 86.99% 80.45%
C4.5 67.90% 79.90% 73.41%
ID3 75.10% 57.70% 65.26%
After applying the complex feature template, 
SVM still keeps the first place in SP boundary 
detection task. In PO boundary detection task, 
SVM successfully takes the place of C4.5, and 
achieves the best recall and F-Score among all 
the algorithms. Although the precision of ID3 is 
a little better than SVM, we still prefer SVM to 
ID3. It seems that the word information in the 
feature vectors is not so beneficial to decision 
tree algorithms as to SVM.  
We also notice that SVM can perform very ef-
ficiently even with a large number of features. In 
the second set experiments, it usually takes sev-
eral hours to train a decision tree model, but for 
SVM, the time cost is no more than 20 minutes. 
In addition, we can expect a better result by add-
ing more information to SVM algorithm without 
worrying about the dimension disaster problem 
in other algorithms. Therefore, we decide to base 
our parsing model on SVM algorithm. 
5 The SVM-based Parsing Model 
5.1 Baseline Models 
In this section, we will build 2 baseline models 
based on SVM for SP and PO boundary detec-
tion tasks respectively. By comprising the results 
of two different feature templates, we will illus-
trate how useful the word information is in our 
SVM based models.  
One feature template we use here is the simple 
template which only takes the POS tag informa-
tion as its features. The other one is the complex 
template which takes both word and tag informa-
tion as its features. To make sure the results are 
comparable, we restrict the context window to 4 
words. 
In the SP boundary detection sub task, we got 
the following results: 
Table 9. SP Boundary Detection Results.  
Feature template Precision Recall F?=1 
t-2t-1t1t2 76.25% 51.99% 61.83%
w-2w-1w1w2t-
2t-1t1t2 
82.25% 61.22% 70.19%
In the PO boundary detection sub task, we got 
the following results: 
Table 10. PO Boundary Detection Results.  
Feature template Precision Recall F?=1 
t-2t-1t1t2 66.42% 65.27% 65.84%
w-2w-1w1w2t-
2t-1t1t2 
74.83% 86.99% 80.45%
 
By taking the complex feature template, we 
have achieved the best F?=1 value of 70.19% in 
SP boundary detection experiment and 80.45% 
in PO experiment, both of which are much 
higher than those of the simple feature templates. 
From these results we can conclude that word 
information is very helpful in our SVM based 
98
models. Thus we will only use the feature tem-
plates with word information in the succeeding 
experiments. 
5.2 Expanding the Context Window 
In the previous section, the feature templates we 
use are restricted to a context window of 4 words, 
which might not be large enough to detect the 
boundaries between complex chunks. For exam-
ple, when parsing the sentence ?[P ??/v 1 [O 
??/a 2 ?/u 3 ??/n 4 ??/n 5 ??/vN 6 ??
/n?, the algorithm fails to detect the PO boundary 
at position 1. If we expand the context window to 
the noun word ???/n?, some of these errors 
may disappear. In the following experiments, we 
will expand the context window from a size of 4 
words to 10 words, and make a comparison be-
tween the different results. 
The 4 feature templates used here are listed 
below: 
T1: w-2w-1w1w2t-2t-1t1t2,  
T2: w-3w-2w-1w1w2w3t-3t-2t-1t1t2t3,  
T3: w-4w-3w-2w-1w1w2w3w4t-4t-3t-2t-
1t1t2t3t4 
T4: w-5w-4w-3w-2w-1w1w2w3w4w5t-5t-4t-
3t-2t-1t1t2t3t4t5.  
SP Boundary Detection Results
82.25% 83.84%
86.44% 86.15%
61.22%
66.34% 67.90%
68.89%70.19%
74.07% 76.06%
76.56%
55.00%
60.00%
65.00%
70.00%
75.00%
80.00%
85.00%
T1 T2 T3 T4
Precision Recall F-Score
  
   Figure 1. SP Boundary Detection Results. 
As we have expected, the performance of SP 
boundary detection experiment has been im-
proved as the context window expands from a 
size of 4 words to 8 words. However, the preci-
sion value meets its turning point at T3 after 
which it goes down, while F-Score and recall 
value still keep rising. From the curves shown in 
figure 1, we can find that the expansion of con-
text window size from 4 words to 6 words has an 
obvious improvement for performance, and after 
that only F-Score and recall could be improved.  
PO Boundary Detection Results
74.87%
77.17% 78.16%
78.74%
86.83% 86.42% 86.17% 86.12%
80.41%
81.53% 81.97% 82.26%
72.00%
74.00%
76.00%
78.00%
80.00%
82.00%
84.00%
86.00%
88.00%
T1 T2 T3 T4
Precision Recall F-Score
 
Figure 2. SP Boundary Detection Results. 
In contrast to the significant improvement we 
have achieved in the SP experiments, the results 
of PO experiments are not so exciting. As the 
context window expands, the precision value 
keeps rising while the recall value keeps declin-
ing. Fortunately, we have obtained a very slight 
increase of F-Score from these efforts.  
Although it is very difficult to improve the 
performance of PO boundary detection by simply 
expanding the context window, we?ve still got a 
better result than that of SP. If we examine the 
results of the two tasks carefully, we can find a 
very interesting difference between them: in SP 
boundary detection task, it?s very easier to get a 
better precision than recall, but in PO experiment, 
as the O chunks have a longer length, they are 
more likely to be cut into small pieces, and thus 
it?s easier to get a better recall than precision. 
5.3 Error Analysis 
In our experiments, the recall value can be sim-
ply raised by adding a positive bias value to the 
SVM classifier. However, we can?t do the same 
thing to improve the precision value. Thus, in the 
following analysis, we are only focus on the er-
rors that deter the improvement of precision 
value. 
There are 2 kinds of errors influencing the 
precision value of the test results: One is the 
wrongly detected chunk boundaries (WDB) 
within chunks (these chunk boundaries are de-
tected by the program, but they don?t exist in the 
training data). This kind of error tends to cut a 
large chunk into several small pieces. The other 
is the misclassification of chunk boundary types 
(MBT) at the chunk boundaries (There exists a 
99
chunk boundary at that position, but chunk 
boundary type labeled by the program is wrong). 
 In the following analysis, by comparing the 
numbers of errors in the test results of T1 (w-2w-
1w1w2t-2t-1t1t2) and T4 (w-5w-4w-3w-2w-
1w1w2w3w4w5t-5t-4t-3t-2t-1t1t2t3t4t5), we 
will point out which kind of errors could be ef-
fectively eliminated by the expansion of context 
window and which of them couldn?t. Through 
this analysis, we hope to get some knowledge of 
what efforts should be made in our further study.  
In SP boundary detection task, we list the 
number of wrongly detected chunk boundaries 
(#WDB) and the corresponding chunk types (CT) 
where WDB arises in the following table. 
Table 11. Wrongly Detected Chunk Bounda-
ries in the Test Results of T1 and T4. 
CT #WDB of T1 #WDB of T4 T4-T1
O 17 18 1 
S 17 18 1 
D 7 6 -1 
C 0 1 1 
P 2 1 -1 
T 1 1 0 
Sum 44 45 1 
From the above table, we find that the number 
of wrongly detected boundaries seems to be un-
changed during the expansion of context window. 
But when we refer to the second type of errors, 
the expansion of context window does help. We 
list the misclassified boundary types (MBT) and 
the error numbers (#MB) in the below table. In 
SP boundary detection task, MBT is wrongly 
recognized as boundary type SP. 
Table 12. Misclassified Chunk Boundaries in 
the Test Results of T1 and T4. 
MBT #MB of T1 #MB of T4 T4-T1
OP 9 3 -6 
JP 8 2 -6 
DP 23 20 -3 
SD 6 6 0 
DS 1 1 0 
Sum 47 32 -15 
From the above table, we can find that the 
misclassifications of OP, JP and DP as SP have 
been largely reduced by expanding the context 
window, but the misclassifications of DS and SD 
remain the same. Therefore, we should try some 
other methods for D chunks in our future work. 
In PO boundary detection task, the expansion 
of context window seems to be very effective. 
We list all the results in the below table: 
Table 14. Wrongly Detected Chunk Bounda-
ries in the Test Results of T1 and T4. 
CT #WDB of T1 #WDB of T4 T4-T1
O 251 196 -55 
S 106 76 -30 
D 92 55 -37 
P 56 64 8 
T 4 4 0 
C 1 1 0 
J 0 1 1 
Sum 510 397 -113 
It?s very exciting to see that by expanding the 
window size, the number of WDB decreases dra-
matically from 510 to 397. But it fails to elimi-
nate the WDB errors within P, T, C, and J 
chunks. 
In PO boundary detection task, MBT is 
wrongly recognized as boundary type PO. We 
list the error data of T1 and T4 in the below table. 
Table 13. Misclassified Chunk Boundaries in 
the Test Results of T1 and T4. 
MBT #MB of T1 #MB of T4 T4-T1
PJ 17 18 1 
PD 9 9 0 
PC 8 8 0 
SP 6 6 0 
PS 5 5 0 
SD 5 4 -1 
DP 3 2 -1 
TS 3 3 0 
OD 1 0 -1 
PY 1 1 0 
Sum 58 56 -2 
In contrast to the results of SP boundary detec-
tion task, the MBT errors could not be largely 
reduced by simply expanding the context win-
dow. Therefore, we need to pay more attention to 
these problems in our future work. 
6 Related works 
After the work of Ramshaw and Marcus (1995) , 
many machine learning techniques have been 
applied to the basic chunking task, such as Sup-
port Vector Machines (Kudo and Matsumoto, 
2001), Hidden Markov Model(Molina and Pla 
2002), Memory Based Learning (Sang, 2002), 
Conditional Random Fields (Sha and Pereira, 
2003), and so on. But only a small amount of 
attention has been paid to the functional chunk 
parsing problem. 
Sandra and Erhard (2001) tried to construct 
the function-argument structures based on the 
pre-chunked input. They proposed a similarity 
based algorithm to assign the functional labels to 
complete syntactic structures, and achieved a 
100
precision of 89.73% and 90.40% for German and 
English respectively. Different from our top-
down scheme, their function-argument structures 
are still constituted from bottom-up, and the pre-
chunked input helps simplify the chunking proc-
ess. 
Elliott and Qiang Zhou (2001) used the BIO 
tagging system to identify the functional chunks 
in a sentence. In their experiments, they used 
C4.5 algorithm to build the parsing model, and 
focused their efforts on the selection of feature 
sets. After testing 5 sets of features, they have 
achieved the best f-measure of 0.741 by using 
feature set E which contains all the features in 
other feature sets. Instead of using BIO tags in 
our chunking task, we introduced chunk bounda-
ries to help us identify the functional chunks, 
which could provide more relational information 
between the functional chunks.  
7 Conclusions and Future Works 
In this paper, we have applied functional chunks 
to Chinese shallow parsing. Since the functional 
chunks have the properties of linearity and ex-
haustibility, we can formulate the functional 
chunk parsing problem as a boundary detection 
task. By applying the divide-and-conquer strat-
egy, we have further decomposed the parsing 
model into a series of sub modules, each of 
which is only in charge of one boundary type. In 
this way, we provide a very flexible framework 
within which different machine learning tech-
niques could be applied. In our experiments, we 
build two sub modules based on SVM for solv-
ing the SP and PO boundary detection tasks. 
Thanks to the good generalization performance 
and high efficiency of SVM, we can successfully 
deal with a large number of features. By expand-
ing the context window, we have achieved the 
best F-Score of 76.56% and 82.26 for SP and PO 
boundary detection tasks. 
The 2 sub modules we have built are only 
parts of the Chinese functional chunk parser. Al-
though the results we have got here seem some-
what coarse, they could already be used in some 
simple tasks. In the future, we will build the 
other sub modules for the remaining types of the 
chunk boundaries. After all these work, there 
may be some inconsistent chunk boundaries in 
the results, thus we need to solve the inconsis-
tency problems and try to identify all the func-
tional chunks in a sentence by combining these 
chunk boundaries. 
Acknowledgements 
This work was supported by the Chinese National 
Science Foundation (Grant No. 60573185, 
60520130299).  
References 
Elliott Franco Dr?bek and Qiang Zhou. 2001. Ex-
periments in Learning Models for Functional 
Chunking of Chinese Text. IEEE International 
Workshop on Natural Language processing and 
Knowledge Engineering, Tucson, Arizona, pages 
859-864. 
E.F. Tjong Kim Sang. 2002. Memory-based shallow 
parsing, Journal of Machine Learning Research 2, 
pages 559-594. 
F. Sha and F. Pereira. 2003. Shallow parsing with 
conditional random fields. In Proceedings of Hu-
man Language Technology Conference / North 
American Chapter of the Association for Computa-
tional Linguistics annual meeting. 
Ian H. Witten and Eibe Frank. 2005. Data Mining: 
Practical machine learning tools and techniques, 
2nd Edition, Morgan Kaufmann, San Francisco.  
Taku Kudo and Yuji Matsumoto. 2001. Chunking 
with support vector machines. Proceedings of the 
Second Meeting of the North American Chapter of 
the Association for Computational Linguistics. 
Pittsburgh, PA. 
Lance Ramshaw and Mitch Marcus. 1995. Text 
chunking using transformation-based learning. In 
Proceedings of the Third Workshop on Very Large 
Corpora, pages 82?94. 
Quinlan, J. Ross. 1986. Induction of decision trees. 
Machine Learning, 1(1), pages 81-106. 
Quinlan, J. Ross. 1993. C4.5: Programs for Machine 
Learning. San Mateo, CA: Morgan Kaufmann. 
Steven Abney. 1991. Parsing by chunks. In Principle-
Based Parsing, Kluwer Academic Publishers, 
Dordrecht, pages 257?278. 
Sandra K?bler and Erhard W. Hinrichs. 2001. From 
chunks to function-argument structure: A similar-
ity-based approach. In Proceedings of ACL/EACL 
2001, Toulouse, France, 2001, pages 338 - 345. 
Thorsten Joachims. 1999. Advances in Kernel Meth-
ods - Support Vector Learning, chapter Making 
large-Scale SVM Learning Practical. MIT-Press. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer, New York. 
101
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 839?849,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Distant Supervision for Relation Extraction with Matrix Completion
Miao Fan
?,?,?
, Deli Zhao
?
, Qiang Zhou
?
, Zhiyuan Liu
,?
, Thomas Fang Zheng
?
, Edward Y. Chang
?
?
CSLT, Division of Technical Innovation and Development,
Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, China.

Department of Computer Science and Technology, Tsinghua University, China.
?
HTC Beijing Advanced Technology and Research Center, China.
?
fanmiao.cslt.thu@gmail.com
Abstract
The essence of distantly supervised rela-
tion extraction is that it is an incomplete
multi-label classification problem with s-
parse and noisy features. To tackle the s-
parsity and noise challenges, we propose
solving the classification problem using
matrix completion on factorized matrix of
minimized rank. We formulate relation
classification as completing the unknown
labels of testing items (entity pairs) in a s-
parse matrix that concatenates training and
testing textual features with training label-
s. Our algorithmic framework is based on
the assumption that the rank of item-by-
feature and item-by-label joint matrix is
low. We apply two optimization model-
s to recover the underlying low-rank ma-
trix leveraging the sparsity of feature-label
matrix. The matrix completion problem is
then solved by the fixed point continuation
(FPC) algorithm, which can find the glob-
al optimum. Experiments on two wide-
ly used datasets with different dimension-
s of textual features demonstrate that our
low-rank matrix completion approach sig-
nificantly outperforms the baseline and the
state-of-the-art methods.
1 Introduction
Relation Extraction (RE) is the process of gen-
erating structured relation knowledge from un-
structured natural language texts. Traditional su-
pervised methods (Zhou et al, 2005; Bach and
Badaskar, 2007) on small hand-labeled corpora,
such as MUC
1
and ACE
2
, can achieve high pre-
cision and recall. However, as producing hand-
labeled corpora is laborius and expensive, the su-
pervised approach can not satisfy the increasing
1
http://www.itl.nist.gov/iaui/894.02/related projects/muc/
2
http://www.itl.nist.gov/iad/mig/tests/ace/
Figure 1: Training corpus generated by the basic
alignment assumption of distantly supervised re-
lation extraction. The relation instances are the
triples related to President Barack Obama in the
Freebase, and the relation mentions are some sen-
tences describing him in the Wikipedia.
demand of building large-scale knowledge reposi-
tories with the explosion of Web texts. To address
the lacking training data issue, we consider the dis-
tant (Mintz et al, 2009) or weak (Hoffmann et al,
2011) supervision paradigm attractive, and we im-
prove the effectiveness of the paradigm in this pa-
per.
The intuition of the paradigm is that one
can take advantage of several knowledge bases,
such as WordNet
3
, Freebase
4
and YAGO
5
, to
automatically label free texts, like Wikipedia
6
and New York Times corpora
7
, based on some
heuristic alignment assumptions. An example
accounting for the basic but practical assumption
is illustrated in Figure 1, in which we know
that the two entities (<Barack Obama,
U.S.>) are not only involved in the rela-
tion instances
8
coming from knowledge bases
(President-of(Barack Obama, U.S.)
and Born-in(Barack Obama, U.S.)),
3
http://wordnet.princeton.edu
4
http://www.freebase.com
5
http://www.mpi-inf.mpg.de/yago-naga/yago
6
http://www.wikipedia.org
7
http://catalog.ldc.upenn.edu/LDC2008T19
8
According to convention, we regard a structured triple
r(e
i
, e
j
) as a relation instance which is composed of a pair of
entities <e
i
, e
j
>and a relation name r with respect to them.
839
Error MatrixCompleted Low?rank Matrix
?
Observed Sparse Matrix
TrainingItems
TestingItems
Incomplete LabelsNoisy Features
Figure 2: The procedure of noise-tolerant low-rank matrix completion. In this scenario, distantly super-
vised relation extraction task is transformed into completing the labels for testing items (entity pairs) in
a sparse matrix that concatenates training and testing textual features with training labels. We seek to
recover the underlying low-rank matrix and to complete the unknown testing labels simultaneously.
but also co-occur in several relation mentions
9
appearing in free texts (Barack Obama is
the 44th and current President of
the U.S. and Barack Obama was born
in Honolulu, Hawaii, U.S., etc.). We
extract diverse textual features from all those
relation mentions and combine them into a rich
feature vector labeled by the relation names
(President-of and Born-in) to produce a
weak training corpus for relation classification.
This paradigm is promising to generate large-
scale training corpora automatically. However, it
comes up against three technical challeges:
? Sparse features. As we cannot tell what
kinds of features are effective in advance, we
have to use NLP toolkits, such as Stanford
CoreNLP
10
, to extract a variety of textual fea-
tures, e.g., named entity tags, part-of-speech
tags and lexicalized dependency paths. Un-
fortunately, most of them appear only once in
the training corpus, and hence leading to very
sparse features.
? Noisy features. Not all relation mentions
express the corresponding relation instances.
For example, the second relation mention in
Figure 1 does not explicitly describe any rela-
tion instance, so features extracted from this
sentence can be noisy. Such analogous cases
commonly exist in feature extraction.
? Incomplete labels. Similar to noisy fea-
9
The sentences that contain the given entity pair are called
relation mentions.
10
http://nlp.stanford.edu/downloads/corenlp.shtml
tures, the generated labels can be in-
complete. For example, the fourth re-
lation mention in Figure 1 should have
been labeled by the relation Senate-of.
However, the incomplete knowledge base
does not contain the corresponding relation
instance (Senate-of(Barack Obama,
U.S.)). Therefore, the distant supervision
paradigm may generate incomplete labeling
corpora.
In essence, distantly supervised relation extrac-
tion is an incomplete multi-label classification task
with sparse and noisy features.
In this paper, we formulate the relation-
extraction task from a novel perspective of using
matrix completion with low rank criterion. To the
best of our knowledge, we are the first to apply this
technique on relation extraction with distant super-
vision. More specifically, as shown in Figure 2, we
model the task with a sparse matrix whose rows
present items (entity pairs) and columns contain
noisy textual features and incomplete relation la-
bels. In such a way, relation classification is trans-
formed into a problem of completing the unknown
labels for testing items in the sparse matrix that
concatenates training and testing textual features
with training labels, based on the assumption that
the item-by-feature and item-by-label joint matrix
is of low rank. The rationale of this assumption
is that noisy features and incomplete labels are
semantically correlated. The low-rank factoriza-
tion of the sparse feature-label matrix delivers the
low-dimensional representation of de-correlation
for features and labels.
840
We contribute two optimization models, DRM-
C
11
-b and DRMC-1, aiming at exploiting the s-
parsity to recover the underlying low-rank matrix
and to complete the unknown testing labels simul-
taneously. Moreover, the logistic cost function is
integrated in our models to reduce the influence of
noisy features and incomplete labels, due to that
it is suitable for binary variables. We also modify
the fixed point continuation (FPC) algorithm (Ma
et al, 2011) to find the global optimum.
Experiments on two widely used datasets
demonstrate that our noise-tolerant approaches
outperform the baseline and the state-of-the-art
methods. Furthermore, we discuss the influence of
feature sparsity, and our approaches consistently
achieve better performance than compared meth-
ods under different sparsity degrees.
2 Related Work
The idea of distant supervision was firstly pro-
posed in the field of bioinformatics (Craven and
Kumlien, 1999). Snow et al (2004) used Word-
Net as the knowledge base to discover more h-
pyernym/hyponym relations between entities from
news articles. However, either bioinformatic
database or WordNet is maintained by a few ex-
perts, thus hardly kept up-to-date.
As we are stepping into the big data era, the
explosion of unstructured Web texts simulates us
to build more powerful models that can automat-
ically extract relation instances from large-scale
online natural language corpora without hand-
labeled annotation. Mintz et al (2009) adopt-
ed Freebase (Bollacker et al, 2008; Bollacker
et al, 2007), a large-scale crowdsourcing knowl-
edge base online which contains billions of rela-
tion instances and thousands of relation names, to
distantly supervise Wikipedia corpus. The basic
alignment assumption of this work is that if a pair
of entities participate in a relation, all sentences
that mention these entities are labeled by that rela-
tion name. Then we can extract a variety of textu-
al features and learn a multi-class logistic regres-
sion classifier. Inspired by multi-instance learn-
ing (Maron and Lozano-P?erez, 1998), Riedel et al
(2010) relaxed the strong assumption and replaced
all sentences with at least one sentence. Hoff-
mann et al (2011) pointed out that many entity
pairs have more than one relation. They extend-
11
It is the abbreviation for Distant supervision for Relation
extraction with Matrix Completion
ed the multi-instance learning framework (Riedel
et al, 2010) to the multi-label circumstance. Sur-
deanu et al (2012) proposed a novel approach to
multi-instance multi-label learning for relation ex-
traction, which jointly modeled all the sentences in
texts and all labels in knowledge bases for a giv-
en entity pair. Other literatures (Takamatsu et al,
2012; Min et al, 2013; Zhang et al, 2013; Xu
et al, 2013) addressed more specific issues, like
how to construct the negative class in learning or
how to adopt more information, such as name en-
tity tags, to improve the performance.
Our work is more relevant to Riedel et al?s
(2013) which considered the task as a matrix fac-
torization problem. Their approach is composed
of several models, such as PCA (Collins et al,
2001) and collaborative filtering (Koren, 2008).
However, they did not concern about the data noise
brought by the basic assumption of distant super-
vision.
3 Model
We apply a new technique in the field of ap-
plied mathematics, i.e., low-rank matrix comple-
tion with convex optimization. The breakthrough
work on this topic was made by Cand`es and Recht
(2009) who proved that most low-rank matrices
can be perfectly recovered from an incomplete
set of entries. This promising theory has been
successfully applied on many active research ar-
eas, such as computer vision (Cabral et al, 2011),
recommender system (Rennie and Srebro, 2005)
and system controlling (Fazel et al, 2001). Our
models for relation extraction are based on the
theoretic framework proposed by Goldberg et al
(2010), which formulated the multi-label trans-
ductive learning as a matrix completion problem.
The new framework for classification enhances the
robustness to data noise by penalizing differen-
t cost functions for features and labels.
3.1 Formulation
Suppose that we have built a training corpus for
relation classification with n items (entity pairs),
d-dimensional textual features, and t labels (rela-
tions), based on the basic alignment assumption
proposed by Mintz et al (2009). Let X
train
?
R
n?d
and Y
train
? R
n?t
denote the feature matrix
and the label matrix for training, respectively. The
linear classifier we adopt aims to explicitly learn
the weight matrix W ? R
d?t
and the bias column
841
vector b ? R
t?1
with the constraint of minimizing
the loss function l,
arg min
W,b
l(Y
train
,
[
1 X
train
]
[
b
T
W
]
), (1)
where 1 is the all-one column vector. Then we can
predict the label matrix Y
test
? R
m?t
of m testing
items with respect to the feature matrix X
test
?
R
m?d
. Let
Z =
[
X
train
Y
train
X
test
Y
test
]
.
This linear classification problem can be trans-
formed into completing the unobservable entries
in Y
test
by means of the observable entries in
X
train
, Y
train
and X
test
, based on the assumption
that the rank of matrix Z ? R
(n+m)?(d+t)
is low.
The model can be written as,
arg min
Z?R
(n+m)?(d+t)
rank(Z)
s.t. ?(i, j) ? ?
X
, z
ij
= x
ij
,
(1 ? i ? n+m, 1 ? j ? d),
?(i, j) ? ?
Y
, z
i(j+d)
= y
ij
,
(1 ? i ? n, 1 ? j ? t),
(2)
where we use ?
X
to represent the index set of ob-
servable feature entries in X
train
and X
test
, and
?
Y
to denote the index set of observable label en-
tries in Y
train
.
Formula (2) is usually impractical for real prob-
lems as the entries in the matrix Z are corrupted
by noise. We thus define
Z = Z
?
+ E,
where Z
?
as the underlying low-rank matrix
Z
?
=
[
X
?
Y
?
]
=
[
X
?
train
Y
?
train
X
?
test
Y
?
test
]
,
and E is the error matrix
E =
[
E
X
train
E
Y
train
E
X
test
0
]
.
The rank function in Formula (2) is a non-convex
function that is difficult to be optimized. The sur-
rogate of the function can be the convex nucle-
ar norm ||Z||
?
=
?
?
k
(Z) (Cand`es and Recht,
2009), where ?
k
is the k-th largest singular val-
ue of Z. To tolerate the noise entries in the error
matrix E, we minimize the cost functions C
x
and
C
y
for features and labels respectively, rather than
using the hard constraints in Formula (2).
According to Formula (1), Z
?
? R
(n+m)?(d+t)
can be represented as [X
?
,WX
?
] instead of
[X
?
, Y
?
], by explicitly modeling the bias vector
b. Therefore, this convex optimization model is
called DRMC-b,
arg min
Z,b
?||Z||
?
+
1
|?
X
|
?
(i,j)??
X
C
x
(z
ij
, x
ij
)
+
?
|?
Y
|
?
(i,j)??
Y
C
y
(z
i(j+d)
+ b
j
, y
ij
),
(3)
where ? and ? are the positive trade-off weights.
More specifically, we minimize the nuclear norm
||Z||
?
via employing the regularization terms, i.e.,
the cost functions C
x
and C
y
for features and la-
bels.
If we implicitly model the bias vector b,
Z
?
? R
(n+m)?(1+d+t)
can be denoted by
[1, X
?
,W
?
X
?
] instead of [X
?
, Y
?
], in which W
?
takes the role of [b
T
; W] in DRMC-b. Then we
derive another optimization model called DRMC-
1,
arg min
Z
?||Z||
?
+
1
|?
X
|
?
(i,j)??
X
C
x
(z
i(j+1)
, x
ij
)
+
?
|?
Y
|
?
(i,j)??
Y
C
y
(z
i(j+d+1)
, y
ij
)
s.t. Z(:, 1) = 1,
(4)
where Z(:, 1) denotes the first column of Z.
For our relation classification task, both features
and labels are binary. We assume that the actual
entry u belonging to the underlying matrix Z
?
is
randomly generated via a sigmoid function (Jor-
dan, 1995): Pr(u|v) = 1/(1 + e
?uv
), given the
observed binary entry v from the observed sparse
matrix Z. Then, we can apply the log-likelihood
cost function to measure the conditional probabil-
ity and derive the logistic cost function for C
x
and
C
y
,
C(u, v) = ? logPr(u|v) = log(1 + e
?uv
),
After completing the entries in Y
test
, we adop-
t the sigmoid function to calculate the conditional
probability of relation r
j
, given entity pair p
i
per-
taining to y
ij
in Y
test
,
Pr(r
j
|p
i
) =
1
1 + e
?y
ij
, y
ij
? Y
test
.
Finally, we can achieve Top-N predicted relation
instances via ranking the values of Pr(r
j
|p
i
).
842
4 Algorithm
The matrix rank minimization problem is NP-
hard. Therefore, Cand?es and Recht (2009) sug-
gested to use a convex relaxation, the nuclear nor-
m minimization instead. Then, Ma et al (2011)
proposed the fixed point continuation (FPC) algo-
rithm which is fast and robust. Moreover, Gold-
frab and Ma (2011) proved the convergence of the
FPC algorithm for solving the nuclear norm mini-
mization problem. We thus adopt and modify the
algorithm aiming to find the optima for our noise-
tolerant models, i.e., Formulae (3) and (4).
4.1 Fixed point continuation for DRMC-b
Algorithm 1 describes the modified FPC algorithm
for solving DRMC-b, which contains two steps for
each iteration,
Gradient step: In this step, we infer the ma-
trix gradient g(Z) and bias vector gradient g(b) as
follows,
g(z
ij
) =
?
?
?
?
?
?
?
1
|?
X
|
?x
ij
1+e
x
ij
z
ij
, (i, j) ? ?
X
?
|?
Y
|
?y
i(j?d)
1+e
y
i(j?d)
(z
ij
+b
j
)
, (i, j ? d) ? ?
Y
0, otherwise
and
g(b
j
) =
?
|?
Y
|
?
i:(i,j)??
Y
?y
ij
1 + e
y
ij
(z
i(j+d)
+b
j
)
.
We use the gradient descents A = Z ? ?
z
g(Z)
and b = b ? ?
b
g(b) to gradually find the global
minima of the cost function terms in Formula (3),
where ?
z
and ?
b
are step sizes.
Shrinkage step: The goal of this step is to min-
imize the nuclear norm ||Z||
?
in Formula (3). We
perform the singular value decomposition (SVD)
(Golub and Kahan, 1965) for A at first, and then
cut down each singular value. During the iteration,
any negative value in ?? ?
z
? is assigned by zero,
so that the rank of reconstructed matrix Z will be
reduced, where Z = Umax(?? ?
z
?, 0)V
T
.
To accelerate the convergence, we use a con-
tinuation method to improve the speed. ? is ini-
tialized by a large value ?
1
, thus resulting in the
fast reduction of the rank at first. Then the conver-
gence slows down as ? decreases while obeying
?
k+1
= max(?
k
?
?
, ?
F
). ?
F
is the final value of
?, and ?
?
is the decay parameter.
For the stopping criteria in inner iterations, we
define the relative error to measure the residual of
matrix Z between two successive iterations,
Algorithm 1 FPC algorithm for solving DRMC-b
Input:
Initial matrix Z
0
, bias b
0
; Parameters ?, ?;
Step sizes ?
z
, ?
b
.
Set Z = Z
0
, b = b
0
.
foreach ? = ?
1
> ?
2
> ... > ?
F
do
while relative error > ? do
Gradient step:
A = Z? ?
z
g(Z),b = b? ?
b
g(b).
Shrinkage step:
U?V
T
= SVD(A),
Z = U max(?? ?
z
?, 0) V
T
.
end while
end foreach
Output: Completed Matrix Z, bias b.
||Z
k+1
? Z
k
||
F
max(1, ||Z
k
||
F
)
? ?,
where ? is the convergence threshold.
4.2 Fixed point continuation for DRMC-1
Algorithm 2 is similar to Algorithm 1 except for
two differences. First, there is no bias vector b.
Second, a projection step is added to enforce the
first column of matrix Z to be 1. In addition, The
matrix gradient g(Z) for DRMC-1 is
g(z
ij
) =
?
?
?
?
?
?
?
1
|?
X
|
?x
i(j?1)
1+e
x
i(j?1)
z
ij
, (i, j ? 1) ? ?
X
?
|?
Y
|
?y
i(j?d?1)
1+e
y
i(j?d?1)
z
ij
, (i, j ? d? 1) ? ?
Y
0, otherwise
.
Algorithm 2 FPC algorithm for solving DRMC-1
Input:
Initial matrix Z
0
; Parameters ?, ?;
Step sizes ?
z
.
Set Z = Z
0
.
foreach ? = ?
1
> ?
2
> ... > ?
F
do
while relative error > ? do
Gradient step: A = Z? ?
z
g(Z).
Shrinkage step:
U?V
T
= SVD(A),
Z = U max(?? ?
z
?, 0) V
T
.
Projection step: Z(:, 1) = 1.
end while
end foreach
Output: Completed Matrix Z.
843
Dataset # of training
tuples
# of testing
tuples
% with more
than one label
# of features # of relation
labels
NYT?10 4,700 1,950 7.5% 244,903 51
NYT?13 8,077 3,716 0% 1,957 51
Table 1: Statistics about the two widely used datasets.
Model NYT?10 (?=2) NYT?10 (?=3) NYT?10 (?=4) NYT?10 (?=5) NYT?13
DRMC-b 51.4 ? 8.7 (51) 45.6 ? 3.4 (46) 41.6 ? 2.5 (43) 36.2 ? 8.8(37) 84.6 ? 19.0 (85)
DRMC-1 16.0 ? 1.0 (16) 16.4 ? 1.1(17) 16 ? 1.4 (17) 16.8 ? 1.5(17) 15.8 ? 1.6 (16)
Table 2: The range of optimal ranks for DRMC-b and DRMC-1 through five-fold cross validation. The
threshold ? means filtering the features that appear less than ? times. The values in brackets pertaining to
DRMC-b and DRMC-1 are the exact optimal ranks that we choose for the completed matrices on testing
sets.
5 Experiments
In order to conduct reliable experiments, we adjust
and estimate the parameters for our approaches,
DRMC-b and DRMC-1, and compare them with
other four kinds of landmark methods (Mintz et
al., 2009; Hoffmann et al, 2011; Surdeanu et al,
2012; Riedel et al, 2013) on two public datasets.
5.1 Dataset
The two widely used datasets that we adopt are
both automatically generated by aligning Freebase
to New York Times corpora. The first dataset
12
,
NYT?10, was developed by Riedel et al (2010),
and also used by Hoffmann et al (2011) and Sur-
deanu et al (2012). Three kinds of features, name-
ly, lexical, syntactic and named entity tag fea-
tures, were extracted from relation mentions. The
second dataset
13
, NYT?13, was also released by
Riedel et al (2013), in which they only regarded
the lexicalized dependency path between two enti-
ties as features. Table 1 shows that the two datasets
differ in some main attributes. More specifically,
NYT?10 contains much higher dimensional fea-
tures than NYT?13, whereas fewer training and
testing items.
5.2 Parameter setting
In this part, we address the issue of setting param-
eters: the trade-off weights ? and ?, the step sizes
?
z
and ?
b
, and the decay parameter ?
?
.
We set ? = 1 to make the contribution of the
cost function terms for feature and label matrices
equal in Formulae (3) and (4). ? is assigned by a
series of values obeying ?
k+1
= max(?
k
?
?
, ?
F
).
12
http://iesl.cs.umass.edu/riedel/ecml/
13
http://iesl.cs.umass.edu/riedel/data-univSchema/
We follow the suggestion in (Goldberg et al,
2010) that ? starts at ?
1
?
?
, and ?
1
is the largest
singular value of the matrix Z. We set ?
?
= 0.01.
The final value of ?, namely ?
F
, is equal to 0.01.
Ma et al (2011) revealed that as long as the non-
negative step sizes satisfy ?
z
< min(
4|?
Y
|
?
, |?
X
|)
and ?
b
<
4|?
Y
|
?(n+m)
, the FPC algorithm will guaran-
tee to converge to a global optimum. Therefore,
we set ?
z
= ?
b
= 0.5 to satisfy the above con-
straints on both two datasets.
5.3 Rank estimation
Even though the FPC algorithm converges in iter-
ative fashion, the value of ? varying with different
datasets is difficult to be decided. In practice, we
record the rank of matrix Z at each round of iter-
ation until it converges at a rather small threshold
? = 10
?4
. The reason is that we suppose the opti-
mal low-rank representation of the matrix Z con-
veys the truly effective information about underly-
ing semantic correlation between the features and
the corresponding labels.
We use the five-fold cross validation on the val-
idation set and evaluate the performance on each
fold with different ranks. At each round of itera-
tion, we gain a recovered matrix and average the
F1
14
scores from Top-5 to Top-all predicted rela-
tion instances to measure the performance. Figure
3 illustrates the curves of average F1 scores. After
recording the rank associated with the highest F1
score on each fold, we compute the mean and the
standard deviation to estimate the range of optimal
rank for testing. Table 2 lists the range of optimal
ranks for DRMC-b and DRMC-1 on NYT?10 and
NYT?13.
14
F1 =
2?precision?recall
precision+recall
844
0 100 200 300 400 5000.122
0.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Ave
rag
e?F
1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
(a) DRMC-b on NYT?10 validation set (? = 5).
0 100 200 300 400 5000.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Ave
rag
e?F
1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
(b) DRMC-1 on NYT?10 validation set (? = 5).
0 100 200 300 400 500
0.104
0.106
0.108
0.11
0.112
0.114
Rank
Ave
rag
e?F
1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
(c) DRMC-b on NYT?13 validation set.
0 100 200 300 400 5000.106
0.108
0.11
0.112
0.114
0.116
0.118
0.12
0.122
0.124
Rank
Ave
rag
e?F
1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
(d) DRMC-1 on NYT?13 validation set.
Figure 3: Five-fold cross validation for rank estimation on two datasets.
On both two datasets, we observe an identical
phenomenon that the performance gradually in-
creases as the rank of the matrix declines before
reaching the optimum. However, it sharply de-
creases if we continue reducing the optimal rank.
An intuitive explanation is that the high-rank ma-
trix contains much noise and the model tends to be
overfitting, whereas the matrix of excessively low
rank is more likely to lose principal information
and the model tends to be underfitting.
5.4 Method Comparison
Firstly, we conduct experiments to compare our
approaches with Mintz-09 (Mintz et al, 2009),
MultiR-11 (Hoffmann et al, 2011), MIML-12 and
MIML-at-least-one-12 (Surdeanu et al, 2012) on
NYT?10 dataset. Surdeanu et al (2012) released
the open source code
15
to reproduce the experi-
mental results on those previous methods. More-
over, their programs can control the feature spar-
15
http://nlp.stanford.edu/software/mimlre.shtml
sity degree through a threshold ? which filters the
features that appears less than ? times. They set
? = 5 in the original code by default. Therefore,
we follow their settings and adopt the same way
to filter the features. In this way, we guarantee
the fair comparison for all methods. Figure 4 (a)
shows that our approaches achieve the significant
improvement on performance.
We also perform the experiments to compare
our approaches with the state-of-the-art NFE-13
16
(Riedel et al, 2013) and its sub-methods (N-13,
F-13 and NF-13) on NYT?13 dataset. Figure 4 (b)
illustrates that our approaches still outperform the
state-of-the-art methods. In practical application-
s, we also concern about the precision on Top-N
predicted relation instances. Therefore, We com-
pare the precision of Top-100s, Top-200s and Top-
500s for DRMC-1, DRMC-b and the state-of-the-
16
Readers may refer to the website,
http://www.riedelcastro.org/uschema for the details of
those methods. We bypass the description due to the
limitation of space.
845
0 0.1 0.2 0.3 0.4 0.50
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Pre
cisi
on
 
 
Mintz?09
MultiR?11
MIML?12
MIML?at?least?one?12
DRMC?1(Rank=17)
DRMC?b(Rank=37)
(a) NYT?10 testing set (? = 5).
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Pre
cisi
on
 
 N?13F?13NF?13NFE?13DRMC?1(Rank=16)DRMC?b(Rank=85)
(b) NYT?13 testing set.
Figure 4: Method comparison on two testing sets.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Prec
ision
 
 DRMC?1(Rank=1879)DRMC?b(Rank=1993)DRMC?1(Rank=1169)DRMC?b(Rank=1307)DRMC?1(Rank=384)DRMC?b(Rank=464)DRMC?1(Rank=17)DRMC?b(Rank=37)
(a) NYT?10 testing set (? = 5).
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Prec
ision
 
 DRMC?1(Rank=1378)DRMC?b(Rank=1861)DRMC?1(Rank=719)DRMC?b(Rank=1703)DRMC?1(Rank=139)DRMC?b(Rank=655)DRMC?1(Rank=16)DRMC?b(Rank=85)
(b) NYT?13 testing set.
Figure 5: Precision-Recall curve for DRMC-b and DRMC-1 with different ranks on two testing sets.
Top-N NFE-13 DRMC-b DRMC-1
Top-100 62.9% 82.0% 80.0%
Top-200 57.1% 77.0% 80.0%
Top-500 37.2% 70.2% 77.0%
Average 52.4% 76.4% 79.0%
Table 3: Precision of NFE-13, DRMC-b and
DRMC-1 on Top-100, Top-200 and Top-500 pre-
dicted relation instances.
art method NFE-13 (Riedel et al, 2013). Table 3
shows that DRMC-b and DRMC-1 achieve 24.0%
and 26.6% precision increments on average, re-
spectively.
6 Discussion
We have mentioned that the basic alignment as-
sumption of distant supervision (Mintz et al,
2009) tends to generate noisy (noisy features and
incomplete labels) and sparse (sparse features) da-
ta. In this section, we discuss how our approaches
tackle these natural flaws.
Due to the noisy features and incomplete label-
s, the underlying low-rank data matrix with tru-
ly effective information tends to be corrupted and
the rank of observed data matrix can be extremely
high. Figure 5 demonstrates that the ranks of da-
ta matrices are approximately 2,000 for the initial
optimization of DRMC-b and DRMC-1. Howev-
er, those high ranks result in poor performance.
As the ranks decline before approaching the op-
timum, the performance gradually improves, im-
plying that our approaches filter the noise in data
and keep the principal information for classifica-
tion via recovering the underlying low-rank data
matrix.
Furthermore, we discuss the influence of the
feature sparsity for our approaches and the state-
846
0 100 200 300 400 5000.122
0.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Avera
ge?F1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
0 100 200 300 400 5000.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Avera
ge?F1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
0 0.1 0.2 0.3 0.4 0.50
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Precis
ion
 
 
Mintz?09MultiR?11MIML?12MIML?at?least?one?12DRMC?1(Rank=17)DRMC?b(Rank=43)
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Precis
ion
 
 DRMC?1(Rank=2148)DRMC?b(Rank=2291)DRMC?1(Rank=1285)DRMC?b(Rank=1448)DRMC?1(Rank=404)DRMC?b(Rank=489)DRMC?1(Rank=17)DRMC?b(Rank=43)
0 100 200 300 400 5000.122
0.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Avera
ge?F1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
0 100 200 300 400 5000.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Avera
ge?F1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
0 0.1 0.2 0.3 0.4 0.50
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Precis
ion
 
 
Mintz?09MultiR?11MIML?12MIML?at?least?one?12DRMC?1(Rank=17)DRMC?b(Rank=46)
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Precis
ion
 
 DRMC?1(Rank=2539)DRMC?b(Rank=2730)DRMC?1(Rank=1447)DRMC?b(Rank=1644)DRMC?1(Rank=433)DRMC?b(Rank=531)DRMC?1(Rank=17)DRMC?b(Rank=46)
0 100 200 300 400 5000.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Avera
ge?F1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
0 100 200 300 400 5000.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Avera
ge?F1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
0 0.1 0.2 0.3 0.4 0.50
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Precis
ion
 
 
Mintz?09MultiR?11MIML?12MIML?at?least?one?12DRMC?1(Rank=16)DRMC?b(Rank=51)
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Precis
ion
 
 DRMC?1(Rank=3186)DRMC?b(Rank=3444)DRMC?1(Rank=1728)DRMC?b(Rank=1991)DRMC?1(Rank=489)DRMC?b(Rank=602)DRMC?1(Rank=16)DRMC?b(Rank=51)
Figure 6: Feature sparsity discussion on NYT?10 testing set. Each row (from top to bottom, ? = 4, 3, 2)
illustrates a suite of experimental results. They are, from left to right, five-fold cross validation for
rank estimation on DRMC-b and DRMC-1, method comparison and precision-recall curve with different
ranks, respectively.
of-the-art methods. We relax the feature filtering
threshold (? = 4, 3, 2) in Surdeanu et al?s (2012)
open source program to generate more sparse fea-
tures from NYT?10 dataset. Figure 6 shows that
our approaches consistently outperform the base-
line and the state-of-the-art methods with diverse
feature sparsity degrees. Table 2 also lists the
range of optimal rank for DRMC-b and DRMC-
1 with different ?. We observe that for each ap-
proach, the optimal range is relatively stable. In
other words, for each approach, the amount of tru-
ly effective information about underlying seman-
tic correlation keeps constant for the same dataset,
which, to some extent, explains the reason why our
approaches are robust to sparse features.
7 Conclusion and Future Work
In this paper, we contributed two noise-tolerant
optimization models
17
, DRMC-b and DRMC-1,
for distantly supervised relation extraction task
from a novel perspective. Our models are based on
matrix completion with low-rank criterion. Exper-
17
The source code can be downloaded from https://
github.com/nlpgeek/DRMC/tree/master
iments demonstrated that the low-rank represen-
tation of the feature-label matrix can exploit the
underlying semantic correlated information for re-
lation classification and is effective to overcome
the difficulties incurred by sparse and noisy fea-
tures and incomplete labels, so that we achieved
significant improvements on performance.
Our proposed models also leave open question-
s for distantly supervised relation extraction task.
First, they can not process new coming testing
items efficiently, as we have to reconstruct the data
matrix containing not only the testing items but al-
so all the training items for relation classification,
and compute in iterative fashion again. Second,
the volume of the datasets we adopt are relatively
small. For the future work, we plan to improve our
models so that they will be capable of incremental
learning on large-scale datasets (Chang, 2011).
Acknowledgments
This work is supported by National Program on
Key Basic Research Project (973 Program) under
Grant 2013CB329304, National Science Founda-
tion of China (NSFC) under Grant No.61373075.
847
References
Nguyen Bach and Sameer Badaskar. 2007. A review
of relation extraction. Literature review for Lan-
guage and Statistics II.
Kurt Bollacker, Robert Cook, and Patrick Tufts. 2007.
Freebase: A shared database of structured general
human knowledge. In Proceedings of the nation-
al conference on Artificial Intelligence, volume 22,
page 1962. AAAI Press.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim S-
turge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247?1250. ACM.
Ricardo S Cabral, Fernando Torre, Jo?ao P Costeira, and
Alexandre Bernardino. 2011. Matrix completion
for multi-label image classification. In Advances in
Neural Information Processing Systems, pages 190?
198.
Emmanuel J Cand`es and Benjamin Recht. 2009. Exact
matrix completion via convex optimization. Foun-
dations of Computational mathematics, 9(6):717?
772.
Edward Y Chang. 2011. Foundations of Large-Scale
Multimedia Information Management and Retrieval.
Springer.
Michael Collins, Sanjoy Dasgupta, and Robert E
Schapire. 2001. A generalization of principal com-
ponents analysis to the exponential family. In Ad-
vances in neural information processing systems,
pages 617?624.
Mark Craven and Johan Kumlien. 1999. Construct-
ing biological knowledge bases by extracting infor-
mation from text sources. In ISMB, volume 1999,
pages 77?86.
Maryam Fazel, Haitham Hindi, and Stephen P Boyd.
2001. A rank minimization heuristic with applica-
tion to minimum order system approximation. In
American Control Conference, 2001. Proceedings of
the 2001, volume 6, pages 4734?4739. IEEE.
Andrew Goldberg, Ben Recht, Junming Xu, Robert
Nowak, and Xiaojin Zhu. 2010. Transduction with
matrix completion: Three birds with one stone. In
Advances in neural information processing systems,
pages 757?765.
Donald Goldfarb and Shiqian Ma. 2011. Conver-
gence of fixed-point continuation algorithms for ma-
trix rank minimization. Foundations of Computa-
tional Mathematics, 11(2):183?210.
Gene Golub and William Kahan. 1965. Calculat-
ing the singular values and pseudo-inverse of a ma-
trix. Journal of the Society for Industrial & Ap-
plied Mathematics, Series B: Numerical Analysis,
2(2):205?224.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 541?550, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Michael Jordan. 1995. Why the logistic function? a
tutorial discussion on probabilities and neural net-
works. Computational Cognitive Science Technical
Report.
Yehuda Koren. 2008. Factorization meets the neigh-
borhood: a multifaceted collaborative filtering mod-
el. In Proceedings of the 14th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, pages 426?434. ACM.
Shiqian Ma, Donald Goldfarb, and Lifeng Chen. 2011.
Fixed point and bregman iterative methods for ma-
trix rank minimization. Mathematical Program-
ming, 128(1-2):321?353.
Oded Maron and Tom?as Lozano-P?erez. 1998. A
framework for multiple-instance learning. Advances
in neural information processing systems, pages
570?576.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 777?782, Atlanta, Georgia, June.
Association for Computational Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003?1011. Association for
Computational Linguistics.
Jasson DM Rennie and Nathan Srebro. 2005. Fast
maximum margin matrix factorization for collabora-
tive prediction. In Proceedings of the 22nd interna-
tional conference on Machine learning, pages 713?
719. ACM.
Sebastian Riedel, Limin Yao, and Andrew McCal-
lum. 2010. Modeling relations and their mention-
s without labeled text. In Machine Learning and
Knowledge Discovery in Databases, pages 148?163.
Springer.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
848
pages 74?84, Atlanta, Georgia, June. Association
for Computational Linguistics.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2004.
Learning syntactic patterns for automatic hypernym
discovery. Advances in Neural Information Process-
ing Systems 17.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 455?
465. Association for Computational Linguistics.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervision
for relation extraction. In Proceedings of the 50th
Annual Meeting of the Association for Computation-
al Linguistics: Long Papers-Volume 1, pages 721?
729. Association for Computational Linguistics.
Wei Xu, Raphael Hoffmann, Le Zhao, and Ralph Gr-
ishman. 2013. Filling knowledge base gaps for dis-
tant supervision of relation extraction. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 665?670, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Xingxing Zhang, Jianwen Zhang, Junyu Zeng, Jun
Yan, Zheng Chen, and Zhifang Sui. 2013. Towards
accurate distant supervision for relational facts ex-
traction. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistic-
s (Volume 2: Short Papers), pages 810?815, Sofi-
a, Bulgaria, August. Association for Computational
Linguistics.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
427?434. Association for Computational Linguistic-
s.
849
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, page 86,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
SemEval-2010 Task 11: Event detection in Chinese news sentences  Qiang Zhou  Tsinghua University, Beijing 100084, P. R. China  zq-lxd@mail.tsinghua.edu.cn   The goal of the task is to detect and analyze the event contents in real world Chinese news texts. It consists of finding key verbs or verb phrases to describe these events in the Chinese sentences af-ter word segmentation and part-of-speech tagging, selecting suitable situation descriptions for them, and anchoring different situation arguments with suitable syntactic chunks in the sentence. Three main sub-tasks are as follows: (1) Target verb WSD; (2) Sentence SRL; (3) Event detection. We will select 100 high-frequency Chinese tar-get verbs for this task. Among them, 30 verbs have multiple senses and 70 verbs have single sense. Each target verb will be assigned more than 50 annotated sentences to consist of training and test sets. Each annotated sentence will have following event information: (1) word segmentation and POS tags; (2) the target verb (or verb phrase) and its position in the sentence; (3) the event description (situation description formula or natural explana-tion text) of the target verb (or verb phrase) in the context of the sentences; (4) the chunks annotated with suitable syntactic constituent tags, functional tags and event argument role tags. The training and test set will be extracted from the data set with ratio 8:2.   For the WSD subtask, we give two evalua-tion measures: WSD-Micro-Accuracy and WSD-Macro-Accuracy. The correct conditions are: the selected situation description formula and natural explanation text of the target verbs will be same 
with the gold-standard codes. We evaluated 27 multiple-sense target verbs in the test set. For the SRL subtask, we give three evaluation measures: Chunk-Precision, Chunk-Recall, and Chunk-F-measure. The correct conditions are: the recognized chunks should have the same bounda-ries, syntactic constituent and functional tags, and situation argument tags with the gold-standard ar-gument chunks of the key verbs or verb phrases. We only select the key argument chunks (with se-mantic role tags: x, y, z, L or O) for evaluation. For the event detection subtask, we give two evaluation measures: Event-Micro-Accuracy and Event-Macro-Accuracy. The correct conditions are: (1) The event situation description formula and natural explanation text of the target verb should be same with the gold-standard ones; (2) All the argument chunks of the event descriptions should be same with the gold-standard ones; (3) The number of the recognized argument chunks should be same with the gold-standard one. 8 participants downloaded the training and test data. Only 3 participants uploaded the final results. Among them, 1 participant (User ID = 156) sub-mitted 4 results and 1 participant (User ID = 485) submitted 2 results. So we received 7 uploaded results for evaluation. The mean elaboration time of the test data is about 30 hours. The following is the evaluation result table. All the results are ranked with Event-Macro-Accuracy.  User ID System ID WSD-Micro-A WSD-Macro-A Chunk-P Chunk-R Chunk-F Event-Micro-A Event-Macro-A Rank 485 480-a 87.54 89.59 80.91 77.91 79.38 52.12 53.76 1 485 480-b 87.24 89.18 80.91 76.95 78.88 50.59 52.05 2 303 109 73.00 70.64 63.50 57.39 60.29 22.85 23.05 3 156 348 79.23 82.18 58.33 53.32 55.71 20.05 20.23 4 156 350 77.74 81.42 58.33 53.32 55.71 20.05 20.22 5 156 347 81.30 83.81 58.33 53.32 55.71 20.33 20.19 6 156 349 79.82 82.58 58.33 53.32 55.71 20.05 20.14 7 The results show the event detection task is still an open problem for exploring in the Chinese language. 
86
Automatic Identification of Chinese Event Descriptive 
Clause 
Liou Chen 
Department of Computer Science 
and technology 
Tsinghua University 
       
chouou@foxmail.com 
Qiang Zhou 
National Laboratory for Informa-
tion Science and Technology,  
Tsinghua University 
zq-
lxd@mail.tsinghua.edu.cn 
 
 
Abstract 
This paper gives a new definition of Chi-
nese clause called ?Event Descriptive 
Clause? and proposes an automatic me-
thod to identify these clauses in Chinese 
sentence. By analyzing the characteristics 
of the clause, the recognition task is formu-
lated as a classification of Chinese punctua-
tions. The maximum entropy classifier is 
trained and two kinds of useful features and 
their combinations are explored in the task. 
Meanwhile, a simple rule-based post 
processing phase is also proposed to im-
prove the recognition performance. Ulti-
mately, we obtain 81.32% F-score on the 
test set. 
1 Introduction 
An important task in natural language 
processing (NLP) is to identify the complete 
structure of a sentence. However, the ambigui-
ties of the natural language make full parsing 
difficult to become a practical and effective tool 
for NLP applications. In order to solve this 
problem, ?partial parsing? is proposed to divide 
complex sentences into simple units, and then 
the complex full-parsing task can be simplified 
to be the analysis of single units and relations 
among them. Ejerhed(1998) once found that a 
parser can benefit from automatically identified 
clause boundaries in discourse, and he showed 
the partial parsing method called ?clause identi-
fication? is useful for full parsing. 
For example, given a Chinese sentence as fol-
lows: 
? ?????????????????
?????????????????
????????????????? 
? Along the way, we see the trees have 
been cut down for regeneration, and the 
trees needed to be cut for building. All of 
them are useful building material. We al-
so see several freight trucks and tractors 
going south and north. 
The illustrative sentence is a long one that is 
difficult to parse with a one-step full parsing 
and will suffer from the error propagation from 
the previous wrong parsing results.  
However, if the sentence is segmented into 
several independent clauses which can be 
parsed separately, the shortening of sentence 
length will make each sub-parsing much easier 
and the independent of each clause can also 
prevent the error-propagation. For example, the 
above sentence can be divided into four parts 
which are labeled with dashed borders shown in 
Figure 1. Each segment can be parsed solely as 
a sub tree and the whole parse tree can be easily 
built through analyzing the event relationships 
among them. Moreover, the parse errors occur-
ring in each sub tree have little effect on the 
whole tree as they are parsed independently in 
each segment region. 
The key issue is how to select a suitable seg-
mentation unit. It is not a trivial question be-
cause it must be based on the characteristics of 
language itself. In English, a clause is a closely 
related group of words that include both a sub-
ject and a verb. The independent sentence is 
usually ended by punctuation and the dependent 
one is often introduced by either a subordinating 
conjunction or a relative pronoun. The structural 
trait of English language is the basic to define 
English clause and clause recognition task, like 
CoNLL-2001 (Erik F et al, 2001). 
However in Chinese, there is no obvious con-
junction between two clauses, especially the 
dependent clauses. The separators used often 
are just punctuations, like commas and periods. 
Therefore the characteristics of Chinese sen-
tence call for a new clause identification scheme 
to spit a sentence into clause segments. 
To meet this need, we define a new clause 
unit called ?Event Descriptive Clause (EDC)? in 
the Chinese sentence. It mainly considers the 
punctuation separators so as to skip the difficul-
ty in identifying different subordination clauses 
without any obvious separating tags. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Parsing result of the example sen-
tence. 
 
According to the definition, we proposed an 
EDC recognition method based on punctuation 
classification. Experimental results show that 
the new definition of Chinese clause identifica-
tion task is reasonable and our feature set is ef-
fective to build a feasible EDC recognition sys-
tem. 
2 EDC Recognition Task 
2.1 Definition of Chinese Clause 
As we discussed before, ?clause identification? 
is a useful step in language processing as it can 
divide a long complex sentence into several 
short meaningful and independent segments. 
Therefore the definition of a clause should 
satisfy two basic requirements: ?meaningful? 
and ?independent?. The previous restriction 
requires each clause to make sense and express 
a full meaning, and the latter one insures that 
each clause can be parsed alone. 
We firstly give the definition of ?Event?. An 
event is expressed by several functional chunks 
(Zhou and Li, 2009) which are controlled by a 
certain predicate. The functional chunks are de-
fined as the subject, predicate, object and ad-
verbial parts of a clause. According to different 
event level, the complex components of a high 
level event may contain some low level events.  
Let us take the second part of Figure 1 as an 
example. The high level event dominated by the 
verbal predicate ???/see? is : ?[S ??/ We] 
[P ??/ see] [C ????????????
???????/ the trees have been cut down 
for regeneration, and the trees needed to be cut 
for building]?. The event is composed of three 
high level functional chunks.  
The complement of above event also contains 
two nested events controlled by the predicate 
???/cut down?. Which are ?[D ????(for 
regeneration)] [P ??(cut down)] ? [H ??
(trees)]? and ?[D????(for building)] [P?
?(cut down)]?[H??(trees)]?. The chunks in 
these two events are low level ones. 
Next, we consider the characteristics of Chi-
nese sentences. Because the punctuations, like 
commas, semicolons, question marks, etc. are 
commonly-used obvious independent event se-
parators. We can use them to segment a word 
sequence as a possible clause in a sentence.  
[D ?? (along the way) ] 
? 
[S?? (we)] 
[P?? (see)] 
[C 
[H 
[D??? (for regeneration)] 
[P ?? (cut down)] 
? (-) 
[H ?? (trees)] 
] 
? 
[H 
[D??? (for building)] 
[P?? (cut down)] 
? (-) 
[H?? (trees)] 
] 
]  
? 
[D? (all)] 
[P? (are)] 
[O???? (useful)] 
? 
[S 
[P???? (freight)] 
? (-) 
[H??\???  
(trucks and tractors)] 
? 
] 
[P???? (going south and north)] 
? 
Then based on the overall consideration of 
the definition of ?Event? and the characteristics 
of Chinese sentence, we define the Event De-
scriptive Clause (EDC) as a word sequence se-
parated by punctuations, the sequence should 
contain either a simple high level event or a 
complex main event with its nested low level 
events. 
Taking some special conditions into consid-
eration, the adverbials to describe common time or 
space situations of several events, and the indepen-
dent components to describe sentence-level paren-
thesis, can also be regarded as special EDCs though 
sometimes they do not contain any predicates.  
In the Chinese language, many events can share 
subject and object with the adjacent events so that 
the subject or object can be omitted. We differen-
tiated them with different tags in our EDC defini-
tion schemes. 
In summary, three types of EDCs are consi-
dered as follows: 
(1) E1: an EDC that includes at least one sub-
ject in the event it contains. 
(2) E2: an EDC that has no subject. 
(3) D/T: an EDC acted as sentence-level ad-
verbial or independent composition. 
Then the above example sentence can be di-
vided into following four EDCs: 
? [D?? ] ?[E1 ??????????
??????????? ]?[E2 ??
???? ] ?[E1 ?????????
???????] ? 
? [D Along the way], [E1 we see the trees 
have been cut down for regeneration, and 
the trees needed to be cut for building]. 
[E2 All of them is useful building materi-
al]. [E1 We also see several freight trucks 
and tractors going south and north]. 
2.2 Task Analyses 
According to the EDC definition, we define the 
Chinese clause identification as a task that re-
cognizing all types of EDCs in an input sen-
tence after word segmentation and POS tagging. 
Like the example in section 2.1, each EDC is 
recognized and enclosed between brackets. The 
task consists of two subtasks. One is to recog-
nize suitable EDC boundaries in a sentence. The 
other is to assign suitable tags for each recog-
nized EDCs. We only focus on the first subtask 
in the paper. Comparing with CoNLL-2010 task, 
our task only recognizes the EDCs that contain 
the highest level events without identifying its 
internal nested structures. 
Since EDC is defined as a word sequence se-
parated by certain punctuations. The identifica-
tion problem can be regarded as a classification 
task to classify the punctuations as one of two 
classes: boundary of an EDC (Free Symbol), or 
not an EDC boundary (Non-Free Symbol). Then 
the words sequence between two Free Symbols 
is an EDC. 
By analysis, we found only several types of 
punctuations could be used as EDC separator 
commonly, including period, question mark, 
exclamatory mark, ellipsis, comma, semicolon , 
colon and brackets. The previous four types of 
punctuations always appear at the end of a sen-
tence so we simply name them as ?End Symbol?. 
The following four types are called ?Non-End 
Symbol? accordingly. The Free-Symbols are 
recognized from these special punctuations. 
3 EDC Recognition System 
3.1 Recognition Process 
Statistical data from the EDC-annotated corpus 
provided by CIPS-ParsEval-2009 task (Zhou 
and Li, 2009) show that 99.87% End Symbols 
act as the boundaries of EDCs. So we can simp-
ly assume them as Free Symbol. But for Non-
End Symbols, the linguistic phenomena are 
complex. If we present a baseline system that 
regards every Non-End Symbol as a Free Sym-
bol rough, only 61% symbols can be correctly 
recognized and the remaining 39% are wrongly 
treated. 
To solve this problem, we implement a clas-
sifier for Non-End Symbol specially. First of all, 
we propose several features that might be useful 
to determine whether a Non-End Symbol is free or 
not. Then, the performance of each feature is 
tested on a maximum entropy classifier to find the 
most effective features and form the final feature 
set. We will discuss them detailed in the follow-
ing sections. 
3.2 Features 
Features are very important in implementing a 
classifier. We consider two types of features: 
As EDC is a word sequence, the word and 
part of speech (POS) features are the most intui-
tional information for clause boundary recogni-
tion. We call the word level features ?basic fea-
tures? as Table 1 shows. 
However, the structural characteristics of a 
sentence cannot be completely reflected by 
words it contains. As the events in an EDC are 
expressed by functional chunks as section 2.1 
presents, the functional chunk (FC) might be 
effective in recognition. They can provide more 
syntactic structure features than the word se-
quences. We consider four types of FC-related 
features as in Table 2. 
Those two major types of features are tested 
and the final feature set will be selected through 
experiments 
 
Feature 
Current POS 
Wordn/POSn 
Adjacent Non-End 
Symbols 
distance 
current word 
adjacent word 
Left verb 
Left preposition 
Adjacent brackets 
distance 
adjacent POS 
Table 1. Basic Features 
 
Feature Description 
Location 
if current punctuation is in a 
functional chunk, the feature 
is 1, else is 0 
Chunkn 
functional tags in different 
positions of local context 
windows 
Chunk 
sequence 
functional tags between 
current punctuation and 
first left Non-End Symbol 
Predicate 
number 
the number of predicates 
between current punctuation 
and first left Non-End Sym-
bol 
Table 2. Extended Features 
3.3 Feature Selection Strategy 
The features listed in Table 1 and Table 2 are 
considered to be useful but whether there are 
actually effective are unknown. Therefore we 
should select the most useful ones through ex-
periments using certain strategy. 
In the paper, we try a greedy strategy. Firstly, 
each feature is used alone to get its ?contribu-
tion? to the classification system. Then after all 
features are tested, they are sorted by their con-
tributions. At last, features are added one by one 
into classifier according to their contribution 
ranks and then pick out the features that can 
improve the performance and take out those 
features that have no effect on performance or 
even lead to the degradation. Eventually, we get 
a proper feature set. 
As shown in Table 1 and Table 2, 
Wordn/POSn and Chunkn tags are used and their 
positions (n) are important. In this paper, we let 
the position window change from [0, 0] to [-5, 5] 
to select the proper position area. 
4 Experimental results 
All data we use in this paper are provided by 
CIPS-ParsEval-2009 task (Zhou and Li, 2009). 
They are automatically extracted from Tsinghua 
Chinese Treebank/TCT (Zhou et al, 1997), in-
cluding 14,248 Chinese sentences as training 
material and 3,751 sentences as test data. We 
used the sentences annotated with Gold-
standard word segmentation and POS tags as 
the input data for EDC recognition.  
4.1 Feature Selection 
We use the 14,248 training sentences to judge 
the contribution of each feature and get final 
feature set. The training corpus is divided into 
two parts with the ratio of 80% and 20%. 80% 
data is used to train classifiers and the remain-
ing 20% for feature selection. 
The maximum entropy toolbox1 is chosen for 
classification due to its training efficiency and 
better performance. A functional chunk parser 
(Yu, 2007) trained on the same CIPS-ParsEval-
2009 FC bank (Zhou and Li, 2009) are used to 
provide extended features. Its F-score is 85%. 
The parser could only provide the lowest level 
functional chunks. For example, given the input 
sentence ????????????????
?/ the freight trucks and tractors going south 
                                                 
1http://homepages.inf.ed.ac.uk/lzhang10/maxent_tool
kit.html 
and north?, the output functional chunk se-
quence are : ?[P???? (freight)] ? [H???
???  (trucks and tractors)]?[P ???? 
(going south and north)]?. 
The evaluation measure is defined as follows: 
Accuracy =
Correctly  classified  Symbols
Total  Non?End  Symbols
          (a) 
The performance of each feature is evaluated 
and ranked as Table 3 shows. 
When selecting the proper position area of 
Chunkn and Wordsn/POSn, the areas change 
from [0, 0] to [-5, 5] and the performance 
curves are shown in Figure 2 and Figure 3. 
Then the feature in Table 3 is added one by 
one into classifier and the feature will be moved 
when it causes performance degradation. Table 
4 presents the accuracy changes on 20% devel-
opment data set. 
Form above experimental figures and tables 
we can get several conclusions: 
Figure 2 and Figure 3 display the perfor-
mance changes under different window sizes 
(from [0, 0] to [-5, 5]). Then the abscissas of 
their highest points are chosen as best window 
sizes. We can find that when the window size is 
large enough, the performance change will be 
inconspicuous, which means the information far 
away from current punctuation has less help in 
judging whether it is free or not. 
Table 3 gives the contribution of each single 
feature in identifying Non-End Symbols. Com-
paring with the baseline system proposed in sec-
tion 3.1, each feature could achieve obvious 
increase. Therefore our attempt that building a 
classifier to identify Free Symbols from Non-
End Symbols is feasible.  
The results in Table 4 show that with features 
added into classifier the performance raises ex-
cept for the fifth one (Left preposition). There-
fore our final feature set will include nine fea-
tures without the ?Left preposition?. 
At the same time, the top four features are all 
extended ones and they can achieve 81.83% 
accuracy while the basic features could only 
increase the performance less than 1% (0.95% 
g). This phenomenon indicates that the syntactic 
information can reflect the structural characte-
ristics of Chinese clauses much better. There-
fore we hypothesize that we can use extended 
features only to build the classifier. 
 
 
Feature Accuracy 
Chunkn (n?[-4, 4]) 80.07 
Chunk sequence 76.51 
Predicate number 75.40 
Location 69.57 
Left preposition 69.40 
Wordsn/POSn (n?[-4, 3]) 68.77 
Left verb 68.77 
Current POS 66.81 
Adjacent Non-End Symbols 66.33 
Adjacent brackets 66.19 
Table 3. Accuracy and rank of each feature 
 
 
Figure 2. Performance of Wordsn/POSn 
 
 
Figure 3. Performance of Chunkn feature un-
der different context windows 
  
Feature Accuracy 
Chunkn (n? [-4, 4]) 80.07 
(+)Chunk sequence 80.43 
(+)Predicates number 80.87 
(+)Location 81.83 
(+)Left preposition 81.67 
(+)Wordsn/POSn (n? [-4, 3]) 81.93 
(+)Left verb 82.04 
(+)Current POS 82.12 
(+)Adjacent Non-End Symbols 82.43 
(+)Adjacent bracket 82.78 
Table 4. Accuracy with adding features on 
development data set. 
68.77
50
55
60
65
70
[0 ,1] [-1,1] [-2,1] [-2,3] [-3,3] [-4,3] [-4,5] [-5,5]
80.07
65
70
75
80
[0,1] [-1,1] [-2,1] [-2,3] [-3,3] [-4,3] [-4,5] [-5,5]
4.2 Evaluating System Performance 
With the feature set selected in section 4.1, the 
EDC identification system can be built. The 
total 14,248 sentences are included to train the 
classifier for classifying the Non-End Symbol 
and all test material is used for evaluating the 
performance of clause recognition. 
We consider different modes to evaluate the 
clause recognition system. One is only using the 
extended features provided by automatic syntac-
tic parser to validate our guess that the syntactic 
features are so effective that they will achieve 
satisfying result without other accessional fea-
tures (mode_1). The second mode is adding ba-
sic word features along with syntactic ones to 
get the best performance that our current system 
can obtain (mode_2). Since the chunk features 
used in this classifier are from the automatic 
analyses. To clear the influence caused by au-
tomatic parsing, we use the lowest level correct 
chunks to provide syntactic features in the third 
method. The entirely correct chunks are pro-
vided by CIPS-ParsEval-2009 FC bank (Zhou 
and Li, 2009). As EDC is defined as the de-
scription of a high level event, we guess that the 
highest level chunks might provide more effec-
tive information. For example, for the same in-
put sentence ??????????????
???/ the freight trucks and tractors going 
south and north?,  its high level chunk sequence 
will be ?[S ??????????? (freight 
trucks and tractors)]?[P???? (going south 
and north)]?.Then model_4 will use the golden-
standard high level chunk features extracted 
from relevant TCT (Zhou et al, 1997) to clear 
the upper bound of system performance. 
The evaluation measure is defined as follows, 
and we only use the F-score. 
Recall =
Correctly  recognized  clauses
Total  correct  clauses
            (b) 
Precision =
Correctly  recognized  clauses
Total  recognized  clauses
       (c) 
 F? score =   
2?Precision ?Recall
Precision +Recall
                 (d) 
Recognition performances of the four modes 
are shown in Table 5. 
In order to deal with some special conditions 
that our classifier cannot treat well to improve 
the performance of whole system, a simple rule-
based post processing phase is designed which aims 
at rectifying wrong recognized sentence-level 
adverbial and independent composition, that is: 
When there are only two EDCs are recog-
nized in a sentence and one of which is an ad-
verbial or independent composition, we simply 
assume that these two EDCs should be merged 
into a single big EDC. 
To estimate the benefit of post-processing, 
we compare the performances before/after add-
ing post-processing. The contrasts are shown in 
Table 6. 
 
 mode1 mode2 mode3 mode4 
Classifier 
Accuracy 
79.64 80.60 83.46 93.34 
System 
F-score 
77.71 78.77 81.29 89.57 
Model 
Size 
181 
KB 
2.2 
MB 
/ / 
Training 
Time 
3.7s 12.6s / / 
Table 5. Performances on four models 
 
 mode1 mode2 mode3 mode4 
F-score 
 (Before) 
77.71 78.77 81.29 89.57 
F-score 
(After) 
79.43 
? 1.72 
81.32 
?2.55 
84.04 
?2.75 
90.65 
?1.08 
Table 6. The Performance changes caused by 
post-processing 
 
The first line of Table 5 is the accuracy of 
Non-End Symbol classifier and the second one 
shows the F-score of whole EDC recognition 
system. From the two lines we can get this con-
clusion that the performance of whole system 
will increase along with the advancement of 
classifier. We also find that the system perfor-
mance under automatic lowest level chunk fea-
ture does not drop too much comparing with the 
one under gold-standard chunks (less than 3%), 
which means existing syntactic parser is good 
enough to provide the low level chunk features. 
However, the recognition F-score will increase 
to nearly 91% when standard high level chunk 
features are used, which proves that the rela-
tionship between high level functional chunks 
and our defined EDCs are much closer that they 
are more efficient in recognition. Therefore we 
can try to build a good high level chunk parser 
in future. Results of  mode_1 and mode_2 show 
that comparing with the classifier that uses all 
features, using only syntactic features can save 
nearly three times of training time and occupy 
only 1/10 storage space without losing too much 
reorganization performance. It tells us that when 
time and storage space is limited we can just use 
syntactic features. 
Table 6 presents the impact of our post- 
processing. We can find that the processing is 
effective though it is simple. This result also 
reflects that current classifier has difficulties to 
distinguish whether an adverbial or independent 
composition is at sentence-level or clause-level. 
5 Discussions 
5.1 EDC Error Types 
Because different EDC recognition errors 
(too long or too short) might cause different 
problems, we define three error types according 
to the boundary differences between the recog-
nized EDCs and the gold-standard ones. 
(1) ?1: N? error: The boundary of a recog-
nized EDC is wider than the gold-standard one.  
(2) ?N: 1? error: The boundary of a gold-
standard EDC is wider than the recognized one. 
(3) ?N: M? error: Several recognized EDCs 
and the gold-standard ones are crossed on their 
boundaries.  
We do some statistical analysis on all 1584 
wrongly recognized EDCs and Table 7 displays 
the distributional ratios of each error type. 
 
Error type 1:N N:1 N:M 
Ratio (%) 59.2 38.9 1.9 
Table 7. Distribution of different EDC recog-
nition errors 
5.2 Error Analysis 
(1) 1:N Error 
When this error happens, it will have no ter-
rible effect on the final whole parse tree if the 
relations between this wrong recognized EDC 
and other EDCs remain the same. Like the ex-
ample sentence in Figure 1, if the second and 
the third EDCs are wrong recognized as a single 
one, it will become a little troublesome to parse 
this EDC as its length is longer than it should be 
but the tree it builds with other two EDCs will 
not change. However, if the wrong EDC causes 
relationship changes, the parse errors might 
happen on the complete tree. In our system 1: N 
errors are mainly the following three types: 
I. Several sentence-level adverbials are com-
bined. 
II. Adjacent EDCs are recognized as a subject 
or object that they are regarded as a single EDC. 
III. Several adverbials at different levels are 
merged to be one adverbial incorrectly. 
For the following sentence: 
? [D ??????]?[D ??????
????]?[E1 ?????????
????] [E2 ?????????]?
[E1 ??????????]? 
? [D For 4.6 billion years], [D in the 
process of the formation of the earth's 
surface], [E1 the climate change regularly 
on land], [E2 the phenomenon presents 
clearly in the mid-latitude regions], [E1 
organisms develop from ocean to land]. 
If the first two adverbials are recognized as a 
single one, error I happens. Then error II occurs 
when E1 and E2 are merged into one EDC. If 
the adverbial ????? /on land? of E1 is 
wrongly recognized as sentence-level and  is 
merged to its adjacent adverbial ??????
?????/in the process of the formation of 
the earth?s surface?, the third error appears. 
The previous two error conditions may not 
affect the final parser tree and could be regarded 
as ?tolerable? error. The third situation will 
change the relationships within EDCs that might 
affect following parser. 
 
(2) N:1 Error 
N: 1 error mainly includes three sub-types. 
I. Complex coordinate structure/adverbial 
clause/attributive clause is wrong separated. 
II. Complex subject/object clause is divided. 
Conditions II is the reflections of sub-type II 
in 1: N error. Therefore it is ?tolerable? error. 
The first errors are caused by complex sentence-
like component, like in Figure 1, when the 
comma in the second EDC is classified as End-
Symbol, the error occurs. To solve this problem, 
one proper method is to consider some features 
of the relationship between two adjacent possi-
ble EDCs. Another way is trying to implement 
high level chunk parser that can provide sen-
tence-level features instead of current bottom 
functional chunks. 
 
(3) N:M Error 
The proportion of this error is less than 2% 
that we will not pay much attention to it now. 
6 Related works 
There have already been some systems for 
clause identification. Abney (1990) used a 
clause filter in his CASS parser. The filter could 
recognize basic clauses and repair difficult cases. 
Leffa (1998) implemented an algorithm for finding 
clauses in English and Portuguese texts. He wrote a 
set of clause identification rules and applied them to 
a small corpus and achieved a good performance 
with recall rates above 90%. Orasan (1990) used a 
hybrid method for clause splitting in the Susanne 
corpus and obtained F-score of about 85% for this 
particular task. In the CoNLL-2001 shared task 
(Erik F et al, 2001), six systems had participated 
to identify English clauses. They used various ma-
chine learning techniques and connectionist me-
thods. On all three parts of the shared task, the 
boosted decision tree system of Carreras and Mar-
quez (2001) performed best. It obtained an F-score 
of 78.63. 
However, as English and Chinese clauses 
have different characteristics, the researches on 
English sometimes ignore punctuation, especial-
ly the comma, or they just use a comma as one 
feature to detect the segmentation without fully 
using the information of punctuations. 
In Chinese, Jin (2004) gave an analysis for 
the complete usages of the comma. Li (2005) 
tried to use punctuations to divide long sentence 
into suitable units to reduce the time consump-
tion in parsing long Chinese sentences. Their 
processing based on simply rules. Yu (2007) 
proved that using clause recognition to divide a 
sentence into independent parts and parse them 
separately could achieve extremely significant 
increase on dependency accuracy compared 
with the deterministic parser which parsed a 
sentence in sequence. The CIPS-ParsEval-2009 
(Zhou and Li, 2009) put forward a task to iden-
tify the Chinese EDC and six systems partici-
pated. Based on the idea of ?HNC? (1998), Wei 
(2009) used a semantic knowledge corpus to 
identify EDCs and achieved the performance of 
F-score 80.84 (open track). Zhou (2009) formu-
lated the task as a sequence labeling problem 
and applied the structured SVMs model. Their 
performance was 78.15. Wang (2009) also re-
garded the task as a sequence labeling problem 
and considered the CRFs to resolve this prob-
lem and got an F-score of 69.08. Chen and Zhou 
(2009) presented a classification method that 
identified the boundaries of EDCs using maxi-
mum entropy classifier, and the system obtained 
an F-score of 79.98. 
Based on our previous work, some new fea-
tures are introduced and the performance of 
each feature is evaluated, our identification sys-
tem achieved an F-score of 81.32. At the same 
time, the comparison between two different 
chunk levels show that high level chunk fea-
tures are much more powerful that we can de-
vote ourselves to building a good high level 
parser in future to increase the performance 
farther.  
7 Conclusions 
In this paper, we compare the different characte-
ristics between Chinese language and English, 
and define a new Chinese clause called ?Event 
Descriptive Clause (EDC)?. Then on the basis 
of this definition, we propose an effective me-
thod for Chinese EDC identification.  
Our work focus on the commas which are 
usually useful in Chinese clause recognition but 
always ignored by researchers, and tries differ-
ent types of features through experiments to 
clear their different effects in identifying EDC 
boundaries from commas. At the same time, our 
statistical model is combined with useful rules 
to deal with the recognition task better. Finally 
our automatic EDC recognition system achieved 
81.32 of F-score, which is higher than other sys-
tems based on the same data set. 
Meanwhile, error analyses show that the cur-
rent identification system has some problems. 
Therefore we propose several possible methods, 
expecting to solve these problems and improve 
the recognition ability of EDC recognition sys-
tem in future. 
Acknowledgements 
This work was supported by National Natural 
Science Foundation of China (No. 60573185, 
60873173), National High Technology Re-
search and Development Projects 863 (No. 
2007AA01Z173) and Tsinghua-Intel Joint Re-
search Project. 
 
References 
Abney Steven, ?Rapid Incremental Parsing with Re-
pair?. In "Proceedings of the 8th New OED Con-
ference: Electronic Text Research", University of 
Waterloo, Ontario, 1990. 
Carreras, X. and Marquez, L. ?Boosting Trees for 
Clause Splitting?. In ?Proceedings of CoNLL-
2001?, Toulouse, France, pp 73-75, 2001. 
Chen Liou, Zhou Qiang. ?Recognition of Event De-
scriptive Clause?. In ?Proceedings of the 1st 
CIPS-ParsEval?, Tsinghua University, Beijing, 
pp.65-72. 2009. 
Ejerhed Eva I., ?Finding Clauses in Unrestricted 
Text by Finitary and Stochastic Methods,? In 
?Proceedings of ANLP ?88?, pp.219-227, 1998. 
Erik F. Tjong Lim Sang and D?jean H. ?Introduction 
to the CoNLL-2001 Shared Task: Clause Identifi-
cation [A]?. In Proc. of CoNLL-2001 [C], Toul-
ouse, France, p53-57, 2001. 
Huang Zengyang. ?Theory of Hierarchical Network 
of Concepts?. Tsinghua University Press, Beijing, 
1998. 
Jin Meixun, Mi-Yong Kim, Dongil Kim and Jong-
Hyeok Lee. ?Segmentation of Chinese Long Sen-
tences Using Commas?. Proc. SIGHAN, Barcelo-
na, Spain, pp. 1-8, 2004. 
Leffa, Vilson J. ?Clause processing in complex sen-
tences, In ?Proceedings of LREC'98?, Granada, 
Espanha, 1998. 
Li Xing and Chengqing Zong. ?A Hierarchical Pars-
ing Approach with Punctuation Processing for 
Long Complex Chinese Sentences.? In Compa-
nion Volume to the Proceedings of Conference 
including Posters/Demos and Tutorial Abstracts, 
IJCNLP2005, Jeju Island, Korea, pp.9-14, 2005. 
Orasan Constantin. ?A hybrid method for clause 
splitting in unrestricted English texts?. In ?Pro-
ceedings of ACIDCA'2000?, Monastir, Tunisia, 
2000. 
Wei Xiangfeng, ?Labeling Functional Chunk and 
Event Sentence Based on the Analysis of Sen-
tence Category?. In ?Proceedings of the 1st CIPS-
ParsEval?, Tsinghua University, Beijing, pp.57-64, 
2009. 
Wang Xi, Wang Jinyong, Liu Chunyang, Wang Qi, 
and Fu Chunyuan. ?CRF-based Chinese Chunking 
and Event Recognition?. In ?Proceedings of the 
1st CIPS-ParsEval?, Tsinghua University, Beijing, 
pp.53-56. 2009. 
Yu Hang. ?Automatic Analysis of Chinese Chunks?, 
Graduation thesis of computer science, Tsinghua 
University?2007. 
Yu Kun, Sadao Kurohashi and Hao Liu. ?A Three-
Step Deterministic Parser for Chinese Dependen-
cy Parsing?. In ?Proceedings of the Human Lan-
guage Technologies 2007 (HLT2007-
NAACL2007)?, Rochester, pp.201-204, 2007. 
Zhou Junsheng, Yabing Zhang, Xinyu Dai, Jiajun 
Chen.  ?Chinese Event Descriptive Clause Split-
ting with Structured SVMs?. In ?Proceedings of 
the 1st CIPS-ParsEval?, Tsinghua University, Bei-
jing, pp.73-80, 2009. 
Zhou Qiang, Yumei Li. ?The Testing Report of 
CIPS-ParsEval-2009 Workshop?. In ?Proceedings 
of the 1st CIPS-ParsEval?, Tsinghua University, 
Beijing, 2009. 
Zhou Qiang. ?Annotation Scheme for Chinese Tree-
bank?. Journal of Chinese Information Processing, 
pp 18-21, 2004. 
Zhou Qiang, Yume Li. ?The Design of Chinese 
Chunk Parsing Task?? The Tenth Chinese Na-
tional Conference on Computational Linguistics 
(CNCCL-2009)?Tsinghua University Press, Bei-
jing,  pp.130-135, 2009 
Zhou Qiang, Wei Zhang, Shiwen Yu, ?Chinese 
Treebank Construction?, Journal of Chinese In-
formation Processing, pp42-51, 1997. 
 
Chinese Syntactic Parsing Evaluation 
Qiang Zhou 
Center for Speech and Language Tech.
Research Institute of Information Tech.
Tsinghua University 
zq-lxd@tsinghua.edu.cn 
Jingbo Zhu 
Natural Language Processing Lab.
Northeastern University 
 zhujingbo@mail.neu.edu.cn 
Abstract 
The paper introduced the task designing 
ideas, data preparation methods, evalua-
tion metrics and results of the second 
Chinese syntactic parsing evaluation 
(CIPS-Bakeoff-ParsEval-2010) jointed 
with SIGHAN Bakeoff tasks. 
1 Introduction 
Syntactic parsing is an important technique in 
the research area of natural language processing. 
The evaluation-driven methodology is a good 
way to spur the its development. Two main parts 
of the method are a benchmark database and 
several well-designed evaluation metrics. Its fea-
sibility has been proven in the English language. 
After the release of the Penn Treebank (PTB) 
(Marcus et al, 1993) and the PARSEVAL me-
trics (Black et al, 1991), some new corpus-
based syntactic parsing techniques were ex-
plored in the English language. Based on them, 
many state-of-art English parser were built, in-
cluding the well-known Collins parser (Collins, 
2003), Charniak parser (Charniak and Johnson, 
2005) and Berkeley parser (Petrov and Klein, 
2007). By automatically transforming the consti-
tuent structure trees annotated in PTB to other 
linguistic formalisms, such as dependency 
grammar, and combinatory categorical grammar 
(Hockenmaier and Steedman, 2007), many syn-
tactic parser other than the CFG formalism were 
also developed. These include Malt Parser (Ni-
vre et al, 2007), MSTParser (McDonald et al, 
2005), Stanford Parser (Klein and Manning, 
2003) and C&C Parser (Clark and Curran, 2007).  
Based on the Penn Chinese Treebank (CTB) 
(Xue et al, 2002) developed on the similar anno-
tation scheme of PTB, these parsing techniques 
were also transferred to the Chinese language. 
(Levy and Manning, 2003) explored the feasibil-
ity of applying lexicalized PCFG in Chinese. (Li 
et al, 2010) proposed a joint syntactic and se-
mantic model for parsing Chinese. But till now, 
there is not a good Chinese parser whose per-
formance can approach the state-of-art English 
parser. It is still an open challenge for parsing 
Chinese sentences due to some special characte-
ristics of the Chinese language. We need to find 
a suitable benchmark database and evaluation 
metrics for the Chinese language.  
Last year, we organized the first Chinese syn-
tactic parsing evaluation --- CIPS-ParsEval-2009 
(Zhou and Zhu, 2009). Five Chinese parsing 
tasks were designed as follows: 
z Task 1: Part-of-speech (POS) tagging; 
z Task 2: Base chunk (BC) parsing 
z Task 3: Functional chunk (FC) parsing 
z Task 4: Event description clause (EDC) 
recognition 
z Task 5: Constituent parsing in EDCs 
They cover different levels of Chinese syntac-
tic parsing, including POS tagging (Task 1), 
shallow parsing (Task 2 & 3), complex sentence 
splitting (Task 4) and constituent tree parsing 
(Task 5). The news and academic articles anno-
tated in the Tsinghua Chinese Treebank (TCT 
ver1.0) were used to build different gold-
standard data for them. Some detailed informa-
tion about CIPS-ParsEval-2009 can be found in 
(Zhou and Li, 2009). 
This evaluation found the following difficult 
points for Chinese syntactic parsing. 
1) There are two difficulties in Chinese POS 
tagging. One is the nominal verbs. The POS ac-
curacy of them is about 17% lower than the 
overall accuracy. The other is the unknown 
words. The POS accuracy of them is about 40-
10% lower than the overall accuracy. 
2) The chunks with complex internal struc-
tures show poor performance in two chunking 
tasks. How to recognize them correctly needs 
more lexical semantic knowledge.  
3) The joint recognition of constituent tag and 
head position show poor performance in the 
constituent parsing task of EDCs. 
Therefore, the second Chinese syntactic pars-
ing evaluation (CIPS-Bakeoff-ParsEval-2010) 
jointed with SIGHAN Bakeoff tasks was pro-
posed to deal with these problems. Some new 
designing ideas are as follows: 
1) We use the segments sentences as the input 
of the syntactic parser to test the effects of POS 
tagging for Chinese parsing. 
2) We design a new metric to evaluate per-
formance of event construction recognition in a 
constituent parser of EDCs. 
3) We try to evaluate the performance of 
event relation recognition in Chinese complex 
sentence. 
In the following sections, we will introduce 
the task designing ideas, data preparation me-
thods, evaluation metrics and results of the eval-
uation.  
2 Task description 
For the syntactic parsing task (Task 2) of the 
CIPS-Bakeoff-2010, we designed two sub-tasks: 
Task 2-1: Parsing the syntactic trees in Chi-
nese event description clauses  
Task 2-2: Parsing the syntactic trees in Chi-
nese sentences. 
Each subtask is separated as close and open 
track. In the close track, only the provided train-
ing data can be used to build the parsing model. 
In the open track, other outside language re-
sources can be freely used. 
We will give two examples to show the de-
tailed goals of these two sub-tasks: 
1) Task 2-1 
Input:  a Chinese event description clause 
with correct word segmentation annotations 
? ?? ??? ?? ?? ? ?? ? ?
? ? ?? ? ? ? ? ? ?? ? ?? 
Ouput: a syntactic parsing tree of the EDC 
with appropriate constitutent tag, head position 
and POS tag annotations. 
? [dj-2 ??/s ?/wP  [dj-1 ??/rNP  [vp-
1 ??/d  [vp-0 ??/v  [np-0-2 [np-2 
[vp-1 [pp-1 ?/p  ??/v  ] [vp-1 ?/cC  
??/v  ] ] ?/uJDE  ??/n  ] ?/wP  
[np-2 [vp-1 [pp-1 ?/p  [vp-0 ?/v  ?
/n  ] ] [vp-1 ?/vM  ??/v  ] ] ?/uJDE  
??/n  ] ] ] ] ] ]1 
2) Task 2-2 
Input:  a Chinese sentence with correct word 
segmentation annotations 
? ??  ? ?? ?? ?? ? ?? ? ?
? ? ?? ? ? ? ? ? ?? ? ?
? ? ? ? ?? ? ?  ? ?? ??  
?  ?? ? ??? ? ????  ?   
Output: a syntactic parsing tree of the sen-
tence with appropriate constitute tag and POS 
tag annotations. 
? [zj [fj [fj [dj??/s  ?/wP  [dj ??/rNP  
[vp ??/d  [vp ??/v  [np [np [vp [pp 
?/p  ??/v  ] [vp ?/cC  ??/v  ] ] ?
/uJDE  ??/n  ] ?/wP  [np [vp [pp ?/p  
[vp ?/v  ?/n  ] ] [vp ?/vM  ??/v  ] ] 
?/uJDE  ??/n  ] ] ] ] ]] ?/wP  [vp ?
/d  [vp ?/v  [np ??/a  ?/uJDE  ?
/n  ] ] ] ]  ?/wP  [dj [np [vp ??/v  ??
/n  ] ?/uJDE  [np ??/n  ?/wD  ??
?/n  ] ] ?/wP  ????/v  ] ]?/wE  ] 
We define a Chinese sentence as the Chinese 
word serials ending with period, question mark 
or exclamation mark in the Chinese text. Usually, 
a Chinese sentence can describe a complex sit-
uation with several inter-related events.  It con-
sists of several clauses separated by commas or 
semicolons to describe one or more detailed 
event content. We call these clauses as event 
description clauses.  
We use the following example to explain the 
relationship between a Chinese sentence and 
                                                 
1 Each bracketed constituent is annotated with consti-
tuent tag and head positions separated by ?-?.   
Constituent tags used in the sentence are: dj-simple 
sentence, vp-verb phrase, np-noun phrase, pp-
preposition phrase. 
POS tags used are: s-space noun, wP-comma, rNP-
personal pronoun, d-adverb, v-verb, p-preposition,  
cC-conjunction, uJDE-particle,  n-noun, vM-
modality verb; 
 
event description clauses.  
? [ ???????????????
????????????? ]?[ ?
????? ] ?[ ?????????
??????? ]?                             (1) 
? [ Along the way, we see the trees have 
been cut down for regeneration, and the 
trees needed to be cut for building ]. [ All 
of them are useful building material ]. 
[ We also see several freight trucks and 
tractors for carry away trees going south 
and north ]. 
The sentence gives us several sequential situa-
tions through the vision changing along the au-
thor?s journey way: Firstly, we see the trees that 
have been cut down. They are useful building 
material. Then, we see several trucks and trac-
tors to carry away these trees. They are going 
south and north busily.  All the above situations 
are described through three EDCs annotated 
with bracket pairs in the sentence.  
Interestingly, in the corresponding English 
translation, the same situation is described 
through three English sentences with complete 
subject and predicate structures.  They show dif-
ference event description characteristics of these 
two languages. 
The Chinese author tends to describe a com-
plex situation through a sentence. Many com-
plex event relations are implicit in the structural 
sequences or semantic connections among the 
EDCs of the sentence. So many subjects or ob-
jects of an EDC can be easily omitted based on 
the adjacent contexts. 
The English author tends to describe a com-
plex situation through several sentences. Each 
sentence can give a complete description of an 
event through the subject and predicate structure. 
The event relations are directly set through the 
paragraph structures and conjunctions. 
The distinction between Chinese sentence and 
EDC can make us focus on different evaluation 
emphasis in the CIPS-Bakeoff-2010 section. 
For an EDC, we can focus on the parsing per-
formance of event content recognition. So we 
design a special metric to evaluate the recall of 
the event recognition based on the syntactic 
parsing results. 
For a sentence, we can focus on the parsing 
performance of event relation recognition. So we 
separate the simple and complex sentence con-
stitutes and give different evaluation metrics for 
them.  
Some detailed designations of the evaluation 
metrics can be found in section 4.  
3 Data preparation 
The evaluation data were extracted from Tsing-
hua Chinese Treebank (TCT) and PKU Chinese 
Treebank (PKU-CTB).  
TCT (Zhou, 2004) adopted a new annotation 
scheme for Chinese Treebank. Under this 
scheme, every Chinese sentence will be anno-
tated with a complete parse tree, where each 
non-terminal constituent is assigned with two 
tags. One is the syntactic constituent tag, such as 
noun phrase(np), verb phrase(vp), simple sen-
tence(dj), complex sentence(fj), etc., which de-
scribes basic syntactic characteristics of a consti-
tuent in the parse tree. The other is the grammat-
ical relation tag, which describes the internal 
structural relation of its sub-components, includ-
ing the grammatical relations among different 
phrases and the event relations among different 
clauses. These two tag sets consist of 16 and 27 
tags respectively.  
Now we have two Chinese treebanks anno-
tated under above scheme: (1) TCT version 1.0, 
which is a 1M words Chinese treebank covering 
a balanced collection of journalistic, literary, 
academic, and other documents; (2) TCT-2010, 
which consists of 100 journalistic annotated ar-
ticles. The following is an annotated sentence 
under TCT scheme: 
? [zj-XX [fj-LS [dj-ZW ??/rN [vp-PO ?/v 
[dj-ZW [np-DZ ?/rN ??/rN ] [vp-PO ?
/v  ??/m  ] ] ] ] ?/? [dj-ZW ?/rN  [vp-
LW [vp-PO ??/v  [sp-DZ ??/n  ??
/s  ] ] [vp-PO ??/v  [np-DZ [mp-DZ ?/m  
?/qN  ] ??/n ] ] ] ] ] ?/? ] 2               (2) 
PKU-CTB (Zhan et al, 2006) adopted a tradi-
tional syntactic annotation scheme. They anno-
tated Chinese sentences with syntactic constitu-
                                                 
2 Some grammatical relation tags used in the sentence 
are as follows: LS?complex timing event relation, 
ZW?subject-predicate relation, DZ?modifier-head 
relation,  PO?predicate-object relation. 
ent and head position tags in a complete parse 
tree. The tag set consists of 22 constituent tags.  
Because every content word is directly annotated 
with suitable constituent tag, there are many un-
ary phrases in PKU-CTB annotated sentences. 
Its current annotation scale is 881,771 Chinese 
words, 55264 sentences. The following is an 
annotated sentence under PKU-CTB scheme: 
? ( zj ( !fj ( !fj ( !dj ( np ( vp ( !v ( ?? ) ) !np 
( !n ( ?? ) ) ) !vp ( !vp ( !v ( ? ) ) np ( !n 
( ? ) ) ) ) wco ( ? ) dj ( np ( ap ( !b ( ?
? ) ) !np ( !n ( ?? ) ) ) !vp ( dp ( !d 
( ? ) ) !vp ( !vp ( !vp ( !v ( ? ) ) v ( ? ) ) 
np ( qp ( mp ( !rm ( ? ) ) !q ( ? ) ) !np ( np 
( !n ( ??? ) ) !np ( !n ( ?? ) ) ) ) ) ) ) ) 
wco ( ? ) vp ( c ( ?? ) !vp ( !v ( ? ) np 
( ap ( !b ( ?? ) ) !np ( !n ( ?? ) ) ) vp 
( !vp ( !v ( ?? ) ) vp ( !vp ( !v ( ?? ) ) 
vp ( !v ( ?? ) ) ) ) ) ) ) wfs ( ? ) ) )3      (3) 
Due to the different annotation schemes and 
formats used in these two treebanks, we pro-
posed the following strategies to build the gold-
standard data set for Task 2-1 and Task 2-2: 
1) Unify POS tag set 
The PKU-CTB has 97 POS tags, and TCT has 
70 POS tags. After analyzing these POS tags, we 
found most of them have same meanings. So we 
designed a unified POS tag set with 58 inter-
sected tags. All the POS tags used in PKU-CTB 
and TCT can be automatically mapped to this 
unified tag set. 
2) Transform PKU-CTB annotations 
Firstly, we mapped the POS tags into the uni-
fied tag set, and transformed the word and POS 
tag format into TCT?s format. Then, we deleted 
all unary constituents in PKU-CTB parse trees 
and transferred the constituent structures and 
tags into TCT?s constituent tags. Finally, we 
manually proofread the transformed parse trees 
to modify some constituent structures that are 
inconsistent with TCT annotation scheme. About 
5% constituents are modified. 
                                                 
3 The PKU-CTB uses the similar POS and constituent 
tags with TCT scheme. The exclamation symbol ?!? is 
used to annotate the head of each constituent in the 
parse tree. 
3) Extract EDCs and event annotations from 
TCT 
Based on the detailed grammatical relation 
tags annotated in TCT, we can easily extract 
each EDC for a TCT sentence (Zhou and Zhu, 
2009). Then, we proposed an algorithm to ex-
tract different event constructions in each EDC 
and build a large scale Chinese event bank. It 
can be used as a gold-standard data to evaluation 
the event recognition performance of an auto-
matic syntactic parser in Task 2-1. 
An event construction is an event chunk serial 
controlled by an event target verb. It is a basic 
unit to describe event content. For example, for 
the first EDC extracted from the above sentence 
(1), we can obtain the follow four event con-
structions for the event target verb ????, ??
??, ???, and ???? . 
? [D-sp ??/s-@] ?/wP [S-np ??/rNP-
@  ] [D-dp ??/d-@  ] [P-vp-Tgt ??/v-
@  ] [O-np ?/p  ??/v  ?/cC  ??/v  ?
/uJDE  ??/n-@  ?/wP  ?/p  ?/v  ?/n  
?/vM  ??/v  ?/uJDE  ??/n-@  ]4 
? [D-pp ?/p  ??/v-@  ] [P-vp-Tgt ?/vM  
??/v-@ ] ?/uJDE  [H-np ??/n-@  ] ? 
? ?  ?/p  [P-vp-Tgt ?/v-@  ] [O-np ?/n-
@  ] ?/vM  ??/v  ?/uJDE  ??/n   
? ? [D-pp ?/p ?/v-@ ?/n ] [P-vp-Tgt ?
/vM ??/v-@ ]?/uJDE  [H-np ??/n-@  ] 
4) Obtain TCT constituent structure trees 
We can easily select all syntactic constituent 
tags annotated in TCT sentences to build the 
gold-standard parsing trees for Task 2-2. 
We mainly used the journalistic and academic 
texts annotated in TCT and PKU-CTB to build 
different training and test set for task 2-1 and 2-2. 
Table 1 summarizes current building status of 
these gold-standard data sets. 
 
 
 
                                                 
4 Each event chunk is annotated with bracket pairs 
with functional and constituent tags. Some functional 
tags used in the EDCs are as follows: D?adverbial, 
S?subject, P?predicate, O?object. The constituent 
tags are same with that ones used in above parse tree. 
The head of each chunk is indicated through ?-@?. 
Data 
set 
Source Genre Methods 
2-1, 
TR 
TCT 
ver1.0 
News, 
Academy 
POS unification, 
EDC and event 
extraction 
2-1, 
TS 
TCT-
2010 
News POS unification, 
EDC and event 
extraction 
2-2, 
TR 
TCT 
ver1.0 
News, 
Academy 
POS unification, 
Parse tree extrac-
tion 
2-2, 
TS 
PKU-
CTB 
Academy POS unification, 
annotation trans-
formation 
Table 1 Gold-standard data building status 
(TR=Training data, TS=Test data) 
We selected all news and academic texts an-
notated in TCT ver1.0 to form the training set of 
Task 2-1 and 2-2. 1000 EDCs extracted from 
TCT-2010 were selected as the test set of Task 
2-1. These sentences are extracted from the 
People?s Daily corpus with the same source of 
TCT ver1.0. 1000 sentences extracted from 
PKU-CTB were selected as the test set of Task 
2-2. Most of them are extracted from the tech-
nical reports or popular science articles. They 
have much more technical terms than the encyc-
lopedic articles used in TCT ver1.0. Table 2 
shows the basic statistics of all the training and 
test sets in Task 2. 
Data set Word 
Sum 
Sent.  
Sum 
Average
Length
2-1, TR 425619 37219 11.44 
2-1, TS 9182 1000 9.18 
2-2, TR 481061 17529 27.44 
2-2, TS 26381 1000 26.38 
Table 2 Basic statistics of Task 2 
4 Evaluation metrics 
For Task 2-1, we designed three kinds of evalua-
tion metrics: 
1) POS accuracy (POS-A) 
This metri is used to evaluate the performance 
of automatic POS tagging. Its computation for-
mula is as follows: 
? POS accuracy = (sum of words with cor-
rect POS tags) / (sum of words in gold-
standard sentences) * 100% 
The correctness criteria of POS tagging is as 
follows:  
? The automatically assigned POS tag is 
same with the gold-standard one. 
2) Constituent parsing evaluation 
We selected three commonly-used metrics to 
evaluation the performance of constituent pars-
ing: labeled precision, recall, and F1-score. 
Their computation formulas are as follows: 
? Precision = (sum of correctly labeled 
constituents ) / (sum of parsed constitu-
ents) * 100% 
? Recall = (sum of correctly labeled consti-
tuents) / (sum of gold-standard constitu-
ents) *100% 
? F1-score = 2*P*R / (P+R) 
Two correctness criteria are used for constitu-
ent parsing evaluation: 
? ?B+C? criteria: the boundaries and syn-
tactic tags of the automatically parsed 
constituents must be same with the gold-
standard ones. 
? ?B+C+H? criteria: the boundaries, syntac-
tic tags and head positions of the auto-
matically parsed constituents must be 
same with the gold-standard ones. 
3) Event recognition evaluation 
We only considered the recognition recall of 
each event construction annotated in the event 
bank, due to the current parsing status of Task 2-
1 output.  For each event target verb annotated in 
the event bank, we computed their Micro and 
Macro average recognition recall. The computa-
tion formulas are as follows: 
? Micro Recall = (sum of all correctly rec-
ognized event constructions) / (sum of all 
gold standard event constructions) * 
100% 
? Macro Recall = (sum of Micro-R of each 
event target verb ) / (sum of event target 
verbs in gold-standard set )  
The correctness criteria of event recognition 
should consider following two matching condi-
tions: 
Condition 1: Each event chunk in a gold-
standard event construction should have a cor-
responding constituent in the automatic parse 
tree. For the single-word chunk, the automatical-
ly assigned POS tag should be same with the 
gold standard one. For the multiword chunk, the 
boundary, syntactic tag and head positions of the 
automatically parsed constituent should be same 
with the gold-standard ones. Meanwhile, the cor-
responding constituents should have the same 
layout sequences with the gold standard event 
construction.  
Condition 2: All event-chunk-corresponding 
constituents should have a common ancestor 
node in the parse tree. One of the left and right 
boundaries of the ancestor node should be same 
with the left and right boundaries of the corres-
ponding event construction. 
For Task 2-2, we design two kinds of evalua-
tion metrics: 
1) POS accuracy (POS-A) 
This index is used to evaluate the performance 
of automatic POS tagging. Its formula and cor-
rectness criteria are same with the above defini-
tions of Task 2-1. 
2) Constituent parsing evaluation 
To evaluate the parsing performance of event 
relation recognition in complex Chinese sen-
tences, we firstly divided all parsed constituents 
into following two parts: 
? Constituent of complex sentence (C_S), 
whose tag is ?fj?; 
? Constituents in simple sentence (S_S), 
whose tags are belong to the tag set {dj, 
vp, ap, np, sp, tp, mp, mbar, dp, pp, bp}. 
Then we computed the labeled precision, re-
call and F1-socre of these two parts and obtain 
the arithmetic mean of these two F1-score as the 
final ranking index. Their computation formulas 
of each part are as follows: 
? Precision = (sum of correctly labeled 
constituents in one part) / (sum of parsed 
constituents in the part) * 100% 
? Recall = (sum of correctly labeled consti-
tuents in one part) / (sum of gold-
standard constituents in the part) *100% 
? F1-score = 2*P*R / (P+R) 
? Total F1-Score = (C_S F1 + S_S F1) / 2 
We use the above ?B+C? correctness criteria 
for constituent evaluation in Task 2-2. 
 
ID Participants Task 2-1 Task 2-2 
TPI Open close TPI open Close
01 School of Computer Sci. and Tech., 
Harbin Institute of Technology 
Y   Y  1 
02 Knowledge Engineering Research 
Center, Shenyang Aerospace Univ. 
Y  3 Y  2 
03 Dalian University of Technology Y  1 Y  1 
04 National Laboratory of Pattern Rec-
ognition Institute of Automation, 
Chinese Academy of Science 
Y 2 2 Y 4 2 
05 Beijing University of Posts and Tele-
communications 
Y  2 Y   
06 University of Science and Technolo-
gy of China 
Y   Y   
07 Dept. of Computer Science and 
Technology, Shanghai Jiao Tong 
University, 
Y  3 Y  3 
08 Soochow University Y   Y   
09 Harbin Institute of Technology Y  1 Y   
10 German Research Center for Artifi-
cial Intelligence 
Y 1 1 Y 1  
11 China Center for Information Indus-
try Development 
N   Y 1  
12 City University of Hong Kong Y   Y   
13 National Central University Y   Y   
Total  12 3 13 13 6 9 
Table 3  Result submission data of all participants in Task 2. (TPI=Take Part In)
5 Evaluation results 
The Task 2 of CIPS-Bakeoff-2010 attracted 13 
participants. Almost all of them took part in the 
two subtasks: Task 2-1 and 2-2. Only one partic-
ipant took part in the Task 2-2 subtask alone.  
Among them, 9 participants submitted parsing 
results.  In Task 2-1, we received 16 parsing re-
sults, including 13 close track systems and 3 
open track systems. In Task 2-2, we received 15 
parsing results, including 9 close track systems 
and 6 open track systems. Table 3  shows the 
submission information of all participants of 
Task 2. 
5.1 Task 2-1 analysis 
We evaluated the parsing performance of EDC 
on the constituent and event level respectively. 
The constituent parsing evaluation only consid-
ers the parsing performance of one single consti-
tuent. The event recognition evaluation will con-
sider the recognition performance of a complete 
event construction. So it can provide more useful 
reference information for event extraction appli-
cation. 
Table 5 and Table 6 show the evaluation re-
sults of constituent parsing in the close and open 
tracks respectively. In the close track, the best 
F1-score under ?B+C? criteria is 85.39%, while 
the best F1 score under ?B+C+H? criteria is 
83.66%.  Compared with the evaluation results 
of the task 5 in CIPS-ParEval-2009 under the 
similar training and test conditions (Zhou and Li, 
2009), the performance of head identification is 
improved about 2%. Table 4 shows the detailed 
comparison data. 
Rank ID ?B+C? ?B+C+H? POS-A
09-1 08 87.22 83.70 Gold 
09-2 15 86.25 81.75 Gold 
10-1 02 85.39 83.66 93.96
10-2 04 84.36 82.51 91.84
Table 4 F1 scores of the Top-2 single-model 
close-track systems in the ParsEval-2009 and 
ParsEval-2010. 
Table 7 and Table 8 show the evaluation re-
sults of event recognition in the close and open 
tracks respectively. When we consider the com-
plete event constructions contained in a parse 
tree, the best Macro-Recall is only about 71%. 
There are still lots of room to improve in the fu-
ture.  
 
ID Sys-ID Model ?B+C? ?B+C+H? POS-A Rank
P R F1 P R F1 
02 SAU01 Single 85.42 85.35 85.39 83.69 83.63 83.66 93.96 1 
02 SAU02 Single 85.02 85.11 85.06 83.21 83.31 83.26 93.96 2 
04 a Single 84.40 84.32 84.36 82.55 82.47 82.51 91.84 3 
04 b Single 83.79 83.74 83.76 81.82 81.78 81.80 91.67 4 
10 DFKI_C Single 82.93 82.85 82.89 80.54 80.46 80.50 81.99 5 
02 SAU03 Single 80.28 79.31 79.79 78.55 77.61 78.08 93.93 6 
07 b Single 78.61 78.76 78.69 76.61 76.75 76.68 92.77 7 
07 c Single 77.78 78.13 77.96 75.78 76.13 75.95 92.77 8 
05 BUPT Single 74.86 76.05 75.45 71.06 72.20 71.63 87.00 9 
05 BUPT Multiple 74.48 75.64 75.05 70.72 71.81 71.26 87.00 10 
03 DLUT Single 71.42 71.19 71.30 69.22 69.00 69.11 86.69 11 
09 InsunP Single 70.69 70.48 70.58 67.07 66.87 66.97 77.87 12 
07 a Single 9.09 12.51 10.53 7.17 9.88 8.31 7.02 13 
Table 5 Constituent parsing evaluation results of Task 2-1 (Close Track), ranked with ?B+C+H?- F1 
ID Sys-ID Model ?B+C? ?B+C+H? POS-A Rank
P R F1 P R F1 
04 a Single 86.07 86.08 86.08 84.27 84.28 84.27 92.51 1 
04 b Single 83.79 83.74 83.76 81.82 81.78 81.80 91.67 2 
10 DFKI_C Single 82.37 83.05 82.71 79.99 80.65 80.32 81.87 3 
Table 6 Constituent parsing evaluation results of Task 2-1 (Open Track), ranked with ?B+C+H?- F1 
 
ID Sys-ID Model Micro-R Macro-R POS-A Rank 
02 SAU01 Single 72.47 71.53 93.96 1 
02 SAU02 Single 72.93 70.71 93.96 2 
04 a Single 67.37 65.05 91.84 3 
04 b Single 67.17 64.23 91.67 4 
02 SAU03 Single 63.73 63.54 93.93 5 
07 c Single 63.14 62.48 92.77 6 
07 b Single 62.74 62.47 92.77 7 
10 DFKI_C Single 55.99 53.58 81.99 8 
03 DLUT Single 51.75 53.33 86.69 9 
05 BUPT Single 53.08 48.82 87.00 10 
05 BUPT Multiple 52.88 48.75 87.00 11 
09 InsunP Single 43.15 43.14 77.87 12 
07 a Single 1.13 0.79 7.02 13 
Table 7  Event recognition evaluation results of Task 2-1 (Close Track), ranked with Macro-R 
ID Sys-ID Model Micro-R Macro-R POS-A Rank 
04 a Single 70.62 69.33 92.51 1 
04 b Single 67.17 64.23 91.67 2 
10 DFKI_C Single 54.47 52.25 81.87 3 
Table 8 Event recognition evaluation results of Task 2-1 (Open Track), ranked with Macro-R 
5.2 Task 2-2 analysis 
Table 9 and Table 10 show the evaluation results 
of constituent parsing in the close and open 
tracks of Task 2-2 respectively. In each track, 
the F1-score of the complex sentence recogni-
tion is about 5-6% lower than that of the consti-
tuents in simple sentences. It indicates the diffi-
cultness of event relation recognition in real 
world Chinese sentences. Some new features 
need to be explored for them.  
Almost all the parsing performances of the 
systems in the open track are better than that 
ones in the close track. It indicates some outside 
language resources may useful for parsing per-
formance improvement. Compared with the 
commonly-used English Treebank PTB with 
about 1M words, our current annotated data may 
be not enough to train a good Chinese parser. 
We may need to collect more useful treebank 
data in the future evaluation tasks.  
The F1-scores of constituent parsing in simple 
sentences of Task 2-2 are still about 5-6% lower 
than that of EDC constituents under ?B+C? crite-
ria in Task 2-1. It indicates some lower level 
errors may be propagated to up-level constitu-
ents during complex sentence parsing. How to 
restrict the error propagation chains is an inter-
esting issue need to be explored.  
5.3 POS tagging analysis 
The best POS accuracy in Task 2-1 is 93.96%, 
approaching to the state-of-art performance of 
the Task 1 in CIPS-ParsEval-2009, under similar 
training and test conditions. But the POS accura-
cy in Task 2-2 is about 3-4% lower than it. A 
possible reason is that there are lots of unknown 
words in the test data of Task 2-2. Most of them 
are technical terms outside the training data lex-
icon. How to deal with the unknown words is 
still an open challenge for POS tagging.  
6 Conclusions 
The paper introduced the task designing ideas, 
data preparation methods, evaluation metrics and 
results of the second Chinese syntactic parsing 
evaluation jointed with SIGHAN Bakeoff tasks. 
Some new contributions of the evaluation are 
as follows: 
1) Set a new metric to evaluate the event 
construction recognition performance in 
the constituent parsing tree; 
 
 
 
ID Sys-ID Model Constituents in S_S C_S constituent Total POS-A Rank
P R F1 P R F1 F1 
04 b Single 77.79 77.47 77.63 69.55 76.50 72.86 75.24 88.79 1 
04 a Single 77.91 77.54 77.73 68.47 76.90 72.44 75.08 88.95 2 
O2 SAU01 Single 78.64 78.73 78.69 70.22 71.62 70.91 74.80 91.05 3 
O2 SAU02 Single 78.46 78.34 78.40 69.48 72.42 70.92 74.66 91.03 4 
03 DLUT Single 61.67 59.75 60.69 65.27 67.31 66.27 63.48 79.67 5 
01 CHP Single 70.20 69.64 69.92 53.95 59.47 56.58 63.25 89.62 6 
07 b Single 55.33 59.57 57.37 6.25 0.64 1.16 29.26 89.01 7 
07 c Single 52.57 57.69 55.01 7.47 1.68 2.74 28.88 89.01 8 
07 a Single 0.71 1.00 0.83 0.00 0.00 0.00 0.42 1.39 9 
Table 9  Constituent parsing evaluation results of Task 2-2 (Close Track), ranked with Tot-F1 
(S_S=simple sentence, C_S=complex sentence) 
ID Sys-ID Model Constituents in S_S C_S constituent Total POS-A Rank
P R F1 P R F1 F1 
04 d Single 80.04 79.68 79.86 70.11 76.50 73.17 76.51 89.59 1 
04 a Single 80.27 79.99 80.13 70.36 75.54 72.86 76.50 89.69 2 
04 c Single 80.25 79.95 80.10 70.40 75.30 72.77 76.44 89.78 3 
04 b Single 80.02 79.68 79.85 69.82 75.62 72.60 76.22 89.75 4 
10 DFKI_C Single 79.37 79.27 79.32 71.06 73.22 72.13 75.72 81.23 5 
11* CCID Single / / / / / / / / / 
Table 10 Constituent parsing evaluation results of Task 2-2 (Open Track), ranked with Tot-F1 
(S_S=simple sentence, C_S=complex sentence) There are some data format errors in the submitted 
results of CCID system (ID=11) 
 
2) Set a separated metric to evaluate the 
event relation recognition performance in 
complex Chinese sentence. 
Through this evaluation, we found: 
1) The event construction recognition in a 
Chinese EDC is still a challenge. Some 
new techniques and machine learning 
models need to be explored for this task. 
2) Compared with about 90% F1-score of 
the state-of-art English parser, the 75% 
F1-score of current Chinese parser is still 
on its primitive stage. There is a long way 
to go in the future. 
3) The event relation recognition in real 
world complex Chinese sentences is a dif-
ficult problem. Some new features and 
methods need to be explored for it. 
They lay good foundations for the new task 
designation in the future evaluation round. 
Acknowledgements 
Thanks Li Yemei for her hard work to organize 
the evaluation. Thanks Li Yanjiao and Li Yumei 
for their hard work to prepare the test data for 
the evaluation.  Thanks Zhu Muhua for making 
the evaluation tools and processing all the sub-
mitted data. Thanks all participants of the evalu-
ation. 
The work was also supported by the research 
projects of National Science Foundation of Chi-
na (Grant No. 60573185, 60873173) and National 
863 High-Tech research projects (Grant No. 
2007AA01Z173). 
References 
E. Black, S. Abney, et al 1991. A Procedure for 
Quantitatively Comparing the Syntactic Coverage 
of English Grammars. In Speech and natural lan-
guage: proceedings of a workshop, held at Pacific 
Grove, California, page 306.  
E. Charniak and M. Johnson. 2005. Coarse-to-fine 
nbest parsing and MaxEnt discriminative rerank-
ing. In Proc. of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, page 180. 
S. Clark and J.R. Curran. 2007. Wide-coverage effi-
cient statistical parsing with CCG and log-linear 
models. Computational Linguistics, 33(4):493?552. 
D. Klein and C. Manning. 2003. Accurate Unlexica-
lized Parsing. In Proc. of ACL-03. 
M. Collins. 2003. Head-driven statistical models for 
natural language parsing. Computational linguis-
tics, 29(4):589?637. 
J. Hockenmaier and M. Steedman. 2007. CCGbank: a 
corpus of CCG derivations and dependency struc-
tures extracted from the Penn Treebank. Computa-
tional Linguistics, 33(3):355?396. 
R. Levy and C. Manning. (2003). Is it harder to parse 
Chinese, or the Chinese Treebank? In Proc. of 
ACL-03. 
J. Li, G. Zhou, and H.T. Ng. 2010. Joint Syntactic 
and Semantic Parsing of Chinese. In Proc. of the 
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1108?1117. 
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 
2005. Non-projective dependency parsing using 
spanning tree algorithms. In Proc. of HLT/EMNLP, 
pages 523?530. 
Mitchell P.Marcus, Mary Ann Marcinkiewicz, and 
Beatrice Santorini. 1993. Building a Large Anno-
tated Corpus of English: The Penn Treebank, 
Computational Linguistics, 19(2): 313-330 
J. Nivre, J. Hall, J. Nilsson, el.al. 2007. Malt-Parser: 
A language-independent system for data driven 
dependency parsing. Natural Language Engineer-
ing, 13(02):95?135. 
S. Petrov and D. Klein. 2007. Improved inference for 
unlexicalized parsing. In Proc. of NAACL HLT 
2007, pages 404?411. 
N. Xue, F. Chiou, and M. Palmer. 2002. Building a 
large-scale annotated Chinese corpus. In Proc. of 
COLING-2002. 
Zhan Weidong, Chang Baobao, Dui Huiming, Zhang 
Huarui. 2006. Recent Developments in Chinese 
Corpus Research. Presented in The 13th NIJL In-
ternational Symposium, Language Corpora: Their 
Compliation and Application. Tokyo, Japan.  
Zhou Qiang, 2004. Chinese Treebank Annotation 
Scheme. Journal of Chinese Information, 18(4):1-
8.  
Zhou Qiang, Li Yuemei. 2009. Evaluation report of 
CIPS-ParsEval-2009.  In Proc. of First Workshop 
on Chinese Syntactic Parsing Evaluation, Beijing 
China.  
Zhou Qiang, Zhu Jingbo. 2009.  Evaluation tasks and 
data preparation of CIPS-ParsEval-2009, 
http://www.ncmmsc.org/ CIPS-ParsEval-20 
 
Proceedings of the TextGraphs-7 Workshop at ACL, pages 44?54,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Bringing the Associative Ability to Social Tag Recommendation 
 
Miao Fan,
?? 
Yingnan Xiao
?
 and Qiang Zhou
?
 
? 
Department of Computer Science and Technology, Tsinghua University 
?
School of Software Engineering, Beijing University of Posts and Telecommunications 
{fanmiao.cslt.thu,lxyxynt}@gmail.com,  
zq-lxd@mail.tsinghua.edu.cn 
 
 
Abstract 
Social tagging systems, which allow users to 
freely annotate online resources with tags, 
become popular in the Web 2.0 era. In order to 
ease the annotation process, research on social 
tag recommendation has drawn much attention 
in recent years. Modeling the social tagging 
behavior could better reflect the nature of this 
issue and improve the result of recommendation. 
In this paper, we proposed a novel approach for 
bringing the associative ability to model the 
social tagging behavior and then to enhance the 
performance of automatic tag recommendation. 
To simulate human tagging process, our 
approach ranks the candidate tags on a 
weighted digraph built by the semantic 
relationships among meaningful words in the 
summary and the corresponding tags for a 
given resource. The semantic relationships are 
learnt via a word alignment model in statistical 
machine translation on large datasets. 
Experiments on real world datasets demonstrate 
that our method is effective, robust and 
language-independent compared with the state-
of-the-art methods. 
1 Introduction 
Social tagging systems, like Flickr 1 , Last.fm 2 , 
Delicious 3  and Douban 4 , have recently become 
major infrastructures on the Web, as they allow 
users to freely annotate online resources with 
personal tags and share them with others. Because 
of the no vocabulary restrictions, there are different 
kinds of tags, such as tags like keywords, category 
names or even named entities. However, we can 
                                                          
1 http://www.flickr.com 
2 http://www.lastfm.com 
3 http://delicious.com 
4 http://www.douban.com 
still find the inner relationship between the tags 
and the resource that they describe. Figure 1 shows 
a snapshot of a social tagging example, where the 
famous artist, Michael Jackson was annotated with 
multiple social tags by users in Last.fm2. Actually, 
Figure 1 can be divided into three parts, which are 
the title, the summary and the tags respectively. 
 
 
Figure 1: A music artist entry from website Last.fm2 
 
We can easily find out that social tags concisely 
indicate the main content of the given online 
resource and some of them even reflect user 
interests. For this reason, social tagging has been 
widely studied and applied in recommender 
systems (Eck et al, 2007; Musto et al, 2009; Zhou 
et al, 2010), advertising (Mirizzi et al, 2010), etc. 
For the sake of easing the process of user 
annotation and providing a better effect of human-
computer interaction, researchers expected to build 
44
automatic social tagging recommender systems, 
which could automatically suggest proper tags for 
a user when he/she wants to annotate an online 
resource. By observing huge amount of online 
resources, researchers found out that most of them 
contain summaries, which could play an important 
role in briefly introducing the corresponding 
resources, such as the artist entry about Michael 
Jackson in Figure 1. Thus some of them proposed 
to automatically suggest tags based on resource 
summaries, which are collectively known as the 
content-based approach (F. Ricci et al, 2011). 
The basic idea of content-based approach in 
recommender systems is to select important words 
from summaries as tags. However, this is far from 
adequate as not all tags are statistically significant 
in the summaries. Some of them even do not 
appear in the corresponding summaries. For 
example, in Figure 1, the popular tag dance does 
not appear in the summary, but why most of users 
choose it as a proper tag to describe Michael 
Jackson.  This ?out-of-summary? phenomenon 
reflects a fact that users usually exploit their own 
knowledge and associative ability to annotate 
online resources. When a summary comes, they 
associate the important words in the summary with 
other semantic-related tags based on their 
knowledge. To improve the automatic tag 
recommendation, a social computing issue (Wang 
et al, 2007), modeling the social tagging behavior 
is the straightforward way. Namely, how to 
analyze the human tagging process and propose a 
suitable approach that can help the computer to 
simulate the process are what we will explore in 
this paper.   
The novel idea of our approach is to rank the 
candidate tags on a weighted digraph built by the 
semantic relationships among meaningful words in 
the summary and the corresponding tags for a 
given resource. The semantic relationships are 
learnt via a word alignment model in statistical 
machine translation. Our approach could bring the 
associative ability to social tag recommendation 
and naturally simulate the whole process of human 
social tagging behavior and then to enhance the 
performance of automatic tag recommendation. So, 
we name this approach for Associative Tag 
Recommendation (ATR). 
The remainder of the paper is organized as 
follows. Section 2 analyzes the process of human 
tagging behavior. Section 3 describes our novel 
approach to simulate the process of human tagging 
behavior for social tag recommendation. Section 4 
compares our approach with the state-of-the-art 
and baseline methods and analyzes the parameter 
influences. Section 5 surveys some related work in 
social tag recommendation. Section 6 concludes 
with our major contributions and proposes some 
open problems for future work. 
 
2 Human Tagging Behavior Analysis 
Here, we will analyze the human tagging process 
to discover the secret why some of the tags are 
widely annotated while are not statistically 
significant or even do not appear in the summaries. 
In most cases, the information in summaries is 
too deficient for users to tag resources or to reflect 
personalities. Users thus exploit their own 
knowledge, which may be partly learnt from other 
resource entries containing both summaries and 
tags in Table 1. Then when they want to tag an 
online resource, they will freely associate 
meaningful words in the summary with other 
semantic related words learnt from former reading 
experiences. However, the result of this association 
behavior will be explosive. Users should judge and 
weigh these candidate tags in brain, usually via 
forming a semantic related word network and 
finally decide the tags that they choose to annotate 
the given resource.  
For example, after browsing plentiful of 
summary-tag pairs, we could naturally acquire the 
semantic relationships between the words, such as 
?singer?, ?pop?, in the summary and the tag, 
?dance?. If we tag the artist entry in Figure 1, the 
tag ?dance? is more likely associated by the words 
like ?pop?, ?artist?, ?Rock & Roll? et al While 
reading the summary of artist Michael Jackson in 
Figure 1, we may construct an abstract tag-network 
in Figure 2 with the important words (king, pop, 
artist et al) in the summary, the associated tags 
(dance, 80s, pop et al and their semantic 
relationships.  
 
Summary: David Lindgren (born April 28, 1982 
in Skelleftea, Sweden) is a Swedish singer and 
musical artist? 
Tags: swedish, pop, dance, musical, david 
lindgren 
Summary: Wanessa God?i Camargo (born on 
45
December 28, 1982), known simply as Wanessa, is 
a Brazilian pop singer? 
Tags: pop, dance, female vocalists, electronic, 
electropop ? 
Table 1: Examples of artist entries from Last.fm2 
 
 
Figure 2: A part of the abstract associative tag-network 
in human brains. 
 
3 Associative Tag Recommendation 
We describe our ATR approach as a three-stage 
procedure by simulating the human annotation 
process analyzed in Section 2. Figure 3 shows the 
overall structure of our approach.  
 
 
Figure 3: The overview of ATR approach. 
 
Stage 1: Summary-tag pairs sampling. Given 
a large collection of tagged resources, we need to 
pre-process the dataset. Generally, the pre-
processing contains tokenizing the summaries, 
extracting the meaningful words and balancing the 
length ratio between the summaries and tags. 
Stage 2: Associative ability acquiring. We 
regard a summary-tag pair as a parallel text. They 
are really suitable to acquire the semantic relation 
knowledge by using word alignment model (In this 
paper, we adopt IBM Model-1) from the large 
amount of summary-tag pairs prepared by Stage 1. 
After gaining the translation probabilities between 
the meaningful words in summaries and tags, our 
social tagging recommender system initially has 
the capability of association, namely from one 
word to many semantic related tags. 
Stage 3: TagRank algorithm for 
recommendation. Stage 2 just helps our 
recommender system acquire the ability of 
associating one word with many semantic related 
tags. However, when the system faces a given 
resource with a long summary, the association 
results may be massive. Thus, we propose a 
TagRank algorithm to order the candidate tags on 
the weighted Tag-digraph, which is built by the 
meaningful words in the summary and their 
semantic related words. 
Before introducing the approach in details, we 
define some general notations, while the other 
specific ones will be introduced in the 
corresponding stage. In our approach, a resource is 
denoted as    , where   is the set of all 
resources. Each resource contains a summary and a 
set of tags. The summary    of resource is simply 
regarded as a bag of meaningful words    
              
  , where     is the count of 
meaningful word    and    is the number of the 
unique meaningful words in  . The tag set 
(annotations)    of resource   is represented as 
                 
  , where     is the count of tag    
and   is the number of the unique tags for  . 
 
3.1 Summary-Tag Pairs Sampling 
We consider that the nouns and tags that appear in 
the corresponding summary are meaningful for our 
tagging recommendation approach.  
46
It is not difficult for language, such as English, 
French et al As for Chinese, Thai and Japanese, 
we still need to do word segmentation (D. D. 
Palmer., 2010). Here, to improve the segmentation 
results of these language texts, we collect all the 
unique tags in resource   as the user dictionary to 
solve the out-of-vocabulary issue. This idea is 
inspired by M. Sun (2011) and we will discuss its 
effort on the performance improvement of our 
system in Section 4.3.  
After the meaningful words have been extracted 
from the summaries, we regard the summary and 
the set of tags as two bags of the sampled words 
without position information for a given resource. 
The IBM Model-1(Brown et al, 1993) was 
adopted for training to gain the translation 
probabilities between the meaningful words in 
summary and the tags. Och and Ney (2003) 
proposed that the performance of word alignment 
models would suffer great loss if the length of 
sentence pairs in the parallel training data set is 
unbalanced. Moreover, some popular online 
resources may be annotated by hundreds of people 
with thousands of tags while the corresponding 
summaries may limit to hundreds of words. So, it 
is necessary to propose a sampling method for 
balanced length of summary-tag pairs.  
One intuitional way is to assign each meaningful 
word in summaries and tags with a term-frequency 
(TF) weight, namely     and    . For each 
extracted meaningful word   in a given summary 
  ,     
   
   
?    
  
   
 and the same tag set 
(annotations)    ,     
   
   
?    
  
   
 . Here, we bring a 
parameter   in this stage, which denotes the length 
ratio between the sampled summary and tag set, 
namely,          
 
3.2 Associative Ability Acquiring 
IBM Model-1 could help our social tagging 
recommender system to learn the lexical 
translation probability between the meaningful 
words in summaries and tags based on the dataset 
provided by stage 1.  We adjust the model to our 
approach, which can be concisely described as, 
 
             ?           
 
                   
 
For each resource  , the relationship between the 
sampled summary   =        
   and the sampled 
tags            
   is connected via a hidden 
variable          
  . For example,      
indicates word    in  at position   is aligned to 
tag    in   at position  . 
For more detail description on mathematics, the 
joint likelihood of   and an alignment   given    
is 
 
             
 
        
 ? (   |     
  
   
       
 
in which                and  (   |      is called 
the translation probability of    given    . The 
alignment is determined by specifying the values 
of    for   from 1 to  , each of which can take any 
value from 0 to  . Therefore, 
 
           
 
        
?  
  
    
 ? ? (   |     
  
   
  
     
 
    
 
The goal is to adjust the translation probabilities 
so as to maximize           subject to the 
constraints that for each  , 
 
?      
 
                                     
 
IBM Model-1 can be trained using Expectation-
Maximization (EM) algorithm (Dempster et al, 
1977) in an unsupervised fashion. At last, we 
obtain the translation probabilities between 
summaries and tags, i.e.,        and        for 
our recommender system acquiring associative 
ability. 
From Eq. (4), we know that IBM Model-1 will 
produce one-to-many alignments from one 
language to another language, and the trained 
model is thus asymmetric. Sometimes, there are a 
few translation pairs appear in both two direction, 
i.e., summary? tag (     ) and tag? summary 
(    ). For this reason, Liu et al (2011) proposed a 
harmonic means to combine the two models.  
 
47
       (
 
         
 
   
         
)
  
              
 
3.3 TagRank Algorithm for Recommendation 
By the time we have generated the ?harmonic? 
translation probability list between meaningful 
words in summaries and tags, our recommender 
system could acquire the capability of association 
like human beings. For instance, it could ?trigger? 
a large amount of semantic related tags from a 
given word: Novel (Figure 4). However, if we 
collected all the ?triggered? tags associated by 
each meaningful word in a given summary, the 
scale would be explosive. Thus we need to explore 
an efficient way that can not only rank these 
candidate tags but also simulate the human tagging 
behavior as much as possible.  
 
 
Figure 4: The association results from the word ?Novel? 
via our social tagging recommender system. 
 
Inspired by the PageRank algorithm (S. Brin and 
L. Page., 1998), we find out that the idea could be 
brought into our approach with a certain degree 
improvement as the human tagging ranking 
process is on a weighted Tag-digraph  . We regard 
the association relationship as one word 
recommending the corresponding candidate tags 
and the degree of preference could be quantified by 
the translation probabilities.  
For a given summary, we firstly sample it via 
the method described in stage 1 to obtain all the 
meaningful words, which are added to the graph as 
a set of seed vertices denoted as   . Then 
according to stage 2, we could obtain a set of 
semantic related vertices associated by these seeds 
denoted as   . We union the    and     to get the 
set of all candidate tags  . For a directed edge     
from    to   , the weight  (   )  equals the 
translation probability from    to   , namely 
 (  |   . So the weighted Tag-digraph could be 
formulized as, 
 
{
 
 
 
 
       
           
   {   }
    {(     )         }
 (   )    (  |   
                       
 
The original TextRank algorithm (Mihalcea et 
al., 2004) just considered the words recommending 
the nearest ones, and assumed that the 
recommending strengths were same. As all the 
words had the equal chance to recommend, it was 
the fact that all the edges in the graph gained no 
direction information. So this method brought little 
improvement on ranking results. In the Eq. (7) they 
used,        represents the set of all the vertices 
that direct to    and         denotes the set of all 
the vertices that direct from   . The factor   is 
usually set to 0.85. 
 
         
         ?
 
|   (  )|         
      (  )             
 
We improve the TextRank model and propose a 
TagRank algorithm (Eq. 8) that is suitable to our 
approach.  For each   , 
 (   )
?  (   )      (  )
 represents 
the proportion of trigger ability from    to   . This 
proportion multiplying the own score of    reflect 
the the degree of recommend contribution to   . 
After we sum up all the vertices willing to 
?recommend?   , namely          , We can 
calculate the score of    in one step. 
Some conceptual words could trigger hundreds 
of tags, so that our recommender system will suffer 
a rather high computation complexity. Thus, we 
add a parameter   which stands for the maximum 
out-degree of the graph  . That means for each 
vertex in the graph  , it can at most trigger top-  
candidate tags with the    highest translation 
probabilities. 
 
48
         
         ?
 (   )
?  (   )      (  )         
      (  ) 
     
 
Starting from vertex initial values assigned to 
the seed nodes (  ) in the graph, the computation 
iterates until convergence below a given threshold 
is achieved. After running the algorithm, a score is 
assigned to each vertex. Finally, our system can 
recommend best   tags with high score for the 
resource. 
 
4 Experiments 
4.1 Datasets and Evaluation Metrics 
Datasets: We prepare two real world datasets with 
diverse properties to test the performance of our 
system in different language environment. Table 2 
lists the statistical information of the English and 
Chinese datasets. 
 
Dataset P Vs Vt Ns Nt 
BOOK 29464 68996 40401 31.5 7.8 
ARTIST 14000 35972 4775 19.0 5.0 
Table 2: Statistical information of two datasets. P , Vs , 
Vt , Ns, and Nt represent the number of parallel texts, the 
vocabulary of summaries, the vocabulary of tags, the 
average number of unique words in each summary and 
the average number of unique tags in each resource 
respectively. 
 
The first dataset, BOOK, was crawled from a 
popular Chinese book review online community 
Douban4, which contains the summaries of books 
and the tags annotated by users. The second dataset, 
ARTIST, was freely obtained via the Last.fm2 API. 
It contains the descriptions of musical artists and 
the tags annotated by users. By comparing the 
characteristics of these two datasets, we find out 
that they differ in language, data size and the 
length ratio (Figure 5). The reason of preparing 
two datasets with diverse characteristics is that we 
would like to demonstrate that our approach is 
effective, robust and language-independent 
compared with others. 
 
Evaluation Metrics: We use precision, recall and 
F-measure to evaluate the performance of our ATR 
approach. Given a resource set  , we regard the set 
of original tags as   , the automatic recommended 
tag set as   . The correctly recommended set of 
tags can be denoted as        . Thus, precision, 
recall and F-measure are defined as5 
 
   
         
    
   
         
    
    
     
     
         
 
The final precision and recall of each method is 
computed by performing 7-fold cross validation on 
both two datasets. 
 
  
 
Figure 5: The length ratio distributions of BOOK and 
ARTIST datasets. 
 
4.2 Methods Comparison  
Baseline Methods: In this section, we compare the 
performance of our associative tagging 
recommendation (ATR) with three other relative 
methods, the state-of-the-art WTM (Liu et al, 
2011), TextRank (Mihalcea et al, 2004) and the 
traditional TFIDF (C. D. Manning et al, 2008; R. 
Baeza-Yates et al, 2011).  
                                                          
5 The reason why we do not calculate the precision, recall and 
F-measure alone is that we cannot guarantee that 
recommending at least one correct tag for each resource. 
49
The reasons we choose those methods to 
compare were as follows. 
?  WTM can reflect the state-of-the-art 
performance on content-based social tag 
recommendation. 
? TextRank can be regarded as a baseline 
method on graph-based social tag 
recommendation. 
? TFIDF, as a traditional method, represents the 
baseline performance and can validate the 
?out-of-summary? phenomenon. 
For the TFIDF value of each word in a given 
summary, it can be calculated by multiplying term 
frequency     
        
   
?    
  
   
 (log 
normalization) by inverted document 
frequency      
          
   
|?           |
  (inverse 
frequency smooth), where |?           | indicates 
the number of resources whose summaries contain 
word  . 
TextRank method regarded the word and its 
forward and backward nearest words as its 
recommendation. Thus, each word in a given 
summary is recommended by its neighborhood 
with no weight. Simply, we use Eq. (7) to calculate 
the final value of each word in a given summary. 
Liu et al (2011) proposed a state of the art 
method which summed up the product the weight 
of a word and its translation probabilities to each 
semantic related tag as the final value of each tag 
in a given resource (Eq. 10). 
 
          ?                
    
                 
 
Experiment Results: Figure 6 illustrates the 
precision-recall curves of ATR, WTM, TextRank 
and TFIDF on two datasets. Each point of a 
precision-recall curve stands for different number 
of recommended tags from     (upper left) to 
     (bottom right). From the Figure 6, we can 
observe that: 
? ATR out-performs WTM, TextRank and 
TFIDF on both datasets. This indicates that 
ATR is a language-independent approach for 
social tag recommendation. 
? ATR shows consistently better performance 
when recommending different number of tags, 
which implies that our approach is efficient 
and robust (Figure 7). 
 
 
 
Figure 6: Performance comparison among ATR, WTM, 
TextRank and TFIDF on BOOK and ARTIST datasets 
when      ,     and vertex initial values are 
assigned to one. 
 
 
 
50
Figure 7: F-measure of ATR, WTM, TextRank and 
TFIDF versus the number of recommended tags ( ) on 
the BOOK and ARTIST datasets when       ,     
and vertex initial values are assigned to one. 
 
4.3 Sampling Methods Discussion 
Section 3.1 proposed an idea on summary-tag pairs 
sampling, which collected all the unique tags as the 
user dictionary to enhance performance of the 
summary segmentation, especially for the Chinese, 
Thai, and Japanese et al Though M. Sun (2011) 
put forward a more general paradigm, few studies 
have verified his proposal. Here, we will discuss 
the efficiency of our sampling method. Figure 8 
shows the comparison of performance between the 
unsampled ATR and (sampled) ATR.  
 
 
Figure 8: Performance comparison between unsampled 
ATR and (sampled) ATR on BOOK datasets when 
     ,     and vertex initial values are assigned to 
one 
 
Experiments on the Chinese dataset BOOK 
demonstrates that our (sampled) ATR approach 
achieves average 19.2% improvement on 
performance compared with the unsampled ATR. 
4.4 Parameter Analysis  
In Section 3, we brought several parameters into 
our approach, namely the harmonic factor  which 
controls the proportion between model      and 
    , the maximum out-degree   which specifies 
the computation complexity of the weighted tag-
digraph and the vertex initial values which may 
affect the final score of some vertices if the 
weighted tag-digraph is not connected. 
We take the BOOK dataset as an example and 
explore their influences to ATR by using 
controlling variables method, which means we 
adjust the focused parameter with the other ones 
stable to observe the results. 
Harmonic factor: In Figure 9, we investigate the 
influence of harmonic factor via the curves of F-
measure of ATR versus the number of 
recommended tags on the BOOK dataset. 
Experiments showed that the performance is 
slightly better when      . As   controls the 
proportion between model      and     ,       
means model      contributes more on 
performance. 
 
 
Figure 9: F-measure of ATR versus the number of 
recommended tags on the BOOK dataset when 
harmonic factor   ranges from 0.0 to 1.0, when     
and vertex initial values are assigned to one. 
 
Maximum out-degree: Actually, during the 
experiments, we have found out that some 
meaningful words could trigger hundreds of 
candidate tags. If we bring all these tags to our 
Tag-Network, the computation complexity will be 
dramatically increased, especially in large datasets. 
To decrease the computation complexity with little 
impact on performance, we need to explore the 
suitable maximum out-degree. Figure 10 illustrates 
how the complexities of tag-digraph will influent 
the performance. We discover that ATR gains 
slight improvement when   is added from 5 to 9 
except the ?leap? from 1 to 5.  It means that     
will be a suitable maximum out-degree, which 
balances the performance and the computation 
complexity. 
51
 
Figure 10: F-measure of ATR versus the number of 
recommended tags on the BOOK dataset, when 
1           and vertex initial values are assigned 
to one. 
 
Vertex initial values: The seeds (meaningful 
words in the summaries) may not be semantic 
related, especially when the maximum out-degree 
is low. As a result, the graph   may be 
disconnected, so that the final score of each vertex 
after iteration may relate to the vertex initial values. 
In Figure 11, we compare three different vertex 
initial values, namely value-one, value of TF (local 
consideration) and value of TFIDF (global 
consideration) to check the influence. However, 
the results show that there is almost no difference 
in F-measure when the maximum out-degree   
ranges from 1 to 9. 
 
Figure 11: F-measure of ATR versus maximum out-
degree on BOOK dataset when the vertex initial values 
equal to Value-One, TF, TFIDF separately with       
and number of recommended tags   = 5.  
5 Related Work  
There are two main stream methods to build a 
social tag recommender system. They are 
collaboration-based method (Herlocker et al, 2004) 
and the content-based approach (Cantador et al, 
2010). 
FolkRank (Jaschke et at., 2008) and Matrix 
Factorization (Rendle et al, 2009) are 
representative collaboration-based methods for 
social tag recommendation. Suggestions of these 
techniques are based on the tagging history of the 
given resource and user, without considering the 
resource summaries. Thus most of these methods 
suffer from the cold-start problem, which means 
they cannot perform effective suggestions for 
resources that no one has annotated. 
To remedy the defect of cold-start problem, 
researchers proposed content-based methods 
exploiting the descriptive information on resources, 
such as summaries. Some of them considered 
social tag recommendation as a classification 
problem by regarding each tag as a category label. 
Various classifiers such as kNN (Fujimura et al, 
2007), SVM (Cao et al, 2009) have been discussed. 
But two issues exposed from these methods. 
? Classification-based methods are highly 
constrained in the quality of annotation, which 
are usually noisy. 
? The training and classification cost are often 
in proportion to the number of classification 
labels, so that these methods may not be 
efficient for real-world social tagging system, 
where thousands of unique tags may belong to 
a resource. 
With the widespread of latent topic models, 
researchers began to pay close attention on 
modeling tags using Latent Dirichlet Allocation 
(LDA) (Blei et al, 2003). Recent studies (Krestel 
et al, 2009; Si and Sun, 2009) assume that both 
tags and words in summary are generated from the 
same set of latent topics. However, most latent 
topic models have to pre-specify the number of 
topic before training. Even though we can use 
cross validation to determine the optimal number 
of topics (Blei et al, 2010), the solution is 
obviously computationally complicated. 
The state of the art research on social tagging 
recommendation (Z. Liu, X. Chen and M. Sun, 
2011) regarded social tagging recommendation 
problem as a task of selecting appropriate tags 
from a controlled tag vocabulary for the given 
resource and bridged the vocabulary gap between 
the summaries and tags using word alignment 
models in statistical machine translation. But they 
simply adopted the weighted sum of the score of 
52
candidate tags, named word trigger method 
(WTM), which cannot reflect the whole process of 
human annotation. 
 
6 Conclusion and Future Work 
In this paper, we propose a new approach for social 
tagging recommendation via analyzing and 
modeling human associative annotation behaviors. 
Experiments demonstrate that our approach is 
effective, robust and language-independent 
compared with the state of the art and baseline 
methods. 
The major contributions of our work are as 
follows. 
? The essential process of human tagging 
process is discovered as the guideline to help 
us build simulating models. 
? A suitable model is proposed to assist our 
social tagging recommender system to learn 
the semantic relationship between the 
meaningful words in summaries and 
corresponding tags.  
? Based on the semantic relationship between 
the meaningful words in the summaries and 
corresponding tags, a weighted Tag-digraph is 
constructed. Then a TagRank algorithm is 
proposed to re-organize and rank the tags. 
Our new approach is also suitable in the tasks 
of keyword extraction, query expansion et al 
where the human associative behavior exists. Thus, 
we list several open problems that we will explore 
in the future: 
? Our approach can be expanded from lexical 
level to sentence level to bring the associative 
ability into semantic-related sentences 
extraction.  
? We will explore the effects on other research 
areas, such as keyword extraction, query 
expansion, where human associative behavior 
exists as well. 
 
Acknowledgements 
The work was supported by the research projects 
of National Science Foundation of China (Grant 
No. 60873173) and National 863 High-Tech research 
projects (Grant No. 2007AA01Z173). The authors 
would like to thank Yi Luo for his insightful 
suggestions. 
References 
R. Baeza-Yates and B. Ribeiro-Neto. 2011. Modern 
information retrieval: the concepts and technology 
behind search, 2nd edition. ACM Press. 
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent 
dirichlet alocation. JMLR, 3:993-1022. 
S. Brin and L. Page. 1998. The anatomy of a large-scale 
hypertextual  web search engine. Computer networks 
and ISDN systems, 30 (1-7): 107-117. 
P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L. 
Mercer. 1993. The mathematics of statistical machine 
translation: Parameter estimation. Computational 
linguistics, 19(2):263-311. 
I. Cantador, A. Bellog?n, D. Vallet. 2010. Content-based 
recommendation in social tagging systems. In 
Proceedings of ACM RecSys, pages 237-240. 
H. Cao, M. Xie, L. Xue, C. Liu, F. Teng, and Y. Huang. 
2009. Social tag predication base on supervised 
ranking model. In Proceeding of ECML/PKDD 2009 
Discovery Challenge Workshop, pages 35-48. 
A. P. Dempster, N. M. Laird, D. B. Rubin, et al 1977. 
Maximum likelihood from incomplete data via the 
em algorithm. Journal of the Royal Statistical Society. 
Series B (Methodological), 39 (1): 1-38. 
D. Eck, P. Lamere, T. Bertin-Mahieux, and S. Green. 
2007. Automatic generation of social tags for music 
recommendation. In Proceedings of NIPS, pages 
385-392. 
S. Fujimura, KO Fujimura, and H. Okuda. 2007. 
Blogosonomy: Autotagging any text using bloggers? 
knowledge. In Proceedings of WI, pages 205-212. 
J. L. Herlocker, J. A. Konstan, L. G. Terveen, and J. T. 
Riedl. 2004. Evaluating collaborative filtering 
recommender systems. ACM Transactions on 
Information Systems, 22(1):5-53. 
R. Jaschke, L. Marinho, A. hotho, L. Schmidt-Thieme, 
and G. Stumme. 2008. Tag recommendations in 
social bookmarking systems. AI Communications, 
21(4):231-247. 
R. Krestel, P. Fankharser, and W. Nejdl. 2009. Latent 
dirichlet alocation for tag recommendation. In 
Proceedings of ACM RecSys, pages 61-68. 
Z. Liu, X. Chen, M. Sun. 2011. A simple word trigger 
method for social tag suggestion. In Proceedings of 
EMNLP, pages 1577-1588. 
C. D. Manning. P. Raghavan, and H. Schtze. 2008. 
Introduction to information retrieval. Cambridge 
University Press, NY, USA. 
53
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing 
order into texts. In Proceedings of EMNLP, pages 
404-411. Poster.  
R. Mirizzi, A. Ragone, T. Di Noia, and E. Di Sciascio. 
2010. Semantic tags generation and retrieval for 
online advertising. In Proceedings of CIKM, pages 
1089-1098. 
C. Musto, F. Narducci, M. de Gemmis, P. Lops, and G. 
Semeraro. 2009. STaR: a social tag recommender 
system. In Proceeding of ECML/PKDD 2009 
Discovery Challenge Workshop, pages 215-227. 
F. J. Och and H. Ney. 2003. A systematic comparison of 
various statistical alignment models. Computational 
linguistics, 29(1): 19-51. 
D. D. Palmer. 2010. Text preprocessing. Handbook of 
natural language processing 2nd edition, chapter 2. 
CRC Press. 
S. Rendle, L. Balby Marinho, A. Nanopoulos, and L. 
Schmidt-Thieme. 2009. Learning optimal ranking 
with ensor factorization for tag recommendation. In 
Proceedings of KDD, pages 727-726. 
F. Ricci, L. Rokach, B. Shapira and P. B. Kantor. 2011. 
Recommender Systems Handbook. Springer Press.  
X. Si and M. Sun. 2009. Tag-LDA for scalable real-time 
tag recommendation. Journal of Computational 
Information Systems, 6(1): 23-31. 
M. Sun. 2011.  Natural language processing based on 
naturally annotated web resources. Journal of 
Chinese Information Processing, 25(6): 26-32 
T. C. Zhou, H. Ma, M. R. Lyu, and I. King. 2010. 
UserRec: A user recommendation approach in social 
tagging systems. In Proceedings of AAAI, pages 
1486-1491. 
54
