Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 28?32,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Recipes for building voice search UIs for automotive
Martin Labsky, Ladislav Kunc, Tomas Macek, Jan Kleindienst, Jan Vystrcil
IBM Prague Research and Development Lab
V Parku 2294/4, 148 00 Prague 4
Czech Republic
{martin.labsky, ladislav kunc1, tomas macek,
jankle, jan vystrcil}@cz.ibm.com
Abstract
In this paper we describe a set of tech-
niques we found suitable for building
multi-modal search applications for au-
tomotive environments. As these ap-
plications often search across different
topical domains, such as maps, weather
or Wikipedia, we discuss the problem
of switching focus between different do-
mains. Also, we propose techniques use-
ful for minimizing the response time of the
search system in mobile environment. We
evaluate some of the proposed techniques
by means of usability tests with 10 novice
test subjects who drove a simulated lane
change test on a driving simulator. We re-
port results describing the induced driving
distraction and user acceptance.
1 Introduction
The task of designing mobile search user inter-
faces (UIs) that combine multiple application do-
mains (such as navigation, POI and web search)
is significantly harder than just placing all sin-
gle domain solutions adjacent to one another. We
propose and evaluate a set of UI techniques use-
ful for implementing such systems. The tech-
niques are exemplified using a prototype multi-
modal search assistant tailored for in-car use. The
prototype supports several application domains in-
cluding navigation and POI search, Wikipedia,
weather forecasts and car owner?s manual. Fi-
nally, we report usability evaluation results using
this prototype.
2 Related Work
Two examples of multi-modal search UIs for au-
tomotive are the Toyota Entune
1
and the Honda
1
http://www.toyota.com/entune/
Link
2
. Both infotainment systems integrate a
set of dedicated mobile applications including
a browser, navigation, music services, stocks,
weather or traffic information. Both use a tablet or
a smartphone to run the mobile applications which
brings the advantage of faster upgrades of the in-
car infotainment suite. Home screens of these sys-
tems consist of a matrix of square tiles that corre-
spond to individual applications.
The answers presented to the user should only
contain highly relevant information, e.g. present-
ing only points of interest that are near the cur-
rent location. This is called conversational maxim
of relevance (Paul, 1975). Many other lessons
learned by evaluating in-car infotainment systems
are discussed in (Green, 2013).
In recent years, personal assistant systems like
Siri (Aron, 2011), Google Now! (Google, 2013)
and the Dragon Mobile Assistant (Nuance, 2013)
started to penetrate the automotive environment.
Most of these applications are being enhanced
with driving modes to enable safer usage while
driving. Dragon Mobile Assistant can detect
whether the user is in a moving car and auto-
matically switches to ?Driver Mode? that relies
on speech recognition and text-to-speech feed-
back. Siri recently added spoken presentation
of incoming text messages and voice mail, and
it also allows to dictate responses. Besides the
speech-activated assistant functionality, Google
Now! tries to exploit various context variables
(e.g. location history, user?s calendar, search his-
tory). Context is used for pro-active reminders that
pop-up in the right time and place. Speech recog-
nition of Google Now! has an interesting feature
that tries to act upon incomplete/interim recogni-
tion results; sometimes the first answer is however
not the right one which is later detected and the
answer is replaced when results are refined.
2
http://owners.honda.com/hondalink/
nextgeneration
28
3 UI techniques to support search while
driving
Below we present selected techniques we found
useful while designing and testing prototype
search UIs for automotive.
3.1 Nearly stateless VUI
While driving and interacting with an application
UI, it often happens that the driver must interrupt
interaction with the system due to a sudden in-
crease of cognitive load associated with the pri-
mary task of driving. The interaction is either
postponed or even abandoned. The UI activity
may later be resumed but often the driver will
not remember the context where s/he left off. In
heavily state-based systems such as those based
on hierarchical menus, reconstruction of applica-
tion context in the driver?s mind may be costly and
associated with multiple glances at the display.
In order to minimize the need for memorizing
or reconstructing the application context, we ad-
vocate UIs that are as stateless as possible from
the user?s point of view. In the context of spoken
input, this means the UI should be able to process
all voice input regardless of its state.
This is important so that the driver does not need
to recall the application state before s/he utters a
request. For instance, being able to ask ?Where
can we get a pizza? only after changing screen to
?POI search? can be problematic as the driver (1)
needs to change screens, (2) needs to remember
what the current screen is, and (3) may need to
look at the display to check the screen state. All
of these issues may increase driver distraction (its
haptic, visual and mental components).
3.2 Self-sufficient auditory channel
According to the subjective results of usability
tests described in Section 6 and according to ear-
lier work on automotive dictation (Macek et al.,
2013), many drivers were observed to rely primar-
ily on the audio-out channel to convey information
from the UI while driving and they also preferred
it to looking at a display. A similar observation
was made also for test drivers who listened to and
navigated news articles and short stories (Kunc et
al., 2014).
Two recommendations could be abstracted from
the above user tests. First, the UI should produce
verbose audio output that fully describes what
happens with the system (in cases when the driver
controls the UI while driving). This includes spo-
ken output as well as earcons indicating important
micro-states of the system such as ?listening? or
?processing?. Second, the UI should enable the
user to easily replay what has been said by the
system, e.g. by pressing a button, to offset the se-
rial character of spoken output. These steps should
make it possible for selected applications to run in
a display-less mode while driving or at least mini-
mize the number of gazes at the display.
3.3 Distinguish domain transition types
By observing users accessing functions of mul-
tiple applications through a common UI, we ob-
served several characteristic transition types.
Hierarchical. The user navigates a menu tree,
often guided by GUI hints.
Within domain. Users often perform multiple
interactions within one application, such as per-
forming several Wikipedia queries, refining them
and browsing the retrieved results.
Application switching. Aware of the namings
of the applications supported by the system, users
often switch explicitly to a chosen domain before
uttering a domain-specific command.
Direct task invocation. Especially in case of UIs
having a unifying persona like Siri (Aron, 2011),
users do not view the system as a set of appli-
cations and instead directly request app-specific
functions, regardless of their past interaction.
Subdialog. The user requests functionality out
of the current application domain. The corre-
sponding application is invoked to handle the re-
quest and then the focus returns automatically to
the original domain. Examples include taking a
note or checking the weather forecast while in the
middle of another task.
Undo. A combined ?undo? or ?go back? fea-
ture accessible globally at a key press proved use-
ful during our usability testing to negate any un-
wanted actions accidentally triggered.
Figure 1 shows samples for the above transi-
tion types using an example multi-domain search
assistant further described in Section 4. Similar
lists of transition types ware described previously,
e.g. (Milward et al., 2006). Based on observing
human interactions with our prototype system, we
built a simple probabilistic model to control the
likelihood of the system taking each of the above
transition types, and used it to rescore the results
of the ASR and NLU systems.
29
Figure 1: Transitions in a multi-domain system.
3.4 Early and incremental feedback about
the application state
Mobile search UIs often depend both on local and
remote resources such as ASR and NLU services
and various data providers. In mobile environ-
ments, availability and response times of remote
services may vary significantly. Most mobile UIs
address this problem by responding with a beep
and displaying a ?processing? sign until the fi-
nal answer is rendered. We describe a UI tech-
nique that combines redundant local and remote
resources (ASR and NLU) to quickly come up
with a partial meaningful response that addresses
the user?s request. Chances are that the first re-
sponse based on partial understanding is wrong
and the following prompt must correct it.
Figure 2 shows a template definition for a sys-
tem prompt that starts playing once the system is
confident enough about the user?s intent being a
weather forecast question. The system provides
forecasts for the current location by default but
can switch to other locations if specified by the
user. Supposing the system is equipped with real-
time ASR and NLU that quickly determine the
high-level intent of the user, such as ?weather fore-
cast?, the initial part of the prompt can start play-
ing almost immediately after the user has stopped
speaking. While a prefix of this prompt is play-
ing, more advanced ASR and NLU models de-
liver a finer-grained and more precise interpreta-
tion of the input, including any slot-value pairs
like ?location=London?. Once this final interpre-
tation is known, the playback can be directed via
the shortest path to the identified variable prompt
segments like <location>. Further, the selec-
tion of prompt prefix to be played can be guided
by a current estimate of service delays to mini-
mize chances of potential pauses before speaking
prompt segments whose values are not yet known.
Figure 2: Sample incremental prompt graph. Seg-
ments are annotated with durations in round brack-
ets and min/max times before an unknown slot
value has to be spoken (ms).
4 Voice search assistant prototype
In this section we briefly present a voice search in-
terface that was developed by incrementaly imple-
menting the four UI techniques presented above.
While interim versions of this system were only
evaluated subjectively, formal evaluation results
are presented for the final version in Section 6.
The voice search assistant covers six applica-
tion domains shown in Figure 3. Navigation ser-
vices include spoken route guidance together with
unified destination entry by voice (addresses and
POIs). Some POIs are accompanied by user re-
views that can be read out as part of POI details.
Figure 3: Prototype home screen (apps as tiles).
Further, the user can search various knowledge
sources like Wikipedia, Wolfram Alpha and the
web. The retrieved results are pre-processed and
the first one is played back to the user with the
possibility of navigating the result list.
To simulate asynchronous events, the system
reads out Skype text messages. The driver can also
create location and time based reminders that pop
up during the journey.
Finally, the system supports full-text search
over the car owner?s manual. Relevant text pas-
sages are read out and displayed based on a prob-
lem description or question uttered by the driver.
30
5 Usability testing setup and procedure
A low-fidelity driving simulator setup similar to
the one described in (Curin et al., 2011) was
used to conduct lane change tests using (Mattes,
2003). Tests were conducted with 10 novice sub-
jects and took approximately 1 hour and 20 min-
utes per participant. At the beginning and at the
end of the test, subjects filled in pre-test and post-
test questionnaires. Before the actual test, each
participant practised both driving and using the
prototype for up to 20 minutes. The evaluated
test consisted of four tasks: an initial undistracted
drive (used to adapt a custom LCT ideal path for
each participant), two distracted driving trips in
counter-balanced order, and a final undistracted
drive (used for evaluation). Each of the four drives
was performed at constant speed of 60km/h and
took about 3.5 minutes. During the distracted
driving tasks, the users were instructed verbally
to perform several search tasks using the proto-
type. During task 1, subjects had to set destina-
tion to ?office?, then find a pharmacy along the
route, check the weather forecast and take a note
about the forecast conditions. Task 2 only dif-
fered slightly by having a different destination and
POI, and by the user searching Wikipedia instead
of asking about weather.
6 Usability testing results
Objective distraction was measured using mean
deviation (MDev) and standard deviation
(SDLP ) of the vehicle?s lateral position (Mattes,
2003). Two versions of both statistics were
obtained: overall (computed over the whole trip)
and using lane-keeping segments only. The graph
in Figure 4 shows averaged results for the final
undistracted drive and for the first and second
distracted driving tasks (reflecting the order of the
tasks, not their types). We observe that using the
search UI led to significant distraction during lane
change segments but not during lane keeping.
Also, the distraction results for the first trip show
higher variance which we attribute to the users
still adapting to the driving simulator and to
using the UI. The observed distraction levels are
comparable to our earlier results obtained for a
text dictation UI when used with a GUI display
(Curin et al., 2011).
Several observations came out of the subjec-
tive feedback collected using forms. The users re-
ported extensive use of the auditory channel (both
00,10,20,30,40,50,60,70,80,91
 
 
 
 
 
Und
istra
cted
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Firs
t tas
k    
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Sec
ond
 
task
[m]
Ove
rall 
MD
ev
Ove
rall 
SDL
P
Lan
e ke
epin
g M
Dev
Lan
e ke
epin
g SD
LP
Figure 4: Driving distraction while using a multi-
modal search UI.
in and out) only with occasional glimpses at the
scre n (w however observed that objectively they
looked at the display more often than they reported
subjectively). Users also missed some informa-
tion in the voice output channel such as audio indi-
cation of route calculation progress (which could
take several seconds). Reading any text from the
screen was found difficult, and users requested that
playback be improved; see related follow-up study
(Kunc et al., 2014). Interestingly, multiple partic-
ipants requested voice commands that would du-
plicate buttons like ?next? and ?previous?, even in
cases where speech would be less efficient. This
may show a tendency to stick with a single modal-
ity as described by (Suhm et al., 2001). Addi-
tionally, the users requested better synchronization
of navigation announcements like ?take exit 4 in
200 metres? with the output of other applications.
The baseline behaviour utilized in the test was
that high-priority navigation prompts interrupted
the output of other applications. Navigation, POI
search, simple note-taking and constrained search
domains like weather and Wikipedia were found
most useful (in this order). Open web search
and browsing an original car owner?s manual were
considered too distracting to use while driving.
7 Conclusion
We described several recipes for building spoken
search applications for automotive and exempli-
fied them on a prototype search UI. Early us-
ability testing results for the prototype were pre-
sented. Our future work focuses on improving the
introduced techniques and exploring alternative UI
paradigms (Macek et al., 2013).
Acknowledgement
The presented work is part of an IBM and Nuance
joint research project.
31
References
Jacob Aron. 2011. How innovative is apple?s new
voice assistant, siri? New Scientist, 212(2836):24.
J. Curin, M. Labsky, T. Macek, J. Kleindienst,
H. Young, A. Thyme-Gobbel, H. Quast, and
L. Koenig. 2011. Dictating and editing short texts
while driving: distraction and task completion. In
Proceedings of the 3rd International Conference on
Automotive User Interfaces and Interactive Vehicu-
lar Applications.
Google. 2013. Google now assistant. Available at
http://www.google.com/landing/now/.
Paul A Green. 2013. Development and evaluation
of automotive speech interfaces: useful information
from the human factors and the related literature. In-
ternational Journal of Vehicular Technology, 2013.
L. Kunc, M. Labsky, T. Macek, J. Vystrcil, J. Klein-
dienst, T. Kasparova, D. Luksch, and Z. Medenica.
2014. Long text reading in a car. In Proceedings
of the 16th International Conference on Human-
Computer Interaction Conference (HCII).
Tom?a?s Macek, Tereza Ka?sparov?a, Jan Kleindienst,
Ladislav Kunc, Martin Labsk?y, and Jan Vystr?cil.
2013. Mostly passive information delivery in a
car. In Proceedings of the 5th International Confer-
ence on Automotive User Interfaces and Interactive
Vehicular Applications, AutomotiveUI ?13, pages
250?253, New York, NY, USA. ACM.
Stefan Mattes. 2003. The lane-change-task as a tool
for driver distraction evaluation. In Proceedings of
the Annual Spring Conference of the GFA/ISOES,
volume 2003.
David Milward, Gabriel Amores, Nate Blaylock,
Staffan Larsson, Peter Ljunglof, Pilar Manchon, and
Guillermo Perez. 2006. D2.2: Dynamic multimodal
interface reconfiguration. In Talk and Look: Tools
for Ambient Linguistic Knowledge IST-507802 De-
liverable D2.2.
Nuance. 2013. Dragon mobile assistant. Available at
http://www.dragonmobileapps.com.
Grice H Paul. 1975. Logic and conversation. Syntax
and semantics, 3:41?58.
Bernhard Suhm, Brad Myers, and Alex Waibel. 2001.
Multimodal error correction for speech user inter-
faces. ACM Transactions on Computer-Human In-
teraction (TOCHI), 8(1):60?98.
32
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 53?57,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Mostly Passive Information Delivery ? a Prototype
J. Vystr
?
cil, T. Macek, D. Luksch, M. Labsk?y, L. Kunc, J. Kleindienst, T. Ka
?
sparov
?
a
IBM Prague Research and Development Lab
V Parku 2294/4, 148 00 Prague 4
Czech Republic
{jan vystrcil, tomas macek, david.luksch, martin labsky,
ladislav kunc1, jankle, tereza.kasparova}@cz.ibm.com
Abstract
In this paper we introduce a new UI
paradigm that mimics radio broadcast
along with a prototype called Radio One.
The approach aims to present useful infor-
mation from multiple domains to mobile
users (e.g. drivers on the go or cell phone
users). The information is served in an en-
tertaining manner in a mostly passive style
? without the user having to ask for it? as
in real radio broadcast. The content is gen-
erated on the fly by a machine and inte-
grates a mix of personal (calendar, emails)
and publicly available but customized in-
formation (news, weather, POIs). Most of
the spoken audio output is machine syn-
thesized. The implemented prototype per-
mits passive listening as well as interaction
using voice commands or buttons. Initial
feedback gathered while testing the proto-
type while driving indicates good accep-
tance of the system and relatively low dis-
traction levels.
1 Introduction
The main purpose of this paper is to describe a
prototype of the Radio One concept. Radio One
presents music, news, emails, relevant POI and
other information to the user in a mostly passive
way, similarly to conventional radios. Users can
interract with the system as well using voice com-
mands or buttons. The concept was refined and
initially tested with prerecorded audio-visual sce-
narios using the Wizard-of-Oz (WOZ) technique
(Macek et al., 2013).
Here we describe the early prototype implemen-
tation of the system and summarize initial feed-
back collected during informal testing.
2 Related Work
Applications that produce customized audio
streams can be found in many online music deliv-
ery services including Spotify, Pandora, or iTunes.
While the above services often focus on music
only, other providers (BBC, CNN) publish their
spoken content in the form of podcasts. Spoken
audio used for podcasts is often recorded by pro-
fessional speakers as opposed to the concept pre-
sented here. The Aha radio (Aha, 2014) provides
various thematic streams of information including
music, news, social network updates or Points of
Interest (POI). Content can be selected manually
by switching between channels. Similar strategies
are utilized by Stitcher (Stitcher, 2014) and other
services. The concept presented here attempts in-
sted to preselect the content automatically and on
the fly while preserving the option to request the
content explicitely.
Many in-car infotainment systems adopted the
use of voice control and utilize information di-
rectly from on-line services; e.g. (BMW, 2014)
and (Ford, 2014). All of the abovementioned ap-
plications use mobile data connection to deliver
audio stream (as opposed to text) to the user. This
can lead to large data downloads and potentially to
high bills from mobile network providers.
3 Radio One Concept
Radio One mimics radio broadcast by generating
infotainment content on the fly. Unlike real radios,
Radio One customizes its content to the particular
listener and should even adapt automatically while
the user interacts with it. In addition to the content
typically played by radios, the synthetic content
also includes private information like calendar or
emails. Most of the spoken output is produced by
a text-to-speech system with the exception of pod-
casts.
The presented information stream is sparse with
53
the intervals between spoken segments filled with
music and moderator small-talk. The content
structure is configurable and can be adapted both
automatically, based on observing habits of the
user, or via explicit voice commands or buttons.
The main benefit of dynamically generated con-
tent is that the system can easily include dynamic
personal content and that the infotainment stream
can be efficiently controlled by the user and in-
fluenced by the environment (such as expected
duration of the drive or current road conditions).
From a technical perspective, the connection re-
quirements are much smaller compared to audio
transfers, as Radio One mostly downloads textual
feeds only. Downloading redundant information
can be avoided by knowing what has already been
presented to the particular user. Further, the user
can navigate in the broadcast, either to specific
topics by using voice commands, or just backward
and forward by using buttons. This option should
reduce potential stress related to a driver concen-
trating on a broadcasted topic knowing s/he would
be unable to replay. The radio presents informa-
tion from the covered domains continuously. The
stream of presented information also serves as a
natural way of teaching the user about the sup-
ported domains. By hearing that news are read
as part of the radio stream, the user finds out that
news is one category that can be requested by
voice commands.
4 System Description
Although previous WOZ tests (Macek et al.,
2013) were sufficient to collect the initial user
feedback, their flexibility and fidelity was limited.
The prototype described in this paper is intended
for testing of concepts and for conducting realistic
usability tests in a car. The implemented prototype
is a fully functioning system, although still with a
limited feature set.
4.1 Architecture
The overall architecture of the system is depicted
in Figure 1. The system collects inputs both from
manual controls (steering wheel buttons, rotary
knob) and from ASR (voice commands). Multi-
ple on-line and off-line data sources provide con-
tent. While driving, GPS information about the
car position is used together with an optional cal-
culated route and POI data to plan overall broad-
casting. The core of the Radio One system (see
Figure 1: Radio One big picture.
Figure 2: Radio One architecture.
Figure 2) is the scheduler. The scheduler is re-
sponsible for planning both the type of content
and the time of its presentation. The content as-
sociated with higher expected cognitive load (e.g.
emails or calendar) can be planned for segments
of the journey that have low driving difficulty (e.g.
straight highway). The overall architecture aims
to be highly configurable and context-aware to be
able to produce heterogeneous content based on
differing user preferences and changing state of
the environment.
4.2 Controls
Multiple button configurations are possible, rang-
ing from a ?speech button-only? setup to several
buttons used to provide quick access to frequently
used functions. For in-car setups, the availabil-
ity of buttons is often limited. A configuration of
3 buttons in a row (in addition to speech button)
can be used to let the user navigate back and forth
using the two outer buttons and request more de-
tails or pause/resume the broadcast with a central
button. Both ?per-item? (e.g. single email, song
or news title) and ?per-bundle? navigation (?bun-
dle? being a coherent group of affiliated items, e.g.
emails) can be supported by short and long presses
of the navigation buttons. Other functions would
54
typically be available through voice commands
only, or also through a touch interface where avail-
able (e.g. on a cell phone or in a parked car).
Alternatively to the buttons on the steering
wheel, a rotary knob can be placed on the side of
the driver?s seat (depicted on the left of Figure 3).
Usually, a single knob press initiates speech input,
while turning the knob navigates back and forth in
items. Per-bundle navigation can be triggered ei-
ther by using greater turns or by turning the knob
while pressed.
The voice control subsystem is hybrid with
speech recognition and understanding being done
both remotely and locally. This way, functions
are available even when off-line while benefit-
ing from improved accuracy and coverage of the
server models when on-line. Free-form commands
are understood (e.g. ?email? or ?would you read
my email please?).
4.3 Content and Presentation
Two modes of operation are implemented. The
off-line mode permits testing with locally saved
data or data specifically tailored for various exper-
iments. The on-line mode collects data (e.g. email,
calendar, news) periodically from the network and
presents it at appropriate times.
News are collected periodically from config-
urable network sources and grouped by topic. Two
forms of news presentation are implemented. A
shorter version is used during news summaries.
A longer version can be requested by an explicit
voice request like ?tell me more? or by pressing a
?details? button.
Emails undergo elementary pre-processing to
improve their suitability for being read out loud.
Emails longer than a configured threshold are
shortened at the end of the sentence. Email his-
tories are also skipped. The user can request a full
version of the email using a voice command like
?read the whole message?.
Moderator commentaries are tailored to the
content they accompany. We use a set of hand-
crafted prompt templates for natural language gen-
eration. Prompt templates are grouped according
to the context that triggers them into pools of al-
ternatives, from which prompts are selected ran-
domly while avoiding repetitions. Moderators can
announce upcoming content or refer to content
that just finished playing. Prompt templates often
contain variables referring to various properties of
the neighbouring content (e.g. name of the preced-
ing song or topic of the upcoming news).
Information is presented as a story, typically
with a brief summary-of-the-broadcast at the be-
ginning. This order can be interrupted by sudden
events (e.g. emails arriving, hot breaking news,
POI announcements) with proper moderator com-
ments to indicate what is happening. The infor-
mation is grouped together in bundles of the same
type (e.g. email summaries are not mixed with cal-
endar or news items). Typical in-car presentation
order starts with music to allow the listener to get
concentrated on driving. Then a summary is pro-
vided followed by blocks of music and informa-
tion bundles.
In contrast to our earlier WOZ study, the cur-
rent version of the prototype does not present any
visual information as we focus on the driving sce-
nario. The previous WOZ study indicated that this
information was distracting to the driver and not
much valued by the participants.
Figure 3: Alternative user interface controls
4.4 Implementation
The prototype is implemented in Java. It uses
a local text-to-speech system (TTS). We use the
Nuance Vocalizer premium voices to provide the
best available TTS quality. Current implementa-
tion is primarily in English (moderators and their
comments) although playback of content in other
languages (currently Czech) is supported as well.
Language detection is done automatically (Cy-
bozu Labs, 2014). The system was tested both
on a PC (Windows 7) and on tablets and phones
(Android, Windows 8). Emails are currently re-
trieved using the IMAP protocol so various email
providers can be used. News are currently down-
loaded from the Feedzilla (Feedzilla, 2014) REST
API and from other RSS feeds.
Calendar events are retrieved from the user?s
Google Calendar account. The radio automati-
cally announces individual upcoming events and
55
also plays summaries about the remaining events
of the day (also can be requested by voice).
Like real radios, we use characteristic earcons
and jingles to introduce particular types of infor-
mation (e.g. email, news or calendar) and other
sounds to separate individual information items
from each other (e.g. earcons between emails or
news titles).
For testing purposes we use infra-red remote
control buttons (see right hand part of Figure 3)
mounted to the steering wheel, with key events re-
ceived by a special purpose hardware and passed
to Radio One via Bluetooth.
We use either an AUX cable or a radio FM
transmitter to integrate with the car?s audio sys-
tem. The current prototype implements music
playback, presents news, email, weather reports
and calendar summaries. Initial work was done
on presenting POIs near the current location. An
arbitrary list of MP3 files can be used as a source
of music. Ideally, user?s own collection of music
is used during the tests. ID3 tags of music files are
utilized in the process of generating voice prompts
spoken by moderators as part of their small talk
(e.g. ?This was a song by the Beatles?).
5 Usability testing
Initially, a WOZ experiment was conducted with-
out having the system implemented. Test subjects
drove a low-fidelity driving simulator while lis-
tening to a radio stream broadcasted by the wiz-
ard, who played pre-recorded audio-visual snip-
pets trying to satisfy user?s requests. We described
results of this experiment previously in (Macek
et al., 2013). The main feedback from this ex-
periment was that the users perceived the quality
of synthesized speech sufficiently. The visual in-
formation shown by the wizard contained mostly
static pictures or short texts in large fonts. Most
of the users did not find the screen useful in this
setup. Therefore the current radio prototype is
screen-less. Two groups of users could be iden-
tified. The first one used the system in the same
way as a standard radio, with minimal interaction.
The other group preferred to be ?in control? and
used both buttons and voice commands to ask for
specific content.
Multiple informal tests were conducted by 4 test
drivers in real traffic. More extensive testing is still
in preparation. The feedback collected so far was
positive, indicating that the TTS quality was suf-
ficient. Even with a small number of test drivers
it became apparent that the roles of customization
and automatic adaptation to preferences of a spe-
cific user will be crucial.
Information-heavy content like certain kinds of
news was sometimes considered difficult to lis-
ten to while driving, which was in part due to
all of the test drivers being non-native speakers
of English. Adding jingles to separate the pre-
sented news items from one another improved the
perception of the system significantly. The news
feeds used by the prototype were originally not
intended for audio presentation, which does im-
pact their understandability, but the effect does not
seem to be major. Lighter content like weather
forecasts and calendar announcements were con-
sidered easy to understand.
The test drivers considered it important to be
able to use their personal data (news, email, mu-
sic). This motivated the inclusion of information
sources in languages other than English and the
addition of automatic language identification so as
to select proper TTS voices. The fact that multi-
ple languages were present in the broadcast was
not perceived adversely. One shortcoming of the
tested system was still a low variability of moder-
ators? comments.
6 Conclusion
We presented a work-in-progress demonstration
prototype of a novel method for presenting in-
formation to users on-the-go. A preceding WOZ
study indicated promising user acceptance which
was also confirmed using the described prototype.
When comparing with existing systems, the sys-
tem presented here has much lower requirements
on communication bandwidth, requires less hu-
man work for content authoring and permits a
higher level of personalization. Amount of inter-
activity depends very much on user preferences.
In future work we would like to pay attention
to evaluation of user feedback on more extensive
usability tests. It will be interesting to see to what
extent the user will opt for active interaction with
the system and for the particular interaction tech-
niques.
Acknowledgments
The presented work is part of an IBM and Nuance
joint research project.
56
References
Harman International Aha. 2014. Aha radio web-
site. Retrieved from http://www.aharadio.
com/.
BMW. 2014. Bmw connecteddrive ser-
vices. Retrieved from http://www.bmw.
com/com/en/insights/technology/
connecteddrive/2013/services_apps/
bmw_connecteddrive_services.html.
Inc. Cybozu Labs. 2014. Language detection li-
brary for java. Retrieved from https://code.
google.com/p/language-detection/.
Feedzilla. 2014. Feedzilla - news feed directory. Re-
trieved from http://www.feedzilla.com/.
Ford. 2014. Sync with myford touch. Retrieved
from http://www.ford.com/technology/
sync/.
Tom?a?s Macek, Tereza Ka?sparov?a, Jan Kleindienst,
Ladislav Kunc, Martin Labsk?y, and Jan Vystr?cil.
2013. Mostly passive information delivery in a
car. In Proceedings of the 5th International Confer-
ence on Automotive User Interfaces and Interactive
Vehicular Applications, AutomotiveUI ?13, pages
250?253, New York, NY, USA. ACM.
Inc. Stitcher. 2014. Stitcher website. Retrieved from
http://www.stitcher.com/.
57
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 58?62,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Navigation Dialog of Blind People: Recovery from Getting Lost
Jan Vystrcil, Ivo Maly, Jan Balata and Zdenek Mikovec
Czech Technical University in Prague, Faculty Of Electrical Engineering
Karlovo nam. 13, 121 35 Praha 2
Czech Republic
{vystrjan, malyi1, balatjan, xmikovec}@fel.cvut.cz
Abstract
Navigation of blind people is different
from the navigation of sighted people and
there is also difference when the blind per-
son is recovering from getting lost. In this
paper we focus on qualitative analysis of
dialogs between lost blind person and nav-
igator, which is done through the mobile
phone. The research was done in two out-
door and one indoor location. The analy-
sis revealed several areas where the dialog
model must focus on detailed information,
like evaluation of instructions provided by
blind person and his/her ability to reliably
locate navigation points.
1 Introduction
When blind people travel independently, it may
happen that they get lost. This happens when they
can not find any useful navigation points. Use of
existing GPS based navigation systems is of no use
as the maps do not provide appropriate navigation
points and the GPS localization is imprecise (tens
of meters, where the lost blind person needs preci-
sion at highest in meters). In such situation blind
people typically use one of two following meth-
ods to recover. First they can ask people in their
surrounding for help. Second they can call friend
or dedicated assistance center. The first method
is currently more favorable for blind people, but
they have experience with both methods. In each
method the dialog has different structure due to the
different context information available to the help-
ing person (called navigator) and lost blind person.
In our research we focus on the second method,
navigation through mobile phone call. Balata et
al. (2013b) showed that such method is usable and
navigator can successfully guide blind person in
outdoor environment. This is because the blind
person is able to efficiently describe his/her po-
sition. Balata et al. (2013a) found that there is
quite good coverage of locations that are very well
known to blind persons and that they should be
able to navigate other lost blind person there.
These findings show that building some kind of
assistance center where blind people can help each
other is a promising idea. Our intention is to ex-
tend such a center in a way that the helping per-
son will be replaced by natural language based di-
alog system. According to Pittermann (2005) this
dialog system belongs to the category ?Dialog as
purposeful activity? with overlapping to the cate-
gory ?Dialogue as collaborative activity?. The key
questions we focus on are:
? How the selection of an appropriate form of
language depends on aspects of the environ-
ment?
? What is the structure of the dialog with re-
spect to the environment?
In the initial step of this work-in-progress re-
search, we want to analyze the communication be-
tween lost blind person and the navigator in or-
der to analyze the dialog structure and make ini-
tial observation about the context information in-
terchange and verification dialog. Such dialog is
very important for navigator to find out, where ex-
actly the blind person get lost. With the knowl-
edge of the way of communication between lost
blind person and navigator we will be able in the
future replace the navigator with a navigation sys-
tem based on natural language understanding.
In order to gather and analyze initial data we
ran an experiment in which the blind person got
lost and was asked to call the assistance center
which mediated connection to suitable navigator.
Together, they tried to find the actual position of
blind person and they tried to navigate him/her to
end of the track.
58
2 Related Work
Many current dialog systems are based on statisti-
cal approach when analyzing the semantics of spo-
ken dialog as presented by Jurcicek (2007). Us-
ing belief state tracking provides better results for
cases of noisy input. Ma et al. (2012) introduced
system that is combining geographical knowledge
of landmarks with dialog system itself and work
with probabilities of particular locations.
Recovering from lost scenario can be also com-
pared to robot localization problem as presented
by Thrun (1998) and Thrun et al. (2001), more
exactly to kidnapped robot problem, where robot
with knowledge of its position is moved to differ-
ent location without providing this information to
the robot. This scenario is testing the ability of
robot to recover from being lost while expecting
to be on another place. These methods are based
on probabilistic algorithms, working with proba-
bilities of measurement while being on a certain
place.
However we do not expect blind person to wear
any precise sensors for distance measurements and
localization, we can benefit from his/her senses
(touch, hearing and olfaction) that can provide set
of reliable observations.
3 Experiment Description
3.1 Collected Data
We set up an experiment in order to collect and an-
alyze initial data about the dialog structure of lost
blind person and sighted navigator person. During
the experiment we recorded the course of the test
with two cameras, one was on the blind person?s
shoulder and one was used for 3rd person view of
the scene in order to show context (environment)
of the test. Moreover, we recorded the blind per-
son?s position using GPS coordinates in outdoor
and blind person?s interaction with mobile naviga-
tion application. Camera recordings and GPS logs
were used only for post-test evaluation. The di-
alog between the blind person and navigator was
recorded and annotated.
3.2 Participants
For the experiment, 13 blind people, 8 female
and 5 male, were invited by e-mail and following
snowball effect. All the participants had blindness
of category 4 and 5 ? according to ICD-10 WHO
classification.
3.3 Procedure
In the experiment, we focused on three types of
location, two outdoor and one indoor: city cen-
ter streets (track A), open city park (track B) and
university building (track C). We selected these
three types of location in order to analyze possi-
ble differences in the dialog structure or types of
provided information.
The script of the experiment was similar for
each type of location. The participant was given a
mobile phone with mobile navigation application
for blind called NaviTerier Vystrcil et al. (2012).
NaviTerier provides TTS synthesized description
of the predefined track divided into segments. For
each segment the description of the environment
and navigation to the next segment was tailored
with respect to the way how blind people navi-
gate. Borders of segments are selected on places
that could be easily recognizable by blind people
(e.g. corner of building, doors, etc.). Each partici-
pant had a goal to go from start point to the end of
the track using the mobile navigation application
for blind. In order to put the participant into the
?recovery from lost? situation, the navigation in-
structions were intentionally modified to represent
a mistake in the track description (a realistic mis-
take), which caused that the participant get lost.
When the participant realized that he/she is lost,
a navigator from assistance center was called and
they tried to find out the location of blind person
and navigate him/her to the end of the track.
Navigator was seated in an office without visual
contact to lost blind person. He knew all three
routes very well. The only source of information
about the lost blind person was dialog done by a
phone call.
3.3.1 Track A - City Center Streets
In track A the participant was asked to navigate
to the Vaclavska passage, see Figure 1. The nav-
igation instruction were changed so that the two
streets (Trojanova and Jenstejnska) were switched
so that the participant get lost at the T crossing of
Jenstejnska and Vaclavska street. The navigation
using the mobile navigation application for blind
in this type of environment was easy for partici-
pants and they all get lost at the desired location.
The navigator and participant had several nav-
igation points there to get oriented. First of all,
there was a nearby busy street Resslova, which can
be heard. Next there was a closed gateway with
59
metal gate, which is quite unique for this location.
There were also waste bins (containers), phone
booth and entrance to the underground garage.
3.3.2 Track B - Open City Park
In track B the participant was asked to navigate
through the park to the restaurant, see Figure 1.
The navigation instruction were changed so that
the two junctions were skipped and the participant
ended near the middle of the park, where fountain
is located.
In this type of location, there were also not
many unique navigation points. The most us-
able were two perpendicular streets with trams, the
fountain in the middle of the park and two unique
stairs. There were also multiple benches and grass
plots.
3.3.3 Track C - Building
In track C the participant was asked to navigate
through the building from the entrance to the yard,
see Figure 1. The navigation instructions were
changed so that instead of taking stairs down, the
participant was asked to take stairs up and he/she
got lost in the small corridor, where the last doors
should be located but they were not there. The
navigation using the NaviTerier application in this
type of environment was easy for the participants
and they all get lost at the desired location.
At the place, where the participant got lost,
there were several navigation points. First point
was showcase from metal and glass at the expected
location of doors to the yard. Then there was
wooden door secured with metal bars and wooden
stairs going up and down.
4 Results and Discussion
In the track A and track C, the participants got lost
at location very well known to the navigator, thus
the identification of lost blind person location was
mostly fast and easy. In the track B, participants
got sometimes lost at locations unfamiliar for the
navigator due to the ambiguity of the environment
and thus the location identification process was
complicated.
The dialog structure of the communication be-
tween lost blind person and the navigator corre-
sponds to the model introduced by Balata et al.
(2013b). At the beginning the blind person de-
scribes his/her location, track instructions and the
problem description, i.e. what is the difference be-
tween instructions of navigation application and
Figure 1: Visualization of individual tracks A, B
and C used in the experiment. The intended path
is shown by solid line. Path shown by dashed line
shows the real path leading to the point, where the
participant got lost ? yellow exclamation mark ?
and from where the participant was navigated back
to the path.
reality. After the beginning the dialog continues
by iterative searching of unique navigation points
that may help the navigator to find the position and
orientation of the lost blind person, until he/she
gets to the location from which he/she can con-
tinue with the track. The dialog system should
take into account following findings about the di-
alog structure.
When the blind person get lost, he/she uses in-
formation, provided by navigation application for
sections that seemed to him/her correct and cor-
responding with reality, for description of his/her
current position, e.g. ?I am in the Vaclavska
street.? The dialog system should take into ac-
count uncertainty of information provided by lost
blind person, possibility that the blind person got
lost much earlier and the navigation instructions
for next several segments were corresponding with
the reality by coincidence.
The fact that the blind person gets lost is lit-
tle bit stressy for him/her. Therefor he/she may
provide illogical answers to some questions, e.g.
60
Q: ?Could you provide me with the description of
your current position?? and A: ?I would rather go
to the start of the track and describe the track from
the beginning.?
Description of current position of blind person
is very different from the description of sighted
person. The dialog system should take into ac-
count that the blind person may not find partic-
ular navigation point, but it does not mean that
the navigation point is not there. Moreover, some
navigation points may be difficult or impossible to
find by blind person. Similar issue is identifica-
tion of particular navigation points. The blind per-
son may have difficulties to distinguish between
bend, turning, intersection and end of pavement.
This may be misleading to dialog system. On the
other hand, when the blind person confirms that
particular navigation point was found, the system
should check, if it is really the one, e.g. when
doors are found, the system should check the ma-
terial or type of the doors.
Blind persons use other senses than sight to scan
the current position and navigation points. Even
though the senses are more sensitive, the provided
information may not be accurate, e.g. the blind
person is reporting inclining pavement and in real-
ity there is flat pavement.
It seems that the preferred sense is connected
with the type of environment. In the track B with
low density of navigation points which are am-
biguous the blind persons preferred hearing.
Some navigation points are not permanent and
may by varying. E.g. when there are two streets,
one near (not busy) and one far (busy) and the
blind person is asked to locate busy street, this
information will depend on the current traffic on
both streets. Together with the fact that term busy
is subjective, the blind person may locate wrong
street.
Some blind persons (the ones with high con-
fidence of independency and orientation skills)
tended to get oriented independently to the dialog
with navigator. That means they provided the nav-
igator with required information, but at the same
time they were moving and they were disrupting
the navigators mental model of the blind person?s
location.
There is not a standardized vocabulary how
blind persons describe objects. Therefore they
tend to use wide range of words and also
metaphoric descriptions to describe the same ob-
ject.
5 Conclusion and Future Work
In this paper, we did initial analysis of dialogs be-
tween blind person, who got lost when walking on
a track with the instructions from mobile naviga-
tion application, and navigator, who is trying to
help him get oriented. The research was done in
three different locations, in city center streets, in
open city park and in building. The dialog be-
tween blind person and navigator was recorded
and qualitatively analyzed in order to reveal dialog
features which can be used for improvement of the
navigation itself and later it can help to replace the
human navigator with automated system.
Initial analysis showed that the type of location
may have impact on strategy, how the blind per-
son explore his/her surroundings and how he/she
tries to get oriented. In city center streets (track
A) and in building (track C) the blind persons
were able to explore their surroundings and they
allowed the navigator to find out, where they prob-
ably are. In open city park (track B) the blind
persons had problem to find navigation points and
sometimes they were trying to get oriented inde-
pendently, which led to the difficulties for naviga-
tor to find their position. In many cases, the blind
persons were using the information from mobile
navigation application until the point where they
got lost. Unfortunately, such information may al-
ready be misleading. As a general finding, the dia-
log should focus also on verification of navigation
points, which may not be permanent (e.g. finding
busy street, when there are more streets around) or
which may be not identified in not enough detail.
In future, we would like to focus on individual
aspects found in qualitative analysis and design
strategies into the dialog model between lost blind
person and navigator and evaluate it quantitatively.
Acknowledgments
This research has been supported by the project
Design of special user interfaces funded by
grant no. SGS13/213/OHK3/3T/13 (FIS 161 ?
832130C000).
References
J. Balata, J. Franc, Z. Mikovec, and P. Slavik. 2013a.
Collaborative navigation of visually impaired. Jour-
nal on Multimodal User Interfaces, pages 1?11.
61
J. Balata, Z. Mikovec, and J. Novacek. 2013b. Field
study: How Blind People Communicate While Re-
covering From Loss of Orientation. In Cognitive
Infocommunications (CogInfoCom), 2013 IEEE 4th
International Conference on, pages 313?317, Bu-
dapest. IEEE Hungary Section, University Obuda.
Filip Jurcicek. 2007. Statistical approach to the se-
mantic analysis of spoken dialogues.
Yi Ma, Antoine Raux, Deepak Ramachandran, and
Rakesh Gupta. 2012. Landmark-based location be-
lief tracking in a spoken dialog system. In Proceed-
ings of the 13th Annual Meeting of the Special In-
terest Group on Discourse and Dialogue, SIGDIAL
?12, pages 169?178, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Johannes Pittermann. 2005. Spoken dialogue tech-
nology: Toward the conversational user interface by
michael f. mctear. Comput. Linguist., 31(3):403?
406, September.
Sebastian Thrun, Dieter Fox, Wolfram Burgard, and
Frank Dellaert. 2001. Robust monte carlo local-
ization for mobile robots. Artificial Intelligence,
128(12):99 ? 141.
Sebastian Thrun. 1998. Bayesian landmark learning
for mobile robot localization. Machine Learning,
33(1):41?76.
J. Vystrcil, Z. Mikovec, and P. Slavik. 2012. Naviterier
? indoor navigation system for visually impaired. In
SMART HOMES 2012, pages 25?28. Czech Techni-
cal University.
62
