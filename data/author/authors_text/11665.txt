Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 107?114,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Interaction Grammar for the Persian Language:  
Noun and Adjectival Phrases 
 
Masood Ghayoomi Bruno Guillaume 
Nancy2 University LORIA - INRIA, BP 239 
54506 Vandoeuvre, Nancy cedex, France 54506 Vandoeuvre, Nancy cedex, France 
masood29@gmail.com Bruno.Guillaume@loria.fr 
 
  
 
 
Abstract 
In this paper we propose a modelization of 
the construction of Persian noun and adjec-
tival phrases in a phrase structure grammar. 
This modelization uses the Interaction 
Grammar (IG) formalism by taking advan-
tage of the polarities on features and tree 
descriptions for the various constructions 
that we studied. The proposed grammar was 
implemented with a Metagrammar compiler 
named XMG. A small test suite was built 
and tested with a parser based on IG, called 
LEOPAR. The experimental results show 
that we could parse the phrases successfully, 
even the most complex ones which have 
various constructions in them. 
1 Introduction 
Interaction Grammar (IG) is a grammatical for-
malism which is based on the notions of polar-
ized features and tree descriptions. 
Polarities express the resource sensitivity of 
natural language by modeling the distinction be-
tween saturated and unsaturated syntactic con-
struction (Guillaume and Perrier, 2008). 
IG focuses on the syntactic level of a natural lan-
guage. This formalism is designed in such a way 
that it can be linked with a lexicon, independent 
of any formalism. The notion of polarity that is at 
the heart of IG will be discussed in section 2.2. 
In IG, the parsing output of a sentence is an or-
dered tree where nodes represent syntactic con-
stituents described by feature structures.  
What we are interested in is studying the con-
struction of constituencies of the Persian lan-
guage according to IG. Among various 
constituencies in the language, we have focused 
on the construction of Persian noun phrases and 
adjectival phrases as the first step to build a 
grammar for this language. 
The current work covers only noun and adjecti-
val phrases; it is only a first step toward a full 
coverage of Persian grammar. The grammar pre-
sented here could have been expressed in Tree 
Adjoining Grammar (TAG) or even in Context 
Free Grammar with features, but we strongly 
believe that the modelization of the verbal con-
struction of Persian, which is much more com-
plex, can benefit from advanced specificities of 
IG, like polarities, underspecifications and trees. 
2 Previous Studies  
2.1 IG for French and English 
The first natural language considered within IG 
was French. A large coverage grammar which 
covers most of the frequent constructions of 
French, including coordination, has been built 
(Perrier, 2007; Le Roux and Perrier, 2007).  
Recently, using the fact that the French and Eng-
lish languages have many syntactic similarities, 
Planul (2008) proposed an English IG built by 
modifying the French one. These two grammars 
were tested on the Test Suite for Natural Lan-
guage Processing (TSNLP; Oepen et al 1996). 
Both cover 85% of the sentences in the TSNLP.  
2.2 Polarity 
The notion of polarity is based on the old idea of 
Tesni?re (1934), Jespersen (1935), and Adjuk-
iewicz (1935) that a sentence is considered as a 
molecule with its words as the atoms; every word 
is equipped with a valence which expresses its 
capacity of interaction with other words, so that 
syntactic composition appears as a chemical re-
action (Gaiffe and Perrier, 2004). Apparently, it 
seems Nasr (1995) was the first to propose a 
107
formalism that explicitly uses the polarized struc-
ture in computational linguistics. Then re-
searches such as Muskens and Krahmer (1998), 
Duchier and Thater (1999), and Perrier (2000) 
proposed grammatical formalisms in which po-
larity is also explicitly used. However, Categorial 
Grammar was the first grammatical formalism 
that exploited implicitly the idea of polarity 
(Lambek, 1958). Recently, Kahane (2006) 
showed that well-known formalisms such as 
CFG, TAG, HPSG, and LFG could be viewed as 
polarized formalisms.  
IG has highlighted the fundamental mechanism 
of neutralization between polarities underlying 
CG in such a way that polarities are attached to 
the features used for describing constituents and 
not to the constituents themselves. Polarization 
of a grammatical formalism consists of adding 
polarities to its syntactic structure to obtain a po-
larized formalism in which neutralization of po-
larities is used to control syntactic composition. 
In this way, the resource sensitivity of syntactic 
composition is made explicit (Kahane, 2004). 
In trees expressing syntactic structures, nodes 
that represent constituents are labeled with po-
larities with the following meanings: A constitu-
ent labeled with a negative polarity (<-) 
represents an expected constituent, whereas a 
constituent labeled with the positive polarity (->) 
represents an available resource. Both of these 
polarities can unify to build a constituent which 
is labeled with a saturated neutral polarity (<=>) 
that cannot interact with any other constituents. 
The composition of structures is guided by the 
principle of neutralization that every positive 
label must unify with a negative label, and vice 
versa. Nodes that are labeled with the simple 
neutral polarity (=) do not behave as consumable 
resources and can be superposed with any other 
nodes any number of times; they represent con-
stituents or features indifferently.  
The notion of saturation in terms of polarity is 
defined as a saturated structure that has all its 
polarities neutral, whereas an unsaturated struc-
ture keeps positive or negative polarities which 
express its ability to interact with other struc-
tures. A complete syntactic tree must be satu-
rated; that means it is without positive or 
negative nodes and it can not be composed with 
other structures: so all labels are associated with 
the polarity of = or <=>. 
The set of polarities {-> , <- , = , <=>} is 
equipped with the operation of compositional 
unification as defined in the table below (Bon-
fante et al 2004): 
 <- -> = <=> 
<-  <=> <-  
-> <=>  ->  
= <- -> = <=> 
<=>   <=>  
 
Table 1. Polarity compositions on the nodes 
2.3 Tree Description Logic in IG 
Another specification of IG is that syntactic 
structures can be underspecified: these structures 
are trees descriptions. It is possible, for instance, 
to impose that a node dominates another node 
without giving the length of the domination path. 
Guillaume and Perrier (2008) have defined four 
kinds of relations: 
- Immediate dominance relations: N > M means 
that M is an immediate sub-constituent of N.  
- Underspecified dominance relations: N >* M 
means that the constituent N includes another 
constituent M at a more or less deep level. (With 
this kind of node relations, long distance depend-
encies and possibilities of applying modifiers 
could be expressed.)  
- Immediate precedence relations: N << M means 
that the constituent M precedes the constituent N 
immediately in the linear order of the sentence. 
- Underspecified precedence relations: N <<+ M 
means that the constituent M precedes the con-
stituent N in the linear order of the sentence but  
the relation between them cannot be identified. 
3 The Persian Language Properties 
Persian is a member of the Indo-European lan-
guage family and has many features in common 
with the other languages in this family in terms 
of morphology, syntax, phonology, and lexicon. 
Although Persian uses a modified version of the 
Arabic alphabet, the two languages differ from 
one another in many respects.  
Persian is a null-subject language with SOV 
word order in unmarked structures. However, the 
word order is relatively free. The subject mood is 
widely used. Verbs are inflected in the language 
and they indicate tense and aspect, and agree 
with subject in person and number. The language 
does not make use of gender (M?hooti?n, 1997). 
In noun phrases, the sequence of words is around 
at least one noun, namely the head word. So, the 
noun phrase could be either a single unit noun, or 
a sequence of other elements with a noun. The 
syntax of Persian allows for having elements be-
fore a noun head _prenominal, and after the noun 
head _postnominal. 
108
To make a phrase, there are some restrictions for 
the elements surrounding a head to make a con-
stituent; otherwise the sequence of elements will 
be ill-formed, that is, ungrammatical.  
Nouns belong to an open class of words. The 
noun could be a common noun, a proper noun, or 
a pronoun. If this noun is not a proper noun or a 
pronoun, some elements can come before it and 
some after it (M?hooti?n, 1997). Some of the 
prenominal elements coming before a noun head 
are cardinal numbers, ordinal numbers, superla-
tive adjectives, and indefinite determiners; post-
nominal elements are nouns and noun phrases, 
adjectives and adjectival phrases, adjectival 
clauses with conjunctions, indefinite post-
determiners, prepositional phrases, adverbs of 
place and time, ordinal numbers, possessive ad-
jectives, and Ezafeh.  
The syntactical structure of an adjectival phrase 
is simple. It is made up of a head adjective and 
elements that come before and after the head. An 
adjectival phrase is a modifier of a noun. The 
elements coming before a simple adjective are 
adverbs of quantity and prepositional phrases. 
4 Required Tools 
4.1 Test Suite 
The test suite is a set of controlled data that is 
systematically organized and documented. In this 
case, the test suite is a kind of reference data dif-
ferent from data in large collections of text cor-
pora. A test suite should have the following 
advantages: it should have a broad coverage on 
the structural level, so you can find many struc-
tures of a language with a minimal lexicon; it 
could be multilingual, so the structure of the lan-
guages could be compared; it should be a consis-
tent and highly structured linguistic annotation. 
The differences between a test suite and a corpus 
are: that in test suite there is a control on the 
data, that the data has a systematic coverage, that 
the data has a non-redundant representation, that 
the data is annotated coherently, and that relevant 
ungrammatical constructions are included inten-
tionally in a test suite (Oepen et al 1996).  
Since our end goal is to develop a fragment of 
Persian grammar, to the best of our knowledge 
no already developed test suite for our target 
constructions was available; so we built a very 
small test suite with only 50 examples based on a 
small lexicon _only 41 entries.  
 
4.2 XMG 
The XMG system is usually called a "meta-
grammar compiler" is a tool for designing large-
scale grammars for natural language. This system 
has been designed and implemented in the 
framework of Benoit Crabb? (2005). 
XMG has provided a compact representation of 
grammatical information which combines ele-
mentary fragments of information to produce a 
fully redundant, strongly lexicalized grammar. 
The role of such a language is to allow us to 
solve two problems that arise while developing 
grammars: to reach a good factorization in the 
shared structures, and to control the way the 
fragments are combined.  
It is possible to use XMG as a tool for both tree 
descriptions in IG and TAG. Since there isnot 
any built-in graphical representation for IG in 
XMG, LEOPAR is used to display the grammar.  
LEOPAR is a parser for processing natural lan-
guages based on the IG formalism. 
4.3 LEOPAR 
LEOPAR is a tool chain constructed based on IG 
(Guillaume et al 2008). It is a parser for IG that 
can be used as a standalone parser in which in-
puts are sentences and outputs are constitu-
ent trees. But it also provides a graphical user 
interface which is mostly useful for testing and 
debugging during the stages of developing the 
grammar. The interface can be used for interac-
tive or automated parsing.  LEOPAR also pro-
vides several visualization modes for the 
different steps in the parsing process. Further-
more, it offers some tools to deal with lexicons:  
they can be expressed in a factorized way and 
they can be compiled to improve parsing effi-
ciency. 
LEOPAR is based on UTF8 encoding, so it sup-
ports Persian characters. It is also modified to 
take into account the right-to-left languages. For 
our designed grammar we have taken the advan-
tage of this parser for IG. 
5 Designing the Grammar 
In this section we explicitly describe the tree 
construction of the Persian noun and adjectival 
phrase structures which are polarized. We have 
provided the elementary syntactic structures de-
rived from the existing rules in the language and 
then polarized the features in the trees which are 
named initial polarized tree descriptions. 
109
To be more comprehensible and clear, nodes are 
indexed for addressing. More importantly, the 
trees should be read from right-to-left to match 
the writing system in the right-to-left language.  
For clarity in the tree representations in this pa-
per, no features are given to the nodes. But while 
developing the grammar with XMG, polarized 
features are given to the nodes to put a control on 
constructing the trees and avoid over-generating 
some constructions.  
There are some constructions whose tree repre-
sentations are the same but represent two differ-
ent constructions, so they could be described 
from two different points of views. Such trees are 
described in the sections corresponding to the 
relevant constructions. Some morphophonemic 
phenomena were considered at the syntactic 
level, while developing our grammar. Such a 
phenomenon is defined at the feature level for 
the lexicon which will be described in their rele-
vant sections. 
5.1 Noun Construction 
A noun phrase could consist of several elements 
or only one head noun element. If the element of 
a noun phrase (N1) is a noun, it is anchored to a 
lexicon item (N2) which could be a common 
noun, or a proper noun. The symbol ? has been 
used for the nodes that are anchored to a lexical 
item.            -> N1 
| 
= N2? 
The tree of a common noun and a proper noun 
are the same, but features should be given to the 
tree to make a distinction between the anchored 
nouns. With the help of features, we can make 
some restrictions to avoid some constructions. 
Features and their values are not fully discussed 
here. 
5.2 Pronoun Construction 
A pronoun can appear both in subject and object 
positions to make a noun. In this construction, 
node N3 is anchored to a pronoun: 
-> N3 
| 
= PRON? 
A pronoun cannot be used in all constructions. 
For example, N3 cannot be plugged into N5 in a 
determiner construction because a determiner 
could not come before a pronoun. To avoid this 
construction, some features have been used for 
the node N5 to stop the unification with some N 
nodes like N3.  
5.3 Determiner Construction 
In Persian a determiner comes before a common 
noun or a noun phrase, and not a proper noun or 
a pronoun.  
Persian does not benefit from the definite deter-
miner, but there are two kinds of indefinite de-
terminers: one comes before a noun as a separate 
lexical item and the other one comes after a noun 
(post-determiner) which is joined to the end of 
the noun as described below: 
If the determiner comes before a noun, there 
must be a tree in which a Det node is anchored to 
a lexicon item that is a determiner and which 
comes immediately before a noun. In other 
words, some lexical items which are determiners 
could attach to this node:  
 -> N4  
   
<- N5  = Det? 
If the determiner comes after a noun (i.e. if it is a 
post-determiner), then it can be joined to the end 
of a noun. The post-determiner (P-Det) and the 
preceding noun (N7), make a noun (N6):  
 -> N6  
   
= P-Det?  <- N7 
The post-determiner has three different written 
forms: ??? /i/, ???? /yi/, and ???? /?i/. The reason 
to have them is phonological. In our formalism 
we have considered this phonological phenome-
non at a syntactic level.  
If the post-determiner construction is used after 
an adjective in the linguistic data, it does not be-
long to the adjective (since the adjective is only 
the modifier of the noun), but it belongs to the 
noun. According to the phonological context and 
the final sound of the adjective, the post-
determiner that belongs to the noun changes and 
takes one of the written forms. 
5.4 Ezafeh Construction 
One of the properties of Persian is that usually 
short vowels are not written. In this language, the 
Ezafeh construction is represented by the short 
vowel ?-?? /e/ after consonants or ???? /ye/ after 
vowels at the end of a noun or an adjective.  
Here we try to give a formal representation of 
such construction that is described from a purely 
syntactical point of view. Ezafeh (Ez) appears on 
(Kahnemuyipour, 2002): a noun before another 
noun (attributive); a noun before an adjective; a 
noun before a possessor (noun or pronoun); an 
adjective before another adjective; a pronoun 
110
before an adjective; first names before last 
names; a combination of the above. 
Note that Ezafeh only appears on a noun when it 
is modified. In other words, it does not appear on 
a bare noun (e.g. ?????? /ket?b/ 'book'). In Ezafeh 
construction, the node Ez is anchored to the 
Ezafeh lexeme. The below tree could make a 
noun phrase (N8) with Ezafeh construction, in 
which a common noun or a proper noun on N9 is 
followed by an Ezafeh (Ez) and another common 
noun, proper noun, pronoun or another noun 
phrase plugs to the node N10: 
      -> N8  
   
<- N10   = N 
    
          = Ez?                                <- N9 
The below tree could make a noun phrase (N11) 
with Ezafeh construction in which a common 
noun or a proper noun on N12 is modified by an 
adjectival phrase on node ADJ1. Ezafeh has to be 
used after the noun to link it to the adjective: 
    -> N11  
   
<- ADJ1   = N 
    
     = Ez?                                <- N12 
Based on the final sound of the word which is 
just before Ezafeh, there are two written forms 
for Ezafeh, depending on whether the noun ends 
with a consonant or a vowel. 
As we have already said, Ezafeh contraction 
could be used for an adjective (ADJ1). After this 
construction, another adjectival phrase (ADJ3 
and ADJ4) with Ezafeh could appear too. It 
should be mentioned that ADJ4 is plugged into 
an adjective without Ezafeh construction:  
 ->ADJ2  
   
<-ADJ4  =ADJ 
    
                        = Ez?   <-ADJ3 
5.5 Possessive Construction 
In Persian there are two different constructions 
for possessive. One is a separate lexical item as a 
common noun, a proper noun, or a pronoun. The 
second is a possessive pronoun that is a kind of 
suffix which attaches to the end of the noun. In 
the first construction, a noun with an Ezafeh con-
struction is used and then a common noun, a 
proper noun, or a pronoun as a separate lexical 
item follows. In the latter construction, there is a 
common noun and the joined possessive pro-
noun. The two constructions are discussed here: 
In section 5.4 we described Ezafeh construction 
(N8). This tree could be used for possessive con-
struction, too. In this tree an Ezafeh is used after 
a common noun and Ezafeh is followed by either 
a common noun or a proper noun. A pronoun 
could not be used in N9 with Ezafeh. Such a kind 
of construction is avoided by defining features. 
The possessive construction as a suffix could 
come after both a noun and an adjective. The 
general property of the joined possessive pro-
nouns is that there is an agreement between the 
subject and the possessive pronoun in terms of 
number and person, no matter whether it is used 
after a noun or an adjective. 
If the joined possessive pronoun (S-P) is used 
after a noun (N14), we would have the tree N13 
in which the possessive pronoun is anchored to 
the suffix (S-P): 
 -> N13 
   
= S-P?  <- N14 
Based on the phonological reasons and consider-
ing Persian syllables, as was discussed previ-
ously in section 5.3, this suffix would have 
different written forms based on the phonological 
context it appears in: after a consonant, the vowel 
/?/, or any other vowels except /?/. For adjec-
tives, there is no suffix possessive pronoun. In 
the linguistic data, this pronoun could appear 
after the adjective. But the point is that the adjec-
tive is only the modifier of the noun. This pos-
sessive pronoun, in fact, belongs to the noun and 
not the adjective, but based on the phonological 
rules (i.e. the final sound of the adjective) only 
one of the written forms would appear after that. 
5.6 Count noun Construction 
There are some nouns in Persian referred to as 
count nouns which have collocational relations 
with the head noun that is counted. So, in such a 
construction, the node C-N is anchored to a lexi-
cal item that is a count noun: 
  -> N15  
   
<- N16  = C-N? 
5.7 Object Construction 
In Persian, a noun phrase can appear both in sub-
ject and object positions. If the noun phrase ap-
pears in a subject position, it does not require any 
indicator. But if the noun phrase appears in the 
direct object position (N18), the marker ???? /r?/ 
is used to indicate that this noun phrase (N17) is 
a direct object. We call this marker ?Object Indi-
111
cator? (O-I) so the node is anchored to the object 
maker. The representation of the tree for the ob-
ject construction (N17) is the followings:  
 -> N17  
   
= O-I?  <- N18 
5.7 Conjunction Construction 
In Persian, there is a construction to modify the 
preceding noun phrase with an adjective clause 
which we have named the Conjunction construc-
tion. In such a construction, there are a noun 
phrase (N20), a conjunctor (Conj), and a clause 
to modify the noun phrase (S1). In the tree, the 
conjunction node is anchored to a conjunctor: 
 -> N19  
   
    = S  <- N20 
   
       <- S1         = Conj? 
5.8 Adjective Constructions 
There are two classes of adjectives: the first class 
comes before a noun head, the second one after.  
There are three kinds of adjectives in the first 
class which can be differentiated from each other 
with the help of features. The first class of adjec-
tives contains superlative adjectives, cardinal 
numbers, and ordinal numbers that modify a 
noun, a count noun, or a noun phrase. Usually, 
the adjectives coming before a noun phrase are in 
complementary distribution; i.e. the presence of 
one means the absence of the two others.  
The following tree represents the adjective con-
struction coming before a noun (N22). The ad-
jective ADJ5 is anchored to a lexical item: 
 ->  N21  
   
<- N22  =ADJ5? 
The second class of adjectives (which comes af-
ter a noun) contains mostly simple adjectives, 
ordinal numbers and comparative adjectives.  
As we have already described tree N11 in section 
5.4, to have an adjective after a noun the noun 
must have an Ezafeh construction. So, this tree 
represents a construction where an adjective 
(ADJ1) comes after a noun (N12). 
To saturate ADJ1, the tree ADJ6 is required 
which is anchored to an adjective lexical item: 
->ADJ6 
| 
=ADJ7? 
In some adjective constructions, a prepositional 
phrase could be used which comes before or after 
some adjective constituents. With the help of 
some features, we have made restrictions on the 
kind of adjective and the preposition lexical item 
that could plug into this node. 
If a preposition is used before the adjective 
(ADJ9), it is a comparative adjective: 
 ->ADJ8  
   
=ADJ9?  <- P1 
If the preposition is used after the adjective 
(ADJ11), it is either a comparative or a simple 
adjective: 
 ->ADJ10  
   
    <- P2                      =ADJ11? 
5.9 Preposition Construction 
In Persian a common noun, a proper noun, a pro-
noun, or a noun phrase could come after a 
preposition (P4) to make a prepositional phrase 
(P3):  -> P3  
   
    <- N23                        = P4? 
If the preposition construction is used in an ad-
jective construction, only some specific preposi-
tions can be used. Once again, the restrictions are 
encoded with features. 
6 Implementation and Results 
So far we have explicitly described the noun and 
adjectival phrase constructions in Persian accord-
ing to the constituency rules that are extracted 
from the linguistic data. These rules are repre-
sented by polarized trees. Since we wanted to 
study the noun and adjectival phrase structures, 
they required data. We have gathered this data 
for our purpose as a test suite. 
To design IG for the constructions that were de-
scribed, we have used XMG as the basic tool to 
have the initial tree descriptions. While describ-
ing the trees in XMG, several operators will be 
used to polarizing features. The categories of the 
nodes are considered as features, so the nodes are 
polarized. Using XMG, we have done factoriza-
tions and defined classes for general trees. Three 
factorized general trees are defined in our XMG 
coding. We have also defined 17 classes for cod-
ing of trees to represent the constructions as de-
scribed. 
The output of XMG is given to LEOPAR to dis-
play the graphical representations of the tree 
structures and also parse the data. The test suite 
is given to LEOPAR for parsing.  
Having the developed trees and the test suite, we 
successfully parsed all available phrases, from 
112
the simplest to the most complex ones that had a 
variety of constructions in them. Example 1 has a 
simple construction, example 2 is of medium 
complexity, and example 3 is the most complex: 
 
1.                                                              ??????????   
/ket?b/ (/e/) /d?niy?l/                           
 book    (Ez)   Daniel 
?the book of Daniel / Daniel?s book? 
 
2.                                  ????? ???? ???????? ?? ??????  
/hamzam?n/   /b?/ /ente??r/  (/e/) /avvalin/ 
in coincidence  with publishing  (Ez)  the first     
/ket?b/ (/e/)   /?u/ 
 book    (Ez)  his/her  
?in coincidence with the publishing of his/her 
first book? 
 
3.                           ???? ?? ?? ?????  ??? ???? ????????  
/?n/ /do/ /jeld/ /ket?b/ (/e/) /jadid/ (/e/)  
that   two    volume    book     (Ez)     new      (Ez)  
/mohem/   (/e/)  /d?niyal/  /r?/    /ke/ 
important   (Ez)    Daniel   POBJ that 
?the two new important book volumes of Daniel 
that? 
 
We know from section 5.4 that Ezafeh is pro-
nounced but not written. Since the anchored 
nodes require a lexical item, we put the word 
??????? /ez?fe/ ?Ezafeh? in the lexicon to have a 
real representation of Ezafeh. Also, wherever 
Ezafeh is used in the test suite, this word is re-
placed. 
As a sample, we give a brief description of pars-
ing the phrases 1 and 2 with LEOPAR and dis-
play the outputs. 
In our test suite, phrase 1 is found as 
????? ????? ???????. In this phrase, the common 
noun /ket?b/ is followed by a proper noun 
/d?niy?l/ with Ezafeh. The possessive construc-
tion (N8) would be used to parse this phrase.  
In parsing this phrase, firstly LEOPAR reads the 
words and matches them with the lexical items 
available in the lexicon to identify their catego-
ries. Then it plugs these words into the nodes in 
the trees that have the same syntactic category 
and have an anchored node. Finally, it gives the 
parsed graphical representation of the phrase.  
For this phrase, the Ezafeh construction tree is 
used in such a way that N2 is anchored to the 
word /ket?b/ and N1 plugs into N9 to saturate it. 
Then, N2 is again anchored to the word /d?niy?l/ 
and N1 plugs in to saturate N10. The final parsed 
phrase is such that all internal nodes are saturated 
and have neutral polarity, as shown in Figure 1. 
As another example, consider phrase 2, which is 
 
 
Figure 1: Parsing the phrase ????? ???????  
with LEOPAR 
 
 
 
 
Figure 2: Parsing the phrase? ???? ?? ?????? ?? ?????? ????? ? 
with LEOPAR 
 
found as ? ?? ????? ????? ???? ??????????? ?? ?????? ? in 
our test-suite. Since some various constructions 
are used to build this phrase, we could say that it 
113
is a complex phrase. Firstly it takes the adjective 
phrase construction (ADJ10). P3, the preposi-
tional phrase, plugs into P2. Since a noun or a 
noun phrase could be used after a preposition 
(N23), the Ezafeh construction (N8) that takes 
the noun plugs to this node. Another Ezafeh con-
struction (N8) will be plugged into N10. The ad-
jective construction (ADJ5) for ordinal numbers 
as the modifier of a noun (N22) could be used 
while a noun (N1) would plug into N22. Finally, 
the pronoun (N3) plugs into the unsaturated noun 
position in the second Ezafeh construction. Pars-
ing the phrase with LEOPAR, the result has all 
internal nodes saturated and neutralized, and no 
polarities on the nodes are left unsaturated, as 
shown in Figure 2. 
7 Conclusion and Future Work 
In our research we have used IG to represent the 
construction of Persian noun and adjectival 
phrases in trees. XMG was used to represent the 
constructions using factorization and inherited 
hierarchy relations. Then, with the help of XMG, 
we defined IG by taking advantage of polarities 
on the features and tree descriptions for the vari-
ous constructions that are introduced. Then, we 
used LEOPAR for the graphical representations 
of the trees and parsing the phrases. Finally, we 
applied our test suite to the parser to check 
whether we had the correct parsing and represen-
tation of the phrases. The experimental results 
showed that we could parse the phrases success-
fully, including the most complex ones, which 
have various constructions in them. 
In the next step of our research, we would like to 
study the construction of prepositions and, more 
importantly, verbs in depth to make it possible to 
parse at the sentence level. 
References  
Adjukiewcz K., 1935. ?Die syntaktiche konnexit?t? 
Studia Philadelphica 1, pp. 1-27. 
Bonfante G. and B. Guillaume and G. Perrier, 2004. 
?Polarization and abstraction of grammatical for-
malism as methods for lexical disambiguation? In 
Proc.s of 20th Int. Conf. on CL, Gen?ve. 
Candito,  M. H., 1996. ?A principle-based hierarchical 
representa-tion of LTAGs?. COLING-96. 
Crabb?, B., 2005. ?Grammatical development with 
XMG?. LACL 05. 
Duchier and Thater, 1999. ?Parsing with tree descrip-
tions: A constraint based approach? In Proc.s of 
NLU and Logic Programming, New Mexico. 
Gaiffe, B. and G. Perrier, 2004. ?Tools for parsing 
natural language? ESSLLI 2004. 
Guillaume B. and G. Perrier, 2008. ?Interaction 
Grammars? INRIA Research Report 6621:  
http://hal.inria.fr/inria-00288376/ 
Guillaume B. and J. Le Roux and J. Marchand and G. 
Perrier and K. Fort and J. Planul, 2008, ?A Tool-
chain for Grammarians? CoLING 08, Manchester. 
Jesperson , O., 1935. Analytic Syntax. Allen and 
Uwin, London. 
Kahane, S., 2004. ?Grammaries d?unification polari-
s?es? In 11i?me Conf. sur le TAL, F?s, Maroc. 
Kahane, S., 2006. ?Polarized unification grammars?. 
In Proce.s of 21st Int. Conf. on CL and 44th An-
nual Meeting of the ACL. Sydney, Australia. 
Kahnemuyipour, A., 2000. "Persian Ezafe construc-
tion revisited: Evidence for modifier phrase," An-
nual Conf. of the Canadian Linguistic Association. 
Lambek, J., 1958. "The mathematics of sentence 
structure", The American Mathematical Monthly 
65: 154?170. 
Leopar: a parser for Interaction Grammar 
http://leopar.loria.fr/ 
Le Roux, J. and G. Perrier, 2007. ?Mod?lisation de la 
coordination dans les Grammaires d?Interaction?, 
Traitement Automatique des Langues (TAL 47-3) 
M?hooti?n, Sh, 1997. Persian. Routledge. 
Muskens and Krahmer, 1998. ?Talking about trees 
and truth conditions?. In Logical Aspects of CL, 
Grenoble, France, Dec 1998.  
Nasr A., 1995. ?A formalism and a parser for lexical-
ized dependency grammars? In Proce.s of 4th Int. 
Workshop on Parsing Technologies, Prague. 
Oepen, S. and K. Netter and J. Klein, 1996. ?TSNLP- 
Test suites for natural language processing?. In 
Linguistic Database, CSLI Lecture Notes. Center 
for the Study of Language and information. 
Perrier, G., 2000. ?Interaction grammar? Coling 2000. 
Perrier, G., 2007. "A French Interaction Grammar", 
RANLP  2007, Borovets Bulgarie.  
Planul, J., 2008. Construction d'une Grammaire d'In-
teraction  pour l'anglais, Master thesis, Universit? 
Nancy 2, France. 
Tesni?re L., 1934. ?Comment construire une syntaxe? 
Bulletin de la Facult? des Lettres de Strasbourg 7-
12i?me. pp. 219-229. 
XMG Documentation 
http/wiki.loria.fr/wiki/XMG/Documentation 
114
Proceedings of the NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing, pages 1?9,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using Variance as a Stopping Criterion
for Active Learning of Frame Assignment
Masood Ghayoomi
German Grammar Group
Freie Universita?t Berlin
Berlin, 14195
masood.ghayoomi@fu-berlin.de
Abstract
Active learning is a promising method to re-
duce human?s effort for data annotation in dif-
ferent NLP applications. Since it is an itera-
tive task, it should be stopped at some point
which is optimum or near-optimum. In this
paper we propose a novel stopping criterion
for active learning of frame assignment based
on the variability of the classifier?s confidence
score on the unlabeled data. The important ad-
vantage of this criterion is that we rely only on
the unlabeled data to stop the data annotation
process; as a result there are no requirements
for the gold standard data and testing the clas-
sifier?s performance in each iteration. Our
experiments show that the proposed method
achieves 93.67% of the classifier maximum
performance.
1 Introduction
Using supervised machine learning methods is very
popular in Natural Language Processing (NLP).
However, these methods are not applicable for most
of the NLP tasks due to the lack of labeled data. Al-
though a huge amount of unlabeled data is freely
available, labeling them for supervised learning
techniques is very tedious, expensive, time consum-
ing, and error prone.
Active learning is a supervised machine learning
method in which informative instances are chosen
by the classifier for labeling. Unlike the normal su-
pervised set-up where data annotation and learning
are completely independent, active learning is a se-
quential process (Settles, 2009; Busser and Morante,
2005). This learning method is used in a variety of
NLP tasks such as information extraction (Thomp-
son et al, 1999), semantic role labeling (Busser and
Morante, 2005), machine translation (Haffari and
Sarkar, 2009), and name entity recognition (Laws
and Schu?tze, 2008). In our study, we apply this
method for the frame assignment task as a kind of
semantic analysis.
The process of active learning is as follows: the
learner takes a set of labeled instances, called seed
data, as an input for initial training of the classifier;
and then a larger set of unlabeled instances will be
selected by the classifier to be labeled with the hu-
man interaction. Even a small set of well selected
samples for labeling can achieve the same level of
performance of a large labeled data set; and the ora-
cle?s effort will be reduced as a result.
The motivation behind active learning is select-
ing the most useful examples for the classifier and
thereby minimizing the annotation effort while still
keeping up the performance level (Thompson et al,
1999). There are two major learning scenarios in
active learning which are very popular among re-
searchers and frequently used in various NLP tasks:
stream-based sampling (Cohn et al, 1994) and pool-
based sampling (Lewis and Gale, 1994).
The samples that are selected should be hard and
very informative. There are different query meth-
ods for sample selection which are independent of
the active learning scenarios (Settles, 2009). Among
them, uncertainty sampling (Lewis and Gale, 1994)
is the most well-known and the simplest sam-
ple selection method which only needs one classi-
fier (Baldridge and Osborne, 2004). In this query
method, the samples that the classifier is least con-
1
Algorithm 1 Uncertainty Sampling in Active Learning
Input: Seed data S, Pool of unlabeled samples U
Use S to train the classifier C
while the stopping criterion is met do
Use C to annotate U
Select the top K samples from U predicted by
C which have the lowest confidence
Label K, augment S with theK samples, and re-
move K from U
Use S to retrain C
end while
fident on their labels are selected and handed out to
the oracle. To this aim, a confidence score is re-
quired which is in fact the prediction of the classi-
fier with the highest probability for the label of the
sample (Busser and Morante, 2005).
The approach taken in active learning for our task
is based on the uncertainty of the classifier with ac-
cess to the pool of data. The learning process is
presented in Algorithm 1. Since active learning is
an iterative process (Busser and Morante, 2005), it
should be stopped at some point which is optimum
or at least near-optimum. A learning curve is used
as a means to illustrate the learning progress of the
learner, so that we can monitor the performance of
the classifier. In fact, the curve signals when the
learning process should stop as almost no increase
or even a drop in the performance of the classifier
is observed. At this point, additional training data
will not increase the performance any more. In this
paper, we propose a new stopping criterion based on
the variability of the classifier?s confidence score on
the selected unlabeled data so that we avoid using
the labeled gold standard.
The structure of the paper is as follows. In Sec-
tion 2, we briefly describe frame semantics as it is
the domain of application for our model. Section 3
introduces our stopping criterion and describes the
idea behind it. In Section 4, we describe our data
set and present the experimental results. In Section
5, related work on stopping criteria is outlined; and
finally Section 6 summarizes the paper.
2 Frame Semantics
Syntactic analysis such as part-of-speech (POS) tag-
ging and parsing has been widely studied and has
achieved a great progress. However, semantic anal-
ysis did not have such a rapid progress. This prob-
lem has recently motivated researches to pay special
attention to natural language understanding since it
is one of the essential parts in information extraction
and question-answering.
Frame semantic structure analysis which is based
on the case grammar of Fillmore (1968) is one of
the understanding techniques to provide the knowl-
edge about the actions, the participants of the ac-
tion, and the relations between them. In Fillmore?s
view, a frame is considered as an abstract scene hav-
ing some participants as the arguments of the pred-
icate, and some sentences to describe the scene. In
fact the frames are the conceptual structures for the
background knowledge of the abstract scenes repre-
sented by the lexical units and provide context to the
elements of the action. FrameNet (Baker and Lowe,
1998) is a data set developed at ICSI Berkley Uni-
versity based on the frame semantics.
In frame semantic structure analysis, the semantic
roles of the elements participating in the action are
identified. Determining and assigning the semantic
roles automatically require two steps: frame assign-
ment, and role assignment (Erk and Pado, 2006).
The first step consists in identifying the frame which
is evoked by the predicate to determine the unique
frame that is appropriate for the sample. The next
step is identifying the arguments of the predicate and
assigning the semantic roles to the syntactic argu-
ments of the given frame. In our research, we study
the first step, and leave the second step for future
work.
3 The Proposed Stopping Criterion
The main idea behind the stopping criteria is to stop
the classifier when it has reached its maximum per-
formance and labeling of further examples from the
unlabeled data set will not increase the classifier?s
performance any more. Determining this point is
very difficult experimentally without access to the
gold standard labels to evaluate the performance;
however, we should find a criterion to stop active
learning in a near-optimum point. To this aim, we
propose a novel stopping criterion which uses the
variance of the classifier?s confidence score for the
predicted labels to represent the degree of spread-
ing out the confidence scores around their mean. We
hypothesize that there is a correlation between the
2
performance saturation of the classifier and the vari-
ability on the confidence of the selected instances.
Generally, as we will see in Section 5, a stopping
criterion could be based either on the performance
of the classifier on the test data, or on the confidence
score of the classifier on the unlabeled data. In our
method, we used the second approach. The biggest
advantage of this model is that no gold standard data
is required to evaluate the performance of the system
in each iteration.
3.1 Mean and Variance
Mean and variance are two of the well-known sta-
tistical metrics. Mean is a statistical measurement
for determining the central tendency among a set of
scores. In our study, we have computed the mean
(M) of the classifier?s confidence score for the pre-
dicted labels of 5 samples selected in each iteration.
Variance is the amount of variability of the scores
around their mean. To compute the variability of the
classifier?s confidence score for the selected samples
in each iteration, the following equation is used in
our task:
V ariance =
?K
i=1(Ci ?M)2
K (1)
where Ci is the confidence score of each selected
sample in each iteration, M is the mean of the confi-
dence scores for the predicted labels, and K is the
number of samples selected in the same iteration
(K=5 in our study).
3.2 The General Idea
According to the pool-based scenario, in each iter-
ation K samples of the extra unlabeled data which
have the lowest confidence score are selected, and
after labeling by the oracle they are added to the
training data. In the early iterations, the mean of the
classifier?s confidence score for the selected samples
is low. Since the classifier is not trained enough in
these iterations, most of the scores are low and they
do not have a high degree of variability. As a result
the variance of the confidence score for these sam-
ples is low. We call this step the untrained stage of
the classifier.
As the classifier is training with more data, the
confidence score of the samples will gradually in-
crease; as a result, there will be a high degree of
variability in the confidence scores which spread out
around their mean. In these iterations, the classifier
is relatively in the borderline of the training stage,
passing from untrained to trained; so that there will
be a high variability of confidence scores which
leads to have a high variance. This is the training
stage of the classifier.
When the classifier is trained, the confidence
score of the classifier on the selected samples will
increase. However, from a certain point that the clas-
sifier is trained enough, all of the confidence scores
are located tightly around their mean with a low de-
gree of variability; as a result, the variance of the
samples decreases. This is the stage that the classi-
fier is trained.
The curve in Figure 1 represents the behavior of
the variance in different iterations such that the x
axis is the number of iterations, and the y axis is the
variance of the confidence scores in each iteration.
Figure 1: Normal distribution of variance for the classi-
fier?s confidence score
Based on our assumption, the best stopping point is
when variance reaches its global peak and starts to
decrease. In this case, the classifier passes the train-
ing stage and enters into the trained stage.
3.3 The Variance Model
It is difficult to determine the peak of the variance
on the fly, i.e. without going through all iterations.
One easy solution is to stop the learning process as
soon as there is a decrease in the variance. However,
as it is very likely to stick in the local maxima of the
variance curve, this criterion does not work well. In
other words, it is possible to have small peaks before
reaching the global peak, the highest variability of
the classifier?s confidence score; so that we might
stop at some point we are not interested in and it
should be ignored.
3
To avoid this problem, we propose a model, called
variance model (VM), to stop active learning when
variance (V) decreases in n sequential iterations; i.e.
Vi < Vi?1 < ... < Vi?n .
There is a possibility that this condition is not satis-
fied at all in different iterations. In such cases, ac-
tive learning will not stop and all data will be la-
beled. This condition is usually met when there are
instances in the data which are inherently ambigu-
ous. Having such data is generally unavoidable and
it is often problematic for the learner.
Although the above model can deal with the lo-
cal maxima problem, there is a possibility that the
decreased variance in n sequential iterations is very
small and it is still possible to stick in the local max-
ima. To avoid this problem and have a better stop-
ping point, we extend the proposed model by setting
a threshold m, called the Extended Variance Model
(EVM), in which the minimum variance decrement
in n sequential iterations must be m; i.e.
Vi < Vi?1 ? m < ... < Vi?n ? m.
4 Experimental Results
4.1 Setup of Experiment
What we aim to do in our study is assigning frames
with active learning. We have chosen the pool-based
scenario by using the uncertainty sampling method.
In our task, since we have a small data set, 5 in-
stances (K=5) with the lowest confidence score of
the predited labels will be selected in each iteration
from the pool of data and handed out to the oracle to
be labeled.
We have used a toolkit for the supervised word
sense disambiguation task called Majo (Rehbein et
al., 2009) which has a graphical user interface (GUI)
for semantic annotation based on active learning.
The toolkit supports German and English; and it
uses the openNLP MAXENT package1 to build the
model. In this toolkit, the confidence score of the
classifier is the posterior probability of the most
probable label assigned to each sample.
In addition, there are some built-in plugins in the
tool for syntactic and semantic pre-processing to
provide the relevant features for the classifier. We
utilized the following plugins that support English:
1http://maxent.sourceforge.net/
? Stanford Word Range Plugin provides features
based on the local context of the surface string
for the target. The window size of the local
context can be set manually in the GUI. Based
on initial experiments for the target verbs, we
found out that a window ?3 performs the best.
? Stanford POS Tag Word Range Plugin provides
the POS tags of the words within a sentence
by using Stanford POS Tagger. In this plugin,
the window size could also be set manually to
extract the POS local context of the target word.
Based on initial experiments, a window of ?3
achieved the best performance.
? Berkley Sentence Phrase Plugin utilizes the
Berkley Parser and provides the syntactic anal-
ysis of the sentence. This plugin is used to ex-
tract all word forms of the children nodes from
a particular syntactic mother node (VP in our
study) and add them to the feature set.
? Berkley Sentence Phrase POS Tag Plugin uses
the Berkley POS tagger such that we define the
mother node of the target word in the parse tree
(VP in our study) and it identifies and extracts
all children of this mother node and uses their
POS as features.
4.2 Corpus
The annotated data that we used for our ex-
periments is the current version of the Berke-
ley FrameNet (Baker and Lowe, 1998) for En-
glish which consists of 139,437 annotated exam-
ples from the British National Corpus for 10,196
predicates. Among the predicates that FrameNet in-
volvs, namely verbs, nouns, adjectives, and prepo-
sitions, we only considered verbs; as a result the
data reduced to 61,792 annotated examples for 2,770
unique verb-frames.
In the next step, we removed all verbs that have
only one frame as they are not ambiguous. Hav-
ing only ambiguous verbs, the number of predicates
reduced to 451 unique verbs. Out of these targets,
there are only 37 verbs which have more than 100
annotated samples. Among these verbs, we concen-
trated on 14 verbs selected randomly; however, in
the selection we tried to have a balance distribution
of frames that the targets have. Therefore, we se-
lected 4 targets (phone, rush, scream, throw) with
4
Table 1: Data distribution of the targets
Verb Frames Freq. S E T
Bend 4 115 11 82 22
Feel 5 134 13 95 26
Follow 3 113 10 81 22
Forget 3 101 9 72 20
Hit 4 142 12 102 28
Look 3 183 15 134 34
Phone 2 166 14 121 31
Rise 4 110 11 77 22
Rush 2 168 14 123 31
Scream 2 148 12 108 28
Shake 4 104 10 73 21
Smell 3 146 13 106 27
Strike 3 105 10 75 20
Throw 2 155 13 113 29
two frames, 5 targets (follow, forget, look, smell,
strike) with three frames, 4 targets (bend, hit, rise,
shake) with four frames, and 1 target (feel) with five
frames.
4.3 Data Distribution
The total amount of data prepared for the 14 verbs
are divided into three non-overlapping sets in a bal-
anced form in terms of both the number of the target
predicate frames, and the relevant instances of each
frame. In other words, the distribution should be
such that different frames of the target verb is found
in each of the three data sets. 10% is considered as
initial seed data (S); 20% as test data (T), and the rest
of 70% as extra unlabeled data (E). Table 1 presents
the data distribution in which 5-fold cross-validation
is performed to minimize the overfitting problem.
As mentioned, our proposed stopping criterion
has two parameters, n and m, that should be tuned.
For this purpose, we divided the 14 targets into the
held-out set and the test set. To this aim, 7 tar-
gets, namely feel, look, phone, rise, shake, smell,
and throw are selected as the held-out set; and 7 tar-
gets, namely bend, follow, forget, hit, rush, scream,
and strike are used as the test set.
4.4 Results
Figures 2 and 3 illustrate the learning curves of the
active learning process and random sampling as the
baseline for the targets look and rise. The curves
are the average of the 5 folds. As can be seen, in
these targets our classifier has beaten the majority
Figure 2: Learning curve of the verb look for 5 folds
Figure 3: Learning curve of the verb rise for 5 folds
class baseline; and also active learning with uncer-
tainty sampling has a relatively better performance
than random sampling.
Figures 4 and 5 present the average variance
curves of 5 folds for the two targets. These curves
verify our assumption about the behavior of the vari-
ance curve as described in Section 3.2. As the
graphs show, following our assumption the variabil-
ity around the mean is tight in the early stages of
training; then as the classifier is trained with more
data, the variability around the mean spreads out;
and finally, the variability will be tight again around
the mean.
Applying our proposed stopping criterion, in each
iteration we compute the variance of the classifier?s
confidence score for the selected samples in each
fold. To evaluate how well our stopping criterion
is, we have compared our results with the maximum
average performance of the classifier for the 5 folds
in which the whole data is labeled.
Applying our model on the held-out set, we found
that n=2 is the best value based on our data set, so
that we stop active learning when variance decreases
in two sequential iterations; i.e.
Vi < Vi?1 and Vi?1 < Vi?2 .
Our idea is shown in Figure 6 for fold 5 of the tar-
get rise, such that the proposed stopping criterion is
satisfied in iteration 11. As shown, the decrement of
5
Figure 4: Variance curve of the verb look for 5 folds
Figure 5: Variance curve of the verb rise for 5 folds
variance in iterations 3, 5, and 7 is the local maxima
so that active learning does not stop in these itera-
tions and they are ignored.
The summary of the result for the uncertainty
sampling method of the test set is shown in Table 2
in which the F-score serves as the evaluation met-
ric. Comparing the applied variance model as the
stopping criterion on the test set with the maximum
performance (M) of the uncertainty sampling as an
upper bound in our experiment, we see that for two
targets (bend, rush) the maximum performance of
the classifier is achieved at the stopping point; for
two targets (follow, hit) there is a minor reduction in
the performance; while for the other targets (forget,
scream, strike) there is a big loss in the performance.
Averagely, the variance model achieved 92.66% of
the maximum performance.
To determine the advantage of our stopping cri-
terion, we present the total numbers of annotated
instances (A) for each target, their relevant num-
bers of annotated instances for the maximum per-
formance, and the variance model in Table 3. Av-
Figure 6: Variance curve of the verb rise
Table 2: The comparison of the average performance of
the classifier (F-score) on the stopping point with the
maximum performance in uncertainty sampling
Verb M VM
Bend 53.00 53.00
Follow 71.81 70.00
Forget 51.00 41.00
Hit 65.71 63.56
Rush 89.03 89.03
Scream 72.14 62.85
Strike 64.00 53.00
Average 66.67 61.78
Table 3: The comparison of the number of the annotated
data for all data, at the maximum performance, and at the
stopping point
Verb A M VM
Bend 93 46 55
Follow 91 75 54
Forget 81 79 51
Hit 114 67 71
Rush 137 24 51
Scream 120 62 64
Strike 85 85 41
Average 103 62.57 55.29
eragely, if we have 103 samples for annotation, we
need to annotate almost 63 instances to reach the
maximum performance of 66.67%; while by apply-
ing our stopping criterion, the learning process stops
by annotating at least 55 instances with 61.78% per-
formance. I.e., annotating a smaller number of in-
stances, our active learner achieves a near-optimum
performance. It is worth to mention that since it is
very difficult to achieve the upper bound of the clas-
sifier?s performance automatically, all data is labeled
to find the maximum performance of the classifier.
Looking carefully on the variance curves of the 5
folds of the held-out set, we have seen that in some
iterations the decreased variance in two sequential
iterations is very small and it may still stick in the
local maxima as can be seen in iteration 8 of fold 3
of the target look in Figure 7.
To avoid sticking in such local maxima, we used
the extended version of our original model and set
a threshold (m) in the held-out set. Experimentally
we found out that the decreasing variance in two se-
quential iterations must be bigger than 0.5; i.e.
Vi < Vi?1 - 0.5 and Vi?1 < Vi?2 - 0.5;
6
Figure 7: Variance curve of the verb look
so that in Figure 7 we stop in iteration 18. We ap-
plied the extended variance model on the test set and
compared the results to our original variance model.
We found out for two targets (forget, scream) the
extended model has achieved a very good perfor-
mance; for four targets (follow, hit, rush, strike) it
was ineffective; and for one target (bend) it caused
to have a small reduction in the performance.
The summary of the classifier performance after
applying the extended model for uncertainty sam-
pling is shown in Table 4. To ease the comparison,
the performance of our original model is repeated
in this table. As presented in the table, the average
performance in the extended model has a 13.70%
relative improvement compared to the average per-
formance in the original variance model.
Table 4: The comparison of the average performance of
the classifier (F-score) on the variance model and the ex-
tended variance model
Verb VM EM
Bend 53.00 52.00
Follow 70.00 70.00
Forget 41.00 46.00
Hit 63.56 63.56
Rush 89.03 89.03
Scream 62.85 63.56
Strike 53.00 53.00
Average 61.78 62.45
5 Related Work on Stopping Criteria
The simplest stopping criterion for active learning
is when the training set has reached a desirable size
or a predefined threshold. In this criterion, the ac-
tive learning process repeatedly provides informa-
tive examples to the oracle for labeling, and updates
the training set, until the desired size is obtained
or the predefined stopping criterion is met. Practi-
cally, it is not clear how much annotation is suffi-
cient for inducing a classifier with maximum effec-
tiveness (Lewis and Gale, 1994).
Schohn and Cohn (2000) have used support vector
machines (SVM) for document classification using
the selective sampling method and they have pro-
posed a criterion to stop the learning process in their
task. Based on their idea, when there is no informa-
tive instance in the pool which is closer to the sep-
arating hyperplane than any of the support vectors,
the margin exhausts and the learning process stops.
Zhu and Hovey (2007) have used a confidence-
based approach for the stopping criteria by utlizing
the maximum confidence and the minimum error of
the classifier. The maximum confidence is based on
the uncertainty measurement when the entropy of
the selected unlabeled sample is less than a prede-
fined threshold close to zero. The minimum error is
the feedback from the oracle when active learning
asks for the true label of the selected unlabeled sam-
ple and the accuracy prediction of the classifier for
the selected unlabeled sample is larger than a prede-
fined accuracy threshold. These criteria are consid-
ered as upper-bound and lower-bound of the stop-
ping condition.
Zhu et al (2008) proposed another stopping
criterion based on a statistical learning approach
called minimum expected error strategy. In this ap-
proach, the maximum effectiveness of the classifier
is reached when the classifier?s expected errors on
future unlabeled data is minimum.
Vlachos (2008) has used the classifier confidence
score as a stopping criterion for the uncertainty sam-
pling. He has applied his model to two NLP tasks:
text classification and named entity recognition. He
has built his models with the SVM and the maxi-
mum entropy. The idea is when the confidence of
the classifier remains at the same level or drops for
a number of consecutive iterations, the learning pro-
cess should terminate.
Laws and Schu?tze (2008) suggested three crite-
ria -minimal absolute performance, maximum pos-
sible performance, and convergence- to stop active
learning for name entity recognition using the SVM
model with the uncertainty sampling method. In
minimal absolute performance, a threshold is pre-
defined by the user; and then the classifier esti-
mates its own performance by using only the unla-
beled reference test set. Since there is no available
7
labeled test set, the evaluation performance is not
possible. The maximum possible performance is a
confidence-based stopping criterion in which active
learning is stopped where the optimal performance
of the classifier is achieved. Again, in this approach
there is no labeled test data to evaluate the perfor-
mance. The convergence criterion is met when more
examples from the pool of unlabeled data do not
contribute more information to the classifier?s per-
formance, so that the classifier has reached its maxi-
mum performance. Laws and Schu?tze computed the
convergence as the gradient of the classifier?s esti-
mated performance or uncertainty.
Tomanek and Hahn (2008) proposed a stopping
criterion based on the performance of the classi-
fier without requiring a labeled gold standard for a
committee-based active learning on the name en-
tity recognition application. In their criterion, they
approximated the progression of the learning curve
based on the disagreement among the committee
members. They have used the validation set agree-
ment curve as an adequate approximation for the
progression of the learning curve. This curve was
based on the data in each active learning iteration
that makes the agreement values comparable be-
tween different active learning iterations.
Bloodgood and Vijay-Shanker (2009) explained
three areas of stopping active learning that should
be improved: applicability (restricting the usage in
certain situation), lack of aggressive stopping (find-
ing the stopping points which are too far, so more
examples than necessary are annotated), instability
(well working of a method on some data set but not
the other data set). Further, they presented a stop-
ping criterion based on stabilizing predictions that
addresses each of the above three areas and provides
a user-adjustable stopping behavior. In this method,
the prediction of active learning was tested on exam-
ples which do not have to be labeled and it is stopped
when the predictions are stabilized. This criterion
was applied to text classification and named entity
recognition tasks using the SVM and the maximum
entropy models.
6 Summary and Future Work
In this paper, after a brief overview of frame seman-
tics and active learning scenarios and query meth-
ods, we performed the frame assignment in the pool-
based active learning with the uncertainty sampling
method. To this end, we chose 14 frequent targets
from FrameNet data set for our task.
One of the properties of active learning is its itera-
tivness which should be stopped when the classifier
has reached its maximum performance. Reaching
this point is very difficult; therefore, we proposed
a stopping criterion which stops active learning in a
near-optimum point. This stopping criterion is based
on the confidence score of the classifier on the extra
unlabeled data such that it uses the variance of the
classifier?s confidence score for the predicted labels
of a certain number of samples selected in each it-
eration. The advantage of this criterion is that there
is no need to the labeled gold standard data and test-
ing the performance of the classifier in each itera-
tion. Based on this idea, we proposed a model which
is satisfied by n sequential decrease on a variance
curve. The original model is expanded by setting a
threshold m on the amount of the decrement of vari-
ance in n sequential iterations. We believe that our
proposed criterion can be applied to any active learn-
ing setting based on uncertainty sampling and it is
not limited to the frame assignment.
To find out how effective our model is, we com-
pared the achieved results of our variance model
with the maximum performance of the classifier and
we found that 92.66% of the performance is kept in
the test data. In the extended variance model, we
achieved a higher performance of the classifier in
which 93.67% of the performance is kept.
For the future word, while in our current re-
search the learner selects 5 instances in each itera-
tion, this number could be different and investiga-
tion is needed to find out how much our proposed
criterion depends on the K. The other possibility to
expand our proposed model is using the variance of
the classifier?s confidence score for the predicted la-
bels of the whole unlabeled data in each iteration and
not the selected samples.
7 Acknowledgments
The author?s special gratitude goes to Caroline
Sporleder and Ines Rehbein at Saarland University
for their support and helpful comments in the re-
search. Masood Ghayoomi is funded by the Ger-
man research council DFG under the contract num-
ber MU 2822/3-1.
8
References
C. F. Baker and C. J. Fillmore J. B. Lowe. 1998. The
berkeley framenet project. In Proceedings of ACL,
pages 86?90, Montreal, QC.
J. Baldridge and M. Osborne. 2004. Active learning
and the total cost of annotation. In Proceedings of
EMNLP, pages 9?16, Barcelona, Spain.
M. Bloodgood and K. Vijay-Sarkar. 2009. A method
for stopping active learning based on stabilizing pre-
dictions and the need for user-adjustable stopping.
In 13th Conf. on Computational Natural Language
Learning, pages 39?47, Boulder, Colorado.
B. Busser and R. Morante. 2005. Designing an active
learning based system for corpus annotation. In Re-
vista de Procesamiento del Lenguaje Natural, num-
ber 35, pages 375?381.
D. Cohn, A. L. Atlas, and R. E. Ladner. 1994. Improving
generalization with active learning. Machine Learn-
ing, 15(2):201?221.
K. Erk and S. Pado. 2006. Shalmaneser - a toolchain for
shallow semantic parsing. In Proceedings of LREC,
Genoa, Italy.
C. J. Fillmore. 1968. The case for case. In Emmon W.
Bach and Robert T. Harms, editors, Universals in Lin-
guistic Theory, pages 1?88, New York. Rinehart and
Winston.
G. Haffari and A. Sarkar. 2009. Active learning for mul-
tilingual statistical machine translation. In Proceed-
ings of the 47th ACL-IJCNLP, Singapore.
F. Laws and H. Schu?tze. 2008. Stopping criteria for ac-
tive learning of named entity recognition. In Proceed-
ings of the 22nd CoLing, pages 465?472, Manchester.
D.D. Lewis and W. Gale. 1994. A sequential algo-
rithm for training text classifiers. In Proceedings of
the ACM SIGIR Conf. on Research and Development
in IR, pages 3?12.
I. Rehbein, J. Ruppenhofer, and J. Sunde. 2009. Majo
- a toolkit for supervised word sense disambiguation
and active learning. In Proceedings of the 8th Int.
Workshop on Treebanks and Linguistic Theories, Mi-
lan, Italy.
G. Schohn and D. Cohn. 2000. Less is more: Active
learning with support vector machines. In Proceed-
ings of 17th Int. Conf. on Machine Learning, Stanford
University.
B. Settles. 2009. Active learning literature survey. Com-
puter Sciences Technical Report 1648, University of
Wisconsin?Madison.
C. A. Thompson, M.E. Califf, and R.J. Mooney. 1999.
Active learning for natural language parsing and in-
formation extraction. In Proceedings of the 16th Int.
Conf. on Machine Learning, pages 406?414.
K. Tomanek and U. Hahn. 2008. Approximating learn-
ing curves for active-learning-driven annotation. In
6th Int. Language Resources and Evaluation Confer-
ence, pages 1319?1324.
A. Vlachos. 2008. A stopping criterion for active learn-
ing. Journal of Computer, Speech and Language,
22(3):295?312.
J. Zhu and E. Hovy. 2007. Active learning for word sense
disambiguation with methods for addressing the class
imbalance problem. In Proceedings of the EMNLP-
CoNLL, pages 783?790, Prague.
J. Zhu, H. Wang, and E. Hovy. 2008. Learning a stop-
ping criterion for active learning for word sense dis-
ambiguation and text classification. In Proceedings of
the 3rd IJNLP, pages 366?372, Heydarabad, India.
9
