Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 678?682,
Dublin, Ireland, August 23-24, 2014.
Turku: Broad-Coverage Semantic Parsing with Rich Features
Jenna Kanerva
?
Department of Information
Technology
University of Turku
Finland
jmnybl@utu.fi
Juhani Luotolahti
?
Department of Information
Technology
University of Turku
Finland
mjluot@utu.fi
Filip Ginter
Department of Information
Technology
University of Turku
Finland
figint@utu.fi
Abstract
In this paper we introduce our system ca-
pable of producing semantic parses of sen-
tences using three different annotation for-
mats. The system was used to partic-
ipate in the SemEval-2014 Shared Task
on broad-coverage semantic dependency
parsing and it was ranked third with an
overall F
1
-score of 80.49%. The sys-
tem has a pipeline architecture, consisting
of three separate supervised classification
steps.
1 Introduction
In the SemEval-2014 Task 8 on semantic parsing,
the objective is to extract for each sentence a rich
set of typed semantic dependencies in three differ-
ent formats: DM, PAS and PCEDT. These formats
differ substantially both in the assignment of se-
mantic heads as well as in the lexicon of seman-
tic dependency types. In the open track of the
shared task, participants were encouraged to use
all resources and tools also beyond the provided
training data. To improve the comparability of the
systems, the organizers provided ready-to-use de-
pendency parses produced using the state-of-the-
art parser of Bohnet and Nivre (2012).
In this paper we describe our entry in the open
track of the shared task. Our system is a pipeline
of three support vector machine classifiers trained
separately for detecting semantic dependencies,
assigning their roles, and selecting the top nodes
of semantic graphs. In this, we loosely follow
the architecture of e.g. the TEES (Bj?orne et al.,
2012) and EventMine (Miwa et al., 2012) systems,
which were found to be effective in the structurally
?
These authors contributed equally.
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
related task of biomedical event extraction. Sim-
ilar classification approach is shown to be effec-
tive also in semantic parsing by e.g. Zhao et al.
(2009), the winner of the CoNLL?09 Shared Task
on Syntactic and Semantic Dependencies in Mul-
tiple Languages (SRL-only subtask) (Haji?c et al.,
2009), where semantic parsing is approached as a
word-pair classification problem and semantic ar-
guments and their roles are predicted simultane-
ously. In preliminary experiments, we also de-
veloped a joint approach to simultaneously iden-
tify semantic dependencies and assign their roles,
but found that the performance of the joint predic-
tion was substantially worse than for the current
pipeline approach. As the source of features, we
rely heavily on the syntactic parses as well as other
external resources such as vector space represen-
tations of words and large-scale syntactic n-gram
statistics.
In the following sections, we describe the three
individual classification steps of our semantic
parsing pipeline.
2 Detecting Semantic Dependencies
The first step of our semantic parsing pipeline
is to detect semantic dependencies, i.e. governor-
dependent pairs which has a semantic relation be-
tween them. The first stage covers only the identi-
fication of such dependencies; the labels describ-
ing the semantic roles of the dependents are as-
signed in a later stage.
The semantic dependencies are identified using
a binary support vector machine classifier from the
LIBSVM package (Chang and Lin, 2011). Each
possible combination of two tokens in the sen-
tence is considered to be a candidate for a seman-
tic dependency in both directions, and thus also
included as a training example. No beforehand
pruning of possible candidates is performed dur-
ing training. However, we correct for the over-
whelming number of negative training examples
678
by setting the weights of positive and negative ex-
amples used during training, so as to maximize the
unlabeled F
1
-score on the development set.
Increasing the recall of semantic dependency
detection can be beneficial for the overall perfor-
mance of the pipeline system, since a candidate
lost in the dependency detection stage cannot be
recovered later. We therefore tested the approach
applied, among others by Bj?orne et al. (2012),
whereby the dependency detection stage heavily
overgenerates candidates and the next stage in the
pipeline is given the option to predict a nega-
tive label, thus removing a candidate dependency.
In preliminary experiments we tried to explicitly
overgenerate the dependency candidates by alter-
ing the classifier threshold, but noticed that heavy
overgeneration of positive examples leads to a de-
creased performance in the role assigning stage.
Instead, the above-mentioned optimization of the
example weights during training results in a clas-
sifier which overgenerates positive examples by
4.4%, achieving the same objective and improving
the overall performance of the system.
Features used during the dependency identifi-
cation are derived from tokens and the syntactic
parse trees provided by the organizers. Our pri-
mary source of features are the syntactic trees,
since 73.2% of semantic dependencies have a cor-
responding undirected syntactic dependency in the
parse tree. Further, the syntactic dependency path
between the governor and the dependent is shorter
than their linear distance in 48.8% of cases (in
43.4% of cases the distance is the same). The final
feature set used in the identification is optimized
by training models with different combinations of
features and selecting the best combination based
on performance on the held-out development set.
Interestingly, the highest performance is achieved
with a rather small set of features, whose full list-
ing is shown in Table 1. The feature vectors are
normalized to unit length prior to classification
and the SVM regularization parameter c is opti-
mized separately for each annotation format.
3 Role Assignment
After the semantic governor-dependent pairs are
identified, the next step is to assign a role for
each pair to constitute a full semantic dependency.
This is done by training a multiclass support vec-
tor machine classifier implemented in the SVM-
multiclass package by Joachims (1999). We it-
Feature D R T
arg.pos X X
arg.deptype X X
arg.lemma X X
pred.pos X X X
pred.deptype X X X
pred.lemma X X X
pred.is predicate X X
arg.issyntaxdep X
arg.issyntaxgov X
arg.issyntaxsibling X
path.length X X
undirected path.deptype X X
directed path.deptype X X
undirected path.pos X X
extended path.deptype X X
simplified path.deptype with len X
simplified path.deptype wo len X
splitted undirected path.deptype X
arg.prev.pos X X
arg.next.pos X X
arg.prev+arg.pos X X
arg.next+arg.pos X X
arg.next+arg+arg.prev.pos X X
pred.prev.pos X X
pred.next.pos X X
pred.prev+pred.pos X X
pred.next+pred.pos X X
pred.next+pred+pred.prev.pos X X
linear route.pos X
arg.child.pos X
arg.child.deptype X
arg.child.lemma X
pred.child.pos X
pred.child.deptype X X
pred.child.lemma X
syntaxgov.child.deptype X
vector similarities X
n-gram frequencies X
pred.sem role X
pred.child.sem role X
pred.syntaxsibling.deptype X
pred.semanticsibling.sem role X
Table 1: Features used in the detection of semantic
dependencies (D), assigning their roles (R) and top
node detection (T). path refers to syntactic depen-
dencies between the argument and the predicate,
and linear route refers to all tokens between the
argument and the predicate. In top node detection,
where only one token is considered at a time, the
pred is used to represent that token.
679
erate through all identified dependencies, and for
each assign a role, or alternatively classify it as a
negative example. This is to account for the 4.4%
of overgenerated dependencies. However, the pro-
portion of negative classifications should stay rel-
atively low and to ensure this, we downsample the
number of negative examples used in training to
contain only 5% of all negative examples. The
downsampling ratio is optimized on the develop-
ment set using grid search and downsampled train-
ing instances are chosen randomly.
The basic features, shown in Table 1, follow the
same style as in dependency identification. We
also combine some of the basic features by creat-
ing all possible feature pairs in a given set, but do
not perform this with the full set of features. In the
open track, participants are also allowed to use ad-
ditional data and tools beyond the official training
data. In addition to the parse trees, we include also
features utilizing syntactic n-gram frequencies and
vector space similarities.
Google has recently released a large corpus
of syntactic n-grams, a collection of depen-
dency subtrees with frequency counts (Goldberg
and Orwant, 2013). The syntactic n-grams are
induced from the Google Books collection, a
350B token corpus of syntactically parsed text.
In this work we are interested in arcs, which
are (governor, dependent, syntactic relation)
triplets associated with their count.
For each governor-dependent pair, we generate
a set of n-gram features by iterating through all
known dependency types and searching from the
syntactic n-grams how many times (if any) the
governor-dependent pair with the particular de-
pendency type is seen. A separate feature is then
created for each dependency type and the counts
are encoded in feature weights compressed using
w = log
10
(count). This approach gives us an op-
portunity to include statistical information about
word relations induced from a very large corpus.
Information is captured also outside the particular
syntactic context, as we iterate through all known
dependency types during the process.
Another source of additional data used in role
classification is a publicly available Google News
vector space model
1
representing word similari-
ties. The vector space model is induced from the
Google News corpus with the word2vec software
(Mikolov et al., 2013) and negative sampling ar-
1
https://code.google.com/p/word2vec/
chitecture, and each vector have 300 dimensions.
The vector space representation gives us an oppor-
tunity to measure word similarities using the stan-
dard cosine similarity function.
The approach to transforming the vector repre-
sentations into features varies with the three dif-
ferent annotation formats. On DM and PAS, we
follow the method of Kanerva and Ginter (2014),
where for each role an average argument vector
is calculated. This is done by averaging all word
vectors seen in the training data as arguments for
the given predicate with a particular role. For each
candidate argument, we can then establish a set of
similarity values to each possible role by taking
the cosine similarity of the argument vector to the
role-wise average vectors. These similarities are
then turned into separate features, where the simi-
larity values are encoded as feature weights.
On PCEDT, preliminary experiments showed
that the best strategy to include word vectors into
classification is by turning them directly into fea-
tures, so that each dimension of the word vector
is represented as a separate feature. Thus, we it-
erate through all 300 vector dimensions and cre-
ate a separate feature representing the position and
value of a particular dimension. Values are again
encoded in feature weights. These features are cre-
ated separately for both the argument and the pred-
icate. The word vectors are pre-normalized to unit
length, so no additional normalization of feature
weights is needed.
Both the n-gram? and vector similarities?based
features give a modest improvement to the classi-
fication performance.
4 Detecting Top Nodes
The last step in the pipeline is the detection of
top nodes. A top node is the semantic head or
the structural root of the sentence. Typically each
sentence annotated in the DM and PAS formats
contains one top node, whereas PCEDT sentences
have on average 1.12 top nodes per sentence.
As in the two previous stages, we predict top
nodes by training a support vector machine clas-
sifier, with each token being considered a candi-
date. Because the top node prediction is the last
step performed, in addition to the basic informa-
tion available in the two previous steps, we are
able to use also predicted arguments as features.
Otherwise, the feature set used in top node detec-
tion follows the same style as in the two previous
680
LP LR LF UF
DM 80.94 82.14 81.53 83.48
PAS 87.33 87.76 87.54 88.97
PCEDT 72.42 72.37 72.40 85.86
Overall 80.23 80.76 80.49 86.10
Table 2: Overall scores of whole task as well as
separately for each annotation format in terms of
labeled precision (LP), recall (LR) and F
1
-score
(LF) as well as unlabeled F
1
-score (UF).
tasks, but is substantially smaller (see Table 1). We
also create all possible feature pairs prior to clas-
sification to simulate the use of a second-degree
polynomial kernel.
For each token in the sentence, we predict
whether it is a top node or not. However, in DM
and PAS, where typically only one top node is al-
lowed, we choose only the token with the maxi-
mum positive value to be the final top node. In
PCEDT, we simply let all positive predictions act
as top nodes.
5 Results
The primary evaluation measure is the labeled F
1
-
score of the predicted dependencies, where the
identification of top nodes is incorporated as an
additional dummy dependency. The overall se-
mantic F
1
-score of our system is 80.49%. The
prediction performance in DM is 81.53%, in PAS
87.54% and in PCEDT 72.40%. The top nodes
are identified with an overall F
1
-score of 87.05%.
The unlabeled F
1
-score reflects the performance
of the dependency detection in isolation from la-
beling task and by comparing the labeled and un-
labeled F
1
-scores from Table 2 we can see that the
most common mistake relates to the identification
of correct governor-dependent pairs. This is espe-
cially true with the DM and PAS formats where the
difference between labeled and unlabeled scores
is very small (1.9pp and 1.4pp), reflecting high
performance in assigning the roles. Instead, in
PCEDT the role assignment accuracy is substan-
tially below the other two and the difference be-
tween unlabeled and labeled F
1
-score is as much
as 13.5pp. One likely reason is the higher number
of possible roles defined in the PCEDT format.
5.1 Discussion
Naturally, our system generally performs better
with frequently seen semantic roles than roles that
are seen rarely. In the case of DM, the 4 most
common semantic roles cover over 87% of the
gold standard dependencies and are predicted with
a macro F
1
-score of 85.3%, while the remaining
35 dependency labels found in the gold standard
are predicted at an average rate of 49.4%. To
give this a perspective, the most common 4 roles
have on average 121K training instances, while the
remaining 35 roles have on average about 2000
training instances. For PAS, the 9 most common
labels, which comprise over 80% of all depen-
dencies in the gold standard data and have on av-
erage about 66K training instances per role, are
predicted with an F
1
-score of 87.6%, while the
remaining 32 labels have on average 4200 train-
ing instance and are predicted with an F
1
-score of
57.8%. The PCEDT format has the highest num-
ber of possible semantic roles and also lowest cor-
relation between the frequency in training data and
F
1
-score. For PCEDT, the 11 most common la-
bels, which cover over 80% of all dependencies in
the gold standard, are predicted with an F
1
-score
of 69.6%, while the remaining 53 roles are pre-
dicted at an average rate of 46.6%. The higher
number of roles also naturally affects the number
of training instances and the 11 most common la-
bels in PCEDT have on average 35K training in-
stances, while the remaining 53 roles have on av-
erage 1600 instances per role.
Similarly, the system performs better with se-
mantic arguments which are nearby the governor.
This is true for both linear distance between the
two tokens and especially for distance measured
by syntactic dependency steps. For example in the
case of DM, semantic dependencies shorter than
3 steps in the syntactic tree cover more than 95%
of the semantic dependencies in the gold standard
and have an F
1
-score of 75.1%, while the rest have
only 32.6%. The same general pattern is also evi-
dent in the other formats.
6 Conclusion
In this paper we presented our system used to
participate in the SemEval-2014 Shared Task on
broad-coverage semantic dependency parsing. We
built a pipeline of three supervised classifiers to
identify semantic dependencies, assign a role for
each dependency and finally, detect the top nodes.
In addition to basic features used in classifica-
tion we have shown that additional information,
such as frequencies of syntactic n-grams and word
681
similarities derived from vector space representa-
tions, can also positively contribute to the classifi-
cation performance.
The overall F
1
-score of our system is 80.49%
and it was ranked third in the open track of the
shared task.
Acknowledgments
This work was supported by the Emil Aaltonen
Foundation and the Kone Foundation. Computa-
tional resources were provided by CSC ? IT Cen-
ter for Science.
References
Jari Bj?orne, Filip Ginter, and Tapio Salakoski. 2012.
University of Turku in the BioNLP?11 shared task.
BMC Bioinformatics, 13(Suppl 11):S4.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1455?1465.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1?27:27.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
Syntactic-Ngrams over time from a very large cor-
pus of English books. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 1: Proceedings of the Main Conference and
the Shared Task: Semantic Textual Similarity, pages
241?247.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, et al. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?18.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Advances in Kernel Meth-
ods - Support Vector Learning, pages 169?184. MIT
Press.
Jenna Kanerva and Filip Ginter. 2014. Post-hoc ma-
nipulations of vector space models with application
to semantic role labeling. In Proceedings of the 2nd
Workshop on Continuous Vector Space Models and
their Compositionality (CVSC)@ EACL, pages 1?
10.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Workshop Proceedings of
International Conference on Learning Representa-
tions.
Makoto Miwa, Paul Thompson, John McNaught, Dou-
glas Kell, and Sophia Ananiadou. 2012. Extracting
semantically enriched events from biomedical liter-
ature. BMC Bioinformatics, 13(1):108.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009. Multilingual dependency learning:
a huge feature engineering method to semantic de-
pendency parsing. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 55?60.
682
Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 1?10,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Post-hoc Manipulations of Vector Space Models
with Application to Semantic Role Labeling
Jenna Kanerva and Filip Ginter
Department of Information Technology
University of Turku, Finland
jmnybl@utu.fi, figint@utu.fi
Abstract
In this paper, we introduce several vector
space manipulation methods that are ap-
plied to trained vector space models in a
post-hoc fashion, and present an applica-
tion of these techniques in semantic role
labeling for Finnish and English. Specifi-
cally, we show that the vectors can be cir-
cularly shifted to encode syntactic infor-
mation and subsequently averaged to pro-
duce representations of predicate senses
and arguments. Further, we show that it is
possible to effectively learn a linear trans-
formation between the vector representa-
tions of predicates and their arguments,
within the same vector space.
1 Introduction
Recently, there has been much progress in the de-
velopment of highly scalable methods for induc-
ing vector space representations of language. In
particular, the word2vec method (Mikolov et al.,
2013b) is capable of training on billions of tokens
in a matter of hours, producing high quality rep-
resentations. An exciting property exhibited by
the vector spaces induced using word2vec is that
they preserve a number of linguistic regularities,
lending themselves to simple algebraic operations
with the vectors (Mikolov et al., 2013c) and linear
mapping between different spaces (Mikolov et al.,
2013a). These can be seen as post-hoc operations
manipulating the vector space with the significant
advantage of not requiring a new task-specific rep-
resentation to be induced, as is customary.
In this paper, we will investigate several addi-
tional such methods. Firstly, we will show how
syntax information can be encoded by the circular
shift operation and demonstrate that such shifted
vectors can be averaged in a meaningful manner to
represent predicate arguments. And secondly, we
will show that linear transformations of the vec-
tor spaces can be successfully applied also within
a single vector space, to tasks such as transform-
ing the vector of a predicate into the vector of its
argument with a particular role.
To test the above-mentioned operations in an
extrinsic setting, we will develop these methods
within the context of the Semantic Role Label-
ing (SRL) task. Automatic Semantic Role Label-
ing is the process of identifying the semantic ar-
guments of predicates, and assigning them labels
describing their roles. A predicate and its argu-
ments form a predicate-argument structure, which
describes events such as who does what to whom,
when and where.
The SRL task is ?semantic? in its nature and
therefore suitable for the application and testing
of vector space representations and methods for
their manipulation. However, rather than merely
adding features derived from vector spaces into
an existing system, we will approach the develop-
ment from a different angle and test whether these
representations of words and the similarities they
induce can be used for predicate argument role as-
signment and predicate sense disambiguation as
the primary source of information, with little ad-
ditional features.
In addition to the standard English CoNLL?09
dataset, we will apply the methods also to Finnish
SRL, testing the applicability of word2vec and the
overall methodology that we will develop in this
paper to this highly inflective language. With its
considerably larger and sparser surface form lex-
icon, Finnish poses interesting challenges of its
own, and only little attention has been dedicated
to the application of distributional semantics meth-
ods specifically to Finnish. This is also partly due
to the lack of sufficiently sized corpora, which we
address in this work by using a 1.5B token corpus
of Internet Finnish.
In order to be able to test the proposed meth-
1
ods on SRL, we need to carry out not only role
labeling and predicate sense disambiguation, but
also argument detection. As a secondary theme,
we thus test whether dependency parse graphs in
the semantically motivated Stanford Dependen-
cies (SD) scheme can be used as-is to perform ar-
gument identification. We are especially interested
in this scheme as it is designed to capture seman-
tically contentful relations (de Marneffe and Man-
ning, 2008) and would thus appear to be the ideal
choice as the underlying syntactic representation
for SRL.
2 Data and Task Setting
Throughout the paper, we will use the exact same
task setting as in the CoNLL?09 Shared Task on
Syntactic and Semantic Dependencies in Multiple
Languages (Haji?c et al., 2009). The input of the
SRL system are automatically generated syntactic
parses and the list of predicate tokens to be con-
sidered in each sentence. For each of the predi-
cates, the SRL system is expected to predict the
sense of the predicate, identify all tokens which
are its arguments, and for each argument, iden-
tify its role. As the primary measure of perfor-
mance, we will use the semantic F-score defined
in the CoNLL shared task. This F-score is cal-
culated from the precision and recall of argument
identification (calculated in the obvious manner)
and also incorporates the sense of the predicate via
an additional ?dummy? argument. We use the of-
ficial implementation of the metric distributed on
the Shared Task site.
1
We will report our results on two SRL datasets:
the Finnish PropBank (Haverinen et al., 2013a)
and the English SRL dataset from the CoNLL?09
Shared Task. The Finnish PropBank is built on
top of the Turku Dependency Treebank (TDT), a
205K token corpus of general Finnish (Haverinen
et al., 2013b) annotated using the SD scheme, in-
cluding manually annotated conjunct propagation
and other dependency relations from the non-basic
layer of the scheme. These extended SD analyzes
are thus not strictly trees, rather they are directed
labeled graphs (see Figure 1). The Finnish Prop-
Bank has 22 different argument roles of which 7
are numbered core roles and 15 are modifier roles.
The Finnish data has 164,530 training tokens with
27,603 occurrences of 2,826 unique predicate-
1
http://ufal.mff.cuni.cz/conll2009-st/
scorer.html
He ate.01 lunch and then washed.01 dishes .<nsubj:A0 dobj:A1> <advmod:AM-TMP dobj:A1>
cc> conj>
<nsubj:A0 punct>
Figure 1: Extended Stanford Dependencies
scheme combined with PropBank annotation.
sense combinations. The English CoNLL data is
derived from the PropBank and NomBank corpora
(Palmer et al., 2005; Meyers et al., 2004) and it has
a total of 54 different argument roles. In addition
to the same 22 roles as Finnish, English also has
discontinuous variants for each role. The English
data has 958,024 training tokens with 178,988 oc-
currences of 15,880 unique predicate-sense com-
binations.
All Finnish results are reported on the test sub-
set of the Finnish PropBank, and have no previ-
ously published baseline to compare with. The
results we report for English are produced on the
official test section of the CoNLL?09 data and are
thus directly comparable to the official results re-
ported in the Shared Task.
In the test phase, we follow the Shared Task set-
ting whereby morphological and syntactic analy-
sis is predicted as well, i.e., no gold standard data
enters the system other than the tokenization and
the information of which tokens constitute predi-
cates. We produce the Finnish morphological and
syntactic analyses for the test set with the parsing
pipeline of Haverinen et al. (2013b), composed of
a morphological analyzer and tagger (Hal?acsy et
al., 2007; Pirinen, 2008; Lind?en et al., 2009), de-
pendency parser (Bohnet, 2010) and a machine-
learning based component for predicting the ex-
tended SD dependencies (Nyblom et al., 2013).
While the English data is provided with automati-
cally produced dependency parses, we are specifi-
cally interested in the SD scheme and therefore we
re-parse the corpus with the Stanford parser
2
tak-
ing a union of the base and collapsed dependency
outputs to match the Finnish data.
The vector space models used throughout this
paper are induced using the word2vec software
(skip-gram architecture with default parameters).
For Finnish, the model is trained on 1.5 billion to-
kens of Finnish Internet texts gathered from the
Common Crawl dataset.
3
The data was sentence-
2
Version 3.3.1, October 2013
3
http://commoncrawl.org/
2
split and tokenized using the OpenNLP
4
toolchain
trained on TDT, and processed in the same man-
ner as the above-mentioned test set. This gives us
the opportunity to build two models, one for the
word forms and the other for the lemmas. Both
Finnish models have 300 dimensions. For En-
glish, the vector representation is induced on the
union of the English Wikipedia (1.7B tokens) and
the English Gigaword corpus (4B tokens), the to-
tal training data size thus being 5.7 billion tokens.
5
Sentence splitting and tokenization was carried out
using the relevant modules from the BRAT pack-
age (Stenetorp et al., 2012).
6
The English model
has 200 dimensions.
3 Method
In this section, we will describe the methods de-
veloped for argument identification, argument role
labeling and predicate sense disambiguation, the
three steps that must be implemented to obtain a
full SRL system.
3.1 Argument identification
In a semantically-oriented dependency scheme,
such as SD, it can be expected that a notable
proportion of arguments (in the SRL sense) are
directly attached to the predicate, and argument
identification can be reduced to assuming that ?
with a limited number of systematic exceptions ?
every argument is a dependent of the predicate.
The most frequent case where the assumption does
not hold in Finnish are the copula verbs, which are
not analyzed as heads in the SD scheme. For En-
glish, a common case are the auxiliaries, which
govern the main verb in the CoNLL data and are
thus marked as arguments for other higher-level
predicates as well. In the SD scheme, on the other
hand, the main verb governs the auxiliary taking
also its place in the syntactic tree. Since the fo-
cus of this paper lies in role assignment, we do
not go beyond developing a simple rule set to deal
with a limited number of such cases. In Section 6,
we will contrast this simple argument identifica-
tion method to that of the winning CoNLL?09 sys-
tem and we will show that while for Finnish the
above holds surprisingly well, the performance on
the English data is clearly sub-optimal.
4
http://opennlp.apache.org/
5
We are thankful to Sampo Pyysalo for providing us with
the English word2vec model.
6
http://brat.nlplab.org
Finnish
eat + A1 AM-TMP
salted fish not until
eggs now
wheat bread again
nuts when
pickled cucumbers then
English
drive + A1 drive + AM-TMP
car immediately
truck morning
cars now
vehicle afternoon
tires finally
Table 1: Five most similar words for the given
average argument vectors. AM-TMP refers to the
temporal modifier role. Note that the average vec-
tors for Finnish modifier roles are estimated inde-
pendently from the predicates (see Section 3.4).
3.2 Role Classification
Our initial role classification algorithm is based on
calculating the vector representation of an ?aver-
age argument? with a given role. For every predi-
cate x and every role r, we calculate the represen-
tation of the average argument with the role as
A(x, r) =
?
(r,x,y)
y?
count
, (1)
where y? refers to the L2 normalized version of y,
and count to the number of training pairs that are
summed over. We are thus averaging the normal-
ized vectors of all words y seen in the training data
as an argument of the predicate x with the role r.
To establish the role for some argument y during
testing, we can simply choose the role whose av-
erage argument vector has the maximal similarity
to y, i.e.
argmax
r
sim(A(x, r), y), (2)
where sim(a, b) is the standard cosine similarity.
To gain an intuitive insight into whether the av-
erage argument vectors behave as expected, we
show in Table 1 the top five most similar words
to the average argument vectors for several roles
and predicates. When evaluated with the data sets
described in Section 2, this initial method leads to
61.32% semantic F-score for Finnish and 65.05%
for English.
3
3.3 Incorporating syntax
As we will demonstrate shortly, incorporating in-
formation about dependency relations can lead to
a substantial performance gain. To incorporate the
dependency relation information into the role clas-
sification method introduced above, we apply the
technique of circular shifting of vectors. This tech-
nique was previously used in the context of Ran-
dom Indexing (RI) to derive new vectors from ex-
isting ones in a deterministic fashion (Basile and
Caputo, 2012). In RI, the shift operation is how-
ever not used on the final vectors, but rather al-
ready during the induction of the vector represen-
tation.
Given a vector representation of an argument y,
we can encode the dependency relation of y and
its predicate by circularly shifting the vector of
y by an offset assigned separately to each possi-
ble dependency relation. The assignment is arbi-
trary, but such that no two relations are assigned
the same offset. We will denote this operation as
yd, meaning the vector y circularly shifted to
the right by the offset assigned to the dependency
relation d. For instance, circularly shifting a vec-
tor a = (1, 2, 3, 4, 5) to the right by an offset of 2
results in a2 = (4, 5, 1, 2, 3).
We can incorporate the dependency relations
when calculating the average vectors representing
arguments as follows:
A(x, r) =
?
(r,d,x,y)
y?d
count
, (3)
where (r, d, x, y) iterates over all predicate-
argument pairs (x, y) where y has the dependency
relation d and role r. The role of an argument in
the test phase is established as before, by taking
the role which maximizes the similarity to the av-
erage vector:
argmax
r
sim(A(x, r), yd) (4)
In the cases, where arguments are not direct de-
pendents of the predicate, we use zero as the shift
offset.
To motivate this approach and illustrate its im-
plications, consider the two sentences (1) The cat
chases the dog. (2) The dog chases the cat. In
the first sentence the dog is an object which cor-
responds to the theme role A1, whereas in the
second sentence it is a subject with the agent
role A0. The role labeling decision is, how-
ever, in both cases based on the similarity value
sim(A(chases, r), dog), predicting A1, which is
incorrect in the latter case. When we incorporate
the syntactic information by shifting the vector ac-
cording to its syntactic relation to the predicate,
we obtain two diverging similarity values because
dog  nsubj and dog  dobj are essentially two
different vectors. This leads to the correct predic-
tion in both cases.
Relative to the base method, incorporating
the syntax improves the semantic F-score from
61.32% to 66.23% for Finnish and from 65.05%
to 66.55% for English. For Finnish, the gain is
rather substantial, while for English we see only
a moderate but nevertheless positive effect. This
demonstrates that, indeed, the circular shifting op-
eration successfully encodes syntactic information
both into the average vectors A and the candidate
argument vectors y.
3.4 Core arguments vs. modifiers
In comparison to modifier roles, the assignment
of core (numbered) argument roles is consider-
ably more influenced by the predicate sense and
therefore must be learned separately, which we
also confirmed in initial experiments. The modi-
fier roles, on the other hand, are global in the sense
that they are not tied to any particular predicate.
This brings out an interesting question of whether
the modifier roles should be learned independently
of the predicate or not. We find that the best strat-
egy is to learn predicate-specific modifier vectors
in English and global modifier vectors in Finnish.
Another problem, particularly common in the
Finnish PropBank stems from the distinction be-
tween core roles and modifier roles. For instance,
for the predicate to move the argument meaning
the destination of the moving action has the core
role A2, while for a number of other predicates
which may optionally take a destination argument,
the directional modifier role AM-DIR would be
used. This leads to a situation where core argu-
ments receive a high score for a modifier role, and
modifier roles are over-predicted at the expense
of core argument roles. To account for this, we
introduce the following simple heuristics. If the
predicate lacks a core role r after prediction, iter-
ate through predicted modifier roles p
1
. . .p
n
and
change the prediction from p
i
to r if r has the max-
imum similarity among the core roles and the dif-
ference sim(p
i
, y) ? sim(r, y) is smaller than a
threshold value optimized on a held-out develop-
4
ment set distinct from the test set.
We observe a 2.05pp gain in Finnish when using
this method, whereas in English this feature is less
significant with an improvement of only 0.3pp.
3.5 Fall-back for unknown words
The above-mentioned techniques based purely on
vector representations with no additional features
fail if the vector space model lacks the argument
token which prevents the calculation of the nec-
essary similarities. To address this problem, we
build separately for each POS a ?generic? repre-
sentation by averaging the vectors of all training
data tokens that have the POS and occurred only
once. These vectors, representing a typical rare
word of a given POS, are then used in place of
words missing from the vector space model.
Another solution taking advantage from the
vector space representation is used in cases where
a predicate is not seen in the training data and
therefore we have no information about its argu-
ment structure. We query for predicates closest
to the unseen predicate and take the average argu-
ment vectors from the most similar predicate that
was seen during the training.
Together, these two techniques result in a mod-
est gain of approximately 1pp for both languages.
3.6 Sense classification
One final step required in SRL is the disambigua-
tion of the sense of the predicate. Here we ap-
ply an approach very similar to that used for role
classification, whereby for every sense of every
predicate, we calculate an average vector repre-
senting that sense. This is done as follows: For
every predicate sense, we average the vector rep-
resentations of all dependents and governors
7
of
all occurrences of that sense in the training data,
circularly shifted to encode their syntactic relation
to the predicate. To assign a sense to a predicate
during testing, we average the shifted vectors cor-
responding to its dependents and governors in the
sentence, and choose the sense whose average vec-
tor is the nearest. Using this approach, we obtain a
84.18% accuracy for Finnish and 92.68% for En-
glish, compared to 79.89% and 92.88% without
the syntax information. This corresponds to a sub-
stantial gain for Finnish but, surprisingly, a small
drop for English. For the rare predicates that are
7
Recall we use the extended SD scheme where a word can
have several governors in various situations.
not seen in the training data, we have no informa-
tion about their sense inventory and therefore we
simply predict the sense ?.01? which is the cor-
rect choice in 79.56% of the cases in Finnish and
86.64% in English.
4 Role Labeling with Linear
Transformations
As we discussed earlier, it was recently shown that
the word2vec spaces preserve a number of lin-
guistic regularities, and an accurate mapping be-
tween two word2vec-induced vector spaces can
be achieved using a simple linear transformation.
Mikolov et al. (2013a) have demonstrated that a
linear transformation trained on source-target lan-
guage word pairs obtained from Google Translate
can surprisingly accurately map word vectors from
one language to another, with obvious applications
to machine translation. It is also worth noting that
this is not universally true of all vector space rep-
resentation methods, as Mikolov et al. have shown
for example for Latent Semantic Analysis, which
exhibits this property to a considerably lesser ex-
tent. In addition to testing the applicability of the
word2vec method in general, we are specifically
interested whether these additional properties can
be exploited in the context of SRL. In particular,
we will test whether a similar linear vector space
transformation can be used to map the vectors of
the predicates onto those of their arguments.
More formally, for each role r, we will learn a
transformation matrix W
r
such that for a vector
representation x of some predicate, W
r
x will be
close to the vector representation of its arguments
with the role r. For instance, if x is the represen-
tation of the predicate (to) eat, we aim for W
A1
x
to be a vector similar to the vectors representing
edible items (role A1). The transformation can be
trained using the tuples (r, x, y) of predicate x and
its argument y with the role r gathered from the
training data, minimizing the error
?
(x,y)
?W
r
x? y?
2
(5)
over all training pairs (separately for each r). We
minimize the error using the standard stochastic
gradient descent method, whereby the transfor-
mation matrix is updated separately for each pair
(x, y) using the equation
W
r
?W
r
? (W
r
x? y)x
T
(6)
5
where  is the learning rate whose suitable value is
selected on the development set. The whole proce-
dure is repeated until convergence is reached, ran-
domly shuffling the training data after each round
of training.
Using the transformation, we can establish the
most likely role for the argument y of a predicate
x as
argmax
r
sim(W
r
x, y) (7)
where sim is the cosine similarity function, i.e. in
the exact same manner as for the average argument
method described in the previous section, with the
difference that the vector for the average argument
is not calculated directly from the training data,
but rather obtained through the linear transforma-
tion of the predicate vector.
As an alternative approach, we can also learn
the reverse transformation RW
r
such that RW
r
y
is close to x, i.e. the transformation of the argu-
ment y onto the predicate x. Note that hereRW
r
is
not the same asW
T
r
; we train this reverse transfor-
mation separately using the same gradient descent
method. We then modify the method for finding
the most likely role for an argument by taking the
average of the forward and reverse transformation
similarities:
argmax
r
sim(W
r
x, y) + sim(x,RW
r
y)
2
(8)
Note that we make no assumptions about the
vector spaces where x and y are drawn from; they
may be different spaces and they do not need to be
matched in their dimensionality either, as there is
no requirement that W and RW be square matri-
ces. In practice, we find that the best strategy for
both Finnish and English is to represent both the
predicates and arguments using the space induced
from word forms, however, we have also tested on
Finnish representing the predicates using the space
induced from lemmas and the arguments using a
space induced from word forms, with only mini-
mally worse results. This shows that the transfor-
mation does not degrade substantially even when
mapping between two different spaces.
With this strategy, we reach an F-score of
62.71% in Finnish and 63.01% in English. These
results are on par with the scores obtained with
the average argument method, showing that a lin-
ear transformation is effective also in this kind of
problems.
To incorporate syntax information, we train
transformation matrices W
r,d
and RW
r,d
for each
dependency relation d rather than relying on the
circular shift operation which cannot be captured
by linear transformations.
8
As some combinations
of r and d may occur only in the test data, we use
the matrices W
r
and RW
r
as a fall-back strategy.
In testing, we found that even if the (r, d) com-
bination is known from the training data, a small
improvement can be obtained by taking the aver-
age of the similarities with and without syntactic
information as the final similarity. Incorporating
these techniques into the basic linear transforma-
tion improves the semantic F-score from 62.71%
to 65.88% for Finnish and from 63.01% to 67.04%
for English. The improvement for both languages
is substantial.
5 Supervised classification approach
In the previous sections, we have studied an ap-
proach to SRL based purely on the vector space
representations with no additional features. We
have addressed the choice of the argument role by
simply assigning the role with the maximum sim-
ilarity to the argument. To test the gain that could
be obtained by employing a more advanced tech-
nique for aggregating the scores and incorporating
additional features, we train a linear multi-class
support vector machine to assign the role to every
detected argument. As features, we use the simi-
larity values for each possible role using the best
performing method for each language,
9
the sense
of the predicate, and ? separately for the predi-
cate and the argument ? the token itself, its POS,
morphological tags, every dependency relation to
its governors, and every dependency relation to
its dependents. The similarities are encoded as
feature weights, while all other features are bi-
nary. We use the multi-class SVM implementa-
tion from the SVM-multiclass package (Joachims,
1999), setting the regularization parameter on the
development set using a grid search.
For both languages, we observe a notable gain
in performance, leading to the best scores so far.
In Finnish the improvement in F-score is from
66.23% to 73.83% and in English from 67.04%
to 70.38%. However, as we will further discuss in
Section 6, in Finnish the contribution of the simi-
larity features is modest.
8
Note that this does not affect the overall computational
cost, as the total number of training examples remains un-
changed and the transformation matrices are small in size.
9
Average vectors for Finnish and transformation for En-
glish (Table 2).
6
Finnish English
Average vectors
full method 66.23 66.55
?modifier vs. core role 64.18 66.25
?syntax 61.32 65.05
Linear transformation
full method 65.88 67.04
?syntax 62.71 63.01
Supervised classification
full method 73.83 70.38
only similarity features 64.21 65.89
?similarity features 73.54 67.51
?lexical features 65.51 58.42
Table 2: Overview of main results and a feature
ablation study. Modifier vs. core role refers to the
algorithm presented in Section 3.4. In the super-
vised classification part, -lexical features refers to
the removal of features based on word forms, pred-
icate sense and role similarities.
6 Results and discussion
All results discussed throughout Sections 3 to 5
are summarized in Table 2, which also serves as
a coarse feature ablation study. Overall, we see
that the average vector and linear transformation
methods perform roughly on par, with the aver-
age vector method being slightly better for Finnish
and slightly worse for English. Both vector space-
based methods gain notably from syntax informa-
tion, confirming that the manner in which this in-
formation is incorporated is indeed meaningful.
Adding the SVM classifier on top of these two
methods results in a substantial further raise in per-
formance, demonstrating that to be competitive on
SRL, it is necessary to explicitly model also ad-
ditional information besides the semantic similar-
ity between the predicate and the argument. This
is particularly pronounced for Finnish where the
present SVM method does not gain substantially
from the similarity-based features, while English
clearly benefits. To shed some light on this differ-
ence, we show in Table 3 the oracle accuracy of
role labeling for top-1 through top-10 roles as or-
dered by their similarity scores. The performance
on English is clearly superior to that on Finnish.
An important factor may be the fact that ? in
terms of token count ? the CoNLL?09 English
training size is nearly six times that of the Finnish
PropBank and the English vector space model was
induced on a nearly four times larger text corpus.
Finnish English
n Recall n Recall
1 58.23 1 67.39
2 68.29 2 82.82
3 74.30 3 88.49
4 78.71 4 91.58
5 82.21 5 93.20
6 84.74 6 94.29
7 87.04 7 94.91
8 88.98 8 95.31
9 90.76 9 95.65
10 92.05 10 95.86
Table 3: A study of how many times (%) the cor-
rect role is among the top n most similar roles
when the arguments are known in advance. Left:
Similarities taken from the average vector method
on Finnish. Right: Similarities from the linear
transformation method on English.
Finnish English
Average vectors 66.23 / 89.89 66.55 / 79.57
Linear transf. 65.88 / 89.92 67.04 / 80.85
Supervised 73.83 / 89.29 70.38 / 78.71
Table 4: Overall results separately for all main
methods (labeled/unlabeled semantic F-score).
Returning to our original question of whether
the SD scheme can be used as-is for argument
identification, we show in Table 4 the unlabeled
F-scores for the main methods. These scores re-
flect the performance of the argument identifica-
tion step in isolation. While Finnish approaches
90% which is comparable to the best systems in
the CoNLL?09 task, English lags behind by over
10pp. To test to what extent the results would be
affected if a more accurate argument identification
system was applied, we used the output of the win-
ning (for English) CoNLL?09 Shared Task system
(Zhao et al., 2009a) as the argument identifica-
tion component, while predicting the roles with
the methods introduced so far. The results are
summarized in Table 5, where we see a substan-
tial gain for all the methods presented in this pa-
per, achieving an F-score of 82.33%, only 3.82pp
lower than best CoNLL?09 system. These results
give a mixed signal as to whether the extended SD
scheme is usable nearly as-is for argument identi-
fication (Finnish) or not (English). Despite our ef-
forts, we were unable to pinpoint the cause for this
difference, beyond the fact that the Finnish Prop-
7
Semantic F-score
CoNLL?09 best 86.15 / 91.97
Average vectors 73.12 / 91.97
Linear transformation 74.41 / 91.97
Supervised classif. 82.33 / 91.97
Table 5: Performance of suggested methods with
argument identification from the top-performing
CoNLL?09 system (labeled/unlabeled F-score).
Bank was originally developed specifically on top
of the SD scheme, while the English PropBank
and NomBank corpora were not.
7 Related work
While different methods have been studied to
build task specific vector space representations,
post-hoc methods to manipulate the vector spaces
without retraining are rare. Current SRL systems
utilize supervised machine learning approaches,
and typically a large set of features. For instance,
the winning system in the CoNLL?09 shared task
(SRL-only) introduces a heavy feature engineer-
ing system, which has about 1000 potential fea-
ture templates from which the system discovers
the best set to be used (Zhao et al., 2009b). Word
similarities are usually introduced to SRL as a
part of unsupervised or semi-supervised meth-
ods. For example, Titov and Klementiev (2012)
present an unsupervised clustering method ap-
plying word representation techniques, and De-
schacht and Moens (2009) used vector similarities
to automatically expand the small training set to
build semi-supervised SRL system. Additionally,
Turian et al. (2010) have shown that word repre-
sentations can be included among the features to
improve the performance of named entity recogni-
tion and chunking systems.
8 Conclusions
We set out to test two post-hoc vector space ma-
nipulation techniques in the context of semantic
role labeling. We found that the circular shift op-
eration can indeed be applied also to other vector
representations as a way to encode syntactic infor-
mation. Importantly, the circular shift is applied to
a pre-existing vector space representation, rather
than during its induction, and is therefore task-
independent. Further, we find that such shifted
vectors can be meaningfully averaged to represent
predicate senses and arguments.
We also extended the study of the linear trans-
formation between two vector spaces and show
that the same technique can be used also within
a single space, mapping the vectors of predicates
onto the vectors of their arguments. This map-
ping produces results that are performance-wise
on par with the average vectors method, demon-
strating a good generalization ability of the lin-
ear mapping and the underlying word2vec vector
space representation. Here it is worth noting that
? if we gloss over some obvious issues of am-
biguity ? the mapping between two languages
demonstrated by Mikolov et al. is conceptually a
one-to-one mapping, at least in contrast to the one-
to-many nature of the mapping between predicates
and their arguments. These results hint at the pos-
sibility that a number of problems which can be re-
duced to the ?predict a word given a word? pattern
may be addressable with this simple technique.
With respect to the application to SRL, we have
shown that it is possible to carry out SRL based
purely on the vector space manipulation meth-
ods introduced in this paper, outperforming sev-
eral entries in the CoNLL-09 Shared Task. How-
ever, it is perhaps not too surprising that much
more is needed to build a competitive SRL sys-
tem. Adding an SVM classifier with few relatively
simple features derived from the syntactic analy-
ses in addition to features based on vector similar-
ities, and especially adding a well-performing ar-
gument identification method, can result in a sys-
tem close to approaching state-of-the-art perfor-
mance, which is encouraging.
As future work, it will be interesting to study to
which extent SRL, and similar applications would
benefit from addressing the one-to-many nature of
the underlying problem. While for some predi-
cates the arguments likely form a cluster that can
be represented as a single average vector, for other
predicates, such as to see, it is not the case. Find-
ing methods which allow us to model this property
of the problem will constitute an interesting direc-
tion with broader applications beyond SRL.
Acknowledgements
This work has been supported by the Emil Aal-
tonen Foundation and Kone Foundation. Com-
putational resources were provided by CSC ? IT
Center for Science. We would also like to thank
Sampo Pyysalo and Hans Moen for comments and
general discussion.
8
References
Pierpaolo Basile and Annalina Caputo. 2012. Encod-
ing syntactic dependencies using Random Indexing
and Wikipedia as a corpus. In Proceedings of the
3rd Italian Information Retrieval (IIR) Workshop,
volume 835, pages 144?154.
Bernd Bohnet. 2010. Very high accuracy and fast de-
pendency parsing is not a contradiction. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, pages 89?97. Association for
Computational Linguistics.
Koen Deschacht and Marie-Francine Moens. 2009.
Semi-supervised semantic role labeling using the la-
tent words language model. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 1-Volume 1, pages
21?29. Association for Computational Linguistics.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, et al. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?18. Associa-
tion for Computational Linguistics.
P?eter Hal?acsy, Andr?as Kornai, and Csaba Oravecz.
2007. HunPos: an open source trigram tagger. In
Proceedings of the 45th annual meeting of the ACL
on interactive poster and demonstration sessions,
pages 209?212. Association for Computational Lin-
guistics.
Katri Haverinen, Veronika Laippala, Samuel Kohonen,
Anna Missil?a, Jenna Nyblom, Stina Ojala, Timo Vil-
janen, Tapio Salakoski, and Filip Ginter. 2013a.
Towards a dependency-based PropBank of general
Finnish. In Proceedings of the 19th Nordic Confer-
ence on Computational Linguistics (NoDaLiDa?13),
pages 41?57.
Katri Haverinen, Jenna Nyblom, Timo Viljanen,
Veronika Laippala, Samuel Kohonen, Anna Mis-
sil?a, Stina Ojala, Tapio Salakoski, and Filip Ginter.
2013b. Building the essential resources for Finnish:
the Turku Dependency Treebank. Language Re-
sources and Evaluation, pages 1?39.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Advances in Kernel Meth-
ods - Support Vector Learning, pages 169?184. MIT
Press.
Krister Lind?en, Miikka Silfverberg, and Tommi Piri-
nen. 2009. HFST tools for morphology ? an effi-
cient open-source package for construction of mor-
phological analyzers. In State of the Art in Com-
putational Morphology, volume 41 of Communica-
tions in Computer and Information Science, pages
28?47.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1?8. Coling 2008 Organiz-
ing Committee.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The NomBank project:
An interim report. In HLT-NAACL 2004 Workshop:
Frontiers in Corpus Annotation, pages 24?31.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013a. Exploiting similarities among languages
for machine translation. CoRR (arxiv.org),
abs/1309.4168.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111?3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746?751. Associa-
tion for Computational Linguistics, June.
Jenna Nyblom, Samuel Kohonen, Katri Haverinen,
Tapio Salakoski, and Filip Ginter. 2013. Pre-
dicting conjunct propagation and other extended
Stanford Dependencies. In Proceedings of the In-
ternational Conference on Dependency Linguistics
(Depling 2013), pages 252?261.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Tommi Pirinen. 2008. Suomen kielen ?a?arellistilainen
automaattinen morfologinen j?asennin avoimen
l?ahdekoodin resurssein. Master?s thesis, University
of Helsinki.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. BRAT: a web-based tool for nlp-assisted
text annotation. In Proceedings of the Demonstra-
tions at the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 102?107. Association for Computational Lin-
guistics.
Ivan Titov and Alexandre Klementiev. 2012. A
Bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 12?22. Association for
Computational Linguistics.
9
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Hai Zhao, Wenliang Chen, Jun?ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009a. Multilin-
gual dependency learning: Exploiting rich features
for tagging syntactic and semantic dependencies.
In Proceedings of the Thirteenth Conference on
Computational Natural Language Learning: Shared
Task, pages 61?66. Association for Computational
Linguistics.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009b. Multilingual dependency learning:
a huge feature engineering method to semantic de-
pendency parsing. In Proceedings of the Thirteenth
Conference on Computational Natural Language
Learning: Shared Task, pages 55?60. Association
for Computational Linguistics.
10
