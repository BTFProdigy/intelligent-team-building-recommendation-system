Nanjing  Normal  University  Segmenter  
for  the  Fourth  SIGHAN  Bakeoff 
Xiaohe CHEN, Bin LI, Junzhi LU, Hongdong NIAN, Xuri TANG 
Nanjing Normal University, 
122, Ninghai Road, Nanjing, P. R. China, 210097 
chenxiaohe5209@msn.com,gothere@126.com, 
lujunzhi@gmail.com,nianhong-dong@hotmail.com, 
tangxuriyz@hotmail.com 
 
 
 
Abstract 
This paper expounds a Chinese word seg-
mentation system built for the Fourth 
SIGHAN Bakeoff. The system participates 
in six tracks, namely the CityU Closed, 
CKIP Closed, CTB Closed, CTB Open, 
SXU Closed and SXU Open tracks. The 
model of Conditional Random Field is used 
as a basic approach in the system, with at-
tention focused on the construction of fea-
ture templates and Chinese character cate-
gorization. The system is also augmented 
with some post-processing approaches such 
as the Extended Word String, model inte-
gration and others. The system performs 
fairly well on the 5 tracks of the Bakeoff. 
1 Introduction 
The Nanjing Normal University (NJNU) team par-
ticipated in CityU Closed, CKIP Closed, CTB 
Closed, CTB Open, SXU Closed, SXU Open 
tracks in the WS bakeoff. The system employed in 
the Bakeoff is based mainly on the model of CRF, 
optimized with some pre-processing and post-
processing methods. The team has focused its at-
tention on the construction of feature templates, 
Chinese character categorization, the use of Ex-
tended Word String and the integration of different 
segmentation models in the hope of achieving bet-
ter performance in both IVs?In Vocabulary 
words? and OOVs (Out Of Vocabulary words). 
Due to time limitations, some of these methods are 
still not fully explored. However, the Bakeoff re-
sults show that the performance of the overall sys-
tem is fairly satisfactory.  
The paper is organized as follows: section 2 
gives a brief description of the system; section 3 
and 4 are devoted to the discussion of the results of 
closed test and open test; a conclusion is given to 
comment on the overall performance of the system. 
2 System Description 
Conditonal Ramdom Field (CRF) has been widely 
used by participants in the basic tasks of NLP since 
Peng(2004). In both SIGHAN 2005 and 2006 
Bakeoffs CRF-based segmenters prove to have a 
better performance over other models. We have 
also chosen CRF as the basic model for the task of 
segmentation and uses the package CRF++ devel-
oped by Taku Kudo1. Some post-processing op-
timizations are also employed to improve the over-
all segmentation performance. The general descrip-
tion of the system is illustrated in Figure 1. The 
basic segmenter and post-processing are explained 
in the next two sections. 
2.1 Basic Segmenter 
As in many other segmentation models, our system 
also treats word segmentation as a task of classifi-
cation problem. During the experiment of the 
model, two aspects are taken into consideration, 
namely tag set and feature template. The 6-tag 
(Table 1) set proposed in Zhao(2006) is employed 
to mark various character position status in a Chi-
nese word. The feature template (Table 2) consid-
                                                 
1 Package CRF++, version 0.49, available at 
http://crfpp.sourceforge.net. 
115
Sixth SIGHAN Workshop on Chinese Language Processing
ers three templates of character features and three 
templates of character type features. The introduc-
tion of character type (Table 3) is based on the ob-
servation that many segmentation errors are caused 
by different segmentation standards among differ-
ent corpora, especially between Traditional Chi-
nese corpora and Simplified Chinese Corpora. 
 
 
Figure 1: Flow Chat 
 
Status Tag 
begin B 
2nd B2 
3rd B3 
middle M 
end E 
single S 
Table 1:6-tag Set 
 
Table 2: Feature Templates in Close Test 
 
Character Type Example 
Chinese Character ? ? 
Serial Number ??  ?? 
Roman Number ??? 
Aribic Number 12?? 
Chinese Number ???? 
Ganzhi ???? 
Foreign Character ??? 
National Pronunciation Letters ??? 
Sentence Punctuation ????  
Hard Punctuation \t\r\n 
Punctuation ??-?'' 
Dun ?? 
Dot1 ?? 
Dot2 .? 
Di ? 
At @ 
Other Character ?? 
Table 3:Character Type 
2.2 Post-Processing 
Two methods are used in post-processing to opti-
mize the results obtained from basic segmenter. 
The first is the binding of digits and English Char-
acters. The second is the use of extended word 
string to solve segmentation ambiguity. 
2.2.1 Binding Digits and Roman Letters 
Digits (ranging from ?0? to ?9?) are always bound 
as a word in Chinese corpora, while roman letters 
are treated differently in different corpora, some 
adding a full-length blank between the letters, 
some not. The system employs rule-based ap-
proach to bind both digits and roman letters. We 
also submitted two segmentation results for the 
Bakeoff, please refer to section 3.2 for discussion 
of these results. 
2.2.2 Extended Word String (EWS) Approach 
The CRF model performs well in segmenting IV 
word strings in general, but not in all contexts. Our 
system thus uses a memory based method, which 
is named as Extended Word String approach, to 
prevent CRF from making such error. All the Chi-
nese word strings, which are of character length 
from 2 to 10 and appear more than two times, are 
stored in a hash table, together with information of 
their segmentation forms. An example of EWS is 
given in Table 5. If the same character string ap-
pears in the test data, the system can easily re-
segment them by querying the hash table. If the 
query finds that the character string has only one 
segmentation form and checking shows that the 
string has no overlapping ambiguity with its left or 
right word, the segmentation of the string is then 
modified according to the stored segmentation type. 
Our experiment shows that the approach can pro-
Type Feature Function 
Char  
Unigram 
Cn, n=-2, 
-1, 0, 1, 2 
Character in position n to 
the current character 
Char  
Bigram 
CnCn+1, 
n=-1,0 
Previous(next) character 
and current character 
Char Jump C-1 C1 
Previous character and 
next character 
CharType 
Unigram 
Tn, 
n=-1, 0, 1 
Type of previous (current, 
next) character 
CharType 
Bigram 
TnTn+1, 
n=-1,0 
Type of previous character 
and next character 
CharType 
Jump T-1 T1 
Type of previous character 
and next character 
Input Character Strings 
Basic Segmenter (CRF Tagging) 
Post-processing 
Output Word Strings 
116
Sixth SIGHAN Workshop on Chinese Language Processing
mote the F-measure by 0.2% to 1% on different 
tracks. 
 
Table 5: Example of EWS 
3 Evaluation Results on Closed Test 
3.1 CKIP Closed Test 
In CKIP Closed Test, another kind of post process-
ing is used for OOVs. Examination on the output 
from basic segmenter shows that some OOVs iden-
tified by CRFs are not OOV errors, but IV errors. 
Sometimes it can not always segment the same 
OOV correctly in different context. For example, 
the person name ????? appears three times in 
the test, but it is only correctly detected twice, and 
for once it is wrongly detected. Our approach is to 
re-segment the OOVs string (with its left and right 
word) twice. Firstly the string is segmented using 
the training data wordlist, followed by a second 
segmentation using the OOV wordlist recognized 
by the Basic Segmenter. The result with the mini-
mum number of words is accepted.  
Example: 
Basic Seg Output?/?/??/??/ 
OOV Adjusting?    /?/???/?/ 
Basic Seg Output?/??/??/?/ 
OOV Adjusting?    /?/??/??/ 
With the OOV Adjusting Approach mentioned 
above, we got the third place in the track (Table 6). 
But when we use it on other corpora, the method 
does not promote the performance. Rather, it low-
ers the performance score. The reason is still not 
clear. 
 
System?rank? F Foov Fiv 
Best(1/21) 0.9510 0.7698 0.9667 
Njnu(3/21) 0.9454 0.7475 0.9637 
Table 6: CityU Closed Test 
3.2 CKIP and CTB Closed Test 
In CKIP Closed Test, only the basic segmenter 
introduced in section 2 is used. Two segmentation 
results, namely a and b (Table 7 and 8) are submit-
ted for the Bakeoff. Result a binds the roman let-
ters as a word, while result b does not. The scores 
of the two results show that the approach is not 
stable in terms of score. We suggest that corpora 
submitted for evaluation purposes should pay more 
attention to non-Chinese word tagging and comply 
with the request of Bakeoff organizers. 
 
System?rank? F Foov Fiv 
Best(1/19) 0.9470 0.7524 0.9623 
Njnu a(6/19) 0.9378 0.6948 0.9580 
Njnu b(9/19) 0.9204 0.6341 0.9452 
Table 7: CKIP Closed Test 
 
System?rank? F Foov Fiv 
Best(1/26) 0.9589 0.7745 0.9697 
Njnu a(9/26) 0.9498 0.7152 0.9645 
Njnu b(7/26) 0.9499 0.7142 0.9647 
Table 8: CTB Closed Test 
3.3 SXU Closed Test 
Four results (a, b, c and d) are submitted for this 
track (Table 9). Results a and b are dealt in the 
same way as described in section 3.2. Result c is 
obtained by incorporating results from a memory-
based segmenter. The memory-based segmenter is 
mainly based on memory-based learning proposed 
by Daelemans(2005). We tested it on the training 
data with 90% as training data and 10% as testing 
data. The result shows that performance is im-
proved. However, when the method is applied on 
the Bakeoff test data, the performance is lowered. 
The reason is not identified yet. 
Result d was based on result c. It incorporates 
OOV words recognized by the system introduced 
in (Li & Chen, 2007) in the post-processing stage. 
Based on suffix arrays, Chinese character strings 
with mutual information value above 8.0 are auto-
matically extracted as words without any manual 
operation. We can see from table 9 that the F-
measure of result d improved and Foov of d got 2rd 
place in the test. And it is likely to get higher score 
if we combine it with result a. 
 
System?rank? F Foov Fiv 
Best(1/29) 0.9623 0.7292 0.9752 
Njnu a(9/29) 0.9539 0.6789 0.9702 
Njnu b(10/29) 0.9538 0.6778 0.9701 
Njnu c(15/29) 0.9526 0.6793 0.9688 
Njnu d(14/29) 0.9532 0.6817 0.9694 
Table 9: Sxu Closed Test 
 
EWS Seg Form Freq 
??? /?/??/ 4 
117
Sixth SIGHAN Workshop on Chinese Language Processing
4 Evaluation Results on Open Test 
4.1 Methods 
More features and resources are used in open test, 
mainly applied in the modification of feature tem-
plates. Besides the features used in the close test, 
we add to feature templates more information 
about Chinese characters, such as the Chinese radi-
cals (????), tones (5 tones), and another 6 Boo-
lean values for each Chinese character. The 6 Boo-
lean values indicate respectively whether the char-
acter is of Chinese surnames (????), or of Chi-
nese names (????), or of characters used for 
western person name translation (????), or of 
character used for English location name transla-
tion(????), or of affixes (??-?,?-??), or of sin-
gle character words (????). The feature tem-
plates constructed in this way is given in Table 10. 
 
Type Feature Function 
Char  
Unigram 
Cn, 
 n=-1,0,1 
The prevoius (current, 
next) character 
Char  
Bigram 
Cn Cn+1,  
n=-1,0 
The previous(next) charac-
ter and current character 
Char Jump C-1 C1 
The previous character 
and next character 
CharType 
Unigram T0 
The type of the current, 
next character 
CharType 
Trigram T-1 T0T1 
The type of the previous, 
current and next character 
Char 
Information 
Unigram 
nT0 , 
 n=1,?,6 
The 6 information of the 
current, next character 
Char 
Information 
Trigram 
nnn TTT 101? , 
 n=1,?,6 
The 6 information of the 
previous, current and next 
character 
Table10: Feature Templates for Open Test 
 
In the post-processing stage, we also add a Chi-
nese idiom dictionary (about 27000 items) to help 
increase the OOV word recall. 
4.2 Results 
In SXU open test, we submitted 3 results (a, b and 
c), but only a achieves the 4th rank in F-measure 
(Table 11). Features and resources added to the 
system turns out not to be of much use in the task, 
compared with our score on the closed test. 
Result b, c and all the results in CTB open test 
submitted have errors due to our pre-processing 
stage with CRF. Thus, the scores of them are very 
low, and some are even lower than our scores in 
closed test (see table 12). 
 
System?rank? F Foov Fiv 
Best(1/9) 0.9735 0.8109 0.9820 
Njnu a(4/12) 0.9559 0.6925 0.9714 
Table 11: SXU Open Test 
 
System?rank? F Foov Fiv 
Best(1/12) 0.9920 0.9654 0.9936 
Njnu a(9/12) 0.9346 0.6341 0.9528 
Table 12: CTB Open Test 
5 Conclusions and Future Work 
This is the first time that the NJNU team takes part 
in SIGHAN WS Bakeoff. In the construction of the 
system, we conducted experiments on the CRF-
based segmenter with different feature templates. 
We also employs different post-processing ap-
proaches, including Extended Word String ap-
proach, digit and western roman letter combination, 
and OOV detection. An initial attempt is also made 
on the integration of different segmentation models. 
Time constraint has prevented the team from fuller 
exploration of the methods used in the system.  
Future efforts will be directed towards more com-
plicated segmentation models, the examination of 
the function of different features in the task, the 
integration of different models, and more efficient 
utility of other relevant resources.  
 
References 
Bin Li, Xiaohe Chen. 2007. A Human-Computer Inter-
action Word Segmentation Method Adapting to Chi-
nese Unknown Texts, Journal of Chinese Informa-
tion Processing, 21(3):92-98. 
Daelemans, W. and Van den Bosch. 2005. Memory-
Based Language Processing. Cambridge University 
Press, Cambridge, UK. 
Fuchun Peng, et al 2004. Chinese Segmentation and 
New Word Detection Using Conditional Random 
Fields, COLING2004, 562-568, 23-27 August, Ge-
neva, Switzerland. 
Gina-Anne Levow. 2006. The Third International Chi-
nese Language Processing Bakeoff: Word Segmenta-
tion and Named Entity Recognition, Proceedings of 
the Fifth SIGHAN Workshop on Chinese Language 
Processing, 108-117, 22-23 July, Sydney, Australia. 
118
Sixth SIGHAN Workshop on Chinese Language Processing
Hai Zhao, et al 2006. An Improved Chinese Word 
Segmentation System with Conditional Random 
Field, Proceedings of the Fifth SIGHAN Workshop 
on Chinese Language Processing, 162-165, 22-23 
July, Sydney, Australia. 
Richard Sproat and Thomas Emerson. 2003. The First 
International Chinese Word Segmentation Bakeoff,  
The Second SIGHAN Workshop on Chinese Lan-
guage Procesing, 133-143, Aspporo, Japan. 
Thomas Emerson. 2005. The Second  International Chi-
nese Word Segmentation Bakeoff, Proceedings of the 
Fourth SIGHAN Workshop on Chinese Language 
Processing, 123-133, Jeju Island, Korea. 
119
Sixth SIGHAN Workshop on Chinese Language Processing
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 309?312,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improving Blog Polarity Classification via Topic Analysis and Adaptive
Methods
Feifan Liu
University of Wisconsin, Milwaukee
liuf@uwm.edu
Dong Wang, Bin Li, Yang Liu
The University of Texas at Dallas
dongwang,yangl@hlt.utdallas.edu
Abstract
In this paper we examine different linguistic features
for sentimental polarity classification, and perform a
comparative study on this task between blog and re-
view data. We found that results on blog are much
worse than reviews and investigated two methods
to improve the performance on blogs. First we ex-
plored information retrieval based topic analysis to
extract relevant sentences to the given topics for po-
larity classification. Second, we adopted an adaptive
method where we train classifiers from review data
and incorporate their hypothesis as features. Both
methods yielded performance gain for polarity clas-
sification on blog data.
1 Introduction
Sentimental analysis is a task of text categorization that
focuses on recognizing and classifying opinionated text
towards a given subject. Different levels of sentimental
analysis has been performed in prior work, from binary
classes to more fine grained categories. Pang et al (2002)
defined this task as a binary classification task and ap-
plied it to movie reviews. More sentiment classes, such
as document objectivity and subjectivity as well as dif-
ferent rating scales on the subjectivity, have also been
taken into consideration (Pang and Lee, 2005; Boiy et
al., 2007). In terms of granularity, this task has been
investigated from building word level sentiment lexicon
(Turney, 2002; Moilanen and Pulman, 2008) to detecting
phrase-level (Wilson et al, 2005; Agarwal et al, 2009)
and sentence-level (Riloff and Wiebe, 2003; Hu and Liu,
2004) sentiment orientation. However, most previous
work has mainly focused on reviews (Pang et al, 2002;
Hu and Liu, 2004), news resources (Wilson et al, 2005),
and multi-domain adaptation (Blitzer et al, 2007; Man-
sour et al, 2008). Sentiment analysis on blogs (Chesley
et al, 2005; Kim et al, 2009) is still at its early stage.
In this paper we investigate binary polarity classifica-
tion (positive vs. negative). We evaluate the genre effect
between blogs and review data and show the difference of
feature effectiveness. We demonstrate improved polarity
classification performance in blogs using two methods:
(a) integrating topic relevance analysis to perform topic
specific polarity classification; (b) adopting an adaptive
method by incorporating multiple classifiers? hypotheses
from different review domains as features. Our manual
analysis also points out some challenges and directions
for further study in blog domain.
2 Features for Polarity Classification
For the binary polarity classification task, we use a super-
vised learning framework to determine whether a docu-
ment is positive or negative. We used a subjective lex-
icon, containing 2304 positive words and 4145 negative
words respectively, based on (Wilson et al, 2005). The
features we explored are listed below.
(i) Lexical features (LF)
We use the bag of words for the lexical features as they
have been shown very useful in previous work.
(ii) Polarized lexical features (PL)
We tagged each sentiment word in our data set with its
polarity tag based on the sentiment lexicon (?POS? for
positive, and ?NEG? for negative), along with its part-
of-speech tag. For example, in the sentence ?It is good,
and I like it?, ?good? is tagged as ?POS/ADJ?, ?like? is
tagged as ?POS/VRB?. Then we encode the number of
the polarized tags in a document as features.
(iii) Polarized bigram features (PB)
Contextual information around the polarized words
can be useful for sentimental analysis. A word may
flip the polarity of its neighboring sentiment words even
though this word itself is not necessarily a negative word.
For example, in ?Given her sudden celebrity with those
on the left...? (a sentence in a political blog), ?sudden?
preceding ?celebrity? implies the author?s negative atti-
tude towards ?her?. We combine the sentiment word?s
polarized tag and its following and preceding word or
its part-of-speech to comprise different bigram features
to represent this kind of contextual information. For ex-
309
ample, in ?I recommend this.?, ?recommend? is a posi-
tive verb, denoted as ?POS/VRB?, and the bigram fea-
tures including this tag and its previous word ?I? are
?I POS/VRB? and ?pron POS/VRB?.
(iv) Transition word features (T)
Transition words, such as ?although?, ?even though?,
serve as function words that may change the literal opin-
ion polarity in the current sentence. This information has
not been widely explored for sentiment analysis. In this
study, we compiled a transition word list containing 31
words. We use the co-occurring feature between a transi-
tion word and its nearby content words (noun, verb, ad-
jective and adverb) or polarized tags of sentiment words
within the same sentence, but not spanning over other
transition words. For example, in ?Although it is good?,
we use features like ?although is?,?although good? and
?although POS/ADJ?, where ?POS/ADJ? is the PL fea-
ture for word ?good?.
3 Feature Effectiveness on Blogs and
Reviews
The blog data we used is from the TREC Blog Track eval-
uation in 2006 and 2007. The annotation was conducted
for the 100 topics used in the evaluation (blogs are rele-
vant to a given topic and also opinionated). We use 6,896
positive and 5,300 negative blogs. For the review data,
we combined multiple review data sets from (Pang et al,
2002; Blitzer et al, 2007) together. It contains reviews
from movies and four product domains (kitchen, elec-
tronics, books, and dvd), each of which has 1000 neg-
ative and 1000 positive samples. For the data without
sentence information (e.g., blog data, some review data),
we generated sentences using the maximum entropy sen-
tence boundary detection tool1. We used TnT tagger to
obtain the part-of-speech tags for these data sets.
For classification, we use the maximum entropy clas-
sifier2 with a Gaussian prior of 1 and 100 iterations in
model training. For all the experiments below, we use
a 10-fold cross validation setup and report the average
classification accuracy. Table 1 shows classification re-
sults using various feature sets on blogs and review data.
We keep the lexical feature (LF) as a base feature, and
investigate the effectiveness of adding more different fea-
tures. We used Wilcox signed test for statistical signifi-
cance test. Symbols ??? and ??? in the table indicate the
significant level of 0.05 and 0.1 respectively, compared to
the baseline performance using LF feature setup.
For the review domain, most of the feature sets can sig-
nificantly improve the classification performance over the
baseline of using ?LF? features. ?PB? features yielded
more significant improvement than ?PL? or ?T? feature
categories. Combining ?PL? and ?T? features resulted in
some slight further improvement, achieving the best ac-
1http://stp.ling.uu.se/?gustav/java/classes/MXTERMINATOR.html
2http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
Feature Set Blogs Reviews
LF 72.07 81.67
LF+PL 70.93 81.93
LF+PB 72.44 83.62?
LF+T 72.17 81.76
LF+PL+PB 70.81 83.61?
LF+PL+T 72.74 82.13?
LF+PB+T 72.29 83.73?
LF+PL+PB+T 71.85 83.94?
Table 1: Polarity classification results (accuracy in %) using
different features for blogs and reviews.
curacy of 83.94%. We notice that incorporating our pro-
posed transition feature (T) always achieves some gain on
different feature settings, suggesting that those transition
features are useful for sentimental analysis on reviews.
From Table 1, we can see that overall the performance
on blogs is worse than on the review data. We hypoth-
esize this may be due to the large variation in terms of
contents and styles in blogs. Regarding the feature ef-
fectiveness, we also observe some differences between
blogs and reviews. Adding the polarized bigram feature
and transition feature (PB and T) individually can yield
some improvement; however, adding both of them did
not result in any further improvement ? performance de-
grades compared to LF+PB. Interestingly, although ?PL?
feature alone does not seem to help, by adding ?PL? and
?T? together, the performance achieved the best accuracy
of 72.74%. We also found that adding all the features
together hurts the performance, suggesting that different
features interact with each other and some do not com-
bine well (e.g., PB and T features). In addition, all the
improvements here are not statistically significant.
Note that for the blog data, we randomly split them for
the cross validation experiments regardless of the queries.
In order to better understand whether the poor results on
blog data is due to the effect of different queries, we per-
formed another experiment where for each query, we ran-
domly divided the corresponding blogs into training and
test splits. Only 66 queries were kept for this experi-
ments ? we did not include those queries that have fewer
than 10 relevant blogs. The results for the query balanced
split on blogs are shown in Figure 1. We also include re-
sults for the five individual review data sets in order to see
the topic effect. We present results using four represen-
tative feature sets chosen according to Table 1. For the
review data, we notice some difference across different
data sets, suggesting their inherent difference in terms of
task difficulty. We observe slight performance increase
for some feature sets using the query balanced setup for
blog data, but overall it is still much worse than the review
data. This shows that the query unbalanced training/test
split does not explain the performance gap between blogs
and reviews. This is consistent with (Zhang et al, 2007)
that found that a query-independent classifier performs
even better than query-dependent one. We expect that the
310
query unbalanced setup is more realistic, therefore, in the
following experiments, we continue with this setup.
7173
7577
7981
8385
87
Accur
acy(%
)
blog_data
blog_data_query_balancedreview_books
review_dvd
review_kitchen
review_movie
review_elec
Figure 1: Polarity classification results on query balanced blog
data and five individual review data sets.
4 Improving Blog Polarity Classification
To improve the performance of polarity classification on
blogs, we propose two methods: (a) extract only topic-
relevant segments from blogs for sentiment analysis; (b)
apply adaptive methods to leverage review data.
4.1 Using topic-relevant blog context
Generally a review is written towards one product or one
kind of service, but a blog may cover several topics with
possibly different opinions towards each topic. The blog
data we used is annotated based on some specific topics
in the TREC Blog Track evaluation. Take topic 870 in
the data as an example, ?Find opinions on alleged use
of steroids by baseball player Barry?. There is one blog
that talks about 5 different baseball players in issues of
using steroids. Since the reference opinion tag of a blog
is determined by polarity towards the given query topic, it
might be confusing for the classifier if we use the whole
blog to derive features. Recently topic analysis has been
used for polarity classification (Zhang et al, 2007; Titov
and McDonald, 2008; Wiegand and Klakow, 2009). We
take a different approach in this study.
In order to obtain a topic-relevant context, we retrieved
the top 10 relevant sentences corresponding to the given
topic using the Lemur toolkit3. Then we used these sen-
tences and their immediate previous and following sen-
tences for feature extraction in the same way as what
we did on the whole blog. In addition to using all the
words in the relevant context, we also investigated using
only content words since those are more topic indicative
than function words. We extracted content words (nouns,
verbs, adjectives and adverbs) from each blog in their
original order and apply the same feature extraction pro-
cess as for using all the words.
3http://www.lemurproject.org/lemur/
Table 2 shows the blog polarity classification results
using the whole blog vs. relevant context composed of
all the words or only content words. For the significance
test, the comparison was done for using relevant context
with all the words vs. using the whole blog; and us-
ing content words only vs. using all the words in rele-
vant context. Each comparison was with respect to the
same feature setup. We observe improved polarity classi-
fication performance when using sentence retrieval based
topic analysis to extract relevant context. Using all the
words in the topic relevant context, all the improvements
compared to using the original entire blog are statistically
significant at the level of 0.01. We also notice that un-
like on the entire blog document, the ?PL? features con-
tribute positively when combined with ?LF?. All the fea-
ture settings with ?PL? perform very well. The best ac-
curacy of 75.32% is achieved using feature combination
of ?LF+PL? or ?LF+PL+T?. This suggests that polarized
lexical features suffered from the off-topic content when
using the entire blog and are more useful within contexts
of certain topics.
When using content words only, we observe consistent
gain across all the feature sets. Three feature settings,
?LF+PB?,?LF+T? and ?LF+PL+PB+T?, achieve statisti-
cally significant further improvement (compared to using
all the words of relevant contexts). The best accuracy
(75.6%) is achieved by using the ?LF+PB? features.
Feature Set Whole Relevant Context
Blog All Words Content Words
LF 72.07 74.92? 75.14
LF+PL 70.93 75.32? 75.34
LF+PB 72.44 75.03? 75.6?
LF+T 72.17 75.01? 75.35?
LF+PL+PB 70.81 75.27? 75.35
LF+PL+T 72.74 75.32? 75.41
LF+PB+T 72.29 75.17? 75.42
LF+PL+PB+T 71.85 75.21? 75.45?
Table 2: Blog polarity classification results (accuracy in %) us-
ing topic relevant context composed of all the words or only
content words.
4.2 Adaptive methods using review data
Domain adaptation has been studied in some previous
work (e.g., (Blitzer et al, 2007; Mansour et al, 2008)).
In this paper, we evaluate two adaptive approaches in or-
der to leverage review data to improve blog polarity clas-
sification. In the first approach, in each of the 10-fold
cross-validation training, we pool the blog training data
(90% of the entire blog data) together with all the review
data from 5 different domains. In the second method, we
augment features with hypotheses obtained from classi-
fiers trained using other domain data. Specifically, we
first trained 5 classifiers from 5 review domain data sets
respectively, and encoded the hypotheses from different
classifiers as features for blog training (together with the
original features of the blog data). Results of these two
approaches are shown in Table 3. We use the topic rele-
311
vant context with content words only in this experiment,
and present results for different feature combinations (ex-
cept the baseline ?LF? setting). The significance test is
conducted in comparison to the results using only blog
data for training, for the same feature setting.
We find that the first approach does not yield any gain,
even though the added data is about the same size as
the blog data. It indicates that due to the large differ-
ence between the two genres, simply combining blogs
and reviews in training is not effective. However, we
can see that using augmented features in training signifi-
cantly improved the performance across different feature
sets. The best result is achieved using ?LF+T? features,
76.84% compared with the best accuracy of 75.6% when
using the blog data only (?LF+PB? features).
Feature Set Only Blog Pool Data Augment Features
LF+PL 75.34 75.05 76.12?
LF+PB 75.6 74.35 76.28?
LF+T 75.35 74.47 76.84?
LF+PL+PB 75.35 74.94 76.7?
LF+PL+T 75.41 74.85 76.32?
LF+PB+T 75.42 74.46 76.3?
LF+PL+PB+T 75.45 74.96 76.53?
Table 3: Results (accuracy in %) of blog polarity classification
using two methods leveraging review data.
4.3 Error analysis
Notice that after achieving some improvements the per-
formance on blogs is still much worse than on review
data. Thus we performed a manual error analysis for a
better understanding of the difficulties of sentiment anal-
ysis on blog data, and identified the following challenges.
(a) Idiomatic expressions. Compared to reviews, blog-
gers seem to use more idioms. For example, ?Of course
he has me over the barrel...? expresses negative opinion,
however, there are no superficially indicative features.
(b) Ironic writing style. Some bloggers prefer ironic
style especially when speaking against something or
somebody, whereas opinions are often expressed using
plain writing style in reviews. Simply using the surface
word level features is not able to model these properly.
(c) Background knowledge. In some political blogs,
the polarized expressions are implicit. Correctly recog-
nizing them requires background knowledge and deeper
language analysis techniques.
5 Conclusions and Future Work
In this paper, we have evaluated various features and the
domain effect on sentimental polarity classification. Our
experiments on blog and review data demonstrated dif-
ferent feature effectiveness and the overall poorer perfor-
mance on blogs than reviews. We found that the polarized
features and the transition word features we introduced
are useful for polarity classification. We also show that
by extracting topic-relevant context and considering only
content words, the system can achieve significantly better
performance on blogs. Furthermore, an adaptive method
using augmented features can effectively leverage data
from other domains, and yield improvement compared
to using in-domain training or training on combined data
from different domains. For our future work, we plan
to investigate other adaption methods, and try to address
some of the problems identified in our error analysis.
6 Acknowledgment
The authors thank the three anonymous reviewers for
their suggestions.
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen McKeown. 2009.
Contextual phrase-level polarity analysis using lexical affect
scoring and syntactic n-grams. In Proc. of EACL.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Bi-
ographies, bollywood, boom-boxes and blenders: Domain
adaptation for sentiment classification. In Proc. of ACL.
Erik Boiy, Pieter Hens, Koen Deschacht, and Marie-Francine
Moens. 2007. Automatic sentiment analysis in on-line text.
In Proc. of ELPUB.
Paula Chesley, Bruce Vincent, Li Xu, and Rohini K. Srihari.
2005. Using verbs and adjectives to automatically classify
blog sentiment. In Proc. of AAAI.
Minqing Hu and Bing Liu. 2004. Mining and summarizing
customer reviews. In Proc. of ACM SIGKDD.
Jungi Kim, Jin-Ji Li, and Jong-Hyeok Lee. 2009. Discovering
the discriminative views: Measuring term weights for senti-
ment analysis. In Proc. of ACL-IJCNLP.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh.
2008. Domain adaptation with multiple sources. In Proc.
of NIPS.
Karo Moilanen and Stephen Pulman. 2008. The good, the bad,
and the unknown: Morphosyllabic sentiment tagging of un-
seen words. In Proc. of ACL.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with respect to rat-
ing scales. In Proc. of ACL.
Bo Pang, Lilian Lee, and Shrivakumar Vaithyanathan. 2002.
Thumbs up? sentiment classification using machine learning
techniques. In Proc. of EMNLP.
Ellen Riloff and Janyce Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proc. of EMNLP.
Ivan Titov and Ryan McDonald. 2008. Modeling online re-
views with multi-grain topic models. In Proc. of WWW.
Peter D. Turney. 2002. Thumbs up or thumbs down? semantic
orientation applied to unsupervised classification of reviews.
In Proc. of ACL.
Michael Wiegand and Dietrich Klakow. 2009. Topic-Related
polarity classification of blog sentences. In Proc. of the 14th
Portuguese Conference on Artificial Intelligence: Progress
in Artificial Intelligence, pages 658?669.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level sentiment
analysis. In Proc. of HLT-EMNLP.
Wei Zhang, Clement Yu, and Weiyi Meng. 2007. Opinion re-
trieval from blogs. In Proc. of CIKM, pages 831?840.
312
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 425?429,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
MIXCD: System Description for Evaluating Chinese Word Similarity at 
SemEval-2012 
Yingjie Zhang 
Nanjing University 
22 Hankou Road 
Jiangsu P. R. China 
jillzhyj@139.c
om 
Bin Li 
Nanjing University 
Nanjing Normal University 
122 Ninghai Road 
Jiangsu P. R. China 
gothere@126.com 
Xinyu Dai 
Nanjing University 
22 Hankou Road 
Jiangsu P. R. China 
dxy@nju.edu.cn 
Jiajun Chen 
Nanjing University 
22 Hankou Road 
Jiangsu P. R. China 
cjj@nju.eud.cn 
 
 
Abstract 
This document describes three systems calcu-
lating semantic similarity between two Chi-
nese words. One is based on Machine 
Readable Dictionaries and the others utilize 
both MRDs and Corpus. These systems are 
performed on SemEval-2012 Task 4: Evaluat-
ing Chinese Word Similarity. 
1 Introduction 
The characteristics of polysemy and synonymy that 
exist in words of natural language have always 
been a challenge in the fields of Natural Language 
Processing (NLP) and Information Retrieval (IR). 
In many cases, humans have little difficulty in de-
termining the intended meaning of an ambiguous 
word, while it is extremely difficult to replicate 
this process computationally. For many tasks in 
psycholinguistics and NLP, a job is often decom-
posed to the requirement of resolving the semantic 
similarity between words or concepts. 
There are two ways to get the similarity between 
two words. One is to utilize the machine readable 
dictionary (MRD). The other is to use the corpus. 
For the 4th task in SemEval-2012 we are re-
quired to evaluate the semantic similarity of Chi-
nese word pairs. We consider 3 methods in this 
study. One uses MRDs only and the other two use 
both MRD and corpus. A post processing will be 
done on the results of these methods to treat syno-
nyms. 
In chapter 2 we introduce the previous works on 
the evaluation of Semantic Similarity. Chapter 3 
shows three methods used in this task. Chapter 4 
reveals the results of these methods. And conclu-
sion is stated in chapter 5. 
2 Related Work 
For words may have more than one sense, similari-
ty between two words can be determined by the 
best score among all the concept pairs which their 
various senses belong to. 
Before constructed dictionary is built, Lesk 
similarity (Lesk, 1986) which is proposed as a so-
lution for word sense disambiguation is often used 
to evaluating the similarity between two concepts. 
This method calculates the overlap between the 
corresponding definitions as provided by a diction-
ary. 
       (     )  |     (  )       (  )| 
Since the availability of computational lexicons 
such as WordNet, the taxonomy can be represented 
as a hierarchical structure. Then we use the struc-
ture information to evaluate the semantic similarity. 
In these methods, the hierarchical structure is often 
seen as a tree and concepts as the nodes of the tree 
while relations between two concepts as the edges. 
(Resnik, 1995) determines the conceptual simi-
larity of two concepts by calculating the infor-
mation content (IC) of the least common subsumer 
(LCS) of them. 
      (     )    (   (     )) 
where the IC of a concept can be quantified as 
follow: 
  ( )        ( ) 
425
This method do not consider the distance of two 
concepts. Any two concepts have the same LCS 
will have the same similarity even if the distances 
between them are different. It is called node-based 
method. 
(Leacock and Chodorow, 1998) develops a simi-
larity measure based on the distance of two senses 
   and   . They focus on hypernymy links and 
scaled the path length by the overall depth   of the 
tree. 
      (     )      
      (     )
   
 
(Wu and Palmer, 1994) combines the depth of 
the LCS of two concepts into a similarity score. 
      (     )  
       (   (     ))
     (  )       (  )
 
These approaches are regarded as edge-based 
methods. They are more natural and direct to eval-
uating semantic similarity in taxonomy. But they 
treat all nodes as the same and do not consider the 
different information of different nodes. 
(Jiang and Conrath, 1998) uses the information 
content of concept instead of its depth. So both 
node and edge information can be considered to 
evaluate the similarity. It performs well in evaluat-
ing semantic similarity between two texts (Zhang 
et al, 2008; Corley and Mihalcea, 2005; Pedersen, 
2010). 
      (     )  
 
  (  )   (  )     (   (     ))
  
SemCor is used in Jiang's work to get the fre-
quency of a word with a specific sense treated by 
the Lagrange Smoothing. 
3 Approaches 
For SemEval-2012 task 4, we use two MRDs and 
one corpus as our knowledge resources. One MRD 
is HIT IR-Lab Tongyici Cilin (Extended) (Cilin) 
and the other is Chinese Concept Dictionary 
(CCD). The corpus we used in our system is Peo-
ple's Daily. Three systems are proposed to evaluate 
the semantic similarity between two Chinese words. 
The first one utilizes both the MRDs called 
MIXCC (Mixture of Cilin and CCD) and other two 
named MIXCD1 (Mixture of Corpus and Diction-
ary) and MIXCD2 respectively combine the infor-
mation derived from both corpus and dictionary 
into the similarity score. A post processing is done 
to trim the similarity of words with the same mean-
ing. 
3.1 Knowledge Resources 
HIT IR-Lab Tongyici Cilin (Extended) is built by 
Harbin Institute of Technology which contained 
77343 word items. Cilin is constructed as a tree 
with five levels. With the increasing of the level, 
word senses are more fine-grained. All word items 
in Cilin are located at the fifth level. The larger 
level the LCS of an item pair has, the closer their 
concepts are. 
Chinese Concept Dictionary (CCD) is a Chinese 
WordNet produced by Peking University. Word 
concepts in it are represented as Synsets and one-
one corresponding to WordNet 1.6. There are 4 
types of hierarchical semantic relations in CCD as 
follows: 
? Synonym: the meanings of two words are 
equivalence 
? Antonym: two synsets contain the words 
with opposite meaning 
? Hypernym and Hyponym: two synsets 
with the IS-A relation 
? Holonym and Meronym: two synsets with 
the IS-PART-OF relation 
Additionally there is another type of semantic 
relation such as Attribute in CCD This relation 
type often happens between two words with differ-
ent part-of-speech. Even though it is not the hierar-
chical relation, this relation type can make two 
words with different POS have a path between 
them. In WordNet it is often shown as a Morpho-
logical transform between two words, while it may 
happen on two different words with closed mean-
ing in CCD. 
The corpus we use in our system is People's 
Daily 2000 from January to June which has been 
manually segmented. 
3.2 MIXCC 
MIXCC utilizes both Cilin and CCD to evaluate 
the semantic similarity of word pair. In this method 
we get the rank in three steps. 
First, we use Cilin to separate the list of word 
pairs into five parts and sort them in descending 
order of LCS's level. The word pairs having the 
same level of LCS will be put in the same part. 
426
Second, for each part we compute the similarity 
almost by Jiang and Conrath's method mentioned 
in Section 2 above. Only Synonym and Hypernym-
Hyponym relations of CCD concepts are consid-
ered in this method. So CCD could be constructed 
as a forest. We add a root node which combined 
the forest into a tree to make sure that there is a 
path between any two concepts. 
      (     )     
          (  )
           (  )
      (     ) 
   and   compose a word pair needed to cal-
culate semantic similarity between them.    (  ) is 
the Synset in CCD which contains   (  ).  
Because there is no sense-tagged corpus for 
CCD, the frequency of every word in each concept 
is always 1. 
After       (     )  of all word pairs in the 
same part are calculated, we sort the scores in a 
decreasing order again. Then we get five groups of 
ranked word pairs. 
At last the five groups are combined together as 
the result shown in table 1. 
3.3 MIXCD 
MIXCD combines the information of corpus and 
MRDs to evaluate semantic similarity. 
In this system we use trial data to learn a multiple 
linear regression function. There are two classes of 
features for this study which are derived from CCD 
and People's Daily respectively. One class of fea-
ture is the mutual information of a word pair and 
the other is the shortest path between two concepts 
containing the words of which the similarity need-
ed to be evaluated. 
We consider CCD as a large directed graph. The 
nodes of the graph are Synsets and edges are the 
semantic relations between two Synsets. All five 
types of semantic relation showed in Section 3.1 
will be used to build the graph. 
For each word pair, the shortest path between 
two Synsets which contain the words respectively 
is found. Then the path is represented in two forms. 
In one form we record the vector consisting of 
the counts of every relation type in the path. The 
system using this path's form is called MIXCD0. 
For example the path between "??? (psy-
chology)" and "???? (psychiatry)" is repre-
sented as (0, 0, 3, 2, 0). It means that "???" and 
"????" are not synonym and the shortest path 
between them contained 3 IS-A relations and 2 IS-
PART-OF relations. 
We suppose that the path's length is a significant 
feature to measure the semantic similarity of a 
word pair. So in the other form the length is added 
into the vector as the first component. And the 
counts of each relation are recorded in proportion 
to the length. This form of path representation is 
used in the submitted system called MIXCD. Then 
the path between "???" and "????" is rep-
resented as (5, 0, 0, 0.6, 0.4, 0). 
In both forms, the Synonym feature will be 1 if 
the length of the path is 0. 
The mutual information of all word pairs is cal-
culated via the segmented People's Daily. 
Last we use the result of multiple linear regres-
sion to forecast the similarity of other word pairs 
and get the rank. 
3.4 Post Processing 
The word pair with the same meaning may be con-
sisted of two same words or two different words 
belong to the same concept. It is difficult for both 
systems to separate one from the other. Therefore 
we display a post processing on our systems to 
make sure that the similarity between the same 
words has a larger rank than two different words of 
the same meaning. 
4 Experiments and Results 
We perform our systems on trial data and then use 
Kendall tau Rank Correlation (Kendall, 1995; 
Wessa, 2012) to evaluate the results shown in Ta-
ble 1. The trial data contains 50 word pairs. The 
similarity of each pair is scored by several experts 
and the mean value is regarded as the standard an-
swer to get the manual ranking. 
 
Method Kendall tau 2-sided p value 
MIXCC 0.273469 0.005208 
MIXCD0 0.152653 0.119741 
MIXCD 0.260408 0.007813 
Manual(upper) 0.441633 6.27E-06 
Table 1: Kendall tau Rank Correlation of systems on trial 
 
From Table 1, we can see the tau value of MIX-
CD0 is 0.1526 and MIXCD is 0.2604. MIXCD 
performed notably better than MIXCD0. It shows 
427
that path's length between two words is on an im-
portant position of measuring semantic similarity. 
This feature does improve the similarity result. The 
2-sided p value of MIXCD0 is 0.1197. It is much 
larger than the value of MIXCD which is 0.0078. 
So the ranking result of MIXCD0 is much more 
occasional than result of MIXCD. 
The tau value of MIXCC is 0.2735 and it is 
much smaller than the manual ranking result which 
is 0.4416 seen as the upper bound. It shows that the 
similarity between two words in human's minds 
dose not only depend on their hierarchical relation 
represented in Dictionary. But the value is larger 
than that of MIXCD. It seems that the mutual in-
formation derived from corpus which is expected 
to improve the result reduces the correction of rank 
result contrarily. There may be two reasons on it. 
First, because of the use of trial data in MIXCD, 
the result of similarity ranking strongly depended 
on this data. The reliability of trial data's ranking 
may influent the performance of our system. We 
calculate the tau value between every manual and 
the correct ranking. The least tau value is 0.4416 
and the largest one is 0.8220 with a large disparity. 
We use the Fleiss' kappa value (Fleiss, 1971) to 
evaluate the agreement of manual ranking and the 
result is 0.1526 which showed the significant disa-
greement. This disagreement may make the regres-
sion result cannot show the relation between 
features and score correctly. To reduce the disa-
greement's influence we calculate the mean of 
manual similarity score omitting the maximum and 
minimum ones and get a new standard rank (trial2). 
Then we perform MIXCD on trail2 and show the 
new result as MIXCD-2 in Form 2. MIXCC's re-
sult is also compared with trail2 shown as MIXCC-
2. 
 
 MIXCC-2 MIXCD-2 MIXCC MIXCD 
Kendall tau 0.297959 0.265306 0.273469 0.260408 
Table 2: tau value on new standard (omit max/min manual 
scores) 
 
From Table 2 we can see the tau values of 
MIXCC rose to 0.2980 and MIXCD to 0.2653. It 
shows that omitting the maximum and minimum 
manual scores can reduce some influence of the 
disagreement of artificial scoring.  
Second, the combination method of mutual in-
formation and semantic path in MRD may also 
influent the performance of our system. The ranks 
between MIXCD and MIXCC are also compared 
and the tau value is 0.2065. It shows a low agree-
ment of semantic similarity measurements between 
MRD and Corpus. The mutual information exerts a 
large influence on the measure of similarity and 
sometimes may bring the noise to the result mak-
ing it worse. 
We also perform our systems on test data con-
taining 297 words pairs in the same form of trial 
data and got the follow result: 
 
Method Kendall tau 
MIXCC 0.050 
MIXCD0 -0.064 
MIXCD 0.040 
Table 3 tau values of the result of test data 
 
The ranking on test data of our systems shows 
an even worse result. Because of the low confi-
dence of trial data ranking, multiple linear regres-
sion function learning from the trial data performs 
bad on other word pairs. 
5 Conclusion 
In this paper we propose three methods to evaluate 
the semantic similarity of Chinese word pairs. The 
first one uses MRDs and the second one adds the 
information derived from corpus. The third one 
uses the same knowledge resources as the second 
one but highlights the path length of the word pair. 
The results of the systems show a large difference 
and all have a low score. From the results we can 
see the similarity showed in corpus is much differ-
ent from the one expressed in MRD. One reason of 
the low score is that the manual rank given by the 
task has a low agreement among them. We get a 
new manual rank which reduces some influence of 
disagreement by calculating the mean value of 
scores omitting the maximum and minimum ones. 
Comparing the result of our systems with the new 
ranking, all of them get a higher tau value. 
Acknowledgement 
This paper is supported in part by National Natural 
Science Fund of China under contract 61170181, 
National Social Science Fund of China under con-
tract 10CYY021, State Key Lab. for Novel Soft-
ware Technology under contract KFKT2011B03, 
Jiangsu PostDoc Fund under contract 1101065C. 
428
References 
Mike E. Lesk, 1986. Automatic sense disambiguation 
using machine readable dictionaries: How to tell a 
pine cone from an ice cream cone. In Proceedings of 
the SIGDOC Conference 1986, Toronto, June. 
Philip Resnik, 1995. Using information content to eval-
uate semantic similarity. In Proceedings of the 14th 
International Joint Conference on Artificial Intelli-
gence, Montreal, Canada. 
Claudia Leacock and Martin Chodorow, 1998. Combin-
ing local context and WordNet sense similiarity for 
word sense disambiguation. In WordNet, An Elec-
tronic Lexical Database. The MIT Press. 
Zhibiao Wu and Martha Palmer, 1994. Verb semantics 
and lexical selection. In Proceedings of the 32nd An-
nual Meeting of the Association for Computational 
Linguistics, Las Cruces, New Mexico. 
Jay J. Jiang and David W. Conrath, 1998. Semantic sim-
ilarity based on corpus statistics and lexical taxono-
my. In Proceedings of the International Conference 
on Research in Computational Linguistics. 
Ce Zhang , Yu-Jing Wang , Bin Cui , Gao Cong, 2008. 
Semantic similarity based on compact concept ontol-
ogy. In Proceeding of the 17th international confer-
ence on World Wide Web, April 21-25, 2008, Beijing, 
China 
Courtney Corley , Rada Mihalcea, 2005. Measuring the 
semantic similarity of texts. In Proceedings of the 
ACL Workshop on Empirical Modeling of Semantic 
Equivalence and Entailment, p.13-18, June 30-30, 
2005, Ann Arbor, Michigan. 
Ted Pedersen, 2010. Information content measures of 
semantic similarity perform better without sense-
tagged text. In Human Language Technologies: The 
2010 Annual Conference of the North American 
Chapter of the Association for Computational Lin-
guistics, p.329-332, June 02-04, 2010, Los Angeles, 
California. 
M. G. Kendall, 1955. Rank Correlation Methods. New 
York: Hafner Publishing Co. 
P. Wessa, 2012. Free Statistics Software, Office for Re-
search Development and Education, version 1.1.23-
r7, URL http://www.wessa.net/ 
Jordan L. Fleiss, 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin, Vol. 
76, No. 5 pp. 378?382. 
429
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 519?523,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
NJU-Parser: Achievements on Semantic Dependency Parsing 
Guangchao Tang1 Bin Li1,2 Shuaishuai Xu1 Xinyu Dai1 Jiajun Chen1 
1 State Key Lab for Novel Software Technology, Nanjing University 
2 Research Center of Language and Informatics, Nanjing Normal University 
Nanjing, Jiangsu, China 
{tanggc, lib, xuss, dxy, chenjj}@nlp.nju.edu.cn 
 
Abstract 
In this paper, we introduce our work on 
SemEval-2012 task 5: Chinese Semantic De-
pendency Parsing. Our system is based on 
MSTParser and two effective methods are 
proposed: splitting sentence by punctuations 
and extracting last character of word as lemma. 
The experiments show that, with a combina-
tion of the two proposed methods, our system 
can improve LAS about one percent and final-
ly get the second prize out of nine participat-
ing systems. We also try to handle the multi-
level labels, but with no improvement. 
1 Introduction 
Task 5 of SemEval-2012 tries to find approaches to 
improve Chinese sematic dependency parsing 
(SDP). SDP is a kind of dependency parsing. Cur-
rently, there are many dependency parsers availa-
ble, such as Eisner?s probabilistic dependency 
parser (Eisner, 1996), McDonald?s MSTParser 
(McDonald et al 2005a; McDonald et al 2005b) 
and Nivre?s MaltParser (Nivre, 2006). 
Despite of elaborate models, lots of problems 
still exist in dependency parsing. For example, sen-
tence length has been proved to show great impact 
on the parsing performance. (Li et al, 2010) used a 
two-stage approach based on sentence fragment for 
high-order graph-based dependency parsing. Lack-
ing of linguistic knowledge is also blamed. 
Three methods are promoted in this paper try-
ing to improve the performance: splitting sentence 
by commas and semicolons, extracting last charac-
ter of word as lemma and handling multi-level la-
bels. Improvements could be achieved through the 
first two methods while not for the third. 
2 Overview of Our System 
Our system is based on MSTParser which is one of 
the state-of-the-art parsers. MSTParser tries to ob-
tain the maximum spanning tree of a sentence. For 
projective parsing task, it takes Eisner?s algorithm 
(Eisner, 1996) to get the dependency tree in O(n3) 
time. Meanwhile, Chu-Liu-Edmond?s algorithm 
(Chu and Liu, 1965) is applied for non-projective 
task, which takes O(n2) time. 
Three methods are adopted to MSTParser in our 
system: 
1) Sentences are split into sub-sentences by 
commas and semicolons, for which there 
are two ways. Splitting sentences by all 
commas and semicolons is used in our 
primary system. In our contrast system, we 
use a classifier to determine whether a 
comma or semicolon can be used to split 
the sentence. In the primary and contrast 
system, the proto sentences and the sub-
sentences are trained and tested separately 
and the outputs are merged in the end. 
2) In a Chinese word, the last character usual-
ly contains main sense or semantic class. 
We treat the last character of the word as 
word lemma and find it gets a slightly im-
provement in the experiment. 
3) An experiment trying to solve the problem 
of multi-level labels was conducted by 
parsing different levels separately and con-
sequently merging the outputs together. 
The experiment results have shown that the first 
two methods could enhance the system perfor-
mance while further improvements could be ob-
tained through a combination of them in our sub-
submitted systems. 
519
 
a) The proto sentence from train data 
                       
b) The first sub sentence of a)                         c) The second sub sentence of a) 
Figure 1. An example of the split procedure. 
 
3 Experiments 
3.1 Split sentences by commas and semicolons 
It is observed that the performance decreases as 
the length of the sentences increases. Table 1 
shows the statistical analysis on the data including 
SemEval-2012, Conll-07?s Chinese corpus and a 
subset extracted from CTB using Penn2Malt. Long 
sentence can be split into sub-sentences to get bet-
ter parsing result.  
 
Items 
SemEval
-2012 
Conll-
07 CN 
CTB 
Postages count 35 13 33 
Dependency 
labels count 
122 69 12 
Average sentence 
length 
30.15 5.92 25.89 
Average 
dependency length 
4.80 1.71 4.36 
LAS 61.37 82.89 67.35 
UAS 80.18 87.64 79.90 
Table 1. Statistical analysis on the data. The CTB data is 
a subset extracted from CTB using Penn2Malt. 
 
Our work can be described as following steps: 
Step 1: Use MSTParser to parse the data. We 
name the result as ?normal output?. 
Step 2: Split train and test data by all commas 
and semicolons. The delimiters are removed in the 
sub sentences. For train data, a word?s dependency 
relation is kept if the word?s head is under the cov-
er of the sub sentence. Otherwise, its head will be 
set to root and its label will be set to ROOT (ROOT 
is the default label of dependency arcs whose head 
is root). We define the word as ?sentence head? if 
its head is root. ?Sub-sentence head? indicates the 
sentence head of a sub-sentence. After splitting, 
there may be more than one sub-sentence heads in 
a sub-sentence. Figure 1 shows an example of the 
split procedure. 
Step 3: Use MSTParser to parse the data gener-
ated in step 2. We name the parsing result ?split 
output?. In split output, there may be more than 
one sub-sentences corresponding to a single sen-
tence in normal output. 
Step 4: Merge the split output and the normal 
output. The outputs of sub-sentences are merged 
with delimiters restored. Dependency relations are 
recovered for all punctuations and sub-sentence 
heads in split output with relations in normal out-
put. The sentence head of normal output is kept in 
final output. The result is called ?merged split out-
put?. This step need to be consummated because it 
may result in a dependency tree not well formed 
with several sentence heads or even circles. 
The results of experiments on develop data and 
test data are showed in table 2. For develop data, 
an improvement of 0.85 could be obtained while 
0.93 for test data, both on LAS. 
In step 2, there is an alternative to split the sen-
tences, i.e., using a classifier to determine which 
comma and semicolon can be split. This method is 
taken in the contrast system. When applying the 
classifier, all commas and semicolons in train data 
520
are labeled with S-IN or S-STOP while other 
words with NULL. If the sub sentence before the 
comma or semicolon has only one sub-sentence 
head, it is labeled with S-STOP, otherwise with S-
IN. A model is built from train data with CRF++ 
and test data is evaluated with it. Features used are 
listed in table 3. Only commas and semicolons 
with label S-STOP can be used to split the sen-
tence in step 2. Other steps are the same as above. 
The result is also shown in table 2 as ?merged split 
output with CRF++?. 
 
Data Methods LAS UAS 
Develop 
data 
normal output 61.37 80.18 
merged split output 62.22 80.56 
merged split output 
with CRF++ 
61.97 80.73 
lemma output 61.64 80.47 
primary system output 62.41 80.96 
contrast system output 62.05 80.90 
Test 
 data 
normal output 60.63 79.37 
merged split output 61.56 80.17 
merged split output 
with CRF++ 
61.42 80.20 
lemma output 60.88 79.42 
primary system output 61.63 80.35 
contrast system output 61.64 80.29 
Table 2. Results of the experiments. 
 
w-4,w-3,w-2,w-1,w,w+1,w+2,w+3,w+4 
p-4,p-3,p-2,p-1,p,p+1,p+2,p+3,p+4 
wp-4,wp-3,wp-2,wp-1,wp wp+1,wp+2,wp+3,wp+4 
w-4|w-3,w-3|w-2,w-2|w-1,w-1|w, 
w|w+1,w+1|w+2,w+2|w+3,w+3|w+4 
p-4|p-3,p-3|p-2,p-2|p-1,p-1|p, 
p|p+1,p+1|p+2,p+2|p+3,p+3|p+4 
first word of sub-sentence before the delimiter 
Table 3. Features used in CRF++. w represents for word 
and p for PosTag. +1 means the index after current 
while -1 means before. 
3.2 Extract last character of word as lemma 
In Chinese, the last character of a word usually 
contains main sense or semantic class, which indi-
cates that it may represent the whole word. For 
example, ? ? ?(country) can represent ? ?
? ?(China) and ?? ?(love) can represent ??
??(crazy love).  
The last character is used as lemma in the ex-
periment, with an improvement of 0.27 for LAS on 
develop data and 0.24 on test data. Details of the 
scores are listed in table 2 as ?lemma output?. 
3.3 Multi-level labels experiment 
A notable characteristic of SemEval-2012?s da-
ta is multi-level labels. It introduces four kinds of 
multi-level labels which are s-X, d-X, j-X and r-X. 
The first level represents the basic semantic rela-
tion of the dependency while the second level 
shows the second import, except that s-X repre-
sents sub-sentence relation.  
The r-X label means that a verb modifies a 
noun and the relation between them is reverse. For 
example, in phrase ???(poor) ??(born) ? ?
?(star)?, ???? is headed to ???? with label r-
agent. It means that ???? is the agent of ????. 
When a verbal noun is the head word and its 
child has indirect relation to it, the dependency is 
labeled with j-X. In phrase ???(school) ??
(construction)?, ???? is the head of ???? with 
label j-content. ???? is the content of ????. 
The d-X label means that the child modifies the 
head with an additional relation. For example, in 
phrase ???(technology) ??(enterprise)?, ??
?? modifies ???? and the domain of ???? is 
????. 
A heuristic method is tried in the experiment. 
The multi-level labels of d-X, j-X and r-X are sep-
arated into two parts for each level. For example, 
?d-content? will be separated to ?d? and ?content?. 
For each part, MSTParser is used to train and test. 
We call the outputs ?first-level output? and ?se-
cond-level output?. The outputs of each level and 
normal output are merged then. 
In our experiments, only the word satisfies the 
following conditions need to be merged: 
a) The dependency label in normal output is 
started with d-, j- or r-. 
b) The dependency label in first-level output is 
d, j or r. 
c) The heads in first-level output and second-
level output are of the same. 
Otherwise, the dependency relation in normal 
output will be kept. There are also three ways in 
merging outputs: 
a) Label in first-level output and label in se-
cond-level output are merged. 
b) First level label in normal output and label 
in second-level output are merged. 
c) Label in first-level output and second level 
label in normal output are merged. 
521
Experiment has been done on develop data. In 
the experiment, 24% of the labels are merged and 
92% of the new merged labels are the same as 
original. The results of three ways are listed in ta-
ble 4. All of them get decline compared to normal 
output. 
 
outputs LAS UAS 
normal output 61.37 80.18 
way a) 61.18 80.18 
way b) 61.25 80.18 
way c) 61.25 80.18 
Table 4. Results of multi-level labels experiment on 
develop data. 
3.4 Combined experiment on split and lemma 
Improvements are achieved by first two meth-
ods in the experiment while a further enhancement 
is made with a combination of them in the submit-
ted systems. The split method and lemma method 
are combined as primary system. The split method 
with CRF++ and lemma method are combined as 
contrast system. When combining the two methods, 
last character of the word is firstly extracted as 
lemma for train data and test data. Then the split or 
split with CRF++ method is used. 
The outputs of the primary system and contrast 
system are listed in table 2.  
4 Analysis and Discussion 
The contrast system presented in this paper finally 
got the second prize among nine systems. The pri-
mary system gets the third. There is an improve-
ment of about one percent for both primary and 
contrast system. The following conclusions can be 
made from the experiments: 
1) Parsing is more effective and accurate on 
short sentences. A word prefers to depend 
on another near to it. A sentence can be 
split to several sub sentences by commas 
and semicolons to get better parsing output. 
Result may be improved with a classifier to 
determine whether a comma or semicolon 
can be used to split the sentence. 
2) Last character of word is a useful feature. 
In the experiment, the last character is 
coarsely used as lemma and a minor im-
provement is achieved. Much more lan-
guage knowledge can be used in parsing. 
3) The label set of the data is worthy to be re-
viewed. The meanings of the labels are not 
given in the task. Some of them are confus-
ing especially the multi-level labels. The 
trying of training and testing multi-level la-
bels separately by levels fails with a slight-
ly decline of the score. Multi-level also 
causes too many labels: any single-level la-
bel can be prefixed to form a new multi-
level label. It?s a great problem for current 
parsers. Whether the label set is suitable to 
Chinese semantic dependency parsing 
should be discussed. 
5 Conclusion and Future Work 
Three methods applied in NJU-Parser are de-
scribed in this paper: splitting sentences by com-
mas and semicolons, taking last character of word 
as lemma and handling multi-level labels. The first 
two get improvements in the experiments. Our 
primary system is a combination of the first two 
methods. The contrast system is the same as prima-
ry system except that it has a classifier implement-
ed in CRF++ to determine whether a comma or a 
semicolon should be used to split the sentence. 
Both of the systems get improvements for about 
one percent on LAS. 
In the future, a better classifier should be devel-
oped to split the sentence. New method should be 
applied in merging split outputs to get a well 
formed dependency tree. And we hope there will 
be a better label set which are more capable of de-
scribing semantic dependency relations for Chi-
nese. 
Acknowledgments 
This paper is supported in part by National Natural 
Science Fund of China under contract 61170181, 
Natural Science Fund of Jiangsu under contract 
BK2011192, and National Social Science Fund of 
China under contract 10CYY021. 
References 
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400. 
MSTParser: 
http://www.seas.upenn.edu/~strctlrn/MSTParser/MS
TParser.html 
522
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. COLING. 
J. Nivre. 2006. Inductive Dependency Parsing. Springer. 
R. McDonald, K. Crammer, and F. Pereira. 2005. 
Online Large-Margin Training of Dependency 
Parsers. 43rd Annual Meeting of the Association for 
Computational Linguistics (ACL 2005). 
R. McDonald, F. Pereira, K. Ribarov, and J. Haji?. 2005. 
Non-projective Dependency Parsing using Spanning 
Tree Algorithms. Proceedings of HLT/EMNLP 2005. 
Zhenghua Li, Wanxiang Che, Ting Liu. 2010. Improv-
ing Dependency Parsing Using Punctuation. Interna-
tional Conference on Asian Language 
Processing(IALP) 2010. 
523
