Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 802?811,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
A Phrase-Based Alignment Model for Natural Language Inference
Bill MacCartney, Michel Galley, Christopher D. Manning
Natural Language Processing Group, Stanford University
{wcmac,mgalley,manning}@stanford.edu
Abstract
The alignment problem?establishing links
between corresponding phrases in two related
sentences?is as important in natural language
inference (NLI) as it is in machine transla-
tion (MT). But the tools and techniques of
MT alignment do not readily transfer to NLI,
where one cannot assume semantic equiva-
lence, and for which large volumes of bitext
are lacking. We present a new NLI aligner,
the MANLI system, designed to address these
challenges. It uses a phrase-based alignment
representation, exploits external lexical re-
sources, and capitalizes on a new set of su-
pervised training data. We compare the per-
formance of MANLI to existing NLI and MT
aligners on an NLI alignment task over the
well-known Recognizing Textual Entailment
data. We show that MANLI significantly out-
performs existing aligners, achieving gains of
6.2% in F1 over a representative NLI aligner
and 10.5% over GIZA++.
1 Introduction
The problem of natural language inference (NLI) is
to determine whether a natural-language hypothesis
H can reasonably be inferred from a given premise
text P . In order to recognize that Kennedy was killed
can be inferred from JFK was assassinated, one
must first recognize the correspondence between
Kennedy and JFK, and between killed and assas-
sinated. Consequently, most current approaches to
NLI rely, implicitly or explicitly, on a facility for
alignment?that is, establishing links between cor-
responding entities and predicates in P and H . Re-
cent entries in the annual Recognizing Textual En-
tailment (RTE) competition (Dagan et al, 2005)
have addressed the alignment problem in a variety
of ways, though often without distinguishing it as
a separate subproblem. Glickman et al (2005) and
Jijkoun and de Rijke (2005), among others, have ex-
plored approaches based on measuring the degree of
lexical overlap between bags of words. While ig-
noring structure, such methods depend on matching
each word in H to the word in P with which it is
most similar?in effect, an alignment. At the other
extreme, Tatu and Moldovan (2007) and Bar-Haim
et al (2007) have formulated the inference problem
as analogous to proof search, using inferential rules
which encode (among other things) knowledge of
lexical relatedness. In such approaches, the corre-
spondence between the words of P andH is implicit
in the steps of the proof.
Increasingly, however, the most successful RTE
systems have made the alignment problem explicit.
Marsi and Krahmer (2005) and MacCartney et al
(2006) first advocated pipelined system architec-
tures containing a distinct alignment component, a
strategy crucial to the top-performing systems of
Hickl et al (2006) and Hickl and Bensley (2007).
However, each of these systems has pursued align-
ment in idiosyncratic and poorly-documented ways,
often using proprietary data, making comparisons
and further development difficult.
In this paper we undertake the first systematic
study of alignment for NLI. We propose a new NLI
alignment system which uses a phrase-based repre-
sentation of alignment, exploits external resources
for knowledge of semantic relatedness, and capi-
talizes on the recent appearance of new supervised
training data for NLI alignment. In addition, we
examine the relation between NLI alignment and
MT alignment, and investigate whether existing MT
aligners can usefully be applied in the NLI setting.
2 NLI alignment vs. MT alignment
The alignment problem is familiar in machine trans-
lation (MT), where recognizing that she came is a
good translation for elle est venue requires establish-
802
ing a correspondence between she and elle, and be-
tween came and est venue. The MT community has
developed not only an extensive literature on align-
ment (Brown et al, 1993; Vogel et al, 1996; Marcu
and Wong, 2002; DeNero et al, 2006), but also
standard, proven alignment tools such as GIZA++
(Och and Ney, 2003). Can off-the-shelf MT aligners
be applied to NLI? There is reason to be doubtful.
Alignment for NLI differs from alignment for MT
in several important respects, including:
1. Most obviously, it is monolingual rather than
cross-lingual, opening the door to utilizing
abundant (monolingual) sources of information
on semantic relatedness, such as WordNet.
2. It is intrinsically asymmetric: P is often much
longer thanH , and commonly contains phrases
or clauses which have no counterpart in H .
3. Indeed, one cannot assume even approximate
semantic equivalence?usually a given in MT.
Because NLI problems include both valid and
invalid inferences, the semantic content of H
may diverge substantially from P . An NLI
aligner must be designed to accommodate fre-
quent unaligned words and phrases.
4. Little training data is available. MT align-
ment models are typically trained in unsu-
pervised fashion, inducing lexical correspon-
dences from massive quantities of sentence-
aligned bitexts. While NLI aligners could in
principle do the same, large volumes of suit-
able data are lacking. NLI aligners must there-
fore depend on smaller quantities of supervised
training data, supplemented by external lexi-
cal resources. Conversely, while existing MT
aligners can make use of dictionaries, they are
not designed to harness other sources of infor-
mation on degrees of semantic relatedness.
Consequently, the tools and techniques of MT align-
ment may not transfer readily to NLI alignment. We
investigate the matter empirically in section 5.2.
3 Data
Until recently, research on alignment for NLI has
been hampered by a paucity of high-quality, publicly
available data from which to learn. Happily, that has
begun to change, with the release by Microsoft Re-
search (MSR) of human-generated alignment anno-
In
most
Pacific
countries
there
are
very
few
women
in
parliament
.
Wom
en
are poor
ly
repre
sente
d
in parlia
ment
.
Figure 1: The MSR gold-standard alignment for problem
116 from the RTE2 development set.
tations (Brockett, 2007) for inference problems from
the second Recognizing Textual Entailment (RTE2)
challenge (Bar-Haim et al, 2006). To our knowl-
edge, this work is the first to exploit this data for
training and evaluation of NLI alignment models.
The RTE2 data consists of a development set and
a test set, each containing 800 inference problems.
Each problem consists of a premise and a hypoth-
esis. The premises contain 29 words on average;
the hypotheses, 11 words. Each problem is marked
as a valid or invalid inference (50% each); how-
ever, these annotations are ignored during align-
ment, since they would not be available during test-
ing of a complete NLI system.
The MSR annotations use an alignment repre-
sentation which is token-based, but many-to-many,
and thus allows implicit alignment of multi-word
phrases. Figure 1 shows an example in which very
few has been aligned with poorly represented.
In the MSR data, every alignment link is marked
as SURE or POSSIBLE. In making this distinction,
the annotators have followed a convention common
in MT, which permits alignment precision to be
measured against both SURE and POSSIBLE links,
while recall is measured against only SURE links.
In this work, however, we have chosen to ignore
POSSIBLE links, embracing the argument made by
(Fraser and Marcu, 2007) that their use has impeded
progress in MT alignment models, and that SURE-
803
only annotation is to be preferred.
Each RTE2 problem was independently annotated
by three people, following carefully designed an-
notation guidelines. Inter-annotator agreement was
high: Brockett (2007) reports Fleiss? kappa1 scores
of about 0.73 (?substantial agreement?) for map-
pings from H tokens to P tokens; and all three an-
notators agreed on ?70% of proposed links, while
at least two of three agreed on more than 99.7%
of proposed links,2 attesting to the high quality of
the annotation data. For this work, we merged the
three independent annotations, using majority rule,3
to obtain a gold-standard annotation containing an
average of 7.3 links per RTE problem.
4 The MANLI aligner
In this section, we describe the MANLI aligner, a
new alignment system designed expressly for NLI
alignment. The MANLI system consists of four el-
ements: (1) a phrase-based representation of align-
ment, (2) a feature-based linear scoring function for
alignments, (3) a decoder which uses simulated an-
nealing to find high-scoring alignments, and (4) per-
ceptron learning to optimize feature weights.
4.1 A phrase-based alignment representation
MANLI uses an alignment representation which is
intrinsically phrase-based. (Following the usage
common in MT, we use ?phrase? to mean any con-
tiguous span of tokens, not necessarily correspond-
ing to a syntactic phrase.) We represent an alignment
E between a premise P and a hypothesis H as a set
of phrase edits {e1, e2, . . .}, each belonging to one
of four types:
? an EQ edit connects a phrase in P with an equal
(by word lemmas) phrase in H
? a SUB edit connects a phrase in P with an un-
equal phrase in H
? a DEL edit covers an unaligned phrase in P
? an INS edit covers an unaligned phrase in H
For example, the alignment shown in fig-
ure 1 can be represented by the set {DEL(In1),
1Fleiss? kappa generalizes Cohen?s kappa to the case where
there are more than two annotators.
2The SURE/POSSIBLE distinction is taken as significant in
computing all these figures.
3The handful of three-way disagreements were treated as
POSSIBLE links, and thus were not used here.
DEL(most2), DEL(Pacific3), DEL(countries4),
DEL(there5), EQ(are6, are2), SUB(very7 few8,
poorly3 represented4), EQ(women9, Women1),
EQ(in10, in5), EQ(parliament11, parliament6),
EQ(.12, .7)}.4
Alignments are constrained to be one-to-one at
the phrase level: every token in P and H belongs
to exactly one phrase, which participates in exactly
one edit (possibly DEL or INS). However, the phrase
representation permits alignments which are many-
to-many at the token level. In fact, this is the chief
motivation for the phrase-based representation: we
can align very few and poorly represented as units,
without being forced to make an arbitrary choice as
to which word goes with which word. Moreover, our
scoring function can make use of lexical resources
which have information about semantic relatedness
of multi-word phrases, not merely individual words.
About 23% of the MSR gold-standard align-
ments are not one-to-one (at the token level), and
are therefore technically unreachable for MANLI,
which is constrained to generate one-to-one align-
ments. However, by merging contiguous token links
into phrase edits of size > 1, most MSR align-
ments (about 92%) can be straightforwardly con-
verted into MANLI-reachable alignments. For the
purpose of model training (but not for the evalua-
tion described in section 5.4), we generated a ver-
sion of the MSR data in which all alignments were
converted to MANLI-reachable form.5
4.2 A feature-based scoring function
To score alignments, we use a simple feature-based
linear scoring function, in which the score of an
alignment is the sum of the scores of the edits it con-
tains (including not only SUB and EQ edits, but also
DEL and INS edits), and the score of an edit is the
dot product of a vector encoding its features and a
vector of weights. If E is a set of edits constituting
4DEL and INS edits of size > 1 are possible in principle, but
are not used in our training data.
5About 8% of the MSR alignments contain non-contiguous
links, most commonly because P contains two references to
an entity (e.g., Christian Democrats and CDU) which are both
linked to a reference to the same entity in H (e.g., Christian
Democratic Union). In such cases, one or more links must be
eliminated to achieve a MANLI-reachable alignment. We used
a string-similarity heuristic to break such conflicts, but were
obliged to make an arbitrary choice in about 2% of cases.
804
an alignment, and ? is a vector of feature functions,
the score s is given by:
s(E) =
?
e?E
s(e) =
?
e?E
w ??(e)
We?ll explain how the feature weights w are set in
section 4.4. The features used to characterize each
edit are as follows:
Edit type features. We begin with boolean fea-
tures encoding the type of each edit. We expect EQs
to score higher than SUBs, and (sinceP is commonly
longer than H) DELs to score higher than INSs.
Phrase features. Next, we have features which
encode the sizes of the phrases involved in the edit,
and whether these phrases are non-constituents (in
syntactic parses of the sentences involved).
Lexical similarity feature. For SUB edits, a very
important feature represents the lexical similarity of
the substituends, as a real value in [0, 1]. This simi-
larity score is computed as a max over a number of
component scoring functions, some based on exter-
nal lexical resources, including:
? various string similarity functions, of which
most are applied to word lemmas
? measures of synonymy, hypernymy, antonymy,
and semantic relatedness, including a widely-
used measure due to Jiang and Conrath (1997),
based on manually constructed lexical re-
sources such as WordNet and NomBank
? a function based on the well-known distribu-
tional similarity metric of Lin (1998), which
automatically infers similarity of words and
phrases from their distributions in a very large
corpus of English text
The ability to leverage external lexical resources?
both manually and automatically constructed?is
critical to the success of MANLI.
Contextual features. Even when the lexical sim-
ilarity for a SUB edit is high, it may not be a
good match. If P or H contains multiple occur-
rences of the same word?which happens frequently
with function words, and occasionally with content
words?lexical similarity may not suffice to deter-
mine the right match. To remedy this, we introduce
contextual features for SUB and EQ edits. A real-
valued distortion feature measures the difference
Inputs
? an alignment problem ?P,H?
? a number of iterations N (e.g. 100)
? initial temperature T0 (e.g. 40) and multiplier r (e.g. 0.9)
? a bound on edit size max (e.g. 6)
? an alignment scoring function, SCORE(E)
Initialize
? Let E be an ?empty? alignment for ?P,H? (containing
only DEL and INS edits, no EQ or SUB edits)
? Set E? = E
Repeat for i = 1 to N
? Let {F1, F2, ...} be the set of possible successors of E.
To generate this set:
? Consider every possible edit f up to size max
? Let C(E, f) be the set of edits in E which ?con-
flict? with f (i.e., involve at least some of the same
tokens as f )
? Let F = E ? {f} \ C(E, f)
? Let s(F ) be a map from successors of E to scores gener-
ated by SCORE
? Set p(F ) = exp s(F ), and then normalize p(F ), trans-
forming the score map to a probability distribution
? Set Ti = r ? Ti?1
? Set p(F ) = p(F )1/Ti , smoothing or sharpening p(F )
? Renormalize p(F )
? Choose a new value for E by sampling from p(F )
? If SCORE(E) > SCORE(E?), set E? = E
Return E?
Figure 2: The MANLI-ALIGN algorithm
between the relative positions of the substituends
within their respective sentences, while boolean
matching neighbors features indicate whether the to-
kens before and after the substituends are equal or
similar.
4.3 Decoding using simulated annealing
The problem of decoding?that is, finding a
high-scoring alignment for a particular inference
problem?is made more complex by our choice of a
phrase-based alignment representation. For a model
which uses a token-based representation (say, one
which simply maps H tokens to P tokens), decod-
ing is trivial, since each token can be aligned inde-
pendently of its neighbors. (This is the case for the
bag-of-words aligner described in section 5.1.) But
with a phrase-based representation, things are more
complicated. The segmentation into phrases is not
given in advance, and every phrase pair considered
for alignment must be consistent with its neighbors
with respect to segmentation. Consequently, the de-
coding problem cannot be factored into a number of
805
independent decisions.
To address this difficulty, we have devised a
stochastic alignment algorithm, MANLI-ALIGN (fig-
ure 2), which uses a simulated annealing strategy.
Beginning from an arbitrary alignment, we make a
series of local steps, at each iteration sampling from
a set of possible successors according to scores as-
signed by our scoring function. The sampling is con-
trolled by a ?temperature? which falls over time. At
the beginning of the process, successors are sampled
with nearly uniform probability, which helps to en-
sure that the space of possibilities is explored and
local maxima are avoided. As the temperature falls,
there is a ever-stronger bias toward high-scoring suc-
cessors, so that the algorithm converges on a near-
optimal alignment. Clever use of memoization helps
to ensure that computational costs remain manage-
able. Using the parameter values suggested in fig-
ure 2, aligning an average RTE problem takes about
two seconds.
While MANLI-ALIGN is not guaranteed to pro-
duce optimal alignments, there is reason to believe
that it usually comes very close. After training, the
alignment found by MANLI scored at least as high
as the gold alignment for 99.6% of RTE problems.6
4.4 Perceptron learning
To tune the parameters w of the model, we use
an adaptation of the averaged perceptron algorithm
(Collins, 2002), which has proven successful on a
range of NLP tasks. The algorithm is shown in fig-
ure 3. After initializing w to 0, we perform N train-
ing epochs. (Our experiments used N = 50.) In
each epoch, we iterate through the training data, up-
dating the weight vector at each training example ac-
cording to the difference between the features of the
target algnment and the features of the alignment
produced by the decoder using the current weight
vector. The size of the update is controlled by a
learning rate which decreases over time. At the end
of each epoch, the weight vector is normalized and
stored. The final result is the average of the stored
6This figure is based on the MANLI-reachable version of
the gold-standard data described in section 4.1. For the raw
gold-standard data, the figure is 88.1%. The difference is almost
entirely attributable to unreachable gold alignments, which tend
to score higher simply because they contain more edits (and
because the learned weights are mostly positive).
Inputs
? training problems ?Pj , Hj?, j = 1..n
? corresponding gold-standard alignments Ej
? a number of learning epochs N (e.g. 50)
? a ?burn-in? period N0 < N (e.g. 10)
? initial learning rate R0 (e.g. 1) and multiplier r (e.g. 0.8)
? a vector of feature functions ?(E)
? an alignment algorithm ALIGN(P,H;w) which finds a
good alignment for ?P,H? using weight vector w
Initialize
? Set w = 0
Repeat for i = 1 to N
? Set Ri = r ?Ri?1, reducing the learning rate
? Randomly shuffle the training problems
? For j = 1 to n:
? Set E?j = ALIGN(Pj , Hj ; w)
? Set w = w + Ri ? (?(Ej)??(E?j))
? Set w = w/?w?2 (L2 normalization)
? Set w[i] = w, storing the weight vector for this epoch
Return an averaged weight vector:
? wavg = 1/(N ?N0)
PN
i=N0+1
w[i]
Figure 3: The MANLI-LEARN algorithm
weight vectors, omitting vectors from a fixed num-
ber of epochs at the beginning of the run (which tend
to be of poor quality). Using the parameter values
suggested in figure 3, training runs on the RTE2 de-
velopment set required about 20 hours.
5 Evaluating aligners on MSR data
In this section, we describe experiments designed to
evaluate the performance of various alignment sys-
tems on the MSR gold-standard data described in
section 3. For each system, we report precision,
recall, and F-measure (F1).7 Note that these are
macro-averaged statistics, computed per problem by
counting aligned token pairs,8 and then averaged
over all problems in a problem set.9 We also re-
7MT researchers conventionally report results in terms of
alignment error rate (AER). Since we use only SURE links in the
gold-standard data (see section 3), AER is equivalent to 1?F1.
8For phrase-based alignments like those generated by
MANLI, two tokens are considered to be aligned iff they are
contained within phrases which are aligned.
9MT evaluations conventionally use micro-averaging, which
gives greater weight to problems containing more aligned pairs.
This makes sense in MT, where the purpose of alignment is to
induce phrase tables. But in NLI, where the ultimate goal is
to maximize the number of inference problems answered cor-
rectly, it is more fitting to give all problems equal weight, and
so we macro-average. We have also generated all results using
micro-averaging, and found that the relative comparisons are
806
port the exact match rate, that is, the proportion of
problems in which the guessed alignment exactly
matches the gold alignment. The results are sum-
marized in table 1.
5.1 A robust baseline: the bag-of-words aligner
As a baseline, we use a simple alignment algorithm
inspired by the lexical entailment model of Glick-
man et al (2005), and similar to the simple heuristic
model described in (Och and Ney, 2003). Each hy-
pothesis word h is aligned to the premise word p to
which it is most similar, according to a lexical sim-
ilarity function sim(p, h) which returns scores in
[0, 1]. While Glickman et al used a function based
on web co-occurrence statistics, we use a much sim-
pler function based on string edit distance:
sim(w1, w2) = 1?
dist(lem(w1), lem(w2))
max(|lem(w1)|, |lem(w2)|)
(Here lem(w) denotes the lemma of word w; dist()
denotes Levenshtein string edit distance; and | ? | de-
notes string length.)
This model can be easily extended to generate an
alignment score, which will be of interest in sec-
tion 6. We define the score for a specific hypoth-
esis token h to be the log of its similarity with
the premise token p to which it is aligned, and the
score for the complete alignment of hypothesis H
to premise P to be the sum of the scores of the to-
kens in H , weighted by inverse document frequency
in a large corpus10 (so that common words get less
weight), and normalized by the length of H:
score(h|P ) = logmax
p?P
sim(p, h)
score(H|P ) =
1
|H|
?
h?H
idf(h) ? score(h|P )
Despite the simplicity of this alignment model, its
performance is fairly robust, with good recall. Its
precision, however, its mediocre?chiefly because,
by design, it aligns every h with some p. The model
could surely be improved by allowing it to leave
some H tokens unaligned, but this was not pursued.
not greatly affected.
10We use idf(w) = log(N/Nw), where N is the number of
documents in the corpus, and Nw is the number of documents
containing word w.
System Data P % R % F1 % E %
Bag-of-words dev 57.8 81.2 67.5 3.5
(baseline) test 62.1 82.6 70.9 5.3
GIZA++ dev 83.0 66.4 72.1 9.4
(using lex, ?) test 85.1 69.1 74.8 11.3
Cross-EM dev 67.6 80.1 72.1 1.3
(using lex, ?) test 70.3 81.0 74.1 0.8
Stanford RTE dev 81.1 61.2 69.7 0.5
test 82.7 61.2 70.3 0.3
Stanford RTE dev 81.1 75.8 78.4 ?
(punct. corr.) test 82.7 75.8 79.1 ?
MANLI dev 83.4 85.5 84.4 21.7
(this work) test 85.4 85.3 85.3 21.3
Table 1: Performance of various aligners on the MSR
RTE2 alignment data. The columns show the data set
used (800 problems each); average precision, recall, and
F-measure; and the exact match rate (see text).
5.2 MT aligners: GIZA++ and Cross-EM
Given the importance of alignment for NLI, and the
availability of standard, proven tools for MT align-
ment, an obvious question presents itself: why not
use an off-the-shelf MT aligner for NLI? Although
we have argued (section 2) that this is unlikely to
succeed, to our knowledge, we are the first to inves-
tigate the matter empirically.11
The best-known MT aligner is undoubtedly
GIZA++ (Och and Ney, 2003), which contains im-
plementations of various IBM models (Brown et al,
1993), as well as the HMM model of Vogel et al
(1996). Most practitioners use GIZA++ as a black
box, via the Moses MT toolkit (Koehn et al, 2007).
We followed this practice, running with Moses? de-
fault parameters on the RTE2 data to obtain asym-
metric word alignments in both directions (P -to-H
and H-to-P ). We then performed symmetrization
using the well-known INTERSECTION heuristic.
Unsurprisingly, the out-of-the-box performance
was quite poor, with most words aligned apparently
at random. Precision was fair (72%) but recall was
very poor (46%). Even equal words were usually not
aligned?because GIZA++ is designed for cross-
linguistic use, it does not consider word equality be-
tween source and target sentences. To remedy this,
we supplied GIZA++ with a lexicon, using a trick
11However, Dolan et al (2004) explore a closely-related
topic: using an MT aligner to identify paraphrases.
807
common in MT: we supplemented the training data
with synthetic data consisting of matched pairs of
equal words. This gives GIZA++ a better chance
of learning that, e.g., man should align with man.
The result was a big boost in recall (+23%), and a
smaller gain in precision. The results for GIZA++
shown in table 1 are based on using the lexicon and
INTERSECTION. With these settings, GIZA++ prop-
erly aligned most pairs of equal words, but contin-
ued to align other words apparently at random.
Next, we compared the performance of INTER-
SECTION with other symmetrization heuristics de-
fined in Moses?including UNION, GROW, GROW-
DIAG, GROW-DIAG-FINAL (the default), and GROW-
DIAG-FINAL-AND?and with asymmetric align-
ments in both directions. While all these alterna-
tives achieved better recall than INTERSECTION, all
showed substantially worse precision and F1. On
the RTE2 test set, the asymmetric alignment from
H to P scored 68% in F1; GROW scored 58%; and
all other alternatives scored below 52%.
As an additional experiment, we tested the Cross-
EM aligner (Liang et al, 2006) from the Berke-
leyAligner package on the MSR data. While this
aligner is in many ways simpler than GIZA++ (it
lacks any model of fertility, for example), its method
of jointly training two simple asymmetric HMM
models has outperformed GIZA++ on standard eval-
uations of MT alignment. As with GIZA++, we ex-
perimented with a variety of symmetrization heuris-
tics, and ran trials with and without a supplemental
lexicon. The results were broadly similar: INTER-
SECTION greatly outperformed alternative heuris-
tics, and using a lexicon provided a big boost (up
to 12% in F1). Under optimal settings, the Cross-
EM aligner showed better recall and worse preci-
sion than GIZA++, with F1 just slightly lower. Like
GIZA++, it did well at aligning equal words, but
aligned most other words at random.
The mediocre performance of MT aligners on
NLI alignment comes as no surprise, for reasons dis-
cussed in section 2. Above all, the quantity of train-
ing data is simply too small for unsupervised learn-
ing to succeed. A successful NLI aligner will need
to exploit supervised training data, and will need ac-
cess to additional sources of knowledge about lexi-
cal relatedness.
5.3 The Stanford RTE aligner
A better comparison is thus to an alignment sys-
tem expressly designed for NLI. For this purpose,
we used the alignment component of the Stanford
RTE system (Chambers et al, 2007). The Stanford
aligner performs decoding and learning in a simi-
lar fashion to MANLI, but uses a simpler, token-
based alignment representation, along with a richer
set of features for alignment scoring. It represents
alignments as an injective map from H tokens to
P tokens. Phrase alignments are not directly repre-
sentable, although the effect can be approximated by
a pre-processing step which collapses multi-token
named entities and certain collocations into single
tokens. The features used for alignment scoring in-
clude not only measures of lexical similarity, but
also syntactic features intended to promote the align-
ment of similar predicate-argument structures.
Despite this sophistication, the out-of-the-box
performance of the Stanford aligner is mediocre, as
shown in table 1. The low recall figures are partic-
ularly noteworthy. However, a partial explanation
is readily available: by design, the Stanford system
ignores punctuation.12 Because punctuation tokens
constitute about 15% of the aligned pairs in the MSR
data, this sharply reduces measured recall. However,
since punctuation matters little in inference, such re-
call errors probably should be forgiven. Thus, ta-
ble 1 also shows adjusted statistics for the Stanford
system in which all recall errors involving punctua-
tion are (generously) ignored.
Even after this adjustment, the recall figures are
unimpressive. Error analysis reveals that the Stan-
ford aligner does a poor job of aligning function
words. About 13% of the aligned pairs in the MSR
data are matching prepositions or articles; the Stan-
ford aligner misses about 67% of such pairs. (By
contrast, MANLI misses only 10% of such pairs.)
While function words matter less in inference than
nouns and verbs, they are not irrelevant, and because
sentences often contain multiple instances of a par-
ticular function word, matching them properly is by
no means trivial. If matching prepositions and ar-
ticles were ignored (in addition to punctuation), the
gap in F1 between the MANLI and Stanford systems
12In fact, it operates on a dependency-graph representation
from which punctuation is omitted.
808
would narrow to about 2.8%.
Finally, the Stanford aligner is handicapped by its
token-based alignment representation, often failing
(partly or completely) to align multi-word phrases
such as peace activists with protesters, or hackers
with non-authorized personnel.
5.4 The MANLI aligner
As table 1 indicates, the MANLI aligner was found
to outperform all other aligners evaluated on ev-
ery measure of performance, achieving an F1 score
10.5% higher than GIZA++ and 6.2% higher than
the Stanford aligner (even with the punctuation cor-
rection).13 MANLI achieved a good balance be-
tween precision and recall, and matched more than
20% of the gold-standard alignments exactly.
Three factors seem to have contributed most to
MANLI?s success. First, MANLI is able to outper-
form the MT aligners principally because it is able
to leverage lexical resources to identify the similar-
ity between pairs of words such as jail and prison,
prevent and stop, or injured and wounded. Second,
MANLI?s contextual features enable it to do bet-
ter than the Stanford aligner at matching function
words, a weakness of the Stanford aligner discussed
in section 5.3. Third, MANLI gains a marginal ad-
vantage because its phrase-based representation of
alignment permits it to properly align phrase pairs
such as death penalty and capital punishment, or ab-
dicate and give up.
However, the phrase-based representation con-
tributed far less than we had hoped. Setting
MANLI?s maximum phrase size to 1 (effectively,
restricting it to token-based alignments) caused F1
to fall by just 0.2%. We do not interpret this to
mean that phrase alignments are not useful?indeed,
about 2.6% of the links in the gold-standard data in-
volve phrases of size > 1. Rather, we think it shows
that we have failed to fully exploit the advantages
of the phrase-based representation, chiefly because
we lack lexical resources providing good informa-
tion on similarity of multi-word phrases.
Error analysis suggests that there is ample room
for improvement. A large proportion of recall errors
(perhaps 40%) occur because the lexical similarity
function assigns too low a value to pairs of words
13Reported results for MANLI are averages over 10 runs.
or phrases which are clearly similar, such as con-
servation and protecting, server and computer net-
works, organization and agencies, or bone fragility
and osteoporosis. Better exploitation of lexical re-
sources could help to reduce such errors. Another
important category of recall errors (about 12%) re-
sult from the failure to identify one- and multi-word
versions of the name of some entity, such as Lennon
and John Lennon, or Nike Inc. and Nike. A special-
purpose similarity function could help here. Note,
however, that about 10% of recall errors are un-
avoidable, given our choice of alignment represen-
tation, since they involve cases where the gold stan-
dard aligns one or more tokens on one side to a non-
contiguous set of tokens on the other side.
Precision errors may be harder to reduce. These
errors are dominated by cases where we mistakenly
align two equal function words (49% of precision er-
rors), two forms of the verb to be (21%), two equal
punctuation marks (7%), or two words or phrases
of other types having equal lemmas (18%). Be-
cause such errors often occur because the aligner
is forced to choose between nearly equivalent alter-
natives, they may be difficult to eliminate. The re-
maining 5% of precision errors result mostly from
aligning words or phrases rightly judged to be highly
similar, such as expanding and increasing, labor and
birth, figures and number, or 223,000 and 220,000.
6 Using alignment to predict RTE answers
In section 5, we evaluated the ability of aligners to
recover gold-standard alignments. But since align-
ment is just one component of the NLI problem, we
might also examine the impact of different align-
ers on the ability to recognize valid inferences. If a
high-scoring alignment indicates a close correspon-
dence between H and P , does this also indicate a
valid inference? We have previously emphasized
(MacCartney et al, 2006) that there is more to infer-
ential validity than close lexical or structural corre-
spondence: negations, modals, non-factive and im-
plicative verbs, and other linguistic constructs can
affect validity in ways hard to capture in alignment.
Nevertheless, alignment score can be a strong pre-
dictor of inferential validity, and some NLI systems
(e.g., (Glickman et al, 2005)) rely entirely on some
measure of alignment quality to predict validity.
809
System data acc % avgP %
Bag-of-words aligner dev 61.3 61.5
test 57.9 58.9
Stanford RTE aligner dev 63.1 64.9
test 60.9 59.2
MANLI aligner dev 59.3 69.0
(this work) test 60.3 61.0
RTE2 entries (average) test 58.5 59.1
LCC (Hickl et al, 2006) test 75.4 80.8
Table 2: Performance of various aligners and complete
RTE systems in predicting RTE2 answers. The columns
show the data set used, accuracy, and average precision
(the recommended metric for RTE2).
If an aligner generates real-valued alignment
scores, we can use the RTE data to test its ability to
predict inferential validity with the following simple
method. For a given RTE problem, we predict YES
(valid) if its alignment score14 exceeds a threshold
? , and NO otherwise. We tune ? to maximize accu-
racy on the RTE2 development set, and then measure
performance on the RTE2 test set using the same ? .
Table 2 shows results for several NLI aligners,
along with some results for complete RTE systems,
including the LCC system (the top performer at
RTE2) and an average of all systems participating in
RTE2. While none of the aligners rivals the perfor-
mance of the LCC system, all achieve respectable
results, and the Stanford and MANLI aligners out-
perform the average RTE2 entry. Thus, even if align-
ment quality does not determine inferential validity,
many NLI systems could be improved by harnessing
a well-designed NLI aligner.
7 Related work
Given the extensive literature on phrase-based MT,
it may be helpful further to situate our phrase-based
alignment model in relation to past work. The stan-
dard approach to training a phrase-based MT system
is to apply phrase extraction heuristics using word-
aligned training sets (Och and Ney, 2003; Koehn
et al, 2007). Unfortunately, word alignment mod-
els assume that source words are individually trans-
14For good results, it may be necessary to normalize the
alignment score. Scores from MANLI were normalized by the
number of tokens in the problem. The Stanford aligner performs
a similar normalization internally.
lated into target words, which stands at odds with
the key assumption in phrase-based systems that
many translations are non-compositional. More re-
cently, several works (Marcu and Wong, 2002; De-
Nero et al, 2006; Birch et al, 2006; DeNero and
Klein, 2008) have presented more unified phrase-
based systems that jointly align and weight phrases,
though these systems have not come close to the
state of the art when evaluated in terms of MT per-
formance.
We would argue that previous work in MT phrase
alignment is orthogonal to our work. In MANLI,
the need for phrases arises when word-based rep-
resentations are not appropriate for alignment (e.g.,
between close down and terminate), though longer
phrases are not needed to achieve good alignment
quality. In MT phrase alignment, it is beneficial to
account for arbitrarily large phrases, since the larger
contexts offered by these phrases can help realize
more dependencies among translated words (e.g.,
word order, agreement, subcategorization). Per-
haps because MT phrase alignment is dealing with
much larger contexts, no existing work in MT phrase
alignment (to our knowledge) directly models word
insertions and deletions, as in MANLI. For exam-
ple, in figure 1, MANLI can just skip In most Pacific
countries there, while an MT phrase-based model
would presumably align In most Pacific countries
there are to Women are. Hence, previous work is
of limited applicability to our problem.
8 Conclusion
While MT aligners succeed by unsupervised learn-
ing of word correspondences from massive amounts
of bitext, NLI aligners are forced to rely on smaller
quantities of supervised training data. With the
MANLI system, we have demonstrated how to over-
come this lack of data by utilizing external lexical
resources, and how to gain additional power from a
phrase-based representation of alignment.
Acknowledgements The authors wish to thank the
anonymous reviewers for their helpful comments on
an earlier draft of this paper. This paper is based
on work funded in part by the Defense Advanced
Research Projects Agency through IBM and in part
by the CIA ATP as part of the OCCAM project.
810
References
R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Giampic-
colo, B. Magnini, and I. Szpektor. 2006. The Sec-
ond PASCAL Recognising Textual Entailment Chal-
lenge. In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognising Textual Entailment.
R. Bar-Haim, I. Dagan, I. Greental, and E. Shnarch. 2007.
Semantic Inference at the Lexical-Syntactic Level. In
Proceedings of AAAI-07.
A. Birch, C. Callison-Burch, M. Osborne, and P. Koehn.
2006. Constraining the Phrase-Based, Joint Probabil-
ity Statistical Translation Model. In Proceedings of the
ACL-06 Workshop on Statistical Machine Translation.
C. Brockett. 2007. Aligning the RTE 2006 Corpus. Tech-
nical Report MSR-TR-2007-77, Microsoft Research.
P. F. Brown, S. D. Pietra, V. J. D. Pietra, and R. L. Mercer.
1993. The Mathematics of Statistical Machine Trans-
lation: Parameter Estimation. Computational Linguis-
tics, 19(2):263?311.
N. Chambers, D. Cer, T. Grenager, D. Hall, C. Kid-
don, B. MacCartney, M. C. de Marneffe, D. Ramage,
E. Yeh, and C. D. Manning. 2007. Learning Align-
ments and Leveraging Natural Logic. In Proceedings
of the ACL-07 Workshop on Textual Entailment and
Paraphrasing.
M. Collins. 2002. Discriminative training methods for
hidden Markov models. In Proceedings of EMNLP-
02.
I. Dagan, O. Glickman, and B. Magnini. 2005. The PAS-
CAL Recognising Textual Entailment Challenge. In
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment.
J. DeNero and D. Klein. 2008. The Complexity of Phrase
Alignment Problems. In Proceedings of ACL/HLT-08:
Short Papers, pages 25?28.
J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006.
Why Generative Phrase Models Underperform Surface
Heuristics. In Proceedings of the ACL-06 Workshop on
Statistical Machine Translation, pages 31?38.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsupervised
construction of large paraphrase corpora. In Proceed-
ings of COLING-04.
A. Fraser and D. Marcu. 2007. Measuring Word
Alignment Quality for Statistical Machine Translation.
Computational Linguistics, 33(3):293?303.
O. Glickman, I. Dagan, and M. Koppel. 2005. Web
based probabilistic textual entailment. In Proceedings
of the PASCAL Challenges Workshop on Recognizing
Textual Entailment.
A. Hickl and J. Bensley. 2007. A Discourse
Commitment-Based Framework for Recognizing Tex-
tual Entailment. In ACL-07 Workshop on Textual En-
tailment and Paraphrasing, Prague.
A. Hickl, J. Williams, J. Bensley, K. Roberts, B. Rink,
and Y. Shi. 2006. Recognizing Textual Entailment
with LCC?s GROUNDHOG System. In Proceedings
of the Second PASCAL Challenges Workshop on Rec-
ognizing Textual Entailment.
J. J. Jiang and D. W. Conrath. 1997. Semantic simi-
larity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference on Re-
search in Computational Linguistics.
V. Jijkoun and M. de Rijke. 2005. Recognizing tex-
tual entailment using lexical similarity. In Proceedings
of the PASCAL Challenges Workshop on Recognizing
Textual Entailment, pages 73?76.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL-07, demonstration session.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
Agreement. In Proceedings of NAACL-06, New York.
D. Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of COLING/ACL-98, pages
768?774, Montreal, Canada.
B. MacCartney, T. Grenager, M. C. de Marneffe, D. Cer,
and C. D. Manning. 2006. Learning to Recognize
Features of Valid Textual Entailments. In Proceedings
of NAACL-06, New York.
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proceedings of EMNLP-02, pages 133?139.
E. Marsi and E. Krahmer. 2005. Classification of se-
mantic relations by humans and machines. In ACL-05
Workshop on Empirical Modeling of Semantic Equiv-
alence and Entailment, Ann Arbor.
F. J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
M. Tatu and D. Moldovan. 2007. COGEX at RTE3. In
Proceedings of ACL-07.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
Proceedings of COLING-96, pages 836?841, Copen-
hagen, Denmark.
811
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 848?856,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
A Simple and Effective Hierarchical Phrase Reordering Model
Michel Galley
Computer Science Department
Stanford University
Stanford, CA 94305-9020
galley@cs.stanford.edu
Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305-9010
manning@cs.stanford.edu
Abstract
While phrase-based statistical machine trans-
lation systems currently deliver state-of-the-
art performance, they remain weak on word
order changes. Current phrase reordering
models can properly handle swaps between
adjacent phrases, but they typically lack the
ability to perform the kind of long-distance re-
orderings possible with syntax-based systems.
In this paper, we present a novel hierarchical
phrase reordering model aimed at improving
non-local reorderings, which seamlessly in-
tegrates with a standard phrase-based system
with little loss of computational efficiency. We
show that this model can successfully han-
dle the key examples often used to motivate
syntax-based systems, such as the rotation of
a prepositional phrase around a noun phrase.
We contrast our model with reordering models
commonly used in phrase-based systems, and
show that our approach provides statistically
significant BLEU point gains for two language
pairs: Chinese-English (+0.53 on MT05 and
+0.71 on MT08) and Arabic-English (+0.55
on MT05).
1 Introduction
Statistical phrase-based systems (Och and Ney,
2004; Koehn et al, 2003) have consistently de-
livered state-of-the-art performance in recent ma-
chine translation evaluations, yet these systems re-
main weak at handling word order changes. The re-
ordering models used in the original phrase-based
systems penalize phrase displacements proportion-
ally to the amount of nonmonotonicity, with no con-
sideration of the fact that some words are far more
M 
M 
D 
S 
D
!"
#$
%&
'(
)*
+,
-.
/
eue
nviro
nme
nt m
inist
ers 
hold
 mee
tings
 in l
uxem
burg
.
01
23
45
67
8
/
the d
evel
opm
ent 
and 
prog
ress
 of
 the 
regi
on 
.
D 
M 
D 
D
(b)(a)
Figure 1: Phase orientations (monotone, swap, discontin-
uous) for Chinese-to-English translation. While previous
work reasonably models phrase reordering in simple ex-
amples (a), it fails to capture more complex reorderings,
such as the swapping of ?of the region? (b).
likely to be displaced than others (e.g., in English-to-
Japanese translation, a verb should typically move to
the end of the clause).
Recent efforts (Tillman, 2004; Och et al, 2004;
Koehn et al, 2007) have directly addressed this issue
by introducing lexicalized reordering models into
phrase-based systems, which condition reordering
probabilities on the words of each phrase pair. These
models distinguish three orientations with respect to
the previous phrase?monotone (M), swap (S), and
discontinuous (D)?and as such are primarily de-
signed to handle local re-orderings of neighboring
phrases. Fig. 1(a) is an example where such a model
effectively swaps the prepositional phrase in Luxem-
bourg with a verb phrase, and where the noun min-
isters remains in monotone order with respect to the
previous phrase EU environment.
While these lexicalized re-ordering models have
shown substantial improvements over unlexicalized
phrase-based systems, these models only have a
848
limited ability to capture sensible long distance re-
orderings, as can be seen in Fig. 1(b). The phrase
of the region should swap with the rest of the noun
phrase, yet these previous approaches are unable to
model this movement, and assume the orientation of
this phrase is discontinuous (D). Observe that, in
a shortened version of the same sentence (without
and progress), the phrase orientation would be dif-
ferent (S), even though the shortened version has es-
sentially the same sentence structure. Coming from
the other direction, such observations about phrase
reordering between different languages are precisely
the kinds of facts that parsing approaches to machine
translation are designed to handle and do success-
fully handle (Wu, 1997; Melamed, 2003; Chiang,
2005).
In this paper, we introduce a novel orientation
model for phrase-based systems that aims to bet-
ter capture long distance dependencies, and that
presents a solution to the problem illustrated in
Fig. 1(b). In this example, our reordering model
effectively treats the adjacent phrases the develop-
ment and and progress as one single phrase, and the
displacement of of the region with respect to this
phrase can be treated as a swap. To be able iden-
tify that adjacent blocks (e.g., the development and
and progress) can be merged into larger blocks, our
model infers binary (non-linguistic) trees reminis-
cent of (Wu, 1997; Chiang, 2005). Crucially, our
work distinguishes itself from previous hierarchical
models in that it does not rely on any cubic-time
parsing algorithms such as CKY (used in, e.g., (Chi-
ang, 2005)) or the Earley algorithm (used in (Watan-
abe et al, 2006)). Since our reordering model does
not attempt to resolve natural language ambigui-
ties, we can effectively rely on (linear-time) shift-
reduce parsing, which is done jointly with left-to-
right phrase-based beam decoding and thus intro-
duces no asymptotic change in running time. As
such, the hierarchical model presented in this pa-
per maintains all the effectiveness and speed advan-
tages of statistical phrase-based systems, while be-
ing able to capture some key linguistic phenomena
(presented later in this paper) which have motivated
the development of parsing-based approaches. We
also illustrate this with results that are significantly
better than previous approaches, in particular the
lexical reordering models of Moses, a widely used
phrase-based SMT system (Koehn et al, 2007).
This paper is organized as follows: the train-
ing of lexicalized re-ordering models is described
in Section 3. In Section 4, we describe how to
combine shift-reduce parsing with left-to-right beam
search phrase-based decoding with the same asymp-
totic running time as the original phrase-based de-
coder. We finally show in Section 6 that our ap-
proach yields results that are significantly better than
previous approaches for two language pairs and dif-
ferent test sets.
2 Lexicalized Reordering Models
We compare our re-ordering model with related
work (Tillman, 2004; Koehn et al, 2007) using a
log-linear approach common to many state-of-the-
art statistical machine translation systems (Och and
Ney, 2004). Given an input sentence f, which is to
be translated into a target sentence e, the decoder
searches for the most probable translation e? accord-
ing to the following decision rule:
e? = argmax
e
{
p(e|f)
}
(1)
= argmax
e
{ J
?
j=1
? jh j(f,e)
}
(2)
h j(f,e) are J arbitrary feature functions over
sentence pairs. These features include lexicalized
re-ordering models, which are parameterized as
follows: given an input sentence f, a sequence of
target-language phrases e = (e1, . . . ,en) currently
hypothesized by the decoder, and a phrase alignment
a = (a1, . . . ,an) that defines a source f ai for each
translated phrase ei, these models estimate the prob-
ability of a sequence of orientations o = (o1, . . . ,on)
p(o|e, f) =
n
?
i=1
p(oi|ei, f ai ,ai?1,ai), (3)
where each oi takes values over the set of possi-
ble orientations O = {M,S,D}.1 The probability is
conditioned on both ai?1 and ai to make sure that
the label oi is consistent with the phrase alignment.
Specifically, probabilities in these models can be
1We note here that the parameterization and terminology in
(Tillman, 2004) is slightly different. We purposely ignore these
differences in order to enable a direct comparison between Till-
man?s, Moses?, and our approach.
849
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
b i
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
b i
..
....
...
....
.
..
....
...
....
.
..
....
...
....
.
..
....
...
....
.
..
....
...
....
.
..
....
...
....
.
..
....
...
....
.
(a)
(b)
(c)
b i
s
u
v
u
v
uv
s
s
Figure 2: Occurrence of a swap according to the three
orientation models: word-based, phrase-based, and hier-
archical. Black squares represent word alignments, and
gray squares represent blocks identified by phrase-extract.
In (a), block bi = (ei, fai) is recognized as a swap accord-
ing to all three models. In (b), bi is not recognized as a
swap by the word-based model. In (c), bi is recognized
as a swap only by the hierarchical model.
greater than zero only if one of the following con-
ditions is true:
? oi = M and ai ?ai?1 = 1
? oi = S and ai ?ai?1 = ?1
? oi = D and |ai ?ai?1| 6= 1
At decoding time, rather than using the log-
probability of Eq. 3 as single feature function, we
follow the approach of Moses, which is to assign
three distinct parameters (?m,?s,?d) for the three
feature functions:
? fm = ?ni=1 log p(oi = M| . . .)
? fs = ?ni=1 log p(oi = S| . . .)
? fd = ?ni=1 log p(oi = D| . . .).
There are two key differences between this work
and previous orientation models (Tillman, 2004;
Koehn et al, 2007): (1) the estimation of factors in
Eq. 3 from data; (2) the segmentation of e and f into
phrases, which is static in the case of (Tillman, 2004;
Koehn et al, 2007), while it is dynamically updated
with hierarchical phrases in our case. These differ-
ences are described in the two next sections.
3 Training
We present here three approaches for computing
p(oi|ei, f ai ,ai?1,ai) on word-aligned data using rel-
ative frequency estimates. We assume here that
phrase ei spans the word range s, . . . , t in the target
sentence e and that the phrase f ai spans the range
ORIENTATION MODEL oi = M oi = S oi = D
word-based (Moses) 0.1750 0.0159 0.8092
phrase-based 0.3192 0.0704 0.6104
hierarchical 0.4878 0.1004 0.4116
Table 1: Class distributions of the three orientation mod-
els, estimated from 12M words of Chinese-English data
using the grow-diag alignment symmetrization heuristic
implemented in Moses, which is similar to the ?refined?
heuristic of (Och and Ney, 2004).
u, . . . ,v in the source sentence f. All phrase pairs in
this paper are extracted with the phrase-extract algo-
rithm (Och and Ney, 2004), with maximum length
set to 7.
Word-based orientation model: This model an-
alyzes word alignments at positions (s?1,u?1)
and (s?1,v+1) in the alignment grid shown in
Fig. 2(a). Specifically, orientation is set to oi =
M if (s? 1,u? 1) contains a word alignment and
(s?1,v+1) contains no word alignment. It is set to
oi = S if (s?1,u?1) contains no word alignment
and (s?1,v+1) contains a word alignment. In all
other cases, it is set to oi = D. This procedure is
exactly the same as the one implemented in Moses.2
Phrase-based orientation model: The model
presented in (Tillman, 2004) is similar to the word-
based orientation model presented above, except
that it analyzes adjacent phrases rather than specific
word alignments to determine orientations. Specif-
ically, orientation is set to oi = M if an adjacent
phrase pair lies at (s?1,u?1) in the alignment
grid. It is set to S if an adjacent phrase pair cov-
ers (s?1,v+1) (as shown in Fig. 2(b)), and is set
to D otherwise.
Hierarchical orientation model: This model an-
alyzes alignments beyond adjacent phrases. Specif-
ically, orientation is set to oi = M if the phrase-
extract algorithm is able to extract a phrase pair
at (s?1,u?1) given no constraint on maximum
phrase length. Orientation is S if the same is true
at (s?1,v+1), and orientation is D otherwise.
Table 1 displays overall class distributions accord-
ing to the three models. It appears clearly that occur-
rences of M and S are too sparsely seen in the word-
based model, which assigns more than 80% of its
2http://www.statmt.org/moses/?n=Moses.AdvancedFeatures
850
word phrase hier.
Monotone with previous p(oi = M|ei, f ai ,ai?1,ai)
1 ,4 and is 0.223 0.672 0.942
2 , and also 0.201 0.560 0.948
Swap with previous p(oi = S|ei, f ai ,ai?1,ai)
3 ?){ of china 0.303 0.617 0.651
4 ?? , he said 0.003 0.030 0.395
Monotone with next p(oi = M|ei, f ai ,ai+1,ai)
5 ??? , he pointed out that 0.601 0.770 0.991
6 l , however , 0.517 0.728 0.968
Swap with next p(oi = S|ei, f ai ,ai+1,ai)
7 {0 the development of 0.145 0.831 0.900
8 {?> at the invitation of 0.272 0.834 0.925
Table 2: Monotone and swap probabilities for specific
phrases according to the three models (word, phrase, and
hierarchical). To ensure probabilities are representative,
we only selected phrase pairs that occur at least 100 times
in the training data.
probability mass to D. Conversely, the hierarchical
model counts considerably less discontinuous cases,
and is the only model that accounts for the fact that
real data is predominantly monotone.
Since D is a rather uninformative default cat-
egory that gives no clue how a particular phrase
should be displaced, we will also provide MT evalu-
ation scores (in Section 6) for a set of classes that
distinguishes between left and right discontinuity
{M,S,Dl,Dr}, a choice that is admittedly more lin-
guistically motivated.
Table 2 displays orientation probabilities for con-
crete examples. Each example was put under one
of the four categories that linguistically seems the
best match, and we provide probabilities for that cat-
egory according to each model. Note that, while
we have so far only discussed left-to-right reorder-
ing models, it is also possible to build right-to-left
models by substituting ai?1 with ai+1 in Eq. 3. Ex-
amples for right-to-left models appear in the second
half of the table. The table strongly suggests that
the hierarchical model more accurately determines
the orientation of phrases with respect to large con-
textual blocks. In Examples 1 and 2, the hierarchi-
cal model captures the fact that coordinated clauses
almost always remain in the same order, and that
words should generally be forbidden to move from
one side of ?and? to the other side, a constraint that
is difficult to enforce with the other two reorder-
ing models. In Example 4, the first two models
completely ignore that ?he said? sometimes rotates
around its neighbor clause.
4 Decoding
Computing reordering scores during decoding with
word-based3 and phrase-based models (Tillman,
2004) is trivial, since they only make use of local
information to determine the orientation of a new in-
coming block bi. For a left-to-right ordering model,
bi is scored based on its orientation with respect to
bi?1. For instance, if bi has a swap orientation with
respect to the previous phrase in the current trans-
lation hypothesis, feature p(oi = S| . . .) becomes ac-
tive.
Computing lexicalized reordering scores with
the hierarchical model is more complex, since the
model must identify contiguous blocks?monotone
or swapping?that can be merged into hierarchical
blocks. The employed method is an instance of the
well-known shift-reduce parsing algorithm, and re-
lies on a stack (S) of foreign substrings that have
already been translated. Each time the decoder adds
a new block to the current translation hypothesis, it
shifts the source-language indices of the block onto
S, then repeatedly tries reducing the top two ele-
ments of S if they are contiguous.4 This parsing
algorithm was first applied in computational geome-
try to identify convex hulls (Graham, 1972), and its
running time was shown to be linear in the length
of the sequence (a proof is presented in (Huang et
al., 2008), which applies the same algorithm to the
binarization of SCFG rules).
Figure 3 provides an example of the execution
of this algorithm for the translation output shown
in Figure 4, which was produced by a decoder in-
corporating our hierarchical reordering model. The
decoder successively pushes source-language spans
[1], [2], [3], which are successively merged into
[1-3], and all correspond to monotone orientations.
3We would like to point out an inconsistency in Moses be-
tween training and testing. Despite the fact that Moses estimates
a word-based orientation model during training (i.e., it analyzes
the orientation of a given phrase with respect to adjacent word
alignments), this model is then treated as a phrase-based orien-
tation model during testing (i.e., as a model that orients phrases
with respect to other phrases).
4It is not needed to store target-language indices onto the
stack, since the decoder proceeds left to right, and thus suc-
cessive blocks are always contiguous with respect to the target
language.
851
Target phrase Source Op. oi Stack
the russian side [1] S M
hopes [2] R M [1]
to [3] R M [1-2]
hold [11] S D [1-3]
consultations [12] R M [11], [1-3]
with iran [9-10] R S [11-12], [1-3]
on this [6-7] S D [9-12], [1-3]
issue [8] R,R M [6-7], [9-12], [1-3]
in the near future [4-5] R,R S [6-12], [1-3]
. [13] R,A M [1-12]
Figure 3: The application of the shift-reduce parsing al-
gorithm for identifying hierarchical blocks. This execu-
tion corresponds to the decoding example of Figure 4.
Operations (Op.) include shift (S), reduce (R), and ac-
cept (A). The source and stack columns contain source-
language spans, which is the only information needed to
determine whether two given blocks are contiguous. oi is
the label predicted by the hierarchical model by compar-
ing the current block to the hierarchical phrase that is at
the top of the stack.
!"
#$
%&
'(
)*
+,
-.
/0
12
34
56
the russi
an
side hope
s
to hold cons
ultati
ons
with iran on this issue in the near future .................. ................. ................. ................. ................. ................. ................. ................. .................
................. ................. .................
.................
h 1
h 2
h 3
Figure 4: Output of our phrase-based decoder using the
hierarchical model on a sentence of MT06. Hierarchical
phrases h1 and h2 indicate that with Iran and in the near
future have a swap orientation. h3 indicates that ?to? and
?.? are monotone. In this particular example, distortion
limit was set to 10.
It then encounters a discontinuity that prevents the
next block [11] from being merged with [1-3]. As
the decoder reaches the last words of the sentence (in
the near future), [4-5] is successively merged with
[6-12], then [1-3], yielding a stack that contains only
[1-12].
A nice property of this parsing algorithm is that
it does not worsen the asymptotic running time
of beam-search decoders such as Moses (Koehn,
2004a). Such decoders run in time O(n2), where
n is the length of the input sentence. Indeed, each
time a partial translation hypothesis is expanded into
a longer one, the decoder must perform an O(n) op-
eration in order to copy the coverage set (indicating
which foreign words have already been translated)
into the new hypothesis. Since this copy operation
must be executed O(n) times, the overall time com-
plexity is quadratic. The incorporation of the shift-
reduce parser into such a decoder does not worsen
overall time complexity: whenever the decoder ex-
pands a given partial translation into a longer hy-
pothesis, it simply copies its stack into the newly
created hypothesis (similarly to copying the cover-
age vector, this is an O(n) operation). Hence, the
incorporation of the hierarchical models described
in the paper into a phrase-based decoder preserves
the O(n2) running time. In practice, we observe
based on a set of experiments for Chinese-English
and Arabic-English translation that our phrase-based
decoder is on average only 1.35 times slower when it
is running using hierarchical reordering features and
the shift-reduce parser.
We finally note that the decoding algorithm pre-
sented in this section can only be applied left-to-
right if the decoder itself is operating left-to-right.
In order to predict orientations relative to the right-
to-left hierarchical reordering model, we must re-
sort to approximations at decoding time. We experi-
mented with different approximations, and the one
that worked best (in the experiments discussed in
Section 6) is described as follows. First, we note that
an analysis of the alignment grid often reveals that
certain orientations are impossible. For instance, the
block issue in Figure 4 can only have discontinuous
orientation with respect to what comes next in En-
glish, since words surrounding the Chinese phrase
have already been translated. When several hier-
archical orientations are possible according to the
alignment grid, we choose according to the follow-
ing order of preference: (1) monotone, (2) swap, (3)
discontinuous. For instance, in the case of with iran
in Figure 4, only swap and discontinuous orienta-
tions are possible (monotone orientation is impossi-
ble because of the block hold consultations), hence
we give preference to swap. This prediction turns
out to be the correct one according to the decoding
852
steps that complete the alignment grid.
5 Discussion
We now analyze the system output of Figure 4 to fur-
ther motivate the hierarchical model, this time from
the perspective of the decoder. We first observe that
the prepositional phrase in the future should rotate
around a relatively large noun phrase headed by con-
sultations. Unfortunately, localized reordering mod-
els such as (Tillman, 2004) have no means of identi-
fying that such a displacement is a swap (S). Accord-
ing to these models, the orientation of in the future
with respect to what comes previously is discontin-
uous (D), which is an uninformative fall-back cate-
gory. By identifying h2 (hold ... issue) as a hierarchi-
cal block, the hierarchical model can properly deter-
mine that the block in the near future should have a
swap orientation.5 Similar observations can be made
regarding blocks h1 and h3, which leads our model
to predict either monotone orientation (between h3
and ?to? and between h3 and ?.?) or swap orienta-
tion (between h1 and with Iran) while local models
would predict discontinuous in all cases.
Another benefit of the hierarchical model is that
its representation of phrases remains the same dur-
ing both training and decoding, which is not the case
for word-based and phrase-based reordering mod-
els. The deficiency of these local models lies in the
fact that blocks handled by phrase-based SMT sys-
tems tend to be long at training time and short at
test time, which has adverse consequences on non-
hierarchical reordering models. For instance, in Fig-
ure 4, the phrase-based reordering model categorizes
the block in the near future as discontinuous, though
if the sentence pair had been a training example,
this block would count as a swap because of the ex-
tracted phrase on this issue.
6 Results
In our experiments, we use a re-implementation
of the Moses decoder (Koehn et al, 2007). Ex-
cept for lexical reordering models, all other fea-
tures are standard features implemented almost
5Note that the hierarchical phrase hold ... issue is not a well-
formed syntactic phrase ? i.e., it neither matches the bracketing
of the verb phrase hold ... future nor matches the noun phrase
consultations ... issue ? yet it enables sensible reordering.
exactly as in Moses: four translation features
(phrase-based translation probabilities and lexically-
weighted probabilities), word penalty, phrase
penalty, linear distortion, and language model score.
We experiment with two language pairs: Chinese-
to-English (C-E) and Arabic-to-English (A-E). For
C-E, we trained translation models using a subset of
the Chinese-English parallel data released by LDC
(mostly news, in particular FBIS and Xinhua News).
This subset comprises 12.2M English words, and
11M Chinese words. Chinese words are segmented
with a conditional random field (CRF) classifier that
conforms to the Chinese Treebank (CTB) standard.
The training set for our A-E systems also includes
mostly news parallel data released by LDC, and
contains 19.5M English words, and 18.7M Arabic
tokens that have been segmented using the Arabic
Treebank (ATB) (Maamouri et al, 2004) standard.6
For our language model, we trained a 5-gram
model using the Xinhua and AFP sections of the
Gigaword corpus (LDC2007T40), in addition to the
target side of the parallel data. For both C-E and
A-E, we manually removed documents of Gigaword
that were released during periods that overlap with
those of our development and test sets. The language
model was smoothed with the modified Kneser-Ney
algorithm, and we kept only trigrams, 4-grams, and
5-grams that respectively occurred two, three, and
three times in the training data.
Parameters were tuned with minimum error-rate
training (Och, 2003) on the NIST evaluation set of
2006 (MT06) for both C-E and A-E. Since MERT
is prone to search errors, especially with large num-
bers of parameters, we ran each tuning experiment
four times with different initial conditions. This pre-
caution turned out to be particularly important in the
case of the combined lexicalized reordering models
(the combination of phrase-based and hierarchical
discussed later), since MERT must optimize up to
26 parameters at once in these cases.7 For testing,
6Catalog numbers for C-E: LDC2002E18, LDC2003E07,
LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E26,
and LDC2006E8. For A-E: LDC2007E103, LDC2005E83,
LDC2006E24, LDC2006E34, LDC2006E85, LDC2006E92,
LDC2007E06, LDC2007E101, LDC2007E46, LDC2007E86,
and LDC2008E40.
7We combine lexicalized reordering models by simply treat-
ing them as distinct features, which incidentally increases the
number of model parameters that must be tuned with MERT.
853
 30.5
 31
 31.5
 32
 32.5
 33
 33.5
 34
 0  2  4  6  8  10  12  14
BL
EU
[%],
 Ch
ines
e-E
ngli
sh
distortion limit
hierarchicalphrase-based
word-basedbaseline
 43
 43.5
 44
 44.5
 45
 45.5
 0  2  4  6  8  10
BL
EU
[%],
 Ara
bic-
Eng
lish
distortion limit
hierarchicalphrase-based
word-basedbaseline
Figure 5: Performance on the Chinese-English and
Arabic-English development sets (MT06) with increas-
ing distortion limits for all lexicalized reordering mod-
els discussed in the paper. Our novel hierarchical model
systematically outperforms all other models for distortion
limit equal to or greater than 4. The baseline is Moses
with no lexicalized reordering model.
we used the NIST evaluation sets of 2005 and 2008
(MT05 and MT08) for Chinese-English, and the test
set of 2005 (MT05) for Arabic-English.
Statistical significance is computed using the
approximate randomization test (Noreen, 1989),
whose application to MT evaluation (Riezler and
Maxwell, 2005) was shown to be less sensitive to
type-I errors (i.e., incorrectly concluding that im-
provement is significant) than the perhaps more
widely used bootstrap resampling method (Koehn,
2004b).
Tuning set performance is shown in Figure 5.
Since this paper studies various ordering models,
it is interesting to first investigate how the distor-
LEXICALIZED REORDERING MT06 MT05 MT08
none 31.85 29.75 25.22
word-based 32.96 31.45 25.86
phrase-based 33.24 31.23 26.01
hierarchical 33.80** 32.20** 26.38
phrase-based + hierarchical 33.86** 32.85** 26.53*
Table 3: BLEU[%] scores (uncased) for Chinese-English
and the orientation categories {M,S,D}. Maximum dis-
tortion is set to 6 words, which is the default in Moses.
The stars at the bottom of the tables indicate when a given
hierarchical model is significantly better than all local
models for a given development or test set (*: signifi-
cance at the .05 level; **: significance at the .01 level).
LEXICALIZED REORDERING MT06 MT05 MT08
phrase-based 33.79 32.32 26.32
hierarchical 34.01 32.35 26.58
phrase-based + hierarchical 34.36** 32.33 27.03**
Table 4: BLEU[%] scores (uncased) for Chinese-English
and the orientation categories {M,S,Dl ,Dr}. Since the
distinction between these four categories is not available
in Moses, hence we have no baseline results for this case.
Maximum distortion is set to 6 words.
tion limit affects performance.8 As has been shown
in previous work in Chinese-English and Arabic-
English translation, limiting phrase displacements to
six source-language words is a reasonable choice.
For both C-E and A-E, the hierarchical model is sig-
nificantly better (p ? .05) than either other models
for distortion limits equal to or greater than 6 (ex-
cept for distortion limit 12 in the case of C-E). Since
a distortion limit of 6 works reasonably well for both
language pairs and is the default in Moses, we used
this distortion limit value for all test-set experiments
presented in this paper.
Our main results for Chinese-English are shown
in Table 3. It appears that hierarchical models pro-
vide significant gains over all non-hierarchical mod-
els. Improvements on MT06 and MT05 are very sig-
nificant (p ? .01). In the case of MT08, significant
improvement is reached through the combination of
both phrase-based and hierarchical models. We of-
ten observe substantial gains when we combine such
models, presumably because we get the benefit of
identifying both local and long-distance swaps.
Since most orientations in the phrase-based model
are discontinuous, it is reasonable to ask whether
8Note that we ran MERT separately for each distinct distor-
tion limit.
854
LEXICALIZED REORDERING MT06 MT05
none 44.03 54.87
word-based 44.64 54.96
phrase-based 45.01 55.09
hierarchical 45.51* 55.50*
phrase-based + hierarchical 45.64** 56.01**
Table 5: BLEU[%] scores (uncased) for Arabic-English
and the reordering categories {M,S,D}.
LEXICALIZED REORDERING MT06 MT05
phrase-based 44.74 55.52
hierarchical 45.53** 56.02**
phrase-based + hierarchical 45.63** 56.07**
Table 6: BLEU[%] scores (uncased) for Arabic-English
and the reordering categories {M,S,Dl ,Dr}.
the relatively poor performance of the phrase-based
model is the consequence of an inadequate set of ori-
entation labels. To try to answer this question, we
use the set of orientation labels {M,S,Dl,Dr} de-
scribed in Section 3. Results for this different set of
orientations are shown in Table 4. While the phrase-
based model appears to benefit more from the dis-
tinction between left- and right-discontinuous, sys-
tems that incorporate hierarchical models remain the
most competitive overall: their best performance on
MT06, MT05, and MT08 are respectively 34.36,
32.85, and 27.03. The best non-hierarchical models
achieve only 33.79, 32.32, and 26.32, respectively.
All these differences (i.e., .57, .53, and .71) are sta-
tistically significant at the .05 level.
Our results for Arabic-English are shown in Ta-
bles 5 and 6. Similarly to C-E, we provide results for
two orientation sets: {M,S,D} and {M,S,Dl,Dr}.
We note that the four-class orientation set is overall
less effective for A-E than for C-E. This is probably
due to the fact that there is less probability mass in
A-E assigned to the D category, and thus it is less
helpful to split the discontinuous category into two.
For both orientation sets, we observe in A-E that
the hierarchical model significantly outperforms the
local ordering models. Gains provided by the hierar-
chical model are no less significant than for Chinese-
to-English. This positive finding is perhaps a bit
surprising, since Arabic-to-English translation gen-
erally does not require many word order changes
compared to Chinese-to-English translation, and this
translation task so far has seldom benefited from hi-
erarchical approaches to MT. In our case, one possi-
ble explanation is that Arabic-English translation is
benefiting from the fact that orientation predictions
of the hierarchical model are consistent across train-
ing and testing, which is not the case for the other
ordering models discussed in this paper (see Sec-
tion 4). Overall, hierarchical models are the most
effective on the two sets: their best performances on
MT06 and MT05 are respectively 45.64 and 56.07.
The best non-hierarchical models obtain only 45.01
and 55.52 respectively for the same sets. All these
differences (i.e., .63 and .55) are statistically signifi-
cant at the .05 level.
7 Conclusions and Future Work
In this paper, we presented a lexicalized orientation
model that enables phrase movements that are more
complex than swaps between adjacent phrases. This
model relies on a hierarchical structure that is built
as a by-product of left-to-right phrase-based decod-
ing without increase of asymptotic running time. We
show that this model provides statistically signifi-
cant improvements for five NIST evaluation sets and
for two language pairs. In future work, we plan
to extend the parameterization of our models to not
only predict phrase orientation, but also the length of
each displacement as in (Al-Onaizan and Papineni,
2006). We believe such an extension would improve
translation quality in the case of larger distortion
limits. We also plan to experiment with discrimi-
native approaches to estimating reordering probabil-
ities (Zens and Ney, 2006; Xiong et al, 2006), which
could also be applied to our work. We think the abil-
ity to condition reorderings on any arbitrary feature
functions is also very effective in the case of our hi-
erarchical model, since information encoded in the
trees would seem beneficial to the orientation pre-
diction task.
8 Acknowledgements
The authors wish to thank the anonymous reviewers
for their comments on an earlier draft of this paper.
This paper is based on work funded by the Defense
Advanced Research Projects Agency through IBM.
The content does not necessarily reflect the views of
the U.S. Government, and no official endorsement
should be inferred.
855
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion models for statistical machine translation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting of
the ACL (COLING/ACL), pages 529?536, Morristown,
NJ, USA.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 263?270, June.
Ronald L. Graham. 1972. An efficient algorithm for de-
termining the convex hull of a finite planar set. Infor-
mation Processing Letters, 1(4):132?133.
Liang Huang, Hao Zhang, Daniel Gildea, and Kevin
Knight. 2008. Binarization of synchronous context-
free grammars. Technical report, University of Penn-
sylvania.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL
?03: Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Demonstration Session.
Philipp Koehn. 2004a. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the Sixth Conference of the
Association for Machine Translation in the Americas,
pages 115?124.
Philipp Koehn. 2004b. Statistical significance tests for
machine translation evaluation. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 388?395.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki.
2004. The Penn Arabic treebank: Building a large-
scale annotated Arabic corpus.
I. Dan Melamed. 2003. Multitext grammars and syn-
chronous parsers. In NAACL ?03: Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology, pages 79?86.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley, New
York.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
F. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord
of features for statistical machine translation. In Pro-
ceedings of HLT-NAACL.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL).
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 57?
64, June.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004: Short Papers, pages 101?104.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In ACL ?06: Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the ACL,
pages 777?784.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In ACL-44: Proceedings
of the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Associ-
ation for Computational Linguistics, pages 521?528.
Richard Zens and Herman Ney. 2006. Discriminative re-
ordering models for statistical machine translation. In
Human Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL): Proceedings of the
Workshop on Statistical Machine Translation, pages
55?63, New York City, NY, June.
856
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 297?305,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Robust Machine Translation Evaluation with Entailment Features?
Sebastian Pado?
Stuttgart University
pado@ims.uni-stuttgart.de
Michel Galley, Dan Jurafsky, Chris Manning
Stanford University
{mgalley,jurafsky,manning}@stanford.edu
Abstract
Existing evaluation metrics for machine translation
lack crucial robustness: their correlations with hu-
man quality judgments vary considerably across lan-
guages and genres. We believe that the main reason
is their inability to properly capture meaning: A good
translation candidate means the same thing as the
reference translation, regardless of formulation. We
propose a metric that evaluates MT output based on
a rich set of features motivated by textual entailment,
such as lexical-semantic (in-)compatibility and ar-
gument structure overlap. We compare this metric
against a combination metric of four state-of-the-
art scores (BLEU, NIST, TER, and METEOR) in
two different settings. The combination metric out-
performs the individual scores, but is bested by the
entailment-based metric. Combining the entailment
and traditional features yields further improvements.
1 Introduction
Constant evaluation is vital to the progress of ma-
chine translation (MT). Since human evaluation is
costly and difficult to do reliably, a major focus of
research has been on automatic measures of MT
quality, pioneered by BLEU (Papineni et al, 2002)
and NIST (Doddington, 2002). BLEU and NIST
measure MT quality by using the strong correla-
tion between human judgments and the degree of
n-gram overlap between a system hypothesis trans-
lation and one or more reference translations. The
resulting scores are cheap and objective.
However, studies such as Callison-Burch et al
(2006) have identified a number of problems with
BLEU and related n-gram-based scores: (1) BLEU-
like metrics are unreliable at the level of individual
sentences due to data sparsity; (2) BLEU metrics
can be ?gamed? by permuting word order; (3) for
some corpora and languages, the correlation to hu-
man ratings is very low even at the system level;
(4) scores are biased towards statistical MT; (5) the
quality gap between MT and human translations is
not reflected in equally large BLEU differences.
?This paper is based on work funded by the Defense Ad-
vanced Research Projects Agency through IBM. The content
does not necessarily reflect the views of the U.S. Government,
and no official endorsement should be inferred.
This is problematic, but not surprising: The met-
rics treat any divergence from the reference as a
negative, while (computational) linguistics has long
dealt with linguistic variation that preserves the
meaning, usually called paraphrase, such as:
(1) HYP: However, this was declared terrorism
by observers and witnesses.
REF: Nevertheless, commentators as well as
eyewitnesses are terming it terrorism.
A number of metrics have been designed to account
for paraphrase, either by making the matching more
intelligent (TER, Snover et al (2006)), or by using
linguistic evidence, mostly lexical similarity (ME-
TEOR, Banerjee and Lavie (2005); MaxSim, Chan
and Ng (2008)), or syntactic overlap (Owczarzak et
al. (2008); Liu and Gildea (2005)). Unfortunately,
each metrics tend to concentrate on one particu-
lar type of linguistic information, none of which
always correlates well with human judgments.
Our paper proposes two strategies. We first ex-
plore the combination of traditional scores into a
more robust ensemble metric with linear regression.
Our second, more fundamental, strategy replaces
the use of loose surrogates of translation quality
with a model that attempts to comprehensively as-
sess meaning equivalence between references and
MT hypotheses. We operationalize meaning equiv-
alence by bidirectional textual entailment (RTE,
Dagan et al (2005)), and thus predict the qual-
ity of MT hypotheses with a rich RTE feature set.
The entailment-based model goes beyond existing
word-level ?semantic? metrics such as METEOR
by integrating phrasal and compositional aspects
of meaning equivalence, such as multiword para-
phrases, (in-)correct argument and modification
relations, and (dis-)allowed phrase reorderings. We
demonstrate that the resulting metric beats both in-
dividual and combined traditional MT metrics. The
complementary features of both metric types can
be combined into a joint, superior metric.
297
HYP: Three aid workers were kidnapped.
REF: Three aid workers were kidnapped by pirates.
no entailment entailment
HYP: The virus did not infect anybody.
REF: No one was infected by the virus.
entailment
entailment
Figure 1: Entailment status between an MT system
hypothesis and a reference translation for equiva-
lent (top) and non-equivalent (bottom) translations.
2 Regression-based MT Quality Prediction
Current MTmetrics tend to focus on a single dimen-
sion of linguistic information. Since the importance
of these dimensions tends not to be stable across
language pairs, genres, and systems, performance
of these metrics varies substantially. A simple strat-
egy to overcome this problem could be to combine
the judgments of different metrics. For example,
Paul et al (2007) train binary classifiers on a fea-
ture set formed by a number of MT metrics. We
follow a similar idea, but use a regularized linear
regression to directly predict human ratings.
Feature combination via regression is a super-
vised approach that requires labeled data. As we
show in Section 5, this data is available, and the
resulting model generalizes well from relatively
small amounts of training data.
3 Textual Entailment vs. MT Evaluation
Our novel approach to MT evaluation exploits the
similarity between MT evaluation and textual en-
tailment (TE). TE was introduced by Dagan et
al. (2005) as a concept that corresponds more
closely to ?common sense? reasoning patterns than
classical, strict logical entailment. Textual entail-
ment is defined informally as a relation between
two natural language sentences (a premise P and
a hypothesis H) that holds if ?a human reading P
would infer that H is most likely true?. Knowledge
about entailment is beneficial for NLP tasks such as
Question Answering (Harabagiu and Hickl, 2006).
The relation between textual entailment and MT
evaluation is shown in Figure 1. Perfect MT output
and the reference translation entail each other (top).
Translation problems that impact semantic equiv-
alence, e.g., deletion or addition of material, can
break entailment in one or both directions (bottom).
On the modelling level, there is common ground
between RTE and MT evaluation: Both have to
distinguish between valid and invalid variation to
determine whether two texts convey the same in-
formation or not. For example, to recognize the
bidirectional entailment in Ex. (1), RTE must ac-
count for the following reformulations: synonymy
(However/Nevertheless), more general semantic
relatedness (observers/commentators), phrasal re-
placements (and/as well as), and an active/passive
alternation that implies structural change (is de-
clared/are terming). This leads us to our main hy-
pothesis: RTE features are designed to distinguish
meaning-preserving variation from true divergence
and are thus also good predictors in MT evaluation.
However, while the original RTE task is asymmet-
ric, MT evaluation needs to determine meaning
equivalence, which is a symmetric relation. We do
this by checking for entailment in both directions
(see Figure 1). Operationally, this ensures we detect
translations which either delete or insert material.
Clearly, there are also differences between the
two tasks. An important one is that RTE assumes
the well-formedness of the two sentences. This is
not generally true in MT, and could lead to de-
graded linguistic analyses. However, entailment
relations are more sensitive to the contribution of
individual words (MacCartney andManning, 2008).
In Example 2, the modal modifiers break the entail-
ment between two otherwise identical sentences:
(2) HYP: Peter is certainly from Lincolnshire.
REF: Peter is possibly from Lincolnshire.
This means that the prediction of TE hinges on
correct semantic analysis and is sensitive to mis-
analyses. In contrast, human MT judgments behave
robustly. Translations that involve individual errors,
like (2), are judged lower than perfect ones, but
usually not crucially so, since most aspects are
still rendered correctly. We thus expect even noisy
RTE features to be predictive for translation quality.
This allows us to use an off-the-shelf RTE system
to obtain features, and to combine them using a
regression model as described in Section 2.
3.1 The Stanford Entailment Recognizer
The Stanford Entailment Recognizer (MacCartney
et al, 2006) is a stochastic model that computes
match and mismatch features for each premise-
hypothesis pair. The three stages of the system
are shown in Figure 2. The system first uses a
robust broad-coverage PCFG parser and a deter-
ministic constituent-dependency converter to con-
struct linguistic representations of the premise and
298
Stage 3: Feature computation (w/ numbers of features)
Premise: India buys 1,000 tanks.
Hypothesis: India acquires arms.
Stage 1: Linguistic analysis
India
buys
1,000 tanks
subj dobj
India
acquires
arms
subj dobj
Stage 2: Alignment
India
buys
1,000 tanks
subj dobj
India
acquires
arms
subj dobj
0.9
1.0
0.7
Alignment (8):
Semantic 
compatibility 
(34): 
Insertions and
deletions (20):
Preservation of 
reference (16):
Structural 
alignment (28):
Overall alignment quality
Modality, Factivity, Polarity, 
Quantification, Lexical-semantic 
relatedness, Tense
Felicity of appositions and adjuncts, 
Types of unaligned material 
Locations, Dates, Entities
Alignment of main verbs and 
syntactically prominent words, 
Argument structure (mis-)matches
Figure 2: The Stanford Entailment Recognizer
the hypothesis. The results are typed dependency
graphs that contain a node for each word and la-
beled edges representing the grammatical relations
between words. Named entities are identified, and
contiguous collocations grouped. Next, it identifies
the highest-scoring alignment from each node in
the hypothesis graph to a single node in the premise
graph, or to null. It uses a locally decomposable
scoring function: The score of an alignment is the
sum of the local word and edge alignment scores.
The computation of these scores make extensive
use of about ten lexical similarity resources, in-
cluding WordNet, InfoMap, and Dekang Lin?s the-
saurus. Since the search space is exponential in
the hypothesis length, the system uses stochastic
(rather than exhaustive) search based on Gibbs sam-
pling (see de Marneffe et al (2007)).
Entailment features. In the third stage, the sys-
tem produces roughly 100 features for each aligned
premise-hypothesis pair. A small number of them
are real-valued (mostly quality scores), but most
are binary implementations of small linguistic the-
ories whose activation indicates syntactic and se-
mantic (mis-)matches of different types. Figure 2
groups the features into five classes. Alignment
features measure the overall quality of the align-
ment as given by the lexical resources. Semantic
compatibility features check to what extent the
aligned material has the same meaning and pre-
serves semantic dimensions such as modality and
factivity, taking a limited amount of context into
account. Insertion/deletion features explicitly ad-
dress material that remains unaligned and assess its
felicity. Reference features ascertain that the two
sentences actually refer to the same events and par-
ticipants. Finally, structural features add structural
considerations by ensuring that argument structure
is preserved in the translation. See MacCartney et
al. (2006) for details on the features, and Sections
5 and 6 for examples of feature firings.
Efficiency considerations. The use of deep lin-
guistic analysis makes our entailment-based met-
ric considerably more heavyweight than traditional
MT metrics. The average total runtime per sentence
pair is 5 seconds on an AMD 2.6GHz Opteron core
? efficient enough to perform regular evaluations on
development and test sets. We are currently investi-
gating caching and optimizations that will enable
the use of our metric for MT parameter tuning in a
Minimum Error Rate Training setup (Och, 2003).
4 Experimental Evaluation
4.1 Experiments
Traditionally, human ratings for MT quality have
been collected in the form of absolute scores on a
five- or seven-point Likert scale, but low reliabil-
ity numbers for this type of annotation have raised
concerns (Callison-Burch et al, 2008). An alter-
native that has been adopted by the yearly WMT
evaluation shared tasks since 2008 is the collection
of pairwise preference judgments between pairs of
MT hypotheses which can be elicited (somewhat)
more reliably. We demonstrate that our approach
works well for both types of annotation and differ-
ent corpora. Experiment 1 models absolute scores
on Asian newswire, and Experiment 2 pairwise
preferences on European speech and news data.
4.2 Evaluation
We evaluate the output of our models both on the
sentence and on the system level. At the sentence
level, we can correlate predictions in Experiment 1
directly with human judgments with Spearman?s ? ,
299
a non-parametric rank correlation coefficient appro-
priate for non-normally distributed data. In Experi-
ment 2, the predictions cannot be pooled between
sentences. Instead of correlation, we compute ?con-
sistency? (i.e., accuracy) with human preferences.
System-level predictions are computed in both
experiments from sentence-level predictions, as the
ratio of sentences for which each system provided
the best translation (Callison-Burch et al, 2008).
We extend this procedure slightly because real-
valued predictions cannot predict ties, while human
raters decide for a significant portion of sentences
(as much as 80% in absolute score annotation) to
?tie? two systems for first place. To simulate this
behavior, we compute ?tie-aware? predictions as
the percentage of sentences where the system?s hy-
pothesis was assigned a score better or at most ?
worse than the best system. ? is set to match the
frequency of ties in the training data.
Finally, the predictions are again correlated with
human judgments using Spearman?s ? . ?Tie aware-
ness? makes a considerable practical difference,
improving correlation figures by 5?10 points.1
4.3 Baseline Metrics
We consider four baselines. They are small regres-
sion models as described in Section 2 over com-
ponent scores of four widely used MT metrics. To
alleviate possible nonlinearity, we add all features
in linear and log space. Each baselines carries the
name of the underlying metric plus the suffix -R.2
BLEUR includes the following 18 sentence-level
scores: BLEU-n and n-gram precision scores
(1? n? 4); BLEU brevity penalty (BP); BLEU
score divided by BP. To counteract BLEU?s brittle-
ness at the sentence level, we also smooth BLEU-n
and n-gram precision as in Lin and Och (2004).
NISTR consists of 16 features. NIST-n scores
(1? n? 10) and information-weighted n-gram
precision scores (1? n? 4); NIST brevity penalty
(BP); and NIST score divided by BP.
1Due to space constraints, we only show results for ?tie-
aware? predictions. See Pado? et al (2009) for a discussion.
2The regression models can simulate the behaviour of each
component by setting the weights appropriately, but are strictly
more powerful. A possible danger is that the parameters over-
fit on the training set. We therefore verified that the three
non-trivial ?baseline? regression models indeed confer a bene-
fit over the default component combination scores: BLEU-1
(which outperformed BLEU-4 in the MetricsMATR 2008 eval-
uation), NIST-4, and TER (with all costs set to 1). We found
higher robustness and improved correlations for the regression
models. An exception is BLEU-1 and NIST-4 on Expt. 1 (Ar,
Ch), which perform 0.5?1 point better at the sentence level.
TERR includes 50 features. We start with the
standard TER score and the number of each of the
four edit operations. Since the default uniform cost
does not always correlate well with human judg-
ment, we duplicate these features for 9 non-uniform
edit costs. We find it effective to set insertion cost
close to 0, as a way of enabling surface variation,
and indeed the new TERp metric uses a similarly
low default insertion cost (Snover et al, 2009).
METEORR consists of METEOR v0.7.
4.4 Combination Metrics
The following three regression models implement
the methods discussed in Sections 2 and 3.
MTR combines the 85 features of the four base-
line models. It uses no entailment features.
RTER uses the 70 entailment features described
in Section 3.1, but no MTR features.
MT+RTER uses all MTR and RTER features,
combining matching and entailment evidence.3
5 Expt. 1: Predicting Absolute Scores
Data. Our first experiment evaluates the models
we have proposed on a corpus with traditional an-
notation on a seven-point scale, namely the NIST
OpenMT 2008 corpus.4 The corpus contains trans-
lations of newswire text into English from three
source languages (Arabic (Ar), Chinese (Ch), Urdu
(Ur)). Each language consists of 1500?2800 sen-
tence pairs produced by 7?15 MT systems.
We use a ?round robin? scheme. We optimize
the weights of our regression models on two lan-
guages and then predict the human scores on the
third language. This gauges performance of our
models when training and test data come from the
same genre, but from different languages, which
we believe to be a setup of practical interest. For
each test set, we set the system-level tie parameter
? so that the relative frequency of ties was equal
to the training set (65?80%). Hypotheses generally
had to receive scores within 0.3?0.5 points to tie.
Results. Table 1 shows the results. We first con-
centrate on the upper half (sentence-level results).
The predictions of all models correlate highly sig-
nificantly with human judgments, but we still see
robustness issues for the individual MT metrics.
3Software for RTER and MT+RTER is available from
http://nlp.stanford.edu/software/mteval.shtml.
4Available from http://www.nist.gov.
300
Evaluation Data Metrics
train test BLEUR METEORR NISTR TERR MTR RTER MT+RTER
Sentence-level
Ar+Ch Ur 49.9 49.1 49.5 50.1 50.1 54.5 55.6
Ar+Ur Ch 53.9 61.1 53.1 50.3 57.3 58.0 62.7
Ch+Ur Ar 52.5 60.1 50.4 54.5 55.2 59.9 61.1
System-level
Ar+Ch Ur 73.9 68.4 50.0 90.0? 92.7? 77.4? 81.0?
Ar+Ur Ch 38.5 44.3 40.0 59.0? 51.8? 47.7 57.3?
Ch+Ur Ar 59.7? 86.3? 61.9? 42.1 48.1 59.7? 61.7?
Table 1: Expt. 1: Spearman?s ? for correlation between human absolute scores and model predictions on
NIST OpenMT 2008. Sentence level: All correlations are highly significant. System level: ?: p<0.05.
METEORR achieves the best correlation for Chi-
nese and Arabic, but fails for Urdu, apparently the
most difficult language. TERR shows the best result
for Urdu, but does worse than METEORR for Ara-
bic and even worse than BLEUR for Chinese. The
MTR combination metric alleviates this problem to
some extent by improving the ?worst-case? perfor-
mance on Urdu to the level of the best individual
metric. The entailment-based RTER system outper-
forms MTR on each language. It particularly im-
proves on MTR?s correlation on Urdu. Even though
METEORR still does somewhat better than MTR
and RTER, we consider this an important confirma-
tion for the usefulness of entailment features in MT
evaluation, and for their robustness.5
In addition, the combined model MT+RTER is
best for all three languages, outperforming METE-
ORR for each language pair. It performs consid-
erably better than either MTR or RTER. This is a
second result: the types of evidence provided by
MTR and RTER appear to be complementary and
can be combined into a superior model.
On the system level (bottom half of Table 1),
there is high variance due to the small number of
predictions per language, and many predictions are
not significantly correlated with human judgments.
BLEUR, METEORR, and NISTR significantly pre-
dict one language each (all Arabic); TERR, MTR,
and RTER predict two languages. MT+RTER is
the only model that shows significance for all three
languages. This result supports the conclusions we
have drawn from the sentence-level analysis.
Further analysis. We decided to conduct a thor-
ough analysis of the Urdu dataset, the most difficult
source language for all metrics. We start with a fea-
5These results are substantially better than the performance
our metric showed in the MetricsMATR 2008 challenge. Be-
yond general enhancement of our model, we attribute the less
good MetricsMATR 2008 results to an infelicitous choice
of training data for the submission, coupled with the large
amount of ASR output in the test data, whose disfluencies
represent an additional layer of problems for deep approaches.
20 40 60 80 1000.4
2
0.46
0.50
0.54
% Training data MT08 Ar+Ch
Spe
arm
an's
 rho
 on 
MT 
08 U
r
l
l
l
l
l l l l l
l l l
l l l l l l
l
l
MetricsMt?RteRRteRMtRMetR
Figure 3: Experiment 1: Learning curve (Urdu).
ture ablation study. Removing any feature group
from RTER results in drops in correlation of at least
three points. The largest drops occur for the struc-
tural (? = ?11) and insertion/deletion (? = ?8)
features. Thus, all feature groups appear to con-
tribute to the good correlation of RTER. However,
there are big differences in the generality of the
feature groups: in isolation, the insertion/deletion
features achieve almost no correlation, and need to
be complemented by more robust features.
Next, we analyze the role of training data. Fig-
ure 3 shows Urdu average correlations for models
trained on increasing subsets of the training data
(10% increments, 10 random draws per step; Ar
and Ch show similar patterns.) METEORR does not
improve, which is to be expected given the model
definition. RTER has a rather flat learning curve
that climbs to within 2 points of the final correla-
tion value for 20% of the training set (about 400
sentence pairs). Apparently, entailment features do
not require a large training set, presumably because
most features of RTER are binary. The remaining
two models, MTR and MT+RTER, show clearer
benefit from more data. With 20% of the total data,
they climb to within 5 points of their final perfor-
mance, but keep slowly improving further.
301
REF: I shall face that fact today.
HYP: Today I will face this reality.
[doc WL-34-174270-7483871, sent 4, system1]
Gold: 6
METEORR: 2.8
RTER: 6.1
? Only function words unaligned (will, this)
? Alignment fact/reality: hypernymy is ok
in upward monotone context
REF: What does BBC?s Haroon Rasheed say after a visit to Lal Masjid Jamia
Hafsa complex? There are no underground tunnels in Lal Masjid or Jamia
Hafsa. The presence of the foreigners could not be confirmed as well. What
became of the extremists like Abuzar?
HYP: BBC Haroon Rasheed Lal Masjid, Jamia Hafsa after his visit to Auob
Medical Complex says Lal Masjid and seminary in under a land mine, not
also been confirmed the presence of foreigners could not be, such as Abu by
the extremist? [doc WL-12-174261-7457007, sent 2, system2]
Gold: 1
METEORR: 4.5
RTER: 1.2
? Hypothesis root node unaligned
? Missing alignments for subjects
? Important entities in hypothesis cannot be
aligned
? Reference, hypothesis differ in polarity
Table 2: Expt. 1: Reference translations and MT output (Urdu). Scores are out of 7 (higher is better).
Finally, we provide a qualitative comparison of
RTER?s performance against the best baseline met-
ric, METEORR. Since the computation of RTER
takes considerably more resources than METEORR,
it is interesting to compare the predictions of RTER
against METEORR. Table 2 shows two classes of
examples with apparent improvements.
The first example (top) shows a good translation
that is erroneously assigned a low score by ME-
TEORR because (a) it cannot align fact and reality
(METEORR aligns only synonyms) and (b) it pun-
ishes the change of word order through its ?penalty?
term. RTER correctly assigns a high score. The
features show that this prediction results from two
semantic judgments. The first is that the lack of
alignments for two function words is unproblem-
atic; the second is that the alignment between fact
and reality, which is established on the basis of
WordNet similarity, is indeed licensed in the cur-
rent context. More generally, we find that RTER
is able to account for more valid variation in good
translations because (a) it judges the validity of
alignments dependent on context; (b) it incorpo-
rates more semantic similarities; and (c) it weighs
mismatches according to the word?s status.
The second example (bottom) shows a very bad
translation that is scored highly by METEORR,
since almost all of the reference words appear either
literally or as synonyms in the hypothesis (marked
in italics). In combination with METEORR?s con-
centration on recall, this is sufficient to yield a
moderately high score. In the case of RTER, a num-
ber of mismatch features have fired. They indicate
problems with the structural well-formedness of
the MT output as well as semantic incompatibil-
ity between hypothesis and reference (argument
structure and reference mismatches).
6 Expt. 2: Predicting Pairwise Preferences
In this experiment, we predict human pairwise pref-
erence judgments (cf. Section 4). We reuse the
linear regression framework from Section 2 and
predict pairwise preferences by predicting two ab-
solute scores (as before) and comparing them.6
Data. This experiment uses the 2006?2008 cor-
pora of the Workshop on Statistical Machine
Translation (WMT).7 It consists of data from EU-
ROPARL (Koehn, 2005) and various news com-
mentaries, with five source languages (French, Ger-
man, Spanish, Czech, and Hungarian). As training
set, we use the portions of WMT 2006 and 2007
that are annotated with absolute scores on a five-
point scale (around 14,000 sentences produced by
40 systems). The test set is formed by the WMT
2008 relative rank annotation task. As in Experi-
ment 1, we set ? so that the incidence of ties in the
training and test set is equal (60%).
Results. Table 4 shows the results. The left result
column shows consistency, i.e., the accuracy on
human pairwise preference judgments.8 The pat-
tern of results matches our observations in Expt. 1:
Among individual metrics, METEORR and TERR
do better than BLEUR and NISTR. MTR and RTER
outperform individual metrics. The best result by a
wide margin, 52.5%, is shown by MT+RTER.
6We also experimented with a logistic regression model
that predicts binary preferences directly. Its performance is
comparable; see Pado? et al (2009) for details.
7Available from http://www.statmt.org/.
8The random baseline is not 50%, but, according to our
experiments, 39.8%. This has two reasons: (1) the judgments
include contradictory and tie annotations that cannot be pre-
dicted correctly (raw inter-annotator agreement on WMT 2008
was 58%); (2) metrics have to submit a total order over the
translations for each sentence, which introduces transitivity
constraints. For details, see Callison-Burch et al (2008).
302
Segment MTR RTER MT+RTER Gold
REF: Scottish NHS boards need to improve criminal records checks for
employees outside Europe, a watchdog has said.
HYP: The Scottish health ministry should improve the controls on extra-
community employees to check whether they have criminal precedents,
said the monitoring committee. [1357, lium-systran]
Rank: 3 Rank: 1 Rank: 2 Rank: 1
REF: Arguments, bullying and fights between the pupils have extended
to the relations between their parents.
HYP: Disputes, chicane and fights between the pupils transposed in
relations between the parents. [686, rbmt4]
Rank: 5 Rank: 2 Rank: 4 Rank: 5
Table 3: Expt. 2: Reference translations and MT output (French). Ranks are out of five (smaller is better).
Feature set Consis-
tency (%)
System-level
correlation (?)
BLEUR 49.6 69.3
METEORR 51.1 72.6
NISTR 50.2 70.4
TERR 51.2 72.5
MTR 51.5 73.1
RTER 51.8 78.3
MT+RTER 52.5 75.8
WMT 08 (worst) 44 37
WMT 08 (best) 56 83
Table 4: Expt. 2: Prediction of pairwise preferences
on the WMT 2008 dataset.
The right column shows Spearman?s ? for the
correlation between human judgments and tie-
aware system-level predictions. All metrics predict
system scores highly significantly, partly due to the
larger number of systems compared (87 systems).
Again, we see better results for METEORR and
TERR than for BLEUR and NISTR, and the indi-
vidual metrics do worse than the combination mod-
els. Among the latter, the order is: MTR (worst),
MT+RTER, and RTER (best at 78.3).
WMT 2009. We submitted the Expt. 2 RTER
metric to the WMT 2009 shared MT evaluation
task (Pado? et al, 2009). The results provide fur-
ther validation for our results and our general ap-
proach. At the system level, RTER made third place
(avg. correlation ? = 0.79), trailing the two top met-
rics closely (? = 0.80, ? = 0.83) and making the
best predictions for Hungarian. It also obtained the
second-best consistency score (53%, best: 54%).
Metric comparison. The pairwise preference an-
notation of WMT 2008 gives us the opportunity to
compare the MTR and RTER models by comput-
ing consistency separately on the ?top? (highest-
ranked) and ?bottom? (lowest-ranked) hypotheses
for each reference. RTER performs about 1.5 per-
cent better on the top than on the bottom hypothe-
ses. The MTR model shows the inverse behavior,
performing 2 percent worse on the top hypothe-
ses. This matches well with our intuitions: We see
some noise-induced degradation for the entailment
features, but not much. In contrast, surface-based
features are better at detecting bad translations than
at discriminating among good ones.
Table 3 further illustrates the difference between
the top models on two example sentences. In the top
example, RTER makes a more accurate prediction
than MTR. The human rater?s favorite translation
deviates considerably from the reference in lexi-
cal choice, syntactic structure, and word order, for
which it is punished by MTR (rank 3/5). In contrast,
RTER determines correctly that the propositional
content of the reference is almost completely pre-
served (rank 1). In the bottom example, RTER?s
prediction is less accurate. This sentence was rated
as bad by the judge, presumably due to the inap-
propriate main verb translation. Together with the
subject mismatch, MTR correctly predicts a low
score (rank 5/5). RTER?s attention to semantic over-
lap leads to an incorrect high score (rank 2/5).
Feature Weights. Finally, we make two observa-
tions about feature weights in the RTER model.
First, the model has learned high weights not
only for the overall alignment score (which be-
haves most similarly to traditional metrics), but also
for a number of binary syntacto-semantic match
and mismatch features. This confirms that these
features systematically confer the benefit we have
shown anecdotally in Table 2. Features with a con-
sistently negative effect include dropping adjuncts,
unaligned or poorly aligned root nodes, incompat-
ible modality between the main clauses, person
and location mismatches (as opposed to general
mismatches) and wrongly handled passives. Con-
303
versely, higher scores result from factors such as
high alignment score, matching embeddings under
factive verbs, and matches between appositions.
Second, good MT evaluation feature weights are
not good weights for RTE. Some differences, par-
ticularly for structural features, are caused by the
low grammaticality of MT data. For example, the
feature that fires for mismatches between depen-
dents of predicates is unreliable on the WMT data.
Other differences do reflect more fundamental dif-
ferences between the two tasks (cf. Section 3). For
example, RTE puts high weights onto quantifier
and polarity features, both of which have the poten-
tial of influencing entailment decisions, but are (at
least currently) unimportant for MT evaluation.
7 Related Work
Researchers have exploited various resources to en-
able the matching between words or n-grams that
are semantically close but not identical. Banerjee
and Lavie (2005) and Chan and Ng (2008) use
WordNet, and Zhou et al (2006) and Kauchak
and Barzilay (2006) exploit large collections of
automatically-extracted paraphrases. These ap-
proaches reduce the risk that a good translation
is rated poorly due to lexical deviation, but do not
address the problem that a translation may contain
many long matches while lacking coherence and
grammaticality (cf. the bottom example in Table 2).
Thus, incorporation of syntactic knowledge has
been the focus of another line of research. Amigo?
et al (2006) use the degree of overlap between the
dependency trees of reference and hypothesis as a
predictor of translation quality. Similar ideas have
been applied by Owczarzak et al (2008) to LFG
parses, and by Liu and Gildea (2005) to features
derived from phrase-structure tress. This approach
has also been successful for the related task of
summarization evaluation (Hovy et al, 2006).
The most comparable work to ours is Gime?nez
and Ma?rquez (2008). Our results agree on the cru-
cial point that the use of a wide range of linguistic
knowledge in MT evaluation is desirable and im-
portant. However, Gime?nez and Ma?rquez advocate
the use of a bottom-up development process that
builds on a set of ?heterogeneous?, independent
metrics each of which measures overlap with re-
spect to one linguistic level. In contrast, our aim
is to provide a ?top-down?, integrated motivation
for the features we integrate through the textual
entailment recognition paradigm.
8 Conclusion and Outlook
In this paper, we have explored a strategy for the
evaluation of MT output that aims at comprehen-
sively assessing the meaning equivalence between
reference and hypothesis. To do so, we exploit the
common ground between MT evaluation and the
Recognition of Textual Entailment (RTE), both of
which have to distinguish valid from invalid lin-
guistic variation. Conceputalizing MT evaluation
as an entailment problem motivates the use of a
rich feature set that covers, unlike almost all earlier
metrics, a wide range of linguistic levels, including
lexical, syntactic, and compositional phenomena.
We have used an off-the-shelf RTE system to
compute these features, and demonstrated that a
regression model over these features can outper-
form an ensemble of traditional MT metrics in two
experiments on different datasets. Even though the
features build on deep linguistic analysis, they are
robust enough to be used in a real-world setting, at
least on written text. A limited amount of training
data is sufficient, and the weights generalize well.
Our data analysis has confirmed that each of the
feature groups contributes to the overall success of
the RTE metric, and that its gains come from its
better success at abstracting away from valid vari-
ation (such as word order or lexical substitution),
while still detecting major semantic divergences.
We have also clarified the relationship between MT
evaluation and textual entailment: The majority of
phenomena (but not all) that are relevant for RTE
are also informative for MT evaluation.
The focus of this study was on the use of an ex-
isting RTE infrastructure for MT evaluation. Future
work will have to assess the effectiveness of individ-
ual features and investigate ways to customize RTE
systems for the MT evaluation task. An interesting
aspect that we could not follow up on in this paper
is that entailment features are linguistically inter-
pretable (cf. Fig. 2) and may find use in uncovering
systematic shortcomings of MT systems.
A limitation of our current metric is that it is
language-dependent and relies on NLP tools in
the target language that are still unavailable for
many languages, such as reliable parsers. To some
extent, of course, this problem holds as well for
state-of-the-art MT systems. Nevertheless, it must
be an important focus of future research to develop
robust meaning-based metrics for other languages
that can cash in the promise that we have shown
for evaluating translation into English.
304
References
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and
Llu??s Ma`rquez. 2006. MT Evaluation: Human-
like vs. human acceptable. In Proceedings of COL-
ING/ACL 2006, pages 17?24, Sydney, Australia.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures, pages 65?72, Ann Ar-
bor, MI.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU
in machine translation research. In Proceedings of
EACL, pages 249?256, Trento, Italy.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the ACL Workshop on Statistical Ma-
chine Translation, pages 70?106, Columbus, OH.
Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM: A
maximum similarity metric for machine translation
evaluation. In Proceedings of ACL-08: HLT, pages
55?62, Columbus, Ohio, June.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entailment
challenge. In Proceedings of the PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Southampton, UK.
Marie-Catherine de Marneffe, Trond Grenager, Bill
MacCartney, Daniel Cer, Daniel Ramage, Chloe?
Kiddon, and Christopher D. Manning. 2007. Align-
ing semantic graphs for textual inference and ma-
chine reading. In Proceedings of the AAAI Spring
Symposium, Stanford, CA.
George Doddington. 2002. Automatic evaluation of
machine translation quality using n-gram cooccur-
rence statistics. In Proceedings of HLT, pages 128?
132, San Diego, CA.
Jesu?s Gime?nez and Llu??s Ma?rquez. 2008. Het-
erogeneous automatic MT evaluation through non-
parametric metric combinations. In Proceedings of
IJCNLP, pages 319?326, Hyderabad, India.
Sanda Harabagiu and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain ques-
tion answering. In Proceedings of ACL, pages 905?
912, Sydney, Australia.
Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Junichi
Fukumoto. 2006. Automated summarization evalu-
ation with basic elements. In Proceedings of LREC,
Genoa, Italy.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for automatic evaluation. In Proceedings of HLT-
NAACL, pages 455?462.
Phillip Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
MT Summit X, Phuket, Thailand.
Chin-Yew Lin and Franz Josef Och. 2004. ORANGE:
a method for evaluating automatic evaluation met-
rics for machine translation. In Proceedings of COL-
ING, pages 501?507, Geneva, Switzerland.
Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In Proceed-
ings of the ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures, pages 25?32, Ann Arbor, MI.
Bill MacCartney and Christopher D. Manning. 2008.
Modeling semantic containment and exclusion in
natural language inference. In Proceedings of COL-
ING, pages 521?528, Manchester, UK.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Man-
ning. 2006. Learning to recognize features of
valid textual entailments. In Proceedings of NAACL,
pages 41?48, New York City, NY.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167, Sapporo, Japan.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2008. Evaluating machine translation with
LFG dependencies. Machine Translation, 21(2):95?
119.
Sebastian Pado?, Michel Galley, Dan Jurafsky, and
Christopher D. Manning. 2009. Textual entailment
features for machine translation evaluation. In Pro-
ceedings of the EACL Workshop on Statistical Ma-
chine Translation, pages 37?41, Athens, Greece.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318, Philadelphia, PA.
Michael Paul, Andrew Finch, and Eiichiro Sumita.
2007. Reducing human assessment of machine
translation quality to binary classifiers. In Proceed-
ings of TMI, pages 154?162, Sko?vde, Sweden.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In Proceedings of AMTA, pages 223?231, Cam-
bridge, MA.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with
a tunable MT metric. In Proceedings of the EACL
Workshop on Statistical Machine Translation, pages
259?268, Athens, Greece.
Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006.
Re-evaluating machine translation results with para-
phrase support. In Proceedings of EMNLP, pages
77?84, Sydney, Australia.
305
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 773?781,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Quadratic-Time Dependency Parsing for Machine Translation
Michel Galley
Computer Science Department
Stanford University
Stanford, CA 94305-9020
mgalley@cs.stanford.edu
Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305-9010
manning@cs.stanford.edu
Abstract
Efficiency is a prime concern in syntactic MT de-
coding, yet significant developments in statisti-
cal parsing with respect to asymptotic efficiency
haven?t yet been explored in MT. Recently,
McDonald et al (2005b) formalized dependency
parsing as a maximum spanning tree (MST) prob-
lem, which can be solved in quadratic time relative
to the length of the sentence. They show that MST
parsing is almost as accurate as cubic-time depen-
dency parsing in the case of English, and that it
is more accurate with free word order languages.
This paper applies MST parsing to MT, and de-
scribes how it can be integrated into a phrase-based
decoder to compute dependency language model
scores. Our results show that augmenting a state-of-
the-art phrase-based system with this dependency
language model leads to significant improvements
in TER (0.92%) and BLEU (0.45%) scores on five
NIST Chinese-English evaluation test sets.
1 Introduction
Hierarchical approaches to machine translation
have proven increasingly successful in recent
years (Chiang, 2005; Marcu et al, 2006; Shen
et al, 2008), and often outperform phrase-based
systems (Och and Ney, 2004; Koehn et al, 2003)
on target-language fluency and adequacy. How-
ever, their benefits generally come with high com-
putational costs, particularly when chart parsing,
such as CKY, is integrated with language models
of high orders (Wu, 1996). Indeed, synchronous
CFG parsing with m-grams runs in O(n3m) time,
where n is the length of the sentence.1
Furthermore, synchronous CFG approaches of-
ten only marginally outperform the most com-
1The algorithmic complexity of (Wu, 1996) is
O(n3+4(m?1)), though Huang et al (2005) present a
more efficient factorization inspired by (Eisner and Satta,
1999) that yields an overall complexity of O(n3+3(m?1)),
i.e., O(n3m). In comparison, phrase-based decoding can run
in linear time if a distortion limit is imposed. Of course, this
comparison holds only for approximate algorithms. Since
exact MT decoding is NP complete (Knight, 1999), there is
no exact search algorithm for either phrase-based or syntactic
MT that runs in polynomial time (unless P = NP).
petitive phrase-based systems in large-scale ex-
periments such as NIST evaluations.2 This lack
of significant difference may not be completely
surprising. Indeed, researchers have shown that
gigantic language models are key to state-of-
the-art performance (Brants et al, 2007), and
the ability of phrase-based decoders to handle
large-size, high-order language models with no
consequence on asymptotic running time during
decoding presents a compelling advantage over
CKY decoders, whose time complexity grows pro-
hibitively large with higher-order language mod-
els.
While context-free decoding algorithms (CKY,
Earley, etc.) may sometimes appear too computa-
tionally expensive for high-end statistical machine
translation, there are many alternative parsing al-
gorithms that have seldom been explored in the
machine translation literature. The parsing liter-
ature presents faster alternatives for both phrase-
structure and dependency trees, e.g., O(n) shift-
reduce parsers and variants ((Ratnaparkhi, 1997;
Nivre, 2003), inter alia). While deterministic
parsers are often deemed inadequate for dealing
with ambiguities of natural language, highly accu-
rate O(n2) algorithms exist in the case of depen-
dency parsing. Building upon the theoretical work
of (Chu and Liu, 1965; Edmonds, 1967), McDon-
ald et al (2005b) present a quadratic-time depen-
dency parsing algorithm that is just 0.7% less ac-
curate than ?full-fledged? chart parsing (which, in
the case of dependency parsing, runs in timeO(n3)
(Eisner, 1996)).
In this paper, we show how to exploit syn-
tactic dependency structure for better machine
translation, under the constraint that the depen-
2Results of the 2008 NIST Open MT evaluation
(http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/
mt08_official_results_v0.html) reveal that, while many of
the best systems in the Chinese-English and Arabic-English
tasks incorporate synchronous CFG models, score differ-
ences with the best phrase-based system were insignificantly
small.
773
dency structure is built as a by-product of phrase-
based decoding, without reliance on a dynamic-
programming or chart parsing algorithm such as
CKY or Earley. Adapting the approach of Mc-
Donald et al (2005b) for machine translation, we
incrementally build dependency structure left-to-
right in time O(n2) during decoding. Most in-
terestingly, the time complexity of non-projective
dependency parsing remains quadratic as the or-
der of the language model increases. This pro-
vides a compelling advantage over previous de-
pendency language models for MT (Shen et al,
2008), which use a 5-gram LM only during rerank-
ing. In our experiments, we build a competi-
tive baseline (Koehn et al, 2007) incorporating a
5-gram LM trained on a large part of Gigaword
and show that our dependency language model
provides improvements on five different test sets,
with an overall gain of 0.92 in TER and 0.45 in
BLEU scores. These results are found to be statis-
tically very significant (p? .01).
2 Dependency parsing for machine
translation
In this section, we review dependency parsing for-
mulated as a maximum spanning tree problem
(McDonald et al, 2005b), which can be solved in
quadratic time, and then present its adaptation and
novel application to phrase-based decoding.
Dependency models have recently gained con-
siderable interest in many NLP applications, in-
cluding machine translation (Ding and Palmer,
2005; Quirk et al, 2005; Shen et al, 2008). De-
pendency structure provides several compelling
advantages compared to other syntactic represen-
tations. First, dependency links are close to the se-
mantic relationships, which are more likely to be
consistent across languages. Indeed, Fox (2002)
found inter-lingual phrasal cohesion to be greater
than for a CFG when using a dependency rep-
resentation, for which she found only 12.6% of
head crossings and 9.2% modifier crossings. Sec-
ond, dependency trees contain exactly one node
per word, which contributes to cutting down the
search space during parsing: indeed, the task of
the parser is merely to connect existing nodes
rather than hypothesizing new ones. Finally, de-
pendency models are more flexible and account
for (non-projective) head-modifier relations that
CFG models fail to represent adequately, which
is problematic with certain types of grammatical
constructions and with free word order languages,
who do you think they hired ? WP VB PRP VB PRP VBD . 1 2 3 4 5 6 7 
<root> <root> 0 
Figure 1: A dependency tree with directed edges going from
heads to modifiers. The edge between who and hired causes
this tree to be non-projective. Such a head-modifier relation-
ship is difficult to represent with a CFG, since all words di-
rectly or indirectly headed by hired (i.e., who, think, they, and
hired) do not constitute a contiguous sequence of words.
as we will see later in this section.
The most standardly used algorithm for parsing
with dependency grammars is presented in (Eis-
ner, 1996; Eisner and Satta, 1999). It runs in time
O(n3), where n is the length of the sentence. Their
algorithm exploits the special properties of depen-
dency trees to reduce the worst-case complexity of
bilexical parsing, which otherwise requires O(n4)
for bilexical constituency-based parsing. While it
seems difficult to improve the asymptotic running
time of the Eisner algorithm beyond what is pre-
sented in (Eisner and Satta, 1999), McDonald et
al. (2005b) show O(n2)-time parsing is possible if
trees are not required to be projective. This re-
laxation entails that dependencies may cross each
other rather than being required to be nested, as
shown in Fig. 1. More formally, a non-projective
tree is any tree that does not satisfy the following
definition of a projective tree:
Definition. Let x = x1 ? ? ?xn be an input sentence,
and let y be a rooted tree represented as a set
in which each element (i, j) ? y is an ordered
pair of word indices of x that defines a depen-
dency relation between a head xi and a modifier
x j. By definition, the tree y is said to be projec-
tive if each dependency (i, j) satisfies the follow-
ing property: each word in xi+1 ? ? ?x j?1 (if i < j)
or in x j+1 ? ? ?xi?1 (if j < i) is a descendent of head
word xi.
This relaxation is key to computational effi-
ciency, since the parser does not need to keep
track of whether dependencies assemble into con-
tiguous spans. It is also linguistically desirable
in the case of free word order languages such as
Czech, Dutch, and German. Non-projective de-
pendency structures are sometimes even needed
for languages like English, e.g., in the case of the
wh-movement shown in Fig. 1. For languages
774
with relatively rigid word order such as English,
there may be some concern that searching the
space of non-projective dependency trees, which
is considerably larger than the space of projective
dependency trees, would yield poor performance.
That is not the case: dependency accuracy for non-
projective parsing is 90.2% for English (McDon-
ald et al, 2005b), only 0.7% lower than a projec-
tive parser (McDonald et al, 2005a) that uses the
same set of features and learning algorithm. In the
case of dependency parsing for Czech, (McDonald
et al, 2005b) even outperforms projective parsing,
and was one of the top systems in the CoNLL-06
shared task in multilingual dependency parsing.
2.1 O(n2)-time dependency parsing for MT
We now formalize weighted non-projective de-
pendency parsing similarly to (McDonald et al,
2005b) and then describe a modified and more ef-
ficient version that can be integrated into a phrase-
based decoder.
Given the single-head constraint, parsing an in-
put sentence x = (x0,x1, ? ? ? ,xn) is reduced to la-
beling each word x j with an index i identifying its
head word xi. We include the dummy root symbol
x0 = ?root? so that each word can be a modifier.
We score each dependency relation using a stan-
dard linear model
s(i, j) = ? ? f(i, j) (1)
whose weight vector ? is trained using
MIRA (Crammer and Singer, 2003) to opti-
mize dependency parsing accuracy (McDonald et
al., 2005a). As is commonly the case in statistical
parsing, the score of the full tree is decomposed
as the sum of the score of all edges:
s(x,y) = ?
(i, j)?y
? ? f(i, j) (2)
When there is no need to ensure projectivity, one
can independently select the highest scoring edge
(i, j) for each modifier x j, yet we generally want to
ensure that the resulting structure is a tree, i.e., that
it does not contain any circular dependencies. This
optimization problem is a known instance of the
maximum spanning tree (MST) problem. In our
case, the graph is directed?indeed, the equality
s(i, j) = s( j, i) is generally not true and would be
linguistically aberrant?so the problem constitutes
an instance of the less-known MST problem for
directed graphs. This problem is solved with the
Chu-Liu-Edmonds (CLE) algorithm (Chu and Liu,
1965; Edmonds, 1967).
Formally, we represent the graph G = (V,E)
with a vertex set V = x = {x0, ? ? ? ,xn} and a set
of directed edges E = [0,n]? [1,n], in which each
edge (i, j), representing the dependency xi ? x j,
is assigned a score s(i, j). Finding the spanning
tree y ? E rooted at x0 that maximizes s(x,y) as
defined in Equation 2 has a straightforward solu-
tion in O(n2 log(n)) time for dense graphs such as
G, though Tarjan (1977) shows that the problem
can be solved in O(n2). Hence, non-projective
dependency parsing is solved in quadratic time.
The main idea behind the CLE algorithm is to
first greedily select for each word x j the incom-
ing edge (i, j) with highest score, then to succes-
sively repeat the following two steps: (a) identify
a loop in the graph, and if there is none, halt; (b)
contract the loop into a single vertex, and update
scores for edges coming in and out of the loop.
Once all loops have been eliminated, the algorithm
maps back the maximum spanning tree of the con-
tracted graph onto the original graph G, and it can
be shown that this yields a spanning tree that is op-
timal with respect to G and s (Georgiadis, 2003).
The greedy approach of selecting the highest
scoring edge (i, j) for each modifier x j can
easily be applied left-to-right during phrase-based
decoding, which proceeds in the same order.
For each hypothesis expansion, our decoder
generates the following information for the new
hypothesis h:
? a partial translation x;
? a coverage set of input words c;
? a translation score ? .
In the case of non-projective dependency parsing,
we need to maintain additional information for
each word x j of the partial translation x:
? a predicted POS tag t j;
? a dependency score s j.
Dependency scores s j are initialized to ??.
Each time a new word is added to a partial hy-
pothesis, the decoder executes the routine shown
in Table 1. To avoid cluttering the pseudo-code,
we make here the simplifying assumption that
each hypothesis expansion adds exactly one word,
though the real implementation supports the case
of phrases of any length. Line 3 determines
whether the translation hypothesis is complete, in
which case it explicitly builds the graph G and
775
Decoding: hypothesis expansion step.
1. Inferer generates new hypothesis h = (x,c,?)
2. j? |x|?1
3. t j? tagger(x j?3, ? ? ? ,x j)
4. if complete(c)
5. Chu-Liu-Edmonds(h)
6. else
7. for i = 1 to j
8. s j = max(s j,s(i, j))
9. si = max(si,s( j, i))
Table 1: Hypothesis expansion with dependency scoring.
finds the maximum spanning tree. Note that it is
impractical to identify loops each time a new word
is added to a translation hypothesis, since this re-
quires explicitly storing the dense graph G, which
would require an O(n2) copy operation during
each hypothesis expansion; this would of course
increase time and space complexity (the max op-
eration in lines 8 and 9 only keeps the current best
scoring edges). If there is any loop, the depen-
dency score is adjusted in the last hypothesis ex-
pansion. In practice, we delay the computation of
dependency scores involving word x j until tag t j+1
is generated, since dependency parsing accuracy is
particularly low (?0.8%) when the next tag is un-
known.
We found that dependency scores with or with-
out loop elimination are generally close and highly
correlated, and that MT performance without fi-
nal loop removal was about the same (generally
less than 0.2% BLEU). While it seems that loopy
graphs are undesirable when the goal is to obtain a
syntactic analysis, that is not necessarily the case
when one just needs a language modeling score.
2.2 Features for dependency parsing
In our experiments, we use sets of features that are
similar to the ones used in the McDonald parser,
though we make a key modification that yields an
asymptotic speedup that ensures a genuine O(n2)
running time.
The three feature sets that were used in our ex-
periments are shown in Table 2. We write h-word,
h-pos, m-word, m-pos to refer to head and modi-
fier words and POS tags, and append a numerical
value to shift the word offset either to the left or to
the right (e.g., h-pos+1 is the POS to the right of
the head word). We use the symbol ? to represent
feature conjunctions. Each feature in the table has
a distinct identifier, so that, e.g., the POS features
Unigram features:
h-word, h-pos, h-word ? h-pos,
m-word, m-pos, m-word ? m-pos
Bigram features:
h-word ? m-word, h-pos ? m-pos,
h-word ? h-pos ? m-word, h-word ? h-pos ? m-pos,
m-word ? m-pos ? h-word, m-word ? m-pos ? h-pos,
h-word ? h-pos ? m-word ? m-pos
Adjacent POS features:
h-pos ? h-pos+1 ? m-pos?1 ? m-pos,
h-pos ? h-pos+1 ? m-pos ? m-pos+1,
h-pos?1 ? h-pos ? m-pos?1 ? m-pos,
h-pos?1 ? h-pos ? m-pos ? m-pos+1
In-between POS features:
if i < j:
h-pos ? h-pos+k ? m-pos k ? [ i,min(i+5, j) ]
h-pos ? m-pos?k ? m-pos k ? [max(i, j?5), j ]
if i > j:
m-pos ? m-pos+k ? h-pos k ? [ j,min( j+5, i) ]
m-pos ? h-pos?k ? h-pos k ? [max( j, i?5), i ]
Table 2: Features for dependency parsing. It is quite similar
to the McDonald (2005a) feature set, except that it does not
include the set of all POS tags that appear between each can-
didate head-modifier pair (i, j). This modification is essential
in order to make our parser run in trueO(n2) time, as opposed
to (McDonald et al, 2005b).
SOURCE IDS GENRE SENTENCES
English CTB 050?325 newswire 3027
English ATB all newswire 13628
OntoNotes all broadcast news 14056
WSJ 02?21 financial news 39832
Total 70543
Table 3: Characteristics of our training data. The second col-
umn identifies documents and sections selected for training.
h-pos are all distinct from m-pos features.3
The primary difference between our feature sets
and the ones of McDonald et al is that their set of
?in between POS features? includes the set of all
tags appearing between each pair of words. Ex-
tracting all these tags takes time O(n) for any arbi-
trary pair (i, j). Since i and j are both free vari-
ables, feature computation in (McDonald et al,
2005b) takes time O(n3), even though parsing it-
self takes O(n2) time. To make our parser gen-
uinely O(n2), we modified the set of in-between
POS features in two ways. First, we restrict ex-
traction of in-between POS tags to those words
that appear within a window of five words rel-
ative to either the head or the modifier. While
this change alone ensures that feature extraction is
now O(1) for each word pair, this causes a fairly
high drop of performance (dependency accuracy
3In addition to these basic features, we follow McDonald
in conjoining most features with two extra pieces of infor-
mation: a boolean variable indicating whether the modifier
attaches to the left or to the right, and the binned distance
between the two words.
776
ALGORITHM TIME SETUP TRAINING TESTING ACCURACY
Projective O(n3) Parsing WSJ(02-21) WSJ(23) 90.60
Chu-Liu-Edmonds O(n3) Parsing WSJ(02-21) WSJ(23) 89.64
Chu-Liu-Edmonds O(n2) Parsing WSJ(02-21) WSJ(23) 89.32
Local classifier O(n2) Parsing WSJ(02-21) WSJ(23) 89.15
Projective O(n3) MT CTB(050-325) CTB(001-049) 86.33
Chu-Liu-Edmonds O(n3) MT CTB(050-325) CTB(001-049) 85.68
Chu-Liu-Edmonds O(n2) MT CTB(050-325) CTB(001-049) 85.43
Local classifier O(n2) MT CTB(050-325) CTB(001-049) 85.22
Projective O(n3) MT CTB(050-325), WSJ(02-21), ATB, OntoNotes CTB(001-049) 87.40(**)
Chu-Liu-Edmonds O(n3) MT CTB(050-325), WSJ(02-21), ATB, OntoNotes CTB(001-049) 86.79
Chu-Liu-Edmonds O(n2) MT CTB(050-325), WSJ(02-21), ATB, OntoNotes CTB(001-049) 86.45(*)
Local classifier O(n2) MT CTB(050-325), WSJ(02-21), ATB, OntoNotes CTB(001-049) 86.29
Table 4: Dependency parsing experiments on test sentences of any length. The projective parsing algorithm is the one imple-
mented as in (McDonald et al, 2005a), which is known as one of the top performing dependency parsers for English. The O(n3)
non-projective parser of (McDonald et al, 2005b) is slightly more accurate than our version, though ours runs in O(n2) time.
?Local classifier? refers to non-projective dependency parsing without removing loops as a post-processing step. The result
marked with (*) identifies the parser used for our MT experiments, which is only about 1% less accurate than a state-of-the-art
dependency parser (**).
on our test was down 0.9%). To make our gen-
uinely O(n2) parser almost as accurate as the non-
projective parser of McDonald et al, we conjoin
each in-between POS with its position relative to
(i, j). This relatively simple change reduces the
drop in accuracy to only 0.34%.4
3 Dependency parsing experiments
In this section, we compare the performance of
our parsing model to the ones of McDonald et al
Since our MT test sets include newswire, web, and
audio, we trained our parser on different genres.
Our training data includes newswire from the En-
glish translation treebank (LDC2007T02) and the
English-Arabic Treebank (LDC2006T10), which
are respectively translations of sections of the Chi-
nese treebank (CTB) and Arabic treebank (ATB).
We also trained the parser on the broadcast-
news treebank available in the OntoNotes corpus
(LDC2008T04), and added sections 02-21 of the
WSJ Penn treebank. Documents 001-040 of the
English CTB data were set aside to constitute a
test set for newswire texts. Our other test set is
the standard Section 23 of the Penn treebank. The
splits and amounts of data used for training are dis-
played in Table 3.
Parsing experiments are shown in Table 4. We
4We need to mention some practical considerations that
make feature computation fast enough for MT. Most features
are precomputed before actual decoding. All target-language
words to appear during beam search can be determined in ad-
vance, and all their unigram feature scores are precomputed.
For features conditioned on both head and modifier, scores
are cached whenever possible. The only features that are not
cached are the ones that include contextual POS tags, since
their miss rate is relatively high.
distinguish two experimental conditions: Parsing
and MT. For Parsing, sentences are cased and tok-
enization abides to the PTB segmentation as used
in the Penn treebank version 3. For the MT set-
ting, texts are all lower case, and tokenization
was changed to improve machine translation (e.g.,
most hyphenated words were split). For this set-
ting, we also had to harmonize the four treebanks.
The most crucial modification was to add NP in-
ternal bracketing to the WSJ (Vadas and Curran,
2007), since the three other treebanks contain that
information. Treebanks were also transformed to
be consistent with MT tokenization. We evaluate
MT parsing models on CTB rather than on WSJ,
since CTB contains newswire and is thus more
representative of MT evaluation conditions.
To obtain part-of-speech tags, we use a
state-of-the-art maximum-entropy (CMM) tagger
(Toutanova et al, 2003). In the Parsing setting, we
use its best configuration, which reaches a tagging
accuracy of 97.25% on standard WSJ test data. In
the MT setting, we need to use a less effective tag-
ger, since we cannot afford to perform Viterbi in-
ference as a by-product of phrase-based decoding.
Hence, we use a simpler tagging model that as-
signs tag ti to word xi by only using features of
words xi?3 ? ? ?xi, and that does not condition any
decision based on any preceding or next tags (ti?1,
etc.). Its performance is 95.02% on the WSJ, and
95.30% on the English CTB. Additional experi-
ments reveal two main contributing factors to this
drop on WSJ: tagging uncased texts reduces tag-
ging accuracy by about 1%, and using only word-
based features further reduces it by 0.6%.
Table 4 shows that the accuracy of our truly
777
O(n2) parser is only .25% to .34% worse than
the O(n3) implementation of (McDonald et al,
2005b).5 Compared to the state-of-the-art projec-
tive parser as implemented in (McDonald et al,
2005a), performance is 1.28% lower on WSJ, but
only 0.95% when training on all our available data
and using the MT setting. Overall, we believe that
the drop of performance is a reasonable price to
pay considering the computational constraints im-
posed by integrating the dependency parser into an
MT decoder.
The table also shows a gain of more than 1% in
dependency accuracy by adding ATB, OntoNotes,
and WSJ to the English CTB training set. The
four sources were assigned non-uniform weights:
we set the weight of the CTB data to be 10 times
larger than the other corpora, which seems to work
best in our parsing experiments. While this im-
provement of 1% may seem relatively small con-
sidering that the amount of training data is more
than 20 times larger in the latter case, it is quite
consistent with previous findings in domain adap-
tation, which is known to be a difficult task. For
example, (Daume III, 2007) shows that training a
learning algorithm on the weighted union of dif-
ferent data sets (which is basically what we did)
performs almost as well as more involved domain
adaptation approaches.
4 Machine translation experiments
In our experiments, we use a re-implementation
of the Moses phrase-based decoder (Koehn et
al., 2007). We use the standard features imple-
mented almost exactly as in Moses: four trans-
lation features (phrase-based translation probabil-
ities and lexically-weighted probabilities), word
penalty, phrase penalty, linear distortion, and lan-
guage model score. We also incorporated the lex-
icalized reordering features of Moses, in order to
experiment with a baseline that is stronger than the
default Moses configuration.
The language pair for our experiments is
Chinese-to-English. The training data consists of
about 28 million English words and 23.3 million
5Note that our results on WSJ are not exactly the same
as those reported in (McDonald et al, 2005b), since we used
slightly different head finding rules. To extract dependencies
from treebanks, we used the LTH Penn Converter (http://
nlp.cs.lth.se/pennconverter/), which extracts
dependencies that are almost identical to those used for the
CoNLL-2008 Shared Task. We constrain the converter not to
use functional tags found in the treebanks, in order to make it
possible to use automatically parsed texts (i.e., perform self-
training) in future work.
Chinese words drawn from various news parallel
corpora distributed by the Linguistic Data Con-
sortium (LDC). In order to provide experiments
comparable to previous work, we used the same
corpora as (Wang et al, 2007): LDC2002E18,
LDC2003E07, LDC2003E14, LDC2005E83,
LDC2005T06, LDC2006E26, LDC2006E8, and
LDC2006G05. Chinese words were automatically
segmented with a conditional random field (CRF)
classifier (Chang et al, 2008) that conforms to the
Chinese Treebank (CTB) standard.
In order to train a competitive baseline given our
computational resources, we built a large 5-gram
language model using the Xinhua and AFP sec-
tions of the Gigaword corpus (LDC2007T40) in
addition to the target side of the parallel data.
This data represents a total of about 700 mil-
lion words. We manually removed documents of
Gigaword that were released during periods that
overlap with those of our development and test
sets. The language model was smoothed with the
modified Kneser-Ney algorithm as implemented
in (Stolcke, 2002), and we only kept 4-grams and
5-grams that occurred at least three times in the
training data.6
For tuning and testing, we use the official NIST
MT evaluation data for Chinese from 2002 to 2008
(MT02 to MT08), which all have four English ref-
erences for each input sentence. We used the 1082
sentences of MT05 for tuning and all other sets for
testing. Parameter tuning was done with minimum
error rate training (Och, 2003), which was used
to maximize BLEU (Papineni et al, 2001). Since
MERT is prone to search errors, especially with
large numbers of parameters, we ran each tuning
experiment three times with different initial condi-
tions. We used n-best lists of size 200 and a beam
size of 200. In the final evaluations, we report re-
sults using both TER (Snover et al, 2006) and the
original BLEU metric as described in (Papineni et
al., 2001). All our evaluations are performed on
uncased texts.
The results for our translation experiments are
shown in Table 5. We compared two systems: one
with the set of features described earlier in this
section. The second system incorporates one ad-
ditional feature, which is the dependency language
6We found that sections of Gigaword other than Xinhua
and AFP provide almost no improvement in our experiments.
By leaving aside the other sections, we were able to increase
the order of the language model to 5-gram and perform rela-
tively little pruning. This LM required 16GB of RAM during
training.
778
BLEU[%]
DEP. LM MT05 (tune) MT02 MT03 MT04 MT06 MT08
no 33.42 33.38 33.13 36.21 32.16 24.83
yes 34.19 (+.77**) 33.85 (+.47) 33.73 (+.6*) 36.67 (+.46*) 32.84 (+.68**) 24.91 (+.08)
TER[%]
DEP. LM MT05 (tune) MT02 MT03 MT04 MT06 MT08
no 57.41 58.07 57.32 56.09 57.24 61.96
yes 56.27 (?1.14**) 57.15 (?.92**) 56.09 (?1.23**) 55.30 (?.79**) 56.05 (?1.19**) 61.41 (?.55*)
MT05 (tune) MT02 MT03 MT04 MT06 MT08
Sentences 1082 878 919 1788 1664 1357
Table 5: MT experiments with and without a dependency language model. We use randomization tests (Riezler and Maxwell,
2005) to determine significance: differences marked with a (*) are significant at the p? .05 level, and those marked as (**) are
significant at the p? .01 level.
model score computed with the dependency pars-
ing algorithm described in Section 2. We used
the dependency model trained on the English CTB
and ATB treebank, WSJ, and OntoNotes.
We see that the Moses decoder with integrated
dependency language model systematically out-
performs the Moses baseline. For BLEU evalu-
ations, differences are significant in four out of
six cases, and in the case of TER, all differences
are significant. Regarding the small difference in
BLEU scores on MT08, we would like to point
out that tuning on MT05 and testing on MT08
had a rather adverse effect with respect to trans-
lation length: while the two systems are rela-
tively close in terms of BLEU scores (24.83 and
24.91, respectively), the dependency LM provides
a much bigger gain when evaluated with BLEU
precision (27.73 vs. 28.79), i.e., by ignoring the
brevity penalty. On the other hand, the difference
on MT08 is significant in terms of TER.
Table 6 provides experimental results on the
NIST test data (excluding the tuning set MT05) for
each of the three genres: newswire, web data, and
speech (broadcast news and conversation). The
last column displays results for all test sets com-
bined. Results do not suggest any noticeable dif-
ference between genres, and the dependency lan-
guage model provides significant gains on all gen-
res, despite the fact that this model was primarily
trained on news data.
We wish to emphasize that our positive re-
sults are particularly noteworthy because they are
achieved over a baseline incorporating a compet-
itive 5-gram language model. As is widely ac-
knowledged in the speech community, it can be
difficult to outperform high-order n-gram models
in large-scale experiments. Finally, we quantified
the effective running time of our phrase-based de-
coder with and without our dependency language
BLEU[%]
DEP. LM newswire web speech all
no 32.86 21.75 36.88 32.29
yes 33.19 22.64 37.51 32.74
(+0.33) (+0.89) (+0.63) (+0.45)
TER[%]
DEP. LM newswire web speech all
no 57.73 62.64 55.16 58.02
yes 56.73 61.97 54.26 57.10
(?1) (?0.67) (?0.9) (?0.92)
newswire web speech all
Sentences 4006 1149 1451 6606
Table 6: Test set performances on MT02-MT04 and MT06-
MT08, where the data was broken down by genre. Given
the large amount of test data involved in this table, all these
results are statistically highly significant (p? .01).
10 20 30 40 50 60 70 80 900
20
40
60
80
100
120
140
160
sentence length
se
co
nds
 
 depLMbaseline
Figure 2: Running time of our phrase-based decoder with and
without quadratic-time dependency LM scoring.
model using MT05 (Fig. 2). In both settings, we
selected the best tuned model, which yield the per-
formance shown in the first column of Table 5.
Our decoder was run on an AMD Opteron Proces-
sor 2216 with 16GB of memory, and without re-
sorting to any rescoring method such as cube prun-
ing. In the case of English translations of 40 words
and shorter, the baseline system took 6.5 seconds
per sentence, whereas the dependency LM system
spent 15.6 seconds per sentence, i.e., 2.4 times the
baseline running time. In the case of translations
779
longer than 40 words, average speeds were respec-
tively 17.5 and 59.5 seconds per sentence, i.e., the
dependency was only 3.4 times slower.7
5 Related work
Perhaps due to the high computational cost of syn-
chronous CFG decoding, there have been various
attempts to exploit syntactic knowledge and hier-
archical structure in other machine translation ex-
periments that do not require chart parsing. Using
a reranking framework, Och et al (2004) found
that various types of syntactic features provided
only minor gains in performance, suggesting that
phrase-based systems (Och and Ney, 2004) should
exploit such information during rather than after
decoding. Wang et al (2007) sidestep the need to
operate large-scale word order changes during de-
coding (and thus lessening the need for syntactic
decoding) by rearranging input words in the train-
ing data to match the syntactic structure of the
target language. Finally, Birch et al (2007) ex-
ploit factored phrase-based translation models to
associate each word with a supertag, which con-
tains most of the information needed to build a full
parse. When combined with a supertag n-gram
language model, it helps enforce grammatical con-
straints on the target side.
There have been various attempts to reduce the
computational expense of syntactic decoding, in-
cluding multi-pass decoding approaches (Zhang
and Gildea, 2008; Petrov et al, 2008) and rescor-
ing approaches (Huang and Chiang, 2007). In the
latter paper, Huang and Chiang introduce rescor-
ing methods named ?cube pruning? and ?cube
growing?, which first use a baseline decoder (ei-
ther synchronous CFG or a phrase-based sys-
tem) and no LM to generate a hypergraph, and
then rescoring this hypergraph with a language
model. Huang and Chiang show significant speed
increases with little impact on translation quality.
We believe that their approach is orthogonal (and
possibly complementary) to our work, since our
paper proposes a new model for fully-integrated
decoding that increases MT performance, and
does not rely on rescoring.
7We note that our Java-based decoder is research rather
than industrial-strength code and that it could be substantially
optimized. Hence, we think the reader should pay more at-
tention to relative speed differences between the two systems
rather than absolute timings.
6 Conclusion and future work
In this paper, we presented a non-projective de-
pendency parser whose time-complexity of O(n2)
improves upon the cubic time implementation of
(McDonald et al, 2005b), and does so with lit-
tle loss in dependency accuracy (.25% to .34%).
Since this parser does not need to enforce projec-
tivity constraints, it can easily be integrated into
a phrase-based decoder during search (rather than
during rescoring). We use dependency scores as
an extra feature in our MT experiments, and found
that our dependency model provides significant
gains over a competitive baseline that incorporates
a large 5-gram language model (0.92% TER and
0.45% BLEU absolute improvements).
We plan to pursue other research directions us-
ing dependency models discussed in this paper.
While we use a dependency language model to
exemplify the use of hierarchical structure within
phrase based decoders, we could extend this work
to incorporate dependency features of both source-
and target side. Since parsing of the source is rel-
atively inexpensive compared to the target side,
it would be relatively easy to condition head-
modifier dependencies not only on the two tar-
get words, but also on their corresponding Chi-
nese words and their relative positions in the Chi-
nese tree. This would enable the decoder to cap-
ture syntactic reordering without requiring trees to
be isomorphic or even projective. It would also
be interesting to apply these models to target lan-
guages that have free word order, which would
presumably benefit more from the flexibility of
non-projective dependency models.
Acknowledgements
The authors wish to thank the anonymous review-
ers for their helpful comments on an earlier draft
of this paper, and Daniel Cer for his implementa-
tion of Phrasal, a phrase-based decoder similar to
Moses. This paper is based on work funded by
the Defense Advanced Research Projects Agency
through IBM. The content does not necessarily re-
flect the views of the U.S. Government, and no of-
ficial endorsement should be inferred.
References
A. Birch, M. Osborne, and P. Koehn. 2007. CCG su-
pertags in factored statistical machine translation. In
Proc. of the Workshop on Statistical Machine Trans-
lation, pages 9?16.
780
T. Brants, A. Popat, P. Xu, F. Och, and J. Dean. 2007.
Large language models in machine translation. In
Proc. of EMNLP-CoNLL, pages 858?867.
P. Chang, M. Galley, and C. Manning. 2008. Optimiz-
ing Chinese word segmentation for machine transla-
tion performance. In Proc. of the ACL Workshop on
Statistical Machine Translation, pages 224?232.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL,
pages 263?270.
Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. Journal
of Machine Learning Research, 3:951?991.
H. Daume III. 2007. Frustratingly easy domain adap-
tation. In Proc. of ACL, pages 256?263.
Y. Ding and M. Palmer. 2005. Machine translation us-
ing probabilistic synchronous dependency insertion
grammars. In Proc. of ACL, pages 541?548.
J. Edmonds. 1967. Optimum branchings. Research of
the National Bureau of Standards, 71B:233?240.
J. Eisner and G. Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head-
automaton grammars. In Proc. of ACL, pages 457?
464.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. of COL-
ING, pages 340?345.
H. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proc. of EMNLP, pages 304?311.
L. Georgiadis. 2003. Arborescence optimization prob-
lems solvable by Edmonds? algorithm. Theoretical
Computer Science, 301(1-3):427?437.
L. Huang and D. Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proc. of ACL, pages 144?151.
L. Huang, H. Zhang, and D. Gildea. 2005. Ma-
chine translation as lexicalized parsing with hooks.
In Proc. of the International Workshop on Parsing
Technology, pages 65?73.
K. Knight. 1999. Decoding complexity in word-
replacement translation models. Computational
Linguistics, 25(4):607?615.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of ACL,
Demonstration Session.
D. Marcu, W.Wang, A. Echihabi, and K. Knight. 2006.
SPMT: Statistical machine translation with syntact-
ified target language phrases. In Proc. of EMNLP,
pages 44?52.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proc. of ACL, pages 91?98.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005b. Non-projective dependency parsing using
spanning tree algorithms. In Proc. of HLT-EMNLP,
pages 523?530.
J. Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proc. of the Inter-
national Workshop on Parsing Technologies (IWPT
03), pages 149?160.
F. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449.
F. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith,
K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smor-
gasbord of features for statistical machine transla-
tion. In Proceedings of HLT-NAACL.
F. Och. 2003. Minimum error rate training for statisti-
cal machine translation. In Proc. of ACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proc. of ACL.
S. Petrov, A. Haghighi, and D. Klein. 2008. Coarse-
to-fine syntactic machine translation using language
projections. In Proc. of EMNLP, pages 108?116.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: syntactically informed
phrasal SMT. In Proc. of ACL, pages 271?279.
A. Ratnaparkhi. 1997. A linear observed time statis-
tical parser based on maximum entropy models. In
Proc. of EMNLP.
S. Riezler and J. Maxwell. 2005. On some pitfalls
in automatic evaluation and significance testing for
MT. In Proc. of the ACL Workshop on Intrinsic and
Extrinsic Evaluation Measures for Machine Trans-
lation and/or Summarization, pages 57?64.
L. Shen, J. Xu, and R. Weischedel. 2008. A new
string-to-dependency machine translation algorithm
with a target dependency language model. In Proc.
of ACL, pages 577?585.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proc. of AMTA,
pages 223?231.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proc. Intl. Conf. on Spoken
Language Processing (ICSLP?2002).
R. Tarjan. 1977. Finding optimum branchings. Net-
works, 7:25?35.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proc. of NAACL,
pages 173?180.
D. Vadas and J. Curran. 2007. Adding noun phrase
structure to the Penn treebank. In Proc. of ACL,
pages 240?247.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese
syntactic reordering for statistical machine transla-
tion. In Proc. of EMNLP-CoNLL, pages 737?745.
D. Wu. 1996. A polynomial-time algorithm for statis-
tical machine translation. In Proc. of ACL.
H. Zhang and D. Gildea. 2008. Efficient multi-pass
decoding for synchronous context free grammars. In
Proc. of ACL, pages 209?217.
781
Proceedings of the Third Workshop on Statistical Machine Translation, pages 224?232,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Optimizing Chinese Word Segmentation for Machine Translation
Performance
Pi-Chuan Chang, Michel Galley, and Christopher D. Manning
Computer Science Department, Stanford University
Stanford, CA 94305
pichuan,galley,manning@cs.stanford.edu
Abstract
Previous work has shown that Chinese word seg-
mentation is useful for machine translation to En-
glish, yet the way different segmentation strategies
affect MT is still poorly understood. In this pa-
per, we demonstrate that optimizing segmentation
for an existing segmentation standard does not al-
ways yield better MT performance. We find that
other factors such as segmentation consistency and
granularity of Chinese ?words? can be more impor-
tant for machine translation. Based on these find-
ings, we implement methods inside a conditional
random field segmenter that directly optimize seg-
mentation granularity with respect to the MT task,
providing an improvement of 0.73 BLEU. We also
show that improving segmentation consistency us-
ing external lexicon and proper noun features yields
a 0.32 BLEU increase.
1 Introduction
Word segmentation is considered an important first
step for Chinese natural language processing tasks,
because Chinese words can be composed of multi-
ple characters but with no space appearing between
words. Almost all tasks could be expected to ben-
efit by treating the character sequence ?Us? to-
gether, with the meaning smallpox, rather than deal-
ing with the individual characters ?U? (sky) and
?s? (flower). Without a standardized notion of a
word, traditionally, the task of Chinese word seg-
mentation starts from designing a segmentation stan-
dard based on linguistic and task intuitions, and then
aiming to building segmenters that output words that
conform to the standard. One widely used standard
is the Penn Chinese Treebank (CTB) Segmentation
Standard (Xue et al, 2005).
It has been recognized that different NLP ap-
plications have different needs for segmentation.
Chinese information retrieval (IR) systems benefit
from a segmentation that breaks compound words
into shorter ?words? (Peng et al, 2002), parallel-
ing the IR gains from compound splitting in lan-
guages like German (Hollink et al, 2004), whereas
automatic speech recognition (ASR) systems prefer
having longer words in the speech lexicon (Gao et
al., 2005). However, despite a decade of very in-
tense work on Chinese to English machine transla-
tion (MT), the way in which Chinese word segmen-
tation affects MT performance is very poorly under-
stood. With current statistical phrase-based MT sys-
tems, one might hypothesize that segmenting into
small chunks, including perhaps even working with
individual characters would be optimal. This is be-
cause the role of a phrase table is to build domain
and application appropriate larger chunks that are
semantically coherent in the translation process. For
example, even if the word for smallpox is treated as
two one-character words, they can still appear in a
phrase like ?U s?smallpox?, so that smallpox
will still be a candidate translation when the system
translates ?U? ?s?. Nevertheless, Xu et al (2004)
show that an MT system with a word segmenter out-
performs a system working with individual charac-
ters in an alignment template approach. On differ-
ent language pairs, (Koehn and Knight, 2003) and
(Habash and Sadat, 2006) showed that data-driven
methods for splitting and preprocessing can improve
Arabic-English and German-English MT.
Beyond this, there has been no finer-grained anal-
ysis of what style and size of word segmentation is
optimal for MT. Moreover, most discussion of seg-
mentation for other tasks relates to the size units to
identify in the segmentation standard: whether to
join or split noun compounds, for instance. People
224
generally assume that improvements in a system?s
word segmentation accuracy will be monotonically
reflected in overall system performance. This is the
assumption that justifies the concerted recent work
on the independent task of Chinese word segmenta-
tion evaluation at SIGHAN and other venues. How-
ever, we show that this assumption is false: aspects
of segmenters other than error rate are more criti-
cal to their performance when embedded in an MT
system. Unless these issues are attended to, sim-
ple baseline segmenters can be more effective inside
an MT system than more complex machine learning
based models, with much lower word segmentation
error rate.
In this paper, we show that even having a ba-
sic word segmenter helps MT performance, and we
analyze why building an MT system over individ-
ual characters doesn?t function as well. Based on
an analysis of baseline MT results, we pin down
four issues of word segmentation that can be im-
proved to get better MT performance. (i) While a
feature-based segmenter, like a support vector ma-
chine or conditional random field (CRF) model, may
have very good aggregate performance, inconsistent
context-specific segmentation decisions can be quite
harmful to MT system performance. (ii)A perceived
strength of feature-based systems is that they can
generate out-of-vocabulary (OOV) words, but these
can hurt MT performance, when they could have
been split into subparts from which the meaning of
the whole can be roughly compositionally derived.
(iii) Conversely, splitting OOV words into non-
compositional subparts can be very harmful to an
MT system: it is better to produce such OOV items
than to split them into unrelated character sequences
that are known to the system. One big source of such
OOV words is named entities. (iv) Since the opti-
mal granularity of words for phrase-based MT is un-
known, we can benefit from a model which provides
a knob for adjusting average word size.
We build several different models to address these
issues and to improve segmentation for the benefit of
MT. First, we emphasize lexicon-based features in
a feature-based sequence classifier to deal with seg-
mentation inconsistency and over-generating OOV
words. Having lexicon-based features reduced the
MT training lexicon by 29.5%, reduced the MT test
data OOV rate by 34.1%, and led to a 0.38 BLEU
point gain on the test data (MT05). Second, we ex-
tend the CRF label set of our CRF segmenter to iden-
tify proper nouns. This gives 3.3% relative improve-
ment on the OOV recall rate, and a 0.32 improve-
ment in BLEU. Finally, we tune the CRF model to
generate shorter or longer words to directly optimize
the performance of MT. For MT, we found that it
is preferred to have words slightly shorter than the
CTB standard.
The paper is organized as follows: we describe
the experimental settings for the segmentation task
and the task in Section 2. In Section 3.1 we demon-
strate that it is helpful to have word segmenters for
MT, but that segmentation performance does not di-
rectly correlate with MT performance. We analyze
what characteristics of word segmenters most affect
MT performance in Section 3.2. In Section 4 and
5 we describe how we tune a CRF model to fit the
?word? granularity and also how we incorporate ex-
ternal lexicon and information about named entities
for better MT performance.
2 Experimental Setting
2.1 Chinese Word Segmentation
For directly evaluating segmentation performance,
we train each segmenter with the SIGHAN Bake-
off 2006 training data (the UPUC data set) and then
evaluate on the test data. The training data contains
509K words, and the test data has 155K words. The
percentage of words in the test data that are unseen
in the training data is 8.8%. Detail of the Bakeoff
data sets is in (Levow, 2006). To understand how
each segmenter learns about OOV words, we will
report the F measure, the in-vocabulary (IV) recall
rate as well as OOV recall rate of each segmenter.
2.2 Phrase-based Chinese-to-English MT
The MT system used in this paper is Moses, a state-
of-the-art phrase-based system (Koehn et al, 2003).
We build phrase translations by first acquiring bidi-
rectional GIZA++ (Och and Ney, 2003) alignments,
and using Moses? grow-diag alignment symmetriza-
tion heuristic.1 We set the maximum phrase length
to a large value (10), because some segmenters
described later in this paper will result in shorter
1In our experiments, this heuristic consistently performed
better than the default, grow-diag-final.
225
words, therefore it is more comparable if we in-
crease the maximum phrase length. During decod-
ing, we incorporate the standard eight feature func-
tions of Moses as well as the lexicalized reordering
model. We tuned the parameters of these features
with Minimum Error Rate Training (MERT) (Och,
2003) on the NIST MT03 Evaluation data set (919
sentences), and then test the MT performance on
NIST MT03 and MT05 Evaluation data (878 and
1082 sentences, respectively). We report the MT
performance using the original BLEU metric (Pap-
ineni et al, 2001). All BLEU scores in this paper are
uncased.
The MT training data was subsampled from
GALE Year 2 training data using a collection
of character 5-grams and smaller n-grams drawn
from all segmentations of the test data. Since
the MT training data is subsampled with charac-
ter n-grams, it is not biased towards any particular
word segmentation. The MT training data contains
1,140,693 sentence pairs; on the Chinese side there
are 60,573,223 non-whitespace characters, and the
English sentences have 40,629,997 words.
Our main source for training our five-gram lan-
guage model was the English Gigaword corpus, and
we also included close to one million English sen-
tences taken from LDC parallel texts: GALE Year 1
training data (excluding FOUO data), Sinorama,
AsiaNet, and Hong Kong news. We restricted the
Gigaword corpus to a subsample of 25 million sen-
tences, because of memory constraints.
3 Understanding Chinese Word
Segmentation for Phrase-based MT
In this section, we experiment with three types
of segmenters ? character-based, lexicon-based and
feature-based ? to explore what kind of characteris-
tics are useful for segmentation for MT.
3.1 Character-based, Lexicon-based and
Feature-based Segmenters
The training data for the segmenter is two orders of
magnitude smaller than for the MT system, it is not
terribly well matched to it in terms of genre and
variety, and the information an MT system learns
about alignment of Chinese to English might be the
basis for a task appropriate segmentation style for
Chinese-English MT. A phrase-based MT system
Segmentation Performance
Segmenter F measure OOV Recall IV Recall
CharBased 0.334 0.012 0.485
MaxMatch 0.828 0.012 0.951
MT Performance
Segmenter MT03 (dev) MT05 (test)
CharBased 30.81 29.36
MaxMatch 31.95 30.73
Table 1: CharBased vs. MaxMatch
like Moses can extract ?phrases? (sequences of to-
kens) from a word alignment and the system can
construct the words that are useful. These observa-
tions suggest the first hypothesis.
Hypothesis 1. A phrase table should capture word
segmentation. Character-based segmentation for
MT should not underperform a lexicon-based seg-
mentation, and might outperform it.
Observation In the experiments we conducted,
we found that the phrase table cannot capture every-
thing a Chinese word segmenter can do, and there-
fore having word segmentation helps phrase-based
MT systems. 2
To show that having word segmentation helps
MT, we compare a lexicon-based maximum-
matching segmenter with character-based segmen-
tation (treating each Chinese character as a word).
The lexicon-based segmenter finds words by greed-
ily matching the longest words in the lexicon in a
left-to-right fashion. We will later refer to this seg-
menter as MaxMatch. The MaxMatch segmenter is a
simple and common baseline for the Chinese word
segmentation task.
The segmentation performance of MaxMatch is
not very satisfying because it cannot generalize to
capture words it has never seen before. How-
ever, having a basic segmenter like MaxMatch still
gives the phrase-based MT system a win over the
character-based segmentation (treating each Chinese
character as a word). We will refer to the character-
based segmentation as CharBased.
In Table 1, we can see that on the Chinese word
segmentation task, having MaxMatch is obviously
better than not trying to identify Chinese words at
all (CharBased). As for MT performance, in Ta-
ble 1 we see that having a segmenter, even as sim-
2Different phrase extraction heuristics might affect the re-
sults. In our experiments, grow-diag outperforms both one-to-
many and many-to-one for both MaxMatch and CharBased. We
report the results only on grow-diag.
226
ple as MaxMatch, can help phrase-based MT system
by about 1.37 BLEU points on all 1082 sentences
of the test data (MT05). Also, we tested the per-
formance on 828 sentences of MT05 where all el-
ements are in vocabulary3 for both MaxMatch and
CharBased. MaxMatch achieved 32.09 BLEU and
CharBased achieved 30.28 BLEU, which shows that
on the sentences where all elements are in vocabu-
lary, there MaxMatch is still significantly better than
CharBased. Therefore, Hypothesis 1 is refuted.
Analysis We hypothesized in Hypothesis 1 that
the phrase table in a phrase-basedMT system should
be able to capture the meaning by building ?phrases?
on top of character sequences. Based on the experi-
mental result in Table 1, we see that using character-
based segmentation (CharBased) actually performs
reasonably well, which indicates that the phrase ta-
ble does capture the meaning of character sequences
to a certain extent. However, the results also show
that there is still some benefit in having word seg-
mentation for MT. We analyzed the decoded out-
put of both systems (CharBased and MaxMatch) on
the development set (MT03). We found that the ad-
vantage of MaxMatch over CharBased is two-fold,
(i) lexical: it enhances the ability to disambiguate
the case when a character has very different meaning
in different contexts, and (ii) reordering: it is easier
to move one unit around than having to move two
consecutive units at the same time. Having words as
the basic units helps the reordering model.
For the first advantage, one example is the char-
acter ???, which can both mean ?intelligence?, or
an abbreviation for Chile (?|). The comparison
between CharBased and MaxMatch is listed in Ta-
ble 2. The word??w (dementia) is unknown for
both segmenters. However, MaxMatch gave a better
translation of the character?. The issue here is not
that the ?????intelligence? entry never appears in
the phrase table of CharBased. The real issue is,
when ? means Chile, it is usually followed by the
character |. So by grouping them together, Max-
Match avoided falsely increasing the probability of
translating the stand-alone ? into Chile. Based on
our analysis, this ambiguity occurs the most when
the character-based system is dealing with a rare or
unseen character sequence in the training data, and
also occurs more often when dealing with translit-
3Except for dates and numbers.
Reference translation:
scientists complete sequencing of the chromosome linked to
early dementia
CharBased segmented input:
? ? [ ? M ' ? ? ? ? w ff / ? N  ? ? S
MaxMatch segmented input:
??[ ? M' ?? ? ? w ff /? N ? ? S
Translation with CharBased segmentation:
scientists at the beginning of the stake of chile lost the genome
sequence completed
Translation with MaxMatch segmentation:
scientists at stake for the early loss of intellectual syndrome
chromosome completed sequencing
Table 2: An example showing that character-based segmenta-
tion provides weaker ability to distinguish character with mul-
tiple unrelated meanings.
erations. The reason is that characters composing
a transliterated foreign named entity usually doesn?t
preserve their meanings; they are just used to com-
pose a Chinese word that sounds similar to the orig-
inal word ? much more like using a character seg-
mentation of English words. Another example of
this kind is ?C_?%?w? (Alzheimer?s dis-
ease). The MT system using CharBased segmenta-
tion tends to translate some characters individually
and drop others; while the system using MaxMatch
segmentation is more likely to translate it right.
The second advantage of having a segmenter like
the lexicon-based MaxMatch is that it helps the re-
ordering model. Results in Table 1 are with the
linear distortion limit defaulted to 6. Since words
in CharBased are inherently shorter than MaxMatch,
having the same distortion limit means CharBased
is limited to a smaller context than MaxMatch. To
make a fairer comparison, we set the linear distor-
tion limit in Moses to unlimited, removed the lexi-
calized reordering model, and retested both systems.
With this setting, MaxMatch is 0.46 BLEU point bet-
ter than CharBased (29.62 to 29.16) on MT03. This
result suggests that having word segmentation does
affect how the reordering model works in a phrase-
based system.
Hypothesis 2. Better Segmentation Performance
Should Lead to Better MT Performance
Observation We have shown in Hypothesis 1 that
it is helpful to segment Chinese texts into words
first. In order to decide a segmenter to use, the
most intuitive thing to do is to find one that gives
higher F measure on segmentation. Our experiments
show that higher F measure does not necessarily
227
lead to higher BLEU score. In order to contrast
with the simple maximum matching lexicon-based
model (MaxMatch), we built another segmenter with
a CRF model. CRF is a statistical sequence model-
ing framework introduced by Lafferty et al (2001),
and was first used for the Chinese word segmenta-
tion task by Peng et al (2004), who treated word
segmentation as a binary decision task. We opti-
mized the parameters with a quasi-Newton method,
and used Gaussian priors to prevent overfitting.
The probability assigned to a label sequence for a
particular sequence of characters by a CRF is given
by the equation:
p? (y|x) =
1
Z(x)
exp
T
?
t=1
K
?
k=1
?k fk(x,yt?1,yt , t) (1)
x is a sequence of T unsegmented characters, Z(x) is
the partition function that ensures that Equation 1 is
a probability distribution, { fk}Kk=1 is a set of feature
functions, and y is the sequence of binary predic-
tions for the sentence, where the prediction yt = +1
indicates the t-th character of the sequence is pre-
ceded by a space, and where yt =?1 indicates there
is none. We trained a CRF model with a set of ba-
sic features: character identity features of the current
character, previous character and next character, and
the conjunction of previous and current characters in
the zero-order templates. We will refer to this seg-
menter as CRF-basic.
Table 3 shows that the feature-based segmenter
CRF-basic outperforms the lexicon-based MaxMatch
by 5.9% relative F measure. Comparing the OOV re-
call rate and the IV recall rate, the reason is that CRF-
basic wins a lot on the OOV recall rate. We see that
a feature-based segmenter like CRF-basic clearly has
stronger ability to recognize unseen words. On
MT performance, however, CRF-basic is 0.38 BLEU
points worse than MaxMatch on the test set. In Sec-
tion 3.2, we will look at how theMT training and test
data are segmented by each segmenter, and provide
statistics and analysis for why certain segmenters are
better than others.
3.2 Consistency Analysis of Different
Segmenters
In Section 3.1 we have refuted two hypotheses. Now
we know that: (i) phrase table construction does not
fully capture what a word segmenter can do. Thus it
Segmentation Performance
Segmenter F measure OOV Recall IV Recall
CRF-basic 0.877 0.502 0.926
MaxMatch 0.828 0.012 0.951
CRF-Lex 0.940 0.729 0.970
MT Performance
Segmenter MT03 (dev) MT05 (test)
CRF-basic 33.01 30.35
MaxMatch 31.95 30.73
CRF-Lex 32.70 30.95
Table 3: CRF-basic vs MaxMatch
Segmenter #MT Training Lexicon Size #MT Test Lexicon Size
CRF-basic 583147 5443
MaxMatch 39040 5083
CRF-Lex 411406 5164
MT Test Lexicon OOV rate Conditional Entropy
CRF-basic 7.40% 0.2306
MaxMatch 0.49% 0.1788
CRF-Lex 4.88% 0.1010
Table 4: MT Lexicon Statistics and Conditional Entropy of Seg-
mentation Variations of three segmetners
is useful to have word segmentation for MT. (ii) a
higher F measure segmenter does not necessarily
outperforms on the MT task.
To understand what factors other than segmen-
tation F measure can affect MT performance, we
introduce another CRF segmenter CRF-Lex that in-
cludes lexicon-based features by using external lex-
icons. More details of CRF-Lex will be described
in Section 5.1. From Table 3, we see that the seg-
mentation F measure is that CRF-Lex > CRF-basic >
MaxMatch. And now we know that the better seg-
mentation F measure does not always lead to better
MT BLEU score, because of in terms of MT perfor-
mance, CRF-Lex > MaxMatch > CRF-basic.
In Table 4, we list some statistics of each seg-
menter to explain this phenomenon. First we look
at the lexicon size of the MT training and test data.
While segmenting the MT data, CRF-basic gener-
ates an MT training lexicon size of 583K unique
word tokens, and MaxMatch has a much smaller lex-
icon size of 39K. CRF-Lex performs best on MT,
but the MT training lexicon size and test lexicon
OOV rate is still pretty high compared to MaxMatch.
Only examining the MT training and test lexicon
size still doesn?t fully explain why CRF-Lex outper-
forms MaxMatch. MaxMatch generates a smaller MT
lexicon and lower OOV rate, but for MT it wasn?t
better than CRF-Lex, which has a bigger lexicon and
higher OOV rate. In order to understand why Max-
Match performs worse on MT than CRF-Lex but bet-
228
ter than CRF-basic, we use conditional entropy of
segmentation variations to measure consistency.
We use the gold segmentation of the SIGHAN
test data as a guideline. For every work type wi,
we collect all the different pattern variations vi j in
the segmentation we want to examine. For exam-
ple, for a word ?ABC? in the gold segmentation, we
look at how it is segmented with a segmenter. There
are many possibilities. If we use cx and cy to indi-
cate other Chinese characters and to indicate white
spaces, ?cx ABC cy? is the correct segmentation,
because the three characters are properly segmented
from both sides, and they are concatenated with each
other. It can also be segmented as ?cx A BC cy?,
which means although the boundary is correct, the
first character is separated from the other two. Or,
it can be segmented as ?cxA BCcy?, which means
the first character was actually part of the previous
word, while BC are the beginning of the next word.
Every time a particular word type wi appears in the
text, we consider a segmenter more consistent if it
can segment wi in the same way every time, but it
doesn?t necessarily have to be the same as the gold
standard segmentation. For example, if ?ABC? is a
Chinese person name which appears 100 times in the
gold standard data, and one segmenter segment it as
cx A BC cy 100 times, then this segmenter is still
considered to be very consistent, even if it doesn?t
exactly match the gold standard segmentation. Us-
ing this intuition, the conditional entropy of segmen-
tation variations H(V |W ) is defined as follows:
H(V |W ) = ??
wi
P(wi)?
vi j
P(vi j|wi) logP(vi j|wi)
= ??
wi
?
vi j
P(vi j,wi) logP(vi j|wi)
Now we can look at the overall conditional en-
tropy H(V |W ) to compare the consistency of each
segmenter. In Table 4, we can see that even though
MaxMatch has a much smaller MT lexicon size than
CRF-Lex, when we examine the consistency of how
MaxMatch segments in context, we find the condi-
tional entropy is much higher than CRF-Lex. We can
also see that CRF-basic has a higher conditional en-
tropy than the other two. The conditional entropy
H(V |W ) shows how consistent each segmenter is,
and it correlates with the MT performance in Ta-
ble 4. Note that consistency is only one of the com-
peting factors of how good a segmentation is for
MT performance. For example, a character-based
segmentation will always have the best consistency
possible, since every word ABC will just have one
pattern: cx A B C cy. But from Section 3.1 we
see that CharBased performs worse than both Max-
Match and CRF-basic on MT, because having word
segmentation can help the granularity of the Chinese
lexicon match that of the English lexicon.
In conclusion, for MT performance, it is helpful
to have consistent segmentation, while still having a
word segmentation matching the granularity of the
segmented Chinese lexicon and the English lexicon.
4 Optimal Average Token Length for MT
We have shown earlier that word-level segmentation
vastly outperforms character based segmentation in
MT evaluations. Since the word segmentation stan-
dard under consideration (Chinese Treebank (Xue
et al, 2005)) was neither specifically designed nor
optimized for MT, it seems reasonable to investi-
gate whether any segmentation granularity in con-
tinuum between character-level and CTB-style seg-
mentation is more effective for MT. In this section,
we present a technique for directly optimizing a seg-
mentation property?characters per token average?
for translation quality, which yields significant im-
provements in MT performance.
In order to calibrate the average word length pro-
duced by our CRF segmenter?i.e., to adjust the rate
of word boundary predictions (yt = +1), we apply
a relatively simple technique (Minkov et al, 2006)
originally devised for adjusting the precision/recall
tradeoff of any sequential classifier. Specifically, the
weight vector w and feature vector of a trained lin-
ear sequence classifier are augmented at test time
to include new class-conditional feature functions to
bias the classifier towards particular class labels. In
our case, since we wish to increase the frequency of
word boundaries, we add a feature function:
f0(x,yt?1,yt , t) =
{
1 if yt = +1
0 otherwise
Its weight ?0 controls the extent of which the classi-
fier will make positive predictions, with very large
positive ?0 values causing only positive predic-
tions (i.e., character-based segmentation) and large
negative values effectively disabling segmentation
boundaries. Table 5 displays how changes of the
229
?0 ?1 0 1 2 4 8 32
len 1.64 1.62 1.61 1.59 1.55 1.37 1
Table 5: Effect of the bias parameter ?0 on the average number
of character per token on MT data.
bias parameter ?0 affect segmentation granularity.4
Since we are interested in analyzing the different
regimes of MT performance between CTB segmen-
tation and character-based, we performed a grid
search in the range between ?0 = 0 (maximum-
likelihood estimate) and ?0 = 32 (a value that is
large enough to produce only positive predictions).
For each ?0 value, we ran an entire MT training and
testing cycle, i.e., we re-segmented the entire train-
ing data, ran GIZA++, acquired phrasal translations
that abide to this new segmentation, and ran MERT
and evaluations on segmented data using the same
?0 values.
 30
 30.5
 31
 31.5
 32
 32.5
 33
-3 -2 -1  0  1  2  3  4  5  6  7  8
bias
BLEU[%] scores
MT03(dev)MT02MT05
 0.8
 0.82
 0.84
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
-3 -2 -1  0  1  2  3  4  5  6  7  8
bias
Segmentation performance
PrecisionRecallF measure
Figure 1: A bias towards more segment boundaries (?0 > 0)
yields better MT performance and worse segmentation results.
Segmentation and MT results are displayed in
Figure 1. First, we observe that an adjustment of
the precision and recall tradeoff by setting nega-
4Note that character-per-token averages provided in the ta-
ble consider each non-Chinese word (e.g., foreign names, num-
bers) as one character, since our segmentation post-processing
prevents these tokens from being segmented.
tive bias values (?0 = ?2) slightly improves seg-
mentation performance. We also notice that rais-
ing ?0 yields relatively consistent improvements in
MT performance, yet causes segmentation perfor-
mance (F measure) to be increasingly worse. While
the latter finding is not particularly surprising, it fur-
ther confirms that segmentation and MT evaluations
can yield rather different outcomes. We chose the
?0 = 2 on another dev set (MT02). On the test set
MT05, ?0 = 2 yields 31.47 BLEU, which represents
a quite large improvement compared to the unbiased
segmenter (30.95 BLEU). Further reducing the av-
erage number of characters per token yields gradual
drops of performance until character-level segmen-
tation (?0 ? 32, 29.36 BLEU).
Here are some examples of how setting ?0 = 2
shortens the words in a way that can help MT.
? separating adjectives and pre-modifying adverbs:
??(very big) ??(very)?(big)
? separating nouns and pre-modifying adjectives:
p??(high blood pressure)
?p(high)??(blood pressure)
? separating compound nouns:
S?(Department of Internal Affairs)
?S(Internal Affairs)?(Department).
5 Improving Segmentation Consistency of
a Feature-based Sequence Model for
Segmentation
In Section 3.1 we showed that a statistical sequence
model with rich features can generalize better than
maximum matching segmenters. However, it also
inconsistently over-generates a big MT training lexi-
con and OOVwords in MT test data, and thus causes
a problem for MT. To improve a feature-based se-
quence model for MT, we propose 4 different ap-
proaches to deal with named entities, optimal length
of word for MT and joint search for segmentation
and MT decoding.
5.1 Making Use of External Lexicons
One way to improve the consistency of the CRF
model is to make use of external lexicons (which
are not part of the segmentation training data) to
add lexicon-based features. All the features we use
are listed in Table 6. Our linguistic features are
adopted from (Ng and Low, 2004) and (Tseng et
al., 2005). There are three categories of features:
230
Lexicon-based Features Linguistic Features
(1.1) LBegin(Cn),n ? [?2,1] (2.1) Cn,n ? [?2,1]
(1.2) LMid(Cn),n ? [?2,1] (2.2) Cn?1Cn,n ? [?1,1]
(1.3) LEnd(Cn),n ? [?2,1] (2.3) Cn?2Cn,n ? [1,2]
(1.4) LEnd(C?1)+LEnd(C0) (2.4) Single(Cn),n ? [?2,1]
+LEnd(C1) (2.5) UnknownBigram(C?1C0)
(1.5) LEnd(C?2)+LEnd(C?1) (2.6) ProductiveA f f ixes(C?1,C0)
+LBegin(C0)+LMid(C0) (2.7) Reduplication(C?1,Cn),n ? [0,1]
(1.6) LEnd(C?2)+LEnd(C?1)
+LBegin(C?1)
+LBegin(C0)+LMid(C0)
Table 6: Features for CRF-Lex
character identity n-grams, morphological and char-
acter reduplication features. Our lexicon-based fea-
tures are adopted from (Shi and Wang, 2007), where
LBegin(C0), LMid(C0) and LEnd(C0) represent the
maximum length of words found in a lexicon that
contain the current character as either the first, mid-
dle or last character, and we group any length equal
or longer than 6 together. The linguistic features
help capturing words that were unseen to the seg-
menter; while the lexicon-based features constrain
the segmenter with external knowledge of what se-
quences are likely to be words.
We built a CRF segmenter with all the features
listed in Table 6 (CRF-Lex). The external lexicons
we used for the lexicon-based features come from
various sources including named entities collected
from Wikipedia and the Chinese section of the UN
website, named entities collected by Harbin Institute
of Technology, the ADSO dictionary, EMM News
Explorer, Online Chinese Tools, Online Dictionary
from Peking University and HowNet. There are
423,224 distinct entries in all the external lexicons.
The MT lexicon consistency of CRF-Lex in Table
4 shows that the MT training lexicon size has been
reduced by 29.5% and the MT test data OOV rate is
reduced by 34.1%.
5.2 Joint training of Word Segmentation and
Proper Noun Tagging
Named entities are an important source for OOV
words, and in particular are ones which it is bad to
break into pieces (particularly for foreign names).
Therefore, we use the proper noun (NR) part-of-
speech tag information from CTB to extend the label
sets of our CRF model from 2 to 4 ({beginning of a
word, continuation of a word} ? {NR, not NR}).
This is similar to the ?all-at-once, character-based?
POS tagging in (Ng and Low, 2004), except that
Segmentation Performance
Segmenter F measure OOV Recall IV Recall
CRF-Lex-NR 0.943 0.753 0.970
CRF-Lex 0.940 0.729 0.970
MT Performance
Segmenter MT03 (dev) MT05 (test)
CRF-Lex-NR 32.96 31.27
CRF-Lex 32.70 30.95
Table 7: CRF-Lex-NR vs CRF-Lex
we are only tagging proper nouns. We call the 4-
label extension CRF-Lex-NR. The segmentation and
MT performance of CRF-Lex-NR is listed in Table 7.
With the 4-label extension, the OOV recall rate im-
proved by 3.29%; while the IV recall rate stays the
same. Similar to (Ng and Low, 2004), we found the
overall F measure only goes up a tiny bit, but we do
find a significant OOV recall rate improvement.
On the MT performance, CRF-Lex-NR has a 0.32
BLEU gain on the test set MT05. In addition to the
BLEU improvement, CRF-Lex-NR also provides ex-
tra information about proper nouns, which can be
combined with postprocessing named entity transla-
tion modules to further improve MT performance.
6 Conclusion
In this paper, we investigated what segmentation
properties can improve machine translation perfor-
mance. First, we found that neither character-based
nor a standard word segmentation standard are opti-
mal for MT, and show that an intermediate granular-
ity is much more effective. Using an already com-
petitive CRF segmentation model, we directly opti-
mize segmentation granularity for translation qual-
ity, and obtain an improvement of 0.73 BLEU point
on MT05 over our lexicon-based segmentation base-
line. Second, we augment our CRF model with
lexicon and proper noun features in order to im-
prove segmentation consistency, which provide a
0.32 BLEU point improvement.
7 Acknowledgement
The authors would like to thank Menqgiu Wang and
Huihsin Tseng for useful discussions. This paper is
based on work funded in part by the Defense Ad-
vanced Research Projects Agency through IBM.
231
References
Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning Huang.
2005. Chinese word segmentation and named entity
recognition: A pragmatic approach. Computational
Linguistics.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
Proceedings of the Human Language Technology Con-
ference of the NAACL, Companion Volume: Short Pa-
pers, pages 49?52, New York City, USA, June. Asso-
ciation for Computational Linguistics.
Vera Hollink, Jaap Kamps, Christof Monz, and Maarten
de Rijke. 2004. Monolingual document retrieval for
European languages. Information Retrieval, 7(1).
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In EACL ?03: Proceed-
ings of the tenth conference on European chapter of
the Association for Computational Linguistics, pages
187?193. Association for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL-HLT.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning.
Gina-Anne Levow. 2006. The third international Chi-
nese language processing bakeoff: Word segmentation
and named entity recognition. In Proc. of the Fifth
SIGHAN Workshop on Chinese Language Processing,
July.
Einat Minkov, Richard Wang, Anthony Tomasic, and
William Cohen. 2006. NER systems that suit user?s
preferences: Adjusting the recall-precision trade-off
for entity extraction. In Proc. of NAACL-HLT, Com-
panion Volume: Short Papers, New York City, USA,
June.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? Word-
based or character-based? In Proc. of EMNLP.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1).
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic eval-
uation of machine translation. In ACL.
Fuchun Peng, Xiangji Huang, Dale Schuurmans, and
Nick Cercone. 2002. Investigating the relationship
between word segmentation performance and retrieval
performance in Chinese IR. In Proc. of the 19th Inter-
national Conference on Computational Linguistics.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proc. of COLING.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
CRFs based joint decoding method for cascaded seg-
mentation and labeling tasks. In IJCAI.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for Sighan bake-
off 2005. In Proc. of the Fourth SIGHAN Workshop on
Chinese Language Processing.
Jia Xu, Richard Zens, and Hermann Ney. 2004. Do
we need Chinese word segmentation for statistical ma-
chine translation. In Proc. of the Third SIGHAN Work-
shop on Chinese Language Learning.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha
Palmer. 2005. Building a large annotated Chinese
corpus: the Penn Chinese treebank. Journal of Nat-
ural Language Engineering, 11(2).
232
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 37?41,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Textual Entailment Features for Machine Translation Evaluation
Sebastian Pado?, Michel Galley, Dan Jurafsky, Christopher D. Manning?
Stanford University
{pado,mgalley,jurafsky,manning}@stanford.edu
Abstract
We present two regression models for the prediction
of pairwise preference judgments among MT hy-
potheses. Both models are based on feature sets that
are motivated by textual entailment and incorporate
lexical similarity as well as local syntactic features
and specific semantic phenomena. One model pre-
dicts absolute scores; the other one direct pairwise
judgments. We find that both models are compet-
itive with regression models built over the scores
of established MT evaluation metrics. Further data
analysis clarifies the complementary behavior of the
two feature sets.
1 Introduction
Automatic metrics to assess the quality of machine trans-
lations have been a major enabler in improving the per-
formance of MT systems, leading to many varied ap-
proaches to develop such metrics. Initially, most metrics
judged the quality of MT hypotheses by token sequence
match (cf. BLEU (Papineni et al, 2002), NIST (Dod-
dington, 2002). These measures rate systems hypothe-
ses by measuring the overlap in surface word sequences
shared between hypothesis and reference translation.
With improvements in the state-of-the-art in machine
translation, the effectiveness of purely surface-oriented
measures has been questioned (see e.g., Callison-Burch
et al (2006)). In response, metrics have been proposed
that attempt to integrate more linguistic information
into the matching process to distinguish linguistically li-
censed from unwanted variation (Gime?nez andMa`rquez,
2008). However, there is little agreement on what types
of knowledge are helpful: Some suggestions concen-
trate on lexical information, e.g., by the integration of
word similarity information as in Meteor (Banerjee and
Lavie, 2005) or MaxSim (Chan and Ng, 2008). Other
proposals use structural information such as dependency
edges (Owczarzak et al, 2007).
In this paper, we investigate an MT evaluation metric
that is inspired by the similarity between this task and
the textual entailment task (Dagan et al, 2005), which
?This paper is based on work funded by the Defense Ad-
vanced Research Projects Agency through IBM. The content
does not necessarily reflect the views of the U.S. Government,
and no official endorsement should be inferred..
HYP: Virus was infected.
REF: No one was infected by the virus.
no entailment
no entailment
HYP: The virus did not infect anybody.
REF: No one was infected by the virus.
entailment
entailment
Figure 1: Entailment status between an MT system hy-
pothesis and a reference translation for good translations
(above) and bad translations (below).
suggests that the quality of an MT hypothesis should be
predictable by a combination of lexical and structural
features that model the matches and mismatches be-
tween system output and reference translation. We use
supervised regression models to combine these features
and analyze feature weights to obtain further insights
into the usefulness of different feature types.
2 Textual Entailment for MT Evaluation
2.1 Textual Entailment vs. MT Evaluation
Textual entailment (TE) was introduced by Dagan et
al. (2005) as a concept that corresponds more closely
to ?common sense? reasoning than classical, categorical
entailment. Textual entailment is defined as a relation
between two natural language sentences (a premise P
and a hypothesis H) that holds if a human reading P
would infer that H is most likely true.
Information about the presence or absence of entail-
ment between two sentences has been found to be ben-
eficial for a range of NLP tasks such as Word Sense
Disambiguation or Question Answering (Dagan et al,
2006; Harabagiu and Hickl, 2006). Our intuition is that
this idea can also be fruitful in MT Evaluation, as illus-
trated in Figure 1. Very good MT output should entail
the reference translation. In contrast, missing hypothesis
material breaks forward entailment; additional material
breaks backward entailment; and for bad translations,
entailment fails in both directions.
Work on the recognition of textual entailment (RTE)
has consistently found that the integration of more syn-
tactic and semantic knowledge can yield gains over
37
surface-based methods, provided that the linguistic anal-
ysis was sufficiently robust. Thus, for RTE, ?deep?
matching outperforms surface matching. The reason is
that linguistic representation makes it considerably eas-
ier to distinguish admissible variation (i.e., paraphrase)
from true, meaning-changing divergence. Admissible
variation may be lexical (synonymy), structural (word
and phrase placement), or both (diathesis alternations).
The working hypothesis of this paper is that the ben-
efits of deeper analysis carry over to MT evaluation.
More specifically, we test whether the features that al-
low good performance on the RTE task can also predict
human judgments for MT output. Analogously to RTE,
these features should help us to differentiate meaning
preserving translation variants from bad translations.
Nevertheless, there are also substantial differences
between TE and MT evaluation. Crucially, TE assumes
the premise and hypothesis to be well-formed sentences,
which is not true in MT evaluation. Thus, a possible crit-
icism to the use of TE methods is that the features could
become unreliable for ill-formed MT output. However,
there is a second difference between the tasks that works
to our advantage. Due to its strict compositional nature,
TE requires an accurate semantic analysis of all sentence
parts, since, for example, one misanalysed negation or
counterfactual embedding can invert the entailment sta-
tus (MacCartney and Manning, 2008). In contrast, hu-
man MT judgments behave more additively: failure of a
translation with respect to a single semantic dimension
(e.g., polarity or tense) degrades its quality, but usually
not crucially so. We therefore expect that even noisy
entailment features can be predictive in MT evaluation.
2.2 Entailment-based prediction of MT quality
Regression-based prediction. Experiences from the
annotation of MT quality judgments show that human
raters have difficulty in consistently assigning absolute
scores to MT system output, due to the number of ways
in which MT output can deviate. Thus, the human an-
notation for the WMT 2008 dataset was collected in
the form of binary pairwise preferences that are con-
siderably easier to make (Callison-Burch et al, 2008).
This section presents two models for the prediction of
pairwise preferences.
The first model (ABS) is a regularized linear regres-
sion model over entailment-motivated features (see be-
low) that predicts an absolute score for each reference-
hypothesis pair. Pairwise preferences are created simply
by comparing the absolute predicted scores. This model
is more general, since it can also be used where absolute
score predictions are desirable; furthermore, the model
is efficient with a runtime linear in the number of sys-
tems and corpus size. On the downside, this model is
not optimized for the prediction of pairwise judgments.
The second model we consider is a regularized logis-
tic regression model (PAIR) that is directly optimized to
predict a weighted binary preference for each hypothe-
sis pair. This model is less efficient since its runtime is
Alignment score(3) Unaligned material (10)
Adjuncts (7) Apposition (2)
Modality (5) Factives (8)
Polarity (5) Quantors (4)
Tense (2) Dates (6)
Root (2) Semantic Relations (4)
Semantic relatedness (7) Structural Match (5)
Compatibility of locations and entities (4)
Table 1: Entailment feature groups provided by the
Stanford RTE system, with number of features
quadratic in the number of systems. On the other hand,
it can be trained on more reliable pairwise preference
judgments. In a second step, we combine the individ-
ual decisions to compute the highest-likelihood total
ordering of hypotheses. The construction of an optimal
ordering from weighted pairwise preferences is an NP-
hard problem (via reduction of CYCLIC-ORDERING;
Barzilay and Elhadad, 2002), but a greedy search yields
a close approximation (Cohen et al, 1999).
Both models can be used to predict system-level
scores from sentence-level scores. Again, we have two
method for doing this. The basic method (BASIC) pre-
dicts the quality of each system directly as the percent-
age of sentences for which its output was rated best
among all systems. However, we noticed that the man-
ual rankings for the WMT 2007 dataset show a tie for
best system for almost 30% of sentences. BASIC is
systematically unable to account for these ties. We
therefore implemented a ?tie-aware? prediction method
(WITHTIES) that uses the same sentence-level output as
BASIC, but computes system-level quality differently,
as the percentage of sentences where the system?s hy-
pothesis was scored better or at most ? worse than the
best system, for some global ?tie interval? ? .
Features. We use the Stanford RTE system (MacCart-
ney et al, 2006) to generate a set of entailment features
(RTE) for each pair of MT hypothesis and reference
translation. Features are generated in both directions
to avoid biases towards short or long translations. The
Stanford RTE system uses a three-stage architecture.
It (a) constructs a robust, dependency-based linguistic
analysis of the two sentences; (b) identifies the best
alignment between the two dependency graphs given
similarity scores from a range of lexical resources, us-
ing a Markov Chain Monte Carlo sampling strategy;
and (c) computes roughly 75 features over the aligned
pair of dependency graphs. The different feature groups
are shown in Table 1. A small number features are
real-valued, measuring different quality aspects of the
alignment. The other features are binary, indicating
matches and mismatches of different types (e.g., align-
ment between predicates embedded under compatible
or incompatible modals, respectively).
To judge to what extent the entailment-based model
delivers improvements that cannot be obtained with es-
tablished methods, we also experiment with a feature set
38
formed from a set of established MT evaluation metrics
(TRADMT). We combine different parametrization of
(smoothed) BLEU (Papineni et al, 2002), NIST (Dod-
dington, 2002), and TER (Snover et al, 2006), to give
a total of roughly 100 features. Finally, we consider a
combination of both feature sets (COMB).
3 Experimental Evaluation
Setup. To assess and compare the performance of our
models, we use corpora that were created by past in-
stances of the WMT workshop. We optimize the feature
weights for the ABS models on the WMT 2006 and
2007 absolute score annotations, and correspondingly
for the PAIR models on the WMT 2007 absolute score
and ranking annotations. All models are evaluated on
WMT 2008 to compare against the published results.
Finally, we need to set the tie interval ? . Since we
did not want to optimize ? , we simply assumed that the
percentage of ties observed on WMT 2007 generalizes
to test sets such as the 2008 dataset. We set ? so that
there are ties for first place on 30% of the sentences,
with good practical success (see below).
Results. Table 2 shows our results. The first results
column (Cons) shows consistency, i.e., accuracy in pre-
dicting human pairwise preference judgments. Note that
the performance of a random baseline is not at 50%, but
substantially lower. This is due to (a) the presence of
contradictions and ties in the human judgments, which
cannot be predicted; and (b) WMT?s requirement to
compute a total ordering of all translations for a given
sentence (rather than independent binary judgments),
which introduces transitivity constraints. See Callison-
Burch et al (2008) for details. Among our models, PAIR
shows a somewhat better consistency than ABS, as can
be expected from a model directly optimized on pair-
wise judgments. Across feature sets, COMB works best
with a consistency of 0.53, competitive with published
WMT 2008 results.
The two final columns (BASIC and WITHTIES) show
Spearman?s ? for the correlation between human judg-
ments and the two types of system-level predictions.
For BASIC system-level predictions, we find that
PAIR performs considerably worse than ABS, by a mar-
gin of up to ? = 0.1. Recall that the system-level analy-
sis considers only the top-ranked hypotheses; apparently,
a model optimized on pairwise judgments has a harder
time choosing the best among the top-ranked hypothe-
ses. This interpretation is supported by the large benefit
that PAIR derives from explicit tie modeling. ABS gains
as well, although not as much, so that the correlation of
the tie-aware predictions is similar for ABS and PAIR.
Comparing different feature sets, BASIC show a simi-
lar pattern to the consistency figures. There is no clear
winner between RTE and TRADMT. The performance
of TRADMT is considerably better than the performance
of BLEU and TER in the WMT 2008 evaluation, where
? ? 0.55. RTE is able to match the performance of an
Model Feature set Cons
(Acc.)
BASIC
(?)
WITHTIES
(?)
ABS TRADMT 0.50 0.74 0.74
ABS RTE 0.51 0.72 0.78
ABS COMB 0.51 0.74 0.74
PAIR TRADMT 0.52 0.63 0.73
PAIR RTE 0.51 0.66 0.77
PAIR COMB 0.53 0.70 0.77
WMT 2008 (worst) 0.44 0.37
WMT 2008 (best) 0.56 0.83
Table 2: Evaluation on the WMT 2008 dataset for our
regression models, compared to results fromWMT 2008
ensemble of state-of-the-art metrics, which validates our
hope that linguistically motivated entailment features
are sufficiently robust to make a positive contribution
in MT evaluation. Furthermore, the two individual fea-
ture sets are outperformed by the combined feature set
COMB. We interpret this as support for our regression-
based combination approach.
Moving to WITHTIES, we see the best results from
the RTE model which improves by ?? = 0.06 for ABS
and ?? = 0.11 for PAIR. There is less improvement for
the other feature sets, in particular COMB. We submitted
the two overall best models, ABS-RTE and PAIR-RTE
with tie-aware prediction, to the WMT 2009 challenge.
Data Analysis. We analyzed at the models? predic-
tions to gain a better understanding of the differences in
the behavior of TRADMT-based and RTE-based mod-
els. As a first step, we computed consistency numbers
for the set of ?top? translations (hypotheses that were
ranked highest for a given reference) and for the set
of ?bottom? translations (hypotheses that were ranked
worst for a given reference). We found small but con-
sistent differences between the models: RTE performs
about 1.5 percent better on the top hypotheses than on
the bottom translations. We found the inverse effect for
the TRADMT model, which performs 2 points worse on
the top hypotheses than on the bottom hypotheses. Re-
visiting our initial concern that the entailment features
are too noisy for very bad translations, this finding indi-
cates some ungrammaticality-induced degradation for
the entailment features, but not much. Conversely, these
numbers also provide support for our initial hypothesis
that surface-based features are good at detecting very
deviant translations, but can have trouble dealing with
legitimate linguistic variation.
Next, we analyzed the average size of the score dif-
ferences between the best and second-best hypotheses
for correct and incorrect predictions. We found that the
RTE-based model predicted on average almost twice the
difference for correct predictions (? = 0.30) than for
incorrect predictions (? = 0.16), while the difference
was considerably smaller for the TRADMT-based model
(? = 0.17 for correct vs. ? = 0.13 for incorrect). We
believe it is this better discrimination on the top hypothe-
39
Segment TRADMT RTE COMB Gold
REF: Scottish NHS boards need to improve criminal records checks for
employees outside Europe, a watchdog has said.
HYP: The Scottish health ministry should improve the controls on extra-
community employees to check whether they have criminal precedents,
said the monitoring committee. [1357, lium-systran]
Rank: 3 Rank: 1 Rank: 2 Rank: 1
REF: Arguments, bullying and fights between the pupils have extended
to the relations between their parents.
HYP: Disputes, chicane and fights between the pupils transposed in
relations between the parents. [686, rbmt4]
Rank: 5 Rank: 2 Rank: 4 Rank: 5
Table 3: Examples of reference translations and MT output from the WMT 2008 French-English News dataset.
Rank judgments are out of five (smaller is better).
ses that explains the increased benefit the RTE-based
model obtains from tie-aware predictions: if the best
hypothesis is wrong, chances are much better than for
the TRADMT-based model that counting the second-
best hypothesis as ?best? is correct. Unfortunately, this
property is not shared by COMB to the same degree, and
it does not improve as much as RTE.
Table 3 illustrates the difference between RTE and
TRADMT. In the first example, RTE makes a more ac-
curate prediction than TRADMT. The human rater?s
favorite translation deviates considerably from the ref-
erence translation in lexical choice, syntactic structure,
and word order, for which it is punished by TRADMT.
In contrast, RTE determines correctly that the propo-
sitional content of the reference is almost completely
preserved. The prediction of COMB is between the two
extremes. The second example shows a sentence where
RTE provides a worse prediction. This sentence was
rated as bad by the judge, presumably due to the inap-
propriate translation of the main verb. This problem,
together with the reformulation of the subject, leads
TRADMT to correctly predict a low score (rank 5/5).
RTE?s deeper analysis comes up with a high score (rank
2/5), based on the existing semantic overlap. The com-
bined model is closer to the truth, predicting rank 4.
Feature Weights. Finally, we assessed the impor-
tance of the different entailment feature groups in the
RTE model.1 Since the presence of correlated features
makes the weights difficult to interpret, we restrict our-
selves to two general observations.
First, we find high weights not only for the score of
the alignment between hypothesis and reference, but
also for a number of syntacto-semantic match and mis-
match features. This means that we do get an additional
benefit from the presence of these features. For example,
features with a negative effect include dropping adjuncts,
unaligned root nodes, incompatible modality between
the main clauses, person and location mismatches (as
opposed to general mismatches) and wrongly handled
passives. Conversely, some factors that increase the
prediction are good alignment, matching embeddings
under factive verbs, and matches between appositions.
1The feature weights are similar for the COMB model.
Second, we find clear differences in the usefulness
of feature groups between MT evaluation and the RTE
task. Some of them, in particular structural features,
can be linked to the generally lower grammaticality of
MT hypotheses. A case in point is a feature that fires
for mismatches between dependents of predicates and
which is too unreliable on the SMT data. Other differ-
ences simply reflect that the two tasks have different
profiles, as sketched in Section 2.1. RTE exhibits high
feature weights for quantifier and polarity features, both
of which have the potential to influence entailment deci-
sions, but are relatively unimportant for MT evaluation,
at least at the current state of the art.
4 Conclusion
In this paper, we have investigated an approach to MT
evaluation that is inspired by the similarity between
this task and textual entailment. Our two models ? one
predicting absolute scores and one predicting pairwise
preference judgments ? use entailment features to pre-
dict the quality of MT hypotheses, thus replacing sur-
face matching with syntacto-semantic matching. Both
models perform similarly, showing sufficient robustness
and coverage to attain comparable performance to a
committee of established MT evaluation metrics.
We have described two refinements: (1) combining
the features into a superior joint model; and (2) adding a
confidence interval around the best hypothesis to model
ties for first place. Both strategies improve correlation;
however, unfortunately the benefits do not currently
combine. Our feature weight analysis indicates that
syntacto-semantic features do play an important role in
score prediction in the RTE model. We plan to assess
the additional benefit of the full entailment feature set
against the TRADMT feature set extended by a proper
lexical similarity metric, such as METEOR.
The computation of entailment features is more
heavyweight than traditional MT evaluation metrics.
We found the speed (about 6 s per hypothesis on a cur-
rent PC) to be sufficient for easily judging the quality of
datasets of the size conventionally used for MT evalua-
tion. However, this may still be too expensive as part of
an MT model that directly optimizes some performance
measure, e.g., minimum error rate training (Och, 2003).
40
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Translation
and Summarization, pages 65?72, Ann Arbor, MI.
R. Barzilay and N. Elhadad. 2002. Inferring strategies
for sentence ordering in multidocument news summa-
rization. Journal of Artificial Intelligence Research,
17:35?55.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU in ma-
chine translation research. In Proceedings of EACL,
pages 249?256, Trento, Italy.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the ACL Workshop on Statistical Machine
Translation, pages 70?106, Columbus, OH.
Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM: A
maximum similarity metric for machine translation
evaluation. In Proceedings of ACL-08: HLT, pages
55?62, Columbus, Ohio.
William W. Cohen, Robert E. Schapire, and Yoram
Singer. 1999. Learning to order things. Journal
of Artificial Intelligence Research, 10:243?270.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entailment
challenge. In Proceedings of the PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Southampton, UK.
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct word
sense matching for lexical substitution. In Proceed-
ings of ACL, Sydney, Australia.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram cooccurrence
statistics. In Proceedings of HLT, pages 128?132,
San Diego, CA.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. A smorgas-
bord of features for automatic MT evaluation. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 195?198, Columbus, Ohio.
Sanda Harabagiu and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain question
answering. In Proceedings of ACL, pages 905?912,
Sydney, Australia.
Bill MacCartney and Christopher D. Manning. 2008.
Modeling semantic containment and exclusion in nat-
ural language inference. In Proceedings of Coling,
pages 521?528, Manchester, UK.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Man-
ning. 2006. Learning to recognize features of valid
textual entailments. In Proceedings of NAACL, pages
41?48, New York City, NY.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167, Sapporo, Japan.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007. Dependency-based automatic evalu-
ation for machine translation. In Proceedings of
the NAACL-HLT / AMTA Workshop on Syntax and
Structure in Statistical Translation, pages 80?87,
Rochester, NY.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
ACL, pages 311?318, Philadelphia, PA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA, pages 223?231, Cambridge,
MA.
41
What?s in a translation rule?
Michel Galley
Dept. of Computer Science
Columbia University
New York, NY 10027
galley@cs.columbia.edu
Mark Hopkins
Dept. of Computer Science
University of California
Los Angeles, CA 90024
mhopkins@cs.ucla.edu
Kevin Knight and Daniel Marcu
Information Sciences Institute
University of Southern California
Marina Del Rey, CA 90292
{knight,marcu}@isi.edu
Abstract
We propose a theory that gives formal seman-
tics to word-level alignments defined over par-
allel corpora. We use our theory to introduce a
linear algorithm that can be used to derive from
word-aligned, parallel corpora the minimal set
of syntactically motivated transformation rules
that explain human translation data.
1 Introduction
In a very interesting study of syntax in statistical machine
translation, Fox (2002) looks at how well proposed trans-
lation models fit actual translation data. One such model
embodies a restricted, linguistically-motivated notion of
word re-ordering. Given an English parse tree, children
at any node may be reordered prior to translation. Nodes
are processed independently. Previous to Fox (2002), it
had been observed that this model would prohibit certain
re-orderings in certain language pairs (such as subject-
VP(verb-object) into verb-subject-object), but Fox car-
ried out the first careful empirical study, showing that
many other common translation patterns fall outside the
scope of the child-reordering model. This is true even
for languages as similar as English and French. For
example, English adverbs tend to move outside the lo-
cal parent/children in environment. The English word
?not? translates to the discontiguous pair ?ne ... pas.?
English parsing errors also cause trouble, as a normally
well-behaved re-ordering environment can be disrupted
by wrong phrase attachment. For other language pairs,
the divergence is expected to be greater.
In the face of these problems, we may choose among
several alternatives. The first is to abandon syntax in
statistical machine translation, on the grounds that syn-
tactic models are a poor fit for the data. On this view,
adding syntax yields no improvement over robust phrase-
substitution models, and the only question is how much
does syntax hurt performance. Along this line, (Koehn
et al, 2003) present convincing evidence that restricting
phrasal translation to syntactic constituents yields poor
translation performance ? the ability to translate non-
constituent phrases (such as ?there are?, ?note that?, and
?according to?) turns out to be critical and pervasive.
Another direction is to abandon conventional English
syntax and move to more robust grammars that adapt to
the parallel training corpus. One approach here is that of
Wu (1997), in which word-movement is modeled by rota-
tions at unlabeled, binary-branching nodes. At each sen-
tence pair, the parse adapts to explain the translation pat-
tern. If the same unambiguous English sentence were to
appear twice in the corpus, with different Chinese trans-
lations, then it could have different learned parses.
A third direction is to maintain English syntax and
investigate alternate transformation models. After all,
many conventional translation systems are indeed based
on syntactic transformations far more expressive than
what has been proposed in syntax-based statistical MT.
We take this approach in our paper. Of course, the broad
statistical MT program is aimed at a wider goal than
the conventional rule-based program ? it seeks to under-
stand and explain human translation data, and automati-
cally learn from it. For this reason, we think it is impor-
tant to learn from the model/data explainability studies of
Fox (2002) and to extend her results. In addition to being
motivated by rule-based systems, we also see advantages
to English syntax within the statistical framework, such
as marrying syntax-based translation models with syntax-
based language models (Charniak et al, 2003) and other
potential benefits described by Eisner (2003).
Our basic idea is to create transformation rules that
condition on larger fragments of tree structure. It is
certainly possible to build such rules by hand, and we
have done this to formally explain a number of human-
translation examples. But our main interest is in collect-
ing a large set of such rules automatically through corpus
analysis. The search for these rules is driven exactly by
the problems raised by Fox (2002) ? cases of crossing
and divergence motivate the algorithms to come up with
better explanations of the data and better rules. Section
2 of this paper describes algorithms for the acquisition
of complex rules for a transformation model. Section 3
gives empirical results on the explanatory power of the
acquired rules versus previous models. Section 4 presents
examples of learned rules and shows the various types of
transformations (lexical and nonlexical, contiguous and
noncontiguous, simple and complex) that the algorithms
are forced (by the data) to invent. Section 5 concludes.
Due to space constraints, all proofs are omitted.
2 Rule Acquisition
Suppose that we have a French sentence, its translation
into English, and a parse tree over the English translation,
as shown in Figure 1. Generally one defines an alignment
as a relation between the words in the French sentence
and the words in the English sentence. Given such an
alignment however, what kinds of rules are we entitled
to learn from this instance? How do we know when it is
valid to extract a particular rule, especially in the pres-
ence of numerous crossings in the alignment? In this sec-
tion, we give principled answers to these questions, by
constructing a theory that gives formal semantics to word
alignments.
2.1 A Theory of Word Alignments
We are going to define a generative process through
which a string from a source alphabet is mapped to a
rooted tree whose nodes are labeled from a target alha-
bet. Henceforth we will refer to symbols from our source
alphabet as source symbols and symbols from our target
alphabet as target symbols. We define a symbol tree over
an alphabet ? as a rooted, directed tree, the nodes of
which are each labeled with a symbol of ?.
We want to capture the process by which a symbol tree
over the target language is derived from a string of source
symbols. Let us refer to the symbol tree that we want to
derive as the target tree. Any subtree of this tree will be
called a target subtree. Furthermore, we define a deriva-
tion string as an ordered sequence of elements, each of
which is either a source symbol or a target subtree.
Now we are ready to define the derivation process.
Given a derivation string S, a derivation step replaces
a substring S? of S with a target subtree T that has the
following properties:
1. Any target subtree in S ? is a subtree of T .
2. Any target subtree in S but not in S ? does not share
nodes with T .
S
NP VP
PRP RBAUX VB
he notdoes go
il vane pas
Figure 1: A French sentence aligned with an English
parse tree.
il ne va pas
ne va pas
he
PRP
NP
ne pas
he
PRP
NP
S
NP VP
PRP RBAUX VB
he notdoes go
VB
go
il ne va pas
ne va pasRB
not
ne heRB
not
S
NP VP
PRP RBAUX VB
he notdoes go
il ne va pas
S
NP VP
PRP RBAUX VB
he notdoes go
NP VP
PRP RBAUX VB
he notdoes go
Figure 2: Three alternative derivations from a source sen-
tence to a target tree.
Moreover, a derivation from a string S of source sym-
bols to the target tree T is a sequence of derivation steps
that produces T from S.
Moving away from the abstract for a moment, let us
revisit the example from Figure 1. Figure 2 shows three
derivations of the target tree from the source string ?il
ne va pas?, which are all consistent with our defini-
tions. However, it is apparent that one of these deriva-
tions seems much more ?wrong? than the other. Specif-
ically, in the second derivation, ?pas? is replaced by the
English word ?he,? which makes no sense. Given the vast
space of possible derivations (according to the definition
above), how do we distinguish between good ones and
bad ones? Here is where the notion of an alignment be-
comes useful.
Let S be a string of source symbols and let T be a target
tree. First observe the following facts about derivations
from S to T (these follow directly from the definitions):
1. Each element of S is replaced at exactly one step of
the derivation.
SNP VP
PRP RBAUX VB
he notdoes go
il vane pas
S
NP VP
PRP RBAUX VB
he notdoes go
il vane pas
S
NP VP
PRP RBAUX VB
he notdoes go
il vane pas
Figure 3: The alignments induced by the derivations in
Figure 2
2. Each node of T is created at exactly one step of the
derivation.
Thus for each element s of S, we can define
replaced(s, D) to be the step of the derivation D during
which s is replaced. For instance, in the leftmost deriva-
tion of Figure 2, ?va? is replaced by the second step of the
derivation, thus replaced(va, D) = 2. Similarly, for each
node t of T , we can define created(t, D) to be the step
of derivation D during which t is created. For instance,
in the same derivation, the nodes labeled by ?AUX? and
?VP? are created during the third step of the derivation,
thus created(AUX, D) = 3 and created(VP, D) = 3.
Given a string S of source symbols and a target tree
T , an alignment A with respect to S and T is a relation
between the leaves of T and the elements of S. Choose
some derivation D from S to T . The alignment A in-
duced by D is created as follows: an element s of S is
aligned with a leaf node t of T iff replaced(s, D) =
created(t, D). In other words, a source word is aligned
with a target word if the target word is created during the
same step in which the source word is replaced. Figure 3
shows the alignments induced by the derivations of Fig-
ure 2.
Now, say that we have a source string, a target tree,
and an alignment A. A key observation is that the set
of ?good? derivations according to A is precisely the set
of derivations that induce alignments A? such that A is
a subalignment of A?. By subalignment, we mean that
A ? A? (recall that alignments are simple mathematical
relations). In other words, A is a subalignment of A? if A
aligns two elements only if A? also aligns them.
We can see this intuitively by examining Figures 2 and
3. Notice that the two derivations that seem ?right? (the
first and the third) are superalignments of the alignment
given in Figure 1, while the derivation that is clearly
wrong is not. Hence we now have a formal definition
of the derivations that we are interested in. We say that
a derivation is admitted by an alignment A if it induces a
superalignment of A. The set of derivations from source
string S to target tree T that are admitted by alignment A
can be denoted ?A(S, T ). Given this, we are ready to ob-
tain a formal characterization of the set of rules that can
ne pas
he
PRP
NP
VB
go
NP VP
PRP RBAUX VB
he notdoes go
Derivationstep: Inducedrule:
input: ne VB?pas
output: VP
RBAUX x2
notdoes
S
NP VP
PRP RBAUX VB
he notdoes go
input: NP?VP
output: S
x1 x2
Figure 4: Two derivation steps and the rules that are in-
duced from them.
be inferred from the source string, target tree, and align-
ment.
2.2 From Derivations to Rules
In essence, a derivation step can be viewed as the applica-
tion of a rule. Thus, compiling the set of derivation steps
used in any derivation of ?A(S, T ) gives us, in a mean-
ingful sense, all relevant rules that can be extracted from
the triple (S, T, A). In this section, we show in concrete
terms how to convert a derivation step into a usable rule.
Consider the second-last derivation step of the first
derivation in Figure 2. In it, we begin with a source sym-
bol ?ne?, followed by a target subtree rooted at V B, fol-
lowed by another source symbol ?pas.? These three ele-
ments of the derivation string are replaced with a target
subtree rooted at V P that discards the source symbols
and contains the target subtree rooted at V B. In general,
this replacement process can be captured by the rule de-
picted in Figure 4. The input to the rule are the roots
of the elements of the derivation string that are replaced
(where we define the root of a symbol to be simply the
symbol itself), whereas the output of the rule is a symbol
tree, except that some of the leaves are labeled with vari-
ables instead of symbols from the target alhabet. These
variables correspond to elements of the input to the rule.
For instance, the leaf labeled x2 means that when this rule
is applied, x2 is replaced by the target subtree rooted at
V B (since V B is the second element of the input). Ob-
serve that the second rule induced in Figure 4 is simply
a CFG rule expressed in the opposite direction, thus this
rule format can (and should) be viewed as a strict gener-
alization of CFG rules.
SNP VP
PRP RBAUX VB
he notdoes go
il vane pas
{ il, ne, va,pas}
{ ne, va,pas}{ il }
{ il }
{ il }
{ il }
{ne,pas} {ne,pas}
{ne,pas} {ne,pas}
{ va }
{ ne } { va }
{ va }
{pas}
Figure 5: An alignment graph. The nodes are annotated
with their spans. Nodes in the frontier set are boldfaced
and italicized.
Every derivation step can be mapped to a rule in this
way. Hence given a source string S, a target tree T , and
an alignment A, we can define the set ?A(S, T ) as the set
of rules in any derivation D ? ?A(S, T ). We can regard
this as the set of rules that we are entitled to infer from
the triple (S, T, A).
2.3 Inferring Complex Rules
Now we have a precise problem statement: learn the set
?A(S, T ). It is not immediately clear how such a set can
be learned from the triple (S, T, A). Fortunately, we can
infer these rules directly from a structure called an align-
ment graph. In fact, we have already seen numerous ex-
amples of alignment graphs. Graphically, we have been
depicting the triple (S, T, A) as a rooted, directed, acyclic
graph (where direction is top-down in the diagrams). We
refer to such a graph as an alignment graph. Formally,
the alignment graph corresponding to S, T , and A is just
T , augmented with a node for each element of S, and
edges from leaf node t ? T to element s ? S iff A aligns
s with t. Although there is a difference between a node
of the alignment graph and its label, we will not make a
distinction, to ease the notational burden.
To make the presentation easier to follow, we assume
throughout this section that the alignment graph is con-
nected, i.e. there are no unaligned elements. All of the
results that follow have generalizations to deal with un-
aligned elements, but unaligned elements incur certain
procedural complications that would cloud the exposi-
tion.
It turns out that it is possible to systematically con-
vert certain fragments of the alignment graph into rules
of ?A(S, T ). We define a fragment of a directed, acyclic
graph G to be a nontrivial (i.e. not just a single node) sub-
graph G? of G such that if a node n is in G? then either n
is a sink node of G? (i.e. it has no children) or all of its
children are in G? (and it is connected to all of them). In
VP
RBAUX VB
notdoes
ne pas
S
NP VP
input: ne VB?pas
output: VP
RBAUX x2
notdoes
input: NP?VP
output: S
x1 x2
{ ne } {pas}
{ va }
{ ne, va,pas}
{ il } { ne, va,pas}
{ il, ne, va,pas}
Figure 6: Two frontier graph fragments and the rules in-
duced from them. Observe that the spans of the sink
nodes form a partition of the span of the root.
Figure 6, we show two examples of graph fragments of
the alignment graph of Figure 5.
The span of a node n of the alignment graph is the
subset of nodes from S that are reachable from n. Note
that this definition is similar to, but not quite the same
as, the definition of a span given by Fox (2002). We
say that a span is contiguous if it contains all elements
of a contiguous substring of S. The closure of span(n)
is the shortest contiguous span which is a superset of
span(n). For instance, the closure of {s2, s3, s5, s7}
would be {s2, s3, s4, s5, s6, s7} The alignment graph in
Figure 5 is annotated with the span of each node.
Take a look at the graph fragments in Figure 6. These
fragments are special: they are examples of frontier
graph fragments. We first define the frontier set of an
alignment graph to be the set of nodes n that satisfy the
following property: for every node n? of the alignment
graph that is connected to n but is neither an ancestor nor
a descendant of n, span(n?) ? closure(span(n)) = ?.
We then define a frontier graph fragment of an align-
ment graph to be a graph fragment such that the root and
all sinks are in the frontier set. Frontier graph fragments
have the property that the spans of the sinks of the frag-
ment are each contiguous and form a partition of the span
of the root, which is also contiguous. This allows the fol-
lowing transformation process:
1. Place the sinks in the order defined by the partition
(i.e. the sink whose span is the first part of the span
of the root goes first, the sink whose span is the sec-
ond part of the span of the root goes second, etc.).
This forms the input of the rule.
2. Replace sink nodes of the fragment with a variable
corresponding to their position in the input, then
take the tree part of the fragment (i.e. project the
fragment on T ). This forms the output of the rule.
Figure 6 shows the rules derived from the given graph
fragments. We have the following result.
Theorem 1 Rules constructed according to the above
procedure are in ?A(S, T ).
Rule extraction: Algorithm 1. Thus we now have a
simple method for extracting rules of ?A(S, T ) from the
alignment graph: search the space of graph fragments for
frontier graph fragments.
Unfortunately, the search space of all fragments of a
graph is exponential in the size of the graph, thus this
procedure can also take a long time to execute. To ar-
rive at a much faster procedure, we take advantage of the
following provable facts:
1. The frontier set of an alignment graph can be identi-
fied in time linear in the size of the graph.
2. For each node n of the frontier set, there is a unique
minimal frontier graph fragment rooted at n (ob-
serve that for any node n? not in the frontier set,
there is no frontier graph fragment rooted at n?, by
definition).
By minimal, we mean that the frontier graph fragment
is a subgraph of every other frontier graph fragment with
the same root. Clearly, for an alignment graph with k
nodes, there are at most k minimal frontier graph frag-
ments. In Figure 7, we show the seven minimal frontier
graph fragments of the alignment graph of Figure 5. Fur-
thermore, all other frontier graph fragments can be cre-
ated by composing 2 or more minimal graph fragments,
as shown in Figure 8. Thus, the entire set of frontier graph
fragments (and all rules derivable from these fragments)
can be computed systematically as follows: compute the
set of minimal frontier graph fragments, compute the set
of graph fragments resulting from composing 2 minimal
frontier graph fragments, compute the set of graph frag-
ments resulting from composing 3 minimal graph frag-
ments, etc. In this way, the rules derived from the min-
imal frontier graph fragments can be regarded as a ba-
sis for all other rules derivable from frontier graph frag-
ments. Furthermore, we conjecture that the set of rules
derivable from frontier graph fragments is in fact equiva-
lent to ?A(S, T ).
Thus we have boiled down the problem of extracting
complex rules to the following simple problem: find the
set of minimal frontier graph fragments of a given align-
ment graph.
The algorithm is a two-step process, as shown below.
Rule extraction: Algorithm 2
1. Compute the frontier set of the alignment graph.
2. For each node of the frontier set, compute the mini-
mal frontier graph fragment rooted at that node.
VP
RBAUX VB
notdoes
ne pas
S
NP VP
NP
PRP
PRP
he
VB
go
go
vahe
il
Figure 7: The seven minimal frontier graph fragments of
the alignment graph in Figure 5
VP
RBAUX VB
notdoes
ne pas
VB
go
+ =
VP
RBAUX VB
notdoes
ne pas
go
S
NP VP
+ + =
NP
PRP
PRP
he
S
NP VP
PRP
he
Figure 8: Example compositions of minimal frontier
graph fragments into larger frontier graph fragments.
Step 1 can be computed in a single traversal of the
alignment graph. This traversal annotates each node with
its span and its complement span. The complement span
is computed as the union of the complement span of its
parent and the span of all its siblings (siblings are nodes
that share the same parent). A node n is in the frontier
set iff complement span(n) ? closure(span(n)) = ?.
Notice that the complement span merely summarizes the
spans of all nodes that are neither ancestors nor descen-
dents of n. Since this step requires only a single graph
traversal, it runs in linear time.
Step 2 can also be computed straightforwardly. For
each node n of the frontier set, do the following: expand
n, then as long as there is some sink node n? of the result-
ing graph fragment that is not in the frontier set, expand
n?. Note that after computing the minimal graph frag-
ment rooted at each node of the frontier set, every node
of the alignment graph has been expanded at most once.
Thus this step also runs in linear time.
For clarity of exposition and lack of space, a couple of
issues have been glossed over. Briefly:
? As previously stated, we have ignored here the is-
sue of unaligned elements, but the procedures can
be easily generalized to accommodate these. The
results of the next two sections are all based on im-
plementations that handle unaligned elements.
? This theory can be generalized quite cleanly to in-
clude derivations for which substrings are replaced
by sets of trees, rather than one single tree. This
corresponds to allowing rules that do not require the
output to be a single, rooted tree. Such a general-
ization gives some nice power to effectively explain
certain linguistic phenomena. For instance, it allows
us to immediately translate ?va? as ?does go? in-
stead of delaying the creation of the auxiliary word
?does? until later in the derivation.
3 Experiments
3.1 Language Choice
We evaluated the coverage of our model of transforma-
tion rules with two language pairs: English-French and
English-Chinese. These two pairs clearly contrast by
the underlying difficulty to understand and model syntac-
tic transformations among pairs: while there is arguably
a fair level of cohesion between English and French,
English and Chinese are syntactically more distant lan-
guages. We also chose French to compare our study with
that of Fox (2002). The additional language pair provides
a good means of evaluating how our transformation rule
extraction method scales to more problematic language
pairs for which child-reordering models are shown not to
explain the data well.
3.2 Data
We performed experiments with two corpora, the FBIS
English-Chinese Parallel Text and the Hansard French-
English corpus.We parsed the English sentences with
a state-of-the-art statistical parser (Collins, 1999). For
the FBIS corpus (representing eight million English
words), we automatically generated word-alignments us-
ing GIZA++ (Och and Ney, 2003), which we trained on
a much larger data set (150 million words). Cases other
than one-to-one sentence mappings were eliminated. For
the Hansard corpus, we took the human annotation of
word alignment described in (Och and Ney, 2000). The
corpus contains two kinds of alignments: S (sure) for
unambiguous cases and P (possible) for unclear cases,
e.g. idiomatic expressions and missing function words
(S ? P ). In order to be able to make legitimate com-
parisons between the two language pairs, we also used
GIZA++ to obtain machine-generated word alignments
for Hansard: we trained it with the 500 sentences and
additional data representing 13.7 million English words
(taken from the Hansard and European parliament cor-
pora).
3.3 Results
From a theoretical point of view, we have shown that our
model can fully explain the transformation of any parse
tree of the source language into a string of the target lan-
guage. The purpose of this section is twofold: to pro-
vide quantitative results confirming the full coverage of
our model and to analyze some properties of the trans-
formation rules that support these derivations (linguistic
analyses of these rules are presented in the next section).
Figure 9 summarizes the coverage of our model with
respect to the Hansard and FBIS corpora. For the for-
mer, we present results for the three alignments: S align-
ments, P alignments, and the alignments computed by
GIZA++. Each plotted value represents a percentage of
parse trees in a corpus that can be transformed into a tar-
get sentence using transformation rules. The x-axis rep-
resents different restrictions on the size of these rules: if
we use a model that restrict rules to a single expansion
of a non-terminal into a sequence of symbols, we are in
the scope of the child-reordering model of (Yamada and
Knight, 2001; Fox, 2002). We see that its explanatory
power is quite poor, with only 19.4%, 14.3%, 16.5%, and
12.1% (for the respective corpora). Allowing more ex-
pansions logically expands the coverage of the model,
until the point where it is total: transformation rules no
larger than 17, 18, 23, and 43 (in number of rule expan-
sions) respectively provide enough coverage to explain
the data at 100% for each of the four cases.
It appears from the plot that the quality of alignments
plays an important role. If we compare the three kinds of
alignments available for the Hansard corpus, we see that
much more complex transformation rules are extracted
from noisy GIZA++ alignments. It also appears that the
language difference produces quite contrasting results.
Rules acquired for the English-Chinese pair have, on av-
erage, many more nodes. Note that the language differ-
ence in terms of syntax might be wider than what the plot
seems to indicate, since word alignments computed for
the Hansard corpus are likely to be more errorful than the
ones for FBIS because the training data used to induce the
latter is more than ten times larger than for the former.
In Figure 10, we show the explanatory power of our
model at the node level. At each node of the frontier
set, we determine whether it is possible to extract a rule
that doesn?t exceed a given limit k on its size. The plot-
ted values represent the percentage of frontier set inter-
nal nodes that satisfy this condition. These results appear
more promising for the child-reordering model, with cov-
erage ranging from 72.3% to 85.1% of the nodes, but we
should keep in mind that many of these nodes are low in
the tree (e.g. base NPs); extraction of 1-level transfor-
mation rules generally present no difficulties when child
nodes are pre-terminals, since any crossings can be re-
solved by lexicalizing the elements involved in it. How-
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 2 3 4 5 6 7 8 9 10 15 20 25 30 35 40 4550
Pa
rse
 tr
ee
 co
ve
ra
ge
Maximum number of rule expansions
"Hansard-S"
"Hansard-P"
"Hansard-GIZA"
"FBIS"
Figure 9: Percentage of parse trees covered by the model
given different constraints on the maximum size of the
transformation rules.
0.7
0.75
0.8
0.85
0.9
0.95
1
1 2 3 4 5 6 7 8 9 10 15 20 25 30 35 40 4550
No
de
 co
ve
ra
ge
Maximum number of rule expansions
"Hansard-S"
"Hansard-P"
"Hansard-GIZA"
"FBIS"
Figure 10: Same as Figure 9, except that here coverage is
evaluated at the node level.
ever, higher level syntactic constituents are more prob-
lematic for child-reordering models, and the main rea-
sons they fail to provide explanation of the parses at the
sentence level.
Table 1 shows that the extraction of rules can be per-
formed quite efficiently. Our first algorithm, which has an
exponential running time, cannot scale to process large
corpora and extract a sufficient number of rules that a
syntax-based statistical MT system would require. The
second algorithm, which runs in linear time, is on the
other hand barely affected by the size of rules it extracts.
k=1 3 5 7 10 20 50
I 4.1 10.2 57.9 304.2 - - -
II 4.3 5.4 5.9 6.4 7.33 9.6 11.8
Table 1: Running time in seconds of the two algorithms
on 1000 sentences. k represent the maximum size of rules
to extract.
NPB
DT NN RB
that Government simply tells
ADVP
VBZ
NPB
DT NNS
the people what is themgood for
WP VBZ JJ IN PRP
NPB
ADJP
VP
SG-A
SBAR-A
VHPN
VP
S
le gouvernement dit tout simplement ? les gens ce qui est bon pour eux
input:
VBZ ADVP ?NPB SBAR -S
output: S
VPx2
x1 x3 x4
Figure 11: Adverb-verb reordering.
4 Discussions
In this section, we present some syntactic transformation
rules that our system learns. Fox (2002) identified three
major causes of crossings between English and French:
the ?ne ... pas? construct, modals and adverbs, which a
child-reordering model doesn?t account for. In section 2,
we have already explained how we learn syntactic rules
involving ?ne ... pas?. Here we describe the other two
problematic cases.
Figure 11 presents a frequent cause of crossings be-
tween English and French: adverbs in French often ap-
pear after the verb, which is less common in English.
Parsers generally create nested verb phrases when ad-
verbs are present, thus no child reordering can allow a
verb and an adverb to be permuted. Multi-level reodering
as the rule in the figure can prevent crossings. Fox?s solu-
tion to the problem of crossings is to flatten verb phrases.
This is a solution for this sentence pair, since this ac-
counts for adverb-verb reorderings, but flattening the tree
structure is not a general solution. Indeed, it can only ap-
ply to a very limited number of syntactic categories, for
which the advantage of having a deep syntactic structure
is lost.
Figure 12 (dotted lines are P alignments) shows an in-
teresting example where flattening the tree structure can-
not resolve all crossings in node-reordering models. In
these models, a crossing remains between MD and AUX
no matter how VPs are flattened. Our transformation rule
model creates a lexicalized rule as shown in the figure,
where the transformation of ?will be? into ?sera? is the
only way to resolve the crossing.
In the Chinese-English domain, the rules extracted by
our algorithm often have the attractive quality that they
are the kind of common-sense constructions that are used
in Chinese language textbooks to teach students. For in-
stance, there are several that illustrate the complex re-
orderings that occur around the Chinese marker word
?de.?
NPB
DT JJ NN
the full report will
MD AUX VB
be coming in before the fall
RB IN DT NN
NPB
PP
VP-A
ADVP
VP
S
le rapport complet sera d?pos? de ici le automne prochain
input: sera  VP-A
output:
VP
VP-Awill/MD
be/AUX
VP-A
x2
Figure 12: Crossing due to a modal.
5 Conclusion
The fundamental assumption underlying much recent
work in statistical machine translation (Yamada and
Knight, 2001; Eisner, 2003; Gildea, 2003) is that lo-
cal transformations (primarily child-node re-orderings)
of one-level parent-children substructures are an adequate
model for parallel corpora. Our empirical results suggest
that this may be too strong of an assumption. To explain
the data in two parallel corpora, one English-French, and
one English-Chinese, we are often forced to learn rules
involving much larger tree fragments. The theory, algo-
rithms, and transformation rules we learn automatically
from data have several interesting aspects.
1. Our rules provide a good, realistic indicator of the
complexities inherent in translation. We believe that
these rules can inspire subsequent developments of
generative statistical models that are better at ex-
plaining parallel data than current ones.
2. Our rules put at the fingertips of linguists a very
rich source of information. They encode translation
transformations that are both syntactically and lex-
ically motivated (some of our rules are purely syn-
tactic; others are lexically grounded). A simple sort
on the counts of our rules makes explicit the trans-
formations that occur most often. A comparison of
the number of rules extracted from parallel corpora
specific to multiple language pairs provide a quanti-
tative estimator of the syntactic ?closeness? between
various language pairs.
3. The theory we proposed in this paper is independent
of the method that one uses to compute the word-
level alignments in a parallel corpus.
4. The theory and rule-extraction algorithm are also
well-suited to deal with the errors introduced by
the word-level alignment and parsing programs one
uses. Our theory makes no a priori assumptions
about the transformations that one is permitted to
learn. If a parser, for example, makes a systematic
error, we expect to learn a rule that can neverthe-
less be systematically used to produce correct trans-
lations.
In this paper, we focused on providing a well-founded
mathematical theory and efficient, linear algorithms
for learning syntactically motivated transformation rules
from parallel corpora. One can easily imagine a range
of techniques for defining probability distributions over
the rules that we learn. We suspect that such probabilis-
tic rules could be also used in conjunction with statistical
decoders, to increase the accuracy of statistical machine
translation systems.
Acknowledgements
This work was supported by DARPA contract N66001-
00-1-9814 and MURI grant N00014-00-1-0617.
References
E. Charniak, K. Knight, and K. Yamada. 2003. Syntax-
based language models for machine translation. In
Proc. MT Summit IX.
M. Collins. 1999. Head-driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania, Philadelphia.
J. Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In Proc. of the 41st Meeting
of the Association for Computational Linguistics.
H. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proc. of Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
D. Gildea. 2003. Loosely tree-based alignment for ma-
chine translation. In Proc. of the 41th Annual Confer-
ence of the Association for Computational Linguistics.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proceedings of HLT/NAACL.
F. Och and H. Ney. 2000. Improved statistical alignment
models. Proc. of the 38th Annual Meeting of the Asso-
ciation for Computational Linguistics.
F. Och and H Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?404.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In ACL, pages 523?530.
Proceedings of NAACL HLT 2007, pages 180?187,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Lexicalized Markov Grammars for Sentence Compression?
Michel Galley and Kathleen R. McKeown
Columbia University
Department of Computer Science
New York, NY 10027, USA
{galley,kathy}@cs.columbia.edu
Abstract
We present a sentence compression system based
on synchronous context-free grammars (SCFG),
following the successful noisy-channel approach
of (Knight and Marcu, 2000). We define a head-
driven Markovization formulation of SCFG dele-
tion rules, which allows us to lexicalize probabili-
ties of constituent deletions. We also use a robust
approach for tree-to-tree alignment between arbi-
trary document-abstract parallel corpora, which lets
us train lexicalized models with much more data
than previous approaches relying exclusively on
scarcely available document-compression corpora.
Finally, we evaluate different Markovized models,
and find that our selected best model is one that ex-
ploits head-modifier bilexicalization to accurately
distinguish adjuncts from complements, and that
produces sentences that were judged more gram-
matical than those generated by previous work.
1 Introduction
Sentence compression addresses the problem of re-
moving words or phrases that are not necessary
in the generated output of, for instance, summa-
rization and question answering systems. Given
the need to ensure grammatical sentences, a num-
ber of researchers have used syntax-directed ap-
proaches that perform transformations on the out-
put of syntactic parsers (Jing, 2000; Dorr et al,
2003). Some of them (Knight and Marcu, 2000;
Turner and Charniak, 2005) take an empirical ap-
proach, relying on formalisms equivalent to proba-
bilistic synchronous context-free grammars (SCFG)
?This material is based on research supported in part
by the U.S. National Science Foundation (NSF) under Grant
No. IIS-05-34871 and the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-06-C-0023.
Any opinions, findings and conclusions or recommendations
expressed in this material are those of the authors and do not
necessarily reflect the views of the NSF or DARPA.
(Lewis and Stearns, 1968; Aho and Ullman, 1969) to
extract compression rules from aligned Penn Tree-
bank (PTB) trees. While their approach proved suc-
cessful, their reliance on standard maximum like-
lihood estimators for SCFG productions results in
considerable sparseness issues, especially given the
relative flat structure of PTB trees; in practice, many
SCFG productions are seen only once. This problem
is exacerbated for the compression task, which has
only scarce training material available.
In this paper, we present a head-driven
Markovization of SCFG compression rules, an
approach that was successfully used in syntactic
parsing (Collins, 1999; Klein and Manning, 2003)
to alleviate issues intrinsic to relative frequency
estimation of treebank productions. Markovization
for sentence compression provides several benefits,
including the ability to condition deletions on
a flexible amount of syntactic context, to treat
head-modifier dependencies independently, and to
lexicalize SCFG productions.
Another part of our effort focuses on better align-
ment models for extracting SCFG compression rules
from parallel data, and to improve upon (Knight
and Marcu, 2000), who could only exploit 1.75% of
the Ziff-Davis corpus because of stringent assump-
tions about human abstractive behavior. To alleviate
their restrictions, we rely on a robust approach for
aligning trees of arbitrary document-abstract sen-
tence pairs. After accounting for sentence pairs with
both substitutions and deletions, we reached a reten-
tion of more than 25% of the Ziff-Davis data, which
greatly benefited the lexical probabilities incorpo-
rated into our Markovized SCFGs.
Our work provides three main contributions:
180
(1) Our lexicalized head-driven Markovization
yields more robust probability estimates, and our
compressions outperform (Knight and Marcu, 2000)
according to automatic and human evaluation.
(2) We provide a comprehensive analysis of the im-
pact of different Markov orders for sentence com-
pression, similarly to a study done for PCFGs (Klein
and Manning, 2003). (3) We provide a framework
for exploiting document-abstract sentence pairs that
are not purely compressive, and augment the avail-
able training resources for syntax-directed sentence
compression systems.
2 Synchronous Grammars for Sentence
Compression
One successful syntax-driven approach (Knight and
Marcu, 2000, henceforth K&M) relies on syn-
chronous context-free grammars (SCFG) (Lewis
and Stearns, 1968; Aho and Ullman, 1969). SCFGs
can be informally defined as context-free grammars
(CFGs) whose productions have two right-hand side
strings instead of one, namely source and target
right-hand side. In the case of sentence compres-
sion, we restrict the target side to be a sub-sequence
of the source side (possibly identical), and we will
call this restricted grammar a deletion SCFG. For in-
stance, a deletion SCFG rule that removes an adver-
bial phrase (ADVP) between an noun phrase (NP)
and a verb phrase (VP) may be written as follows:
S ? ?NP ADVP VP, NP VP?
In a sentence compression framework similar to
the one presented by K&M, we build SCFGs that
are fully trainable from a corpus of document and
reduced sentences. Such an approach comprises
two subproblems: (1) transform tree pairs into syn-
chronous grammar derivations; (2) based on these
derivations, assign probabilities to deletion SCFG
productions, and more generally, to compressions
produced by such grammars. Since the main point of
our paper lies in the exploration of better probability
estimates through Markovization and lexicalization
of SCFGs, we first address the latter problem, and
discuss the task of building synchronous derivations
only later in Section 4.
2.1 Stochastic Synchronous Grammars
The overall goal of a sentence compression system is
to transform a given input sentence f into a concise
and grammatical sentence c ? C, which is a sub-
sequence of f . Similarly to K&M and many suc-
cessful syntactic parsers (Collins, 1999; Klein and
Manning, 2003), our sentence compression system
is generative, and attempts to find the optimal com-
pression c? by estimating the following function:1
c? = argmax
c?C
{
p(c|f)
}
= argmax
c?C
{
p(f , c)
}
(1)
If ?(f , c) is the set of all tree pairs that yield (f , c)
according to some underlying SCFG, we can esti-
mate the probability of the sentence pair using:
p(f , c) =
?
(pif ,pic)??(f ,c)
P (pif , pic) (2)
We note that, in practice (and as in K&M), Equa-
tion 2 is often approximated by restricting ?(f , c)
to a unique full tree p?if , the best hypothesis of an
off-the-shelf syntactic parser. This implies that each
possible compression c is the target-side yield of at
most one SCFG derivation.
As in standard PCFG history-based models, the
probability of the entire structure (Equation 2) is fac-
tored into probabilities of grammar productions. If
? is a derivation ? = r1 ? ? ? ? ? rj ? ? ? ? rJ , where
rj denotes the SCFG rule lj ? ??jf , ?
j
c?, we get:
p(pif , pic) =
J?
j=1
p(?jf , ?
j
c|l
j) (3)
The question we will now address is how to esti-
mate the probability p(?jf , ?
j
c|lj) of each SCFG pro-
duction.
2.2 Lexicalized Head-Driven Markovization of
Synchronous Grammars
A main issue in our enterprise is to reliably estimate
productions of deletion SCFGs. In a sentence com-
pression framework as the one presented by K&M,
we use aligned trees of the form of the Penn Tree-
bank (PTB) (Marcus et al, 1994) to acquire and
score SCFG productions. However, the use of the
PTB structure faces many challenges also encoun-
tered in probabilistic parsing.
1In their noisy-channel approach, K&M further break down
p(c, f) into p(f |c) ? p(c), which we refrain from doing for rea-
sons that will become obvious later.
181
Firstly, PTB tree structures are relatively flat, par-
ticularly within noun phrases. For instance, adjec-
tive phrases (ADJP)?which are generally good can-
didates for deletions?appear in 90 different NP-
rooted SCFG productions in Ziff-Davis,2 61 of
which appear only once, e.g., NP ? ?DT ADJP JJ
NN NN, DT JJ NN NN?. While it may seem ad-
vantageous to maintain many constituents within the
same domain of locality of an SCFG production, as
we may hope to exploit its large syntactic context to
condition deletions more accurately, the sparsity of
such productions make them poor candidates for rel-
ative frequency estimation, especially in a task with
limited quantities of training material. Indeed, our
base training corpus described in Section 4 contains
only 951 SCFG productions, 593 appearing once.
Secondly, syntactic categories in the PTB are par-
ticularly coarse grained, and lead to many incorrect
context-free assumptions. Some important distinc-
tions, such as between arguments and adjuncts, are
beyond the scope of the PTB annotation, and it is
often difficult to determine out of context whether a
given constituent can safely be deleted from a right-
hand side.
One first type of annotation that can effectively be
added to each syntactic category is its lexical head
and head part-of-speech (POS), following work in
syntactic parsing (Collins, 1999). This type of an-
notation is particular beneficial in the case of, e.g.,
prepositional phrases (PP), which may be either
complement or adjunct. As in the case of Figure 1
(in which adjuncts appear in italic), knowing that the
PP headed by ?from? appears in a VP headed by
?fell? helps us to determine that the PP is a com-
plement to the verb ?fell?, and that it should pre-
sumably not be deleted. Conversely, the PP headed
by ?because? modifying the same verb is an adjunct,
and can safely be deleted if unimportant.3 Also, as
discussed in (Klein and Manning, 2003), POS an-
notation can be useful as a means of backing off
to more frequently occurring head-modifier POS oc-
currences (e.g., VBD-IN) when specific bilexical co-
2Details about the SCFG extraction procedure are given in
Section 4. In short, we refer here to a grammar generated from
823 sentence pairs.
3The PP headed by ?from? is an optional argument, and thus
may still be deleted. Our point is that lexical information in gen-
eral should help give lower scores to deletions of constituents
that are grammatically more prominent.
NN
Earning
NP
RB
also
ADVP
VBD
fell IN
from DT
the
JJ
year-ago
NN
period
NP
PP
IN
because
IN
of VBG
slowing
NN
microchip
NN
demand
NP
PP
VP .
.
S
Figure 1: Penn Treebank tree with adjuncts in italic.
occurrences are sparsely seen (e.g., ?fell?-?from?).
At a lower level, lexicalization is clearly desirable
for pre-terminals. Indeed, current SCFG models
such as K&M have no direct way of preventing
highly improbable single word removals, such as
deletions of adverbs ?never? or ?nowhere?, which
may turn a negative statement into a positive one.4
A second type of annotation that can be added to
syntactic categories is the so-called parent annota-
tion (Johnson, 1998), which was effectively used in
syntactic parsing to break unreasonable context-free
assumptions. For instance, a PP with a VP parent
is marked as PP?VP. It is reasonable to assume that,
e.g., that constituents deep inside a PP have more
chances to be removed than otherwise expected, and
one may seek to increase the amount of vertical
context that is available for conditioning each con-
stituent deletion.
To achieve the above desiderata for better SCFG
probability estimates?i.e., reduce the amount of
sister annotation within each SCFG production, by
conditioning deletions on a context smaller than an
entire right-hand side, and at the same time in-
crease the amount of ancestor and descendent an-
notation through parent (or ancestor) annotation and
lexicalization?we follow the approach of (Collins,
1999; Klein and Manning, 2003), i.e., factor-
ize n-ary grammar productions into products of n
right-hand side probabilities, a technique sometimes
called Markovization.
Markovization is generally head-driven, i.e., re-
flects a decomposition centered around the head of
each CFG production:
l ? ?Lm ? ? ?L1HR1 ? ? ?Rn? (4)
4K&M incorporate lexical probabilities through n-gram
models, but such language models are obviously not good for
preventing such unreasonable deletions.
182
where H is the head, L1, . . . , Lm the left modi-
fiers, R1, . . . , Rn are right modifiers, and ? termi-
nation symbols needed for accurate probability es-
timations (e.g., to capture the fact that certain con-
stituents are more likely than others to be the right-
most constituent); for simplicity, we will ignore ?
in later discussions. For a given SCFG production
l ? ??f , ?c?, we ask, given the source RHS ?f
that is assumed given (e.g., provided by a syntactic
parser), which of its RHS elements are also present
in ?c. That is, we write:
p(?c|?f , l) = (5)
p(kml , ? ? ? , k
1
l , kh, k
1
r , ? ? ? , k
n
r |?f , l)
where kh, kil , k
j
r (?k? for keep) are binary variables
that are true if and only if constituents H,Li, Rj (re-
spectively) of the source RHS ?f are present in the
target side ?c. Note that the conditional probabil-
ity in Equation 5 enables us to estimate Equation 3,
since p(?f , ?c|l) = p(?c|?f , l) ? p(?f |l). We can
rely on a state-of-the-art probabilistic parser to ef-
fectively compute either p(?f |l) or the probability
of the entire tree pif , and need not worry about esti-
mating this term. In the case of sentence compres-
sion from the one-best hypothesis of the parser, we
can ignore p(?f |l) altogether, since pif is the same
for all compressions.
We can rewrite Equation 5 exactly using a head-
driven infinite-horizon Markovization:
p(?c|?f , l) = p(kh|?f , l) (6)
?
?
i=1...m
p(kil |k
1
l , ? ? ? , k
i?1
l , kh, ?f , l)
?
?
i=1...n
p(kir|k
1
r , ? ? ? , k
i?1
r , kh,?, ?f , l)
where ? = (k1l , ? ? ? , k
m
l ) is a term needed by the
chain rule. One key issue is to make linguistically
plausible assumptions to determine which condi-
tioning variables in the terms should be deleted. Fol-
lowing our discussion in the first part of this section,
we may start by making an order-s Markov approx-
imation centered around the head, i.e., we condi-
tion each binary variable (e.g., kir) on a context of
up to s sister constituents between the current con-
stituent and the head (e.g., (Ri?s, . . . , Ri)). In or-
der to incorporate bilexical dependencies between
the head and each modifier, we also condition all
modifier probabilities on head variables H (and kh).
These assumptions are overall quite similar to the
ones made in Markovized parsing models. If we as-
sume that all other conditioning variables in Equa-
tion 6 are irrelevant, we write:
p(?c|?f , l) = ph(kh|H, l) (7)
?
?
i=1...m
pl(k
i
l |L
i?s, ..., Li, ki?sl , ..., k
i?1
l ,H, kh, l)
?
?
i=1...n
pr(k
i
r|R
i?s, ..., Ri, ki?sr , ..., k
i?1
r ,H, kh, l)
Note that it is important to condition deletions on
both constituent histories (Ri?s, . . . , Ri) and non-
deletion histories (ki?sr , . . . , k
i?1
r ); otherwise we
would be unable to perform deletions that must op-
erate jointly, as in production S??ADVP COMMA
NP VP, NP VP? (in which the ADVP should not be
deleted without the comma). Without binary his-
tories, we often observed superfluous punctuation
symbols and dangling coordinate conjunctions ap-
pearing in our outputs.
Finally, we label l with an order-v ancestor anno-
tation, e.g., for the VP in Figure 1, l = ? for v = 0,
l =VP?S for v = 2, and so on. We also replace H
and modifiers Li and Ri by lexicalized entries, e.g.,
H =(VP,VBD,fell) and Ri =(PP,IN,from). Note
that to estimate pl(kil | ? ? ? ), we only lexicalize L
i
andH , and none of the other conditioning modifiers,
since this would, of course, introduce too many con-
ditioning variables (the same goes for pr(kir| ? ? ? )).
The question of how much sister and vertical (s and
v) context is needed for effective sentence compres-
sion, and whether to use lexical or POS annotation,
will be evaluated in detail in Section 5.
3 The Data
To acquire SCFG productions, we used Ziff-Davis,
a corpus of technical articles and human abstractive
summaries. Articles and summaries are paired by
document, so the first step was to perform sentence
alignment. In the particular case of sentence com-
pression, a simple approach is to just consider com-
pression pairs (f,c), where c is a substring of f. K&M
identified only 1,087 such paired sentences in the en-
tire corpus, which represents a recall of 1.75%.
For our empirical evaluations, we split the data as
follows: among the 1,055 sentences that were taken
183
to train systems described in K&M, we selected the
first 32 sentence pairs to be an auxiliary test corpus
(for future work), the next 200 sentences to be our
development corpus, and the remaining 823 to be
our base training corpus (ZD-0), which will be aug-
mented with additional data as explained in the next
section. We feel it is important to use a relatively
large development corpus, since we will provide in
Section 5 detailed analyses of model selection on
the development set (e.g., by evaluating different
Markov structures), and we want these findings to
be as significant as possible. Finally, we used the
same test data as K&M for human evaluation pur-
poses (32 sentence pairs).
4 Tree Alignment and Synchronous Gram-
mar Inference
We now describe methods to train SCFG models
from sentence pairs. Given a tree pair (f , c), whose
respective parses (pif , pic) were generated by the
parser described in (Charniak and Johnson, 2005),
the goal is to transform the tree pair into SCFG
derivations, in order to build relative frequency es-
timates for our Markovized models from observed
SCFG productions. Clearly, the two trees may
sometimes be structurally quite different (e.g., a
given PP may attach to an NP in pif , while attach-
ing to VP in pic), and it is not always possible to
build an SCFG derivation given the constraints in
(pif , pic). The approach taken by K&M is to analyze
both trees and count an SCFG rule whenever two
nodes are ?deemed to correspond?, i.e., roots are the
same, and ?c is a sub-sequence of ?f . This leads
to a quite restricted number of different productions
on our base training set (ZD-0): 823 different pro-
ductions were extracted, 593 of which appear only
once. This first approach has serious limitations;
the assumption that sentence compression appropri-
ately models human abstractive data is particularly
problematic. This considerably limits the amount
of training data that can be exploited in Ziff-Davis
(which contains overall more than 4,000 documents-
abstract pairs), and this makes it very difficult to
train lexicalized models.
An approach to slightly loosen this assumption
is to consider document-abstract sentence pairs in
which the condensed version contains one or more
substitutions or insertions. Consider for example
DT[3]
The[4]
JJ[5]
second[6]
NN[7]
computer[8]
NP[2]
VBD
started RP
up
PRT
VP CC
and VBD[10]
ran[11] IN[13]
without[14] NN[16]
incident[17]
NP[15]
PP[12]
VP[9]
VP .[18]
.[19]
S[1]
DT[3]
The[4]
JJ[5]
second[6]
NN[7]
unit[8]
NP[2]
VBD[10]
ran[11] IN[13]
without[14] NN[16]
incident[17]
NP[15]
PP[12]
VP[9] .[18]
.[19]
S[1]
Figure 2: Full sentence and its revision. While the latter is not a
compression of the former, it could still be used to gather statis-
tics to train a sentence compression system, e.g., to learn the
reduction of a VP coordination.
the tree pair in Figure 2: the two sentences are syn-
tactically very close, but the substitution of ?com-
puter? with ?unit? makes this sentence pair unus-
able in the framework presented in K&M. Arguably,
there should be ways to exploit abstract sentences
that are slightly reworded in addition to being com-
pressed. To use sentence pairs with insertions and
substitutions, we must find a way to align tree pairs
in order to identify SCFG productions. More specif-
ically, we must define a constituent alignment be-
tween the paired abstract and document sentences,
which determine how the two trees are synchronized
in a derivation. Obtaining this alignment is no triv-
ial matter as the number of non-deleting edits in-
creases. To address this, we synchronized tree pairs
by finding the constituent alignment that minimizes
the edit distance between the two trees, i.e., mini-
mize the number of terminals and non-terminals in-
sertions, substitutions and deletions.5 While criteria
5The minimization problem is known to be NP hard, so we
used an approximation algorithm (Zhang and Shasha, 1989) that
184
other than minimum tree edit distance may be effec-
tive, we found?after manual inspections of align-
ments between sentences with less than five non-
deleting edits?that this method generally produces
good alignments. A sample alignment is provided in
Figure 2. Once a constituent alignment is available,
it is then trivial to extract all deletion SCFG rules
available in a tree pair, e.g., NP ? ?DT JJ NN, DT
JJ NN? in the figure.
We also exploited more general tree productions
known as synchronous tree substitution grammar
(STSG) rules, in an approach quite similar to (Turner
and Charniak, 2005). For instance, the STSG rule
rooted at S can be decomposed into two SCFG pro-
ductions if we allow unary rules such as VP?VP to
be freely added to the compressed tree. More specif-
ically, we decompose any STSG rule that has in its
target (compressed) RHS a single context free pro-
duction, and that contains in its source (full) RHS
a single context free production adjoined with any
number of tree adjoining grammar (TAG) auxiliary
trees (Joshi et al, 1975). In the figure, the initial tree
is S ? NP VP, and the adjoined (auxiliary) tree is
VP ? VP CC VP.6 We found this approach quite
helpful, since most useful compressions that mimic
TAG adjoining operations are missed by the extrac-
tion procedure of K&M.
Since we found that exploiting sentence pairs con-
taining insertions had adverse consequences in terms
of compression accuracies, we only report experi-
ments with sentence pairs containing no insertions.
We gathered sentence pairs with up to six substi-
tutions using minimum edit distance matching (we
will refer to these sets as ZD-0 to ZD-6). With a
limit of up to six substitutions (ZD-6), we were able
to train our models on 16,787 sentences, which rep-
resents about 25% of the total number of summary
sentences of the Ziff-Davis corpus.
5 Experiments
All experiments presented in this section are per-
formed on the Ziff-Davis corpus. We note first that
all probability estimates of our Markovized gram-
runs in polynomial time.
6To determine whether a given one-level tree is an auxiliary,
we simply check the following properties: all its leaves but one
(the ?foot node?) must be nodes attached to deleted subtrees
(e.g., VP and CC in the figure), and the foot node (VP[9]) must
have the same syntactic category as the root node.
mars are smoothed. Indeed, incorporating lexical
dependencies within models trained on data sets as
small as 16,000 sentence pairs would be quite fu-
tile without incorporating robust smoothing tech-
niques. Different smoothing techniques were eval-
uated with our models, and we found that interpo-
lated Witten-Bell discounting was the method that
performed best. We used relative frequency es-
timates for each of the models presented in Sec-
tion 2.2 (i.e., ph, pl, pr), and trained pl separately
from pr. We interpolated our most specific models
(lexical heads, POS tags, ancestor and sister annota-
tion) with lower-order models.7
Automatic evaluation on development sets is per-
formed using word-level classification accuracy, i.e.,
the number of words correctly classified as being
either deleted or not deleted, divided by the to-
tal number of words. In our first evaluation, we
experimented with different horizontal and vertical
Markovizations (Table 1). First, it appears that ver-
tical annotation is moderately helpful. It provides
gains in accuracy ranging from .5% to .9% for v = 1
over a simpler models (v = 0), but higher orders
(v > 1) have a tendency to decrease performance.
On the other hand, sister annotation of order 1 is
much more critical, and provides 4.1% improvement
over a simpler model (s = 0, v = 0). Manual exami-
nations of compression outputs confirmed this anal-
ysis: without sister annotation, deletion of punctu-
ation and function words (determiners, coordinate
conjunctions, etc.) is often inaccurate, and compres-
sions clearly lack fluency. This annotation is also
helpful for phrasal deletions; for instance, we found
that PPs are deleted in 31.4% of cases in Ziff-Davis
if they do not immediately follow the head con-
stituent, but this percentage drops to 11.1% for PPs
that immediately follow the head. It seems, how-
ever, that increasing sister annotation beyond s > 1
only provide limited improvements.
In our second evaluation reported in Table 2, we
7We relied on the SRI language modeling (SRILM) toolkit
library for all smoothing experiments. We used the following
order in our deleted interpolation of ph: lexical head, head POS,
ancestor annotation, and head category. For pl and pr , we re-
moved first: lexical head, lexical head of the modifier, head
POS, head POS of the modifier, sister annotation (Li deleted
before kil ), kh, category of the head, category of the modifier.
We experimented with different deletion interpolation order-
ings, and this ordering appears to work quite well in practice,
and was used in all experiments reported in this paper.
185
assessed the usefulness of lexical and POS anno-
tation (setting s and v to 0). In the table, we use
M to denote any of the modifiers Li or Ri, and
c, t, w respectively represent syntactic constituent,
POS, and lexical conditioning. While POS annota-
tion is clearly advantageous compared to using only
syntactic categories, adding lexical variables to the
model also helps. As is shown in the table, it is es-
pecially important to know the lexical head of the
modifier we are attempting to delete. The addition of
wm to conditioning variables provides an improve-
ment of 1.3% (from 66.5% to 67.8%) on our op-
timal Ziff-Davis training corpus (ZD-6). Further-
more, bilexical head-modifier dependencies provide
a relatively small improvement of .5% (from 69.8%
to 70.3%) over the best model that does not incor-
porate the lexical head wh. Note that lexical con-
ditioning also helps in the case where the training
data is relatively small (ZD-0), though differences
are less significant, and bilexical dependencies actu-
ally hurt performance. In subsequent experiments,
we experimented with different Markovizations and
lexical dependency combination, and finally settled
with a model (s = 1 and v = 1) incorporating all
conditioning variables listed in the last line of Ta-
ble 2. This final tuning was combined with human
inspection of generated outputs, since certain modi-
fications that positively impacted output quality sel-
dom changed accuracies.
We finally took the best configuration selected
above, and evaluated our model against the noisy-
channel model of K&M on the 32 test sentences se-
lected by them. We performed both automatic and
human evaluation against the output produced by
Knight and Marcu?s original implementation of their
noisy channel model (Table 3). In the former case,
we also provide Simple String Accuracies (SSA).8
For human evaluation, we hired six native-speaker
judges who scored grammaticality and content (im-
portance) with scores from 1 to 5, using instructions
as described in K&M. Both types of evaluations fa-
vored our Markovized model against the noisy chan-
nel model.
Table 4 shows several outputs of our system
8SSA is defined as: SSA = 1 ? (I + D + S)/R. The
numerator terms are respectively the number of inserts, deletes,
and substitutions, and R is the length of the reference compres-
sion.
Vertical Horizontal Order
Order s = 0 s = 1 s = 2 s = 3
v = 0 63 67.1 67.2 67.2
v = 1 63.9 67.6 67.7 67.7
v = 2 65.7 66.6 66.9 66.9
v = 3 65.2 66.8 67.1 67
Table 1: Markovizations accuracies on Ziff-Davis devel set.
Conditioning Variables ZD-0 ZD-3 ZD-6
M=cm H=ch 62.2 62.4 64.4
M=(cm, tm) H=ch 63.0 63.4 66.5
M=(cm, wm) H=ch 64.2 65.2 66.7
M=(cm, tm, wm) H=ch 63.8 65.8 67.8
M=(cm, tm, wm) H=(ch, th) 66.7 68.6 69.8
M=(cm, tm, wm) H=(ch, wh) 66.9 68.9 70.3
M=(cm, tm, wm) H=(ch, th, wh) 66.3 69.1 69.8
Table 2: Accuracies on Ziff-Davis devel set with different head-
modifier annotations.
Models Acc SSA Grammar Content Len(%)
NoisyC 61.3 14.6 4.37 ? 0.5 3.87 ? 1.2 70.4
Markov 67.9 31.7 4.68 ? 0.4 4.22 ? 0.4 62.7
Human - - 4.95 ? 0.1 4.43 ? 0.3 53.3
Table 3: Accuracies on Ziff-Davis test set.
(Markov) that significantly differed from the output
of the noisy channel model (NoisyC), which con-
firms our finding that Markovized models can pro-
duce quite grammatical output. Our compression for
the first sentence underlines one of the advantages of
constituent-based classifiers, which have the ability
of deleting a very long phrase (here, a PP) at once.
The three next sentences display some advantages
of our approach over the K&M model: here, the lat-
ter model performs deletion with too little lexico-
syntactic information, and accidentally removes cer-
tain modifiers that are sometimes, but not always,
good candidates for deletions (e.g., ADJP in Sen-
tence 2, PP in sentences 3 and 4). On the other hand,
our model keeps these constituent intact. Finally, the
fifth and last example is one of the only three cases
(among the 32 sentences) where our model produced
a sentence we judged clearly ungrammatical. After
inspection, we found that our parser assigned par-
ticularly errorful trees to those inputs, which may
partially explain these ungrammatical outputs.
6 Related Work
A relatively large body of work addressed the prob-
lem of sentence compression. One successful recent
approach (McDonald, 2006) combines a discrimi-
native framework with a set of features that cap-
ture information similar to the K&M model. Mc-
186
Input Many debugging features, including user-defined break points
and variable-watching and message-watching windows, have
been added.
NoisyC Many debugging features, including user-defined points and
variable-watching and message-watching windows, have been
added.
Markov Many debugging features have been added.
Human Many debugging features have been added.
Input The chemical etching process used for glare protection is effec-
tive and will help if your office has the fluorescent-light overkill
that ?s typical in offices.
NoisyC The process used for glare protection is and will help if your
office has the overkill
Markov The chemical etching process used for glare protection is ef-
fective.
Human Glare protection is effective.
Input The utilities will be bundled with Quickdex II in a $90 pack-
age called super quickdex, which is expected to ship in late
summer.
NoisyC The utilities will be bundled
Markov The utilities will be bundled with Quickdex II.
Human The utilities will be bundled with Quickdex II.
Input The discounted package for the SparcServer 470 is priced at
$89,900, down from the regular $107,795.
NoisyC The package for the 470 is priced
Markov The discounted package for the SparcServer 470 is at $89,900.
Human The SparcServer 470 is priced at $89,900, down from the reg-
ular $107,795.
Input Prices range from $5,000 for a microvax 2000 to $179,000 for
the vax 8000 or higher series.
NoisyC Prices range from $5,000 for a 2000 to $179,000 for the vax
8000 or higher series.
Markov Prices range from $5,000 for a microvax for the vax.
Human Prices range from $5,000 to $179,000.
Table 4: Compressions of sample test sentences.
Donald?s features include compression bigrams, as
well as soft syntactic evidence extracted from parse
trees and dependency trees. The strength of McDon-
ald?s approach partially stems from its robustness
against redundant and noisy features, since each fea-
ture is weighted proportionally to its discriminative
power, and his approach is thus hardly penalized
by uninformative features. In contrast, our work
puts much more emphasis on feature analysis than
on efficient optimization, and relies on a statisti-
cal framework (maximum-likelihood estimates) that
strives for careful feature selection and combination.
It also describes and evaluates models incorporating
syntactic evidence that is new to the sentence com-
pression literature, such as head-modifier bilexical
dependencies, and nth-order sister and vertical an-
notation. We think this work leads to a better un-
derstanding of what type of syntactic and lexical ev-
idence makes sentence compression work. Further-
more, our work leaves the door open to uses of our
factored model in a constituent-based or word-based
discriminative framework, in which each elemen-
tary lexico-syntactic structure of this paper can be
discriminatively weighted to directly optimize com-
pression quality. Since McDonald?s approach does
not incorporate SCFG deletion rules, and conditions
deletions on less lexico-syntactic context, we believe
this will lead to levels of performance superior to
both papers.
7 Conclusions
We presented a sentence compression system based
on SCFG deletion rules, for which we defined
a head-driven Markovization formulation. This
Markovization enabled us to incorporate lexical con-
ditioning variables into our models. We empirically
evaluated different Markov structures, and obtained
a best system that generates particularly grammati-
cal sentences according to a human evaluation. Our
sentence compression system is freely available for
research and educational purposes.
Acknowledgments
We would like to thank Owen Rambow, Michael
Collins, Julia Hirschberg, and Daniel Ellis for their
helpful comments and suggestions.
References
A. Aho and J. Ullman. 1969. Syntax directed translations and
the pushdown assembler. 3:37?56.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best pars-
ing and maxent discriminative reranking. In Proc. of ACL.
M. Collins. 1999. Head-driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, U. of Pennsylvania.
B. Dorr, D. Zajic, and R. Schwartz. 2003. Hedge: A parse-and-
trim approach to headline generation. In Proc. of DUC.
H. Jing. 2000. Sentence reduction for automatic text summa-
rization. In Proc. of NAACL, pages 310?315.
M. Johnson. 1998. PCFG models of linguistic tree representa-
tions. Computational Linguistics, 24(4):613?632.
A. Joshi, L. Levy, and M. Takahashi. 1975. Tree adjunct gram-
mar. Journal of Computer and System Science, 21(2).
D. Klein and C. Manning. 2003. Accurate unlexicalized pars-
ing. In Proc. of ACL.
K. Knight and D. Marcu. 2000. Statistics-based summarization
? step one: Sentence compression. In Proc. of AAAI.
P. Lewis and R. Stearns. 1968. Syntax-directed transduction.
In Journal of the Association for Computing Machinery, vol-
ume 15, pages 465?488.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1994. Build-
ing a large annotated corpus of english: The penn treebank.
Computational Linguistics, 19(2):313?330.
R. McDonald. 2006. Discriminative sentence compression
with soft syntactic constraints. In Proc. of EACL.
J. Turner and E. Charniak. 2005. Supervised and unsupervised
learning for sentence compression. In Proc. of ACL.
K. Zhang and D. Shasha. 1989. Simple fast algorithms for the
editing distance between trees and related problems. SIAM
J. Comput., 18(6):1245?1262.
187
Discourse Segmentation of Multi-Party Conversation
Michel Galley Kathleen McKeown
Columbia University
Computer Science Department
1214 Amsterdam Avenue
New York, NY 10027, USA
{galley,kathy}@cs.columbia.edu
Eric Fosler-Lussier
Columbia University
Electrical Engineering Department
500 West 120th Street
New York, NY 10027, USA
fosler@ieee.org
Hongyan Jing
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598, USA
hjing@us.ibm.com
Abstract
We present a domain-independent topic
segmentation algorithm for multi-party
speech. Our feature-based algorithm com-
bines knowledge about content using a
text-based algorithm as a feature and
about form using linguistic and acous-
tic cues about topic shifts extracted from
speech. This segmentation algorithm uses
automatically induced decision rules to
combine the different features. The em-
bedded text-based algorithm builds on lex-
ical cohesion and has performance compa-
rable to state-of-the-art algorithms based
on lexical information. A significant er-
ror reduction is obtained by combining the
two knowledge sources.
1 Introduction
Topic segmentation aims to automatically divide text
documents, audio recordings, or video segments,
into topically related units. While extensive research
has targeted the problem of topic segmentation of
written texts and spoken monologues, few have stud-
ied the problem of segmenting conversations with
many participants (e.g., meetings). In this paper, we
present an algorithm for segmenting meeting tran-
scripts. This study uses recorded meetings of typi-
cally six to eight participants, in which the informal
style includes ungrammatical sentences and overlap-
ping speakers. These meetings generally do not have
pre-set agendas, and the topics discussed in the same
meeting may or may not related.
The meeting segmenter comprises two compo-
nents: one that capitalizes on word distribution to
identify homogeneous units that are topically cohe-
sive, and a second component that analyzes conver-
sational features of meeting transcripts that are in-
dicative of topic shifts, like silences, overlaps, and
speaker changes. We show that integrating features
from both components with a probabilistic classifier
(induced with c4.5rules) is very effective in improv-
ing performance.
In Section 2, we review previous approaches to
the segmentation problem applied to spoken and
written documents. In Section 3, we describe the
corpus of recorded meetings intended to be seg-
mented, and the annotation of its discourse structure.
In Section 4, we present our text-based segmenta-
tion component. This component mainly relies on
lexical cohesion, particularly term repetition, to de-
tect topic boundaries. We evaluated this segmenta-
tion against other lexical cohesion segmentation pro-
grams and show that the performance is state-of-the-
art. In the subsequent section, we describe conver-
sational features, such as silences, speaker change,
and other features like cue phrases. We present a
machine learning approach for integrating these con-
versational features with the text-based segmenta-
tion module. Experimental results show a marked
improvement in meeting segmentation with the in-
corporation of both sets of features. We close with
discussions and conclusions.
2 Related Work
Existing approaches to textual segmentation can be
broadly divided into two categories. On the one
hand, many algorithms exploit the fact that topic
segments tend to be lexically cohesive. Embodi-
ments of this idea include semantic similarity (Mor-
ris and Hirst, 1991; Kozima, 1993), cosine similarity
in word vector space (Hearst, 1994), inter-sentence
similarity matrix (Reynar, 1994; Choi, 2000), en-
tity repetition (Kan et al, 1998), word frequency
models (Reynar, 1999), or adaptive language models
(Beeferman et al, 1999). Other algorithms exploit
a variety of linguistic features that may mark topic
boundaries, such as referential noun phrases (Pas-
sonneau and Litman, 1997).
In work on segmentation of spoken docu-
ments, intonational, prosodic, and acoustic indica-
tors are used to detect topic boundaries (Grosz and
Hirschberg, 1992; Nakatani et al, 1995; Hirschberg
and Nakatani, 1996; Passonneau and Litman, 1997;
Hirschberg and Nakatani, 1998; Beeferman et al,
1999; Tu?r et al, 2001). Such indicators include
long pauses, shifts in speaking rate, great range in
F0 and intensity, and higher maximum accent peak.
These approaches use different learning mecha-
nisms to combine features, including decision trees
(Grosz and Hirschberg, 1992; Passonneau and Lit-
man, 1997; Tu?r et al, 2001) exponential models
(Beeferman et al, 1999) or other probabilistic mod-
els (Hajime et al, 1998; Reynar, 1999).
3 The ICSI Meeting Corpus
We have evaluated our segmenter on the ICSI Meet-
ing corpus (Janin et al, 2003). This corpus is one of
a growing number of corpora with human-to-human
multi-party conversations. In this corpus, record-
ings of meetings ranged primarily over three differ-
ent recurring meeting types, all of which concerned
speech or language research.1 The average duration
is 60 minutes, with an average of 6.5 participants.
They were transcribed, and each conversation turn
was marked with the speaker, start time, end time,
and word content.
From the corpus, we selected 25 meetings to be
segmented, each by at least three subjects. We
opted for a linear representation of discourse, since
finer-grained discourse structures (e.g. (Grosz and
Sidner, 1986)) are generally considered to be diffi-
cult to mark reliably. Subjects were asked to mark
each speaker change (potential boundary) as either
boundary or non-boundary. In the resulting anno-
tation, the agreed segmentation based on majority
1While it would be desirable to have a broader variety of
meetings, we hope that experiments on this corpus will still
carry some generality.
opinion contained 7.5 segments per meeting on av-
erage, while the average number of potential bound-
aries is 770. We used Cochran?s Q (1950) to eval-
uate the agreement among annotators. Cochran?s
test evaluates the null hypothesis that the number
of subjects assigning a boundary at any position is
randomly distributed. The test shows that the inter-
judge reliability is significant to the 0.05 level for 19
of the meetings, which seems to indicate that seg-
ment identification is a feasible task.2
4 Segmentation based on Lexical Cohesion
Previous work on discourse segmentation of written
texts indicates that lexical cohesion is a strong in-
dicator of discourse structure. Lexical cohesion is
a linguistic property that pertains to speech as well,
and is a linguistic phenomenon that can also be ex-
ploited in our case: while our data does not have
the same kind of syntactic and rhetorical structure
as written text, we nonetheless expect that informa-
tion from the written transcription alone should pro-
vide indications about topic boundaries. In this sec-
tion, we describe our work on LCseg, a topic seg-
menter based on lexical cohesion that can handle
both speech and text, but that is especially designed
to generate the lexical cohesion feature used in the
feature-based segmentation described in Section 5.
4.1 Algorithm Description
LCseg computes lexical chains, which are thought
to mirror the discourse structure of the underly-
ing text (Morris and Hirst, 1991). We ignore syn-
onymy and other semantic relations, building a re-
stricted model of lexical chains consisting of sim-
ple term repetitions, hypothesizing that major topic
shifts are likely to occur where strong term repeti-
tions start and end. While other relations between
lexical items also work as cohesive factors (e.g. be-
tween a term and its super-ordinate), the work on
linear topic segmentation reporting the most promis-
ing results account for term repetitions alone (Choi,
2000; Utiyama and Isahara, 2001).
The preprocessing steps of LCseg are common to
many segmentation algorithms. The input document
is first tokenized, non-content words are removed,
2Four other meetings failed short the significance test, while
there was little agreement on the two last ones (p > 0.1).
and remaining words are stemmed using an exten-
sion of Porter?s stemming algorithm (Xu and Croft,
1998) that conflates stems using corpus statistics.
Stemming will allow our algorithm to more accu-
rately relate terms that are semantically close.
The core algorithm of LCseg has two main parts:
a method to identify and weight strong term repeti-
tions using lexical chains, and a method to hypothe-
size topic boundaries given the knowledge of multi-
ple, simultaneous chains of term repetitions.
A term is any stemmed content word within the
text. A lexical chain is constructed to consist of all
repetitions ranging from the first to the last appear-
ance of the term in the text. The chain is divided into
subchains when there is a long hiatus of h consecu-
tive sentences with no occurrence of the term, where
h is determined experimentally. For each hiatus, a
new division is made and thus, we avoid creating
weakly linked chains.
For all chains that have been identified, we use a
weighting scheme that we believe is appropriate to
the task of inducing the topical or sub-topical struc-
ture of text. The weighting scheme depends on two
factors:
Frequency: chains containing more repeated
terms receive a higher score.
Compactness: shorter chains receive a higher
weight than longer ones. If two chains of different
lengths contain the same number of terms, we assign
a higher score to the shortest one. Our assumption
is that the shorter one, being more compact, seems
to be a better indicator of lexical cohesion.3
We apply a variant of a metric commonly used
in information retrieval, TF.IDF (Salton and Buck-
ley, 1988), to score term repetitions. If R1 . . . Rn is
the set of all term repetitions collected in the text,
t1 . . . tn the corresponding terms, L1 . . . Ln their re-
spective lengths,4 and L the length of the text, the
adapted metric is expressed as follows, combining
frequency (freq(ti)) of a term ti and the compact-
ness of its underlying chain:
score(Ri) = freq(ti) ? log( LLi )
3The latter parameter might seem controversial at first, and
one might assume that longer chains should receive a higher
score. However we point out that in a linear model of dis-
course, chains that almost span the entire text are barely indica-
tive of any structure (assuming boundaries are only hypothe-
sized where chains start and end).
4All lengths are expressed in number of sentences.
In the second part of the algorithm, we combine
information from all term repetitions to compute a
lexical cohesion score at each sentence break (or,
in the case of spoken conversations, speaker turn
break). This step of our algorithm is very similar
in spirit to TextTiling (Hearst, 1994). The idea is to
work with two adjacent analysis windows, each of
fixed size k. For each sentence break, we determine
a lexical cohesion function by computing the cosine
similarity at the transition between the two windows.
Instead of using word counts to compute similarity,
we analyze lexical chains that overlap with the two
windows. The similarity between windows (A and
B) is computed with:5
cosine(A,B) =
?
i
wi,A?wi,B??
i
w2i,A
?
i
w2i,B
where
wi,? =
{
score(Ri) if Ri overlaps ? ? {A,B}
0 otherwise
The similarity computed at each sentence break
produces a plot that shows how lexical cohesion
changes over time; an example is shown in Figure 1.
The lexical cohesion function is then smoothed us-
ing a moving average filter, and minima become po-
tential segment boundaries. Then, in a manner quite
similar to (Hearst, 1994), the algorithm determines
for every local minimum mi how sharp of a change
there is in the lexical cohesion function. The algo-
rithm looks on each side of mi for maxima of cohe-
sion, and once it eventually finds one on each side (l
and r), it computes the hypothesized segmentation
probability:
p(mi) = 12 [LCF(l) + LCF(r) ? 2 ? LCF(m)]
where LCF(x) is the value of the lexical cohesion
function at x.
This score is supposed to capture the sharpness of
the change in lexical cohesion, and give probabilities
close to 1 for breaks like sentence 179 in Figure 1.
Finally, the algorithm selects the hypothesized
boundaries with the highest computed probabilities.
If the number of reference boundaries is unknown,
the algorithm has to make a guess. It computes the
5Normalizing anything in these windows has little ef-
fect, since the cosine similarity is scale invariant, that is
cosine(?xa, xb) = cosine(xa, xb) for ? > 0.
20 40 60 80 100 120 140 160 180 200 220 240 260
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 1: Application of the LCseg algorithm on the concatenation of 16 WSJ stories. Numbers on the
x-axis represent sentence indices, and y-axis represents the lexical cohesion function. The representative
example presented here is segmented by LCseg with an error of Pk = 15.79, while the average performance
of the algorithm is Pk = 15.31 on the WSJ test corpus (unknown number of segments).
mean and the variance of the hypothesized probabil-
ities of all potential boundaries (local minima). As
we can see in Figure 1, there are many local minima
that do not correspond to actual boundaries. Thus,
we ignore all potential boundaries with a probability
lower than plimit. For the remaining points, we com-
pute the threshold using the average (?) and standard
deviation (?) of the p(mi) values, and each potential
boundary mi above the threshold ??? ?? is hypoth-
esized as a real boundary.
4.2 Evaluation
We evaluate LCseg against two state-of-the-art seg-
mentation algorithms based on lexical cohesion
(Choi, 2000; Utiyama and Isahara, 2001). We use
the error metric Pk proposed by Beeferman et al
(1999) to evaluate segmentation accuracy. It com-
putes the probability that sentences k units (e.g. sen-
tences) apart are incorrectly determined as being ei-
ther in different segments or in the same one. Since
it has been argued in (Pevzner and Hearst, 2002) that
Pk has some weaknesses, we also include results ac-
cording to the WindowDiff (WD) metric (which is
described in the same work).
A test corpus of concatenated6 texts extracted
from the Brown corpus was built by Choi (2000)
to evaluate several domain-independent segmenta-
tion algorithms. We reuse the same test corpus for
our evaluation, in addition to two other test corpora
we constructed to test how segmenters scale across
genres and how they perform with texts with various
6Concatenated documents correspond to reference seg-
ments.
number of segments.7 We designed two test corpora,
each of 500 documents, using concatenated texts
extracted from the TDT and WSJ corpora, ranging
from 4 to 22 in number of segments.
LCseg depends on several parameters. Parameter
tuning was performed on three tuning corpora of one
thousand texts each.8 We performed searches for the
optimal settings of the four tunable parameters in-
troduced above; the best performance was achieved
with h = 11 (hiatus length for dividing a chain into
parts), k = 2 (analysis window size), plimit = 0.1
and ? = 12 (thresholding limits for the hypothesized
boundaries).
As shown in Table 1, our algorithm is signifi-
cantly better than (Choi, 2000) (labeled C99) on
all three test corpora, according to a one-sided t-
test of the null hypothesis of equal mean at the 0.01
level. It is not clear whether our algorithm is better
than (Utiyama and Isahara, 2001) (U00). When the
number of segments is provided to the algorithms,
our algorithm is significantly better than Utiyama?s
on WSJ, better on Brown (but not significant), and
significantly worse on TDT. When the number of
boundaries is unknown, our algorithm is insignifi-
cantly worse on Brown, but significantly better on
WSJ and TDT ? the two corpora designed to have
a varying number of segments per document. In the
case of the Meeting corpus, none of the algorithms
are significantly different than the others, due to the
7All texts in Choi?s test corpus have exactly 10 segments.
We are concerned that the adjustments of any algorithm param-
eters might overfit this predefined number of segments.
8These texts are different from the ones used for evaluation.
Brown corpus
known unknown
Pk WD Pk WD
C99 11.19% 13.86% 12.07% 14.57%
U00 8.77% 9.44% 9.76% 10.32%
LCseg 8.69% 9.42% 10.49% 11.37%
p-val. 0.42 0.48 0.027 0.0037
TDT corpus
C99 9.37% 11.91% 10.18% 12.72%
U00 4.70% 6.29% 8.70% 11.12%
LCseg 6.15% 8.41% 6.95% 9.09%
p-val. 1.1e-05 2.8e-07 4.5e-05 2.8e-05
WSJ corpus
C99 19.61% 26.42% 22.32% 29.81%
U00 15.18% 21.54% 17.71% 24.06%
LCseg 12.21% 18.25% 15.31% 22.14%
p-val. 1.4e-08 1.7e-08 2.6e-04 0.0063
Meeting corpus
C99 33.79% 37.25% 47.42% 58.08%
U00 31.99% 34.49% 37.39% 40.43%
LCseg 26.37% 29.40% 31.91% 35.88%
p-val. 0.026 0.14 0.14 0.23
Table 1: Comparison C99 and U00. The p-values in
the table are the results of significance tests between
U00 and LCseg. Bold-faced values are scores that
are statistically significant.
small test set size.
In conclusion, LCseg has a performance compara-
ble to state-of-the-art text segmentation algorithms,
with the added advantage of computing a segmen-
tation probability at each potential boundary. This
information can be effectively used in the feature-
based segmenter to account for lexical cohesion, as
described in the next section.
5 Feature-based Segmentation
In the previous section, we have concentrated exclu-
sively on the consideration of content (through lexi-
cal cohesion) to determine the structure of texts, ne-
glecting any influence of form. In this section, we
explore formal devices that are indicative of topic
shifts, and explain how we use these cues to build a
segmenter targeting conversational speech.
5.1 Probabilistic Classifiers
Topic segmentation is reduced here to a classifica-
tion problem, where each utterance break Bi is ei-
ther considered a topic boundary or not. We use
statistical modeling techniques to build a classifier
that uses local features (e.g. cue phrases, pauses)
to determine if an utterance break corresponds to
a topic boundary. We chose C4.5 and C4.5rules
(Quinlan, 1993), two programs to induce classifi-
cation rules in the form of decision trees and pro-
duction rules (respectively). C4.5 generates an un-
pruned decision tree, which is then analyzed by
C4.5rules to generate a set of pruned production
rules (it tries to find the most useful subset of them).
The advantage of pruned rules over decision trees is
that they are easier to analyze, and allow combina-
tion of features in the same rule (feature interactions
are explicit).
The greedy nature of decision rule learning algo-
rithms implies that a large set of features can lead
to bad performance and generalization capability. It
is desirable to remove redundant and irrelevant fea-
tures, especially in our case since we have little data
labeled with topic shifts; with a large set of fea-
tures, we would risk overfitting the data. We tried
to restrict ourselves to features whose inclusion is
motivated by previous work (pauses, speech rate)
and added features that are specific to multi-speaker
speech (overlap, changes in speaker activity).
5.2 Features
Cue phrases: previous work on segmentation has
found that discourse particles like now, well pro-
vide valuable information about the structure of texts
(Grosz and Sidner, 1986; Hirschberg and Litman,
1994; Passonneau and Litman, 1997). We analyzed
the correlation between words in the meeting cor-
pus and labeled topic boundaries, and automatically
extracted utterance-initial cue phrases9 that are sta-
tistically correlated with boundaries. For every word
in the meeting corpus, we counted the number of its
occurrences near any topic boundary, and its num-
ber of appearances overall. Then, we performed ?2
significance tests (e.g. figure 2 for okay) under the
null hypothesis that no correlation exists. We se-
lected terms whose ?2 value rejected the hypothesis
under a 0.01-level confidence (the rejection criterion
is ?2 ? 6.635). Finally, induced cue phrases whose
usage has never been described in other work were
removed (marked with ? in Table 3). Indeed, there
is a risk that the automatically derived list of cue
phrases could be too specific to the word usage in
9As in (Litman and Passonneau, 1995), we restrict ourselves
to the first lexical item of any utterance, plus the second one if
the first item is also a cue word.
Near boundary Distant
okay 64 740
Other 657 25896
Table 2: okay (?2 = 89.11, df = 1, p < 0.01).
okay 93.05 but 13.57
shall ? 27.34 so 11.65
anyway 23.95 and 10.99
we?re ? 17.67 should ? 10.21
alright 16.09 good ? 7.70
let?s ? 14.54
Table 3: Automatically selected cue phrases.
these meetings.
Silences: previous work has found that ma-
jor shifts in topic typically show longer silences
(Passonneau and Litman, 1993; Hirschberg and
Nakatani, 1996). We investigated the presence of
silences in meetings and their correlation with topic
boundaries, and found it necessary to make a distinc-
tion between pauses and gaps (Levinson, 1983). A
pause is a silence that is attributable to a given party,
for example in the middle of an adjacency pair, or
when a speaker pauses in the middle of her speech.
Gaps are silences not attributable to any party, and
last until a speaker takes the initiative of continuing
the discussion. As an approximation of this distinc-
tion, we classified a silence that follows a question or
in the middle of somebody?s speech as a pause, and
any other silences as a gap. While the correlation be-
tween long silences and discourse boundaries seem
to be less pervasive in meetings than in other speech
corpora, we have noticed that some topic boundaries
are preceded (within some window) by numerous
gaps. However, we found little correlation between
pauses and topic boundaries.
Overlaps: we also analyzed the distribution of
overlapping speech by counting the average overlap
rate within some window. We noticed that, many
times, the beginning of segments are characterized
by having little overlapping speech.
Speaker change: we sometimes noticed a corre-
lation between topic boundaries and sudden changes
in speaker activity. For example, in Figure 2, it
is clear that the contribution of individual speakers
to the discussion can greatly change from one dis-
course unit to the next. We try to capture significant
changes in speakership by measuring the dissimilar-
ity between two analysis windows. For each poten-
tial boundary, we count for each speaker i the num-
ber of words that are uttered before (Li) and after
(Ri) the potential boundary (we limit our analysis
to a window of fixed size). The two distributions
are normalized to form two probability distributions
l and r, and significant changes of speakership are
detected by computing their Jensen-Shannon diver-
gence:
JS(l, r) = 12 [D(l||avgl,r) + D(r||avgl,r)]
where D(l||r) is the KL-divergence between the
two distributions.
Lexical cohesion: we also incorporated the lexi-
cal cohesion function computed by LCseg as a fea-
ture of the multi-source segmenter in a manner simi-
lar to the knowledge source combination performed
by (Beeferman et al, 1999) and (Tu?r et al, 2001).
Note that we use both the posterior estimate com-
puted by LCseg and the raw lexical cohesion func-
tion as features of the system.
5.3 Features: Selection and Combination
For every potential boundary Bi, the classifier ana-
lyzes features in a window surrounding Bi to decide
whether it is a topic boundary or not. It is generally
unclear what is the optimal window size and how
features should be analyzed. Windows of various
sizes can lead to different levels of prediction, and
in some cases, it might be more appropriate to only
extract features preceding or following Bi.
We avoided making arbitrary choices of parame-
ters; instead, for any feature F and a set F1, . . . , Fn
of possible ways to measure the feature (different
window sizes, different directions), we picked the Fi
that is in isolation the best predictor of topic bound-
aries (among F1, . . . , Fn). Table 4 presents for each
feature the analysis mode that is the most useful on
the training data.
5.4 Evaluation
We performed 25-fold cross-validation for evaluat-
ing the induced probabilistic classifier, computing
the average of Pk and WD on the held-out meet-
ings. Feature selection and decision rule learning
0 10 20 30
Figure 2: speaker activity in a meeting. Each row represent the speech activity of one speaker, utterance of
words being represented as black. Vertical lines represent topic shifts. The x-axis represents time.
Feature Tag Size (sec.) Side
Cue phrases CUE 5 both
Silence (gaps) SIL 30 left
Overlap? OVR 30 right
Speaker activity ACT 5 both
Lexical cohesion LC 30 both
?: the size of the window that was used to compute the
JS-divergence was also determined automatically.
Table 4: Parameters for feature analysis.
is always performed on sets of 24 meetings, while
the held-out data is used for testing. Table 5 gives
some examples of the type of rules that are learned.
The first rule states that if the value for the lexical
cohesion (LC) function is low at the current sen-
tence break, there is at least one CUE phrase, there
is less than three seconds of silence to the left of the
break,10 and a single speaker holds the floor for a
longer period of time than usual to the right of the
break, then we have a topic break. In general, we
found that the derived rules show that lexical cohe-
sion plays a stronger role than most other features
in determining topic breaks. Nonetheless, the quan-
titative results summarized in table 6, which corre-
spond to the average performance on the held-out
sets, show that the integration of conversational fea-
tures with the text-based segmenter outperforms ei-
ther alone.
6 Conclusions
We presented a domain-independent segmentation
algorithm for multi-party conversation that inte-
grates features based on content with features based
on form. The learned combination of features results
in a significant increase in accuracy over previous
10Note that rules are not always meaningful in isolation and
it is likely that a subordinate rule in the tree to this one would do
further tests on silence to determine if a topic boundary exists.
Condition Decision Conf.
LC ? 0.67,CUE ? 1,
OVR ? 1.20,SIL ? 3.42 yes 94.1
LC ? 0.35,SIL > 3.42,
OVR ? 4.55 yes 92.2
CUE ? 1,ACT > 0.1768,
OVR ? 1.20,LC ? 0.67 yes 91.6
. . .
default no
Table 5: A selection of the most useful rules learned
by C4.5rules along with their confidence levels.
Times for OVR and SIL are expressed in seconds.
Pk WD
feature-based 23.00% 25.47%
LCseg 31.91% 35.88%
U00 37.39% 40.43%
p-value 2.14e-04 3.30e-04
Table 6: Performance of the feature-based seg-
menter on the test data.
approaches to segmentation when applied to meet-
ings. Features based on form that are likely to in-
dicate topic shifts are automatically extracted from
speech. Content based features are computed by a
segmentation algorithm that utilizes a metric of lex-
ical cohesion and that performs as well as state-of-
the-art text-based segmentation techniques. It works
both with written and spoken texts. The text-based
segmentation approach alone, when applied to meet-
ings, outperforms all other segmenters, although the
difference is not statistically significant.
In future work, we would like to investigate the
effects of adding prosodic features, such as pitch
ranges, to our segmenter, as well as the effect of
using errorful speech recognition transcripts as op-
posed to manually transcribed utterances.
An implementation of our lexical cohesion seg-
menter is freely available for educational or research
purposes.11
Acknowledgments
We are grateful to Julia Hirschberg, Dan Ellis, Eliz-
abeth Shriberg, and Mari Ostendorf for their helpful
advice. We thank our ICSI project partners for grant-
ing us access to the meeting corpus and for useful
discussions. This work was funded under the NSF
project Mapping Meetings (IIS-012196).
References
D. Beeferman, A. Berger, and J. Lafferty. 1999. Statisti-
cal models for text segmentation. Machine Learning,
34(1?3):177?210.
F. Choi. 2000. Advances in domain independent linear
text segmentation. In Proc. of NAACL?00.
W. Cochran. 1950. The comparison of percentages in
matched samples. Biometrika, 37:256?266.
B. Grosz and J. Hirschberg. 1992. Some intonational
characteristics of discourse structure. In Proc. of
ICSLP-92, pages 429?432.
B. Grosz and C. Sidner. 1986. Attention, intentions and
the structure of discourse. Computational Linguistics,
12(3).
M. Hajime, H. Takeo, and O. Manabu. 1998. Text seg-
mentation with multiple surface linguistic cues. In
COLING-ACL, pages 881?885.
M. Hearst. 1994. Multi-paragraph segmentation of ex-
pository text. In Proc. of the ACL.
J. Hirschberg and D. Litman. 1994. Empirical studies
on the disambiguation of cue phrases. Computational
Linguistics, 19(3):501?530.
J. Hirschberg and C. Nakatani. 1996. A prosodic anal-
ysis of discourse segments in direction-giving mono-
logues. In Proc. of the ACL.
J. Hirschberg and C. Nakatani. 1998. Acoustic indicators
of topic segmentation. In Proc. of ICSLP.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The ICSI meeting corpus.
In Proc. of ICASSP-03, Hong Kong (to appear).
11http://www.cs.columbia.edu/?galley/research.html
M.-Y. Kan, J. Klavans, and K. McKeown. 1998. Linear
segmentation and segment significance. In Proc. 6th
Workshop on Very Large Corpora (WVLC-98).
H. Kozima. 1993. Text segmentation based on similarity
between words. In Proc. of the ACL.
S. Levinson. 1983. Pragmatics. Cambridge University
Press.
D. Litman and R. Passonneau. 1995. Combining multi-
ple knowledge sources for discourse segmentation. In
Proc. of the ACL.
J. Morris and G. Hirst. 1991. Lexcial cohesion computed
by thesaural relations as an indicator of the structure of
text. Computational Linguistics, 17:21?48.
C. Nakatani, J. Hirschberg, and B. Grosz. 1995. Dis-
course structure in spoken language: Studies on
speech corpora. In AAAI-95 Symposium on Empirical
Methods in Discourse Interpretation.
R. Passonneau and D. Litman. 1993. Intention-based
segmentation: Human reliability and correlation with
linguistic cues. In Proc. of the ACL.
R. Passonneau and D. Litman. 1997. Discourse seg-
mentation by human and automated means. Compu-
tational Linguistics, 23(1):103?139.
L. Pevzner and M. Hearst. 2002. A critique and im-
provement of an evaluation metric for text segmenta-
tion. Computational Linguistics, 28 (1):19?36.
R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Machine Learning. Morgan Kaufmann.
J. Reynar. 1994. An automatic method of finding topic
boundaries. In Proc. of the ACL.
J. Reynar. 1999. Statistical models for topic segmenta-
tion. In Proc. of the ACL.
G. Salton and C. Buckley. 1988. Term weighting ap-
proaches in automatic text retrieval. Information Pro-
cessing and Management, 24(5):513?523.
G. Tu?r, D. Hakkani-Tu?r, A. Stolcke, and E. Shriberg.
2001. Integrating prosodic and lexical cues for auto-
matic topic segmentation. Computational Linguistics,
27(1):31?57.
M. Utiyama and H. Isahara. 2001. A statistical model
for domain-independent text segmentation. In Proc. of
the ACL.
J. Xu and B. Croft. 1998. Corpus-based stemming using
cooccurrence of word variants. ACM Transactions on
Information Systems, 16(1):61?81.
Identifying Agreement and Disagreement in Conversational Speech:
Use of Bayesian Networks to Model Pragmatic Dependencies
Michel Galley   , Kathleen McKeown   , Julia Hirschberg   ,
  Columbia University
Computer Science Department
1214 Amsterdam Avenue
New York, NY 10027, USA

galley,kathy,julia  @cs.columbia.edu
and Elizabeth Shriberg 
 SRI International
Speech Technology and Research Laboratory
333 Ravenswood Avenue
Menlo Park, CA 94025, USA
ees@speech.sri.com
Abstract
We describe a statistical approach for modeling
agreements and disagreements in conversational in-
teraction. Our approach first identifies adjacency
pairs using maximum entropy ranking based on a
set of lexical, durational, and structural features that
look both forward and backward in the discourse.
We then classify utterances as agreement or dis-
agreement using these adjacency pairs and features
that represent various pragmatic influences of pre-
vious agreement or disagreement on the current ut-
terance. Our approach achieves 86.9% accuracy, a
4.9% increase over previous work.
1 Introduction
One of the main features of meetings is the occur-
rence of agreement and disagreement among par-
ticipants. Often meetings include long stretches
of controversial discussion before some consensus
decision is reached. Our ultimate goal is auto-
mated summarization of multi-participant meetings
and we hypothesize that the ability to automatically
identify agreement and disagreement between par-
ticipants will help us in the summarization task.
For example, a summary might resemble minutes of
meetings with major decisions reached (consensus)
along with highlighted points of the pros and cons
for each decision. In this paper, we present a method
to automatically classify utterances as agreement,
disagreement, or neither.
Previous work in automatic identification of
agreement/disagreement (Hillard et al, 2003)
demonstrates that this is a feasible task when var-
ious textual, durational, and acoustic features are
available. We build on their approach and show
that we can get an improvement in accuracy when
contextual information is taken into account. Our
approach first identifies adjacency pairs using maxi-
mum entropy ranking based on a set of lexical, dura-
tional and structural features that look both forward
and backward in the discourse. This allows us to ac-
quire, and subsequently process, knowledge about
who speaks to whom. We hypothesize that prag-
matic features that center around previous agree-
ment between speakers in the dialog will influence
the determination of agreement/disagreement. For
example, if a speaker disagrees with another per-
son once in the conversation, is he more likely to
disagree with him again? We model context using
Bayesian networks that allows capturing of these
pragmatic dependencies. Our accuracy for classify-
ing agreements and disagreements is 86.9%, which
is a 4.9% improvement over (Hillard et al, 2003).
In the following sections, we begin by describ-
ing the annotated corpus that we used for our ex-
periments. We then turn to our work on identify-
ing adjacency pairs. In the section on identification
of agreement/disagreement, we describe the contex-
tual features that we model and the implementation
of the classifier. We close with a discussion of future
work.
2 Corpus
The ICSI Meeting corpus (Janin et al, 2003) is
a collection of 75 meetings collected at the In-
ternational Computer Science Institute (ICSI), one
among the growing number of corpora of human-
to-human multi-party conversations. These are nat-
urally occurring, regular weekly meetings of vari-
ous ICSI research teams. Meetings in general run
just under an hour each; they have an average of 6.5
participants.
These meetings have been labeled with adja-
cency pairs (AP), which provide information about
speaker interaction. They reflect the structure of
conversations as paired utterances such as question-
answer and offer-acceptance, and their labeling is
used in our work to determine who are the ad-
dressees in agreements and disagreements. The an-
notation of the corpus with adjacency pairs is de-
scribed in (Shriberg et al, 2004; Dhillon et al,
2004).
Seven of those meetings were segmented into
spurts, defined as periods of speech that have no
pauses greater than .5 second, and each spurt was
labeled with one of the four categories: agreement,
disagreement, backchannel, and other.1 We used
spurt segmentation as our unit of analysis instead of
sentence segmentation, because our ultimate goal is
to build a system that can be fully automated, and
in that respect, spurt segmentation is easy to ob-
tain. Backchannels (e.g. ?uhhuh? and ?okay?) were
treated as a separate category, since they are gener-
ally used by listeners to indicate they are following
along, while not necessarily indicating agreement.
The proportion of classes is the following: 11.9%
are agreements, 6.8% are disagreements, 23.2% are
backchannels, and 58.1% are others. Inter-labeler
reliability estimated on 500 spurts with 2 labelers
was considered quite acceptable, since the kappa
coefficient was .63 (Cohen, 1960).
3 Adjacency Pairs
3.1 Overview
Adjacency pairs (AP) are considered fundamental
units of conversational organization (Schegloff and
Sacks, 1973). Their identification is central to our
problem, since we need to know the identity of
addressees in agreements and disagreements, and
adjacency pairs provide a means of acquiring this
knowledge. An adjacency pair is said to consist of
two parts (later referred to as A and B) that are or-
dered, adjacent, and produced by different speakers.
The first part makes the second one immediately rel-
evant, as a question does with an answer, or an offer
does with an acceptance. Extensive work in con-
versational analysis uses a less restrictive definition
of adjacency pair that does not impose any actual
adjacency requirement; this requirement is prob-
lematic in many respects (Levinson, 1983). Even
when APs are not directly adjacent, the same con-
straints between pairs and mechanisms for select-
ing the next speaker remain in place (e.g. the case
of embedded question and answer pairs). This re-
laxation on a strict adjacency requirement is partic-
ularly important in interactions of multiple speak-
ers since other speakers have more opportunities to
insert utterances between the two elements of the
AP construction (e.g. interrupted, abandoned or ig-
nored utterances; backchannels; APs with multiple
second elements, e.g. a question followed by an-
swers of multiple speakers).2
Information provided by adjacency pairs can be
used to identify the target of an agreeing or dis-
agreeing utterance. We define the problem of AP
1Part of these annotated meetings were provided by the au-
thors of (Hillard et al, 2003).
2The percentage of APs labeled in our data that have non-
contiguous parts is about 21%.
identification as follows: given the second element
(B) of an adjacency pair, determine who is the
speaker of the first element (A). A quite effective
baseline algorithm is to select as speaker of utter-
ance A the most recent speaker before the occur-
rence of utterance B. This strategy selects the right
speaker in 79.8% of the cases in the 50 meetings that
were annotated with adjacency pairs. The next sub-
section describes the machine learning framework
used to significantly outperform this already quite
effective baseline algorithm.
3.2 Maximum Entropy Ranking
We view the problem as an instance of statisti-
cal ranking, a general machine learning paradigm
used for example in statistical parsing (Collins,
2000) and question answering (Ravichandran et al,
2003).3 The problem is to select, given a set of  
possible candidates 			
 (in our case, po-
tential A speakers), the one candidate  that maxi-
mizes a given conditional probability distribution.
We use maximum entropy modeling (Berger et
al., 1996) to directly model the conditional proba-
bility  Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 961?968,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Scalable Inference and Training of
Context-Rich Syntactic Translation Models
Michel Galley*, Jonathan Graehl?, Kevin Knight??, Daniel Marcu??,
Steve DeNeefe?, Wei Wang? and Ignacio Thayer?
*Columbia University
Dept. of Computer Science
New York, NY 10027
galley@cs.columbia.edu, {graehl,knight,marcu,sdeneefe}@isi.edu,
wwang@languageweaver.com, thayer@google.com
?University of Southern California
Information Sciences Institute
Marina del Rey, CA 90292
?Language Weaver, Inc.
4640 Admiralty Way
Marina del Rey, CA 90292
Abstract
Statistical MT has made great progress in the last
few years, but current translation models are weak
on re-ordering and target language fluency. Syn-
tactic approaches seek to remedy these problems.
In this paper, we take the framework for acquir-
ing multi-level syntactic translation rules of (Gal-
ley et al, 2004) from aligned tree-string pairs, and
present two main extensions of their approach: first,
instead of merely computing a single derivation that
minimally explains a sentence pair, we construct
a large number of derivations that include contex-
tually richer rules, and account for multiple inter-
pretations of unaligned words. Second, we pro-
pose probability estimates and a training procedure
for weighting these rules. We contrast different
approaches on real examples, show that our esti-
mates based on multiple derivations favor phrasal
re-orderings that are linguistically better motivated,
and establish that our larger rules provide a 3.63
BLEU point increase over minimal rules.
1 Introduction
While syntactic approaches seek to remedy word-
ordering problems common to statistical machine
translation (SMT) systems, many of the earlier
models?particularly child re-ordering models?
fail to account for human translation behavior.
Galley et al (2004) alleviate this modeling prob-
lem and present a method for acquiring millions
of syntactic transfer rules from bilingual corpora,
which we review below. Here, we make the fol-
lowing new contributions: (1) we show how to
acquire larger rules that crucially condition on
more syntactic context, and show how to com-
pute multiple derivations for each training exam-
ple, capturing both large and small rules, as well
as multiple interpretations for unaligned words;
(2) we develop probability models for these multi-
level transfer rules, and give estimation methods
for assigning probabilities to very large rule sets.
We contrast our work with (Galley et al, 2004),
highlight some severe limitations of probability
estimates computed from single derivations, and
demonstrate that it is critical to account for many
derivations for each sentence pair. We also use
real examples to show that our probability mod-
els estimated from a large number of derivations
favor phrasal re-orderings that are linguistically
well motivated. An empirical evaluation against
a state-of-the-art SMT system similar to (Och and
Ney, 2004) indicates positive prospects. Finally,
we show that our contextually richer rules provide
a 3.63 BLEU point increase over those of (Galley
et al, 2004).
2 Inferring syntactic transformations
We assume we are given a source-language (e.g.,
French) sentence f , a target-language (e.g., En-
glish) parse tree pi, whose yield e is a translation
of f , and a word alignment a between f and e.
Our aim is to gain insight into the process of trans-
forming pi into f and to discover grammatically-
grounded translation rules. For this, we need
a formalism that is expressive enough to deal
with cases of syntactic divergence between source
and target languages (Fox, 2002): for any given
(pi, f ,a) triple, it is useful to produce a derivation
that minimally explains the transformation be-
tween pi and f , while remaining consistent with a.
Galley et al (2004) present one such formalism
(henceforth ?GHKM?).
2.1 Tree-to-string alignments
It is appealing to model the transformation of pi
into f using tree-to-string (xRs) transducers, since
their theory has been worked out in an exten-
sive literature and is well understood (see, e.g.,
(Graehl and Knight, 2004)). Formally, transfor-
mational rules ri presented in (Galley et al, 2004)
are equivalent to 1-state xRs transducers mapping
a given pattern (subtree to match in pi) to a right
hand side string. We will refer to them as lhs(ri)
and rhs(ri), respectively. For example, some xRs
961
rules may describe the transformation of does not
into ne ... pas in French. A particular instance may
look like this:
VP(AUX(does), RB(not), x0:VB) ? ne, x0, pas
lhs(ri) can be any arbitrary syntax tree fragment.
Its leaves are either lexicalized (e.g. does) or vari-
ables (x0, x1, etc). rhs(ri) is represented as a se-
quence of target-language words and variables.
Now we give a brief overview of how such
transformational rules are acquired automatically
in GHKM.1 In Figure 1, the (pi, f ,a) triple is rep-
resented as a directed graph G (edges going down-
ward), with no distinction between edges of pi and
alignments. Each node of the graph is labeled with
its span and complement span (the latter in italic
in the figure). The span of a node n is defined by
the indices of the first and last word in f that are
reachable from n. The complement span of n is
the union of the spans of all nodes n? in G that
are neither descendants nor ancestors of n. Nodes
of G whose spans and complement spans are non-
overlapping form the frontier set F ? G.
What is particularly interesting about the fron-
tier set? For any frontier of graph G containing
a given node n ? F , spans on that frontier de-
fine an ordering between n and each other frontier
node n?. For example, the span of VP[4-5] either
precedes or follows, but never overlaps the span of
any node n? on any graph frontier. This property
does not hold for nodes outside of F . For instance,
PP[4-5] and VBG[4] are two nodes of the same
graph frontier, but they cannot be ordered because
of their overlapping spans.
The purpose of xRs rules in this framework is
to order constituents along sensible frontiers in G,
and all frontiers containing undefined orderings,
as between PP[4-5] and VBG[4], must be disre-
garded during rule extraction. To ensure that xRs
rules are prevented from attempting to re-order
any such pair of constituents, these rules are de-
signed in such a way that variables in their lhs can
only match nodes of the frontier set. Rules that
satisfy this property are said to be induced by G.2
For example, rule (d) in Table 1 is valid accord-
ing to GHKM, since the spans corresponding to
1Note that we use a slightly different terminology.
2Specifically, an xRs rule ri is extracted fromG by taking
a subtree ? ? pi as lhs(ri), appending a variable to each
leaf node of ? that is internal to pi, adding those variables to
rhs(ri), ordering them in accordance to a, and if necessary
inserting any word of f to ensure that rhs(ri) is a sequence of
contiguous spans (e.g., [4-5][6][7-8] for rule (f) in Table 1).
DT
CD
VBP
NNS
IN
NNP
NP
NNS
VBG
3
2
2
1
7-8
4
4
5
9
1
2
3
4
5
6
7
8
9
3 1-2,4
-9
2 1-9
2 1-9
1 2-9
7-8 1-5,9
4 1-9
4 1-9
5 1-4,7
-9
9 1-8
1-2 3-9
NP 7-8 1-5,9
NP 5 1-4, 7
-9
PP 4-5 1-4,7
-9
VP 4-5 1-3,7
-9
NP 4-8 1-3,9
VP 3-8 1-2,9
S 1-9 ?
7!
"#
$
%&
'(
)
*+
,
.
Thes
e
peop
le
inclu
de
astro
naut
s
com
ing
from
Fran
ce
..
7
-
Figure 1: Spans and complement-spans determine what
rules are extracted. Constituents in gray are members of the
frontier set; a minimal rule is extracted from each of them.
(a) S(x0:NP, x1:VP, x2:.) ? x0, x1, x2
(b) NP(x0:DT, CD(7), NNS(people)) ? x0, 7?
(c) DT(these) ??
(d) VP(x0:VBP, x1:NP) ? x0, x1
(e) VBP(include) ?-?
(f) NP(x0:NP, x1:VP) ? x1,?, x0
(g) NP(x0:NNS) ? x0
(h) NNS(astronauts) ??*,X
(i) VP(VBG(coming), PP(IN(from), x0:NP)) ?e?, x0
(j) NP(x0:NNP) ? x0
(k) NNP(France) ???
(l) .(.) ? .
Table 1: A minimal derivation corresponding to Figure 1.
its rhs constituents (VBP[3] and NP[4-8]) do not
overlap. Conversely, NP(x0:DT, x1:CD:, x2:NNS)
is not the lhs of any rule extractible from G, since
its frontier constituents CD[2] and NNS[2] have
overlapping spans.3 Finally, the GHKM proce-
dure produces a single derivation from G, which
is shown in Table 1.
The concern in GHKM was to extract minimal
rules, whereas ours is to extract rules of any arbi-
trary size. Minimal rules defined over G are those
that cannot be decomposed into simpler rules in-
duced by the same graph G, e.g., all rules in Ta-
ble 1. We call minimal a derivation that only con-
tains minimal rules. Conversely, a composed rule
results from the composition of two or more min-
imal rules, e.g., rule (b) and (c) compose into:
NP(DT(these), CD(7), NNS(people)) ??, 7?
3It is generally reasonable to also require that the root n
of lhs(ri) be part of F , because no rule induced by G can
compose with ri at n, due to the restrictions imposed on the
extraction procedure, and ri wouldn?t be part of any valid
derivation.
962
OR
NP
(x0
:NP
, x1
:V
P) 
!
x1
,!
, x0
VP
(x0
:VB
P, 
x1:
NP
) 
!
x0
 , x
1
S(x
0:N
P, 
x1:
VP
, x
2:.
) 
!
x0
 , x
1, x
2
NP
(x0
:DT
 CD
(7)
, N
NS
(pe
opl
e))
 
!
x0
, 7"
.(.)
 
!
.
DT
(th
ese
) 
!
#
VB
P(i
ncl
ude
) 
!
$%
&
NP
(x0
:NP
, x1
:V
P) 
!
x1
, x0
NP
(x0
:NP
, x1
:V
P) 
!
x1
, x0
VP
(V
BG
(co
mi
ng)
, 
PP
(IN
(fro
m),
 x0
:N
P))
  
!
'(
, x0
, !
VP
(V
BG
(co
mi
ng)
, 
PP
(IN
(fro
m),
 x0
:N
P))
 
!
'(
, x0
NP
(x0
:NN
S) 
!
x0
NP
(x0
:NN
S) 
!
!,
 x0
NP
(x0
:NN
P) 
!
x0
, !
NN
P(F
ran
ce)
 
!
)*
NN
S(a
stro
nau
ts) 
!
+,
, -
OR
OR N
NS
(as
tro
nau
ts) 
!!
,+
,,
 -
OR
NP
(x0
:NN
P) 
!
x0
NP
(x0
:NN
P) 
!
x0
NN
P(F
ran
ce)
 
!
)*
, !
NP
(x0
:NN
S) 
!
x0
VP
(V
BG
(co
mi
ng)
, 
PP
(IN
(fro
m),
 x0
:N
P))
 
!
'(
, x0
co
min
g
fro
m
NN
S
IN
NN
P
NP
VP
NP
VB
G
PP
NP
7-8
5
7-8
5
7-8
4
4
5
4
5
6
7
8
4
4
4-5
4-5
4-8
NN
P(F
ran
ce)
 
!)
*,
 !
NP
(x0
:NN
P) 
!
x0
, !
VP
(V
BG
(co
mi
ng)
, 
PP
(IN
(fro
m)
, 
x0:
NP
))  
!
'(
, x0
, !
NN
S(a
stro
nau
ts) 
!
! ,
 +
,,
 -
NP
(x0
:NN
S) 
!
! ,
 x0
NP
(x0
:NP
, x1
:V
P) 
!
x1
, !
, x0
(a)
(b)
-
'(
)*
!
+,
as
tro
na
uts
Fra
nce
Figure 2: (a) Multiple ways of aligning? to constituents in the tree. (b) Derivation corresponding to the parse tree in Figure 1,
which takes into account all alignments of? pictured in (a).
Note that these properties are dependent on G, and
the above rule would be considered a minimal rule
in a graph G? similar to G, but additionally con-
taining a word alignment between 7 and ?. We
will see in Sections 3 and 5 why extracting only
minimal rules can be highly problematic.
2.2 Unaligned words
While the general theory presented in GHKM ac-
counts for any kind of derivation consistent with
G, it does not particularly discuss the case where
some words of the source-language string f are
not aligned to any word of e, thus disconnected
from the rest of the graph. This case is highly fre-
quent: 24.1% of Chinese words in our 179 mil-
lion word English-Chinese bilingual corpus are
unaligned, and 84.8% of Chinese sentences con-
tain at least one unaligned word. The question is
what to do with such lexical items, e.g., ? in
Figure 2(a). The approach of building one mini-
mal derivation for G as in the algorithm described
in GHKM assumes that we commit ourselves to
a particular heuristic to attach the unaligned item
to a certain constituent of pi, e.g., highest attach-
ment (in the example, ? is attached to NP[4-8]
and the heuristic generates rule (f)). A more rea-
sonable approach is to invoke the principle of in-
sufficient reason and make no a priori assump-
tion about what is a ?correct? way of assigning
the item to a constituent, and return all derivations
that are consistent with G. In Section 4, we will
see how to use corpus evidence to give preference
to unaligned-word attachments that are the most
consistent across the data. Figure 2(a) shows the
six possible ways of attaching ? to constituents
of pi: besides the highest attachment (rule (f)),?
can move along the ancestors of France, since it is
to the right of the translation of that word, and be
considered to be part of an NNP, NP, or VP rule.
We make the same reasoning to the left: ? can
either start the NNS of astronauts, or start an NP.
Our account of all possible ways of consistently
attaching ? to constituents means we must ex-
tract more than one derivation to explain transfor-
mations in G, even if we still restrict ourselves to
minimal derivations (a minimal derivation for G
is unique if and only if no source-language word
in G is unaligned). While we could enumerate
all derivations separately, it is much more effi-
cient both in time and space to represent them as a
derivation forest, as in Figure 2(b). Here, the for-
est covers all minimal derivations that correspond
to G. It is necessary to ensure that for each deriva-
tion, each unaligned item (here ?) appears only
once in the rules of that derivation, as shown in
Figure 2 (which satisfies the property). That re-
quirement will prove to be critical when we ad-
dress the problem of estimating probabilities for
our rules: if we allowed in our example to spuri-
ously generate?s in multiple successive steps of
the same derivation, we would not only represent
the transformation incorrectly, but also ?-rules
would be disproportionately represented, leading
to strongly biased estimates. We will now see how
to ensure this constraint is satisfied in our rule ex-
traction and derivation building algorithm.
963
2.3 Algorithm
The linear-time algorithm presented in GHKM is
only a particular case of the more general one we
describe here, which is used to extract all rules,
minimal and composed, induced by G. Similarly
to the GHKM algorithm, ours performs a top-
down traversal of G, but differs in the operations
it performs at each node n ? F : we must explore
all subtrees rooted at n, find all consistent ways
of attaching unaligned words of f, and build valid
derivations in accordance to these attachments.
We use a table or-dforest[x, y, c] to store OR-
nodes, in which each OR-node can be uniquely
defined by a syntactic category c and a span [x, y]
(which may cover unaligned words of f). This ta-
ble is used to prevent the same partial derivation
to be followed multiple times (the in-degrees of
OR-nodes generally become large with composed
rules). Furthermore, to avoid over-generating un-
aligned words, the root and variables in each rule
are represented with their spans. For example, in
Figure 2(b), the second and third child of the top-
most OR-node respectively span across [4-5][6-8]
and [4-6][7-8] (after constituent reordering). In
the former case, ? will eventually be realized in
an NP, and in the latter case, in a VP.
The preprocessing step consists of assigning
spans and complement spans to nodes of G, in
the first case by a bottom-up exploration of the
graph, and in the latter by a top-down traversal.
To assign complement spans, we assign the com-
plement span of any node n to each of its children,
and for each of them, add the span of the child
to the complement span of all other children. In
another traversal of G, we determine the minimal
rule extractible from each node in F .
We explore all tree fragments rooted at n by
maintaining an open and a closed queue of rules
extracted from n (qo and qc). At each step, we
pick the smallest rule in qo, and for each of its
variable nodes, try to discover new rules (?succes-
sor rules?) by means of composition with minimal
rules, until a given threshold on rule size or maxi-
mum number of rules in qc is reached. There may
be more that one successor per rule, since we must
account for all possible spans than can be assigned
to non-lexical leaves of a rule. Once a threshold is
reached, or if the open queue is empty, we connect
a new OR-node to all rules that have just been ex-
tracted from n, and add it to or-dforest. Finally,
we proceed recursively, and extract new rules from
each node at the frontier of the minimal rule rooted
at n. Once all nodes of F have been processed, the
or-dforest table contains a representation encod-
ing only valid derivations.
3 Probability models
The overall goal of our translation system is to
transform a given source-language sentence f
into an appropriate translation e in the set E
of all possible target-language sentences. In a
noisy-channel approach to SMT, we uses Bayes?
theorem and choose the English sentence e? ? E
that maximizes:4
e? = argmax
e?E
{
Pr(e) ? Pr(f |e)
}
(1)
Pr(e) is our language model, and Pr(f |e) our
translation model. In a grammatical approach to
MT, we hypothesize that syntactic information
can help produce good translation, and thus
introduce dependencies on target-language syntax
trees. The function to optimize becomes:
e? = argmax
e?E
{
Pr(e) ?
?
pi??(e)
Pr(f |pi) ?Pr(pi|e)
}
(2)
?(e) is the set of all English trees that yield the
given sentence e. Estimating Pr(pi|e) is a prob-
lem equivalent to syntactic parsing and thus is not
discussed here. Estimating Pr(f |pi) is the task of
syntax-based translation models (SBTM).
Given a rule set R, our SBTM makes the
common assumption that left-most compositions
of xRs rules ?i = r1 ? ... ? rn are independent
from one another in a given derivation ?i ? ?,
where ? is the set of all derivations constructible
from G = (pi, f ,a) using rules of R. Assuming
that ? is the set of all subtree decompositions of pi
corresponding to derivations in ?, we define the
estimate:
Pr(f |pi) =
1
|?|
?
?i??
?
rj??i
p(rhs(rj)|lhs(rj)) (3)
under the assumption:
?
rj?R:lhs(rj)=lhs(ri)
p(rhs(rj)|lhs(rj)) = 1 (4)
It is important to notice that the probability
distribution defined in Equation 3 requires a
normalization factor (|?|) in order to be tight, i.e.,
sum to 1 over all strings fi ? F that can be derived
4We denote general probability distributions with Pr(?)
and use p(?) for probabilities assigned by our models.
964
Xa
Y b
a?
b?
c?c
(!,f 1
,a 1):
X
a
Y b
b?
a?
c?c
(!,f 2
,a 2):
Figure 3: Example corpus.
from pi. A simple example suffices to demonstrate
it is not tight without normalization. Figure 3
contains a sample corpus from which four rules
can be extracted:
r1: X(a, Y(b, c)) ? a?, b?, c?
r2: X(a, Y(b, c)) ? b?, a?, c?
r3: X(a, x0:Y) ? a?, x0
r4: Y(b, c) ? b?, c?
From Equation 4, the probabilities of r3 and r4
must be 1, and those of r1 and r2 must sum to
1. Thus, the total probability mass, which is dis-
tributed across two possible output strings a?b?c?
and b?a?c?, is: p(a?b?c?|pi) + p(b?a?c?|pi) = p1 +
p3 ? p4 + p2 = 2, where pi = p(rhs(ri)|lhs(ri)).
It is relatively easy to prove that the probabil-
ities of all derivations that correspond to a given
decomposition ?i ? ? sum to 1 (the proof is omit-
ted due to constraints on space). From this prop-
erty we can immediately conclude that the model
described by Equation 3 is tight.5
We examine two estimates p(rhs(r)|lhs(r)).
The first one is the relative frequency estimator
conditioning on left hand sides:
p(rhs(r)|lhs(r)) =
f(r)
?
r?:lhs(r?)=lhs(r) f(r
?)
(5)
f(r) represents the number of times rule r oc-
curred in the derivations of the training corpus.
One of the major negative consequences of
extracting only minimal rules from a corpus is
that an estimator such as Equation 5 can become
extremely biased. This again can be observed
from Figure 3. In the minimal-rule extraction of
GHKM, only three rules are extracted from the ex-
ample corpus, i.e. rules r2, r3, and r4. Let?s as-
sume now that the triple (pi, f1,a1) is represented
99 times, and (pi, f2,a2) only once. Given a tree
pi, the model trained on that corpus can generate
the two strings a?b?c? and b?a?c? only through two
derivations, r3 ? r4 and r2, respectively. Since
all rules in that example have probability 1, and
5If each tree fragment in pi is the lhs of some rule in R,
then we have |?| = 2n, where n is the number of nodes of
the frontier set F ? G (each node is a binary choice point).
given that the normalization factor |?| is 2, both
probabilities p(a?b?c?|pi) and p(b?a?c?|pi) are 0.5.
On the other hand, if all rules are extracted and
incorporated into our relative-frequency probabil-
ity model, r1 seriously counterbalances r2 and the
probability of a?b?c? becomes: 12 ?(
99
100+1) = .995
(since it differs from .99, the estimator remains bi-
ased, but to a much lesser extent).
An alternative to the conditional model of
Equation 3 is to use a joint model conditioning on
the root node instead of the entire left hand side:
p(r|root(r)) =
f(r)
?
r?:root(r?)=root(r) f(r
?)
(6)
This can be particularly useful if no parser or
syntax-based language model is available, and we
need to rely on the translation model to penalize
ill-formed parse trees. Section 6 will describe an
empirical evaluation based on this estimate.
4 EM training
In our previous discussion of parameter estima-
tion, we did not explore the possibility that one
derivation in a forest may be much more plau-
sible than the others. If we knew which deriva-
tion in each forest was the ?true? derivation, then
we could straightforwardly collect rule counts off
those derivations. On the other hand, if we had
good rule probabilities, we could compute the
most likely (Viterbi) derivations for each training
example. This is a situation in which we can em-
ploy EM training, starting with uniform rule prob-
abilities. For each training example, we would like
to: (1) score each derivation ?i as a product of the
probabilities of the rules it contains, (2) compute
a conditional probability pi for each derivation ?i
(conditioned on the observed training pair) by nor-
malizing those scores to add to 1, and (3) collect
weighted counts for each rule in each ?i, where
the weight is pi. We can then normalize the counts
to get refined probabilities, and iterate; the corpus
likelihood is guaranteed to improve with each it-
eration. While it is infeasible to enumerate the
millions of derivations in each forest, Graehl and
Knight (2004) demonstrate an efficient algorithm.
They also analyze how to train arbitrary tree trans-
ducers into two steps. The first step is to build a
derivation forest for each training example, where
the forest contains those derivations licensed by
the (already supplied) transducer?s rules. The sec-
ond step employs EM on those derivation forests,
running in time proportional to the size of the
965
Best minimal-rule derivation (Cm) p(r)
(a) S(x0:NP-C x1:VP x2:.) ? x0 x1 x2 .845
(b) NP-C(x0:NPB) ? x0 .82
(c) NPB(DT(the) x0:NNS) ? x0 .507
(d) NNS(gunmen) ??K .559
(e) VP(VBD(were) x0:VP-C) ? x0 .434
(f) VP-C(x0:VBN x1:PP) ? x1 x0 .374
(g) PP(x0:IN x1:NP-C) ? x0 x1 .64
(h) IN(by) ?? .0067
(i) NP-C(x0:NPB) ? x0 .82
(j) NPB(DT(the) x0:NN) ? x0 .586
(k) NN(police) ?f? .0429
(l) VBN(killed) ??? .0072
(m) .(.) ? . .981
.
 
The
gunm
enw
ere
killed
by
the
polic
e.
DT
VBD
VBN
DT
NN
NP
PP
VP-C
VPS
NNS
IN
NP
.
!"
#$
%
&'
Best composed-rule derivation (C4) p(r)
(o) S(NP-C(NPB(DT(the) NNS(gunmen))) x0:VP .(.)) ??K x0 . 1
(p) VP(VBD(were) VP-C(x0:VBN PP(IN(by) x1:NP-C))) ?? x1 x0 0.00724
(q) NP-C(NPB(DT(the) NN(police))) ?f? 0.173
(r) VBN(killed) ??? 0.00719
Figure 4: Two most probable derivations for the graph on the right: the top table restricted to minimal rules; the bottom one,
much more probable, using a large set of composed rules. Note: the derivations are constrained on the (pi, f ,a) triple, and thus
include some non-literal translations with relatively low probabilities (e.g. killed, which is more commonly translated as{?).
rule nb. of nb. of deriv- EM-
set rules nodes time time
Cm 4M 192M 2 h. 4 h.
C3 142M 1255M 52 h. 34 h.
C4 254M 2274M 134 h. 60 h.
Table 2: Rules and derivation nodes for a 54M-word, 1.95M
sentence pair English-Chinese corpus, and time to build
derivations (on 10 cluster nodes) and run 50 EM iterations.
forests. We only need to borrow the second step
for our present purposes, as we construct our own
derivation forests when we acquire our rule set.
A major challenge is to scale up this EM train-
ing to large data sets. We have been able to run
EM for 50 iterations on our Chinese-English 54-
million word corpus. The derivation forests for
this corpus contain 2.2 billion nodes; the largest
forest contains 1.1 million nodes. The outcome
is to assign probabilities to over 254 million rules.
Our EM runs with either lhs normalization or lhs-
root normalization. In the former case, each lhs
has an average of three corresponding rhs?s that
compete with each other for probability mass.
5 Model coverage
We now present some examples illustrating the
benefit of composed rules. We trained three
p(rhs(ri)|lhs(ri)) models on a 54 million-word
English-Chinese parallel corpus (Table 2): the first
one (Cm) with only minimal rules, and the two
others (C3 and C4) additionally considering com-
posed rules with no more than three, respectively
four, internal nodes in lhs(ri). We evaluated these
models on a section of the NIST 2002 evaluation
corpus, for which we built derivation forests and
lhs: S(x0:NP-C VP(x1:VBD x2:NP-C) x3:.)
corpus rhsi p(rhsi|lhs)
Chinese x1 x0 x2 x3 .3681
(minimal) x0 x1 , x3 x2 .0357
x2 , x0 x1 x3 .0287
x0 x1 , x3 x2 . .0267
Chinese x0 x1 x2 x3 .9047
(composed) x0 x1 , x2 x3 .016
x0 , x1 x2 x3 .0083
x0 x1 ? x2 x3 .0072
Arabic x1 x0 x2 x3 .5874
(composed) x0 x1 x2 x3 .4027
x1 x2 x0 x3 .0077
x1 x0 x2 " x3 .0001
Table 3: Our model transforms English subject-verb-object
(SVO) structures into Chinese SVO and into Arabic VSO.
With only minimal rules, Chinese VSO is wrongly preferred.
extracted the most probable one (Viterbi) for each
sentence pair (based on an automatic alignment
produced by GIZA). We noticed in general that
Viterbi derivations according to C4 make exten-
sive usage of composed rules, as it is the case in
the example in Figure 4. It shows the best deriva-
tion according to Cm and C4 on the unseen (pi,f,a)
triple displayed on the right. The second deriva-
tion (log p = ?11.6) is much more probable than
the minimal one (log p = ?17.7). In the case
of Cm, we can see that many small rules must be
applied to explain the transformation, and at each
step, the decision regarding the re-ordering of con-
stituents is made with little syntactic context. For
example, from the perspective of a decoder, the
word by is immediately transformed into a prepo-
sition (IN), but it is in general useful to know
which particular function word is present in the
sentence to motivate good re-orderings in the up-
966
lhs1: NP-C(x0:NPB PP(IN(of) x1:NP-C)) (NP-of-NP)
lhs2: PP(IN(of) NP-C(x0:NPB PP(IN(of) NP-C(x1:NPB x2:VP)))) (of-NP-of-NP-VP)
lhs3: VP(VBD(said) SBAR-C(IN(that) x0:S-C)) (said-that-S)
lhs4: SBAR(WHADVP(WRB(when)) S-C(x0:NP-C VP(VBP(are) x1:VP-C))) (when-NP-are-VP)
rhs1i p(rhs1i|lhs1) rhs2i p(rhs2i|lhs2) rhs3i p(rhs3i|lhs3) rhs4i p(rhs4i|lhs4)
x1 x0 .54 x2 ? x1 ? x0 .6754 ? , x0 .6062 ( x1 x0 ? .6618
x0 x1 .2351 ( x2 ? x1 ? x0 .035 ? x0 .1073 S x1 x0 ? .0724
x1 ? x0 .0334 x2 ? x1 ? x0 , .0263 h: , x0 .0591 ( x1 x0 ? , .0579
x1 x0 ? .026 x2 ? x1 ? x0 	 .0116 ? ? , x0 .0234 , ( x1 x0 ? .0289
Table 4: Translation probabilities promote linguistically motivated constituent re-orderings (for lhs1 and lhs2), and enable
non-constituent (lhs3) and non-contiguous (lhs4) phrasal translations.
per levels of the tree. A rule like (e) is particu-
larly unfortunate, since it allows the word were to
be added without any other evidence that the VP
should be in passive voice. On the other hand, the
composed-rule derivation of C4 incorporates more
linguistic evidence in its rules, and re-orderings
are motivated by more syntactic context. Rule
(p) is particularly appropriate to create a passive
VP construct, since it expects a Chinese passive
marker (?), an NP-C, and a verb in its rhs, and
creates the were ... by construction at once in the
left hand side.
5.1 Syntactic translation tables
We evaluate the promise of our SBTM by analyz-
ing instances of translation tables (t-table). Table 3
shows how a particular form of SVO construc-
tion is transformed into Chinese, which is also an
SVO language. While the t-table for Chinese com-
posed rules clearly gives good estimates for the
?correct? x0 x1 ordering (p = .9), i.e. subject be-
fore verb, the t-table for minimal rules unreason-
ably gives preference to verb-subject ordering (x1
x0, p = .37), because the most probable transfor-
mation (x0 x1) does not correspond to a minimal
rule. We obtain different results with Arabic, an
VSO language, and our model effectively learns
to move the subject after the verb (p = .59).
lhs1 in Table 4 shows that our model is able
to learn large-scale constituent re-orderings, such
as re-ordering NPs in a NP-of-NP construction,
and put the modifier first as it is more commonly
the case in Chinese (p = .54). If more syntac-
tic context is available as in lhs2, our model
provides much sharper estimates, and appropri-
ately reverses the order of three constituents with
high probability (p = .68), inserting modifiers first
(possessive markers? are needed here for better
syntactic disambiguation).
A limitation of earlier syntax-based systems is
their poor handling of non-constituent phrases.
Table 4 shows that our model can learn rules for
such phrases, e.g., said that (lhs3). While the that
has no direct translation, our model effectively
learns to separate? (said) from the relative clause
with a comma, which is common in Chinese.
Another promising prospect of our model seems
to lie in its ability to handle non-contiguous
phrases, a feature that state of the art systems
such as (Och and Ney, 2004) do not incorpo-
rate. The when-NP-are-VP construction of lhs4
presents such a case. Our model identifies that are
needs to be deleted, that when translates into the
phrase( ...?, and that the NP needs to be moved
after the VP in Chinese (p = .66).
6 Empirical evaluation
The task of our decoder is to find the most likely
English tree pi that maximizes all models involved
in Equation 2. Since xRs rules can be converted to
context-free productions by increasing the number
of non-terminals, we implemented our decoder as
a standard CKY parser with beam search. Its rule
binarization is described in (Zhang et al, 2006).
We compare our syntax-based system against
an implementation of the alignment template
(AlTemp) approach to MT (Och and Ney, 2004),
which is widely considered to represent the state
of the art in the field. We registered both systems
in the NIST 2005 evaluation; results are presented
in Table 5. With a difference of 6.4 BLEU points
for both language pairs, we consider the results
of our syntax-based system particularly promis-
ing, since these are the highest scores to date that
we know of using linguistic syntactic transforma-
tions. Also, on the one hand, our AlTemp sys-
tem represents quite mature technology, and in-
corporates highly tuned model parameters. On
the other hand, our syntax decoder is still work in
progress: only one model was used during search,
i.e., the EM-trained root-normalized SBTM, and
as yet no language model is incorporated in the
search (whereas the search in the AlTemp sys-
tem uses two phrase-based translation models and
967
Syntactic AlTemp
Arabic-to-English 40.2 46.6
Chinese-to-English 24.3 30.7
Table 5: BLEU-4 scores for the 2005 NIST test set.
Cm C3 C4
Chinese-to-English 24.47 27.42 28.1
Table 6: BLEU-4 scores for the 2002 NIST test set, with rules
of increasing sizes.
12 other feature functions). Furthermore, our de-
coder doesn?t incorporate any syntax-based lan-
guage model, and admittedly our ability to penal-
ize ill-formed parse trees is still limited.
Finally, we evaluated our system on the NIST-
02 test set with the three different rule sets (see
Table 6). The performance with our largest rule
set represents a 3.63 BLEU point increase (14.8%
relative) compared to using only minimal rules,
which indicates positive prospects for using even
larger rules. While our rule inference algorithm
scales to higher thresholds, one important area of
future work will be the improvement of our de-
coder, conjointly with analyses of the impact in
terms of BLEU of contextually richer rules.
7 Related work
Similarly to (Poutsma, 2000; Wu, 1997; Yamada
and Knight, 2001; Chiang, 2005), the rules dis-
cussed in this paper are equivalent to productions
of synchronous tree substitution grammars. We
believe that our tree-to-string model has several
advantages over tree-to-tree transformations such
as the ones acquired by Poutsma (2000). While
tree-to-tree grammars are richer formalisms that
provide the potential benefit of rules that are lin-
guistically better motivated, modeling the syntax
of both languages comes as an extra cost, and it
is admittedly more helpful to focus our syntac-
tic modeling effort on the target language (e.g.,
English) in cases where it has syntactic resources
(parsers and treebanks) that are considerably more
available than for the source language. Further-
more, we think there is, overall, less benefit in
modeling the syntax of the source language, since
the input sentence is fixed during decoding and is
generally already grammatical.
With the notable exception of Poutsma, most
related works rely on models that are restricted
to synchronous context-free grammars (SCFG).
While the state-of-the-art hierarchical SMT sys-
tem (Chiang, 2005) performs well despite strin-
gent constraints imposed on its context-free gram-
mar, we believe its main advantage lies in its
ability to extract hierarchical rules across phrasal
boundaries. Context-free grammars (such as Penn
Treebank and Chiang?s grammars) make indepen-
dence assumptions that are arguably often unrea-
sonable, but as our work suggests, relaxations
of these assumptions by using contextually richer
rules results in translations of increasing quality.
We believe it will be beneficial to account for this
finding in future work in syntax-based SMT and in
efforts to improve upon (Chiang, 2005).
8 Conclusions
In this paper, we developed probability models for
the multi-level transfer rules presented in (Galley
et al, 2004), showed how to acquire larger rules
that crucially condition on more syntactic context,
and how to pack multiple derivations, including
interpretations of unaligned words, into derivation
forests. We presented some theoretical arguments
for not limiting extraction to minimal rules, val-
idated them on concrete examples, and presented
experiments showing that contextually richer rules
provide a 3.63 BLEU point increase over the min-
imal rules of (Galley et al, 2004).
Acknowledgments
We would like to thank anonymous review-
ers for their helpful comments and suggestions.
This work was partially supported under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-C-0022.
References
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of ACL.
H. Fox. 2002. Phrasal cohesion and statistical machine trans-
lation. In Proc. of EMNLP, pages 304?311.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule? In Proc. of HLT/NAACL-04.
J. Graehl and K. Knight. 2004. Training tree transducers. In
Proc. of HLT/NAACL-04, pages 105?112.
F. Och and H. Ney. 2004. The alignment template approach
to statistical machine translation. Computational Linguis-
tics, 30(4):417?449.
A. Poutsma. 2000. Data-oriented translation. In Proc. of
COLING, pages 635?641.
D. Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?404.
K. Yamada and K. Knight. 2001. A syntax-based statistical
translation model. In Proc. of ACL, pages 523?530.
H. Zhang, L. Huang, D. Gildea, and K. Knight. 2006. Syn-
chronous binarization for machine translation. In Proc. of
HLT/NAACL.
968
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 364?372,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Skip-Chain Conditional Random Field for
Ranking Meeting Utterances by Importance?
Michel Galley
Columbia University
Department of Computer Science
New York, NY 10027, USA
galley@cs.columbia.edu
Abstract
We describe a probabilistic approach to content se-
lection for meeting summarization. We use skip-
chain Conditional Random Fields (CRF) to model
non-local pragmatic dependencies between paired
utterances such as QUESTION-ANSWER that typi-
cally appear together in summaries, and show that
these models outperform linear-chain CRFs and
Bayesian models in the task. We also discuss dif-
ferent approaches for ranking all utterances in a se-
quence using CRFs. Our best performing system
achieves 91.3% of human performance when evalu-
ated with the Pyramid evaluation metric, which rep-
resents a 3.9% absolute increase compared to our
most competitive non-sequential classifier.
1 Introduction
Summarization of meetings faces many challenges
not found in texts, i.e., high word error rates, ab-
sence of punctuation, and sometimes lack of gram-
maticality and coherent ordering. On the other
hand, meetings present a rich source of structural
and pragmatic information that makes summariza-
tion of multi-party speech quite unique. In par-
ticular, our analyses of patterns in the verbal ex-
change between participants found that adjacency
pairs (AP), a concept drawn from the conver-
sational analysis literature (Schegloff and Sacks,
1973), have particular relevance to summarization.
APs are pairs of utterances such as QUESTION-
ANSWER or OFFER-ACCEPT, in which the second
utterance is said to be conditionally relevant on the
first. We show that there is a strong correlation be-
tween the two elements of an AP in summariza-
tion, and that one is unlikely to be included if the
other element is not present in the summary.
Most current statistical sequence models in nat-
ural language processing (NLP), such as hidden
?This material is based on research supported in part by
the U.S. National Science Foundation (NSF) under Grants
No. IIS-0121396 and IIS-05-34871, and the Defense Ad-
vanced Research Projects Agency (DARPA) under Contract
No. HR0011-06-C-0023. Any opinions, findings and con-
clusions or recommendations expressed in this material are
those of the author and do not necessarily reflect the views of
the NSF or DARPA.
Markov models (HMMs) (Rabiner, 1989), are lin-
ear chains that only encode local dependencies
between utterances to be labeled. In multi-party
speech, the two elements of an AP are gener-
ally arbitrarily distant, and such models can only
poorly account for dependencies underlying APs
in summarization. We use instead skip-chain se-
quence models (Sutton and McCallum, 2004),
which allow us to explicitly model dependencies
between distant utterances, and turn out to be par-
ticularly effective in the summarization task.
In this paper, we compare two types of network
structures?linear-chain and skip-chain?and two
types of network semantics?Bayesian Networks
(BNs) and Conditional Random Fields (CRFs).
We discuss the problem of estimating the class
posterior probability of each utterance in a se-
quence in order to extract the N most proba-
ble ones, and show that the cost assigned by a
CRF to each utterance needs to be locally nor-
malized in order to outperform BNs. After ana-
lyzing the predictive power of a large set of dura-
tional, acoustical, lexical, structural, and informa-
tion retrieval features, we perform feature selec-
tion to have a competitive set of predictors to test
the different models. Empirical evaluations using
two standard summarization metrics?the Pyra-
mid method (Nenkova and Passonneau, 2004b)
and ROUGE (Lin, 2004)?show that the best
performing system is a CRF incorporating both
order-2 Markov dependencies and skip-chain de-
pendencies, which achieves 91.3% of human per-
formance in Pyramid score, and outperforms our
best-performing non-sequential model by 3.9%.
2 Corpus
The work presented here was applied to the ICSI
Meeting Corpus (Janin et al, 2003), a corpus
of ?naturally-occurring? meetings, i.e. meetings
that would have taken place anyway. Their style
is quite informal, and topics are primarily con-
cerned with speech, natural language, artificial
364
intelligence, and networking research. The cor-
pus contains 75 meetings, which are 60 minutes
long on average, and involve a number of partic-
ipants ranging from 3 to 10 (6 on average). The
total number of unique speakers is 60, includ-
ing 26 non-native English speakers. Experiments
in this paper are based either on human ortho-
graphic transcriptions or automatic speech recog-
nition output, which were available for all meet-
ings. For automatic recognition, we used the ICSI-
SRI-UW speech recognition system (Mirghafori
et al, 2004), a state-of-the-art conversational tele-
phone speech (CTS) recognizer whose language
and acoustic models were adapted to the meeting
domain. It achieves 34.8% WER on the ICSI cor-
pus, which is indicative of the difficulty involved
in processing meetings automatically.
We also used additional annotation that has
been developed to support higher-level analyses of
meeting structure, in particular the ICSI Meeting
Recorder Dialog act (MRDA) corpus (Shriberg et
al., 2004). Dialog act (DA) labels describe the
pragmatic function of utterances, e.g. a STATE-
MENT or a BACKCHANNEL. This auxiliary cor-
pus consists of over 180,000 human-annotated
dialog act labels (? = .8), for which so-called
adjacency pair (AP) relations (e.g., APOLOGY-
DOWNPLAY) were also labeled. This latter anno-
tation was used to train an AP classifier that is in-
strumental in automatically determining the struc-
ture of our sequence models. Note that, in the case
of three or more speakers, adjacency pair is ad-
mittedly an unfortunate term, since labeled APs
are generally not adjacent (e.g., see Table 1), but
we will nevertheless use the same terminology to
enforce consistency with previous work.
To train and evaluate our summarizer, we used
a corpus of extractive summaries produced at the
University of Edinburgh (Murray et al, 2005). For
each of the 75 meetings, human judges were asked
to select transcription utterances segmented by DA
to include in summaries, resulting in an average
compression ratio of 6.26% (though no strict limit
was imposed). Inter-labeler agreement was mea-
sured using six meetings that were summarized by
multiple coders (average ? = .323). While this
level of agreement is quite low, this situation is
not uncommon to summarization, since there may
be many good summaries for a given document;
a main challenge lies in using evaluation schemes
that properly accounts for this diversity.
3 Content selection
State sequence Markov models such as hidden
Markov models (Rabiner, 1989) have been highly
successful in many speech and natural language
processing applications, including summarization.
Following an intuition that the probability of a
given sentence may be locally conditioned on the
previous one, Conroy (2004) built a HMM-based
summarizer that consistently ranked among the
top systems in recent Document Understanding
Conference (DUC) evaluations.
Inter-sentential influences become more com-
plex in the case of dialogues or correspondences,
especially when they involve multiple parties.
In the case of summarization of conversational
speech, Zechner (2002) found, for instance, that
a simple technique consisting of linking together
questions and answers in summaries?and thus
preventing the selection of orphan questions or
answers?significantly improved their readability
according to various human summary evaluations.
In email summarization (Rambow et al, 2004),
Shrestha and McKeown (2004) obtained good per-
formance in automatic detection of questions and
answers, which can help produce summaries that
highlight or focus on the question and answer ex-
change. In a combined chat and email summariza-
tion task, a technique (Zhou and Hovy, 2005) con-
sisting of identifying APs and appending any rele-
vant responses to topic initiating messages was in-
strumental in outperforming two competitive sum-
marization baselines.
The need to model pragmatic influences, such
as between a question and an answer, is also preva-
lent in meeting summarization. In fact, question-
answer pairs are not the only discourse relations
that we need to preserve in order to create co-
herent summaries, and, as we will see, most in-
stances of APs would need to be preserved to-
gether, either inside or outside the summary. Ta-
ble 1 displays an AP construction with one state-
ment (A part) and three respondents (B parts).
This example illustrates that the number of turns
between constituents of APs is variable and thus
difficult to model with standard sequence models.
This example also illustrates some of the predic-
tors investigated in this paper. First, many speak-
ers respond to A?s utterance, which is generally a
strong indicator that the A utterance should be in-
cluded. Secondly, while APs are generally char-
acterized in terms of pre-defined dialog acts, such
365
Time Speaker AP Transcript
1480.85-1493.91 1 A are - are those d- delays adjustable? see a lot of people who actually build stuff
with human computer interfaces understand that delay, and - and so when you -
by the time you click it it?ll be right on because it?ll go back in time to put the -
1489.71-1489.94 2 yeah.
1493.95-1495.41 3 B yeah, uh, not in this case.
1494.31-1495.83 2 B it could do that, couldn?t it.
1495.1-1497.07 4 B we could program that pretty easily , couldn?t we?
Table 1: Snippet of a meeting displaying an AP construction, where a question (A) initiates three responses (B). Sentences in
italic are not present in the reference summary.
as OFFER-ACCEPT, we found that the type of di-
alog act has much less importance than the ex-
istence of the AP connection itself (APs in the
data represent a great variety of DA pairs, includ-
ing many that are not characterized as APs in the
litterature?e.g., STATEMENT-STATEMENT in the
table). Since DAs seem to matter less than adja-
cency pairs, the aim will be to build techniques to
automatically identify such relations and exploit
them in utterance selection.
In the current work, we use skip-chain sequence
models (Sutton and McCallum, 2004) to repre-
sent dependencies between both contiguous ut-
terances and paired utterances appearing in the
same AP constructions. The graphical represen-
tations of skip-chain models, such as the CRF rep-
resented in Figure 1, are composed of two types of
edges: linear-chain and skip-chain edges. The lat-
ter edges model AP links, which we represent as
a set of (s, d) index pairs (note that no more than
one AP may share the same second element d).
The intuition that the summarization labels (?1
or 1) are highly correlated with APs is confirmed
in Table 2. While contiguous labels yt?1 and yt
seem to seldom influence each other, the correla-
tion between AP elements ys and yd is particularly
strong, and they have a tendency to be either both
included or both excluded. Note that the second
table is not symmetric, because the data allows an
A part to be linked to multiple B parts, but not
vice-versa. While counts in Table 2 reflect hu-
man labels, we only use automatically predicted
(s, d) pairs in the experiments of the remaining
part of this paper. To find these pairs automati-
cally, we trained a non-sequential log-linear model
that achieves a .902 accuracy (Galley et al, 2004).
4 Skip-Chain Sequence Models
In this paper, we investigate conditional models
for paired sequences of observations and labels. In
the case of utterance selection, the observation se-
quence x = x1:T = (x1, . . . , xT ) represents local
State
men
t
x 1
x 2
x 3
x 4
x 5
Back
Chan
nel
State
men
tSt
atem
ent
State
men
t
y 1
y 2
y 3
y 4
y 5
Figure 1: A skip-chain CRF with pragmatic-level links.
Linear-chain edges yt = 1 yt = ?1
yt?1 = 1 529 7742
yt?1 = ?1 7742 116040
Skip-chain edges yd = 1 yd = ?1
ys = 1 6792 2191
ys = ?1 1479 121591
Table 2: Contingency tables: while the correlation between
adjacent labels yt?1 and yt is not significant (?2 = 2.3,
p > .05), empirical evidence clearly shows that ys and yd
influence each other (?2 = 78948, p < .001).
summarization predictors (see Section 6), and the
binary sequence y = y1:T = (y1, . . . , yT ) (where
yt ? {?1, 1}) determines which utterances must
be included in the summary. In a discriminative
framework, we concentrate our modeling effort on
estimating p(y|x) from data, and do not explicitly
model the prior probability p(x), since x is fixed
during testing anyway.
Many probabilistic approaches to modeling se-
quences have relied on directed graphical mod-
els, also known as Bayesian networks (BN),1 in
particular hidden Markov models (Rabiner, 1989)
and conditional Markov models (McCallum et al,
2000). However, prominent recent approaches
have focused on undirected graphical models, in
particular conditional random fields (CRF) (Laf-
ferty et al, 2001), and provided state-of-the-art
performance in many NLP tasks. In our work, we
will provide empirical results for state sequence
models of both semantics, and we will now de-
1In the existing literature, sequence models that satisfy the
Markovian condition?i.e., the state of the system at time t
depend only on its immediate past t? k:t? 1 (typically just
t? 1)?are generally termed dynamic Bayesian networks
(DBN). Since the particular models under investigation, i.e.
skip-chain models, do not have this property, we will simply
refer to them as Bayesian networks.
366
scribe skip-chain models for both BNs and CRFs.
In a BN, the probability of the sequence y fac-
torizes as a product of probabilities of local predic-
tions yt conditioned on their parents pi(yt) (Equa-
tion 1). In a CRF, the probability of the sequence y
factorizes according to a set of clique potentials
{?c}c?C , where C is represents the cliques of the
underlying graphical model (Equation 2).
pBN(y|x) =
T?
i=1
pBN(yt|x, pi(yt)) (1)
pCRF(y|x) ?
?
c?C
?c(xc,yc) (2)
We parameterize these BNs and CRFs as log-
linear models, and factorize both BN?s local pre-
diction probabilities and CRF?s clique potentials
using two types of feature functions. Linear-chain
feature functions fj(yt?k:t,x, t) represent local
dependencies that are consistent with an order-k
Markov assumption. For instance, one such func-
tion could be a predicate that is true if and only if
yt?1 = 1, yt = ?1, and (xt?1, xt) indicates that
both utterances are produced by the same speaker.
Given a set of skip edges S = {(st, t)} specifying
source and destination indices, skip-chain feature
functions gj(yst , yt,x, st, t) exploit dependencies
between variables that are arbitrarily distant in
the chain. For instance, the finding that OFFER-
REJECT pairs are often linked in summaries might
be encoded as a skip-chain feature predicate that
is true if and only if yst = 1, yt = 1, and the first
word of the t-th utterance is ?no?.
Log-linear models for skip-chain sequence
models are defined in terms of weights {?k} and
{?k}, one for each feature function. In the case of
BNs, we write:
log pBN(yt|x, pi(yt)) ?
J?
j=1
?jfj(x,yt?k:t, t) +
J ??
j=1
?jgj(x, yst , yt, st, t)
We can reduce a particular skip-chain CRF to rep-
resent only the set of cliques along (yt?1, yt) adja-
cency edges and (yst , yt) skip edges, resulting in
only two potential functions:
log ?LIN(x,yt?k:t, t) =
J?
j=1
?jfj(x,yt?k:t, t)
log ?SKIP(x, yst , yt, t) =
J ??
j=1
?jgj(x, yst , yt, st, t)
4.1 Inference and Parameter Estimation
Our CRF and BN models were designed us-
ing MALLET (McCallum, 2002), which provides
tools for training log-linear models with L-BFGS
optimization techniques and maximize the log-
likelihood of our training dataD = (x(i),y(i))
N
i=1,
and provides probabilistic inference algorithms for
linear-chain BNs and CRFs.
Most previous work with CRFs containing non-
local dependencies used approximate probabilis-
tic inference techniques, including TRP (Sutton
and McCallum, 2004) and Gibbs sampling (Finkel
et al, 2005). Approximation is needed when
the junction tree of a graphical model is associ-
ated with prohibitively large cliques. For exam-
ple, the worse case reported in (Sutton and Mc-
Callum, 2004) is a clique of 61 nodes. In the
case of skip-chain models representing APs, the
inference problem is somewhat simpler: loops in
the graph are relatively short, 98% of AP edges
span no more than 5 time slices, and the maximum
clique size in the entire data is 5. While exact in-
ference might be possible in our case, we used the
simpler approach of adapting standard inference
algorithms for linear-chain models.
Specifically, to account for skip-edges, we used
a technique inspired by (Sha and Pereira, 2003),
in which multiple state dependencies, such as an
order-2 Markov model, are encoded using auxil-
iary tags. For instance, an order-2 Markov model
is parameterized using state triples yt?2:t, and each
possible triple is converted to a label zt = yt?2:t.
Using these auxiliary labels only, we can then
use the standard forward-backward algorithm for
computing marginal distributions in linear-chain
CRFs, and Viterbi decoding in linear-chain CRFs
and BNs. The only requirement is to ensure that
a transition between zt and zt+1 is forbidden if
the sub-states yt?1:t common to both states differ,
i.e., is assigned an infinite cost. This approach can
be extended to the case of skip-chain transitions.
For instance, an order-1 Markov model with skip-
edges can be constructed using zt = (yst , yt?1, yt)
triples, where the first element yst represents the
label at the source of the skip-edge. Similarly to
the case of order-2 Markov models, we need to
ensure that only valid sequences of labels are con-
sidered, which is trivial to enforce if we assume
that no skip edge ranges more than a predefined
threshold of k time slices.
While this approach is not exact, it still provides
367
competitive performance as we will see in Sec-
tion 8. In future work, we plan to explore more
accurate probabilistic inference techniques.
5 Ranking Utterances by Importance
As we will see in Section 8, using the actual
{?1, 1} label predictions of our BNs and CRFs
leads to significantly sub-optimal results, which
might be explained by the following reasons. First,
our models are optimized to maximize the condi-
tional log-likelihood of the training data, a mea-
sure that does not correlate well with utility mea-
sures generally used in retrieval oriented tasks
such as summarization, especially when faced
with a significant class imbalance (only 6.26%
of reference instances are positive). Second, the
MAP decision rule doesn?t give us the freedom to
select an arbitrary number of sentences in order
to satisfy any constraint on length. Instead of us-
ing actual predictions, it seems more reasonable
to compute the posterior probability of each lo-
cal prediction yt, and extract the N most probable
summary sentences (yr1 , . . . , yrk), where N may
depend on a length expressed in number of words,
as it is the case in our evaluation in Section 7.
BNs assign probability distributions over entire
sequences by estimating the probability of each in-
dividual instance yt in the sequence (Equation 1),
and seem thus particularly suited for ranking utter-
ances. A first approach is then to rank utterances
according to the cost of predicting yt = 1 at each
time step on the Viterbi path. While these costs
are well-formed (negative log) probabilities in the
case of BNs, they cannot be interpreted as such in
the case of CRFs, and turn out to produce poor re-
sults with CRFs. Indeed, the set of CRF potentials
associated with each time step have no immedi-
ate probabilistic interpretation, and cannot be used
directly to rank sentences. Since BNs and CRFs
are here parameterized as log-linear models and
rely on the same set of feature functions, a second
approach is to use CRF-trained model parameters
to build a BN classifier that assigns a probability
to each yt. Specifically, the CRF model is first
used to generate label predicitons y?, from which
the locally-normalized model estimates the cost
of predicting y?t = 1 given a label history y?1:t?1.
This ensures that we have a well-formed probabil-
ity distribution at each time slice, while capitaliz-
ing on the good performance of CRF models.
Lexical features:
? n-grams (n ? 3)
? number of words
? number of digits
? number of consecutive repeats
Information retrieval features:
? max/sum/mean frequency of all terms in ut
? max/sum/mean idf score
? max/sum/mean tf ?idf score
? cosine similarity between word vector of ut with cen-
troid of of the meeting
? scores of LSA with 5, 10, 50, 100, 200, 300 concepts
Acoustic features:
? seconds of silence before/during/after the turn
? speech rate
? min/max/mean/median/stddev/onset/outset f0 of utter-
ance t, and of first and last word
? min/max/mean/stddev energy
? .05, .25, .5, .75, .95 quantiles of f0 and energy
? pitch range
? f0 mean absolute slope
Durational and structural features:
? duration of the previous/current/next utterance
? relative position within meeting (i.e., index t)
? relative position within speaker turn
? large number of structural predicates, i.e. ?is the previ-
ous utterance of the same speaker??
? number of APs initiated in yt
Discourse features:
? lexical cohesion score (for topic shifts) (Hearst, 1994)
? first and second word of utterance, if in cue word list
? number of pronouns
? number of fillers and fluency devices (e.g., ?uh?, ?um?)
? number of backchannel and acknowledgment tokens
(e.g., ?uh-huh?, ?ok?, ?right?)
Table 3: Features for extractive summarization. Unless oth-
erwise mentioned, we refer to features of utterance t whose
label yt we are trying to predict.
6 Features for extractive summarization
We started our analyses with a large collection
of features found to be good predictors in ei-
ther speech (Inoue et al, 2004; Maskey and
Hirschberg, 2005; Murray et al, 2005) or text
summarization (Mani and Maybury, 1999). Our
goal is to build a very competitive feature set that
capitalizes on recent advances in summarization of
both genres. Table 3 lists some important features.
There is strong evidence that lexical cues such
as ?significant? and ?great? are strong predictors
in many summarization tasks (Edmundson, 1968).
Such cues are admittedly quite genre specific,
so we did not want to commit ourselves to any
specific list, which may not carry over well to
our specific speech domain, and we automatically
selected a list of n-grams (n ? 3) using cross-
validation on the training data. More specifically,
we computed the mutual information of each n-
368
Tra
nsc
ript
:
I th
ink 
-on
e th
ing
 tha
t m
ake
s a
 dif
fere
nce
 is t
his 
DC
 off
set
 co
mp
ens
atio
n. 1-
13
Did
 yo
u h
ave
 a l
ook
 at 
me
etin
g d
igits
 if t
hey
 ha
ve 
a th
em
? 14-
26
I di
dn'
t. N
o. 27
-29
Hm
m. 3
0
No.
 Th
e D
C c
om
pon
ent
 is n
egl
igib
le. 
All 
mik
es 
hav
e D
C r
em
ova
l. 31-
41
Yea
h. 42
Bec
aus
e th
ere
's a
 sa
mp
le a
nd 
hol
d in
 the
 A-t
o-D
. 43-5
1
And
 I a
lso,
 um
, di
d s
om
e e
xpe
rim
ent
s a
bou
t no
rma
lizin
g th
e p
has
e. 52
-62
And
 ca
me
 up
 wit
h a
 we
b p
age
 pe
opl
e c
an 
tak
e a
 loo
k a
t. 63-
75
Mo
del
 1 (
len
=20
):
31-
41
43-
51
Mo
del
 2 (
len
=22
):
31-
41
52-
62
Mo
del
 3 (
len
=24
):
52-
62
63-
75
Pee
r (le
n=2
2):
1-1
3
43-
51
Op
tim
al (
len
=22
):
31-
41
52-
62
1 1 2 3 4 3 3 2 2
Spe
ake
r:
Figure 2: Model, peer, and ?optimal? summaries are all extracts taken from the same transcription.
gram with the class variable, and selected for each
n the 200 best scoring n-grams. Other lexical fea-
tures include: the number of digits, which is help-
ful for identifying sections of the meetings where
participants collect data by recording digits; the
number of repeats, which may indicate the kind of
hesitations and disfluencies that negatively corre-
lates with what is included in the summary.
The information retrieval feature set contains
many features that are generally found helpful in
summarization, in particular tf ?idf and scores de-
rived from centroid methods. In particular, we
used the latent semantic analysis (LSA) feature
discussed in (Murray et al, 2005), which attempts
to determine sentence importance through singu-
lar value decomposition, and whose resulting sin-
gular values and singular vectors can be exploited
to associate each utterance a degree of relevance to
one of the top-n concepts of the meetings (where n
represents the number of dimensions in the LSA).
We used the same scoring mechanism as (Mur-
ray et al, 2005), though we extracted features for
many different n values.
Acoustic features extracted with Praat
(Boersma and Weenink, 2006) were normal-
ized by channel and speaker, including many
raw features such as f0 and energy. Structural
features listed in the table are those computed
from the sequence model before decoding, e.g.,
the duration that separates the two elements
of an AP. Finally, discourse features represent
predictors that may substitute to DA labels. While
DA tagging is not directly our concern, it is
presumably helpful to capitalize on discourse
characteristics of utterances involved in adjacency
pairs, since different types of dialog acts may be
unequally likely to appear in a summary.
7 Evaluation
Evaluating summarization is a difficult problem
and there is no broad consensus on how to best
perform this task. Two metrics have become
quite popular in multi-document summarization,
namely the Pyramid method (Nenkova and Pas-
sonneau, 2004b) and ROUGE (Lin, 2004). Pyra-
mid and ROUGE are techniques looking for con-
tent units repeated in different model summaries,
i.e., summary content units (SCUs) such as clauses
and noun phrases for the Pyramid method, and n-
grams for ROUGE. The underlying hypothesis is
that different model sentences, clauses, or phrases
may convey the same meaning, which is a reason-
able assumption when dealing with reference sum-
maries produced by different authors, since it is
quite unlikely that any two abstractors would use
the exact same words to convey the same idea.
Our situation is however quite different, since
all model summaries of a given document are ut-
terance extracts of that same document, as this can
been seen in the excerpt of Figure 2. In our own
annotation of three meetings with SCUs defined
as in (Nenkova and Passonneau, 2004a), we found
that repetitions and reformulation of the same in-
formation are particularly infrequent, and that tex-
tual units that express the same content among
model summaries are generally originating from
the same document sentence (e.g., in the figure,
the first sentence in model 1 and 2 emanate from
the same document sentence). Very short SCUs
(e.g., base noun phrases) sometimes appeared in
different locations of a meeting, but we think it is
problematic to assume that connections between
such short units are indicative of any similarity
of sentential meaning: the contexts are different,
and words may be uttered by different speakers,
which may lead to unrelated or conflicting prag-
matic forces. For instance, an SCU realized as
?DC offset? and ?DC component? appears in two
different sentences in the figure, i.e. those iden-
tified as 1-13 and 31-41. However, the two sen-
tences have contradictory meanings, and it would
be unfortunate to increase the score of a peer sum-
mary containing the former sentence because the
369
latter is included in some model summaries.
For all these reasons, we believe that sum-
marization evaluation in our case should rely on
the following restrictive matching: two summary
units should be considered equivalent if and only
if they are extracted from the same location in
the original document (e.g., the ?DC? appearing
in models 1 and 2 is not the same as the ?DC? in
the peer summary, since they are extracted from
different sentences). This constraint on the match-
ing is reflected in our Pyramid evaluation, and we
define an SCU as a word and its document po-
sition, which lets us distinguish (?DC?,11) from
(?DC?,33). While this restriction on SCUs forces
us to disregard scarcely occurring paraphrases and
repetitions of the same information, it provides the
benefit of automated evaluation.
Once all SCUs have been identified, the Pyra-
mid method is applied as in (Nenkova and Passon-
neau, 2004b): we compute a scoreD by adding for
each SCU present in the summary a score equal
to the number of model summaries in which that
SCU appears. The Pyramid score P is computed
by dividing D by the maximum D? value that is
obtainable given the constraint on length. For in-
stance, the peer summary in the figure gets a score
D = 9 (since the 9 SCUs in range 43-51 occur in
one model), and the maximum obtainable score is
D? = 44 (all SCUs of the optimal summary ap-
pear in exactly two model summaries), hence the
peer summary?s score is P = .204.
While our evaluation scheme is similar to com-
paring the binary predictions of model and peer
summaries?each prediction determining whether
a given transcription word is included or not?
and averaging precision scores over all peer-model
pairs, the Pyramid evaluation differs on an im-
portant point, which makes us prefer the Pyramid
evaluation method: the maximum possible Pyra-
mid score is always guaranteed to be 1, but av-
erage precision scores can become arbitrarily low
as the consensus between summary annotators de-
creases. For instance, the average precision score
of the optimal summary in the figure is PR = 23 .
2
2Precision scores of the optimal summary compared
against the the three model summaries are .5, 1, and .5, re-
spectively, and hence average 23 . We can show that P =
PR/PR?, where PR? is the average precision of the op-
timal summary. Lack of space prevent us from providing a
proof, so we will just show that the equality holds in our ex-
ample: since the peer summary?s precision scores against the
three model summaries are respectively 922 , 0, and 0, we have
PR/PR? = ( 966 )/(
2
3 ) =
9
44 = P .
FEATURE F?=1
1 utterance duration .246
2 100-dimension LSA .268
3 duration of utterance t? 1 .275
4 time between utterances s and d = t .281
5 IDF mean .284
6 meeting position .286
7 number of APs initiated in t .288
8 duration of utterance t + 1 .288
9 number of fillers .289
10 .25-quantile of energy .290
11 number of lexical repeats .292
12 lexical cohesion score .294
13 f0 mean of last word of utterance t .294
14 LSA 50 dimensions .295
15 utterances (t,t + 1) by same speaker .298
16 speech rate .302
17 ?is that? .303
18 ?for the? .303
19 (ut?1,ut) by same speaker .305
20 ?to try? .305
21 ?meetings? .305
22 utterance starts with ?and? .306
23 ?we have? .306
24 ?new? .307
25 utterance starts with ?what? .307
Table 4: Forward feature selection.
In the case of the six test meetings, which all have
either 3 or 4 model summaries, the maximum pos-
sible average precision is .6405.
8 Experiments
We follow (Murray et al, 2005) in using the same
six meetings as test data, since each of these meet-
ings has multiple reference summaries. The re-
maining 69 meetings were used for training, which
represent in total more than 103,000 training in-
stances (or DA units), of which 6,464 are posi-
tives (6.24%). The multi-reference test set con-
tains more than 28,000 instances.
The goal of a preliminary experiment was to de-
vise a set of useful predictors from a full set of
1171. We performed feature selection by incre-
mentally growing a log-linear model with order-
0 features f(x, yt) using a forward feature selec-
tion procedure similar to (Berger et al, 1996).
Probably due to the imbalance between positive
and negative samples, we found it more effective
to rank candidate features by gains in F -measure
(through 5-fold cross validation on the entire train-
ing set). The increase inF1 by adding new features
to the model is displayed in Table 4; this greedy
search resulted in a set S of 217 features.
We now analyze the performance of different
sequence models on our test set. The target length
of each summary was set to 12.7% of the number
of words of the full document, which is the aver-
370
age on the entire training data (the average on the
test data is 12.9%). In Table 5, we use an order-0
CRF to compare S against all features and various
categorical groupings. Overall, we notice lexical
predictors and statistics derived from them (e.g.
LSA features) represent the most helpful feature
group (.497), though all other features combined
achieve a competitive performance (.476).
Table 6 displays performance for sequence
models incorporating linear-chain features of in-
creasing order k. Its second column indicates
what criterion was used to rank utterances. In the
case of ?pred?, we used actual model {?1, 1} pre-
dictions, which in all cases generated summaries
much shorted than the allowable length, and pro-
duced poor performance. ?Costs? and ?norm-CRF?
refer to the two ranking criteria presented in Sec-
tion 5, and it is clear that the performance of CRFs
degrades with increasing orders without local nor-
malization. While the contingency counts in Ta-
ble 2 only hinted a limited benefit of linear-chain
features, empirical results show the contrary?
especially for order k = 2. However, the further
increase of k causes overfitting, and skip-chain
features seem a better way to capture non-local
dependencies while keeping the number of model
parameters relatively small. Overall, the addition
of skip-chain edges to linear-chain models provide
noticeable improvement in Pyramid scores. Our
system that performed best on cross-validation
data is an order-2 CRF with skip-chain transitions,
which achieves a Pyramid score of P = .554.
We now assess the significance of our results
by comparing our best system against: (1) a lead
summarizer that always selects the first N utter-
ances to match the predefined length; (2) human
performance, which is obtained by leave-one-out
comparisons among references (Table 7); (3) ?op-
timal? summaries generated using the procedure
explained in (Nenkova and Passonneau, 2004b)
by ranking document utterances by the number of
model summaries in which they appear. It ap-
pears that our system is considerably better than
the baseline, and achieves 91.3% of human per-
formance in terms of Pyramid scores, and 83% if
using ASR transcription. This last result is partic-
ularly positive if we consider our strong reliance
on lexical features.
For completeness, we also included standard
ROUGE (1, 2, and L) scores in Table 7, which
were obtained using parameters defined for the
FEATURE SET P
lexical .471
IR .415
lexical + IR .497
acoustic .407
structural/durational .478
acoustic + structural/durational .476
all features .507
selected features (S) .515
Table 5: Pyramid score for each feature set.
MODEL RANKING k = 1 2 3
linear-chain BN pred .241 .267 .269
linear-chain BN costs .512 .519 .525
skip-chain BN costs .543 .549 .542
linear-chain CRF pred .326 .36 .348
linear-chain CRF costs .508 .475 .447
linear-chain CRF norm-CRF .53 .548 .54
skip-chain CRF norm-CRF .541 .554 .559
Table 6: Pyramid scores for different sequence models, where
k stands for the order of linear-chain features. The value in
bold is the performance of the model that was selected after
a 5-fold cross validation on the training data, which obtained
the highest F1 score.
SUMMARIZER P R-1 R-2 R-L
baseline .188 .501 .210 .495
skip-chain CRF (transcript) .554 .715 .442 .709
skip-chain CRF (ASR) .504 .714 .42 .706
human .607 .720 .477 .715
optimal 1 .791 .648 .788
Table 7: Pyramid, and average ROUGE scores for summaries
produces by a baseline (lead summarizer), our best system,
humans, and the optimal summarizer.
DUC-05 evaluation. Since system summaries
have on average approximately the same length
as references, we only report recall measures of
ROUGE (precision and F averages are within ?
.002).3 It may come as a surprise that our best sys-
tem (both with ASR and true words) performs al-
most as well as humans; it seems more reasonable
to conclude that, in our case, ROUGE has trouble
discriminating between systems with moderately
close performance. This seems to confirm our im-
pression that content evaluation in our task should
be based on exact matches.
We performed a last experiment to compare our
best system against Murray et al (2005), who used
the same test data, but constrained summary sizes
in terms of number of DA units instead of words.
In their experiments, 10% of DAs had to be se-
lected. Our system achieves .91 recall, .5 preci-
sion, and .64 F1 with the same length constraint.
3Human performance with ROUGE was assessed by
cross-validating reference summaries of each meeting (i.e.,
n references for a given meeting resulted in n evaluations
against the other references). We used the same leave-one-
out procedure with other summarizers, in order to get results
comparable to humans.
371
The discrepancy between recall and precision is
largely due to the fact that generated summaries
are on average much longer than model summaries
(10% vs. 6.26% of DAs), which explains why our
precision is relatively low in this last evaluation.
The best ROUGE-1 measure reported in (Murray
et al, 2005) is .69 recall, which is significantly
lower than ours according to confidence intervals.
9 Conclusion
An order-2 CRF with skip-chain dependencies de-
rived from the automatic analysis of participant
interaction was shown to outperform linear-chain
BNs and CRFs, despite the incorporation in all
cases of the same competitive set of predictors
resulting from cross-validated feature selection.
Compared to an order-0 CRF model, the absolute
increase in performance is 3.9% (7.5% relative in-
crease), which indicates that it is helpful to use
skip-chain sequence models in the summarization
task. Our best performing system reaches 91.3%
of human performance, and scales relatively well
on automatic speech recognition output.
Acknowledgments
This work has benefited greatly from suggestions
and advice from Kathleen McKeown. I also would
like to thank Jean Carletta, Steve Renals and
Gabriel Murray for giving me access to their sum-
marization corpus, Ani Nenkova for helpful dis-
cussions about summarization evaluation, Michael
Collins, Daniel Ellis, Julia Hirschberg, and Owen
Rambow for useful preliminary discussions, and
three anonymous reviewers for their insightful
comments on an earlier version of this paper.
References
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A max-
imum entropy approach to natural language processing.
Computational Linguistics, 22(1):39?72.
P. Boersma and D. Weenink. 2006. Praat: doing phonetics
by computer. http://www.praat.org/.
J. Conroy, J. Schlesinger, J. Goldstein, and D. O?Leary. 2004.
Left-brain/right-brain multi-document summarization. In
DUC 04 Conference Proceedings.
H.P. Edmundson. 1968. New methods in automatic extract-
ing. Journal of the ACM, 16(2):264?285.
J. Finkel, T. Grenager, and C. Manning. 2005. Incorporating
non-local information into information extraction systems
by gibbs sampling. In Proc. of ACL, pages 363?370.
M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg.
2004. Identifying agreement and disagreement in conver-
sational speech: Use of bayesian networks to model prag-
matic dependencies. In Proc. of ACL, pages 669?676.
M. Hearst. 1994. Multi-paragraph segmentation of exposi-
tory text. In Proc. of ACL, pages 9?16.
A. Inoue, T. Mikami, and Y. Yamashita. 2004. Improvement
of speech summarization using prosodic information. In
Proc. of Speech Prosody.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Mor-
gan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and
C. Wooters. 2003. The ICSI meeting corpus. In Proc.
of ICASSP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and
labeling sequence data. In Proc. of ICML, pages 282?289.
C.-Y. Lin. 2004. ROUGE: a package for automatic evalua-
tion of summaries. In Proc. of workshop on text summa-
rization, ACL-04.
I. Mani and M. Maybury. 1999. Advances in Automatic Text
Summarization. MIT Press.
S. Maskey and J. Hirschberg. 2005. Comparing lexial,
acoustic/prosodic, discourse and structural features for
speech summarization. In Proc. of Eurospeech.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy markov models for information extraction
and segmentation. In Proc. of ICML.
A. McCallum. 2002. MALLET: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
N. Mirghafori, A. Stolcke, C. Wooters, T. Pirinen, I. Bulyko,
D. Gelbart, M. Graciarena, S. Otterson, B. Peskin, and
M. Ostendorf. 2004. From switchboard to meetings: De-
velopment of the 2004 ICSI-SRI-UW meeting recognition
system. In Proc. of ICSLP.
G. Murray, S. Renals, J. Carletta, and J. Moore. 2005. Eval-
uating automatic summaries of meeting recordings. In
Proc. of the ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization.
A. Nenkova and R. Passonneau. 2004a. Evaluating con-
tent selection in human- or machine-generated summaries:
The pyramid scoring method. Technical Report CUCS-
025-03, Columbia University, CS Department.
A. Nenkova and R. Passonneau. 2004b. Evaluating con-
tent selection in summarization: The pyramid method. In
Proc. of HLT/NAACL, pages 145?152.
L. Rabiner. 1989. A tutorial on hidden markov models and
selected applications in speech recogntion. Proc. of the
IEEE, 77(2):257?286.
O. Rambow, L. Shrestha, J. Chen, and C. Lauridsen. 2004.
Summarizing email threads. In Proc. of HLT-NAACL.
E. Schegloff and H. Sacks. 1973. Opening up closings.
Semiotica, 7-4:289?327.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of NAACL, pages 134?141.
L. Shrestha and K. McKeown. 2004. Detection of question-
answer pairs in email conversations. In Proc. of COLING,
pages 889?895.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey.
2004. The ICSI meeting recorder dialog act (MRDA) cor-
pus. In SIGdial Workshop on Discourse and Dialogue,
pages 97?100.
C. Sutton and A. McCallum. 2004. Collective segmenta-
tion and labeling of distant entities in information extrac-
tion. Technical Report TR # 04-49, University of Mas-
sachusetts.
K. Zechner. 2002. Automatic summarization of open domain
multi-party dialogues in diverse genres. Computational
Liguistics, 28(4):447?485.
L. Zhou and E. Hovy. 2005. Digesting virtual ?geek? culture:
The summarization of technical internet relay chats. In
Proc. of ACL, pages 298?305.
372
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 38?49,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Optimal Search for Minimum Error Rate Training
Michel Galley
Microsoft Research
Redmond, WA 98052, USA
mgalley@microsoft.com
Chris Quirk
Microsoft Research
Redmond, WA 98052, USA
chrisq@microsoft.com
Abstract
Minimum error rate training is a crucial compo-
nent to many state-of-the-art NLP applications,
such as machine translation and speech recog-
nition. However, common evaluation functions
such as BLEU or word error rate are generally
highly non-convex and thus prone to search
errors. In this paper, we present LP-MERT, an
exact search algorithm for minimum error rate
training that reaches the global optimum using
a series of reductions to linear programming.
Given a set of N -best lists produced from S
input sentences, this algorithm finds a linear
model that is globally optimal with respect to
this set. We find that this algorithm is poly-
nomial in N and in the size of the model, but
exponential in S. We present extensions of this
work that let us scale to reasonably large tuning
sets (e.g., one thousand sentences), by either
searching only promising regions of the param-
eter space, or by using a variant of LP-MERT
that relies on a beam-search approximation.
Experimental results show improvements over
the standard Och algorithm.
1 Introduction
Minimum error rate training (MERT)?also known
as direct loss minimization in machine learning?is a
crucial component in many complex natural language
applications such as speech recognition (Chou et al,
1993; Stolcke et al, 1997; Juang et al, 1997), statisti-
cal machine translation (Och, 2003; Smith and Eisner,
2006; Duh and Kirchhoff, 2008; Chiang et al, 2008),
dependency parsing (McDonald et al, 2005), summa-
rization (McDonald, 2006), and phonetic alignment
(McAllester et al, 2010). MERT directly optimizes
the evaluation metric under which systems are being
evaluated, yielding superior performance (Och, 2003)
when compared to a likelihood-based discriminative
method (Och and Ney, 2002). In complex text gener-
ation tasks like SMT, the ability to optimize BLEU
(Papineni et al, 2001), TER (Snover et al, 2006), and
other evaluation metrics is critical, since these met-
rics measure qualities (such as fluency and adequacy)
that often do not correlate well with task-agnostic
loss functions such as log-loss.
While competitive in practice, MERT faces several
challenges, the most significant of which is search.
The unsmoothed error count is a highly non-convex
objective function and therefore difficult to optimize
directly; prior work offers no algorithm with a good
approximation guarantee. While much of the ear-
lier work in MERT (Chou et al, 1993; Juang et al,
1997) relies on standard convex optimization tech-
niques applied to non-convex problems, the Och al-
gorithm (Och, 2003) represents a significant advance
for MERT since it applies a series of special line min-
imizations that happen to be exhaustive and efficient.
Since this algorithm remains inexact in the multidi-
mensional case, much of the recent work on MERT
has focused on extending Och?s algorithm to find
better search directions and starting points (Cer et al,
2008; Moore and Quirk, 2008), and on experiment-
ing with other derivative-free methods such as the
Nelder-Mead simplex algorithm (Nelder and Mead,
1965; Zens et al, 2007; Zhao and Chen, 2009).
In this paper, we present LP-MERT, an exact
search algorithm for N -best optimization that ex-
ploits general assumptions commonly made with
MERT, e.g., that the error metric is decomposable
by sentence.1 While there is no known optimal algo-
1Note that MERT makes two types of approximations. First,
the set of all possible outputs is represented only approximately,
by N -best lists, lattices, or hypergraphs. Second, error func-
tions on such representations are non-convex and previous work
only offers approximate techniques to optimize them. Our work
avoids the second approximation, while the first one is unavoid-
able when optimization and decoding occur in distinct steps.
38
rithm to optimize general non-convex functions, the
unsmoothed error surface has a special property that
enables exact search: the set of translations produced
by an SMT system for a given input is finite, so the
piecewise-constant error surface contains only a fi-
nite number of constant regions. As in Och (2003),
one could imagine exhaustively enumerating all con-
stant regions and finally return the best scoring one?
Och does this efficiently with each one-dimensional
search?but the idea doesn?t quite scale when search-
ing all dimensions at once. Instead, LP-MERT ex-
ploits algorithmic devices such as lazy enumeration,
divide-and-conquer, and linear programming to effi-
ciently discard partial solutions that cannot be max-
imized by any linear model. Our experiments with
thousands of searches show that LP-MERT is never
worse than the Och algorithm, which provides strong
evidence that our algorithm is indeed exact. In the
appendix, we formally prove that this search algo-
rithm is optimal. We show that this algorithm is
polynomial in N and in the size of the model, but
exponential in the number of tuning sentences. To
handle reasonably large tuning sets, we present two
modifications of LP-MERT that either search only
promising regions of the parameter space, or that rely
on a beam-search approximation. The latter modifica-
tion copes with tuning sets of one thousand sentences
or more, and outperforms the Och algorithm on a
WMT 2010 evaluation task.
This paper makes the following contributions. To
our knowledge, it is the first known exact search
algorithm for optimizing task loss on N -best lists in
general dimensions. We also present an approximate
version of LP-MERT that offers a natural means of
trading speed for accuracy, as we are guaranteed to
eventually find the global optimum as we gradually
increase beam size. This trade-off may be beneficial
in commercial settings and in large-scale evaluations
like the NIST evaluation, i.e., when one has a stable
system and is willing to let MERT run for days or
weeks to get the best possible accuracy. We think this
work would also be useful as we turn to more human
involvement in training (Zaidan and Callison-Burch,
2009), as MERT in this case is intrinsically slow.
2 Unidimensional MERT
Let fS1 = f1 . . . fS denote the S input sentences
of our tuning set. For each sentence fs, let Cs =
es,1 . . . es,N denote a set of N candidate translations.
For simplicity and without loss of generality, we
assume that N is constant for each index s. Each
input and output sentence pair (fs, es,n) is weighted
by a linear model that combines model parameters
w = w1 . . . wD ? RD with D feature functions
h1(f , e,?) . . . hD(f , e,?), where ? is the hidden
state associated with the derivation from f to e, such
as phrase segmentation and alignment. Furthermore,
let hs,n ? RD denote the feature vector representing
the translation pair (fs, es,n).
In MERT, the goal is to minimize an error count
E(r, e) by scoring translation hypotheses against a
set of reference translations rS1 = r1 . . . rS . As-
suming as in Och (2003) that error count is addi-
tively decomposable by sentence?i.e., E(rS1 , eS1 ) =?
sE(rs, es)?this results in the following optimiza-
tion problem:2
w? = argminw
{ S?
s=1
E(rs, e?(fs;w))
}
= argminw
{ S?
s=1
N?
n=1
E(rs, es,n)?(es,n, e?(fs;w))
}
(1)where
e?(fs;w) = argmax
n?{1...N}
{w?hs,n
}
The quality of this approximation is dependent on
how accurately the N -best lists represent the search
space of the system. Therefore, the hypothesis list is
iteratively grown: decoding with an initial parameter
vector seeds the N -best lists; next, parameter esti-
mation and N -best list gathering alternate until the
search space is deemed representative.
The crucial observation of Och (2003) is that the
error count along any line is a piecewise constant
function. Furthermore, this function for a single sen-
tence may be computed efficiently by first finding the
hypotheses that form the upper envelope of the model
score function, then gathering the error count for each
hypothesis along the range for which it is optimal. Er-
ror counts for the whole corpus are simply the sums
of these piecewise constant functions, leading to an
2A metric such as TER is decomposable by sentence. BLEU
is not, but its sufficient statistics are, and the literature offers
several sentence-level approximations of BLEU (Lin and Och,
2004; Liang et al, 2006).
39
efficient algorithm for finding the global optimum of
the error count along any single direction.
Such a hill-climbing algorithm in a non-convex
space has no optimality guarantee: without a perfect
direction finder, even a globally-exact line search may
never encounter the global optimum. Coordinate as-
cent is often effective, though conjugate direction set
finding algorithms, such as Powell?s method (Powell,
1964; Press et al, 2007), or even random directions
may produce better results (Cer et al, 2008). Ran-
dom restarts, based on either uniform sampling or a
random walk (Moore and Quirk, 2008), increase the
likelihood of finding a good solution. Since random
restarts and random walks lead to better solutions
and faster convergence, we incorporate them into our
baseline system, which we refer to as 1D-MERT.
3 Multidimensional MERT
Finding the global optimum of Eq. 1 is a difficult
task, so we proceed in steps and first analyze the
case where the tuning set contains only one sentence.
This gives insight on how to solve the general case.
With only one sentence, one of the two summations
in Eq. 1 vanishes and one can exhaustively enumer-
ate the N translations e1,n (or en for short) to find
the one that yields the minimal task loss. The only
difficulty with S = 1 is to know for each translation
en whether its feature vector h1,n (or hn for short)
can be maximized using any linear model. As we
can see in Fig. 1(a), some hypotheses can be maxi-
mized (e.g., h1, h2, and h4), while others (e.g., h3
and h5) cannot. In geometric terminology, the former
points are commonly called extreme points, and the
latter are interior points.3 The problem of exactly
optimizing a single N -best list is closely related to
the convex hull problem in computational geometry,
for which generic solvers such as the QuickHull al-
gorithm exist (Eddy, 1977; Bykat, 1978; Barber et
al., 1996). A first approach would be to construct the
convex hull conv(h1 . . .hN ) of the N -best list, then
identify the point on the hull with lowest loss (h1 in
Fig. 1) and finally compute an optimal weight vector
using hull points that share common facets with the
3Specifically, a point h is extreme with respect to a convex
set C (e.g., the convex hull shown in Fig. 1(a)) if it does not lie
in an open line segment joining any two points of C. In a minor
abuse of terminology, we sometimes simply state that a given
point h is extreme when the nature of C is clear from context.
w h1 h3: 0.41 
h1: 0.43 h4: 0.48 
h5: 0.46 h2: 0.51 
LM 
CM 
(a) (b) 
Figure 1: N -best list (h1 . . .hN ) with associated losses
(here, TER scores) for a single input sentence, whose
convex hull is displayed with dotted lines in (a). For effec-
tive visualization, our plots use only two features (D = 2).
While we can find a weight vector that maximizes h1 (e.g.,
the w in (b)), no linear model can possibly maximize any
of the points strictly inside the convex hull.
optimal feature vector (h2 and h4). Unfortunately,
this doesn?t quite scale even with a single N -best list,
since the best known convex hull algorithm runs in
O(N bD/2c+1) time (Barber et al, 1996).4
Algorithms presented in this paper assume that D
is unrestricted, therefore we cannot afford to build
any convex hull explicitly. Thus, we turn to linear
programming (LP), for which we know algorithms
(Karmarkar, 1984) that are polynomial in the number
of dimensions and linear in the number of points, i.e.,
O(NT ), where T = D3.5. To check if point hi is
extreme, we really only need to know whether we can
define a half-space containing all points h1 . . .hN ,
with hi lying on the hyperplane delimiting that half-
space, as shown in Fig. 1(b) for h1. Formally, a
vertex hi is optimal with respect to argmaxi{w?hi}
if and only if the following constraints hold:5
w?hi = y (2)
w?hj ? y, for each j 6= i (3)
w is orthogonal to the hyperplane defining the half-
space, and the intercept y defines its position. The
4A convex hull algorithm polynomial in D is very unlikely.
Indeed, the expected number of facets of high-dimensional con-
vex hulls grows dramatically, and?assuming a uniform distribu-
tion of points, D = 10, and a sufficiently large N?the expected
number of facets is approximately 106N (Buchta et al, 1985).
In the worst case, the maximum number of facets of a convex
hull is O(NbD/2c/bD/2c!) (Klee, 1966).
5A similar approach for checking whether a given point is
extreme is presented in http://www.ifor.math.ethz.
ch/?fukuda/polyfaq/node22.html, but our method
generates slightly smaller LPs.
40
above equations represent a linear program (LP),
which can be turned into canonical form
maximize c? w
subject to Aw ? b
by substituting y with w?hi in Eq. 3, by defining
A = {an,d}1?n?N ;1?d?D with an,d = hj,d ? hi,d
(where hj,d is the d-th element of hj), and by setting
b = (0, . . . , 0)? = 0. The vertex hi is extreme if
and only if the LP solver finds a non-zero vector w
satisfying the canonical system. To ensure that w is
zero only when hi is interior, we set c = hi ? h?,
where h? is a point known to be inside the hull (e.g.,
the centroid of the N -best list).6 In the remaining
of this section, we use this LP formulation in func-
tion LINOPTIMIZER(hi;h1 . . .hN ), which returns
the weight vector w? maximizing hi, or which returns
0 if hi is interior to conv(h1 . . .hN ). We also use
conv(hi;h1 . . .hN ) to denote whether hi is extreme
with respect to this hull.
Algorithm 1: LP-MERT (for S = 1).
input : sent.-level feature vectors H = {h1 . . .hN}
input : sent.-level task losses E1 . . . EN , where
En := E(r1, e1,n)
output :optimal weight vector w?
1 begin
. sort N -best list by increasing losses:
2 (i1 . . . iN )? INDEXSORT(E1 . . . EN )
3 for n? 1 to N do
. find w? maximizing in-th element:
4 w?? LINOPTIMIZER(hin ;H)
5 if w? 6= 0 then
6 return w?
7 return 0
An exact search algorithm for optimizing a single
N -best list is shown above. It lazily enumerates fea-
ture vectors in increasing order of task loss, keeping
only the extreme ones. Such a vertex hj is known to
be on the convex hull, and the returned vector w? max-
imizes it. In Fig. 1, it would first run LINOPTIMIZER
on h3, discard it since it is interior, and finally accept
the extreme point h1. Each execution of LINOPTI-
MIZER requires O(NT ) time with the interior point
6We assume that h1 . . .hN are not degenerate, i.e., that they
collectively span RD . Otherwise, all points are necessarily on
the hull, yet some of them may not be uniquely maximized.
0.001
0.01
0.1
1
10
100
1000
10000
100000
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
QuickHull
LP
Dimensions 
Sec
ond
s 
Figure 2: Running times to exactly optimize N -best lists
with an increasing number of dimensions. To determine
which feature vectors were on the hull, we use either linear
programming (Karmarkar, 1984) or one of the most effi-
cient convex hull computation tools (Barber et al, 1996).
method of (Karmarkar, 1984), and since the main
loop may run O(N) times in the worst case, time
complexity is O(N2T ). Finally, Fig. 2 empirically
demonstrates the effectiveness of a linear program-
ming approach, which in practice is seldom affected
by D.
3.1 Exact search: general case
We now extend LP-MERT to the general case, in
which we are optimizing multiple sentences at once.
This creates an intricate optimization problem, since
the inner summations over n = 1 . . . N in Eq. 1
can?t be optimized independently. For instance,
the optimal weight vector for sentence s = 1 may
be suboptimal with respect to sentence s = 2.
So we need some means to determine whether a
selection m = m(1) . . .m(S) ? M = [1, N ]S of
feature vectors h1,m(1) . . .hS,m(S) is extreme, that is,
whether we can find a weight vector that maximizes
each hs,m(s). Here is a reformulation of Eq. 1 that
makes this condition on extremity more explicit:
m? = argmin
conv(h[m];H)
m?M
{ S?
s=1
E(rs, es,m(n))
}
(4)
where
h[m] =
S?
s=1
hs,m(s)
H =
?
m??M
h[m?]
41
One na??ve approach to address this optimization
problem is to enumerate all possible combinations
among the S distinct N -best lists, determine for each
combination m whether h[m] is extreme, and return
the extreme combination with lowest total loss. It is
evident that this approach is optimal (since it follows
directly from Eq. 4), but it is prohibitively slow since
it processes O(NS) vertices to determine whether
they are extreme, which thus requires O(NST ) time
per LP optimization and O(N2ST ) time in total. We
now present several improvements to make this ap-
proach more practical.
3.1.1 Sparse hypothesis combination
In the na??ve approach presented above, each LP
computation to evaluate conv(h[m];H) requires
O(NST ) time since H contains NS vertices, but
we show here how to reduce it to O(NST ) time.
This improvement exploits the fact that we can elimi-
nate the majority of the NS points of H , since only
S(N ?1)+1 are really needed to determine whether
h[m] is extreme. This is best illustrated using an ex-
ample, as shown in Fig. 3. Both h1,1 and h2,1 in (a)
and (b) are extreme with respect to their own N -best
list, and we ask whether we can find a weight vector
that maximizes both h1,1 and h2,1. The algorith-
mic trick is to geometrically translate one of the two
N -best lists so that h1,1 = h?2,1, where h?2,1 is the
translation of h?2,1. Then we use linear programming
with the new set of 2N ? 1 points, as shown in (c), to
determine whether h1,1 is on the hull, in which case
the answer to the original question is yes. In the case
of the combination of h1,1 and h2,2, we see in (d) that
the combined set of points prevents the maximization
h1,1, since this point is clearly no longer on the hull.
Hence, the combination (h1,1,h2,2) cannot be maxi-
mized using any linear model. This trick generalizes
to S ? 2. In both (c) and (d), we used S(N ? 1) + 1
points instead of NS to determine whether a given
point is extreme. We show in the appendix that this
simplification does not sacrifice optimality.
3.1.2 Lazy enumeration, divide-and-conquer
Now that we can determine whether a given combi-
nation is extreme, we must next enumerate candidate
combinations to find the combination that has low-
est task loss among all of those that are extreme.
Since the number of feature vector combinations is
O(NS), exhaustive enumeration is not a reasonable
h1,1  h2,2  h2,1  
(a) (b) 
h1,1  h?2,2  
(c)  (d)  
h1,1 h?2,1  
Figure 3: Given two N -best lists, (a) and (b), we use
linear programming to determine which hypothesis com-
binations are extreme. For instance, the combination h1,1
and h2,1 is extreme (c), while h1,1 and h2,2 is not (d).
option. Instead, we use lazy enumeration to pro-
cess combinations in increasing order of task loss,
which ensures that the first extreme combination for
s = 1 . . . S that we encounter is the optimal one. An
S-ary lazy enumeration would not be particularly ef-
ficient, since the runtime is still O(NS) in the worst
case. LP-MERT instead uses divide-and-conquer
and binary lazy enumeration, which enables us to
discard early on combinations that are not extreme.
For instance, if we find that (h1,1,h2,2) is interior for
sentences s = 1, 2, the divide-and-conquer branch
for s = 1 . . . 4 never actually receives this bad com-
bination from its left child, thus avoiding the cost
of enumerating combinations that are known to be
interior, e.g., (h1,1,h2,2,h3,1,h4,1).
The LP-MERT algorithm for the general case is
shown as Algorithm 2. It basically only calls a re-
cursive divide-and-conquer function (GETNEXTBEST)
for sentence range 1 . . . S. The latter function uses bi-
nary lazy enumeration in a manner similar to (Huang
and Chiang, 2005), and relies on two global variables:
I and L. The first of these, I , is used to memoize the
results of calls to GETNEXTBEST; given a range of
sentences and a rank n, it stores the nth best combina-
tion for that range of sentences. The global variable
L stores hypotheses combination matrices, one ma-
trix for each range of sentences (s, t) as shown in
42
h11 h12 
h21 h22 h23 
69.1 69.2 69.3 69.2 69.4 h31 h32 h33 
h41 h42 
56.8 57.1 57.3 57.6 
h23 
57.9 
{h11, h23} 
{h31, h41} 
126.0 126.5 
126.1 
{h32, h41} 
{h12, h21} 
Combinations checked:  {h11, h23, h31, h41} {h12, h21, h31, h41} 
Combinations discarded:  {h11, h21, h31, h41} {h12, h22, h31, h41} {h12, h12, h31, h42} (and 7 others) h13 
h24 
69.9 70.0 
L[3,4] L[1,2] 
Figure 4: LP-MERT minimizes loss (TER) on four sen-
tences. O(N4) translation combinations are possible,
but the LP-MERT algorithm only tests two full combi-
nations. Without divide-and-conquer?i.e., using 4-ary
lazy enumeration?ten full combinations would have been
checked unnecessarily.
Algorithm 2: LP-MERT
input : feature vectors H = {hs,n}1?s?S;1?n?N
input : task losses E = {Es,n}1?s?S;1?n?N ,
where sent.-level costs Es,n := E(rs, es,n)
output :optimal weight vector w? and its loss L
1 begin
. sort N -best lists by increasing losses:
2 for s? 1 to S do
3 (is,1..is,N )? INDEXSORT(Es,1..Es,N )
. find best hypothesis combination for 1 . . . S:
4 (h?, H?, L)? GETNEXTBEST(H,E, 1, S)
5 w?? LINOPTIMIZER(h?;H?)
6 return (w?, L)
Fig. 4, to determine which combination to try next.
The function EXPANDFRONTIER returns the indices
of unvisited cells that are adjacent (right or down) to
visited cells and that might correspond to the next
best hypothesis. Once no more cells need to be added
to the frontier, LP-MERT identifies the lowest loss
combination on the frontier (BESTINFRONTIER), and
uses LP to determine whether it is extreme. To do so,
it first generates an LP using COMBINE, a function
that implements the method described in Fig. 3. If
the LP offers no solution, this combination is ignored.
LP-MERT iterates until it finds a cell entry whose
combination is extreme. Regarding ranges of length
one (s = t), lines 3-10 are similar to Algorithm 1 for
S = 1, but with one difference: GETNEXTBEST may
be called multiple times with the same argument s,
since the first output of GETNEXTBEST might not be
extreme when combined with other feature vectors.
Lines 3-10 of GETNEXTBEST handle this case effi-
ciently, since the algorithm resumes at the (n+1)-th
Function GetNextBest(H,E,s,t)
input : sentence range (s, t)
output :h?: current best extreme vertex
output :H?: constraint vertices
output :L: task loss of h?
. Losses of partial hypotheses:
1 L? L[s, t]
2 if s = t then
. n is the index where we left off last time:
3 n? NBROWS(L)
4 Hs ? {hs,1 . . .hs,N}
5 repeat
6 n? n+ 1
7 w?? LINOPTIMIZER(hs,in ;Hs)
8 L[n, 1]? Es,in
9 until w? 6= 0
10 return (hs,in , Hs,L[n, 1])
11 else
12 u? b(s+ t)/2c, v ? u+ 1
13 repeat
14 while HASINCOMPLETEFRONTIER(L) do
15 (m,n)? EXPANDFRONTIER(L)
16 x? NBROWS(L)
17 y ? NBCOLUMNS(L)
18 form? ? x+ 1 tom do
19 I[s, u,m?]? GETNEXTBEST(H,E, s, u)
20 for n? ? y + 1 to n do
21 I[v, t, n?]? GETNEXTBEST(H,E, v, t)
22 L[m,n]? LOSS(I[s, u,m])+LOSS(I[v, t, n])
23 (m,n)? BESTINFRONTIER(L)
24 (hm, Hm, Lm)? I[s, u,m]
25 (hn, Hn, Ln)? I[v, t, n]
26 (h?, H?)? COMBINE(hm, Hm,hn, Hn)
27 w?? LINOPTIMIZER(h?;H?)
28 until w? 6= 0
29 return (h?, H?,L[m,n])
element of the N -best list (where n is the position
where the previous execution left off).7 We can see
that a strength of this algorithm is that inconsistent
combinations are deleted as soon as possible, which
allows us to discard fruitless candidates en masse.
3.2 Approximate Search
We will see in Section 5 that our exact algorithm
is often too computationally expensive in practice
to be used with either a large number of sentences
or a large number of features. We now present two
7Each N -best list is augmented with a placeholder hypothesis
with loss +?. This ensures n never runs out of bounds at line 7.
43
Function Combine(h, H,h?, H ?)
input :H,H ?: constraint vertices
input :h,h?: extreme vertices, wrt. H and H ?
output :h?, H?: combination as in Sec. 3.1.1
1 for i? 1 to size(H) do
2 Hi ? Hi + h?
3 for i? 1 to size(H ?) do
4 H ?i ? H ?i + h
5 return (h+ h?, H ?H ?)
approaches to make LP-MERT more scalable, with
the downside that we may allow search errors.
In the first case, we make the assumption that we
have an initial weight vector w0 that is a reasonable
approximation of w?, where w0 may be obtained ei-
ther by using a fast MERT algorithm like 1D-MERT,
or by reusing the weight vector that is optimal with
respect to the previous iteration of MERT. The idea
then is to search only the set of weight vectors that
satisfy cos(w?,w0) ? t, where t is a threshold on
cosine similarity provided by the user. The larger the
t, the faster the search, but at the expense of more
search errors. This is implemented with two simple
changes in our algorithm. First, LINOPTIMIZER sets
the objective vector c = w0. Second, if the output
w? originally returned by LINOPTIMIZER does not
satisfy cos(w?,w0) ? t, then it returns 0. While this
modification of our algorithm may lead to search
errors, it nevertheless provides some theoretical guar-
antee: our algorithm finds the global optimum if it
lies within the region defined by cos(w?,w0) ? t.
The second method is a beam approximation of LP-
MERT, which normally deals with linear programs
that are increasingly large in the upper branches of
GETNEXTBEST?s recursive calls. The main idea is
to prune the output of COMBINE (line 26) by model
score with respect to wbest, where wbest is our cur-
rent best model on the entire tuning set. Note that
beam pruning can discard h? (the current best ex-
treme vertex), in which case LINOPTIMIZER returns
0. wbest is updated as follows: each time we pro-
duce a new non-zero w?, run wbest ? w? if w? has a
lower loss than wbest on the entire tuning set. The
idea of using a beam here is similar to using cosine
similarity (since wbest constrains the search towards
a promising region), but beam pruning also helps
reduce LP optimization time and thus enables us to
explore a wider space. Since wbest often improves
during search, it is useful to run multiple iterations of
LP-MERT until wbest doesn?t change. Two or three
iterations suffice in our experience. In our experi-
ments, we use a beam size of 1000.
4 Experimental Setup
Our experiments in this paper focus on only the ap-
plication of machine translation, though we believe
that the current approach is agnostic to the particular
system used to generate hypotheses. Both phrase-
based systems (e.g., Koehn et al (2007)) and syntax-
based systems (e.g., Li et al (2009), Quirk et al
(2005)) commonly use MERT to train free param-
eters. Our experiments use a syntax-directed trans-
lation approach (Quirk et al, 2005): it first applies
a dependency parser to the source language data at
both training and test time. Multi-word translation
mappings constrained to be connected subgraphs of
the source tree are extracted from the training data;
these provide most lexical translations. Partially lexi-
calized templates capturing reordering and function
word insertion and deletion are also extracted. At
runtime, these mappings and templates are used to
construct transduction rules to convert the source tree
into a target string. The best transduction is sought
using approximate search techniques (Chiang, 2007).
Each hypothesis is scored by a relatively standard
set of features. The mappings contain five features:
maximum-likelihood estimates of source given target
and vice versa, lexical weighting estimates of source
given target and vice versa, and a constant value that,
when summed across a whole hypothesis, indicates
the number of mappings used. For each template,
we include a maximum-likelihood estimate of the
target reordering given the source structure. The
system may fall back to templates that mimic the
source word order; the count of such templates is a
feature. Likewise we include a feature to count the
number of source words deleted by templates, and a
feature to count the number of target words inserted
by templates. The log probability of the target string
according to a language models is also a feature; we
add one such feature for each language model. We
include the number of target words as features to
balance hypothesis length.
For the present system, we use the training data of
WMT 2010 to construct and evaluate an English-to-
44
-1
0
1
2
3
4
5
6
7
0 100 200 300 400 500 600 700 800 900 1000
S=8
S=4
S=2
? 
BL
EU
[%
] 
Figure 5: Line graph of sorted differences in
BLEUn4r1[%] scores between LP-MERT and 1D-MERT
on 1000 tuning sets of size S = 2, 4, 8. The highest differ-
ences for S = 2, 4, 8 are respectively 23.3, 19.7, 13.1.
German translation system. This consists of approx-
imately 1.6 million parallel sentences, along with a
much larger monolingual set of monolingual data.
We train two language models, one on the target side
of the training data (primarily parliamentary data),
and the other on the provided monolingual data (pri-
marily news). The 2009 test set is used as develop-
ment data for MERT, and the 2010 one is used as test
data. The resulting system has 13 distinct features.
5 Results
The section evaluates both the exact and beam ver-
sion of LP-MERT. Unless mentioned otherwise, the
number of features isD = 13 and theN -best list size
is 100. Translation performance is measured with
a sentence-level version of BLEU-4 (Lin and Och,
2004), using one reference translation. To enable
legitimate comparisons, LP-MERT and 1D-MERT
are evaluated on the same combined N -best lists,
even though running multiple iterations of MERT
with either LP-MERT or 1D-MERT would normally
produce different combined N -best lists. We use
WMT09 as tuning set, and WMT10 as test set. Be-
fore turning to large tuning sets, we first evaluate
exact LP-MERT on data sizes that it can easily han-
dle. Fig. 5 offers a comparison with 1D-MERT, for
which we split the tuning set into 1,000 overlapping
subsets for S = 2, 4, 8 on a combined N -best after
five iterations of MERT with an average of 374 trans-
lation per sentence. The figure shows that LP-MERT
never underperforms 1D-MERT in any of the 3,000
experiments, and this almost certainly confirms that
length tested comb. total comb. order
8 639,960 1.33? 1020 O(N8)
4 134,454 2.31? 1010 O(2N4)
2 49,969 430,336 O(4N2)
1 1,059 2,624 O(8N)
Table 1: Number of tested combinations for the experi-
ments of Fig. 5. LP-MERT with S = 8 checks only 600K
full combinations on average, much less than the total
number of combinations (which is more than 1020).
1
10
100
1,000
10,000
2 3 4 5 6 7 8 9
se
co
nd
s 
1024
256
128
64
32
16
8
4
2
1dimension (D) 
Figure 6: Effect of the number of features (runtime on
1 CPU of a modern computer). Each curve represents a
different number of tuning sentences.
LP-MERT systematically finds the global optimum.
In the case S = 1, Powell rarely makes search er-
rors (about 15%), but the situation gets worse as S
increases. For S = 4, it makes search errors in 90%
of the cases, despite using 20 random starting points.
Some combination statistics for S up to 8 are
shown in Tab. 1. The table shows the speedup pro-
vided by LP-MERT is very substantial when com-
pared to exhaustive enumeration. Note that this is
using D = 13, and that pruning is much more ef-
fective with less features, a fact that is confirmed in
Fig. 6. D = 13 makes it hard to use a large tuning
set, but the situation improves with D = 2 . . . 5.
Fig. 7 displays execution times when LP-MERT
constrains the output w? to satisfy cos(w0, w?) ? t,
where t is on the x-axis of the figure. The figure
shows that we can scale to 1000 sentences when
(exactly) searching within the region defined by
cos(w0, w?) ? .84. All these running times would
improve using parallel computing, since divide-and-
conquer algorithms are generally easy to parallelize.
We also evaluate the beam version of LP-MERT,
which allows us to exploit tuning sets of reasonable
45
110
100
1,000
10,000
0.99 0.98 0.96 0.92 0.84 0.68 0.36 -0.28 -1
se
co
nd
s 
1024
512
256
128
64
32
16
8
4
2
1cosine 
Figure 7: Effect of a constraint on w (runtime on 1 CPU).
32 64 128 256 512 1024
1D-MERT 22.93 20.70 18.57 16.07 15.00 15.44
our work 25.25 22.28 19.86 17.05 15.56 15.67
+2.32 +1.59 +1.29 +0.98 +0.56 +0.23
Table 2: BLEUn4r1[%] scores for English-German on
WMT09 for tuning sets ranging from 32 to 1024 sentences.
size. Results are displayed in Table 2. The gains
are fairly substantial, with gains of 0.5 BLEU point
or more in all cases where S ? 512.8 Finally, we
perform an end-to-end MERT comparison, where
both our algorithm and 1D-MERT are iteratively used
to generate weights that in turn yield newN -best lists.
Tuning on 1024 sentences of WMT10, LP-MERT
converges after seven iterations, with a BLEU score
of 16.21%; 1D-MERT converges after nine iterations,
with a BLEU score of 15.97%. Test set performance
on the full WMT10 test set for LP-MERT and 1D-
MERT are respectively 17.08% and 16.91%.
6 Related Work
One-dimensional MERT has been very influential. It
is now used in a broad range of systems, and has been
improved in a number of ways. For instance, lattices
or hypergraphs may be used in place of N -best lists
to form a more comprehensive view of the search
space with fewer decoding runs (Macherey et al,
2008; Kumar et al, 2009; Chatterjee and Cancedda,
2010). This particular refinement is orthogonal to our
approach, though. We expect to extend LP-MERT
8One interesting observation is that the performance of 1D-
MERT degrades as S grows from 2 to 8 (Fig. 5), which contrasts
with the results shown in Tab. 2. This may have to do with the
fact that N -best lists with S = 2 have much fewer local maxima
than with S = 4, 8, in which case 20 restarts is generally enough.
to hypergraphs in future work. Exact search may be
challenging due to the computational complexity of
the search space (Leusch et al, 2008), but approxi-
mate search should be feasible.
Other research has explored alternate methods
of gradient-free optimization, such as the downhill-
simplex algorithm (Nelder and Mead, 1965; Zens
et al, 2007; Zhao and Chen, 2009). Although the
search space is different than that of Och?s algorithm,
it still relies on one-dimensional line searches to re-
flect, expand, or contract the simplex. Therefore, it
suffers the same problems of one-dimensional MERT:
feature sets with complex non-linear interactions are
difficult to optimize. LP-MERT improves on these
methods by searching over a larger subspace of pa-
rameter combinations, not just those on a single line.
We can also change the objective function in a
number of ways to make it more amenable to op-
timization, leveraging knowledge from elsewhere
in the machine learning community. Instance re-
weighting as in boosting may lead to better param-
eter inference (Duh and Kirchhoff, 2008). Smooth-
ing the objective function may allow differentiation
and standard ML learning techniques (Och and Ney,
2002). Smith and Eisner (2006) use a smoothed ob-
jective along with deterministic annealing in hopes
of finding good directions and climbing past locally
optimal points. Other papers use margin methods
such as MIRA (Watanabe et al, 2007; Chiang et al,
2008), updated somewhat to match the MT domain,
to perform incremental training of potentially large
numbers of features. However, in each of these cases
the objective function used for training no longer
matches the final evaluation metric.
7 Conclusions
Our primary contribution is the first known exact
search algorithm for direct loss minimization on N -
best lists in multiple dimensions. Additionally, we
present approximations that consistently outperform
standard one-dimensional MERT on a competitive
machine translation system. While Och?s method of
MERT is generally quite successful, there are cases
where it does quite poorly. A more global search
such as LP-MERT lowers the expected risk of such
poor solutions. This is especially important for cur-
rent machine translation systems that rely heavily on
MERT, but may also be valuable for other textual ap-
46
plications. Recent speech recognition systems have
also explored combinations of more acoustic and lan-
guage models, with discriminative training of 5-10
features rather than one million (Lo?o?f et al, 2010);
LP-MERT could be valuable here as well.
The one-dimensional algorithm of Och (2003)
has been subject to study and refinement for nearly
a decade, while this is the first study of multi-
dimensional approaches. We demonstrate the poten-
tial of multi-dimensional approaches, but we believe
there is much room for improvement in both scalabil-
ity and speed. Furthermore, a natural line of research
would be to extend LP-MERT to compact representa-
tions of the search space, such as hypergraphs.
There are a number of broader implications from
this research. For instance, LP-MERT can aid in the
evaluation of research on MERT. This approach sup-
plies a truly optimal vector as ground truth, albeit
under limited conditions such as a constrained direc-
tion set, a reduced number of features, or a smaller
set of sentences. Methods can be evaluated based on
not only improvements over prior approaches, but
also based on progress toward a global optimum.
Acknowledgements
We thank Xiaodong He, Kristina Toutanova, and
three anonymous reviewers for their valuable sug-
gestions.
Appendix A: Proof of optimality
In this appendix, we prove that LP-MERT (Algorithm 2)
is exact. As noted before, the na??ve approach of solving
Eq. 4 is to enumerate allO(NS) hypotheses combinations
inM, discard the ones that are not extreme, and return
the best scoring one. LP-MERT relies on algorithmic
improvements to speed up this approach, and we now show
that none of them affect the optimality of the solution.
Divide-and-conquer. Divide-and-conquer in Algo-
rithm 2 discards any partial hypothesis combination
h[m(j) . . .m(k)] if it is not extreme, even before consid-
ering any extension h[m(i) . . .m(j) . . .m(k) . . .m(l)].
This does not sacrifice optimality, since if conv(h;H)
is false, then conv(h;H ?G) is false for any set G.
Proof: Assume conv(h;H) is false, so h is interior to
H . By definition, any interior point h can be written as
a linear combination of other points: h =?i ?ihi, with
?i(hi ? H , hi 6= h, ?i ? 0) and ?i ?i = 1. This samecombination of points also demonstrates that h is interior
to H ?G, thus conv(h;H ?G) is false as well.
Sparse hypothesis combination. We show here
that the simplification of linear programs in Section 3.1.1
from size O(NS) to size O(NS) does not change the
value of conv(h;H). More specifically, this means that
linear optimization of the output of the COMBINE method
at lines 26-27 of function GETNEXTBEST does not
introduce any error. Let (g1 . . .gU ) and (h1 . . .hV ) be
two N -best lists to be combined, then:
conv
(
gu + hv;
U?
i=1
(gi + hv) ?
V?
j=1
(gu + hj)
)
= conv
(
gu + hv;
U?
i=1
V?
j=1
(gi + hj)
)
Proof: To prove this equality, it suffices to show that: (1)
if gu+hv is interior wrt. the first conv binary predicate
in the above equation, then it is interior wrt. the second
conv, and (2) if gu+hv is interior wrt. the second conv,
then it is interior wrt. the first conv. Claim (1) is evident,
since the set of points in the first conv is a subset of the
other set of points. Thus, we only need to prove (2). We
first geometrically translate all points by ?gu?hv . Since
gu+hv is interior wrt. the second conv, we can write:
0 =
U?
i=1
V?
j=1
?i,j(gi + hj ? gu ? hv)
=
U?
i=1
V?
j=1
?i,j(gi ? gu) +
U?
i=1
V?
j=1
?i,j(hj ? hv)
=
U?
i=1
(gi ? gu)
V?
j=1
?i,j +
V?
j=1
(hj ? hv)
U?
i=1
?i,j
=
U?
i=1
??i(gi ? gu) +
V?
j=1
??U+j(hj ? hv)
where {??i}1?i?U+V values are computed from
{?i,j}1?i?U,1?j?V as follows: ??i =
?
j ?i,j , i ? [1, U ]and ??U+j =
?
i ?i,j , j ? [1, V ]. Since the interiorpoint is 0, ??i values can be scaled so that they sum to 1
(necessary condition in the definition of interior points),
which proves that the following predicate is false:
conv
(
0;
U?
i=1
(gi ? gu) ?
V?
j=1
(hj ? hv)
)
which is equivalent to stating that the following is false:
conv
(
gu + hv;
U?
i=1
(gi + hv) ?
V?
j=1
(gu + hj)
)
47
References
C. Bradford Barber, David P. Dobkin, and Hannu Huhdan-
paa. 1996. The QuickHull algorithm for convex hulls.
ACM Trans. Math. Softw., 22:469?483.
C. Buchta, J. Muller, and R. F. Tichy. 1985. Stochastical
approximation of convex bodies. Math. Ann., 271:225?
235.
A. Bykat. 1978. Convex hull of a finite set of points in
two dimensions. Inf. Process. Lett., 7(6):296?298.
Daniel Cer, Dan Jurafsky, and Christopher D. Manning.
2008. Regularization and search for minimum error
rate training. In Proceedings of the Third Workshop on
Statistical Machine Translation, pages 26?34.
Samidh Chatterjee and Nicola Cancedda. 2010. Min-
imum error rate training by sampling the translation
lattice. In Proceedings of the 2010 Conference on Em-
pirical Methods in Natural Language Processing, pages
606?615. Association for Computational Linguistics.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and structural
translation features. In EMNLP.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
W. Chou, C. H. Lee, and B. H. Juang. 1993. Minimum
error rate training based on N-best string models. In
Proc. IEEE Int?l Conf. Acoustics, Speech, and Signal
Processing (ICASSP ?93), pages 652?655, Vol. 2.
Kevin Duh and Katrin Kirchhoff. 2008. Beyond log-
linear models: boosted minimum error rate training for
programming N-best re-ranking. In Proceedings of the
46th Annual Meeting of the Association for Computa-
tional Linguistics on Human Language Technologies:
Short Papers, pages 37?40, Stroudsburg, PA, USA.
William F. Eddy. 1977. A new convex hull algorithm for
planar sets. ACM Trans. Math. Softw., 3:398?403.
Liang Huang and David Chiang. 2005. Better k-best pars-
ing. In Proceedings of the Ninth International Work-
shop on Parsing Technology, pages 53?64, Stroudsburg,
PA, USA.
Biing-Hwang Juang, Wu Hou, and Chin-Hui Lee. 1997.
Minimum classification error rate methods for speech
recognition. Speech and Audio Processing, IEEE Trans-
actions on, 5(3):257?265.
N. Karmarkar. 1984. A new polynomial-time algorithm
for linear programming. Combinatorica, 4:373?395.
Victor Klee. 1966. Convex polytopes and linear program-
ming. In Proceedings of the IBM Scientific Computing
Symposium on Combinatorial Problems.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of ACL, Demonstration Session.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum Bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1, pages
163?171.
Gregor Leusch, Evgeny Matusov, and Hermann Ney.
2008. Complexity of finding the BLEU-optimal hy-
pothesis in a confusion network. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing, pages 839?847, Stroudsburg, PA, USA.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitke-
vitch, Sanjeev Khudanpur, Lane Schwartz, Wren N. G.
Thornton, Jonathan Weese, and Omar F. Zaidan. 2009.
Joshua: an open source toolkit for parsing-based MT.
In Proc. of WMT.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In International Conference on Com-
putational Linguistics and Association for Computa-
tional Linguistics (COLING/ACL).
Chin-Yew Lin and Franz Josef Och. 2004. ORANGE:
a method for evaluating automatic evaluation metrics
for machine translation. In Proceedings of the 20th
international conference on Computational Linguistics,
Stroudsburg, PA, USA.
Jonas Lo?o?f, Ralf Schlu?ter, and Hermann Ney. 2010. Dis-
criminative adaptation for log-linear acoustic models.
In INTERSPEECH, pages 1648?1651.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum error
rate training for statistical machine translation. In Pro-
ceedings of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 725?734.
David McAllester, Tamir Hazan, and Joseph Keshet. 2010.
Direct loss minimization for structured prediction. In
Advances in Neural Information Processing Systems
23, pages 1594?1602.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
91?98.
Ryan McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proceedings of
EACL, pages 297?304.
Robert C. Moore and Chris Quirk. 2008. Random restarts
in minimum error rate training for statistical machine
translation. In Proceedings of the 22nd International
48
Conference on Computational Linguistics - Volume 1,
pages 585?592.
J. A. Nelder and R. Mead. 1965. A simplex method for
function minimization. Computer Journal, 7:308?313.
Franz Josef Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 295?302.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proc. of ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic evalu-
ation of machine translation. In Proc. of ACL.
M.J.D. Powell. 1964. An efficient method for finding
the minimum of a function of several variables without
calculating derivatives. Comput. J., 7(2):155?162.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2007. Numerical Recipes:
The Art of Scientific Computing. Cambridge University
Press, 3rd edition.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: syntactically informed
phrasal SMT. In Proc. of ACL, pages 271?279.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proceed-
ings of the COLING/ACL on Main conference poster
sessions, pages 787?794, Stroudsburg, PA, USA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation. In
Proc. of AMTA, pages 223?231.
Andreas Stolcke, Yochai Knig, and Mitchel Weintraub.
1997. Explicit word error minimization in N-best list
rescoring. In In Proc. Eurospeech, pages 163?166.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statisti-
cal machine translation. In EMNLP-CoNLL.
Omar F. Zaidan and Chris Callison-Burch. 2009. Feasibil-
ity of human-in-the-loop minimum error rate training.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 1 -
Volume 1, pages 52?61.
Richard Zens, Sasa Hasan, and Hermann Ney. 2007.
A systematic comparison of training criteria for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 524?532,
Prague, Czech Republic.
Bing Zhao and Shengyuan Chen. 2009. A simplex Armijo
downhill algorithm for optimizing statistical machine
translation decoding parameters. In Proceedings of
Human Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, Companion Vol-
ume: Short Papers, pages 21?24.
49
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1044?1054,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Joint Language and Translation Modeling with Recurrent Neural Networks
Michael Auli, Michel Galley, Chris Quirk, Geoffrey Zweig
Microsoft Research
Redmond, WA, USA
{michael.auli,mgalley,chrisq,gzweig}@microsoft.com
Abstract
We present a joint language and transla-
tion model based on a recurrent neural net-
work which predicts target words based on
an unbounded history of both source and tar-
get words. The weaker independence as-
sumptions of this model result in a vastly
larger search space compared to related feed-
forward-based language or translation models.
We tackle this issue with a new lattice rescor-
ing algorithm and demonstrate its effective-
ness empirically. Our joint model builds on a
well known recurrent neural network language
model (Mikolov, 2012) augmented by a layer
of additional inputs from the source language.
We show competitive accuracy compared to
the traditional channel model features. Our
best results improve the output of a system
trained on WMT 2012 French-English data by
up to 1.5 BLEU, and by 1.1 BLEU on average
across several test sets.
1 Introduction
Recently, several feed-forward neural network-
based language and translation models have
achieved impressive accuracy improvements on sta-
tistical machine translation tasks (Allauzen et al,
2011; Le et al, 2012b; Schwenk et al, 2012). In this
paper we focus on recurrent neural network archi-
tectures, which have recently advanced the state of
the art in language modeling (Mikolov et al, 2010;
Mikolov et al, 2011a; Mikolov, 2012), outperform-
ing multi-layer feed-forward based networks in both
perplexity and word error rate in speech recognition
(Arisoy et al, 2012; Sundermeyer et al, 2013). The
major attraction of recurrent architectures is their
potential to capture long-span dependencies since
predictions are based on an unbounded history of
previous words. This is in contrast to feed-forward
networks as well as conventional n-gram models,
both of which are limited to fixed-length contexts.
Building on the success of recurrent architectures,
we base our joint language and translation model
on an extension of the recurrent neural network lan-
guage model (Mikolov and Zweig, 2012) that intro-
duces a layer of additional inputs (?2).
Most previous work on neural networks for
speech recognition or machine translation used a
rescoring setup based on n-best lists (Arisoy et al,
2012; Mikolov, 2012) for evaluation, thereby side
stepping the algorithmic and engineering challenges
of direct decoder-integration.1 Instead, we exploit
lattices, which offer a much richer representation
of the decoder output, since they compactly encode
an exponential number of translation hypotheses in
polynomial space. In contrast, n-best lists are typi-
cally very redundant, representing only a few com-
binations of top scoring arcs in the lattice. A major
challenge in lattice rescoring with a recurrent neural
network model is the effect of the unbounded history
on search since the usual dynamic programming as-
sumptions which are exploited for efficiency do not
hold up anymore. We apply a novel algorithm to the
task of rescoring with an unbounded language model
and empirically demonstrate its effectiveness (?3).
The algorithm proves robust, leading to signif-
icant improvements with the recurrent neural net-
work language model over a competitive n-gram
baseline across several language pairs. We even ob-
serve consistent gains when pairing the model with a
large n-gram model trained on up to 575 times more
1One notable exception is Le et al (2012a) who rescore reorder-
ing lattices with a feed-forward network-based model.
1044
data, demonstrating that the model provides comple-
mentary information (?4).
Our joint modeling approach is based on adding a
continuous space representation of the foreign sen-
tence as an additional input to the recurrent neu-
ral network language model. With this extension,
the language model can measure the consistency
between the source and target words in a context-
sensitive way. The model effectively combines the
functionality of both the traditional channel and lan-
guage model features. We test the power of this
new model by using it as the only source of tradi-
tional channel information. Overall, we find that the
model achieves accuracy competitive with the older
channel model features and that it can improve over
the gains observed with the recurrent neural network
language model (?5).
2 Model Structure
We base our model on the recurrent neural network
language model of Mikolov et al (2010) which is
factored into an input layer, a hidden layer with re-
current connections, and an output layer (Figure 1).
The input layer encodes the target language word at
time t as a 1-of-N vector et, where |V | is the size
of the vocabulary, and the output layer yt represents
a probability distribution over target words; both of
size |V |. The hidden layer state ht encodes the his-
tory of all words observed in the sequence up to time
step t. This model is extended by an auxiliary input
layer ft which provides complementary information
to the input layer (Mikolov and Zweig, 2012). While
the auxiliary input layer can be used to feed in arbi-
trary additional information, we focus on encodings
of the foreign sentence (?5).
The state of the hidden layer is determined by the
input layer, the auxiliary input layer and the hidden
layer configuration of the previous time step ht?1.
The weights of the connections between the layers
are summarized in a number of matrices: U, F and
W, represent weights from the input layer to the hid-
den layer, from the auxiliary input layer to the hid-
den layer, and from the previous hidden layer to the
current hidden layer, respectively. Matrix V repre-
sents connections between the current hidden layer
and the output layer; G represents direct weights be-
tween the auxiliary input and output layers.
et
ht-1
ft
ht
yt
V
G
F
W
U
D
Figure 1: Structure of the recurrent neural network
model, including the auxiliary input layer ft.
The hidden and output layers are computed via a
series of matrix-vector products and non-linearities:
ht = s(Uet +Wht?1 + Ff t)
yt = g(Vht +Gf t)
where
s(z) =
1
1 + exp {?z}
, g(zm) =
exp {zm}
?
k exp {zk}
are sigmoid and softmax functions, respectively.
Additionally, the network is interpolated with a
maximum entropy model of sparse n-gram features
over input words (Mikolov et al, 2011a).2 The max-
imum entropy weights are added to the output acti-
vations before computing the softmax.
The model is optimized via a maximum likeli-
hood objective function using stochastic gradient
descent. Training is based on the back propaga-
tion through time algorithm, which unrolls the net-
work and then computes error gradients over mul-
tiple time steps (Rumelhart et al, 1986). Af-
ter training, the output layer represents posteriors
p(et+1|ett?n+1,ht, ft); the probabilities of words in
the output vocabulary given the n previous input
words ett?n+1, the hidden layer configuration ht as
well as the auxiliary input layer configuration ft.
2While these features depend on multiple input words, we de-
picted them for simplicity as a connection between the current
input word vector et and the output layer (D).
1045
Na??ve computation of the probability distribution
over the next word is very expensive for large vo-
cabularies. A well established efficiency trick uses
word-classing to create a more efficient two-step
process (Goodman, 2001; Emami and Jelinek, 2005;
Mikolov et al, 2011b) where each word is assigned
a unique class. To compute the probability of a
word, we first compute the probability of its class,
and then multiply it by the probability of the word
conditioned on the class:
p(et+1|e
t
t?n+1,ht, ft) =
p(ci|e
t
t?n+1,ht, ft)? p(et+1|ci, e
t
t?n+1,ht, ft)
This factorization reduces the complexity of com-
puting the output probabilities from O(|V |) to
O(|C| + maxi |ci|) where |C| is the number of
classes and |ci| is the number of words in class
ci. The best case complexity O(
?
|V |) requires the
number of classes and words to be evenly balanced,
i.e., each class contains exactly as many words as
there are classes.
3 Lattice Rescoring with an Unbounded
Language Model
We evaluate our joint language and translation
model in a lattice rescoring setup, allowing us to
search over a much larger space of translations than
would be possible with n-best lists. While very
space efficient, lattices also impose restrictions on
the context available to features, a particularly chal-
lenging setting for our model which depends on the
entire prefix of a translation. In the ensuing de-
scription we introduce a new algorithm to efficiently
tackle this issue.
Phrase-based decoders operate by maintaining a
set of states representing competing translations, ei-
ther partial or complete. Each state is scored by a
number of features including the n-gram language
model. The independence assumptions of the fea-
tures determine the amount of context each state
needs to maintain in order for it to be possible to
assign a score to it. For example, a trigram language
model is indifferent to any context other than the
two immediately preceding words. Assuming the
trigram model dominates the Markov assumptions
of all other features, which is typically the case, then
we have to maintain at least two words at each state,
also known as the n-gram context.
1: function RESCORELATTICE(k, V , E, s, T )
2: Q? TOPOLOGICALLY-SORT(V )
3: for all v in V do . Heaps of split-states
4: Hv ? MINHEAP()
5: end for
6: h0 ? ~0 . Initialize start-state
7: Hs.ADD(h0)
8: for all v in Q do . Examine outgoing arcs
9: for ?v, x? in E do
10: for h in Hv do . Extend LM states
11: h? ? SCORERNN(h, phrase(h))
12: parent(h?)? h . Backpointers
13: if Hx.size() ? k? . Beam width
14: Hx.MIN()<score(h?) then
15: Hx.REMOVEMIN()
16: if Hx.size()<k then
17: Hx.ADD(h?)
18: end for
19: end for
20: end for
21: I = MAXHEAP()
22: for all t in T do . Find best final split-state
23: I.MERGE(Ht)
24: end for
25: return I.MAX()
26: end function
Figure 2: Push-forward rescoring with a recurrent neu-
ral network language model given a beam-width for lan-
guage model split-states k, decoder states V , edges E, a
start state s and final states T .
However, a recurrent neural network language
model makes much weaker independence assump-
tions. In fact, the predictions of such a model depend
on all previous words in the sentence, which would
imply a potentially very large context. But storing
all words is an inefficient solution from a dynamic
programming point of view. Fortunately, we do not
need to maintain entire translations as context in the
states: the recurrent model compactly encodes the
entire history of previous words in the hidden layer
configuration hi. It is therefore sufficient to add hi
as context, instead of the entire translation. The lan-
guage model can then simply score any new words
1046
based on hi from the previous state when a new state
is created.
A much larger problem is that items, that were
previously equivalent from a dynamic programming
perspective, may now be different. Standard phrase-
based decoders (Koehn et al, 2007) recombine de-
coder states with the same context into a single
state because they are equivalent to the model fea-
tures; usually recombination retains only the high-
est scoring candidate.3 However, if the context is
large, then the amount of recombination will de-
crease significantly, leading to less variety in the de-
coder beam. This was confirmed in preliminary ex-
periments where we simulated context sizes of up to
100 words but found that accuracy dropped by be-
tween 0.5-1.0 BLEU.
Integrating a long-span language model na??vely
requires to keep context equivalent to the entire left
prefix of the translation, a setting which would per-
mit very little recombination. Instead of using ineffi-
cient long-span contexts, we propose to maintain the
usual n-gram context and to keep a fixed number of
hidden layer configurations k at each decoder state.
This leads to a new split-state dynamic program
which splits each decoder state into at most k new
items, each with a separate hidden layer configura-
tion representing an unbounded history (Figure 2).
This maintains diversity in the explored translation
hypothesis space and preserves high-scoring hidden
layer configurations.
What is the effect of this strategy? To answer
this question we measured translation accuracy for
various settings of k on our lattice rescoring setup
(see ?4 for details). In the same experiment, we
compare lattices to n-best lists in terms of accuracy,
model score and wall time impact.4 The results (Ta-
ble 1 and Figure 3) show that reranking accuracy on
lattices is not significantly better, however, rescor-
ing lattices with k = 1 is much faster than n-best
lists. Similar observations have been made in previ-
ous work on minimum error-rate training (Macherey
3Assuming a max-translation decision rule. In a minimum-risk
setting, we may assign the sum of the scores of all candidates
to the retained item.
4We measured running times on an HP z800 workstation
equipped with 24 GB main memory and two Xeon E5640
CPUs with four cores each, clocked at 2.66 GHz. All experi-
ments were run single-threaded.
BLEU oracle sec/sent
Baseline 28.25 - 0.173
100-best 28.90 37.22 0.470
1000-best 28.99 40.06 3.920
lattice (k = 1) 29.00 43.50 0.093
lattice (k = 10) 29.04 43.50 0.599
lattice (k = 100) 29.03 43.50 4.531
Table 1: Rescoring n-best lists and lattices with various
language model beam widths k. Accuracy is based on
the news2011 French-English task. Timing results are in
addition to the baseline.
Figure 3: BLEU vs. log probabilities of 1-best transla-
tions when rescoring n-best lists and lattices (cf. Table 1).
et al, 2008). The recurrent language model adds an
overhead of about 54% at k = 1 on top of the time
to produce the baseline 1-best output, a consider-
able but not necessarily prohibitive overhead. Larger
values of k return higher probability solutions, but
there is little impact on accuracy: the BLEU score
is nearly identical when retaining up to 100 histories
compared to keeping only the highest scoring.
While surprising at first, we believe that this ef-
fect is due to the high similarity of the translations
represented by the histories in the beam. Each his-
tory represents a different translation but all transla-
tion hypothesis share the same n-gram context, and,
more importantly, they are translations of the same
foreign words, since they have exactly the same cov-
erage vector. These commonalities are likely to re-
sult in similar recurrent histories, which in turn re-
duces the effect of aggressive pruning.
4 Language Model Experiments
Recurrent neural network language models have
previously only been used in n-best rescoring
1047
settings and on small-scale tasks with baseline
language models trained on only 17.5m words
(Mikolov, 2012). We extend this work by experi-
menting on lattices using strong baselines with n-
gram models trained on over one billion words and
by evaluating on a number of language pairs.
4.1 Experimental Setup
Baseline. We experiment with an in-house phrase-
based system similar to Moses (Koehn et al,
2003), scoring translations by a set of common fea-
tures including maximum likelihood estimates of
source given target mappings pMLE(e|f) and vice
versa pMLE(f |e), as well as lexical weighting es-
timates pLW (e|f) and pLW (f |e), word and phrase-
penalties, a linear distortion feature and a lexicalized
reordering feature. Log-linear weights are estimated
with minimum error rate training (Och, 2003).
Evaluation. We use training and test data
from the WMT 2012 campaign and report results
on French-English, German-English and English-
German. Translation models are estimated on 102m
words of parallel data for French-English, 91m
words for German-English and English-German; be-
tween 3.5-5m words are newswire, depending on the
language pair, and the remainder are parliamentary
proceedings. The baseline systems use two 5-gram
modified Kneser-Ney language models; the first is
estimated on the target-side of the parallel data,
while the second is based on a large newswire corpus
released as part of the WMT campaign. For French-
English and German-English we use a language
model based on 1.15bn words, and for English-
German we train a model on 327m words. We eval-
uate on the newswire test sets from 2010-2011 con-
taining between 2034-3003 sentences. Log-linear
weights are estimated on the 2009 data set compris-
ing 2525 sentences. We rescore the lattices produced
by the baseline systems with an aggressive but effec-
tive context beam of k = 1 that did not harm accu-
racy in preliminary experiments (?3).
Neural Network Language Model. The vocab-
ularies of the language models are comprised of
the words in the training set after removing single-
tons. We obtain word-classes using a version of
Brown-Clustering with an additional regularization
term to optimize the runtime of the language model
(Brown et al, 1992; Zweig and Makarychev, 2013).
Direct connections use maximum entropy features
over unigrams, bigrams and trigrams (Mikolov et al,
2011a). We use the standard settings for the model
with the default learning rate ? = 0.1 that decays
exponentially if the validation set entropy does not
increase after each epoch. Back propagation through
time computes error gradients over the past twenty
time steps. Training is stopped after 20 epochs or
when the validation entropy does not decrease over
two epochs. We experiment with varying training
data sizes and randomly draw the data from the same
corpora used for the baseline systems. Throughout,
we use a hidden layer size of 100 which provided a
good trade-off between time and accuracy in initial
experiments.
4.2 Results
Training times for neural networks can be a major
bottleneck. Recurrent architectures are particularly
hard to parallelize due to their inherent dependence
on the previous hidden layer configuration. One
straightforward way to influence training time is to
change the size of the training corpus.
Our results (Table 2, Table 3 and Table 4) show
that even small models trained on only two million
words significantly improve over the 1-best decoder
output (Baseline); this represents only 0.6 percent
of the data available to the n-gram model used by
the baseline. Models of this size can be trained in
only about 3.5 hours. A model trained on 50m words
took 63 hours to train. When paired with an n-gram
model trained on 25 times more data, accuracy im-
proved by up to 0.7 BLEU on French-English.
5 Joint Model Experiments
In the next set of experiments, we turn to the joint
language and translation model, an extension of the
recurrent neural network language model with ad-
ditional inputs for the foreign sentence. We first
introduce two continuous space representations of
the foreign sentence (?5.1). Using these represen-
tations we evaluate the accuracy of the joint model
in the lattice rescoring setup and compare against the
traditional translation channel model features (?5.2).
Next, we establish an upper bound on accuracy for
the joint model via an oracle experiment (?5.3). In-
spired by the results of the oracle experiment we
1048
dev news2010 news2011 newssyscomb2011 Avg(test)
Baseline 26.6 27.6 28.3 27.5 27.8
+RNNLM (2m) 27.5 28.1 28.6 28.1 28.3
+RNNLM (50m) 27.7 28.2 29.0 28.1 28.5
Table 2: French-English results when rescoring with the recurrent neural network language model; the baseline relies
on an n-gram model trained on 1.15bn words.
dev news2010 news2011 newssyscomb2011 Avg(test)
Baseline 21.2 20.7 19.2 20.6 20.0
+RNNLM (2m) 21.8 20.9 19.4 20.9 20.3
+RNNLM (50m) 22.1 21.1 19.7 21.0 20.5
Table 3: German-English results when rescoring with the recurrent neural network language model.
dev news2010 news2011 newssyscomb2011 Avg(test)
Baseline 15.2 15.6 14.3 15.7 15.1
+RNNLM (2m) 15.7 15.9 14.6 16.0 15.4
+RNNLM (50m) 15.8 15.9 14.7 16.1 15.5
Table 4: English-German results when rescoring with the recurrent neural network language model; the baseline relies
on an n-gram model trained on 327m words.
train a transform between the source words and the
reference representations. This leads to the best re-
sults improving 1.5 BLEU over the 1-best decoder
output and adding 0.2 BLEU on average to the gains
achieved by the recurrent language model (?5.4).
Setup. Conventional language models can be
trained on monolingual or bilingual data; however,
the joint model can only be trained on the latter.
In order to control for data size effects, we restrict
training of all models, including the baseline n-gram
model, to the target side of the parallel corpus, about
102m words for French-English. Furthermore we
train recurrent models only on the newswire portion
(about 3.5m words for training and 250k words for
validation) since initial experiments showed compa-
rable results to using the full parallel corpus, avail-
able to the baseline. This is reasonable since the test
data is newswire. Also, it allows for more rapid ex-
perimentation.
5.1 Foreign Sentence Representations
We represent foreign sentences either by latent se-
mantic analysis (LSA; Deerwester et al 1990) or by
word encodings produced as a by-product of train-
ing the recurrent neural network language model on
the source words.
LSA is widely used for representing words and
documents in low-dimensional vector space. The
method applies reduced singular value decomposi-
tion (SVD) to a matrix M of word counts; in our
setting, rows represent sentences and columns rep-
resent foreign words. SVD reduces the number
of columns while preserving similarity among the
rows, effectively mapping from a high-dimensional
representation of a sentence, as a set of words, to a
low-dimensional set of concepts. The output of SVD
is an approximation of M by three matrices: T con-
tains single word representations, R represents full
sentences, and S is a diagonal scaling matrix:
M ? TSRT
Given vocabulary V and n sentences, we construct
M as a matrix of size |V ?n|. The ij-th entry is the
number of times word i occurs in sentence j, also
known as the term frequency value; the entry is also
weighted by the inverse document frequency, the rel-
ative importance of word i among all sentences, ex-
pressed as the negative logarithm of the fraction of
sentences in which word i occurs.
As a second representation we use single word
1049
embeddings implicitly learned by the input layer
weights U of the recurrent neural network language
model (?2), denoted as RNN. Each word is repre-
sented by a vector of size |hi|, the number of neu-
rons in the hidden layer; in our experiments, we
consider concatenations of individual word vectors
to represent foreign word contexts. These encodings
have previously been found to capture syntactic and
semantic regularities (Mikolov et al, 2013) and are
readily available in our experimental framework via
training a recurrent neural network language model
on the source-side of the parallel corpus.
5.2 Results
We first experiment with the two previously intro-
duced representations of the source-side sentence.
Table 5 shows the results compared to the 1-best de-
coder output and an RNN language model (target-
only). We first try LSA encodings of the entire
foreign sentence as 80 or 240 dimensional vectors
(sent-lsa-dim80, sent-lsa-dim240). Next, we experi-
ment with single-word RNN representations of slid-
ing word-windows in the hope of representing rel-
evant context more precisely. Word-windows are
constructed relative to the source words aligned to
the current target word, and individual word vec-
tors are concatenated into a single vector. We
first try contexts which do not include the aligned
source words, in the hope of capturing information
not already modeled by the channel models, start-
ing with the next five words (ww-rnn-dim50.n5),
the five previous and the next five words (ww-rnn-
dim50.p5n5) as well as the previous three words
(ww-rnn-dim50.p3). Next, we experiment with
word-windows of up to five aligned source words
(ww-rnn-dim50.c5). Finally, we try contexts based
on LSA word vectors (ww-lsa-dim50.n5, ww-lsa-
dim50.p3).5
While all models improve over the baseline, none
significantly outperforms the recurrent neural net-
work language model in terms of BLEU. However,
the perplexity results suggest that the models uti-
lize the foreign representations since all joint mod-
els improve vastly over the target-only language
5We ignore the coverage vector when determining word-
windows which risks including already translated words.
Building word-windows based on the coverage vector requires
additional state in a rescoring setting meant to be light-weight.
?p(f |e)
?p(e|f) ?p(e|f)
Baseline without CM 24.0 22.5
+ target-only 24.5 22.6
+ sent-lsa-dim240 24.9 23.3
+ ww-rnn-dim50.n5 24.9 24.0
+ ww-rnn-dim50.p5n5 24.6 23.7
+ ww-rnn-dim50.p3 24.6 22.3
+ ww-rnn-dim50.c5 24.9 24.0
+ ww-lsa-dim50.n5 24.8 23.9
+ ww-lsa-dim50.p3 23.8 23.2
Table 6: Comparison of the joint model and the chan-
nel model features (CM) by removing channel features
corresponding to ?p(e|f) from the lattices, or both di-
rections ?p(e|f),?p(f |e) and replacing them by vari-
ous joint models. We re-tuned the log-linear weights for
different feature-sets. Accuracy is based on the average
BLEU over news2010, newssyscomb2010, news2011.
model. The lowest perplexity is achieved by the
context covering the aligned source words (ww-rnn-
dim50.c5) since the source words are a better pre-
dictor of the target words than outside context.
The experiments so far measured if the joint
model can improve in addition to the four channel
model features used by the baseline, that is, the max-
imum likelihood and lexical translation features in
both translation directions. The joint model clearly
overlaps with these features, but how well does
the recurrent model perform compared against the
channel model features? To answer this question,
we removed channel model features corresponding
to the same translation direction as the joint model,
specifically pMLE(e|f) and pLW (e|f), from the lat-
tices and measured the effect of adding the joint
models.
The results (Table 6, column ?p(e|f)) clearly
show that our joint models are competitive with the
channel model features by outperforming the orig-
inal baseline with all channel model features (24.7
BLEU) by 0.2 BLEU (ww-rnn-dim50.n5, ww-rnn-
dim50.c5). As a second experiment, we removed all
channel model features (column ?p(e|f), p(f |e)),
diminishing baseline accuracy to 22.5 BLEU. In this
setting, the best joint model is able to make up 1.5
of the 2.2 BLEU lost due to removal of the channel
1050
dev news2010 news2011 newssyscomb2010 Avg(test) PPL
Baseline 24.3 24.4 25.1 24.3 24.7 341
target-only 25.1 25.1 26.4 25.0 25.6 218
sent-lsa-dim80 25.2 25.2 26.3 25.1 25.6 147
sent-lsa-dim240 25.1 25.0 26.2 24.9 25.4 126
ww-rnn-dim50.n5 24.9 25.0 26.3 24.8 25.4 61
ww-rnn-dim50.p5n5 25.0 24.8 26.2 24.7 25.3 59
ww-rnn-dim50.p3 25.1 25.1 26.5 24.9 25.6 143
ww-rnn-dim50.c5 24.8 24.9 26.0 24.8 25.3 16
ww-lsa-dim50.n5 25.0 25.0 26.2 24.8 25.4 76
ww-lsa-dim50.p3 25.1 25.1 26.5 24.9 25.6 151
Table 5: Translation accuracy of the joint model with various encodings of the foreign sentence measured on the
French-English task. Perplexity (PPL) is based on news2011.
model features, while modeling only a single trans-
lation direction. This setup also shows the negligible
effect of the target-only language model in the ab-
sence of translation scores, whereas the joint models
are much more effective since they do model transla-
tion. Overall, the best joint models prove very com-
petitive to the traditional channel features.
5.3 Oracle Experiment
The previous section examined the effect of a set
of basic foreign sentence representations. Although
we find some benefit from these representations, the
differences are not large. One might naturally ask
whether there is greater potential upside from this
channel model. Therefore we turn to measuring the
upper bound on accuracy for the joint approach as a
whole.
Specifically, we would like to find a bound on ac-
curacy given an ideal representation of the source
sentence. To answer this question, we conducted an
experiment where the joint model has access to an
LSA representation of the reference translation.
Table 7 shows that the joint approach has an ora-
cle accuracy of up to 4.3 BLEU above the baseline.
This clearly confirms that the joint approach can ex-
ploit the additional information to improve BLEU,
given a good enough representation of the foreign
sentence. In terms of perplexity, we see an improve-
ment of up to 65% over the target-only model. It
should be noted that since LSA representations are
computed on reference words, perplexity no longer
has its standard meaning.
BLEU PPL
Baseline 25.2 341
target-only 26.4 218
oracle (sent-lsa-dim40) 27.7 124
oracle (sent-lsa-dim80) 28.5 103
oracle (sent-lsa-dim160) 29.0 86
oracle (sent-lsa-dim240) 29.5 76
Table 7: Oracle accuracy of the joint model when us-
ing an LSA encoding of the references, measured on the
news2011 French-English task.
5.4 Target Language Projections
Our experiments so far showed that joint models
based on direct representations of the source words
are very competitive to the traditional channel mod-
els (?5.2). However, these experiments have not
shown any improvements over the normal recurrent
neural network language model. The previous sec-
tion demonstrated that good representations can lead
to substantial gains (?5.3). In order to bridge the gap,
we propose to learn a separate transform from the
foreign words to an encoding of the reference target
words, thus making the source-side representations
look more like the target-side encodings used in the
oracle experiment.
Specifically, we learn a linear transform
d? : x? r mapping directly from a vector en-
coding of the foreign sentence x to an l-dimensional
LSA representation r of the reference sentence. At
test and training time we apply d? to the foreign
words and use the transformation instead of a direct
1051
dev news2010 news2011 newssyscomb2010 Avg(test) PPL
Baseline 24.3 24.4 25.1 24.3 24.7 341
target-only 25.1 25.1 26.4 25.0 25.6 218
proj-lsa-dim40 25.1 25.3 26.5 25.2 25.8 145
proj-lsa-dim80 25.1 25.3 26.6 25.2 25.8 134
Table 8: Translation accuracy of the joint model with a source-target transform, measured on the French-English task.
Perplexity (PPL) is based on news2011; differences to target-only are significant at the p < 0.001 level.
source-side representation.
The transform models all foreign words in the par-
allel corpus except singletons, which are collapsed
into a unique class, similar to the recurrent neural
network language model. We train the transform to
minimize the squared error with respect to the ref-
erence LSA vector using an SGD online learner:
?? = argmin
?
n?
i=1
(
ri ? d?(xi)
)2
(1)
We found a simple constant learning rate, tuned
on the validation data, to be as effective as sched-
ules based on constant decay, or reducing the learn-
ing rate when the validation error increased. Our
feature-set includes unigram and bigram word fea-
tures. The value of unigram features is simply the
unigram count in that sentence; bigram features re-
ceive a weight of the bigram count divided by two
to help prevent overfitting. Then the vector for each
sentence was divided by its L2 norm. Both weight-
ing and normalization led to substantial improve-
ments in test set error. More complex features such
as skip-bigrams, trigrams and character n-grams did
not yield any significant improvements. Even this
representation of sentences is composed of a large
number of instances, and so we resorted to feature
hashing by computing feature ids as the least signif-
icant 20 bits of each feature name. Our best trans-
form achieved a cosine similarity of 0.816 on the
training data, 0.757 on the validation data, and 0.749
on news2011.
The results (Table 8) show that the transform im-
proves over the recurrent neural network language
model on all test sets and by 0.2 BLEU on average.
We verified significance over the target-only model
using paired bootstrap resampling (Koehn, 2004)
over all test sets (7526 sentences) at the p < 0.001
level. Overall, we improve accuracy by up to 1.5
BLEU and by 1.1 BLEU on average across all test
sets over the decoder 1-best with our joint language
and translation model.
6 Related Work
Our approach of combining language and translation
modeling is very much in line with recent work on
n-gram-based translation models (Crego and Yvon,
2010), and more recently continuous space-based
translation models (Le et al, 2012a; Gao et al,
2013). The joint model presented in this paper dif-
fers in a number of key aspects: we use a recur-
rent architecture representing an unbounded history
of both source and target words, rather than a feed-
forward style network. Feed-forward networks and
n-gram models have a finite history which makes
predictions independent of anything but a small his-
tory of words. Furthermore, we only model the
target-side which is different to previous work mod-
eling both sides.
We introduced a new algorithm to tackle lattice
rescoring with an unbounded model. The auto-
matic speech recognition community has previously
addressed this issue by either approximating long-
span language models via simpler but more tractable
models (Deoras et al, 2011b), or by identifying con-
fusable subsets of the lattice from which n-best lists
are constructed and rescored (Deoras et al, 2011a).
We extend their work by directly mapping a recur-
rent neural network model onto the structure of the
lattice, rescoring all states instead of focusing only
on subsets.
7 Conclusion and Future Work
Joint language and translation modeling with recur-
rent neural networks leads to substantial gains over
the 1-best decoder output, raising accuracy by up
to 1.5 BLEU and by 1.1 BLEU on average across
1052
several test sets. The joint approach also improves
over the gains of the recurrent neural network lan-
guage model, adding 0.2 BLEU on average across
several test sets. Our models are competitive to the
traditional channel models, outperforming them in a
head-to-head comparison.
Furthermore, we tackled the issue of lattice
rescoring with an unbounded recurrent model by
means of a novel algorithm that keeps a beam of re-
current histories. Finally, we have shown that the
recurrent neural network language model can sig-
nificantly improve over n-gram baselines across a
range of language-pairs, even when the baselines
were trained on 575 times more data.
In future work we plan to directly learn represen-
tations of the source-side during training of the joint
model. Thus, the model itself can decide which en-
coding is best for the task. We also plan to change
the cross entropy objective to a BLEU-inspired ob-
jective in a discriminative training regime, which we
hope to be more effective. We would also like to ap-
ply recent advances in tackling the vanishing gradi-
ent problem (Pascanu et al, 2013) using a regular-
ization term to maintain the magnitude of the gradi-
ents during back propagation through time. Finally,
we would like to integrate the recurrent model di-
rectly into first-pass decoding, a straightforward ex-
tension of lattice rescoring using the algorithm we
developed.
Acknowledgments
We would like to thank Anthony Aue, Hany Has-
san Awadalla, Jon Clark, Li Deng, Sauleh Eetemadi,
Jianfeng Gao, Qin Gao, Xiaodong He, Will Lewis,
Arul Menezes, and Kristina Toutanova for helpful
discussions related to this work as well as for com-
ments on previous drafts. We would also like to
thank the anonymous reviewers for their comments.
References
Alexandre Allauzen, He?le`ne Bonneau-Maynard, Hai-Son
Le, Aure?lien Max, Guillaume Wisniewski, Franc?ois
Yvon, Gilles Adda, Josep Maria Crego, Adrien
Lardilleux, Thomas Lavergne, and Artem Sokolov.
2011. LIMSI @ WMT11. In Proc. of WMT, pages
309?315, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and Bhu-
vana Ramabhadran. 2012. Deep Neural Network
Language Models. In NAACL-HLT Workshop on the
Future of Language Modeling for HLT, pages 20?28,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479, Dec.
Josep Crego and Franois Yvon. 2010. Factored bilingual
n-gram language models for statistical machine trans-
lation. Machine Translation, 24(2):159?175.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Anoop Deoras, Toma?s? Mikolov, and Kenneth Church.
2011a. A Fast Re-scoring Strategy to Capture Long-
Distance Dependencies. In Proc. of EMNLP, pages
1116?1127, Stroudsburg, PA, USA, July. Association
for Computational Linguistics.
Anoop Deoras, Toma?s? Mikolov, Stefan Kombrink,
M. Karafiat, and Sanjeev Khudanpur. 2011b. Varia-
tional Approximation of Long-Span Language Models
for LVCSR. In Proc. of ICASSP, pages 5532?5535.
Ahmad Emami and Frederick Jelinek. 2005. A Neural
Syntactic Language Model. Machine Learning, 60(1-
3):195?227, September.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng.
2013. Learning Semantic Representations for the
Phrase Translation Model. Technical Report MSR-
TR-2013-88, Microsoft Research, September.
Joshua Goodman. 2001. Classes for Fast Maximum En-
tropy Training. In Proc. of ICASSP.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc.
of HLT-NAACL, pages 127?133, Edmonton, Canada,
May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Proc.
of ACL Demo and Poster Sessions, pages 177?180,
Prague, Czech Republic, Jun.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proc. of EMNLP,
pages 388?395, Barcelona, Spain, Jul.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012a. Continuous Space Translation Models with
1053
Neural Networks. In Proc. of HLT-NAACL, pages 39?
48, Montre?al, Canada. Association for Computational
Linguistics.
Hai-Son Le, Thomas Lavergne, Alexandre Allauzen,
Marianna Apidianaki, Li Gong, Aure?lien Max, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois Yvon.
2012b. LIMSI @ WMT12. In Proc. of WMT, pages
330?337, Montre?al, Canada, June. Association for
Computational Linguistics.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uszkoreit. 2008. Lattice-based Minimum
Error Rate Training for Statistical Machine Transla-
tion. In Proc. of EMNLP, pages 725?734, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Toma?s? Mikolov and Geoffrey Zweig. 2012. Con-
text Dependent Recurrent Neural Network Language
Model. In Proc. of Spoken Language Technologies
(SLT), pages 234?239, Dec.
Toma?s? Mikolov, Karafia?t Martin, Luka?s? Burget, Jan Cer-
nocky?, and Sanjeev Khudanpur. 2010. Recurrent
Neural Network based Language Model. In Proc. of
INTERSPEECH, pages 1045?1048.
Toma?s? Mikolov, Anoop Deoras, Daniel Povey, Luka?s?
Burget, and Jan C?ernocky?. 2011a. Strategies for
Training Large Scale Neural Network Language Mod-
els. In Proc. of ASRU, pages 196?201.
Toma?s? Mikolov, Stefan Kombrink, Luka?s? Burget, Jan
Cernocky?, and Sanjeev Khudanpur. 2011b. Exten-
sions of Recurrent Neural Network Language Model.
In Proc. of ICASSP, pages 5528?5531.
Toma?s? Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic Regularities in Continuous Space-
Word Representations. In Proc. of NAACL, pages
746?751, Stroudsburg, PA, USA, June. Association
for Computational Linguistics.
Toma?s? Mikolov. 2012. Statistical Language Models
based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of ACL,
pages 160?167, Sapporo, Japan, July.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training Recurrent Neural
Networks. Proc. of ICML, abs/1211.5063.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning Internal Representations
by Error Propagation. In Symposium on Parallel and
Distributed Processing.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, Pruned or Continuous Space Lan-
guage Models on a GPU for Statistical Machine Trans-
lation. In NAACL-HLT Workshop on the Future of
Language Modeling for HLT, pages 11?19. Associa-
tion for Computational Linguistics.
Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,
Ben Freiberg, Ralf Schlu?ter, and Hermann Ney. 2013.
Comparison of Feedforward and Recurrent Neural
Network Language Models. In IEEE International
Conference on Acoustics, Speech, and Signal Process-
ing, pages 8430?8434, Vancouver, Canada, May.
Geoff Zweig and Konstantin Makarychev. 2013. Speed
Regularization and Optimality in Word Classing. In
Proc. of ICASSP.
1054
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1948?1959,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Regularized Minimum Error Rate Training
Michel Galley
Microsoft Research
mgalley@microsoft.com
Chris Quirk
Microsoft Research
chrisq@microsoft.com
Colin Cherry
National Research Council
colin.cherry@nrc-cnrc.gc.ca
Kristina Toutanova
Microsoft Research
kristout@microsoft.com
Abstract
Minimum Error Rate Training (MERT) re-
mains one of the preferred methods for tun-
ing linear parameters in machine translation
systems, yet it faces significant issues. First,
MERT is an unregularized learner and is there-
fore prone to overfitting. Second, it is com-
monly used on a noisy, non-convex loss func-
tion that becomes more difficult to optimize
as the number of parameters increases. To ad-
dress these issues, we study the addition of
a regularization term to the MERT objective
function. Since standard regularizers such as
`2 are inapplicable to MERT due to the scale
invariance of its objective function, we turn to
two regularizers?`0 and a modification of `2?
and present methods for efficiently integrating
them during search. To improve search in large
parameter spaces, we also present a new direc-
tion finding algorithm that uses the gradient of
expected BLEU to orient MERT?s exact line
searches. Experiments with up to 3600 features
show that these extensions of MERT yield re-
sults comparable to PRO, a learner often used
with large feature sets.
1 Introduction
Minimum Error Rate Training emerged a decade
ago (Och, 2003) as a superior training method for
small numbers of linear model parameters of machine
translation systems, improving over prior work using
maximum likelihood criteria (Och and Ney, 2002).
This technique quickly rose to prominence, becom-
ing standard in many research and commercial MT
systems. Variants operating over lattices (Macherey
et al, 2008) or hypergraphs (Kumar et al, 2009) were
subsequently developed, with the benefit of reducing
the approximation error from n-best lists.
The primary advantages of MERT are twofold. It
directly optimizes the evaluation metric under consid-
eration (e.g., BLEU) instead of some surrogate loss.
Secondly, it offers a globally optimal line search. Un-
fortunately, there are several potential difficulties in
scaling MERT to larger numbers of features, due
to its non-convex loss function and its lack of reg-
ularization. These challenges have prompted some
researchers to move away from MERT, in favor of lin-
early decomposable approximations of the evaluation
metric (Chiang et al, 2009; Hopkins and May, 2011;
Cherry and Foster, 2012), which correspond to easier
optimization problems and which naturally incorpo-
rate regularization. In particular, recent work (Chiang
et al, 2009) has shown that adding thousands or tens
of thousands of features can improve MT quality
when weights are optimized using a margin-based
approximation. On simulated datasets, Hopkins and
May (2011) found that conventional MERT strug-
gles to find reasonable parameter vectors, where a
smooth loss function based on Pairwise Ranking Op-
timization (PRO) performs much better; on real data,
this PRO method appears at least as good as MERT
on small feature sets, and also scales better as the
number of features increases.
In this paper, we seek to preserve the advantages
of MERT while addressing its shortcomings in terms
of regularization and search. The idea of adding a
regularization term to the MERT objective function
can be perplexing at first, because the most common
regularizers, such as `1 and `2, are not directly appli-
cable to MERT. Indeed, these regularizers are scale
sensitive, while the MERT objective function is not:
scaling the weight vector neither changes the predic-
tions of the linear model nor affects the error count.
Hence, MERT can hedge any regularization penalty
by maximally scaling down linear model weights.
The first contribution of this paper is to analyze var-
ious forms of regularization that are not susceptible
to this scaling problem. We analyze and experiment
with `0, a form of regularization that is scale insen-
sitive. We also present new parameterizations of `2
1948
regularization, where we apply `2 regularization to
scale-senstive linear transforms of the original linear
model. In addition, we introduce efficient methods
of incorporating regularization in Och (2003)?s exact
line searches. For all of these regularizers, our meth-
ods let us find the true optimum of the regularized
objective function along the line.
Finally, we address the issue of searching in a
high-dimensional space by using the gradient of ex-
pected BLEU (Smith and Eisner, 2006) to find better
search directions for our line searches. This direction
finder addresses one of the serious concerns raised
by Hopkins and May (2011): MERT widely failed
to reach the optimum of a synthetic linear objective
function. In replicating Hopkins and May?s experi-
ments, we confirm that existing search algorithms for
MERT?including coordinate ascent, Powell?s algo-
rithm (Powell, 1964), and random direction sets (Cer
et al, 2008)?perform poorly in this experimental
condition. However, when using our gradient-based
direction finder, MERT has no problem finding the
true optimum even in a 1000-dimensional space.
Our results suggest that the combination of a reg-
ularized objective function and a gradient-informed
line search algorithm enables MERT to scale well
with a large number of features. Experiments with
up to 3600 features show that these extensions of
MERT yield results comparable to PRO (Hopkins
and May, 2011), a parameter tuning method known
to be effective with large feature sets.
2 Unregularized MERT
Prior to introducing regularized MERT, we briefly
review standard unregularized MERT (Och, 2003).
We use fS1 = {f1 . . . fS} to denote the S input sen-
tences of a given tuning set. For each sentence fs, let
Cs = {es,1 . . . es,M} denote the list of M -best can-
didate translations. Each input and output sentence
pair (fs, es,m) is weighted using a linear model that
applies model parameters w = (w1 . . . wD) ? RD
to D feature functions h1(f , e,?) . . . hD(f , e,?),
where ? is the hidden state associated with the
derivation from f to e, such as phrase segmenta-
tion and alignment. Furthermore, let hs,m ? RD
denote the feature vector representing the translation
pair (fs, es,m).
In MERT, the goal is to minimize a loss function
E(r, e) that scores translation hypotheses against a
set of reference translations rS1 = {r1 . . . rS}. This
yields the following optimization problem:
w? = argmin
w
{ S?
s=1
E(rs, e?(fs;w))
}
=
argmin
w
{ S?
s=1
M?
m=1
E(rs, es,m)?(es,m, e?(fs;w))
}
(1)
where
e?(fs;w) = argmax
m?{1...M}
{
w?hs,m
}
(2)
While the error surface of Equation 1 is only an
approximation of the true error surface of the MT
decoder, the quality of this approximation depends
on the size of the hypothesis space represented by the
M -best list. Therefore, the hypothesis list is grown
iteratively: decoding with an initial parameter vector
seeds the M -best lists; next, parameter estimation
and M -best list gathering alternate until the cumula-
tive M -best list no longer grows, or until changes of
w between two decoding runs are deemed too small.
To increase the size of the hypothesis space, subse-
quent work (Macherey et al, 2008) instead operated
on lattices, but this paper focuses on M -best lists.
A crucial observation is that the unsmoothed error
count represented in Equation 1 is a piecewise con-
stant function. This enabled Och (2003) to devise a
line search algorithm guaranteed to find the optimum
point along the line. To extend the search from one
to multiple dimensions, MERT applies a sequence
of line optimizations along some fixed or variable
set of search directions {dt} until some convergence
criteria are met. Considering a given point wt and
a given direction dt at iteration t, finding the most
probable translation hypothesis in the set of candi-
dates translations Cs = {es,1 . . . es,M} corresponds
to solving the following optimization problem:
e?(fs; ?) = argmax
m?{1...M}
{
(wt + ? ? dt)
?hs,m
}
(3)
The function in this equation is piecewise linear (Pa-
pineni, 1999), which enables an efficient exhaustive
computation. Specifically, this function is optimized
by enumerating the up to M hypotheses that form
the upper envelope of the model score function. The
error count, then, is a piecewise constant function
1949
defined by the points ?fs1 < ? ? ? < ?
fs
M at which an in-
crease in ? causes a change of optimum in Equation 3.
Error counts for the whole corpus are simply the sums
of sentence-level piecewise constant functions aggre-
gated over all sentences of the corpus.1 The optimal ?
is finally computed by enumerating all piecewise con-
stant intervals of the corpus-level error function, and
by selecting the one that has the lowest error count
(or, correspondingly, highest BLEU score). Assum-
ing the optimum is found in the interval [?k?1, ?k],
we define ?opt = (?k?1 + ?k)/2 and change the pa-
rameters using the update wt+1 = wt + ?opt ? dt.
Finally, this method is turned into a global D-
dimensional search using algorithms that repeat-
edly use the aforementioned exact line search algo-
rithm. Och (2003) first advocated the use of Powell?s
method (Powell, 1964; Press et al, 2007). Pharaoh
(Koehn, 2004) and subsequently Moses (Koehn et al,
2007) instead use coordinate ascent, and more recent
work often uses random search directions (Cer et al,
2008; Macherey et al, 2008). In Section 4, we will
present a novel direction finder for maximum-BLEU
optimization, which uses the gradient of expected
BLEU to find directions where the BLEU score is
most likely to increase.
3 Regularization for MERT
Because MERT is prone to overfitting when a large
number of parameters must be optimized, we study
the addition of a regularization term to the objective
function. One conventional approach is to regularize
the objective function with a penalty based on the
Euclidean norm ||w||2 =
??
iw
2
i , also known as `2
regularization. In the case of MERT, this yields the
following objective function:2
w? = argmin
w
{ S?
s=1
E(rs, e?(fs;w)) +
||w||22
2?2
}
(4)
1This assumes that the sufficient statistics of the metric under
consideration are additively decomposable by sentence, which
is the case with most popular evaluation metrics such as BLEU
(Papineni et al, 2001).
2The `2 regularizer is often used in conjunction with log-
likelihood objectives. The regularization term of Equation 4
could similarly be added to the log of an objective?e.g.,
log(BLEU) instead of BLEU?but we found that the distinc-
tion doesn?t have much of an impact in practice.
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
-0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4
MERT
Max at 0.225
?
?
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
-0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4
MERT? `2
Max at -0.018
?
?
?`2
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
-0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4
?, the step size in the current direction
MERT? `0
Max at 0
?
?
`0
Figure 1: Example MERT values along one coordi-
nate, first unregularized. When regularized with `2, the
piecewise constant function becomes piecewise quadratic.
When using `0, the function remains piecewise constant
with a point discontinuity at 0.
where the regularization term 1/2?2 is a free param-
eter that controls the strength of the regularization
penalty. Similar regularizers have also been used
in conjunction with other norms, such as `1 and `0
norms. The `1 norm, defined as ||w||1 =
?
i |wi|,
applies a constant force toward zero, preferring vec-
tors with fewer non-zero components; `0, defined as
||w||0 = |{i | wi 6= 0}|, simply counts the number of
non-zero components of the weight vector, encoding
a preference for sparse vectors.
Geometrically, `2 is a parabola, `1 is the wedge-
shaped absolute value function, and `0 is an impulse
function with a spike at 0. The original formulation
(Equation 1) of MERT consists of a piecewise con-
stant representation of the loss, as a function of the
step size in a given direction. But with these three reg-
1950
ularization terms, the function respectively becomes
piecewise quadratic, piecewise linear, or piecewise
constant with a potential impulse jump for each dis-
tinct choice of regularizer. Figure 1 demonstrates this
effect graphically.
As discussed in (McAllester and Keshet, 2011),
the problem with optimizing Equation 4 directly is
that the output of the underlying linear classifier, and
therefore the error count, are not sensitive to the scale
of w. Moreover, `2 regularization (as well as `1 reg-
ularization) is scale sensitive, which means any op-
timizer of this function can drive the regularization
term down to zero by scaling down w. As special
treatments for `2, we evaluate three linear transforms
of the weight vector, where the vector w of the regu-
larization term ||w||22/2?
2 is replaced with either:
1. an affine transform: w? w0
2. a vector with only (D ? 1) free parameters, e.g.,
(1, w?2, ? ? ? , w
?
D)
3. an `1 renormalization: w/||w||1
In (1), regularization is biased towards w0, a weight
vector previously optimized using a competitive yet
much smaller feature set, such as core features of
a phrase-based (Koehn et al, 2007) or hierarchical
(Chiang, 2007) system. The requirement that this
feature set be small is to prevent overfitting. Other-
wise, any regularization toward an overfit parameter
vector w0 would defeat the purpose of introducing
a regularization term in the first place.3 In (2), the
transformation is motivated by the observation that
the D-parameter linear model of Equation 2 only
needs (D ? 1) degrees of freedom. Fixing one of
the components of w to any non-zero constant and
allowing the others to vary, the new linear model re-
tains the same modeling power, but the (D ? 1) free
parameters are no longer scale invariant, i.e., scaling
the (D ? 1)-dimensional vector now has an effect on
linear model predictions. In (3), the weight vector
is normalized as to have an `1-norm equal to 1. In
contrast, the `0 norm is scale insensitive, thus not
affected by this problem.
3.1 Exact line search with regularization
Optimizing with a regularized error surface requires
a change in the line search algorithm presented in
3(Gimpel and Smith, 2012, footnote 6) briefly mentions the
use of such a regularizer with its ramp loss objective function.
Section 2, but the other aspects of MERT remain the
same, and we can still use global search algorithms
such as coordinate ascent, Powell, and random di-
rections exactly the same way as with unregularized
MERT. Line search with a regularization term is still
as efficient as in (Och, 2003), and it is still guar-
anteed to find the optimum of the (now regularized)
objective function along the line. Considering again a
given point wt and a given direction dt at line search
iteration t, finding the optimum ?opt corresponds to
finding ? that minimizes:
S?
s=1
E(rs, e?(fs; ?)) +
||wt + ? ? dt||22
2?2
(5)
Since regularization does not affect the points at
which e?(fs; ?) changes its optimum, the points
?fs1 < ? ? ? < ?
fs
M of intersection in the upper enve-
lope remain the same, so the points of discontinuity
in the error surface remain the same. The difference
now is that the error count on each segment [?i?1, ?i]
is no longer constant. This means we need to adjust
the final step of line search, which consists of enu-
merating all [?i?1, ?i], and keeping the optimum of
Equation 5 for each segment. e?(fs; ?) remains con-
stant within the segment, so we only need to consider
the expression ||wt + ? ? dt||22 to select a segment
point. The optimum is either at the left edge, the right
edge, or in the middle if the vertex of the parabola
happens to lie within that segment.4 We compute
this optimum by finding the value ? for which the
derivative of the regularization term is zero. There is
an easy closed-form solution:
d
d?
[
||wt + ? ? dt||22
2?2
]
= 0
d
d?
[
?
i
(w2t,i + 2 ? ? ? wt,i ? dt,i + ?
2 ? d2t,i)
]
= 0
?
i
(2 ? wt,i ? dt,i + 2 ? ? ? d
2
t,i) = 0
? = ?
(?
i
wt,i ? dt,i
)/(?
i
d2t,i
)
= ?
wt?dt
dt?dt
This closed-form solution is computed in time pro-
portional to D, which doesn?t slow down the com-
4When the optimum is either at the left edge ?i?1 or right
edge ?i of a segment, we select a point at a small relative distance
within the segment (.999?i?1 + .001?i, in the former case) to
avoid ties in objective values.
1951
putation of Equation 5 for each segment (the con-
struction of each segment of the upper envelope is
proportional to D anyway).
We also use `0 regularization. While minimiza-
tion of the `0-norm is known to be NP-hard in gen-
eral (Hyder and Mahata, 2009), this optimization is
relatively trivial in the case of a line search. Indeed,
for a given segment, the value in Equation 5 is con-
stant everywhere except where we intersect any of
the coordinate hyperplanes, i.e., where one of the
coordinates is zero. Thus, our method consists of
evaluating Equation 5 at the intersection points be-
tween the line and coordinate hyperplanes, returning
the optimal point within the given segment. For any
segment that doesn?t cross any of these hyperplanes,
we evaluate the objective function at any point of the
segment (since the value is constant across the entire
segment).
4 Direction finding
4.1 A Gradient-based direction finder
Perhaps the greatest obstacle in scaling MERT
to many dimensions is finding good search direc-
tions. In problems of lower dimensions, iterating
through all the coordinates is computationally feasi-
ble, though not guaranteed to find a global maximum
even in the case of a perfect line search. As the
number of dimensions increases by orders of mag-
nitude, this coordinate direction approach becomes
less and less tractable, and the quality of the search
also suffers (Hopkins and May, 2011).
Optimization has traditionally relied on finding the
direction of steepest ascent: the gradient. Unfortu-
nately, the objective function optimized by MERT is
piecewise constant; while it may admit a subgradi-
ent, this direction is generally not very informative.
Instead we may consider a smoothed variation of the
original approximation. While some variants have
been considered (Och, 2003; Flanigan et al, 2013),
we use an expected BLEU approximation, assum-
ing hypotheses are drawn from a log-linear distri-
bution according to their parameter values (Smith
and Eisner, 2006). That is, we assume the proba-
bility of a translation candidate es,m is proportional
to (exp (w?hs,m))
?, where w are the parameters be-
ing optimized, hs,m is the vector of the features for
es,m, and ? is a scaling parameter. As ? approaches
infinity, the distribution places all its weight on the
highest scoring candidate.
The log of the BLEU score may be written as:
min
(
1?
R
C
, 0
)
+
1
N
N?
n=1
(logmn ? log cn)
where R is the sum of reference lengths across the
corpus, C is the sum of candidate lengths, mn is the
number of matched n-grams (potentially clipped),
and cn is the number of n-grams in all candidates.
Given a distribution over candidates, we can use
the expected value of the log of the BLEU score. This
is a smooth approximation to the BLEU score, which
asymptotically approaches the true BLEU score as
the scaling parameter ? approaches infinity. While
this expectation is difficult to compute exactly, we
can compute approximations thereof using Taylor se-
ries. Although prior work demonstrates that a second-
order Taylor approximation is feasible to compute
(Smith and Eisner, 2006), we find that a first-order
approximation is faster and very close to the second-
order approximation.5 The first order Taylor approxi-
mation is as follows:
min
(
1?
R
E[C]
, 0
)
+
1
N
N?
n=1
(logE[mn]? logE[cn])
where E is the expectation operator using the proba-
bility distribution P (h;w, ?).
First we note that the gradient ??wiP (h;w, ?) is
P (h;w, ?)
(
hi ?
?
h?
h?iP (h
?;w, ?)
)
Using the chain rule, the gradient of the first order
approximation to BLEU is as follows:
1
N
N?
n=1
( 1
E[mn]
?
h
mn(h)
?P (h;w, ?)
?wi
?
1
E[cn]
?
h
cn(h)
?P (h;w, ?)
?wi
)
+
{
0 if E[C] > R
R
E[C]2
?
h c1(h)
?P (h;w,?)
?wi
otherwise
5Experimentally, we compared our analytical gradient of
the first-order Taylor approximation with the finite-difference
gradients of the first- and second-order approximations, and we
found these three gradients to be very close in terms of cosine
similarity (> 0.99). We performed these measurements both at
arbitrary points and at points of convergence of MERT.
1952
In the case of `2-regularized MERT, the final gradi-
ent also includes the partial derivative of the regular-
ization penalty of Equation 4, which is wi/?2 for a
given component i of the gradient. We do not update
the gradient in the case of `0 regularization since the
`0-norm is not differentiable.
4.2 Search
Our search strategy consists of looking at the direc-
tions of steepest increase of expected BLEU, which
is similar to that of Smith and Eisner (2006), but with
the difference that we do so in the context of MERT.
We think this difference provides two benefits. First,
while the smooth approximation of BLEU reduces
the likelihood of remaining trapped in a local opti-
mum, we avoid approximation error by retaining the
original objective function. Second, the benefit of
exact line searches in MERT is that there is no need
to be concerned about step size, since step size in
MERT line searches is guaranteed to be optimal with
respect to the direction under consideration.
Finally, our gradient-based search algorithm oper-
ates as follows. Considering the current point wt, we
compute the gradient gt of the first order Taylor ap-
proximation at that point, using the current scaling pa-
rameter ?. (We initialize the search with ? = 0.01.)
We find the optimum along the line wt+? ?gt. When-
ever any given line search yields no improvement
larger than a small tolerance threshold, we multiply
? by two and perform a new line search. The increase
of this parameter ? corresponds to a cooling schedule
(Smith and Eisner, 2006), which progressively sharp-
ens the objective function to get a better estimate of
BLEU as the search converges to an optimum. We
repeatedly perform new line searches until ? exceeds
1000. The inability to improve the current optimum
with a sharp approximation (? > 1000) doesn?t mean
line searches would fail with smaller values, so we
find it helpful to repeat the above procedure until a
full pass of updates of ? from 0.01 to 1000 yields no
improvement.
4.3 Computational complexity
Computing the gradient increases the computational
cost of MERT, though not its asymptotic complexity.
The cost of a single exhaustive line search is
O (SM(D + logM + logS))
where S is the number of sentences, each with M
possible translations, andD is the number of features.
For each sentence, we first identify the model score
as a linear function of the step size, requiring two
dot products for an overall cost of O(SMD).6 Next
we construct the upper envelope for each sentence:
first the equations are sorted in increasing order of
slope, and then they are merged in linear time to form
an envelope, with an overall cost of O(SM logM).
A linear pass through the envelope converts these
into piecewise constant (or linear, or quadratic) repre-
sentations of the (regularized) loss function. Finally
the per-sentence envelopes are merged into a global
representation of the loss along that direction. Our
implementation successively merges adjacent pairs
of piecewise smooth loss function representations
until a single list remains. These logS passes lead to
a merging runtime of O(SM logS).
The time required to compute a gradient is pro-
portional to O(SMD). For each sentence, we first
gather the probability and its gradient, then use this to
compute expected n-gram counts and matches as well
as those gradients in time O(MD). A constant num-
ber of arithmetic operations suffice to compute the
final expected loss value and its gradient. Therefore,
computing the gradient does not increase the algo-
rithmic complexity when compared to conventional
approaches using coordinate ascent and random di-
rections. Likewise the runtime of a single iteration
is competitive with PRO, given that gradient finding
is generally the most expensive part of convex opti-
mization. Of course, it is difficult to compare overall
runtime of convex optimization with that of MERT,
as we know of no way to bound the number of gradi-
ent evaluations required for convergence with MERT.
Therefore, we resort to empirical comparison later in
the paper, and find that the two methods appear to
have comparable runtime.
6In the special case where the difference between the prior
direction and the current direction is sparse, we may update the
individual linear functions in time proportional to the number of
changed dimensions. Coordinate ascent in particular can update
the linear functions in time O(SM): to the intercept of the
equation for each translation, we may add the prior step size
multiplied by the feature value in the prior coordinate, and the
slope becomes the feature value in the new coordinate. However,
this optimization does not appear to be widely adopted, likely
because it does not lead to any speedup when random vectors,
conjugate directions, or other non-sparse directions are used.
1953
Language pair Train Tune Dev Test
G
B
M
Chinese-English 0.99M 1,797 1,000 1,082
(mt02+03) (mt05)
Finnish-English 2.20M 11,935 2,001 4,855
S
pa
rs
eH
R
M Chinese-English 3.51M 1,894 1,664 1,357
(mt05) (mt06) (mt08)
Arabic-English 1.49M 1,663 1,360 1,313
(mt06) (mt08) (mt09)
Table 1: Datasets for the two experimental conditions.
5 Experimental Design
Following Hopkins and May (2011), our experimen-
tal setup utilizes both real and synthetic data. The
motivation for using synthetic data is that it is a way
of gauging the quality of optimization methods, since
the data is constructed knowing the global optimum.
Hopkins and May also note that the use of an ob-
jective function that is linear in some gold weight
vector makes the search much simpler than in a real
translation setting, and they suggest that a learner
that performs poorly in such a simple scenario has
little hope of succeeding in a more complex one.
The setup of our synthetic data experiment is al-
most the same as that performed by Hopkins and
May (2011). We generate feature vectors of dimen-
sionality ranging from 10 to 1000. These features are
generated by drawing random numbers uniformly in
the interval [0, 500]. This synthetic dataset consists
of S=1000 source ?sentences?, and M=500 ?trans-
lation? hypotheses for each sentence. A pseudo
?BLEU? score is then computed for each hypothe-
sis, by computing the dot product between a prede-
fined gold weight vector w? and each feature vector
hs,m. By this linear construction, w? is guaranteed
to be a global optimum.7 The pseudo-BLEU score is
normalized for each M -best list, so that the transla-
tion with highest model score according to w? has
a BLEU score of 1, and so that the translation with
lowest model score for the sentence gets a BLEU of
zero. This normalization has no impact on search,
but makes results more interpretable.
For our translation experiments, we use multi-
stack phrase-based decoding (Koehn et al, 2007).
We report results for two feature sets: non-linear
features induced using Gradient Boosting Machines
(Toutanova and Ahn, 2013) and sparse lexicalized
7The objective function remains piecewise constant, and the
plateau containingw? maps to the optimal value of the function.
reordering features (Cherry, 2013). We exploit these
feature sets (GBM and SparseHRM, respectively) in
two distinct experimental conditions, which we de-
tail in the two next paragraphs. Both GBM and
SparseHRM augment baseline features similar to
Moses?: relative frequency and lexicalized phrase
translation scores for both translation directions; one
or two language model features, depending on the
language pair; distortion penalty; word and phrase
count; six lexicalized reordering features. For both
experimental conditions, phrase tables have maxi-
mum phrase length of 7 words on either side. In
reference to Table 1, we used the training set (Train)
for extracting phrase tables and language models; the
Tune set for optimization with MERT or PRO; the
Dev set for selecting hyperparameters of PRO and
regularized MERT; and the Test set for reporting fi-
nal results. In each experimental condition, we first
trained weights for the base feature sets, and then
decoded the Tune, Dev, and Test datasets, generating
500-best lists for each set. All results report rerank-
ing performance on these lists with different feature
sets and optimization methods, based on lower-cased
BLEU (Papineni et al, 2001).
The GBM feature set (Toutanova and Ahn, 2013)
consists of about 230 features automatically induced
using decision tree weak learners, which derive fea-
tures using various word-level, phrase-level, and mor-
phological attributes. For Chinese-English, the train-
ing corpus consists of approximately one million sen-
tence pairs from the FBIS and Hong Kong portions
of the LDC data for the NIST MT evaluation and the
Tune and Test sets are from NIST competitions. A
4-gram language model was trained on the Xinhua
portion of the English Gigaword corpus and on the
target side of the bitext. For Finnish-English we used
a dataset from a technical domain of software man-
uals. For this language pair we used two language
models: one very large model trained on billions of
words, and another language model trained from the
target side of the parallel training set.
The SparseHRM set (Cherry, 2013) contains 3600
sparse reordering features. For each phrase, the fea-
tures take the form of indicators describing its orienta-
tion in the derivation, and its lexical content in terms
of word clusters or frequent words. For both Chinese-
English and Arabic-English, systems are trained on
data from the NIST 2012 MT evaluation. 4-gram
1954
 0
 0.2
 0.4
 0.6
 0.8
 1
50 100 500 1000 20  200
BL
EU
number of features
expected BLEU gradient
random directionsPowell
coordinate ascent
 0
 0.2
 0.4
 0.6
 0.8
 1
50 100 500 1000 20  200
co
sin
e
number of features
expected BLEU gradient
random directionsPowell
coordinate ascent
Figure 2: Change in BLEU score and cosine similarity
to the gold weight vector w? as the number of features
increases, using the noisy synthetic experiments. The
gradient-based direction finding method is barely affected
by the noise. The increase of the number of dimensions en-
ables our direction finder to find a slightly better optimum,
which moved away from w? due to noise.
language models were trained on the target side of
the parallel training data for both Arabic and Chinese.
The Chinese systems development set is taken from
the NIST mt05 evaluation set, augmented with some
material reserved from our NIST training corpora in
order to better cover newsgroup and weblog domains.
6 Results
We conducted experiments with the synthetic data
scenario described in the previous section, as well
as with noise added to the data (Hopkins and May,
2011). The purpose of adding noise is to make the
optimization task more realistic. Specifically, af-
ter computing all pseudo-BLEU scores, we added
noise to each feature vector hs,m by drawing from
a zero-mean Gaussian with standard deviation 200.
Our results with both noiseless and noisy data yield
the same conclusion as Hopkins and May: standard
MERT struggles with many dimensions, and fails
to recover w?. However, our experiments with the
gradient direction finder of Section 4 are much more
positive. This direction finder not only recovers w?
 40
 50
 60
 70
 80
 90
 100
 1  10  100  1000
BL
EU
line search iteration
expected BLEU gradient(noisy) expected BLEU gradient
coordinate ascent(noisy) coordinate ascent
Figure 3: Comparison of rate of convergence between
coordinate ascent and our expected BLEU direction finder
(D = 500). Noisy refers to the noisy experimental setting.
(cosine > 0.999) even with 1000 dimensions, but its
effectiveness is also visible with noisy data, as seen
in Figure 2. The decrease of its cosine is relatively
small compared to other search algorithms, and this
decrease is not necessarily a sign of search errors
since the addition of noise causes the true optimum
to be different from w?. Finally, Figure 3 shows our
rate of convergence compared to coordinate ascent.
Our experimental results with the GBM feature
set data are shown in Table 2. Each table is di-
vided into three sections corresponding respectively
to MERT (Och, 2003) with Koehn-style coordinate
ascent (Koehn, 2004), PRO, and our optimizer featur-
ing both regularization and the gradient-based direc-
tion finder. All variants of MERT are initialized with
a single starting point, which is either uniform weight
or w0. Instead of providing MERT with additional
random starting points as in Moses, we use random
walks as in (Moore and Quirk, 2008) to attempt to
move out of local optima.8 Since PRO and our opti-
mizer have hyperparameters, we use a held-out set
(Dev) for adjusting them. For PRO, we adjust three
parameters: a regularization penalty for `2, the pa-
rameter ? in the add-? smoothed sentence-level ver-
sion of BLEU (Lin and Och, 2004), and a parameter
for scaling the corpus-level length of the references.
The latter scaling parameter is discussed in (He and
8In the case of the gradient-based direction finder, we also
use the following strategy whenever optimization converges to
a (possibly local) optimum. We run one round of coordinate
ascent, and continue with the gradient direction finder as soon as
the optimum improves. If the none of the coordinate directions
helped, we stop the search.
1955
Chinese-English Finnish-English
Method Starting pt. # feat. Tune Dev Test # feat. Tune Dev Test
MERT uniform 14 33.2 19.9 32.9 15 53.0 52.6 54.8
MERT uniform 224 33.0 19.2 32.1 232 53.2 51.7 53.8
MERT w0 224 34.1 20.1 33.0 232 53.9 52.5 54.7
PRO w0 224 33.4 20.1 33.3 232 53.3 52.9 55.3
`2 MERT (v1: ||w ?w0||) w0 224 33.2 20.3 33.5 232 53.2 52.7 55.2
`2 MERT (v2: D ? 1 dimensions) w0 224 33.0 20.4 33.2 232 52.9 52.6 55.0
`2 MERT (v3: `1-renormalized) w0 224 33.1 20.0 33.3 232 53.1 52.5 55.1
`0 MERT w0 224 33.4 20.3 33.2 232 53.2 52.6 55.1
Table 2: BLEU scores for GBM features. Model parameters were optimized on the Tune set. For PRO and regularized
MERT, we optimized with different hyperparameters (regularization weight, etc.), and retained for each experimental
condition the model that worked best on Dev. The table shows the performance of these retained models.
 51.2
 51.4
 51.6
 51.8
 52
 52.2
 52.4
 52.6
 1e-05  0.0001  0.001  0.01  0.1  1  10
BL
EU
regularization weight
expected BLEU gradient
coordinate ascent
Figure 4: BLEU score on the Finnish Dev set (GBM)
with different values for the 1/2?2 regularization weight.
To enable comparable results, the other hyperparameter
(length) is kept fixed.
Deng, 2012; Nakov et al, 2012) and addresses the
problem that systems tuned with PRO tend to pro-
duce sentences that are too short. On the other hand,
regularized MERT only requires one hyperparameter
to tune: a regularization penalty for `2 or `0. How-
ever, since PRO optimizes translation length on the
Dev dataset and MERT does so using the Tune set, a
comparison of the two systems would yield a discrep-
ancy in length that would be undesirable. Therefore,
we add another hyperparameter to regularized MERT
to tune length in the same manner using the Dev set.
Table 2 offers several findings. First, unregular-
ized MERT can achieve competitive results with a
small set of highly engineered features, but adding a
large set of more than 200 features causes MERT to
perform poorly, particularly on the test set. However,
unregularized MERT can recover much of this drop
of performance if it is given a good sparse initializer
w0. Regularized MERT (v1) provides an increase in
the order of 0.5 BLEU on the test set compared to
the best results with unregularized MERT. Regular-
ized MERT is competitive with PRO, even though the
number of features is relatively large. Using the same
GBM experimental setting, Figure 4 compares regu-
larized MERT using the gradient direction finder and
coordinate ascent. At the best regularization setting,
the two algorithms are comparable in terms of BLEU
(though coordinate ascent is slower due to its lack of
a good direction finder), but our method seems more
robust with suboptimal regularization parameters.
Our results with the SparseHRM feature set data
are shown in Table 3. As with the GBM feature set,
we find again that the version of `2 MERT regular-
ized towards ||w ?w0|| is competitive with PRO,
even though we train MERT with a large set of 3601
features.9 One remaining question is whether MERT
remains practical with large feature sets. As noted
in the complexity analysis of Section 4.3, MERT
has a dependence on the number of features that is
comparable to PRO, i.e., it is linear in both cases.
Practically, we find that optimization time is com-
parable between the two systems. In the case of
Chinese-English for the GBM feature set, one run of
the PRO optimizer took 26 minutes on average, while
regularized MERT with the gradient direction finder
took 37 minutes on average, taking into account the
time to compute w0. In the case of Chinese-English
for the SparseHRM feature set, average optimization
times for PRO and our method were 3.10 hours and
3.84 hours on average, respectively.
9We note that the experimental setup of (Cherry, 2013) inte-
grates the Sparse HRM features into the decoder, while we use
them in an M -best reranking scenario. The reranking setup of
this paper yields smaller improvements for both PRO and MERT
than those of (Cherry, 2013).
1956
Chinese-English Arabic-English
Method Starting pt. # feat. Tune Dev Test # feat. Tune Dev Test
MERT uniform 14 25.7 34.0 27.8 14 43.2 42.8 45.5
MERT uniform 3601 25.4 33.1 27.3 3601 45.7 42.3 44.9
MERT w0 3601 27.7 33.5 27.5 3601 46.0 42.4 45.2
PRO w0 3601 25.9 34.3 28.1 3601 44.6 43.4 46.1
`2 MERT (v1: ||w ?w0||) w0 3601 26.3 34.3 28.3 3601 45.2 43.2 46.0
`2 MERT (v2: D ? 1 dimensions) w0 3601 26.4 34.1 28.2 3601 45.0 43.4 45.9
`2 MERT (v3: `1-renormalized) w0 3601 26.1 34.0 27.9 3601 44.9 43.3 45.7
`0 MERT w0 3601 26.5 34.2 28.1 3601 45.4 43.1 46.0
Table 3: BLEU scores for SparseHRM features. Notes in Table 2 also apply here.
Finally, as shown in Table 2, we see that MERT ex-
periments that rely on a good initial starting point w0
generally perform better than when starting from
a uniform vector. While having to compute w0 in
the first place is a bit of a disadvantage compared
to standard MERT, the need for good initializer is
hardly surprising in the context of non-convex op-
timization. Other non-convex problems in machine
learning, such as deep neural networks (DNN) and
word alignment models, commonly require such ini-
tializers in order to obtain decent performance. In
the case of DNN, extensive research is devoted to the
problem of finding good initializers.10 In the case of
word alignment, it is common practice to initialize
search in non-convex optimization problems?such
as IBM Model 3 and 4 (Brown et al, 1993)?with
solutions of simpler models?such as IBM Model 1.
7 Related work
MERT and its extensions have been the target of ex-
tensive research (Och, 2003; Macherey et al, 2008;
Cer et al, 2008; Moore and Quirk, 2008; Kumar et
al., 2009; Galley and Quirk, 2011). More recent work
has focused on replacing MERT with a linearly de-
composable approximations of the evaluation metric
(Smith and Eisner, 2006; Liang et al, 2006; Watan-
abe et al, 2007; Chiang et al, 2008; Hopkins and
May, 2011; Rosti et al, 2011; Gimpel and Smith,
2012; Cherry and Foster, 2012), which generally
involve a surrogate loss function incorporating a reg-
ularization term such as the `2-norm. While we are
not aware of any previous work adding a penalty on
10For example, (Larochelle et al, 2009) presents a pre-trained
DNN that outperforms a shallow network, but the performance
of the DNN becomes much worse relative to the shallow network
once pre-training is turned off.
the weights in the context of MERT, (Cer et al, 2008)
achieves a related effect. Cer et al?s goal is to achieve
a more regular or smooth objective function, while
ours is to obtain a more regular set of parameters.
The two approaches may be complementary.
More recently, new research has explored direction
finding using a smooth surrogate loss function (Flani-
gan et al, 2013). Although this method is successful
in helping MERT find better directions, it also exac-
erbates the tendency of MERT to overfit.11 As an
indirect way of controlling overfitting on the tuning
set, their line searches are performed over directions
estimated over a separate dataset.
8 Conclusion
In this paper, we have shown that MERT can scale to
a much larger number of features than previously
thought, thanks to regularization and a direction
finder that directs the search towards the greatest
increase of expected BLEU score. While our best
results are comparable to PRO and not significantly
better, we think that this paper provides a deeper un-
derstanding of why standard MERT can fail when
handling an increasingly larger number of features.
Furthermore, this paper complements the analysis
by Hopkins and May (2011) of the differences be-
tween MERT and optimization with a surrogate loss
function.
Acknowledgments
We thank the anonymous reviewers for their helpful
comments and suggestions.
11Indeed, in their Table 3, a comparison between HILS and
HOLS suggests tuning set performance improves substantially,
while held out performance degrades.
1957
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: parameter estimation.
Comput. Linguist., 19(2):263?311.
Daniel Cer, Dan Jurafsky, and Christopher D. Manning.
2008. Regularization and search for minimum error
rate training. In Proceedings of the Third Workshop on
Statistical Machine Translation, pages 26?34.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 427?436.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceedings
of the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 22?31.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and structural
translation features. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 224?233.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 218?226.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell. 2013.
Large-scale discriminative training for statistical ma-
chine translation using held-out line search. In Pro-
ceedings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 248?258.
Michel Galley and Chris Quirk. 2011. Optimal search
for minimum error rate training. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, pages 38?49.
Kevin Gimpel and Noah A. Smith. 2012. Structured ramp
loss minimization for machine translation. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 221?231.
Xiaodong He and Li Deng. 2012. Maximum expected
BLEU training of phrase and lexicon translation mod-
els. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics: Long
Papers - Volume 1, pages 292?301.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352?1362.
M. Hyder and K. Mahata. 2009. An approximate L0
norm minimization algorithm for compressed sens-
ing. In Acoustics, Speech and Signal Processing,
2009. ICASSP 2009. IEEE International Conference
on, pages 3365?3368.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of ACL, Demonstration Session.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation models.
In Proc. of AMTA, pages 115?124.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 163?171.
Hugo Larochelle, Yoshua Bengio, Je?ro?me Louradour, and
Pascal Lamblin. 2009. Exploring strategies for training
deep neural networks. J. Mach. Learn. Res., 10:1?40.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In International Conference on Com-
putational Linguistics and Association for Computa-
tional Linguistics (COLING/ACL).
Chin-Yew Lin and Franz Josef Och. 2004. ORANGE:
a method for evaluating automatic evaluation metrics
for machine translation. In Proceedings of the 20th
international conference on Computational Linguistics,
Stroudsburg, PA, USA.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum error
rate training for statistical machine translation. In Pro-
ceedings of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 725?734.
David McAllester and Joseph Keshet. 2011. Generaliza-
tion bounds and consistency for latent structural probit
and ramp loss. In Advances in Neural Information
Processing Systems 24, pages 2205?2212.
Robert C. Moore and Chris Quirk. 2008. Random restarts
in minimum error rate training for statistical machine
translation. In Proceedings of the 22nd International
Conference on Computational Linguistics - Volume 1,
pages 585?592.
1958
Preslav Nakov, Francisco Guzman, and Stephan Vogel.
2012. Optimizing for sentence-level BLEU+1 yields
short translations. In Proceedings of COLING 2012,
pages 1979?1994.
Franz Josef Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of 40th Annual
Meeting of the Association for Computational Linguis-
tics, pages 295?302.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic evalu-
ation of machine translation. In Proc. of ACL.
Kishore Papineni. 1999. Discriminative training via linear
programming. In Proceedings IEEE International Con-
ference on Acoustics, Speech, and Signal Processing
(ICASSP), volume 2, pages 561?564, Vol. 2.
M.J.D. Powell. 1964. An efficient method for finding
the minimum of a function of several variables without
calculating derivatives. Comput. J., 7(2):155?162.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2007. Numerical Recipes:
The Art of Scientific Computing. Cambridge University
Press, 3rd edition.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and
Richard Schwartz. 2011. Expected BLEU training
for graphs: BBN system description for WMT11 sys-
tem combination task. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
159?165.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proceed-
ings of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 787?794.
Kristina Toutanova and Byung-Gyu Ahn. 2013. Learn-
ing non-linear features for machine translation using
gradient boosting machines. In Proceedings of the 51st
Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 406?411.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 764?773.
1959
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1250?1260,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Large-scale Expected BLEU Training of Phrase-based Reordering Models
Michael Auli, Michel Galley, Jianfeng Gao
Microsoft Research
Redmond, WA, USA
{michael.auli,mgalley,jfgao}@microsoft.com
Abstract
Recent work by Cherry (2013) has shown
that directly optimizing phrase-based re-
ordering models towards BLEU can lead
to significant gains. Their approach is lim-
ited to small training sets of a few thou-
sand sentences and a similar number of
sparse features. We show how the ex-
pected BLEU objective allows us to train
a simple linear discriminative reordering
model with millions of sparse features on
hundreds of thousands of sentences re-
sulting in significant improvements. A
comparison to likelihood training demon-
strates that expected BLEU is vastly more
effective. Our best results improve a hi-
erarchical lexicalized reordering baseline
by up to 2.0 BLEU in a single-reference
setting on a French-English WMT 2012
setup.
1 Introduction
Modeling reordering for phrase-based machine
translation has been a long standing problem.
Contrary to synchronous context free grammar-
based translation models (Wu, 1997; Galley et al.,
2004; Galley et al., 2006; Chiang, 2007), phrase-
based models (Koehn et al., 2003; Och and Ney,
2004) have no in-built notion of reordering beyond
what is captured in a single phrase pair, and the
first phrase-based decoders simply scored inter-
phrase reorderings using a restricted linear dis-
tortion feature, which scores a phrase reordering
proportionally to the length of its displacement.
While phrase-based models allow in theory com-
pletely unrestricted reordering patterns, move-
ments are generally limited to a finite distance for
complexity reasons. To address this limitation,
extensive prior work focused on richer feature
sets, in particular on lexicalized reordering mod-
els trained with maximum likelihood-based ap-
proaches (Tillmann, 2003; Xiong et al., 2006; Gal-
ley and Manning, 2008; Nguyen et al.,2009;?2).
More recently, Cherry (2013) proposed a very
effective sparse ordering model relying on a set
of only a few thousand indicator features which
are trained towards a task-specific metric such as
BLEU (Papineni et al., 2002). These features
are simply added to the log-linear framework of
translation that is trained with the Margin Infused
Relaxed Algorithm (MIRA; Chiang et al., 2009)
on a small development set of a few thousand
sentences. While simple, the approach outper-
forms the state-of-the-art hierarchical reordering
model of Galley and Manning (2008), a maximum
likelihood-based model trained on millions of sen-
tences to fit millions of parameters.
Ideally, we would like to scale sparse reorder-
ing models to similar dimensions but recent at-
tempts to increase the amount of training data for
MIRA was met with little success (Eidelman et
al., 2013). In this paper we propose much larger
sparse ordering models that combine the scalabil-
ity of likelihood-based approaches with the higher
accuracy of maximum BLEU training (?3). We
train on the output of a hierarchical reordering
model-based system and scale to millions of fea-
tures learned on hundreds of thousands of sen-
tences (?4). Specifically, we use the expected
BLEU objective function (Rosti et al., 2010; Rosti
et al., 2011; He and Deng, 2012; Gao and He,
2013; Gao et al., 2014; Green et al., 2014) which
allows us to train models that use training data and
feature sets that are two to three orders of magni-
tudes larger than in previous work (?5).
Our models significantly outperform the
state-of-the-art hierarchical lexicalized reordering
model on two language pairs and we demonstrate
that richer feature sets result in significantly
higher accuracy than with a feature set similar
to Cherry (2013). We also demonstrate that our
1250
approach greatly benefits from more training
data than is typically used for maximum BLEU
training. Previous work concluded that sparse
reordering models perform better than maximum
entropy models, however, the two approaches
do not only differ in the objective function but
also the type of training data (Cherry, 2013). Our
analysis isolates the objective function and shows
that expected BLEU optimization is the most
important factor to train accurate ordering models.
Finally, we compare expected BLEU training to
pair-wise ranked optimization (PRO) on a feature
set similar to Cherry (2013; ?7).
2 Reordering Models
Reordering models for phrase-based translation
are typically part of the log-linear framework
which forms the basis of many statistical machine
translation systems (Och and Ney, 2004).
Formally, we are given K training pairs D =
(f
(1)
, e
(1)
)...(f
(K)
, e
(K)
), where each f
(i)
? F
is drawn from a set of possible foreign sentences,
and each English sentence e
(i)
? E(f
(i)
) is drawn
from a set of possible English translations of f
(i)
.
The log-linear model is parameterized by m pa-
rameters ? where each ?
k
? ? is the weight of
an associated feature h
k
(f, e) such as a language
model or a reordering model. Function h(f, e)
maps foreign and English sentences to the vector
h
1
(f, e)...h
m
(f, e), and we usually choose trans-
lations e? according to the following decision rule:
e? = arg max
e?E(f)
?
T
h(f, e) (1)
In practice, computing e? exactly is intractable and
we resort to an approximate but more efficient
beam search (Och and Ney, 2004).
Early phrase-based models simply relied on a
linear distortion feature, which measures the dis-
tance between the first word of the current source
phrase and the last word of the previous source
phrase (Koehn et al., 2003; Och and Ney, 2004).
Unfortunately, this approach is agnostic to the ac-
tual phrases being reordered, and does not take
into account that certain phrases are more likely
to be reordered than others. This shortcoming led
to a range of lexicalized reordering models that
capture exactly those preferences for individual
phrases (Tillmann, 2003; Koehn et al., 2007).
Reordering models generally assume a se-
quence of English phrases e = {e?
1
, . . . , e?
n
} cur-
rently hypothesized by the decoder, a phrase align-
ment a = {a
1
, . . . , a
n
} that defines a foreign
phrase
?
f
a
i
for each English phrase e?
i
, and an ori-
entation o
i
which describes how a phrase pair
should be reordered with respect to the previous
phrases. There are typically three orientation types
and the exact definition depends on the specific
models which we describe below. Orientations can
be determined during decoding and from word-
aligned training corpora. Most models estimate
a probability distribution p(o
i
|pp
i
, a
1
, . . . , a
i
) for
the i-th phrase pair pp
i
= ?e?
i
,
?
f
a
i
? and the align-
ments a
1
, . . . , a
i
of the previous target phrases.
Lexicalized Reordering. This model defines the
three orientation types based only on the posi-
tion of the current and previously translated source
phrase a
i
and a
i?1
, respectively (Tillmann, 2003;
Koehn et al., 2007). The orientation types gen-
erally are: monotone (M), indicating that a
i?1
is
directly followed by a
i
. swap (S) assumes that a
i
precedes a
i?1
, i.e., the two phrases swap places.
Finally, discontinuous (D) indicates that a
i
is not
adjacent to a
i?1
. The probability distribution over
these reordering events is based on a maximum
likelihood estimate:
p(o|pp, a
i?1
, a
i
) =
cnt(o, pp)
cnt(pp)
(2)
where o ? {M,S,D} and cnt returns smoothed
frequency counts over a word-aligned corpus.
Hierarchical Reordering. An extension of the
lexicalized reordering model better handles long-
distance reordering by conditioning the orientation
of the current phrase on a context larger than just
the previous phrase (Galley and Manning, 2008).
In particular, the hierarchical reordering model
does so by building a compact representations
of the preceding context using an efficient shift-
reduce parser. During translation new phrases get
moved on a stack and are then combined with any
previous phrase if they are adjacent. Figure 1
shows an illustrative example: when the decoder
shifts phrase pp
8
onto the stack, this phrase is then
merged with pp
7
(reduce operation), which then
can be merged with previous phrases to finally
form a hierarchical block h
1
. These merge opera-
tions stop once we reach a phrase (here, pp
3
) that
is not contiguous with the current block. Then, as
another phrase (pp
9
) is hypothesized, the decoder
uses the hierarchical block at the top of the stack
(h
1
) to determine the orientation of the current
1251
    	 
       therussiansidehopestoholdconsultationswithiranonthisissueinthenearfuture..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
h1
pp9
pp8
pp7
pp3
d
e
c
o
d
e
d
 
o
u
t
p
u
t
input sentence
Figure 1: The hierarchical reordering model
(HRM) analyzes a non-local context to determine
the orientation of the current phrase. For exam-
ple, the phrase pair pp
9
has a swap orientation
(o
9
= S) with respect to a hierarchical block (h
1
)
that comprises the five preceding phrase pairs.
phrase pp
9
, which in this case is a swap (S) orien-
tation.
1
The model has the advantage that the ori-
entations computed are more robust to derivational
ambiguity of the underlying translation model. A
given surface translation may be derived through
different phrases but the shift-reduce parser com-
bines them into a single representation which is
more consistent with the orientations observed in
the word-aligned training data.
Maximum Entropy-based models. The statis-
tics used to estimate the lexicalized and the hierar-
chical reordering models are based on very sparse
estimates, simply because certain phrases are not
very frequent. Maximum entropy models address
this problem by estimating Eq. 2 through sparse
indicator features over phrase pairs instead, but
prior work with such models still relies on word
aligned corpora for estimation (Xiong et al., 2006;
Nguyen et al., 2009). However, recent evalua-
tions of the approach show little gain over the sim-
pler frequency-based estimation method (Cherry,
2013).
Sparse Hierarchical Reordering model. All of
the models so far are trained to maximize the like-
lihood of reordering decisions observed in word
aligned corpora. Cherry (2013) argues that it
is probably too difficult to learn human reorder-
ing patterns through noisy word alignments that
1
Galley and Manning (2008) provide a more formal ex-
planation.
were generated by unsupervised methods. Instead,
he proposes to learn a discriminative reordering
model based on the outputs of the actual machine
translation system, adjusting the feature weights
to maximize a task-specific objective, which is
BLEU in their case. Their model is based on a
set of sparse features derived from the hierarchi-
cal reordering model which we scale to millions
of features (?6).
3 A Simple Linear Reordering Model
Our reordering model is defined as a simple linear
model over the basic orientation types, similar to
Cherry (2013). In particular, our model defines
score s
?
(o, e, f) over orientations o = {M,S,D},
and a sentence pair {e, f, a} with alignment a as a
linear combination of weighted indicator features:
s
?
(o, e, f, a) = ?
T
u(o, e, f, a)
=
I
?
i=1
?
T
u(o, pp
i
, c
i
)
=
I
?
i=1
s
?
(o, pp
i
, c
i
) (3)
where ? is a vector of weights, {pp
i
}
I
i=1
is a
set of phrases that decompose the sentence pair
{e, f, a}, and u(o, pp
i
, c
i
) is a function that maps
orientation o, phrase pair pp
i
and local context c
i
to a sparse vector of indicator features. The lo-
cal context c
i
represents information used by the
model that is in addition to the phrase pair. For
example, the features of Cherry (2013) condition
on the top-stack of the hierarchical shift reduce
parser, information that is non-local with respect
to the phrase pair. In our experiments, we use fea-
tures that go beyond the top-stack, in order to con-
dition on various parts of the source and target side
contexts (?7).
4 Model Training
Optimization of our model is based on standard
stochastic gradient descent (SGD; Bottou, 2004)
with an expected BLEU loss l(?) which we detail
next (?5). The update is:
?
t
= ?
t?1
? ?
?l(?
t?1
)
??
t?1
(4)
where ?
t
and ?
t?1
are model weights at time t and
t? 1 respectively, and ? is a learning rate.
We add the model as a small number of dense
features to the log-linear framework of translation
1252
(Eq. 1). Specifically, we extend the m baseline
features by a set of new features h
m+1
, . . . , h
m+j
,
where each represents a linear combination of
sparse indicator features corresponding to one of
the orientation types. Exposing each orientation
as a separate dense feature within the log-linear
model is common practice for lexicalized reorder-
ing models (Koehn et al., 2005):
h
m+j
= s
?
(o
j
, e, f, a)
where o
j
? {M,S,D}.
The translation model is then parameterized by
both ?, the log-linear weights of the baseline fea-
tures, as well as ?, the weights of the reordering
model. The reordering model is learned as follows
(Gao and He, 2013; Gao et al., 2014):
1. We first train a baseline translation system to
learn ?, without the discriminative reordering
model, i.e., we set ?
m+1
= 0, . . . , ?
m+j
= 0.
2. Using these weights, we generate n-best lists
for the foreign sentences in the training data
using the setup described in the experimental
section (?7). The n-best lists serve as an ap-
proximation to E(f), the set of possible trans-
lations of f , used in the next step for expected
BLEU training of the reordering model (?5).
3. Next, we fix ?, set ?
m+1
= 1, . . . ?
m+j
= 1
and optimize ? with respect to the loss func-
tion on the training data using stochastic gra-
dient descent.
2
4. Finally, we fix ? and re-optimize ? in the
presence of the discriminative reordering
model using Minimum Error Rate Training
(MERT; Och 2003; ?7).
We found that re-optimizing ? after a few iter-
ations of stochastic gradient descent in step 3 did
not improve accuracy.
5 Expected BLEU Objective Function
The expected BLEU objective (Gao and He, 2013;
Gao et al., 2014) allows us to efficiently optimize
a large scale discriminative reordering model to-
wards the desired task-specific metric, which in
our setting is BLEU.
2
We tuned ?
m+1
, . . . ?
m+j
on the development set but
found that setting them uniformly to one resulted in faster
training and equal accuracy.
Formally, we define our loss function l(?) as
the negative expected BLEU score, denoted as
xBLEU(?), for a given foreign sentence f and a
log-linear parameter set ?:
l(?) =? xBLEU(?)
=?
?
e?E(f)
p
?,?
(e|f) sBLEU(e, e
(i)
) (5)
where sBLEU(e, e
(i)
) is a smoothed sentence-
level BLEU score with respect to the reference
translation e
(i)
, and E(f) is the generation set ap-
proximated by an n-best list. In our experiments
we use n-best lists with unique entries and there-
fore our definitions do not take into account mul-
tiple derivations of the same translation. Specif-
ically, our n-best lists are generated by choosing
the highest scoring derivation e? amongst string
identical translations e for f . We use a sentence-
level BLEU approximation similar to Gao et al.
(2014).
3
Finally, p
?,?
(e|f) is the normalized prob-
ability of translation e given f , defined as:
p
?,?
(e|f) =
exp{??
T
h(f, e)}
?
e
?
?E(f)
exp{??
T
h(f, e
?
)}
(6)
where ?
T
h(f, e) includes the discriminative re-
ordering model h
m+1
(e, f), . . . , h
m+j
(e, f) pa-
rameterized by ?, and ? ? [0, inf) is a tuned scal-
ing factor that flattens the distribution for ? < 1
and sharpens it for ? > 1 (Tromble et al., 2008).
4
Next, we define the gradient of the expected
BLEU loss function l(?). To simplify our notation
we omit the local context c in s
?
(o, pp, c) (Eq. 3)
from now on and assume it to be part of pp. Us-
ing the observation that the loss does not explicitly
depend on ?, we get:
?l(?)
??
=
?
o,pp
?l(?)
?s
?
(o, pp)
?s
?
(o, pp)
??
=
?
o,pp
??
o,pp
u(o, pp)
where ?
o,pp
is the error term for orientation o of
phrase pair pp:
?
o,pp
= ?
?l(?)
?s
?
(o, pp)
3
We found in early experiments that the BLEU+1 approx-
imation used by Liang et al. (2006) and Nakov et. al (2012)
worked equally well in our setting.
4
? is only used during expected BLEU training.
1253
The error term indicates how the expected BLEU
loss changes with the reordering score which we
derive in the next section.
Finally, the gradient of the reordering score
s
?
(o, pp) with respect to ? is simply given by this:
?s
?
(o, pp)
??
=
??
T
u(o, pp)
??
= u(o, pp)
5.1 Derivation of the Error Term ?
o,pp
We rewrite the loss function (Eq. 5) using Eq. 6
and separate it into two terms G(?) and Z(?):
l(?) = ?xBLEU(?) = ?
G(?)
Z(?)
(7)
= ?
?
e?E(f)
exp{??
T
h(f, e)} sBLEU(e, e
(i)
)
?
e
?
?E(f)
exp{??
T
h(f, e
?
)}
Next, we apply the quotient rule of differentiation:
?
o,pp
=
?xBLEU(?)
?s
?
(o, pp)
=
?(G(?)/Z(?))
?s
?
(o, pp)
=
1
Z(?)
(
?G(?)
?s
?
(o, pp)
?
?Z(?)
?s
?
(o, pp)
xBLEU(?)
)
The gradients for G(?) and Z(?) with respect to
s
?
(o, pp) are:
?G(?)
?s
?
(o, pp)
=
?
e?E(f)
sBLEU(e, e
(i)
)
? exp{??
T
h(f, e)}
?s
?
(o, pp)
?Z(?)
?s
?
(o, pp)
=
?
e?E(f)
? exp{??
T
h(f, e)}
?s
?
(o, pp)
By using the following definition:
U(?, e) = sBLEU(e, e
(i)
)? xBLEU(?)
together with the chain rule, Eq. 6 and Eq. 7, we
can rewrite ?
o,pp
as follows:
?
o,pp
=
1
Z(?)
?
e?E(f)
(
? exp{??
T
h(f, e)}
?s
?
(o, pp)
U(?, e)
)
=
?
e?E(f)
(
p
?,?
(e|f)
???
T
h(f, e)
?s
?
(o, pp)
U(?, e)
)
Because ? is only relevant to the reordering
model, represented by h
m+1
, . . . , h
m+j
, we have:
???
T
h(f, e)
?s
?
(o, pp)
= ??
k
?h
k
(e, f)
?s
?
(o, pp)
= ??
k
N (o, pp, e, f)
1: function TRAINSGD(D, ?)
2: t? 0
3: for all (f
(i)
, e
(i)
) in D do
4: xBLEU = 0 . Compute xBLEU
5: for all e in E(f
(i)
) do
6: wBLEU? p
?,?
t
(e|f) sBLEU(e, e
(i)
)
7: xBLEU? xBLEU + wBLEU
8: end for
9: for all e in E(f
(i)
) do
10: D = sBLEU(e, e
(i)
)? xBLEU
11: for all o, pp in ?e, f
(i)
? do
12: N = N (o, pp, e, f)
13: ?
o,pp
= p
?,?
t
(e|f
(i)
)??
k
ND
14: ?
t+1
= ?
t
? ??
o,pp
u(o, pp))
15: end for
16: end for
17: t? t+ 1
18: end for
19: end function
Figure 2: Algorithm for computing the expected
BLEU loss with SGD updates (Eq. 4) based on
training data D and learning rate ?.
where m + 1 ? k ? m + j and N (o, pp, e, f) is
the number of times pp with orientation o occurs
in the current sentence pair.
This simplifies the error term to:
?
o,pp
=
?
e?E(f)
p
?,?
(e|f)??
k
N (o, pp, e, f)U(?, e)
(8)
where ?
k
is the weight of the dense feature sum-
marizing orientation o in the log-linear model. We
use Eq. 8 in a simple algorithm to train our model
(Figure 2). Our SGD trainer uses a mini-batch size
of a single sentence (?7) which entails all hypoth-
esis in the n-best list for this sentence and the pa-
rameters are updated after each mini-batch.
6 Feature Sets
Our features are inspired by Cherry (2013)
who bases his features on the local phrase-pair
pp = ?e?,
?
f? as well as the top stack of the shift re-
duce parser of the baseline hierarchical ordering
model. We experiment with these variants and ex-
tensions:
? SparseHRMLocal: This feature set is exclu-
sively based on the local phrase-pair and
1254
consists of features over the first and last
word of both the source and target phrase.
5
We use four different word representations:
The word identity itself, but only for the
80 most common source and target language
words. The three other word representations
are based on Brown clustering with either 20,
50 or 80 classes (Brown et al., 1992). There
is one feature for every orientation type.
? SparseHRM: The main feature set of Cherry
(2013). This is an extension of SparseHRM-
Local adding features based on the first and
last word of both the source and the target of
the hierarchical block at the top of the stack.
There are also features based on the source
words in-between the current phrase and the
hierarchical block at the top of the stack.
? SparseHRM+UncommonWords: This set is
identical to SparseHRM, except that word-
identity features are not restricted to the 80
most frequent words, but can be instantiated
for all words, regardless of frequency.
? SparseHRM+BiPhrases: This augments
SparseHRM by phrase-identity features re-
sulting in millions of instances compared to
only a few thousand for SparseHRM. We add
three features for each possible phrase pair:
the source phrase, the target phrase, and the
whole phrase pair.
The baseline hierarchical lexicalized reorder-
ing model is most similar to SparseHRM+BiPhrases
feature set since both have parameters for phrase,
orientation pairs.
6
The feature set closest to
Cherry (2013) is SparseHRM. However, while
Cherry had to severely restrict his features for
batch lattice MIRA-based training, our maximum
expected BLEU approach can handle millions of
features.
7 Experiments
Baseline. We experiment with a phrase-based
system similar to Moses (Koehn et al., 2007),
5
Phrase-local features allow pre-computation which re-
sults in significant speed-ups at run-time. Cherry (2013)
shows that local features are responsible for most of his gains.
6
Although, our model is likely to learn significantly fewer
parameters since many phrase, orientation pairs will only be
seen in the word-aligned data but not in actual machine trans-
lation output.
scoring translations by a set of common fea-
tures including maximum likelihood estimates
of source given target phrases p
MLE
(e|f) and
vice versa, p
MLE
(f |e), lexically weighted esti-
mates p
LW
(e|f) and p
LW
(f |e), word and phrase-
penalties, as well as a linear distortion feature.
The baseline uses a hierarchical reordering model
with five orientation types, including monotone
and swap, described in ?2, as well as two discon-
tinuous orientations, distinguishing if the previous
phrase is to the left or right of the current phrase.
Finally, monotone global indicates that all previ-
ous phrases can be combined into a single hier-
archical block. The baseline includes a modified
Kneser-Ney word-based language model trained
on the target-side of the parallel data, which is de-
scribed below. Log-linear weights are estimated
with MERT (Och, 2003). We regard the 1-best
output of the phrase-based decoder with the hierar-
chical reordering model as the baseline accuracy.
Evaluation. We use training and test data from
the WMT 2012 campaign and report results on
French-English and German-English translation
(Callison-Burch et al., 2012). Translation mod-
els are estimated on 102M words of parallel data
for French-English and 91M words for German-
English; between 7.5-8.2M words are newswire,
depending on the language pair, and the remainder
are parliamentary proceedings. All discrimina-
tive reordering models are trained on the newswire
subset since we found this portion of the data to be
most useful in initial experiments. We evaluate on
six newswire domain test sets from 2008, 2010 to
2013 as well as the 2010 system combination test
set containing between 2034 to 3003 sentences.
Log-linear weights are estimated on the 2009 data
set comprising 2525 sentences. We evaluate using
BLEU with a single reference.
Discriminative Reordering Model. We use 100-
best lists generated by the phrase-based decoder
to train the discriminative reordering model. The
n-best lists are generated by ten systems, each
trained on 90% of the available data in order to de-
code the remaining 10%. The purpose of this pro-
cedure is to avoid a bias introduced by generating
n-best lists for sentences on which the translation
model was previously trained.
7
Unless otherwise
7
Later, we found that the bias has only a negligible effect
on end-to-end accuracy since we obtained very similar results
when decoding with a system trained on all data. This setting
increased the training data BLEU score from 27.5 to 37.8. We
used a maximum source and target phrase length of 7 words.
1255
dev 2008 2010 sc2010 2011 2012 2013 AllTest FeatTypes
noRM 23.37 20.18 24.24 24.18 24.83 24.23 24.85 23.93 -
HRM (baseline) 24.11 20.85 24.92 24.83 25.68 25.11 25.76 24.72 -
SparseHRMLocal 25.24 21.26 25.99 25.93 26.98 26.34 26.77 25.77 4,407
SparseHRM 25.29 21.43 26.17 26.14 26.99 26.63 27.01 25.95 9,463
+UncommonWords 25.32 21.76 26.30 26.29 27.15 26.77 27.18 26.12 897,537
+BiPhrases 25.46 21.67 26.19 26.19 27.55 27.07 27.41 26.26 3,043,053
Table 1: French-English results of expected BLEU trained sparse reordering models compared to no
reordering model at all (noRM) and the likelihood trained baseline hierarchical reordering model (HRM)
on WMT test sets; sc2010 is the 2010 system combination test set. FeatTypes is the number of different
types and AllTest is the average BLEU score over all the test sets, weighted by corpus size. All results
for our sparse reordering models include a likelihood-trained hierarchical reordering model.
dev 2008 2010 sc2010 2011 2012 2013 AllTest FeatTypes
noRM 18.54 19.28 20.14 20.01 18.90 18.87 21.60 19.81 -
HRM (baseline) 19.35 19.96 20.87 20.66 19.60 19.80 22.48 20.58 -
SparseHRMLocal 19.89 19.86 21.11 20.84 20.04 20.21 22.93 20.88 4,410
SparseHRM 19.83 20.27 21.26 21.05 20.22 20.44 23.17 21.11 9,477
+UncommonWords 20.06 20.35 21.45 21.31 20.28 20.55 23.30 21.24 1,136,248
+BiPhrases 20.09 20.33 21.62 21.47 20.66 20.75 23.27 21.40 3,640,693
Table 2: German-English results of expected BLEU trained sparse reordering models (cf. Table 1).
mentioned, we train our reordering model on the
news portion of the parallel data, corresponding to
136K-150K sentences, depending on the language
pair. We tuned the various hyper-parameters on a
held-out set, including the learning rate, for which
we found a simple setting of 0.1 to be useful. To
prevent overfitting, we experimented with `
2
regu-
larization, but found that it did not improve test ac-
curacy. We also tuned the probability scaling pa-
rameter ? (Eq. 6) but found ? = 1 to be very good
among other settings. We evaluate the perfor-
mance on a held-out validation set during training
and stop whenever the objective changes less than
a factor of 0.0003. For our PRO experiments, we
tuned three hyper-parameters controlling `
2
reg-
ularization, sentence-level BLEU smoothing, and
length. The latter is important to eliminate PRO?s
tendency to produce too short translations (Nakov
et al., 2012).
7.1 Scaling the Feature Set
We first compare our baseline, a likelihood trained
hierarchical reordering model (HRM; Galley &
Manning, 2008), to various expected BLEU
trained models, starting with SparseHRMLocal,
inspired by Cherry (2013) and compare it to
SparseHRM+BiPhrases, a set that is three orders of
magnitudes larger.
Our results on French-English translation (Ta-
ble 1) and German-English translation (Table 2)
show that the expected BLEU trained models scale
to millions of features and that we outperform the
baseline by up to 2.0 BLEU on newstest2012 for
French-English and by up to 1.1 BLEU on new-
stest2011 for German-English.
8
Increasing the
size of the feature set improves accuracy across
the board: The average accuracy over all test sets
improves from 1.0 BLEU for the most basic fea-
ture set to 1.5 BLEU for the largest feature set
on French-English and from 0.3 BLEU to 0.8
BLEU on German-English.
9
The most compa-
rable setting to Cherry (2013) is the feature set
SparseHRM, which we outperform by up to 0.5
BLEU on French-English and by 0.3 BLEU on av-
erage on both language pairs, demonstrating the
benefit of being able to effectively train large fea-
ture sets. Furthermore, the increase in the num-
ber of features does not affect runtime, since most
8
Different to the setups of Galley & Manning (2008) and
Cherry (2013) our WMT evaluation framework uses only one
instead of four references, which makes our BLEU score im-
provements not directly comparable.
9
We attribute smaller improvements on German-English
to the low distortion limit of only six words of our system and
the more difficult reordering patterns when translating from
German which may require more elaborate features.
1256
features can be pre-computed and stored in the
phrase-table, only requiring a constant time table-
lookup, similar to traditional reordering models.
Another appeal of our approach is that train-
ing is very fast given a set of n-best lists for the
training data. The SparseHRM model with 4,407
features is trained in only 26 minutes, while the
SparseHRM+BiPhrases model with over three mil-
lion parameters can be trained in just over two
hours (136K sentences and 100 epochs in both
cases). We attribute this to the training regime
(?4), which does not iteratively re-decode the
training data for expected BLEU training.
10
7.2 Varying Training Set Size
Previous work on sparse reordering models was
restricted to small data sets (Cherry, 2013) due
to the limited ability of standard machine trans-
lation optimizers to handle more than a few thou-
sand sentences. In particular, recent attempts to
scale the margin-infused relaxation algorithm, a
variation which was also used by Cherry (2013),
to larger data sets showed that more data does not
necessarily help to improve test set accuracy for
large feature sets (Eidelman et al., 2013).
In the next set of experiments, we shed light on
the advantage of training discriminative reordering
models with expected BLEU on large training sets.
Specifically, we start off by estimating a reorder-
ing model on only 2,000 sentences, similar to the
size of the development set used by Cherry (2013),
and incrementally increase the amount of training
data to nearly three hundred thousand sentences.
To avoid overfitting to small data sets we experi-
ment with our most basic feature set SparseHRM-
Local, comprising of just over 4,400 types.
For this experiment only, we measure accuracy
in a re-ranking framework for faster experimen-
tation where we use the 100-best output of the
baseline system relying on a likelihood-based hi-
erarchical reordering model. We re-estimate the
log-linear weights by running a further iteration of
MERT on the n-best list of the development set
which is augmented by scores corresponding to
the discriminative reordering model. The weights
of those features are initially set to one and we
use 20 random restarts for MERT. At test time we
rescore the 100-best list of the test set using the
new set of log-linear weights learned previously.
10
We would expect better accuracy when iteratively decod-
ing the training data but did not do so in this study for effi-
ciency reasons.
24.4
24.6
24.8
25.0
25.2
2K 4K 8K 16K 32K 64K 136K 272K
BLE
U
Training set size
dev
26.6 Training set size
25.6
25.8
26.0
26.2
26.4
26.6
2K 4K 8K 16K 32K 64K 136K 272K
BLE
U
Training set size
news2011
Figure 3: Effect of increasing the training set size
from 2,000 to 272,000 sentences measured on the
dev set (top) and news2011 (bottom) in an n-best
list rescoring setting.
Figure 3 confirms that more training data in-
creases accuracy and that the best model requires
a substantially larger amount of training data than
what is typically used for maximum BLEU train-
ing. We expect an even steeper curve for larger
feature sets where more parameters need to be es-
timated and where the amount of training data is
likely to have an even larger effect.
7.3 Likelihood versus BLEU Optimization
Previous research has shown that directly training
a reordering model for BLEU can vastly outper-
form a likelihood trained maximum entropy re-
ordering model (Cherry, 2013). However, the two
approaches do not only differ in the objectives
used, but also in the type of training data. The
maximum entropy reordering model is trained on
a word-aligned corpus, trying to learn human re-
ordering patterns, whereas the sparse reordering
model is trained on machine translation output,
trying to learn from the mistakes made by the ac-
tual system. It is therefore not clear how much
either one contributes to good accuracy.
Our next experiment teases those two aspects
apart and clearly shows the effect of the objec-
tive function. Specifically, we compare the tra-
ditionally used conditional log-likelihood (CLL)
objective to expected BLEU on the French-
English translation task in a small feature con-
dition (SparseHRM) of about 9K features and
1257
dev 2008 2010 sc2010 2011 2012 2013 AllTest
noRM 23.37 20.18 24.24 24.18 24.83 24.23 24.85 23.93
HRM (baseline) 24.11 20.85 24.92 24.83 25.68 25.11 25.76 24.72
SparseHRM (CLL) 24.28 21.02 25.11 25.10 25.92 25.24 25.76 24.88
SparseHRM (xBLEU) 25.29 21.43 26.17 26.14 26.99 26.63 27.01 25.95
SparseHRM+BiPhrases (CLL) 24.42 21.17 25.12 25.00 25.86 25.36 26.18 24.98
SparseHRM+BiPhrases (xBLEU) 25.46 21.67 26.19 26.19 27.55 27.07 27.41 26.26
Table 3: French-English results comparing the baseline hierarchical reordering model (HRM) to sparse
reordering model trained towards conditional log-likelihood (CLL) and expected BLEU (xBLEU).
dev 2008 2010 sc2010 2011 2012 2013 AllTest
PRO 24.05 20.90 25.42 25.28 25.79 25.09 26.07 24.94
xBLEU 25.24 21.26 25.99 25.93 26.98 26.34 26.77 25.77
Table 4: French-English results on the SparseHRMLocal feature set when when trained with pair-wise
ranked optimization (PRO) and expected BLEU (xBLEU).
a large feature setting of over 3M features
(SparseHRM+BiPhrases). In the CLL setting, we
maximize the likelihood of the hypothesis with the
highest BLEU score in the n-best list of each train-
ing sentence.
Our results (Table 3) show that CLL training
achieves only a fraction of the gains yielded by
the expected BLEU objective. For SparseHRM,
CLL improves the baseline by less than 0.2 BLEU
on average across all test sets, whereas expected
BLEU achieves 1.2 BLEU. Increasing the number
of features to 3M (SparseHRM+BiPhrases) results
in a slightly better average gain of 0.3 BLEU for
CLL but but expected BLEU still achieves a much
higher improvement of 1.5 BLEU. Because our
gains with likelihood training are similar to what
Cherry (2013) reported for his maximum entropy
model, we conclude that the objective function is
the most important factor to achieving good accu-
racy.
7.4 Comparison to PRO
In our final experiment we compare expected
BLEU training to pair-wise ranked optimization
(PRO), a popular off the shelf trainer for ma-
chine translation models with large feature sets
(Hopkins and May, 2011).
11
Previous work has
shown that PRO does not scale to truly large fea-
ture sets with millions of types (Yu et al., 2013)
and we therefore restrict ourselves to our smallest
11
MIRA is another popular optimizer but as previously
mentioned, even the best publicly available implementation
does not scale to large training sets (Eidelman et al., 2013).
set (SparseHRMLocal) of just over 4.4K features.
We train PRO on the development set compris-
ing of 2,525 sentences, a setup that is commonly
used by standard machine translation optimizers.
In this setting, PRO directly learns weights for the
baseline features (?7) as well as the 4.4K indica-
tor features corresponding to the sparse reordering
model. For expected BLEU training we use the
full 136K sentences from the training data. The
results (Table 4) demonstrate that expected BLEU
outperforms a typical setup commonly used to
train large feature sets.
8 Conclusion and Future Work
The expected BLEU objective is a simple and ef-
fective approach to train large-scale discriminative
reordering models. We have demonstrated that
it scales to millions of features, which is orders
of magnitudes larger than other modern machine
translation optimizers can currently handle.
Empirically, our sparse reordering model im-
proves machine translation accuracy across the
board, outperforming a strong hierarchical lexi-
calized reordering model by up to 2.0 BLEU on
a French to English WMT2012 setup, where the
baseline was trained on over two million sentence
pairs. We have shown that scaling to large train-
ing sets is crucial to good performance and that
the best performance is reached when hundreds
of thousands of training sentences are used. Fur-
thermore, we demonstrate that task-specific train-
ing towards expected BLEU is much more effec-
tive than optimizing conditional log-likelihood as
1258
is usually done. We attribute this to the fact that
likelihood is a strict zero-one loss that does not as-
sign credit to partially correct solutions, whereas
expected BLEU does.
In future work we plan to extend expected
BLEU training to lattices and to evaluate the ef-
fect of estimating weights for the dense baseline
features as well. Our current training procedure
(Gao and He, 2013; Gao et al., 2014) decodes
the training data only once. In future work, we
would like to compare this to repeated decoding
as done by conventional optimization methods as
well as other large-scale discriminative training
approaches (Yu et al., 2013). We expect this to
yield additional accuracy gains.
Acknowledgements
We would like to thank Arul Menezes and Xi-
aodong He for helpful discussion related to this
work and the three anonymous reviewers for their
comments.
References
L?eon Bottou. 2004. Stochastic learning. In
Olivier Bousquet and Ulrike von Luxburg, edi-
tors, Advanced Lectures in Machine Learning, Lec-
ture Notes in Artificial Intelligence, pages 146?168.
Springer Verlag, Berlin.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479,
Dec.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proc. of WMT, pages 10?51.
Association for Computational Linguistics, June.
Colin Cherry. 2013. Improved Reordering for Phrase-
Based Translation using Sparse Features. In Proc. of
NAACL, pages 9?14. Association for Computational
Linguistics, June.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 New Features for Statistical Machine Trans-
lation. In Proc. of NAACL, pages 218?226. Associ-
ation for Computational Linguistics, June.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Vladimir Eidelman, Ke Wu, Ferhan Ture1, Philip
Resnik, and Jimmy Lin. 2013. Mr. MIRA:
Open-Source Large-Margin Structured Learning on
MapReduce. In Proc. of ACL, pages 199?204. As-
sociation for Computational Linguistics, August.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proc. of EMNLP, pages 848?856.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. of HLT-NAACL, pages 273?280, Boston,
MA, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models. In
Proc. of ACL, pages 961?968, Sydney, Australia,
June.
Jianfeng Gao and Xiaodong He. 2013. Training MRF-
Based Phrase Translation Models using Gradient
Ascent. In Proc. of NAACL-HLT, pages 450?459.
Association for Computational Linguistics, June.
Jianfeng Gao, Xiaodong He, Scott Wen tau Yih, and
Li Deng. 2014. Learning Continuous Phrase Rep-
resentations for Translation Modeling. In Proc.
of ACL. Association for Computational Linguistics,
June.
Spence Green, Daniel Cer, and Christopher Manning.
2014. An Empirical Comparison of Features and
Tuning for Phrase-based Machine Translation. In
Proc. of WMT. Association for Computational Lin-
guistics, June.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proc. of ACL, pages 8?14. Association
for Computational Linguistics, July.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proc. of EMNLP. Association for Com-
putational Linguistics, July.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In
Proc. of HLT-NAACL, pages 127?133, Edmonton,
Canada, May.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
In Proc. of IWSLT.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of ACL Demo and Poster Sessions, pages
177?180, Prague, Czech Republic, Jun.
Percy Liang, Alexandre Bouchard-C?ot?e, Ben Taskar,
and Dan Klein. 2006. An end-to-end discriminative
approach to machine translation. In Proc. of ACL-
COLING, pages 761?768, Jul.
1259
Preslav Nakov, Francisco Guzman, and Stephan Vo-
gel. 2012. Optimizing for Sentence-Level BLEU+1
Yields Short Translations. In Proc. of COLING. As-
sociation for Computational Linguistics.
Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen,
and Thai Phuong Nguyen. 2009. Improving A Lex-
icalized Hierarchical Reordering Model Using Max-
imum Entropy. In MT Summit XII. Association for
Computational Linguistics, August.
Franz Josef Och and Hermann Ney. 2004. The
alignment template approach to machine translation.
Computational Linguistics, 30(4):417?449, June.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of ACL,
pages 160?167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of ACL,
pages 311?318, Philadelphia, PA, USA, Jul.
Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN System De-
scription for WMT10 System Combination Task.
In Proc. of WMT, pages 321?326. Association for
Computational Linguistics, July.
Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2011. Expected BLEU
Training for Graphs: BBN System Description for
WMT11 System Combination Task. In Proc. of
WMT, pages 159?165. Association for Computa-
tional Linguistics, July.
Christoph Tillmann. 2003. A Unigram Orientation
Model for Statistical Machine Translation. In Proc.
of NAACL, pages 106?108. Association for Compu-
tational Linguistics, June.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice Minimum
Bayes-Risk Decoding for Statistical Machine Trans-
lation. In Proc. of EMNLP, pages 620?629. Associ-
ation for Computational Linguistics, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for
statistical machine translation. In Proc. of ACL-
COLING, pages 521?528, Sydney, Jul.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-Violation Perceptron and Forced Decod-
ing for Scalable MT Training. In Proc. of EMNLP,
pages 1112?1123. Association for Computational
Linguistics, October.
1260
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 867?875,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improved Models of Distortion Cost for Statistical Machine Translation
Spence Green, Michel Galley, and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{spenceg,mgalley,manning}@stanford.edu
Abstract
The distortion cost function used in Moses-
style machine translation systems has two
flaws. First, it does not estimate the future
cost of known required moves, thus increas-
ing search errors. Second, all distortion is
penalized linearly, even when appropriate re-
orderings are performed. Because the cost
function does not effectively constrain search,
translation quality decreases at higher dis-
tortion limits, which are often needed when
translating between languages of different ty-
pologies such as Arabic and English. To ad-
dress these problems, we introduce a method
for estimating future linear distortion cost, and
a new discriminative distortion model that pre-
dicts word movement during translation. In
combination, these extensions give a statis-
tically significant improvement over a base-
line distortion parameterization. When we
triple the distortion limit, our model achieves
a +2.32 BLEU average gain over Moses.
1 Introduction
It is well-known that translation performance in
Moses-style (Koehn et al, 2007) machine transla-
tion (MT) systems deteriorates when high distortion
is allowed. The linear distortion cost model used in
these systems is partly at fault. It includes no es-
timate of future distortion cost, thereby increasing
the risk of search errors. Linear distortion also pe-
nalizes all re-orderings equally, even when appro-
priate re-orderings are performed. Because linear
distortion, which is a soft constraint, does not effec-
tively constrain search, a distortion limit is imposed
on the translation model. But hard constraints are
ultimately undesirable since they prune the search
space. For languages with very different word or-
Followers of all of the Christian and Islamic sects
engaged

Verb

NP-OBJ
	

PP

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 966?974,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Accurate Non-Hierarchical Phrase-Based Translation
Michel Galley and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{mgalley,manning}@cs.stanford.edu
Abstract
A principal weakness of conventional (i.e.,
non-hierarchical) phrase-based statistical machine
translation is that it can only exploit continuous
phrases. In this paper, we extend phrase-based
decoding to allow both source and target phrasal
discontinuities, which provide better generalization
on unseen data and yield significant improvements
to a standard phrase-based system (Moses).
More interestingly, our discontinuous phrase-
based system also outperforms a state-of-the-art
hierarchical system (Joshua) by a very significant
margin (+1.03 BLEU on average on five Chinese-
English NIST test sets), even though both Joshua
and our system support discontinuous phrases.
Since the key difference between these two systems
is that ours is not hierarchical?i.e., our system
uses a string-based decoder instead of CKY, and it
imposes no hard hierarchical reordering constraints
during training and decoding?this paper sets
out to challenge the commonly held belief that
the tree-based parameterization of systems such
as Hiero and Joshua is crucial to their good
performance against Moses.
1 Introduction
Phrase-based machine translation models (Och and
Ney, 2004) advanced the state of the art by extend-
ing the basic translation unit from words to phrases.
By conditioning translations on more than a sin-
gle word, a statistical machine translation (SMT)
system benefits from the larger context of a phrase
pair to properly handle multi-word units and lo-
cal reorderings. Experimentally, it was found that
longer phrases yield better MT output (Koehn et al,
2003). However, while it is computationally feasi-
ble at training time to extract phrase pairs of nearly
unbounded size (Zhang and Vogel, 2005; Callison-
Burch et al, 2005), phrase pairs applicable at test
time tend to be fairly short. Indeed, data sparsity
often forces conventional phrase-based systems to
segment test sentences into small phrases, and there-
fore to translate dependent words (e.g., the French
ne . . . pas) separately instead of jointly.
We present a solution to this sparsity problem by
going beyond using only continuous phrases, and
instead define our translation unit as any subset of
words of a sentence, i.e., a discontinuous phrase.
We generalize conventional multi-beam string-based
decoding (Koehn, 2004) to allow variable-size dis-
continuities in both source and target phrases. Since
each sentence pair can be more flexibly decomposed
into translation units, it is possible to exploit the rich
context of longer (possibly discontinuous) phrases
to improve translation quality. Our decoder provides
two extensions to Moses (Koehn et al, 2007): (a) to
cope with source gaps, we follow (Lopez, 2007) to
efficiently find all discontinuous phrases in the train-
ing data that also appear in the input sentence; (b) to
enable target discontinuities, we augment transla-
tion hypotheses to not only record the current par-
tial translation, but also a set of subphrases that may
be appended to the partial translation at some later
stages of decoding. With these enhancements, our
best discontinuous system outperforms Moses with
lexicalized reordering by 0.77 BLEU and 1.53 TER
points on average.
We also show that our approach compares favor-
ably to binary synchronous context-free grammar
(2-SCFG) systems such as Hiero (Chiang, 2007),
even though 2-SCFG systems also allow phrasal dis-
continuities. Part of this difference may be due to a
difference of expressiveness, since 2-SCFG models
impose hard hierarchical constraints that our mod-
els do not impose. Recent work (Wellington et
al., 2006; S?gaard and Kuhn, 2009; S?gaard and
966
ai
ak am
bj
bl bn
ai akbj
bl bnam
source: ai ckbj dl
bm apdn cttarget:
(iii)(ii)(i)
Figure 1: 2-SCFG systems such as Hiero are unable to in-
dependently generate translation units a, b, c, and d with
the following types of alignments: (i) inside-out (Wu,
1997); (ii) cross-serial DTU (S?gaard and Kuhn, 2009);
(iii) ?bonbon? (Simard et al, 2005). Standard phrase-
based decoders cope with (i), but not (ii) and (iii). Our
phrase-based decoder handles all three cases.
Wu, 2009) has questioned the empirical adequacy of
2-SCFG systems, which are unable to perform any
of the transformations shown in Fig. 1. For instance,
using manually-aligned bitexts for 12 European lan-
guages pairs, S?gaard and Kuhn found that inside-
out and cross-serial discontinuous translation units
(DTU) account for 1.6% (Danish-English) to 18.6%
(French-English) of all translation units. The em-
pirical adequacy of 2-SCFG models would presum-
ably be lower with automatically-aligned texts and if
the study also included non-European languages. In
contrast, phrase-based systems can properly handle
inside-out alignments when used with a reasonably
large distortion limit, and all configurations in Fig. 1
are accounted for in our system. In our experiments,
we show that our discontinuous phrase-based sys-
tem outperforms Joshua (Li et al, 2009), a reimple-
mentation of Hiero, by 1.03 BLEU points and 1.19
TER points on average. A final compelling advan-
tage of our decoder is that it preserves the compu-
tational efficiency of Moses (i.e., time complexity is
linear when a distortion limit is used), while SCFG
decoders have a running time that is at least cubic
(Huang et al, 2005).
2 Discontinuous Phrase Extraction
In this section, we introduce the extraction of dis-
continuous phrases for phrase-based MT. We will
describe a decoder that can handle such phrases
in the next section. Formally, we define the dis-
continuous phrase-based translation problem as fol-
lows. We are given a source sentence f = fJ1 =
f1, . . . , fj, . . . , fJ , which is to be translated into a
target sentence e = eI1 = e1, . . . , ei, . . . , fI . Un-
like (Och and Ney, 2004), in this work, a sentence
pair may be segmented into phrases that are not con-
Hiero:
This work:
ne veux plus X 
je ne veux plus X 
do not want X anymore
I do not want X anymore
veux
ne ... plus
je ne ... plus
ne veux plus
je ne veux plus
veux ... jouer
do ... want
not ... anymore
I ... not ... anymore
do not want ... anymore
I do not want ... anymore
do ... want to play
je
ne
veux
plus
jouer
I
do
not
want
to
play
anymore
Figure 2: Due to hierarchical constraints, Hiero only ex-
tracts two discontinuous phrases from the alignment on
the left, but our system extracts 11 (only 6 are shown).
tinuous, so each phrase is characterized by a cover-
age set, i.e., a set of word indices. Assuming that
the sentence pair (f , e) is decomposed into K dis-
continuous phrases, we use s = (s1, . . . , sK) and
t = (t1, . . . , tK) to respectively represent the de-
composition of the source and target sentence into
K word subsets that are complementary and non-
overlapping. A pair of coverage sets (sk, tk) is said
to be consistent with the word alignment A if the
following condition holds:
?(i, j) ? A : i ? sk ?? j ? tk (1)
For continuous phrases, finding all phrase pairs
that satisfy this condition can be done in O(nm3)
time (Och and Ney, 2004), where n is the length of
the sentence and m is the maximum phrase length.
The set of discontinuous phrases is exponential in
the maximum span length, so phrase extraction must
be tailored to a specific text (e.g., a given test sen-
tence) for relatively large m values. Lopez (2007)
presents an efficient solution using suffix arrays for
finding all discontinuous phrases of the training data
that are relevant to a given test sentence or test set.
A complete overview of this technique is beyond
the scope of this paper, though we will mention
that it solves a phrase collocation problem by effi-
ciently identifying collocated continuous phrases of
the training data that also happen to be collocated in
the test sentence. While this technique was primar-
ily designed for extracting hierarchical phrases for
Hiero (Chiang, 2007), it can readily be applied to
the problem of finding all discontinuous phrases for
our phrase-based system. Indeed, the suffix-array
technique gives us for each input sentence a list of
relevant source coverage sets. For each such sk, we
can easily enumerate each tk satisfying Eq. 1. The
967
!!"! !!# $% & ' ( )* +, -.!!
he said are to this
one access
make arrangements
he said are ... for this
visit
arrangements ... made
he said
oo-------
score = -1.3
are
for this | made
ooooo--oo
score = -4.8
arrangements
made
oo-----oo
score = -3.2
made
for this
ooooo--oo
score = -6.1
for this
ooooo--oo
score = -7.2
visit
ooooooooo
score = -8.5
source:
translation
options
(subset):
state
expansions:
* *
Figure 3: A particular decoder search path for the input shown at the top. Note that this example contains a cross-serial
DTU (which interleaves arrangements ... made with are ... for this), a structure Hiero can?t handle.
key difference between Hiero-style extraction and
our work is that Eq. 1 is the only constraint.1 Since
our decoder doesn?t impose hierarchical constraints,
we exploit all discontinuous phrase pairs consis-
tent with the word alignment, which often includes
sound translations not captured by Hiero (e.g., ne . . .
plus translating to not . . . anymore in Fig. 2).
3 Decoder
The core engine of our phrase-based system, Phrasal
(Cer et al, 2010), is a multi-stack decoder similar to
Moses (Koehn, 2004), which we extended to sup-
port variable-size gaps in the source and the target.
InMoses, partial translation hypotheses are arranged
into different stacks according to the total number of
input words they cover. At every translation step,
stacks are pruned using partial translation cost and a
lower bound on the estimated future cost. Pruning
is implemented using both threshold and histogram
pruning, and Moses allows for hypothesis recombi-
nation between hypotheses that are indistinguishable
according to the underlying models.
The key difference between Moses and our sys-
tem is that, in order to account for target disconti-
nuities, phrases that contains gaps in the target are
appended to a partial translation hypothesis in mul-
tiple steps. Specifically, each translation hypothesis
in our decoder is not only represented as a transla-
tion prefix and a coverage set as in Moses, but it also
contains a set of isolated phrases (shown in italic in
Fig. 3) that must be added to the translation at some
later time. For instance, the figure shows how the
1In order to keep the number of phrases manageable, we
additionally require that each (maximal) contiguous substring
of sk and tk be connected with at least one word alignment.
Beam search algorithm.
1 create initial hypothesis H?; add it to Sg0
2 for j = 0 to J
3 if j > 0 then
4 for n = 1 to N
5 for each Hnew in consolidate(Hcjn)
6 add Hnew to Sgj
7 if j < J then
8 for n = 1 to N
9 Hold := Hgjn
10 u := first uncovered source word of Hold
11 for m = u to u + distortionLimit
12 for each (sk, tk) in translation options(m)
13 if source sk does not overlap Hold then
14 Hnew :=combine(Hold, sk, tk)
15 add Hnew to Scj+l, where l = |sk|
16 return argmax(SgJ )
Table 1: Discontinuous phrase-based MT.
phrase pair (\???, arrangements ... made) is be-
ing added to a partial translation. The prefix (ar-
rangements) is immediately appended to form the
hypothesis (he said arrangements), and the isolated
phrase (made) is stored for later use.
A beam search algorithm for discontinuous
phrase-based MT is shown in Table 1. Pruning is
done implicitly in the table to avoid cluttering the
pseudo-code. The algorithm handles 2J + 1 stacks
Sg0 , S
g
1 , . . . , S
g
J and Sc1, . . . , ScJ , where each stack
may contain up to N hypotheses Hj1, . . . ,HjN .
The main loop of the algorithm alternates two
stages: grow (lines 7?15) and consolidate (lines 3?
6).2 The grow stage is similar to standard phrase-
2The distinction between Sgi and S
c
i stacks ensures that the
consolidate operation does not read and write hypotheses on the
same stack. While it may seem effective to store hypotheses in
968
based MT: we take a hypothesis Hgjn from S
g
j and
combine it with a translation option (sk, tk), which
yields a new hypothesis that is added to stack Scj+l
(where l = |sk|). The second stage, consolidate, lets
the decoder select any number of isolated phrases
(not necessarily all, and possibly zero) and append
them in any order at the end of the current trans-
lation.3 Consolidation operations are marked with
stars in the figure (for simplicity, the figure does
not display consolidations that keep hypotheses un-
changed). We limit the number of isolated phrases
to 4, which is generally enough to account for most
transformations seen in the data. Any hypothesis in
the last beam SgJ is automatically discarded if it con-
tains any isolated phrase.
One last difference with standard decoders is
that we also handle source discontinuities. This
problem is a known instance of MT by pattern
matching (Lopez, 2007), which we already men-
tioned in the previous section. The function transla-
tion options(m) of Table 1 returns the set of options
applicable at position m using this pattern match-
ing algorithm. Since this function is invoked a large
number of times, it is important to precompute its
return values for each m prior to decoding.
4 Features
Our system incorporates the same eight baseline fea-
tures of Moses: two relative-frequency phrase trans-
lation probabilities p(e|f) and p(f |e), two lexically-
weighted phrase translation probabilities (Koehn et
al., 2003) lex(e|f) and lex(f |e), a language model
probability, word penalty, phrase penalty, and linear
distortion, and we optionally add 6 lexicalized re-
ordering features as computed in Moses.
Our computation of linear distortion is different
from the one in Moses, since we need to account
for discontinuous phrases. We found that it is
crucial to penalize discontinuous phrases that have
relatively long gaps. Hence, in our computation of
different stacks depending on the number of isolated phrases,
we have not found various implementations of this idea to work
better than the algorithm described here.
3We let isolated phrases be reordered freely, with only three
constraints: (1) the internal word order must be preserved, i.e., a
phrase may not be split or reordered. (2) isolated phrases drawn
from the same discontinuous phrase must appear in the specified
order (i.e., the phrase A ... B ... C may not yield the translation
A ... C ... B). (3) Empty gaps are forbidden.
Figure 4: Linear distortion computed using both continu-
ous and discontinuous phrase.
linear distortion, we treat continuous subphrases
of each discontinuous phrase as if they were
continuous phrases on their own. Specifically,
let s? = (s?1, . . . , s?L) be the list of L (maximal)
continuous subphrases of the K source phrases
(L ? K) selected for a given translation hypothesis.
Subphrases in s? are enumerated according to their
order in the target language, which may be different
from the source-side ordering. We then compute
the linear distortion between pair of successive
elements (s?i, s?i+1) as follows:
d(?s) = s?first1 +
L
?
i=2
?
?
?
s?lasti?1 + 1? s?firsti
?
?
?
where the superscripts first and last respectively
refer to source position of the first and last word
of a given subphrase. Fig. 4 shows an example of
how distortion is computed for phrases (s1, s2, s3),
including the discontinuous phrase s2 split into three
continuous subphrases. In practice, we compute
intra-phrase (shown with thin arrows in the figure)
and inter-phrase linear distortion separately in order
to produce two distinct features, since translation
tends to improves when the intra-phrase cost has a
lower feature weight.
Finally, we add two features that are not present
in Moses. First, we penalize target discontinuities
by including a feature that is the sum of the lengths
of all target gaps. The second feature is the count
of discontinuous phrases that are in configurations
(cross-serial DTU (S?gaard and Kuhn, 2009) and
?bonbon? (Simard et al, 2005)) that can?t be han-
dled by 2-SCFG systems. The advantage of such
features is two-fold. First, similarly to hierarchi-
cal systems, they prevent many distorted reorderings
that are unlikely to correspond to quality transla-
tions. Second, it imposes soft rather than hard con-
straints, which means that the decoder is entirely
free to violate hierarchical constraints when these
violations are supported by other features.
969
5 Experimental Setup
Three systems are evaluated in this paper: Moses
(Koehn et al, 2007), Joshua (Li et al, 2009) ? a
reimplementation of Hiero, and our phrase-based
system. We made our best attempts to make our sys-
tem comparable to Moses. That is, when no discon-
tinuous phrases are provided to our system, it gener-
ates an output that is almost identical to Moses (only
about 1% of translations differ on average). In both
systems, we use the default settings of Moses, i.e.,
we set the beam size to 200, the distortion limit to 6,
we limit to 20 the number of target phrases that are
loaded for each source phrase, and we use the same
default eight features of Moses. We use version 1.3
of Joshua with its default settings. Both Moses and
our system are evaluated with and without lexical-
ized reordering (Tillmann, 2004).4 We believe it
to be fair to compare Joshua against phrase-based
systems that exploit lexicalized reordering, since Hi-
ero?s hierarchical rules are also lexically sensitive.5
The language pair for our experiments is Chinese-
to-English. The training data consists of about 28
million English words and 23.3 million Chinese
words drawn from various news parallel corpora dis-
tributed by the Linguistic Data Consortium (LDC).
In order to provide experiments comparable to previ-
ous work, we used the same corpora as (Wang et al,
2007). We performed word alignment using a cross-
EM word aligner (Liang et al, 2006). For this, we
ran two iterations of IBM Model 1 and two HMM
iterations. Finally, we generated a symmetric word
alignment from cross-EM Viterbi alignment using
the Moses grow-diag heuristic in the case Moses and
our system. In the case of Joshua, we used the grow-
diag-final heuristic since this gave better results.
In order to train a competitive baseline given our
computational resources, we built a large 5-gram
language model using the Xinhua and AFP sections
4We use Moses? default orientations: monotone, swap, and
discontinuous. As far as this reordering model is concerned,
we treat discontinuous phrases as continuous, i.e., we simply
ignore what lies within gaps to determine phrase orientation.
5(Tillmann, 2004) learns for each phrase a tendency to ei-
ther remain monotone or to swap with other phrases. As noted
in (Lopez, 2008), Hiero can represent the same information
with hierarchical rules of the form uX, Xu, and XuX. Hi-
ero actually models lexicalized reordering patterns that (Till-
mann, 2004) does not account for, e.g., a transformation from
X1uX2v to X2u?v?X1.
of the Gigaword corpus (LDC2007T40) in addition
to the target side of the parallel data. This data rep-
resents a total of about 700 million words. We man-
ually removed documents of Gigaword that were re-
leased during periods that overlap with those of our
development and test sets. The language model was
smoothed with the modified Kneser-Ney algorithm
as implemented in SRILM (Stolcke, 2002), and we
only kept 4-grams and 5-grams that occurred at least
three times in the training data.
For tuning and testing, we use the official NIST
MT evaluation data for Chinese from 2003 to 2008
(MT03 to MT08), which all have four English ref-
erences for each input sentence. We used the 1664
sentences of MT06 for tuning and development and
all other sets for testing. Parameter tuning was
done with minimum error rate training (Och, 2003),
which was used to maximize IBM BLEU-4 (Pap-
ineni et al, 2001). Since MERT is prone to search
errors, especially with large numbers of parameters,
we ran each tuning experiment four times with dif-
ferent initial conditions. We used n-best lists of size
200. In the final evaluations, we report results using
both TER version 0.7.25 (Snover et al, 2006) and
BLEU-4 (both uncased).
6 Results
We start by comparing some translations generated
by the best configurations of Joshua, Moses, and our
phrase-based decoder, systems we will empirically
evaluate later in this section. Fig. 5 shows trans-
lations of our development set MT06, which were
selected because our system makes a crucial use of
discontinuous phrases. In the first example, the Chi-
nese input contains S ... ?, which typically trans-
lates as when. Lacking an entry for the input phrase
S}C?e:? in its phrase table, Moses is
unable to translate this segment appropriately, and
must instead split this phrase to generate the trans-
lation when the right was deprived of, where ? is
translated into of. This is evidently a poor transla-
tion. Conversely, our system uses a discontinuous
phrase to translate S ... ?, and translates the inter-
vening words separately.
The remaining three translations all contain cross-
serial DTUs (S?gaard and Kuhn, 2009) and thus
would be difficult to generate using 2-SCFG sys-
tems. The second example motivates the idea
970
MT06 ? segment 1589
Reference: Under such cir-
cumstances, when the right
of existence was deprived,
the only way remaining was
to overthrow the existing
dynasty by force and try to
replace it.
Joshua: Under such cir-
cumstances, when life be
deprived, can only resort to
violence to overthrow the
current dynasty, trying to re-
place,
Moses: Under such circum-
stances, when the right was
deprived of, can only adopt
the means of violence, in
an attempt to overthrow the
present dynasty replaced,
This work: Under such cir-
cumstances, when he was de-
prived of the right to life, it
can only resort to violence in
an attempt to overthrow the
current dynasty replaced,
in this kind case when life right was deprive when only can use violence of means
( ? ? ??  , S } C ? e: ? , ? ? ?? ?? ? K? ...
under such circumstances , when he was deprived of the right to life , it can only resort to violence ...
MT06 ? segment 1044
Reference: CCP organi-
zation ministry demands
to further enlarge strength
of supervision of leading
cadres and cadre selection
and appointment
Joshua: Department de-
mands further intensify su-
pervision over the work
of selecting and appointing
leading cadres, and inten-
sify
Moses: The central organi-
zation department, called on
leading cadres, further in-
crease the intensity of super-
vision over work of selecting
and appointing cadres.
This work: The central orga-
nization department has called
for further increase the inten-
sity of supervision of leading
cadres and the work of select-
ing and appointing cadres.
CCP request further increase to leading cadres and cadre selection appointment work of supervision intensity
-?? ?B ? e ?' ? ?? r? ? r? 	? ?( ?\ ? ? ?
the central organization department has called for further increase the intensity of supervision of leading cadres and ... ...
MT06 ? segment 559
Reference: The government
will take all possible mea-
sures to prevent similar inci-
dents from happening in the
future.
Joshua: Government will
take all measures to prevent
the re-occurrence of similar
incidents in the future.
Moses: The government will
take all measures to prevent
the occurrence of similar inci-
dents in the future.
This work: The government
will take all measures to pre-
vent similar incidents from
happening again in the future.
government will take all measure to prevent future again happen similar of incidents
??  Proceedings of the NAACL HLT 2010: Demonstration Session, pages 9?12,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Phrasal: A Toolkit for Statistical Machine Translation
with Facilities for Extraction and Incorporation of Arbitrary Model Features
Daniel Cer, Michel Galley, Daniel Jurafsky and Christopher D. Manning
Stanford University
Stanford, CA 94305, USA
Abstract
We present a new Java-based open source
toolkit for phrase-based machine translation.
The key innovation provided by the toolkit
is to use APIs for integrating new fea-
tures (/knowledge sources) into the decod-
ing model and for extracting feature statis-
tics from aligned bitexts. The package in-
cludes a number of useful features written to
these APIs including features for hierarchi-
cal reordering, discriminatively trained linear
distortion, and syntax based language models.
Other useful utilities packaged with the toolkit
include: a conditional phrase extraction sys-
tem that builds a phrase table just for a spe-
cific dataset; and an implementation of MERT
that allows for pluggable evaluation metrics
for both training and evaluation with built in
support for a variety of metrics (e.g., TERp,
BLEU, METEOR).
1 Motivation
Progress in machine translation (MT) depends crit-
ically on the development of new and better model
features that allow translation systems to better iden-
tify and construct high quality machine translations.
The popular Moses decoder (Koehn et al, 2007)
was designed to allow new features to be defined us-
ing factored translation models. In such models, the
individual phrases being translated can be factored
into two or more abstract phrases (e.g., lemma, POS-
tags) that can be translated individually and then
combined in a seperate generation stage to arrive at
the final target translation. While greatly enriching
the space of models that can be used for phrase-
based machine translation, Moses only allows fea-
tures that can be defined at the level of individual
words and phrases.
The Phrasal toolkit provides easy-to-use APIs
for the development of arbitrary new model fea-
tures. It includes an API for extracting feature
statistics from aligned bitexts and for incor-
porating the new features into the decoding
model. The system has already been used to
develop a number of innovative new features
(Chang et al, 2009; Galley and Manning, 2008;
Galley and Manning, 2009; Green et al, 2010) and
to build translation systems that have placed well
at recent competitive evaluations, achieving second
place for Arabic to English translation on the NIST
2009 constrained data track.1
We implemented the toolkit in Java because it of-
fers a good balance between performance and de-
veloper productivity. Compared to C++, develop-
ers using Java are 30 to 200% faster, produce fewer
defects, and correct defects up to 6 times faster
(Phipps, 1999). While Java programs were histori-
cally much slower than similar programs written in
C or C++, modern Java virtual machines (JVMs) re-
sult in Java programs being nearly as fast as C++
programs (Bruckschlegel, 2005). Java also allows
for trivial code portability across different platforms.
In the remainder of the paper, we will highlight
various useful capabilities, components and model-
ing features included in the toolkit.
2 Toolkit
The toolkit provides end-to-end support for the cre-
ation and evaluation of machine translation models.
Given sentence-aligned parallel text, a new transla-
tion system can be built using a single command:
java edu.stanford.nlp.mt.CreateModel \
(source.txt) (target.txt) \
(dev.source.txt) (dev.ref) (model_name)
Running this command will first create word
level alignments for the sentences in source.txt
and target.txt using the Berkeley cross-EM aligner
1http://www.itl.nist.gov/iad/mig/tests
/mt/2009/ResultsRelease/currentArabic.html
9
Figure 1: Chinese-to-English translation using discontinuous phrases.
(Liang et al, 2006).2 From the word-to-word
alignments, the system extracts a phrase ta-
ble (Koehn et al, 2003) and hierarchical reorder-
ing model (Galley and Manning, 2008). Two n-
gram language models are trained on the tar-
get.txt sentences: one over lowercased target sen-
tences that will be used by the Phrasal decoder
and one over the original source sentences that
will be used for truecasing the MT output. Fi-
nally, the system trains the feature weights for the
decoding model using minimum error rate train-
ing (Och, 2003) to maximize the system?s BLEU
score (Papineni et al, 2002) on the development
data given by dev.source.txt and dev.ref. The toolkit
is distributed under the GNU general public license
(GPL) and can be downloaded from http://
nlp.stanford.edu/software/phrasal.
3 Decoder
Decoding Engines The package includes two de-
coding engines, one that implements the left-to-
right beam search algorithm that was first intro-
duced with the Pharaoh machine translation system
(Koehn, 2004), and another that provides a recently
developed decoding algorithm for translating with
discontinuous phrases (Galley and Manning, 2010).
Both engines use features written to a common but
extensible feature API, which allows features to be
written once and then loaded into either engine.
Discontinuous phrases provide a mechanism for
systematically translating grammatical construc-
tions. As seen in Fig. 1, using discontinuous phrases
allows us to successfully capture that the Chinese
construction? X? can be translated as when X.
Multithreading The decoder has robust support
for multithreading, allowing it to take full advantage
of modern hardware that provides multiple CPU
cores. As shown in Fig. 2, decoding speed scales
well when the number of threads being used is in-
creased from one to four. However, increasing the
2Optionally, GIZA++ (Och and Ney, 2003) can also be used
to create the word-to-word alignments.
1 2 3 4 5 6 7 8
15
25
35
Cores
tra
n
la
tio
ns
 p
er
 m
in
u
te
Figure 2: Multicore translations per minute on a sys-
tem with two Intel Xeon L5530 processors running at
2.40GHz.
threads past four results in only marginal additional
gains as the cost of managing the resources shared
between the threads is starting to overwhelm the
value provided by each additional thread. Moses
also does not run faster with more than 4-5 threads.3
Feature API The feature API was designed to
abstract away complex implementation details of
the underlying decoding engine and provide a sim-
ple consistent framework for creating new decoding
model features. During decoding, as each phrase
that is translated, the system constructs a Featuriz-
able object. As seen in Table 1, Featurizable objects
specify what phrase was just translated and an over-
all summary of the translation being built. Code that
implements a feature inspects the Featurizable and
returns one or more named feature values. Prior to
translating a new sentence, the sentence is passed to
the active features for a decoding model, so that they
can perform any necessary preliminary analysis.
Comparison with Moses Credible research into
new features requires baseline system performance
that is on par with existing state-of-the-art systems.
Seen in Table 2, Phrasal meets the performance of
Moses when using the exact same decoding model
feature set as Moses and outperforms Moses signifi-
cantly when using its own default feature set.4
3http://statmt.org/moses
/?n=Moses.AdvancedFeatures (April 6, 2010)
4Phrasal was originally written to replicate Moses as it was
implemented in 2007 (release 2007-05-29), and the current ver-
10
Featurizable
Last Translated Phrase Pair
Source and Target Alignments
Partial Translation
Source Sentence
Current Source Coverage
Pointer to Prior Featurizable
Table 1: Information passed to features in the form of a
Featurizable object for each translated phrase.
System Features MT06 (tune) MT03 MT05
Moses Moses 34.23 33.72 32.51
Phrasal Moses 34.25 33.72 32.49
Phrasal Default 35.02 34.98 33.21
Table 2: Comparison of two configurations of Phrasal
to Moses on Chinese-to-English. One Phrasal configura-
tion uses the standard Moses feature set for single factor
phrase-based translation with distance and phrase level
msd-bidirectional-fe reordering features. The other uses
the default configuration of Phrasal, which replaces the
phrase level msd-bidirectional-fe feature with a heirarchi-
cal reordering feature.
4 Features
The toolkit includes the basic eight phrase-based
translation features available in Moses as well as
Moses? implementation of lexical reordering fea-
tures. In addition to the common Moses features, we
also include innovative new features that improve
translation quality. One of these features is a hier-
archical generalization of the Moses lexical reorder-
ing model. Instead of just looking at the reorder-
ing relationship between individual phrases, the new
feature examines the reordering of blocks of ad-
jacent phrases (Galley and Manning, 2008) and im-
proves translation quality when the material being
reordered cannot be captured by single phrase. This
hierarchical lexicalized reordering model is used by
default in Phrasal and is responsible for the gains
shown in Table 2 using the default features.
To illustrate how Phrasal can effectively be used
to design rich feature sets, we present an overview
of various extensions that have been built upon the
sion still almost exactly replicates this implementation when
using only the baseline Moses features. To ensure this con-
figuration of the decoder is still competitive, we compared it
against the current Moses implementation (release 2009-04-
13) and found that the performance of the two systems is still
close. Tthe current Moses implementation obtains slightly
lower BLEU scores, respectively 33.98 and 32.39 on MT06 and
MT05.
Phrasal feature API. These extensions are currently
not included in the release:
Target Side Dependency Language Model The
n-gram language models that are traditionally used
to capture the syntax of the target language do a
poor job of modeling long distance syntactic rela-
tionships. For example, if there are a number of
intervening words between a verb and its subject,
n-gram language models will often not be of much
help in selecting the verb form that agrees with the
subject. The target side dependency language model
feature captures these long distance relationships by
providing a dependency score for the target transla-
tions produced by the decoder. This is done using
an efficient quadratic time algorithm that operates
within the main decoding loop rather than in a sepa-
rate reranking stage (Galley and Manning, 2009).
Discriminative Distortion The standard distor-
tion cost model used in phrase-based MT systems
such as Moses has two problems. First, it does not
estimate the future cost of known required moves,
thus increasing search errors. Second, the model pe-
nalizes distortion linearly, even when appropriate re-
orderings are performed. To address these problems,
we used the Phrasal feature API to design a new
discriminative distortion model that predicts word
movement during translation and that estimates fu-
ture cost. These extensions allow us to triple the
distortion limit and provide a statistically significant
improvement over the baseline (Green et al, 2010).
Discriminative Reordering with Chinese Gram-
matical Relations During translation, a source
sentence can be more accurately reordered if the
system knows something about the syntactic rela-
tionship between the words in the phrases being re-
ordered. The discriminative reordering with Chinese
grammatical relations feature examines the path be-
tween words in a source-side dependency tree and
uses it to evaluate the appropriateness of candidate
phrase reorderings (Chang et al, 2009).
5 Other components
Training Decoding Models The package includes
a comprehensive toolset for training decoding mod-
els. It supports MERT training using coordinate de-
scent, Powell?s method, line search along random
search directions, and downhill Simplex. In addi-
tion to the BLEU metric, models can be trained
11
to optimize other popular evaluation metrics such
as METEOR (Lavie and Denkowski, 2009), TERp
(Snover et al, 2009), mWER (Nie?en et al, 2000),
and PER (Tillmann et al, 1997). It is also possible
to plug in other new user-created evaluation metrics.
Conditional Phrase Table Extraction Rather
than first building a massive phrase table from a par-
allel corpus and then filtering it down to just what
is needed for a specific data set, our toolkit sup-
ports the extraction of just those phrases that might
be used on a given evaluation set. In doing so, it
dramatically reduces the time required to build the
phrase table and related data structures such as re-
ordering models.
Feature Extraction API In order to assist in the
development of new features, the toolkit provides
an API for extracting feature statistics from a word-
aligned parallel corpus. This API ties into the condi-
tional phrase table extraction utility, and thus allows
for the extraction of just those feature statistics that
are relevant to a given data set.
6 Conclusion
Phrasal is an open source state-of-the-art Java-
based machine translation system that was designed
specifically for research into new decoding model
features. The system supports traditional phrase-
based translation as well as translation using discon-
tinuous phrases. It includes a number of new and
innovative model features in addition to those typi-
cally found in phrase-based translation systems. It is
also packaged with other useful components such as
tools for extracting feature statistics, building phrase
tables for specific data sets, and MERT training rou-
tines that support a number of optimization tech-
niques and evaluation metrics.
Acknowledgements
The Phrasal decoder has benefited from the help-
ful comments and code contributions of Pi-Chuan
Chang, Spence Green, Karthik Raghunathan,
Ankush Singla, and Huihsin Tseng. The software
presented in this paper is based on work work was
funded by the Defense Advanced Research Projects
Agency through IBM. The content does not neces-
sarily reflect the views of the U.S. Government, and
no official endorsement should be inferred.
References
Thomas Bruckschlegel. 2005. Microbenchmarking C++,
C#, and Java. C/C++ Users Journal.
P. Chang, H. Tseng, D. Jurafsky, and C.D. Manning.
2009. Discriminative reordering with Chinese gram-
matical relations features. In SSST Workshop at
NAACL.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP.
Michel Galley and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine trans-
lation. In ACL.
Michel Galley and Christopher Manning. 2010. Improv-
ing phrase-based machine translation with discontigu-
ous phrases. In NAACL.
Spence Green, Michel Galley, and Christopher D. Man-
ning. 2010. Improved models of distortion cost for
statistical machine translation. In In NAACL.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In AMTA.
Alon Lavie and Michael J. Denkowski. 2009. The
METEOR metric for automatic evaluation of machine
translation. Machine Translation, 23.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In NAACL.
Sonja Nie?en, Franz Josef Och, and Hermann Ney. 2000.
An evaluation tool for machine translation: Fast eval-
uation for MT research. In LREC.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL.
Geoffrey Phipps. 1999. Comparing observed bug and
productivity rates for java and C++. Softw. Pract. Ex-
per., 29(4):345?358.
M. Snover, N. Madnani, B.J. Dorr, and R. Schwartz.
2009. Fluency, adequacy, or HTER?: exploring dif-
ferent human judgments with a tunable MT metric. In
SMT workshop at EACL.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf.
1997. Accelerated DP based search for statistical
translation. In In Eurospeech.
12
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 461?466,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Why Initialization Matters for IBM Model 1:
Multiple Optima and Non-Strict Convexity
Kristina Toutanova
Microsoft Research
Redmond, WA 98005, USA
kristout@microsoft.com
Michel Galley
Microsoft Research
Redmond, WA 98005, USA
mgalley@microsoft.com
Abstract
Contrary to popular belief, we show that the
optimal parameters for IBM Model 1 are not
unique. We demonstrate that, for a large
class of words, IBM Model 1 is indifferent
among a continuum of ways to allocate prob-
ability mass to their translations. We study the
magnitude of the variance in optimal model
parameters using a linear programming ap-
proach as well as multiple random trials, and
demonstrate that it results in variance in test
set log-likelihood and alignment error rate.
1 Introduction
Statistical alignment models have become widely
used in machine translation, question answering,
textual entailment, and non-NLP application areas
such as information retrieval (Berger and Lafferty,
1999) and object recognition (Duygulu et al, 2002).
The complexity of the probabilistic models
needed to explain the hidden correspondence among
words has necessitated the development of highly
non-convex and difficult to optimize models, such
as HMMs (Vogel et al, 1996) and IBM Models 3
and higher (Brown et al, 1993). To reduce the im-
pact of getting stuck in bad local optima the orig-
inal IBM paper (Brown et al, 1993) proposed the
idea of training a sequence of models from simpler
to complex, and using the simpler models to initial-
ize the more complex ones. IBM Model 1 was the
first model in this sequence and was considered a
reliable initializer due to its convexity.
In this paper we show that although IBM Model 1
is convex, it is not strictly convex, and there is a large
space of parameter values that achieve the same op-
timal value of the objective.
We study the magnitude of this problem by for-
mulating the space of optimal parameters as solu-
tions to a set of linear equalities and seek maximally
different parameter values that reach the same objec-
tive, using a linear programming approach. This lets
us quantify the percentage of model parameters that
are not uniquely defined, as well as the number of
word types that have uncertain translation probabil-
ities. We additionally study the achieved variance in
parameters resulting from different random initial-
ization in EM, and the impact of initialization on test
set log-likelihood and alignment error rate. These
experiments suggest that initialization does matter
in practice, contrary to what is suggested in (Brown
et al, 1993, p. 273).1
2 Preliminaries
In Appendix A we define convexity and strict con-
vexity of functions following (Boyd and Vanden-
berghe, 2004). In this section we detail the gener-
ative model for Model 1.
2.1 IBM Model 1
IBM Model 1 (Brown et al, 1993) defines a genera-
tive process for a source sentences f = f1 . . . fm and
alignments a = a1 . . . am given a corresponding tar-
get translation e = e0 . . . el. The generative process
is as follows: (i) pick a length m using a uniform
distribution with mass function proportional to ; (ii)
for each source word position j, pick an alignment
1When referring to Model 1, Brown et al (1993) state that
?details of our initial guesses for t(f |e) are unimportant?.
461
position in the target sentence aj ? 0, 1, . . . , l from
a uniform distribution; and (iii) generate a source
word using the translation probability distribution
t(fj |eaj ). A special empty word (NULL) is assumed
to be part of the target vocabulary and to occupy
the first position in each target language sentence
(e0=NULL).
The trainable parameters of Model 1 are the lex-
ical translation probabilities t(f |e), where f and e
range over the source and target vocabularies, re-
spectively. The log-probability of a single source
sentence f given its corresponding target sentence e
and values for the translation parameters t(f |e) can
be written as follows (Brown et al, 1993):
m?
j=1
log
l?
i=0
t(fj |ei)?m log(l + 1) + log 
The parameters of IBM Model 1 are usu-
ally derived via maximum likelihood estimation
from a corpus, which is equivalent to negative
log-likelihood minimization. The negative log-
likelihood for a parallel corpus D is:
LD(T ) = ?
?
f ,e
m?
j=1
log
l?
i=0
t(fj |ei) +B (1)
where T is the matrix of translation probabilities
and B represents the other terms of Model 1 (string
length probability and alignment probability), which
are constant with respect to the translation parame-
ters t(f |e).
We can define the optimization problem as the
one of minimizing negative log-likelihood LD(T )
subject to constraints ensuring that the parameters
are well-formed probabilities, i.e., that they are non-
negative and summing to one. It is well-known that
the EM algorithm for this problem converges to a lo-
cal optimum of the objective function (Dempster et
al., 1977).
3 Convexity analysis for IBM Model 1
In this section we show that, contrary to the claim in
(Brown et al, 1993), the optimization problem for
IBM Model 1 is not strictly convex, which means
that there could be multiple parameter settings that
achieve the same globally optimal value of the ob-
jective.2
The function ? log(x) is strictly convex (Boyd
and Vandenberghe, 2004). Each term in the nega-
tive log-likelihood is a negative logarithm of a sum
of parameters. The negative logarithm of a sum is
not strictly convex, as illustrated by the following
simple counterexample. Let?s look at the function
? log(x1 +x2). We can express it in vector notation
using ? log(1Tx), where 1 is a vector with all ele-
ments equal to 1. We will come up with two param-
eter settings x,y and a value ? that violate the defini-
tion of strict convexity. Take x = [x1, x2] = [.1, .2],
y = [y1, y2] = [.2, .1] and ? = .5. We have
z = ?x + (1 ? ?)y = [z1, z2] = [.15, .15]. Also
? log(1T (?x + (1 ? ?)y)) = ? log(z1 + z2) =
? log(.3). On the other hand, ?? log(x1 + x2) ?
(1??) log(y1+y2) = ? log(.3). Strict convexity re-
quires that the former expression be strictly smaller
than the latter, but we have equality. Therefore, this
function is not strictly convex. It is however con-
vex as stated in (Brown et al, 1993), because it is a
composition of log and a linear function.
We thus showed that every term in the negative
log-likelihood objective is convex but not strictly
convex and thus the overall objective is convex, but
not strictly convex. Because the objective is con-
vex, the inequality constraints are convex, and the
equality constraints are affine, the IBM Model 1 op-
timization problem is a convex optimization prob-
lem. Therefore every local optimum is a global op-
timum. But since the objective is not strictly con-
vex, there might be multiple distinct parameter val-
ues achieving the same optimal value. In the next
section we study the actual space of optima for small
and realistically-sized parallel corpora.
2Brown et al (1993, p. 303) claim the following about
the log-likelihood function (Eq. 51 and 74 in their paper, and
Eq. 1 in ours): ?The objective function (51) for this model is a
strictly concave function of the parameters?, which is equivalent
to claiming that the negative log-likelihood function is strictly
convex. In this section, we will theoretically demonstrate that
Brown et al?s claim is in fact incorrect. Furthermore, we will
empirically show in Sections 4 and 5 that multiple distinct pa-
rameter values can achieve the global optimum of the objective
function, which also disproves Brown et al?s claim about the
strict convexity of the objective function. Indeed, if a function
is strictly convex, it admits a unique globally optimum solution
(Boyd and Vandenberghe, 2004, p. 151), so our experiments
prove by modus tollens that Brown et al?s claim is wrong.
462
4 Solution Space
In this section, we characterize the set of parameters
that achieve the maximum of the log-likelihood of
IBM Model 1. As illustrated with the following
simple example, it is relatively easy to establish
cases where the set of optimal parameters t(f |e) is
not unique:
e : short sentence f : phrase courte
If the above sentence pair represents the entire
training data, Model 1 likelihood (ignoring NULL
words) is proportional to
[
t(phrase|short) + t(phrase|sentence)
]
?
[
t(courte|short) + t(courte|sentence)
]
which can be maximized in infinitely many differ-
ent ways. For instance, setting t(phrase|sentence) =
t(courte|short) = 1 yields the maximum likelihood
value with (0 + 1)(1 + 0) = 1, but the most
divergent set of parameters (t(courte|sentence) =
t(phrase|sentence) = 1) also reaches the same op-
timum: (1+0)(0+1) = 1. While this example may
not seem representative given the small size of this
data, the laxity of Model 1 that we observe in this
example also surfaces in real and much larger train-
ing sets. Indeed, it suffices that a given pair of target
words (e1,e2) systematically co-occurs in the data
(as with e1 = short e2 = sentence) to cause Model 1
to fail to distinguish the two.3
To characterize the solution space, we use the def-
inition of IBM Model 1 log-likelihood from Eq. 1 in
Section 2.1. We ask whether distinct sets of parame-
ters yield the same minimum negative log-likelihood
value of Eq. 1, i.e., whether we can find distinct
models t(f |e) and t?(f |e) so that:
?
f ,e
m?
j=1
log
l?
i=0
t(fj |ei) =
?
f ,e
m?
j=1
log
l?
i=0
t?(fj |ei)
Since the negative logarithm is strictly convex, the
3Since e1 and e2 co-occur with exactly the same source
words, one can redistribute the probability mass between
t(f |e1) and t(f |e2) without affecting the log-likelihood.
This is true if (a) the two distributions remain well-formed:
?
j t(fj |ei) = 1 for i ? {1, 2}; (b) any adjustments to param-
eters of fj leave each estimate t(fj |e1) + t(fj |e2) unchanged.
above equation can be satisfied for optimal parame-
ters only if the following holds for each f , e pair:
l?
i=0
t(fj |ei) =
l?
i=0
t?(fj |ei), j = 1 . . .m (2)
We can further simplify the above equation if we re-
call that both t(f |e) and t?(f |e) are maximum log-
likelihood parameters, and noting it is generally easy
to obtain one such set of parameters, e.g., by run-
ning the EM algorithm until convergence. Using
these EM parameters (?) in the right hand side of
the equation, we replace these right hand sides with
EM?s estimate t?(fj |e). This finally gives us the fol-
lowing linear program (LP), which characterizes the
solution space of the maximum log-likelihood:4
l?
i=0
t(fj |ei) = t?(fj |e), j = 1 . . .m ?f , e (3)
?
f
t(f |e) = 1, ?e (4)
t(f |e) ? 0, ?e, f (5)
The two conditions in Eq. 4-5 are added to ensure
that t(f |e) is well-formed. To solve this LP, we use
the interior-point method of (Karmarkar, 1984).
To measure the maximum divergence in optimal
model parameters, we solve the LP of Eq. 3-5 by
minimizing the linear objective function xTk?1xk,
where xk is the column-vector representing all pa-
rameters of the model t(f |e) currently optimized,
and where xk?1 is a pre-existing set of maximum
log-likelihood parameters. Starting with x0 defined
using EM parameters, we are effectively searching
for the vector x1 with lowest cosine similarity to x0.
We repeat with k > 1 until xk doesn?t reduce the
cosine similarity with any of the previous parameter
vectors x0 . . .xk?1 (which generally happens with
k = 3).5
4In general, an LP admits either (a) an infinity of solutions,
when the system is underconstrained; (b) exactly one solution;
(c) zero solutions, when it is ill-posed. The latter case never
occurs in our case, since the system was explicitly constructed
to allow at least one solution: the parameter set returned by EM.
5Note that this greedy procedure is not guaranteed to find the
two points of the feasible region (a convex polytope) with mini-
mum cosine similarity. This problem is related to finding the di-
ameter of this polytope, which is known to be NP-hard when the
number of variables is unrestricted (Kaibel et al, 2002). Never-
theless, divergences found by this procedure are fairly substan-
tial, as shown in Section 5.
463
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
EM-LP-1
EM-LP-8
EM-LP-32
EM-LP-128
EM-rand-1
EM-rand-8
EM-rand-32
EM-rand-128
EM-rand-1K
EM-rand-10K
cum
ula
tive
 pe
rce
nta
ge 
cosine similarity [c] 
Figure 1: Percentage of target words for which we found
pairs of distributions t(f |e) and t?(f |e) whose cosine
similarity drops below a given threshold c (x-axis).
5 Experiments
In this section, we show that the solution space
defined by the LP of Eq. 3-5 can be fairly large.
We demonstrate this with Bulgarian-English paral-
lel data drawn from the JRC-AQUIS corpus (Stein-
berger et al, 2006). Our training data consists of up
to 10,000 sentence pairs, which is representative of
the amount of data used to train SMT systems for
language pairs that are relatively resource-poor.
Figure 1 relies on two methods for determining to
what extent the model t(f |e) can vary while remain-
ing optimal. The EM-LP-N method consists of ap-
plying the method described at the end of Section 4
with N training sentence pairs. For EM-rand-N , we
instead run EM 100 times (also onN sentence pairs)
until convergence using different random starting
points, and then use cosine similarity to compare the
resulting models.6 Figure 1 shows some surprising
results: First, EM-LP-128 finds that, for about 68%
of target token types, cosine similarity between con-
trastive models is equal to 0. A cosine of zero es-
sentially means that we can turn 1?s into 0?s with-
out affecting log-likelihood, as in the short sentence
example in Section 4. Second, with a much larger
training set, EM-rand-10K finds a cosine similarity
lower or equal to 0.5 for 30% of word types, which
is a large portion of the vocabulary.
6While the first method is better at finding divergent optimal
model parameters, it needs to construct large linear programs
that do not scale with large training sets (linear systems quickly
reach millions of entries, even with 128 sentence pairs). We use
EM-rand to assess the model space on larger training set, while
we use EM-LP mainly to illustrate that divergence between op-
timal models can be much larger than suggested by EM-rand.
train coupled non-unique log-lik
all c. non-c. stdev unif
1 100 100 100 - 2.9K -4.9K
8 83.6 89.0 100 33.3 2.3K -2.3K
32 77.8 81.8 100 17.9 874 74.4
128 67.8 73.3 99.7 17.7 270 272
1K 52.6 64.1 99.8 24.0 220 281
10K 30.3 47.33 99.9 24.4 150 300
Table 1: Results using 100 random initialization trials.
In Table 1 we show additional statistics computed
from the EM-rand-N experiments. Every row repre-
sents statistics for a given training set size (in num-
ber of sent. pairs, first column); the second column
shows the percent of target word types that always
co-occur with another word type (we term these
words coupled); the third, fourth, and fifth columns
show the percent of word types whose translation
distributions were found to be non-unique, where
we define the non-unique types to be ones where the
minimum cosine between any two different optimal
parameter vectors was less than .95. The percent
of non-unique types are reported overall, as well as
only among coupled words (c.) and non-coupled
words (non-c.). The last two columns show the stan-
dard deviation in test set log-likelihood across differ-
ent random trials, as well as the difference between
the log-likelihood of the uniformly initialized model
and the best model from the random trials.
We can see that as the training set size increases,
the percentage of words that have non-unique trans-
lation probabilities goes down but is still very large.
The coupled words almost always end up having
varying translation parameters at convergence (more
than 99.5% of these words). This also happens for
a sizable portion of the non-coupled words, which
suggests that there are additional patterns of co-
occurrence that result in non-determinism.7 We also
computed the percent of word types that are coupled
for two more-realistically sized data-sets: we found
that in a 1.6 million sent pair English-Bulgarian cor-
pus 15% of Bulgarian word types were coupled and
in a 1.9 million English-German corpus from the
WMT workshop (Callison-Burch et al, 2010), 13%
of the German word types were coupled.
The log-likelihood statistics show that although
7We did not perform such experiments for larger data-sets,
since EM takes thousands of iterations to converge.
464
the standard deviation goes down with training set
size, it is still large at reasonable data sizes. Inter-
estingly, the uniformly initialized model performs
worse for a very small data size, but it catches up and
surpasses the random models at data sizes greater
than 100 sentence pairs.
To further evaluate the impact of initialization for
IBM Model 1, we report on a set of experiments
looking at alignment error rate achieved by differ-
ent models. We report the performance of Model 1,
as well as the performance of the more competitive
HMM alignment model (Vogel et al, 1996), initial-
ized from IBM-1 parameters. The dataset for these
experiments is English-French parallel data from
Hansards. The manually aligned data for evaluation
consists of 137 sentences (a development set from
(Och and Ney, 2000)).
We look at two different training set sizes, a
small set consisting of 1000 sentence pairs, and
a reasonably-sized dataset containing 100,000 sen-
tence pairs. In each data size condition, we report on
the performance achieved by IBM-1, and the perfor-
mance achieved by HMM initialized from the IBM-
1 parameters. For IBM Model 1 training, we either
perform only 5 EM iterations (the standard setting
in GIZA++), or run it to convergence. For each of
these two settings, we either start training from uni-
form t(f |e) parameters, or random parameters. Ta-
ble 2 details the results of these experiments.
Each row in the table represents an experimental
condition, indicating the training data size (1K in the
first four rows and 100K in the next four rows), the
type of initialization (uniform versus random) and
the number of iterations EM was run for Model 1 (5
iterations versus unlimited (to convergence, denoted
?)). The numbers in the table are alignment error
rates, achieved at the end of Model 1 training, and
at 5 iterations of HMM. When random initialization
is used, we run 20 random trials with different ini-
tialization, and report the min, max, and mean AER
achieved in each setting.
From the table, we can draw several conclusions.
First, in agreement with current practice using only
5 iterations of Model 1 training results in better fi-
nal performance of the HMM model (even though
the performance of Model 1 is higher when ran to
convergence). Second, the minimum AER achieved
by randomly initialized models was always smaller
setting IBM-1 HMM
min mean max min mean max
1K-unif-5 42.99 - - 22.53 - -
1K-rand-5 42.90 44.07 45.08 22.26 22.99 24.01
1K-unif-? 42.10 - - 28.09 - -
1K-rand-? 41.72 42.61 43.63 27.88 28.47 28.89
100K-unif-5 28.98 - - 12.68 - -
100K-rand-5 28.63 28.99 30.13 12.25 12.62 12.89
100K-unif-? 28.18 - - 16.84 - -
100K-rand-? 27.95 28.22 30.13 16.66 16.78 16.85
Table 2: AER results for Model 1 and HMM using uni-
form and random initialization. We do not report mean
and max for uniform, since they are identical to min.
than the AER of the uniform-initialized models. In
some cases, even the mean of the random trials was
better than the corresponding uniform model. Inter-
estingly, the advantage of the randomly initialized
models in AER does not seem to diminish with in-
creased training data size like their advantage in test
set perplexity.
6 Conclusions
Through theoretical analysis and three sets of ex-
periments, we showed that IBM Model 1 is not
strictly convex and that there is large variance in
the set of optimal parameter values. This variance
impacts a significant fraction of word types and re-
sults in variance in predictive performance of trained
models, as measured by test set log-likelihood and
word-alignment error rate. The magnitude of this
non-uniqueness further supports the development of
models that can use information beyond simple co-
occurrence, such as positional and fertility informa-
tion like higher order alignment models, as well as
models that look beyond the surface form of a word
and reason about morphological or other properties
(Berg-Kirkpatrick et al, 2010).
In future work we would like to study the im-
pact of non-determinism on higher order models in
the standard alignment model sequence and to gain
more insight into the impact of finer-grained features
in alignment.
Acknowledgements
We thank Chris Quirk and Galen Andrew for valu-
able discussions and suggestions.
465
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless unsu-
pervised learning with features. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics. Association for Computational
Linguistics.
Adam Berger and John Lafferty. 1999. Information re-
trieval as statistical translation. In Proceedings of the
1999 ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval.
Stephen Boyd and Lieven Vandenberghe. 2004. Convex
Optimization. Cambridge University Press.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert. L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, and Omar Zaidan, editors. 2010. Pro-
ceedings of the Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the royal statistical society, se-
ries B, 39(1).
Pinar Duygulu, Kobus Barnard, Nando de Freitas,
P. Duygulu, K. Barnard, and David Forsyth. 2002.
Object recognition as machine translation: Learning a
lexicon for a fixed image vocabulary. In Proceedings
of ECCV.
Volker Kaibel, Marc E. Pfetsch, and TU Berlin. 2002.
Some algorithmic problems in polytope theory. In
Dagstuhl Seminars, pages 23?47.
N. Karmarkar. 1984. A new polynomial-time algorithm
for linear programming. Combinatorica, 4:373?395,
December.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, and Dan Tufis. 2006.
The JRC-acquis: A multilingual aligned parallel cor-
pus with 20+ languages. In Proceedings of the 5th
International Conference on Language Resources and
Evaluation (LREC).
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th Int. Conf. on
Computational Linguistics (COLING). Association for
Computational Linguistics.
Appendix A: Convex functions and convex
optimization problems
We denote the domain of a function f by dom f .
Definition A function f : Rn ? R is convex if and only
if dom f is a convex set and for all x, y ? dom f and
? ? 0, ? ? 1:
f(?x+ (1? ?)y) ? ?f(x) + (1? ?)f(y) (6)
Definition A function f is strictly convex iff dom f is a
convex set and for all x 6= y ? dom f and ? > 0, ? < 1:
f(?x+ (1? ?)y) < ?f(x) + (1? ?)f(y) (7)
Definition A convex optimization problem is defined by:
min f0(x)
subject to
fi(x) ? 0, i = 1 . . . k
aTj x = bj , j = 1 . . . l
Where the functions f0 to fk are convex and the equal-
ity constraints are affine.
It can be shown that the feasible set (the set of points
that satisfy the constraints) is convex and that any local
optimum for the problem is a global optimum. If f0
is strictly convex then any local optimum is the unique
global optimum.
466
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 110?120,
Dublin, Ireland, August 23-24 2014.
See No Evil, Say No Evil:
Description Generation from Densely Labeled Images
Mark Yatskar
1
?
my89@cs.washington.edu
1
Computer Science & Engineering
University of Washington
Seattle, WA, 98195, USA
Michel Galley
2
mgalley@microsoft.com
Lucy Vanderwende
2
lucyv@microsoft.com
Luke Zettlemoyer
1
lsz@cs.washington.edu
2
Microsoft Research
One Microsoft Way
Redmond, WA, 98052, USA
Abstract
This paper studies generation of descrip-
tive sentences from densely annotated im-
ages. Previous work studied generation
from automatically detected visual infor-
mation but produced a limited class of sen-
tences, hindered by currently unreliable
recognition of activities and attributes. In-
stead, we collect human annotations of ob-
jects, parts, attributes and activities in im-
ages. These annotations allow us to build
a significantly more comprehensive model
of language generation and allow us to
study what visual information is required
to generate human-like descriptions. Ex-
periments demonstrate high quality output
and that activity annotations and relative
spatial location of objects contribute most
to producing high quality sentences.
1 Introduction
Image descriptions compactly summarize com-
plex visual scenes. For example, consider the de-
scriptions of the image in Figure 1, which vary in
content but focus on the women and what they are
doing. Automatically generating such descriptions
is challenging: a full system must understand the
image, select the relevant visual content to present,
and construct complete sentences. Existing sys-
tems aim to address all of these challenges but
use visual detectors for only a small vocabulary
of words, typically nouns, associated with objects
that can be reliably found.
1
Such systems are blind
?
This work was conducted at Microsoft Research.
1
While object recognition is improving (ImageNet accu-
racy is over 90% for 1000 classes) progress in activity recog-
nition has been slower; the state of the art is below 50% mean
average precision for 40 activity classes (Yao et al., 2011).
cars (Count:3) Isa: ride, vehicle,? Doing: parking,? Has: steering wheel,? Attrib: black, shiny,? 
children (Count:2) Isa: kids, children ? Doing: biking, riding ? Has: pants, bike ? Attrib: young, small ? 
bike (Count:1) Isa: bike, bicycle,? Doing: playing,? Has: chain, pedal,? Attrib: silver, white,? 
women(Count:3) Isa: girls, models,? Doing: smiling,...  Has: shorts, bags,? Attrib: young, tan,? 
purses(Count:3) Isa: accessory,? Doing: containing,? Has: body, straps,? Attrib: black, soft,? 
sidewalk(Count:1) Isa: sidewalk, street,? Doing: laying,? Has: stone, cracks,? Attrib: flat, wide,? 
woman(Count:1) Isa: person, female,? Doing: pointing,? Has: nose, legs,? Attrib: tall, skinny,? 
tree(Count:1) Isa: plant,? Doing: growing,? Has: branches,? Attrib: tall, green,? 
kids(Count:5) Isa: group, teens,? Doing: walking,? Has: shoes, bags,? Attrib: young,? 
Five young people on the street, two sharing a bicycle.
Several young people are walking near parked vehicles.
Three girls with large handbags walking down the sidewalk.
Three women walk down a city street, as seen from above.
Three young woman walking down a sidewalk looking up.
Figure 1: An annotated image with human generated sen-
tence descriptions. Each bounding polygon encompasses one
or more objects and is associated with a count and text la-
bels.This image has 9 high level objects annotated with over
250 textual labels.
to much of the visual content needed to generate
complete, human-like sentences.
In this paper, we instead study generation with
more complete visual support, as provided by hu-
man annotations, allowing us to develop more
comprehensive models than previously consid-
ered. Such models have the dual benefit of (1)
providing new insights into how to construct more
human-like sentences and (2) allowing us to per-
form experiments that systematically study the
contribution of different visual cues in generation,
suggesting which automatic detectors would be
most beneficial for generation.
In an effort to approximate relatively complete
visual recognition, we collected manually labeled
representations of objects, parts, attributes and ac-
tivities for a benchmark caption generation dataset
that includes images paired with human authored
110
descriptions (Rashtchian et al., 2010).
2
As seen
in Figure 1, the labels include object boundaries
and descriptive text, here including the facts that
the children are ?riding? and ?walking? and that
they are ?young.? Our goal is to be as exhaustive
as possible, giving equal treatment to all objects.
For example, the annotations in Figure 1 contain
enough information to generate the first three sen-
tences and most of the content in the remaining
two. Labels gathered in this way are a type of fea-
ture norms (McRae et al., 2005), which have been
used in the cognitive science literature to approxi-
mate human perception and were recently used as
a visual proxy in distributional semantics (Silberer
and Lapata, 2012). We present the first effort, that
we are aware of, for using feature norms to study
image description generation.
Such rich data allows us to develop significantly
more comprehensive generation models. We di-
vide generation into choices about which visual
content to select and how to realize a sentence that
describes that content. Our approach is grammar-
based, feature-rich, and jointly models both deci-
sions. The content selection model includes la-
tent variables that align phrases to visual objects
and features that, for example, measure how vi-
sual salience and spatial relationships influence
which objects are mentioned. The realization ap-
proach considers a number of cues, including lan-
guage model scores, word specificity, and relative
spatial information (e.g. to produce the best spa-
tial prepositions), when producing the final sen-
tence. When used with a reranking model, includ-
ing global cues such as sentence length, this ap-
proach provides a full generation system.
Our experiments demonstrate high quality vi-
sual content selection, within 90% of human per-
formance on unigram BLEU, and improved com-
plete sentence generation, nearly halving the dif-
ference from human performance to two base-
lines on 4-gram BLEU. In ablations, we measure
the importance of different annotations and visual
cues, showing that annotation of activities and rel-
ative bounding box information between objects
are crucial to generating human-like description.
2 Related Work
A number of approaches have been proposed
for constructing sentences from images, includ-
ing copying captions from other images (Farhadi
2
Available at : http://homes.cs.washington.edu/?my89/
et al., 2010; Ordonez et al., 2011), using text
surrounding an image in a news article (Feng
and Lapata, 2010), filling visual sentence tem-
plates (Kulkarni et al., 2011; Yang et al., 2011;
Elliott and Keller, 2013), and stitching together ex-
isting sentence descriptions (Gupta and Mannem,
2012; Kuznetsova et al., 2012). However, due to
the lack of reliable detectors, especially for activi-
ties, many previous systems have a small vocab-
ulary and must generate many words, including
verbs, with no direct visual support. These prob-
lems also extend to video caption systems (Yu and
Siskind, 2013; Krishnamoorthy et al., 2013).
The Midge algorithm (Mitchell et al., 2012)
is most closely related to our approach, and will
provide a baseline in our experiments. Midge is
syntax-driven but again uses a small vocabulary
without direct visual support for every word. It
outputs a large set of sentences to describe all
triplets of recognized objects in the scene, but does
not include a content selection model to select the
best sentence. We extend Midge with content and
sentence selection rules to use it as a baseline.
The visual facts we annotate are motivated by
research in machine vision. Attributes are a
good intermediate representation for categoriza-
tion (Farhadi et al., 2009). Activity recognition
is an emerging area in images (Li and Fei-Fei,
2007; Yao et al., 2011; Sharma et al., 2013) and
video (Weinland et al., 2011), although less stud-
ied than object recognition. Also, parts have been
widely used in object recognition (Felzenszwalb
et al., 2010). Yet, no work tests the contribution of
these labels for sentence generation.
There is also a significant amount of work
on other grounded language problems, where re-
lated models have been developed. Visual re-
ferring expression generation systems (Krahmer
and Van Deemter, 2012; Mitchell et al., 2013;
FitzGerald et al., 2013) aim to identify specific
objects, a sub-problem we deal with when de-
scribing images more generally. Other research
generates descriptions in simulated worlds and,
like this work, uses feature rich models (Angeli
et al., 2010), or syntactic structures like PCFGs
(Chen et al., 2010; Konstas and Lapata, 2012) but
does not combine the two. Finally, Zitnick and
Parikh (2013) study sentences describing clipart
scenes. They present a number of factors influenc-
ing overall descriptive quality, several of which we
use in sentence generation for the first time.
111
3 Dataset
We collected a dataset of richly annotated images
to approximate gold standard visual recognition.
In collecting the data, we sought a visual annota-
tion with sufficient coverage to support the gen-
eration of as many of the words in the original
image descriptions as possible. We also aimed to
make it as visually exhaustive as possible?giving
equal treatment to all visible objects. This ensures
less bias from annotators? perception about which
objects are important, since one of the problems
we would like to solve is content selection. This
dataset will be available for future experiments.
We built on the dataset from (Rashtchian et
al., 2010) which contained 8,000 Flickr images
and associated descriptions gathered using Ama-
zon Mechanical Turk (MTurk). Restricting our-
selves to Creative Commons images, we sampled
500 images for annotation.
We collected annotations of images in three
stages using MTurk, and assigned each annotation
task to 3-5 workers to improve quality through re-
dundancy (Callison-Burch, 2009). Below we de-
scribe the process for annotating a single image.
Stage 1: We prompted five turkers to list all ob-
jects in an image, ignoring objects that are parts of
larger objects (e.g., the arms of a person), which
we collected later in Stage 3. This list also in-
cluded groups, such as crowds of people.
Stage 2: For each unique object label from
Stage 1, we asked two turkers to draw a polygon
around the object identified.
3
In cases where the
object is a group, we also asked for the number of
objects present (1-6 or many). Finally, we created
a list of all references to the object from the first
stage, which we call the Object facet.
Stage 3: For each object or group, we prompted
three turkers to provide descriptive phrases of:
? Doing ? actions the object participates in, e.g.
?jumping.?
? Parts ? physical parts e.g. ?legs?, or other
items in the possession of the object e.g.
?shirt.?
? Attributes ? adjectives describing the object,
e.g. ?red.?
? Isa ? alternative names for a object e.g.
?boy?, ?rider.?
Figure 1 shows more examples for objects
3
We modified LabelMe (Torralba et al., 2010).
in a labeled image.
4
We refer to all of these
annotations, including the merged Object la-
bels, as facets. These labels provide feature
norms (McRae et al., 2005), which have recently
used as a visual proxy in distributional seman-
tics (Silberer and Lapata, 2012; Silberer et al.,
2013) but have not been previous studied for gen-
eration. This annotation of 500 images (2500
sentences) yielded over 4000 object instances and
100,000 textual labels.
4 Approach
Given such rich annotations, we can now de-
velop significantly more comprehensive genera-
tion models. In this section, we present an ap-
proach that first uses a generative model and then
a reranker. The generative model defines a dis-
tribution over content selection and content real-
ization choices, using diverse cues from the image
annotations. The reranker trades off our generative
model score, language model score (to encourage
fluency), and length to produce the final sentence.
Generative Model We want to generate a sen-
tence ~w = ?w
1
. . . w
n
? where each word w
i
? V
comes from a fixed vocabulary V . The vocabu-
lary V includes all 2700 words used in descriptive
sentences in the training set.
5
The model conditions on an annotated image I
that contains a set of objects O, where each ob-
ject o ? O has a bounding polygon and a number
of facets containing string labels. To model the
naming of specific objects, words w
i
can be asso-
ciated with alignment variables a
i
that range over
O. One such variable is introduced for each head
noun in the sentence. Figure 2 shows alignment
variable settings with colors that match objects in
the image. Finally, as a byproduct of the hierarchi-
cal generative process, we construct an undirected
dependency tree
~
d over the words in ~w.
The complete generative model defines the
probability p(~w,~a,
~
d | I) of a sentence ~w, word
alignments ~a, and undirected dependency tree
~
d,
given the annotated input image I . The overall
process unfolds recursively, as seen in Figure 3.
4
In the experiments, Parts and Isa facets do not improve
performance, so we do not use them in the final model. Isa
is redundant with the Object facet, as seen in Figure 1. Also
parts like clothing, were often annotated as separate objects.
5
We do not generate from image facets directly, because
only 20% of the sentences in our data can be produced like
this. Instead, we develop features which consider the similar-
ity between labels in the image and words in the vocabulary.
112
Three girls  with large  handbags  walking  down  the  sidewalk 
? = ?= ? = 
Figure 2: One path through the generative model and the
Bayesian network it induces. The first row of colored circles
are alignment variables to objects in the image. The second
row is words, generated conditioned on alignments.
The main clause is produced by first selecting the
subject alignment a
s
followed by the subject word
w
s
. It then chooses the verb and optionally the ob-
ject alignment a
o
and word w
o
. The process then
continues recursively, modifying the subject, verb,
and object of the sentence with noun and prepo-
sitional modifiers. The recursion begins at Step
2 in Figure 3. Given a parent word w and that
word?s relevant alignment variable a, the model
creates attachments where w is the grammatical
head of subsequently produced words. Choices
about whether to create noun modifiers or preposi-
tional modifiers are made in steps (a) and (b). The
process chooses values for the alignment variables
and then chooses content words, adding connec-
tive prepositions in the case of prepositional mod-
ifiers. It then chooses to end or submits new word-
alignment pairs to be recursively modified.
Each line defines a decision that must be made
according to a local probability distribution. For
example, Step 1.a defines the probability of align-
ing a subject word to various objects in the im-
age. The distributions are maximum entropy mod-
els, similar to previous work (Angeli et al., 2010),
using features described in the next section. The
induced undirected dependency tree
~
d has an edge
between each word and the previously generated
word (or the input word w in Steps 2.a.i and 2.a.ii,
when no previous word is available). Figure 2
shows a possible output from the process, along
with the Bayesian network that encodes what each
decision was conditioned on during generation.
Learning We learn the model from data
{(~w
i
,
~
d
i
, I
i
) | i = 1 . . .m} containing sentences
~w
i
, dependency trees
~
d
i
, computed with the Stan-
ford parser (de Marneffe et al., 2006), and images
1. for a main clause (d,e are optional), select:
(a) subject a
s
alignment from p
a
(a).
(b) subject word w
s
from p
n
(w | a
s
,
~
d
c
)
(c) verb word w
v
from p
v
(w | a
s
,
~
d
c
)
(d) object alignment a
o
from p
a
(a
?
| a
s
, w
v
,
~
d
c
)
(e) object word w
o
from p
n
(w | a
o
,
~
d
c
)
(f) end with p
stop
or go to (2) with (w
s
, a
s
)
(g) end with p
stop
or go to (2) with (w
v
, a
s
)
(h) end with p
stop
or go to (2) with (w
o
, a
o
)
2. for a (word, alignment) (w
?
, a) (a,b are optional):
(a) if w
?
not verb: modify w
?
with noun, select:
i. modifier word w
n
from p
n
(w | a,
~
d
c
).
ii. end with p
stop
or go to (2) with (a
m
, w
n
)
(b) modify w
?
with preposition, select:
i. preposition word w
p
if w
?
not a verb: from p
p
(w | a,
~
d
c
)
else: from p
p
(w | a,w
v
,
~
d
c
)
ii. object alignment a
p
from p
a
(a
?
| a,w
p
,
~
d
c
)
iii. object word w
n
from p
n
(w | a
p
,
~
d
c
).
iv. end with p
stop
or go to (2) with (a
p
, w
n
)
Figure 3: Generative process for producing words ~w, align-
ments ~a and dependencies
~
d. Each distribution is conditioned
on the partially complete path through generative process
~
d
c
to establish sentence context. The notation p
stop
is short hand
for p
stop
(STOP |~w,
~
d
c
) the stopping distribution.
I
i
. The dependency trees define the path that was
taken through the generative process in Figure 3
and are used to create a Bayesian network for ev-
ery sentence, like in Figure 2. However, object
alignments ~a
i
are latent during learning and we
must marginalize over them.
The model is trained to maximize the condi-
tional marginal log-likelihood of the data with reg-
ularization:
L(?) =
?
i
log
?
~a
p(~a, ~w
i
,
~
d
i
| I
i
; ?)? r|?|
2
where ? is the set of parameters and r is the regu-
larization coefficient. In essence, we maximize the
likelihood of every sentence?s observed Bayesian
network, while marginalizing over content selec-
tion variables we did not observe.
Because the model only includes pairwise de-
pendencies between the hidden alignment vari-
ables ~a, the inference problem is quadratic in the
number of objects and non-convex because ~a is
unobserved. We optimize this objective directly
with L-BFGS, using the junction-tree algorithm to
compute the sum and the gradient.
6
6
To compute the gradient, we differentiate the recurrence
in the junction-tree algorithm by applying the product rule.
113
Inference To describe an image, we need to
maximize over word, alignment, and the depen-
dency parse variables:
argmax
~w,~a,
~
d
p(~w,~a,
~
d | I)
This computation is intractable because we
need to consider all possible sentences, so we use
beam search for strings up to a fixed length.
Reranking Generating directly from the process
in Figure 3 results in sentences that may be short
and repetitive because the model score is a product
of locally normalized distributions. The reranker
takes as input a candidate list c, for an image I , as
decoded from the generative model. The candidate
list includes the top-k scoring hypotheses for each
sentence length up to a fixed maximum. A linear
scoring function is used for reranking optimized
with MERT (Och, 2003) to maximize BLEU-2.
5 Features
We construct indicator features to capture vari-
ation in usage in different parts of the sen-
tence, types of objects that are mentioned, visual
salience, and semantic and visual coordination be-
tween objects. The features are included in the
maximum entropy models used to parameterize
the distributions described in Figure 3. Whenever
possible, we use WordNet Synsets (Miller, 1995)
instead of lexical features to limit over-fitting.
Features in the generative model use tests for
local properties, such as the identity of a synset
of a word in WordNet, conjoined with an iden-
tifier that indicates context in the generative pro-
cess.
7
Generative model features indicate (1) vi-
sual and semantic information about objects in dis-
tributions over alignments (content selection) and
(2) preferences for referring to objects in distribu-
tions over words (content realization). Features in
the reranking model indicate global properties of
candidate sentences. Exact formulas for comput-
ing the features are in the appendix.
Visual features, such as an object?s position in
the image, are used for content selection. Pairwise
visual information between two objects, for exam-
ple the bounding box overlap between objects or
the relative position of the two objects, is included
in distributions where selection of an alignment
7
For example, in Figure 2 the context for the word ?side-
walk? would be ?word,syntactic-object,verb,preposition? in-
dicating it is a word, in the syntactic object of a preposition,
which was attached to a verb modifying prepositional phrase.
variable conditions on previously generated align-
ments. For verbs (Step 1.d in Figure 3) and prepo-
sitions (Step 2.b.ii), these features are conjoined
with the stem of the connective.
Semantic types of objects are also used in con-
tent selection. We define semantic types by finding
synsets of labels in objects that correspond to high
level types, a list motivated by the animacy hierar-
chy (Zaenen et al., 2004).
8
Type features indicate
the type of the object referred to by an alignment
variable as well as the cross product of types when
an alignment variable is on conditioning side of
a distribution (e.g. Step 1.d). Like above, in the
presence of a connective word, these features are
conjoined with the stem of the connective.
Content realization features help select words
when conditioning on chosen alignments (e.g.
Step 1.b). These features include the identity of
the WordNet synset corresponding to a word, the
word?s depth in the synset hierarchy, the language
model score for adding that word
9
and whether the
word matches labels in facets corresponding to the
object referenced by an alignment variable.
Reranking features are primarily used to over-
come issues of repetition and length in the genera-
tive distributions, more commonly used for align-
ment, than to create sentences. We use only four
features: length, the number of repetitions, gener-
ative model score, and language model score.
6 Experimental Setup
Data We used 70% of the data for training (1750
sentences, 350 images), 15% for development, and
15% for testing (375 sentences, 75 images).
Parameters The regularization parameter was
set on the held out data to r = 8. The reranker
candidate list included the top 500 sentences for
each sentence length up to 15 and weights were
optimized with Z-MERT (Zaidan, 2009).
Metrics Our evaluation is based on BLEU-n
(Papineni et al., 2001), which considers all n-
grams up to length n. To assess human perfor-
mance using BLEU, we score each of the five ref-
erences against the four other ones and finally av-
erage the five BLEU scores. In order to make these
results comparable to BLEU scores for our model
8
For example, human, animal, artifact (a human created
object), natural body (trees, water, ect.), or natural artifact
(stick, leaf, rock).
9
We use tri-grams with Kneser-Ney smoothing over the 1
million caption data set (Ordonez et al., 2011).
114
and baselines, we perform the same five-fold aver-
aging when computing BLEU for each system.
We also compute accuracy for different syn-
tactic positions in the sentence. We look at a
number of categories: the main clause?s compo-
nents (S,V,O), prepositional phrase components,
the preposition (Pp) and their objects (Po) and
noun modifying words (N), including determiners.
Phrases match if they have an exact string match
and share context identifiers as defined in the fea-
tures sections.
Human Evaluation Annotators rated sentences
output by our full model against either human or a
baseline system generated descriptions. Three cri-
teria were evaluated: grammaticality, which sen-
tence is more complete and well formed; truthful-
ness, which sentence is more accurately capturing
something true in the image; and salience, which
sentence is capturing important things in the image
while still being concise. Two annotators anno-
tated all test pairs for all criteria for a given pair of
systems. Six annotators were used (none authors)
and agreement was high (Cohen?s kappa = 0.963,
0.823 and 0.703 for grammar, truth and salience).
Machine Translation Baseline The first base-
line is designed to see if it is possible to generate
good sentences from the facet string labels alone,
with no visual information. We use an extension of
phrase-based machine translation techniques (Och
et al., 1999). We created a virtual bitext by pair-
ing each image description (the target sentence)
with a sequence
10
of visual identifiers (the source
?sentence?) listing strings from the facet labels.
Since phrases produced by turkers lack many of
the functions words needed to create fluent sen-
tences, we added one of 47 function words either
at the start or the end of each output phrase.
The translation model included standard fea-
tures such as language model score (using our cap-
tion language model described previously), word
count, phrase count, linear distortion, and the
count of deleted source words. We also define
three features that count the number of Object, Isa,
and Doing phrases, to learn a preference for types
of phrases. The feature weights are tuned with
MERT (Och, 2003) to maximize BLEU-4.
Midge Baseline As described in related work,
the Midge system creates a set of sentences to de-
scribe everything in an input image. These sen-
10
We defined a consistent ordering of visual identifiers and
set the distortion limit of the phrase-based decoder to infinity.
BL-1 BL-2 BL-3 BL-4
Human 61.0 42.0 27.8 18.3
Full Model 57.1 35.7 18.3 9.5
MT Baseline 39.8 23.6 13.2 6.1
Midge Baseline 43.5 20.2 9.4 0.0
Table 1: Results for the test set for the BLEU1-4 metrics.
Grammar Full Other Equal
Full vs Human 7.65 19.4 72.94
Full vs MT 6.47 5.29 88.23
Full vs Midge 40.59 15.88 43.53
Truth Full Other Equal
Full vs Human 0.59 67.65 31.76
Full vs MT 30.0 10.59 59.41
Full vs Midge 51.76 27.71 23.53
Salience Full Other Equal
Full vs Human 8.82 88.24 2.94
Full vs MT 51.76 16.47 31.77
Full vs Midge 71.18 14.71 14.12
Table 2: Human evaluation of our Full-Model in heads
up tests against Human authored sentences and baseline sys-
tems, the machine translation baseline (MT) and the Midge
inspired baseline. Bold indicates the better system. Other is
not the Full system. Equal indicates neither sentence is better.
tences must all be true, but do not have to select
the same content that a person would. It can be
adapted to our task by adding object selection and
sentence ranking rules. For object selection, we
choose the three most frequently named objects
in the scene according to a background corpus of
image descriptions. For sentence selection, we
take all sentences within one word of the average
length of a sentence in our corpus, 11, and select
the one with best Midge generation score.
7 Results
We report experiments for our generation pipeline
and ablations that remove data and features.
Overall Performance Table 1 shows the re-
sults on the test set. The full model consis-
tently achieves the highest BLEU scores. Overall,
these numbers suggest strong content selection by
getting high recall for individual words (BLEU-
1), but fall further behind human performance as
the length of the n-gram grows (BLEU-2 through
BLEU-4). These number match our perception
that the model is learning to produce high quality
sentences, but does not always describe all of the
important aspects of the scene or use exactly the
expected wording. Table 4 presents example out-
put, which we will discuss in more detail shortly.
115
Model BL-1 BL-2 BL-3 BL-4 S V O Pp Po N
Human 64.7 46.0 31.5 20.1 - - - - - -
Full-Model 59.0 36.9 19.3 10.5 64.9 40.4 36.8 50.0 20.7 69.1
? doing 51.1 32.6 16.9 9.2 63.2 15.8 10.5 45.5 21.6 69.7
? count 55.4 33.5 16.0 8.5 59.6 35.1 15.4 53.7 19.5 66.7
? properties 57.8 37.2 18.8 10.0 61.4 36.8 36.8 47.1 20.7 73.5
? visual 56.7 35.1 18.9 9.4 64.9 36.8 50.0 41.8 15.3 71.6
? pairwise 56.9 35.5 16.5 8.2 64.9 40.4 45.5 42.4 21.2 70.9
Table 3: Ablation results on development data using BLEU1-4 and reporting match accuracy for sentence structures.
S: A girl playing a
guitar in the grass
R: A woman with a nylon stringed
guitar is playing in a field
S: A man playing with two
dogs in the water
R: A man is throwing a log into
a waterway while two dogs watch
S: Two men playing with
a bench in the grass
R: Nine men are playing a game
in the park, shirts versus skins
S: Three kids sitting on a road
R: A boy runs in a race
while onlookers watch
Table 4: Two good examples of output (top), and two ex-
amples of poor performance (bottom). Each image has two
captions, the system output S and a human reference R.
Human Evaluation Table 2 presents the results
of a human evaluation. The full model outper-
forms all baselines on every measure, but is not
always competitive with human descriptions. It
performs the best on grammaticality, where it is
judged to be as grammatical as humans. How-
ever, surprisingly, in many cases it is also often
judged equal to the other baselines. Examination
of baseline output reveals that the MT baseline of-
ten generates short sentences, having little chance
of being judged ungrammatical. Furthermore, the
Midge baseline, like our system, is a syntax-based
system and therefore often produces grammatical
sentences. Although our system performs well
with respect to the baselines on truthfulness, of-
ten the system constructs sentences with incorrect
prepositions, an issue that could be improved with
better estimates of 3-d position in the image. On
truthfulness, the MT baseline is comparable to our
system, often being judged equal, because its out-
put is short. Our system?s strength is salience, a
factor the baselines do not model.
Data Ablation Table 3 shows annotation abla-
tion experiments on the development set, where
we remove different classes of data labels to mea-
sure the performance that can be achieved with
less visual information. In all cases, the overall
behavior of the system varies, as it tries to learn to
compensate for the missing information.
Ablating actions is by far the most detrimental.
Overall BLEU score suffers and prediction accu-
racy of the verb (V) degrades significantly causing
cascading errors that affect the object of the verb
(O). Removing count information affects noun at-
tachment (N) performance. Images where deter-
miner use is important or where groups of objects
are best identified by the number (for example,
three dogs) are difficult to describe naturally. Fi-
nally, we see a tradeoff when removing properties.
There is an increase in noun modifier accuracy (N)
but a decrease in content selection quality (BL-1),
showing recall has gone down. In essence, the ap-
proach learns to stop trying to generate adjectives
and other modifiers that would rely on the missing
properties. The difference in BLEU score with the
Full-Model is small, even without these modifiers,
because there often still exists a a short output with
high accuracy.
Feature Ablation The bottom two rows in Ta-
ble 3 show ablations of the visual and pairwise
features, measuring the contribution of the visual
information provided by the bounding box anno-
tations. The ablated visual information includes
bounding-box positions and relative pairwise vi-
sual information. The pairwise ablation removes
the ability to model any interactions between ob-
jects, for example, relative bounding box or pair-
wise object type information.
Overall, prepositional phrase accuracy is most
affected. Ablating visual features significantly im-
pacts accuracy of prepositional phrases (Pp and
Po), affecting the use of preposition words the
most, and lowering fluency (BL-4). Precision in
116
the object of the verb (O) rises; the model makes
? 50% fewer predictions in that position than the
Full-Model because it lacks features to coordinate
subject and object of the verb. Ablating pairwise
features has similar results. While the model cor-
rects errors in the object of the preposition (Po)
with the addition of visual features, fluency is still
worse than Full-Model, as reflected by BL-4.
Qualitative Results Table 4 has examples of
good and bad system output. The first two im-
ages are good examples, including both system
output (S) and a human reference (R). The sec-
ond two contain lower quality outputs. Overall,
the model captures common ways to refer to peo-
ple and scenes. However, it does better for images
with fewer sentient objects because content selec-
tion is less ambiguous.
Our system does well at finding important ob-
jects. For example, in the first good image, we
mention the guitar instead of the house, both of
which are prominent and have high overlap with
the woman. In the second case, we identify that
both dogs and humans tend to be important actors
in scenes but poorly identify their relationship.
The bad examples show difficult scenes. In the
first description the broad context is not identi-
fied, instead focusing on the bench (highlighted in
red). The second example identifies a weakness
in our annotation: it encodes contradictory group-
ings of the people. The groupings covers all of
the children, including the boy running, and many
subsets of the people near the grass. This causes
ambiguity and our methods cannot differentiate
them, incorrectly mentioning just the children and
picking an inappropriate verb (one participant in
the group is not sitting). Improved annotation of
groups would enable the study of generation for
more complex scenes, such as these.
8 Conclusion
In this work we used dense annotations of images
to study description generation. The annotations
allowed us to not only develop new models, better
capable of generating human-like sentences, but
also to explore what visual information is crucial
for description generation. Experiments showed
that activity and bounding-box information is im-
portant and demonstrated areas of future work. In
images that are more complex, for example multi-
ple sentient objects, object grouping and reference
will be important to generating good descriptions.
Issues of this type can be explored with annota-
tions of increasing complexity.
Appendix A
This appendix describes the feature templates for
the generative model in greater detail.
Features in the generative model conjoin indica-
tors for local tests, such as STEM(w) which in-
dicates the stem of a wordw, with a global contex-
tual identifier CONTEXT(v, d) that indicates
properties of the generation history, as described
in detail below. Table 5 provides a reference for
which feature templates are used in the generative
model distributions, as defined in Figure 3.
8.1 Feature Templates
CONTEXT(n, d) is an indicator for a contex-
tual identifier for a variable n in the model de-
pending on the dependency structure d. There is
an indicator for all combinations of the type of n
(alignment or word), the position of n (subject,
syntactic object, verb, noun-modifier, or preposi-
tion), the position of the earliest variable along
the path to generate n, and the type of attach-
ment to that variable (noun or prepositional mod-
ifier). For example, in Figure 2 the context for
the word ?sidewalk? would be ?word,syntactic-
object,verb,preposition? indicating it is a word, the
object of a preposition, whose path was along a
verb modifying prepositional phrase.
11
TYPE(a) indicates the high level type of an
object referred to by alignment variable a. We
use synsets to define high level types including
human, animal, artifact, natural artifact and var-
ious synsets that capture scene information,
12
a
list motivated by the animacy hierarchy (Zaenen
et al., 2004). Each object is assigned a type by
finding the synset for its name (object facet), and
tracing the hypernym structure in Wordnet to find
the appropriate class, if one exists. Additionally,
the type indicates whether the object is a group or
not. For example, in Figure 2, the blue polygon
has type ?person,group?, or the red bike polygon
has type ?artifact,single.?
11
Similarly ?large? is ?word,noun,subject,preposition?
while ?girls? is special cased to ?word,subject,root? be-
cause it has no initial attachment. The alignment vari-
able above the word handbags is ?alignment,syntactic-
object,subject,preposition? because it an alignment variable,
is in the syntactic object position of a preposition and can be
located by following a subject attached pp.
12
WordNet divides these into synsets expressing water,
weather, nature and a few more.
117
Feature Family Included In Steps
CONTEXT(a
?
,
~
d
c
)?
{TYPE(a
?
),MENTION(a
?
, do),MENTION(a
?
, obj ),VISUAL(a
?
)}
p
a
(a
?
|
~
d
c
)
p
a
(a
?
| a,w,
~
d
c
)
1.a, 1.d, 2.b.ii
CONTEXT(a
?
,
~
d
c
)? {TYPE(a)?TYPE(a
?
),VISUAL2(a, a
?
)} p
a
(a
?
| a,w,
~
d
c
) 1.d, 2.b.i
CONTEXT(a
?
,
~
d
c
)?
{TYPE(a)?TYPE(a
?
)? STEM(w),VISUAL2(a, a
?
)? STEM(w)}
p
a
(a
?
| a,w,
~
d
c
) 1.d, 2.b.i
CONTEXT(a,
~
d
c
)?
{WORDNET(w),MATCH(w, a),SPECIFICITY(w, a),
ADJECTIVE(w, a),DETERMINER(w, a)}
p
n
(w | a,
~
d
c
) 1.b, 1.e, 2.a.i
2.b.ii
CONTEXT(a,
~
d
c
)? {MATCH(w, a),TYPE(a)? STEM(w)} p
v
(w | a,
~
d
c
) 1.c
CONTEXT(a
?
,
~
d
c
)?TYPE(a)? STEM(w
p
) p
p
(w | a,
~
d
c
)
p
p
(w | a,w
v
,
~
d
c
)
2.b.i
CONTEXT(a
?
,
~
d
c
)? STEM(w
v
)? STEM(w) p
p
(w | a,w
v
,
~
d
c
) 2.b.i
Table 5: Feature families and distributions that include them. ? indicates the cross-product of the indi-
cator features. Distributions are listed more than once to indicate they use multiple feature families.
VISUAL(a) returns indicators for visual facts
about the object that a aligns to. There is an in-
dicator for two quantities: (1) overlap of object?s
polygon with every horizontal third of the image,
as a fraction of the object?s area, and (2) the ob-
ject?s distance to the center of the image as frac-
tion of the diagonal of the image. Each quantity,
v, is put into three overlapping buckets: if v > .1,
if v > .5, and if v > .9.
VISUAL2(a, a
?
) indicates pairwise visual
facts about two objects. There is an indicator for
the following quantities bucketed: the amount of
overlap between the polygons for a and a
?
as a
fraction of the size of a?s polygon, the distance
between the center of the polygon for a and a
?
as
a fraction of image?s diagonal, and the slope be-
tween the center of a and a
?
. Each quantity, v, is
put into three overlapping buckets: if v > .1, if
v > .5, and if v > .9. There is an indicator for the
relative position of extremities a and a
?
: whether
the rightmost point of a is further right than a
?
?s
rightmost or leftmost point, and the same for top,
left, and bottom.
WORDNET(w) returns indicators for all hy-
pernyms of a word w. The two most specific
synsets are not used when there at least 8 options.
MENTION(a, facet) returns the union of the
WORDNET(w) features for all words w in the
facet facet for the object referred to alignment a.
ADJECTIVE(w, a) indicates four types
of features specific to adjective usage. If
MENTION(w,Attributes) is not empty, indi-
cate : (1) the satellite adjective synset of w in
Wordnet, (2) the head adjective synset of w in
Wordnet, (3) the head adjective synset conjoined
withTYPE(a), and (4) the number of times there
exists a label in the Attributes facet of a that has
the same head adjective synset as w.
DETERMINER(w, a) indicates four deter-
miner specific features. If w is a determiner, then
indicate : (1) the identity of w conjoined with the
count (the label for numerosity) of a, (2) the iden-
tity of w conjoined with an indicator for if the
count of a is greater than one, (3) the identity of w
conjoined with TYPE(a) and (4) the frequency
with which w appears before its head word in the
Flikr corpus (Ordonez et al., 2011).
MATCH(w, a), indicates all facets of object
a that contain words with the same stem as w.
SPECIFICITY(w, a) is an indicator of the
specificity of the word w when referring to the ob-
ject aligned to a. Indicates the relative depth of
w in Wordnet, as compared to all words w
?
where
MATCH(w
?
, a) is not empty. The depth is buck-
eted into quintiles.
STEM(w) returns the Porter2 stem of w.
13
The distribution for stopping, p
stop
(STOP |
~
d
c
, ~w), contains two types of features. (1) Struc-
tural features indicating for the number of times
a contextual identifier has appeared so far in the
derivation and (2) mention features indicating the
types of objects mentioned.
14
To compute men-
tion features, we consider all possible types of ob-
jects, t, then there is an indicator for: (1) if ?o, ?w ?
~w : MATCH(w, o) 6= ? ?TYPE(o) = t, (2) whether
?o, 6 ?w ? ~w : MATCH(w, o) 6= ??TYPE(o) = t and
(3) if (1) does not hold but (2) does.
Acknowledgments This work is partially funded by DARPA
CSSG (D11AP00277) and ARO (W911NF-12-1-0197). We
thank L. Zitnick, B. Dolan, M. Mitchell, C. Quirk, A. Farhadi,
B. Russell for helpful conversations. Also, L. Zilles, Y. Atrzi,
N. FitzGerald, T. Kwiatkowski and reviewers for comments.
13
http://snowball.tartarus.org/algorithms/english/stemmer.html
14
Object mention features cannot contain ~a because that
creates large dependencies in inference for learning.
118
References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach
to generation. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In EMNLP, pages 286?295, August.
David L. Chen, Joohyun Kim, and Raymond J.
Mooney. 2010. Training a multilingual sportscaster:
Using perceptual context to learn language. JAIR,
37:397?435.
Marie-Catherine de Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generating
typed dependency parses from phrase structure
parses. In LREC, volume 6, pages 449?454.
Desmond Elliott and Frank Keller. 2013. Image De-
scription using Visual Dependency Representations.
In EMNLP.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their at-
tributes. In Computer Vision and Pattern Recogni-
tion, 2009. CVPR 2009. IEEE Conference on, pages
1778?1785. IEEE.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: Generating sentences from images.
In Proceedings of the 11th European conference on
Computer Vision, ECCV?10, pages 15?29.
Pedro F Felzenszwalb, Ross B Girshick, David
McAllester, and Deva Ramanan. 2010. Object
detection with discriminatively trained part-based
models. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 32(9):1627?1645.
Yansong Feng and Mirella Lapata. 2010. How many
words is a picture worth? Automatic caption gener-
ation for news images. In ACL, pages 1239?1249.
Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-
moyer. 2013. Learning distributions over logi-
cal forms for referring expression generation. In
EMNLP.
Ankush Gupta and Prashanth Mannem. 2012. From
image annotation to image description. In NIPS,
volume 7667, pages 196?204.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text generation via discriminative reranking. In
ACL, pages 369?378.
Emiel Krahmer and Kees Van Deemter. 2012. Compu-
tational generation of referring expressions: A sur-
vey. Computational Linguistics, 38(1):173?218.
Niveda Krishnamoorthy, Girish Malkarnenkar, Ray-
mond Mooney, Kate Saenko, and Sergio Guadar-
rama. 2013. Generating natural-language video de-
scriptions using text-mined knowledge. Procedings
of AAAI, 2013(2):3.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A.C.
Berg, and T.L. Berg. 2011. Baby talk: Understand-
ing and generating simple image descriptions. In
Computer Vision and Pattern Recognition (CVPR),
pages 1601?1608.
Polina Kuznetsova, Vicente Ordonez, Alexander C
Berg, Tamara L Berg, and Yejin Choi. 2012. Col-
lective generation of natural image descriptions. In
ACL, pages 359?368.
Li-Jia Li and Li Fei-Fei. 2007. What, where and who?
Classifying events by scene and object recognition.
In ICCV, pages 1?8. IEEE.
Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris Mcnorgan. 2005. Semantic feature produc-
tion norms for a large set of living and nonliving
things. Behavior Research Methods, 37(4):547?
559.
George A Miller. 1995. WordNet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa
Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi,
Tamara Berg, Karl Stratos, and Hal Daum?e, III.
2012. Midge: Generating image descriptions from
computer vision detections. In EACL, pages 747?
756.
Margaret Mitchell, Kees van Deemter, and Ehud Re-
iter. 2013. Generating expressions that refer to vis-
ible objects. In Proceedings of NAACL-HLT, pages
1174?1184.
F. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proc. of the Joint Conf. of Empirical Methods in
Natural Language Processing and Very Large Cor-
pora, pages 20?28.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL, pages 160?
167.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2Text: Describing images using 1 million
captioned photographs. In NIPS, pages 1143?1151.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proc. of ACL.
C. Rashtchian, P. Young, M. Hodosh, and J. Hock-
enmaier. 2010. Collecting image annotations us-
ing Amazon?s Mechanical Turk. In Proceedings
of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, pages 139?147.
119
Gaurav Sharma, Fr?ed?eric Jurie, Cordelia Schmid, et al.
2013. Expanded parts model for human attribute
and action recognition in still images. In CVPR.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In EMNLP, July.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of semantic representation with visual
attributes. In ACL, pages 572?582.
Antonio Torralba, Bryan C Russell, and Jenny Yuen.
2010. LabelMe: Online image annotation and appli-
cations. Proceedings of the IEEE, 98(8):1467?1484.
Daniel Weinland, Remi Ronfard, and Edmond Boyer.
2011. A survey of vision-based methods for action
representation, segmentation and recognition. Com-
puter Vision and Image Understanding, 115(2):224?
241.
Yezhou Yang, Ching Lik Teo, Hal Daum?e III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods in
Natural Language Processing.
Bangpeng Yao, Xiaoye Jiang, Aditya Khosla, Andy Lai
Lin, Leonidas J. Guibas, and Li Fei-Fei. 2011.
Action recognition by learning bases of action at-
tributes and parts. In ICCV, Barcelona, Spain,
November.
Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded
language learning from video described with sen-
tences. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, volume 1, pages 53?63.
Annie Zaenen, Jean Carletta, Gregory Garretson,
Joan Bresnan, Andrew Koontz-Garboden, Tatiana
Nikitina, M Catherine O?Connor, and Tom Wasow.
2004. Animacy encoding in English: why and how.
In ACL Workshop on Discourse Annotation, pages
118?125.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
C. Lawrence Zitnick and Devi Parikh. 2013. Bring-
ing semantics into focus using visual abstraction. In
CVPR.
120
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 468?479,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Direct Error Rate Minimization for Statistical Machine Translation
Tagyoung Chung?
University of Rochester
Rochester, NY 14627, USA
chung@cs.rochester.edu
Michel Galley
Microsoft Research
Redmond, WA 98052, USA
mgalley@microsoft.com
Abstract
Minimum error rate training is often the pre-
ferred method for optimizing parameters of
statistical machine translation systems. MERT
minimizes error rate by using a surrogate rep-
resentation of the search space, such as N -
best lists or hypergraphs, which only offer
an incomplete view of the search space. In
our work, we instead minimize error rate di-
rectly by integrating the decoder into the min-
imizer. This approach yields two benefits.
First, the function being optimized is the true
error rate. Second, it lets us optimize param-
eters of translations systems other than stan-
dard linear model features, such as distortion
limit. Since integrating the decoder into the
minimizer is often too slow to be practical, we
also exploit statistical significance tests to ac-
celerate the search by quickly discarding un-
promising models. Experiments with a phrase-
based system show that our approach is scal-
able, and that optimizing the parameters that
MERT cannot handle brings improvements to
translation results.
1 Introduction
Minimum error rate training (Och, 2003) is a com-
mon method for optimizing linear model parame-
ters, which is an important part of building good ma-
chine translation systems. MERT minimizes an arbi-
trary loss function, usually an evaluation metric such
as BLEU (Papineni et al, 2002) or TER (Snover
et al, 2006) from a surrogate representation of the
search space, such as the N -best candidate transla-
tions of a development set. Much of the recent work
? This research was conducted during the author?s intern-
ship at Microsoft Research.
on minimum error rate training focused on improv-
ing the method by Och (2003). Recent efforts ex-
tended MERT to work on lattices (Macherey et al,
2008) and hypergraphs (Kumar et al, 2009). Ran-
dom restarts and random walks (Moore and Quirk,
2008) are commonly used to combat the fact the
search space is highly non-convex, often with mul-
tiple minima.
Several problems still remain with MERT, three
of which are addressed by this work. First, the N -
best error surface explored by MERT is generally
not the same as the true error surface, which means
that the error rate at an optimum1 of the N -best er-
ror surface is not guaranteed to be any close to an
optimum of the true error surface. Second, most
SMT decoders make search errors, yet MERT ig-
nores the fact that the error surface of an error-prone
decoder differs from the one of an exact decoder
(Chang and Collins, 2011). MERT calculates an en-
velope from candidate translations and assumes all
translations on the envelope are reachable by the de-
coder, but these translations may become unreach-
able due to search errors. Third, MERT is only used
to tune linear model parameters, yet SMT systems
have many free decoder parameters?such as distor-
tion limit and beam size?that are not handled by
MERT. MERT does not provide a principled way to
set these parameters.
In order to overcome these issues, we explore the
application of direct search methods (Wright, 1995)
to SMT. To do this, we integrate the decoder and
the evaluation metric inside the objective function,
1The optimum found by MERT (Och, 2003) is generally not
globally optimal. An alternative that optimizes N -best lists ex-
actly is presented by Galley and Quirk (2011), and we do not
discuss it further here.
468
which takes source sentences and a set of weights as
inputs, and outputs the evaluation score (e.g., BLEU
score) computed on the decoded sentences. Since it
is impractical to calculate derivatives of this func-
tion, we use derivative-free optimization methods
such as the downhill simplex method (Nelder and
Mead, 1965) and Powell?s method (Powell, 1964),
which generally handle such difficult search condi-
tions relatively well. This approach confers several
benefits over MERT. First, the function being opti-
mized is the true error rate. Second, integrating the
decoder inside the objective function forces the op-
timizer to account for possible search errors. Third,
contrary to MERT, our approach does not require in-
put parameters to be those of a linear model, so our
approach can tune a broader range of features, in-
cluding non-linear and hidden-state parameters (e.g.,
distortion limit, beam size, and weight vector ap-
plied to future cost estimates).
In this paper, we make direct search reasonably
fast thanks to two speedup techniques. First, we
use a model selection acceleration technique called
racing (Moore and Lee, 1994) in conjunction with
randomization tests (Riezler and Maxwell, 2005) to
avoid decoding the entire development set at each
function evaluation. This approach discards the
current model whenever performance on the trans-
lated subset of the development data is deemed sig-
nificantly worse in comparison to the current best
model. Second, we store and re-use search graphs
across function evaluations, which eliminates some
of the redundancy of regenerating the same transla-
tions in different optimization steps.
Our experiments with a strong phrase-based trans-
lation system show that the direct search approach is
an effective alternative to MERT. The speed of direct
search is generally comparable to MERT, and trans-
lation accuracy is generally superior. The non-linear
and hidden-state features tuned in this work bring
gains on three language pairs, with improvements
ranging between 0.27 and 0.35 BLEU points.
2 Direct error rate minimization
Most current machine translation systems use a log-
linear model:
p(e|f) ? exp
(
?
i
?ihi(e, f)
)
where f is a source sentence, e is a target sentence,
hi is a feature function, and ?i is the weight of this
feature. Given a source sentence f , finding the best
target sentence e? according to the model is a search
problem, which is called decoding:
e? = argmax
e
exp
(
?
i
?ihi(e, f)
)
The target sentence e? is automatically evaluated
against a reference translation r using any metric
that is known to be relatively well correlated with
human judgment, such as BLEU or TER. Let us re-
fer to such error function as E
(
?
)
. Then, the process
of finding the best set of weights ?? according to an
error function E is another search:
?? = argmin
?
E
(
r; argmax
e
exp
(
?
i
?ihi(e, f)
)
)
The typical MERT process solves the problem in an
iterative fashion. At each step i, it produces N -best
lists by decoding with ??i, then uses these lists to
find ??i+1. Och (2003) presents an efficient multi-
directional line search algorithm, which is based on
the fact that the error count along each line is piece-
wise constant and thus easy to optimize exactly. The
process is repeated until a certain convergence crite-
rion is met, or until no new candidate sentences are
added to the pool. The left side of Figure 1 summa-
rizes this process.
Though simple and effective, there are several lim-
itations to this approach. The primary reason is that
it can only tune parameters that are part of the log-
linear model. Aside from having parameters from
the log-linear model, decoders generally have free
parameters ? that needs to be set manually, such
as beam size and distortion limit. These decoder-
related parameters have complex interactions with
linear model parameters, thus, ideally, we would
want to tune them jointly with decoder parameters
such as distortion limit.
Direct search addresses these problems by includ-
ing all feature parameters and all decoder-related pa-
rameters within the optimization framework. Fig-
ure 1 contrasts MERT with direct search. Rather
than optimizing candidate pools of translations, di-
rect search treats the decoder and the evaluation tool
469
decoder
candidate
pool
??
BLEU
optimization
input f
other params ?
model params ?
output e
repeat
decoder
??, ??
1-best BLEU
optimization
input f
model params ?
other params ?
Figure 1: Comparison of MERT (left) and direct search (right).
as a single function:
?(f, r;?,?) = E
(
r; argmax
e
exp
(
?
i
?ihi(e, f)
)
)
Then, it uses an optimization method to minimize
the function:
argmin
?,?
?(f, r;?,?)
This formulation solves the problem mentioned pre-
viously, since we jointly optimize ? and ?, thus
accounting for the dependencies between the two.
However, there are two problems to address with di-
rect error minimization. First, this approach requires
the entire development set to be re-decoded every
time the function is evaluated, which can be pro-
hibitively expensive. To address this problem, we
present several methods to speed up the search pro-
cess in Section 5. Second, since the gradient of stan-
dard evaluation metrics such as BLEU is not known
and since methods for estimating the gradient numer-
ically require too many function evaluations, we can-
not use common search methods that use derivatives
of a function. Therefore, we need robust derivative-
free optimization methods. We discuss such opti-
mization methods in Section 3.
3 Derivative-free optimization
As discussed in the previous sections, we need to
rely on derivative-free optimization methods for di-
rect search. We consider two such optimization
methods:
Powell?s method For each iteration, Powell?s
method tries to find a good direction along which the
function can be minimized. This direction is deter-
mined by searching along each standard base vector.
Then, a line search is performed along the direction
by using line search methods such as golden section
search or Fibonacci search. The process is repeated
until convergence. We implement the golden sec-
tion search as presented by Press et al (1992) in our
experiments. Although the golden section search is
only exact when the function is unimodal, we found
that it works quite well in practice. More details are
presented by Powell (1964).
Nelder-Mead method This approach sets up a
simplex on the search space, which is a polytope
with D + 1 vertices when there are D dimensions,
and successively moves the simplex to a lower point
to find a minimum of the function. The simplex is
moved using different actions, which are taken when
certain conditions are met. The basic idea behind
these actions is to replace the worst point in the sim-
plex with a new and better point, thereby moving the
simplex towards a minimum. This method has the
advantage of being able to deal with ?bumpy? func-
tions and depending on the configuration of the sim-
plex at the time, it is possible to escape some local
minima. This is often refer to as downhill simplex
method and more details are presented by Nelder
and Mead (1965).
4 Parameters
In this section, we discuss the parameters that we
optimize with direct search, in addition to standard
470
linear model parameters:
4.1 Distortion limit
Distortion limit is one of decoder parameters that
sets a limit on the number of words the decoder
is allowed to skip when deciding which source
phrase to translate in order to allow reordering. Fig-
ure 2 shows a translation example from English to
Japanese. Every word jumped over incurs a dis-
tortion cost, which is usually one of the transla-
tion model parameters, which thereby discourages
reordering of words unless language model supports
the reordering.
Since having a large distortion limit leads to
slower decoding, having the smallest possible dis-
tortion limit that still facilitates correct reordering
would be ideal. Not only this speeds up translation,
but this also leads to better translation quality by
minimizing search errors. Since a larger distortion
limit means there are more possible re-orderings of
translations, it is prone to more search errors. In fact,
there are evidences that tuning the distortion limit
is beneficial in improving quality of translation by
limiting search errors. Galley and Manning (2008)
conduct a line search along increments of distor-
tion limit and separately tune the translation model
parameters for each increment of distortion limit.
The result shows significant difference in translation
quality when distortion limit is tuned along with the
model parameters. Separately tuning model param-
eters for different distortion limit is necessary be-
cause model parameters are coupled with distortion
limit. A representative example: when distortion
limit is zero, the distortion penalty feature can have
any weight and not affect BLEU scores, but this is
not the case when distortion limit is larger than zero.
Tuning distortion limit in direct search in conjunc-
tion with related features such linear distortion elim-
inates the need for a line search for distortion limit.
4.2 Polynomial features
Most phrase-based decoders typically use a dis-
tortion penalty feature to discourage (or maybe
sometimes encourage) reordering. Whereas distor-
tion limit is a hard constraint?since the decoder
never considers jumps larger than the given limit?
distortion penalty is a soft constraint, since it penal-
izes reordering proportionally to the length of the
I did not see the book you borrowed
?? ???? ??? ?? ????
+5
-3
-3
Figure 2: Reordering in phrase-based translation. A min-
imum distortion limit of five is needed to correctly trans-
late this example. The source sentence is relatively sim-
ple but a relatively large distortion limit is needed to ac-
commodate the correct reordering due to typological dif-
ference between two languages.
jump. The total distortion penalty is calculated as
follows:
D(e, f) = ?d
?
j
|dj |pd
where ?d is the weight for distortion penalty feature,
and dj is the size of the jump needed to translate
the j-th phrase pair. For example, in Figure 2, the
total distortion penalty feature value is 11, which is
multiplied with ?d to get the total distortion cost of
translating the example sentence. Although pd is typ-
ically set to one (linear), one may consider polyno-
mial distortion penalty (Green et al, 2010). Green et
al. (2010) show that setting pd to a higher value than
one improves the translation quality, but uses a pre-
determined value for pd. Instead of manually setting
the value of pd, it can be given a value tuned with di-
rect search. Although we only discussed distortion
penalty here, it is straightforward to tune pi for each
feature hi(e, f)pi using direct error rate minimiza-
tion, where hi(e, f) is any linear model feature of
the decoder.
4.3 Future cost estimates
Since beam search involves pruning, it is crucial to
have good future cost estimation in order to min-
imize the number of search errors (Koehn et al,
2003). The concept of future cost estimation is re-
lated to heuristic functions in the A* search algo-
rithm. The total cost f(x) of a partial translation
hypothesis is estimated by combining g(x), which is
the actual current cost from the beginning of a sen-
tence to point x and h(x), which is the future cost
471
estimate from point x to the end of the sentence:
f(x) = g(x) + h(x)
In SMT decoding, the same feature weight vec-
tor is generally used when computing g(x) and h(x).
However, this may not be ideal since future cost esti-
mators use different heuristics depending on the fea-
tures. For example, the future cost estimator (Green
et al, 2010) for linear distortion always underesti-
mates completion cost, which is generally deemed
a good property. Unfortunately, some features have
estimators that tend to overestimate completion cost,
as it is the case with the language model. This prob-
lem is illustrated in Figure 3. The Figure shows
that the ratio between the estimated total cost and
the actual total cost converges to 1.0. However,
in earlier stages of translations, the estimated fu-
ture cost for language model is larger than it should
be, which leads to higher total estimated cost. In
the A* search parlance, we are using an inadmissi-
ble heuristic since the future cost is overestimated,
which leads to suboptimal search. This suggests that
separately tuning parameters that are involved in the
future cost estimation will lead to better pruning de-
cisions. This essentially doubles the number of lin-
ear model parameters, since for every feature used in
future cost estimation, we create a counterpart and
tune its weight independently.
4.4 Search parameters
In addition to the parameters listed above, we also
tune general decoder parameters that affect the
search quality: beam size and parameters controlling
histogram pruning and threshold pruning. While it
makes sense to set these parameters automatically
instead of manually, the methods we have presented
thus far are not particularly fit for this type of pa-
rameters. Indeed, if the sole goal is to maximize
translation quality (e.g., as measured by standard
BLEU), a larger beam size and less pruning is usu-
ally preferable. To address this problem, we opti-
mize these three parameters using a slightly different
objective function. When tuning any of these three
features, the goal of translation is to get the most ac-
curate translation given a pre-defined time limit, so
we change the objective to be a time-sensitive objec-
tive function. Much akin to brevity penalty in BLEU,
0 0.2 0.4 0.6 0.8 1
0
1
2
3
4
5
Figure 3: y axis is ratio between estimated total cost vs.
actual total cost of language model for thousands of trans-
lations. 1.0 means the estimated total cost and the actual
total cost are exactly the same, and anything higher than
1.0 means the future cost has been overestimated thereby
inflating the estimated total cost. The x-axis represents
how much translation has been completed. 0.1 means
10% of a sentence has been translated.
we define time penalty as:
TP
(
?
)
=
{
1.0 ti ? td
exp
(
1 ? titd
)
ti > td
where TP
(
?
)
is a time penalty that is multiplied
to BLEU, ti is the time it takes to translate devel-
opment set under current parameters, and td is the
desired time limit for translating the development
set. With this error metric, we still optimize for
the translation quality as long as the translation hap-
pens within desired time td. With the modified time-
sensitive BLEU score as error metric, direct search
may tune the parameters that have the speed and ac-
curacy trade-off that we want.2
5 Speeding up direct search
Optimizing the true error surface is generally more
computationally expensive than with any surrogate
error surface, since each function evaluation usually
requires decoding or re-decoding the entire devel-
opment set. Since SMT tuning sets used for error
2A disadvantage of using time in the definition of TP
`
?
?
is that it adds non-determinism that can make optimization un-
stable. Our solution is to replace time with pseudo-time, a de-
terministic substitute expressed as a linear combination of the
number of n-gram lookups and hypothesis expansions (these
two quantities correlate quite well with decoding time).
472
rate minimization often comprise one thousand sen-
tences or more, each function evaluation can take
minutes or more. However, this problem is some-
what mitigated by the fact that translating in batches
is highly parallelizable. Since MERT (Och, 2003) is
also easily parallelizable, we need to resort to other
speedup techniques to make direct search a practi-
cal alternative to MERT. We now present two tech-
niques that make optimization of the true error sur-
face more efficient.
5.1 A racing algorithm for speeding up SMT
model selection
Error rate minimization as presented in this paper
can be seen as a form of model selection, which
has been the focus of a lot of work in the learn-
ing literature. The most popular approaches to
model selection?such as minimizing cross valida-
tion error?tend to be very slow in practice; there-
fore, researchers have addressed the problem of ac-
celerating model selection using statistical tests.
Prior to considering the SMT case, we review one
of these methods in the case of leave-one-out cross
validation (LOOCV). Racing for model selection
(Maron and Moore, 1994; Moore and Lee, 1994)
works as follows: we are given a collection of Nm
models and Nd data points, and we must find the
model that minimizes the mean e?j = 1Nd
?
i ej(i),
where ej(i) is the classification error of model Mj
on the ith datapoint when trained on all datapoints
except the ith point. The models are evaluated con-
currently, and at any given step k ? [1, Nd], each
model Mj is associated with two pieces of informa-
tion: the current estimate of its mean error rate, and
the estimate of its variance. As evaluations progress,
we eliminate any model that is significantly worse
than any other model.3 We also note that the Rac-
ing technique first randomizes the order of the data
points to ensure that prefixes of the dataset are gen-
3The details of these statistical tests are not so important
here since we use different ones in the case of SMT, but we
briefly summarize them as follows: Maron and Moore (1994)
use a non-parametric method (Hoeffding bounds (Hoeffding,
1963)) for confidence estimation, and places confidence inter-
vals on the mean value of the random variable representing
ej(i). A model is discarded if its confidence interval no longer
overlaps with the confidence interval of the current best model.
Moore and Lee (1994) use a similar technique, but relies on
Bayesian statistics instead of Hoeffding bounds.
erally representative of the entire set.
In this work, we use Racing to speed up direct
search for SMT, but this requires two main adjust-
ments compared to the LOOCV case. First, our
models have real-valued parameters, so we cannot
exhaustively evaluate the set of all models since it is
infinite. Instead, we use direct search to select which
models compete against each other during Racing.
In the case of Powell?s method, all points of a grid
along the current search direction are evaluated in
parallel using Racing, before we turn to the next
line search. In the case of the downhill simplex op-
timizer and in the case of line searches other than
grid search (e.g., golden section search), the use of
Racing is more difficult because the function eval-
uations requested by these optimizers have depen-
dencies that generally prevent concurrent function
evaluations. Since functions in downhill simplex are
evaluated in sequence and not in parallel, our solu-
tion is to race the current model against our current
best model.4 When the evaluation of a model M is
interrupted because it is deemed significantly worse
than the current best model M? , the error rate of M
on the entire development set is extrapolated from
its relative performance on the decoded subset.5
The second main difference with the LOOCV
case is that we do not use confidence intervals to de-
termine which of two or models are best. In SMT, it
is common to use either bootstrap resampling (Efron
and Tibshirani, 1993; Och, 2003) or randomization
tests (Noreen, 1989). In this paper, we use the ran-
domization test for discarding unpromising models,
since this statistical test was shown to be less likely
to cause type-I errors6 than bootstrap methods (Rie-
zler and Maxwell, 2005). Since both kinds of statisti-
cal tests involve a time-consuming sampling step, it
4Since Racing only discards suboptimal models, the current
best model M? is one for which we have decoded the entire de-
velopment set. Once a new model M is evaluated, we perform
at step j a significance test to determine whether M ?s transla-
tion of sentences 1 . . . j is better or worse than M? translation
for the same range of sentences. If M is significantly worse, we
discard it. If M? is worse, we continue evaluating the perfor-
mance of M , since we need M ?s output for the full development
set if M eventually becomes the new best model.
5For example, if error rates of M? and M are respectively
10% and 11% on the subset decoded by both models and M? ?s
error on the entire set is 20%, M ?s extrapolated error is 22%.
6A type I error rejects a null hypothesis that is true.
473
is somewhat wasteful to perform a new test after the
decoding of each sentence, so we translate sentences
in small batches of K sentences before performing
each randomization test.7
We finally note that Racing no longer guarantees
that the error function observed by the optimizer is
the true error function. Racing causes some approx-
imations of the error function, but the degree of ap-
proximation is designed to be small in regions with
low error rates, and Racing ensures that the most
promising function evaluations in our progression to-
wards an optimum are unaffected. In contrast, the
approximation of the error function computed from
N -best lists or lattice does not share this property.8
To further speed up function evaluations in direct
search, we employ a method meant to deal with mod-
els that are nearly identical, a situation in which Rac-
ing usually does not help much. Indeed, when two
models produce very similar outputs, we often need
to run the race through every sentence of the devel-
opment set since none of the two models end up be-
ing significantly better. A solution to this problem
consists of discarding models that are nearly identi-
cal to other models, where similarity between mod-
els is solely measured from their outputs.9 To do
this, we resort again to a randomization test: Given
two models Ma and Mb, this test performs random
permutations between outputs of Ma and Mb, that is,
it determines for each sentence of index i whether or
not to permute the two model outputs, with proba-
bility p = 0.5. When Ma and Mb are very similar,
these permutations have little effect, even when we
repeat this sampling process many times. To cope
with this problem, we slightly modify the random-
7In our experiments, we set K = 50. Some other practi-
cal considerations: the significance level used for discarding
unpromising models is p ? .05. The randomization test is a
sampling-based technique, for which we must specify a sample
size R. In this paper, we use R = 5000.
8In the case of N -best MERT, it is not even guaranteed that
we find the true error rate of our current best model M while
searching the N -best error surface. In fact, if we take the pa-
rameters of our best model M and re-decode the development
set, we may get an error rate that is different from what was pre-
dicted from the N -best list. With direct search and Racing, no
such approximation affects our current best model.
9Measuring model similarity only based on parameter val-
ues is less effective, since features and other parameters are
sometimes redundant, and two models may behave similarly
while having fairly distinct parameter values.
ization test to discard one of the two nearly iden-
tical models. Specifically, we compute the gap?
measured in error rate?between the best random-
ized output and the worst randomized output. If this
gap is lower than a pre-defined threshold, we only
keep the best model.10 This adjustment to the sig-
nificance test makes direct search reasonably fast,
since Racing is effective during the initial steps of
search (when steps tend to be relatively big, and
when differences in error rate are pretty significant),
and our modification to randomization tests helps
while search converges towards an optimum using
increasingly smaller steps.
5.2 Lattice-based decoding
We use another technique to speed up direct search
by storing and re-using search graphs, which con-
sist of lattices in the case of phrase-based decod-
ing (Och et al, 1999) and hypergraphs in the case
of hierarchical decoding (Chiang, 2005). The suc-
cessive expansion of translation options in order to
construct the search graph is generally done from
scratch, but this can be wasteful when the same
sentences are translated multiple times, as it is the
case with direct search. Even when the parame-
ters of the decoder change across function evalua-
tions, some partial translation are likely to be con-
structed multiple times, and this is more likely to
happen when changes in parameters are relatively
small. To overcome this inefficiency, we memoize
hypotheses expansions made in all function evalu-
ations, which then allows us to reuse some edges
(or hyperedges) from previous iterations to construct
the current graph (or hypergraph). Since feature
values?including expensive features like language
model score?are stored into each edge, the speedup
is roughly proportional to the percentage of edges
we can reuse.
A more radical way of exploiting search graphs
of previous iterations is to use them as constraints in
a forced decoding approach. In this framework, the
decoder takes as input not only an input sentence,
but also a constraining search graph. During decod-
ing, it is forced to discard any translation hypothe-
10In the case where we compare our current best model and
a model that is currently being evaluated, we discard the latter.
In our experiments with BLEU, we discard if the gap is smaller
than 0.1 BLEU point.
474
decoder
constrained
decoder
??, ??
1-best BLEU
optimization
input f
other params ?
model params ?
lattice ?
repeat
Figure 4: Lattice-constrained decoding for direct search.
ses that violate the constraining search graph. This
makes the memoization method presented in the pre-
vious paragraph maximally efficient, since lattice-
constrained decoding has all linear model feature
values already pre-computed. While this approach
is similar in spirit to lattice-based MERT (Macherey
et al, 2008), there is a crucial difference. The opti-
mization steps in lattice MERT bypass the decoder,
but the lattice-based approach presented here does
not. The distinction is important when it comes to
tuning non-linear and hidden state parameters of the
decoder. For instance, the initial lattice may have
been constructed with a distortion limit of 4, while
the current model specifies a distortion limit of 2.
At that stage, optimization via lattice-constrained de-
coding instead of lattice-based MERT ensures that
we will never select a path of the input lattice that
corresponds to a distortion limit of more than 2. This
is important since the error rate must reflect the fact
that jumps of two or more words are not allowed.
Figure 4 shows how direct search with lattice-
constrained decoding is structured. Similarly to
MERT and as opposed to straight direct search, opti-
mization is repeated multiple times. Since each opti-
mization in the lattice-constrained case does not re-
quire recomputing any features, it usually turns into
very significant gains in terms of translation speed,
though it also causes a small loss of translation ac-
curacy in general. The overall approach depicted in
Figure 4 works as follows: a first set of lattices is
generated using an initial ?0 and ?0. We then run
direct search with a decoder constrained on this set
of lattices. After optimization has converged, the op-
Train MERT dev. Test
Korean-English 7.9M 1000 6000
Arabic-English 11.1M 1000 6000
Farsi-English 739K 1000 2000
Table 1: Size of bitexts in number of sentence pairs.
timal ?? and ?? are provided as input ?1 and ?1 to
start a new iteration of this process. Note that the
constraining lattices built at each iteration are always
merged with those of the previous ones, so constrain-
ing lattices grow over time. The two stopping crite-
ria are similar to MERT: if the norm of the difference
between the previous parameter vector?including
? and ??and the current vector falls below a pre-
defined tolerance value, we do not continue to the
next iteration. Alternatively, if a new pass of un-
constrained decoding generates lattices that are sub-
sumed by lattices constructed at previous iteration,
we stop and do not run the next optimization step.
6 Experiments
6.1 Setup
For our experiments, we use a phrase-based transla-
tion system similar to Moses (Koehn et al, 2007).
Our decoder uses many of the same features as
Moses, including four phrasal and lexicalized trans-
lation scores, phrase penalty, word penalty, a lan-
guage model score, linear distortion, and six lexical-
ized reordering scores. Unless specified otherwise,
the decoder?s stack size is 50, and the number of
translation options per input phrase is 25.
Table 1 summarizes the amount of training data
used to train translation systems from Korean, Ara-
bic, and Farsi into English. These data sets are
drawn from various sources, which include news,
web, and technical data, as well as United Nations
data in the case of Arabic. In order to get the sense
of how presented techniques generalize, we evalu-
ate our systems on a fairly broad domain. We use
development and test sets are a mix of news, web,
and technical data. All systems translate into En-
glish, for which we built a 5-gram language model
with cutoff counts 1, 1, 1, 2, 3 for unigrams to 5-
grams, using a corpus of roughly seven billion En-
glish words. This includes the target side of the par-
allel training data, plus a significant amount of data
gathered from the web.
475
# Minimizer Optimized parameters Arabic Korean Farsi
1 MERT with grid search lin, DL 29.12 (14.6) 23.30 (20.8) 32.16 (11.7)
2 Direct search (simplex) lin, DL 29.07 (1.2) 23.42 (4.4) 32.22 (1.3)
3 Direct search (Powell) lin, DL 29.20 (2.3) 23.39 (5.6) 32.28 (2.1)
4 Direct search (Powell) lin, extended, DL 29.39 (4.4) 23.61 (8.9) 32.51 (4.9)
5 Lattice-constrained (Powell) lin, extended, DL 29.27 (0.7) 23.43 (1.3) 32.42 (1.1)
6 Direct search (Powell) lin, extended, DL, search 29.31 (6.5) 23.46 (9.7) 32.62 (6.2)
Table 2: BLEU-4 scores (%) with one reference, translating into English; the numbers in parentheses are times in
hours to run parameter optimization end-to-end. ?Lin? refers to Moses linear model features; ?extended? refers to non-
linear and hidden state features (polynomial features, future cost); ?DL? refers to distortion limit; ?search? is the set of
parameters controlling search quality (parameters controlling beam size, histogram pruning, and threshold pruning).
Our baseline system is trained for each language
pair by running minimum error rate training (Och,
2003) on 1000 sentences. Each iteration of MERT
utilizes 19 random starting points, plus the points of
convergence at all previous iterations of MERT, and
a uniform weight vector. That is, the first iteration
of MERT uses 20 starting points, the second uses 21
points, etc. Since MERT is not able to directly opti-
mize search parameters such as distortion limit and
beam size, our baseline system uses grid search to
optimize them. To make this search more tractable,
we only perform the grid search for a single param-
eter: the distortion limit. For each language pair,
the grid search consists of repeating MERT for eight
distinct distortion limits ranging from 3 to 10. The
optimal distortion limits found for Korean, Arabic,
and Farsi, are 8, 5, and 6, respectively.11 To ensure
that the comparison with our approach is consistent,
this grid search is made on the MERT dev set itself.
The next subsection contrasts the different direct
search methods presented in this paper. Note that
all these experiments use the speedup techniques
based on statistical significance test presented in Sec-
tion 5. Indeed, we found that using these techniques
resulted in faster speeds without affecting the search
in any significant way. Models tuned with or without
significance tests often ended up identical.
6.2 Results
The main results are shown in Table 2, and are com-
puted using standard BLEU-4 (Papineni et al, 2002)
11We rerun MERT for each different distortion limit because
of the dependencies between this parameter and linear model
features, particularly linear distortion and lexicalized reordering
scores. A linear model that is effective with a distortion limit of
4 can be suboptimal for a limit of 8.
using one reference translation, and ignoring case.
Row 1 displays results of the MERT baseline, with
a distortion limit that was found optimal using a
grid search on the development set. Rows 2 and 3
show results of direct error rate minimization with
downhill simplex and Powell?s method, where di-
rect search optimizes both linear model parameters
and the distortion limit. We see here that the per-
formance of direct search is comparable and some-
times better than MERT, but the benefit of direct
search here is that it does not require an external grid
search to find an effective distortion limit (each di-
rect search is initialized with a distortion limit of 10).
Row 4 shows the performance of Powell?s method
using the extended parameter set (Section 4), which
includes model weights for future costs and polyno-
mial features. We lack space to present an exten-
sive analysis of the relative impact of the different
non-linear features and parameters discussed in this
paper, but we generally find that the following pa-
rameters work best: distortion limit, polynomial dis-
tortion penalty, and weight of future cost estimate of
the language model. The fact that Moses-style future
cost estimation for language models often overesti-
mates probably explains why the latter feature helps.
In the last row of Table 2, optimization is done
using the time-sensitive variant of BLEU presented
in Section 4.4, and the set of parameters tuned here
includes all the previous ones, in addition to beam
size, and the two parameters controlling histogram
and threshold pruning in beam search. Clearly, run-
ning direct search to directly optimize BLEU would
yield a very large beam size and would set pruning
parameters that are so permissive that they would al-
most completely disable pruning. The benefit of us-
ing the time-sensitive variant of BLEU is that direct
476
search is forced to find parameter weights that of-
fer a good balance between accuracy and speed. To
make our results in row 6 as comparable as possible
to row 4, we use the running time (on the develop-
ment set) of row 4 as a time constraint for the model
of row 6, which is to decode the entire development
set at least as fast. In other words, the system of
row 6 is optimized to be no slower than the system
of row 4, and is otherwise penalized due to the time
penalty. The effect of this is that translation speed at
tuning time is almost the same, and speed of systems
4 and 6 is roughly the same at test time. A com-
parison between rows 4 and 6 suggests that tuning
search parameters such as beam size and without af-
fecting time does not provide much gain in terms of
translation quality, but the method nevertheless has
one advantage: one can target a specific translation
speed without having to manually tune any param-
eter such as beam size, and without even having to
decide which parameter to manually tune.
Times to run optimizations end-to-end are re-
ported in parentheses in Table 2 and they take into
account the time to run the grid search in the case
of MERT. Times to decode test sets are not reported
here since they are roughly the same across all mod-
els. While translation accuracy with MERT and di-
rect search is roughly the same when the underly-
ing parameter set is the same, direct search wins in
running time when it comes to optimizing search pa-
rameters like distortion limit. Since each grid search
runs MERT eight times, MERT is generally faster
than direct search, but the difference of speed re-
mains reasonable if the number of tuned parameters
is the same, and direct search is rarely twice as slow.
We finally discuss the case of lattice-constrained
decoding, which is shown in row 5 of Table 2. This
method is not applicable when tuning parameters
that affect search thoroughness (row 6), such as
beam size. The reason is that lattice-constrained
decoding is a form of forced decoding that con-
siderably narrows the search space. Under a con-
strained decoding setting, it appears that a large
beam size seldom affects translation speed, but this
is misleading and largely due to constraints cre-
ated by the lattice. We thus evaluate the lattice-
constrained case without tuning ?search? features,
and find that direct search is significantly faster us-
ing lattice-constrained, with only a slight degrada-
tion of translation quality. Lattice constraints are
augmented 2-5 times before it converges.
7 Related work
The use of derivative-free optimization methods to
tune machine translation parameters has been tried
before. Bender et al (2004) used the Nelder-Mead
method to tune model parameters for a phrase-based
translation system. However, their way of making
direct search fast and practical is to set distortion
limit to zero, which results in poor translation qual-
ity for many language pairs. Zens et al (2007) also
use the Nelder-Mead method to tune parameters in a
log-linear model to maximize expected BLEU. Zhao
and Chen (2009) proposes changes to Nelder-Mead
method to better fit parameter tuning in their ma-
chine translation setting. They show the modifica-
tion brings better search of parameters over the regu-
lar Nelder-Mead method. Our work is related to the
search-based structured prediction (SEARN) model
of Daume? (2006), in the sense that direct search also
accounts for what happens during search (including
search errors) to try to find parameters that are not
only good for prediction, but for search as well.
8 Conclusion
This paper addressed the problem of minimizing er-
ror rate at a corpus level. We show that a technique
to directly minimize the true error rate, rather than
one estimated from a surrogate representation such
as an N -best list, is in fact feasible. We present two
techniques that make this minimization significantly
faster, to the point where this technique is a viable
alternative to MERT. In the case where free param-
eters of the decoder (such as distortion limit) also
need to be optimized, our technique is in fact much
faster. We also optimize non-linear and hidden state
features that cannot be tuned using MERT, which
yield improvements in translation accuracy. Experi-
ments on large test sets yield gains on three language
pairs, and our best configuration outperforms MERT
by 0.27 to 0.35 BLEU points using a baseline system
trained on large amounts of data.
Acknowledgments
We thank anonymous reviewers, Chris Quirk, Kristina
Toutanova, and Anthony Aue for valuable suggestions.
477
References
Oliver Bender, Richard Zens, Evgeny Matusov, and Her-
mann Ney. 2004. Alignment templates: the RWTH
SMT system. In Proc. of the International Workshop
on Spoken Language Translation, pages 79?84, Kyoto,
Japan.
Yin-Wen Chang and Michael Collins. 2011. Exact de-
coding of phrase-based translation models through La-
grangian relaxation. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 26?37, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Conference of the Association for
Computational Linguistics (ACL-05), pages 263?270,
Ann Arbor, MI.
Hal Daume?, III. 2006. Practical Structured Learning
Techniques for Natural Language Processing. Ph.D.
thesis, University of Southern California, Los Angeles,
CA, USA.
B. Efron and R. J. Tibshirani. 1993. An Introduction to
the Bootstrap. Chapman & Hall, New York.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 848?856, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
Michel Galley and Chris Quirk. 2011. Optimal search
for minimum error rate training. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, pages 38?49, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
Spence Green, Michel Galley, and Christopher D. Man-
ning. 2010. Improved models of distortion cost for
statistical machine translation. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 867?875, Los Angeles,
California, June. Association for Computational Lin-
guistics.
Wassily Hoeffding. 1963. Probability inequalities for
sums of bounded random variables. Journal of the
American Statistical Association, 58(301):13?30.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Meeting of the North American chap-
ter of the Association for Computational Linguistics
(NAACL-03), Edmonton, Alberta.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
ACL, Demonstration Session, pages 177?180.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum Bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 163?171, Sun-
tec, Singapore, August. Association for Computational
Linguistics.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum error
rate training for statistical machine translation. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 725?734,
Honolulu, Hawaii, October. Association for Computa-
tional Linguistics.
Oded Maron and Andrew W. Moore. 1994. Hoeffding
races: Accelerating model selection search for classi-
fication and function approximation. In Advances in
neural information processing systems 6, pages 59?66.
Morgan Kaufmann.
Andrew Moore and Mary Soon Lee. 1994. Efficient
algorithms for minimizing cross validation error. In
W. W. Cohen and H. Hirsh, editors, Proceedings of the
11th International Confonference on Machine Learn-
ing, pages 190?198. Morgan Kaufmann.
Robert C. Moore and Chris Quirk. 2008. Random
restarts in minimum error rate training for statistical
machine translation. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 585?592, Manchester, UK, Au-
gust. Coling 2008 Organizing Committee.
J. A. Nelder and R. Mead. 1965. A simplex method
for function minimization. Computer Journal, 7:308?
313.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses : An Introduction. Wiley-
Interscience.
Franz Josef Och, Christoph Tillmann, Hermann Ney, and
Lehrstuhl Fiir Informatik. 1999. Improved alignment
models for statistical machine translation. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing and Very Large Corpora,
pages 20?28.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Conference of the Association for Compu-
tational Linguistics (ACL-03).
478
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Conference of the Association for Compu-
tational Linguistics (ACL-02).
M. J. D. Powell. 1964. An efficient method for finding
the minimum of a function of several variables without
calculating derivatives. The Computer Journal, 7:155?
162.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 1992. Numerical recipes
in C (2nd ed.): the art of scientific computing. Cam-
bridge University Press, New York, NY, USA.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 57?64,
June.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In Pro-
ceedings of Association for Machine Translation in the
Americas, pages 223?231.
M H Wright. 1995. Direct search methods: Once
scorned, now respectable. Numerical Analysis,
344:191?208.
Richard Zens, Sasa Hasan, and Hermann Ney. 2007. A
systematic comparison of training criteria for statisti-
cal machine translation. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 524?532.
Bing Zhao and Shengyuan Chen. 2009. A simplex
Armijo downhill algorithm for optimizing statistical
machine translation decoding parameters. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics, Compan-
ion Volume: Short Papers, pages 21?24, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
479
