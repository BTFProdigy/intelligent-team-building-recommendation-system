Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 66?70,
Baltimore, Maryland USA, June 26 2014.
c
?2014 Association for Computational Linguistics
Intermediary Semantic Representation
through Proposition Structures
Gabriel Stanovsky
?
, Jessica Ficler
?
, Ido Dagan, Yoav Goldberg
Computer Science Department, Bar-Ilan University
?Both authors equally contributed to this paper
{gabriel.satanovsky,jessica.ficler,yoav.goldberg}@gmail.com
dagan@cs.biu.ac.il
Abstract
We propose an intermediary-level seman-
tic representation, providing a higher level
of abstraction than syntactic parse trees,
while not committing to decisions in cases
such as quantification, grounding or verb-
specific roles assignments. The proposal
is centered around the proposition struc-
ture of the text, and includes also im-
plicit propositions which can be inferred
from the syntax but are not transparent in
parse trees, such as copular relations intro-
duced by appositive constructions. Other
benefits over dependency-trees are ex-
plicit marking of logical relations between
propositions, explicit marking of multi-
word predicate such as light-verbs, and a
consistent representation for syntactically-
different but semantically-similar struc-
tures. The representation is meant to
serve as a useful input layer for semantic-
oriented applications, as well as to provide
a better starting point for further levels of
semantic analysis such as semantic-role-
labeling and semantic-parsing.
1 Introduction
Parsers for semantic formalisms (such as Neo-
davidsonian (Artzi and Zettlemoyer, 2013) and
DRT (Kamp, 1988)) take unstructured natural lan-
guage text as input, and output a complete seman-
tic representation, aiming to capture the mean-
ing conveyed by the text. We suggest that this
task may be effectively separated into a sequential
combination of two different tasks. The first of
these tasks is syntactic abstraction over phenom-
ena such as expression of tense, negation, modal-
ity, and passive versus active voice, which are all
either expressed or implied from syntactic struc-
ture. The second task is semantic interpretation
over the syntactic abstraction, deriving quantifi-
cation, grounding, etc. Current semantic parsers
(such as Boxer (Bos, 2008)) tackle these tasks si-
multaneously, mixing syntactic and semantic is-
sues in a single framework. We believe that sepa-
rating semantic parsing into two well defined tasks
will help to better target and identify challenges
in syntactic and semantic domains. Challenges
which are often hidden due to the one-step archi-
tecture of current parsers.
Many of today?s semantic parsers, and semantic
applications in general, leverage dependency pars-
ing (De Marneffe and Manning, 2008a) as an ab-
straction layer, since it directly represents syntac-
tic dependency relations between predicates and
arguments. Some systems exploit Semantic Role
Labeling (SRL) (Carreras and M?arquez, 2005),
where predicate-argument relationships are cap-
tured at a thematic (rather than syntactic) level,
though current SRL technology is less robust and
accurate for open domains than syntactic pars-
ing. While dependency structures and semantic
roles capture much of the proposition structure of
sentences, there are substantial aspects which are
not covered by these representations and therefore
need to be handled by semantic applications on
their own (or they end up being ignored).
Such aspects, as detailed in Section 3, include
propositions which are not expressed directly as
such but are rather implied by syntactic struc-
ture, like nominalizations, appositions and pre-
modifying adjectives. Further, the same proposi-
tion structure may be expressed in many differ-
ent ways by the syntactic structure, forcing sys-
tems to recognize this variability and making the
task of recognizing semantic roles harder. Other
aspects not addressed by common representations
include explicit marking of links between propo-
sitions within a sentence, which affect their asser-
tion or truth status, and the recognition of multi-
word predicates (e.g., considering ?take a deci-
66
Figure 1: Proposed representation for the sentence: ?If you
leave the park, you will find the Peak Tram terminal?
sion? as a single predicate, rather than considering
decision as an argument).
In this position paper we propose an intermedi-
ary representation level for the first syntactic ab-
straction phase described above, intended to re-
place syntactic parsing as a more abstract repre-
sentation layer. It is designed to capture the full
proposition structure which is expressed, either
explicitly or implicitly, by the syntactic structure
of sentences. Thus, we aim to both extract im-
plicit propositions as well as to abstract away syn-
tactic variations which yield the same proposition
structure. At the same time, we aim to remain at
a representation level that corresponds to syntac-
tic properties and relationships, while avoiding se-
mantic interpretations, to be targeted by systems
implementing the further step of semantic inter-
pretation, as discussed above.
In addition, we suggest our representation as a
useful input for semantic applications which need
to recognize the proposition structure of sentences
in order to identify targeted information, such as
Question Answering(QA), Information Extraction
(IE) and multidocument summarization. We ex-
pect that our representation may be more useful
in comparison with current popular use of depen-
dency parsing, in such applications.
2 Representation Scheme
Our representation is centered around proposi-
tions, where a proposition is a statement for which
a truth-value can be assigned. We propose to rep-
resent sentences as a set of inter-linked proposi-
tions. Each proposition is composed of one pred-
icate and a set of arguments. An example rep-
resentation can be seen in Figure 1. Predicates
are usually centered around verbs, and we con-
sider multi-word verbs (e.g., ?take apart?) as sin-
gle predicates. Both the predicates and arguments
are represented as sets of feature-value pairs. Each
argument is marked with a relation to its predicate,
and the same argument can appear in different
propositions. The relation-set we use is syntactic
in nature, including relations such as Subject,
Object, and Preposition with, in contrast
to semantic relations such as instrument.
Canonical Representation The same proposi-
tion can be realized syntactically in many forms.
An important goal of our proposal is abstracting
over idiosyncrasies in the syntactic structure and
presenting unified structures when possible. We
canonicalize on two levels:
? We canonicalize each predicate and argument
by representing each predicate as its main
lemma, and indicating other aspects of the
predication (e.g., tense, negation and time) as
features; Similarly, we mark arguments with
features such as definiteness and plurality.
? We canonicalize the argument structure by
abstracting away over word order and phe-
nomena such as topicalization and pas-
sive/active voice, and present a unified rep-
resentation in terms of the argument roles (so
that, for example, in the sentence ?the door
was opened? the argument ?door? will re-
ceive the object role, with the passive be-
ing indicated as a feature of the predicate).
Relations Between Propositions Some propo-
sitions must be interpreted taking into account
their relations to other propositions. These in-
clude conditionals (?if congress does nothing,
President Bush will have won? (wsj 0112));
temporal relations (?UAL?s announcement came
after the market closed yesterday?(wsj 0112));
and conjunctions (?They operate ships and
banks.?(wsj 0083)).
We model such relations as typed links between
extracted propositions. Figure 1 presents an exam-
ple of handling a conditional relation: the depen-
dence between the propositions is made explicit by
the Cond(if) relation.
3 Implicit Propositions
Crucially, our proposal aims to capture not only
explicit but also implicit propositions ? proposi-
tions that can be inferred from the syntactic struc-
67
ture but which are not explicitly marked in syn-
tactic dependency trees, as we elaborate below.
Some of these phenomena are relatively easy to
address by post-processing over syntactic parsers,
and could thus be included in a first implemen-
tation that produces our proposed representations.
Other phenomena are more subtle and would re-
quire further research, yet they seem important
while not being addressed by current techniques.
The syntactic structures giving rise to implicit
propositions include:
Copular sentences such as ?This is not a triv-
ial issue.? (wsj 0108) introduces a proposition by
linking between a non-verbal predicate and its ar-
gument. We represent this by making ?not a triv-
ial issue? a predicate, and ?this? an argument of
type Predication.
Appositions, we distinguish between co-reference
and predicative appositions. In Co-reference in-
dication appositions (?The company, Random
House, doesn?t report its earnings.? (adaption of
wsj 0111)) we produce a proposition to indicate
the co-reference between two lexical items. Other
propositions relating to the entity use the main
clause as the referent for this entity. In this ex-
ample, we will produce:
1. Random House == the company.
2. The company doesn?t report its earnings.
In Predicative appositions (?Pierre Vinken, 61
years old, will join the board as a nonexecutive di-
rector Nov. 29.? (wsj 0001)) an apposition is used
in order to convey knowledge about an entity. In
our representation this will produce:
1. Pierre Vinken is 61 years old (which is canoni-
calized to the representation of copular sentences)
2. Pierre Vinken will join the board as a nonexec-
utive director Nov. 29.
Adjectives, as in the sentence ?you emphasized
the high prevalence of mental illness? (wsj 0105).
Here an adjective is used to describe a definite sub-
ject and introduces another proposition, namely
the high prevalence of mental illness.
Nominalizations, for instance in the sentence
?Googles acquisition of Waze occurred yester-
day?, introduce the implicit proposition that
?Google acquired Waze?. Such propositions were
studied and annotated in the NOMLEX (Macleod
et al., 1998) and NOMBANK (Meyers et al., 2004)
resources. It remains an open issue how to repre-
sent or distinguish cases in which nominalization
introduce an underspecified proposition. For ex-
ample, consider ?dancing? in ?I read a book about
dancing?.
Possessives, such as ?John?s book? introduce the
proposition that John has a book. Similarly, ex-
amples such as ?John?s Failure? combine a pos-
sessive construction with nominalization and in-
troduce the proposition that John has failed.
Conjunctions - for example in ?They operate
ships and banks.? (wsj 0083), introduce several
propositions in one sentence:
1. They operate ships
2. They operate banks
We mark that they co-refer to the same lexical unit
in the original sentence. Such cases are already
represented explicitly in the ?collapsed? version
of Stanford-dependencies (De Marneffe and Man-
ning, 2008a).
1
Implicit future tense indication, for instance
in ?I?m going to vote for it? (wsj 0098) and
?The economy is about to slip into recession.?
(wsj 0036), verbs like ?going to? and ?about to?
are used as future-tense markers of the proposi-
tion following them, rather than predicates on their
own. We represent these as a single predicate
(?vote?) in which the tense is marked as a fea-
ture.
2
Other phenomena, omitted for lack of space,
include propositional modifiers (e.g., relative
clause modifiers), propositional arguments (such
as ?John asserted that he will go home?), condi-
tionals, and the canonicalization of passive and
active voice.
4 Relation to Other Representations
Our proposed representation is intended to serve
as a bridging layer between purely syntactic rep-
resentations such as dependency trees, and seman-
tic oriented applications. In particular, we explic-
itly represent many semantic relations expressed
in a sentence that are not captured by contempo-
rary proposition-directed semantic representations
(Baker et al., 1998; Kingsbury and Palmer, 2003;
Meyers et al., 2004; Carreras and M`arquez, 2005).
Compared to dependency-based representations
such as Stanford-dependency trees (De Marneffe
1
A case of conjunctions requiring special treatment is in-
troduced by reciprocals, in which the entities roles are ex-
changeable. For example: ?John and Mary bet against each
other on future rates? (adaption of wsj 0117).
2
Care needs to be taken to distinguish from cases such as
?going to Italy? in which ?going to? is not followed by a
verbal predicate.
68
and Manning, 2008b), we abstract away over
many syntactic details (e.g., the myriad of ways
of expressing tense, negation and modality, or the
difference between passive and active) which are
not necessary for semantic interpretation and mark
them instead using a unified set of features and ar-
gument types. We make explicit many relations
that can be inferred from the syntax but which
are not directly encoded in dependency relations.
We directly connect predicates with all of their ar-
guments in e.g., conjunctions and embedded con-
structions, and we do not commit to a tree struc-
ture. We also explicitly mark predicate and argu-
ment boundaries, and explicitly mark multi-word
predicates such as light-verb constructions.
Compared to proposition-based semantic rep-
resentations, we do not attempt to assign frame-
specific thematic roles, nor do we attempt to dis-
ambiguate or interpret word meanings. We restrict
ourselves to representing predicates by their (lem-
matized) surface forms, and labeling arguments
based on a ?syntactic? role inventory, similar to the
label-sets available in dependency representations.
This design choice makes our representation much
easier to assign automatically to naturally occur-
ring text (perhaps pre-annotated using a syntactic
parser) than it is to assign semantic roles. At the
same time, as described in Section 3, we capture
many relations that are currently not annotated in
resources such as FrameNet, and provide a com-
prehensive set of propositions present in the sen-
tence (either explicitly or implicitly) as well as the
relations between them ? an objective which is not
trivial even when presented with full semantic rep-
resentation.
Compared to more fine-grained semantic repre-
sentations used in semantic-parsers (i.e. lambda-
calculus (Zettlemoyer and Collins, 2005), neo-
davidsonian semantics (Artzi and Zettlemoyer,
2013), DRT (Kamp, 1988) or the DCS represen-
tation of Liang (2011)), we do not attempt to
tackle quantification, nor to ground the arguments
and predicates to a concrete domain-model or on-
tology. These important tasks are orthogonal to
our representation, and we believe that semantic-
parsers can benefit from our proposal by using it
as input in addition to or instead of the raw sen-
tence text ? quantification, binding and grounding
are hard enough without needing to deal with the
subtleties of syntax or the identification of implicit
propositions.
5 Conclusion and Future Work
We proposed an intermediate semantic repre-
sentation through proposition extraction, which
captures both explicit and implicit propositions,
while staying relatively close to the syntactic
level. We believe that this kind of representation
will serve not only as an advantageous input for
semantically-centered applications, such as ques-
tion answering, summarization and information
extraction, but also serve as a rich representation
layer that can be used as input for systems aiming
to provide a finer level of semantic analysis, such
as semantic-parsers.
We are currently at the beginning of our in-
vestigation. In the near future we plan to semi-
automatically annotate the Penn Tree Bank (Mar-
cus et al., 1993) with these structures, as well as
to provide software for deriving (some of) the im-
plicit and explicit annotations from automatically
produced parse-trees. We believe such resources
will be of immediate use to semantic-oriented ap-
plications. In the longer term, we plan to inves-
tigate dedicated algorithms for automatically pro-
ducing such representation from raw text.
The architecture we describe can easily accom-
modate additional layers of abstraction, by en-
coding these layers as features of propositions,
predicates or arguments. Such layers can include
the marking of named entities, the truth status of
propositions and author commitment.
In the current version infinitive constructions
are treated as nested propositions, similar to their
representation in syntactic parse trees. Providing
a consistent, useful and transparent representation
for infinitive constructions is a challenging direc-
tion for future research.
Other extensions of the proposed representa-
tion are also possible. One appealing direction
is going beyond the sentence level and represent-
ing discourse level relations, including implied
propositions and predicate - argument relation-
ships expressed by discourse (Stern and Dagan,
2014; Ruppenhofer et al., 2010; Gerber and Chai,
2012). Such an extension may prove useful as an
intermediary representation for parsers of seman-
tic formalisms targeted at the discourse level (such
as DRT).
6 Acknowledgments
This work was partially supported by the Eu-
ropean Community?s Seventh Framework Pro-
69
gramme (FP7/2007-2013) under grant agreement
no. 287923 (EXCITEMENT).
References
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised
learning of semantic parsers for mapping instructions to
actions. Transactions of the Association for Computa-
tional Linguistics, 1(1):49?62.
Collin F Baker, Charles J Fillmore, and John B Lowe. 1998.
The berkeley framenet project. In Proceedings of ACL,
pages 86?90. Association for Computational Linguistics.
Johan Bos. 2008. Wide-coverage semantic analysis with
boxer. In Proceedings of the 2008 Conference on Seman-
tics in Text Processing, pages 277?286. Association for
Computational Linguistics.
Xavier Carreras and Llu??s M`arquez. 2005. Introduction to
the conll-2005 shared task: Semantic role labeling. In
Proceedings of CONLL, pages 152?164.
Marie-Catherine De Marneffe and Christopher D Manning.
2008a. Stanford typed dependencies manual. Technical
report, Stanford University.
Marie-Catherine De Marneffe and Christopher D Manning.
2008b. The stanford typed dependencies representation.
In Coling 2008: Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation, pages
1?8.
Matthew Gerber and Joyce Y Chai. 2012. Semantic role la-
beling of implicit arguments for nominal predicates. Com-
putational Linguistics, 38(4):755?798.
Hans Kamp. 1988. Discourse representation theory. In Nat-
ural Language at the computer, pages 84?111. Springer.
Paul Kingsbury and Martha Palmer. 2003. Propbank: the
next level of treebank. In Proceedings of Treebanks and
lexical Theories, volume 3.
P. Liang, M. I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In Proceed-
ings of ACL, pages 590?599.
Catherine Macleod, Ralph Grishman, Adam Meyers, Leslie
Barrett, and Ruth Reeves. 1998. Nomlex: A lexicon
of nominalizations. In Proceedings of EURALEX, vol-
ume 98, pages 187?193.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus of
english: The penn treebank. Computational linguistics,
19(2):313?330.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph Gr-
ishman. 2004. The nombank project: An interim report.
In HLT-NAACL 2004 workshop: Frontiers in corpus an-
notation, pages 24?31.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2010. Semeval-2010
task 10: Linking events and their participants in discourse.
In Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 45?50. Association for Compu-
tational Linguistics.
Asher Stern and Ido Dagan. 2014. Recognizing implied
predicate-argument relationships in textual inference. In
Proceedings of ACL. Association for Computational Lin-
guistics.
Luke S. Zettlemoyer and Michael Collins. 2005. Learning
to map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In UAI, pages
658?666. AUAI Press.
70
Proceedings of the AHA! Workshop on Information Discovery in Text, pages 19?24,
Dublin, Ireland, August 23 2014.
Proposition Knowledge Graphs
Gabriel Stanovsky Omer Levy Ido Dagan
Computer Science Department, Bar-Ilan University
{gabriel.satanovsky, omerlevy}@gmail.com
dagan@cs.biu.ac.il
Abstract
Open Information Extraction (Open IE) is a promising approach for unrestricted Information
Discovery (ID). While Open IE is a highly scalable approach, allowing unsupervised relation
extraction from open domains, it currently has some limitations. First, it lacks the expressiveness
needed to properly represent and extract complex assertions that are abundant in text. Second, it
does not consolidate the extracted propositions, which causes simple queries above Open IE as-
sertions to return insufficient or redundant information. To address these limitations, we propose
in this position paper a novel representation for ID ? Propositional Knowledge Graphs (PKG).
PKGs extend the Open IE paradigm by representing semantic inter-proposition relations in a
traversable graph. We outline an approach for constructing PKGs from single and multiple texts,
and highlight a variety of high-level applications that may leverage PKGs as their underlying
information discovery and representation framework.
1 Introduction
Information discovery from text (ID) aims to provide a consolidated and explorable data representation of
an input document or a collection of documents addressing a common topic. Ideally, this representation
would separate the input into logically discrete units, omit redundancies in the original text, and provide
semantic relations between the basic units of the representation. This representation can then be used
by human readers as a convenient and succinct format, or by subsequent NLP tasks (such as question
answering and multidocument summarization) as a structured input representation.
A common approach to ID is to extract propositions conveyed in the text by applying either supervised
Information Extraction (IE) techniques (Cowie and Lehnert, 1996), to recover propositions covering a
predefined set of relations (Auer et al., 2007; Suchanek et al., 2008), or more recently, Open Information
Extraction (Open IE) (Etzioni et al., 2008), which discovers open-domain relations (Zhu et al., 2009;
Wu et al., 2008). In Open IE, natural language propositions are extracted from text, based on surface
or syntactic patterns, and are then represented as predicate-argument tuples, where each element is a
natural language string. While Open IE presents a promising direction for ID, thanks to its robustness
and scalability across domains, we argue that it currently lacks representation power in two major aspects:
representing complex propositions extracted from discourse, such as interdependent propositions or
implicitly conveyed propositions, and consolidating propositions extracted across multiple sources,
which leads to either insufficient or redundant information when exploring a set of Open IE extractions.
In this position paper we outline Propositional Knowledge Graphs (PKG), a representation which
addresses both of Open IE?s mentioned drawbacks. The graph?s nodes are discrete propositions extracted
from text, and edges are drawn where semantic relations between propositions exists. Such relations can
be inferred from a single discourse, or from multiple text fragments along with background knowledge ?
by applying methods such as textual entailment recognition (Dagan et al., 2013) ? which consolidates the
information within the graph. We discuss this representation as a useful input for semantic applications,
and describe work we have been doing towards implementing such a framework.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
19
Figure 1: An excerpt from a PKG, containing a few propositions extracted from news reports about Curiosity (the Mars rover)
and their relations. The dashed boundaries in the figure denote paraphrase cliques, meaning that all propositions within them
are mutually entailing. Some of these propositions are complex, and the bottom-right corner illustrates how one of them can be
represented by inter-connected sub-propositions.
2 Approach: Discover Inter-Proposition Relations
We propose a novel approach for textual information discovery and representation that enhances the
expressiveness of Open IE with structural power similar to traditional knowledge graphs. Our represen-
tation aims to extract all the information conveyed by text to a traversable graph format ? a Propositional
Knowledge Graph (PKG). The graph?s nodes are natural language propositions and its labeled edges are
semantic relations between these propositions. Figure 1 illustrates an excerpt of a PKG.
We separate the construction of such graphs into two phases, each of which addresses one of the afore-
mentioned limitations of current Open IE. The first phase (described in section 2.1) is the extraction of
complex propositions from a single discourse. This phase extends upon the definition of Open IE ex-
tractions to gain a more expressive paradigm and improve the recall of extracted propositions. In this
extension, a single assertion is represented by a set of interconnected propositions. An example can be
seen in the bottom right of Figure 1. The second phase (described in section 2.2) deals with the consolida-
tion of propositions extracted in the first phase. This is done by drawing relations such as entailment and
temporal succession between these propositions, which can be inferred utilizing background knowledge
applied on multiple text fragments.
2.1 Relations Implied by Discourse
Current Open IE representation schemes lack the expressibility to represent certain quite common propo-
sitions implied by syntax, hindering Open IE?s potential as an information discovery framework. We dis-
cuss several cases in which this limitation is evident, and describe possible solutions within our proposed
framework.
Embedded and Interrelated Propositions Common Open IE systems retrieve only propositions in
which both predicates and arguments are instantiated in succession in the surface form. For such propo-
sitions, these systems produce independent tuples (typically a (subject, verb, object) triplet) consisting of
a predicate and a list of its arguments, all expressed in natural language, in the same way they originally
appeared in the sentence. This methodology lacks the ability to represent cases in which propositions are
inherently embedded, such as conditionals and propositional arguments (e.g. ?Senator Kennedy asked
congress to pass the bill?). Mausam et al. (2012) introduced a context analysis layer, extending this
20
representation with an additional field per tuple, which intends to represent the factuality of the extrac-
tion, accounting specifically for cases of conditionals and attribution. For instance, the assertion ?If he
wins five key states, Romney will be elected President? will be represented as ((Romney; will be elected;
President) ClausalModifier if; he wins five key states).
While these methods capture some of the propositions conveyed by text, they fail to retrieve other
propositions expressed by more sophisticated syntactic constructs. Consider the sentence from Figure 1
?Curiosity will look for evidence that Mars might have had conditions for supporting life?. It exhibits a
construction which the independent tuples format seems to fall short from representing. Our proposed
representation for this sentence is depicted in the bottom right of Figure 1. We represent the complexity
of the sentence through a nested structure of interlinked propositions, each composed of a single pred-
icate and its syntactic arguments and modifiers. In addition, we model certain syntactic variabilities as
features, such as tense, negation, passive voice, etc. Thus, a single assertion is represented through the
discrete propositions it conveys, along with their inter-relations. In addition to the expressibility that this
representation offers, an immediate gain is the often recurring case in which a part of a proposition (for
example, one of the arguments) immediately implies another proposition. For instance, ?The Mars rover
Curiosity is a mobile science lab? implies that ?Curiosity is a rover?, and does so syntactically.
Implicit propositions Certain propositions which are conveyed by the text are not explicitly expressed
in the surface form. Consider, for instance, the sentence ?Facebook?s acquisition of WhatsApp occurred
yesterday?. It introduces the proposition (Facebook, acquired, WhatsApp) through nominalization. Cur-
rent Open IE formalisms are unable to extract such triplets, since the necessary predicate (namely ?ac-
quired?) does not appear in the surface form. Implicit propositions might be introduced in many other
linguistic constructs, such as: appositions (?The company, Random House, doesn?t report its earnings.?
implies that Random House is a company), adjectives (?Tall John walked home? implies that John is tall),
and possessives (?John?s book is on the table? implies that John has a book). We intend to syntactically
identify these implicit propositions, and make them explicit in our representation.
For further analysis of syntax-driven proposition representation, see our recent work (Stanovsky et al.,
2014). We believe that this extension of Open IE representation is feasibly extractable from syntactic
parse trees, and are currently working on automatic conversion from Stanford dependencies (de Marneffe
and Manning, 2008) to interconnected propositions as described.
2.2 Consolidating Information across Propositions
While Open IE is indeed much more scalable than supervised approaches, it does not consolidate natu-
ral language expressions, which leads to either insufficient or redundant information when accessing a
repository of Open IE extractions. As an illustrating example, querying the University of Washington?s
Open IE demo (openie.cs.washington.edu) for the generally equivalent relieves headache or
treats headache returns two different lists of entities; out of the top few results, the only answers these
queries seem to agree on are caffeine and sex. Desirably, an information discovery platform should re-
turn identical results (or at least very similar ones) to these queries. This is a major drawback relative
to supervised knowledge representations, such as Freebase (Bollacker et al., 2008), which map natural
language expressions to canonical formal representations (e.g. the treatments relation in Freebase).
While much relational information can be salvaged from the original text, many inter-propositional
relations stem from background knowledge and our understanding of language. Perhaps the most promi-
nent of these is the entailment relation, as demonstrated in Figure 1. We rely on the definition of textual
entailment as defined by Dagan et al. (2013): proposition T entails proposition H if humans reading T
would typically infer that H is most likely true. Entailment provides an effective structure for aggregat-
ing natural-language based information; it merges semantically equivalent propositions into cliques, and
induces specification-generalization edges between them (if T entails H , then H is more general).
Figure 1 demonstrates the usefulness of entailment in organizing the propositions within a PKG. For
example, the two statements describing Curiosity as a mobile science lab (middle right) originated from
two different texts. However, in a PKG, they are marked as paraphrases (mutually entailing), and both
entail an additional proposition from a third source: ?Curiosity is a lab?. If one were to query all the
21
propositions that entail ?Curiosity is a lab? ? e.g. in response to the query ?What is Curiosity?? ? all
three propositions would be retrieved, even though their surface forms may have ?functions as? instead
of ?is? or ?laboratory? instead of ?lab?.
We have recently taken some first steps in this direction, investigating algorithms for constructing
entailment edges over sets of related propositions (Levy et al., 2014). Even between simple propositions,
recognizing entailment is challenging. We are currently working on new methods that will leverage
structured and unstructured data to recognize entailment for Open IE propositions. There are additional
relations, besides entailment, that should desirably be represented in PKGs as well. Two such examples
are temporal relations (depicted in Figure 1) and causality. Investigating and adapting methods for
recognizing and utilizing these relations is intended for future work.
3 Applications
An appealing application of knowledge graphs is question answering (QA). In this section we demon-
strate how our representation may facilitate more sophisticated information access scenarios.
Structured Queries Queries over structured data give the user the power to receive targeted answers
for her queries. Consider for example the query ?electric cars on sale in Canada?. PKGs can give the
power of queries over structured data to the domain of unstructured information. To answer our query,
we can search the PKG for all of the propositions that entail these two propositions: (1) ?X is an electric
car?, (2) ?X is on sale in Canada?, where X is a variable. The list of X instantiations is the answer
to our structured query. Our knowledge structure enables even more sophisticated queries that involve
more than one variable. For example, ?Japanese corporations that bought Australian start-ups? retrieves
a collection of pairs (X,Y ) where X is the Japanese corporation that bought Y , an Australian start-up.
Summarization Multi-document summarization gives the user the ability to compactly assimilate in-
formation from multiple documents on the same topic. PKGs can be a natural platform leveraged by
summarization because: (1) they would contain the information from those documents as fine-grained
propositions (2) they represent the semantic relations between those propositions. These semantic re-
lations can be leveraged to create high-quality summarizations. For example, the paraphrase (mutual
entailment) relation prevents redundancy. Links of a temporal or causal nature can also dictate the order
in which each proposition is presented. A recent method of summarizing text with entailment graphs
(Gupta et al., 2014) demonstrates the appeal and feasibility of this application.
Faceted Search Faceted search allows a user to interactively navigate a PKG. Adler et al. (2012)
demonstrate this concept on a limited proposition graph. When searching for ?headache? in their demo,
the user can drill-down to find possible causes or remedies, and even focus on subcategories of those;
for example, finding the foods which relieve headaches. As opposed to the structured query application,
retrieval is not fully automated, but rather interactive. It thus allows users to explore and discover new
information they might not have considered a-priori.
4 Discussion
In this position paper we outlined a framework for information discovery that leverages and extends Open
IE, while addressing two of its current major drawbacks. The proposed framework enriches Open IE by
representing natural language in a traversable graph, composed of propositions and their semantic inter-
relations ? A Propositional Knowledge Graph (PKG). The resulting structure provides a representation
in two levels: locally, at sentence level, by representing the syntactic proposition structure embedded in a
single sentence, and globally, at inter-proposition level, where relations are drawn between propositions
from discourse, or from various sources.
At the sentence level, PKG can be compared to Abstract Meaning Representation (AMR) (Banarescu
et al., 2013), which maps a sentence onto a hierarchical structure of propositions (predicate-argument
relations) - a ?meaning representation?. AMR uses Propbank (Kingsbury and Palmer, 2003) for pred-
icates? meaning representation, where possible, and ungrounded natural language, where no respective
22
Propbank lexicon entry exists. While AMR relies on a deep semantic interpretation, our sentence level
representation is more conservative (and thus, hopefully, more feasible) and can be obtained by syntactic
interpretation.
At inter-proposition level, PKG can be compared with traditional Knowledge Graphs (such as Freebase
and Google?s Knowledge Graph). These Knowledge Graphs, in contrast with PKGs, require manual
intervention and aim to cover a rich set of relations using formal language and a pre-specified schema,
thus many relations are inevitably left out (e.g. the relation cracked, as in (Alan Turing, cracked, the
Enigma) does not exist in Freebase).
We believe that PKGs are a promising extension of Open IE?s unsupervised traits, for combining as-
pects of information representation - on a local scale, providing a rich schema for representing sentences,
and on a global scale providing an automated and consolidated method for structuring knowledge.
References
Meni Adler, Jonathan Berant, and Ido Dagan. 2012. Entailment-based text exploration with application to the
health-care domain. In Proceedings of the System Demonstrations of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL 2012), pages 79?84.
S?oren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. Dbpe-
dia: A nucleus for a web of open data. In The semantic web, pages 722?735. Springer.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp
Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively
created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD interna-
tional conference on Management of data, pages 1247?1250. ACM.
Jim Cowie and Wendy Lehnert. 1996. Information extraction. Communications of the ACM, 39(1):80?91.
Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. 2013. Recognizing textual entailment:
Models and applications. Synthesis Lectures on Human Language Technologies, 6(4):1?220.
Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representa-
tion. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,
pages 1?8, Manchester, UK, August. Coling 2008 Organizing Committee.
Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. 2008. Open information extraction from
the Web. Communications of the ACM, 51(12):68?74.
Anand Gupta, Manpreet Kathuria, Shachar Mirkin, Adarsh Singh, and Aseem Goyal. 2014. Text summarization
through entailment-based minimum vertex cover. In *SEM.
Paul Kingsbury and Martha Palmer. 2003. Propbank: the next level of treebank. In Proceedings of Treebanks and
lexical Theories, volume 3.
Omer Levy, Ido Dagan, and Jacob Goldberger. 2014. Focused entailment graphs for open ie propositions. In
Proceedings of the Eighteenth Conference on Computational Natural Language Learning, Baltimore, Maryland,
USA, June. Association for Computational Linguistics.
Mausam, Michael Schmitz, Stephen Soderland, Robert Bart, and Oren Etzioni. 2012. Open language learning
for information extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Language Learning, pages 523?534, Jeju Island, Korea, July.
Association for Computational Linguistics.
Gabriel Stanovsky, Jessica Ficler, Ido Dagan, and Yoav Goldberg. 2014. Intermediary semantic representation
through proposition structures. In Workshop on Semantic Parsing, Baltimore, Maryland, USA, June. Associa-
tion for Computational Linguistics.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. Yago: A large ontology from wikipedia and
wordnet. Web Semantics: Science, Services and Agents on the World Wide Web, 6(3):203?217.
23
Fei Wu, Raphael Hoffmann, and Daniel S Weld. 2008. Information extraction from wikipedia: Moving down the
long tail. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data
mining, pages 731?739. ACM.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-Rong Wen. 2009. Statsnowball: a statistical approach to
extracting entity relationships. In Proceedings of the 18th international conference on World wide web, pages
101?110. ACM.
24
