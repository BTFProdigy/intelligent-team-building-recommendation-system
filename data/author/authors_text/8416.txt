Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 697?704,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Multi-Tagging for Lexicalized-Grammar Parsing
James R. Curran
School of IT
University of Sydney
NSW 2006, Australia
james@it.usyd.edu.au
Stephen Clark
Computing Laboratory
Oxford University
Wolfson Building
Parks Road
Oxford, OX1 3QD, UK
sclark@comlab.ox.ac.uk
David Vadas
School of IT
University of Sydney
NSW 2006, Australia
dvadas1@it.usyd.edu.au
Abstract
With performance above 97% accuracy for
newspaper text, part of speech (POS) tag-
ging might be considered a solved prob-
lem. Previous studies have shown that
allowing the parser to resolve POS tag
ambiguity does not improve performance.
However, for grammar formalisms which
use more fine-grained grammatical cate-
gories, for example TAG and CCG, tagging
accuracy is much lower. In fact, for these
formalisms, premature ambiguity resolu-
tion makes parsing infeasible.
We describe a multi-tagging approach
which maintains a suitable level of lexical
category ambiguity for accurate and effi-
cient CCG parsing. We extend this multi-
tagging approach to the POS level to over-
come errors introduced by automatically
assigned POS tags. Although POS tagging
accuracy seems high, maintaining some
POS tag ambiguity in the language pro-
cessing pipeline results in more accurate
CCG supertagging.
1 Introduction
State-of-the-art part of speech (POS) tagging ac-
curacy is now above 97% for newspaper text
(Collins, 2002; Toutanova et al, 2003). One pos-
sible conclusion from the POS tagging literature
is that accuracy is approaching the limit, and any
remaining improvement is within the noise of the
Penn Treebank training data (Ratnaparkhi, 1996;
Toutanova et al, 2003).
So why should we continue to work on the POS
tagging problem? Here we give two reasons. First,
for lexicalized grammar formalisms such as TAG
and CCG, the tagging problem is much harder.
Second, any errors in POS tagger output, even at
97% acuracy, can have a significant impact on
components further down the language processing
pipeline. In previous work we have shown that us-
ing automatically assigned, rather than gold stan-
dard, POS tags reduces the accuracy of our CCG
parser by almost 2% in dependency F-score (Clark
and Curran, 2004b).
CCG supertagging is much harder than POS tag-
ging because the CCG tag set consists of fine-
grained lexical categories, resulting in a larger tag
set ? over 400 CCG lexical categories compared
with 45 Penn Treebank POS tags. In fact, using
a state-of-the-art tagger as a front end to a CCG
parser makes accurate parsing infeasible because
of the high supertagging error rate.
Our solution is to use multi-tagging, in which
a CCG supertagger can potentially assign more
than one lexical category to a word. In this
paper we significantly improve our earlier ap-
proach (Clark and Curran, 2004a) by adapting the
forward-backward algorithm to a Maximum En-
tropy tagger, which is used to calculate a proba-
bility distribution over lexical categories for each
word. This distribution is used to assign one or
more categories to each word (Charniak et al,
1996). We report large increases in accuracy over
single-tagging at only a small cost in increased
ambiguity.
A further contribution of the paper is to also
use multi-tagging for the POS tags, and to main-
tain some POS ambiguity in the language process-
ing pipeline. In particular, since POS tags are im-
portant features for the supertagger, we investigate
how supertagging accuracy can be improved by
not prematurely committing to a POS tag decision.
Our results first demonstrate that a surprising in-
697
crease in POS tagging accuracy can be achieved
with only a tiny increase in ambiguity; and second
that maintaining some POS ambiguity can signifi-
cantly improve the accuracy of the supertagger.
The parser uses the CCG lexical categories to
build syntactic structure, and the POS tags are
used by the supertagger and parser as part of their
statisical models. We show that using a multi-
tagger for supertagging results in an effective pre-
processor for CCG parsing, and that using a multi-
tagger for POS tagging results in more accurate
CCG supertagging.
2 Maximum Entropy Tagging
The tagger uses conditional probabilities of the
form P (y|x) where y is a tag and x is a local
context containing y. The conditional probabili-
ties have the following log-linear form:
P (y|x) =
1
Z(x)
e
?
i
?ifi(x,y) (1)
where Z(x) is a normalisation constant which en-
sures a proper probability distribution for each
context x.
The feature functions fi(x, y) are binary-
valued, returning either 0 or 1 depending on the
tag y and the value of a particular contextual pred-
icate given the context x. Contextual predicates
identify elements of the context which might be
useful for predicting the tag. For example, the fol-
lowing feature returns 1 if the current word is the
and the tag is DT; otherwise it returns 0:
fi(x, y) =
{
1 if word(x) = the & y = DT
0 otherwise
(2)
word(x) = the is an example of a contextual
predicate. The POS tagger uses the same con-
textual predicates as Ratnaparkhi (1996); the su-
pertagger adds contextual predicates correspond-
ing to POS tags and bigram combinations of POS
tags (Curran and Clark, 2003).
Each feature fi has an associated weight ?i
which is determined during training. The training
process aims to maximise the entropy of the model
subject to the constraints that the expectation of
each feature according to the model matches the
empirical expectation from the training data. This
can be also thought of in terms of maximum like-
lihood estimation (MLE) for a log-linear model
(Della Pietra et al, 1997). We use the L-BFGS op-
timisation algorithm (Nocedal and Wright, 1999;
Malouf, 2002) to perform the estimation.
MLE has a tendency to overfit the training data.
We adopt the standard approach of Chen and
Rosenfeld (1999) by introducing a Gaussian prior
term to the objective function which penalises fea-
ture weights with large absolute values. A param-
eter defined in terms of the standard deviation of
the Gaussian determines the degree of smoothing.
The conditional probability of a sequence of
tags, y1, . . . , yn, given a sentence, w1, . . . , wn, is
defined as the product of the individual probabili-
ties for each tag:
P (y1, . . . , yn|w1, . . . , wn) =
n?
i=1
P (yi|xi) (3)
where xi is the context for word wi. We use the
standard approach of Viterbi decoding to find the
highest probability sequence.
2.1 Multi-tagging
Multi-tagging ? assigning one or more tags to a
word ? is used here in two ways: first, to retain
ambiguity in the CCG lexical category sequence
for the purpose of building parse structure; and
second, to retain ambiguity in the POS tag se-
quence. We retain ambiguity in the lexical cate-
gory sequence since a single-tagger is not accurate
enough to serve as a front-end to a CCG parser, and
we retain some POS ambiguity since POS tags are
used as features in the statistical models of the su-
pertagger and parser.
Charniak et al (1996) investigated multi-POS
tagging in the context of PCFG parsing. It was
found that multi-tagging provides only a minor
improvement in accuracy, with a significant loss
in efficiency; hence it was concluded that, given
the particular parser and tagger used, a single-tag
POS tagger is preferable to a multi-tagger. More
recently, Watson (2006) has revisited this question
in the context of the RASP parser (Briscoe and Car-
roll, 2002) and found that, similar to Charniak et
al. (1996), multi-tagging at the POS level results in
a small increase in parsing accuracy but at some
cost in efficiency.
For lexicalized grammars, such as CCG and
TAG, the motivation for using a multi-tagger to as-
sign the elementary structures (supertags) is more
compelling. Since the set of supertags is typ-
ically much larger than a standard POS tag set,
the tagging problem becomes much harder. In
698
fact, when using a state-of-the-art single-tagger,
the per-word accuracy for CCG supertagging is so
low (around 92%) that wide coverage, high ac-
curacy parsing becomes infeasible (Clark, 2002;
Clark and Curran, 2004a). Similar results have
been found for a highly lexicalized HPSG grammar
(Prins and van Noord, 2003), and also for TAG.
As far as we are aware, the only approach to suc-
cessfully integrate a TAG supertagger and parser is
the Lightweight Dependency Analyser of Banga-
lore (2000). Hence, in order to perform effective
full parsing with these lexicalized grammars, the
tagger front-end must be a multi-tagger (given the
current state-of-the-art).
The simplest approach to CCG supertagging is
to assign all categories to a word which the word
was seen with in the data. This leaves the parser
the task of managing the very large parse space re-
sulting from the high degree of lexical category
ambiguity (Hockenmaier and Steedman, 2002;
Hockenmaier, 2003). However, one of the orig-
inal motivations for supertagging was to signifi-
cantly reduce the syntactic ambiguity before full
parsing begins (Bangalore and Joshi, 1999). Clark
and Curran (2004a) found that performing CCG
supertagging prior to parsing can significantly in-
crease parsing efficiency with no loss in accuracy.
Our multi-tagging approach follows that of
Clark and Curran (2004a) and Charniak et al
(1996): assign all categories to a word whose
probabilities are within a factor, ?, of the proba-
bility of the most probable category for that word:
Ci = {c | P (Ci = c|S) > ? P (Ci = cmax|S)}
Ci is the set of categories assigned to the ith word;
Ci is the random variable corresponding to the cat-
egory of the ith word; cmax is the category with the
highest probability of being the category of the ith
word; and S is the sentence. One advantage of this
adaptive approach is that, when the probability of
the highest scoring category is much greater than
the rest, no extra categories will be added.
Clark and Curran (2004a) propose a simple
method for calculating P (Ci = c|S): use the
word and POS features in the local context to cal-
culate the probability and ignore the previously
assigned categories (the history). However, it is
possible to incorporate the history in the calcula-
tion of the tag probabilities. A greedy approach is
to use the locally highest probability history as a
feature, which avoids any summing over alterna-
tive histories. Alternatively, there is a well-known
dynamic programming algorithm ? the forward
backward algorithm ? which efficiently calcu-
lates P (Ci = c|S) (Charniak et al, 1996).
The multitagger uses the following conditional
probabilities:
P (yi|w1,n) =
?
y1,i?1,yi+1,n
P (yi, y1,i?1, yi+1,n|w1,n)
where xi,j = xi, . . . xj . Here yi is to be thought of
as a fixed category, whereas yj (j 6= i) varies over
the possible categories for word j. In words, the
probability of category yi, given the sentence, is
the sum of the probabilities of all sequences con-
taining yi. This sum is calculated efficiently using
the forward-backward algorithm:
P (Ci = c|S) = ?i(c)?i(c) (4)
where ?i(c) is the total probability of all the cate-
gory sub-sequences that end at position i with cat-
egory c; and ?i(c) is the total probability of all the
category sub-sequences through to the end which
start at position i with category c.
The standard description of the forward-
backward algorithm, for example Manning and
Schutze (1999), is usually given for an HMM-style
tagger. However, it is straightforward to adapt the
algorithm to the Maximum Entropy models used
here. The forward-backward algorithm we use is
similar to that for a Maximum Entropy Markov
Model (Lafferty et al, 2001).
POS tags are very informative features for the
supertagger, which suggests that using a multi-
POS tagger may benefit the supertagger (and ulti-
mately the parser). However, it is unclear whether
multi-POS tagging will be useful in this context,
since our single-tagger POS tagger is highly accu-
rate: over 97% for WSJ text (Curran and Clark,
2003). In fact, in Clark and Curran (2004b) we re-
port that using automatically assigned, as opposed
to gold-standard, POS tags as features results in a
2% loss in parsing accuracy. This suggests that re-
taining some ambiguity in the POS sequence may
be beneficial for supertagging and parsing accu-
racy. In Section 4 we show this is the case for
supertagging.
3 CCG Supertagging and Parsing
Parsing using CCG can be viewed as a two-stage
process: first assign lexical categories to the words
in the sentence, and then combine the categories
699
The WSJ is a paper that I enjoy reading
NP/N N (S [dcl ]\NP)/NP NP/N N (NP\NP)/(S [dcl ]/NP) NP (S [dcl ]\NP)/(S [ng ]\NP) (S [ng ]\NP)/NP
Figure 1: Example sentence with CCG lexical categories.
together using CCG?s combinatory rules.1 We per-
form stage one using a supertagger.
The set of lexical categories used by the su-
pertagger is obtained from CCGbank (Hocken-
maier, 2003), a corpus of CCG normal-form
derivations derived semi-automatically from the
Penn Treebank. Following our earlier work, we
apply a frequency cutoff to the training set, only
using those categories which appear at least 10
times in sections 02-21, which results in a set of
425 categories. We have shown that the resulting
set has very high coverage on unseen data (Clark
and Curran, 2004a). Figure 1 gives an example
sentence with the CCG lexical categories.
The parser is described in Clark and Curran
(2004b). It takes POS tagged sentences as input
with each word assigned a set of lexical categories.
A packed chart is used to efficiently represent
all the possible analyses for a sentence, and the
CKY chart parsing algorithm described in Steed-
man (2000) is used to build the chart. A log-linear
model is used to score the alternative analyses.
In Clark and Curran (2004a) we described a
novel approach to integrating the supertagger and
parser: start with a very restrictive supertagger set-
ting, so that only a small number of lexical cate-
gories is assigned to each word, and only assign
more categories if the parser cannot find a span-
ning analysis. This strategy results in an efficient
and accurate parser, with speeds up to 35 sen-
tences per second. Accurate supertagging at low
levels of lexical category ambiguity is therefore
particularly important when using this strategy.
We found in Clark and Curran (2004b) that a
large drop in parsing accuracy occurs if automat-
ically assigned POS tags are used throughout the
parsing process, rather than gold standard POS
tags (almost 2% F-score over labelled dependen-
cies). This is due to the drop in accuracy of the
supertagger (see Table 3) and also the fact that
the log-linear parsing model uses POS tags as fea-
tures. The large drop in parsing accuracy demon-
strates that improving the performance of POS tag-
1See Steedman (2000) for an introduction to CCG, and
see Hockenmaier (2003) for an introduction to wide-coverage
parsing using CCG.
TAGS/WORD ? WORD ACC SENT ACC
1.00 1 96.7 51.8
1.01 0.8125 97.1 55.4
1.05 0.2969 98.3 70.7
1.10 0.1172 99.0 80.9
1.20 0.0293 99.5 89.3
1.30 0.0111 99.6 91.7
1.40 0.0053 99.7 93.2
4.23 0 99.8 94.8
Table 1: POS tagging accuracy on Section 00 for
different levels of ambiguity.
gers is still an important research problem. In this
paper we aim to reduce the performance drop of
the supertagger by maintaing some POS ambiguity
through to the supertagging phase. Future work
will investigate maintaining some POS ambiguity
through to the parsing phase also.
4 Multi-tagging Experiments
We performed several sets of experiments for
POS tagging and CCG supertagging to explore the
trade-off between ambiguity and tagging accuracy.
For both POS tagging and supertagging we varied
the average number of tags assigned to each word,
to see whether it is possible to significantly in-
crease tagging accuracy with only a small increase
in ambiguity. For CCG supertagging, we also com-
pared multi-tagging approaches, with a fixed cate-
gory ambiguity of 1.4 categories per word.
All of the experiments used Section 02-21 of
CCGbank as training data, Section 00 as develop-
ment data and Section 23 as final test data. We
evaluate both per-word tag accuracy and sentence
accuracy, which is the percentage of sentences for
which every word is tagged correctly. For the
multi-tagging results we consider the word to be
tagged correctly if the correct tag appears in the
set of tags assigned to the word.
4.1 Results
Table 1 shows the results for multi-POS tagging
for different levels of ambiguity. The row corre-
sponding to 1.01 tags per word shows that adding
700
METHOD GOLD POS AUTO POS
WORD SENT WORD SENT
single 92.6 36.8 91.5 32.7
noseq 96.2 51.9 95.2 46.1
best hist 97.2 63.8 96.3 57.2
fwdbwd 97.9 72.1 96.9 64.8
Table 2: Supertagging accuracy on Section 00 us-
ing different approaches with multi-tagger ambi-
guity fixed at 1.4 categories per word.
TAGS/ GOLD POS AUTO POS
WORD ? WORD SENT WORD SENT
1.0 1 92.6 36.8 91.5 32.7
1.2 0.1201 96.8 63.4 95.8 56.5
1.4 0.0337 97.9 72.1 96.9 64.8
1.6 0.0142 98.3 76.4 97.5 69.3
1.8 0.0074 98.4 78.3 97.7 71.0
2.0 0.0048 98.5 79.4 97.9 72.5
2.5 0.0019 98.7 80.6 98.1 74.3
3.0 0.0009 98.7 81.4 98.3 75.6
12.5 0 98.9 82.3 98.8 80.1
Table 3: Supertagging accuracy on Section 00 for
different levels of ambiguity.
even a tiny amount of ambiguity (1 extra tag in ev-
ery 100 words) gives a reasonable improvement,
whilst adding 1 tag in 20 words, or approximately
one extra tag per sentence on the WSJ, gives a sig-
nificant boost of 1.6% word accuracy and almost
20% sentence accuracy.
The bottom row of Table 1 gives an upper bound
on accuracy if the maximum ambiguity is allowed.
This involves setting the ? value to 0, so all feasi-
ble tags are assigned. Note that the performance
gain is only 1.6% in sentence accuracy, compared
with the previous row, at the cost of a large in-
crease in ambiguity.
Our first set of CCG supertagging experiments
compared the performance of several approaches.
In Table 2 we give the accuracies when using gold
standard POS tags, and also POS tags automatically
assigned by our POS tagger described above. Since
POS tags are important features for the supertagger
maximum entropy model, erroneous tags have a
significant impact on supertagging accuracy.
The singlemethod is the single-tagger supertag-
ger, which at 91.5% per-word accuracy is too inac-
curate for use with the CCG parser. The remaining
rows in the table give multi-tagger results for a cat-
egory ambiguity of 1.4 categories per word. The
noseq method, which performs significantly better
than single, does not take into account the previ-
ously assigned categories. The best hist method
gains roughly another 1% in accuracy over noseq
by taking the greedy approach of using only the
two most probable previously assigned categories.
Finally, the full forward-backward approach de-
scribed in Section 2.1 gains roughly another 0.6%
by considering all possible category histories. We
see the largest jump in accuracy just by returning
multiple categories. The other more modest gains
come from producing progressively better models
of the category sequence.
The final set of supertagging experiments in Ta-
ble 3 demonstrates the trade-off between ambigu-
ity and accuracy. Note that the ambiguity levels
need to be much higher to produce similar perfor-
mance to the POS tagger and that the upper bound
case (? = 0) has a very high average ambiguity.
This is to be expected given the much larger CCG
tag set.
5 Tag uncertainty thoughout the pipeline
Tables 2 and 3 show that supertagger accuracy
when using gold-standard POS tags is typically
1% higher than when using automatically assigned
POS tags. Clearly, correct POS tags are important
features for the supertagger.
Errors made by the supertagger can multiply
out when incorrect lexical categories are passed
to the parser, so a 1% increase in lexical category
error can become much more significant in the
parser evaluation. For example, when using the
dependency-based evaluation in Clark and Curran
(2004b), getting the lexical category wrong for a
ditransitive verb automatically leads to three de-
pendencies in the output being incorrect.
We have shown that multi-tagging can signif-
icantly increase the accuracy of the POS tagger
with only a small increase in ambiguity. What
we would like to do is maintain some degree of
POS tag ambiguity and pass multiple POS tags
through to the supertagging stage (and eventually
the parser). There are several ways to encode mul-
tiple POS tags as features. The simplest approach
is to treat all of the POS tags as binary features,
but this does not take into account the uncertainty
in each of the alternative tags. What we need is a
way of incorporating probability information into
the Maximum Entropy supertagger.
701
6 Real-values in ME models
Maximum Entropy (ME) models, in the NLP lit-
erature, are typically defined with binary features,
although they do allow real-valued features. The
only constraint comes from the optimisation algo-
rithm; for example, GIS only allows non-negative
values. Real-valued features are commonly used
with other machine learning algorithms.
Binary features suffer from certain limitations
of the representation, which make them unsuitable
for modelling some properties. For example, POS
taggers have difficulty determining if capitalised,
sentence initial words are proper nouns. A useful
way to model this property is to determine the ra-
tio of capitalised and non-capitalised instances of
a particular word in a large corpus and use a real-
valued feature which encodes this ratio (Vadas and
Curran, 2005). The only way to include this fea-
ture in a binary representation is to discretize (or
bin) the feature values. For this type of feature,
choosing appropriate bins is difficult and it may be
hard to find a discretization scheme that performs
optimally.
Another problem with discretizing feature val-
ues is that it imposes artificial boundaries to define
the bins. For the example above, we may choose
the bins 0 ? x < 1 and 1 ? x < 2, which sepa-
rate the values 0.99 and 1.01 even though they are
close in value. At the same time, the model does
not distinguish between 0.01 and 0.99 even though
they are much further apart.
Further, if we have not seen cases for the bin
2 ? x < 3, then the discretized model has no evi-
dence to determine the contribution of this feature.
But for the real-valued model, evidence support-
ing 1 ? x < 2 and 3 ? x < 4 provides evidence
for the missing bin. Thus the real-valued model
generalises more effectively.
One issue that is not addressed here is the inter-
action between the Gaussian smoothing parameter
and real-valued features. Using the same smooth-
ing parameter for real-valued features with vastly
different distributions is unlikely to be optimal.
However, for these experiments we have used the
same value for the smoothing parameter on all
real-valued features. This is the same value we
have used for the binary features.
7 Multi-POS Supertagging Experiments
We have experimented with four different ap-
proaches to passing multiple POS tags as features
through to the supertagger. For the later exper-
iments, this required the existing binary-valued
framework to be extended to support real values.
The level of POS tag ambiguity was varied be-
tween 1.05 and 1.3 POS tags per word on average.
These results are shown in Table 4.
The first approach is to treat the multiple POS
tags as binary features (bin). This simply involves
adding the multiple POS tags for each word in
both the training and test data. Every assigned
POS tag is treated as a separate feature and con-
sidered equally important regardless of its uncer-
tainty. Here we see a minor increase in perfor-
mance over the original supertagger at the lower
levels of POS ambiguity. However, as the POS
ambiguity is increased, the performance of the
binary-valued features decreases and is eventually
worse than the original supertagger. This is be-
cause at the lowest levels of ambiguity the extra
POS tags can be treated as being of similar reli-
ability. However, at higher levels of ambiguity
many POS tags are added which are unreliable and
should not be trusted equally.
The second approach (split) uses real-valued
features to model some degree of uncertainty in
the POS tags, dividing the POS tag probability mass
evenly among the alternatives. This has the ef-
fect of giving smaller feature values to tags where
many alternative tags have been assigned. This
produces similar results to the binary-valued fea-
tures, again performing best at low levels of ambi-
guity.
The third approach (invrank) is to use the in-
verse rank of each POS tag as a real-valued feature.
The inverse rank is the reciprocal of the tag?s rank
ordered by decreasing probability. This method
assumes the POS tagger correctly orders the alter-
native tags, but does not rely on the probability
assigned to each tag. Overall, invrank performs
worse than split.
The final and best approach is to use the prob-
abilities assigned to each alternative tag as real-
valued features:
fi(x, y) =
{
p(POS(x) = NN) if y = NP
0 otherwise
(5)
This model gives the best performance at 1.1 POS
tags per-word average ambiguity. Note that, even
when using the probabilities as features, only a
small amount of additional POS ambiguity is re-
quired to significantly improve performance.
702
METHOD POS AMB WORD SENT
orig 1.00 96.9 64.8
bin 1.05 97.3 67.7
1.10 97.3 66.3
1.20 97.0 63.5
1.30 96.8 62.1
split 1.05 97.4 68.5
1.10 97.4 67.9
1.20 97.3 67.0
1.30 97.2 65.1
prob 1.05 97.5 68.7
1.10 97.5 69.1
1.20 97.5 68.7
1.30 97.5 68.7
invrank 1.05 97.3 68.0
1.10 97.4 68.0
1.20 97.3 67.1
1.30 97.3 67.1
gold - 97.9 72.1
Table 4: Multi-POS supertagging on Section 00
with different levels of POS ambiguity and using
different approaches to POS feature encoding.
Table 5 shows our best performance figures for
the multi-POS supertagger, against the previously
described method using both gold standard and au-
tomatically assigned POS tags.
Table 6 uses the Section 23 test data to
demonstrate the improvement in supertagging
when moving from single-tagging (single) to sim-
ple multi-tagging (noseq); from simple multi-
tagging to the full forward-backward algorithm
(fwdbwd); and finally when using the probabilities
of multiply-assigned POS tags as features (MULTI-
POS column). All of these multi-tagging experi-
ments use an ambiguity level of 1.4 categories per
word and the last result uses POS tag ambiguity of
1.1 tags per word.
8 Conclusion
The NLP community may consider POS tagging to
be a solved problem. In this paper, we have sug-
gested two reasons why this is not the case. First,
tagging for lexicalized-grammar formalisms, such
as CCG and TAG, is far from solved. Second,
even modest improvements in POS tagging accu-
racy can have a large impact on the performance of
downstream components in a language processing
pipeline.
TAGS/ AUTO POS MULTI POS GOLD POS
WORD WORD SENT WORD SENT WORD SENT
1.0 91.5 32.7 91.9 34.3 92.6 36.8
1.2 95.8 56.5 96.3 59.2 96.8 63.4
1.4 96.9 64.8 97.5 67.0 97.9 72.1
1.6 97.5 69.3 97.9 73.3 98.3 76.4
1.8 97.7 71.0 98.2 76.1 98.4 78.3
2.0 97.9 72.5 98.4 77.4 98.5 79.4
2.5 98.1 74.3 98.5 78.7 98.7 80.6
3.0 98.3 75.6 98.6 79.7 98.7 81.4
Table 5: Best multi-POS supertagging accuracy on
Section 00 using POS ambiguity of 1.1 and the
probability real-valued features.
METHOD AUTO POS MULTI POS GOLD POS
single 92.0 - 93.3
noseq 95.4 - 96.6
fwdbwd 97.1 97.7 98.2
Table 6: Final supertagging results on Section 23.
We have developed a novel approach to main-
taining tag ambiguity in language processing
pipelines which avoids premature ambiguity res-
olution. The tag ambiguity is maintained by using
the forward-backward algorithm to calculate indi-
vidual tag probabilities. These probabilities can
then be used to select multiple tags and can also
be encoded as real-valued features in subsequent
statistical models.
With this new approach we have increased POS
tagging accuracy significantly with only a tiny am-
biguity penalty and also significantly improved on
previous CCG supertagging results. Finally, us-
ing POS tag probabilities as real-valued features in
the supertagging model, we demonstrated perfor-
mance close to that obtained with gold-standard
POS tags. This will significantly improve the ro-
bustness of the parser on unseen text.
In future work we will investigate maintaining
tag ambiguity further down the language process-
ing pipeline and exploiting the uncertainty from
previous stages. In particular, we will incorporate
real-valued POS tag and lexical category features
in the statistical parsing model. Another possibil-
ity is to investigate whether similar techniques can
improve other tagging tasks, such as Named Entity
Recognition.
This work can be seen as part of the larger
goal of maintaining ambiguity and exploiting un-
703
certainty throughout language processing systems
(Roth and Yih, 2004), which is important for cop-
ing with the compounding of errors that is a sig-
nificant problem in language processing pipelines.
Acknowledgements
We would like to thank the anonymous reviewers
for their helpful feedback. This work has been
supported by the Australian Research Council un-
der Discovery Project DP0453131.
References
Srinivas Bangalore and Aravind Joshi. 1999. Supertagging:
An approach to almost parsing. Computational Linguis-
tics, 25(2):237?265.
Srinivas Bangalore. 2000. A lightweight dependency anal-
yser for partial parsing. Natural Language Engineering,
6(2):113?138.
Ted Briscoe and John Carroll. 2002. Robust accurate statis-
tical annotation of general tex. In Proceedings of the 3rd
LREC Conference, pages 1499?1504, Las Palmas, Gran
Canaria.
Eugene Charniak, Glenn Carroll, John Adcock, Anthony
Cassandra, Yoshihiko Gotoh, Jeremy Katz, Michael
Littman, and John McCann. 1996. Taggers for parsers.
Artificial Intelligence, 85:45?57.
Stanley Chen and Ronald Rosenfeld. 1999. A Gaussian prior
for smoothing maximum entropy models. Technical re-
port, Carnegie Mellon University, Pittsburgh, PA.
Stephen Clark and James R. Curran. 2004a. The impor-
tance of supertagging for wide-coverage CCG parsing.
In Proceedings of COLING-04, pages 282?288, Geneva,
Switzerland.
Stephen Clark and James R. Curran. 2004b. Parsing the
WSJ using CCG and log-linear models. In Proceedings of
the 42nd Meeting of the ACL, pages 104?111, Barcelona,
Spain.
Stephen Clark. 2002. A supertagger for Combinatory Cate-
gorial Grammar. In Proceedings of the TAG+ Workshop,
pages 19?24, Venice, Italy.
Michael Collins. 2002. Discriminative training methods for
Hidden Markov Models: Theory and experiments with
perceptron algorithms. In Proceedings of the EMNLP
Conference, pages 1?8, Philadelphia, PA.
James R. Curran and Stephen Clark. 2003. Investigating GIS
and smoothing for maximum entropy taggers. In Proceed-
ings of the 10th Meeting of the EACL, pages 91?98, Bu-
dapest, Hungary.
Stephen Della Pietra, Vincent Della Pietra, and John Laf-
ferty. 1997. Inducing features of random fields. IEEE
Transactions Pattern Analysis and Machine Intelligence,
19(4):380?393.
Julia Hockenmaier and Mark Steedman. 2002. Generative
models for statistical parsing with Combinatory Categorial
Grammar. In Proceedings of the 40th Meeting of the ACL,
pages 335?342, Philadelphia, PA.
Julia Hockenmaier. 2003. Data and Models for Statistical
Parsing with Combinatory Categorial Grammar. Ph.D.
thesis, University of Edinburgh.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proceedings of
the 18th International Conference on Machine Learning,
pages 282?289, Williams College, MA.
Robert Malouf. 2002. A comparison of algorithms for max-
imum entropy parameter estimation. In Proceedings of
the Sixth Workshop on Natural Language Learning, pages
49?55, Taipei, Taiwan.
Christopher Manning and Hinrich Schutze. 1999. Foun-
dations of Statistical Natural Language Processing. The
MIT Press, Cambridge, Massachusetts.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical Op-
timization. Springer, New York, USA.
Robbert Prins and Gertjan van Noord. 2003. Reinforcing
parser preferences through tagging. Traitement Automa-
tique des Langues, 44(3):121?139.
Adwait Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the EMNLP Conference,
pages 133?142, Philadelphia, PA.
D. Roth and W. Yih. 2004. A linear programming for-
mulation for global inference in natural language tasks.
In Hwee Tou Ng and Ellen Riloff, editors, Proc. of the
Annual Conference on Computational Natural Language
Learning (CoNLL), pages 1?8. Association for Computa-
tional Linguistics.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, MA.
Kristina Toutanova, Dan Klein, Christopher Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech tag-
ging with a cyclic dependency network. In Proceedings
of the HLT/NAACL conference, pages 252?259, Edmon-
ton, Canada.
David Vadas and James R. Curran. 2005. Tagging un-
known words with raw text features. In Proceedings of the
Australasian Language Technology Workshop 2005, pages
32?39, Sydney, Australia.
Rebecca Watson. 2006. Part-of-speech tagging models for
parsing. In Proceedings of the Computaional Linguistics
in the UK Conference (CLUK-06), Open University, Mil-
ton Keynes, UK.
704
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 240?247,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Adding Noun Phrase Structure to the Penn Treebank
David Vadas and James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
 
dvadas1, james  @it.usyd.edu.au
Abstract
The Penn Treebank does not annotate
within base noun phrases (NPs), commit-
ting only to flat structures that ignore the
complexity of English NPs. This means
that tools trained on Treebank data cannot
learn the correct internal structure of NPs.
This paper details the process of adding
gold-standard bracketing within each
noun phrase in the Penn Treebank. We
then examine the consistency and reliabil-
ity of our annotations. Finally, we use
this resource to determine NP structure
using several statistical approaches, thus
demonstrating the utility of the corpus.
This adds detail to the Penn Treebank that
is necessary for many NLP applications.
1 Introduction
The Penn Treebank (Marcus et al, 1993) is perhaps
the most influential resource in Natural Language
Processing (NLP). It is used as a standard train-
ing and evaluation corpus in many syntactic analysis
tasks, ranging from part of speech (POS) tagging and
chunking, to full parsing.
Unfortunately, the Penn Treebank does not anno-
tate the internal structure of base noun phrases, in-
stead leaving them flat. This significantly simplified
and sped up the manual annotation process.
Therefore, any system trained on Penn Treebank
data will be unable to model the syntactic and se-
mantic structure inside base-NPs.
The following NP is an example of the flat struc-
ture of base-NPs within the Penn Treebank:
(NP (NNP Air) (NNP Force) (NN contract))
Air Force is a specific entity and should form a sep-
arate constituent underneath the NP, as in our new
annotation scheme:
(NP
(NML (NNP Air) (NNP Force))
(NN contract))
We use NML to specify that Air Force together is a
nominal modifier of contract. Adding this annota-
tion better represents the true syntactic and seman-
tic structure, which will improve the performance of
downstream NLP systems.
Our main contribution is a gold-standard labelled
bracketing for every ambiguous noun phrase in the
Penn Treebank. We describe the annotation guide-
lines and process, including the use of named en-
tity data to improve annotation quality. We check
the correctness of the corpus by measuring inter-
annotator agreement, by reannotating the first sec-
tion, and by comparing against the sub-NP structure
in DepBank (King et al, 2003).
We also give an analysis of our extended Tree-
bank, quantifying how much structure we have
added, and how it is distributed across NPs. Fi-
nally, we test the utility of the extended Treebank for
training statistical models on two tasks: NP bracket-
ing (Lauer, 1995; Nakov and Hearst, 2005) and full
parsing (Collins, 1999).
This new resource will allow any system or anno-
tated corpus developed from the Penn Treebank, to
represent noun phrase structure more accurately.
240
2 Motivation
Many approaches to identifying base noun phrases
have been explored as part of chunking (Ramshaw
and Marcus, 1995), but determining sub-NP struc-
ture is rarely addressed. We could use multi-word
expressions (MWEs) to identify some structure. For
example, knowing stock market is a MWE may help
bracket stock market prices correctly, and Named
Entities (NEs) can be used the same way. However,
this only resolves NPs dominating MWEs or NEs.
Understanding base-NP structure is important,
since otherwise parsers will propose nonsensical
noun phrases like Force contract by default and pass
them onto downstream components. For example,
Question Answering (QA) systems need to supply
an NP as the answer to a factoid question, often us-
ing a parser to identify candidate NPs to return to
the user. If the parser never generates the correct
sub-NP structure, then the system may return a non-
sensical answer even though the correct dominating
noun phrase has been found.
Base-NP structure is also important for anno-
tated data derived from the Penn Treebank. For
instance, CCGbank (Hockenmaier, 2003) was cre-
ated by semi-automatically converting the Treebank
phrase structure to Combinatory Categorial Gram-
mar (CCG) (Steedman, 2000) derivations. Since CCG
derivations are binary branching, they cannot di-
rectly represent the flat structure of the Penn Tree-
bank base-NPs.
Without the correct bracketing in the Treebank,
strictly right-branching trees were created for all
base-NPs. This has an unwelcome effect when con-
junctions occur within an NP (Figure 1). An addi-
tional grammar rule is needed just to get a parse, but
it is still not correct (Hockenmaier, 2003, p. 64). The
awkward conversion results in bracketing (a) which
should be (b):
(a) (consumer ((electronics) and
(appliances (retailing chain))))
(b) ((((consumer electronics) and
appliances) retailing) chain)
We have previously experimented with using NEs to
improve parsing performance on CCGbank. Due to
the mis-alignment of NEs and right-branching NPs,
the increase in performance was negligible.
N
N/N
consumer
N
N/N
electronics
N
conj
and
N
N/N
appliances
N
N/N
retailing
N
chain
Figure 1: CCG derivation from Hockenmaier (2003)
3 Background
The NP bracketing task has often been posed in
terms of choosing between the left or right branch-
ing structure of three word noun compounds:
(a) (world (oil prices)) ? Right-branching
(b) ((crude oil) prices) ? Left-branching
Most approaches to the problem use unsupervised
methods, based on competing association strength
between two of the words in the compound (Mar-
cus, 1980, p. 253). There are two possible models
to choose from: dependency or adjacency. The de-
pendency model compares the association between
words 1-2 to words 1-3, while the adjacency model
compares words 1-2 to words 2-3.
Lauer (1995) has demonstrated superior perfor-
mance of the dependency model using a test set
of 244 (216 unique) noun compounds drawn from
Grolier?s encyclopedia. This data has been used to
evaluate most research since. He uses Roget?s the-
saurus to smooth words into semantic classes, and
then calculates association between classes based
on their counts in a ?training set? also drawn from
Grolier?s. He achieves 80.7% accuracy using POS
tags to indentify bigrams in the training set.
Lapata and Keller (2004) derive estimates from
web counts, and only compare at a lexical level,
achieving 78.7% accuracy. Nakov and Hearst (2005)
also use web counts, but incorporate additional
counts from several variations on simple bigram
queries, including queries for the pairs of words con-
catenated or joined by a hyphen. This results in an
impressive 89.3% accuracy.
There have also been attempts to solve this task
using supervised methods, even though the lack of
gold-standard data makes this difficult. Girju et al
241
(2005) draw a training set from raw WSJ text and use
it to train a decision tree classifier achieving 73.1%
accuracy. When they shuffled their data with Lauer?s
to create a new test and training split, their accu-
racy increased to 83.1% which may be a result of
the  10% duplication in Lauer?s test set.
We have created a new NP bracketing data set
from our extended Treebank by extracting all right-
most three noun sequences from base-NPs. Our ini-
tial experiments are presented in Section 6.1.
4 Corpus Creation
According to Marcus et al (1993), asking annota-
tors to markup base-NP structure significantly re-
duced annotation speed, and for this reason base-
NPs were left flat. The bracketing guidelines (Bies
et al, 1995) also mention the considerable difficulty
of identifying the correct scope for nominal modi-
fiers. We found however, that while there are cer-
tainly difficult cases, the vast majority are quite sim-
ple and can be annotated reliably. Our annotation
philosophy can be summarised as:
1. most cases are easy and fit a common pattern;
2. prefer the implicit right-branching structure for
difficult decisions. Finance jargon was a com-
mon source of these;
3. mark very difficult to bracket NPs and discuss
with other annotators later;
During this process we identified numerous cases
that require a more sophisticated annotation scheme.
There are genuine flat cases, primarily names like
John A. Smith, that we would like to distinguish from
implicitly right-branching NPs in the next version of
the corpus. Although our scheme is still developing,
we believe that the current annotation is already use-
ful for statistical modelling, and we demonstrate this
empirically in Section 6.
4.1 Annotation Process
Our annotation guidelines1 are based on those de-
veloped for annotating full sub-NP structure in the
biomedical domain (Kulick et al, 2004). The anno-
tation guidelines for this biomedical corpus (an ad-
dendum to the Penn Treebank guidelines) introduce
the use of NML nodes to mark internal NP structure.
1The guidelines and corpus are available on our webpages.
In summary, our guidelines leave right-branching
structures untouched, and insert labelled brackets
around left-branching structures. The label of the
newly created constituent is NML or JJP, depending
on whether its head is a noun or an adjective. We
also chose not to alter the existing Penn Treebank
annotation, even though the annotators found many
errors during the annotation process. We wanted to
keep our extended Treebank as similar to the origi-
nal as possible, so that they remain comparable.
We developed a bracketing tool, which identifies
ambiguous NPs and presents them to the user for
disambiguation. An ambiguous NP is any (possibly
non-base) NP with three or more contiguous chil-
dren that are either single words or another NP. Cer-
tain common patterns, such as three words begin-
ning with a determiner, are unambiguous, and were
filtered out. The annotator is also shown the entire
sentence surrounding the ambiguous NP.
The bracketing tool often suggests a bracket-
ing using rules based mostly on named entity tags,
which are drawn from the BBN corpus (Weischedel
and Brunstein, 2005). For example, since Air Force
is given ORG tags, the tool suggests that they be
bracketed together first. Other suggestions come
from previous bracketings of the same words, which
helps to keep the annotator consistent.
Two post processes were carried out to increase
annotation consistency and correctness. 915 diffi-
cult NPs were marked by the annotator and were then
discussed with two other experts. Secondly, cer-
tain phrases that occurred numerous times and were
non-trivial to bracket, e.g. London Interbank Of-
fered Rate, were identified. An extra pass was made
through the corpus, ensuring that every instance of
these phrases was bracketed consistently.
4.2 Annotation Time
Annotation initially took over 9 hours per section of
the Treebank. However, with practice this was re-
duced to about 3 hours per section. Each section
contains around 2500 ambiguous NPs, i.e. annotat-
ing took approximately 5 seconds per NP. Most NPs
require no bracketing, or fit into a standard pattern
which the annotator soon becomes accustomed to,
hence the task can be performed quite quickly.
For the original bracketing of the Treebank, anno-
tators performed at 375?475 words per hour after a
242
PREC. RECALL F-SCORE
Brackets 89.17 87.50 88.33
Dependencies 96.40 96.40 96.40
Brackets, revised 97.56 98.03 97.79
Dependencies, revised 99.27 99.27 99.27
Table 1: Agreement between annotators
few weeks, and increased to about 1000 words per
hour after gaining more experience (Marcus et al,
1993). For our annotation process, counting each
word in every NP shown, our speed was around 800
words per hour. This figure is not unexpected, as the
task was not large enough to get more than a month?s
experience, and there is less structure to annotate.
5 Corpus Analysis
5.1 Inter-annotator Agreement
The annotation was performed by the first author.
A second Computational Linguistics PhD student
also annotated Section 23, allowing inter-annotator
agreement, and the reliability of the annotations, to
be measured. This also maximised the quality of the
section used for parser testing.
We measured the proportion of matching brack-
ets and dependencies between annotators, shown in
Table 1, both before and after they discussed cases
of disagreement and revised their annotations. The
number of dependencies is fixed by the length of the
NP, so the dependency precision and recall are the
same. Counting matched brackets is a harsher eval-
uation, as there are many NPs that both annotators
agree should have no additional bracketing, which
are not taken into account by the metric.
The disagreements occurred for a small number
of repeated instances, such as this case:
(NP (NP (NNP Goldman)
(NML (NNP Goldman) (, ,)
(, ,) (NNP Sachs)
(NNP Sachs) ) (CC &) (NNP Co) )
(CC &) (NNP Co) )
The first annotator felt that Goldman , Sachs
should form their own NML constituent, while the
second annotator did not.
We can also look at exact matching on NPs, where
the annotators originally agreed in 2667 of 2908
cases (91.71%), and after revision, in 2864 of 2907
cases (98.52%). These results demonstrate that high
agreement rates are achievable for these annotations.
MATCHED TOTAL %
By dependency 1409 (1315) 1479 95.27 (88.91)
By noun phrase 562 (489) 626 89.78 (78.12)
By dependency,
only annotated NPs 578 (543) 627 92.19 (86.60)
By noun phrase,
only annotated NPs 186 (162) 229 81.22 (70.74)
Table 2: Agreement with DepBank
5.2 DepBank Agreement
Another approach to measuring annotator reliabil-
ity is to compare with an independently annotated
corpus on the same text. We used the PARC700 De-
pendency Bank (King et al, 2003) which consists of
700 Section 23 sentences annotated with labelled de-
pendencies. We use the Briscoe and Carroll (2006)
version of DepBank, a 560 sentence subset used to
evaluate the RASP parser.
Some translation is required to compare our
brackets to DepBank dependencies. We map the
brackets to dependencies by finding the head of the
NP, using the Collins (1999) head finding rules,
and then creating a dependency between each other
child?s head and this head. This does not work per-
fectly, and mismatches occur because of which de-
pendencies DepBank marks explicitly, and how it
chooses heads. The errors are investigated manually
to determine their cause.
The results are shown in Table 2, with the num-
ber of agreements before manual checking shown in
parentheses. Once again the dependency numbers
are higher than those at the NP level. Similarly, when
we only look at cases where we have inserted some
annotations, we are looking at more difficult cases
and the score is not as high.
The results of this analysis are quite positive.
Over half of the disagreements that occur (in ei-
ther measure) are caused by how company names
are bracketed. While we have always separated the
company name from post-modifiers such as Corp
and Inc, DepBank does not in most cases. These
results show that consistently and correctly bracket-
ing noun phrase structure is possible, and that inter-
annotator agreement is at an acceptable level.
5.3 Corpus Composition and Consistency
Looking at the entire Penn Treebank corpus, the
annotation tool finds 60959 ambiguous NPs out of
the 432639 NPs in the corpus (14.09%). 22851 of
243
LEVEL COUNT POS TAGS EXAMPLE
1073 JJ JJ NNS big red cars
1581 DT JJ NN NN a high interest rateNP 1693 JJ NN NNS high interest rates
3557 NNP NNP NNP John A. Smith
1468 NN NN (interest rate) rises
1538 JJ NN (heavy truck) rentalsNML 1650 NNP NNP NNP (A. B. C.) Corp
8524 NNP NNP (John Smith) Jr.
82 JJ JJ (dark red) car
114 RB JJ (very high) ratesJJP 122 JJ CC JJ (big and red) apples
160 ? JJ ? (? smart ?) cars
Table 3: Common POS tag sequences
these (37.49%) had brackets inserted by the annota-
tor. This is as we expect, as the majority of NPs are
right-branching. Of the brackets added, 22368 were
NML nodes, while 863 were JJP.
To compare, we can count the number of existing
NP and ADJP nodes found in the NPs that the brack-
eting tool presents. We find there are 32772 NP chil-
dren, and 579 ADJP, which are quite similar num-
bers to the amount of nodes we have added. From
this, we can say that our annotation process has in-
troduced almost as much structural information into
NPs as there was in the original Penn Treebank.
Table 3 shows the most common POS tag se-
quences for NP, NML and JJP nodes. An example
is given showing typical words that match the POS
tags. For NML and JJP, we also show the words
bracketed, as they would appear under an NP node.
We checked the consistency of the annotations by
identifying NPs with the same word sequence and
checking whether they were always bracketed iden-
tically. After the first pass through, there were 360
word sequences with multiple bracketings, which
occurred in 1923 NP instances. 489 of these in-
stances differed from the majority case for that se-
quence, and were probably errors.
The annotator had marked certain difficult and
commonly repeating NPs. From this we generated a
list of phrases, and then made another pass through
the corpus, synchronising all instances that con-
tained one of these phrases. After this, only 150 in-
stances differed from the majority case. Inspecting
these remaining inconsistencies showed cases like:
(NP-TMP (NML (NNP Nov.) (CD 15))
(, ,)
(CD 1999))
where we were inconsistent in inserting the NML node
because the Penn Treebank sometimes already has
the structure annotated under an NP node. Since we
do not make changes to existing brackets, we cannot
fix these cases. Other inconsistencies are rare, but
will be examined and corrected in a future release.
The annotator made a second pass over Section
00 to correct changes made after the beginning of
the annotation process. Comparing the two passes
can give us some idea of how the annotator changed
as he grew more practiced at the task.
We find that the old and new versions are identi-
cal in 88.65% of NPs, with labelled precision, recall
and F-score being 97.17%, 76.69% and 85.72% re-
spectively. This tells us that there were many brack-
ets originally missed that were added in the second
pass. This is not surprising since the main problem
with how Section 00 was annotated originally was
that company names were not separated from their
post-modifier (such as Corp). Besides this, it sug-
gests that there is not a great deal of difference be-
tween an annotator just learning the task, and one
who has had a great deal of experience with it.
5.4 Named Entity Suggestions
We have also evaluated how well the suggestion fea-
ture of the annotation tool performs. In particular,
we want to determine how useful named entities are
in determining the correct bracketing.
We ran the tool over the original corpus, follow-
ing NE-based suggestions where possible. We find
that when evaluated against our annotations, the F-
score is 50.71%. We need to look closer at the pre-
cision and recall though, as they are quite different.
The precision of 93.84% is quite high. However,
there are many brackets where the entities do not
help at all, and so the recall of this method was only
34.74%. This suggests that a NE feature may help to
identify the correct bracketing in one third of cases.
6 Experiments
Having bracketed NPs in the Penn Treebank, we now
describe our initial experiments on how this addi-
tional level of annotation can be exploited.
6.1 NP Bracketing Data
The obvious first task to consider is noun phrase
bracketing itself. We implement a similar system to
244
CORPUS # ITEMS LEFT RIGHT
Penn Treebank 5582 58.99% 41.01%
Lauer?s 244 66.80% 33.20%
Table 4: Comparison of NP bracketing corpora
N-GRAM MATCH
Unigrams 51.20%
Adjacency bigrams 6.35%
Dependency bigrams 3.85%
All bigrams 5.83%
Trigrams 1.40%
Table 5: Lexical overlap
Lauer (1995), described in Section 3, and report on
results from our own data and Lauer?s original set.
First, we extracted three word noun sequences
from all the ambiguous NPs. If the last three chil-
dren are nouns, then they became an example in our
data set. If there is a NML node containing the first
two nouns then it is left-branching, otherwise it is
right-branching. Because we are only looking at the
right-most part of the NP, we know that we are not
extracting any nonsensical items. We also remove
all items where the nouns are all part of a named
entity to eliminate flat structure cases.
Statistics about the new data set and Lauer?s data
set are given in Table 4. As can be seen, the Penn
Treebank based corpus is significantly larger, and
has a more even mix of left and right-branching noun
phrases. We also measured the amount of lexical
overlap between the two corpora, shown in Table 5.
This displays the percentage of n-grams in Lauer?s
corpus that are also in our corpus. We can clearly
see that the two corpora are quite dissimilar, as even
on unigrams barely half are shared.
6.2 NP Bracketing Results
With our new data set, we began running experi-
ments similar to those carried out in the literature
(Nakov and Hearst, 2005). We implemented both an
adjacency and dependency model, and three differ-
ent association measures: raw counts, bigram proba-
bility, and  . We draw our counts from a corpus of
n-gram counts calculated over 1 trillion words from
the web (Brants and Franz, 2006).
The results from the experiments, on both our and
Lauer?s data set, are shown in Table 6. Our results
ASSOC. MEASURE LAUER PTB
Raw counts, adj. 75.41% 77.46%
Raw counts, dep. 77.05% 68.85%
Probability, adj. 71.31% 76.42%
Probability, dep. 80.33% 69.56%
 , adj. 71.31% 77.93%
  , dep. 74.59% 68.92%
Table 6: Bracketing task, unsupervised results
FEATURES LAUER 10-FOLD CROSS
All features 80.74% 89.91% (1.04%)
Lexical 71.31% 84.52% (1.77%)
n-gram counts 75.41% 82.50% (1.49%)
Probability 72.54% 78.34% (2.11%)
	 75.41% 80.10% (1.71%)
Adjacency model 72.95% 79.52% (1.32%)
Dependency model 78.69% 72.86% (1.48%)
Both models 76.23% 79.67% (1.42%)
-Lexical 79.92% 85.72% (0.77%)
-n-gram counts 80.74% 89.11% (1.39%)
-Probability 79.10% 89.79% (1.22%)
- 	 80.74% 89.79% (0.98%)
-Adjacency model 81.56% 89.63% (0.96%)
-Dependency model 81.15% 89.72% (0.86%)
-Both models 81.97% 89.63% (0.95%)
Table 7: Bracketing task, supervised results
on Lauer?s corpus are similar to those reported pre-
viously, with the dependency model outperforming
the adjacency model on all measures. The bigram
probability scores highest out of all the measures,
while the   score performed the worst.
The results on the new corpus are even more sur-
prising, with the adjacency model outperforming the
dependency model by a wide margin. The 
 mea-
sure gives the highest accuracy, but still only just
outperforms the raw counts. Our analysis shows
that the good performance of the adjacency model
comes from the large number of named entities in
the corpus. When we remove all items that have any
word as an entity, the results change, and the de-
pendency model is superior. We also suspect that
another cause of the unusual results is the different
proportions of left and right-branching NPs.
With a large annotated corpus, we can now run
supervised NP bracketing experiments. We present
two configurations in Table 7: training on our corpus
and testing on Lauer?s set; and performing 10-fold
cross validation using our corpus alone.
The feature set we explore encodes the informa-
tion we used in the unsupervised experiments. Ta-
245
OVERALL ONLY NML JJP NOT NML JJP
PREC. RECALL F-SCORE PREC. RECALL F-SCORE PREC. RECALL F-SCORE
Original 88.93 88.90 88.92 ? ? ? 88.93 88.90 88.92
NML and JJP bracketed 88.63 88.29 88.46 77.93 62.93 69.63 88.85 88.93 88.89
Relabelled brackets 88.17 87.88 88.02 91.93 51.38 65.91 87.86 88.65 88.25
Table 8: Parsing performance
ble 7 shows the performance with: all features, fol-
lowed by the individual features, and finally, after
removing individual features.
The feature set includes: lexical features for each
n-gram in the noun compound; n-gram counts for
unigrams, bigrams and trigrams; raw probability and
 association scores for all three bigrams in the
compound; and the adjacency and dependency re-
sults for all three association measures. We dis-
cretised the non-binary features using an implemen-
tation of Fayyad and Irani?s (1993) algorithm, and
classify using MegaM2.
The results on Lauer?s set demonstrate that the
dependency model performs well by itself but not
with the other features. In fact, a better result comes
from using every feature except those from the de-
pendency and adjacency models. It is also impres-
sive how good the performance is, considering the
large differences between our data set and Lauer?s.
These differences also account for the disparate
cross-validation figures. On this data, the lexical fea-
tures perform the best, which is to be expected given
the nature of the corpus. The best model in this case
comes from using all the features.
6.3 Collins Parsing
We can also look at the impact of our new annota-
tions upon full statistical parsing. We use Bikel?s
implementation (Bikel, 2004) of Collins? parser
(Collins, 1999) in order to carry out these experi-
ments, using the non-deficient Collins settings. The
numbers we give are labelled bracket precision, re-
call and F-scores for all sentences. Bikel mentions
that base-NPs are treated very differently in Collins?
parser, and so it will be interesting to observe the
results using our new annotations.
Firstly, we compare the parser?s performance on
the original Penn Treebank and the new NML and JJP
bracketed version. Table 8 shows that the new brack-
ets make parsing marginally more difficult overall
2Available at http://www.cs.utah.edu/ hal/megam/
(by about 0.5% in F-score).
The performance on only the new NML and JJP
brackets is not very high. This shows the difficulty
of correctly bracketing NPs. Conversely, the figures
for all brackets except NML and JJP are only a tiny
amount less in our extended corpus. This means that
performance for other phrases is hardly changed by
the new NP brackets.
We also ran an experiment where the new NML and
JJP labels were relabelled as NP and ADJP. These
are the labels that would be given if NPs were orig-
inally bracketed with the rest of the Penn Treebank.
This meant the model would not have to discrim-
inate between two different types of noun and ad-
jective structure. The performance, as shown in Ta-
ble 8, was even lower with this approach, suggesting
that the distinction is larger than we anticipated. On
the other hand, the precision on NML and JJP con-
stituents was quite high, so the parser is able to iden-
tify at least some of the structure very well.
7 Conclusion
The work presented in this paper is a first step to-
wards accurate representation of noun phrase struc-
ture in NLP corpora. There are several distinctions
that our annotation currently ignores that we would
like to identify correctly in the future. Firstly, NPs
with genuine flat structure are currently treated as
implicitly right branching. Secondly, there is still
ambiguity in determining the head of a noun phrase.
Although Collins? head finding rules work in most
NPs, there are cases such as IBM Australia where
the head is not the right-most noun. Similarly, ap-
position is very common in the Penn Treebank, in
NPs such as John Smith , IBM president. We would
like to be able to identify these multi-head constructs
properly, rather than simply treating them as a single
entity (or even worse, as two different entities).
Having the correct NP structure also means that
we can now represent the true structure in CCGbank,
one of the problems we described earlier. Transfer-
246
ring our annotations should be fairly simple, requir-
ing just a few changes to how NPs are treated in the
current translation process.
The addition of consistent, gold-standard, noun
phrase structure to a large corpus is a significant
achievement. We have shown that the these anno-
tations can be created in a feasible time frame with
high inter-annotator agreement of 98.52% (measur-
ing exact NP matches). The new brackets cause only
a small drop in parsing performance, and no signifi-
cant decrease on the existing structure. As NEs were
useful for suggesting brackets automatically, we in-
tend to incorporate NE information into statistical
parsing models in the future.
Our annotated corpus can improve the perfor-
mance of any system that relies on NPs from parsers
trained on the Penn Treebank. A Collins? parser
trained on our corpus is now able to identify sub-
NP brackets, making it of use in other NLP systems.
QA systems, for example, will be able to exploit in-
ternal NP structure. In the future, we will improve
the parser?s performance on NML and JJP brackets.
We have provided a significantly larger corpus
for analysing NP structure than has ever been made
available before. This is integrated within perhaps
the most influential corpus in NLP. The large num-
ber of systems trained on Penn Treebank data can all
benefit from the extended resource we have created.
Acknowledgements
We would like to thank Matthew Honnibal, our sec-
ond annotator, who also helped design the guide-
lines; Toby Hawker, for implementing the dis-
cretiser; Mark Lauer for releasing his data; and
the anonymous reviewers for their helpful feed-
back. This work has been supported by the Aus-
tralian Research Council under Discovery Projects
DP0453131 and DP0665973.
References
Ann Bies, Mark Ferguson, Karen Katz, and Robert MacIntyre.
1995. Bracketing guidelines for Treebank II style Penn Tree-
bank project. Technical report, University of Pennsylvania.
Dan Bikel. 2004. On the Parameter Space of Generative Lexi-
calized Statistical Parsing Models. Ph.D. thesis, University
of Pennsylvania.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram version
1. Linguistic Data Consortium.
Ted Briscoe and John Carroll. 2006. Evaluating the accuracy
of an unlexicalized statistical parser on the PARC DepBank.
In Proceedings of the Poster Session of COLING/ACL-06.
Sydney, Australia.
Michael Collins. 1999. Head-Driven Statistical Models for Nat-
ural Language Parsing. Ph.D. thesis, University of Pennsyl-
vania.
Usama M. Fayyad and Keki B. Irani. 1993. Multi-interval dis-
cretization of continuous-valued attributes for classification
learning. In Proceedings of the 13th International Joint Con-
ference on Artifical Intelligence (IJCAI?93), pages 1022?
1029. Chambery, France.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel Antohe.
2005. On the semantics of noun compounds. Journal of
Computer Speech and Language - Special Issue on Multi-
word Expressions, 19(4):313?330.
Julia Hockenmaier. 2003. Data and Models for Statistical Pars-
ing with Combinatory Categorial Grammar. Ph.D. thesis,
University of Edinburgh.
Tracy Holloway King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC700
dependency bank. In Proceedings of the 4th International
Workshop on Linguistically Interpreted Corpora (LINC-03).
Budapest, Hungary.
Seth Kulick, Ann Bies, Mark Libeman, Mark Mandel, Ryan
McDonald, Martha Palmer, Andrew Schein, and Lyle Ungar.
2004. Integrated annotation for biomedical information ex-
traction. In Proceedings of the Human Language Technology
Conference of the North American Chapter of the Associa-
tion for Computational Linguistics. Boston.
Mirella Lapata and Frank Keller. 2004. The web as a base-
line: Evaluating the performance of unsupervised web-based
models for a range of NLP tasks. In Proceedings of the Hu-
man Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Linguis-
tics, pages 121?128. Boston.
Mark Lauer. 1995. Corpus statistics meet the compound noun:
Some empirical results. In Proceedings of the 33rd Annual
Meeting of the Association for Computational Linguistics.
Cambridge, MA.
Mitchell Marcus. 1980. A Theory of Syntactic Recognition for
Natural Language. MIT Press, Cambridge, MA.
Mitchell Marcus, Beatrice Santorini, and Mary Marcinkiewicz.
1993. Building a large annotated corpus of English: The
Penn Treebank. Computational Linguistics, 19(2):313?330.
Preslav Nakov and Marti Hearst. 2005. Search engine statistics
beyond the n-gram: Application to noun compound brack-
eting. In Proceedings of CoNLL-2005, Ninth Conference on
Computational Natural Language Learning. Ann Arbor, MI.
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunk-
ing using transformation-based learning. In Proceedings of
the Third ACL Workshop on Very Large Corpora. Cambridge
MA, USA.
Mark Steedman. 2000. The Syntactic Process. MIT Press, Cam-
bridge, MA.
Ralph Weischedel and Ada Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Technical report, Lin-
guistic Data Consortium.
247
Proceedings of ACL-08: HLT, pages 335?343,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Parsing Noun Phrase Structure with CCG
David Vadas and James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
{dvadas1, james}@it.usyd.edu.au
Abstract
Statistical parsing of noun phrase (NP) struc-
ture has been hampered by a lack of gold-
standard data. This is a significant problem for
CCGbank, where binary branching NP deriva-
tions are often incorrect, a result of the auto-
matic conversion from the Penn Treebank.
We correct these errors in CCGbank using a
gold-standard corpus of NP structure, result-
ing in a much more accurate corpus. We also
implement novel NER features that generalise
the lexical information needed to parse NPs
and provide important semantic information.
Finally, evaluating against DepBank demon-
strates the effectiveness of our modified cor-
pus and novel features, with an increase in
parser performance of 1.51%.
1 Introduction
Internal noun phrase (NP) structure is not recovered
by a number of widely-used parsers, e.g. Collins
(2003). This is because their training data, the Penn
Treebank (Marcus et al, 1993), does not fully anno-
tate NP structure. The flat structure described by the
Penn Treebank can be seen in this example:
(NP (NN lung) (NN cancer) (NNS deaths))
CCGbank (Hockenmaier and Steedman, 2007) is
the primary English corpus for Combinatory Cate-
gorial Grammar (CCG) (Steedman, 2000) and was
created by a semi-automatic conversion from the
Penn Treebank. However, CCG is a binary branch-
ing grammar, and as such, cannot leave NP structure
underspecified. Instead, all NPs were made right-
branching, as shown in this example:
(N
(N/N lung)
(N
(N/N cancer) (N deaths) ) )
This structure is correct for most English NPs and
is the best solution that doesn?t require manual re-
annotation. However, the resulting derivations often
contain errors. This can be seen in the previous ex-
ample, where lung cancer should form a con-
stituent, but does not.
The first contribution of this paper is to correct
these CCGbank errors. We apply an automatic con-
version process using the gold-standard NP data an-
notated by Vadas and Curran (2007a). Over a quar-
ter of the sentences in CCGbank need to be altered,
demonstrating the magnitude of the NP problem and
how important it is that these errors are fixed.
We then run a number of parsing experiments us-
ing our new version of the CCGbank corpus. In
particular, we implement new features using NER
tags from the BBN Entity Type Corpus (Weischedel
and Brunstein, 2005). These features are targeted at
improving the recovery of NP structure, increasing
parser performance by 0.64% F-score.
Finally, we evaluate against DepBank (King et al,
2003). This corpus annotates internal NP structure,
and so is particularly relevant for the changes we
have made to CCGbank. The CCG parser now recov-
ers additional structure learnt from our NP corrected
corpus, increasing performance by 0.92%. Applying
the NER features results in a total increase of 1.51%.
This work allows parsers trained on CCGbank to
model NP structure accurately, and then pass this
crucial information on to downstream systems.
335
(a) (b)
N
N /N
cotton
N
conj
and
N
N /N
acetate
N
fibers
N
N /N
N /N
cotton
N /N [conj ]
conj
and
N /N
acetate
N
fibers
Figure 1: (a) Incorrect CCG derivation from Hockenmaier and Steedman (2007) (b) The correct derivation
2 Background
Parsing of NPs is typically framed as NP bracketing,
where the task is limited to discriminating between
left and right-branching NPs of three nouns only:
? (crude oil) prices ? left-branching
? world (oil prices) ? right-branching
Lauer (1995) presents two models to solve this prob-
lem: the adjacency model, which compares the as-
sociation strength between words 1?2 to words 2?3;
and the dependency model, which compares words
1?2 to words 1?3. Lauer (1995) experiments with a
data set of 244 NPs, and finds that the dependency
model is superior, achieving 80.7% accuracy.
Most NP bracketing research has used Lauer?s
data set. Because it is a very small corpus, most
approaches have been unsupervised, measuring as-
sociation strength with counts from a separate large
corpus. Nakov and Hearst (2005) use search engine
hit counts and extend the query set with typographi-
cal markers. This results in 89.3% accuracy.
Recently, Vadas and Curran (2007a) annotated in-
ternal NP structure for the entire Penn Treebank, pro-
viding a large gold-standard corpus for NP bracket-
ing. Vadas and Curran (2007b) carry out supervised
experiments using this data set of 36,584 NPs, out-
performing the Collins (2003) parser.
The Vadas and Curran (2007a) annotation scheme
inserts NML and JJP brackets to describe the correct
NP structure, as shown below:
(NP (NML (NN lung) (NN cancer) )
(NNS deaths) )
We use these brackets to determine new gold-
standard CCG derivations in Section 3.
2.1 Combinatory Categorial Grammar
Combinatory Categorial Grammar (CCG) (Steed-
man, 2000) is a type-driven, lexicalised theory of
grammar. Lexical categories (also called supertags)
are made up of basic atoms such as S (Sentence)
and NP (Noun Phrase), which can be combined to
form complex categories. For example, a transitive
verb such as bought (as in IBM bought the
company) would have the category: (S\NP)/NP .
The slashes indicate the directionality of arguments,
here two arguments are expected: an NP subject on
the left; and an NP object on the right. Once these
arguments are filled, a sentence is produced.
Categories are combined using combinatory rules
such as forward and backward application:
X /Y Y ? X (>) (1)
Y X \Y ? X (<) (2)
Other rules such as composition and type-raising are
used to analyse some linguistic constructions, while
retaining the canonical categories for each word.
This is an advantage of CCG, allowing it to recover
long-range dependencies without the need for post-
processing, as is the case for many other parsers.
In Section 1, we described the incorrect NP struc-
tures in CCGbank, but a further problem that high-
lights the need to improve NP derivations is shown
in Figure 1. When a conjunction occurs in an NP, a
non-CCG rule is required in order to reach a parse:
conj N ? N (3)
This rule treats the conjunction in the same manner
as a modifier, and results in the incorrect derivation
shown in Figure 1(a). Our work creates the correct
CCG derivation, shown in Figure 1(b), and removes
the need for the grammar rule in (3).
Honnibal and Curran (2007) have also made
changes to CCGbank, aimed at better differentiat-
ing between complements and adjuncts. PropBank
(Palmer et al, 2005) is used as a gold-standard to in-
form these decisions, similar to the way that we use
the Vadas and Curran (2007a) data.
336
(a) (b) (c)
N
N /N
lung
N
N /N
cancer
N
deaths
N
???
???
lung
???
cancer
???
deaths
N
N /N
(N /N )/(N /N )
lung
N /N
cancer
N
deaths
Figure 2: (a) Original right-branching CCGbank (b) Left-branching (c) Left-branching with new supertags
2.2 CCG parsing
The C&C CCG parser (Clark and Curran, 2007b) is
used to perform our experiments, and to evaluate
the effect of the changes to CCGbank. The parser
uses a two-stage system, first employing a supertag-
ger (Bangalore and Joshi, 1999) to propose lexi-
cal categories for each word, and then applying the
CKY chart parsing algorithm. A log-linear model is
used to identify the most probable derivation, which
makes it possible to add the novel features we de-
scribe in Section 4, unlike a PCFG.
The C&C parser is evaluated on predicate-
argument dependencies derived from CCGbank.
These dependencies are represented as 5-tuples:
?hf , f , s, ha, l?, where hf is the head of the predi-
cate; f is the supertag of hf ; s describes which ar-
gument of f is being filled; ha is the head of the
argument; and l encodes whether the dependency is
local or long-range. For example, the dependency
encoding company as the object of bought (as in
IBM bought the company) is represented by:
?bought, (S\NP1 )/NP2 , 2, company,?? (4)
This is a local dependency, where company is fill-
ing the second argument slot, the object.
3 Conversion Process
This section describes the process of converting the
Vadas and Curran (2007a) data to CCG derivations.
The tokens dominated by NML and JJP brackets in
the source data are formed into constituents in the
corresponding CCGbank sentence. We generate the
two forms of output that CCGbank contains: AUTO
files, which represent the tree structure of each sen-
tence; and PARG files, which list the word?word de-
pendencies (Hockenmaier and Steedman, 2005).
We apply one preprocessing step on the Penn
Treebank data, where if multiple tokens are enclosed
by brackets, then a NML node is placed around those
tokens. For example, we would insert the NML
bracket shown below:
(NP (DT a) (-LRB- -LRB-)
(NML (RB very) (JJ negative) )
(-RRB- -RRB-) (NN reaction) )
This simple heuristic captures NP structure not ex-
plicitly annotated by Vadas and Curran (2007a).
The conversion algorithm applies the following
steps for each NML or JJP bracket:
1. Identify the CCGbank lowest spanning node,
the lowest constituent that covers all of the
words in the NML or JJP bracket;
2. flatten the lowest spanning node, to remove the
right-branching structure;
3. insert new left-branching structure;
4. identify heads;
5. assign supertags;
6. generate new dependencies.
As an example, we will follow the conversion pro-
cess for the NML bracket below:
(NP (NML (NN lung) (NN cancer) )
(NNS deaths) )
The corresponding lowest spanning node, which
incorrectly has cancer deaths as a constituent,
is shown in Figure 2(a). To flatten the node, we re-
cursively remove brackets that partially overlap the
NML bracket. Nodes that don?t overlap at all are left
intact. This process results in a list of nodes (which
may or may not be leaves), which in our example is
[lung, cancer, deaths]. We then insert the cor-
rect left-branching structure, shown in Figure 2(b).
At this stage, the supertags are still incomplete.
Heads are then assigned using heuristics adapted
from Hockenmaier and Steedman (2007). Since we
are applying these to CCGbank NP structures rather
than the Penn Treebank, the POS tag based heuristics
are sufficient to determine heads accurately.
337
Finally, we assign supertags to the new structure.
We want to make the minimal number of changes
to the entire sentence derivation, and so the supertag
of the dominating node is fixed. Categories are then
propagated recursively down the tree. For a node
with category X , its head child is also given the cat-
egory X . The non-head child is always treated as
an adjunct, and given the category X /X or X \X as
appropriate. Figure 2(c) shows the final result of this
step for our example.
3.1 Dependency generation
The changes described so far have generated the new
tree structure, but the last step is to generate new de-
pendencies. We recursively traverse the tree, at each
level creating a dependency between the heads of
the left and right children. These dependencies are
never long-range, and therefore easy to deal with.
We may also need to change dependencies reaching
from inside to outside the NP, if the head(s) of the
NP have changed. In these cases we simply replace
the old head(s) with the new one(s) in the relevant
dependencies. The number of heads may change be-
cause we now analyse conjunctions correctly.
In our example, the original dependencies were:
?lung,N /N1 , 1, deaths,?? (5)
?cancer,N /N1 , 1, deaths,?? (6)
while after the conversion process, (5) becomes:
?lung, (N /N1 )/(N /N )2 , 2, cancer,?? (7)
To determine that the conversion process worked
correctly, we manually inspected its output for
unique tree structures in Sections 00?07. This iden-
tified problem cases to correct, such as those de-
scribed in the following section.
3.2 Exceptional cases
Firstly, when the lowest spanning node covers the
NML or JJP bracket exactly, no changes need to be
made to CCGbank. These cases occur when CCG-
bank already received the correct structure during
the original conversion process. For example, brack-
ets separating a possessive from its possessor were
detected automatically.
A more complex case is conjunctions, which do
not follow the simple head/adjunct method of as-
signing supertags. Instead, conjuncts are identified
during the head-finding stage, and then assigned the
supertag dominating the entire coordination. Inter-
vening non-conjunct nodes are given the same cate-
gory with the conj feature, resulting in a derivation
that can be parsed with the standard CCGbank bi-
nary coordination rules:
conj X ? X[conj] (8)
X X[conj] ? X (9)
The derivation in Figure 1(b) is produced by these
corrections to coordination derivations. As a result,
applications of the non-CCG rule shown in (3) have
been reduced from 1378 to 145 cases.
Some POS tags require special behaviour. De-
terminers and possessive pronouns are both usually
given the supertag NP [nb]/N , and this should not
be changed by the conversion process. Accordingly,
we do not alter tokens with POS tags of DT and PRP$.
Instead, their sibling node is given the category N
and their parent node is made the head. The parent?s
sibling is then assigned the appropriate adjunct cat-
egory (usually NP\NP ). Tokens with punctuation
POS tags1 do not have their supertag changed either.
Finally, there are cases where the lowest span-
ning node covers a constituent that should not be
changed. For example, in the following NP:
(NP
(NML (NN lower) (NN court) )
(JJ final) (NN ruling) )
with the original CCGbank lowest spanning node:
(N (N/N lower)
(N (N/N court)
(N (N/N final) (N ruling) ) ) )
the final ruling node should not be altered.
It may seem trivial to process in this case, but
consider a similarly structured NP: lower court
ruling that the U.S. can bar the use
of... Our minimalist approach avoids reanalysing
the many linguistic constructions that can be dom-
inated by NPs, as this would reinvent the creation
of CCGbank. As a result, we only flatten those
constituents that partially overlap the NML or JJP
bracket. The existing structure and dependencies of
other constituents are retained. Note that we are still
converting every NML and JJP bracket, as even in
the subordinate clause example, only the structure
around lower court needs to be altered.
1period, comma, colon, and left and right bracket.
338
the world ?s largest aid donor
NP [nb]/N N /N N NP\NP NP\NP NP\NP
>
N
>
NP
<
NP
<
NP
<
NP
the world ?s largest aid donor
NP [nb]/N N (NP [nb]/N )\NP N /N N /N N
> >
NP N
< >
NP [nb]/N N
>
NP
(a) (b)
Figure 3: CCGbank derivations for possessives
# %
Possessive 224 43.75
Left child contains DT/PRP$ 87 16.99
Couldn?t assign to non-leaf 66 12.89
Conjunction 35 6.84
Automatic conversion was correct 26 5.08
Entity with internal brackets 23 4.49
DT 22 4.30
NML/JJP bracket is an error 12 2.34
Other 17 3.32
Total 512 100.00
Table 1: Manual analysis
3.3 Manual annotation
A handful of problems that occurred during the con-
version process were corrected manually. The first
indicator of a problem was the presence of a pos-
sessive. This is unexpected, because possessives
were already bracketed properly when CCGbank
was originally created (Hockenmaier, 2003, ?3.6.4).
Secondly, a non-flattened node should not be as-
signed a supertag that it did not already have. This
is because, as described previously, a non-leaf node
could dominate any kind of structure. Finally, we
expect the lowest spanning node to cover only the
NML or JJP bracket and one more constituent to the
right. If it doesn?t, because of unusual punctuation
or an incorrect bracket, then it may be an error. In
all these cases, which occur throughout the corpus,
we manually analysed the derivation and fixed any
errors that were observed.
512 cases were flagged by this approach, or
1.90% of the 26,993 brackets converted to CCG. Ta-
ble 1 shows the causes of these problems. The most
common cause of errors was possessives, as the con-
version process highlighted a number of instances
where the original CCGbank analysis was incorrect.
An example of this error can be seen in Figure 3(a),
where the possessive doesn?t take any arguments.
Instead, largest aid donor incorrectly modifies the
NP one word at a time. The correct derivation after
manual analysis is in (b).
The second-most common cause occurs when
there is apposition inside the NP. This can be seen
in Figure 4. As there is no punctuation on which
to coordinate (which is how CCGbank treats most
appositions) the best derivation we can obtain is to
have Victor Borge modify the preceding NP.
The final step in the conversion process was
to validate the corpus against the CCG grammar,
first by those productions used in the existing
CCGbank, and then against those actually licensed
by CCG (with pre-existing ungrammaticalities re-
moved). Sixteen errors were identified by this pro-
cess and subsequently corrected by manual analysis.
In total, we have altered 12,475 CCGbank sen-
tences (25.5%) and 20,409 dependencies (1.95%).
4 NER features
Named entity recognition (NER) provides informa-
tion that is particularly relevant for NP parsing, sim-
ply because entities are nouns. For example, know-
ing that Air Force is an entity tells us that Air
Force contract is a left-branching NP.
Vadas and Curran (2007a) describe using NE tags
during the annotation process, suggesting that NER-
based features will be helpful in a statistical model.
There has also been recent work combining NER and
parsing in the biomedical field. Lewin (2007) exper-
iments with detecting base-NPs using NER informa-
tion, while Buyko et al (2007) use a CRF to identify
339
a guest comedian Victor Borge
NP [nb]/N N /N N /N N /N N
>
N
>
N
>
N
>
NP
a guest comedian Victor Borge
NP [nb]/N N /N N (NP\NP)/(NP\NP) NP\NP
> >
N NP\NP
>
NP
<
NP
(a) (b)
Figure 4: CCGbank derivations for apposition with DT
coordinate structure in biological named entities.
We draw NE tags from the BBN Entity Type
Corpus (Weischedel and Brunstein, 2005), which
describes 28 different entity types. These in-
clude the standard person, location and organization
classes, as well person descriptions (generally occu-
pations), NORP (National, Other, Religious or Po-
litical groups), and works of art. Some classes also
have finer-grained subtypes, although we use only
the coarse tags in our experiments.
Clark and Curran (2007b) has a full description
of the C&C parser?s pre-existing features, to which
we have added a number of novel NER-based fea-
tures. Many of these features generalise the head
words and/or POS tags that are already part of the
feature set. The results of applying these features
are described in Sections 5.3 and 6.
The first feature is a simple lexical feature, de-
scribing the NE tag of each token in the sentence.
This feature, and all others that we describe here,
are not active when the NE tag(s) are O, as there is no
NER information from tokens that are not entities.
The next group of features is based on the lo-
cal tree (a parent and two child nodes) formed by
every grammar rule application. We add a fea-
ture where the rule being applied is combined with
the parent?s NE tag. For example, when joining
two constituents2 : ?five, CD, CARD, N /N ? and
?Europeans, NNPS, NORP, N ?, the feature is:
N ? N /N N + NORP
as the head of the constituent is Europeans.
In the same way, we implement features that com-
bine the grammar rule with the child nodes. There
are already features in the model describing each
combination of the children?s head words and POS
tags, which we extend to include combinations with
2These 4-tuples are the node?s head, POS, NE, and supertag.
the NE tags. Using the same example as above, one
of the new features would be:
N ? N /N N + CARD + NORP
The last group of features is based on the NE
category spanned by each constituent. We iden-
tify constituents that dominate tokens that all have
the same NE tag, as these nodes will not cause a
?crossing bracket? with the named entity. For ex-
ample, the constituent Force contract, in the
NP Air Force contract, spans two different
NE tags, and should be penalised by the model. Air
Force, on the other hand, only spans ORG tags, and
should be preferred accordingly.
We also take into account whether the constituent
spans the entire named entity. Combining these
nodes with others of different NE tags should not
be penalised by the model, as the NE must combine
with the rest of the sentence at some point.
These NE spanning features are implemented as
the grammar rule in combination with the parent
node or the child nodes. For the former, one fea-
ture is active when the node spans the entire entity,
and another is active in other cases. Similarly, there
are four features for the child nodes, depending on
whether neither, the left, the right or both nodes span
the entire NE. As an example, if the Air Force
constituent were being joined with contract, then
the child feature would be:
N ? N /N N + LEFT + ORG + O
assuming that there are more O tags to the right.
5 Experiments
Our experiments are run with the C&C CCG parser
(Clark and Curran, 2007b), and will evaluate the
changes made to CCGbank, as well as the effective-
ness of the NER features. We train on Sections 02-
21, and test on Section 00.
340
PREC RECALL F-SCORE
Original 91.85 92.67 92.26
NP corrected 91.22 92.08 91.65
Table 2: Supertagging results
PREC RECALL F-SCORE
Original 85.34 84.55 84.94
NP corrected 85.08 84.17 84.63
Table 3: Parsing results with gold-standard POS tags
5.1 Supertagging
Before we begin full parsing experiments, we eval-
uate on the supertagger alone. The supertagger is
an important stage of the CCG parsing process, its
results will affect performance in later experiments.
Table 2 shows that F-score has dropped by 0.61%.
This is not surprising, as the conversion process has
increased the ambiguity of supertags in NPs. Previ-
ously, a bare NP could only have a sequence of N /N
tags followed by a final N . There are now more
complex possibilities, equal to the Catalan number
of the length of the NP.
5.2 Initial parsing results
We now compare parser performance on our NP cor-
rected version of the corpus to that on original CCG-
bank. We are using the normal-form parser model
and report labelled precision, recall and F-score for
all dependencies. The results are shown in Table 3.
The F-score drops by 0.31% in our new version of
the corpus. However, this comparison is not entirely
fair, as the original CCGbank test data does not in-
clude the NP structure that the NP corrected model is
being evaluated on. Vadas and Curran (2007a) expe-
rienced a similar drop in performance on Penn Tree-
bank data, and noted that the F-score for NML and
JJP brackets was about 20% lower than the overall
figure. We suspect that a similar effect is causing the
drop in performance here.
Unfortunately, there are no explicit NML and JJP
brackets to evaluate on in the CCG corpus, and so an
NP structure only figure is difficult to compute. Re-
call can be calculated by marking those dependen-
cies altered in the conversion process, and evaluating
only on them. Precision cannot be measured in this
PREC RECALL F-SCORE
Original 83.65 82.81 83.23
NP corrected 83.31 82.33 82.82
Table 4: Parsing results with automatic POS tags
PREC RECALL F-SCORE
Original 86.00 85.15 85.58
NP corrected 85.71 84.83 85.27
Table 5: Parsing results with NER features
way, as NP dependencies remain undifferentiated in
parser output. The result is a recall of 77.03%, which
is noticeably lower than the overall figure.
We have also experimented with using automat-
ically assigned POS tags. These tags are accurate
with an F-score of 96.34%, with precision 96.20%
and recall 96.49%. Table 4 shows that, unsur-
prisingly, performance is lower without the gold-
standard data. The NP corrected model drops an ad-
ditional 0.1% F-score over the original model, sug-
gesting that POS tags are particularly important for
recovering internal NP structure. Evaluating NP de-
pendencies only, in the same manner as before, re-
sults in a recall figure of 75.21%.
5.3 NER features results
Table 5 shows the results of adding the NER fea-
tures we described in Section 4. Performance has
increased by 0.64% on both versions of the corpora.
It is surprising that the NP corrected increase is not
larger, as we would expect the features to be less
effective on the original CCGbank. This is because
incorrect right-branching NPs such as Air Force con-
tract would introduce noise to the NER features.
Table 6 presents the results of using automati-
cally assigned POS and NE tags, i.e. parsing raw
text. The NER tagger achieves 84.45% F-score on
all non-O classes, with precision being 78.35% and
recall 91.57%. We can see that parsing F-score
has dropped by about 2% compared to using gold-
standard POS and NER data, however, the NER fea-
tures still improve performance by about 0.3%.
341
PREC RECALL F-SCORE
Original 83.92 83.06 83.49
NP corrected 83.62 82.65 83.14
Table 6: Parsing results with automatic POS and NE tags
6 DepBank evaluation
One problem with the evaluation in the previous sec-
tion, is that the original CCGbank is not expected to
recover internal NP structure, making its task eas-
ier and inflating its performance. To remove this
variable, we carry out a second evaluation against
the Briscoe and Carroll (2006) reannotation of Dep-
Bank (King et al, 2003), as described in Clark and
Curran (2007a). Parser output is made similar to the
grammatical relations (GRs) of the Briscoe and Car-
roll (2006) data, however, the conversion remains
complex. Clark and Curran (2007a) report an upper
bound on performance, using gold-standard CCG-
bank dependencies, of 84.76% F-score.
This evaluation is particularly relevant for NPs, as
the Briscoe and Carroll (2006) corpus has been an-
notated for internal NP structure. With our new ver-
sion of CCGbank, the parser will be able to recover
these GRs correctly, where before this was unlikely.
Firstly, we show the figures achieved using gold-
standard CCGbank derivations in Table 7. In the NP
corrected version of the corpus, performance has in-
creased by 1.02% F-score. This is a reversal of the
results in Section 5, and demonstrates that correct
NP structure improves parsing performance, rather
than reduces it. Because of this increase to the up-
per bound of performance, we are now even closer
to a true formalism-independent evaluation.
We now move to evaluating the C&C parser it-
self and the improvement gained by the NER fea-
tures. Table 8 show our results, with the NP cor-
rected version outperforming original CCGbank by
0.92%. Using the NER features has also caused an
increase in F-score, giving a total improvement of
1.51%. These results demonstrate how successful
the correcting of NPs in CCGbank has been.
Furthermore, the performance increase of 0.59%
on the NP corrected corpus is more than the 0.25%
increase on the original. This demonstrates that NER
features are particularly helpful for NP structure.
PREC RECALL F-SCORE
Original 86.86 81.61 84.15
NP corrected 87.97 82.54 85.17
Table 7: DepBank gold-standard evaluation
PREC RECALL F-SCORE
Original 82.57 81.29 81.92
NP corrected 83.53 82.15 82.84
Original, NER 82.87 81.49 82.17
NP corrected, NER 84.12 82.75 83.43
Table 8: DepBank evaluation results
7 Conclusion
The first contribution of this paper is the application
of the Vadas and Curran (2007a) data to Combina-
tory Categorial Grammar. Our experimental results
have shown that this more accurate representation
of CCGbank?s NP structure increases parser perfor-
mance. Our second major contribution is the intro-
duction of novel NER features, a source of semantic
information previously unused in parsing.
As a result of this work, internal NP structure is
now recoverable by the C&C parser, a result demon-
strated by our total performance increase of 1.51%
F-score. Even when parsing raw text, without gold
standard POS and NER tags, our approach has re-
sulted in performance gains.
In addition, we have made possible further in-
creases to NP structure accuracy. New features can
now be implemented and evaluated in a CCG pars-
ing context. For example, bigram counts from a very
large corpus have already been used in NP bracket-
ing, and could easily be applied to parsing. Sim-
ilarly, additional supertagging features can now be
created to deal with the increased ambiguity in NPs.
Downstream NLP components can now exploit the
crucial information in NP structure.
Acknowledgements
We would like to thank Mark Steedman and
Matthew Honnibal for help with converting the NP
data to CCG; and the anonymous reviewers for their
helpful feedback. This work has been supported by
the Australian Research Council under Discovery
Project DP0665973.
342
References
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237?265.
Ted Briscoe and John Carroll. 2006. Evaluating the accu-
racy of an unlexicalized statistical parser on the PARC
DepBank. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 41?48. Syd-
ney, Australia.
Ekaterina Buyko, Katrin Tomanek, and Udo Hahn.
2007. Resolution of coordination ellipses in biological
named entities with conditional random fields. In Pro-
ceedings of the 10th Conference of the Pacific Associa-
tion for Computational Linguistics (PACLING-2007),
pages 163?171. Melbourne, Australia.
Stephen Clark and James R. Curran. 2007a. Formalism-
independent parser evaluation with CCG and Dep-
Bank. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL-
07), pages 248?255. Prague, Czech Republic.
Stephen Clark and James R. Curran. 2007b. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493?552.
Michael Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
Julia Hockenmaier. 2003. Data and Models for Statis-
tical Parsing with Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh.
Julia Hockenmaier and Mark Steedman. 2005. CCGbank
manual. Technical Report MS-CIS-05-09, Department
of Computer and Information Science, University of
Pennsylvania.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Compu-
tational Linguistics, 33(3):355?396.
Matthew Honnibal and James R. Curran. 2007. Im-
proving the complement/adjunct distinction in CCG-
bank. In Proceedings of the 10th Conference of
the Pacific Association for Computational Linguistics
(PACLING-07), pages 210?217. Melbourne, Australia.
Tracy Holloway King, Richard Crouch, Stefan Riezler,
Mary Dalrymple, and Ronald M. Kaplan. 2003. The
PARC700 dependency bank. In Proceedings of the 4th
International Workshop on Linguistically Interpreted
Corpora (LINC-03). Budapest, Hungary.
Mark Lauer. 1995. Corpus statistics meet the compound
noun: Some empirical results. In Proceedings of the
33rd Annual Meeting of the Association for Computa-
tional Linguistics, pages 47?54. Cambridge, MA.
Ian Lewin. 2007. BaseNPs that contain gene names: do-
main specificity and genericity. In Biological, trans-
lational, and clinical language processing workshop,
pages 163?170. Prague, Czech Republic.
Mitchell Marcus, Beatrice Santorini, and Mary
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Preslav Nakov and Marti Hearst. 2005. Search engine
statistics beyond the n-gram: Application to noun
compound bracketing. In Proceedings of the 9th Con-
ference on Computational Natural Language Learning
(CoNLL-05), pages 17?24. Ann Arbor, MI.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1):71?106.
Mark Steedman. 2000. The Syntactic Process. MIT Press,
Cambridge, MA.
David Vadas and James R. Curran. 2007a. Adding noun
phrase structure to the Penn Treebank. In Proceed-
ings of the 45th Annual Meeting of the Association of
Computational Linguistics (ACL-07), pages 240?247.
Prague, Czech Republic.
David Vadas and James R. Curran. 2007b. Large-scale
supervised models for noun phrase bracketing. In Pro-
ceedings of the 10th Conference of the Pacific Associa-
tion for Computational Linguistics (PACLING-2007),
pages 104?112. Melbourne, Australia.
Ralph Weischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus. Technical
report.
343
