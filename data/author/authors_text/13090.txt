Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 43?51,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
You talking to me? A predictive model for zero auxiliary constructions
Andrew Caines
Computation, Cognition & Language Group
RCEAL, University of Cambridge, UK
apc38@cam.ac.uk
Paula Buttery
Computation, Cognition & Language Group
RCEAL, University of Cambridge, UK
pjb48@cam.ac.uk
Abstract
As a consequence of the established prac-
tice to prefer training data obtained from
written sources, NLP tools encounter
problems in handling data from the spo-
ken domain. However, accurate models
of spoken data are increasingly in demand
for naturalistic speech generation and ma-
chine translations in speech-like contexts
(such as chat windows and SMS). There
is a widely held assumption in the lin-
guistic field that spoken language is an
impoverished form of written language.
However, we show that spoken data is
not unpredictably irregular and that lan-
guage models can benefit from detailed
consideration of spoken language features.
This paper considers one specific con-
struction which is largely restricted to the
spoken domain - the ZERO AUXILIARY -
and makes a predictive model of that con-
struction for native speakers of British En-
glish. The model can predict zero auxil-
iary occurrence in the BNC with 96.9%
accuracy. We will demonstrate how this
model can be integrated into existing pars-
ing tools, increasing the number of suc-
cessful parses for this zero auxiliary con-
struction by around 30%, and thus improv-
ing the performance of NLP applications
which rely on parsing.
1 Introduction
Up to this point, statistical Natural Language Pro-
cessing (NLP) tools have generally been trained
on corpora that are representative of written rather
than spoken language. A major factor behind this
decision to use written data is that it is far easier to
collect than spoken data. Newswire, for instance,
may be harvested readily and in abundance. Once
collected, written language requires relatively lit-
tle processing before it can be used for training a
statistical model.
Processing of spoken data, on the other hand,
involves at the very least transcription - which usu-
ally requires a human transcriber. Since transcrip-
tion is a slow and laborious task, the collection of
spoken data is highly resource intensive. But this
relative difficulty in collection is not the only rea-
son that spoken language data has been sidelined.
Had spoken data been considered to be crucial to
the production of NLP applications greater efforts
might have been made to obtain it. However, on
account of some of its characteristic features such
as hesitations, interruptions and ellipsis, spoken
language is often dismissed as nothing more than
a noisy approximation to ?real? or ?intended? lan-
guage.
In some forums, written language is held up
as an idealised form of language toward which
speakers aspire and onto which spoken lan-
guage should be retrofitted. This is an arte-
fact of the theoretical notion of a ?competence?-
?performance? dichotomy (Chomsky 1965) with
the latter deemed irrelevant and ignored in main-
stream linguistic research.
The consequence of the established practice to
sideline spoken data is that NLP tools are inher-
ently error prone when handling data from the spo-
ken domain. With increasing calls for speech to be
considered the primary form of language and to be
treated as such (Sampson 2001: 7 1; Cerma?k 2009:
115 2; Haugh 2009: 74 3) and a growing trend for
NLP techniques to be integrated into cognitive and
neurolinguistic research as well as forensic appli-
1Speech is ?unquestionably the more natural, basic mode
of language behaviour?.
2?From a linguistic point of view, spoken corpora should
be primary for research but that has not been the case so far?.
3Haugh observes that ?spoken language and interaction
lie at the core of human experience? but bemoans the ?relative
neglect of spoken language in corpora to date?.
43
cations, there are now compelling reasons to ex-
amine spoken data more closely. Accurate mod-
els of spoken data are increasingly in demand for
naturalistic speech generation and machine trans-
lations in speech-like contexts (such as human-
machine dialogue, chat windows and SMS).
The main research aim of our work is to show
that spoken data should not be considered error
prone and therefore unpredictably irregular. We
show that language models can be improved in in-
crements as we deepen our understanding of spo-
ken language features. We investigate ZERO AUX-
ILIARY progressive aspect constructions - those
which do not feature the supposedly obligatory
auxiliary verb, as in (1a) below (cf. 1b):
(1a) What you doing? Who you looking
for? You been working?
(1b) What are you doing? Who are you
looking for? Have you been working?
The zero auxiliary is a non-standard feature
which for the most part is known to be restricted
to speech. A corpus study of spoken British En-
glish indicates that in progressive aspect interroga-
tives with second person subjects (as in (1) above)
the auxiliary occurs in zero form in 27% of con-
structions found. The equivalent figure from the
written section of the corpus is just 5.4%. Con-
sequently, existing NLP techniques - since they
are based on written training data - are unlikely
to deal appropriately with zero auxiliary construc-
tions. We report below on the corpus study in full
and use the results of logistic regression to design
a predictive model of zero auxiliary occurrence
in spoken English. The model is based on con-
textual grammatical features and can predict zero
auxiliary occurrence in the British National Cor-
pus (BNC; 2007) with 96.9% accuracy. Finally,
we discuss how this model can be used to improve
the performance of NLP techniques in the spoken
domain, demonstrating its implementation in the
RASP system (Robust Accurate Statistical Pars-
ing; (Briscoe, Carroll and Watson, 2006)).
This paper underlines why awareness of non-
standard linguistic features matters. Targeted data
extraction from large corpus resources allows the
construction of more informed language models
which have been trained on naturalistic spoken
usage rather than standard and restricted rules of
written language. Such work has only been made
possible with the advent of large spoken language
corpora such as the BNC. Even so, the resource-
heavy nature of spoken data collection means that
speech transcriptions constitute only one tenth of
this 100 million word corpus 4. Nevertheless, it
is an invaluable resource made up of a range of
speech genres including spontaneous face-to-face
conversation, a fact which makes it unique among
corpora. Since conversational dialogue is the pre-
dominant language medium, the BNC offers the
best chance of modelling speech as it occurs natu-
rally.
This work has important implications for both
computational and theoretical linguistics. On the
one hand, we can improve various NLP techniques
with more informed language models, and on the
other hand we are reminded that the space of
grammatical possibility is not restricted and that
continued empirical investigation is key in order
to arrive at the fullest possible description of lan-
guage use.
2 Spoken and written language
In the modern mainstream fields of linguistic
research, based on Chomsky?s ?ideal speaker-
listener? (1965), spoken language has been all
too easily dismissed from consideration on the
grounds that it is more error-prone and less impor-
tant than written language. In this idealisation, the
speaker-listener is ?unaffected by such grammati-
cally irrelevant conditions as memory limitations,
distractions, shifts of attention and interest, and er-
rors (random or characteristic)? (Chomsky, 1965).
The ?errors? Chomsky refers to are features of
speech production such as pauses, filled silence,
hesitation, repetition and elision, or of dialogue
such as backgrounding, overlap and truncation be-
tween speakers. Thus ?error? is essentially here de-
fined as that which is not normally found in well-
formed written data, that which is ?noisy? and ?un-
predictable?. It is on these grounds - the gram-
matical rigidity of the written medium relative to
speech - that the divide between spoken and writ-
ten language modelling has grown up.
The opposing, usage-based view is that spoken
language is systematic and that it should be mod-
elled as it is rather than as a crude approximation
of the written form. On this view, the speech pro-
duction and dialogue features listed above are not
4Cerma?k estimates our experience with each language
medium is in fact this ratio in reverse - 90:10 spoken to writ-
ten (2009: 115).
44
considered mistakes but ?regular products of the
system of spoken English? (Halliday, 1994). ?Er-
ror? is thus seen as a misnomer for these features
because they are in fact all regular in some way.
For example, the fact that people tend to put filler
pauses in specific places.
We propose a middle way: that which builds
on the NLP tools available, even though they are
trained on written data, and on top of these models
the features of spoken language as ?noise? in the
communicative channel. This is a pragmatic ap-
proach which recognises the considerable amount
of work underpinning existing NLP tools and sees
no value in discarding these and starting again
from scratch. We demonstrate that spoken lan-
guage is model-able and predictable, even with
a feature which would not be seen as ?correct?
in written form. For practical purposes we need
to recognise the regularities in the apparently ?in-
correct? features of speech and build these into
the functioning language models we already have
through statistical analysis of corpus distributions
and appropriate adjustment to parser tools.
3 The zero auxiliary construction
According to standard grammatical rules, the aux-
iliary verb is an obligatory feature of progressive
aspect constructions. However, this rule is based
on norms of written language and is in fact not al-
ways adhered to in the production of speech. As a
result, some progressive constructions do not fea-
ture an auxiliary verb. These are termed ?zero aux-
iliary? constructions and have been previously ex-
amined in studies of dialect (Labov, 1969; Ander-
sen, 1995) and first language acquisition (Brown,
1973; Rizzi, 1993/1994; Wexler, 1994; Lieven et
al, 2003; Wilson, 2003; Theakston et al 2005).
There are copious anecdotal examples of the
zero auxiliary:
(2) You talking to me? Travis Bickle in
Taxi Driver (1976).
(3) Where he going? Avon Barksdale
in The Wire, Season 1: ?Game Day?
(2002).
(4) What you doing? Holly Golightly in
Breakfast at Tiffany?s (1961).
Natural language data taken from the spoken
section of the British National Corpus (sBNC)
shows that the zero auxiliary features in 1330
(27%) of the 4923 second person progressive in-
terrogative constructions; as in (1), (2), (4) above.
In first person singular declaratives (cf. (5a) and
(5b)), in contrast, the proportion of zero auxiliary
occurrence is just 0.9% (158 of 17,838 construc-
tions). This already indicates the way that the zero
auxiliary occurs in predictable contexts and how
grammatical properties will feature in the predic-
tive model.
(5a) What I saying? I annoying you?
Why I doing this?
(5b) What am I saying? Am I annoying
you? Why am I doing this?
Subject person, subject number, subject type
(pronoun or other noun) and clause type (declar-
ative or interrogative) are four of the eight syntac-
tic properties incorporated in the predictive model.
The four other properties are clause tense (6), per-
fect or non-perfect aspect (7), clause polarity (8)
and presence or absence of subject (9).
(6) You are debugging. You were de-
bugging.
(7) We have been looking for a present.
We are looking for a present.
(8) She is watching the grand prix. He
is not watching the grand prix.
(9) I am going to town in a minute. Go-
ing to town in a minute.
We employ logistic regression to investigate the
precise nature of the relationships between zero
auxiliary use and these various linguistic vari-
ables. This allows us to build a predictive model
of zero auxiliary occurrence in spoken language
which will be useful for several reasons relating
to parsing of natural spoken language. Firstly,
for automatic parsing of spoken data being able
to predict when a zero auxiliary is likely to oc-
cur enables the parser to relax its normal rules
which are based on written standards. Secondly,
as technology improves and interaction with com-
puters becomes more humanistic the need to repli-
cate human-like communication increases in im-
portance: by knowing in which contexts the aux-
iliary verb might be absent, researchers can build
a language model which is more realistic and so
the user experience is improved and made more
naturalistic. Thirdly, a missing auxiliary might be
problematic for machine translation since it could
45
result in the loss of tense and aspect information,
but with the ability to predict where a zero auxil-
iary might occur, the auxiliary can be restored so
that translation can be performed with appropriate
tense and aspect.
For all these reasons, the zero auxiliary in spo-
ken English is an appropriate case study for find-
ing the common ground between NLP and Lin-
guistics. Awareness of this particular linguis-
tic phenomenon through corpus study allows the
construction of more informed language models
which in turn enhance relevant NLP techniques.
The cross-pollination of research from NLP and
linguistics benefits both fields and ties in with the
emergence of linguistic theories that ?conceive of
structure as gradient, malleable and probabilistic?
and incorporate ?knowledge of the frequency and
probability of use of these categories in speakers?
experience? (Tily et al 2009). These are collec-
tively known as ?usage-based? approaches to lan-
guage theory and are exerting a growing influ-
ence on the field (e.g. Barlow and Kemmer 2000;
Bybee and Hopper 2001; Bod, Hay and Jannedy
2003).
4 Corpus study
Training data was obtained through manual anno-
tation of progressive constructions in the British
National Corpus (2007). A preliminary study of
interrogatives with second person subjects con-
firmed that the zero auxiliary is more a feature
of the spoken rather than the written domain (Ta-
ble 1). Therefore a focus on the spoken section of
the corpus (sBNC) was justified and so we under-
took a comprehensive study of all progressive con-
structions in sBNC. The genres contained in sBNC
include a range of settings and levels of formality
- from academic lectures to radio programmes to
spontaneous, face-to-face conversation.
We extracted 93,253 sentences featuring a pro-
gressive construction from sBNC and each was
manually annotated for auxiliary realisation and
the eight syntactic properties described in Ta-
ble 2. In Table 3 the progressive constructions are
classified by auxiliary realisation. With approxi-
mately 4.2% occurrence in progressive construc-
tions, zero auxiliaries are a low frequency feature
of spoken language but ones which are significant
for the fact that existing NLP tools cannot suc-
cessfully parse them, thus one in twenty-five pro-
gressive constructions will not be fully parsed. We
Corpus Auxiliary
Full Contracted Zero
wBNC 3220 27 187
sBNC 3498 95 1330
Table 1: Auxiliary realisation in second person
progressive interrogatives in the BNC.
use the annotated corpus of these progressive con-
structions to design the predictive model described
below.
Properties Value encodings
Aux realisation
Zero auxiliary full(0), contracted(1), zero(2)
Variables
Subject person 1st(1), 2nd(2), 3rd(3)
Subject number singular(0), plural(1)
Subject type other noun(0), pronoun(1)
Subj supplied zero subj(0), subj supplied(1)
Clause type declarative(0), interrogative(1)
Clause tense present(0), past(1)
Perfect aspect non-perfect(0), perfect(1)
Polarity positive(0), negative(1)
Table 2: Syntactic features and their encodings in
the annotated sBNC Progressive Corpus
5 Model
To predict the zero auxiliary in spoken language
we use logistic regression. To train this model
we took a 90% sample from our corpus of 93,253
progressive constructions extracted from the spo-
ken section of the BNC, as described above and in
Caines 2010. The dataset was split into two cat-
egories: those sentences which exhibited the zero
auxiliary and those which did not5. A logistic re-
gression was then performed to ascertain the prob-
ability of category membership using the eight
previously described syntactic properties. Note
that subject person is arguably not a scalar vari-
5Contracted auxiliaries thus belong in the ?not zero auxil-
iary? category.
Corpus Full Contracted Zero
sBNC 38,015 51,295 3943
Table 3: Auxiliary realisation in progressive con-
structions in sBNC.
46
able and therefore is re-analysed as three boolean
variables with separate binary values for use of the
first, second and third person. However, the three
subject person variables are dependent (ie. If the
subject is not first or second person it will be in the
third). Thus the eight syntactic properties become
nine explanatory variables in the predictive model,
as reported in Table 4.
Corpus Predictor Coefficient
subject person: 1st 0.171
subject person: 2nd 1.280
plural subject -0.300
pronoun subject -0.470
zero subject 5.711
interrogative clause 2.139
past tense clause -4.852
perfect aspect -0.280
negated clause -1.163
constant -4.033
Table 4: Predictor coefficients for the presence of
a zero auxiliary construction.
5.1 Model Evaluation
The logistic function is defined by:
f(Z) =
1
1 + e?z
(1)
The variable z is representative of the set of pre-
dictors and is defined by:
z = ?0 + ?1x1 + ?2x2 + . . .+ ?kxk (2)
where ?0, ?1, ?2 ... ?k are the regression coeffi-
cients of predictors x1, x2 .. xk respectively. The
predictors explored in this paper are encodings of
the syntactic properties of the annotated sentences.
The predictors and their encodings are indicated in
Table 2.
The logistic function is constrained to values
between 0 and 1 and represents the probability
of membership of one of the two categories (zero
auxiliary or auxiliary supplied). In our case an
f(z) > 0.5 indicates that there is likely to be a
zero auxiliary.
The logistic function defined by the coefficients
in Table 4 is able to predict correct category mem-
bership for 96.9% of the sentences in the annotated
corpus. All coefficients are highly significant to
the logistic function (p<0.001) with the exception
of perfect aspect and first person subject - which
are both significant nevertheless (p<0.05).
For this model, positive coefficients indicate
that the associated syntactic properties raise the
probability of a zero auxiliary occurring. Large
coefficients more strongly influence the probabil-
ity of the zero auxiliary whereas near-zero coeffi-
cients have little influence. From the coefficients
in Table 4 we see that the strongest predictor of
a zero auxiliary is the occurrence of a zero sub-
ject (as in the utterance, ?leaving now.?). An inter-
rogative utterance is also a good candidate, as is
the second person subject (e.g. ?you eating those
olives??). However, a past tense utterance is an un-
likely candidate for a zero auxiliary construction,
as is a negated utterance.
6 Discussion ? using the predictive
model to aid parsing
As mentioned above, since parsers are trained on
written data they can often display poor perfor-
mance on text transcribed from the spoken do-
main. From the results of our corpus study we
know that the zero auxiliary occurs in approxi-
mately 4.2% of progressive constructions in spo-
ken language and we can extrapolate that it will
occur in less than 1% (approximately 0.8%) of
all progressive constructions in written language.
A statistical parser trained on written language
will therefore be prone to undergo parsing fail-
ure for every one in twenty-five progressive sen-
tences. This is no insignificant problem, espe-
cially when it is remarked that the progressive is
in high frequency usage (there are one thousand
ING-forms featuring in progressive constructions
for every one million words of sBNC) and that its
use is known to be spreading (Leech et al 2009).
Compounded with those parser breakdowns
caused by other speech phenomena (for instance,
repetition and elision), high numbers of parse fail-
ures on progressive constructions will render NLP
accuracy on spoken language intolerable for any
applications which rely on accurate parsing as a
foundation. However, we have shown above that
features of spoken language such as the zero aux-
iliary should not be thought of as errors or as un-
predictable deviations from the written form, but
rather can be considered to be consistent and pre-
dictable events. In this section we illustrate how
our predictive model for zero auxiliary occurrence
47
(|relation| |head| |dependant|) (3)
(|ncsubj| |play + ing : V V G| |you : PPY |)(4)
(|obj| |play + ing : V V G| |what : DDQ|) (5)
(|aux| |play + ing : V V G| |be+ : V BR|) (6)
(|arg| |play + ing : V V G| |you : PPY |) (7)
(|relation| |verb : V V G| |dependant|) (8)
Figure 1: Example grammatical relations from
RASP.
may be integrated into a parser pipeline in order
to aid the parsing of spoken language. In this way
we build on the increasingly robust engineering of
statistical NLP tools trained on written language
by allowing them to adapt to the spoken domain
on the basis of the linguistic study of speech phe-
nomena.
In general the notion of ?parsing? an utterance
involves a chain of several processes: utterance
boundary detection, tokenization, part-of-speech
tagging, and then parsing. We suggest that when it
is known that the language to be parsed is from the
spoken domain the pipeline of processes should be
run in a SPEECH AWARE MODE. Extra functional-
ity would be incorporated into each of the stages
according to the findings of linguistic research into
spoken language. In other work we have adapted
the tokenization and tagging stages of the pipeline
based on predictors that indicate when interjec-
tions (e.g. ?umm?, ?err? and ?ah?) have been ?used?
as punctuation or lexical items. We also incorpo-
rate intonation phrases as predictors for utterance
boundary detection (Buttery and Caines: in prepa-
ration). Here, we augment the parsing stage of the
pipeline by allowing an informed re-parse of ut-
terances in which a parse failure is likely to have
been caused by a zero auxiliary.
We present this section with reference to the
specific mechanics and output formats of the
RASP system but our algorithm is by no means
parser specific and could be adapted for other
parsers quite easily. Utterances parsed with
RASP may be expressed as ?grammatical rela-
tions?. RASP?s grammatical relations are theory-
general, binary relations between lexical terms and
are expressed in the form of head-dependancy re-
lations as shown in (3), Figure 1.
Consider the utterance ?what are you playing??.
When we parse this with RASP we get grammati-
cal relations (4), (5) and (6) in Figure 1. The capi-
tal letter codes following the ?:? symbols are part-
of-speech tags (from the CLAWS-2 tagset (Gar-
side, 1987)) which have been assigned to the lexi-
cal tokens by the tagger of the RASP system. Here
PPY indicates the pronoun ?you?; VVG indicates
the ING-form of lexical verb; VBR indicates ?be?
in 3rd person present tense; and DDQ indicates
a wh-determiner. The relation (4) tells us that
?you? is the subject of ?playing?; relation (5) tells
us that ?what? is taking the place of the object be-
ing played; and relation (6) tells us that there is an
auxiliary relationship between ?are? and ?playing?.
This is much as we would expect. However, if we
try to parse ?what you playing?? the parse fails.
The single relation (4) is returned where ideally
we would like both (4) and (5), as we did when
the auxiliary was present.
For the utterance, ?you playing?? RASP returns
the under-specified grammatical relation (7) which
is simply indicating that ?you? is an argument of
?playing? but not which type of argument (whether
a subject, direct object, etc). Ideally we would
like to retrieve at least (4) as we would have if we
parsed the utterance ?are you playing??. For these
examples, we shall consider the failure to identify
the correct subject and object of the progressive
verb to be a parsing failure.
We integrate the zero auxiliary predictive model
with parsing technology to improve the parsing of
zero auxiliaries in spoken language. Note that we
use the RASP system but our algorithm is by no
means parser specific. The only prerequisite is
that the parser must be able to identify relations
of some kind between the subject noun and ING-
form (possibly via a parsing rule) and also be able
extract values for the predictors (through either a
rich tagset or from the identification of key speech
tokens). The illustrative method we discuss here
is integrated into the parsing pipeline in the event
of a parse failure but there are several alternative
methods that might also be considered.
For instance, by using the predictive model
earlier in the parsing system pipeline a modified
tagset could be used which updates the ING-form
tag with a new tag to indicate that there is also a
missing auxiliary. Another method might involve
altering rule probabilities or adding extra parser
rules so that parsing only has to occur once. Our
other work in this area suggests that the final deci-
48
sion on where to add the spoken language modifi-
cations within the parsing pipeline will largely de-
pend on the interaction of the phenomena in ques-
tion with other speech phenomena.6
With the proviso that it is a preliminary integra-
tion of the predictive model into a parsing system,
we propose the following algorithm for zero aux-
iliaries in spoken language. When ?speech aware
mode? is activated, if we encounter a parse failure
then we first check the part-of-speech tags of the
utterance to ascertain if the sentence contains the
ING-form requisite for a progressive construction:
? IF no ING-form is found: STOP. Our
model predicts zero auxiliaries in progressive
constructions?there is nothing more we can
do with the input.
? ELSE: An ING-form is found. Extract all
grammatical relations that were obtained by
the parse which contained the ING-form in
the head position (these would be grammat-
ical relations that have the general format
of (8) in Figure 1). We will refer to this set of
grammatical relations as GRS.
? IF there is an auxiliary relation
present in GRS: STOP. If at least one
of the extracted grammatical relations is
an auxiliary relation, similar to (6) in
Figure 1, an auxiliary is present?we do
not have a zero auxiliary construction.7
? ELSE: The utterance is a candidate for
zero auxiliary.
Having determined a possible candidate for zero
auxiliary we carry out the following steps:
1. Ascertain values for the zero auxiliary pre-
dictors (explained in more detail below).
2. Calculate the value of the logistic function
f(z) using the obtained predictor values with
their coefficients (shown in Table 4).
3. If f(z) > 0.5, assume an auxiliary is miss-
ing.
6Although, another major consideration is the overall
computational efficiency of the parsing system.
7This step is actually subtly more complicated?auxiliary
relations involving ?been? are allowed to be present in GRS
(this allows us to capture zero auxiliaries in the perfect such
as ?been coming here long??) but if there is any other auxil-
iary relation present in GRS then we STOP here.
4. Add the auxiliary to the sentence (choos-
ing which auxiliary based on the predictor
values?see below).
5. Re-parse the sentence.
6. Remove (or flag) the auxiliary grammatical
relation from the newly obtained parser out-
put.8
For step 1 above properties of the current utter-
ance have to be obtained. The subject person, plu-
ral subject, zero subject and pronoun subject prop-
erties are ascertained by looking at the part-of-
speech of the dependant noun/pronoun within any
subject relations occurring in the set GRS (gram-
matical relations headed by the ING-form). Sub-
ject relations would look similar to (4) in Figure 1.
If there is no subject grammatical relation, any un-
derspecified ?arg? relation (such as (7) in Figure 1)
are considered. If neither of these relations are
present in GRS then a zero subject is inferred.
The person and plurality of the subject noun is en-
coded within its CLAWS2 part-of-speech tag. For
instance, a PPHS1 tag, which is used to indicate
?him? or ?her? would tell us we have a third per-
son, singular pronoun.9
The other properties are all ascertained by the
presence or absence of a token within the utter-
ance: interrogative property is inferred when the
utterance ends with a question mark; the nega-
tion property when either ?not? or ?n?t? (which are
tagged XX) is present; the perfect is inferred from
the presence of the word ?been?; and past tense is
ascertained from a set of temporal marker lexical
items (e.g. ?yesterday, ?before?). Once extracted
the properties are encoded as shown in Table 4 for
use as the predictor values in the logistic function.
In order to select the correct auxiliary and loca-
tion for insertion in step 4 the utterance values are
consulted. For instance, an interrogative utterance
in the present tense, not in perfect aspect, with a
second person singular subject will require inser-
tion of the auxiliary ?are? after the subject. A zero
subject zero auxiliary, on the other hand, requires
restoration of both subject and auxiliary. Where a
question mark indicates it has been used in an in-
terrogative clause the subject is assumed to be sec-
8We also remove (or flag) the subject relation in cases
where a subject also had to be added in step 4. This would
occur when the original utterance exhibited a zero subject.
9All common nouns are assumed to be 3rd person and all
instances of ?you? were considered to be singular (as was the
case during corpus annotation).
49
ond person - as is the case in most questions - and
so the auxiliary-subject combination ?are you? is
restored before the ING-form. Without a question
mark, the clause is assumed to be declarative and
so the first person singular subject-auxiliary com-
bination ?I am? is restored before the ING-form 10.
We withheld 10% of the zero auxiliary corpus
for test purposes. The integration of the predic-
tive model into the parser allowed us to success-
fully parse 31.4% of previously unparsable zero-
auxilaries. On cleaned spoken transcripts (i.e.
with speech phenomena other than the zero aux-
iliary, such as repetitions, removed) this algorithm
allows us to retrieve the correct subject-object re-
lations for an extra 1238 utterances within our
annotated corpus (which again accounts for ap-
proximately one third of the previously unparsable
zero-auxilaries). This is a significant step forward
for any applications building on top of a parsing
infrastructure.
7 Conclusion
We have shown how awareness of a specific
linguistic phenomenon enables improvements in
NLP techniques. The zero auxiliary is mainly a
feature of spoken language and so is not on the
whole handled successfully by existing parsers,
trained as they are on written data. As a so-
lution, rather than proposing the construction of
new models specifically designed for spoken lan-
guage, thereby doing away with all previous work
on NLP tools and starting again from scratch, we
demonstrated how new training data from a spo-
ken source could be applied to an existing parser
- RASP. We designed a predictive model of zero
auxiliary occurrence based on logistic regression
with nine syntactic variables. The data came from
an annotated corpus of 93,253 progressive con-
structions which showed zero auxiliary frequency
to be 4.2%. Without this new predictive informa-
tion in the parser, the status quo would continue
whereby one in twenty-five progressive construc-
tions would continue to be mis-parsed. We found
that instead the noise was regular and could be
modelled, and we illustrated how this specific lin-
guistic data could be integrated into existing NLP
technology. This is a case study of one specific
linguistic phenomenon. Our belief is that other
10A sample of one hundred zero subject declarative zero
auxiliaries indicates that the first person singular is the ap-
propriate subject type to restore on 60% of occasions.
such spoken language phenomena can be mod-
elled in the same way, given an appropriate corpus
resource, accurate annotation and implementation
into a parser.
By running in a ?speech aware mode? which
supplements existing parsing architecture we ben-
efit from the training that has already been under-
taken on a large scale based on written data and
complement it with specialized and predictable
linguistic properties of speech. Ideally, we would
like to train an entire parsing system on spoken
language but until spoken corpora become more
readily available this is not a practical option: the
resulting parser would suffer greatly from data
sparsity issues. Frustratingly, there is a circular
problem in generating corpora of an appropriate
size for training since until highly accurate mod-
els for spoken language are built we can not expect
speech-to-text systems to provide highly accurate
transcripts. But to build these highly accurate
models of spoken language in the first place a large
amount of data is required. Augmenting the ex-
isting statistical NLP tools trained on written lan-
guage with specialized linguistic knowledge from
the spoken domain is a pragmatic short-term fix
for this problem.
We should note that tailoring parsers to deal
with spoken language is by no means unheard of:
the RASP system itself, for example (which parses
using a probabilistic context-free grammar), al-
ready has several rules in its grammar which are
more appropriate for parsing spoken language.
However, use of these rules can contribute to much
over-generation and complexity in the parse for-
est (the parser internal structure which holds all
the possible parses for an utterance). In conse-
quence, the specialized rules have to be expertly
selected or deselected when configuring the parser.
This work - and our research program as a whole
- would instead allow parser configuration deci-
sions and algorithmic adaptions to be made non-
expertly and on-the-fly when running in ?speech
aware mode?. All rule activations and algorithm
adaptions would be made based on predictions
constructed from expert linguistic analysis of the
spoken domain.
Acknowledgements
This work was supported by the AHRC. We thank
three anonymous reviewers for their comments,
Andrew Rice and Barbara Jones.
50
References
Gisle Andersen. 1995. Omission of the primary verbs
BE and HAVE in London teenage speech - a so-
ciolinguistic study. Hovedfag thesis, University of
Bergen, Norway.
Michael Barlow and Suzanne Kemmer. 2000. Usage-
based models of language. Chicago: CSLI.
Rens Bod, Jennifer Hay and Stefanie Jannedy (eds.).
2003. Probabilistic Linguistics. Cambridge, MA:
MIT Press.
Ted Briscoe, John Carroll and Rebecca Watson. 2007.
The second release of the RASP system. Proceed-
ings of the COLING/ACL on Interactive presenta-
tion sessions, July 17-18, 2006, Sydney, Australia.
The British National Corpus, version 3. 2007.
Distributed by Oxford University Computing Ser-
vices on behalf of the BNC Consortium. URL:
http://www.natcorp.ox.ac.uk/
Roger Brown. 1973. A First Language: the early
stages. London: George Allen and Unwin.
Paula Buttery and Andrew Caines. In preparation. An
Empirical Approach to First Language Acquisition.
Cambridge: Cambridge University Press.
Joan Bybee and Paul Hopper (eds.). 2001. Frequency
and the emergence of linguistic structure. Amster-
dam: John Benjamins.
Andrew Caines. 2010. You talking to me? Zero aux-
iliary constructions in British English. Ph.D thesis,
University of Cambridge.
Frantisek Cerma?k. 2009. Spoken corpora design.
Their constitutive parameters. International Journal
of Corpus Linguistics 14: 113-123.
Noam Chomsky. 1965. Aspects of the Theory of Syn-
tax. Cambridge, MA: MIT Press.
Roger Garside. 1987. The CLAWS Word-tagging Sys-
tem. In: Roger Garside, Geoffrey Leech and Ge-
offrey Sampson (eds.), The Computational Analy-
sis of English: A Corpus-based Approach. London:
Longman.
Michael A. K. Halliday. 1994. Spoken and Writ-
ten Modes of Meaning. In: David Graddol and
Oliver Boyd-Barrett (eds.), Media Texts: Authors
and Readers. Clevedon: Multilingual Matters.
Michael Haugh. 2009. Designing a Multimodal
Spoken Component of the Australian National Cor-
pus. In: Michael Haugh, Kate Burridge, Jean Mul-
der, and Pam Peters (eds.), Selected Proceedings of
the 2008 HCSNet Workshop on Designing the Aus-
tralian National Corpus. Somerville, MA: Cas-
cadilla Proceedings Project.
William Labov. 1969. Contraction, deletion, and in-
herent variability of the English copula. Language
45: 715-762.
Geoffrey Leech, Marianne Hundt, Christian Mair and
Nicholas Smith. 2009. Change in Contemporary
English: a grammatical study. Cambridge: Cam-
bridge University Press.
Elena Lieven, Heike Behrens, Jennifer Speares and
Michael Tomasello. 2003. Early syntactic creativ-
ity: a usage-based approach. Journal of Child Lan-
guage 30: 333-370.
Luigi Rizzi. 1993/1994. Some notes on linguistic the-
ory and language development: The case of root in-
finitives. Language Acquisition 3: 371-393.
Geoffrey Sampson. 2001. Empirical Linguistics. Lon-
don: Continuum.
Anna Theakston, Elena Lieven, Julian Pine and Caro-
line Rowland. 2005. The acquisition of auxiliary
syntax: BE and HAVE. Cognitive Linguistics 16:
247-277.
Harry Tily, Susanne Gahl, Inbal Arnon, Neal Snider,
Anubha Kothari and Joan Bresnan. 2009. Syntactic
probabilities affect pronunciation variation in spon-
taneous speech. Language and Cognition 1: 147-
165.
Kenneth Wexler. 1994. Optional Infinitives, head
movement and the economy of derivations. In:
David Lightfoot and Norbert Hornstein (eds.), Verb
Movement. Cambridge: Cambridge University
Press.
Stephen Wilson. 2003. Lexically specific construc-
tions in the acquisition of inflection in English.
Journal of Child Language 30: 75-115.
51
First Joint Workshop on Statistical Parsing of Morphologically Rich Languages
and Syntactic Analysis of Non-Canonical Languages, pages 74?81 Dublin, Ireland, August 23-29 2014.
The effect of disfluencies and learner errors on the parsing of spoken
learner language
Andrew Caines Paula Buttery
Institute for Automated Language Teaching and Assessment
Department of Theoretical and Applied Linguistics
University of Cambridge, Cambridge, U.K.
(apc38|pjb48)@cam.ac.uk
Abstract
NLP tools are typically trained on written data from native speakers. However, research into
language acquisition and tools for language teaching & proficiency assessment would benefit
from accurate processing of spoken data from second language learners. In this paper we dis-
cuss manual annotation schemes for various features of spoken language; we also evaluate the
automatic tagging of one particular feature (filled pauses) ? finding a success rate of 81%; and we
evaluate the effect of using our manual annotations to ?clean up? the transcriptions for sentence
parsing, resulting in a 25% improvement in parse success rate by completely cleaning the texts of
disfluencies and errors. We discuss the need to adapt existing NLP technology to non-canonical
domains such as spoken learner language, while emphasising the worth of continued integration
of manual and automatic annotation.
1 Introduction
Natural language processing (NLP) tools are typically trained on written data from native speakers.
However, research into language acquisition and tools for language proficiency assessment & language
teaching ? such as learner dialogue and feedback systems ? would benefit from accurate processing of
spoken data from second language learners. Being able to convert the text from unparseable to parseable
form will enable us to (a) posit a target hypothesis that the learner intended to produce, and (b) provide
feedback on this target based on the information removed or repaired in achieving that parseable form.
To proceed towards this goal, we need to adapt current NLP tools to the non-canonical domain of
spoken learner language in a persistent fashion rather than use ad hoc post-processing steps to ?correct?
the non-canonical data. Outcomes of this approach have been reported in the literature (e.g. Rimell &
Clark (2009) in the biomedical domain; Caines & Buttery (2010) for spoken language). These fully
adaptive approaches require large amounts of annotated data to be successful and, as we intend to work
along these lines in future, the discussion in this paper is pointed in that direction.
The work presented here will act as a foundation for more permanent adaptations to existing tools.
We annotate transcriptions of speech for linguistic features that are known to interfere with standard
NLP to assess whether large-scale annotation of these features will be useful for training purposes. Ob-
vious instances of this include disfluencies (e.g. filled pauses, false starts, repetition), formal errors of
morphology and syntax, as well as ?errors? of word and phrase selection1.
Since manual annotation is costly in terms of time and often money, one might question whether so
many feature types are strictly necessary or even helpful for the task in hand. Indeed, filled pauses
such as ?oh? and ?um? are already accounted for in the part-of-speech (POS) tagset we use (CLAWS2
(Garside, 1987)); and one might also argue that lexico-semantic errors might be dismissed a priori on
the assumption that both the original and proposed forms are of the same POS (and thus won?t affect
a parser that performs tagging before the parse). We investigate the contribution of these features to
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1The word ?error? appears here in quotes as it might be argued that questionable lexical selections are more a matter of
infelicity and improbability than any strict dichotomy; we put this concern aside for now as a matter for future research.
74
parsing success. From a theoretical perspective we are interested in these features with regard to second
language acquisition and therefore need to analyse them closely.
In this paper we describe our initial efforts to address the challenge of parsing learner speech with
tools trained on native speaker writing. We also present empirical results that demonstrate the utility of
annotated spoken transcription with respect to both tagging and parsing. We investigate: [i] the frequency
of disfluencies, formal errors of morpho-syntax, and idiomatic errors of lexico-semantics in a corpus of
spoken learner language; [ii] the accuracy of part-of-speech labels produced by the tagger associated
with the Robust Accurate Statistical Parsing System (RASP (Briscoe et al., 2006)) for a particular type
of disfluency (the filled pause)2; [iii] parse success rates and parse likelihoods using the RASP System
with the texts in various ?modes? ranging from unaltered transcription to fully edited and corrected3.
We find that in our spoken learner corpus of 2262 words, (i) around a quarter of words are annotated
as disfluencies or errors; (ii) 81% of filled pauses were correctly tagged, meaning 1 in 5 are incorrectly
tagged; (iii) mean parse likelihood for the text ?as is?, unaltered, is ?2.599 with a parse success rate of
47%, whereas completely ?cleaned up? text improves those scores to ?1.995 and 72%4. We discuss the
implications of these results below, along with the background context for our study and a more detailed
description of our investigations.
2 Background
Previous analyses of the NLP of learner language include various experiments on the tagging and parsing
of errorful text. Geertzen et al. (2013) employed the Stanford parser on written English learner data and
achieved labelled and unlabelled attachment scores (LAS, UAS)5 of 89.6% and 92.1%. They found that
errors at the morphological level lead to incorrect POS-tagging, which in turn can result in an erroneous
parse. Others have focused only on the POS-tagging of written learner corpora ? for example with
English (van Rooy and Sch?fer, 2003) and French learner data (Thou?sny, 2011) ? demonstrating that
post-hoc corrections for the most frequent tagging errors results in significant parse error reduction.
In other investigations of standard NLP tools on learner corpora, Ott & Ziai (2010) report general
robustness using MaltParser on written German learner language; however, they found that by manually
correcting POS tags, LAS improved from 79.15% to 85.71% and UAS from 84.81% to 90.22%. Wagner
& Foster (2009) ran a series of parsing experiments using parallel errorful/corrected corpora, including
a spoken learner corpus in which the likelihood of the highest ranked tree for corrected sentences was
higher than that of uncorrected sentences in 69.6% of 500 instances. Taken together, these studies suggest
that existing NLP tools remain robust to learner data, even more so if the original texts can be corrected
and if the tagging stage is in some way verified, or adapted (e.g. Zinsmeister et al. (2014)).
On the other hand, D?az-Negrillo et al. (2010) argue that treating learner data as a ?noisy variant? of
native language glosses over systematic differences between the two, and instead call for a dedicated tag-
ging format for ?interlanguage?, one that encodes distributional, morphological and lexical information.
For instance, ?walks? in ?John has walks? would morphologically be tagged as a present tense 3rd-person
verb, but distributionally tagged as a past participle6. This is the kind of adaptation of existing tools that
we advocate, though we would add that this system should be available for not just interlanguage but all
data, allowing for non-canonical language use by native speakers as much as learners.
As for spoken language, Caines & Buttery (2010) among others suggest that adaptation can also be
made to the parser, such that it enters a ?speech-aware mode? in which the parser refers to additional
and/or replacement rules adapted to the particular features of spoken language. They demonstrated this
with the omission of auxiliary verbs in progressive aspect sentences (?you talking to me??, ?how you
2The RASP POS-tagger was evaluated on the 560 randomly selected sentences from The Wall Street Journal that constitute
the PARC dependency bank (DepBank; (King et al., 2003)) and achieved 97% accuracy (Briscoe et al., 2006).
3The RASP parser achieves a 79.7% microaveraged F1 score on grammatical relations in DepBank (Briscoe and Carroll,
2006).
4N.B. the closer the parse likelihood to zero, the more probable the parse in the English language.
5LAS indicates the proportion of tokens that are assigned both the correct head and the correct dependency label; UAS
indicates the proportion of tokens assigned the correct head, irrespective of dependency label.
6We thank reviewer #1 for this example.
75
doing??) and achieved a 30% improvement in parsing success rate for this construction type.
3 The corpus
Our speech data consist of recordings from Business Language Testing Service (BULATS) speaking
tests7. In the test, learners are required to undertake five tasks; we exclude the tasks involving brief
question-answering (?can you tell me your full name??, ?where are you from??, etc) and elicited imitation,
leaving us with three free-form speech tasks. For this particular test the tasks were: [a] talk about some
advice from a colleague (monologue), [b] talk about a series of charts from Business Today magazine
(monologue), [c] give advice on starting a new retail business (dialogue with examiner).
In our full dataset the candidates come from India, Pakistan and Brazil, with various first languages
(L1) including Hindi, Gujarati, Malayalam, Urdu, Pashto and Portuguese, and an age range of 16 to 47
at the time of taking the test. However, in this analysis we have only sampled recordings from candidates
deemed to be at ?B2? upper intermediate level on the CEFR scale8, so that the proficiency level of
language used (and how that relates to NLP) is controlled for. In addition the L1s in our sample are
Gujarati, Punjabi and Urdu only. This gives us a sample corpus of 2262 tokens in ?as-is? format (i.e. the
true transcriptions before any corrections are made).
4 Manual annotation
The recordings were manually transcribed and annotated for various features falling into three categories
described and exemplified in the following non-exhaustive list.
? disfluencies ? interruptions to the flow of otherwise fluent speech;
? <fp> filled pauses (tokens such as uh, er, um that serve to fill time and hold the turn) and <rep
n="n"> repetition (the speaker repeats a word or phrase one or several times):
?or the other way is to <fp>um</fp> <rep n="1">is to</rep> raise finance?
? <false> false starts ? the speaker begins to express a word or phrase which he then corrects:
?in two thousand eight it was <false>thirty five p</false> thirty percent?
? formal errors of morpho-syntax, such as number agreement, verb inflection and word order errors;
? noun form: ?for becoming a chartered <NS type="FN"><i>accountants</i><c>accountant
</c></NS>?
? missing verb: ?as the charts <NS type="MV"><c>show</c></NS> its sales increased?
? word order: ?<NS type="W"><i>how it would be help for you mention</i><c>mention how
it helped you</c></NS>?
? idiomatic ?errors? ? infelicities in lexical selection, failure to express intended meaning, or less-
than-natural phrasing;
? idiomatic: ?all my class <NS type="ID"><i>fellows</i><c>mates</c></NS>?
? idiomatic: ?to <NS type="ID"><i>get in</i><c>make a</c></NS> profit?
? replace quantifier: ?for a bank to grant us <NS type="RQ"><i>some</i><c>a</c></NS> loan?
The annotation scheme for formal and idiomatic errors comes from the project to annotate the Cambridge
Learner Corpus (Briscoe et al., 2010). The ?error zone? is denoted by <NS> tags, with any original
token(s) enclosed by <i> and any proposed correction enclosed by <c>. The various error types are
defined in Nicholls (2003) and the categories are similar to the ones given: either self-defining (?ID? for
idiom error, ?W? for word order, etc) or a combination of operation plus part-of-speech (?FN? form of
noun, ?MV? missing verb, ?RQ? replace quantifier, etc).
In Table 1 we report the number of errors and disfluencies found in our corpus along with a relative
frequency per 100 words. Just under a quarter of the thousand tokens in our corpus are affected by
disfluencies and errors, with the former being far more prevalent.
7We thank Cambridge English Language Assessment for releasing these recordings for this pilot study; for further informa-
tion on BULATS go to http://www.bulats.org/
8The ?Common European Framework of Reference for Languages?: a schema for grading an individual learner?s language
level. For further information go to http://www.coe.int/lang-CEFR
76
All transcription and annotation has been carried out by a single annotator (the first author). It would
be interesting to obtain measures of inter-annotator agreement to assess the extent to which the nature of
error judgement (particularly in judgements as to idiomaticity) is subjective.
type instances in corpus relative frequency (per 100 words)
disfluency 316 14
formal error 143 6
idiomatic error 70 3
total 529 23
Table 1: Error counts in our corpus
5 Automated annotation: part-of-speech tagging
Since filled pauses such as ?er? and ?um? are included in the CLAWS2 tagset used by the RASP System
as UH, ?interjection?, one might question the worth of manually annotating filled pauses (FPs). Of the
disfluency set, it might be one small time-saving to leave these to the tagger. However, ?interjection? is
not a homogeneous set, as UH also covers exclamations of surprise (?oh?) and assent (?yes?). Moreover,
we find that the POS-tagging of tokens annotated as FPs is not entirely appropriate in this non-canonical
domain. Table 2 shows that the majority of FPs are correctly tagged UH, though others are tagged as
nouns (NN), verbs (VV), adjectives (JJ), adverbs (RR) and foreign words (&FW)9.
Token UH &FW JJ NN RR VV total
er 104 0 0 0 0 0 104
mm 0 0 0 8 0 0 8
uh 0 0 0 1 2 4 7
um 2 5 0 0 0 0 7
nuh 0 0 0 0 2 1 3
buh 0 0 0 1 0 1 2
other 2 0 1 0 0 0 3
total 108 5 1 10 6 9 134
Table 2: POS tagging of filled pauses
One possible solution is to append a dictionary of known FP tokens to the tagger, and specify that
they should be tagged UH, or even better, a new tag such as FP. But as the Table demonstrates, there
are standard, highly frequent FPs such as ?er?, ?uh? and ?um?, and then there are novel forms such as
?nuh?, ?buh? and ?nna? which we found to be rather idiosyncratic ? i.e. there might be novel FPs for every
individual. Moreover, the introduction of a closed class depends on consistent transcription practice, not
necessarily a given with even a lone annotator, let alone more than one.
Automatic identification and repair of disfluencies is a well-developed research topic, with continuing
refinements to joint parsing and disfluency detection models (e.g. Qian & Liu (2013), Rasooli & Tetreault
(2014), Honnibal & Johnson (2014)), plus applied work in the domains of automatic speech recognition
(Fitzgerald et al., 2009) and machine translation (Cho et al., 2014). We note the linguistic rules included
in the Lease, Johnson & Charniak (2006) tree adjoining grammar (TAG) noisy-channel model ? lexical,
POS and syntactic rules that reduce errors in the TAG model. This is another case of improvements to
NLP tools thanks to data-driven linguistic insight, and a design that we could incorporate into our work
on automated assessment and feedback.
9The ?other? filled pauses are singleton forms: eh, nna, ah.
77
6 Automated annotation: sentence parsing
In this section we report the results of our parsing experiment in which transcribed learner utterances
were processed by the RASP system in four different forms:
(A) as-is: without alteration;
(B) less-disfluency: with disfluencies removed;
(C) less-form-error: with morpho-syntactic errors corrected;
(D) less-lex-error: with semantic/idiomatic improvements.
We investigated the effect on the parsing output of each transcription format compared to the (A) format
as a baseline. We processed each format in turn singularly, as well as cumulative combinations of (B),
(C) and (D) in every possible order. The results are set out in Table 3, with mean likelihoods of the
highest ranked parse for each sentence (?)10, differences between this mean and the baseline where
applicable (?base), and success rates in terms of non-fragmentary tree outputs (i.e. parses labelled other
than ?T/frag? in the RASP System).
mode ? ?base ?T/frag mode ? ?base ?T/frag mode ? ?base ?T/frag
(A) ?2.599 0 .471 (A) ?2.599 0 .471 (A) ?2.599 0 .471
(B) ?2.094 +.505 .623 (BC) ?2.032 +.567 .689 (BCD) ?1.995 +.604 .715
(C) ?2.574 +.025 .484 (BD) ?2.049 +.550 .649 - - -
(D) ?2.563 +.036 .503 (CD) ?2.545 +.054 .523 - - -
Table 3: Mean parse likelihoods, deltas to baseline and parse success rates in all transcription modes
As can be seen in Table 3, the removal of disfluencies (B) is the single move of greatest benefit to
parse likelihood scores and parse tree success rates compared to the ?as-is? baseline (A). The correction
of morpho-syntactic (C) and idiomatic errors (D) have a lesser effect. All pairings have a positive effect
on parse likelihoods, especially those featuring disfluency removal (B); and the three ?corrective? steps
combined (BCD) have the greatest effect of all.
However, we show by analysis of two candidates in our corpus that these effects can differ on an
individual basis. In Figure 1, the candidate on the left has a less pronounced effect of disfluency removal
(B) compared to the baseline (A) than the candidate on the right. The effect of both formal (C) and
idiomatic (D) error correction are also seen to make improvements over (A), which is not the case for
the second candidate. Such observations serve as a reminder that when generalising about overall corpus
patterns we collapse over so many individual language models. It may well turn out that disfluencies are
an especially idiosyncratic type of language use, an avenue we will explore in future work.
7 Discussion
In this paper we have investigated NLP of transcribed learner speech, questioning how tools trained on
native speaker written data would handle such data. We found that the majority (81%) of filled pauses
were correctly tagged ?UH?, though this only covers three of eleven FP forms (er, um, eh). We propose a
dictionary of FPs and a specific FP POS-tag, while suggesting that the dictionary will not catch all novel
FPs (since they seem to be idiosyncratic) and that we can turn to state-of-the-art research on automated
disfluency detection to help us.
We also showed that sentence parsing could be improved from a 47% ?success? rate (i.e. non-
fragmentary (T/frag in RASP parlance) parse trees) in the ?as-is? transcriptions, to 72% in transcrip-
tions with disfluencies removed and errors corrected (see Table 3). We found that disfluency removal
is the main contributor to this improvement, though this was found to be somewhat idiosyncratic (as in
Figure 1).
10Note that parse likelihoods have been normalised for word length, as they increase in a near-linear manner according to the
number of terminal nodes in a tree.
78
l l l
l
l
l
l
l
l
l
l
l
l
S2BWWT9EVS S3R66XVRQ2
?6
?4
?2
0
A B C D BC BD CD ABC A B C D BC BD CD ABC
transcription mode
pa
rs
e 
lik
el
ih
oo
d 
(n
or
ma
lis
ed
 fo
r w
o
rd
 le
ng
th
)
Figure 1: Parse likelihoods for each transcription mode, for two individuals in our corpus; the whiskers
indicate the largest and smallest observation within 1.5?IQR (inter-quartile range; the distance between
first and third quartiles), while the upper hinge indicates the third quartile (75th percentile), the middle
is the median, the lower hinge is the first quartile (25th percentile), and the points are outliers.
The motivation for this work is to investigate what is required to convert texts from unparseable to
parseable form. The steps taken to achieve this can be used to inform automated learner dialogue or
feedback systems. We note that automated assessment may be improved by parse trees but may well
be performed without them: it can proceed on the basis of superficial detection of features known to
correlate with high grades (possibly including certain disfluency types, for instance). But to be able to
diagnose how the learner can improve, we need a deeper structural analysis of the text ? i.e. requiring
that the text is in parseable form. Our manual annotations are one step towards this goal.
Our annotations also indicate that spoken learner data features many disfluencies and errors, with over
a quarter of the 2262-word testset affected in some way. Automatic error detection (and correction) is
a burgeoning field (see for example the work on learner data by Briscoe et al. (2010), Andersen (2011)
and Kochmar & Briscoe (2014), as well as the most recent shared task on grammatical error correction
at CoNLL-2014 (Ng et al., 2014)). Such studies are based on written language. We envisage adding
speech-specific information and adaptations to such systems on the basis of our fuller annotation project.
Indeed, it so happens that the problem of NLP in the spoken domain is one we address here with
learner data. However, we do not assume that the problem of adapting or building NLP tools for spoken
data is substantially different for native speaker data. We intend to collect recordings of native speakers
undertaking the same tasks as the BULATS candidates, allowing for comparative studies of errors and
disfluencies in native and learner data, with the task and topic variables held constant as far as possible.
Finally, we emphasise that we intend to add to the corpus with more annotated data from a wider
range of L1s and a wider range of proficiency levels. We can then investigate the possible effects of more
varied syntactic complexity, lexical diversity and error types.
79
Acknowledgments
We thank Cambridge English Language Assessment for funding this work and providing the data. We
also thank Ted Briscoe, Mike McCarthy and ?istein Andersen for their support and advice, and we are
grateful to the three anonymous reviewers for their helpful comments and suggestions.
References
?istein E. Andersen. 2011. Semi-automatic ESOL error annotation. English Profile Journal, 2:e1.
Ted Briscoe and John Carroll. 2006. Evaluating the accuracy of an unlexicalized statistical parser on the PARC
DepBank. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions. Association for Com-
putational Linguistics.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The second release of the RASP System. In Proceedings
of the COLING/ACL 2006 Interactive Presentations Session. Association for Computational Linguistics.
Ted Briscoe, Ben Medlock, and ?istein E. Andersen. 2010. Automated assessment of ESOL free text examina-
tions. University of Cambridge Computer Laboratory Technical Reports, 790.
Andrew Caines and Paula J. Buttery. 2010. ?You talking to me?? A predictive model for zero auxiliary construc-
tions. In Proceedings of the Workshop on Natural Language Processing and Linguistics, Finding the Common
Ground, Annual Meeting of the Association for Computational Linguistics (ACL) 2010. Association for Com-
putational Linguistics.
Eunah Cho, Jan Niehues, and Alex Waibel. 2014. Tight integration of speech disfluency removal into SMT. In
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics
(EACL 2014). Association for Computational Linguistics.
Ana D?az-Negrillo, Detmar Meurers, Salvador Valera, and Holger Wunsch. 2010. Towards interlanguage POS
annotation for effective learner corpora in SLA and FLT. Language Forum, 36:139?154.
Alison Edwards. 2014. The progressive aspect in the Netherlands and the ESL/EFL continuum. World Englishes,
33:173?194.
Erin Fitzgerald, Keith Hall, and Frederick Jelinek. 2009. Reconstructing false start errors in spontaneous speech
text. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational
Linguistics (EACL 2009). Association for Computational Linguistics.
Roger Garside. 1987. The CLAWS word-tagging system. In Roger Garside, Geoffrey Leech, and Geoffrey
Sampson, editors, The Computational Analysis of English: A Corpus-based Approach. London: Longman.
Jeroen Geertzen, Theodora Alexopoulou, and Anna Korhonen. 2013. Automatic linguistic annotation of large
scale L2 databases: the EF-Cambridge Open Language Database (EFCAMDAT). In Proceedings of the 31st
Second Language Research Forum. Somerville, MA: Cascadilla Proceedings Project.
Matthew Honnibal and Mark Johnson. 2014. Joint incremental disfluency detection and dependency parsing.
Transactions of the Association for Computational Linguistics, 2:131?142.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary Dalrymple, and Ronald M. Kaplan. 2003. The PARC700
Dependency Bank. In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora
(LINC 2003).
Ekaterina Kochmar and Ted Briscoe. 2014. Detecting learner errors in the choice of content words using com-
positional distributional semantics. In Proceedings of the 25th International Conference on Computational
Linguistics (COLING 2014). Association for Computational Linguistics.
Matthew Lease, Mark Johnson, and Eugene Charniak. 2006. Recognizing disfluencies in conversational speech.
IEEE Transactions on Audio, Speech, and Language Processing, 14:1566?1573.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christopher
Bryant. 2014. The CoNLL-2014 Shared Task on Grammatical Error Correction. In Eighteenth Conference
on Computational Natural Language Learning, Proceedings of the Shared Task. Association for Computational
Linguistics.
80
Diane Nicholls. 2003. The Cambridge Learner Corpus: error coding and analysis for lexicography and ELT. In
Dawn Archer, Paul Rayson, Andrew Wilson, and Tony McEnery, editors, Proceedings of the Corpus Linguistics
2003 conference; UCREL technical paper number 16. Lancaster University.
Niels Ott and Ramon Ziai. 2010. Evaluating dependency parsing performance on German learner language. In
Proceedings of the Ninth International Workshop on Treebanks and Linguistic Theories (NEALT 2010).
Xian Qian and Yang Liu. 2013. Disfluency detection using multi-step stacked learning. In Proceedings of the
2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies (NAACL-HLT).
Mohammad Sadegh Rasooli and Joel Tetreault. 2014. Non-monotonic parsing of Fluent umm I Mean disfluent
sentences. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational
Linguistics (EACL 2014). Association for Computational Linguistics.
Laura Rimell and Stephen Clark. 2009. Porting a lexicalized-grammar parser to the biomedical domain. Journal
of Biomedical Informatics, 42:852?865.
Sylvie Thou?sny. 2011. Increasing the reliability of a part-of-speech tagging tool for use with learner language. In
Proceedings of the Pre-conference Workshop on Automatic Analysis of Learner Language, CALICO Conference
2009.
Bertus van Rooy and Lande Sch?fer. 2003. An evaluation of three POS taggers for the tagging of the Tswana
Learner English Corpus. In Proceedings of the Corpus Linguistics 2003 Conference. Lancaster University.
Joachim Wagner and Jennifer Foster. 2009. The effect of correcting grammatical errors on parse probabilities. In
Proceedings of the 11th International Conference on Parsing Technologies.
Heike Zinsmeister, Ulrich Heid, and Kathrin Beck. 2014. Adapting a part-of-speech tagset to non-standard text:
the case of STTS. In Proceedings of the Ninth International Conference on Language Resources and Evaluation
(LREC 2014). European Language Resources Association.
81
