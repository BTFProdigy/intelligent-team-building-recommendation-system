Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 105?113,
Dublin, Ireland, August 23, 2014.
Exploring Mental Lexicon in an Efficient and Economic Way:
Crowdsourcing Method for Linguistic Experiments
Shichang Wang, Chu-Ren Huang, Yao Yao, Angel Chan
Department of Chinese and Bilingual Studies
The Hong Kong Polytechnic University
Hung Hom, Kowloon, Hong Kong
shi-chang.wang@connect.polyu.hk
{churen.huang, y.yao, angel.ws.chan}@polyu.edu.hk
Abstract
Mental lexicon plays a central role in human language competence and inspires the creation of
new lexical resources. The traditional linguistic experiment methodwhich is used to exploremen-
tal lexicon has some disadvantages. Crowdsourcing has become a promising method to conduct
linguistic experiments which enables us to explore mental lexicon in an efficient and economic
way. We focus on the feasibility and quality control issues of conducting Chinese linguistic ex-
periments to collect Chinese word segmentation and semantic transparency data on the interna-
tional crowdsourcing platforms Amazon Mechanical Turk and Crowdflower. Through this work,
a framework for crowdsourcing linguistic experiments is proposed.
1 Introduction
Mental lexicon as a theoretical construct has two important implications. For an individual, it is where all
the grammatical and world information is stored and organized to enable speech. For a group of speakers
of the same language, however, the mental lexicon is a shared knowledge structure allowing speakers
to process and understand what each other said. WordNets, for example the English WordNet (Miller,
1995) and the Chinese WordNet (CWN) (Huang et al., 2003), and ontologies, for example the Suggested
Upper Merged Ontology (SUMO) (Niles and Pease, 2001) and the Sinica BOW (Huang et al., 2010),
have been proposed as a representational framework for this shared mental lexicon; and psycho- and
neuro-linguistic experiments have been designed to explore how individuals access their mental lexicon.
However, the question of whether there is a shared principle or strategy of mental lexicon by all speakers
of the same language was never seriously studied as the cognitive experimental paradigm does not allow
manipulation of a large number of subjects simultaneously. In this paper, we explore the possibility of
conducting lexical access related experiments through crowdsourcing. With the crowdsourcing experi-
ments, we intend to ask specific question about the share strategy of determination of lexical units, as
well as determination of semantic transparencies, two issues that would have direct implications of how
individuals access their mental lexicon.
Many scholars discuss applying crowdsourcing method to language resource construction recent years
(Snow et al., 2008; Callison-Burch and Dredze, 2010; Munro et al., 2010; Gurevych and Zesch, 2013).
Crowdsourcing has been proved to be an efficient tool to build lexical resources, for example, Wiktionary,
whose goal is to become the free online dictionary for all the words in all languages; Biemann (2013)
presents another example which creates the Turk BootstrapWord Sense Inventory for 397 frequent nouns
from scratch using AmazonMechanical Turk. And there is more andmore literature focusing on conduct-
ing experiments on crowdsourcing platforms (Schnoebelen and Kuperman, 2010; Paolacci et al., 2010;
Berinsky et al., 2011; Rand, 2012; Mason and Suri, 2012; Crump et al., 2013). Using crowdsourcing
method, it is easier to access highly diverse and huge amount of participants, so it is possible to obtain
more representative language behavioral data. The anonymous nature of crowdsourcing makes the par-
ticipants more open to contribute sensitive data. And Crowdsourcing experiments are usually much faster
and cheaper than laboratory experiments which enables ? faster iteration between developing theory and
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
105
executing experiments? (Mason and Suri, 2012). It can be a promising tool to explore mental lexicon in
an efficient and economic way.
MTurk and Crowdflower are perhaps the two most important MTurk-like crowdsourcing platforms.
MTurk is the platform appears most frequently in the literature, so popular that it represents a major genre
of crowdsourcing and its name has become the name of that genre. Crowdflower is a rapid developing
platform and is drawing more and more attention. Although they are both MTurk-like platforms, they
differ from each other. On the MTurk platform, invalid responses submitted can be manually rejected
which is a very convenient quality control method; however Crowdflower has a much larger worker pool
than MTurk. Since MTurk is one channel of Crowdflower and Crowdflower can access the worker pool
of MTurk1, besides MTurk, Crowdflower has several dozens of other channels to which it can distribute
tasks. More importantly, Crowdflower is more accessible to requesters outside the U.S. (MTurk does not
support requesters outside the U.S. by now). Crowdflower basically doesn?t support manual rejection
of invalid responses but it integrates an effective quality control method named Test Questions which
uses predefined gold standard questions to measure the quality of contributions of workers and screens
low quality workers automatically in order to produce high quality data. Unfortunately, it is not suitable
to our task, for it requires multiple submissions form a worker. Neither MTurk nor Crowdflower is a
native Chinese crowdsourcing platform, so we can suppose that native Chinese speakers can only occupy
a small proposition in their worker pools, in this case, a larger worker pool means higher possibility of
successful data collection.
We have two objectives in this study: (1) to check if it is feasible to conduct Chinese language experi-
ments to collect Chinese word segmentation and semantic transparency (Libben, 1998) data which can be
used to explore the mental lexicon of Chinese speakers on international crowdsourcing platforms, such as
Amazon Mechanical Turk (MTurk) and Crowdflower; (2) to identify and solve some quality control and
experimental design issues in order to obtain high quality data and to establish a preliminary framework
for crowdsourcing linguistic experiments.
2 Initial Calibration Tests
Before the experiment, we conducted initial calibration tests. The purpose is to lay a basic foundation
(e.g., general experimental parameters, quality control methods, etc.) for the experiment. There are four
tests. We employed a problem-driven bootstrapping strategy in the design and conduct of these tests in
order to accumulate knowledge effectively. The repeated procedure is like this: one test is started and
once a problem has been identified, the test will be paused or stopped; after a proper solution has been
found, a modified version of that test will be resumed or a new test will be designed and started.
2.1 Parameters
Both MTurk and Crowdflower will be tested, however we cannot access MTurk directly as a requester
since it doesn?t support requesters outside the U.S. by now. Because MTurk is a channel of Crowdflower,
Crowdflower can distribute jobs to it, we can access it indirectly through Crowdflower. So Crowdflower
is selected as our job publishing platform. Whenwe testMTurk, wewill require Crowdflower to distribute
our jobs to the MTurk channel only; and when we test Crowdflower, we will conduct Crowdflower to
distribute our jobs to all of its channels.
The jobs published on Crowdflower can be divided into two types according to the existence or absence
of data-sets to be processed. A job without a data-set to be processed is a survey. The survey type fits
our objectives best since we want to collect data from different individuals and each person can only
participate in any one of the tests/experiments once. In order to ensure this one-time participation, we
use the following constrains: (1) each worker account can only submit one response, and (2) each IP
address can only submit one response.
Crowdflower allows us to specify ?included countries? or ?excluded countries? as an access control
method. We only use ?included countries? in our tests/experiments, only the countries and regions we
1However, this has become history, in December 2013, Crowdflower announced that Amazon Mechanical Turk would no
longer be a partner channel, see http://www.crowdflower.com/blog/2014/01/crowdflower-drops-mechanical-turk-to-ensure-the-
best-results-for-its-customers (Retrieved May 24, 2014).
106
selected are allowed to access our jobs. According to the distribution of Chinese people, we select the
following countries and regions: Mainland China, Hong Kong, Macau, Taiwan, Singapore, Malaysia,
Japan, Korea, Australia, Canada, the United States, the United Kingdom, France, Germany, Russia, and
New Zealand.
2.2 Test 1
The objective is to evaluate the feasibility to collect Chinese language data from MTurk. We published a
survey on Crowdflower and it was distributed to the MTurk channel only. The questionnaire contains 3
questions: (1) what place of China do you come from, (2) what country or region are you in now, (3) what
dialect of Chinese do you speak; all the questions are in Chinese. There is a text-box for each question
which allows the participants to input their answers. All the questions must be answered or the data are
not allowed to be submitted. It only takes 10 to 15 seconds to fill up the questionnaire. The unit price
of this survey is one cent. This test only collected 2 responses in 21 hours. Judged from the speed, we
can preliminarily conclude that MTurk is not a feasible platform for Chinese language data collecting
tasks. Because of the properties of crowdsourcing environment, this result can be accidental, so we will
continue to open the MTurk channel to validate this result.
2.3 Test 2
We published a new test on Crowdflower in order to evaluate the feasibility of collecting Chinese lan-
guage data from Crowdflower. It is mostly the same as Test 1, the only difference is that all the channels
of Crowdflower are enabled so we can access a much larger worker pool. This time, we collected 23
responses in 2 hours. Nine of them (39.1%) are valid, 14 (60.9%) are invalid. The speed is good, but the
data quality is not acceptable. In this test, we didn?t use powerful quality control method, thus large num-
ber of invalid responses were submitted. Invalid responses deteriorated data quality. This demonstrates
that quality control is essential to crowdsourcing practices.
2.4 Test 3
It?s important to detect and identify invalid responses. ?Checkpoint questions? (see section 5) can be
used to distinguish valid responses from invalid responses. This test attempts to test the effectiveness of
checkpoint questions. We added a Chinese character identification question to the questionnaire of Test
2. The participants are required to identify a Chinese character in a picture and then input this character
into a text-box. The frequency of that character is very high, so it is easy for Chinese native speakers
to identify. Because this an open-ended question, so it is robust enough. This question satisfies the
conditions to be a checkpoint question (see section 5). Then the test was resumed. After 2 hours, 9 new
responses were received (23 responses had been received since Test 2). Four of them (44.4%) are valid
responses, 5 (55.6 %) are invalid. All of the responses with correct answers to the checkpoint question
were checked to be valid responses.
Logically, correctly answering checkpoint questions doesn?t definitely mean the other questions are
also carefully answered. But human behaviors have certain consistency to some extent. If they carefully
answered checkpoint questions, then they are likely to answer the other questions carefully. Although it
is not 100% reliable to identify invalid/valid responses by checkpoint questions, it is acceptable if there
is no better method.
2.5 Test 4
Checkpoint questions can identify invalid responses but they cannot block them. We can set some condi-
tions for the submission of responses. Only the responses which satisfy these conditions can be submitted.
We call these submission conditions ?validations? (see section 5). Since checkpoint questions can be used
to identify invalid responses, we can set validations on them in order to block the submission of invalid
responses. We set a validation on the Chinese character identification checkpoint question: the response
can be submitted only when the checkpoint question is correctly answered. Then the test was resumed
and 28 new responses were received (a total of 60 responses had been received since Test 2). 26 of them
(92.9%) are valid responses, 2 (7.1%) are invalid. Before the adoption of validation, the proportion of
107
valid response is only 40.6%, after the adoption, it?s 92.9%. This basically shows that it can effectively
block invalid responses to set validation on checkpoint questions.
2.6 Summary
We collected 60 responses in Tests 1 to 4; among them, there are only two responses from MTurk. This
verified the result of Test 1, i.e., it is not quite feasible to collect Chinese language data fromMTurk at least
by now. However Crowdflower is a feasible choice since it has a much larger worker pool. Because of
the nature of Crowdsourcing, noise is everywhere. It is practically unacceptable to collect data without
effective quality control methods; otherwise more invalid responses than valid ones will be received.
Checkpoint questions can be used to identify valid and invalid responses. Validations are effective to
block the submission of invalid responses. It is a good strategy to set validations on checkpoint questions
in order to block invalid responses.
3 Experiment
The experiment was divided into two stages, and there was a time interval of about two months between
them. Based on the initial calibration tests, the experiment was conducted to test the feasibility of col-
lecting Chinese language data on international crowdsourcing platforms and to identify and solve some
quality control and experimental design issues.
Our original plan was to conduct one experiment to collect a sample of 200 responses. But after we
had collected 135 responses, we found a serious spammer problem which must be properly solved oth-
erwise the data quality would be greatly threatened and the feasibility of our task would be questionable.
Meanwhile, we found the amounts of responses from the region of mainland China and the channel ?bit-
coinget? were unexpectedly large, we doubted that it might result from the frequent media reports on
bitcoin at that time in mainland China. When the media reports ebbed, would the experiment be replica-
ble? Thus we thought it?s necessary to pause the experiment to seek a solution for the spammer problem
and to evade the strong external factor of media report. Thus the experiment was divided into two stages.
We chose to pause the experiment instead of stopping it so that the participants who had already taken
part in the experiment (Stage 1) could not take part again when the experiment was resumed (Stage 2).
The experiment was resumed after two months, with a spammer monitor program based on the API of
Crowdflower which could detect and combat spammers automatically. Other aspects of the experiment
remained unchanged. The Stage 2 experiment could be used to check the experimental repeatability and
to solve the spammer problem found in Stage 1.
3.1 Experimental Design
Questionnaire
The experiment we ran was a self-paced online questionnaire, consisting of 46 questions divided into
three parts. The first part contained 10 screening questions designed to verify that the participants were
(1) human and (2) native Chinese speakers. The second part of the experiment was a task of Chinese word
segmentation. The participants were presented with 12 Chinese sentences and their task was to put a ?/?
sign at the word boundary that they perceived. The third part of the experiment was a semantic trans-
parency data collection task. Semantic similarity rating tasks were used to obtain semantic transparency
data. 12 di-morphemic Chinese compounds, (e.g., ?? bangzhu, help-assist, ?help?) were shown in
12 carrier sentences (one target compound per carrier sentence). The participant?s task was to rate, on
a 5-point scale, the degree of semantic similarity between the meaning of each character in the target
compound and the meaning when it is used alone. In view of the different character systems used in
different Chinese-speaking regions, we implemented two versions of the questionnaire: a simplified Chi-
nese character version for participants from Mainland China and a traditional Chinese character version
for participants from Hong Kong.
Experiment Control
Experiment control measures are used to ensure the validity of participants and their participations. Be-
cause we cannot access the real identities of the participants, we can only use some indirect methods
108
which are not completely reliable but can satisfy our demands at large. Firstly, all the participants must
be native Chinese speakers. The questionnaire was displayed in Chinese characters which can be a natural
barrier to non-native Chinese speakers. Ten screening questions are designed in the questionnaire to test
the language backgrounds of the participants. By the above measures, we can effectively discriminate
native Chinese speakers from non-native ones. Chinese learners are not a major threat due to their small
amount and low overall Chinese fluency. We invited two Chinese learners to test our questionnaire;
neither of them could finish it. Secondly, one participant can only submit one response. We used the
methods which are already explained in 2.1: one account can only submit one response; one IP address
can only submit one response.
Quality Control
In addition to experiment control measures, quality control measures are used to further prevent invalid
responses. We used checkpoint questions and other measures for data validation. Only those responses
that fulfill the following conditions were considered as valid responses: (1) the screening questions in Part
1 were correctly answered, (2) the answers in Part 2 followed the correct format, and (3) the completion
time was equal or greater than 5 minutes. Those that failed one or more conditions were considered as
invalid. The effectiveness of the validation measures is discussed in 5. After the Stage 1 experiment, we
found a serious spammer problem. After adopting the above quality control measures, spammers became
the biggest threat to data quality. It can be exhausted to combat spammers manually due to their high
speeds and randomness. Thus, based on the API of Crowdflower we wrote a spammer monitor program
to detect and combat spammers automatically.
Parameters
The experiment uses the parameters described in 2.1. Besides that, the unit price of our task is set to
US$0.25. Pricing strategy should be carefully chosen in crowdsourcing practices. High prices tend to
attract cheating, but low prices may fail to attract enough participations, see (Mason and Watts, 2010).
4 Results and Evaluation
Stage 1 of the experiment lasted for about two days, with multiple manual pauses in between to resist
spamming attempts. A total of 135 responses were received, out of which 88 (65.19%) were valid and 47
(34.81%) were invalid according to the criteria stated above. Among the valid responses, 81 (92.05%)
were contributed by participants who claimed to be from Mainland China and only 7 (7.95%) by partici-
pants from Hong Kong. 38 out of the 47 invalid responses (80.85%) were probably produced by spam-
mers because their completion times were very short and/or the validation measures were bypassed. The
3 largest source channels of valid responses were bitcoinget (n=52, 59.09%), prodege (n=11, 12.50%)
and getpaid (n=7, 7.95%), while the 3 largest source regions (based on the IP addresses) were Mainland
China (n=54, 61.36%), USA (n=14, 15.91%) and Canada (n=6, 6.82%).
Stage 2 of the experiment lasted for about 4 days also with several breaks. 65 responses were received
in Stage 2, among which 54 (83.08%) were valid and 11 (11.92%) were invalid. 46 (85.19%) of the
valid responses were contributed by participants from Mainland China and 8 (14.81%) by participants
from Hong Kong. 6 (54.55%) of the invalid responses were probably produced by spammers. The main
contributing source channels and regions of valid data in Stage 2 were slightly different from Stage 1. Top
3 source channels were prodege (n=25, 46.30%), bitcoinget (n=7, 12.96%) and instagc (n=5, 9.26 %);
top 3 source regions were Canada (n=22, 40.74%), USA (n=15, 27.78%) and Mainland China (n=11,
20.37%). Despite the different distributions of source channels and regions, the data obtained from Stage
1 and Stage 2 were highly similar, suggesting that the experiment was highly replicable.
In total, we obtained 200 responses in this experiment, among which 142 (71%) were valid. The valid
responses showed high consistency in their answers to the language tasks in Part 2 and Part 3. For exam-
ple, among the 127 valid responses fromMainland China, the answers to the word segmentation questions
in Part 2 had an average consistency2 of 74.30% (SD=12.94%), while the semantic similarity ratings in
2Consistency here means the percentages of the majority-voted answers; if we consider the second most frequent answers,
109
Part 3 had an average consistency of 58.46% (SD=21.97%). Majority-voted answers and ratings were
verified by a team of trained linguists as the most likely segmentations/ratings of the given linguistic
materials, while the less popular answers were also verified as possible or reasonable alternatives. These
results suggest that the language behavioral data acquired in this experiment, when pruned of invalid
responses, were largely consistent with expectations for native language users? judgment.
4.1 Chinese Word Segmentation Data Example
In the experiment, the participants were required to segment 12 short Chinese sentences; because of space
limitation, we will only present the results of one representative sentence here. The theoretical segmen-
tation result of the target Chinese sentence ?????????????? (lit., character by character:
only-have-rely on-depend on-crowd-mass-only-can-do-well-job-work, ?The job can only be done well
by relying on the messes? ) is ???/??/??/?/?/?/?/??? (lit. word by word: only/rely on/the
messes/only/can/do/well/job) in which the symbol ?/? indicates word boundaries. The segmentation re-
sults of this sentence obtained in the experiment are listed in Table 1. We can see that the consistency
is high, however the majority-voted result ???/??/??/??/??/??? is different from the the-
oretical segmentation result. Most participants treat the slice ???? as one word instead of two words
and the same thing happened to the slice ????. Speakers? intuition can be different from theoretical
analysis: this is an important clue to investigate the representation of Chinese words in the mental lexicon
of Chinese speakers.
Segmentation Result n %
??/??/??/??/??/?? 100 78.74
??/??/??/??/?/?/?? 11 8.66
??/??/??/?/?/??/?? 5 3.94
??/??/??/?/?/?/?/?? 4 3.15
??/??/??/?/???/?? 2 1.57
??/??/??/?????? 1 0.79
??/??/??/??/???? 1 0.79
??/????/??/???? 1 0.79
??/????/??/?/?/?? 1 0.79
?/?/??/??/?/?/?/?/?? 1 0.79
Total 127 100
Table 1: Chinese Word Segmentation Data Example
4.2 Semantic Similarity Rating Data Example
Semantic transparency affects the representation and processing of compounds (Libben, 1998; Han et al.,
2014). In the experiment, we use semantic similarity rating tasks to collect semantic transparency data
of 12 compounds which can be used in the studies of mental lexicon. Here we will only discuss two of
them in detail. In Chinese, ???? (dongxi, east-west, ?thing?) is a typical semantically opaque word,
because its literal meaning is ?east and west? but its actual meaning is ?thing?: we can hardly find any link
between the two. In contrast, ???? (bangzhu, help-assist, ?help?) is a typical semantically transparent
word, for its literal meaning equals its actual meaning. In our experiment, for each target word, we ask
the participants to rate to what extent the meaning of each character when it is used alone is similar to
its meaning in the target word. This kind of semantic similarity rating task enables us to estimate the
semantic transparency of the target words. The semantic similarity rating data of the above two words
are shown in Table 2, and for the results of all the words, see Table 3.
the consistency numbers can be much larger than the reported ones, especially the ones of semantic similarity rating results (see
Table 3).
110
?? dongxi, east-west, ?thing? ?? bangzhu, help-assist, ?help?
Rating Score ? dong, ?east? ? xi, ?west? ? bang, ?help? ? zhu, ?assist?
1 115 121 6 4
2 2 2 2 13
3 1 1 8 7
4 0 1 23 38
5 8 1 88 63
? 1 1 0 2
Total 127 127 127 127
Table 2: Semantic Similarity Rating Data Example
In the tables, the rating scores 1 to 5 and ??? mean ?not similar at all?, ?slightly similar?, ?moderately
similar?, ?very similar?, ?identical?, and ?unable to rate? respectively. The consistency of the semantic
similarity rating data is also very high. For example, most participants (115 out of 127) think the meaning
of ??? (dong, ?east?) when it is used alone is not similar at all to its meaning in the word ???? (dongxi,
east-west, ?thing?), and most participants (121 out of 127) think the meaning of ??? (xi, ?west?)when
it is used alone is not similar at all to its meaning in the word ???? (dongxi, east-west, ?thing?). The
consistency of the rating data of ???? (bangzhu, help-assist, ?help?) is not as high as ???? (dongxi,
east-west, ?thing?), but most participants choose 5 which is our expectation and it is also normal that
many participants choose 4, since it is next to 5. The semantic transparency estimation of the two words
based on these data is quite consistent with our expectation.
5 The Quality Control Issues
In order to obtain high quality data in crowdsourcing environments, it is fundamental to identify invalid
responses. Checkpoint questions can be used to identify them. Checkpoint questions should satisfy two
conditions. Firstly, a checkpoint question should be super easy, since making wrong judgments to super
easy questions is a clear signal of carelessness. Secondly, a checkpoint question should have a publicly
recognized correct answer or it cannot act as a standard. Checkpoint questions can be open-ended or
close-ended. Open-ended questions are usually more robust than close-ended ones, since their answers
are difficult to guess.
There are at least 3 basic measures to deal with invalid responses: (1) blocking the submission of in-
valid responses; (2) rejecting the invalid responses that have been submitted; (3) refining the data-set
received and filter out invalid responses before analysis. Adopting validations on checkpoint questions is
a good strategy. A validation is a submission condition and the submission of responses will be blocked
if the validations of them are failed. Since checkpoint questions can identify invalid responses, using val-
idations on checkpoint questions can block the submissions of invalid responses. Crowdflower supports
validation but it is implemented on the client end, so can be bypassed; but average participants usually
don?t have the required expertise to do that, so it is largely reliable.
After the adoption of the above quality control measures, spammers are the major threats to data qual-
ity. It can be exhausted to combat spammers manually, because of their high speed and randomness, so
automatic monitor programs should be used to combat them. Monitor programs use patterns to detect
spammers. Patterns may depend on the specifics of different crowdsourcing practices, but there are some
general patterns which are based on the typical behaviors of spammers and can be applied to almost all
crowdsourcing practices. One pattern is the ?temporal pattern?, abnormal high speed is an obvious feature
of spammers and can be used as a general pattern. There are two cases. One case is that the completion
time of a response is abnormally short. For instance, the normal completion time of a response is around
9 minutes, but the human spammers only needed an average of 138 seconds and the robot spammers
only needed an average of 20 seconds. The other case is that the time interval between 2 responses is
111
Rating Score
Word Character 1 2 3 4 5 ? Total
?? ? 115 2 1 0 8 1 127? 121 2 1 1 1 1 127
?? ? 94 12 8 3 9 1 127? 100 11 8 2 4 2 127
?? ? 79 15 10 9 11 3 127? 63 32 15 7 5 5 127
?? ? 109 8 3 0 7 0 127? 84 29 7 2 3 2 127
?? ? 97 13 4 3 8 2 127? 110 7 3 0 3 4 127
?? ? 80 15 15 3 9 5 127? 98 12 6 0 3 8 127
?? ? 6 2 8 23 88 0 127? 4 13 7 38 63 2 127
?? ? 2 8 12 27 78 0 127? 32 29 19 24 20 3 127
?? ? 20 23 24 26 32 2 127? 19 41 30 21 13 3 127
?? ? 4 22 20 43 36 2 127? 12 25 31 33 24 2 127
?? ? 3 13 13 44 54 0 127? 3 8 16 42 56 2 127
?? ? 3 5 16 41 62 0 127? 2 11 21 43 50 0 127
Table 3: The Complete List of Semantic Similarity Rating Data
abnormally short and several such events take place one after another. This temporal pattern can be used
to detect concurrent attacks. The other pattern is the ?violation of validations?. If the validations of a
response failed but it was still submitted, then the validations were bypassed and this is a typical behavior
of spammers. Once a spammer is detected, we can block it and reject all the responses it submitted if
the crowdsourcing platform supports these methods, otherwise we can just pause the task for a while in
order to avoid or reduce its attack.
The effect of any single quality control measures is limited; multiple measures should be used at the
same time to form a quality control system with much more control power. A reasonable quality control
system should notice two key points: (1) maximally block the submission of invalid responses, and (2)
maximally filter invalid responses out.
6 Conclusion
Our study showed that crowdsourcing is a very powerful experimental design for exploration cognitive
access to the shared Mental Lexicon of the speakers of the same language. We showed that Mandarin
speakers shared the same strategy in determination of lexical units. The strategy seems to be match more
closely with distributional information. This suggests an empirical approach to lexical unit determination
which is then subject to the influence of language use and can lead to changes in the mental lexicon.
Although our study is far from conclusive as a proof for the shared lexical access strategy, it does point
out to the great potential of pursuing this issue using crowdsourcing experiments.
112
Acknowledgements
The work described in this paper was supported by a grant from the Research Grants Council of the Hong
Kong Special Administrative Region, China (Project No. 544011).
References
Adam J Berinsky, Gregory A Huber, and Gabriel S Lenz. 2011. Using mechanical turk as a subject recruitment
tool for experimental research. Submitted for review.
Chris Biemann. 2013. Creating a system for lexical substitutions from scratch using crowdsourcing. Language
Resources and Evaluation, 47(1):97?122.
Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with amazon?s mechanical
turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s
Mechanical Turk, pages 1?12. Association for Computational Linguistics.
Matthew JC Crump, John V McDonnell, and Todd M Gureckis. 2013. Evaluating amazon?s mechanical turk as a
tool for experimental behavioral research. PloS one, 8(3):e57410.
Iryna Gurevych and Torsten Zesch. 2013. Collective intelligence and language resources: introduction to the
special issue on collaboratively constructed language resources. Language Resources and Evaluation, 47(1):1?
7.
Yi-Jhong Han, Shuo-chieh Huang, Chia-Ying Lee, Wen-Jui Kuo, and Shih-kuen Cheng. 2014. The modulation
of semantic transparency on the recognition memory for two-character chinese words. Memory & Cognition,
pages 1?10.
Chu-Ren Huang, Elanna I. J. Tseng, Dylan B. S. Tsai, and Brian Murphy. 2003. Cross-lingual Portability of Se-
mantic relations: Bootstrapping Chinese WordNet with English WordNet Relations. Language and Linguistics,
4.3:509?532.
Chu-Ren Huang, Ru-Yng Chang, and Shiang bin Li, 2010. Ontology and the Lexicon, chapter Sinica BOW:
Integration of Bilingual WordNet and SUMO, pages 201?211. Cambridge University Press, Cambridge.
Gary Libben. 1998. Semantic transparency in the processing of compounds: Consequences for representation,
processing, and impairment. Brain and Language, 61(1):30 ? 44.
Winter Mason and Siddharth Suri. 2012. Conducting behavioral research on amazon?s mechanical turk. Behavior
research methods, 44(1):1?23.
Winter Mason and Duncan J Watts. 2010. Financial incentives and the performance of crowds. ACM SigKDD
Explorations Newsletter, 11(2):100?108.
George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39?41.
Robert Munro, Steven Bethard, Victor Kuperman, Vicky Tzuyin Lai, Robin Melnick, Christopher Potts, Tyler
Schnoebelen, and Harry Tily. 2010. Crowdsourcing and language studies: the new generation of linguistic data.
In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s
Mechanical Turk, pages 122?130. Association for Computational Linguistics.
Ian Niles and Adam Pease. 2001. Towards a standard upper ontology. In Proceedings of the international confer-
ence on Formal Ontology in Information Systems-Volume 2001, pages 2?9. ACM.
Gabriele Paolacci, Jesse Chandler, and Panagiotis G Ipeirotis. 2010. Running experiments on amazon mechanical
turk. Judgment and Decision making, 5(5):411?419.
David G Rand. 2012. The promise of mechanical turk: How online labor markets can help theorists run behavioral
experiments. Journal of theoretical biology, 299:172?179.
Tyler Schnoebelen and Victor Kuperman. 2010. Using amazon mechanical turk for linguistic research. Psi-
hologija, 43(4):441?464.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and Andrew Y Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language tasks. In Proceedings of the conference on empirical
methods in natural language processing, pages 254?263. Association for Computational Linguistics.
113
Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 147?156,
Coling 2014, Dublin, Ireland, August 24 2014.
Building a Semantic Transparency Dataset of Chinese Nominal
Compounds: A Practice of Crowdsourcing Methodology
Shichang Wang, Chu-Ren Huang, Yao Yao, Angel Chan
Department of Chinese and Bilingual Studies
The Hong Kong Polytechnic University
Hung Hom, Kowloon, Hong Kong
shi-chang.wang@connect.polyu.hk
{churen.huang, y.yao, angel.ws.chan}@polyu.edu.hk
Abstract
This paper describes the work which aimed to create a semantic transparency dataset of Chi-
nese nominal compounds (SemTransCNC 1.0) by crowdsourcing methodology. We firstly se-
lected about 1,200 Chinese nominal compounds from a lexicon of modern Chinese and the Sinica
Corpus. Then through a series of crowdsourcing experiments conducted on the Crowdflower
platform, we successfully collected both overall semantic transparency and constituent semantic
transparency data for each of them. According to our evaluation, the data quality is good. This
work filled a gap in Chinese language resources and also practiced and explored the crowdsourc-
ing methodology for linguistic experiment and language resource construction.
1 Introduction
The meaning of ???? (m?hu, horse-tiger, ?careless?) has nearly nothing to do with neither ??? (m?,
?horse?) nor ??? (h?, ?tiger?). However the meaning of ???? (d?ol?, road-way, ?road?) is basically
equal to ??? (d?o, ?road?) or ??? (l?, ?way?). And there are intermediate cases too, for instance, ??
?? (ji?ngh?, river-lake, ?all corners of the country?), its meaning is not equal to ??? (ji?ng, ?river?)
plus ??? (h?, ?lake?), but clear relatedness between them can be observed. This phenomenon is called
semantic transparency of compounds. We distinguish between overall semantic transparency (OST) and
constituent semantic transparency (CST). The semantic transparency of a compound, i.e., the overall se-
mantic transparency, is the extent to which the compound retains its literal meaning in its actual meaning.
The semantic transparency of a constituent of a compound, i.e., the constituent semantic transparency, is
the extent to which the constituent retains its meaning in the actual meaning of the compound. Semantic
similarity between the literal meaning and the actual meaning of a compound can be used to estimate the
overall semantic transparency of a compound, for the more the literal meaning is retained in the actual
meaning, the more similar they are. The same technique can be used to estimate constituent semantic
transparency. Semantic transparency can be quantified; if we assign 0 to ?fully opaque? and assign 1 to
?fully transparent?, then semantic transparency can be quantified as a closed interval [0, 1].
The quantitative analysis of semantic transparency must be supported by semantic transparency
datasets. In previous semantic transparency related studies on Chinese compounds, some researchers
created some datasets to support their own studies. But this kind of datasets are usually relatively small
and restrictive, so cannot be used widely, for example, (??? and??, 2001; Myers et al., 2004;?
??, 2008; Mok, 2009), etc. Some datasets, although large enough and can be used in other studies, are
not publicly accessible, for example, (??? and???, 1999;?? and???, 2005), etc. A large
and publicly accessible semantic transparency dataset of Chinese compounds is still a gap in Chinese
language resources.
Crowdsourcing, as an emergingmethod of data collection and resource construction (Snow et al., 2008;
Callison-Burch and Dredze, 2010; Munro et al., 2010; Schnoebelen and Kuperman, 2010; Gurevych and
Zesch, 2013; Wang et al., 2013) and an emerging method of behavioral experiment (Paolacci et al., 2010;
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
147
Berinsky et al., 2011; Mason and Suri, 2012; Rand, 2012; Crump et al., 2013), is attracting more andmore
attention from the field of language study and language computing. As a method of data collection and
resource construction, it has the advantages of high speed and low cost, etc. It can use redundancy to filter
out noise in order to improve data quality; if used properly, it can produce expert-level data. As a method
of experiment, besides the above advantages, it also has the following ones, (1) it is easier to obtain large
samples, because the amount of potential participants is huge; (2) the diversity of participants is good,
because the participants are from different places and have different backgrounds; (3) crowdsourcing
environments are usually anonymous, so it is easier to collect certain sensitive data.
2 Method
2.1 Compound Selection
We use the following criteria to select compounds, (1) they are disyllabic nominal compounds; (2) each
of them has the structure NN, AN, or VN; (3) they are composed of free morphemes; (4) they have
mid-range word frequencies; and (5) they are used in both Mainland China and Taiwan. And we select
compounds according to the following procedure:
(1) Extract monosyllabic nouns, adjectives and verbs mainly according to ?The Dictionary of Con-
temporary Chinese (the 6th edition)? (??????,? 6?), and thus we get three sets, a) the set of
monosyllabic nouns, N; b) the set of monosyllabic adjectives, A; and c) the set of monosyllabic verbs, V.
(2) Extract the words of the structure NN, AN, or VN 1 from the ?Lexicon of Common Words in
Contemporary Chinese? (????????). In this step, NN means both morphemes of the word
appear in the set N; AN means the first morpheme appears in the set A and the second appears in the set
N; VN means the first morpheme appears in the set V and the second appears in the set N. After this step,
we get ?word list 1?.
(3) Extract the words which have mid-range frequencies 2 from the Sinica Corpus 4.0 (Chen et al.,
1996). These words are represented in traditional Chinese characters. We convert them into simplified
Chinese characters and only reserve the words which also appear in ?word list 1?. After this step, we get
?word list 2?.
(4) Manually verify ?word list 2? to generate the final list. Things need to be verified include the
following aspects. (a) Because in ?word list 2? word structures are judged automatically, there are many
errors, so we have to verify the correctness of the word structure judgments. (b) We have to make sure
that the morphemes of each word are free morphemes. (c) We also need to delete some proper nouns.
The words we selected appear in both Sinica Corpus 4.0 and ?Lexicon of Common Words in Contem-
porary Chinese?. Since there is no completely reliable criterion to identify Chinese word, appearing in
two lexicons ensures their word identity. This also ensures that they are used in both Mainland China and
Taiwan, and further means they are quite possible to be shared in other Chinese language communities,
for example Hong Kong, Macau, and Singapore, etc.
According to above criteria and procedure, we selected a total of 1,176 words. 664 (56.46%) of them
have the structure NN; 322 (27.38%) have the structure AN; and 190 (16.16%) have the structure VN.
2.2 Experimental Design
Normally, a crowdsourcing experiment should be reasonably small in size. We randomly divide these
1,176 words into 21 groups, G
i
(i = 1, 2, 3, ..., 21); each group has 56 words.
1See??? and??? (1998), and Huang (1998) for relevant statistics.
2We use cumulative frequency feature to determine mid-range frequency. Sort the word frequency list of Sinica Corpus
4.0 descendingly; then calculate cumulative frequency word by word until each word corresponds with a cumulative frequency
value; finally, plot a curve on a coordinate plane whose x-axis represents the ranks of words in the sorted list, and the y-axis
represents cumulative frequency values. Very apparently, this curve can be divided into three successive phases; the words
within each phase have similar word frequency features. According to this, we identify three word frequency categories, 5,163
high-frequency words (frequency range: [182, 581823], cumulative frequency range: [0%, 80%]), 19,803 mid-range frequency
words (frequency range: [23, 181], cumulative frequency range: (80%, 93%]), and 177,496 low-frequency words (frequency
range: [1, 22], cumulative frequency range: (93%, 100%]). Sinica Corpus 4.0 contains about 11.2 million word tokens.
148
Questionnaires
We collect overall semantic transparency (OST) and constituent semantic transparency (CST) data of
these words. In order to avoid interaction, we designed two kinds of questionnaires to collect OST data
and CST data respectively. SoG
i
(i = 1, 2, 3, ..., 21) has two questionnaires, one OST questionnaire for
OST data collection and one CST questionnaire for CST data collection. Besides titles and instructions,
each questionnaire has 3 sections. Section 1 is used to collect identity information includes gender, age,
education and location. Section 2 contains four very simple questions about the Chinese language; the
first two questions are open-ended Chinese character identification questions, the third question is a close-
ended homophonic character identification question, and the fourth one is a close-ended antonymous
character identification question; different questionnaires use different questions. Section 3 contains the
questions for semantic transparency data collection. Suppose AB is a disyllabic nominal compound, we
use the following question to collect its OST rating scores: ?How is the sum of the meanings of A and
B similar to the meaning of AB?? And use the following two questions to collect its CST rating scores
of its two constituents: ?How is the meaning of A when it is used alone similar to its meaning in AB??
and ?How is the meaning of B when it is used alone similar to its meaning in AB??. 7-point scales are
used in section 3; 1 means ?not similar at all? and 7 means ?almost the same?.
In order to evaluate the data received in the experiments, we embedded some evaluation devices in the
questionnaires. We mainly evaluated intra-group and inter-group consistency; and if the data have good
intra-group and inter-group consistency, we can believe that the data quality is good. In each group we
choose two words and make them appear twice, we call them intra-group repeated words and we can use
them to evaluate the intra-group consistency. We insert into each group two same extra words, w
1
??
??, w
2
????, to evaluate the inter-group consistency.
Quality Control Measures
On a crowdsourcing platform like Crowdflower, the participants are anonymous, they may try to cheat
and submit invalid data, and they may come from different countries and speak different languages rather
than the required one. There may be spammers who continuously submit invalid data at very high speed
and they may even bypass the quality control measures to cheat for money. In order to ensure that the
participants are native Chinese speakers and to improve data quality, we use the following measures, (1)
a participant must correctly answer the first two Chinese character identification questions in the section
2s of the questionnaires, and he/she must correctly answer at least one of the last two questions in these
section 2s; (2) If a participant do not satisfy the above conditions, he/she will not see Section 3s; (3) each
word stimulus in section 3s has an option which allows the participants to skip it in case he/she does not
recognize that word; (4) all the questions in the questionnaires must be answered except the ones which
allow to be skipped and are explicitly claimed to be skipped; (5) we wrote a monitor program to detect
and resist spammers automatically; (6) after the experiment is finished, we will analyze the data and filter
out invalid data, and we will discuss this in detail in section 3.
2.3 Experimental Platform and Procedure
We choose Crowdflower as our experimental platform, because according to our previous experiments,
it is a feasible crowdsourcing platform to collect Chinese language data. We create one task for each
questionnaire on the platform; there are 21 groups of word and each group has one OST questionnaire
and one CST questionnaire, so there are a total of 42 tasksT ost
i
, T
cst
i
(i = 1, 2, 3, ..., 21). We publish these
42 tasks successively, and for each task we create a monitor program to detect and resist spammers. All
of these tasks use the following parameters: (1) each task will collect 90 responses; (2) we pay 0.15USD
for each response of OST questionnaire and pay 0.25USD for each response of CST questionnaire; (3)
each worker account of Crowdflower can only submit one response for each questionnaire and each IP
address can only submit one response for each questionnaire; (4) we only allow the workers from the
following regions (according to IP addresses) to submit data: Mainland China, Hong Kong, Macau,
Taiwan, Singapore, Malaysia, USA, UK, Canada, Australia, Germany, France, Italy, New Zealand, and
Indonesia; and we can dynamically disable or enable certain regions on demand in order to ensure both
data quality and quantity.
149
3 Data Refinement and Result Calculation
TheOST dataset produced by theOST taskT ost
i
(i = 1, 2, 3, ..., 21) isDost
i
. The CST dataset produced by
the CST task T cst
i
is Dcst
i
. Each dataset contains 90 responses. Because of the nature of crowdsourcing
environment, there are many invalid responses in each dataset; so firstly we need to filter them out in
order to refine the data. A response is invalid if (1) its completion time is less than 135 seconds (for
OST responses); its completion time is less than 250 seconds (for CST responses) 3; or (2) it failed to
correctly answer the first two questions of section 2s of the questionnaires; or (3) it wrongly answered
the last two questions of section 2s of the questionnaires; or (4) it skipped one or more words in section
3s of the questionnaires; or (5) it used less than two numbers on the 7-point scales in section 3s of the
questionnaires. The statistics of valid response are shown in Table 1.
The OST dataset Dost
i
(i = 1, 2, 3, ..., 21) contains n
i
valid responses; it means word w in the OST
dataset of the ith group has n
i
OST rating scores; the arithmetic mean of these n
i
OST rating scores is
the OST result of word w. The CST results of the two constituents of word w are calculated using the
same algorithm.
OST CST
G
i
n % n %
G
1
54 60 59 65.56
G
2
60 66.67 59 65.56
G
3
55 61.11 60 66.67
G
4
59 65.56 59 65.56
G
5
50 55.56 55 61.11
G
6
55 61.11 52 57.78
G
7
53 58.89 53 58.89
G
8
60 66.67 50 55.56
G
9
48 53.33 52 57.78
G
10
57 63.33 62 68.89
G
11
46 51.11 56 62.22
G
12
48 53.33 58 64.44
G
13
51 56.67 52 57.78
G
14
50 55.56 50 55.56
G
15
52 57.78 52 57.78
G
16
57 63.33 56 62.22
G
17
50 55.56 46 50.55
G
18
51 56.67 53 58.89
G
19
50 55.56 49 54.44
G
20
50 55.56 47 52.22
G
21
50 55.56 50 55.56
Max 60 66.67 62 68.89
Min 46 51.11 46 50.55
Median 51.5 57.22 53 58.89
Mean 52.67 58.52 53.81 59.76
SD 4.09 4.55 4.49 5.04
Table 1: The Amount of Valid Response in the OST and CST Datasets of Each Group
4 Evaluation
Three kinds of evaluation measures are used, (1) the intra-group consistency of the OST and CST results,
(2) the inter-group consistency of the OST and CST results, and (3) the correlation between the OST and
CST results.
3Each OST questionnaire has about 70 questions, and each CST questionnaire has about 130; in an OST or CST question-
naire, almost all the questions are the same except the stimuli words and can be instantly answered by intuition; note that a
participant can take part in as many as 42 tasks; according to our test, if a participant is familiar with the tasks, he/she can
answer each question in less than 2 seconds (less than 1 second to identify the stimulus word and another less than 1 second
to rate it) without difficulty. 70 ? 2 = 140 seconds, the expected time should be less than this, so we use 135 seconds as
the temporal threshold for valid OST responses. The calculation of the temporal threshold for valid CST responses is similar,
130? 2 = 260 seconds, the expected time should be less than this, so we use 250 seconds.
150
4.1 Intra-group Consistency
In each group G
i
(i = 1, 2, 3, ..., 21), we selected two words w
i,1
, w
i,2
(intra-group repeated words) and
made them appear twice between which there is enough distance; we can calculate the difference values
between the results of the two appearances of these words.
Intra-group Consistency of OST Results
There are 21 groups and in each group there are two intra-group repeated words, so there are a total of 42
such words. Each intra-group repeated word appears twice, so we can obtain two OST results r
1
, r
2
. The
difference value between the two results, d = |r
1
? r
2
|, of each intra-group repeated word is calculated,
so there are 42 difference values. Among them, the maximum value is 0.29; the minimum value is 0;
the median is 0.1; their mean is 0.11; and their standard deviation is 0.08; all of these values are low and
indicate that these OST datasets have good intra-group consistency (see Table 2).
Intra-group Consistency of CST Results
Each intra-group repeated word has two constituents, c
1
, c
2
, so each constituent gets two CST results, i.e.,
r
c1,1
, r
c1,2
and r
c2,1
, r
c2,2
. We calculate the difference values for the two constituents, d
1
= |r
c1,1
?r
c1,2
|
and d
2
= |r
c2,1
? r
c2,2
|, and get 42 difference values of the first constituents and 42 difference values
of the second constituents. Among the difference values of the first constituents, the maximum value
is 0.27; the minimum value is 0; the median is 0.09; their mean is 0.1, and their standard deviation is
0.07; all of these values are low, this indicates that the CST results of the first constituents in the CST
datasets of the 21 groups have good intra-group consistency. Among the difference values of the second
constituents, the maximum value is 0.36; the minimum value is 0; the median is 0.07; their mean is 0.09,
and their standard deviation is 0.09; all of these values are low; this indicates that the CST results of the
second constituents in the CST datasets of the 21 groups have good intra-group consistency (see Table
3). So these 21 CST datasets have good intra-group consistency.
4.2 Inter-group Consistency
We inserted two inter-group repeated words, w
1
????, w
2
????, into all of these 21 groups G
i
(i =
1, 2, 3, ..., 21); we can evaluate the inter-group consistency by comparing their semantic transparency
rating results in different groups. Since w
1
, w
2
appear in all OST and CST questionnaires of 21 groups,
we can obtain (1) 21 OST results of w
1
, (2) 21 OST results of w
2
, (3) 21 CST results of each of the two
constituents w
1,c1
, w
1,c2
of w
1
, and (4) 21 CST results of each of the two constituents w
2,c1
, w
2,c2
of w
2
.
Standard deviation can be used to measure difference, for example, the standard deviation of the 21 OST
results of w
1
is 0.2; this value is small and indicates high consistency; because these 21 results are from
the OST datasets of 21 groups respectively, so we can say that these 21 OST datasets have good inter-
group consistency. The standard deviation of the 21 OST results of w
2
is 0.14; the standard deviation of
21 CST results of the first constituent of w
1
is 0.2, and that of the second is 0.18; the standard deviation
of 21 CST results of the first constituent of w
2
is 0.15, and that of the second is 0.2; all of these values
are small and all of them indicate good inter-group consistency (see Table 4).
4.3 Correlation between OST and CST Results
Each compound in the datasets has two constituents; both constituents affect the OST of the compound,
but neither of them can solely determine the OST of the compound. So the mean of the two CST values
of a compound is a fairly good estimation of its OST value. Therefore, if the datasets are reliable, in each
group, we should observe strong correlation between the OST results and their corresponding means of
the CST results. For each group, we calculate three Pearson product-moment correlation coefficients (r);
r
1
is the r between the OST results and their corresponding CST results of the first constituents; r
2
is
the r between the OST results and their corresponding CST results of the second constituents; and r
3
is
the r between the OST results and their corresponding means of the CST results. The r
3
values of the 21
groups are all greater than 0.9 which indicates very strong correlation; among them, the maximum value
is 0.96; the minimum value is 0.91; and their mean is 0.94 (SD = 0.02); the r
1
and r
2
values are also
151
Gi
w
i,1/2
r
1
r
2
d
G
1
?? 5.26 5.26 0
?? 3.57 3.61 0.04
G
2
?? 5.63 5.75 0.12
?? 2.68 2.9 0.22
G
3
?? 5.67 5.58 0.09
?? 3.51 3.62 0.11
G
4
?? 5.31 5.32 0.02
?? 3.19 3.02 0.17
G
5
?? 5.36 5.32 0.04
?? 3.12 3.3 0.18
G
6
?? 5.53 5.4 0.13
?? 5.25 4.96 0.29
G
7
?? 5.25 5.23 0.02
?? 4.19 4.11 0.08
G
8
?? 5.48 5.33 0.15
?? 3.2 3.37 0.17
G
9
?? 5.19 5.19 0
?? 3.69 3.75 0.06
G
10
?? 5.49 5.63 0.14
?? 3.46 3.54 0.09
G
11
?? 5.48 5.39 0.09
?? 3.26 3.24 0.02
G
12
?? 5.19 5.4 0.21
?? 3.6 3.54 0.06
G
13
?? 5.47 5.39 0.08
?? 3.37 3.41 0.04
G
14
?? 5.54 5.52 0.02
?? 3.46 3.56 0.1
G
15
?? 5.54 5.37 0.17
?? 3.29 3.56 0.27
G
16
?? 5.49 5.53 0.04
?? 3.82 4.07 0.25
G
17
?? 5.2 5.38 0.18
?? 3.76 3.76 0
G
18
?? 5.31 5.18 0.14
?? 3.41 3.25 0.16
G
19
?? 5.22 5.28 0.06
?? 4.04 3.88 0.16
G
20
?? 5.28 5.18 0.1
?? 4.04 3.84 0.2
G
21
?? 5.06 5.02 0.04
?? 3.8 4 0.2
Max 0.29
Min 0
Median 0.1
Mean 0.11
SD 0.08
Table 2: The Intra-group Consistency of the OST Results of Each Group
reasonably high (see Table 5)4. The results support the reliability of these datasets.
5 Merging and Normalization
The evaluation results show that the collected data are generally reliable and have relatively high intra-
group and inter-group consistency which further indicate that these datasets share similar scale and are
basically comparable, so we can merge the 21 OST datasets into one big OST dataset D
ost
and merge
the 21 CST datasets into one big CST dataset D
cst
. When we merge these datasets, we delete all the
extra words which are used to evaluate the inter-group consistency; for the repeated words which are
4After merging and normalization (see Section 5), we calculated these three correlation coefficients betweenD
ost
andD
cst
,
the results are r
1
= 0.68, r
2
= 0.68, r
3
= 0.87.
152
c1
c
2
G
i
w
i,1/2
r
c1,1
r
c1,2
d
1
r
c2,1
r
c2,2
d
2
G
1
?? 3.83 4.05 0.22 5.49 5.42 0.07
?? 2.88 3.03 0.15 3.92 3.92 0
G
2
?? 5.12 5.22 0.1 5.24 5.1 0.14
?? 4.27 4.27 0 2.19 2.51 0.32
G
3
?? 5.12 5.08 0.03 5.35 5.4 0.05
?? 2.92 2.95 0.03 3.22 3.42 0.2
G
4
?? 4.51 4.34 0.17 5.56 5.27 0.29
?? 2.39 2.49 0.1 4.22 4.12 0.1
G
5
?? 4.75 4.64 0.11 5.09 5.15 0.05
?? 2.29 2.4 0.11 4.67 4.76 0.09
G
6
?? 5.4 5.23 0.17 5.35 5.4 0.06
?? 5.08 5.02 0.06 5.38 5.46 0.08
G
7
?? 4.7 4.83 0.13 5.13 5.13 0
?? 3.85 3.94 0.09 4.45 4.57 0.11
G
8
?? 5.06 4.88 0.18 5.28 5.3 0.02
?? 3.24 3.14 0.1 3.36 3.16 0.2
G
9
?? 5 4.98 0.02 5 4.98 0.02
?? 3.63 3.71 0.08 3.71 3.83 0.12
G
10
?? 4.53 4.6 0.06 5.37 5.39 0.02
?? 3.13 3.21 0.08 3.15 3.16 0.02
G
11
?? 4.45 4.55 0.11 5.36 5.55 0.2
?? 3.8 3.79 0.02 2.64 3 0.36
G
12
?? 4.69 4.52 0.17 4.97 4.9 0.07
?? 3.03 3.21 0.17 3.28 3.4 0.12
G
13
?? 4.15 4.19 0.04 5.15 5.27 0.12
?? 2.52 2.79 0.27 3.44 3.42 0.02
G
14
?? 4.42 4.36 0.06 5.14 5.12 0.02
?? 3.56 3.5 0.06 3.08 3.06 0.02
G
15
?? 5.08 5.02 0.06 5.06 5.13 0.08
?? 3.21 3 0.21 3.46 3.5 0.04
G
16
?? 4.34 4.34 0 5.11 5.09 0.02
?? 3.8 3.63 0.18 3.32 3.38 0.05
G
17
?? 4.76 4.72 0.04 4.74 4.87 0.13
?? 3.93 3.96 0.02 3.89 3.87 0.02
G
18
?? 4.26 4.32 0.06 4.77 4.7 0.08
?? 3.4 3.36 0.04 2.74 2.68 0.06
G
19
?? 4.63 4.61 0.02 4.57 4.49 0.08
?? 3.55 3.29 0.27 3.53 3.41 0.12
G
20
?? 4.98 4.91 0.06 5.15 5.17 0.02
?? 2.94 2.96 0.02 4.7 4.45 0.26
G
21
?? 4.68 4.56 0.12 5 4.98 0.02
?? 3.68 3.88 0.2 3.66 3.6 0.06
Max 0.27 0.36
Min 0 0
Median 0.09 0.07
Mean 0.1 0.09
SD 0.07 0.09
Table 3: The Intra-group Consistency of the CST Results of Each Group
used to evaluate the intra-group consistency, the final result of each of them is the mean of its two results.
According to our definition, the range of semantic transparency value is [0, 1], but the experimental results
are obtained using 7-point scales, so we need to normalize these results in order to map them to the range
[0, 1]. The normalized OST and CST results will be merged into D
ost
and D
cst
respectively. Assume
that, in the dataset D
ost
, the OST result of the ith (i = 1, 2, 3, ..., 1176) word is Sw
i
, and the normalized
result is S?w
i
, then,
S
?w
i
=
S
w
i
? 1
6
153
OST CST
G
i
w
1
w
2
w
1,c1
w
1,c2
w
2,c1
w
2,c2
G
1
2.94 5.52 2.85 2.97 4.56 5.56
G
2
3.6 5.55 3.15 3.2 4.92 5.75
G
3
3.51 5.64 3.17 3.23 4.75 5.58
G
4
3.81 5.68 3.53 3.59 4.58 5.42
G
5
3.74 5.46 3.38 3.56 4.64 5.55
G
6
3.65 5.55 3.63 3.56 4.85 5.65
G
7
3.58 5.51 3.47 3.58 4.75 5.23
G
8
3.22 5.53 3.4 3.36 4.8 5.48
G
9
3.31 5.15 3.48 3.52 4.69 5.42
G
10
3.58 5.53 3.42 3.34 4.69 5.27
G
11
3.7 5.67 3.46 3.32 4.52 5.36
G
12
3.33 5.71 3.19 3.28 4.41 5.14
G
13
3.47 5.78 3.58 3.56 4.73 5.38
G
14
3.48 5.58 2.94 2.94 4.42 5.3
G
15
3.4 5.42 3.42 3.27 4.62 5.1
G
16
3.47 5.56 3.34 3.25 4.59 5.16
G
17
3.6 5.56 3.3 3.26 4.5 5.17
G
18
3.67 5.67 3.36 3.34 4.47 5
G
19
3.28 5.56 3.2 3.29 4.37 5.18
G
20
3.56 5.48 3.21 3.36 4.72 5.34
G
21
3.62 5.32 3.2 3.28 4.5 5.24
Max 3.81 5.78 3.63 3.59 4.92 5.75
Min 2.94 5.15 2.85 2.94 4.37 5
Median 3.56 5.55 3.36 3.32 4.62 5.34
Mean 3.5 5.54 3.32 3.34 4.62 5.35
SD 0.2 0.14 0.2 0.18 0.15 0.2
Table 4: The Inter-group Consistency of the OST and CST Results
And assume that, in the datasetD
cst
, the CST result of the jth (j = 1, 2) constituent of the ith word is
S
c
i,j
, and the normalized result is S?c
i,j
, then,
S
?c
i,j
=
S
c
i,j
? 1
6
6 Distribution
Influenced by outliers and perhaps other factors, the OST and CST results cannot cover the whole range
of the scale [0, 1]; both ends shrink towards the central point 0.5, and the shrinkage of each end is about
0.2; nevertheless, the results can still assign proper ranks of semantic transparency to the compounds and
their constituents which are generally consistent with our intuitions. Among the normalized OST results,
the maximum is 0.81; the minimum is 0.28; the median is 0.63; and their mean is 0.62 (SD = 0.09).
Among the normalized CST results of the first constituents (C1.CST results), the maximum is 0.77; the
minimum is 0.19; the median is 0.57; and their mean is 0.56 (SD = 0.09). And among the normalized
CST results of the second constituents (C2.CST results), the maximum is 0.79; the minimum is 0.22; the
median is 0.6; and their mean is 0.58 (SD = 0.1). The distributions of OST, C1.CST, and C2.CST results
are similar; all of them are negatively skewed (see Figure 1), and their estimated skewnesses are ?0.66,
?0.77, and ?0.63 respectively. These distributions exhibit that more compounds and their constituents
in our datasets have relatively high semantic transparency values.
7 Conclusion
This work created a dataset of semantic transparency of Chinese nominal compounds (SemTransCNC
1.0), which filled a gap in Chinese language resources. It contains the overall and constituent semantic
transparency data of about 1,200 Chinese disyllabic nominal compounds and can support semantic trans-
parency related studies of Chinese compounds, for example, theoretical, statistical, psycholinguistic, and
154
Gi
r
1
r
2
r
3
G
1
0.68 0.68 0.91
G
2
0.72 0.72 0.93
G
3
0.76 0.78 0.96
G
4
0.76 0.77 0.96
G
5
0.75 0.56 0.95
G
6
0.63 0.72 0.91
G
7
0.83 0.78 0.94
G
8
0.76 0.77 0.96
G
9
0.68 0.81 0.95
G
10
0.84 0.83 0.95
G
11
0.78 0.71 0.91
G
12
0.72 0.77 0.95
G
13
0.85 0.86 0.96
G
14
0.69 0.85 0.95
G
15
0.68 0.82 0.95
G
16
0.82 0.85 0.95
G
17
0.79 0.83 0.94
G
18
0.81 0.86 0.96
G
19
0.76 0.8 0.95
G
20
0.76 0.75 0.94
G
21
0.73 0.86 0.96
Max 0.85 0.86 0.96
Min 0.63 0.56 0.91
Median 0.76 0.78 0.95
Mean 0.75 0.78 0.94
SD 0.06 0.07 0.02
Table 5: The Correlation Coefficients between the OST and CST Results
Normalized OST Results
Freq
uen
cy
0.0 0.2 0.4 0.6 0.8 1.0
0
100
200
300
Normalized C1.CST Results
Freq
uen
cy
0.0 0.2 0.4 0.6 0.8 1.0
0
100
200
300
Normalized C2.CST Results
Freq
uen
cy
0.0 0.2 0.4 0.6 0.8 1.0
0
100
200
300
Figure 1: The Distributions of the Normalized OST and CST Results
computational studies, etc. And this work was also a successful practice of crowdsourcing method for lin-
guistic experiment and language resource construction. Large scale language data collection experiments
which require large amount of participants are usually very difficult to conduct in laboratories using the
traditional paradigm. Crowdsourcing method enabled us to finish the data collection task within rela-
tively short period of time and relatively low budget (1,000USD); during the process of the experiment,
we needed not to organize and communicate with the participants, it saved a lot of time and energy. The
participants are from all over the world, so it is better than traditional laboratory method in the aspect
of participant diversity. The data collected have very good intra-group and inter-group consistency, the
OST and CST data highly correlate with each other as expected, and the results are consistent with our
intuitions: all of these indicate good data quality. The methods of questionnaire design, quality control,
data refinement, evaluation, emerging, and normalization can be used in crowdsourcing practices of the
same kind.
155
Acknowledgements
The work described in this paper was supported by grants from the Research Grants Council of the Hong
Kong Special Administrative Region, China (Project No. 544011 & 543512).
References
Adam J Berinsky, Gregory A Huber, and Gabriel S Lenz. 2011. Using mechanical turk as a subject recruitment
tool for experimental research. Submitted for review.
Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with amazon?s mechanical
turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s
Mechanical Turk, pages 1?12. Association for Computational Linguistics.
Keh-Jiann Chen, Chu-Ren Huang, Li-Ping Chang, and Hui-Li Hsu. 1996. Sinica Corpus: Design Methodology
for Balanced Corpora. In B.-S. Park and J.B. Kim, editors, Proceeding of the 11th Pacific Asia Conference on
Language, Information and Computation, pages 167?176. Seoul:Kyung Hee University.
Matthew JC Crump, John V McDonnell, and Todd M Gureckis. 2013. Evaluating amazon?s mechanical turk as a
tool for experimental behavioral research. PloS one, 8(3):e57410.
Iryna Gurevych and Torsten Zesch. 2013. Collective intelligence and language resources: introduction to the
special issue on collaboratively constructed language resources. Language Resources and Evaluation, 47(1):1?
7.
Shuanfan Huang. 1998. Chinese as a headless language in compounding morphology. New approaches to Chinese
word formation: Morphology, phonology and the lexicon in modern and ancient Chinese, pages 261?284.
Winter Mason and Siddharth Suri. 2012. Conducting behavioral research on amazon?s mechanical turk. Behavior
research methods, 44(1):1?23.
Leh Woon Mok. 2009. Word-superiority effect as a function of semantic transparency of chinese bimorphemic
compound words. Language and Cognitive Processes, 24(7-8):1039?1081.
Robert Munro, Steven Bethard, Victor Kuperman, Vicky Tzuyin Lai, Robin Melnick, Christopher Potts, Tyler
Schnoebelen, and Harry Tily. 2010. Crowdsourcing and language studies: the new generation of linguistic data.
In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s
Mechanical Turk, pages 122?130. Association for Computational Linguistics.
James Myers, Bruce Derwing, and Gary Libben. 2004. The effect of priming direction on reading chinese com-
pounds. Mental Lexicon Working Papers, 1:69?86.
Gabriele Paolacci, Jesse Chandler, and Panagiotis G Ipeirotis. 2010. Running experiments on amazon mechanical
turk. Judgment and Decision making, 5(5):411?419.
David G Rand. 2012. The promise of mechanical turk: How online labor markets can help theorists run behavioral
experiments. Journal of theoretical biology, 299:172?179.
Tyler Schnoebelen and Victor Kuperman. 2010. Using amazon mechanical turk for linguistic research. Psi-
hologija, 43(4):441?464.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and Andrew Y Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language tasks. In Proceedings of the conference on empirical
methods in natural language processing, pages 254?263. Association for Computational Linguistics.
AoboWang, CongDuyVuHoang, andMin-YenKan. 2013. Perspectives on crowdsourcing annotations for natural
language processing. Language Resources and Evaluation, 47:9?31.
???. 2008. ????????????????????. ??????, 1:82?90.
??? and??. 2001. ??????????????????. ??????, 1:53?59.
??? and???. 1999. ?????????,??????????. ????, 31(3):266?273.
??? and???. 1998. ?????????????????. ??????, 2(1):13.
?? and???. 2005. ????????????????????. ????, 28(6):1358?1360.
156
