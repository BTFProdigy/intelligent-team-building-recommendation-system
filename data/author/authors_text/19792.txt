Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1457?1467, Dublin, Ireland, August 23-29 2014.
A Beam-Search Decoder for Disfluency Detection
Xuancong Wang
1,3
Hwee Tou Ng
1,2
Khe Chai Sim
2
1
NUS Graduate School for Integrative Sciences and Engineering
2
Department of Computer Science, National University of Singapore
3
Human Language Technology, Institute for Infocomm Research, Singapore
xuancong84@gmail.com, {nght, simkc}@comp.nus.edu.sg
Abstract
In this paper
1
, we present a novel beam-search decoder for disfluency detection. We first pro-
pose node-weighted max-margin Markov networks (M3N) to boost the performance on words
belonging to specific part-of-speech (POS) classes. Next, we show the importance of measur-
ing the quality of cleaned-up sentences and performing multiple passes of disfluency detection.
Finally, we propose using the beam-search decoder to combine multiple discriminative models
such as M3N and multiple generative models such as language models (LM) and perform multi-
ple passes of disfluency detection. The decoder iteratively generates new hypotheses from current
hypotheses by making incremental corrections to the current sentence based on certain patterns
as well as information provided by existing models. It then rescores each hypothesis based on
features of lexical correctness and fluency. Our decoder achieves an edit-word F1 score higher
than all previous published scores on the same data set, both with and without using external
sources of information.
1 Introduction
Disfluency detection is a useful and important task in Natural Language Processing (NLP) because spon-
taneous speech contains a significant proportion of disfluency. The disfluencies in speech introduce noise
in downstream tasks like machine translation and information extraction. Thus, the task of disfluency
detection not only can help improve the readability of automatically transcribed speech, but also the
performance of downstream NLP tasks.
There are mainly two types of disfluencies: filler words and edit words. Filler words include filled
pauses (e.g., ?uh?, ?um?) and discourse markers (e.g., ?I mean?, ?you know?). They are insertions in
spontaneous speech that indicate pauses or mark boundaries in discourse. Thus, they do not convey
useful content information. Edit words are words that are spoken wrongly and then corrected by the
speaker. For example, consider the utterance:
I want a flight
Edit
? ?? ?
to Boston
Filler
? ?? ?
uh I mean
Repair
? ?? ?
to Denver
The phrase ?to Boston? forms the edit region to be replaced by ?to Denver?. The words ?uh I mean? are
filler words that serve to cue the listener about the error and subsequent correction. So, the cleaned-up
sentence would be ?I want a flight to Denver?, which is what the speaker originally intended to say. In
general, edit words are more difficult to detect than filler words, and so edit word prediction accuracy is
much lower. Thus, in this work, we mainly focus on edit word detection.
In Section 2, we briefly introduce previous work. In Section 3, we describe our improved baseline
system that will be integrated into our beam-search decoder. Section 4 presents our beam-search decoder
in detail. In Section 5, we describe our experiments and results. Section 6 gives the conclusion.
1
The research reported in this paper was carried out as part of the PhD thesis research of Xuancong Wang at the NUS
Graduate School for Integrated Sciences and Engineering.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1457
2 Previous Work
Researchers have tried many models for disfluency detection. Johnson and Charniak (2004) proposed a
TAG-based (Tree-Adjoining Grammar) noisy channel model, which showed great improvement over a
boosting-based classifier (Charniak and Johnson, 2001). Maskey et al. (2006) proposed a phrase-level
machine translation approach for this task. Liu et al. (2006) used conditional random fields (CRFs) (Laf-
ferty et al., 2001) for sentence boundary and edit word detection. They showed that CRFs significantly
outperformed maximum entropy models and hidden Markov models (HMM). Zwarts and Johnson (2011)
extended this model using minimal expected F-loss oriented n-best reranking. Georgila (2009) presented
a post-processing method during testing based on integer linear programming (ILP) to incorporate local
and global constraints. In addition to textual information, prosodic features extracted from speech have
been incorporated to detect edit words in some previous work (Kahn et al., 2005; Liu et al., 2006; Zhang
et al., 2006). Zwarts and Johnson (2011) also trained extra language models on additional corpora, and
compared the effects of adding scores from different language models as features during reranking. They
reported that the language models gained approximately 3% in F1-score for edit word detection on the
Switchboard development dataset. Qian and Liu (2013) proposed multi-step disfluency detection using
weighted max-margin Markov networks (M3N) and achieved the highest F-score of 84.1% without using
any external source of information. In this paper, we incorporate the M3N model into our beam-search
decoder framework with some additional features to further improve the result.
3 The Improved Baseline System
Weighted max-margin Markov networks (M3N) (Taskar et al., 2003) have been shown to outperform
CRF in (Qian and Liu, 2013), since it can balance precision and recall easily by assigning different loss
penalty to different label misclassification pairs. In this work, we made use of M3N in expanding the
search space and rescoring the hypotheses. To facilitate the integration of the M3N system into our
decoder framework, we made several modifications that slightly improve the M3N baseline system. Our
improved baseline system has two stages: the first stage is filler word prediction using M3N to detect
words which can potentially be fillers, and the second stage is joint edit and filler word prediction using
M3N. The output of the first stage is passed as features into the second stage. Both stages perform
filler word prediction, since we found that joint edit and filler word detection performs better than edit
word detection alone as edit words tend to co-occur with filler words, and the first-stage output can be
fed into the second stage to extract additional features. We also augmented the M3N toolkit to support
additional feature functions, allow weighting of individual nodes, and control the total number of model
parameters.
2
3.1 Node-Weighted and Label-Weighted Max-Margin Markov Networks (M3N)
A max-margin Markov network (M3N) (Taskar et al., 2003) is a sequence labeling model. It has the
same structure as conditional random fields (CRF) (Lafferty et al., 2001) but with a different objective
function. A CRF is trained to maximize the conditional probability of the true label sequence given the
observed input sequence, while an M3N is trained to maximize the difference between the conditional
probability of the true label sequence and the incorrectly predicted label sequence (i.e., maximizing the
margin). Thus, we can regard M3N as a support vector machine (SVM) analogue of CRF (Suykens and
Vandewalle, 1999).
The dual form of the training objective function of M3N is formulated as follows:
min
?
1
2
C?
?
x,y
?
x,y
?f(x,y)?
2
2
+
?
x,y
?
x,y
L(x,y)
s.t.
?
y
?
x,y
= 1,?x and ?
x,y
? 0, ?x,y
(1)
2
The source code of our augmented M3N toolkit can be downloaded at http://code.google.com/p/m3n-ext/
1458
where x is the observed input sequence, y ? Y is the output label sequence, ?
x,y
are the dual variables
to be optimized, and C ? 0 is the regularization parameter to be tuned. ?f(x,y) = f(x,
?
y) ? f(x,
?
y)
is the residual feature vector, where
?
y is the true label sequence,
?
y is the predicted label sequence given
the model, and f(x,y) is the feature vector. It is implemented as a sum over all nodes:
f(x,y) =
?
t
f(x,y, t) (2)
where t is the position index of the node in the sequence. Each component of f(x,y, t) is a feature
function, f(x,y, t). For example, f(w
0
=?so?, y
0
=?F?, y
?1
=?O?, t) has a value of 1 only when the word at
node t is ?so?, the label at node t is ?F? (filler word), and the label at node (t?1) is ?O? (outside edit/filler
region, i.e., fluent). The maximum length of the y history (for this feature function, it is 2 since only y
0
and y
?1
are covered) is called the clique order of the feature. L(x,y) is the loss function. A standard
M3N uses an unweighted hamming loss, which is the number of incorrect nodes:
L(x,y) =
?
t
?(y?
t
, y?
t
) where ?(a, b) =
{
1, if a = b
0, otherwise
(3)
Qian and Liu (2013) proposed using label-weighted M3N to balance precision and recall by adjusting
the penalty on false positives and false negatives, i.e., v(y?
t
, y?
t
) in Eqn. 4. In this work, we further extend
this technique to individual nodes to train expert models, each specialized in a specific part-of-speech
(POS) class. Our loss function is:
L(x,y) =
?
t
u
c
(t)v(y?
t
, y?
t
)?(y?
t
, y?
t
) where u
c
(t) =
{
B
c
, if POS(t) ? S
c
1, otherwise
(4)
whereB
c
is a factor which controls the extent to which the model is biased to minimize errors on specific
nodes, POS(t) is the POS tag of the word at node t, and S
c
is the set of POS tags corresponding to that
expert class c. We show that by integrating these expert models into our decoder framework, we can
achieve further improvement.
3.2 Features
The feature templates for filler word and edit word prediction are listed in Table 1 and Table 2 re-
spectively. w
i
refers to the word at the i
th
position relative to the current node; window size is the
maximum number of words before and after the current word that the template covers, e.g., w
?1
w
0
with a window size of 4 means w
?4
w
?3
, w
?3
w
?2
, ..., w
3
w
4
; p
i
refers to the POS tag at the i
th
po-
sition relative to the current node; w
i?j
refers to any word from the i
th
position to the j
th
position
relative to the current node; w
i, 6=F
refers to the i
th
word (w.r.t. the current node) not being a filler
word; the multi-pair comparison function I(a, b, c, ...) indicates whether each pair (a and b, b and c,
and so on) are identical, for example, if a = b 6= c = d, it will output ?101? (?1? for being equal, ?0?
for being unequal); and ngram-score features are the natural logarithm of the following probabilities:
P (w
?3
, w
?2
, w
?1
, w
0
), P (w
0
|w
?3
, w
?2
, w
?1
), P (w
?3
, w
?2
, w
?1
), P (?/s?|w
?3
, w
?2
, w
?1
), P (w
?3
),
P (w
?2
), P (w
?1
), P (w
0
) (??/s?? denotes sentence-end). We use language models (LM) in two ways:
individual n-gram scores as M3N features, and an overall sentence-level score for rescoring in our beam-
search decoder. Our experiments show that this way of using LM gives the best performance.
We set the frequency pruning threshold to 5, so that the resulting model has about the same total
number of parameters (7.6M) as (Qian and Liu, 2013). The clique order for each template is determined
by considering the total number of features given that template. For example, for pause duration, there are
10 features (after cumulative binning), so we can set its clique order to 3 since there will be 10?3
3
= 270
weights; but for word 3-grams, there are 5M features, so setting its clique order to 3 or 2 will give rise to
too many weights (5M ? 3
3
= 135M for order 3; 5M ? 3
2
= 45M for order 2), thus we will reduce its
clique order to 1. The same principle applies to other feature templates.
1459
Feature Template Window Size Clique Order
w
0
4 1
w
?1
w
0
4 2
I(w
i
, w
j
) 10 2
I(w
i
, w
j
, w
i+1
, w
j+1
) 10 2
p
0
4 1
p
?1
p
0
4 2
p
?2
p
?1
p
0
4 2
I(p
i
, p
j
) 10 2
I(p
i
, p
j
, p
i+1
, p
j+1
) 10 2
transitions 0 3
Table 1: Feature templates for filler word prediction
Feature Template Window Size Clique Order
w
?2
w
?1
w
0
4 2
I(w
i
, w
j
)(w
i
if w
i
=w
j
) 10 2
w
0
w
?6??1
, w
0
w
1?6
0 1
I(p
i
, p
j
) 10 3
I(p
i
, p
j
)(p
i
if p
i
=p
j
) 10 3
p
?1
w
0
2 2
w
?1
p
0
2 2
w
?2, 6=F
w
?1, 6=F
0 2
w
?3, 6=F
w
?2, 6=F
w
?1, 6=F
0 2
p
?2, 6=F
p
?1, 6=F
0 2
p
?3, 6=F
p
?2, 6=F
p
?1, 6=F
0 2
ngram-score features 0 3
pause duration before w
0
0 3
pause duration after w
0
0 3
all features for filler word prediction same same
Table 2: Feature templates for edit word prediction
4 The Beam-Search Decoder Framework
4.1 Motivation
There are several limitations in the current M3N or CRF approach. Firstly, current models do not measure
the quality of the cleaned-up sentences, i.e., the resulting sentence after removing all predicted filler and
edit words. Secondly, one pass of disfluency detection may not be sufficient to detect all disfluencies.
Qian and Liu (2013) showed that we can improve the performance significantly by running a second
pass of edit detection. Our preliminary experiments also show that additional passes of edit detection
further improve the performance. Lastly, we find that edit word detection accuracy differs significantly
on words of different POS tags (Table 3). This is because words of different POS tags have different
feature distributions. Thus, depending on the POS tag of the current word, the same feature may have
different implications for disfluency. For example, consider the feature I(p
0
, p
2
). When the current word
is a determiner, the feature does not strongly suggest an edit word. In ?You give me a book, a pen,
a pencil, ...?, the determiner ?a? gets repeated. However, when the current word is a verb, the feature
strongly suggests it is an edit word. In ?The hardest thing for us has been is to ...?, both ?has? and ?is?
are third-person singular verbs. Hence, it might be helpful if we train expert models each specialized
in detecting edit words belonging to a specific POS and combine them dynamically according to the
POS. Motivated by the beam-search decoder for grammatical error correction (Dahlmeier and Ng, 2012)
and social media text normalization (Wang and Ng, 2013), we propose a novel beam-search decoder for
1460
disfluency detection to overcome these limitations.
POS Freq. (%) Edit F1 (%)
PRP 25.5 92.33
DT 14.2 88.95
IN 10.4 84.45
VBP 8.3 86.88
RB 7.1 81.78
CC 4.6 86.76
BES 4.2 93.37
NN 3.4 52.30
VBD 3.1 86.51
VB 2.1 70.42
VBZ 1.9 79.70
... ... ...
Table 3: Baseline edit F1 scores for different POS tags
4.2 General Framework
The goal of the decoder is to find the best hypothesis for a given input sentence w. A hypothesis h
is a label sequence, one label for every word in the sentence. To find the best hypothesis, the decoder
iteratively generates new hypotheses from current ones using a set of hypothesis producers and rescores
each hypothesis using a set of hypothesis evaluators. For each hypothesis produced, the decoder cleans
up the sentence by removing all the predicted filler words and edit words so that subsequent operations
can act on the cleaned-up sentence w? if needed. Each hypothesis evaluator produces a score f which
measures the quality of the current hypothesis based on certain aspects of fluency specific to that hypoth-
esis evaluator. The overall score of a hypothesis is the weighted sum of the scores from all the hypothesis
evaluators:
score(h,w) =
?
i
?
i
f
i
(h,w) (5)
The weights ?
i
s are tuned on the development set using minimum error rate training (MERT) (Och,
2003). The decoding algorithm is shown in Algorithm 1.
In our description, h
i
denotes the hypothesized label at the i
th
position; w
i
denotes the word at the
i
th
position; |h| denotes the length of the label sequence; f
M3N
(h
i
,w) denotes the M3N log-posterior
probability of the label (at the i
th
position of hypothesis h) being the hypothesized label; f
M3N
(h,w)
denotes the normalized joint log probability of hypothesis h given the M3N model (?normalized? means
divided by the length of the label sequence); w? denotes the cleaned-up sentence; f
LM
(w?) = f
LM
(h,w)
denotes the language model score of the sentence w? (cleaned up according to hypothesis h) divided by
sentence length; and
?
h denotes the sub-hypothesis obtained by running M3N on the cleaned-up sentence
with updated features. Note that a sub-hypothesis will have a shorter label sequence if some words
are labeled as filler word or edit word in the parent hypothesis. We can obtain h from
?
h by inserting
all predicted filler and edit words from the parent hypothesis into the sub-hypothesis so that its label
sequence has the same length as the original sentence.
4.3 Hypothesis Producers
The goal of hypothesis producers is to create a search space for rescoring using various hypothesis eval-
uators. Based on the information provided by the existing models and certain patterns where disfluencies
may occur, we propose the following hypothesis producers for our beam-search decoder:
Confusable-phrase-dictionary: The motivation of using this hypothesis producer is to hypothesize
labels for phrases which are commonly misclassified in the development data. We build a dictionary of
1461
Algorithm 1
The beam-search decoding algorithm for a sentence. S: hypothesis stack; h: hypothesis; f : hypothe-
sis evaluator score vector; ?: hypothesis evaluator weight vector
INPUT: a sentence w with N words
OUTPUT: a sequence of N labels, from {E, F, O}
1: initialize hypothesis h
0
, h
i
=?O??i ? [1, N ]
2: S
A
? {h
0
}, S
B
? ?
3: for iter = [1, maxIter] do
4: for each h in S
A
do
5: for each producer in hypothesisProducers do
6: for each h
?
in producer(h) do
7: compute f(h
?
,w) from hypothesisEvaluators
8: compute score(h
?
,w) = ?
>
? f(h
?
,w)
9: S
B
? S
B
+ {h
?
}
10: prune S
B
according to score
11: S
A
? S
B
, S
B
= ?
12: return argmax
h
{score(h,w)},h ? S
A
phrases (up to 5 words) and their corresponding true labels by considering the most frequent incorrectly
predicted phrases in the development set. During decoding, whenever such a phrase occurs in a sentence
and its label is not the same as that in the dictionary, it is changed to that in the dictionary and a new
hypothesis is produced. For example, if the phrase ?you know? has occurred 1144 times and has been
misclassified 31 times, out of which 9 times it should be ?O?, then an entry ?you know O || 1144 31 9?
will be added to the dictionary. If the original sentence contains ?you know? and it is not labeled as O, a
new hypothesis will be generated by labeling it as ?O?.
Repetition: Whenever the i
th
word and the j
th
word (j > i) are the same, all words from the i
th
position (inclusive) to the j
th
position (exclusive) are labeled as edit words. For example, in ?I want to
be able to um I just want it more for multi-tasking?, three hypotheses are produced. The first hypothesis
is produced by labeling every word in ?I want to be able to um? as edit words since ?I? is repeated. The
second hypothesis is produced by labeling every word in ?want to be able to um I just? as edit words
since ?want? is repeated. The third hypothesis is produced by labeling ?to be able? as edit words since
?to? is repeated. The window size within which we search for repetitions is set to 12 (i.e., j ? i ? 12),
since the longest edit region (due to repetition) in the development set is of that size. We introduce this
hypothesis producer because the baseline system tends to miss long edit regions, especially when very
few words in the region are repeated. However, sometimes a speaker does change what he intends to say
by aborting a sentence so that only the beginning few words are repeated, as in the above sentence.
Filler-word-marker: We trained an M3N model for filler word detection. Multiple passes of filler
word detection on cleaned-up sentences can sometimes detect filler words that are missed in earlier
passes. This hypothesis producer runs before every iteration starts. It performs filler word prediction and
modifies the feature table by setting the filler-indicator feature to true so that subsequent operations see
the updated feature. However, it does not remove filler words during the clean up process because some
words are defined as both filler word and edit word simultaneously.
Edit-word-marker: We run our baseline M3N (the second stage) on the cleaned-up sentence and
obtain the N -best hypotheses, i.e., the top N hypotheses h with max{f
M3N
(
?
h, w?)}. This producer
essentially performs multiple passes of disfluency detection.
4.4 Hypothesis Evaluators
Our decoder uses the following hypothesis evaluators to select the best hypothesis:
Fluent language model score: This is the normalized language model score of the cleaned-up sen-
tence, i.e., f
fluentLM
(w?). A 4-gram language model is trained on the cleaned-up version of the training
1462
texts (both filler words and edit words are removed). This score measures the fluency of the resulting
cleaned-up sentence w.r.t. a fluent language model.
Disfluent language model score: This is the normalized language model score of the cleaned-up sen-
tence, i.e., f
disfluentLM
(w?). A 4-gram language model is trained on the original training texts which
contain disfluencies. This score measures the fluency of the resulting cleaned-up sentence w.r.t. a disflu-
ent language model. These two LM scores provide contrastive measures because if a cleaned up sentence
still contains disfluencies, the disfluent LM will be preferred over the fluent LM.
M3N disfluent score: This is the normalized joint log probability score of the current hypothesis h,
i.e., f
M3N
(h,w). This score measures how much the baseline M3N model favors the disfluency label
assignment of the current hypothesis.
M3N fluent score: This is the normalized joint log probability score of labeling the entire cleaned-up
sentence as fluent, i.e.,
f
M3N
(
?
h=O, w?) =
1
|
?
h|
|
?
h|
?
i=1
f
M3N
(
?
h
i
=?O?, w?) (6)
This score measures how much the baseline M3N model favors the cleaned-up sentence of the current
hypothesis. It acts as a discriminative LM in measuring the fluency of the cleaned-up sentence. If the
cleaned-up sentence contains disfluencies, this evaluator function will tend to give a lower score.
Expert-POS-class c disfluent score: This is the normalized joint log probability score of the current
hypothesis h under the expert M3N model for POS class c dynamically combined with the baseline M3N
model, i.e.,
f
c
(h,w) =
1
|h|
|h|
?
i=1
g
c
(h
i
,w), g
c
(h
i
,w) =
{
f
M3N-c
(h
i
,w) if POS(w
i
) ? S
c
f
M3N
(h
i
,w) if POS(w
i
) /? S
c
(7)
Training of the expert M3N models is described in Section 4.6.
4.5 Integrating M3N into the Decoder Framework
In most previous work such as (Liu et al., 2006) and (Qian and Liu, 2013) that performed filler and edit
word detection using sequence models, the begin-inside-end-single (BIES) labeling scheme was adopted,
i.e., for edit words (E), 4 labels are defined: E B (beginning of an edit region), E I (inside an edit region),
E E (end of an edit region), and E S (single-word edit region). However, since our beam-search decoder
needs to change the labels dynamically among filler words, edit words, and fluent words, it will be
problematic if the label sequence has to conform to the BIES constraint especially when the posteriors
are concerned. Thus, we use the minimal set of labels: E (Edit word), F (Filler word), O (Outside edit
and filler region).
For the first-stage filler word detection, only ?F? and ?O? are used. To compensate for degradation
in performance, we increase the clique order of features to 3. We found that increasing the clique
order has a similar effect as using the BIES labeling scheme. For example, f(w
i
=?so?, y
0
=?E B?, t)
means the previous word is not an edit, both the current word and the next word are edit words, i.e.,
the previous word, the current word, and the next word can be either O-E-E or F-E-E. So in our mini-
mal labeling scheme, this feature will be decomposed into f(w
i
=?so?, y
?1
=?O?, y
0
=?E?, y
+1
=?E?, t) and
f(w
i
=?so?, y
?1
=?F?, y
0
=?E?, y
+1
=?E?, t), both having a higher clique order.
Our preliminary experiments show that by increasing the clique order of features while reducing the
number of labels (keeping about the same total number of parameters), we can maintain the same per-
formance. However, training takes a longer time.
4.6 POS-Class Specific Expert Models
We trained 6 expert M3N models, each focusing on disfluency prediction of words belonging to the
corresponding set of POS tags. The expert M3N models are trained in the same way as the baseline
M3N model, except that we increase the loss weights (Eqn. 4) if the word of that node belongs to
1463
the corresponding POS class. That is, M3N-Expert-POS-class-1 is trained to optimize performance on
words belonging to POS-class-1. Nonetheless, it can still predict disfluency for words in other POS
classes, except that the error rate may be higher because of the way training is biased.
Class POS tags Freq. (%) F1 range
1 RBS POS PDT NNPS HVS PRP$ BES PRP 33.5 92.3 ? 100
2 MD EX CC DT VBP WP WRB 32.2 86.0 ? 90.8
3 RB IN 16.7 82.8 ? 83.8
4 TO VBD WDT RP JJS 5.1 80.0 ? 82.1
5 VBZ VB VBN JJ 6.1 69.3 ? 78.1
6 VBG NN CD JJR UH NNS NNP XX RBR 3.2 42.1 ? 64.2
Table 4: POS classes for expert M3N models and their baseline F1 scores
We split all POS tags into 6 classes, by first sorting all POS tags in descending order of their F1
scores. Next, for POS tags with higher F1 scores, we form larger classes (higher total proportion), and
for POS tags with lower F1 scores, we form smaller classes. The POS classes are shown in Table 4. The
algorithm dynamically selects posteriors from different M3N models, depending on the POS tag of the
current word.
5 Experiments
5.1 Experimental Setup
We tested all the systems on the Switchboard Treebank corpus (LDC99T42), using the same
train/develop/test split as previous work (Johnson and Charniak, 2004; Qian and Liu, 2013). We removed
all partial words and punctuation symbols to simulate the condition when automatic speech recognition
(ASR) output is used. Our training set contains 1.3M words in 174K sentences; our development set
contains 86K words in 10K sentences; and our test set contains 66K words in 8K sentences. The original
system has high precision but low recall, i.e., the system tends to miss out edit words. The imbalance can
be solved by setting a larger penalty for mis-labeling edits as fluent, i.e., 2 instead of 1 for the weighted
hamming loss. We used the loss matrix, v(y?
t
, y?
t
), in Table 5 to balance precision and recall. We set
the biasing factor B
c
to 2, for every class c. We also added two pause duration features (pause duration
before and after the current word) from the corresponding Switchboard speech corpus (LDC97S62). We
trained our acoustic model on the Fisher corpus and used it to perform forced alignment on the Switch-
board corpus to obtain the word boundary time information for calculating pause durations. For the
ngram-score features, we used the small 4-gram language model trained on the training set with filler
words and edit words removed. All continuous features are quantized into 10 discrete bins using cumu-
lative binning (Liu et al., 2006). We set maxIter to 4 in Algorithm 1. The regularization parameter C
is set to 0.006, obtained by tuning on the development set.
Label E F O
E 0 1 2
F 1 0 2
O 1 1 0
Table 5: Weighted hamming loss, v(y?
t
, y?
t
) for M3N for both stages
5.2 Results
We use the standard F1 score as our evaluation metric, the same as (Qian and Liu, 2013). Performance
comparison of the baseline model and expert models on subsets belonging to specific POS classes is
shown in Table 6. It shows that by assigning larger loss weights to nodes belonging to a specific POS
class, we can to various extent boost the performance on words in that POS class. However, doing so
1464
will sacrifice the overall performance on the entire data set especially on POS classes with lower baseline
scores (see Table 7). But since we have several expert models, if we combine them, they can complement
each other?s weakness and give an overall slightly better performance. The result also shows that the
gain by training expert models decreases as the baseline performance on that POS class increases. For
example, POS class 6 has the poorest baseline performance and the gain is 2.1%. This gain decreases
gradually as we move up the table rows because the baseline performance gets better.
POS class Expert-M3N F1 Baseline-M3N F1
1 92.5 92.2
2 87.1 86.9
3 84.9 84.7
4 85.3 84.1
5 71.8 70.4
6 57.3 55.2
Table 6: Edit detection F1 scores (%) of expert models on all words belonging to that POS class in the
test set (expert-M3N column), and baseline model on all words belonging to that POS class in the test
set (baseline-M3N column)
System F1 (%)
Baseline-M3N 84.7
Expert-M3N(1) 84.6
Expert-M3N(2) 84.4
Expert-M3N(3) 84.3
Expert-M3N(4) 84.6
Expert-M3N(5) 84.0
Expert-M3N(6) 83.8
Table 7: Degradation of the overall performance by expert models compared to the baseline model
Table 8 shows the performance comparison of our baseline models and our beam-search decoder.
Statistical significance tests show that our best decoder model incorporating all hypothesis evaluators
gives a higher F1 score than the 3-label baseline M3N model (statistically significant at p < 0.001), and
the 3-label baseline M3N model gives a higher F1 score than the M3N system of (Qian and Liu, 2013)
(statistically significant at p = 0.02). Our three baseline models have about the same total number of
parameters (7.6M). The BIES baseline M3N system uses the same feature templates as shown in Table 2
with reduced clique order. The 2-label baseline M3N system uses the same feature templates with the
same clique order. Our results also show that joint filler and edit word prediction performs 0.4% better
than edit word prediction alone. Direct combination of expert models is done by first running the general
model and the expert models on each sentence to obtain all the label sequences (one for each model).
Then for every word in the sentence, if its POS belongs to any one of those POS classes, we choose its
label from the output of the corresponding expert model; otherwise, we choose its label from the output
of the baseline model.
For the decoder, M3N-disfluent-score needs to be present in all cases (except when POS experts are
present); otherwise, the F1 score is much worse because the entire sequence is not covered (i.e., just
looking at the scores from the cleaned-up sentences is not sufficient in deciding how well filler and edit
words have been removed). Adding M3N-fluent-score, Fluent-LM, or Disfluent-LM alone with M3N-
disfluent-score gives about the same improvement; but when combined, higher improvement is achieved.
Similar to (Qian and Liu, 2013), our system does not make use of any external sources of information
except for the last two rows in Table 8 where we added pause duration features. We found that adding
pause duration features gave a small but consistent improvement in all experiments, about 0.3% absolute
gain in F1 score. Our beam-search decoder (multi-threaded implementation) is about 4.5 times slower
1465
System
F1
(%)
M3N system of (Qian and Liu, 2013) 84.1
Our baseline M3N (using BIES for E and F) 84.4
Our baseline M3N (using 2 labels: E,O) 84.3
Our baseline M3N (using 3 labels: E,F,O) 84.7
Direct combination of the 6 POS-expert-models according to each word?s POS 85.2
Decoder: M3N-disfluent-score + M3N-fluent-score 85.1
Decoder: M3N-disfluent-score + Fluent-LM 85.2
Decoder: M3N-disfluent-score + Disfluent-LM 85.1
Decoder: M3N-disfluent-score + POS-experts 85.2
Decoder: M3N-disfluent-score + M3N-fluent-score + Fluent-LM + Disfluent-LM 85.6
Decoder: M3N-disfluent-score + M3N-fluent-score + Fluent-LM + Disfluent-LM + POS-
experts
85.7
Decoder: M3N-disfluent-score + M3N-fluent-score + Fluent-LM + Disfluent-LM + PauseDur 85.9
Decoder: M3N-disfluent-score + M3N-fluent-score + Fluent-LM + Disfluent-LM + POS-
experts + PauseDur
86.1
Table 8: Performance of the beam-search decoder with different combinations of components
than our baseline M3N model (single-threaded). Overall, it took about 0.4 seconds to detect disfluencies
in one sentence with our proposed beam-search decoder approach.
To the best of our knowledge, the best published F1 score on the Switchboard Treebank corpus is
84.1% (Qian and Liu, 2013) without the use of external sources of information, and 85.7% (Zwarts and
Johnson, 2011) with the use of external sources of information (large language models from additional
corpora were used in (Zwarts and Johnson, 2011)). Without the use of external sources of information,
our decoder approach achieves an F1 score of 85.7%, significantly higher than the best published F1
score of 84.1% of (Qian and Liu, 2013). Our decoder approach also achieves an F1 score of 86.1% after
adding external sources of information (pause duration features), higher than the F1 score of 85.7% of
(Zwarts and Johnson, 2011).
5.3 Discussion
We have manually analyzed the improvement of our decoder over the M3N baseline. For example,
consider the sentence in Table 9. Both the baseline M3N system and the first pass output of the decoder
will give the cleaned-up sentence ?are these do these programs ...?, which is still disfluent and has a
relatively lower fluent LM score but a relatively higher disfluent LM score because of the erroneous
n-gram ?are these do these?. The decoder makes use of the fluent LM and disfluent LM hypothesis
evaluators during the beam search and performs additional passes of cleaning and eventually gives the
correct output.
Sentence Um and uh are these like uh do these programs ...
Reference F F F E E E F O O O ...
M3N baseline F F F O O F F O O O ...
Decoder F F F E E E F O O O ...
Table 9: An example showing the effect of measuring the quality of the cleaned-up sentence.
Overall, our proposed decoder framework outperforms existing approaches. It also overcomes the lim-
itations mentioned in Section 4.1. For example, hypothesis evaluators like fluent language model score
and M3N fluent score achieve the purpose of measuring the quality of cleaned-up sentences. Repeatedly
applying the edit-word-marker hypothesis producer on a sentence achieves the purpose of cleaning up
1466
the sentence in multiple passes. Hypothesis evaluators corresponding to expert models achieve the pur-
pose of combining POS class-specific expert models. All of these components extend the flexibility of
the decoder framework in performing disfluency detection.
6 Conclusion
In conclusion, we have proposed a beam-search decoder approach for disfluency detection. Our beam-
search decoder performs multiple passes of disfluency detection on cleaned-up sentences. It evaluates the
quality of cleaned-up sentences and use it as a feature to rescore hypotheses. It also combines multiple
expert models to deal with edit words belonging to a specific POS class. In addition, we also proposed a
way (using node-weighted M3N in addition to label-weighted M3N) to train expert models each focusing
on minimizing errors on words belonging to a specific POS class. Our experiments show that combining
the outputs of the expert models directly according to POS tags can give rise to some improvement.
Combining the expert model scores with language model scores in a weighted manner using our beam-
search decoder achieves further improvement. To the best of our knowledge, our decoder has achieved
the best published edit-word F1 score on the Switchboard Treebank corpus, both with and without using
external sources of information.
7 Acknowledgments
This research is supported by the Singapore National Research Foundation under its International Re-
search Centre @ Singapore Funding Initiative and administered by the IDM Programme Office.
References
Eugene Charniak and Mark Johnson. 2001. Edit detection and parsing for transcribed speech. In Proc. of NAACL.
Daniel Dahlmeier and Hwee Tou Ng. 2012. A beam-search decoder for grammatical error correction. In Proc. of
EMNLP-CoNLL.
Kallirroi Georgila. 2009. Using integer linear programming for detecting speech disfluencies. In Proc. of NAACL.
Mark Johnson and Eugene Charniak. 2004. A TAG-based noisy channel model of speech repairs. In Proc. of
ACL.
Jeremy G. Kahn, Matthew Lease, Eugene Charniak, Mark Johnson, and Mari Ostendorf. 2005. Effective use of
prosody in parsing conversational speech. In Proc. of EMNLP.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001 Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc. of ICML.
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Dustin Hillard, Mari Ostendorf, and Mary Harper. 2006. Enrich-
ing speech recognition with automatic detection of sentence boundaries and disfluencies. IEEE Transactions on
Audio, Speech, and Language Processing, 14(5).
Sameer Maskey, Bowen Zhou, and Yuqing Gao. 2006. A phrase-level machine translation approach for disfluency
detection using weighted finite state transducers. In Proc. of INTERSPEECH.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL.
Xian Qian and Yang Liu. 2013. Disfluency detection using multi-step stacked learning. In Proc. of NAACL.
J.A.K. Suykens and J. Vandewalle. 1999 Least squares support vector machine classifiers. Neural Processing
Letters, 9.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003. Max-margin Markov networks. In Proc. of NIPS.
Pidong Wang and Hwee Tou Ng. 2013. A beam-search decoder for normalization of social media text with
application to machine translation. In Proc. of NAACL.
Qi Zhang, Fuliang Weng, and Zhe Feng. 2006. A progressive feature selection algorithm for ultra large feature
spaces. In Proc. of ACL.
Simon Zwarts and Mark Johnson. 2011. The impact of language models and loss functions on repair disfluency
detection. In Proc. of ACL.
1467
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 121?130,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Combining Punctuation and Disfluency Prediction: An Empirical Study
Xuancong Wang
1,3
Khe Chai Sim
2
Hwee Tou Ng
1,2
1
NUS Graduate School for Integrative Sciences and Engineering
2
Department of Computer Science, National University of Singapore
3
Human Language Technology, Institute for Infocomm Research, Singapore
xuancong84@gmail.com, {simkc, nght}@comp.nus.edu.sg
Abstract
Punctuation prediction and disfluency pre-
diction can improve downstream natural
language processing tasks such as ma-
chine translation and information extrac-
tion. Combining the two tasks can poten-
tially improve the efficiency of the over-
all pipeline system and reduce error prop-
agation. In this work
1
, we compare var-
ious methods for combining punctuation
prediction (PU) and disfluency prediction
(DF) on the Switchboard corpus. We com-
pare an isolated prediction approach with
a cascade approach, a rescoring approach,
and three joint model approaches. For
the cascade approach, we show that the
soft cascade method is better than the hard
cascade method. We also use the cas-
cade models to generate an n-best list, use
the bi-directional cascade models to per-
form rescoring, and compare that with the
results of the cascade models. For the
joint model approach, we compare mixed-
label Linear-chain Conditional Random
Field (LCRF), cross-product LCRF and 2-
layer Factorial Conditional Random Field
(FCRF) with soft-cascade LCRF. Our re-
sults show that the various methods link-
ing the two tasks are not significantly dif-
ferent from one another, although they
perform better than the isolated predic-
tion method by 0.5?1.5% in the F1 score.
Moreover, the clique order of features also
shows a marked difference.
1 Introduction
The raw output from automatic speech recogni-
tion (ASR) systems does not have sentence bound-
1
The research reported in this paper was carried out as
part of the PhD thesis research of Xuancong Wang at the NUS
Graduate School for Integrated Sciences and Engineering.
aries or punctuation symbols. Spontaneous speech
also contains a significant proportion of disflu-
ency. Researchers have shown that splitting input
sequences into sentences and adding in punctua-
tion symbols improve machine translation (Favre
et al., 2008; Lu and Ng, 2010). Moreover, dis-
fluencies in speech also introduce noise in down-
stream tasks like machine translation and informa-
tion extraction (Wang et al., 2010). Thus, punc-
tuation prediction (PU) and disfluency prediction
(DF) are two important post-processing tasks for
automatic speech recognition because they im-
prove not only the readability of ASR output, but
also the performance of downstream Natural Lan-
guage Processing (NLP) tasks.
The task of punctuation prediction is to insert
punctuation symbols into conversational speech
texts. Punctuation prediction on long, unseg-
mented texts also achieves the purpose of sentence
boundary prediction, because sentence boundaries
are identified by sentence-end punctuation sym-
bols: periods, question marks, and exclamation
marks. Consider the following example,
How do you feel about the Viet Nam War ? Yeah ,
I saw that as well .
The question mark splits the sequence into two
sentences. This paper deals with this task which is
more challenging than that on text that has already
been split into sentences.
The task of disfluency prediction is to identify
word tokens that are spoken incorrectly due to
speech disfluency. There are two main types of
disfluencies: filler words and edit words. Filler
words mainly include filled pauses (e.g., ?uh?,
?um?) and discourse markers (e.g., ?I mean?, ?you
know?). As they are insertions in spontaneous
speech to indicate pauses or mark boundaries in
discourse, they do not convey useful content in-
formation. Edit words are words that are spoken
wrongly and then corrected by the speaker. For
example, consider the following utterance:
121
I want a flight
Edit
? ?? ?
to Boston
Filler
? ?? ?
uh I mean
Repair
? ?? ?
to Denver
The phrase ?to Boston? forms the edit region to be
replaced by ?to Denver?. The words ?uh I mean?
are filler words that serve to cue the listener about
the error and subsequent corrections.
The motivation of combining the two tasks can
be illustrated by the following two utterances:
I am uh I am not going with you .
I am sorry . I am not going with you .
Notice that the bi-gram ?I am? is repeated in
both sentences. For the first utterance, if punctu-
ation prediction is performed first, it might break
the utterance both before and after ?uh? so that the
second-stage disfluency prediction will treat the
whole utterance as three sentences, and thus may
not be able to detect any disfluency because each
one of the three sentences is legitimate on its own.
On the other hand, for the second utterance, if dis-
fluency prediction is performed first, it might mark
?I am sorry? as disfluent in the first place and re-
move it before passing into the second-stage punc-
tuation prediction. Therefore, no matter which
task is performed first, certain utterances can al-
ways cause confusion.
There are many ways to combine the two tasks.
For example, we can perform one task first fol-
lowed by another, which is called the cascade ap-
proach. We can also mix the labels, or take the
cross-product of the labels, or use joint prediction
models. In this paper, we study the mutual influ-
ence between the two tasks and compare a variety
of common state-of-the-art joint prediction tech-
niques on this joint task.
In Section 2, we briefly introduce previous work
on the two tasks. In Section 3, we describe our
baseline system which performs punctuation and
disfluency prediction separately (i.e., in isolation).
In Section 4, we compare the soft cascade ap-
proach with the hard cascade approach. We also
examine the effect of task order, i.e., performing
which task first benefits more. In Section 5, we
compare the cascade approach with bi-directional
n-best rescoring. In Section 6, we compare the 2-
layer Factorial CRF (Sutton et al., 2007) with the
cross-product LCRF (Ng and Low, 2004), mixed-
label LCRF (Stolcke et al., 1998), the cascade ap-
proach, and the baseline isolated prediction. Sec-
tion 7 gives a summary of our overall findings.
Section 8 gives the conclusion.
2 Previous Work
There were many works on punctuation prediction
or disfluency prediction as an isolated task. For
punctuation prediction, Huang and Zweig (2002)
used maximum entropy model; Christensen et al.
(2001) used finite state and multi-layer perceptron
method; Liu et al. (2005) used conditional ran-
dom fields; Lu and Ng (2010) proposed using dy-
namic conditional random fields for joint sentence
boundary type and punctuation prediction; Wang
et al. (2012) has added prosodic features for the
dynamic conditional random field approach and
Zhang et al. (2013) used transition-based parsing.
For disfluency prediction, Shriberg et al. (1997)
uses purely prosodic features to perform the task.
Johnson and Charniak (2004) proposed a TAG-
based (Tree-Adjoining Grammar) noisy channel
model. Maskey et al. (2006) proposed a phrase-
level machine translation approach for this task.
Georgila (2009) used integer linear programming
(ILP) which can incorporate local and global con-
straints. Zwarts and Johnson (2011) has inves-
tigated the effect of using extra language mod-
els as features in the reranking stage. Qian and
Liu (2013) proposed using weighted Max-margin
Markov Networks (M3N) to balance precision and
recall to further improve the F1-score. Wang et al.
(2014) proposed a beam-search decoder which in-
tegrates M3N and achieved further improvements.
There were also some works that addressed both
tasks. Liu et al. (2006) and Baron et al. (1998)
carried out sentence unit (SU) and disfluency pre-
diction as separate tasks. The difference between
SU prediction and punctuation prediction is only
in the non-sentence-end punctuation symbols such
as commas. Stolcke et al. (1998) mixed sen-
tence boundary labels with disfluency labels so
that they do not predict punctuation on disfluent
tokens. Kim (2004) performed joint SU and In-
terruption Point (IP) prediction, deriving edit and
filler word regions from predicted IPs using a rule-
based system as a separate step.
In this paper, we treat punctuation prediction
and disfluency prediction as a joint prediction task,
and compare various state-of-the-art joint predic-
tion methods on this task.
122
3 The Baseline System
3.1 Experimental Setup
We use the Switchboard corpus (LDC99T42) in
our experiment with the same train/develop/test
split as (Qian and Liu, 2013) and (Johnson and
Charniak, 2004). The corpus statistics are shown
in Table 1. Since the proportion of exclamation
marks and incomplete SU boundaries is too small,
we convert all exclamation marks to periods and
remove all incomplete SU boundaries (treat as no
punctuation). In the Switchboard corpus, the ut-
terances of each speaker have already been seg-
mented into short sentences when used in (Qian
and Liu, 2013; Johnson and Charniak, 2004). In
our work, we concatenate the utterances of each
speaker to form one long sequence of words for
use as the input to punctuation prediction and dis-
fluency prediction. This form of input where,
utterances are not pre-segmented into short sen-
tences, better reflects the real-world scenarios and
provides a more realistic test setting for punctu-
ation and disfluency prediction. Punctuation pre-
diction also gives rise to sentence segmentation in
this setting.
Data set train develop test
# of tokens 1.3M 85.9K 65.5K
# of sentences 173.7K 10.1K 7.9K
# of sequences* 1854 174 134
# of edit words 63.6K 4.7K 3.7K
# of filler words 137.1K 9.6K 7.3K
# of Commas 52.7K 1.8K 2.1K
# of Periods 97.6K 6.5K 4.5K
# of Questions 6.8K 363 407
# of Exclamations 67 4 1
# of Incomplete 189 2 0
Table 1: Corpus statistics for all the experiments.
*: each conversation produces two long/sentence-
joined sequences, one from each speaker.
Our baseline system uses M3N (Taskar et al.,
2004), one M3N for punctuation prediction and
the other for disfluency prediction. We use the
same set of punctuation and disfluency labels (as
shown in Table 2) throughout this paper. To com-
pare the various isolated, cascade, and joint pre-
diction models, we use the same feature templates
for both tasks as listed in Table 3. Since some of
the feature templates require predicted filler labels
and part-of-speech (POS) tags, we have trained a
POS tagger and a filler predictor both using CRF
(i.e., using the same approach as that in Qian and
Liu (2013)). The same predicted POS tags and
fillers are used for feature extraction in all the
experiments in this paper for a fair comparison.
The degradation on disfluency prediction due to
the concatenation of utterances of each speaker
is shown in Table 4. The pause duration fea-
tures are extracted by running forced alignment
on the corresponding Switchboard speech corpus
(LDC97S62).
Task Label Meaning
Disfluency
prediction
E edit word
F filler word
O otherwise / fluent
Punctuation
prediction
Comma comma
Period full-stop
QMark question mark
None no punctuation
Table 2: Labels for punctuation prediction and dis-
fluency prediction.
3.2 Features
We use the standard NLP features such as
F (w
?1
w
0
=?so that?), i.e., the word tokens at
the previous and current node position are
?so? and ?that? respectively. Each feature is
associated with a clique order. For example,
since the clique order of this feature template
is 2 (see Table 3), its feature functions can be
f(w
?1
w
0
=?so that?, y
0
=?F?, y
?1
=?O?, t). The
example has a value of 1 only when the words
at node t ? 1 and t are ?so that?, and the labels
at node t and t ? 1 are ?F? and ?O? respectively.
The maximum length of the y history is called the
clique order of the feature (in this feature func-
tion, it is 2 since only y
0
and y
?1
are covered).
The feature templates are listed in Table 3. w
i
refers to the word at the i
th
position relative to
the current node; window size is the maximum
span of words centered at the current word that
the template covers, e.g., w
?1
w
0
with a window
size of 9 means w
?4
w
?3
, w
?3
w
?2
, ..., w
3
w
4
; p
i
refers to the POS tag at the i
th
position relative
to the current node; w
i?j
refers to any word
from the i
th
position to the j
th
position relative to
the current node, this template can capture word
pairs which can potentially indicate a repair, e.g.,
?was ... is ...?, the speaker may have spoken any
123
word(s) in between and it is very difficult for the
standard n-gram features to capture all possible
variations; w
i, 6=F
refers to the i
th
non-filler word
with respect to the current position, this template
can extract n-gram features skipping filler words;
the multi-pair comparison function I(a, b, c, ...)
indicates whether each pair (a and b, b and c, and
so on) is identical, for example, if a = b 6= c = d,
it will output ?101? (?1? for being equal, ?0?
for being unequal), this feature template can
capture consecutive word/POS repetitions which
can further improve upon the standard repetition
features; and ngram-score features are the nat-
ural logarithm of the following 8 probabilities:
P (w
?3
, w
?2
, w
?1
, w
0
), P (w
0
|w
?3
, w
?2
, w
?1
),
P (w
?3
, w
?2
, w
?1
), P (?/s?|w
?3
, w
?2
, w
?1
),
P (w
?3
), P (w
?2
), P (w
?1
) and P (w
0
) (where
??/s?? denotes sentence-end).
Feature Template
Window
Size
Clique
Order
w
0
9 1
w
?1
w
0
9 2
w
?2
w
?1
w
0
9 2
p
0
9 1
p
?1
p
0
9 2
p
?2
p
?1
p
0
9 2
w
0
w
?6??1
, w
0
w
1?6
1 1
I(w
i
, w
j
) 21 2
I(w
i
, w
j
, w
i+1
, w
j+1
) 21 2
I(w
i
, w
j
)(w
i
if w
i
=w
j
) 21 2
I(p
i
, p
j
) 21 3
I(p
i
, p
j
, p
i+1
, p
j+1
) 21 3
I(p
i
, p
j
)(p
i
if p
i
=p
j
) 21 3
p
?1
w
0
5 2
w
?1
p
0
5 2
w
?2, 6=F
w
?1, 6=F
1 2
w
?3, 6=F
w
?2, 6=F
w
?1, 6=F
1 2
p
?2, 6=F
p
?1, 6=F
1 2
p
?3, 6=F
p
?2, 6=F
p
?1, 6=F
1 2
ngram-score features 1 3
pause duration before w
0
1 3
pause duration after w
0
1 3
transitions 1 3
Table 3: Feature templates for disfluency predic-
tion, or punctuation prediction, or joint prediction
for all the experiments in this paper.
The performance of the system can be fur-
ther improved by adding additional prosodic fea-
tures (Savova and Bachenko, 2003; Shriberg et al.,
1998; Christensen et al., 2001) apart from pause
durations. However, since in this work we focus
on model-level comparison, we do not use other
prosodic features for simplicity.
3.3 Evaluation and Results
Experiment
F1
(PU)
F1
(DF)
Short sentences, with preci-
sion/recall balancing, clique or-
der of features up to 3, and la-
bels {E,F,O}
N.A. 84.7
Short sentences, with preci-
sion/recall balancing, clique or-
der of features up to 3, and la-
bels {E,O}
N.A. 84.3
Join utterances into long sen-
tences
71.1 79.2
Join utterances into long sen-
tences + remove precision/recall
balancing
71.1 78.2
Join utterances into long sen-
tences + remove precision/recall
balancing + reduce clique order
of all features
68.5 76.4
Table 4: Baseline results showing the degradation
by joining utterances into long sentences, remov-
ing precision/recall balancing, and reducing the
clique order of features. All models are trained
using M3N.
We use the standard F1 score as our evaluation
metric and this is similar to that in Qian and Liu
(2013). For training, we set the frequency prun-
ing threshold to 5 to control the number of pa-
rameters. The regularization parameter is tuned
on the development set. Since the toolkits used
to run different experiments have slightly differ-
ent limitations, in order to make fair comparisons
across different toolkits, we do not use weighting
to balance precision and recall when training M3N
and we have reduced the clique order of transi-
tion features to two and all the other features to
one in some of our experiments. Since the per-
formance of filler word prediction on this dataset
is already very high, (>97%), we only focus on
the F1 score of edit word prediction in this pa-
per when reporting the performance of disfluency
prediction. Table 4 shows our baseline results.
Our preliminary study shows the following gen-
124
eral trends: (i) for disfluency prediction: joining
utterances into long sentences will cause a 5?6%
drop in F1 score; removing precision/recall bal-
ance in M3N will cause about 1% drop in F1 score;
and reducing the clique order in Table 3 will cause
about 1?2% drop in F1 score; and (ii) for punctua-
tion prediction: removing precision/recall balance
in M3N will cause negligible drop in F1 score; and
reducing clique order will cause about 2?3% drop
in F1 score. Conventionally, the degradation from
reducing the clique orders can be mostly compen-
sated by using the BIES (Begin, Inside, End, and
Single) labeling scheme. In this work, for con-
sistency and comparability across various experi-
ments, we will stick to the same set of labels in
Table 2.
4 The Cascade Approach
Instead of decomposing the joint prediction of
punctuation and disfluency into two independent
tasks, the cascade approach considers one task to
be conditionally dependent on the other task such
that the predictions are performed in sequence,
where the results from the first step is used in the
second step. In this paper, we compare two types
of cascade: hard cascade versus soft cascade.
4.1 Hard Cascade
For the hard cascade, we use the output from the
first step to modify the input sequence before ex-
tracting features for the second step. For PU?DF
(PUnctuation prediction followed by DisFluency
prediction), we split the input sequence into sen-
tences according to the sentence-end punctuation
symbols predicted by the first step, and then per-
form the DF prediction on the short/sentence-split
sequences in the second step. For DF?PU, we
remove the edit and filler words predicted by the
first step, and then predict the punctuations using
the cleaned-up input sequence. The hard cascade
method may be helpful because the disfluency pre-
diction on short/sentence-split sequences is better
than on long/sentence-joined sequences (see the
second and third rows in Table 4). On the other
hand, the punctuation prediction on fluent text is
more accurate than that on non-fluent text based
on our preliminary study.
For this experiment, four models are trained
using M3N without balancing precision/recall.
For the first step, two models are trained on
long/sentence-joined sequences with disfluent to-
kens - one for PU prediction and the other for DF
prediction. These are simply the isolated base-
line systems. For the second step, the DF predic-
tion model is trained on the short/sentence-split se-
quences with disfluent tokens while the PU predic-
tion model is trained on the long/sentence-joined
sequences with disfluent tokens removed. Note
that in the second step of DF?PU, punctuation la-
bels are predicted only for the fluent tokens since
the disfluent tokens predicted by the first step has
already been removed. Therefore, during evalua-
tion, if the first step makes a false positive by pre-
dicting a fluent token as an edit or filler, we set its
punctuation label to the neutral label, None. All
the four models are trained using the same feature
templates as shown in Table 3. The regularization
parameter is tuned on the development set.
4.2 Soft Cascade
For the soft cascade method, we use the labels pre-
dicted from the first step as additional features for
the second step. For PU?DF, we model the joint
probability as:
P (DF,PU|x) = P (PU|x)? P (DF|PU,x) (1)
Likewise, we model the joint probability for
DF?PU as:
P (DF,PU|x) = P (DF|x)? P (PU|DF,x) (2)
For this experiment, four models are trained us-
ing M3N without balancing precision/recall. As
with the case of hard cascade, the two models
used in the first step are simply the isolated base-
line systems. For the second step, in addition to
the feature templates in Table 3, we also pass on
the labels (at the previous, current and next posi-
tion) predicted by the first step as three third-order-
clique features. We also tune the regularization pa-
rameter on the development set to obtain the best
model.
4.3 Experimental Results
Table 5 compares the performance of the hard
and soft cascade methods with the isolated base-
line systems. In addition, we have also included
the results of using the true labels in place of
the labels predicted by the first step to indicate
the upper-bound performance of the cascade ap-
proaches. The results show that both the hard and
soft cascade methods outperform the baseline sys-
tems, with the latter giving a better performance
125
Experiment F1 for PU F1 for DF
isolated baseline 71.1 78.2
hard cascade 71.2 79.1
hard cascade
(using true labels)
72.6 83.5
soft cascade 71.6 79.6
soft cascade
(using true labels)
72.1 82.7
Table 5: Performance comparison between the
hard cascade method and the soft cascade method
with respect to the baseline isolated prediction.
All models are trained using M3N without balanc-
ing precision and recall.
(statistical significance at p=0.01). However, hard
cascade has a higher upper-bound than soft cas-
cade. This observation can be explained as fol-
lows.
For hard cascade, the input sequence is modi-
fied prior to feature extraction. Therefore, many
of the features generated by the feature templates
given in Table 3 will be affected by these modi-
fications. So, provided that the modifications are
based on the correct information, the resulting fea-
tures will not contain unwanted artefacts caused
by the absence of the sentence boundary informa-
tion for the presence of disfluencies. For exam-
ple, in ?do you do you feel that it was worthy?,
the punctuation prediction system tends to insert a
sentence-end punctuation after the first ?do you?
because the speaker restarts the sentence.
If the disfluency was correctly predicted in the
first step, then the hard cascade method would
have removed the first ?do you? and eliminated
the confusion. Similarly, in ?I ?m sorry . I ?m not
going with you tomorrow . ?, the first ?I ?m? is
likely to be incorrectly detected as disfluent tokens
since consecutive repetitions are a strong indica-
tion of disfluency. In the case of hard cascade,
PU?DF, the input sequence would have been split
into sentences and the repetition feature would not
be activated. However, since the hard cascade
method has a greater influence on the features for
the second step, it is also more sensitive to the pre-
diction errors from the first step.
Another observation from Table 5 is that the
improvement of the soft cascade over the isolate
baseline is much larger on DF (1.4% absolute)
than that on PU (only 0.5% absolute). The same
holds true for the hard cascade, despite the fact
that there are more DF labels than PU labels in this
corpus (see Table 1) and the first step prediction is
more accurate on DF than on PU. This suggests
that their mutual influence is not symmetrical, in
the way that the output from punctuation predic-
tion provides more useful information for disflu-
ency prediction than the other way round.
5 The Rescoring Approach
In Section 4, we have described that the two tasks
can be cascaded in either order, i.e., PU?DF and
DF?PU. However, the performance of the sec-
ond step greatly depends on that of the first step.
In order to reduce sensitivity to the errors made
in the first step, one simple approach is to prop-
agate multiple hypotheses from the first step to
the second step to obtain a list of joint hypothe-
ses (with both the DF and PU labels). We then
rerank these hypotheses based on the joint proba-
bility and pick the best. We call this the rescoring
approach. From (1) and (2), the joint probabilities
can be expressed in terms of the probabilities gen-
erated by four models: P (PU|x), P (DF|PU,x),
P (DF|x), and P (PU|DF,x). We can combine the
four models to form the following joint probability
function for rescoring:
P (DF,PU|x) = P (DF|x)
?
1
? P (PU|DF,x)
?
2
? P (PU|x)
?
1
? P (DF|PU,x)
?
2
where ?
1
, ?
2
, ?
1
, and ?
2
are used to weight
the relative importance between (1) and (2); and
between the first and second steps. In practice,
the probabilities are computed in the log domain
where the above expression becomes a weighted
sum of the log probabilities. A similar rescoring
approach using two models is described in Shi and
Wang (2007).
The experimental framework is shown in Fig-
ure 1. For PU?DF, we first use P (PU|x) to gen-
erate an n-best list. Then, for each hypothesis in
the n-best list, we use P (DF|PU,x) to obtain an-
other n-best list. So we have n
2
-best joint hy-
potheses. We do the same for DF?PU to ob-
tain another n
2
-best joint hypotheses. We rescore
the 2n
2
-best list using the four models. The four
weights ?
1
, ?
2
, ?
1
, and ?
2
are tuned to opti-
mize the overall F1 score on the development set.
We used the MERT (minimum-error-rate training,
(Och, 2003)) algorithm to tune the weights. We
also vary the size of n.
126
P(PU|x) 
P(DF|PU,x) 
Input 
Sequence P(PU|DF,x) 
P(DF|x) 
PU-hypo-1 
PU-hypo-n 
?
 
DF-hypo-1 
DF-hypo-n 
?
 
2 n-best 
joint-hypo-1 
?
 
?
 
joint-hypo-1 
2 n2-best 
joint-hypo-2 
joint-hypo-2 
?
 
?
 
Rescore using: 
?1 ? log? PU|x  
+?2 ? log? DF|PU, x  
+?1 ? log? DF|x  
+?2 ? log? PU|DF, x  
Figure 1: Illustration of the rescoring pipeline framework using the four M3N models used in the soft-
cascade method: P (PU|x), P (DF|PU,x), P (DF|x) and P (PU|DF,x)
The results shown in Table 6 suggest that the
rescoring method does not improve over the soft-
cascade baseline. This can be due to the fact that
we are using the same four models for the soft-
cascade and the rescoring methods. It may be
possible that the information contained in the two
models for the soft-cascade PU?DF mostly over-
laps with the information contained in the other
two models for the soft-cascade DF?PU since all
the four models are trained using the same fea-
tures. Thus, no additional information is gained
by combining the four models.
6 The Joint Approach
In this section, we compare 2-layer FCRF (Lu and
Ng, 2010) with mixed-label LCRF (Stolcke et al.,
1998) and cross-product LCRF on the joint predic-
tion task. For the 2-layer FCRF, we use punctua-
tion labels for the first layer and disfluency labels
for the second layer (see Table 2). For the mixed-
label LCRF, we split the neutral label {O} into
{Comma, Period, QMark, None} so that we have
six labels in total, {E, F, Comma, Period, QMark,
None}. In this approach, disfluent tokens do not
have punctuation labels because in real applica-
tions, if we just want to get the cleaned-up/fluent
text with punctuations, we do not need to predict
punctuations on disfluent tokens as they will be
removed during the clean-up process. Since this
approach does not predict punctuation labels on
disfluent tokens, its punctuation F1 score is only
evaluated on those fluent tokens. For the cross-
product LCRF, we compose each of the three dis-
fluency labels with the four punctuation labels to
get 12 PU-DF-joint labels (Ng and Low, 2004).
Figure 2 shows a comparison of these three models
in the joint prediction of punctuation and disflu-
ency. All the LCRF and FCRF models are trained
using the GRMM toolkit (Sutton, 2006). We use
the same feature templates (Table 3) to generate
all the features for the toolkit. However, to reduce
the training time, we have set clique order to 2 for
the transitions and 1 for all other features. We tune
the Gaussian prior variance on the development set
for all the experiments to obtain the best model for
testing.
Table 7 shows the comparison of results. On
DF alone, the improvement of the cross-product
LCRF over the mixed-label LCRF, and the im-
provement of the mixed-label LCRF over the
isolated baseline are not statistically significant.
However, if we test the statistical significance on
the overall performance of both PU and DF, both
the 2-layer FCRF and the cross-product LCRF
perform better than the mixed-label LCRF. And
we also obtain the same conclusion as Stolcke
et al. (1998) that mixed-label LCRF performs
better than isolated prediction. However, for the
comparison between the 2-layer FCRF and the
cross-product LCRF, although the 2-layer FCRF
performs better than the cross-product LCRF on
disfluency prediction, it does worse on punctua-
tion prediction. Overall, the two methods perform
about the same, their difference is not statistically
significant. In addition, both the 2-layer FCRF
and the cross-product LCRF slightly outperform
the soft cascade method (statistical significance at
p=0.04).
127
Experiment F1 for PU F1 for DF
isolated baseline 71.1 78.2
soft-cascade 71.6 79.6
rescore n=1 71.5 (72.5) 79.3 (81.1)
rescore n=2 71.2 (73.0) 79.3 (81.8)
rescore n=3 71.2 (73.3) 79.9 (82.6)
rescore n=4 71.2 (73.6) 79.8 (82.8)
rescore n=5 71.2 (73.9) 79.4 (83.3)
rescore n=6 71.1 (74.0) 79.6 (83.5)
rescore n=8 71.2 (74.2) 79.8 (84.0)
rescore n=10 * 71.2 (74.4) 79.8 (84.3)
rescore n=12 71.1 (74.5) 79.7 (84.6)
rescore n=15 71.2 (74.8) 79.8 (84.9)
rescore n=18 71.1 (74.9) 79.7 (85.1)
rescore n=25 70.7 (75.2) 79.3 (85.5)
Table 6: Performance comparison between the
rescoring method and the soft-cascade method
with respect to the baseline isolated prediction.
The rescoring is done on 2n
2
hypotheses. All
models are trained using M3N without balancing
precision and recall. Figures in the bracket are the
oracle F1 scores of the 2n
2
hypotheses. *:on the
development set, the best overall result is obtained
at n = 10.
7 Discussion
In this section, we will summarise our observa-
tions based on the empirical studies that we have
conducted in this paper.
Firstly, punctuation prediction and disfluency
prediction do influence each other. The output
from one task does provide useful information that
can improve the other task. All the approaches
studied in this work, which link the two tasks
together, perform better than their corresponding
Experiment F1 for PU F1 for DF
isolated baseline 68.7 77.0
soft cascade 69.0 77.5
mixed-label LCRF 69.0 77.2
cross-product LCRF 69.9 77.3
2-layer FCRF 69.2 77.8
Table 7: Performance comparison among 2-
layer FCRF, mixed-label LCRF and cross-product
LCRF, with respect to the soft-cascade and the iso-
lated prediction baseline. All models are trained
using GRMM (Sutton, 2006), with reduced clique
orders.
E  
Ref: it was n?t , you know , it was never announced . 
Token: it was n?t you know it was never announced 
PU: None  None  Comma  None  Comma  None  None  None  Perio d  
DF: E  E  E  F  F  O  O  O  O  
(a) 
Mixed-
label 
LCRF 
(b) 
Cross-
product 
LCRF 
(c) 
2-layer 
FCRF 
E  Period  E  F  F  O  O  O  
None  
E  
Period  
O  
None  
E  
C omma  
E  
None  
F  
C omma  
F  
None  
O  
None  
O  
None  
O  
None  Period  None  Comma  None  C omma  None  None  None  
E  O  E  E  F  F  O  O  O  
edit  filler  
x 1  x 2 x 9  x 3  x 4  x 5  x 6  x 7  x 8  
x 1  x 2 x 9  x 3  x 4  x 5  x 6  x 7  x 8  
x 1  x 2 x 9  x 3  x 4  x 5  x 6  x 7  x 8  
Figure 2: Illustration using (a) mixed-label LCRF;
(b) cross-product LCRF; and (c) 2-layer FCRF, for
joint punctuation (PU) and disfluency (DF) predic-
tion. Shaded nodes are observations and unshaded
nodes are variables to be predicted.
isolated prediction baseline.
Secondly, as compared to the soft cascade, the
hard cascade passes more information from the
first step into the second step, and thus is much
more sensitive to errors in the first step. In prac-
tice, unless the first step has very high accuracy,
soft cascade is expected to do better than hard cas-
cade.
Thirdly, if we train a model using a fine-grained
label set but test it on the same coarse-grained la-
bel set, we are very likely to get improvement. For
example:
? The edit word F1 for mixed edit and filler pre-
diction using {E, F, O} is better than that for
edit prediction using {E, O} (see the second
and third rows in Table 4). This is because the
former actually splits the O in the latter into
F and O. Thus, it has a finer label granularity.
? Disfluency prediction using mixed-label
LCRF (using label set {E, F, Comma, Pe-
riod, Question, None}) performs better than
that using isolated LCRF (using label set {E,
F, O}) (see the second and fourth rows in
Table 7). This is because the former dis-
128
tinguishes between different punctuations for
fluent tokens and thus has a finer label granu-
larity.
? Both the cross-product LCRF and 2-layer
FCRF perform better than mixed-label LCRF
because the former two distinguish between
different punctuations for edit, filler and flu-
ent tokens while the latter distinguishes be-
tween different punctuations only for fluent
tokens. Thus, the former has a much finer la-
bel granularity.
From the above comparisons, we can see that
increasing the label granularity can greatly im-
prove the accuracy of a model. However, this
may also increase the model complexity dramat-
ically, especially when higher clique order is used.
Although the joint approach (2-layer FCRF and
cross-product LCRF) are better than the soft-
cascade approach, they cannot be easily scaled up
to using higher order cliques, which greatly limits
their potential. In practice, the soft cascade ap-
proach offers a simpler and more efficient way to
achieve a joint prediction of punctuations and dis-
fluencies.
8 Conclusion
In general, punctuation prediction and disfluency
prediction can improve downstream NLP tasks.
Combining the two tasks can potentially improve
the efficiency of the overall framework and mini-
mize error propagation. In this work, we have car-
ried out an empirical study on the various methods
for combining the two tasks. Our results show that
the various methods linking the two tasks perform
better than the isolated prediction. This means
that punctuation prediction and disfluency predic-
tion do influence each other, and the prediction
outcome in one task can provide useful informa-
tion that helps to improve the other task. Specifi-
cally, we compare the cascade models and the joint
prediction models. For the cascade approach, we
show that soft cascade is less sensitive to predic-
tion errors in the first step, and thus performs bet-
ter than hard cascade. For joint model approach,
we show that, when clique order of one is used, all
the three joint model approaches perform signifi-
cantly better than the isolated prediction baseline.
Moreover, the 2-layer FCRF and the cross-product
LCRF perform slightly better than the mix-label
LCRF and the soft-cascade approach, suggesting
that modelling at a finer label granularity is po-
tentially beneficial. However, the soft cascade ap-
proach is more efficient than the joint approach
when a higher clique order is used.
Acknowledgments
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
References
Don Baron, Elizabeth Shriberg, and Andreas Stolcke.
2002. Automatic punctuation and disfluency detec-
tion in multi-party meetings using prosodic and lex-
ical cues. In Proc. of ICSLP.
Heidi Christensen, Yoshihiko Gotoh, and Steve Re-
nals. 2001. Punctuation annotation using statisti-
cal prosody models. In ISCA Tutorial and Research
Workshop (ITRW) on Prosody in Speech Recognition
and Understanding.
Benoit Favre, Ralph Grishman, Dustin Hillard, Heng
Ji, Dilek Hakkani-Tur, and Mari Ostendorf. 2008.
Punctuating speech for information extraction. In
Proc. of ICASSP.
Kallirroi Georgila. 2009. Using integer linear pro-
gramming for detecting speech disfluencies. In
Proc. of NAACL.
Jing Huang and Geoffrey Zweig. 2002. Maximum en-
tropy model for punctuation annotation from speech.
In Proc. of INTERSPEECH.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy-channel model of speech repairs. In
Proc. of ACL.
Joungbum Kim. 2004. Automatic detection of sen-
tence boundaries, disfluencies, and conversational
fillers in spontaneous speech. Master dissertation of
University of Washington.
Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and
Mary Harper. 2005. Using conditional random
fields for sentence boundary detection in speech. In
Proc. of ACL.
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Dustin
Hillard, Mari Ostendorf, and Mary Harper. 2006.
Enriching speech recognition with automatic detec-
tion of sentence boundaries and disfluencies. IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 14(5):1526?1540.
Wei Lu and Hwee Tou Ng. 2010. Better punctuation
prediction with dynamic conditional random fields.
In Proc. of EMNLP.
129
Sameer Maskey, Bowen Zhou, and Yuqing Gao. 2006.
A phrase-level machine translation approach for dis-
fluency detection using weighted finite state trans-
ducers. In Proc. of INTERSPEECH.
Hwee Tou Ng and Jin Kiat Low. 2004. Chi-
nese part-of-speech tagging: One-at-a-time or all-at-
once? Word-based or character-based? In Proc. of
EMNLP.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL.
Xian Qian and Yang Liu. 2013. Disfluency detec-
tion using multi-step stacked learning. In Proc. of
NAACL.
Guergana Savova, Joan Bachenko. 2003. Prosodic fea-
tures of four types of disfluencies. In ISCA Tuto-
rial and Research Workshop on Disfluency in Spon-
taneous Speech.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
CRFs based joint decoding method for cascaded seg-
mentation and labeling tasks. In Proc. of IJCAI.
Elizabeth Shriberg, Rebecca Bates and Andreas Stol-
cke. 1997. A prosody-only decision-tree model for
disfluency detection. In Proc. of Eurospeech.
Elizabeth Shriberg, Andreas Stolcke, Daniel Jurafsky,
Noah Coccaro, Marie Meteer, Rebecca Bates, Paul
Taylor, Klaus Ries, Rachel Martin, and Carol Van
Ess-Dykema. 1998. Can prosody aid the auto-
matic classification of dialog acts in conversational
speech? In Language and speech 41, no. 3-4: 443-
492.
Andreas Stolcke, Elizabeth Shriberg, Rebecca A.
Bates, Mari Ostendorf, Dilek Hakkani, Madelaine
Plauche, Gokhan Tur, and Yu Lu. 1998. Auto-
matic detection of sentence boundaries and disfluen-
cies based on recognized words. In Proc. of ICSLP.
Charles Sutton. 2006. GRMM: GRaphical Models in
Mallet. http://mallet.cs.umass.edu/grmm/
Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic conditional random
fields: factorized probabilistic models for labeling
and segmenting sequence data. In Journal of Ma-
chine Learning Research, 8: 693?723.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2004.
Max-margin Markov networks. In Proc. of NIPS.
Wen Wang, Gokhan Tur, Jing Zheng, and Necip Fazil
Ayan. 2010. Automatic disfluency removal for im-
proving spoken language translation. In Proc. of
ICASSP.
Xuancong Wang, Hwee Tou Ng, and Khe Chai Sim.
2012. Dynamic conditional random fields for joint
sentence boundary and punctuation prediction. In
Proc. of Interspeech.
Xuancong Wang, Hwee Tou Ng, and Khe Chai Sim.
2014. A beam-search decoder for disfluency detec-
tion. In Proc. of COLING.
Dongdong Zhang, Shuangzhi Wu, Nan Yang, and Mu
Li. 2013. Punctuation prediction with transition-
based parsing. In Proc. of ACL.
Simon Zwarts and Mark Johnson. 2011. The impact
of language models and loss functions on repair dis-
fluency detection. In Proc. of ACL.
130
